file_path,api_count,code
attacks/__init__.py,0,b'from .fast_gradient import *\nfrom .saliency_map import *\nfrom .deepfool import *\nfrom .cw import *\n'
attacks/cw.py,22,"b'import tensorflow as tf\n\n\n__all__ = [\'cw\']\n\n\ndef cw(model, x, y=None, eps=1.0, ord_=2, T=2,\n       optimizer=tf.train.AdamOptimizer(learning_rate=0.1), alpha=0.9,\n       min_prob=0, clip=(0.0, 1.0)):\n    """"""CarliniWagner (CW) attack.\n\n    Only CW-L2 and CW-Linf are implemented since I do not see the point of\n    embedding CW-L2 in CW-L1.  See https://arxiv.org/abs/1608.04644 for\n    details.\n\n    The idea of CW attack is to minimize a loss that comprises two parts: a)\n    the p-norm distance between the original image and the adversarial image,\n    and b) a term that encourages the incorrect classification of the\n    adversarial images.\n\n    Please note that CW is a optimization process, so it is tricky.  There are\n    lots of hyper-parameters to tune in order to get the best result.  The\n    binary search process for the best eps values is omitted here.  You could\n    do grid search to find the best parameter configuration, if you like.  I\n    demonstrate binary search for the best result in an example code.\n\n    :param model: The model wrapper.\n    :param x: The input clean sample, usually a placeholder.  NOTE that the\n              shape of x MUST be static, i.e., fixed when constructing the\n              graph.  This is because there are some variables that depends\n              upon this shape.\n    :param y: The target label.  Set to be the least-likely label when None.\n    :param eps: The scaling factor for the second penalty term.\n    :param ord_: The p-norm, 2 or inf.  Actually I only test whether it is 2\n        or not 2.\n    :param T: The temperature for sigmoid function.  In the original paper,\n              the author used (tanh(x)+1)/2 = sigmoid(2x), i.e., t=2.  During\n              our experiment, we found that this parameter also affects the\n              quality of generated adversarial samples.\n    :param optimizer: The optimizer used to minimize the CW loss.  Default to\n        be tf.AdamOptimizer with learning rate 0.1. Note the learning rate is\n        much larger than normal learning rate.\n    :param alpha: Used only in CW-L0.  The decreasing factor for the upper\n        bound of noise.\n    :param min_prob: The minimum confidence of adversarial examples.\n        Generally larger min_prob wil lresult in more noise.\n    :param clip: A tuple (clip_min, clip_max), which denotes the range of\n        values in x.\n\n    :return: A tuple (train_op, xadv, noise).  Run train_op for some epochs to\n             generate the adversarial image, then run xadv to get the final\n             adversarial image.  Noise is in the sigmoid-space instead of the\n             input space.  It is returned because we need to clear noise\n             before each batched attacks.\n    """"""\n    xshape = x.get_shape().as_list()\n    noise = tf.get_variable(\'noise\', xshape, tf.float32,\n                            initializer=tf.initializers.zeros)\n\n    # scale input to (0, 1)\n    x_scaled = (x - clip[0]) / (clip[1] - clip[0])\n\n    # change to sigmoid-space, clip to avoid overflow.\n    z = tf.clip_by_value(x_scaled, 1e-8, 1-1e-8)\n    xinv = tf.log(z / (1 - z)) / T\n\n    # add noise in sigmoid-space and map back to input domain\n    xadv = tf.sigmoid(T * (xinv + noise))\n    xadv = xadv * (clip[1] - clip[0]) + clip[0]\n\n    ybar, logits = model(xadv, logits=True)\n    ydim = ybar.get_shape().as_list()[1]\n\n    if y is not None:\n        y = tf.cond(tf.equal(tf.rank(y), 0),\n                    lambda: tf.fill([xshape[0]], y),\n                    lambda: tf.identity(y))\n    else:\n        # we set target to the least-likely label\n        y = tf.argmin(ybar, axis=1, output_type=tf.int32)\n\n    mask = tf.one_hot(y, ydim, on_value=0.0, off_value=float(\'inf\'))\n    yt = tf.reduce_max(logits - mask, axis=1)\n    yo = tf.reduce_max(logits, axis=1)\n\n    # encourage to classify to a wrong category\n    loss0 = tf.nn.relu(yo - yt + min_prob)\n\n    axis = list(range(1, len(xshape)))\n    ord_ = float(ord_)\n\n    # make sure the adversarial images are visually close\n    if 2 == ord_:\n        # CW-L2 Original paper uses the reduce_sum version.  These two\n        # implementation does not differ much.\n\n        # loss1 = tf.reduce_sum(tf.square(xadv-x), axis=axis)\n        loss1 = tf.reduce_mean(tf.square(xadv-x))\n    else:\n        # CW-Linf\n        tau0 = tf.fill([xshape[0]] + [1]*len(axis), clip[1])\n        tau = tf.get_variable(\'cw8-noise-upperbound\', dtype=tf.float32,\n                              initializer=tau0, trainable=False)\n        diff = xadv - x - tau\n\n        # if all values are smaller than the upper bound value tau, we reduce\n        # this value via tau*0.9 to make sure L-inf does not get stuck.\n        tau = alpha * tf.to_float(tf.reduce_all(diff < 0, axis=axis))\n        loss1 = tf.nn.relu(tf.reduce_sum(diff, axis=axis))\n\n    loss = eps*loss0 + loss1\n    train_op = optimizer.minimize(loss, var_list=[noise])\n\n    # We may need to update tau after each iteration.  Refer to the CW-Linf\n    # section in the original paper.\n    if 2 != ord_:\n        train_op = tf.group(train_op, tau)\n\n    return train_op, xadv, noise\n'"
attacks/deepfool.py,63,"b'import tensorflow as tf\n\n\n__all__ = [\'deepfool\']\n\n\ndef deepfool(model, x, noise=False, eta=0.01, epochs=3, batch=False,\n             clip_min=0.0, clip_max=1.0, min_prob=0.0):\n    """"""DeepFool implementation in Tensorflow.\n\n    The original DeepFool will stop whenever we successfully cross the\n    decision boundary.  Thus it might not run total epochs.  In order to force\n    DeepFool to run full epochs, you could set batch=True.  In that case the\n    DeepFool will run until the max epochs is reached regardless whether we\n    cross the boundary or not.  See https://arxiv.org/abs/1511.04599 for\n    details.\n\n    :param model: Model function.\n    :param x: 2D or 4D input tensor.\n    :param noise: Also return the noise if True.\n    :param eta: Small overshoot value to cross the boundary.\n    :param epochs: Maximum epochs to run.\n    :param batch: If True, run in batch mode, will always run epochs.\n    :param clip_min: Min clip value for output.\n    :param clip_max: Max clip value for output.\n    :param min_prob: Minimum probability for adversarial samples.\n\n    :return: Adversarials, of the same shape as x.\n    """"""\n    y = tf.stop_gradient(model(x))\n\n    fns = [[_deepfool2, _deepfool2_batch], [_deepfoolx, _deepfoolx_batch]]\n\n    i = int(y.get_shape().as_list()[1] > 1)\n    j = int(batch)\n    fn = fns[i][j]\n\n    if batch:\n        delta = fn(model, x, eta=eta, epochs=epochs, clip_min=clip_min,\n                   clip_max=clip_max)\n    else:\n        def _f(xi):\n            xi = tf.expand_dims(xi, axis=0)\n            z = fn(model, xi, eta=eta, epochs=epochs, clip_min=clip_min,\n                   clip_max=clip_max, min_prob=min_prob)\n            return z[0]\n\n        delta = tf.map_fn(_f, x, dtype=(tf.float32), back_prop=False,\n                          name=\'deepfool\')\n\n    if noise:\n        return delta\n\n    xadv = tf.stop_gradient(x + delta*(1+eta))\n    xadv = tf.clip_by_value(xadv, clip_min, clip_max)\n    return xadv\n\n\ndef _prod(iterable):\n    ret = 1\n    for x in iterable:\n        ret *= x\n    return ret\n\n\ndef _deepfool2(model, x, epochs, eta, clip_min, clip_max, min_prob):\n    """"""DeepFool for binary classifiers.\n\n    Note that DeepFools that binary classifier outputs +1/-1 instead of 0/1.\n    """"""\n    y0 = tf.stop_gradient(tf.reshape(model(x), [-1])[0])\n    y0 = tf.to_int32(tf.greater(y0, 0.0))\n\n    def _cond(i, z):\n        xadv = tf.clip_by_value(x + z*(1+eta), clip_min, clip_max)\n        y = tf.stop_gradient(tf.reshape(model(xadv), [-1])[0])\n        y = tf.to_int32(tf.greater(y, 0.0))\n        return tf.logical_and(tf.less(i, epochs), tf.equal(y0, y))\n\n    def _body(i, z):\n        xadv = tf.clip_by_value(x + z*(1+eta), clip_min, clip_max)\n        y = tf.reshape(model(xadv), [-1])[0]\n        g = tf.gradients(y, xadv)[0]\n        dx = - y * g / (tf.norm(g) + 1e-10)  # off by a factor of 1/norm(g)\n        return i+1, z+dx\n\n    _, noise = tf.while_loop(_cond, _body, [0, tf.zeros_like(x)],\n                             name=\'_deepfool2\', back_prop=False)\n    return noise\n\n\ndef _deepfool2_batch(model, x, epochs, eta, clip_min, clip_max):\n    """"""DeepFool for binary classifiers in batch mode.\n    """"""\n    xshape = x.get_shape().as_list()[1:]\n    dim = _prod(xshape)\n\n    def _cond(i, z):\n        return tf.less(i, epochs)\n\n    def _body(i, z):\n        xadv = tf.clip_by_value(x + z*(1+eta), clip_min, clip_max)\n        y = tf.reshape(model(xadv), [-1])\n        g = tf.gradients(y, xadv)[0]\n        n = tf.norm(tf.reshape(g, [-1, dim]), axis=1) + 1e-10\n        d = tf.reshape(-y / n, [-1] + [1]*len(xshape))\n        dx = g * d\n        return i+1, z+dx\n\n    _, noise = tf.while_loop(_cond, _body, [0, tf.zeros_like(x)],\n                             name=\'_deepfool2_batch\', back_prop=False)\n    return noise\n\n\ndef _deepfoolx(model, x, epochs, eta, clip_min, clip_max, min_prob):\n    """"""DeepFool for multi-class classifiers.\n\n    Assumes that the final label is the label with the maximum values.\n    """"""\n    y0 = tf.stop_gradient(model(x))\n    y0 = tf.reshape(y0, [-1])\n    k0 = tf.argmax(y0)\n\n    ydim = y0.get_shape().as_list()[0]\n    xdim = x.get_shape().as_list()[1:]\n    xflat = _prod(xdim)\n\n    def _cond(i, z):\n        xadv = tf.clip_by_value(x + z*(1+eta), clip_min, clip_max)\n        y = tf.reshape(model(xadv), [-1])\n        p = tf.reduce_max(y)\n        k = tf.argmax(y)\n        return tf.logical_and(tf.less(i, epochs),\n                              tf.logical_or(tf.equal(k0, k),\n                                            tf.less(p, min_prob)))\n\n    def _body(i, z):\n        xadv = tf.clip_by_value(x + z*(1+eta), clip_min, clip_max)\n        y = tf.reshape(model(xadv), [-1])\n\n        gs = [tf.reshape(tf.gradients(y[i], xadv)[0], [-1])\n              for i in range(ydim)]\n        g = tf.stack(gs, axis=0)\n\n        yk, yo = y[k0], tf.concat((y[:k0], y[(k0+1):]), axis=0)\n        gk, go = g[k0], tf.concat((g[:k0], g[(k0+1):]), axis=0)\n\n        yo.set_shape(ydim - 1)\n        go.set_shape([ydim - 1, xflat])\n\n        a = tf.abs(yo - yk)\n        b = go - gk\n        c = tf.norm(b, axis=1)\n        score = a / c\n        ind = tf.argmin(score)\n\n        si, bi = score[ind], b[ind]\n        dx = si * bi\n        dx = tf.reshape(dx, [-1] + xdim)\n        return i+1, z+dx\n\n    _, noise = tf.while_loop(_cond, _body, [0, tf.zeros_like(x)],\n                             name=\'_deepfoolx\', back_prop=False)\n    return noise\n\n\ndef _deepfoolx_batch(model, x, epochs, eta, clip_min, clip_max):\n    """"""DeepFool for multi-class classifiers in batch mode.\n    """"""\n    y0 = tf.stop_gradient(model(x))\n    B, ydim = tf.shape(y0)[0], y0.get_shape().as_list()[1]\n\n    k0 = tf.argmax(y0, axis=1, output_type=tf.int32)\n    k0 = tf.stack((tf.range(B), k0), axis=1)\n\n    xshape = x.get_shape().as_list()[1:]\n    xdim = _prod(xshape)\n\n    perm = list(range(len(xshape) + 2))\n    perm[0], perm[1] = perm[1], perm[0]\n\n    def _cond(i, z):\n        return tf.less(i, epochs)\n\n    def _body(i, z):\n        xadv = tf.clip_by_value(x + z*(1+eta), clip_min, clip_max)\n        y = model(xadv)\n\n        gs = [tf.gradients(y[:, i], xadv)[0] for i in range(ydim)]\n        g = tf.stack(gs, axis=0)\n        g = tf.transpose(g, perm)\n\n        yk = tf.expand_dims(tf.gather_nd(y, k0), axis=1)\n        gk = tf.expand_dims(tf.gather_nd(g, k0), axis=1)\n\n        a = tf.abs(y - yk)\n        b = g - gk\n        c = tf.norm(tf.reshape(b, [-1, ydim, xdim]), axis=-1)\n\n        # Assume 1) 0/0=tf.nan 2) tf.argmin ignores nan\n        score = a / c\n\n        ind = tf.argmin(score, axis=1, output_type=tf.int32)\n        ind = tf.stack((tf.range(B), ind), axis=1)\n\n        si, bi = tf.gather_nd(score, ind), tf.gather_nd(b, ind)\n        si = tf.reshape(si, [-1] + [1]*len(xshape))\n        dx = si * bi\n        return i+1, z+dx\n\n    _, noise = tf.while_loop(_cond, _body, [0, tf.zeros_like(x)],\n                             name=\'_deepfoolx_batch\', back_prop=False)\n    return noise\n'"
attacks/fast_gradient.py,35,"b'import tensorflow as tf\n\n\n__all__ = [\n    \'fgm\',                      # fast gradient method\n    \'fgmt\'                      # fast gradient method with target\n]\n\n\ndef fgm(model, x, eps=0.01, epochs=1, sign=True, clip_min=0., clip_max=1.):\n    """"""\n    Fast gradient method.\n\n    See https://arxiv.org/abs/1412.6572 and https://arxiv.org/abs/1607.02533\n    for details.  This implements the revised version since the original FGM\n    has label leaking problem (https://arxiv.org/abs/1611.01236).\n\n    :param model: A wrapper that returns the output as well as logits.\n    :param x: The input placeholder.\n    :param eps: The scale factor for noise.\n    :param epochs: The maximum epoch to run.\n    :param sign: Use gradient sign if True, otherwise use gradient value.\n    :param clip_min: The minimum value in output.\n    :param clip_max: The maximum value in output.\n\n    :return: A tensor, contains adversarial samples for each input.\n    """"""\n    xadv = tf.identity(x)\n\n    ybar = model(xadv)\n    yshape = ybar.get_shape().as_list()\n    ydim = yshape[1]\n\n    indices = tf.argmax(ybar, axis=1)\n    target = tf.cond(\n        tf.equal(ydim, 1),\n        lambda: tf.nn.relu(tf.sign(ybar - 0.5)),\n        lambda: tf.one_hot(indices, ydim, on_value=1.0, off_value=0.0))\n\n    if 1 == ydim:\n        loss_fn = tf.nn.sigmoid_cross_entropy_with_logits\n    else:\n        loss_fn = tf.nn.softmax_cross_entropy_with_logits\n\n    if sign:\n        noise_fn = tf.sign\n    else:\n        noise_fn = tf.identity\n\n    eps = tf.abs(eps)\n\n    def _cond(xadv, i):\n        return tf.less(i, epochs)\n\n    def _body(xadv, i):\n        ybar, logits = model(xadv, logits=True)\n        loss = loss_fn(labels=target, logits=logits)\n        dy_dx, = tf.gradients(loss, xadv)\n        xadv = tf.stop_gradient(xadv + eps*noise_fn(dy_dx))\n        xadv = tf.clip_by_value(xadv, clip_min, clip_max)\n        return xadv, i+1\n\n    xadv, _ = tf.while_loop(_cond, _body, (xadv, 0), back_prop=False,\n                            name=\'fast_gradient\')\n    return xadv\n\n\ndef fgmt(model, x, y=None, eps=0.01, epochs=1, sign=True, clip_min=0.,\n         clip_max=1.):\n    """"""\n    Fast gradient method with target\n\n    See https://arxiv.org/pdf/1607.02533.pdf.  This method is different from\n    FGM that instead of decreasing the probability for the correct label, it\n    increases the probability for the desired label.\n\n    :param model: A model that returns the output as well as logits.\n    :param x: The input placeholder.\n    :param y: The desired target label, set to the least-likely class if None.\n    :param eps: The noise scale factor.\n    :param epochs: Maximum epoch to run.\n    :param sign: Use gradient sign if True, otherwise gradient values.\n    :param clip_min: Minimum value in output.\n    :param clip_max: Maximum value in output.\n    """"""\n    xadv = tf.identity(x)\n\n    ybar = model(xadv)\n    ydim = ybar.get_shape().as_list()[1]\n    n = tf.shape(ybar)[0]\n\n    if y is None:\n        indices = tf.argmin(ybar, axis=1)\n    else:\n        indices = tf.cond(tf.equal(0, tf.rank(y)),\n                          lambda: tf.zeros([n], dtype=tf.int32) + y,\n                          lambda: tf.zeros([n], dtype=tf.int32))\n\n    target = tf.cond(\n        tf.equal(ydim, 1),\n        lambda: 1 - ybar,\n        lambda: tf.one_hot(indices, ydim, on_value=1.0, off_value=0.0))\n\n    if 1 == ydim:\n        loss_fn = tf.nn.sigmoid_cross_entropy_with_logits\n    else:\n        loss_fn = tf.nn.softmax_cross_entropy_with_logits\n\n    if sign:\n        noise_fn = tf.sign\n    else:\n        noise_fn = tf.identity\n\n    eps = -tf.abs(eps)\n\n    def _cond(xadv, i):\n        return tf.less(i, epochs)\n\n    def _body(xadv, i):\n        ybar, logits = model(xadv, logits=True)\n        loss = loss_fn(labels=target, logits=logits)\n        dy_dx, = tf.gradients(loss, xadv)\n        xadv = tf.stop_gradient(xadv + eps*noise_fn(dy_dx))\n        xadv = tf.clip_by_value(xadv, clip_min, clip_max)\n        return xadv, i+1\n\n    xadv, _ = tf.while_loop(_cond, _body, (xadv, 0), back_prop=False,\n                            name=\'fast_gradient_target\')\n    return xadv\n'"
attacks/saliency_map.py,43,"b'import tensorflow as tf\n\n\n__all__ = [\'jsma\']\n\n\ndef jsma(model, x, y=None, epochs=1, eps=1.0, k=1, clip_min=0.0, clip_max=1.0,\n         score_fn=lambda t, o: t * tf.abs(o)):\n    """"""\n    Jacobian-based saliency map approach.\n\n    See https://arxiv.org/abs/1511.07528 for details.  During each iteration,\n    this method finds the pixel (or two pixels) that has the most influence on\n    the result (most salient pixel) and add noise to the pixel.\n\n    :param model: A wrapper that returns the output tensor of the model.\n    :param x: The input placeholder a 2D or 4D tensor.\n    :param y: The desired class label for each input, either an integer or a\n              list of integers.\n    :param epochs: Maximum epochs to run.  If it is a floating number in [0,\n        1], it is treated as the distortion factor, i.e., gamma in the\n        original paper.\n    :param eps: The noise added to input per epoch.\n    :param k: number of pixels to perturb at a time.  Values other than 1 and\n              2 will raise an error.\n    :param clip_min: The minimum value in output tensor.\n    :param clip_max: The maximum value in output tensor.\n    :param score_fn: Function to calculate the saliency score.\n\n    :return: A tensor, contains adversarial samples for each input.\n    """"""\n    n = tf.shape(x)[0]\n\n    target = tf.cond(tf.equal(0, tf.rank(y)),\n                     lambda: tf.zeros([n], dtype=tf.int32) + y,\n                     lambda: y)\n    target = tf.stack((tf.range(n), target), axis=1)\n\n    if isinstance(epochs, float):\n        tmp = tf.to_float(tf.size(x[0])) * epochs\n        epochs = tf.to_int32(tf.floor(tmp))\n\n    if 2 == k:\n        _jsma_fn = _jsma2_impl\n    else:\n        _jsma_fn = _jsma_impl\n\n    return _jsma_fn(model, x, target, epochs=epochs, eps=eps,\n                    clip_min=clip_min, clip_max=clip_max, score_fn=score_fn)\n\n\ndef _prod(iterable):\n    ret = 1\n    for x in iterable:\n        ret *= x\n    return ret\n\n\ndef _jsma_impl(model, x, yind, epochs, eps, clip_min, clip_max, score_fn):\n\n    def _cond(i, xadv):\n        return tf.less(i, epochs)\n\n    def _body(i, xadv):\n        ybar = model(xadv)\n\n        dy_dx = tf.gradients(ybar, xadv)[0]\n\n        # gradients of target w.r.t input\n        yt = tf.gather_nd(ybar, yind)\n        dt_dx = tf.gradients(yt, xadv)[0]\n\n        # gradients of non-targets w.r.t input\n        do_dx = dy_dx - dt_dx\n\n        c0 = tf.logical_or(eps < 0, xadv < clip_max)\n        c1 = tf.logical_or(eps > 0, xadv > clip_min)\n        cond = tf.reduce_all([dt_dx >= 0, do_dx <= 0, c0, c1], axis=0)\n        cond = tf.to_float(cond)\n\n        # saliency score for each pixel\n        score = cond * score_fn(dt_dx, do_dx)\n\n        shape = score.get_shape().as_list()\n        dim = _prod(shape[1:])\n        score = tf.reshape(score, [-1, dim])\n\n        # find the pixel with the highest saliency score\n        ind = tf.argmax(score, axis=1)\n        dx = tf.one_hot(ind, dim, on_value=eps, off_value=0.0)\n        dx = tf.reshape(dx, [-1] + shape[1:])\n\n        xadv = tf.stop_gradient(xadv + dx)\n        xadv = tf.clip_by_value(xadv, clip_min, clip_max)\n\n        return i+1, xadv\n\n    _, xadv = tf.while_loop(_cond, _body, (0, tf.identity(x)),\n                            back_prop=False, name=\'_jsma_batch\')\n\n    return xadv\n\n\ndef _jsma2_impl(model, x, yind, epochs, eps, clip_min, clip_max, score_fn):\n\n    def _cond(k, xadv):\n        return tf.less(k, epochs)\n\n    def _body(k, xadv):\n        ybar = model(xadv)\n\n        dy_dx = tf.gradients(ybar, xadv)[0]\n\n        # gradients of target w.r.t input\n        yt = tf.gather_nd(ybar, yind)\n        dt_dx = tf.gradients(yt, xadv)[0]\n\n        # gradients of non-targets w.r.t input\n        do_dx = dy_dx - dt_dx\n\n        c0 = tf.logical_or(eps < 0, xadv < clip_max)\n        c1 = tf.logical_or(eps > 0, xadv > clip_min)\n        cond = tf.reduce_all([dt_dx >= 0, do_dx <= 0, c0, c1], axis=0)\n        cond = tf.to_float(cond)\n\n        # saliency score for each pixel\n        score = cond * score_fn(dt_dx, do_dx)\n\n        shape = score.get_shape().as_list()\n        dim = _prod(shape[1:])\n        score = tf.reshape(score, [-1, dim])\n\n        a = tf.expand_dims(score, axis=1)\n        b = tf.expand_dims(score, axis=2)\n        score2 = tf.reshape(a + b, [-1, dim*dim])\n        ij = tf.argmax(score2, axis=1)\n\n        i = tf.to_int32(ij / dim)\n        j = tf.to_int32(ij) % dim\n\n        dxi = tf.one_hot(i, dim, on_value=eps, off_value=0.0)\n        dxj = tf.one_hot(j, dim, on_value=eps, off_value=0.0)\n        dx = tf.reshape(dxi + dxj, [-1] + shape[1:])\n\n        xadv = tf.stop_gradient(xadv + dx)\n        xadv = tf.clip_by_value(xadv, clip_min, clip_max)\n\n        return k+1, xadv\n\n    _, xadv = tf.while_loop(_cond, _body, (0, tf.identity(x)),\n                            back_prop=False, name=\'_jsma2_batch\')\n    return xadv\n'"
example/compare_all.py,37,"b'""""""\nCode to visualize noise of all adversarial algorithm.\n""""""\nimport os\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import fgm, jsma, deepfool\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nimg_size = 28\nimg_chan = 1\nn_classes = 10\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nto_categorical = tf.keras.utils.to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=10, name=\'logits\')\n    y = tf.nn.softmax(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\'):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, n_classes), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar, logits = model(env.x, logits=True, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    with tf.variable_scope(\'loss\'):\n        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n                                                       logits=logits)\n        env.loss = tf.reduce_mean(xent, name=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        env.train_op = optimizer.minimize(env.loss)\n\n    env.saver = tf.train.Saver()\n\nwith tf.variable_scope(\'model\', reuse=True):\n    env.adv_eps = tf.placeholder(tf.float32, (), name=\'adv_eps\')\n    env.adv_epochs = tf.placeholder(tf.int32, (), name=\'adv_epochs\')\n    env.adv_y = tf.placeholder(tf.int32, (), name=\'adv_y\')\n\n    env.x_fgsm = fgm(model, env.x, epochs=env.adv_epochs, eps=env.adv_eps)\n    env.x_deepfool = deepfool(model, env.x, epochs=env.adv_epochs, batch=True)\n    env.x_jsma = jsma(model, env.x, env.adv_y, eps=env.adv_eps,\n                      epochs=env.adv_epochs)\n\n\nprint(\'\\nInitializing graph\')\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\n\ndef evaluate(sess, env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(sess, env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                              env.y: y_data[start:end],\n                                              env.training: True})\n        if X_valid is not None:\n            evaluate(sess, env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(sess, \'model/{}\'.format(name))\n\n\ndef predict(sess, env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = sess.run(env.ybar, feed_dict={env.x: X_data[start:end]})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef make_fgsm(sess, env, X_data, epochs=1, eps=0.01, batch_size=128):\n    print(\'\\nMaking adversarials via FGSM\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        feed_dict = {env.x: X_data[start:end], env.adv_eps: eps,\n                     env.adv_epochs: epochs}\n        adv = sess.run(env.x_fgsm, feed_dict=feed_dict)\n        X_adv[start:end] = adv\n    print()\n\n    return X_adv\n\n\ndef make_jsma(sess, env, X_data, epochs=0.2, eps=1.0, batch_size=128):\n    print(\'\\nMaking adversarials via JSMA\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        feed_dict = {\n            env.x: X_data[start:end],\n            env.adv_y: np.random.choice(n_classes),\n            env.adv_epochs: epochs,\n            env.adv_eps: eps}\n        adv = sess.run(env.x_jsma, feed_dict=feed_dict)\n        X_adv[start:end] = adv\n    print()\n\n    return X_adv\n\n\ndef make_deepfool(sess, env, X_data, epochs=1, eps=0.01, batch_size=128):\n    print(\'\\nMaking adversarials via FGSM\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        feed_dict = {env.x: X_data[start:end], env.adv_epochs: epochs}\n        adv = sess.run(env.x_deepfool, feed_dict=feed_dict)\n        X_adv[start:end] = adv\n    print()\n\n    return X_adv\n\n\nprint(\'\\nTraining\')\n\ntrain(sess, env, X_train, y_train, X_valid, y_valid, load=True, epochs=5,\n      name=\'mnist\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(sess, env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial data\')\n\nprint(\'\\nRandomly sample adversarial data from each category\')\n\nwhile True:\n    ind = np.random.choice(X_test.shape[0])\n    xorg, y0 = X_test[ind], y_test[ind]\n\n    xorg = np.expand_dims(xorg, axis=0)\n    z0 = np.argmax(y0)\n    z1 = np.argmax(predict(sess, env, xorg))\n\n    if z0 != z1:\n        continue\n\n    xadvs = [make_fgsm(sess, env, xorg, eps=0.02, epochs=10),\n             make_jsma(sess, env, xorg, eps=0.5, epochs=40),\n             make_deepfool(sess, env, xorg, epochs=1)]\n    y2 = [predict(sess, env, xi).flatten() for xi in xadvs]\n    p2 = [np.max(yi) for yi in y2]\n    z2 = [np.argmax(yi) for yi in y2]\n\n    if np.all([z0 != z2]):\n        break\n\nfig = plt.figure(figsize=(4.2, 2.2))\ngs = gridspec.GridSpec(2, 5, width_ratios=[1, 1, 1, 1, 0.1], wspace=0.01,\n                       hspace=0.01)\nlabel = [\'Clean\', \'FGM\', \'JSMA\', \'DeepFool\']\n\nxorg = np.squeeze(xorg)\nxadvs = [xorg] + xadvs\nxadvs = [np.squeeze(e) for e in xadvs]\n\np2 = [np.max(y0)] + p2\nz2 = [z0] + z2\n\nfor i in range(len(label)):\n    x = xadvs[i]\n\n    ax = fig.add_subplot(gs[0, i])\n    ax.imshow(x, cmap=\'gray\', interpolation=\'none\')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    ax.set_xlabel(label[i])\n    ax.xaxis.set_label_position(\'top\')\n\n    ax = fig.add_subplot(gs[1, i])\n    img = ax.imshow(x-xorg, cmap=\'RdBu_r\', vmin=-1, vmax=1,\n                    interpolation=\'none\')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    ax.set_xlabel(\'{0} ({1:.2f})\'.format(z2[i], p2[i]), fontsize=12)\n\nax = fig.add_subplot(gs[1, 4])\ndummy = plt.cm.ScalarMappable(cmap=\'RdBu_r\',\n                              norm=plt.Normalize(vmin=-1, vmax=1))\ndummy.set_array([])\nfig.colorbar(mappable=dummy, cax=ax, ticks=[-1, 0, 1], ticklocation=\'right\')\n\nprint(\'\\nSaving figure\')\n\ngs.tight_layout(fig)\nos.makedirs(\'../out\', exist_ok=True)\nplt.savefig(\'../out/compare.png\')\n'"
example/cw2_mnist.py,39,"b'""""""\nUse CW method to craft adversarial on MNIST.\n\nNote that instead of find the optimized image for each image, we do a batched\nattack without binary search for the best possible solution.  Thus, the result\nis worse than reported in the original paper.  To achieve the best result\nrequires more computation, as demonstrated in another example.\n""""""\nimport os\nfrom timeit import default_timer\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import cw\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nimg_size = 28\nimg_chan = 1\nn_classes = 10\nbatch_size = 32\n\n\nclass Timer(object):\n    def __init__(self, msg=\'Starting.....\', timer=default_timer, factor=1,\n                 fmt=""------- elapsed {:.4f}s --------""):\n        self.timer = timer\n        self.factor = factor\n        self.fmt = fmt\n        self.end = None\n        self.msg = msg\n\n    def __call__(self):\n        """"""\n        Return the current time\n        """"""\n        return self.timer()\n\n    def __enter__(self):\n        """"""\n        Set the start time\n        """"""\n        print(self.msg)\n        self.start = self()\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        """"""\n        Set the end time\n        """"""\n        self.end = self()\n        print(str(self))\n\n    def __repr__(self):\n        return self.fmt.format(self.elapsed)\n\n    @property\n    def elapsed(self):\n        if self.end is None:\n            # if elapsed is called in the context manager scope\n            return (self() - self.start) * self.factor\n        else:\n            # if elapsed is called out of the context manager scope\n            return (self.end - self.start) * self.factor\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nto_categorical = tf.keras.utils.to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=10, name=\'logits\')\n    y = tf.nn.softmax(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\', reuse=tf.AUTO_REUSE):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, n_classes), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar, logits = model(env.x, logits=True, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    with tf.variable_scope(\'loss\'):\n        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n                                                       logits=logits)\n        env.loss = tf.reduce_mean(xent, name=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        vs = tf.global_variables()\n        env.train_op = optimizer.minimize(env.loss, var_list=vs)\n\n    env.saver = tf.train.Saver()\n\n    # Note here that the shape has to be fixed during the graph construction\n    # since the internal variable depends upon the shape.\n    env.x_fixed = tf.placeholder(\n        tf.float32, (batch_size, img_size, img_size, img_chan),\n        name=\'x_fixed\')\n    env.adv_eps = tf.placeholder(tf.float32, (), name=\'adv_eps\')\n    env.adv_y = tf.placeholder(tf.int32, (), name=\'adv_y\')\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n    env.adv_train_op, env.xadv, env.noise = cw(model, env.x_fixed,\n                                               y=env.adv_y, eps=env.adv_eps,\n                                               optimizer=optimizer)\n\nprint(\'\\nInitializing graph\')\n\nenv.sess = tf.InteractiveSession()\nenv.sess.run(tf.global_variables_initializer())\nenv.sess.run(tf.local_variables_initializer())\n\n\ndef evaluate(env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = env.sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(env.sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            env.sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                                  env.y: y_data[start:end],\n                                                  env.training: True})\n        if X_valid is not None:\n            evaluate(env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(env.sess, \'model/{}\'.format(name))\n\n\ndef predict(env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = env.sess.run(env.ybar, feed_dict={env.x: X_data[start:end]})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef make_cw(env, X_data, epochs=1, eps=0.1, batch_size=batch_size):\n    """"""\n    Generate adversarial via CW optimization.\n    """"""\n    print(\'\\nMaking adversarials via CW\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        with Timer(\'Batch {0}/{1}   \'.format(batch + 1, n_batch)):\n            end = min(n_sample, (batch+1) * batch_size)\n            start = end - batch_size\n            feed_dict = {\n                env.x_fixed: X_data[start:end],\n                env.adv_eps: eps,\n                env.adv_y: np.random.choice(n_classes)}\n\n            # reset the noise before every iteration\n            env.sess.run(env.noise.initializer)\n            for epoch in range(epochs):\n                env.sess.run(env.adv_train_op, feed_dict=feed_dict)\n\n            xadv = env.sess.run(env.xadv, feed_dict=feed_dict)\n            X_adv[start:end] = xadv\n\n    return X_adv\n\n\nprint(\'\\nTraining\')\n\ntrain(env, X_train, y_train, X_valid, y_valid, load=False, epochs=5,\n      name=\'mnist\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial data\')\n\n# It takes a while to run through the full dataset, thus, we demo the result\n# through a smaller dataset.  We could actually find the best parameter\n# configuration on a smaller dataset, and then apply to the full dataset.\nn_sample = 128\nind = np.random.choice(X_test.shape[0], size=n_sample, replace=False)\nX_test = X_test[ind]\ny_test = y_test[ind]\n\nX_adv = make_cw(env, X_test, eps=0.002, epochs=100)\n\nprint(\'\\nEvaluating on adversarial data\')\n\nevaluate(env, X_adv, y_test)\n\nprint(\'\\nRandomly sample adversarial data from each category\')\n\ny1 = predict(env, X_test)\ny2 = predict(env, X_adv)\n\nz0 = np.argmax(y_test, axis=1)\nz1 = np.argmax(y1, axis=1)\nz2 = np.argmax(y2, axis=1)\n\nind = np.logical_and(z0 == z1, z1 != z2)\n# print(\'success: \', np.sum(ind))\n\nind = z0 == z1\n\nX_test = X_test[ind]\nX_adv = X_adv[ind]\nz1 = z1[ind]\nz2 = z2[ind]\ny2 = y2[ind]\n\nind, = np.where(z1 != z2)\ncur = np.random.choice(ind, size=n_classes)\nX_org = np.squeeze(X_test[cur])\nX_tmp = np.squeeze(X_adv[cur])\ny_tmp = y2[cur]\n\nfig = plt.figure(figsize=(n_classes+0.2, 3.2))\ngs = gridspec.GridSpec(3, n_classes+1, width_ratios=[1]*n_classes + [0.1],\n                       wspace=0.01, hspace=0.01)\n\nlabel = np.argmax(y_tmp, axis=1)\nproba = np.max(y_tmp, axis=1)\n\nfor i in range(n_classes):\n    ax = fig.add_subplot(gs[0, i])\n    ax.imshow(X_org[i], cmap=\'gray\', interpolation=\'none\')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    ax = fig.add_subplot(gs[1, i])\n    img = ax.imshow(X_tmp[i]-X_org[i], cmap=\'RdBu_r\', vmin=-1,\n                    vmax=1, interpolation=\'none\')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    ax = fig.add_subplot(gs[2, i])\n    ax.imshow(X_tmp[i], cmap=\'gray\', interpolation=\'none\')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    ax.set_xlabel(\'{0} ({1:.2f})\'.format(label[i], proba[i]), fontsize=12)\n\nax = fig.add_subplot(gs[1, n_classes])\ndummy = plt.cm.ScalarMappable(cmap=\'RdBu_r\', norm=plt.Normalize(vmin=-1,\n                                                                vmax=1))\ndummy.set_array([])\nfig.colorbar(mappable=dummy, cax=ax, ticks=[-1, 0, 1], ticklocation=\'right\')\n\nprint(\'\\nSaving figure\')\n\ngs.tight_layout(fig)\nos.makedirs(\'img\', exist_ok=True)\nplt.savefig(\'img/cw2_mnist.png\')\n'"
example/cw2_mnist_binary_search.py,39,"b'""""""\nCW attack with binary search for best result.\n\nThe CW loss is calculated as loss0+eps*loss1.  A randomly select eps value\nwill not yeild the best result in general.  The author uses binary search to\nfind the best one.  And each image does need a slightly different eps value.\nNote that this is time consuming if you want to do this over the entire\ndataset.\n\nI create this demo mainly to demonstrate the correctness of my implementation\nof CW.  Since I do not get very good results with hand-chosen eps value and\nbatched attacking.\n""""""\nimport os\nfrom timeit import default_timer\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import cw\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nimg_size = 28\nimg_chan = 1\nn_classes = 10\nbatch_size = 1\n\nLEARNING_RATE = 1e-2\nBINARY_EPOCHS = 10\nEPOCHS = 1000\nBOUND = (1e-6, 1)\n\n\nclass Timer(object):\n    def __init__(self, msg=\'Starting.....\', timer=default_timer, factor=1,\n                 fmt=""------- elapsed {:.4f}s --------""):\n        self.timer = timer\n        self.factor = factor\n        self.fmt = fmt\n        self.end = None\n        self.msg = msg\n\n    def __call__(self):\n        """"""\n        Return the current time\n        """"""\n        return self.timer()\n\n    def __enter__(self):\n        """"""\n        Set the start time\n        """"""\n        print(self.msg)\n        self.start = self()\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        """"""\n        Set the end time\n        """"""\n        self.end = self()\n        print(str(self))\n\n    def __repr__(self):\n        return self.fmt.format(self.elapsed)\n\n    @property\n    def elapsed(self):\n        if self.end is None:\n            # if elapsed is called in the context manager scope\n            return (self() - self.start) * self.factor\n        else:\n            # if elapsed is called out of the context manager scope\n            return (self.end - self.start) * self.factor\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nto_categorical = tf.keras.utils.to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=10, name=\'logits\')\n    y = tf.nn.softmax(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\', reuse=tf.AUTO_REUSE):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, n_classes), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar, logits = model(env.x, logits=True, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    with tf.variable_scope(\'loss\'):\n        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n                                                       logits=logits)\n        env.loss = tf.reduce_mean(xent, name=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        vs = tf.global_variables()\n        env.train_op = optimizer.minimize(env.loss, var_list=vs)\n\n    env.saver = tf.train.Saver()\n\n    # Note here that the shape has to be fixed during the graph construction\n    # since the internal variable depends upon the shape.\n    env.x_fixed = tf.placeholder(\n        tf.float32, (batch_size, img_size, img_size, img_chan),\n        name=\'x_fixed\')\n    env.adv_eps = tf.placeholder(tf.float32, (), name=\'adv_eps\')\n    env.adv_y = tf.placeholder(tf.int32, (), name=\'adv_y\')\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n    env.adv_train_op, env.xadv, env.noise = cw(model, env.x_fixed,\n                                               y=env.adv_y, eps=env.adv_eps,\n                                               optimizer=optimizer)\n\nprint(\'\\nInitializing graph\')\n\nenv.sess = tf.InteractiveSession()\nenv.sess.run(tf.global_variables_initializer())\nenv.sess.run(tf.local_variables_initializer())\n\n\ndef evaluate(env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = env.sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(env.sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            env.sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                                  env.y: y_data[start:end],\n                                                  env.training: True})\n        if X_valid is not None:\n            evaluate(env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(env.sess, \'model/{}\'.format(name))\n\n\ndef predict(env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    # print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        # print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = env.sess.run(env.ybar, feed_dict={env.x: X_data[start:end]})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef cw_binary_search(env, image, target, epochs=1, bound=(1e-8, 1e2),\n                     binary_epochs=10):\n    """"""\n    Search for the best result on one image sample.\n    """"""\n    print(\'\\nMaking adversarials via CW\')\n\n    mindist = float(\'inf\')\n    advimg, advprob = None, -1\n\n    lo, hi = bound\n    for epoch in range(binary_epochs):\n        eps = lo + (hi - lo) / 2\n\n        with Timer(\'Epoch {0}/{1} lo: {2:g} hi: {3:g}\'\n                   .format(epoch+1, binary_epochs, lo, hi)):\n            feed_dict = {\n                env.x_fixed: image,\n                env.adv_eps: eps,\n                env.adv_y: target}\n\n            # reset the noise before every iteration\n            env.sess.run(env.noise.initializer)\n            for epoch in range(epochs):\n                env.sess.run(env.adv_train_op, feed_dict=feed_dict)\n\n            xadv = env.sess.run(env.xadv, feed_dict=feed_dict)\n            ybar = predict(env, xadv, 1)\n            label, prob = np.argmax(ybar), np.max(ybar)\n\n            if label != target:\n                lo = eps\n                if lo > bound[1]:\n                    eps *= 10\n            else:\n                dist = np.linalg.norm(xadv.flatten()-image.flatten())\n                if mindist < 0 or dist < mindist:\n                    mindist = dist\n                    advimg = xadv\n                    advprob = prob\n                hi = eps\n\n    print(\'Min distance: {:g}\'.format(mindist))\n    return advimg, advprob\n\n\nprint(\'\\nTraining\')\n\ntrain(env, X_train, y_train, X_valid, y_valid, load=False, epochs=5,\n      name=\'mnist\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial data\')\n\nz0 = np.argmax(y_test, axis=1)\ny1 = predict(env, X_test)\nz1 = np.argmax(y1, axis=1)\n\nind = z0 = z1\n\nX_test = X_test[ind]\ny_test = y_test[ind]\n\nX_tmp = np.zeros((n_classes, img_size, img_size))\ny_tmp = np.zeros(n_classes)\n\nind = np.random.choice(X_test.shape[0])\nimage = np.expand_dims(X_test[ind], axis=0)\nlabel = np.argmax(y_test[ind])\ny_tmp[label] = np.max(y1[ind])\n\nfor i in range(n_classes):\n    print(\'Label {0} --> {1}\'.format(label, i))\n    if i == label:\n        X_tmp[i] = np.squeeze(image)\n        continue\n\n    xadv, yadv = cw_binary_search(env, image, target=i, epochs=EPOCHS,\n                                  bound=BOUND, binary_epochs=10)\n    if yadv > 0:\n        X_tmp[i] = np.squeeze(xadv)\n    y_tmp[i] = yadv\n\n# Plot it!!\n\nfig = plt.figure(figsize=(n_classes+0.2, 2.2))\ngs = gridspec.GridSpec(2, n_classes+1, width_ratios=[1]*n_classes + [0.1],\n                       wspace=0.01, hspace=0.01)\n\nfor i in range(n_classes):\n    ax = fig.add_subplot(gs[0, i])\n    ax.imshow(X_tmp[i], cmap=\'gray\', interpolation=\'none\')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    ax = fig.add_subplot(gs[1, i])\n    img = ax.imshow(X_tmp[i]-X_tmp[label], cmap=\'RdBu_r\', vmin=-1,\n                    vmax=1, interpolation=\'none\')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    ax.set_xlabel(\'{0} ({1:.2f})\'.format(i, y_tmp[i]), fontsize=12)\n\nax = fig.add_subplot(gs[1, n_classes])\ndummy = plt.cm.ScalarMappable(cmap=\'RdBu_r\', norm=plt.Normalize(vmin=-1,\n                                                                vmax=1))\ndummy.set_array([])\nfig.colorbar(mappable=dummy, cax=ax, ticks=[-1, 0, 1], ticklocation=\'right\')\n\nprint(\'\\nSaving figure\')\n\ngs.tight_layout(fig)\nos.makedirs(\'img\', exist_ok=True)\nplt.savefig(\'img/cw2_mnist_binary_search.png\')\n'"
example/cw8_mnist.py,39,"b'""""""\nUse CW method to craft adversarial on MNIST.\n""""""\nimport os\nfrom timeit import default_timer\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import cw\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nimg_size = 28\nimg_chan = 1\nn_classes = 10\nbatch_size = 64\n\n\nclass Timer(object):\n    def __init__(self, msg=\'Starting.....\', timer=default_timer, factor=1,\n                 fmt=""------- elapsed {:.4f}s --------""):\n        self.timer = timer\n        self.factor = factor\n        self.fmt = fmt\n        self.end = None\n        self.msg = msg\n\n    def __call__(self):\n        """"""\n        Return the current time\n        """"""\n        return self.timer()\n\n    def __enter__(self):\n        """"""\n        Set the start time\n        """"""\n        print(self.msg)\n        self.start = self()\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        """"""\n        Set the end time\n        """"""\n        self.end = self()\n        print(str(self))\n\n    def __repr__(self):\n        return self.fmt.format(self.elapsed)\n\n    @property\n    def elapsed(self):\n        if self.end is None:\n            # if elapsed is called in the context manager scope\n            return (self() - self.start) * self.factor\n        else:\n            # if elapsed is called out of the context manager scope\n            return (self.end - self.start) * self.factor\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nto_categorical = tf.keras.utils.to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=10, name=\'logits\')\n    y = tf.nn.softmax(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\', reuse=tf.AUTO_REUSE):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, n_classes), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar, logits = model(env.x, logits=True, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    with tf.variable_scope(\'loss\'):\n        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n                                                       logits=logits)\n        env.loss = tf.reduce_mean(xent, name=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        vs = tf.global_variables()\n        env.train_op = optimizer.minimize(env.loss, var_list=vs)\n\n    env.saver = tf.train.Saver()\n\n    # Note here that the shape has to be fixed during the graph construction\n    # since the internal variable depends upon the shape.\n    env.x_fixed = tf.placeholder(\n        tf.float32, (batch_size, img_size, img_size, img_chan),\n        name=\'x_fixed\')\n    env.adv_eps = tf.placeholder(tf.float32, (), name=\'adv_eps\')\n    env.adv_y = tf.placeholder(tf.int32, (), name=\'adv_y\')\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n    env.adv_train_op, env.xadv, env.noise = cw(model, env.x_fixed, ord_=\'inf\',\n                                               y=env.adv_y, eps=env.adv_eps,\n                                               optimizer=optimizer)\n\nprint(\'\\nInitializing graph\')\n\nenv.sess = tf.InteractiveSession()\nenv.sess.run(tf.global_variables_initializer())\nenv.sess.run(tf.local_variables_initializer())\n\n\ndef evaluate(env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = env.sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(env.sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            env.sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                                  env.y: y_data[start:end],\n                                                  env.training: True})\n        if X_valid is not None:\n            evaluate(env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(env.sess, \'model/{}\'.format(name))\n\n\ndef predict(env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = env.sess.run(env.ybar, feed_dict={env.x: X_data[start:end]})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef make_cw(env, X_data, epochs=1, eps=0.1, batch_size=batch_size):\n    """"""\n    Generate adversarial via CW optimization.\n    """"""\n    print(\'\\nMaking adversarials via CW\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        with Timer(\'Batch {0}/{1}   \'.format(batch + 1, n_batch)):\n            end = min(n_sample, (batch+1) * batch_size)\n            start = end - batch_size\n            feed_dict = {\n                env.x_fixed: X_data[start:end],\n                env.adv_eps: eps,\n                env.adv_y: np.random.choice(n_classes)}\n\n            env.sess.run(env.noise.initializer)\n            for epoch in range(epochs):\n                env.sess.run(env.adv_train_op, feed_dict=feed_dict)\n\n            xadv = env.sess.run(env.xadv, feed_dict=feed_dict)\n            X_adv[start:end] = xadv\n\n    return X_adv\n\n\nprint(\'\\nTraining\')\n\ntrain(env, X_train, y_train, X_valid, y_valid, load=True, epochs=5,\n      name=\'mnist\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial data\')\n\n# It takes a while to run through the full dataset, thus, we demo the result\n# through a smaller dataset.  We could actually find the best parameter\n# configuration on a smaller dataset, and then apply to the full dataset.\nn_sample = 128\nind = np.random.choice(X_test.shape[0], size=n_sample)\nX_test = X_test[ind]\ny_test = y_test[ind]\n\nX_adv = make_cw(env, X_test, eps=3, epochs=100)\n\nprint(\'\\nEvaluating on adversarial data\')\n\nevaluate(env, X_adv, y_test)\n\nprint(\'\\nRandomly sample adversarial data from each category\')\n\ny1 = predict(env, X_test)\ny2 = predict(env, X_adv)\n\nz0 = np.argmax(y_test, axis=1)\nz1 = np.argmax(y1, axis=1)\nz2 = np.argmax(y2, axis=1)\n\nind = np.logical_and(z0 == z1, z1 != z2)\n# print(\'success: \', np.sum(ind))\n\nind = z0 == z1\n\nX_test = X_test[ind]\nX_adv = X_adv[ind]\nz1 = z1[ind]\nz2 = z2[ind]\ny2 = y2[ind]\n\nind, = np.where(z1 != z2)\ncur = np.random.choice(ind, size=n_classes)\nX_org = np.squeeze(X_test[cur])\nX_tmp = np.squeeze(X_adv[cur])\ny_tmp = y2[cur]\n\nfig = plt.figure(figsize=(n_classes+0.2, 3.2))\ngs = gridspec.GridSpec(3, n_classes+1, width_ratios=[1]*n_classes + [0.1],\n                       wspace=0.01, hspace=0.01)\n\nlabel = np.argmax(y_tmp, axis=1)\nproba = np.max(y_tmp, axis=1)\n\nfor i in range(n_classes):\n    ax = fig.add_subplot(gs[0, i])\n    ax.imshow(X_org[i], cmap=\'gray\', interpolation=\'none\')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    ax = fig.add_subplot(gs[1, i])\n    img = ax.imshow(X_tmp[i]-X_org[i], cmap=\'RdBu_r\', vmin=-1,\n                    vmax=1, interpolation=\'none\')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    ax = fig.add_subplot(gs[2, i])\n    ax.imshow(X_tmp[i], cmap=\'gray\', interpolation=\'none\')\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    ax.set_xlabel(\'{0} ({1:.2f})\'.format(label[i], proba[i]), fontsize=12)\n\nax = fig.add_subplot(gs[1, n_classes])\ndummy = plt.cm.ScalarMappable(cmap=\'RdBu_r\', norm=plt.Normalize(vmin=-1,\n                                                                vmax=1))\ndummy.set_array([])\nfig.colorbar(mappable=dummy, cax=ax, ticks=[-1, 0, 1], ticklocation=\'right\')\n\nprint(\'\\nSaving figure\')\n\ngs.tight_layout(fig)\nos.makedirs(\'img/\', exist_ok=True)\nplt.savefig(\'img/cw8_mnist.png\')\n'"
example/deepfool_mnist.py,35,"b'""""""\nUse DeepFool to craft adversarials on MNIST.\n""""""\nimport os\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import deepfool\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nimg_size = 28\nimg_chan = 1\nn_classes = 10\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nto_categorical = tf.keras.utils.to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=10, name=\'logits\')\n    y = tf.nn.softmax(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\'):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, n_classes), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar, logits = model(env.x, logits=True, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    with tf.variable_scope(\'loss\'):\n        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n                                                       logits=logits)\n        env.loss = tf.reduce_mean(xent, name=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        env.train_op = optimizer.minimize(env.loss)\n\n    env.saver = tf.train.Saver()\n\nwith tf.variable_scope(\'model\', reuse=True):\n    env.adv_epochs = tf.placeholder(tf.int32, (), name=\'adv_epochs\')\n    env.xadv = deepfool(model, env.x, epochs=env.adv_epochs)\n\nprint(\'\\nInitializing graph\')\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\n\ndef evaluate(sess, env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(sess, env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                              env.y: y_data[start:end],\n                                              env.training: True})\n        if X_valid is not None:\n            evaluate(sess, env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(sess, \'model/{}\'.format(name))\n\n\ndef predict(sess, env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = sess.run(env.ybar, feed_dict={env.x: X_data[start:end]})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef make_deepfool(sess, env, X_data, epochs=1, eps=0.01, batch_size=128):\n    """"""\n    Generate DeepFool by running env.xadv.\n    """"""\n    print(\'\\nMaking adversarials via DeepFool\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        adv = sess.run(env.xadv, feed_dict={env.x: X_data[start:end],\n                                            env.adv_epochs: epochs})\n        X_adv[start:end] = adv\n    print()\n\n    return X_adv\n\n\nprint(\'\\nTraining\')\n\ntrain(sess, env, X_train, y_train, X_valid, y_valid, load=False, epochs=5,\n      name=\'mnist\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(sess, env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial data\')\n\nX_adv = make_deepfool(sess, env, X_test, epochs=3)\n\nprint(\'\\nEvaluating on adversarial data\')\n\nevaluate(sess, env, X_adv, y_test)\n\nprint(\'\\nRandomly sample adversarial data from each category\')\n\ny1 = predict(sess, env, X_test)\ny2 = predict(sess, env, X_adv)\n\nz0 = np.argmax(y_test, axis=1)\nz1 = np.argmax(y1, axis=1)\nz2 = np.argmax(y2, axis=1)\n\nprint(\'\\nPlotting results\')\nfig = plt.figure(figsize=(10, 2.2))\ngs = gridspec.GridSpec(2, 10, wspace=0.05, hspace=0.05)\n\nfor i in range(10):\n    print(\'Target {0}\'.format(i))\n    ind, = np.where(np.all([z0 == i, z1 == i, z2 != i], axis=0))\n    ind = np.random.choice(ind)\n    xcur = [X_test[ind], X_adv[ind]]\n    ycur = y2[ind]\n    zcur = z2[ind]\n\n    for j in range(2):\n        img = np.squeeze(xcur[j])\n        ax = fig.add_subplot(gs[j, i])\n        ax.imshow(img, cmap=\'gray\', interpolation=\'none\')\n        ax.set_xticks([])\n        ax.set_yticks([])\n    ax.set_xlabel(\'{0} ({1:.2f})\'.format(zcur, ycur[zcur]), fontsize=12)\n\nprint(\'\\nSaving figure\')\ngs.tight_layout(fig)\nos.makedirs(\'img\', exist_ok=True)\nplt.savefig(\'img/deepfool_mnist.png\')\n'"
example/deepfool_mnist2.py,34,"b'""""""\nUse DeepFool to craft adversarial on binary labels.\n\nBinary labels are randomly selected from MNIST.\n\nNote that DeepFool assumes that the output of binary classifier is +1/-1,\ne.g., tf.tanh instead of tf.sigmoid.  As a result, we need MSE instead of\ncross entropy as loss function.\n""""""\nimport os\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import deepfool\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nimg_size = 28\nimg_chan = 1\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nprint(\'\\nRandomly selecting two classes\')\n# c0, c1 = np.random.choice(10, size=2, replace=False)\nc0, c1 = 8, 1\n\ni0 = y_train == c0\ni1 = y_train == c1\nX_train = np.vstack((X_train[i0], X_train[i1]))\ny_train = np.vstack((-np.ones([np.sum(i0), 1]), np.ones([np.sum(i1), 1])))\n\ni0 = y_test == c0\ni1 = y_test == c1\nX_test = np.vstack((X_test[i0], X_test[i1]))\ny_test = np.vstack((-np.ones([np.sum(i0), 1]), np.ones([np.sum(i1), 1])))\n# y_test = np.vstack((np.ones([np.sum(i0), 1]), np.zeros([np.sum(i1), 1])))\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=1, name=\'logits\')\n\n    # NOTE: DeepFool assumes outputs of +1/-1 for binary classifier.\n    y = tf.tanh(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\'):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, 1), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar = model(env.x, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.logical_not(tf.logical_xor(tf.greater(env.y, 0.0),\n                                              tf.greater(env.ybar, 0.0)))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    env.loss = tf.losses.mean_squared_error(labels=env.y,\n                                            predictions=env.ybar,\n                                            scope=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        env.train_op = optimizer.minimize(env.loss)\n\n    env.saver = tf.train.Saver()\n\nwith tf.variable_scope(\'model\', reuse=True):\n    env.adv_epochs = tf.placeholder(tf.int32, (), name=\'adv_epochs\')\n    env.xadv = deepfool(model, env.x, epochs=env.adv_epochs)\n\nprint(\'\\nInitializing graph\')\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\n\ndef evaluate(sess, env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(sess, env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                              env.y: y_data[start:end],\n                                              env.training: True})\n        if X_valid is not None:\n            evaluate(sess, env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(sess, \'model/{}\'.format(name))\n\n\ndef predict(sess, env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = sess.run(env.ybar, feed_dict={env.x: X_data[start:end]})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef make_deepfool(sess, env, X_data, epochs=1, batch_size=128):\n    """"""\n    Generate DeepFool by running env.xadv.\n    """"""\n    print(\'\\nMaking adversarials via DeepFool\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        adv = sess.run(env.xadv, feed_dict={env.x: X_data[start:end],\n                                            env.adv_epochs: epochs})\n        X_adv[start:end] = adv\n    print()\n\n    return X_adv\n\n\nprint(\'\\nTraining\')\n\ntrain(sess, env, X_train, y_train, X_valid, y_valid, load=False, epochs=5,\n      name=\'mnist2\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(sess, env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial data\')\n\nX_adv = make_deepfool(sess, env, X_test, epochs=3)\n\nprint(\'\\nEvaluating on adversarial data\')\n\nevaluate(sess, env, X_adv, y_test)\n\nprint(\'\\nRandomly sample adversarial data from each category\')\n\ny1 = predict(sess, env, X_test).flatten()\ny2 = predict(sess, env, X_adv).flatten()\n\nz0 = (y_test > 0).flatten().astype(np.int32)\nz1 = (y1 > 0).astype(np.int32)\nz2 = (y2 > 0).astype(np.int32)\n\nprint(\'\\nPlotting results\')\nfig = plt.figure(figsize=(2, 2.2))\ngs = gridspec.GridSpec(2, 2)\n\nfor i in range(2):\n    print(\'Target {0}\'.format(i))\n    ind, = np.where(np.all([z0 == i, z1 == i, z2 != i], axis=0))\n    ind = np.random.choice(ind)\n    xcur = [X_test[ind], X_adv[ind]]\n    ycur = [y1[ind], y2[ind]]\n\n    for j in range(2):\n        img = np.squeeze(xcur[j])\n        ax = fig.add_subplot(gs[j, i])\n        ax.imshow(img, cmap=\'gray\', interpolation=\'none\')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_xlabel(\'{0:.2f}\'.format(ycur[j]), fontsize=10)\n\nprint(\'\\nSaving figure\')\ngs.tight_layout(fig)\nos.makedirs(\'img\', exist_ok=True)\nplt.savefig(\'img/deepfool_mnist2.png\')\n'"
example/deepfool_mnist2_batch.py,34,"b'""""""\nUse DeepFool to craft adversarial on binary labels.\n\nBinary labels are randomly selected from MNIST.\n\nNote that DeepFool assumes that the output of binary classifier is +1/-1,\ne.g., tf.tanh instead of tf.sigmoid.  As a result, we need MSE instead of\ncross entropy as loss function.\n""""""\nimport os\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import deepfool\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nimg_size = 28\nimg_chan = 1\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nprint(\'\\nRandomly selecting two classes\')\n# c0, c1 = np.random.choice(10, size=2, replace=False)\nc0, c1 = 8, 1\n\ni0 = y_train == c0\ni1 = y_train == c1\nX_train = np.vstack((X_train[i0], X_train[i1]))\ny_train = np.vstack((-np.ones([np.sum(i0), 1]), np.ones([np.sum(i1), 1])))\n\ni0 = y_test == c0\ni1 = y_test == c1\nX_test = np.vstack((X_test[i0], X_test[i1]))\ny_test = np.vstack((-np.ones([np.sum(i0), 1]), np.ones([np.sum(i1), 1])))\n# y_test = np.vstack((np.ones([np.sum(i0), 1]), np.zeros([np.sum(i1), 1])))\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=1, name=\'logits\')\n\n    # NOTE: DeepFool assumes outputs of +1/-1 for binary classifier.\n    y = tf.tanh(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\'):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, 1), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar = model(env.x, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.logical_not(tf.logical_xor(tf.greater(env.y, 0.0),\n                                              tf.greater(env.ybar, 0.0)))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    env.loss = tf.losses.mean_squared_error(labels=env.y,\n                                            predictions=env.ybar,\n                                            scope=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        env.train_op = optimizer.minimize(env.loss)\n\n    env.saver = tf.train.Saver()\n\nwith tf.variable_scope(\'model\', reuse=True):\n    env.adv_epochs = tf.placeholder(tf.int32, (), name=\'adv_epochs\')\n    env.xadv = deepfool(model, env.x, epochs=env.adv_epochs, batch=True)\n\nprint(\'\\nInitializing graph\')\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\n\ndef evaluate(sess, env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(sess, env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                              env.y: y_data[start:end],\n                                              env.training: True})\n        if X_valid is not None:\n            evaluate(sess, env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(sess, \'model/{}\'.format(name))\n\n\ndef predict(sess, env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = sess.run(env.ybar, feed_dict={env.x: X_data[start:end]})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef make_deepfool(sess, env, X_data, epochs=1, batch_size=128):\n    """"""\n    Generate DeepFool by running env.xadv.\n    """"""\n    print(\'\\nMaking adversarials via DeepFool\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        adv = sess.run(env.xadv, feed_dict={env.x: X_data[start:end],\n                                            env.adv_epochs: epochs})\n        X_adv[start:end] = adv\n    print()\n\n    return X_adv\n\n\nprint(\'\\nTraining\')\n\ntrain(sess, env, X_train, y_train, X_valid, y_valid, load=False, epochs=5,\n      name=\'mnist2\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(sess, env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial data\')\n\nX_adv = make_deepfool(sess, env, X_test, epochs=2)\n\nprint(\'\\nEvaluating on adversarial data\')\n\nevaluate(sess, env, X_adv, y_test)\n\nprint(\'\\nRandomly sample adversarial data from each category\')\n\ny1 = predict(sess, env, X_test).flatten()\ny2 = predict(sess, env, X_adv).flatten()\n\nz0 = (y_test > 0).flatten().astype(np.int32)\nz1 = (y1 > 0).astype(np.int32)\nz2 = (y2 > 0).astype(np.int32)\n\nprint(\'\\nPlotting results\')\nfig = plt.figure(figsize=(2, 2.2))\ngs = gridspec.GridSpec(2, 2)\n\nfor i in range(2):\n    print(\'Target {0}\'.format(i))\n    ind, = np.where(np.all([z0 == i, z1 == i, z2 != i], axis=0))\n    ind = np.random.choice(ind)\n    xcur = [X_test[ind], X_adv[ind]]\n    ycur = [y1[ind], y2[ind]]\n\n    for j in range(2):\n        img = np.squeeze(xcur[j])\n        ax = fig.add_subplot(gs[j, i])\n        ax.imshow(img, cmap=\'gray\', interpolation=\'none\')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_xlabel(\'{0:.2f}\'.format(ycur[j]), fontsize=10)\n\nprint(\'\\nSaving figure\')\ngs.tight_layout(fig)\nos.makedirs(\'img\', exist_ok=True)\nplt.savefig(\'img/deepfool_mnist2_batch.png\')\n'"
example/deepfool_mnist_batch.py,35,"b'""""""\nUse DeepFool to craft adversarials on MNIST.\n""""""\nimport os\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import deepfool\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nimg_size = 28\nimg_chan = 1\nn_classes = 10\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nto_categorical = tf.keras.utils.to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=10, name=\'logits\')\n    y = tf.nn.softmax(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\'):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, n_classes), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar, logits = model(env.x, logits=True, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    with tf.variable_scope(\'loss\'):\n        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n                                                       logits=logits)\n        env.loss = tf.reduce_mean(xent, name=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        env.train_op = optimizer.minimize(env.loss)\n\n    env.saver = tf.train.Saver()\n\nwith tf.variable_scope(\'model\', reuse=True):\n    env.adv_epochs = tf.placeholder(tf.int32, (), name=\'adv_epochs\')\n    env.xadv = deepfool(model, env.x, epochs=env.adv_epochs, batch=True)\n\nprint(\'\\nInitializing graph\')\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\n\ndef evaluate(sess, env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(sess, env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                              env.y: y_data[start:end],\n                                              env.training: True})\n        if X_valid is not None:\n            evaluate(sess, env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(sess, \'model/{}\'.format(name))\n\n\ndef predict(sess, env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = sess.run(env.ybar, feed_dict={env.x: X_data[start:end]})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef make_deepfool(sess, env, X_data, epochs=1, eps=0.01, batch_size=128):\n    """"""\n    Generate DeepFool by running env.xadv.\n    """"""\n    print(\'\\nMaking adversarials via DeepFool\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        adv = sess.run(env.xadv, feed_dict={env.x: X_data[start:end],\n                                            env.adv_epochs: epochs})\n        X_adv[start:end] = adv\n    print()\n\n    return X_adv\n\n\nprint(\'\\nTraining\')\n\ntrain(sess, env, X_train, y_train, X_valid, y_valid, load=False, epochs=5,\n      name=\'mnist\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(sess, env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial data\')\n\nX_adv = make_deepfool(sess, env, X_test, epochs=3)\n\nprint(\'\\nEvaluating on adversarial data\')\n\nevaluate(sess, env, X_adv, y_test)\n\nprint(\'\\nRandomly sample adversarial data from each category\')\n\ny1 = predict(sess, env, X_test)\ny2 = predict(sess, env, X_adv)\n\nz0 = np.argmax(y_test, axis=1)\nz1 = np.argmax(y1, axis=1)\nz2 = np.argmax(y2, axis=1)\n\nprint(\'\\nPlotting results\')\nfig = plt.figure(figsize=(10, 2.2))\ngs = gridspec.GridSpec(2, 10, wspace=0.05, hspace=0.05)\n\nfor i in range(10):\n    print(\'Target {0}\'.format(i))\n    ind, = np.where(np.all([z0 == i, z1 == i, z2 != i], axis=0))\n    ind = np.random.choice(ind)\n    xcur = [X_test[ind], X_adv[ind]]\n    ycur = y2[ind]\n    zcur = z2[ind]\n\n    for j in range(2):\n        img = np.squeeze(xcur[j])\n        ax = fig.add_subplot(gs[j, i])\n        ax.imshow(img, cmap=\'gray\', interpolation=\'none\')\n        ax.set_xticks([])\n        ax.set_yticks([])\n    ax.set_xlabel(\'{0} ({1:.2f})\'.format(zcur, ycur[zcur]), fontsize=12)\n\nprint(\'\\nSaving figure\')\ngs.tight_layout(fig)\nos.makedirs(\'img\', exist_ok=True)\nplt.savefig(\'img/deepfool_mnist_batch.png\')\n'"
example/deepfool_noise.py,35,"b'""""""\nUse DeepFool to craft adversarials on MNIST.\n""""""\nimport os\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import deepfool\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nimg_size = 28\nimg_chan = 1\nn_classes = 10\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nto_categorical = tf.keras.utils.to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=10, name=\'logits\')\n    y = tf.nn.softmax(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\'):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, n_classes), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar, logits = model(env.x, logits=True, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    with tf.variable_scope(\'loss\'):\n        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n                                                       logits=logits)\n        env.loss = tf.reduce_mean(xent, name=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        env.train_op = optimizer.minimize(env.loss)\n\n    env.saver = tf.train.Saver()\n\nwith tf.variable_scope(\'model\', reuse=True):\n    env.adv_epochs = tf.placeholder(tf.int32, (), name=\'adv_epochs\')\n    env.noise = deepfool(model, env.x, epochs=env.adv_epochs, batch=True,\n                         noise=True)\n\nprint(\'\\nInitializing graph\')\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\n\ndef evaluate(sess, env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(sess, env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                              env.y: y_data[start:end],\n                                              env.training: True})\n        if X_valid is not None:\n            evaluate(sess, env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(sess, \'model/{}\'.format(name))\n\n\ndef predict(sess, env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = sess.run(env.ybar, feed_dict={env.x: X_data[start:end]})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef make_deepfool(sess, env, X_data, epochs=1, eps=0.01, batch_size=128):\n    """"""\n    Generate DeepFool by running env.xadv.\n    """"""\n    print(\'\\nMaking adversarials via DeepFool\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_noise = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        noise = sess.run(env.noise, feed_dict={env.x: X_data[start:end],\n                                               env.adv_epochs: epochs})\n        X_noise[start:end] = noise\n    print()\n\n    return X_noise\n\n\nprint(\'\\nTraining\')\n\ntrain(sess, env, X_train, y_train, X_valid, y_valid, load=False, epochs=5,\n      name=\'mnist\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(sess, env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial data\')\n\nX_noise = make_deepfool(sess, env, X_test, epochs=3)\nX_adv = np.clip(X_test + 1.02*X_noise, 0, 1)\nprint(np.min(X_noise), np.max(X_noise))\n\nprint(\'\\nEvaluating on adversarial data\')\n\nevaluate(sess, env, X_adv, y_test)\n\nprint(\'\\nRandomly sample adversarial data from each category\')\n\ny1 = predict(sess, env, X_test)\ny2 = predict(sess, env, X_adv)\n\nz0 = np.argmax(y_test, axis=1)\nz1 = np.argmax(y1, axis=1)\nz2 = np.argmax(y2, axis=1)\n\nprint(\'\\nPlotting results\')\nfig = plt.figure(figsize=(10, 2.2))\ngs = gridspec.GridSpec(2, 10, wspace=0.05, hspace=0.05)\n\nfor i in range(10):\n    print(\'Target {0}\'.format(i))\n    ind, = np.where(np.all([z0 == i, z1 == i, z2 != i], axis=0))\n    ind = np.random.choice(ind)\n    xcur = [X_test[ind], X_adv[ind]]\n    ycur = y2[ind]\n    zcur = z2[ind]\n\n    for j in range(2):\n        img = np.squeeze(xcur[j])\n        ax = fig.add_subplot(gs[j, i])\n        ax.imshow(img, cmap=\'gray\', interpolation=\'none\')\n        ax.set_xticks([])\n        ax.set_yticks([])\n    ax.set_xlabel(\'{0} ({1:.2f})\'.format(zcur, ycur[zcur]), fontsize=12)\n\nprint(\'\\nSaving figure\')\ngs.tight_layout(fig)\nos.makedirs(\'img\', exist_ok=True)\nplt.savefig(\'img/deepfool_mnist.png\')\n'"
example/fgmt_mnist.py,36,"b'""""""\nUse fast gradient sign method to craft adversarial on MNIST.\n\nDependencies: python3, tensorflow v1.4, numpy, matplotlib\n""""""\nimport os\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import fgmt\n\n\nimg_size = 28\nimg_chan = 1\nn_classes = 10\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nto_categorical = tf.keras.utils.to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=10, name=\'logits\')\n    y = tf.nn.softmax(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\'):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, n_classes), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar, logits = model(env.x, logits=True, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    with tf.variable_scope(\'loss\'):\n        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n                                                       logits=logits)\n        env.loss = tf.reduce_mean(xent, name=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        env.train_op = optimizer.minimize(env.loss)\n\n    env.saver = tf.train.Saver()\n\nwith tf.variable_scope(\'model\', reuse=True):\n    env.fgsm_eps = tf.placeholder(tf.float32, (), name=\'fgsm_eps\')\n    env.fgsm_epochs = tf.placeholder(tf.int32, (), name=\'fgsm_epochs\')\n    env.x_fgmt = fgmt(model, env.x, epochs=env.fgsm_epochs, eps=env.fgsm_eps)\n\nprint(\'\\nInitializing graph\')\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\n\ndef evaluate(sess, env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(sess, env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                              env.y: y_data[start:end],\n                                              env.training: True})\n        if X_valid is not None:\n            evaluate(sess, env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(sess, \'model/{}\'.format(name))\n\n\ndef predict(sess, env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = sess.run(env.ybar, feed_dict={env.x: X_data[start:end]})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef make_fgmt(sess, env, X_data, epochs=1, eps=0.01, batch_size=128):\n    """"""\n    Generate FGSM by running env.x_fgsm.\n    """"""\n    print(\'\\nMaking adversarials via FGSM\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        adv = sess.run(env.x_fgmt, feed_dict={\n            env.x: X_data[start:end],\n            env.fgsm_eps: eps,\n            env.fgsm_epochs: epochs})\n        X_adv[start:end] = adv\n    print()\n\n    return X_adv\n\n\nprint(\'\\nTraining\')\n\ntrain(sess, env, X_train, y_train, X_valid, y_valid, load=False, epochs=5,\n      name=\'mnist\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(sess, env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial data\')\n\nX_adv = make_fgmt(sess, env, X_test, eps=0.02, epochs=12)\n\nprint(\'\\nEvaluating on adversarial data\')\n\nevaluate(sess, env, X_adv, y_test)\n\nprint(\'\\nRandomly sample adversarial data from each category\')\n\ny1 = predict(sess, env, X_test)\ny2 = predict(sess, env, X_adv)\n\nz0 = np.argmax(y_test, axis=1)\nz1 = np.argmax(y1, axis=1)\nz2 = np.argmax(y2, axis=1)\n\nX_tmp = np.empty((10, 28, 28))\ny_tmp = np.empty((10, 10))\nfor i in range(10):\n    print(\'Target {0}\'.format(i))\n    ind, = np.where(np.all([z0 == i, z1 == i, z2 != i], axis=0))\n    cur = np.random.choice(ind)\n    X_tmp[i] = np.squeeze(X_adv[cur])\n    y_tmp[i] = y2[cur]\n\nprint(\'\\nPlotting results\')\n\nfig = plt.figure(figsize=(10, 1.2))\ngs = gridspec.GridSpec(1, 10, wspace=0.05, hspace=0.05)\n\nlabel = np.argmax(y_tmp, axis=1)\nproba = np.max(y_tmp, axis=1)\nfor i in range(10):\n    ax = fig.add_subplot(gs[0, i])\n    ax.imshow(X_tmp[i], cmap=\'gray\', interpolation=\'none\')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel(\'{0} ({1:.2f})\'.format(label[i], proba[i]),\n                  fontsize=12)\n\nprint(\'\\nSaving figure\')\n\ngs.tight_layout(fig)\nos.makedirs(\'img\', exist_ok=True)\nplt.savefig(\'img/fgmt_mnist.png\')\n'"
example/fgmt_mnist2.py,37,"b'""""""\nUse fast gradient sign method to craft adversarial on MNIST.\n\nDependencies: python3, tensorflow v1.4, numpy, matplotlib\n""""""\nimport os\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import fgmt\n\n\nimg_size = 28\nimg_chan = 1\nn_classes = 10\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nto_categorical = tf.keras.utils.to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=10, name=\'logits\')\n    y = tf.nn.softmax(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\'):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, n_classes), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar, logits = model(env.x, logits=True, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    with tf.variable_scope(\'loss\'):\n        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n                                                       logits=logits)\n        env.loss = tf.reduce_mean(xent, name=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        env.train_op = optimizer.minimize(env.loss)\n\n    env.saver = tf.train.Saver()\n\nwith tf.variable_scope(\'model\', reuse=True):\n    env.adv_eps = tf.placeholder(tf.float32, (), name=\'adv_eps\')\n    env.adv_epochs = tf.placeholder(tf.int32, (), name=\'adv_epochs\')\n    env.adv_y = tf.placeholder(tf.int32, (), name=\'adv_y\')\n    env.x_fgmt = fgmt(model, env.x, y=env.adv_y, epochs=env.adv_epochs,\n                      eps=env.adv_eps)\n\nprint(\'\\nInitializing graph\')\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\n\ndef evaluate(sess, env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(sess, env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                              env.y: y_data[start:end],\n                                              env.training: True})\n        if X_valid is not None:\n            evaluate(sess, env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(sess, \'model/{}\'.format(name))\n\n\ndef predict(sess, env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = sess.run(env.ybar, feed_dict={env.x: X_data[start:end]})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef make_fgmt(sess, env, X_data, epochs=1, eps=0.01, batch_size=128):\n    """"""\n    Generate FGSM by running env.x_fgsm.\n    """"""\n    print(\'\\nMaking adversarials via FGSM\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        adv = sess.run(env.x_fgmt, feed_dict={\n            env.x: X_data[start:end],\n            env.adv_y: np.random.choice(n_classes),\n            env.adv_eps: eps,\n            env.adv_epochs: epochs})\n        X_adv[start:end] = adv\n    print()\n\n    return X_adv\n\n\nprint(\'\\nTraining\')\n\ntrain(sess, env, X_train, y_train, X_valid, y_valid, load=True, epochs=5,\n      name=\'mnist\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(sess, env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial data\')\n\nX_adv = make_fgmt(sess, env, X_test, eps=0.02, epochs=8)\n\nprint(\'\\nEvaluating on adversarial data\')\n\nevaluate(sess, env, X_adv, y_test)\n\nprint(\'\\nRandomly sample adversarial data from each category\')\n\ny1 = predict(sess, env, X_test)\ny2 = predict(sess, env, X_adv)\n\nz0 = np.argmax(y_test, axis=1)\nz1 = np.argmax(y1, axis=1)\nz2 = np.argmax(y2, axis=1)\n\nX_tmp = np.empty((10, 28, 28))\ny_tmp = np.empty((10, 10))\nfor i in range(10):\n    print(\'Target {0}\'.format(i))\n    ind, = np.where(np.all([z0 == i, z1 == i, z2 != i], axis=0))\n    cur = np.random.choice(ind)\n    X_tmp[i] = np.squeeze(X_adv[cur])\n    y_tmp[i] = y2[cur]\n\nprint(\'\\nPlotting results\')\n\nfig = plt.figure(figsize=(10, 1.2))\ngs = gridspec.GridSpec(1, 10, wspace=0.05, hspace=0.05)\n\nlabel = np.argmax(y_tmp, axis=1)\nproba = np.max(y_tmp, axis=1)\nfor i in range(10):\n    ax = fig.add_subplot(gs[0, i])\n    ax.imshow(X_tmp[i], cmap=\'gray\', interpolation=\'none\')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel(\'{0} ({1:.2f})\'.format(label[i], proba[i]),\n                  fontsize=12)\n\nprint(\'\\nSaving figure\')\n\ngs.tight_layout(fig)\nos.makedirs(\'img\', exist_ok=True)\nplt.savefig(\'img/fgmt_mnist2.png\')\n'"
example/fgsm_mnist.py,36,"b'""""""\nUse fast gradient sign method to craft adversarial on MNIST.\n\nDependencies: python3, tensorflow v1.4, numpy, matplotlib\n""""""\nimport os\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import fgm\n\n\nimg_size = 28\nimg_chan = 1\nn_classes = 10\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nto_categorical = tf.keras.utils.to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=10, name=\'logits\')\n    y = tf.nn.softmax(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\'):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, n_classes), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar, logits = model(env.x, logits=True, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    with tf.variable_scope(\'loss\'):\n        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n                                                       logits=logits)\n        env.loss = tf.reduce_mean(xent, name=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        env.train_op = optimizer.minimize(env.loss)\n\n    env.saver = tf.train.Saver()\n\nwith tf.variable_scope(\'model\', reuse=True):\n    env.fgsm_eps = tf.placeholder(tf.float32, (), name=\'fgsm_eps\')\n    env.fgsm_epochs = tf.placeholder(tf.int32, (), name=\'fgsm_epochs\')\n    env.x_fgsm = fgm(model, env.x, epochs=env.fgsm_epochs, eps=env.fgsm_eps)\n\nprint(\'\\nInitializing graph\')\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\n\ndef evaluate(sess, env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(sess, env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                              env.y: y_data[start:end],\n                                              env.training: True})\n        if X_valid is not None:\n            evaluate(sess, env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(sess, \'model/{}\'.format(name))\n\n\ndef predict(sess, env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = sess.run(env.ybar, feed_dict={env.x: X_data[start:end]})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef make_fgsm(sess, env, X_data, epochs=1, eps=0.01, batch_size=128):\n    """"""\n    Generate FGSM by running env.x_fgsm.\n    """"""\n    print(\'\\nMaking adversarials via FGSM\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        adv = sess.run(env.x_fgsm, feed_dict={\n            env.x: X_data[start:end],\n            env.fgsm_eps: eps,\n            env.fgsm_epochs: epochs})\n        X_adv[start:end] = adv\n    print()\n\n    return X_adv\n\n\nprint(\'\\nTraining\')\n\ntrain(sess, env, X_train, y_train, X_valid, y_valid, load=False, epochs=5,\n      name=\'mnist\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(sess, env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial data\')\n\nX_adv = make_fgsm(sess, env, X_test, eps=0.02, epochs=12)\n\nprint(\'\\nEvaluating on adversarial data\')\n\nevaluate(sess, env, X_adv, y_test)\n\nprint(\'\\nRandomly sample adversarial data from each category\')\n\ny1 = predict(sess, env, X_test)\ny2 = predict(sess, env, X_adv)\n\nz0 = np.argmax(y_test, axis=1)\nz1 = np.argmax(y1, axis=1)\nz2 = np.argmax(y2, axis=1)\n\nX_tmp = np.empty((10, 28, 28))\ny_tmp = np.empty((10, 10))\nfor i in range(10):\n    print(\'Target {0}\'.format(i))\n    ind, = np.where(np.all([z0 == i, z1 == i, z2 != i], axis=0))\n    cur = np.random.choice(ind)\n    X_tmp[i] = np.squeeze(X_adv[cur])\n    y_tmp[i] = y2[cur]\n\nprint(\'\\nPlotting results\')\n\nfig = plt.figure(figsize=(10, 1.2))\ngs = gridspec.GridSpec(1, 10, wspace=0.05, hspace=0.05)\n\nlabel = np.argmax(y_tmp, axis=1)\nproba = np.max(y_tmp, axis=1)\nfor i in range(10):\n    ax = fig.add_subplot(gs[0, i])\n    ax.imshow(X_tmp[i], cmap=\'gray\', interpolation=\'none\')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel(\'{0} ({1:.2f})\'.format(label[i], proba[i]),\n                  fontsize=12)\n\nprint(\'\\nSaving figure\')\n\ngs.tight_layout(fig)\nos.makedirs(\'img\', exist_ok=True)\nplt.savefig(\'img/fgsm_mnist.png\')\n'"
example/jsma_mnist_10x10.py,37,"b'import os\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import jsma\n\n\nimg_size = 28\nimg_chan = 1\nn_classes = 10\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nto_categorical = tf.keras.utils.to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=10, name=\'logits\')\n    y = tf.nn.softmax(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\'):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, n_classes), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar, logits = model(env.x, logits=True, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    with tf.variable_scope(\'loss\'):\n        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n                                                       logits=logits)\n        env.loss = tf.reduce_mean(xent, name=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        env.train_op = optimizer.minimize(env.loss)\n\n    env.saver = tf.train.Saver()\n\nwith tf.variable_scope(\'model\', reuse=True):\n    env.target = tf.placeholder(tf.int32, (), name=\'target\')\n    env.adv_epochs = tf.placeholder_with_default(20, shape=(), name=\'epochs\')\n    env.adv_eps = tf.placeholder_with_default(0.2, shape=(), name=\'eps\')\n    env.x_jsma = jsma(model, env.x, env.target, eps=env.adv_eps,\n                      epochs=env.adv_epochs)\n\nprint(\'\\nInitializing graph\')\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\n\ndef evaluate(sess, env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(sess, env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                              env.y: y_data[start:end],\n                                              env.training: True})\n        if X_valid is not None:\n            evaluate(sess, env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(sess, \'model/{}\'.format(name))\n\n\ndef predict(sess, env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = sess.run(env.ybar, feed_dict={\n            env.x: X_data[start:end],\n            env.target: np.random.choice(n_classes)})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef make_jsma(sess, env, X_data, epochs=0.2, eps=1.0, batch_size=128):\n    """"""\n    Generate JSMA by running env.x_jsma.\n    """"""\n    print(\'\\nMaking adversarials via JSMA\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        feed_dict = {\n            env.x: X_data[start:end],\n            env.target: np.random.choice(n_classes),\n            env.adv_epochs: epochs,\n            env.adv_eps: eps}\n        adv = sess.run(env.x_jsma, feed_dict=feed_dict)\n        X_adv[start:end] = adv\n    print()\n\n    return X_adv\n\n\nprint(\'\\nTraining\')\n\ntrain(sess, env, X_train, y_train, X_valid, y_valid, load=False, epochs=5,\n      name=\'mnist\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(sess, env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial data\')\n\nX_adv = make_jsma(sess, env, X_test, epochs=30, eps=1.0)\n\nprint(\'\\nEvaluating on adversarial data\')\n\nevaluate(sess, env, X_adv, y_test)\n\nprint(\'\\nRandomly sample adversarial data from each category\')\n\nz0 = np.argmax(y_test, axis=1)\nz1 = np.argmax(predict(sess, env, X_test), axis=1)\nind = z0 == z1\n\nX_data = X_test[ind]\nlabels = z0[ind]\n\nX_adv = np.empty((10, 10, 28, 28))\n\nfor source in np.arange(10):\n    print(\'Source label {0}\'.format(source))\n\n    X_i = X_data[labels == source]\n\n    for i, xi in enumerate(X_i):\n        found = True\n        xi = xi[np.newaxis, :]\n\n        for target in np.arange(10):\n            print(\' [{0}/{1}] {2} -> {3}\'\n                  .format(i+1, X_i.shape[0], source, target), end=\'\')\n\n            if source == target:\n                xadv = xi.copy()\n            else:\n                feed_dict = {env.x: xi, env.target: target, env.adv_epochs: 30,\n                             env.adv_eps: 1.0}\n                xadv = sess.run(env.x_jsma, feed_dict=feed_dict)\n\n            yadv = predict(sess, env, xadv)\n            label = np.argmax(yadv.flatten())\n            found = target == label\n\n            if not found:\n                print(\' Fail\')\n                break\n\n            X_adv[source, target] = np.squeeze(xadv)\n            print(\' res: {0} {1:.2f}\'.format(label, np.max(yadv)))\n\n        if found:\n            break\n\nprint(\'\\nGenerating figure\')\n\nfig = plt.figure(figsize=(10, 10))\ngs = gridspec.GridSpec(10, 10, wspace=0.1, hspace=0.1)\n\nfor i in range(10):\n    for j in range(10):\n        ax = fig.add_subplot(gs[i, j])\n        ax.imshow(X_adv[i, j], cmap=\'gray\', interpolation=\'none\')\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        if i == j:\n            for spine in ax.spines:\n                ax.spines[spine].set_color(\'green\')\n                ax.spines[spine].set_linewidth(5)\n\n        if ax.is_first_col():\n            ax.set_ylabel(i, fontsize=20, rotation=\'horizontal\', ha=\'right\')\n        if ax.is_last_row():\n            ax.set_xlabel(j, fontsize=20)\n\ngs.tight_layout(fig)\nos.makedirs(\'img\', exist_ok=True)\nplt.savefig(\'img/jsma_mnist_10x10.png\')\n'"
example/jsma_mnist_diff.py,37,"b'import os\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import jsma\n\n\nimg_size = 28\nimg_chan = 1\nn_classes = 10\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nto_categorical = tf.keras.utils.to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=10, name=\'logits\')\n    y = tf.nn.softmax(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\'):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, n_classes), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar, logits = model(env.x, logits=True, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    with tf.variable_scope(\'loss\'):\n        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n                                                       logits=logits)\n        env.loss = tf.reduce_mean(xent, name=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        env.train_op = optimizer.minimize(env.loss)\n\n    env.saver = tf.train.Saver()\n\nwith tf.variable_scope(\'model\', reuse=True):\n    env.target = tf.placeholder(tf.int32, (), name=\'target\')\n    env.adv_epochs = tf.placeholder_with_default(20, shape=(), name=\'epochs\')\n    env.adv_eps = tf.placeholder_with_default(0.2, shape=(), name=\'eps\')\n    env.x_jsma = jsma(model, env.x, env.target, eps=env.adv_eps,\n                      epochs=env.adv_epochs, score_fn=lambda t, o: t - o)\n\nprint(\'\\nInitializing graph\')\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\n\ndef evaluate(sess, env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(sess, env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                              env.y: y_data[start:end],\n                                              env.training: True})\n        if X_valid is not None:\n            evaluate(sess, env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(sess, \'model/{}\'.format(name))\n\n\ndef predict(sess, env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = sess.run(env.ybar, feed_dict={\n            env.x: X_data[start:end],\n            env.target: np.random.choice(n_classes)})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef make_jsma(sess, env, X_data, epochs=0.2, eps=1.0, batch_size=128):\n    """"""\n    Generate JSMA by running env.x_jsma.\n    """"""\n    print(\'\\nMaking adversarials via JSMA\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        feed_dict = {\n            env.x: X_data[start:end],\n            env.target: np.random.choice(n_classes),\n            env.adv_epochs: epochs,\n            env.adv_eps: eps}\n        adv = sess.run(env.x_jsma, feed_dict=feed_dict)\n        X_adv[start:end] = adv\n    print()\n\n    return X_adv\n\n\nprint(\'\\nTraining\')\n\ntrain(sess, env, X_train, y_train, X_valid, y_valid, load=True, epochs=5,\n      name=\'mnist\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(sess, env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial data\')\n\nX_adv = make_jsma(sess, env, X_test, epochs=30, eps=1.0)\n\nprint(\'\\nEvaluating on adversarial data\')\n\nevaluate(sess, env, X_adv, y_test)\n\nprint(\'\\nRandomly sample adversarial data from each category\')\n\nz0 = np.argmax(y_test, axis=1)\nz1 = np.argmax(predict(sess, env, X_test), axis=1)\nind = z0 == z1\n\nX_data = X_test[ind]\nlabels = z0[ind]\n\nX_adv = np.empty((10, 10, 28, 28))\n\nfor source in np.arange(10):\n    print(\'Source label {0}\'.format(source))\n\n    X_i = X_data[labels == source]\n\n    for i, xi in enumerate(X_i):\n        found = True\n        xi = xi[np.newaxis, :]\n\n        for target in np.arange(10):\n            print(\' [{0}/{1}] {2} -> {3}\'\n                  .format(i+1, X_i.shape[0], source, target), end=\'\')\n\n            if source == target:\n                xadv = xi.copy()\n            else:\n                feed_dict = {env.x: xi, env.target: target, env.adv_epochs: 30,\n                             env.adv_eps: 1.0}\n                xadv = sess.run(env.x_jsma, feed_dict=feed_dict)\n\n            yadv = predict(sess, env, xadv)\n            label = np.argmax(yadv.flatten())\n            found = target == label\n\n            if not found:\n                print(\' Fail\')\n                break\n\n            X_adv[source, target] = np.squeeze(xadv)\n            print(\' res: {0} {1:.2f}\'.format(label, np.max(yadv)))\n\n        if found:\n            break\n\nprint(\'\\nGenerating figure\')\n\nfig = plt.figure(figsize=(10, 10))\ngs = gridspec.GridSpec(10, 10, wspace=0.1, hspace=0.1)\n\nfor i in range(10):\n    for j in range(10):\n        ax = fig.add_subplot(gs[i, j])\n        ax.imshow(X_adv[i, j], cmap=\'gray\', interpolation=\'none\')\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        if i == j:\n            for spine in ax.spines:\n                ax.spines[spine].set_color(\'green\')\n                ax.spines[spine].set_linewidth(5)\n\n        if ax.is_first_col():\n            ax.set_ylabel(i, fontsize=20, rotation=\'horizontal\', ha=\'right\')\n        if ax.is_last_row():\n            ax.set_xlabel(j, fontsize=20)\n\ngs.tight_layout(fig)\nos.makedirs(\'img\', exist_ok=True)\nplt.savefig(\'img/jsma_mnist_diff.png\')\n'"
example/jsma_mnist_scratch.py,37,"b'import os\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')           # noqa: E402\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport tensorflow as tf\n\nfrom attacks import jsma\n\n\nimg_size = 28\nimg_chan = 1\nn_classes = 10\n\n\nprint(\'\\nLoading MNIST\')\n\nmnist = tf.keras.datasets.mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\nX_train = X_train.astype(np.float32) / 255\nX_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\nX_test = X_test.astype(np.float32) / 255\n\nto_categorical = tf.keras.utils.to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(\'\\nSpliting data\')\n\nind = np.random.permutation(X_train.shape[0])\nX_train, y_train = X_train[ind], y_train[ind]\n\nVALIDATION_SPLIT = 0.1\nn = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\nX_valid = X_train[n:]\nX_train = X_train[:n]\ny_valid = y_train[n:]\ny_train = y_train[:n]\n\nprint(\'\\nConstruction graph\')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope(\'conv0\'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'conv1\'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding=\'same\', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope(\'flatten\'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope(\'mlp\'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=10, name=\'logits\')\n    y = tf.nn.softmax(logits_, name=\'ybar\')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope(\'model\'):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name=\'x\')\n    env.y = tf.placeholder(tf.float32, (None, n_classes), name=\'y\')\n    env.training = tf.placeholder_with_default(False, (), name=\'mode\')\n\n    env.ybar, logits = model(env.x, logits=True, training=env.training)\n\n    with tf.variable_scope(\'acc\'):\n        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name=\'acc\')\n\n    with tf.variable_scope(\'loss\'):\n        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n                                                       logits=logits)\n        env.loss = tf.reduce_mean(xent, name=\'loss\')\n\n    with tf.variable_scope(\'train_op\'):\n        optimizer = tf.train.AdamOptimizer()\n        env.train_op = optimizer.minimize(env.loss)\n\n    env.saver = tf.train.Saver()\n\nwith tf.variable_scope(\'model\', reuse=True):\n    env.target = tf.placeholder(tf.int32, (), name=\'target\')\n    env.adv_epochs = tf.placeholder_with_default(20, shape=(), name=\'epochs\')\n    env.adv_eps = tf.placeholder_with_default(0.2, shape=(), name=\'eps\')\n    env.x_jsma = jsma(model, env.x, env.target, eps=env.adv_eps, k=1,\n                      epochs=env.adv_epochs)\n\nprint(\'\\nInitializing graph\')\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\n\ndef evaluate(sess, env, X_data, y_data, batch_size=128):\n    """"""\n    Evaluate TF model by running env.loss and env.acc.\n    """"""\n    print(\'\\nEvaluating\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    loss, acc = 0, 0\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        cnt = end - start\n        batch_loss, batch_acc = sess.run(\n            [env.loss, env.acc],\n            feed_dict={env.x: X_data[start:end],\n                       env.y: y_data[start:end]})\n        loss += batch_loss * cnt\n        acc += batch_acc * cnt\n    loss /= n_sample\n    acc /= n_sample\n\n    print(\' loss: {0:.4f} acc: {1:.4f}\'.format(loss, acc))\n    return loss, acc\n\n\ndef train(sess, env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n          load=False, shuffle=True, batch_size=128, name=\'model\'):\n    """"""\n    Train a TF model by running env.train_op.\n    """"""\n    if load:\n        if not hasattr(env, \'saver\'):\n            return print(\'\\nError: cannot find saver op\')\n        print(\'\\nLoading saved model\')\n        return env.saver.restore(sess, \'model/{}\'.format(name))\n\n    print(\'\\nTrain model\')\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    for epoch in range(epochs):\n        print(\'\\nEpoch {0}/{1}\'.format(epoch + 1, epochs))\n\n        if shuffle:\n            print(\'\\nShuffling data\')\n            ind = np.arange(n_sample)\n            np.random.shuffle(ind)\n            X_data = X_data[ind]\n            y_data = y_data[ind]\n\n        for batch in range(n_batch):\n            print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n            start = batch * batch_size\n            end = min(n_sample, start + batch_size)\n            sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n                                              env.y: y_data[start:end],\n                                              env.training: True})\n        if X_valid is not None:\n            evaluate(sess, env, X_valid, y_valid)\n\n    if hasattr(env, \'saver\'):\n        print(\'\\n Saving model\')\n        os.makedirs(\'model\', exist_ok=True)\n        env.saver.save(sess, \'model/{}\'.format(name))\n\n\ndef predict(sess, env, X_data, batch_size=128):\n    """"""\n    Do inference by running env.ybar.\n    """"""\n    print(\'\\nPredicting\')\n    n_classes = env.ybar.get_shape().as_list()[1]\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample+batch_size-1) / batch_size)\n    yval = np.empty((n_sample, n_classes))\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        y_batch = sess.run(env.ybar, feed_dict={\n            env.x: X_data[start:end],\n            env.target: np.random.choice(n_classes)})\n        yval[start:end] = y_batch\n    print()\n    return yval\n\n\ndef make_jsma(sess, env, X_data, y, epochs=0.2, eps=1.0, batch_size=128):\n    """"""\n    Generate JSMA by running env.x_jsma.\n    """"""\n    print(\'\\nMaking adversarials via JSMA\')\n\n    n_sample = X_data.shape[0]\n    n_batch = int((n_sample + batch_size - 1) / batch_size)\n    X_adv = np.empty_like(X_data)\n\n    for batch in range(n_batch):\n        print(\' batch {0}/{1}\'.format(batch + 1, n_batch), end=\'\\r\')\n        start = batch * batch_size\n        end = min(n_sample, start + batch_size)\n        feed_dict = {\n            env.x: X_data[start:end],\n            env.target: y,\n            env.adv_epochs: epochs,\n            env.adv_eps: eps}\n        adv = sess.run(env.x_jsma, feed_dict=feed_dict)\n        X_adv[start:end] = adv\n    print()\n\n    return X_adv\n\n\nprint(\'\\nTraining\')\n\ntrain(sess, env, X_train, y_train, X_valid, y_valid, load=False, epochs=5,\n      name=\'mnist\')\n\nprint(\'\\nEvaluating on clean data\')\n\nevaluate(sess, env, X_test, y_test)\n\nprint(\'\\nGenerating adversarial from scratch\')\n\nblank = np.zeros((1, 28, 28, 1))\nX_adv = np.empty((10, 28, 28))\nproba = np.empty((10, 10))\nfor i in np.arange(10):\n    print(\'Target label {0}\'.format(i), end=\'\', flush=True)\n    xi = make_jsma(sess, env, blank, y=i, epochs=100, eps=0.5)\n    yi = predict(sess, env, xi)\n    X_adv[i] = np.squeeze(xi)\n    proba[i] = np.squeeze(yi)\n    print(\' proba: {0:.2f}\'.format(np.max(proba[i])))\n\nprint(\'\\nGenerating figure\')\n\nfig = plt.figure(figsize=(10, 1.2))\ngs = gridspec.GridSpec(1, 10, wspace=0.05, hspace=0.05)\n\nfor i in range(10):\n    ax = fig.add_subplot(gs[0, i])\n    bimg = np.copy(X_adv[i])\n    bimg[bimg < 0.5] = 0.0\n    bimg[bimg > 0.4] = 1.0\n    ax.imshow(bimg, cmap=\'gray\', interpolation=\'none\')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel(\'{0} ({1:.2f})\'.format(np.argmax(proba[i]),\n                                         np.max(proba[i])), fontsize=12)\n\ngs.tight_layout(fig)\nos.makedirs(\'img\', exist_ok=True)\nplt.savefig(\'img/jsma_mnist_scratch.png\')\n'"
example/jsma_profile.py,28,"b""import os\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python.client import timeline\n\nfrom attacks import jsma\n\n\nimg_size = 28\nimg_chan = 1\nn_classes = 10\n\nprint('\\nConstruction graph')\n\n\ndef model(x, logits=False, training=False):\n    with tf.variable_scope('conv0'):\n        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n                             padding='same', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope('conv1'):\n        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n                             padding='same', activation=tf.nn.relu)\n        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n\n    with tf.variable_scope('flatten'):\n        shape = z.get_shape().as_list()\n        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n\n    with tf.variable_scope('mlp'):\n        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n        z = tf.layers.dropout(z, rate=0.25, training=training)\n\n    logits_ = tf.layers.dense(z, units=10, name='logits')\n    y = tf.nn.softmax(logits_, name='ybar')\n\n    if logits:\n        return y, logits_\n    return y\n\n\nclass Dummy:\n    pass\n\n\nenv = Dummy()\n\n\nwith tf.variable_scope('model'):\n    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n                           name='x')\n    env.y = tf.placeholder(tf.float32, (None, n_classes), name='y')\n    env.training = tf.placeholder_with_default(False, (), name='mode')\n\n    env.ybar, logits = model(env.x, logits=True, training=env.training)\n\nwith tf.variable_scope('model', reuse=True):\n    env.target = tf.placeholder(tf.int32, (), name='target')\n    env.adv_epochs = tf.placeholder_with_default(20, shape=(), name='epochs')\n    env.adv_eps = tf.placeholder_with_default(0.2, shape=(), name='eps')\n    env.x_jsma = jsma(model, env.x, env.target, eps=env.adv_eps, k=1,\n                      epochs=env.adv_epochs)\n\nprint('\\nInitializing graph')\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.local_variables_initializer())\n\noptions = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\nrun_metadata = tf.RunMetadata()\n\nX_data = np.random.random((1000, 28, 28, 1))\nbatch_size = 128\nn_sample = X_data.shape[0]\nn_batch = int((n_sample + batch_size - 1) / batch_size)\nX_adv = np.empty_like(X_data)\n\nfor batch in range(n_batch):\n    print(' batch {0}/{1}'.format(batch + 1, n_batch), end='\\r')\n    start = batch * batch_size\n    end = min(n_sample, start + batch_size)\n    feed_dict = {\n        env.x: X_data[start:end],\n        env.target: np.random.choice(n_classes),\n        env.adv_epochs: 1000,\n        env.adv_eps: 0.1}\n    adv = sess.run(env.x_jsma, feed_dict=feed_dict, options=options,\n                   run_metadata=run_metadata)\n    X_adv[start:end] = adv\n    print()\n\n    fetched_timeline = timeline.Timeline(run_metadata.step_stats)\n    chrome_trace = fetched_timeline.generate_chrome_trace_format()\n    with open('timeline_{}.json'.format(batch), 'w') as f:\n        f.write(chrome_trace)\n"""
