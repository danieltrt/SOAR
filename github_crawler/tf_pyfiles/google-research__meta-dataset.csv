file_path,api_count,code
meta_dataset/__init__.py,0,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n'"
meta_dataset/analyze.py,23,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Script for aggregating the eval summaries into dicts.\n\nThis script assumes that the evaluation has already been ran (and has produced\nthe eval summaries from which this script reads).\n\nCreates four dicts: One maps each observed \'shot\' to a list of class precisions\nobtained by classes that had that shot (regardless of shots of other classes in\nthe same episode). The second maps each observed \'way\' of an episode to a list\nof accuracies of the episodes with that way. The third maps each observed height\n(of the lowest common ancestor of pairs of leaves corresponding to the Synsets\nof ImageNet binary classification tasks from the training subgraph) to the\naccuracy of those tasks, aiming to study how the fine- or coarse- grainedness of\na task affects its difficulty. The fourth maps each observed degree of imbalance\n(w.r.t the numbers of shots of the different classes in the task) to the\naccuracy of the corresponding episodes.\nSummarized versions are also created that keep only the mean and confidence\nintervals instead of the list of all precisons or accuracies, resp. as the\nvalues of these dicts.\n\nSample command:\n# pylint: disable=line-too-long\npython -m meta_dataset.analyze \\\n  --alsologtostderr \\\n  --eval_finegrainedness \\\n  --eval_finegrainedness_split=test \\\n  --root_dir=<root_dir> \\\n# pylint: enable=line-too-long\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\n\nfrom absl import logging\nfrom meta_dataset.data import dataset_spec\nfrom meta_dataset.data import imagenet_specification as imagenet_spec\nfrom meta_dataset.data import learning_spec\nimport numpy as np\nfrom six.moves import range\nimport six.moves.cPickle as pkl\nimport tensorflow.compat.v1 as tf\n\nFLAGS = tf.flags.FLAGS\n\n# Will search for all event files in this root dir.\ntf.flags.DEFINE_string(\n    \'root_dir\',\n    \'\',\n    \'The root \'\n    \'directory to look in for sub-directory trees containing event files.\')\n\ntf.flags.DEFINE_bool(\n    \'eval_imbalance\', False, \'Whether to perform only 2-way evaluation for \'\n    \'assessing performance as a function of how imbalanced each task is.\')\n\ntf.flags.DEFINE_bool(\n    \'eval_finegrainedness\', False, \'Whether to perform only 2-way ImageNet \'\n    \'evaluation for assessing performance as a function of how finegrained \'\n    \'each task is. This differs from usual ImageNet eval in the sampling \'\n    \'procedure used to get episodes, and therefore requires its own setting.\')\n\ntf.flags.DEFINE_enum(\n    \'eval_finegrainedness_split\', \'test\', [\'train\', \'valid\', \'test\'], \'The \'\n    \'split whose results we want to use for the fine-grainedness analysis.\'\n    \'Contrary to most analyses which are performed on the test split only, the \'\n    \'fine-grainedness analysis may also be performed on the train or valid \'\n    \'sub-graphs of ImageNet too, since the test sub-graph evidently does not \'\n    \'exhibit enough variation in the fine-grainedness of its different tasks \'\n    \'to allow for a meaningful analysis.\')\n\n# To restrict to evaluating on ImageNet, the following should be set to \'2\'.\n# The valid sub-experiment id\'s start from \'1\'.\n# TODO(etriantafillou): Adapt the following for external users. In particular,\n# we shouldn\'t necessarily assume the directory structure where there is one\n# directory per experiment id, which corresponds to different hyperparams.\ntf.flags.DEFINE_enum(\n    \'restrict_to_subexperiment\', \'0\', [str(num) for num in range(11)], \'If \'\n    \'positive, restricts to using the summaries in the sub-experiment whose id \'\n    \'is the given number. This corresponds to a specific hyper (e.g. choice of \'\n    \'evaluation dataset). Valid experiment ids start from ""1"".\')\n\ntf.flags.DEFINE_bool(\n    \'force_recompute\', False, \'Whether to always re-compute (and overwrite) \'\n    \'the dictionaries regardless of whether they have already been computed.\')\ntf.flags.DEFINE_string(\'records_root_dir\', \'\',\n                       \'Root directory containing a subdirectory per dataset.\')\nFLAGS = tf.flags.FLAGS\n\n\ndef compute_class_precision(class_id, logits, targets):\n  """"""Computes the precision for class_id.\n\n  The precision for a class is defined as the number of examples of that class\n  that are correctly classified over its total number of examples.\n\n  Args:\n    class_id: An int, in the range between 0 and the number of classes.\n    logits: A float array, of shape [num_test_examples, num_classes].\n    targets: An int array, of the same shape as logits.\n\n  Returns:\n    precision: A float. The precision for the given class.\n  """"""\n  # Get the section of the logits that correspond to class_id.\n  class_logits_ids = np.where(targets == class_id)[0]\n  # [# test examples of class_id, way].\n  class_logits = logits[class_logits_ids]\n  # [# test examples of class_id]\n  class_preds = np.argmax(class_logits, axis=1)\n  precision = np.mean(np.equal(class_preds, class_id))\n  return precision\n\n\ndef compute_episode_accuracy(logits, targets):\n  """"""Computes the accuracy for the episode.\n\n  The accuracy for the episode is the proportion of correctly classified test\n  examples from the overall number of test examples.\n\n  Args:\n    logits: A float array, of shape [num_test_examples, num_classes].\n    targets: An int array, of the same shape as logits.\n\n  Returns:\n    accuracy: A float. The precision for the given class.\n  """"""\n  preds = np.argmax(logits, axis=1)\n  return np.mean(np.equal(preds, targets))\n\n\ndef get_shot_to_precision(shots, logits, targets):\n  """"""Performance of a particular class as a function of its \'shot\'.\n\n  Args:\n    shots: A list containing a np.array per episode. The shape of an episode\'s\n      array is the [way]. Stores the \'shot\' of each class, ie. the number of\n      training examples that that class has in the support set.\n    logits: A list containing a np.array per episode. The shape of an episode\'s\n      array is [size of test set, way].\n    targets: A list containing a np.array per episode. The shape of an episode\'s\n      array is [size of test set]. This contains integers in the range from 0 to\n      the way of the episode.\n\n  Returns:\n    shot_to_precision: A dict that maps each \'observed\' shot to a list that\n      stores the precision obtained for that shot (each entry in this list is\n      the precision of a particular class that had this shot, regardless of the\n      shots of the other classes in the episode.).\n  """"""\n  shot_to_precision = collections.defaultdict(list)\n  for episode_num, episode_shots in enumerate(shots):\n    episode_logits = logits[episode_num]\n    episode_targets = targets[episode_num]\n    for class_id, class_shot in enumerate(episode_shots):\n      class_precision = compute_class_precision(class_id, episode_logits,\n                                                episode_targets)\n      shot_to_precision[class_shot].append(class_precision)\n  return shot_to_precision\n\n\ndef get_imbalance_to_accuracy(class_props, logits, targets):\n  """"""Accuracy as a function of imabalance.\n\n  Args:\n    class_props: A list containing a np.array per episode. The shape of an\n      episode\'s array is the [way]. Stores the \'normalized shot\' of each class,\n      ie. the proportion of the examples of that class that are in the support\n      set of the episode.\n    logits: A list containing a np.array per episode. The shape of an episode\'s\n      array is [size of test set, way].\n    targets: A list containing a np.array per episode. The shape of an episode\'s\n      array is [size of test set]. This contains integers in the range from 0 to\n      the way of the episode.\n\n  Returns:\n    imbalance_to_accuracy: A dict mapping each observed imbalance (a float) to a\n    list that stores the accuracy of the episodes characterized by that degree\n    of imbalance.\n\n  Raises:\n    ValueError: There should have been exactly 2 elements in the list of each\n      episode\'s class id\'s.\n  """"""\n  imbalance_to_accuracy = collections.defaultdict(list)\n  for episode_num, episode_class_props in enumerate(class_props):\n    if len(episode_class_props) != 2:\n      raise ValueError(\n          \'There should have been exactly 2 elements in the list \'\n          ""of each episode\'s class_props (we only perform the ""\n          \'imbalance analysis on binary tasks). Instead, found: {}\'.format(\n              len(episode_class_props)))\n    # Compute imbalance.\n    imbalance = max(episode_class_props) - min(episode_class_props)\n    # Compute the accuracy of the episode.\n    episode_logits = logits[episode_num]\n    episode_targets = targets[episode_num]\n    episode_acc = compute_episode_accuracy(episode_logits, episode_targets)\n    imbalance_to_accuracy[imbalance].append(episode_acc)\n  return imbalance_to_accuracy\n\n\ndef get_way_to_accuracy(ways, logits, targets):\n  """"""Accuracy as a function of the episode\'s way.\n\n  Args:\n    ways: A list containing the \'way\' of each episode.\n    logits: A list containing a np.array per episode. The shape of an episode\'s\n      array is [size of test set, way].\n    targets: A list containing a np.array per episode. The shape of an episode\'s\n      array is [size of test set]. This contains integers in the range from 0 to\n      the way of the episode.\n\n  Returns:\n    way_to_accuracy: A dict that maps each \'observed\' way to a list that\n      stores the accuracy obtained for different episodes of that way.\n  """"""\n  way_to_accuracy = collections.defaultdict(list)\n  for episode_num, episode_way in enumerate(ways):\n    episode_logits = logits[episode_num]\n    assert episode_way == episode_logits.shape[1]\n    episode_targets = targets[episode_num]\n    episode_acc = compute_episode_accuracy(episode_logits, episode_targets)\n    way_to_accuracy[episode_way].append(episode_acc)\n  return way_to_accuracy\n\n\ndef get_finegrainedness_split_enum():\n  """"""Returns the Split corresponding to FLAGS.eval_finegrainedness_split.""""""\n  if FLAGS.eval_finegrainedness_split == \'train\':\n    split_enum = learning_spec.Split.TRAIN\n  elif FLAGS.eval_finegrainedness_split == \'valid\':\n    split_enum = learning_spec.Split.VALID\n  elif FLAGS.eval_finegrainedness_split == \'test\':\n    split_enum = learning_spec.Split.TEST\n  return split_enum\n\n\ndef get_synsets_from_class_ids(class_ids):\n  """"""Returns the Synsets of the appropriate subgraph corresponding to class_ids.\n\n  For each class id in class_ids, the corresponding Synset is found among the\n  Synsets of the subgraph corresponding to the split that is chosen for the\n  fine-grainedness analysis.\n\n  Args:\n    class_ids: A np.array of ints in the range between 1 and the total number of\n      classes that contains the two class id\'s chosen for an episode.\n\n  Returns:\n    A list of Synsets.\n\n  Raises:\n    ValueError: The dataset specification is not found in the expected location.\n  """"""\n  # First load the DatasetSpecification of ImageNet.\n  dataset_records_path = os.path.join(FLAGS.records_root_dir, \'ilsvrc_2012\')\n  imagenet_data_spec = dataset_spec.load_dataset_spec(dataset_records_path)\n\n  # A set of Synsets of the split\'s subgraph.\n  split_enum = get_finegrainedness_split_enum()\n  split_subgraph = imagenet_data_spec.split_subgraphs[split_enum]\n\n  # Go from class_ids (integers in the range from 1 to the total number of\n  # classes in the Split) to WordNet id\'s, e.g n02075296.\n  wn_ids = []\n  for class_id in class_ids:\n    wn_ids.append(imagenet_data_spec.class_names[class_id])\n\n  # Find the Synsets in split_subgraph whose WordNet id\'s are wn_ids.\n  synsets = imagenet_spec.get_synsets_from_ids(wn_ids, split_subgraph)\n  return [synsets[wn_id] for wn_id in wn_ids]\n\n\ndef get_height_to_accuracy(class_ids, logits, targets):\n  """"""Accuracy as a function of the height of class\' the lowest common ancestor.\n\n  This is only applicable to 2-way ImageNet episodes. Given the class set of\n  each episode, we find the corresponding 2 leaves of the ImageNet graph and\n  compute the lowest common ancestor of those leaves. Its height is computed as\n  the maximum over the length of the paths from that node to each of the two\n  leaves. This height is the estimate of fine-grainedness. Intuitively, the\n  larger the height, the more coarse-grained the episode\'s classification task.\n\n  Args:\n    class_ids: A list containing a np.array per episode that contains the two\n      class id\'s chosen for the episode\'s binary classification task. These id\'s\n      are ints in the range between 1 and the total number of classes.\n    logits: A list containing a np.array per episode. The shape of an episode\'s\n      array is [size of test set, way].\n    targets: A list containing a np.array per episode. The shape of an episode\'s\n      array is [size of test set]. This contains integers in the range from 0 to\n      the way of the episode.\n\n  Returns:\n    height_to_accuracy: A dict that maps each \'observed\' height to a list that\n      contains the accuracies obtained for different episodes with that height.\n\n  Raises:\n    ValueError: There should have been exactly 2 elements in the list of each\n      episode\'s class id\'s.\n  """"""\n  height_to_accuracy = collections.defaultdict(list)\n  for episode_num, episode_class_ids in enumerate(class_ids):\n    if len(episode_class_ids) != 2:\n      raise ValueError(\'There should have been exactly 2 elements in the list \'\n                       ""of each episode\'s class id\'s."")\n    # Get the Synsets corresponding to the class id\'s episode_class_ids.\n    episode_synsets = get_synsets_from_class_ids(episode_class_ids)\n    assert len(episode_synsets) == 2, (\'Fine- vs coarse- grained analysis \'\n                                       \'should be restricted to binary tasks.\')\n    # Compute the height of the lowest common ancestor of the episode\'s Synsets.\n    _, height = imagenet_spec.get_lowest_common_ancestor(\n        episode_synsets[0], episode_synsets[1])\n    # Compute the accuracy of the episode.\n    episode_logits = logits[episode_num]\n    episode_targets = targets[episode_num]\n    episode_acc = compute_episode_accuracy(episode_logits, episode_targets)\n    height_to_accuracy[height].append(episode_acc)\n  return height_to_accuracy\n\n\ndef summarize_values_stats(d):\n  """"""Summarizes each list value of dict d into a mean and confidence interval.\n\n  The summarized version of an empty dictionary, is also empty.\n\n  Args:\n    d: A dict where each value is a list.\n\n  Returns:\n    d_mean_ci: If d is not empty, a dict with the same keys as d but with each\n      value which was originally a list replaced with a tuple of the mean of\n      that list and the corresponding confidence interval.\n\n  Raises:\n    ValueError: The values of d are not lists.\n  """"""\n  if not d:\n    return {}\n\n  for v in d.values():\n    if not isinstance(v, list):\n      raise ValueError(\'The values of the provided dict are not lists.\')\n\n  d_mean_ci = {}\n  for k, v in d.items():\n    mean = np.mean(v)\n    ci = np.std(v) * 1.96 / np.sqrt(len(v))\n    d_mean_ci[k] = (mean, ci)\n  return d_mean_ci\n\n\ndef read_data(input_path, do_finegrainedness_analysis, do_imbalance_analysis):\n  """"""Reads the data from the evaluation files.\n\n  Args:\n    input_path: The path to the event file to read from.\n    do_finegrainedness_analysis: Whether to perform analysis of fine- vs coarse-\n      grained tasks. This affects the tags that are necessary to find in the\n      event files.\n    do_imbalance_analysis: Whether to analyze performance for episodes that are\n      characterized by different degrees of imbalance.\n\n  Returns:\n    ways: A list containing the \'way\' of each episode.\n    shots: A list containing a np.array per episode. The shape of an episode\'s\n      array is the [way].\n    class_ids: A list containing a np.array per episode which contains two class\n      id\'s representing the two classes chosen for that binary classification.\n    test_logits: A list containing a np.array per episode. The shape of an\n      episode\'s array is [size of test set, way].\n    test_targets: A list containing a np.array per episode. The shape of an\n      episode\'s array is [size of test set]. This contains integers in the range\n      from 0 to the way of the episode.\n\n  Raises:\n    ValueError: Finegrainedness analysis is requested but no summaries of\n      class_ids are found for the provided split, or imbalance analysis is\n      requested but no summaries of class_props are found.\n  """"""\n  split = (\n      FLAGS.eval_finegrainedness_split\n      if FLAGS.eval_finegrainedness else \'test\')\n  logging.info(\'Reading event file %s for summaries of split %s.\', input_path,\n               split)\n  (ways, shots, class_props, class_ids, test_logits,\n   test_targets) = [], [], [], [], [], []\n  tags = set()\n  for e in tf.train.summary_iterator(input_path):\n    for v in e.summary.value:\n      tags.add(v.tag)\n      if v.tag == \'{}_way\'.format(split):\n        ways.append(v.simple_value)\n      elif v.tag == \'{}_shots\'.format(split):\n        shots.append(tf.make_ndarray(v.tensor))\n      elif v.tag == \'{}_class_props\'.format(split):\n        class_props.append(tf.make_ndarray(v.tensor))\n      elif v.tag == \'{}_class_ids\'.format(split):\n        class_ids.append(tf.make_ndarray(v.tensor))\n      elif v.tag == \'{}_test_logits\'.format(split):\n        test_logits.append(tf.make_ndarray(v.tensor))\n      elif v.tag == \'{}_test_targets\'.format(split):\n        test_targets.append(tf.make_ndarray(v.tensor))\n  if do_finegrainedness_analysis and not class_ids:\n    raise ValueError(\n        \'No summaries found with tag: {}_class_ids. The tags that exist in the \'\n        \'event file are: {}.\'.format(split, list(tags)))\n  if do_imbalance_analysis and not class_props:\n    raise ValueError(\n        \'No summaries found with tag: {}_class_props. The tags that exist in \'\n        \'the event file are: {}.\'.format(split, list(tags)))\n  return ways, shots, class_props, class_ids, test_logits, test_targets\n\n\ndef write_pkl(output_data, output_path):\n  """"""Save output_data to the pickle at output_path.""""""\n  with tf.io.gfile.GFile(output_path, \'wb\') as f:\n    pkl.dump(output_data, f, protocol=pkl.HIGHEST_PROTOCOL)\n  logging.info(\'Dumped data with keys: %s to location %s\',\n               list(output_data.keys()), output_path)\n\n\ndef read_pkl(output_path):\n  """"""Returns the contents of a pickle file or False if it doesn\'t exist.""""""\n  if tf.io.gfile.exists(output_path):\n    with tf.io.gfile.GFile(output_path, \'rb\') as f:\n      data = pkl.load(f)\n      logging.info(\'Read data with keys: %s\', list(data.keys()))\n      return data\n  else:\n    return False\n\n\ndef get_event_files(root_dir):\n  """"""Returns all event files from the subdirectories of root_dir.\n\n  Args:\n    root_dir: A str. The root directory of evaluation experiments.\n  Assumes the following directory organization: root_dir contains a sub-\n    directory for every dataset, and each of those contains a directory named\n    \'summaries\' where an event file can be found.\n  """"""\n  paths_to_events = []\n  summaries_dir = os.path.join(root_dir, \'summaries\')\n  assert tf.io.gfile.isdir(summaries_dir), (\'Could not find summaries in %s.\' %\n                                            root_dir)\n\n  if int(FLAGS.restrict_to_subexperiment) > 0:\n    child_dirs = [os.path.join(summaries_dir, FLAGS.restrict_to_subexperiment)]\n  else:\n    child_dirs = [\n        os.path.join(summaries_dir, f)\n        for f in tf.io.gfile.listdir(summaries_dir)\n    ]\n  # Filter out non-directory files, if any.\n  child_dirs = [child for child in child_dirs if tf.io.gfile.isdir(child)]\n  logging.info(\'Looking for events in dirs: %s\', child_dirs)\n  for child_dir in child_dirs:\n    for file_name in tf.io.gfile.listdir(child_dir):\n      if \'event\' in file_name:\n        paths_to_events.append(os.path.join(child_dir, file_name))\n  logging.info(\'Found events: %s\', paths_to_events)\n  return paths_to_events\n\n\ndef get_output_path(path_to_event_file):\n  """"""Returns the path where the pickle of output data will be stored.\n\n  Args:\n    path_to_event_file: The path where the event file lives. Used so that the\n      output pickle is stored in that same directory.\n  """"""\n  # Get the directory where the event file was found.\n  event_dir, _ = os.path.split(path_to_event_file)\n  out_pickle_path = os.path.join(event_dir, \'aggregated_summary_dicts.pklz\')\n  return out_pickle_path\n\n\ndef combine_dicts(dict_list):\n  """"""Combines the dictionaries in dict_list.\n\n  Args:\n    dict_list: A list of dicts. Each dict maps integers to lists.\n\n  Returns:\n    combined: A dict that has for every key the \'combined\' values of all dicts\n    in dict list that have that key. Combining the values for a key amounts to\n    concatenating the corresponding lists.\n  """"""\n  combined = collections.defaultdict(list)\n  for d in dict_list:\n    for k, v in d.items():\n      combined[k].extend(v)\n  return combined\n\n\ndef analyze_events(paths_to_event_files, experiment_root_dir,\n                   do_finegrainedness_analysis, do_imbalance_analysis,\n                   force_recompute):\n  """"""Analyzes each event file and stores the .pklz in the corresponding dir.""""""\n\n  # Aggregate stats across all event files and write (the summarized version of\n  # those) to the root directory.\n  shot_to_precision_all = []\n  way_to_accuracy_all = []\n  height_to_accuracy_all = []\n  imbalance_to_accuracy_all = []\n\n  for path_to_event in paths_to_event_files:\n    output_pickle = get_output_path(path_to_event)\n\n    # First check if the required data is already computed and written.\n    maybe_data = False if force_recompute else read_pkl(output_pickle)\n    if maybe_data:\n      logging.info(\'Output %s already exists. Skipping it.\', output_pickle)\n      shot_to_precision = maybe_data[\'shot_to_precision\']\n      way_to_accuracy = maybe_data[\'way_to_accuracy\']\n      height_to_accuracy = maybe_data[\'height_to_accuracy\']\n      imbalance_to_accuracy = maybe_data[\'imbalance_to_accuracy\']\n\n    else:\n      # Read the data from the event files.\n      (ways, shots, class_props, class_ids, test_logits,\n       test_targets) = read_data(path_to_event, do_finegrainedness_analysis,\n                                 do_imbalance_analysis)\n\n      # A dict mapping each observed \'shot\' to a list of class precisions\n      # obtained by classes that had that shot (regardless of shots of other\n      # classes in the same episode).\n      shot_to_precision = get_shot_to_precision(shots, test_logits,\n                                                test_targets)\n\n      # A dict mapping each observed \'way\' of an episode to a list of accuracies\n      # of the episodes with that way.\n      way_to_accuracy = get_way_to_accuracy(ways, test_logits, test_targets)\n\n      # A dict mapping the height of the lowest common ancestor of each pair of\n      # leaves defining the binary classiifcation task to the task\'s accuracy.\n      height_to_accuracy = {}\n      if do_finegrainedness_analysis:\n        height_to_accuracy = get_height_to_accuracy(class_ids, test_logits,\n                                                    test_targets)\n\n      # A dict mapping the degree of imabalance of tasks to their accuracy.\n      imbalance_to_accuracy = {}\n      if do_imbalance_analysis:\n        imbalance_to_accuracy = get_imbalance_to_accuracy(\n            class_props, test_logits, test_targets)\n\n      # Keep only the mean and confidence intervals instead of the list of all\n      # precisons or accuracies, resp. as the values of these dicts.\n      shot_to_precision_summarized = summarize_values_stats(shot_to_precision)\n      way_to_accuracy_summarized = summarize_values_stats(way_to_accuracy)\n      height_to_accuracy_summarized = summarize_values_stats(height_to_accuracy)\n      imbalance_to_accuracy_summarized = summarize_values_stats(\n          imbalance_to_accuracy)\n\n      # Save the two dicts to a pickle at the designated location.\n      output_data = {\n          \'shot_to_precision\': shot_to_precision,\n          \'way_to_accuracy\': way_to_accuracy,\n          \'height_to_accuracy\': height_to_accuracy,\n          \'imbalance_to_accuracy\': imbalance_to_accuracy,\n          \'shot_to_precision_summarized\': shot_to_precision_summarized,\n          \'way_to_accuracy_summarized\': way_to_accuracy_summarized,\n          \'height_to_accuracy_summarized\': height_to_accuracy_summarized,\n          \'imbalance_to_accuracy_summarized\': imbalance_to_accuracy_summarized,\n      }\n      write_pkl(output_data, output_pickle)\n\n    shot_to_precision_all.append(shot_to_precision)\n    way_to_accuracy_all.append(way_to_accuracy)\n    height_to_accuracy_all.append(height_to_accuracy)\n    imbalance_to_accuracy_all.append(imbalance_to_accuracy)\n\n  # Now aggregate the stats across datasets.\n  shot_to_precision_all = combine_dicts(shot_to_precision_all)\n  way_to_accuracy_all = combine_dicts(way_to_accuracy_all)\n  height_to_accuracy_all = combine_dicts(height_to_accuracy_all)\n  imbalance_to_accuracy_all = combine_dicts(imbalance_to_accuracy_all)\n\n  # Summarize them.\n  shot_to_precision_all_summarized = summarize_values_stats(\n      shot_to_precision_all)\n  way_to_accuracy_all_summarized = summarize_values_stats(way_to_accuracy_all)\n  height_to_accuracy_all_summarized = summarize_values_stats(\n      height_to_accuracy_all)\n  imbalance_to_accuracy_all_summarized = summarize_values_stats(\n      imbalance_to_accuracy_all)\n\n  # Save the dicts to a pickle at the designated location.\n  output_data = {\n      \'shot_to_precision\': shot_to_precision_all,\n      \'way_to_accuracy\': way_to_accuracy_all,\n      \'height_to_accuracy\': height_to_accuracy_all,\n      \'shot_to_precision_summarized\': shot_to_precision_all_summarized,\n      \'way_to_accuracy_summarized\': way_to_accuracy_all_summarized,\n      \'height_to_accuracy_summarized\': height_to_accuracy_all_summarized,\n      \'imbalance_to_accuracy_summarized\': imbalance_to_accuracy_all_summarized\n  }\n  pickle_name_base = \'aggregated_summary_dicts\'\n  if int(FLAGS.restrict_to_subexperiment) > 0:\n    pickle_name_base += \'_eval_{}\'.format(FLAGS.restrict_to_subexperiment)\n  output_pickle = os.path.join(experiment_root_dir, pickle_name_base + \'.pklz\')\n  write_pkl(output_data, output_pickle)\n\n\ndef main(argv):\n  del argv\n  paths_to_event_files = get_event_files(FLAGS.root_dir)\n  if not paths_to_event_files:\n    logging.info(\'No event files found.\')\n    return\n  analyze_events(paths_to_event_files, FLAGS.root_dir,\n                 FLAGS.eval_finegrainedness, FLAGS.eval_imbalance,\n                 FLAGS.force_recompute)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.app.run(main)\n'"
meta_dataset/learner.py,241,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Learner related code.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport functools\nfrom absl import logging\nimport gin.tf\nimport six\nfrom six.moves import range\nfrom six.moves import zip\nimport tensorflow.compat.v1 as tf\n\nFLAGS = tf.flags.FLAGS\n\n\ndef conv2d(x, w, stride=1, b=None, padding=\'SAME\'):\n  """"""conv2d returns a 2d convolution layer with full stride.""""""\n  h = tf.nn.conv2d(x, w, strides=[1, stride, stride, 1], padding=padding)\n  if b is not None:\n    h += b\n  return h\n\n\ndef relu(x, use_bounded_activation=False):\n  if use_bounded_activation:\n    return tf.nn.relu6(x)\n  else:\n    return tf.nn.relu(x)\n\n\ndef _compute_prototypes(embeddings, labels):\n  """"""Computes class prototypes over the last dimension of embeddings.\n\n  Args:\n    embeddings: Tensor of examples of shape [num_examples, embedding_size].\n    labels: Tensor of one-hot encoded labels of shape [num_examples,\n      num_classes].\n\n  Returns:\n    prototypes: Tensor of class prototypes of shape [num_classes,\n    embedding_size].\n  """"""\n  labels = tf.cast(labels, tf.float32)\n\n  # [num examples, 1, embedding size].\n  embeddings = tf.expand_dims(embeddings, 1)\n\n  # [num examples, num classes, 1].\n  labels = tf.expand_dims(labels, 2)\n\n  # Sums each class\' embeddings. [num classes, embedding size].\n  class_sums = tf.reduce_sum(labels * embeddings, 0)\n\n  # The prototype of each class is the averaged embedding of its examples.\n  class_num_images = tf.reduce_sum(labels, 0)  # [way].\n  prototypes = class_sums / class_num_images\n\n  return prototypes\n\n\ndef compute_prototypes(embeddings, labels):\n  """"""Computes class prototypes over features.\n\n  Flattens and reshapes the features if they are not already flattened.\n  Args:\n    embeddings: Tensor of examples of shape [num_examples, embedding_size] or\n      [num_examples, spatial_dim, spatial_dim n_features].\n    labels: Tensor of one-hot encoded labels of shape [num_examples,\n      num_classes].\n\n  Returns:\n    prototypes: Tensor of class prototypes of shape [num_classes,\n      embedding_size].\n  """"""\n  if len(embeddings.shape) > 2:\n    feature_shape = embeddings.shape.as_list()[1:]\n    n_images = tf.shape(embeddings)[0]\n    n_classes = tf.shape(labels)[-1]\n\n    vectorized_embedding = tf.reshape(embeddings, [n_images, -1])\n    vectorized_prototypes = _compute_prototypes(vectorized_embedding, labels)\n    prototypes = tf.reshape(vectorized_prototypes, [n_classes] + feature_shape)\n  else:\n    prototypes = _compute_prototypes(embeddings, labels)\n\n  return prototypes\n\n\n# TODO(tylerzhu): Accumulate batch norm statistics (moving {var, mean})\n# during training and use them during testing. However need to be careful\n# about leaking information across episodes.\n# Note: we should use ema object to accumulate the statistics for compatibility\n# with TF Eager.\ndef bn(x, params=None, moments=None, backprop_through_moments=True):\n  """"""Batch normalization.\n\n  The usage should be as follows: If x is the support images, moments should be\n  None so that they are computed from the support set examples. On the other\n  hand, if x is the query images, the moments argument should be used in order\n  to pass in the mean and var that were computed from the support set.\n\n  Args:\n    x: inputs.\n    params: None or a dict containing the values of the offset and scale params.\n    moments: None or a dict containing the values of the mean and var to use for\n      batch normalization.\n    backprop_through_moments: Whether to allow gradients to flow through the\n      given support set moments. Only applies to non-transductive batch norm.\n\n  Returns:\n    output: The result of applying batch normalization to the input.\n    params: The updated params.\n    moments: The updated moments.\n  """"""\n  params_keys, params_vars, moments_keys, moments_vars = [], [], [], []\n\n  with tf.variable_scope(\'batch_norm\'):\n    scope_name = tf.get_variable_scope().name\n    if moments is None:\n      # If not provided, compute the mean and var of the current batch.\n      mean, var = tf.nn.moments(\n          x, axes=list(range(len(x.shape) - 1)), keep_dims=True)\n    else:\n      if backprop_through_moments:\n        mean = moments[scope_name + \'/mean\']\n        var = moments[scope_name + \'/var\']\n      else:\n        # This variant does not yield good resutls.\n        mean = tf.stop_gradient(moments[scope_name + \'/mean\'])\n        var = tf.stop_gradient(moments[scope_name + \'/var\'])\n\n    moments_keys += [scope_name + \'/mean\']\n    moments_vars += [mean]\n    moments_keys += [scope_name + \'/var\']\n    moments_vars += [var]\n\n    if params is None:\n      offset = tf.get_variable(\n          \'offset\',\n          shape=mean.get_shape().as_list(),\n          initializer=tf.initializers.zeros())\n      scale = tf.get_variable(\n          \'scale\',\n          shape=var.get_shape().as_list(),\n          initializer=tf.initializers.ones())\n    else:\n      offset = params[scope_name + \'/offset\']\n      scale = params[scope_name + \'/scale\']\n\n    params_keys += [scope_name + \'/offset\']\n    params_vars += [offset]\n    params_keys += [scope_name + \'/scale\']\n    params_vars += [scale]\n\n    output = tf.nn.batch_normalization(x, mean, var, offset, scale, 0.00001)\n    params = collections.OrderedDict(zip(params_keys, params_vars))\n    moments = collections.OrderedDict(zip(moments_keys, moments_vars))\n    return output, params, moments\n\n\ndef weight_variable(shape):\n  """"""weight_variable generates a weight variable of a given shape.""""""\n  initial = tf.initializers.truncated_normal(stddev=0.1)\n  return tf.get_variable(\n      \'weight\', shape=shape, initializer=initial, regularizer=tf.nn.l2_loss)\n\n\ndef bias_variable(shape):\n  """"""bias_variable generates a bias variable of a given shape.""""""\n  initial = tf.initializers.constant(0.1)\n  return tf.get_variable(\'bias\', shape=shape, initializer=initial)\n\n\ndef dense(x, output_size, activation_fn=tf.nn.relu, params=None):\n  """"""Fully connected layer implementation.\n\n  Args:\n    x: tf.Tensor, input.\n    output_size: int, number features in  the fully connected layer.\n    activation_fn: function, to process pre-activations, namely x*w+b.\n    params: None or a dict containing the values of the wieght and bias params.\n      If None, default variables are used.\n\n  Returns:\n    output: The result of applying batch normalization to the input.\n    params: dict, that includes parameters used during the calculation.\n  """"""\n  with tf.variable_scope(\'dense\'):\n    scope_name = tf.get_variable_scope().name\n\n    if len(x.shape) > 2:\n      x = tf.layers.flatten(x),\n    input_size = x.get_shape().as_list()[-1]\n\n    w_name = scope_name + \'/kernel\'\n    b_name = scope_name + \'/bias\'\n    if params is None:\n      w = weight_variable([input_size, output_size])\n      b = bias_variable([output_size])\n    else:\n      w = params[w_name]\n      b = params[b_name]\n\n  x = tf.nn.xw_plus_b(x, w, b)\n  params = collections.OrderedDict(zip([w_name, b_name], [w, b]))\n  x = activation_fn(x)\n  return x, params\n\n\ndef conv(x, conv_size, depth, stride, padding=\'SAME\', params=None):\n  """"""A block that performs convolution.""""""\n  params_keys, params_vars = [], []\n  scope_name = tf.get_variable_scope().name\n  input_depth = x.get_shape().as_list()[-1]\n  if params is None:\n    w_conv = weight_variable([conv_size[0], conv_size[1], input_depth, depth])\n  else:\n    w_conv = params[scope_name + \'/kernel\']\n\n  params_keys += [scope_name + \'/kernel\']\n  params_vars += [w_conv]\n\n  x = conv2d(x, w_conv, stride=stride, padding=padding)\n  params = collections.OrderedDict(zip(params_keys, params_vars))\n\n  return x, params\n\n\ndef conv_bn(x,\n            conv_size,\n            depth,\n            stride,\n            padding=\'SAME\',\n            params=None,\n            moments=None,\n            backprop_through_moments=True):\n  """"""A block that performs convolution, followed by batch-norm.""""""\n  params_keys, params_vars = [], []\n  moments_keys, moments_vars = [], []\n  x, conv_params = conv(\n      x, conv_size, depth, stride, padding=padding, params=params)\n  params_keys.extend(conv_params.keys())\n  params_vars.extend(conv_params.values())\n\n  x, bn_params, bn_moments = bn(\n      x,\n      params=params,\n      moments=moments,\n      backprop_through_moments=backprop_through_moments)\n  params_keys.extend(bn_params.keys())\n  params_vars.extend(bn_params.values())\n  moments_keys.extend(bn_moments.keys())\n  moments_vars.extend(bn_moments.values())\n\n  params = collections.OrderedDict(zip(params_keys, params_vars))\n  moments = collections.OrderedDict(zip(moments_keys, moments_vars))\n\n  return x, params, moments\n\n\ndef bottleneck(x,\n               depth,\n               stride=1,\n               params=None,\n               moments=None,\n               use_project=False,\n               backprop_through_moments=True,\n               use_bounded_activation=False):\n  """"""ResNet18 residual block.""""""\n  params_keys, params_vars = [], []\n  moments_keys, moments_vars = [], []  # means and vars of different layers.\n  with tf.variable_scope(\'conv1\'):\n    h, conv_bn_params, conv_bn_moments = conv_bn(\n        x, [3, 3],\n        depth[0],\n        stride,\n        params=params,\n        moments=moments,\n        backprop_through_moments=backprop_through_moments)\n    params_keys.extend(conv_bn_params.keys())\n    params_vars.extend(conv_bn_params.values())\n    moments_keys.extend(conv_bn_moments.keys())\n    moments_vars.extend(conv_bn_moments.values())\n\n    h = relu(h, use_bounded_activation=use_bounded_activation)\n\n  with tf.variable_scope(\'conv2\'):\n    h, conv_bn_params, conv_bn_moments = conv_bn(\n        h, [3, 3],\n        depth[1],\n        stride=1,\n        params=params,\n        moments=moments,\n        backprop_through_moments=backprop_through_moments)\n    if use_bounded_activation:\n      h = tf.clip_by_value(h, -6.0, 6.0)\n\n    params_keys.extend(conv_bn_params.keys())\n    params_vars.extend(conv_bn_params.values())\n    moments_keys.extend(conv_bn_moments.keys())\n    moments_vars.extend(conv_bn_moments.values())\n\n  with tf.variable_scope(\'identity\'):\n    if use_project:\n      with tf.variable_scope(\'projection_conv\'):\n        x, conv_bn_params, conv_bn_moments = conv_bn(\n            x, [1, 1],\n            depth[1],\n            stride,\n            params=params,\n            moments=moments,\n            backprop_through_moments=backprop_through_moments)\n        params_keys.extend(conv_bn_params.keys())\n        params_vars.extend(conv_bn_params.values())\n        moments_keys.extend(conv_bn_moments.keys())\n        moments_vars.extend(conv_bn_moments.values())\n    x = relu(x + h, use_bounded_activation=use_bounded_activation)\n\n  params = collections.OrderedDict(zip(params_keys, params_vars))\n  moments = collections.OrderedDict(zip(moments_keys, moments_vars))\n  return x, params, moments\n\n\ndef _resnet(x,\n            is_training,\n            scope,\n            reuse=tf.AUTO_REUSE,\n            params=None,\n            moments=None,\n            backprop_through_moments=True,\n            use_bounded_activation=False,\n            keep_spatial_dims=False):\n  """"""A ResNet18 network.""""""\n  # `is_training` will be used when start to use moving {var, mean} in batch\n  # normalization. This refers to \'meta-training\'.\n  del is_training\n  x = tf.stop_gradient(x)\n  params_keys, params_vars = [], []\n  moments_keys, moments_vars = [], []\n  with tf.variable_scope(scope, reuse=reuse):\n    # We use DeepLab feature alignment rule to determine the input size.\n    # Since the image size in the meta-dataset pipeline is a multiplier of 42,\n    # e.g., [42, 84, 168], we align them to the closest sizes that conform to\n    # the alignment rule and at the same time are larger. They are [65, 97, 193]\n    # respectively. The aligned image size for 224 used in the ResNet work is\n    # 225.\n    #\n    # References:\n    # 1. ResNet https://arxiv.org/abs/1512.03385\n    # 2. DeepLab https://arxiv.org/abs/1606.00915\n    size = tf.cast(tf.shape(x)[1], tf.float32)\n    aligned_size = tf.cast(tf.ceil(size / 32.0), tf.int32) * 32 + 1\n    x = tf.image.resize_bilinear(\n        x, size=[aligned_size, aligned_size], align_corners=True)\n\n    with tf.variable_scope(\'conv1\'):\n      x, conv_bn_params, conv_bn_moments = conv_bn(\n          x, [7, 7],\n          64,\n          2,\n          params=params,\n          moments=moments,\n          backprop_through_moments=backprop_through_moments)\n      params_keys.extend(conv_bn_params.keys())\n      params_vars.extend(conv_bn_params.values())\n      moments_keys.extend(conv_bn_moments.keys())\n      moments_vars.extend(conv_bn_moments.values())\n\n      x = relu(x, use_bounded_activation=use_bounded_activation)\n\n    def _bottleneck(x, i, depth, params, moments, stride=2):\n      """"""Wrapper for bottleneck.""""""\n      output_stride = stride if i == 0 else 1\n      use_project = True if i == 0 else False\n      x, bottleneck_params, bottleneck_moments = bottleneck(\n          x, (depth, depth),\n          output_stride,\n          params=params,\n          moments=moments,\n          use_project=use_project,\n          backprop_through_moments=backprop_through_moments)\n      return x, bottleneck_params, bottleneck_moments\n\n    with tf.variable_scope(\'conv2_x\'):\n      x = tf.nn.max_pool(\n          x, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n      for i in range(2):\n        with tf.variable_scope(\'bottleneck_%d\' % i):\n          x, bottleneck_params, bottleneck_moments = _bottleneck(\n              x, i, 64, params, moments, stride=1)\n          params_keys.extend(bottleneck_params.keys())\n          params_vars.extend(bottleneck_params.values())\n          moments_keys.extend(bottleneck_moments.keys())\n          moments_vars.extend(bottleneck_moments.values())\n\n    with tf.variable_scope(\'conv3_x\'):\n      for i in range(2):\n        with tf.variable_scope(\'bottleneck_%d\' % i):\n          x, bottleneck_params, bottleneck_moments = _bottleneck(\n              x, i, 128, params, moments)\n          params_keys.extend(bottleneck_params.keys())\n          params_vars.extend(bottleneck_params.values())\n          moments_keys.extend(bottleneck_moments.keys())\n          moments_vars.extend(bottleneck_moments.values())\n\n    with tf.variable_scope(\'conv4_x\'):\n      for i in range(2):\n        with tf.variable_scope(\'bottleneck_%d\' % i):\n          x, bottleneck_params, bottleneck_moments = _bottleneck(\n              x, i, 256, params, moments)\n          params_keys.extend(bottleneck_params.keys())\n          params_vars.extend(bottleneck_params.values())\n          moments_keys.extend(bottleneck_moments.keys())\n          moments_vars.extend(bottleneck_moments.values())\n\n    with tf.variable_scope(\'conv5_x\'):\n      for i in range(2):\n        with tf.variable_scope(\'bottleneck_%d\' % i):\n          x, bottleneck_params, bottleneck_moments = _bottleneck(\n              x, i, 512, params, moments)\n          params_keys.extend(bottleneck_params.keys())\n          params_vars.extend(bottleneck_params.values())\n          moments_keys.extend(bottleneck_moments.keys())\n          moments_vars.extend(bottleneck_moments.values())\n    x = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n    # x.shape: [?, 1, 1, 512]\n    if not keep_spatial_dims:\n      x = tf.reshape(x, [-1, 512])\n    params = collections.OrderedDict(zip(params_keys, params_vars))\n    moments = collections.OrderedDict(zip(moments_keys, moments_vars))\n\n    return_dict = {\'embeddings\': x, \'params\': params, \'moments\': moments}\n    return return_dict\n\n\ndef resnet(x,\n           is_training,\n           params=None,\n           moments=None,\n           reuse=tf.AUTO_REUSE,\n           scope=\'resnet18\',\n           backprop_through_moments=True,\n           use_bounded_activation=False,\n           keep_spatial_dims=False):\n  return _resnet(\n      x,\n      is_training,\n      scope,\n      reuse=reuse,\n      params=params,\n      moments=moments,\n      backprop_through_moments=backprop_through_moments,\n      use_bounded_activation=use_bounded_activation,\n      keep_spatial_dims=keep_spatial_dims)\n\n\ndef wide_resnet_block(x,\n                      depth,\n                      stride=1,\n                      params=None,\n                      moments=None,\n                      use_project=False,\n                      backprop_through_moments=True,\n                      use_bounded_activation=False):\n  """"""Wide ResNet residual block.""""""\n  params_keys, params_vars = [], []\n  moments_keys, moments_vars = [], []\n  with tf.variable_scope(\'conv1\'):\n    bn_1, bn_params, bn_moments = bn(\n        x,\n        params=params,\n        moments=moments,\n        backprop_through_moments=backprop_through_moments)\n    params_keys.extend(bn_params.keys())\n    params_vars.extend(bn_params.values())\n    moments_keys.extend(bn_moments.keys())\n    moments_vars.extend(bn_moments.values())\n\n    out_1 = relu(bn_1, use_bounded_activation=use_bounded_activation)\n\n    h_1, conv_params = conv(out_1, [3, 3], depth, stride, params=params)\n    params_keys.extend(conv_params.keys())\n    params_vars.extend(conv_params.values())\n  with tf.variable_scope(\'conv2\'):\n    bn_2, bn_params, bn_moments = bn(\n        h_1,\n        params=params,\n        moments=moments,\n        backprop_through_moments=backprop_through_moments)\n    params_keys.extend(bn_params.keys())\n    params_vars.extend(bn_params.values())\n    moments_keys.extend(bn_moments.keys())\n    moments_vars.extend(bn_moments.values())\n\n    out_2 = relu(bn_2, use_bounded_activation=use_bounded_activation)\n\n    h_2, conv_params = conv(out_2, [3, 3], depth, stride=1, params=params)\n    params_keys.extend(conv_params.keys())\n    params_vars.extend(conv_params.values())\n\n  h = h_2\n  if use_bounded_activation:\n    h = tf.clip_by_value(h, -6, 6)\n\n  with tf.variable_scope(\'identity\'):\n    if use_project:\n      with tf.variable_scope(\'projection_conv\'):\n        x, conv_params = conv(out_1, [1, 1], depth, stride, params=params)\n        params_keys.extend(conv_params.keys())\n        params_vars.extend(conv_params.values())\n\n  params = collections.OrderedDict(zip(params_keys, params_vars))\n  moments = collections.OrderedDict(zip(moments_keys, moments_vars))\n\n  if use_bounded_activation:\n    out = tf.clip_by_value(x + h, -6, 6)\n  else:\n    out = x + h\n  return out, params, moments\n\n\ndef _wide_resnet(x,\n                 is_training,\n                 scope,\n                 n,\n                 k,\n                 reuse=tf.AUTO_REUSE,\n                 params=None,\n                 moments=None,\n                 backprop_through_moments=True,\n                 use_bounded_activation=False,\n                 keep_spatial_dims=False):\n  """"""A wide ResNet.""""""\n  # `is_training` will be used when start to use moving {var, mean} in batch\n  # normalization.\n  del is_training\n  widths = [i * k for i in (16, 32, 64)]\n  params_keys, params_vars = [], []\n  moments_keys, moments_vars = [], []\n\n  def _update_params_lists(params_dict, params_keys, params_vars):\n    params_keys.extend(params_dict.keys())\n    params_vars.extend(params_dict.values())\n\n  def _update_moments_lists(moments_dict, moments_keys, moments_vars):\n    moments_keys.extend(moments_dict.keys())\n    moments_vars.extend(moments_dict.values())\n\n  with tf.variable_scope(scope, reuse=reuse):\n    with tf.variable_scope(\'conv1\'):\n      x, conv_params = conv(x, [3, 3], 16, 1, params=params)\n      _update_params_lists(conv_params, params_keys, params_vars)\n\n    def _wide_resnet_block(x, depths, stride, use_project, moments):\n      """"""Wrapper for a wide resnet block.""""""\n      x, block_params, block_moments = wide_resnet_block(\n          x,\n          depths,\n          stride=stride,\n          params=params,\n          moments=moments,\n          use_project=use_project,\n          backprop_through_moments=backprop_through_moments,\n          use_bounded_activation=use_bounded_activation)\n      return x, block_params, block_moments\n\n    with tf.variable_scope(\'conv2_x\'):\n      with tf.variable_scope(\'wide_block_0\'):\n        if widths[0] == 16:\n          use_project = False\n        else:\n          use_project = True\n        x, block_params, block_moments = _wide_resnet_block(\n            x, widths[0], 1, use_project, moments=moments)\n        _update_params_lists(block_params, params_keys, params_vars)\n        _update_moments_lists(block_moments, moments_keys, moments_vars)\n      for i in range(1, n):\n        with tf.variable_scope(\'wide_block_%d\' % i):\n          x, block_params, block_moments = _wide_resnet_block(\n              x, widths[0], 1, use_project, moments=moments)\n          _update_params_lists(block_params, params_keys, params_vars)\n          _update_moments_lists(block_moments, moments_keys, moments_vars)\n\n    with tf.variable_scope(\'conv3_x\'):\n      with tf.variable_scope(\'wide_block_0\'):\n        x, block_params, block_moments = _wide_resnet_block(\n            x, widths[1], 2, True, moments=moments)\n        _update_params_lists(block_params, params_keys, params_vars)\n        _update_moments_lists(block_moments, moments_keys, moments_vars)\n      for i in range(1, n):\n        with tf.variable_scope(\'wide_block_%d\' % i):\n          x, block_params, block_moments = _wide_resnet_block(\n              x, widths[1], 1, use_project, moments=moments)\n          _update_params_lists(block_params, params_keys, params_vars)\n          _update_moments_lists(block_moments, moments_keys, moments_vars)\n\n    with tf.variable_scope(\'conv4_x\'):\n      with tf.variable_scope(\'wide_block_0\'):\n        x, block_params, block_moments = _wide_resnet_block(\n            x, widths[2], 2, True, moments=moments)\n        _update_params_lists(block_params, params_keys, params_vars)\n        _update_moments_lists(block_moments, moments_keys, moments_vars)\n      for i in range(1, n):\n        with tf.variable_scope(\'wide_block_%d\' % i):\n          x, block_params, block_moments = _wide_resnet_block(\n              x, widths[2], 1, use_project, moments=moments)\n          _update_params_lists(block_params, params_keys, params_vars)\n          _update_moments_lists(block_moments, moments_keys, moments_vars)\n\n    with tf.variable_scope(\'embedding_layer\'):\n      x, bn_params, bn_moments = bn(\n          x,\n          params=params,\n          moments=moments,\n          backprop_through_moments=backprop_through_moments)\n      _update_params_lists(bn_params, params_keys, params_vars)\n      _update_moments_lists(bn_moments, moments_keys, moments_vars)\n\n      x = relu(x, use_bounded_activation=use_bounded_activation)\n    img_w, img_h = x.get_shape().as_list()[1:3]\n    x = tf.nn.avg_pool(\n        x, ksize=[1, img_w, img_h, 1], strides=[1, 1, 1, 1], padding=\'VALID\')\n    # x.shape: [X, 1, 1, 128]\n    if not keep_spatial_dims:\n      x = tf.reshape(x, [-1, widths[2]])\n    params = collections.OrderedDict(zip(params_keys, params_vars))\n    moments = collections.OrderedDict(zip(moments_keys, moments_vars))\n\n    return_dict = {\'embeddings\': x, \'params\': params, \'moments\': moments}\n    return return_dict\n\n\ndef wide_resnet(x,\n                is_training,\n                params=None,\n                moments=None,\n                reuse=tf.AUTO_REUSE,\n                scope=\'wide_resnet\',\n                backprop_through_moments=True,\n                use_bounded_activation=False,\n                keep_spatial_dims=False):\n  return _wide_resnet(\n      x,\n      is_training,\n      scope,\n      2,\n      2,\n      reuse=reuse,\n      params=params,\n      moments=moments,\n      backprop_through_moments=backprop_through_moments,\n      use_bounded_activation=use_bounded_activation,\n      keep_spatial_dims=keep_spatial_dims)\n\n\ndef _four_layer_convnet(inputs,\n                        scope,\n                        reuse=tf.AUTO_REUSE,\n                        params=None,\n                        moments=None,\n                        depth_multiplier=1.0,\n                        backprop_through_moments=True,\n                        use_bounded_activation=False,\n                        keep_spatial_dims=False):\n  """"""A four-layer-convnet architecture.""""""\n  layer = tf.stop_gradient(inputs)\n  model_params_keys, model_params_vars = [], []\n  moments_keys, moments_vars = [], []\n\n  with tf.variable_scope(scope, reuse=reuse):\n    for i in range(4):\n      with tf.variable_scope(\'layer_{}\'.format(i), reuse=reuse):\n        depth = int(64 * depth_multiplier)\n        layer, conv_bn_params, conv_bn_moments = conv_bn(\n            layer, [3, 3],\n            depth,\n            stride=1,\n            params=params,\n            moments=moments,\n            backprop_through_moments=backprop_through_moments)\n        model_params_keys.extend(conv_bn_params.keys())\n        model_params_vars.extend(conv_bn_params.values())\n        moments_keys.extend(conv_bn_moments.keys())\n        moments_vars.extend(conv_bn_moments.values())\n\n      if use_bounded_activation:\n        layer = tf.nn.relu6(layer)\n      else:\n        layer = tf.nn.relu(layer)\n      layer = tf.layers.max_pooling2d(layer, [2, 2], 2)\n      logging.info(\'Output of block %d: %s\', i, layer.shape)\n\n    model_params = collections.OrderedDict(\n        zip(model_params_keys, model_params_vars))\n    moments = collections.OrderedDict(zip(moments_keys, moments_vars))\n    if not keep_spatial_dims:\n      layer = tf.layers.flatten(layer)\n    return_dict = {\n        \'embeddings\': layer,\n        \'params\': model_params,\n        \'moments\': moments\n    }\n\n    return return_dict\n\n\ndef _relation_net(inputs,\n                  scope,\n                  reuse=tf.AUTO_REUSE,\n                  params=None,\n                  moments=None,\n                  depth_multiplier=1.0,\n                  backprop_through_moments=True,\n                  use_bounded_activation=False):\n  """"""A 2-layer-convnet architecture with fully connected layers.""""""\n  model_params_keys, model_params_vars = [], []\n  moments_keys, moments_vars = [], []\n  layer = inputs\n  with tf.variable_scope(scope, reuse=reuse):\n    for i in range(2):\n      with tf.variable_scope(\'layer_{}\'.format(i), reuse=reuse):\n        depth = int(64 * depth_multiplier)\n        # Note that original has `valid` padding where we use `same`.\n        layer, conv_bn_params, conv_bn_moments = conv_bn(\n            layer, [3, 3],\n            depth,\n            stride=1,\n            params=params,\n            moments=moments,\n            backprop_through_moments=backprop_through_moments)\n        model_params_keys.extend(conv_bn_params.keys())\n        model_params_vars.extend(conv_bn_params.values())\n        moments_keys.extend(conv_bn_moments.keys())\n        moments_vars.extend(conv_bn_moments.values())\n\n      layer = relu(layer, use_bounded_activation=use_bounded_activation)\n      # This is a hacky way preventing max pooling if the spatial dimensions\n      # are already reduced.\n      if layer.shape[1] > 1:\n        layer = tf.layers.max_pooling2d(layer, [2, 2], 2)\n      tf.logging.info(\'Output of block %d: %s\' % (i, layer.shape))\n\n    layer = tf.layers.flatten(layer)\n    relu_activation_fn = functools.partial(\n        relu, use_bounded_activation=use_bounded_activation)\n    with tf.variable_scope(\'layer_2_fc\', reuse=reuse):\n      layer, dense_params = dense(layer, 8, activation_fn=relu_activation_fn)\n      tf.logging.info(\'Output layer_2_fc: %s\' % layer.shape)\n      model_params_keys.extend(dense_params.keys())\n      model_params_vars.extend(dense_params.values())\n    with tf.variable_scope(\'layer_3_fc\', reuse=reuse):\n      output, dense_params = dense(layer, 1, activation_fn=tf.nn.sigmoid)\n      tf.logging.info(\'Output layer_3_fc: %s\' % output.shape)\n      model_params_keys.extend(dense_params.keys())\n      model_params_vars.extend(dense_params.values())\n\n    model_params = collections.OrderedDict(\n        zip(model_params_keys, model_params_vars))\n    moments = collections.OrderedDict(zip(moments_keys, moments_vars))\n    return_dict = {\'output\': output, \'params\': model_params, \'moments\': moments}\n\n    return return_dict\n\n\ndef relationnet_embedding(inputs,\n                          is_training,\n                          params=None,\n                          moments=None,\n                          depth_multiplier=1.0,\n                          reuse=tf.AUTO_REUSE,\n                          scope=\'relationnet_convnet\',\n                          backprop_through_moments=True,\n                          use_bounded_activation=False,\n                          keep_spatial_dims=False):\n  """"""A 4-layer-convnet architecture for relationnet embedding.\n\n  This is almost like the `learner.four_layer_convnet` embedding function except\n  for the following differences: (1) no padding for the first 3 layers, (2) no\n  maxpool on the last (4th) layer, and (3) no flatten.\n\n  Paper: https://arxiv.org/abs/1711.06025\n  Code:\n  https://github.com/floodsung/LearningToCompare_FSL/blob/master/miniimagenet/miniimagenet_train_few_shot.py\n\n  Args:\n    inputs: Tensors of shape [None, ] + image shape, e.g. [15, 84, 84, 3]\n    is_training: Whether we are in the training phase.\n    params: None will create new params (or reuse from scope), otherwise an\n      ordered dict of convolutional kernels and biases such that\n      params[\'kernel_0\'] stores the kernel of the first convolutional layer,\n      etc.\n    moments: A dict of the means and vars of the different layers to use for\n      batch normalization. If not provided, the mean and var are computed based\n      on the given inputs.\n    depth_multiplier: The depth multiplier for the convnet channels.\n    reuse: Whether to reuse the network\'s weights.\n    scope: An optional scope for the tf operations.\n    backprop_through_moments: Whether to allow gradients to flow through the\n      given support set moments. Only applies to non-transductive batch norm.\n    use_bounded_activation: Whether to enable bounded activation. This is useful\n      for post-training quantization.\n    keep_spatial_dims: bool, if True the spatial dimensions are kept.\n\n  Returns:\n    A 2D Tensor, where each row is the embedding of an input in inputs.\n  """"""\n  del is_training\n\n  layer = tf.stop_gradient(inputs)\n  model_params_keys, model_params_vars = [], []\n  moments_keys, moments_vars = [], []\n\n  with tf.variable_scope(scope, reuse=reuse):\n    for i in range(4):\n      with tf.variable_scope(\'layer_{}\'.format(i), reuse=reuse):\n        depth = int(64 * depth_multiplier)\n        # The original implementation had VALID padding for the first two layers\n        # that are followed by pooling. The rest (last two) had `SAME` padding.\n        # In our setting, to avoid OOM, we pool (and apply VALID padding) to\n        # the first three layers, and use SAME padding only in the last one.\n        layer, conv_bn_params, conv_bn_moments = conv_bn(\n            layer, [3, 3],\n            depth,\n            stride=1,\n            padding=\'VALID\' if i < 3 else \'SAME\',\n            params=params,\n            moments=moments,\n            backprop_through_moments=backprop_through_moments)\n        model_params_keys.extend(conv_bn_params.keys())\n        model_params_vars.extend(conv_bn_params.values())\n        moments_keys.extend(conv_bn_moments.keys())\n        moments_vars.extend(conv_bn_moments.values())\n\n      layer = relu(layer, use_bounded_activation=use_bounded_activation)\n      if i < 3:\n        layer = tf.layers.max_pooling2d(layer, [2, 2], 2)\n      tf.logging.info(\'Output of block %d: %s\' % (i, layer.shape))\n\n    model_params = collections.OrderedDict(\n        zip(model_params_keys, model_params_vars))\n    moments = collections.OrderedDict(zip(moments_keys, moments_vars))\n    if not keep_spatial_dims:\n      layer = tf.layers.flatten(layer)\n    return_dict = {\n        \'embeddings\': layer,\n        \'params\': model_params,\n        \'moments\': moments\n    }\n\n    return return_dict\n\n\ndef four_layer_convnet(inputs,\n                       is_training,\n                       params=None,\n                       moments=None,\n                       depth_multiplier=1.0,\n                       reuse=tf.AUTO_REUSE,\n                       scope=\'four_layer_convnet\',\n                       backprop_through_moments=True,\n                       use_bounded_activation=False,\n                       keep_spatial_dims=False):\n  """"""Embeds inputs using a standard four-layer convnet.\n\n  Args:\n    inputs: Tensors of shape [None, ] + image shape, e.g. [15, 84, 84, 3]\n    is_training: Whether we are in the training phase.\n    params: None will create new params (or reuse from scope), otherwise an\n      ordered dict of convolutional kernels and biases such that\n      params[\'kernel_0\'] stores the kernel of the first convolutional layer,\n      etc.\n    moments: A dict of the means and vars of the different layers to use for\n      batch normalization. If not provided, the mean and var are computed based\n      on the given inputs.\n    depth_multiplier: The depth multiplier for the convnet channels.\n    reuse: Whether to reuse the network\'s weights.\n    scope: An optional scope for the tf operations.\n    backprop_through_moments: Whether to allow gradients to flow through the\n      given support set moments. Only applies to non-transductive batch norm.\n    use_bounded_activation: Whether to enable bounded activation. This is useful\n      for post-training quantization.\n    keep_spatial_dims: bool, if True the spatial dimensions are kept.\n\n  Returns:\n    A 2D Tensor, where each row is the embedding of an input in inputs.\n  """"""\n  del is_training\n  return _four_layer_convnet(\n      inputs,\n      scope,\n      reuse=reuse,\n      params=params,\n      moments=moments,\n      depth_multiplier=depth_multiplier,\n      backprop_through_moments=backprop_through_moments,\n      use_bounded_activation=use_bounded_activation,\n      keep_spatial_dims=keep_spatial_dims)\n\n\n@gin.configurable(\n    \'fully_connected_network\', whitelist=[\n        \'n_hidden_units\',\n        \'use_batchnorm\',\n    ])\ndef fully_connected_network(inputs,\n                            is_training,\n                            params=None,\n                            moments=None,\n                            n_hidden_units=(64,),\n                            use_batchnorm=False,\n                            reuse=tf.AUTO_REUSE,\n                            scope=\'fully_connected\',\n                            use_bounded_activation=False,\n                            backprop_through_moments=None,\n                            keep_spatial_dims=None):\n  """"""A fully connected linear network.\n\n  Since there is no batch-norm, `moments` and `backprop_through_moments` flags\n  are not used.\n\n  Args:\n    inputs: Tensor of shape [None, num_features], where `num_features` is the\n      number of input features.\n    is_training: not used.\n    params: None will create new params (or reuse from scope), otherwise an\n      ordered dict of fully connected weights and biases such that\n      params[\'weight_0\'] stores the kernel of the first fully-connected layer,\n      etc.\n    moments: not used.\n    n_hidden_units: tuple, Number of hidden units for each layer. If empty, it\n      is the identity mapping.\n    use_batchnorm: bool, Whether to use batchnorm after layers, except last.\n    reuse: Whether to reuse the network\'s weights.\n    scope: An optional scope for the tf operations.\n    use_bounded_activation: Whether to enable bounded activation. This is useful\n      for post-training quantization.\n    backprop_through_moments: not used.\n    keep_spatial_dims: not used.\n\n  Returns:\n    A 2D Tensor, where each row is the embedding of an input in inputs.\n  """"""\n  del is_training, keep_spatial_dims\n  layer = inputs\n  model_params_keys, model_params_vars = [], []\n  moments_keys, moments_vars = [], []\n  activation_fn = functools.partial(\n      relu, use_bounded_activation=use_bounded_activation)\n  with tf.variable_scope(scope, reuse=reuse):\n    for i, n_unit in enumerate(n_hidden_units):\n      with tf.variable_scope(\'layer_%d\' % i, reuse=reuse):\n        layer, dense_params = dense(\n            layer, n_unit, activation_fn=activation_fn, params=params)\n        model_params_keys.extend(dense_params.keys())\n        model_params_vars.extend(dense_params.values())\n        if use_batchnorm:\n          layer, bn_params, bn_moments = bn(\n              layer,\n              params=params,\n              moments=moments,\n              backprop_through_moments=backprop_through_moments)\n          model_params_keys.extend(bn_params.keys())\n          model_params_keys.extend(bn_params.values())\n          moments_keys.extend(bn_moments.keys())\n          moments_vars.extend(bn_moments.values())\n\n  model_params = collections.OrderedDict(\n      zip(model_params_keys, model_params_vars))\n  moments = collections.OrderedDict(zip(moments_keys, moments_vars))\n  return_dict = {\n      \'embeddings\': layer,\n      \'params\': model_params,\n      \'moments\': moments\n  }\n  return return_dict\n\n\nNAME_TO_EMBEDDING_NETWORK = {\n    \'resnet\': resnet,\n    \'relationnet_embedding\': relationnet_embedding,\n    \'four_layer_convnet\': four_layer_convnet,\n    \'fully_connected_network\': fully_connected_network,\n    \'wide_resnet\': wide_resnet,\n}\n\n\ndef get_embeddings_vars_copy_ops(embedding_vars_dict, make_copies):\n  """"""Gets copies of the embedding variables or returns those variables.\n\n  This is useful at meta-test time for MAML and the finetuning baseline. In\n  particular, at meta-test time, we don\'t want to make permanent updates to\n  the model\'s variables, but only modifications that persist in the given\n  episode. This can be achieved by creating copies of each variable and\n  modifying and using these copies instead of the variables themselves.\n\n  Args:\n    embedding_vars_dict: A dict mapping each variable name to the corresponding\n      Variable.\n    make_copies: A bool. Whether to copy the given variables. If not, those\n      variables themselves will be returned. Typically, this is True at meta-\n      test time and False at meta-training time.\n\n  Returns:\n    embedding_vars_keys: A list of variable names.\n    embeddings_vars: A corresponding list of Variables.\n    embedding_vars_copy_ops: A (possibly empty) list of operations, each of\n      which assigns the value of one of the provided Variables to a new\n      Variable which is its copy.\n  """"""\n  embedding_vars_keys = []\n  embedding_vars = []\n  embedding_vars_copy_ops = []\n  for name, var in six.iteritems(embedding_vars_dict):\n    embedding_vars_keys.append(name)\n    if make_copies:\n      with tf.variable_scope(\'weight_copy\'):\n        shape = var.shape.as_list()\n        var_copy = tf.Variable(\n            tf.zeros(shape), collections=[tf.GraphKeys.LOCAL_VARIABLES])\n        var_copy_op = tf.assign(var_copy, var)\n        embedding_vars_copy_ops.append(var_copy_op)\n      embedding_vars.append(var_copy)\n    else:\n      embedding_vars.append(var)\n  return embedding_vars_keys, embedding_vars, embedding_vars_copy_ops\n\n\ndef get_fc_vars_copy_ops(fc_weights, fc_bias, make_copies):\n  """"""Gets copies of the classifier layer variables or returns those variables.\n\n  At meta-test time, a copy is created for the given Variables, and these copies\n  copies will be used in place of the original ones.\n\n  Args:\n    fc_weights: A Variable for the weights of the fc layer.\n    fc_bias: A Variable for the bias of the fc layer.\n    make_copies: A bool. Whether to copy the given variables. If not, those\n      variables themselves are returned.\n\n  Returns:\n    fc_weights: A Variable for the weights of the fc layer. Might be the same as\n      the input fc_weights or a copy of it.\n    fc_bias: Analogously, a Variable for the bias of the fc layer.\n    fc_vars_copy_ops: A (possibly empty) list of operations for assigning the\n      value of each of fc_weights and fc_bias to a respective copy variable.\n  """"""\n  fc_vars_copy_ops = []\n  if make_copies:\n    with tf.variable_scope(\'weight_copy\'):\n      # fc_weights copy\n      fc_weights_copy = tf.Variable(\n          tf.zeros(fc_weights.shape.as_list()),\n          collections=[tf.GraphKeys.LOCAL_VARIABLES])\n      fc_weights_copy_op = tf.assign(fc_weights_copy, fc_weights)\n      fc_vars_copy_ops.append(fc_weights_copy_op)\n\n      # fc_bias copy\n      fc_bias_copy = tf.Variable(\n          tf.zeros(fc_bias.shape.as_list()),\n          collections=[tf.GraphKeys.LOCAL_VARIABLES])\n      fc_bias_copy_op = tf.assign(fc_bias_copy, fc_bias)\n      fc_vars_copy_ops.append(fc_bias_copy_op)\n\n      fc_weights = fc_weights_copy\n      fc_bias = fc_bias_copy\n  return fc_weights, fc_bias, fc_vars_copy_ops\n\n\ndef gradient_descent_step(loss,\n                          variables,\n                          stop_grads,\n                          allow_grads_to_batch_norm_vars,\n                          learning_rate,\n                          get_update_ops=True):\n  """"""Returns the updated vars after one step of gradient descent.""""""\n  grads = tf.gradients(loss, variables)\n\n  if stop_grads:\n    grads = [tf.stop_gradient(dv) for dv in grads]\n\n  def _apply_grads(variables, grads):\n    """"""Applies gradients using SGD on a list of variables.""""""\n    v_new, update_ops = [], []\n    for (v, dv) in zip(variables, grads):\n      if (not allow_grads_to_batch_norm_vars and\n          (\'offset\' in v.name or \'scale\' in v.name)):\n        updated_value = v  # no update.\n      else:\n        updated_value = v - learning_rate * dv  # gradient descent update.\n        if get_update_ops:\n          update_ops.append(tf.assign(v, updated_value))\n      v_new.append(updated_value)\n    return v_new, update_ops\n\n  updated_vars, update_ops = _apply_grads(variables, grads)\n  return {\'updated_vars\': updated_vars, \'update_ops\': update_ops}\n\n\n@gin.configurable\nclass Learner(object):\n  """"""A Learner.""""""\n\n  def __init__(\n      self,\n      is_training,\n      logit_dim,\n      transductive_batch_norm,\n      backprop_through_moments,\n      embedding_fn,\n      weight_decay,\n  ):\n    """"""Initializes a Learner.\n\n    Note that Gin configuration of subclasses of `Learner` will override any\n    corresponding Gin configurations of `Learner`, since parameters are passed\n    to the `Learner` base class\'s constructor (See\n    https://github.com/google/gin-config/blob/master/README.md) for more\n    details).\n\n    Args:\n      is_training: Whether the learning is in training mode.\n      logit_dim: An integer; the maximum dimensionality of output predictions.\n      transductive_batch_norm: Whether to batch normalize in the transductive\n        setting where the mean and variance for normalization are computed from\n        both the support and query sets.\n      backprop_through_moments: Whether to allow gradients to flow through the\n        given support set moments. Only applies to non-transductive batch norm.\n      embedding_fn: A string; the name of the function that embeds images.\n      weight_decay: coefficient for L2 regularization.\n    """"""\n    self.is_training = is_training\n    self.logit_dim = logit_dim\n    self.transductive_batch_norm = transductive_batch_norm\n    self.backprop_through_moments = backprop_through_moments\n    self.embedding_fn = NAME_TO_EMBEDDING_NETWORK[embedding_fn]\n    self.weight_decay = weight_decay\n\n    if self.transductive_batch_norm:\n      logging.info(\'Using transductive batch norm!\')\n\n  def compute_loss(self, onehot_labels, predictions):\n    """"""Computes the CE loss of `predictions` with respect to `onehot_labels`.\n\n    Args:\n      onehot_labels: A `tf.Tensor` containing the the class labels; each vector\n        along the class dimension should hold a valid probability distribution.\n      predictions: A `tf.Tensor` containing the the class predictions,\n        interpreted as unnormalized log probabilities.\n\n    Returns:\n       A `tf.Tensor` representing the average loss.\n    """"""\n    cross_entropy_loss = tf.losses.softmax_cross_entropy(\n        onehot_labels=onehot_labels, logits=predictions)\n    regularization = tf.reduce_sum(\n        tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n    loss = cross_entropy_loss + self.weight_decay * regularization\n    return loss\n\n  def compute_accuracy(self, labels, predictions):\n    """"""Computes the accuracy of `predictions` with respect to `labels`.\n\n    Args:\n      labels: A `tf.Tensor` containing the the class labels; each vector along\n        the class dimension should hold a valid probability distribution.\n      predictions: A `tf.Tensor` containing the the class predictions,\n        interpreted as unnormalized log probabilities.\n\n    Returns:\n       A `tf.Tensor` representing the average accuracy.\n    """"""\n    correct = tf.equal(labels, tf.to_int32(tf.argmax(predictions, -1)))\n    return tf.reduce_mean(tf.cast(correct, tf.float32))\n\n  def forward_pass(self, data):\n    """"""Returns the (query if episodic) logits.""""""\n    raise NotImplementedError\n\n\nclass EpisodicLearner(Learner):\n  """"""An episodic learner.""""""\n\n  pass\n\n\nclass BatchLearner(Learner):\n  """"""A batch learner.""""""\n\n  pass\n\n\n@gin.configurable\nclass PrototypicalNetworkLearner(EpisodicLearner):\n  """"""A Prototypical Network.""""""\n  keep_spatial_dims = False\n\n  def __init__(self, **kwargs):\n    super(PrototypicalNetworkLearner, self).__init__(**kwargs)\n    # `PrototypicalNetworkLearner`s and subclasses don\'t require a pre-specified\n    # output dimensionality.\n    delattr(self, \'logit_dim\')\n\n  def forward_pass(self, data):\n    """"""Embeds all (training and testing) images of the episode.\n\n    Args:\n      data: A `meta_dataset.providers.EpisodeDataset` containing the data for\n        the episode.\n\n    Returns:\n      The predictions for the query set within the episode.\n    """"""\n    # Compute the support set\'s mean and var and use these as the moments for\n    # batch norm on the query set.\n    train_embeddings_dict = self.embedding_fn(\n        data.train_images,\n        self.is_training,\n        keep_spatial_dims=self.keep_spatial_dims)\n    train_embeddings = train_embeddings_dict[\'embeddings\']\n    support_set_moments = None\n    if not self.transductive_batch_norm:\n      support_set_moments = train_embeddings_dict[\'moments\']\n    test_embeddings_dict = self.embedding_fn(\n        data.test_images,\n        self.is_training,\n        moments=support_set_moments,\n        keep_spatial_dims=self.keep_spatial_dims,\n        backprop_through_moments=self.backprop_through_moments)\n    test_embeddings = test_embeddings_dict[\'embeddings\']\n\n    test_logits = self.compute_logits(\n        train_embeddings,\n        test_embeddings,\n        data.onehot_train_labels,\n    )\n\n    return test_logits\n\n  def compute_logits(self, support_embeddings, query_embeddings,\n                     onehot_support_labels):\n    """"""Computes the negative distances of each query point to each prototype.""""""\n\n    # [num test images, 1, embedding size].\n    query_embeddings = tf.expand_dims(query_embeddings, 1)\n\n    prototypes = compute_prototypes(support_embeddings, onehot_support_labels)\n\n    # [1, num_clases, embedding_size].\n    prototypes = tf.expand_dims(prototypes, 0)\n\n    # Squared euclidean distances between each test embedding / prototype pair.\n    distances = tf.reduce_sum(tf.square(query_embeddings - prototypes), 2)\n    return -distances\n\n\n@gin.configurable\nclass MatchingNetworkLearner(PrototypicalNetworkLearner):\n  """"""A Matching Network.""""""\n  keep_spatial_dims = False\n\n  def __init__(self, exact_cosine_distance, **kwargs):\n    """"""Initializes the Matching Networks instance.\n\n    Args:\n      exact_cosine_distance: If True then the cosine distance is used, otherwise\n        the query set embeddings are left unnormalized when computing the dot\n        product.\n      **kwargs: Keyword arguments common to all PrototypicalNetworkLearners.\n    """"""\n    self.exact_cosine_distance = exact_cosine_distance\n    super(MatchingNetworkLearner, self).__init__(**kwargs)\n\n  def compute_logits(self, support_embeddings, query_embeddings,\n                     onehot_support_labels):\n    """"""Computes the class logits.\n\n    Probabilities are computed as a weighted sum of one-hot encoded training\n    labels. Weights for individual support/query pairs of examples are\n    proportional to the (potentially semi-normalized) cosine distance between\n    the embeddings of the two examples.\n\n    Args:\n      support_embeddings: A Tensor of size [num_train_images, embedding dim].\n      query_embeddings: A Tensor of size [num_test_images, embedding dim].\n      onehot_support_labels: A Tensor of size [batch size, way].\n\n    Returns:\n      The query set logits as a [num_test_images, way] matrix.\n    """"""\n    # Undocumented in the paper, but *very important*: *only* the support set\n    # embeddings is L2-normalized, which means that the distance is not exactly\n    # a cosine distance. For comparison we also allow for the actual cosine\n    # distance to be computed, which is controlled with the\n    # `exact_cosine_distance` instance attribute.\n    train_embeddings = tf.nn.l2_normalize(support_embeddings, 1, epsilon=1e-3)\n    test_embeddings = query_embeddings\n    if self.exact_cosine_distance:\n      test_embeddings = tf.nn.l2_normalize(test_embeddings, 1, epsilon=1e-3)\n    # [num_test_images, num_train_images]\n    similarities = tf.matmul(\n        test_embeddings, train_embeddings, transpose_b=True)\n    attention = tf.nn.softmax(similarities)\n\n    # [num_test_images, way]\n    probs = tf.matmul(attention, tf.cast(onehot_support_labels, tf.float32))\n    return tf.log(probs)\n\n\n@gin.configurable\nclass RelationNetworkLearner(PrototypicalNetworkLearner):\n  """"""A Relation Network.""""""\n  keep_spatial_dims = True\n\n  def compute_logits(self, support_embeddings, query_embeddings,\n                     onehot_support_labels):\n    """"""Computes the relation score of each query example to each prototype.""""""\n    # [n_test, 21, 21, n_features].\n    test_embed_shape = query_embeddings.shape.as_list()\n    n_feature = test_embed_shape[3]\n    out_shape = test_embed_shape[1:3]\n    n_test = tf.shape(query_embeddings)[0]\n\n    # [n_test, num_clases, 21, 21, n_feature].\n    # It is okay one of the elements in the list to be tensor.\n    prototypes = compute_prototypes(support_embeddings, onehot_support_labels)\n\n    prototype_extended = tf.tile(\n        tf.expand_dims(prototypes, 0), [n_test, 1, 1, 1, 1])\n    # [num_clases, n_test, 21, 21, n_feature].\n    test_f_extended = tf.tile(\n        tf.expand_dims(query_embeddings, 1),\n        [1, tf.shape(onehot_support_labels)[-1], 1, 1, 1])\n    relation_pairs = tf.concat((prototype_extended, test_f_extended), 4)\n    # relation_pairs.shape.as_list()[-3:] == [-1] + out_shape + [n_feature*2]\n    relation_pairs = tf.reshape(relation_pairs,\n                                [-1] + out_shape + [n_feature * 2])\n    relationnet_dict = _relation_net(relation_pairs, \'relationnet\')\n    way = tf.shape(onehot_support_labels)[-1]\n    relations = tf.reshape(relationnet_dict[\'output\'], [-1, way])\n    return relations\n\n  def compute_loss(self, onehot_labels, predictions):\n    """"""Computes the MSE loss of `predictions` with respect to `onehot_labels`.\n\n    Args:\n      onehot_labels: A `tf.Tensor` containing the the class labels; each vector\n        along the class dimension should hold a valid probability distribution.\n      predictions: A `tf.Tensor` containing the the class predictions,\n        interpreted as unnormalized log probabilities.\n\n    Returns:\n       A `tf.Tensor` representing the average loss.\n    """"""\n    mse_loss = tf.losses.mean_squared_error(onehot_labels, predictions)\n    regularization = tf.reduce_sum(\n        tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n    loss = mse_loss + self.weight_decay * regularization\n    return loss\n\n\ndef linear_classifier_forward_pass(embeddings, w_fc, b_fc, cosine_classifier,\n                                   cosine_logits_multiplier, use_weight_norm):\n  """"""Passes embeddings through the linear layer defined by w_fc and b_fc.\n\n  Args:\n    embeddings: A Tensor of size [batch size, embedding dim].\n    w_fc: A Tensor of size [embedding dim, num outputs].\n    b_fc: Either None, or a Tensor of size [num outputs] or []. If\n      cosine_classifier is False, it can not be None.\n    cosine_classifier: A bool. If true, a cosine classifier is used which does\n      not require the bias b_fc.\n    cosine_logits_multiplier: A float. Only used if cosine_classifier is True,\n      and multiplies the resulting logits.\n    use_weight_norm: A bool. Whether weight norm was used. If so, then if using\n      cosine classifier, normalize only the embeddings but not the weights.\n\n  Returns:\n    logits: A Tensor of size [batch size, num outputs].\n  """"""\n  if cosine_classifier:\n    # Each column of the weight matrix may be interpreted as a class\n    # representation (of the same dimenionality as the embedding space). The\n    # logit for an embedding vector belonging to that class is the cosine\n    # similarity between that embedding and that class representation.\n    embeddings = tf.nn.l2_normalize(embeddings, axis=1, epsilon=1e-3)\n    if not use_weight_norm:\n      # Only normalize the weights if weight norm was not used.\n      w_fc = tf.nn.l2_normalize(w_fc, axis=0, epsilon=1e-3)\n    logits = tf.matmul(embeddings, w_fc)\n    # Scale the logits as passing numbers in [-1, 1] to softmax is not very\n    # expressive.\n    logits *= cosine_logits_multiplier\n  else:\n    assert b_fc is not None\n    logits = tf.matmul(embeddings, w_fc) + b_fc\n  return logits\n\n\ndef linear_classifier_logits(embeddings, num_classes, cosine_classifier,\n                             cosine_logits_multiplier, use_weight_norm):\n  """"""Forward pass through a linear classifier, possibly a cosine classifier.""""""\n\n  # A variable to keep track of whether the initialization has already happened.\n  data_dependent_init_done = tf.get_variable(\n      \'data_dependent_init_done\',\n      initializer=0,\n      dtype=tf.int32,\n      trainable=False)\n\n  embedding_dims = embeddings.get_shape().as_list()[-1]\n\n  if use_weight_norm:\n    w_fc = tf.get_variable(\n        \'w_fc\', [embedding_dims, num_classes],\n        initializer=tf.random_normal_initializer(0, 0.05),\n        trainable=True)\n    # This init is temporary as it needs to be done in a data-dependent way.\n    # It will be overwritten during the first forward pass through this layer.\n    g = tf.get_variable(\n        \'g\',\n        dtype=tf.float32,\n        initializer=tf.ones([num_classes]),\n        trainable=True)\n    b_fc = None\n    if not cosine_classifier:\n      # Also initialize a bias.\n      b_fc = tf.get_variable(\n          \'b_fc\', initializer=tf.zeros([num_classes]), trainable=True)\n\n    def _do_data_dependent_init():\n      """"""Returns ops for the data-dependent init of g and maybe b_fc.""""""\n      w_fc_normalized = tf.nn.l2_normalize(w_fc.read_value(), [0])\n      output_init = tf.matmul(embeddings, w_fc_normalized)\n      mean_init, var_init = tf.nn.moments(output_init, [0])\n      # Data-dependent init values.\n      g_init_value = 1. / tf.sqrt(var_init + 1e-10)\n      ops = [tf.assign(g, g_init_value)]\n      if not cosine_classifier:\n        # Also initialize a bias in a data-dependent way.\n        b_fc_init_value = -mean_init * g_init_value\n        ops.append(tf.assign(b_fc, b_fc_init_value))\n      # Mark that the data-dependent initialization is done to prevent it from\n      # happening again in the future.\n      ops.append(tf.assign(data_dependent_init_done, 1))\n      return tf.group(*ops)\n\n    # Possibly perform data-dependent init (if it hasn\'t been done already).\n    init_op = tf.cond(\n        tf.equal(data_dependent_init_done, 0), _do_data_dependent_init,\n        tf.no_op)\n\n    with tf.control_dependencies([init_op]):\n      # Apply weight normalization.\n      w_fc *= g / tf.sqrt(tf.reduce_sum(tf.square(w_fc), [0]))\n      # Forward pass through the layer defined by w_fc and b_fc.\n      logits = linear_classifier_forward_pass(embeddings, w_fc, b_fc,\n                                              cosine_classifier,\n                                              cosine_logits_multiplier, True)\n\n  else:\n    # No weight norm.\n    w_fc = weight_variable([embedding_dims, num_classes])\n    b_fc = None\n    if not cosine_classifier:\n      # Also initialize a bias.\n      b_fc = bias_variable([num_classes])\n    # Forward pass through the layer defined by w_fc and b_fc.\n    logits = linear_classifier_forward_pass(embeddings, w_fc, b_fc,\n                                            cosine_classifier,\n                                            cosine_logits_multiplier, False)\n  return logits\n\n\n# TODO(tylerzhu): Consider adding an episodic kNN learner as well so we can\n# create a baseline leaner by composing a batch learner and the evaluation\n# process of an episodic kNN learner.\n@gin.configurable\nclass BaselineLearner(BatchLearner):\n  """"""A Baseline Network.""""""\n\n  def __init__(self, knn_in_fc, knn_distance, cosine_classifier,\n               cosine_logits_multiplier, use_weight_norm, **kwargs):\n    """"""Initializes a baseline learner.\n\n    Args:\n      knn_in_fc: Whether kNN is performed in the space of fc activations or\n        embeddings. If True, the logits from the last fc layer are used as the\n        embedding on which kNN lookup is performed. Otherwise, the penultimate\n        layer is what the kNN lookup is performed on.\n      knn_distance: The distance measurement used by kNN lookup. \'l2\', \'cosine\'\n      cosine_classifier: A bool. Whether to use a cosine classifier at training\n        time when performing the all-way classification task to train the\n        backbone.\n      cosine_logits_multiplier: A float. A scalar that will multiply the logits\n        computed by the cosine classifier (if applicable) before passing them\n        into the softmax.\n      use_weight_norm: A bool. Whether to apply weight normalization to the\n        linear classifier layer.\n      **kwargs: Keyword arguments common to all BatchLearners.\n    """"""\n    self.knn_in_fc = knn_in_fc\n    self.distance = knn_distance\n    self.cosine_classifier = cosine_classifier\n    self.cosine_logits_multiplier = cosine_logits_multiplier\n    self.use_weight_norm = use_weight_norm\n    logging.info(\n        \'BaselineLearner: \'\n        \'distance %s, \'\n        \'cosine_classifier: %s\'\n        \'knn_in_fc %s\', knn_distance, cosine_classifier, knn_in_fc)\n\n    super(BaselineLearner, self).__init__(**kwargs)\n\n  def forward_pass(self, data):\n    if self.is_training:\n      images = data.images\n      embeddings_params_moments = self.embedding_fn(images, self.is_training)\n      train_embeddings = embeddings_params_moments[\'embeddings\']\n      train_logits = self.forward_pass_fc(train_embeddings)\n      return train_logits\n    else:\n      train_embeddings_params_moments = self.embedding_fn(\n          data.train_images, self.is_training)\n      train_embeddings = train_embeddings_params_moments[\'embeddings\']\n      support_set_moments = None\n      if not self.transductive_batch_norm:\n        support_set_moments = train_embeddings_params_moments[\'moments\']\n      test_embeddings = self.embedding_fn(\n          data.test_images,\n          self.is_training,\n          moments=support_set_moments,\n          backprop_through_moments=self.backprop_through_moments)\n      test_embeddings = test_embeddings[\'embeddings\']\n\n      # TODO(eringrant): The `BaselineFinetuneLearner` subclass is not yet\n      # refactored to obey the interface of `Learner.compute_logits`.\n      if isinstance(self, BaselineFinetuneLearner):\n        test_logits = self.compute_logits(data)  # pylint: disable=no-value-for-parameter\n      else:\n        test_logits = self.compute_logits(train_embeddings, test_embeddings,\n                                          data.onehot_train_labels)\n\n      return test_logits\n\n  def forward_pass_fc(self, embeddings):\n    """"""Passes the provided embeddings through the fc layer to get the logits.\n\n    Args:\n      embeddings: A Tensor of the penultimate layer activations as computed by\n        BaselineLearner.forward_pass.\n\n    Returns:\n      The fc layer activations.\n    """"""\n    with tf.variable_scope(\'fc\', reuse=tf.AUTO_REUSE):\n      # Always maps to a space whose dimensionality is the number of classes\n      # at meta-training time.\n      logits = linear_classifier_logits(embeddings, self.logit_dim,\n                                        self.cosine_classifier,\n                                        self.cosine_logits_multiplier,\n                                        self.use_weight_norm)\n      return logits\n\n  def compute_logits(self, support_embeddings, query_embeddings,\n                     onehot_support_labels):\n    """"""Computes the class logits for the episode.\n\n    Args:\n      support_embeddings: A Tensor of size [num_train_images, embedding dim].\n      query_embeddings: A Tensor of size [num_test_images, embedding dim].\n      onehot_support_labels: A Tensor of size [batch size, way].\n\n    Returns:\n      The query set logits as a [num_test_images, way] matrix.\n\n    Raises:\n      ValueError: Distance must be one of l2 or cosine.\n    """"""\n\n    if self.knn_in_fc:\n      # Recompute the train and test embeddings that were originally computed\n      # in self.forward_pass() to be the fc layer activations.\n      support_embeddings = self.forward_pass_fc(support_embeddings)\n      query_embeddings = self.forward_pass_fc(query_embeddings)\n\n    # ------------------------ K-NN look up -------------------------------\n    # For each testing example in an episode, we use its embedding\n    # vector to look for the closest neighbor in all the training examples\'\n    # embeddings from the same episode and then assign the training example\'s\n    # class label to the testing example as the predicted class label for it.\n    if self.distance == \'l2\':\n      #  [1, num_support, embed_dims]\n      support_embeddings = tf.expand_dims(support_embeddings, axis=0)\n      #  [num_query, 1, embed_dims]\n      query_embeddings = tf.expand_dims(query_embeddings, axis=1)\n      #  [num_query, num_support]\n      distance = tf.norm(query_embeddings - support_embeddings, axis=2)\n    elif self.distance == \'cosine\':\n      support_embeddings = tf.nn.l2_normalize(support_embeddings, axis=1)\n      query_embeddings = tf.nn.l2_normalize(query_embeddings, axis=1)\n      distance = -1 * tf.matmul(\n          query_embeddings, support_embeddings, transpose_b=True)\n    else:\n      raise ValueError(\'Distance must be one of l2 or cosine.\')\n    #  [num_query]\n    _, indices = tf.nn.top_k(-distance, k=1)\n    indices = tf.squeeze(indices, axis=1)\n    #  [num_query, num_classes]\n    query_logits = tf.gather(onehot_support_labels, indices)\n    return query_logits\n\n\n@gin.configurable\nclass BaselineFinetuneLearner(BaselineLearner):\n  """"""A Baseline Network with test-time finetuning.""""""\n\n  def __init__(self,\n               num_finetune_steps,\n               finetune_lr,\n               debug_log=False,\n               finetune_all_layers=False,\n               finetune_with_adam=False,\n               **kwargs):\n    """"""Initializes a baseline learner.\n\n    Args:\n      num_finetune_steps: number of finetune steps.\n      finetune_lr: the learning rate used for finetuning.\n      debug_log: If True, print out debug logs.\n      finetune_all_layers: Whether to finetune all embedding variables. If\n        False, only trains a linear classifier on top of the embedding.\n      finetune_with_adam: Whether to use Adam for the within-episode finetuning.\n        If False, gradient descent is used instead.\n      **kwargs: Keyword arguments common to all `BaselineLearner`s (including\n        `knn_in_fc` and `knn_distance`, which are not used by\n        `BaselineFinetuneLearner` but are used by the parent class).\n    """"""\n    self.num_finetune_steps = num_finetune_steps\n    self.finetune_lr = finetune_lr\n    self.debug_log = debug_log\n    self.finetune_all_layers = finetune_all_layers\n    self.finetune_with_adam = finetune_with_adam\n    if finetune_with_adam:\n      self.finetune_opt = tf.train.AdamOptimizer(self.finetune_lr)\n    super(BaselineFinetuneLearner, self).__init__(**kwargs)\n\n  def compute_logits(self, data):\n    """"""Computes the class logits for the episode.\n\n    Args:\n      data: A `meta_dataset.providers.EpisodeDataset`.\n\n    Returns:\n      The query set logits as a [num_test_images, way] matrix.\n\n    Raises:\n      ValueError: Distance must be one of l2 or cosine.\n    """"""\n    # ------------------------ Finetuning -------------------------------\n    # Possibly make copies of embedding variables, if they will get modified.\n    # This is for making temporary-only updates to the embedding network\n    # which will not persist after the end of the episode.\n    make_copies = self.finetune_all_layers\n\n    # TODO(eringrant): Reduce the number of times the embedding function graph\n    # is built with the same input.\n    train_embeddings_params_moments = self.embedding_fn(data.train_images,\n                                                        self.is_training)\n    train_embeddings = train_embeddings_params_moments[\'embeddings\']\n    train_embeddings_var_dict = train_embeddings_params_moments[\'params\']\n\n    (embedding_vars_keys, embedding_vars,\n     embedding_vars_copy_ops) = get_embeddings_vars_copy_ops(\n         train_embeddings_var_dict, make_copies)\n\n    embedding_vars_copy_op = tf.group(*embedding_vars_copy_ops)\n\n    # Compute the initial training loss (only for printing purposes). This\n    # line is also needed for adding the fc variables to the graph so that the\n    # tf.all_variables() line below detects them.\n    logits = self._fc_layer(train_embeddings)[:, 0:data.way]\n    finetune_loss = self.compute_loss(\n        onehot_labels=data.onehot_train_labels,\n        predictions=logits,\n    )\n\n    # Decide which variables to finetune.\n    fc_vars, vars_to_finetune = [], []\n    for var in tf.trainable_variables():\n      if \'fc_finetune\' in var.name:\n        fc_vars.append(var)\n        vars_to_finetune.append(var)\n    if self.finetune_all_layers:\n      vars_to_finetune.extend(embedding_vars)\n    logging.info(\'Finetuning will optimize variables: %s\', vars_to_finetune)\n\n    for i in range(self.num_finetune_steps):\n      if i == 0:\n        # Randomly initialize the fc layer.\n        fc_reset = tf.variables_initializer(var_list=fc_vars)\n        # Adam related variables are created when minimize() is called.\n        # We create an unused op here to put all adam varariables under\n        # the \'adam_opt\' namescope and create a reset op to reinitialize\n        # these variables before the first finetune step.\n        adam_reset = tf.no_op()\n        if self.finetune_with_adam:\n          with tf.variable_scope(\'adam_opt\'):\n            unused_op = self.finetune_opt.minimize(\n                finetune_loss, var_list=vars_to_finetune)\n          adam_reset = tf.variables_initializer(self.finetune_opt.variables())\n        with tf.control_dependencies(\n            [fc_reset, adam_reset, finetune_loss, embedding_vars_copy_op] +\n            vars_to_finetune):\n          print_op = tf.no_op()\n          if self.debug_log:\n            print_op = tf.print([\n                \'step: %d\' % i, vars_to_finetune[0][0, 0], \'loss:\',\n                finetune_loss\n            ])\n\n          with tf.control_dependencies([print_op]):\n            # Get the operation for finetuning.\n            # (The logits and loss are returned just for printing).\n            logits, finetune_loss, finetune_op = self._get_finetune_op(\n                data, embedding_vars_keys, embedding_vars, vars_to_finetune,\n                train_embeddings if not self.finetune_all_layers else None)\n\n            if self.debug_log:\n              # Test logits are computed only for printing logs.\n              test_embeddings = self.embedding_fn(\n                  data.test_images,\n                  self.is_training,\n                  params=collections.OrderedDict(\n                      zip(embedding_vars_keys, embedding_vars)),\n                  reuse=True)[\'embeddings\']\n              test_logits = (self._fc_layer(test_embeddings)[:, 0:data.way])\n\n      else:\n        with tf.control_dependencies([finetune_op, finetune_loss] +\n                                     vars_to_finetune):\n          print_op = tf.no_op()\n          if self.debug_log:\n            print_op = tf.print([\n                \'step: %d\' % i,\n                vars_to_finetune[0][0, 0],\n                \'loss:\',\n                finetune_loss,\n                \'accuracy:\',\n                self.compute_accuracy(\n                    labels=data.train_labels, predictions=logits),\n                \'test accuracy:\',\n                self.compute_accuracy(\n                    labels=data.test_labels, predictions=test_logits),\n            ])\n\n          with tf.control_dependencies([print_op]):\n            # Get the operation for finetuning.\n            # (The logits and loss are returned just for printing).\n            logits, finetune_loss, finetune_op = self._get_finetune_op(\n                data, embedding_vars_keys, embedding_vars, vars_to_finetune,\n                train_embeddings if not self.finetune_all_layers else None)\n\n            if self.debug_log:\n              # Test logits are computed only for printing logs.\n              test_embeddings = self.embedding_fn(\n                  data.test_images,\n                  self.is_training,\n                  params=collections.OrderedDict(\n                      zip(embedding_vars_keys, embedding_vars)),\n                  reuse=True)[\'embeddings\']\n              test_logits = (self._fc_layer(test_embeddings)[:, 0:data.way])\n\n    # Finetuning is now over, compute the test performance using the updated\n    # fc layer, and possibly the updated embedding network.\n    with tf.control_dependencies([finetune_op] + vars_to_finetune):\n      test_embeddings = self.embedding_fn(\n          data.test_images,\n          self.is_training,\n          params=collections.OrderedDict(\n              zip(embedding_vars_keys, embedding_vars)),\n          reuse=True)[\'embeddings\']\n      test_logits = self._fc_layer(test_embeddings)[:, 0:data.way]\n\n      if self.debug_log:\n        # The train logits are computed only for printing.\n        train_embeddings = self.embedding_fn(\n            data.train_images,\n            self.is_training,\n            params=collections.OrderedDict(\n                zip(embedding_vars_keys, embedding_vars)),\n            reuse=True)[\'embeddings\']\n        logits = self._fc_layer(train_embeddings)[:, 0:data.way]\n\n      print_op = tf.no_op()\n      if self.debug_log:\n        print_op = tf.print([\n            \'accuracy:\',\n            self.compute_accuracy(labels=data.train_labels, predictions=logits),\n            \'test accuracy:\',\n            self.compute_accuracy(\n                labels=data.test_labels, predictions=test_logits),\n        ])\n      with tf.control_dependencies([print_op]):\n        test_logits = self._fc_layer(test_embeddings)[:, 0:data.way]\n\n    return test_logits\n\n  def _get_finetune_op(self,\n                       data,\n                       embedding_vars_keys,\n                       embedding_vars,\n                       vars_to_finetune,\n                       train_embeddings=None):\n    """"""Returns the operation for performing a finetuning step.""""""\n    if train_embeddings is None:\n      train_embeddings = self.embedding_fn(\n          data.train_images,\n          self.is_training,\n          params=collections.OrderedDict(\n              zip(embedding_vars_keys, embedding_vars)),\n          reuse=True)[\'embeddings\']\n    logits = self._fc_layer(train_embeddings)[:, 0:data.way]\n    finetune_loss = self.compute_loss(\n        onehot_labels=data.onehot_train_labels,\n        predictions=logits,\n    )\n    # Perform one step of finetuning.\n    if self.finetune_with_adam:\n      finetune_op = self.finetune_opt.minimize(\n          finetune_loss, var_list=vars_to_finetune)\n    else:\n      # Apply vanilla gradient descent instead of Adam.\n      update_ops = gradient_descent_step(finetune_loss, vars_to_finetune, True,\n                                         False, self.finetune_lr)[\'update_ops\']\n      finetune_op = tf.group(*update_ops)\n    return logits, finetune_loss, finetune_op\n\n  def _fc_layer(self, embedding):\n    """"""The fully connected layer to be finetuned.""""""\n    with tf.variable_scope(\'fc_finetune\', reuse=tf.AUTO_REUSE):\n      logits = linear_classifier_logits(embedding, self.logit_dim,\n                                        self.cosine_classifier,\n                                        self.cosine_logits_multiplier,\n                                        self.use_weight_norm)\n    return logits\n\n\n@gin.configurable\nclass MAMLLearner(EpisodicLearner):\n  """"""Model-Agnostic Meta Learner.""""""\n\n  def __init__(self, num_update_steps, additional_test_update_steps,\n               first_order, alpha, train_batch_norm, debug, zero_fc_layer,\n               proto_maml_fc_layer_init, **kwargs):\n    """"""Initializes a baseline learner.\n\n    Args:\n      num_update_steps: The number of inner-loop steps to take.\n      additional_test_update_steps: The number of additional inner-loop steps to\n        take on meta test and meta validation set.\n      first_order: If True, ignore second-order gradients (faster).\n      alpha: The inner-loop learning rate.\n      train_batch_norm: If True, train batch norm during meta training.\n      debug: If True, print out debug logs.\n      zero_fc_layer: Whether to use zero fc layer initialization.\n      proto_maml_fc_layer_init: Whether to use ProtoNets equivalent fc layer\n        initialization.\n      **kwargs: Keyword arguments common to all EpisodicLearners.\n\n    Raises:\n      ValueError: The embedding function must be MAML-compatible.\n      RuntimeError: Requested to meta-learn the initialization of the linear\n        layer weights but they are unexpectedly omitted from saving/restoring.\n    """"""\n    if not zero_fc_layer and not proto_maml_fc_layer_init:\n      # So the linear classifier weights initialization is meta-learned.\n      if \'linear_classifier\' in FLAGS.omit_from_saving_and_reloading:\n        raise RuntimeError(\'The linear layer is requested to be meta-learned \'\n                           \'since both zero_fc_layer and \'\n                           \'proto_maml_fc_layer_init are False, but the \'\n                           \'linear_classifier weights are found in \'\n                           \'FLAGS.omit_from_saving_and_reloading so they will \'\n                           \'not be properly restored. Please exclude these \'\n                           \'weights from omit_from_saving_and_reloading for \'\n                           \'this setting to work as expected.\')\n\n    self.alpha = alpha\n    self.num_update_steps = num_update_steps\n    self.additional_test_update_steps = additional_test_update_steps\n    self.first_order = first_order\n    self.train_batch_norm = train_batch_norm\n    self.debug_log = debug\n    self.zero_fc_layer = zero_fc_layer\n    self.proto_maml_fc_layer_init = proto_maml_fc_layer_init\n\n    logging.info(\'alpha: %s, num_update_steps: %d\', self.alpha,\n                 self.num_update_steps)\n\n    super(MAMLLearner, self).__init__(**kwargs)\n\n  def proto_maml_fc_weights(self, prototypes, zero_pad_to_max_way=False):\n    """"""Computes the Prototypical MAML fc layer\'s weights.\n\n    Args:\n      prototypes: Tensor of shape [num_classes, embedding_size]\n      zero_pad_to_max_way: Whether to zero padd to max num way.\n\n    Returns:\n      fc_weights: Tensor of shape [embedding_size, num_classes] or\n        [embedding_size, self.logit_dim] when zero_pad_to_max_way is True.\n    """"""\n    fc_weights = 2 * prototypes\n    fc_weights = tf.transpose(fc_weights)\n    if zero_pad_to_max_way:\n      paddings = [[0, 0], [0, self.logit_dim - tf.shape(fc_weights)[1]]]\n      fc_weights = tf.pad(fc_weights, paddings, \'CONSTANT\', constant_values=0)\n    return fc_weights\n\n  def proto_maml_fc_bias(self, prototypes, zero_pad_to_max_way=False):\n    """"""Computes the Prototypical MAML fc layer\'s bias.\n\n    Args:\n      prototypes: Tensor of shape [num_classes, embedding_size]\n      zero_pad_to_max_way: Whether to zero padd to max num way.\n\n    Returns:\n      fc_bias: Tensor of shape [num_classes] or [self.logit_dim]\n        when zero_pad_to_max_way is True.\n    """"""\n    fc_bias = -tf.square(tf.norm(prototypes, axis=1))\n    if zero_pad_to_max_way:\n      paddings = [[0, self.logit_dim - tf.shape(fc_bias)[0]]]\n      fc_bias = tf.pad(fc_bias, paddings, \'CONSTANT\', constant_values=0)\n    return fc_bias\n\n  def forward_pass(self, data):\n    """"""Computes the test logits of MAML.\n\n    Args:\n      data: An EpisodeDataset. Computes the test logits of MAML on the query\n        (test) set after running meta update steps on the support (train) set.\n\n    Returns:\n      The output logits for the query data in this episode.\n    """"""\n    # Have to use one-hot labels since sparse softmax doesn\'t allow\n    # second derivatives.\n    train_embeddings_ = self.embedding_fn(\n        data.train_images, self.is_training, reuse=tf.AUTO_REUSE)\n    train_embeddings = train_embeddings_[\'embeddings\']\n    embedding_vars_dict = train_embeddings_[\'params\']\n\n    with tf.variable_scope(\'linear_classifier\', reuse=tf.AUTO_REUSE):\n      embedding_depth = train_embeddings.shape.as_list()[-1]\n      fc_weights = weight_variable([embedding_depth, self.logit_dim])\n      fc_bias = bias_variable([self.logit_dim])\n\n    # A list of variable names, a list of corresponding Variables, and a list\n    # of operations (possibly empty) that creates a copy of each Variable.\n    (embedding_vars_keys, embedding_vars,\n     embedding_vars_copy_ops) = get_embeddings_vars_copy_ops(\n         embedding_vars_dict, make_copies=not self.is_training)\n\n    # A Variable for the weights of the fc layer, a Variable for the bias of the\n    # fc layer, and a list of operations (possibly empty) that copies them.\n    (fc_weights, fc_bias, fc_vars_copy_ops) = get_fc_vars_copy_ops(\n        fc_weights, fc_bias, make_copies=not self.is_training)\n\n    fc_vars = [fc_weights, fc_bias]\n    num_embedding_vars = len(embedding_vars)\n    num_fc_vars = len(fc_vars)\n\n    def _cond(step, *args):\n      del args\n      num_steps = self.num_update_steps\n      if not self.is_training:\n        num_steps += self.additional_test_update_steps\n      return step < num_steps\n\n    def _body(step, *args):\n      """"""The inner update loop body.""""""\n      updated_embedding_vars = args[0:num_embedding_vars]\n      updated_fc_vars = args[num_embedding_vars:num_embedding_vars +\n                             num_fc_vars]\n      train_embeddings = self.embedding_fn(\n          data.train_images,\n          self.is_training,\n          params=collections.OrderedDict(\n              zip(embedding_vars_keys, updated_embedding_vars)),\n          reuse=True)[\'embeddings\']\n\n      updated_fc_weights, updated_fc_bias = updated_fc_vars\n      train_logits = tf.matmul(train_embeddings,\n                               updated_fc_weights) + updated_fc_bias\n\n      train_logits = train_logits[:, 0:data.way]\n      loss = tf.losses.softmax_cross_entropy(data.onehot_train_labels,\n                                             train_logits)\n\n      print_op = tf.no_op()\n      if self.debug_log:\n        print_op = tf.print([\'step: \', step, updated_fc_bias[0], \'loss:\', loss])\n\n      with tf.control_dependencies([print_op]):\n        updated_embedding_vars = gradient_descent_step(\n            loss, updated_embedding_vars, self.first_order,\n            self.train_batch_norm, self.alpha, False)[\'updated_vars\']\n        updated_fc_vars = gradient_descent_step(loss, updated_fc_vars,\n                                                self.first_order,\n                                                self.train_batch_norm,\n                                                self.alpha,\n                                                False)[\'updated_vars\']\n\n        step = step + 1\n      return tuple([step] + list(updated_embedding_vars) +\n                   list(updated_fc_vars))\n\n    # MAML meta updates using query set examples from an episode.\n    if self.zero_fc_layer:\n      # To account for variable class sizes, we initialize the output\n      # weights to zero. See if truncated normal initialization will help.\n      zero_weights_op = tf.assign(fc_weights, tf.zeros_like(fc_weights))\n      zero_bias_op = tf.assign(fc_bias, tf.zeros_like(fc_bias))\n      fc_vars_init_ops = [zero_weights_op, zero_bias_op]\n    else:\n      fc_vars_init_ops = fc_vars_copy_ops\n\n    if self.proto_maml_fc_layer_init:\n      train_embeddings = self.embedding_fn(\n          data.train_images,\n          self.is_training,\n          params=collections.OrderedDict(\n              zip(embedding_vars_keys, embedding_vars)),\n          reuse=True)[\'embeddings\']\n\n      prototypes = compute_prototypes(train_embeddings,\n                                      data.onehot_train_labels)\n      pmaml_fc_weights = self.proto_maml_fc_weights(\n          prototypes, zero_pad_to_max_way=True)\n      pmaml_fc_bias = self.proto_maml_fc_bias(\n          prototypes, zero_pad_to_max_way=True)\n      fc_vars = [pmaml_fc_weights, pmaml_fc_bias]\n\n    # These control dependencies assign the value of each variable to a new copy\n    # variable that corresponds to it. This is required at test time for\n    # initilizing the copies as they are used in place of the original vars.\n    with tf.control_dependencies(fc_vars_init_ops + embedding_vars_copy_ops):\n      # Make step a local variable as we don\'t want to save and restore it.\n      step = tf.Variable(\n          0,\n          trainable=False,\n          name=\'inner_step_counter\',\n          collections=[tf.GraphKeys.LOCAL_VARIABLES])\n      loop_vars = [step] + embedding_vars + fc_vars\n      step_and_all_updated_vars = tf.while_loop(\n          _cond, _body, loop_vars, swap_memory=True)\n      step = step_and_all_updated_vars[0]\n      all_updated_vars = step_and_all_updated_vars[1:]\n      updated_embedding_vars = all_updated_vars[0:num_embedding_vars]\n      updated_fc_weights, updated_fc_bias = all_updated_vars[\n          num_embedding_vars:num_embedding_vars + num_fc_vars]\n\n    # Forward pass the training images with the updated weights in order to\n    # compute the means and variances, to use for the query\'s batch norm.\n    support_set_moments = None\n    if not self.transductive_batch_norm:\n      support_set_moments = self.embedding_fn(\n          data.train_images,\n          self.is_training,\n          params=collections.OrderedDict(\n              zip(embedding_vars_keys, updated_embedding_vars)),\n          reuse=True)[\'moments\']\n\n    test_embeddings = self.embedding_fn(\n        data.test_images,\n        self.is_training,\n        params=collections.OrderedDict(\n            zip(embedding_vars_keys, updated_embedding_vars)),\n        moments=support_set_moments,  # Use support set stats for batch norm.\n        reuse=True,\n        backprop_through_moments=self.backprop_through_moments)[\'embeddings\']\n\n    test_logits = (tf.matmul(test_embeddings, updated_fc_weights) +\n                   updated_fc_bias)[:, 0:data.way]\n\n    return test_logits\n'"
meta_dataset/train.py,16,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Script for training models on the benchmark.\n\nLaunching command for batch baseline:\n# pylint: disable=line-too-long\npython -m meta_dataset.train \\\n  --train_checkpoint_dir=/tmp/bench --summary_dir=/tmp/bench \\\n  --records_root_dir=<records_root> \\\n  --alsologtostderr \\\n  --gin_config=meta_dataset/learn/gin/default/<exp_name>.gin\n  --gin_bindings=""Trainer.experiment_name=\'<exp_name>\'""\n# pylint: enable=line-too-long\n\nwhere:\n  <exp_name> is e.g. \'debug_proto_mini_imagenet\'\n\nTo override elements from the config, you can use arguments of the form:\n  For gin: --gin_bindings=\'foo = 1000000\'\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\n\nfrom absl import app\nfrom absl import logging\nimport gin\nimport gin.tf\nfrom meta_dataset import data\nfrom meta_dataset import trainer\nfrom meta_dataset.data import config  # pylint: disable=unused-import\nimport tensorflow.compat.v1 as tf\n\nDEFAULT_SAVING_DIRECTORY = \'/tmp/metadataset\'\n\ntf.flags.DEFINE_string(\'train_checkpoint_dir\',\n                       os.path.join(DEFAULT_SAVING_DIRECTORY, \'checkpoints\'),\n                       \'The directory to save checkpoints.\')\ntf.flags.DEFINE_string(\'summary_dir\',\n                       os.path.join(DEFAULT_SAVING_DIRECTORY, \'summaries\'),\n                       \'The directory for writing summaries.\')\ntf.flags.DEFINE_bool(\n    \'reload_checkpoint_gin_config\', False,\n    \'Whether to reload an operative Gin configuration along with a checkpoint.\')\ntf.flags.DEFINE_string(\'records_root_dir\', \'\',\n                       \'Root directory containing a subdirectory per dataset.\')\ntf.flags.DEFINE_bool(\n    \'is_training\', True, \'Whether we are in the training phase. \'\n    \'Used to control whether to perform training or evaluation.\')\n\ntf.flags.DEFINE_multi_string(\'gin_config\', None,\n                             \'List of paths to the config files.\')\ntf.flags.DEFINE_multi_string(\'gin_bindings\', None,\n                             \'List of Gin parameter bindings.\')\n\ntf.flags.DEFINE_string(\n    \'eval_imbalance_dataset\', \'\', \'A dataset on which to perform evaluation \'\n    \'for assessing how class imbalance affects performance in binary episodes. \'\n    \'By default it is empty and no imbalance analysis is performed.\')\n\n# TODO(crisnv): eval_split is supposed to substitute eval_finegrainedness and\n# eval_finegrainedness_split in the future\ntf.flags.DEFINE_enum(\n    \'eval_split\', None,\n    [trainer.TRAIN_SPLIT, trainer.VALID_SPLIT, trainer.TEST_SPLIT],\n    \'Override the evaluation split. If None (default), regular logic is used, \'\n    \'that is, if ""is_training"" is True, ""trainer.VALID_SPLIT"" is used, \'\n    \'otherwise ""trainer.TEST_SPLIT"" is used. The ""is_training"" case also uses \'\n    \'the value of eval_finegrainedness_split if eval_finegrainedness is True.\')\n\ntf.flags.DEFINE_bool(\n    \'eval_finegrainedness\', False, \'Whether to perform only 2-way ImageNet \'\n    \'evaluation for assessing performance as a function of how finegrained \'\n    \'each task is. This differs from usual ImageNet eval in the sampling \'\n    \'procedure used to get episodes, and therefore requires its own setting.\')\n\ntf.flags.DEFINE_enum(\n    \'eval_finegrainedness_split\', trainer.TRAIN_SPLIT,\n    [trainer.TRAIN_SPLIT, trainer.VALID_SPLIT, trainer.TEST_SPLIT], \'The \'\n    \'split whose results we want to use for the fine-grainedness analysis.\'\n    \'Contrary to most analyses which are performed on the test split only, the \'\n    \'fine-grainedness analysis may also be performed on the train or valid \'\n    \'sub-graphs of ImageNet too, since the test sub-graph evidently does not \'\n    \'exhibit enough variation in the fine-grainedness of its different tasks \'\n    \'to allow for a meaningful analysis.\')\n# The following flag specifies substrings of variable names that should not be\n# reloaded. `num_left_in_epoch\' is a variable that influences the behavior of\n# the EpochTrackers. Since the state of those trackers is not reloaded, neither\n# should this variable. `fc_finetune\' is a substring of the names of the\n# variables in the episode-specific linear layer of the finetune baseline (used\n# at meta-validation and meta-test times). Since this layer gets re-initialized\n# to random weights in each new episode, there is no need to ever restore these\n# weights. `linear_classifier\' plays that role but for the MAML model: similarly\n# in each new episode it is re-initialized (e.g. set to zeros or to the\n# prototypes in the case of proto-MAML), so there is no need to restore these\n# weights. `adam_opt\' captures the variables of the within-episode optimizer of\n# the finetune baseline when it is configured to perform that finetuning with\n# adam. `fc\' captures the variable names of the fully-connected layer for the\n# all-way classification problem that the baselines solve at training time.\n# There are certain situations where we need to omit reloading these weights to\n# avoid getting an error. Consider for example the experiments where we train\n# a baseline model, starting from weights that were previously trained on\n# ImageNet. If this training now takes place on all datasets, the size of the\n# all-way classification layer is now different (equal to the number of\n# meta-training classes of all datasets not just of ImageNet). Thus when\n# training baselines from pre-trained weights, we only reload the backbone and\n# not the `fc\' all-way classification layer (similarly for inference-only\n# experiments for the same reason).\ntf.flags.DEFINE_multi_enum(\n    \'omit_from_saving_and_reloading\', [\n        \'num_left_in_epoch\', \'fc_finetune\', \'linear_classifier\', \'adam_opt\',\n        \'weight_copy\'\n    ], [\n        \'num_left_in_epoch\', \'fc_finetune\', \'linear_classifier\', \'adam_opt\',\n        \'weight_copy\', \'fc\'\n    ],\n    \'A comma-separated list of substrings such that all variables containing \'\n    \'them should not be saved and reloaded.\')\n\nFLAGS = tf.flags.FLAGS\n\n\ndef parse_cmdline_gin_configurations():\n  """"""Parse Gin configurations from all command-line sources.""""""\n  with gin.unlock_config():\n    gin.parse_config_files_and_bindings(\n        FLAGS.gin_config, FLAGS.gin_bindings, finalize_config=True)\n\n\ndef operative_config_path(operative_config_dir,\n                          operative_config_filename=\'operative_config.gin\'):\n  return os.path.join(operative_config_dir, operative_config_filename)\n\n\ndef load_operative_gin_configurations(operative_config_dir):\n  """"""Load operative Gin configurations from the given directory.""""""\n  gin_log_file = operative_config_path(operative_config_dir)\n  with gin.unlock_config():\n    gin.parse_config_file(gin_log_file)\n  gin.finalize()\n  logging.info(\'Operative Gin configurations loaded from %s.\', gin_log_file)\n\n\ndef record_operative_gin_configurations(operative_config_dir):\n  """"""Record operative Gin configurations in the given directory.""""""\n  gin_log_file = operative_config_path(operative_config_dir)\n  # If it exists already, rename it instead of overwriting it.\n  # This just saves the previous one, not all the ones before.\n  if tf.io.gfile.exists(gin_log_file):\n    tf.io.gfile.rename(gin_log_file, gin_log_file + \'.old\', overwrite=True)\n  with tf.io.gfile.GFile(gin_log_file, \'w\') as f:\n    f.write(gin.operative_config_str())\n\n\ndef main(unused_argv):\n\n  # Parse Gin configurations passed to this script.\n  parse_cmdline_gin_configurations()\n\n  if FLAGS.reload_checkpoint_gin_config:\n    # Try to reload a previously recorded Gin configuration from an operative\n    # Gin configuration file in one of the provided directories.\n    # TODO(eringrant): Allow querying of a value to be bound without binding it\n    # to avoid the redundant call to `parse_cmdline_gin_configurations` below.\n    try:\n      checkpoint_to_restore = gin.query_parameter(\n          \'Trainer.checkpoint_to_restore\')\n    except ValueError:\n      checkpoint_to_restore = None\n\n    # Load the operative Gin configurations from the checkpoint directory.\n    if checkpoint_to_restore:\n      restore_checkpoint_dir = os.path.dirname(checkpoint_to_restore)\n      load_operative_gin_configurations(restore_checkpoint_dir)\n\n      # Reload the command-line Gin configuration to allow overriding of the Gin\n      # configuration loaded from the checkpoint directory.\n      parse_cmdline_gin_configurations()\n\n  # Wrap object instantiations to print out full Gin configuration on failure.\n  try:\n    (train_datasets, eval_datasets, restrict_classes,\n     restrict_num_per_class) = trainer.get_datasets_and_restrictions()\n\n    # Get a trainer or evaluator.\n    trainer_instance = trainer.Trainer(\n        is_training=FLAGS.is_training,\n        train_dataset_list=train_datasets,\n        eval_dataset_list=eval_datasets,\n        restrict_classes=restrict_classes,\n        restrict_num_per_class=restrict_num_per_class,\n        checkpoint_dir=FLAGS.train_checkpoint_dir,\n        summary_dir=FLAGS.summary_dir,\n        records_root_dir=FLAGS.records_root_dir,\n        eval_finegrainedness=FLAGS.eval_finegrainedness,\n        eval_finegrainedness_split=FLAGS.eval_finegrainedness_split,\n        eval_imbalance_dataset=FLAGS.eval_imbalance_dataset,\n        omit_from_saving_and_reloading=FLAGS.omit_from_saving_and_reloading,\n        eval_split=FLAGS.eval_split,\n    )\n  except ValueError as e:\n    logging.info(\'Full Gin configurations:\\n%s\', gin.config_str())\n    raise e\n\n  # All configurable objects/functions should have been instantiated/called.\n  # TODO(evcu): Tie saving of Gin configuration at training and evaluation time.\n  logging.info(\'Operative Gin configurations:\\n%s\', gin.operative_config_str())\n  if FLAGS.is_training and FLAGS.train_checkpoint_dir:\n    record_operative_gin_configurations(FLAGS.train_checkpoint_dir)\n  elif not FLAGS.is_training and FLAGS.summary_dir:\n    record_operative_gin_configurations(FLAGS.summary_dir)\n\n  datasets = train_datasets if FLAGS.is_training else eval_datasets\n  logging.info(\'Starting %s for dataset(s) %s...\',\n               \'training\' if FLAGS.is_training else \'evaluation\', datasets)\n  if FLAGS.is_training:\n    trainer_instance.train()\n  elif set(datasets).intersection(trainer.DATASETS_WITH_EXAMPLE_SPLITS):\n    if not data.POOL_SUPPORTED:\n      raise NotImplementedError(\'Example-level splits or pools not supported.\')\n  else:\n    if len(datasets) != 1:\n      raise ValueError(\'Requested datasets {} for evaluation, but evaluation \'\n                       \'should be performed on individual datasets \'\n                       \'only.\'.format(datasets))\n\n    if FLAGS.eval_finegrainedness:\n      eval_split = FLAGS.eval_finegrainedness_split\n    elif FLAGS.eval_split:\n      eval_split = FLAGS.eval_split\n    else:\n      eval_split = trainer.TEST_SPLIT\n\n    _, _, acc_summary, ci_acc_summary = trainer_instance.evaluate(eval_split)\n    if trainer_instance.summary_writer:\n      trainer_instance.summary_writer.add_summary(acc_summary)\n      trainer_instance.summary_writer.add_summary(ci_acc_summary)\n\n  # Flushes the event file to disk and closes the file.\n  if trainer_instance.summary_writer:\n    trainer_instance.summary_writer.close()\n\n\nprogram = main\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  app.run(program)\n'"
meta_dataset/trainer.py,55,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Interface for a learner that uses BenchmarkReaderDataSource to get data.""""""\n# TODO(lamblinp): Update variable names to be more consistent\n# - target, class_idx, label\n# - support, query\n# TODO(lamblinp): Simplify the logic around performing evaluation on the\n# `TRAIN_SPLIT` by, for instance, recording which data is episodic, and which\n# split it is coming from (independently from how it is used).\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport functools\nimport os\nimport re\n\nfrom absl import logging\nimport gin.tf\nfrom meta_dataset import learner as learner_lib\nfrom meta_dataset.data import dataset_spec as dataset_spec_lib\nfrom meta_dataset.data import learning_spec\nfrom meta_dataset.data import pipeline\nfrom meta_dataset.data import providers\n\nimport numpy as np\nimport six\nfrom six.moves import range\nfrom six.moves import zip\nimport tensorflow.compat.v1 as tf\n\n# Enable TensorFlow optimizations. It can add a few minutes to the first\n# calls to session.run(), but decrease memory usage.\nENABLE_TF_OPTIMIZATIONS = True\n# Enable tf.data optimizations, which are applied to the input data pipeline.\n# It may be helpful to disable them when investigating regressions due to\n# changes in tf.data (see b/121130181 for instance), but they seem to be helpful\n# (or at least not detrimental) in general.\nENABLE_DATA_OPTIMIZATIONS = True\n\nDATASETS_WITH_EXAMPLE_SPLITS = ()\nTF_DATA_OPTIONS = tf.data.Options()\nif not ENABLE_DATA_OPTIMIZATIONS:\n  # The Options object can be used to control which static or dynamic\n  # optimizations to apply.\n  TF_DATA_OPTIONS.experimental_optimization.apply_default_optimizations = False\n\n\n# TODO(eringrant): Use `learning_spec.Split.TRAIN`, `learning_spec.Split.VALID`,\n# and `learning_spec.Split.TEST` instead of string constants, and replace all\n# remaining string redefinitions.\nTRAIN_SPLIT = \'train\'\nVALID_SPLIT = \'valid\'\nTEST_SPLIT = \'test\'\n\nFLAGS = tf.flags.FLAGS\n\n\n\n\nclass UnexpectedSplitError(ValueError):\n\n  def __init__(self,\n               unexpected_split,\n               expected_splits=(TRAIN_SPLIT, TEST_SPLIT, VALID_SPLIT)):\n    super(UnexpectedSplitError,\n          self).__init__(\'Split must be one of {}, but received `{}`. \'.format(\n              expected_splits, unexpected_split))\n\n\n@gin.configurable(\'benchmark\')\ndef get_datasets_and_restrictions(train_datasets=\'\',\n                                  eval_datasets=\'\',\n                                  use_dumped_episodes=False,\n                                  restrict_classes=None,\n                                  restrict_num_per_class=None):\n  """"""Gets the list of dataset names and possible restrictions on their classes.\n\n  Args:\n    train_datasets: A string of comma-separated dataset names for training.\n    eval_datasets: A string of comma-separated dataset names for evaluation.\n    use_dumped_episodes: bool, if True `eval_datasets` are prefixed with\n      `dumped` to trigger evaluation on dumped episodes instead of on the fly\n      sampling.\n    restrict_classes: If provided, a dict that maps dataset names to a dict that\n      specifies for each of `TRAIN_SPLIT`, `VALID_SPLIT` and `TEST_SPLIT` the\n      number of classes to restrict to. This can lead to some classes of a\n      particular split of a particular dataset never participating in episode\n      creation.\n    restrict_num_per_class: If provided, a dict that maps dataset names to a\n      dict that specifies for each of `meta_dataset.trainer.TRAIN_SPLIT`,\n      `meta_dataset.trainer.VALID_SPLIT` and `meta_dataset.trainer.TEST_SPLIT`\n      the number of examples per class to restrict to. For datasets / splits\n      that are not specified, no restriction is applied.\n\n  Returns:\n    Two lists of dataset names and two possibly empty dictionaries.\n  """"""\n  if restrict_classes is None:\n    restrict_classes = {}\n  if restrict_num_per_class is None:\n    restrict_num_per_class = {}\n\n  train_datasets = [d.strip() for d in train_datasets.split(\',\')]\n  eval_datasets = [d.strip() for d in eval_datasets.split(\',\')]\n  if use_dumped_episodes:\n    eval_datasets = [\'dumped_%s\' % ds for ds in eval_datasets]\n  return train_datasets, eval_datasets, restrict_classes, restrict_num_per_class\n\n\ndef apply_dataset_options(dataset):\n  """"""Apply the module-wide set of dataset options to dataset.\n\n  In particular, this is used to enable or disable tf.data optimizations.\n  This applies to the whole pipeline, so we can just set it at the end.\n\n  Args:\n    dataset: a tf.data.Dataset object.\n\n  Returns:\n    A tf.data.Dataset object with options applied.\n  """"""\n  return dataset.with_options(TF_DATA_OPTIONS)\n\n\ndef compute_class_proportions(unique_class_ids, shots, dataset_spec):\n  """"""Computes the proportion of the total number of examples appearing as shots.\n\n  Args:\n    unique_class_ids: A 1D int Tensor of unique class IDs.\n    shots: A 1D Tensor of the number of shots for each class in\n      `unique_class_ids`.\n    dataset_spec: A DatasetSpecification that contains informations about the\n      class labels in `unique_class_ids`.\n\n  Returns:\n    A 1D Tensor with the proportion of examples appearing as shots per class in\n    `unique_class_ids`, normalized by the total number of examples for each\n    class in the dataset according to `dataset_spec`.\n  """"""\n  # Get the total number of examples of each class in the dataset.\n  num_dataset_classes = len(dataset_spec.images_per_class)\n  num_images_per_class = [\n      dataset_spec.get_total_images_per_class(class_id)\n      for class_id in range(num_dataset_classes)\n  ]\n\n  # Make sure that `unique_class_ids` are valid indices of\n  # `num_images_per_class`. This is important since `tf.gather` will fail\n  # silently and return zeros otherwise.\n  num_classes = tf.shape(num_images_per_class)[0]\n  check_valid_inds_op = tf.assert_less(unique_class_ids, num_classes)\n  with tf.control_dependencies([check_valid_inds_op]):\n    # Get the total number of examples of each class that is in the episode.\n    num_images_per_class = tf.gather(num_images_per_class,\n                                     unique_class_ids)  # [?, ]\n\n  # Get the proportions of examples of each class that appear in the episode.\n  class_props = tf.truediv(shots, num_images_per_class)\n  return class_props\n\n\ndef get_split_enum(split):\n  """"""Returns the Enum value corresponding to the given split.\n\n  Args:\n    split: A string, one of TRAIN_SPLIT, VALID_SPLIT, TEST_SPLIT.\n\n  Raises:\n    UnexpectedSplitError: split not TRAIN_SPLIT, VALID_SPLIT, or TEST_SPLIT.\n  """"""\n  # Get the int representing the chosen split.\n  if split == TRAIN_SPLIT:\n    split_enum = learning_spec.Split.TRAIN\n  elif split == VALID_SPLIT:\n    split_enum = learning_spec.Split.VALID\n  elif split == TEST_SPLIT:\n    split_enum = learning_spec.Split.TEST\n  else:\n    raise UnexpectedSplitError(split)\n  return split_enum\n\n\nOPTIMIZER_KEYWORDS = (\'Adam:\', \'Adam_1:\')\nEMBEDDING_KEYWORDS = (\'conv\', \'resnet\')\n\n\ndef is_backbone_variable(variable, only_if=lambda x: True):\n  """"""Returns True if `variable` is a backbone (embedding function) variable.\n\n  Args:\n    variable: A `tf.Variable` whose `name` attribute will be checked to\n      determine whether it belongs to the backbone (embedding function) of a\n      `Learner`.\n    only_if: A callable that returns `True` when a `tf.Variable` satisfies some\n      condition; by default `only_if` returns `True` for any argument.\n\n  Returns:\n    `True` if `variable` belongs to a backbone (embedding function) and\n    `only_if(variable)` is also satisfied.\n  """"""\n\n  # We restore all embedding variables.\n  is_embedding_var = any(\n      keyword in variable.name for keyword in EMBEDDING_KEYWORDS)\n\n  # We exclude \'relationnet*\' variables as they are not present in a pretrained\n  # checkpoint.\n  is_relationnet_var = variable.name.startswith(\'relationnet\')\n\n  # We exclude optimizer variables, as the episodic finetuning procedure is a\n  # different optimization problem than the original training objective.\n  is_optimizer_var = any(\n      keyword in variable.name for keyword in OPTIMIZER_KEYWORDS)\n\n  if (only_if(variable) and is_embedding_var and not is_relationnet_var and\n      not is_optimizer_var):\n    if \'adam\' in variable.name.lower():\n      logging.error(\n          \'Variable name unexpectedly indicates it is both related \'\n          \'to an embedding, and to the `AdamOptimizer`: %s\', variable.name)\n    else:\n      return True\n\n  return False\n\n\n# TODO(eringrant): Split the current `Trainer` class into `Trainer` and\n# `Evaluator` classes to partition the constructor arguments into meaningful\n# groups.\n# TODO(eringrant): Refactor the current `Trainer` class to more transparently\n# deal with operations per split, since the present logic surrounding the\n# `eval_finegrainedness_split` is confusing.\n# TODO(eringrant): Better organize `Trainer` Gin configurations, which are\n# currently set in many configuration files.\n@gin.configurable\nclass Trainer(object):\n  """"""A Trainer for training a Learner on data provided by ReaderDataSource.""""""\n\n  def __init__(\n      self,\n      num_updates,\n      batch_size,\n      num_eval_episodes,\n      checkpoint_every,\n      validate_every,\n      log_every,\n      train_learner_class,\n      eval_learner_class,\n      is_training,\n      checkpoint_to_restore,\n      learning_rate,\n      decay_learning_rate,\n      decay_every,\n      decay_rate,\n      experiment_name,\n      pretrained_source,\n      train_dataset_list,\n      eval_dataset_list,\n      restrict_classes,\n      restrict_num_per_class,\n      checkpoint_dir,\n      summary_dir,\n      records_root_dir,\n      eval_finegrainedness,\n      eval_finegrainedness_split,\n      eval_imbalance_dataset,\n      omit_from_saving_and_reloading,\n      eval_split,\n      train_episode_config,\n      eval_episode_config,\n      data_config,\n  ):\n    # pyformat: disable\n    """"""Initializes a Trainer.\n\n    Args:\n      num_updates: An integer, the number of training updates.\n      batch_size: An integer, the size of batches for non-episodic models.\n      num_eval_episodes: An integer, the number of episodes for evaluation.\n      checkpoint_every: An integer, the number of episodes between consecutive\n        checkpoints.\n      validate_every: An integer, the number of episodes between consecutive\n        validations.\n      log_every: An integer, the number of episodes between consecutive logging.\n      train_learner_class: A Learner to be used for meta-training.\n      eval_learner_class: A Learner to be used for meta-validation or\n        meta-testing.\n      is_training: Bool, whether or not to train or just evaluate.\n      checkpoint_to_restore: A string, the path to a checkpoint from which to\n        restore variables.\n      learning_rate: A float, the meta-learning learning rate.\n      decay_learning_rate: A boolean, whether to decay the learning rate.\n      decay_every: An integer, the learning rate is decayed for every multiple\n        of this value.\n      decay_rate: A float, the decay to apply to the learning rate.\n      experiment_name: A string, a name for the experiment.\n      pretrained_source: A string, the pretraining setup to use.\n      train_dataset_list: A list of names of datasets to train on. This can be\n        any subset of the supported datasets.\n      eval_dataset_list: A list of names of datasets to evaluate on either for\n        validation during train or for final test evaluation, depending on the\n        nature of the experiment, as dictated by `is_training\'.\n      restrict_classes: A dict that maps dataset names to a dict that specifies\n        for each of TRAIN_SPLIT, VALID_SPLIT and TEST_SPLIT the number of\n        classes to restrict to. This can lead to some classes of a particular\n        split of a particular dataset never participating in episode creation.\n      restrict_num_per_class: A dict that maps dataset names to a dict that\n        specifies for each of TRAIN_SPLIT, VALID_SPLIT and TEST_SPLIT the number\n        of examples per class to restrict to. For datasets / splits that are not\n        mentioned, no restriction is applied. If restrict_num_per_class is the\n        empty dict, no restriction is applied to any split of any dataset.\n      checkpoint_dir: A string, the path to the checkpoint directory, or None if\n        no checkpointing should occur.\n      summary_dir: A string, the path to the checkpoint directory, or None if no\n        summaries should be saved.\n      records_root_dir: A string, the path to the dataset records directory.\n      eval_finegrainedness: Whether to perform binary ImageNet evaluation for\n        assessing the performance on fine- vs coarse- grained tasks.\n      eval_finegrainedness_split: The subgraph of ImageNet to perform the\n        aforementioned analysis on. Notably, if this is TRAIN_SPLIT, we need to\n        ensure that an training data is used episodically, even if the given\n        model is the baseline model which usually uses batches for training.\n      eval_imbalance_dataset: A dataset on which to perform evaluation for\n        assessing how class imbalance affects performance in binary episodes. By\n        default it is empty and no imbalance analysis is performed.\n      omit_from_saving_and_reloading: A list of strings that specifies\n        substrings of variable names that should not be reloaded.\n      eval_split: One of the constants TRAIN_SPLIT, VALID_SPLIT, TEST_SPLIT\n        or None, according to the split whose results we want to\n        use for the analysis.\n      train_episode_config: An instance of EpisodeDescriptionConfig (in\n        data/config.py). This is a config for setting the ways and shots of\n        training episodes or the parameters for sampling them, if variable.\n      eval_episode_config: An instance of EpisodeDescriptionConfig. Analogous to\n        train_episode_config but used for eval episodes (validation or testing).\n      data_config: A DataConfig, the data configuration.\n\n    Raises:\n      UnexpectedSplitError: If split configuration is not as expected.\n    """"""\n    # pyformat: enable\n    self.num_updates = num_updates\n    self.batch_size = batch_size\n    self.num_eval_episodes = num_eval_episodes\n    self.checkpoint_every = checkpoint_every\n    self.validate_every = validate_every\n    self.log_every = log_every\n\n    self.checkpoint_to_restore = checkpoint_to_restore\n    self.learning_rate = learning_rate\n    self.decay_learning_rate = decay_learning_rate\n    self.decay_every = decay_every\n    self.decay_rate = decay_rate\n    self.experiment_name = experiment_name\n    self.pretrained_source = pretrained_source\n\n    self.train_learner_class = train_learner_class\n    self.eval_learner_class = eval_learner_class\n    self.is_training = is_training\n    self.train_dataset_list = train_dataset_list\n    self.eval_dataset_list = eval_dataset_list\n    self.restrict_classes = restrict_classes\n    self.restrict_num_per_class = restrict_num_per_class\n    self.checkpoint_dir = checkpoint_dir\n    self.summary_dir = summary_dir\n    self.records_root_dir = records_root_dir\n    self.eval_finegrainedness = eval_finegrainedness\n    self.eval_finegrainedness_split = eval_finegrainedness_split\n    self.eval_imbalance_dataset = eval_imbalance_dataset\n    self.omit_from_saving_and_reloading = omit_from_saving_and_reloading\n\n    if eval_finegrainedness:\n      # The fine- vs coarse- grained evaluation may potentially be performed on\n      # the training graph as it exhibits greater variety in this aspect.\n      self.eval_split = eval_finegrainedness_split\n    elif eval_split:\n      if eval_split not in (TRAIN_SPLIT, VALID_SPLIT, TEST_SPLIT):\n        raise UnexpectedSplitError(eval_split)\n      self.eval_split = eval_split\n    elif is_training:\n      self.eval_split = VALID_SPLIT\n    else:\n      self.eval_split = TEST_SPLIT\n\n    if eval_finegrainedness or eval_imbalance_dataset:\n      # We restrict this analysis to the binary classification setting.\n      logging.info(\n          \'Forcing the number of %s classes to be 2, since \'\n          \'the finegrainedness analysis is applied on binary \'\n          \'classification tasks only.\', eval_finegrainedness_split)\n      if eval_finegrainedness and eval_finegrainedness_split == TRAIN_SPLIT:\n        train_episode_config.num_ways = 2\n      else:\n        eval_episode_config.num_ways = 2\n\n    self.num_train_classes = train_episode_config.num_ways\n    self.num_test_classes = eval_episode_config.num_ways\n    self.num_support_train = train_episode_config.num_support\n    self.num_query_train = train_episode_config.num_query\n    self.num_support_eval = eval_episode_config.num_support\n    self.num_query_eval = eval_episode_config.num_query\n\n    self.train_episode_config = train_episode_config\n    self.eval_episode_config = eval_episode_config\n\n    self.data_config = data_config\n    # Get the image shape.\n    self.image_shape = [data_config.image_height] * 2 + [3]\n\n    # Create the benchmark specification.\n    self.benchmark_spec = self.get_benchmark_specification()\n\n    # Which splits to support depends on whether we are in the meta-training\n    # phase or not. If we are, we need the train split, and the valid one for\n    # early-stopping. If not, we only need the test split.\n    self.required_splits = [TRAIN_SPLIT] if self.is_training else []\n    self.required_splits += [self.eval_split]\n\n    # Get the training, validation and testing specifications.\n    # Each is either an EpisodeSpecification or a BatchSpecification.\n    self.split_episode_or_batch_specs = dict(\n        zip(self.required_splits,\n            map(self.get_batch_or_episodic_specification,\n                self.required_splits)))\n\n    # Get the next data (episode or batch) for the different splits.\n    self.next_data = dict(\n        zip(self.required_splits, map(self.build_data, self.required_splits)))\n\n    # Create the global step to pass to the learners.\n    global_step = tf.train.get_or_create_global_step()\n\n    # Initialize the learners.\n    self.learners = {}\n    for split in self.required_splits:\n      if split == TRAIN_SPLIT:\n        # The learner for the training split should only be in training mode if\n        # the evaluation split is not the training split.\n        learner_is_training = self.eval_split != TRAIN_SPLIT\n        learner_class = self.train_learner_class\n      else:\n        learner_is_training = False\n        learner_class = self.eval_learner_class\n      self.learners[split] = self.create_learner(\n          is_training=learner_is_training,\n          learner_class=learner_class,\n          split=get_split_enum(split))\n\n    # Build the prediction, loss and accuracy graphs for each learner.\n    predictions, losses, accuracies = zip(*[\n        self.build_learner(split, global_step) for split in self.required_splits\n    ])\n    self.predictions = dict(zip(self.required_splits, predictions))\n    self.losses = dict(zip(self.required_splits, losses))\n    self.accuracies = dict(zip(self.required_splits, accuracies))\n\n    # Set self.way, self.shots to Tensors for the way/shots of the next episode.\n    self.set_way_shots_classes_logits_targets()\n\n    # Get an optimizer and the operation for meta-training.\n    self.train_op = None\n    if self.is_training:\n      learning_rate = self.learning_rate\n      if self.decay_learning_rate:\n        learning_rate = tf.train.exponential_decay(\n            self.learning_rate,\n            global_step,\n            decay_steps=self.decay_every,\n            decay_rate=self.decay_rate,\n            staircase=True)\n      tf.summary.scalar(\'learning_rate\', learning_rate)\n      self.optimizer = tf.train.AdamOptimizer(learning_rate)\n      self.train_op = self.get_train_op(global_step)\n\n    if self.checkpoint_dir is not None:\n      if not tf.io.gfile.exists(self.checkpoint_dir):\n        tf.io.gfile.makedirs(self.checkpoint_dir)\n\n    # Initialize a Session.\n    self.initialize_session()\n    self.initialize_saver()\n    self.create_summary_writer()\n\n  def build_learner(self, split, global_step):\n    """"""Compute predictions, losses and accuracies of the learner for split.""""""\n\n    # TODO(eringrant): Pass `global_step` and `summaries_collection` to\n    # `Learner.forward_pass` when the new interface is in use.\n    summaries_collection = \'{}/learner_summaries\'.format(split)\n    del global_step\n    del summaries_collection\n\n    with tf.name_scope(split):\n      data = self.next_data[split]\n      predictions = self.learners[split].forward_pass(data=data)\n      loss = self.learners[split].compute_loss(\n          predictions=predictions, onehot_labels=data.onehot_labels)\n      accuracy = self.learners[split].compute_accuracy(\n          predictions=predictions, labels=data.labels)\n      return predictions, loss, accuracy\n\n  def set_way_shots_classes_logits_targets(self):\n    """"""Sets the Tensors for the info about the learner\'s next episode.""""""\n    # The batch trainer receives episodes only for the valid and test splits.\n    # Therefore for the train split there is no defined way and shots.\n    (way, shots, class_props, class_ids, test_logits,\n     test_targets) = [], [], [], [], [], []\n    for split in self.required_splits:\n\n      if isinstance(self.next_data[split], providers.Batch):\n        (way_, shots_, class_props_, class_ids_, test_logits_,\n         test_targets_) = [None] * 6\n      else:\n        data = self.next_data[split]\n        way_ = data.way\n        shots_ = data.train_shots\n        class_ids_ = data.unique_class_ids\n        class_props_ = None\n        if self.eval_imbalance_dataset:\n          class_props_ = compute_class_proportions(\n              class_ids_, shots_, self.eval_imbalance_dataset_spec)\n        test_logits_ = self.predictions[split]\n        test_targets_ = self.next_data[split].test_labels\n      way.append(way_)\n      shots.append(shots_)\n      class_props.append(class_props_)\n      class_ids.append(class_ids_)\n      test_logits.append(test_logits_)\n      test_targets.append(test_targets_)\n\n    self.way = dict(zip(self.required_splits, way))\n    self.shots = dict(zip(self.required_splits, shots))\n    self.class_props = dict(zip(self.required_splits, class_props))\n    self.class_ids = dict(zip(self.required_splits, class_ids))\n    self.test_logits = dict(zip(self.required_splits, test_logits))\n    self.test_targets = dict(zip(self.required_splits, test_targets))\n\n  def create_summary_writer(self):\n    """"""Create summaries and writer.""""""\n    # Add summaries for the losses / accuracies of the different learners.\n    standard_summaries = []\n    for split in self.required_splits:\n      loss_summary = tf.summary.scalar(\'%s_loss\' % split, self.losses[split])\n      acc_summary = tf.summary.scalar(\'%s_acc\' % split, self.accuracies[split])\n      standard_summaries.append(loss_summary)\n      standard_summaries.append(acc_summary)\n\n    # Add summaries for the way / shot / logits / targets of the learners.\n    evaluation_summaries = self.add_eval_summaries()\n\n    # All summaries.\n    self.standard_summaries = tf.summary.merge(standard_summaries)\n    self.evaluation_summaries = tf.summary.merge(evaluation_summaries)\n\n    # Get a writer.\n    self.summary_writer = None\n    if self.summary_dir is not None:\n      self.summary_writer = tf.summary.FileWriter(self.summary_dir)\n      if not tf.io.gfile.exists(self.summary_dir):\n        tf.io.gfile.makedirs(self.summary_dir)\n\n  def create_learner(self, is_training, learner_class, split):\n    """"""Instantiates a `Learner`.""""""\n    if issubclass(learner_class, learner_lib.BatchLearner):\n      logit_dim = self._get_num_total_classes(split)\n    elif issubclass(learner_class, learner_lib.EpisodicLearner):\n      logit_dim = (\n          self.train_episode_config.max_ways\n          if is_training else self.eval_episode_config.max_ways)\n    else:\n      raise ValueError(\n          \'The specified `learner_class` should be a subclass of \'\n          \'`learner_lib.BatchLearner` or `learner_lib.EpisodicLearner`, \'\n          \'but received {}.\'.format(learner_class))\n    return learner_class(\n        is_training=is_training,\n        logit_dim=logit_dim,\n    )\n\n  def get_benchmark_specification(self, records_root_dir=None):\n    """"""Returns a BenchmarkSpecification.\n\n    Args:\n      records_root_dir: Optional. If provided, a list or string that sets the\n        directory in which a child directory will be searched for each dataset\n        to locate that dataset\'s records and dataset specification. If it\'s a\n        string, that path will be used for all datasets. If it\'s a list, its\n        length must be the same as the number of datasets, in order to specify a\n        different such directory for each. If None, self.records_root_dir will\n        be used for all datasets.\n\n    Raises:\n      RuntimeError: Incorrect file_pattern detected in a dataset specification.\n    """"""\n    (data_spec_list, has_dag_ontology, has_bilevel_ontology,\n     splits_to_contribute) = [], [], [], []\n    seen_datasets = set()\n\n    eval_dataset_list = self.eval_dataset_list\n    if self.is_training:\n      benchmark_datasets = self.train_dataset_list + eval_dataset_list\n    else:\n      benchmark_datasets = eval_dataset_list\n\n    if isinstance(records_root_dir, list):\n      if len(records_root_dir) != len(benchmark_datasets):\n        raise ValueError(\'The given records_root_dir is a list whose length is \'\n                         \'not the same as the number of benchmark datasets. \'\n                         \'Found datasets {} (for the {} phase) but \'\n                         \'len(records_root_dir) is {}. Expected their lengths \'\n                         \'to match or records_path to be a string\').format(\n                             benchmark_datasets, len(records_root_dir))\n      records_roots_for_datasets = records_root_dir\n    elif isinstance(records_root_dir, six.text_type):\n      records_roots_for_datasets = [records_root_dir] * len(benchmark_datasets)\n    elif records_root_dir is None:\n      records_roots_for_datasets = [self.records_root_dir\n                                   ] * len(benchmark_datasets)\n\n    for dataset_name, dataset_records_root in zip(benchmark_datasets,\n                                                  records_roots_for_datasets):\n\n      # Might be seeing a dataset for a second time if it belongs to both the\n      # train and eval dataset lists.\n      if dataset_name in seen_datasets:\n        continue\n\n      dataset_records_path = os.path.join(dataset_records_root, dataset_name)\n      data_spec = dataset_spec_lib.load_dataset_spec(dataset_records_path)\n      # Only ImageNet has a DAG ontology.\n      has_dag = (dataset_name.startswith(\'ilsvrc_2012\'))\n      # Only Omniglot has a bi-level ontology.\n      is_bilevel = (dataset_name == \'omniglot\')\n\n      # The meta-splits that this dataset will contribute data to.\n      if not self.is_training:\n        # If we\'re meta-testing, all datasets contribute only to meta-test.\n        splits = {self.eval_split}\n      else:\n        splits = set()\n        if dataset_name in self.train_dataset_list:\n          splits.add(TRAIN_SPLIT)\n        if dataset_name in self.eval_dataset_list:\n          splits.add(VALID_SPLIT)\n\n      # By default, all classes of each split will eventually be used for\n      # episode creation. But it might be that for some datasets, it is\n      # requested to restrict the available number of classes of some splits.\n      restricted_classes_per_split = {}\n      if dataset_name in self.restrict_classes:\n        classes_per_split = self.restrict_classes[dataset_name]\n        for split, num_classes in classes_per_split.items():\n          # The option to restrict classes is not supported in conjuction with\n          # non-uniform (bilevel or hierarhical) class sampling.\n          episode_descr_config = (\n              self.train_episode_config\n              if split == TRAIN_SPLIT else self.eval_episode_config)\n          if has_dag and not episode_descr_config.ignore_dag_ontology:\n            raise ValueError(\'Restrictions on the class set of a dataset with \'\n                             \'a DAG ontology are not supported when \'\n                             \'ignore_dag_ontology is False.\')\n          if is_bilevel and not episode_descr_config.ignore_bilevel_ontology:\n            raise ValueError(\'Restrictions on the class set of a dataset with \'\n                             \'a bilevel ontology are not supported when \'\n                             \'ignore_bilevel_ontology is False.\')\n\n          restricted_classes_per_split[get_split_enum(split)] = num_classes\n        # Initialize the DatasetSpecificaton to account for this restriction.\n        data_spec.initialize(restricted_classes_per_split)\n\n        # Log the applied restrictions.\n        logging.info(\'Restrictions for dataset %s:\', dataset_name)\n        for split in list(splits):\n          num_classes = data_spec.get_classes(get_split_enum(split))\n          logging.info(\'\\t split %s is restricted to %d classes\', split,\n                       num_classes)\n\n      # Add this dataset to the benchmark.\n      logging.info(\'Adding dataset %s\', data_spec.name)\n      data_spec_list.append(data_spec)\n      has_dag_ontology.append(has_dag)\n      has_bilevel_ontology.append(is_bilevel)\n      splits_to_contribute.append(splits)\n\n      # Book-keeping.\n      seen_datasets.add(dataset_name)\n\n    if self.eval_imbalance_dataset:\n      self.eval_imbalance_dataset_spec = data_spec\n      assert len(data_spec_list) == 1, (\'Imbalance analysis is only \'\n                                        \'supported on one dataset at a time.\')\n\n    benchmark_spec = dataset_spec_lib.BenchmarkSpecification(\n        \'benchmark\', self.image_shape, data_spec_list, has_dag_ontology,\n        has_bilevel_ontology, splits_to_contribute)\n\n    # Logging of which datasets will be used for the different meta-splits.\n    splits_to_datasets = collections.defaultdict(list)\n    for dataset_spec, splits_to_contribute in zip(data_spec_list,\n                                                  splits_to_contribute):\n      for split in splits_to_contribute:\n        splits_to_datasets[split].append(dataset_spec.name)\n    for split, datasets in splits_to_datasets.items():\n      logging.info(\'Episodes for split %s will be created from %s\', split,\n                   datasets)\n\n    return benchmark_spec\n\n  def initialize_session(self):\n    """"""Initializes a tf.Session.""""""\n    if ENABLE_TF_OPTIMIZATIONS:\n      self.sess = tf.Session()\n    else:\n      session_config = tf.ConfigProto()\n      rewrite_options = session_config.graph_options.rewrite_options\n      rewrite_options.disable_model_pruning = True\n      rewrite_options.constant_folding = rewrite_options.OFF\n      rewrite_options.arithmetic_optimization = rewrite_options.OFF\n      rewrite_options.remapping = rewrite_options.OFF\n      rewrite_options.shape_optimization = rewrite_options.OFF\n      rewrite_options.dependency_optimization = rewrite_options.OFF\n      rewrite_options.function_optimization = rewrite_options.OFF\n      rewrite_options.layout_optimizer = rewrite_options.OFF\n      rewrite_options.loop_optimization = rewrite_options.OFF\n      rewrite_options.memory_optimization = rewrite_options.NO_MEM_OPT\n      self.sess = tf.Session(config=session_config)\n\n    # Restore or initialize the variables.\n    self.sess.run(tf.global_variables_initializer())\n    self.sess.run(tf.local_variables_initializer())\n\n  def initialize_saver(self):\n    """"""Initializes a tf.train.Saver and possibly restores parameters.""""""\n\n    # We omit from saving and restoring any variables that contains as a\n    # substring anything in the list `self.omit_from_saving_and_reloading.\n    # For example, those that track iterator state.\n    logging.info(\n        \'Omitting from saving / restoring any variable that \'\n        \'contains any of the following substrings: %s\',\n        self.omit_from_saving_and_reloading)\n\n    def is_not_requested_to_omit(var):\n      return all([\n          substring not in var.name\n          for substring in self.omit_from_saving_and_reloading\n      ])\n\n    var_list = list(filter(is_not_requested_to_omit, tf.global_variables()))\n    if var_list:\n      self.saver = tf.train.Saver(var_list=var_list, max_to_keep=500)\n    else:\n      self.saver = None\n      logging.info(\'Variables not being saved since no variables left after \'\n                   \'filtering.\')\n\n    if self.checkpoint_to_restore:\n      if not self.saver:\n        raise ValueError(\n            \'Checkpoint not restored, since there is no Saver created. This is \'\n            \'likely due to no parameters being available. If you intend to run \'\n            \'parameterless training, set `checkpoint_to_restore` to None.\')\n\n    if self.is_training:\n\n      # To handle pre-emption, we continue from the latest checkpoint if\n      # checkpoints already exist in the checkpoint directory.\n      latest_checkpoint = None\n      if self.checkpoint_dir is not None:\n        latest_checkpoint = tf.train.latest_checkpoint(self.checkpoint_dir)\n\n      if latest_checkpoint is not None:\n        if not self.saver:\n          raise ValueError(\n              \'Checkpoint not restored, since there is no Saver created. This \'\n              \'is likely due to no parameters being available. \')\n        self.saver.restore(self.sess, latest_checkpoint)\n        logging.info(\'Restored latest checkpoint from training: %s\',\n                     latest_checkpoint)\n        logging.info(\'(Provided `checkpoint_to_restore` overriden: %s)\',\n                     self.checkpoint_to_restore)\n\n      elif self.checkpoint_to_restore:\n        logging.info(\'No training checkpoints found.\')\n        # For training episodic models from a checkpoint, we restore the\n        # backbone weights but omit other (e.g., optimizer) parameters.\n        backbone_vars_to_reload = [\n            var for var in tf.global_variables()\n            if is_backbone_variable(var, only_if=is_not_requested_to_omit)\n        ]\n        backbone_saver = tf.train.Saver(\n            var_list=backbone_vars_to_reload, max_to_keep=1)\n        backbone_saver.restore(self.sess, self.checkpoint_to_restore)\n        logging.info(\n            \'Restored only vars %s from provided `checkpoint_to_restore`: %s\',\n            [var.name for var in backbone_vars_to_reload],\n            self.checkpoint_to_restore)\n\n      else:\n        logging.info(\n            \'No checkpoints found; training from random initialization.\')\n\n    elif self.checkpoint_to_restore:\n      # For evaluation, we restore more than the backbone (embedding function)\n      # variables from the provided checkpoint.\n      self.saver.restore(self.sess, self.checkpoint_to_restore)\n      logging.info(\'Restored checkpoint for evaluation: %s\',\n                   self.checkpoint_to_restore)\n\n    else:\n      logging.info(\n          \'No checkpoints found; evaluating with a random initialization.\')\n\n  def get_batch_or_episodic_specification(self, split):\n    if split == TRAIN_SPLIT:\n      return self._create_train_specification()\n    else:\n      return self._create_held_out_specification(split)\n\n  def _create_train_specification(self):\n    """"""Returns an EpisodeSpecification or BatchSpecification for training.""""""\n    if (issubclass(self.train_learner_class, learner_lib.EpisodicLearner) or\n        self.eval_split == TRAIN_SPLIT):\n      return learning_spec.EpisodeSpecification(learning_spec.Split.TRAIN,\n                                                self.num_train_classes,\n                                                self.num_support_train,\n                                                self.num_query_train)\n    elif issubclass(self.train_learner_class, learner_lib.BatchLearner):\n      return learning_spec.BatchSpecification(learning_spec.Split.TRAIN,\n                                              self.batch_size)\n    else:\n      raise ValueError(\n          \'The specified `learner_class` should be a subclass of \'\n          \'`learner_lib.BatchLearner` or `learner_lib.EpisodicLearner`, \'\n          \'but received {}.\'.format(self.train_learner_class))\n\n  def _create_held_out_specification(self, split=TEST_SPLIT):\n    """"""Create an EpisodeSpecification for either validation or testing.\n\n    Note that testing is done episodically whether or not training was episodic.\n    This is why the different subclasses should not override this method.\n\n    Args:\n      split: one of VALID_SPLIT or TEST_SPLIT\n\n    Returns:\n      an EpisodeSpecification.\n\n    Raises:\n      ValueError: Invalid split.\n    """"""\n    split_enum = get_split_enum(split)\n    return learning_spec.EpisodeSpecification(split_enum, self.num_test_classes,\n                                              self.num_support_eval,\n                                              self.num_query_eval)\n\n  def _restrict_dataset_list_for_split(self, split, splits_to_contribute,\n                                       dataset_list):\n    """"""Returns the restricted dataset_list for the given split.\n\n    Args:\n      split: A string, either TRAIN_SPLIT, VALID_SPLIT or TEST_SPLIT.\n      splits_to_contribute: A list whose length is the number of datasets in the\n        benchmark. Each element is a set of strings corresponding to the splits\n        that the respective dataset will contribute to.\n      dataset_list: A list which has one element per selected dataset (same\n        length as splits_to_contribute), e.g. this can be one of the lists\n        dataset_spec_list, has_dag_ontology, has_bilevel_ontology of the\n        BenchmarkSpecification.\n    """"""\n    updated_list = []\n    for dataset_num, dataset_splits in enumerate(splits_to_contribute):\n      if split in dataset_splits:\n        updated_list.append(dataset_list[dataset_num])\n    return updated_list\n\n  def get_num_to_take(self, dataset_name, split):\n    """"""Return the number of examples to restrict to for a dataset/split pair.""""""\n    num_to_take = -1  # By default, no restriction.\n    if dataset_name in self.restrict_num_per_class:\n      dataset_restrict_num_per_class = self.restrict_num_per_class[dataset_name]\n      if split in dataset_restrict_num_per_class:\n        num_to_take = dataset_restrict_num_per_class[split]\n    return num_to_take\n\n  def build_data(self, split):\n    """"""Builds the data for the learner for `split`.""""""\n    learner_class = (\n        self.train_learner_class\n        if split == TRAIN_SPLIT else self.eval_learner_class)\n    if (issubclass(learner_class, learner_lib.BatchLearner) and\n        split != self.eval_split):\n      return self._build_batch(split)\n    elif (issubclass(learner_class, learner_lib.EpisodicLearner) or\n          split == self.eval_split):\n      return self._build_episode(split)\n    else:\n      raise ValueError(\n          \'The `Learner` for `split` should be a subclass of \'\n          \'`learner_lib.BatchLearner` or `learner_lib.EpisodicLearner`, \'\n          \'but received {}.\'.format(learner_class))\n\n\n  def _build_episode(self, split):\n    """"""Builds an EpisodeDataset containing the next data for ""split"".\n\n    Args:\n      split: A string, either TRAIN_SPLIT, VALID_SPLIT, or TEST_SPLIT.\n\n    Returns:\n      An EpisodeDataset.\n\n    Raises:\n      UnexpectedSplitError: If split not as expected for this episode build.\n    """"""\n    shuffle_buffer_size = self.data_config.shuffle_buffer_size\n    read_buffer_size_bytes = self.data_config.read_buffer_size_bytes\n    num_prefetch = self.data_config.num_prefetch\n    (_, image_shape, dataset_spec_list, has_dag_ontology, has_bilevel_ontology,\n     splits_to_contribute) = self.benchmark_spec\n\n    # Choose only the datasets that are chosen to contribute to the given split.\n    dataset_spec_list = self._restrict_dataset_list_for_split(\n        split, splits_to_contribute, dataset_spec_list)\n    has_dag_ontology = self._restrict_dataset_list_for_split(\n        split, splits_to_contribute, has_dag_ontology)\n    has_bilevel_ontology = self._restrict_dataset_list_for_split(\n        split, splits_to_contribute, has_bilevel_ontology)\n\n    episode_spec = self.split_episode_or_batch_specs[split]\n    dataset_split = episode_spec[0]\n    # TODO(lamblinp): Support non-square shapes if necessary. For now, all\n    # images are resized to square, even if it changes the aspect ratio.\n    image_size = image_shape[0]\n    if image_shape[1] != image_size:\n      raise ValueError(\n          \'Expected a square image shape, not {}\'.format(image_shape))\n\n    if split == TRAIN_SPLIT:\n      episode_descr_config = self.train_episode_config\n    elif split in (VALID_SPLIT, TEST_SPLIT):\n      episode_descr_config = self.eval_episode_config\n    else:\n      raise UnexpectedSplitError(split)\n\n    # Decide how many examples per class to restrict to for each dataset for the\n    # given split (by default there is no restriction).\n    num_per_class = []  # A list whose length is the number of datasets.\n    for dataset_spec in dataset_spec_list:\n      num_per_class.append(self.get_num_to_take(dataset_spec.name, split))\n\n    # TODO(lamblinp): pass specs directly to the pipeline builder.\n    # TODO(lamblinp): move the special case directly in make_..._pipeline\n    if len(dataset_spec_list) == 1:\n\n      use_dag_ontology = has_dag_ontology[0]\n      if self.eval_finegrainedness or self.eval_imbalance_dataset:\n        use_dag_ontology = False\n      data_pipeline = pipeline.make_one_source_episode_pipeline(\n          dataset_spec_list[0],\n          use_dag_ontology=use_dag_ontology,\n          use_bilevel_ontology=has_bilevel_ontology[0],\n          split=dataset_split,\n          episode_descr_config=episode_descr_config,\n          shuffle_buffer_size=shuffle_buffer_size,\n          read_buffer_size_bytes=read_buffer_size_bytes,\n          num_prefetch=num_prefetch,\n          image_size=image_size,\n          num_to_take=num_per_class[0])\n    else:\n      data_pipeline = pipeline.make_multisource_episode_pipeline(\n          dataset_spec_list,\n          use_dag_ontology_list=has_dag_ontology,\n          use_bilevel_ontology_list=has_bilevel_ontology,\n          split=dataset_split,\n          episode_descr_config=episode_descr_config,\n          shuffle_buffer_size=shuffle_buffer_size,\n          read_buffer_size_bytes=read_buffer_size_bytes,\n          num_prefetch=num_prefetch,\n          image_size=image_size,\n          num_to_take=num_per_class)\n    data_pipeline = apply_dataset_options(data_pipeline)\n\n    iterator = data_pipeline.make_one_shot_iterator()\n    episode, _ = iterator.get_next()\n    (support_images, support_labels, support_class_ids, query_images,\n     query_labels, query_class_ids) = episode\n\n    return providers.EpisodeDataset(\n        train_images=support_images,\n        test_images=query_images,\n        train_labels=support_labels,\n        test_labels=query_labels,\n        train_class_ids=support_class_ids,\n        test_class_ids=query_class_ids)\n\n  def _build_batch(self, split):\n    """"""Builds a Batch object containing the next data for ""split"".\n\n    Args:\n      split: A string, either TRAIN_SPLIT, VALID_SPLIT, or TEST_SPLIT.\n\n    Returns:\n      An EpisodeDataset.\n    """"""\n    shuffle_buffer_size = self.data_config.shuffle_buffer_size\n    read_buffer_size_bytes = self.data_config.read_buffer_size_bytes\n    num_prefetch = self.data_config.num_prefetch\n    (_, image_shape, dataset_spec_list, _, _,\n     splits_to_contribute) = self.benchmark_spec\n\n    # Choose only the datasets that are chosen to contribute to the given split.\n    dataset_spec_list = self._restrict_dataset_list_for_split(\n        split, splits_to_contribute, dataset_spec_list)\n\n    # Decide how many examples per class to restrict to for each dataset for the\n    # given split (by default there is no restriction).\n    num_per_class = []  # A list whose length is the number of datasets.\n    for dataset_spec in dataset_spec_list:\n      num_per_class.append(self.get_num_to_take(dataset_spec.name, split))\n\n    dataset_split, batch_size = self.split_episode_or_batch_specs[split]\n    for dataset_spec in dataset_spec_list:\n      if dataset_spec.name in DATASETS_WITH_EXAMPLE_SPLITS:\n        raise ValueError(\n            \'Batch pipeline is used only at meta-train time, and does not \'\n            \'handle datasets with example splits, which should only be used \'\n            \'at meta-test (evaluation) time.\')\n    # TODO(lamblinp): pass specs directly to the pipeline builder.\n    # TODO(lamblinp): move the special case directly in make_..._pipeline\n    if len(dataset_spec_list) == 1:\n      data_pipeline = pipeline.make_one_source_batch_pipeline(\n          dataset_spec_list[0],\n          split=dataset_split,\n          batch_size=batch_size,\n          shuffle_buffer_size=shuffle_buffer_size,\n          read_buffer_size_bytes=read_buffer_size_bytes,\n          num_prefetch=num_prefetch,\n          image_size=image_shape[0],\n          num_to_take=num_per_class[0])\n    else:\n      data_pipeline = pipeline.make_multisource_batch_pipeline(\n          dataset_spec_list,\n          split=dataset_split,\n          batch_size=batch_size,\n          shuffle_buffer_size=shuffle_buffer_size,\n          read_buffer_size_bytes=read_buffer_size_bytes,\n          num_prefetch=num_prefetch,\n          image_size=image_shape[0],\n          num_to_take=num_per_class)\n\n    data_pipeline = apply_dataset_options(data_pipeline)\n    iterator = data_pipeline.make_one_shot_iterator()\n    (images, class_ids), dataset_index = iterator.get_next()\n\n    # The number of available classes for each dataset\n    all_n_classes = [\n        len(dataset_spec.get_classes(get_split_enum(split)))\n        for dataset_spec in dataset_spec_list\n    ]\n    if len(dataset_spec_list) == 1:\n      n_classes = all_n_classes[0]\n    elif gin.query_parameter(\'BatchSplitReaderGetReader.add_dataset_offset\'):\n      # The total number of classes is the sum for all datasets\n      n_classes = sum(all_n_classes)\n    else:\n      # The number of classes is the one of the current dataset\n      n_classes = tf.convert_to_tensor(all_n_classes)[dataset_index]\n    return providers.Batch(images=images, labels=class_ids, n_classes=n_classes)\n\n  def get_train_op(self, global_step):\n    """"""Returns the operation that performs a training update.""""""\n    # UPDATE_OPS picks up batch_norm updates.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = self.optimizer.minimize(\n          self.losses[TRAIN_SPLIT], global_step=global_step)\n    return train_op\n\n  def get_updated_global_step(self):\n    with tf.control_dependencies([self.train_op]):\n      global_step = tf.identity(tf.train.get_global_step())\n    return global_step\n\n  def train(self):\n    """"""The training loop.""""""\n    global_step = self.sess.run(tf.train.get_global_step())\n    logging.info(\'Starting training from global_step: %d\', global_step)\n    updated_global_step = self.get_updated_global_step()\n\n    # Dummy variables so that logging works even if called before evaluation.\n    self.valid_acc = np.nan\n    self.valid_ci = np.nan\n\n    should_save = self.checkpoint_dir is not None\n    if should_save and global_step == 0:\n      # Save the initialization weights.\n      save_path = self.saver.save(\n          self.sess, os.path.join(self.checkpoint_dir, \'model_0.ckpt\'))\n      logging.info(\'Model initialization saved: %s\', save_path)\n\n    # Compute the initial validation performance before starting the training.\n    self.maybe_evaluate(global_step)\n\n    while global_step < self.num_updates:\n      # Perform the next update.\n      (_, train_loss, train_acc, global_step) = self.sess.run([\n          self.train_op, self.losses[TRAIN_SPLIT], self.accuracies[TRAIN_SPLIT],\n          updated_global_step\n      ])\n\n      # Maybe validate, depending on the global step\'s value.\n      self.maybe_evaluate(global_step)\n\n      # Log training progress.\n      if not global_step % self.log_every:\n        message = (\n            \'Update %d. Train loss: %f, Train accuracy: %f, \'\n            \'Valid accuracy %f +/- %f.\\n\' %\n            (global_step, train_loss, train_acc, self.valid_acc, self.valid_ci))\n        logging.info(message)\n\n        # Update summaries.\n        summaries = self.sess.run(self.standard_summaries)\n        if self.summary_writer:\n          self.summary_writer.add_summary(summaries, global_step)\n\n      if should_save and global_step % self.checkpoint_every == 0:\n        save_path = self.saver.save(\n            self.sess,\n            os.path.join(self.checkpoint_dir, \'model_%d.ckpt\' % global_step))\n        logging.info(\'Model checkpoint saved: %s\', save_path)\n\n  def maybe_evaluate(self, global_step):\n    """"""Maybe perform evaluation, depending on the value of global_step.""""""\n    if not global_step % self.validate_every:\n      # Get the validation accuracy and confidence interval.\n      (valid_acc, valid_ci, valid_acc_summary,\n       valid_ci_summary) = self.evaluate(\n           VALID_SPLIT, step=global_step)\n      # Validation summaries are updated every time validation happens which is\n      # every validate_every steps instead of log_every steps.\n      if self.summary_writer:\n        self.summary_writer.add_summary(valid_acc_summary, global_step)\n        self.summary_writer.add_summary(valid_ci_summary, global_step)\n      self.valid_acc = valid_acc\n      self.valid_ci = valid_ci\n\n\n# TODO(evcu) Improve this so that if the eval_only loads a global_step, it is\n# used at logging instead of value 0.\n\n  def evaluate(self, split, step=0):\n    """"""Returns performance metrics across num_eval_trials episodes / batches.""""""\n    num_eval_trials = self.num_eval_episodes\n    logging.info(\'Performing evaluation of the %s split using %d episodes...\',\n                 split, num_eval_trials)\n    accuracies = []\n    for eval_trial_num in range(num_eval_trials):\n      acc, summaries = self.sess.run(\n          [self.accuracies[split], self.evaluation_summaries])\n      accuracies.append(acc)\n      # Write complete summaries during evaluation, but not training.\n      # Otherwise, validaation summaries become too big.\n      if not self.is_training and self.summary_writer:\n        self.summary_writer.add_summary(summaries, eval_trial_num)\n    logging.info(\'Done.\')\n\n    mean_acc = np.mean(accuracies)\n    ci_acc = np.std(accuracies) * 1.96 / np.sqrt(len(accuracies))  # confidence\n    if not self.is_training:\n      # Logging during training is handled by self.train() instead.\n      logging.info(\'Accuracy on the meta-%s split: %f, +/- %f.\\n\', split,\n                   mean_acc, ci_acc)\n\n    mean_acc_summary = tf.Summary()\n    mean_acc_summary.value.add(tag=\'mean %s acc\' % split, simple_value=mean_acc)\n    ci_acc_summary = tf.Summary()\n    ci_acc_summary.value.add(tag=\'%s acc CI\' % split, simple_value=ci_acc)\n\n    return mean_acc, ci_acc, mean_acc_summary, ci_acc_summary\n\n  def add_eval_summaries(self):\n    """"""Returns summaries of way / shot / classes/ logits / targets.""""""\n    evaluation_summaries = []\n    for split in self.required_splits:\n      if isinstance(self.next_data[split], providers.EpisodeDataset):\n        evaluation_summaries.extend(self._add_eval_summaries_split(split))\n    return evaluation_summaries\n\n  def _add_eval_summaries_split(self, split):\n    """"""Returns split\'s summaries of way / shot / classes / logits / targets.""""""\n    split_eval_summaries = []\n    way_summary = tf.summary.scalar(\'%s_way\' % split, self.way[split])\n    shots_summary = tf.summary.tensor_summary(\'%s_shots\' % split,\n                                              self.shots[split])\n    classes_summary = tf.summary.tensor_summary(\'%s_class_ids\' % split,\n                                                self.class_ids[split])\n    logits_summary = tf.summary.tensor_summary(\'%s_test_logits\' % split,\n                                               self.test_logits[split])\n    targets_summary = tf.summary.tensor_summary(\'%s_test_targets\' % split,\n                                                self.test_targets[split])\n    if self.eval_imbalance_dataset:\n      class_props_summary = tf.summary.tensor_summary(\'%s_class_props\' % split,\n                                                      self.class_props[split])\n      split_eval_summaries.append(class_props_summary)\n    split_eval_summaries.append(way_summary)\n    split_eval_summaries.append(shots_summary)\n    split_eval_summaries.append(classes_summary)\n    split_eval_summaries.append(logits_summary)\n    split_eval_summaries.append(targets_summary)\n    return split_eval_summaries\n\n  def _get_num_total_classes(self, split):\n    """"""Returns the total number of classes in a split of the benchmark.""""""\n    total_classes = 0\n    for dataset_spec in self.benchmark_spec.dataset_spec_list:\n      total_classes += len(dataset_spec.get_classes(split))\n    return total_classes\n'"
meta_dataset/trainer_test.py,3,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Tests for trainer.py.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\n\nimport gin.tf\nfrom meta_dataset import learner as learner_lib\nfrom meta_dataset import trainer\nfrom meta_dataset.data import config\nfrom meta_dataset.data import decoder\nfrom meta_dataset.data import providers\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\ntf.flags.DEFINE_string(\'records_root_dir\', \'\',\n                       \'Root directory containing a subdirectory per dataset.\')\nFLAGS = flags.FLAGS\n\n\n\nclass TrainerTest(tf.test.TestCase):\n  """"""Test for the Trainer class.\n\n  In order to run this test, the records root directory needs to be set via the\n  `--records_root_dir` flag.\n  """"""\n\n  def test_trainer(self):\n    # PrototypicalNetworkLearner is built automatically and this test does not\n    # have the opportunity to pass values to its constructor except through gin.\n    gin.bind_parameter(\'PrototypicalNetworkLearner.weight_decay\', 1e-4)\n    gin.bind_parameter(\'PrototypicalNetworkLearner.backprop_through_moments\',\n                       True)\n    gin.bind_parameter(\'PrototypicalNetworkLearner.transductive_batch_norm\',\n                       False)\n    gin.bind_parameter(\'PrototypicalNetworkLearner.embedding_fn\',\n                       \'four_layer_convnet\')\n\n    # Values that can\'t be passed directly to EpisodeDescriptionConfig\n    gin.bind_parameter(\'process_episode.support_decoder\',\n                       decoder.ImageDecoder())\n    gin.bind_parameter(\'process_episode.query_decoder\', decoder.ImageDecoder())\n\n    episode_config = config.EpisodeDescriptionConfig(\n        num_ways=None,\n        num_support=None,\n        num_query=None,\n        min_ways=5,\n        max_ways_upper_bound=50,\n        max_num_query=10,\n        max_support_set_size=500,\n        max_support_size_contrib_per_class=100,\n        min_log_weight=np.log(0.5),\n        max_log_weight=np.log(2),\n        ignore_dag_ontology=False,\n        ignore_bilevel_ontology=False)\n\n    # Inspired from `learn/gin/default/debug_proto_mini_imagenet.gin`, but\n    # building the objects explicitly.\n    data_config = config.DataConfig(\n        image_height=84,\n        shuffle_buffer_size=20,\n        read_buffer_size_bytes=(1024**2),\n        num_prefetch=2,\n    )\n\n    trainer_instance = trainer.Trainer(\n        train_learner_class=learner_lib.PrototypicalNetworkLearner,\n        eval_learner_class=learner_lib.PrototypicalNetworkLearner,\n        is_training=True,\n        train_dataset_list=[\'mini_imagenet\'],\n        eval_dataset_list=[\'mini_imagenet\'],\n        restrict_classes={},\n        restrict_num_per_class={},\n        checkpoint_dir=\'\',\n        summary_dir=\'\',\n        records_root_dir=FLAGS.records_root_dir,\n        eval_split=trainer.VALID_SPLIT,\n        eval_finegrainedness=False,\n        eval_finegrainedness_split=\'\',\n        eval_imbalance_dataset=\'\',\n        omit_from_saving_and_reloading=\'\',\n        train_episode_config=episode_config,\n        eval_episode_config=episode_config,\n        data_config=data_config,\n        num_updates=100,\n        batch_size=8,  # unused\n        num_eval_episodes=10,\n        checkpoint_every=10,\n        validate_every=5,\n        log_every=1,\n        checkpoint_to_restore=None,\n        learning_rate=1e-4,\n        decay_learning_rate=True,\n        decay_every=5000,\n        decay_rate=0.5,\n        experiment_name=\'test\',\n        pretrained_source=\'\',\n    )\n\n    # Get the next train / valid / test episodes.\n    train_episode = trainer_instance.next_data[trainer.TRAIN_SPLIT]\n    self.assertIsInstance(train_episode, providers.EpisodeDataset)\n\n    # This isn\'t really a test. It just checks that things don\'t crash...\n    print(\n        trainer_instance.sess.run([\n            trainer_instance.train_op,\n            trainer_instance.losses[trainer.TRAIN_SPLIT],\n            trainer_instance.accuracies[trainer.TRAIN_SPLIT]\n        ]))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
meta_dataset/analysis/__init__.py,0,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n'"
meta_dataset/analysis/select_best_model.py,24,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""A script for choosing the best variant of a model automatically.\n\nIt takes as input the root directory of all experiments, and a list of names of\ndirectories in that root, each storing the data of an experiment with multiple\nvariants accross which we want to select the best. Each experiment directory\nshould contain a directoy named \'summaries\' that hosts subdirectories for the\ndifferent runs with each one containing event files. These event files are read\nto figure out which is best in terms of mean validation accuracy, and at which\nstep of that run this best value occurs in.\n\nFor each of the experiment directories provided, the output information is saved\nin a \'best.pklz\' file in that directory. This file contains a dict with keys\n\'best_variant\', \'best_valid_acc\', and \'best_update_num\' where the name of the\nvariant is simply the name of the sub-directory corresponding to that variant.\n\nExample directory structure (after the script is ran):\nRoot contains: \'Exp1\', \'Exp2\'.\n  Exp1 contains: \'checkpoints\', \'summaries\', and best.pklz\n    summaries contains: \'1\', \'2\', \'3\', ..., \'20\'\n      \'1\' contains event files\n      \'2\' contains event files\n      ...\n      \'20\' contains event files\n\nSample command:\n# pylint: disable=line-too-long\npython -m meta_dataset.analysis.select_best_model \\\n  --alsologtostderr \\\n  --all_experiments_root=<experiments_root> \\\n  --experiment_dir_basenames=baseline_imagenet_icml2019_1/3602170,baselinefinetune_imagenet_icml2019_1/3581340\n# pylint: enable=line-too-long\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\n\nfrom absl import logging\nimport numpy as np\nfrom six.moves import range\nfrom six.moves import zip\nimport six.moves.cPickle as pkl\nimport tensorflow.compat.v1 as tf\n\nFLAGS = tf.flags.FLAGS\n\ntf.flags.DEFINE_string(\n    \'all_experiments_root\',\n    \'\',\n    \'The overall experiments directory root.\')\n\ntf.flags.DEFINE_string(\n    \'experiment_dir_basenames\', \'\'\n    \'baseline_imagenet_icml2019_1/3602170,\'\n    \'baselinefinetune_imagenet_icml2019_1/3581340\',\n    \'A comma-separated list of directory basenames. Adding each basename as a \'\n    \'suffix to FLAGS.all_experiments_root forms a path that stores the data of \'\n    \'an experiment with multiple variants accross which we want to select the \'\n    \'best. Each such path is expected to host a directory named ""summaries"" \'\n    \'that contains subdirectories for the different runs with each such \'\n    \'subdirectory containing event files.\')\n\n# TODO(etriantafillou): This assumes the variants to omit are the same for all\n# experiments that model selection will be ran for which doesn\'t make much\n# sense. Maybe just remove this altogether?\ntf.flags.DEFINE_string(\n    \'restrict_to_variants\', \'\', \'A comma-separated list of \'\n    \'variants to restrict to for model selection. This is \'\n    \'useful for example for finding the best out of all \'\n    \'variants that use a specific embedding or image size.\')\n\ntf.flags.DEFINE_string(\n    \'restrict_to_variants_by_range\', \'\', \'A comma-separated list of \'\n    \'two integers that represent the start and end range (both inclusive) \'\n    \'of variant ids to restrict to.\')\n\ntf.flags.DEFINE_string(\n    \'description\', \'best\', \'The description for the output. The output will \'\n    \'then be named as description.pklz and description.txt. For example, this \'\n    \'can be used to reflect that some variants were omitted.\')\n\n# The following two flags assume that the parameters of the experiments have\n# been logged (they attempt to read from them). If this is not the case, the\n# restrict_to_variants flag should be used instead.\ntf.flags.DEFINE_string(\n    \'restrict_to_architectures\', \'\', \'The comma-separated names of the \'\n    \'embedding networks to restrict to for model selection.\')\n\ntf.flags.DEFINE_enum(\n    \'restrict_to_pretrained_source\', \'\', [\'\', \'scratch\', \'imagenet\'],\n    \'The name of a  pretrained_source to \'\n    \'restrict to for model selection.\')\n\ntf.flags.DEFINE_integer(\n    \'smooth_window\', 1, \'rolling average window to be \'\n    \'applied before the best model selection. \'\n    \'Set 1 for no smoothing.\')\n\n\ndef get_value_from_params_dir(params_dir, param_names):\n  """"""Gets the first found value from `param_names` in `params_dir`.""""""\n\n  def _load_params(param_name, params_file, loader, mode):\n    with tf.io.gfile.GFile(params_file, mode) as f:\n      params = loader(f)\n    logging.info(\'Found params file %s\', params_file)\n    return params[param_name]\n\n  for param_name in param_names:\n    try:\n      try:\n        return _load_params(param_name, os.path.join(params_dir, \'params.json\'),\n                            json.load, \'r\')\n      except tf.errors.NotFoundError:\n        logging.info(\'%s does not exist in %s\', \'params.json\', params_dir)\n\n      try:\n        return _load_params(param_name, os.path.join(params_dir, \'params.pkl\'),\n                            pkl.load, \'rb\')\n      except tf.errors.NotFoundError:\n        logging.info(\'%s does not exist in %s\', \'params.pkl\', params_dir)\n\n    except KeyError:\n      pass\n\n  raise ValueError(\'Did not find any of the following keys: %s\' % param_names)\n\n\ndef get_paths_to_events(root_dir,\n                        restrict_to_architectures,\n                        restrict_to_pretrained_source,\n                        restrict_to_variants=None):\n  """"""Returns a dict that maps each variant name to its event file.\n\n  The name of the variant is the basename of the directory where it\'s stored.\n  Assumes the following directory organization root_dir contains a sub-directory\n  for every variant where event files can be found.\n\n  There may be more than one event file for each variant, e.g. a new one will be\n  created upon restarting an experiment that was pre-empted. So later event\n  files contain the summaries for larger values of \'step\'. We need all of them\n  for determining the global \'best\'.\n\n  Args:\n    root_dir: A str. The root directory of experiments of all models variants.\n    restrict_to_architectures: A list of names of architectures to restrict to\n      when choosing the best variant.\n    restrict_to_pretrained_source: A string. The pretrained_source to restrict\n      to when choosing the best variant.\n    restrict_to_variants: Optionally, a set of variant names to restrict to.\n  """"""\n  params_dir = os.path.join(root_dir, \'params\')\n  summary_dir = os.path.join(root_dir, \'summaries\')\n\n  def get_variant_architecture(name):\n    """"""Return the architecture of the given variant if recorded; o/w None.""""""\n    variant_params_dir = os.path.join(params_dir, name)\n    architecture = get_value_from_params_dir(\n        variant_params_dir,\n        (\n            \'_gin.Learner.embedding_fn\',\n            # The following are for backwards compatibility.\n            \'_gin.Trainer.embedding_network\',\n            \'_gin.LearnerConfig.embedding_network\',\n        ))\n\n    return architecture\n\n  def get_variant_pretrained_source(name):\n    """"""Return the pretrained src of the given variant if recorded; o/w None.""""""\n    variant_params_dir = os.path.join(params_dir, name)\n    pretrained_source = get_value_from_params_dir(\n        variant_params_dir, \'_gin.Trainer.pretrained_source\')\n\n    if not pretrained_source:\n      # Backwards compatibility.\n      pretrained_source = get_value_from_params_dir(\n          variant_params_dir, \'_gin.LearnerConfig.pretrained_source\')\n\n    return pretrained_source\n\n  def keep_variant(name):\n    """"""Determine if the variant in directory name should be considered.""""""\n    value_error_msg = (\n        \'Requested to restrict to an architecture or \'\n        \'pretrained_source but the given experiment does not \'\n        \'have its params recorded. Looked in: {}\'.format(params_dir))\n\n    if restrict_to_architectures:\n      architecture = get_variant_architecture(name)\n      if architecture is None:\n        raise ValueError(value_error_msg)\n    valid_architecture = (not restrict_to_architectures or\n                          architecture in restrict_to_architectures)\n\n    if restrict_to_pretrained_source:\n      pretrained_source = get_variant_pretrained_source(name)\n      if pretrained_source is None:\n        raise ValueError(value_error_msg)\n    valid_pretrained_source = (\n        not restrict_to_pretrained_source or\n        pretrained_source == restrict_to_pretrained_source)\n\n    valid_variant_name = True\n    if restrict_to_variants is not None:\n      valid_variant_name = name in restrict_to_variants\n\n    return (valid_architecture and valid_pretrained_source and\n            valid_variant_name)\n\n  variant_names = [\n      fname for fname in tf.io.gfile.listdir(summary_dir)\n      if tf.io.gfile.isdir(os.path.join(summary_dir, fname))\n  ]\n\n  if not variant_names:\n    # Maybe there are no variants, and we are already in the directory that\n    # contains the summaries. In this case, we consider that the current\n    # directory (.) is the only variant.\n    variant_names = [\'.\']\n\n  # Further filter variant names based on the given restrictions.\n  variant_names = [name for name in variant_names if keep_variant(name)]\n\n  if not variant_names:\n    raise ValueError(\'Found no subdirectories in {}. Was expecting a \'\n                     \'subdirectory per variant.\'.format(summary_dir))\n  variant_paths = [\n      os.path.join(summary_dir, variant_dir) for variant_dir in variant_names\n  ]\n\n  event_paths = {}\n  for variant_path, variant_name in zip(variant_paths, variant_names):\n    event_filenames = [\n        f_name for f_name in tf.io.gfile.listdir(variant_path)\n        if f_name.startswith(\'events.out.tfevents\')\n    ]\n\n    if len(event_filenames) < 1:\n      logging.warn(\'Skipping empty variant %s.\', variant_path)\n      logging.info(\n          \'Was expecting at least one event file \'\n          \'in directory %s. Instead, found %d.\', variant_path,\n          len(event_filenames))\n      continue\n    event_paths[variant_name] = [\n        os.path.join(variant_path, event_filename)\n        for event_filename in event_filenames\n    ]\n\n  logging.info(\'Found event files for variants: %s\', list(event_paths.keys()))\n  return event_paths\n\n\n# TODO(crisnv): add smooth_type=\'uniform\' that defines the smooth policy\ndef moving_average(x, smooth_window):\n  """"""Returns a smoothed version of x.\n\n  This smoothes the x array according to the smooth_window parameter.\n\n  Args:\n    x: The array to smooth.\n    smooth_window: An integer that defines the neighborhood to be used in\n      smoothing.\n  """"""\n  conv_filter = getattr(moving_average, \'conv_filter\', None)\n  if conv_filter is None or (moving_average.conv_filter_size != smooth_window):\n    moving_average.conv_filter = np.ones((smooth_window,)) / smooth_window\n    moving_average.conv_filter_size = smooth_window\n  # if smooth_window is even, pad accordingly to keep stream size\n  x = np.pad(x, (smooth_window // 2, smooth_window - 1 - (smooth_window // 2)),\n             \'reflect\')\n  return np.convolve(x, moving_average.conv_filter, mode=\'valid\')\n\n\ndef extract_best_from_event_file(event_path, smooth_window, log_details=False):\n  """"""Returns the best accuracy and the step it occurs in in the given events.\n\n  This searches the summaries written in a given event file, which may be only a\n  subset of the total summaries of a run, since the summaries of a run are\n  sometimes split into multiple event files.\n\n  Args:\n    event_path: A string. The path to an event file.\n    smooth_window: An integer that defines the neighborhood to be used in\n      smoothing before the argmax (use <=1 for no smoothing)\n    log_details: A boolean. Whether to log details regarding skipped event paths\n      in which locating the tag ""mean valid acc"" failed.\n  """"""\n  steps, valid_accs = [], []\n  try:\n    for event in tf.train.summary_iterator(event_path):\n      step = event.step\n      for value in event.summary.value:\n        if value.tag == \'mean valid acc\':\n          steps.append(step)\n          valid_accs.append(value.simple_value)\n  except tf.errors.DataLossError:\n    if log_details:\n      tf.logging.info(\n          \'Omitting events from event_path {} because \'\n          \'tf.train.summary_iterator(event_path) failed.\'.format(event_path))\n    return 0, 0\n  if not valid_accs:\n    # Could happen if there is no DataLossError above but for some reason\n    # there is no \'mean valid acc\' tag found in the summary values.\n    tf.logging.info(\n        \'Did not find any ""mean valid acc"" tags in event_path {}\'.format(\n            event_path))\n    return 0, 0\n  if smooth_window > 1:\n    valid_accs = moving_average(valid_accs, smooth_window)\n  argmax_ind = np.argmax(valid_accs)\n  best_acc = valid_accs[argmax_ind]\n  best_step = steps[argmax_ind]\n  if log_details:\n    tf.logging.info(\'Successfully read event_path {} with best_acc {}\'.format(\n        event_path, best_acc))\n  return best_acc, best_step\n\n\ndef extract_best_from_variant(event_paths, smooth_window):\n  """"""Returns the best accuracy and the step it occurs in for the given run.\n\n  Args:\n    event_paths: A list of strings. The event files of the given run.\n    smooth_window:  An integer that defines the neighborhood to be used in\n      smoothing before the argmax (use <=1 for no smoothing)\n\n  Raises:\n    RuntimeError: No \'valid\' event file for the given variant (\'valid\' here\n      refers to an event file that has a ""mean valid acc"" tag).\n  """"""\n  best_step = best_acc = -1\n  for event_path in event_paths:\n    best_acc_, best_step_ = extract_best_from_event_file(\n        event_path, smooth_window)\n    if best_acc_ > best_acc:\n      best_acc = best_acc_\n      best_step = best_step_\n  if best_acc <= 0:\n    raise RuntimeError(\'Something went wrong with the summary event reading.\')\n  return best_acc, best_step\n\n\ndef main(argv):\n  del argv\n  experiment_paths = [\n      os.path.join(FLAGS.all_experiments_root, basename)\n      for basename in FLAGS.experiment_dir_basenames.split(\',\')\n  ]\n  # Perform model selection for each provided experiment root.\n  for root_experiment_dir in experiment_paths:\n    stars_string = \'**************************************\\n\'\n    architecture_string = \'\'\n    if FLAGS.restrict_to_architectures:\n      architecture_string = \' out of the {} variants\'.format(\n          FLAGS.restrict_to_architectures)\n    logging.info(\'%sSelecting the best variant for: %s%s.%s\', stars_string,\n                 root_experiment_dir, architecture_string, stars_string)\n\n    if FLAGS.restrict_to_variants_by_range and FLAGS.restrict_to_variants:\n      raise ValueError(\'Please provide only one of \'\n                       \'FLAGS.restrict_to_variants_by_range and \'\n                       \'FLAGS.restrict_to_variants, not both.\')\n\n    restrict_to_variants = None\n    if FLAGS.restrict_to_variants_by_range:\n      start, end = FLAGS.restrict_to_variants_by_range.split(\',\')\n      start, end = int(start), int(end)\n      restrict_to_variants = set(\n          [str(variant_id) for variant_id in range(start, end + 1)])\n    if FLAGS.restrict_to_variants:\n      restrict_to_variants = set(FLAGS.restrict_to_variants.split(\',\'))\n\n    restrict_to_architectures = []\n    if FLAGS.restrict_to_architectures:\n      restrict_to_architectures = FLAGS.restrict_to_architectures.split(\',\')\n\n    smooth_window = FLAGS.smooth_window\n    event_paths = get_paths_to_events(\n        root_experiment_dir,\n        restrict_to_architectures,\n        FLAGS.restrict_to_pretrained_source,\n        restrict_to_variants=restrict_to_variants)\n    # Read the event file of each variant to find the highest mean validation\n    # accuracy reached with it.\n    best_variant = \'\'\n    best_valid_acc = -1\n    best_step = -1\n    for variant_name, event_path in event_paths.items():\n      best_valid_acc_, best_step_ = extract_best_from_variant(\n          event_path, smooth_window)\n      if best_valid_acc_ > best_valid_acc:\n        best_variant = variant_name\n        best_valid_acc = best_valid_acc_\n        best_step = best_step_\n\n    output_dict = {\n        \'best_variant\': best_variant,\n        \'best_valid_acc\': best_valid_acc,\n        \'best_update_num\': best_step\n    }\n\n    # Create a more informative description if necessary.\n    description = FLAGS.description\n    if FLAGS.restrict_to_architectures and FLAGS.description == \'best\':\n      description += \'_{}\'.format(FLAGS.restrict_to_architectures)\n\n    if (FLAGS.restrict_to_pretrained_source and FLAGS.description == \'best\'):\n      if FLAGS.restrict_to_pretrained_source == \'scratch\':\n        description += \'_trained_from_scratch\'\n      else:\n        description += \'_pretrained_on_{}\'.format(\n            FLAGS.restrict_to_pretrained_source)\n    if FLAGS.smooth_window > 1:\n      description += \'_smoothed_by_window_{}\'.format(smooth_window)\n\n    output_path_pklz = os.path.join(root_experiment_dir,\n                                    \'{}.pklz\'.format(description))\n    with tf.io.gfile.GFile(output_path_pklz, \'wb\') as f:\n      pkl.dump(output_dict, f, protocol=pkl.HIGHEST_PROTOCOL)\n\n    # Also write this info as a .txt file for easier reading.\n    output_path_txt = os.path.join(root_experiment_dir,\n                                   \'{}.txt\'.format(description))\n    with tf.io.gfile.GFile(output_path_txt, \'w\') as f:\n      f.write(\n          \'best_variant: {}\\nbest_valid_acc: {}\\nbest_update_num: {}\\n\'.format(\n              best_variant, best_valid_acc, best_step))\n    logging.info(\n        \'Best variant: %s. Best valid acc: %s. Best update num: %d. \'\n        \'Just wrote this info to %s and %s\', best_variant, best_valid_acc,\n        best_step, output_path_pklz, output_path_txt)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.app.run(main)\n'"
meta_dataset/data/__init__.py,0,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Sub-module for reading data and assembling episodes.""""""\n# Whether datasets with example-level splits, or pools, are supported.\n# Currently, this is not implemented.\nPOOL_SUPPORTED = False\n'"
meta_dataset/data/config.py,2,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Configuration classes for data processing.\n\nConfig classes that parametrize the behaviour of different stages of the data\nprocessing pipeline, and are set up via `gin`.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin.tf\n\n\n@gin.configurable\nclass DataConfig(object):\n  """"""Common configuration options for creating data processing pipelines.""""""\n\n  def __init__(\n      self,\n      image_height,\n      shuffle_buffer_size,\n      read_buffer_size_bytes,\n      num_prefetch,\n  ):\n    """"""Initialize a DataConfig.\n\n    Args:\n      image_height: An integer, the desired height for the images output by the\n        data pipeline. Images are squared and have 3 channels (RGB), so each\n        image will have shape [image_height, image_height, 3],\n      shuffle_buffer_size: An integer, the size of the example buffer in the\n        tf.data.Dataset.shuffle operations (there is typically one shuffle per\n        class in the episodic setting, one per dataset in the batch setting).\n        Classes with fewer examples as this number are shuffled in-memory.\n      read_buffer_size_bytes: An integer, the size (in bytes) of the read buffer\n        for each tf.data.TFRecordDataset (there is typically one for each class\n        of each dataset).\n      num_prefetch: int, the number of examples to prefetch for each class of\n        each dataset. Prefetching occurs just after the class-specific Dataset\n        object is constructed. If < 1, no prefetching occurs.\n    """"""\n    self.image_height = image_height\n    self.shuffle_buffer_size = shuffle_buffer_size\n    self.read_buffer_size_bytes = read_buffer_size_bytes\n    self.num_prefetch = num_prefetch\n\n\n@gin.configurable\nclass DataAugmentation(object):\n  """"""Configurations for performing data augmentation.""""""\n\n  def __init__(self, enable_jitter, jitter_amount, enable_gaussian_noise,\n               gaussian_noise_std):\n    """"""Initialize a DataAugmentation.\n\n    Args:\n      enable_jitter: bool whether to use image jitter (pad each image using\n        reflection along x and y axes and then random crop).\n      jitter_amount: amount (in pixels) to pad on all sides of the image.\n      enable_gaussian_noise: bool whether to use additive Gaussian noise.\n      gaussian_noise_std: Standard deviation of the Gaussian distribution.\n    """"""\n    self.enable_jitter = enable_jitter\n    self.jitter_amount = jitter_amount\n    self.enable_gaussian_noise = enable_gaussian_noise\n    self.gaussian_noise_std = gaussian_noise_std\n\n\n@gin.configurable\nclass EpisodeDescriptionConfig(object):\n  """"""Configuration options for episode characteristics.""""""\n\n  def __init__(self,\n               num_ways,\n               num_support,\n               num_query,\n               min_ways,\n               max_ways_upper_bound,\n               max_num_query,\n               max_support_set_size,\n               max_support_size_contrib_per_class,\n               min_log_weight,\n               max_log_weight,\n               ignore_dag_ontology,\n               ignore_bilevel_ontology,\n               min_examples_in_class=0):\n    """"""Initialize a EpisodeDescriptionConfig.\n\n    This is used in sampling.py in Trainer and in EpisodeDescriptionSampler to\n    determine the parameters of episode creation relating to the ways and shots.\n\n    Args:\n      num_ways: Integer, fixes the number of classes (""ways"") to be used in each\n        episode. None leads to variable way.\n      num_support: Integer, fixes the number of examples for each class in the\n        support set.\n      num_query: Integer, fixes the number of examples for each class in the\n        query set.\n      min_ways: Integer, the minimum value when sampling ways.\n      max_ways_upper_bound: Integer, the maximum value when sampling ways. Note\n        that the number of available classes acts as another upper bound.\n      max_num_query: Integer, the maximum number of query examples per class.\n      max_support_set_size: Integer, the maximum size for the support set.\n      max_support_size_contrib_per_class: Integer, the maximum contribution for\n        any given class to the support set size.\n      min_log_weight: Float, the minimum log-weight to give to any particular\n        class when determining the number of support examples per class.\n      max_log_weight: Float, the maximum log-weight to give to any particular\n        class.\n      ignore_dag_ontology: Whether to ignore ImageNet\'s DAG ontology when\n        sampling classes from it. This has no effect if ImageNet is not part of\n        the benchmark.\n      ignore_bilevel_ontology: Whether to ignore Omniglot\'s DAG ontology when\n        sampling classes from it. This has no effect if Omniglot is not part of\n        the benchmark.\n      min_examples_in_class: An integer, the minimum number of examples that a\n        class has to contain to be considered. All classes with fewer examples\n        will be ignored. 0 means no classes are ignored, so having classes with\n        no examples may trigger errors later. For variable shots, a value of 2\n        makes it sure that there are at least one support and one query samples.\n        For fixed shots, you could set it to `num_support + num_query`.\n\n    Raises:\n      RuntimeError: if incompatible arguments are passed.\n    """"""\n    arg_groups = {\n        \'num_ways\': (num_ways, (\'min_ways\', \'max_ways_upper_bound\'),\n                     (min_ways, max_ways_upper_bound)),\n        \'num_query\': (num_query, (\'max_num_query\',), (max_num_query,)),\n        \'num_support\':\n            (num_support,\n             (\'max_support_set_size\', \'max_support_size_contrib_per_class\',\n              \'min_log_weight\', \'max_log_weight\'),\n             (max_support_set_size, max_support_size_contrib_per_class,\n              min_log_weight, max_log_weight)),\n    }\n\n    for first_arg_name, values in arg_groups.items():\n      first_arg, required_arg_names, required_args = values\n      if ((first_arg is None) and any(arg is None for arg in required_args)):\n        # Get name of the nones\n        none_arg_names = [\n            name for var, name in zip(required_args, required_arg_names)\n            if var is None\n        ]\n        raise RuntimeError(\n            \'The following arguments: %s can not be None, since %s is None. \'\n            \'Arguments can be set up with gin, for instance by providing \'\n            \'`--gin_file=learn/gin/setups/data_config.gin` or calling \'\n            \'`gin.parse_config_file(...)` in the code. Please ensure the \'\n            \'following gin arguments of EpisodeDescriptionConfig are set: \'\n            \'%s\' % (none_arg_names, first_arg_name, none_arg_names))\n\n    self.num_ways = num_ways\n    self.num_support = num_support\n    self.num_query = num_query\n    self.min_ways = min_ways\n    self.max_ways_upper_bound = max_ways_upper_bound\n    self.max_num_query = max_num_query\n    self.max_support_set_size = max_support_set_size\n    self.max_support_size_contrib_per_class = max_support_size_contrib_per_class\n    self.min_log_weight = min_log_weight\n    self.max_log_weight = max_log_weight\n    self.ignore_dag_ontology = ignore_dag_ontology\n    self.ignore_bilevel_ontology = ignore_bilevel_ontology\n    self.min_examples_in_class = min_examples_in_class\n\n  @property\n  def max_ways(self):\n    """"""Returns the way (maximum way if variable) of the episode.""""""\n    return self.num_ways or self.max_ways_upper_bound\n'"
meta_dataset/data/dataset_spec.py,5,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Interfaces for dataset specifications.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport json\nimport os\n\nfrom absl import logging\nfrom meta_dataset import data\nfrom meta_dataset.data import imagenet_specification\nfrom meta_dataset.data import learning_spec\nimport numpy as np\nimport six\nfrom six.moves import cPickle as pkl\nfrom six.moves import range\nfrom six.moves import zip\nimport tensorflow.compat.v1 as tf\n\n\ndef get_classes(split, classes_per_split):\n  """"""Gets the sequence of class labels for a split.\n\n  Class id\'s are returned ordered and without gaps.\n\n  Args:\n    split: A Split, the split for which to get classes.\n    classes_per_split: Matches each Split to the number of its classes.\n\n  Returns:\n    The sequence of classes for the split.\n\n  Raises:\n    ValueError: An invalid split was specified.\n  """"""\n  num_classes = classes_per_split[split]\n\n  # Find the starting index of classes for the given split.\n  if split == learning_spec.Split.TRAIN:\n    offset = 0\n  elif split == learning_spec.Split.VALID:\n    offset = classes_per_split[learning_spec.Split.TRAIN]\n  elif split == learning_spec.Split.TEST:\n    offset = (\n        classes_per_split[learning_spec.Split.TRAIN] +\n        classes_per_split[learning_spec.Split.VALID])\n  else:\n    raise ValueError(\'Invalid dataset split.\')\n\n  # Get a contiguous range of classes from split.\n  return range(offset, offset + num_classes)\n\n\ndef _check_validity_of_restricted_classes_per_split(\n    restricted_classes_per_split, classes_per_split):\n  """"""Check the validity of the given restricted_classes_per_split.\n\n  Args:\n    restricted_classes_per_split: A dict mapping Split enums to the number of\n      classes to restrict to for that split.\n    classes_per_split: A dict mapping Split enums to the total available number\n      of classes for that split.\n\n  Raises:\n    ValueError: if restricted_classes_per_split is invalid.\n  """"""\n  for split_enum, num_classes in restricted_classes_per_split.items():\n    if split_enum not in [\n        learning_spec.Split.TRAIN, learning_spec.Split.VALID,\n        learning_spec.Split.TEST\n    ]:\n      raise ValueError(\'Invalid key {} in restricted_classes_per_split.\'\n                       \'Valid keys are: learning_spec.Split.TRAIN, \'\n                       \'learning_spec.Split.VALID, and \'\n                       \'learning_spec.Split.TEST\'.format(split_enum))\n    if num_classes > classes_per_split[split_enum]:\n      raise ValueError(\'restricted_classes_per_split can not specify a \'\n                       \'number of classes greater than the total available \'\n                       \'for that split. Specified {} for split {} but have \'\n                       \'only {} available for that split.\'.format(\n                           num_classes, split_enum,\n                           classes_per_split[split_enum]))\n\n\ndef get_total_images_per_class(data_spec, class_id=None, pool=None):\n  """"""Returns the total number of images of a class in a data_spec and pool.\n\n  Args:\n    data_spec: A DatasetSpecification, or BiLevelDatasetSpecification.\n    class_id: The class whose number of images will be returned. If this is\n      None, it is assumed that the dataset has the same number of images for\n      each class.\n    pool: A string (\'train\' or \'test\', optional) indicating which example-level\n      split to select, if the current dataset has them.\n\n  Raises:\n    ValueError: when\n      - no class_id specified and yet there is class imbalance, or\n      - no pool specified when there are example-level splits, or\n      - pool is specified but there are no example-level splits, or\n      - incorrect value for pool.\n    RuntimeError: the DatasetSpecification is out of date (missing info).\n  """"""\n  if class_id is None:\n    if len(set(data_spec.images_per_class.values())) != 1:\n      raise ValueError(\'Not specifying class_id is okay only when all classes\'\n                       \' have the same number of images\')\n    class_id = 0\n\n  if class_id not in data_spec.images_per_class:\n    raise RuntimeError(\'The DatasetSpecification should be regenerated, as \'\n                       \'it does not have a non-default value for class_id {} \'\n                       \'in images_per_class.\'.format(class_id))\n  num_images = data_spec.images_per_class[class_id]\n\n  if pool is None:\n    if isinstance(num_images, collections.Mapping):\n      raise ValueError(\'DatasetSpecification {} has example-level splits, so \'\n                       \'the ""pool"" argument has to be set (to ""train"" or \'\n                       \'""test"".\'.format(data_spec.name))\n  elif not data.POOL_SUPPORTED:\n    raise NotImplementedError(\'Example-level splits or pools not supported.\')\n\n  return num_images\n\n\nclass BenchmarkSpecification(\n    collections.namedtuple(\n        \'BenchmarkSpecification\', \'name, image_shape, dataset_spec_list,\'\n        \'has_dag_ontology, has_bilevel_ontology, splits_to_contribute\')):\n  """"""The specification of a benchmark, consisting of multiple datasets.\n\n    Args:\n      name: string, the name of the benchmark.\n      image_shape: a sequence of dimensions representing the shape that each\n        image (of each dataset) will be resized to.\n      dataset_spec_list: a list of DatasetSpecification or\n        HierarchicalDatasetSpecification instances for the benchmarks\' datasets.\n      has_dag_ontology: A list of bools, whose length is the same as the number\n        of datasets in the benchmark. Its elements indicate whether each dataset\n        (in the order specified in the benchmark_spec.dataset_spec_list list)\n        has a DAG-structured ontology. In that case, the corresponding dataset\n        specification must be an instance of HierarchicalDatasetSpecification.\n      has_bilevel_ontology: A list of bools of the same length and structure as\n        has_dag_ontology, this time indicating whether each dataset has a\n        bi-level ontology (comprised of superclasses and subclasses). In that\n        case, the corresponding dataset specification must be an instance of\n        BiLevelDatasetSpecification.\n      splits_to_contribute: A list of sets of the same length as the number of\n        datasets in the benchmark. Each element is a set which can be one of\n        {\'train\'}, {\'valid\'}, {\'train\', \'valid\'} or {\'test\'} indicating which\n        meta-splits the corresponding dataset should contribute to. Note that a\n        dataset can not contribute to a split if it has zero classes assigned to\n        that split. But we do have the option to ignore a dataset for a\n        particular split even if it has a non-zero number of classes for it.\n  """"""\n\n  def __new__(cls, name, image_shape, dataset_spec_list, has_dag_ontology,\n              has_bilevel_ontology, splits_to_contribute):\n    if len(has_dag_ontology) != len(dataset_spec_list):\n      raise ValueError(\'The length of has_dag_ontology must be the number of \'\n                       \'datasets.\')\n    if len(has_bilevel_ontology) != len(dataset_spec_list):\n      raise ValueError(\'The length of has_bilevel_ontology must be the number \'\n                       \'of datasets.\')\n    if len(splits_to_contribute) != len(dataset_spec_list):\n      raise ValueError(\'The length of splits_to_contribute must be the number \'\n                       \'of datasets.\')\n    # Ensure that HierarchicalDatasetSpecification is used iff has_dag_ontology.\n    for i, has_dag in enumerate(has_dag_ontology):\n      if has_dag and not isinstance(dataset_spec_list[i],\n                                    HierarchicalDatasetSpecification):\n        raise ValueError(\'Dataset {} has dag ontology, but does not have a \'\n                         \'hierarchical dataset specification.\'.format(i))\n      if not has_dag and isinstance(dataset_spec_list[i],\n                                    HierarchicalDatasetSpecification):\n        raise ValueError(\'Dataset {} has no dag ontology, but is represented \'\n                         \'using a HierarchicalDatasetSpecification.\'.format(i))\n    # Ensure that BiLevelDatasetSpecification is used iff has_bilevel_ontology.\n    for i, is_bilevel in enumerate(has_bilevel_ontology):\n      if is_bilevel and not isinstance(dataset_spec_list[i],\n                                       BiLevelDatasetSpecification):\n        raise ValueError(\'Dataset {} has bilevel ontology, but does not have a \'\n                         \'bilevel dataset specification.\'.format(i))\n      if not is_bilevel and isinstance(dataset_spec_list[i],\n                                       BiLevelDatasetSpecification):\n        raise ValueError(\n            \'Dataset {} has no bilevel ontology, but is \'\n            \'represented using a BiLevelDatasetSpecification.\'.format(i))\n    # Check the validity of the given value for splits_to_contribute.\n    valid_values = [{\'train\'}, {\'valid\'}, {\'train\', \'valid\'}, {\'test\'}]\n    for splits in splits_to_contribute:\n      if splits not in valid_values:\n        raise ValueError(\n            \'Found an invalid element: {} in splits_to_contribute. \'\n            \'Valid elements are: {}\'.format(splits, valid_values))\n    # Ensure that no dataset is asked to contribute to a split for which it does\n    # not have any classes.\n    for dataset_spec, dataset_splits in zip(dataset_spec_list,\n                                            splits_to_contribute):\n      dataset_spec.initialize()\n      if isinstance(dataset_spec, BiLevelDatasetSpecification):\n        classes_per_split = dataset_spec.superclasses_per_split\n      else:\n        classes_per_split = dataset_spec.classes_per_split\n      invalid_train_split = (\'train\' in dataset_splits and\n                             not classes_per_split[learning_spec.Split.TRAIN])\n      invalid_valid_split = (\'valid\' in dataset_splits and\n                             not classes_per_split[learning_spec.Split.VALID])\n      invalid_test_split = (\'test\' in dataset_splits and\n                            not classes_per_split[learning_spec.Split.TEST])\n      if invalid_train_split or invalid_valid_split or invalid_test_split:\n        raise ValueError(\'A dataset can not contribute to a split if it has \'\n                         \'no classes assigned to that split.\')\n    self = super(BenchmarkSpecification,\n                 cls).__new__(cls, name, image_shape, dataset_spec_list,\n                              has_dag_ontology, has_bilevel_ontology,\n                              splits_to_contribute)\n    return self\n\n\nclass DatasetSpecification(\n    collections.namedtuple(\'DatasetSpecification\',\n                           (\'name, classes_per_split, images_per_class, \'\n                            \'class_names, path, file_pattern\'))):\n  """"""The specification of a dataset.\n\n    Args:\n      name: string, the name of the dataset.\n      classes_per_split: a dict specifying the number of classes allocated to\n        each split.\n      images_per_class: a dict mapping each class id to its number of images.\n        Usually, the number of images is an integer, but if the dataset has\n        \'train\' and \'test\' example-level splits (or ""pools""), then it is a dict\n        mapping a string (the pool) to an integer indicating how many examples\n        are in that pool. E.g., the number of images could be {\'train\': 5923,\n        \'test\': 980}.\n      class_names: a dict mapping each class id to the corresponding class name.\n      path: the path to the dataset\'s files.\n      file_pattern: a string representing the naming pattern for each class\'s\n        file. This string should be either \'{}.tfrecords\' or \'{}_{}.tfrecords\'.\n        The first gap will be replaced by the class id in both cases, while in\n        the latter case the second gap will be replaced with by a shard index,\n        or one of \'train\', \'valid\' or \'test\'. This offers support for multiple\n        shards of a class\' images if a class is too large, that will be merged\n        later into a big pool for sampling, as well as different splits that\n        will be treated as disjoint pools for sampling the support versus query\n        examples of an episode.\n  """"""\n\n  def initialize(self, restricted_classes_per_split=None):\n    """"""Initializes a DatasetSpecification.\n\n    Args:\n      restricted_classes_per_split: A dict that specifies for each split, a\n        number to restrict its classes to. This number must be no greater than\n        the total number of classes of that split. By default this is None and\n        no restrictions are applied (all classes are used).\n\n    Raises:\n      ValueError: Invalid file_pattern provided.\n    """"""\n    # Check that the file_pattern adheres to one of the allowable forms\n    if self.file_pattern not in [\'{}.tfrecords\', \'{}_{}.tfrecords\']:\n      raise ValueError(\'file_pattern must be either ""{}.tfrecords"" or \'\n                       \'""{}_{}.tfrecords"" to support shards or splits.\')\n    if restricted_classes_per_split is not None:\n      _check_validity_of_restricted_classes_per_split(\n          restricted_classes_per_split, self.classes_per_split)\n      # Apply the restriction.\n      for split, restricted_num_classes in restricted_classes_per_split.items():\n        self.classes_per_split[split] = restricted_num_classes\n\n  def get_total_images_per_class(self, class_id=None, pool=None):\n    """"""Returns the total number of images for the specified class.\n\n    Args:\n      class_id: The class whose number of images will be returned. If this is\n        None, it is assumed that the dataset has the same number of images for\n        each class.\n      pool: A string (\'train\' or \'test\', optional) indicating which\n        example-level split to select, if the current dataset has them.\n\n    Raises:\n      ValueError: when\n        - no class_id specified and yet there is class imbalance, or\n        - no pool specified when there are example-level splits, or\n        - pool is specified but there are no example-level splits, or\n        - incorrect value for pool.\n      RuntimeError: the DatasetSpecification is out of date (missing info).\n    """"""\n    return get_total_images_per_class(self, class_id, pool=pool)\n\n  def get_classes(self, split):\n    """"""Gets the sequence of class labels for a split.\n\n    Labels are returned ordered and without gaps.\n\n    Args:\n      split: A Split, the split for which to get classes.\n\n    Returns:\n      The sequence of classes for the split.\n\n    Raises:\n      ValueError: An invalid split was specified.\n    """"""\n    return get_classes(split, self.classes_per_split)\n\n  def to_dict(self):\n    """"""Returns a dictionary for serialization to JSON.\n\n    Each member is converted to an elementary type that can be serialized to\n    JSON readily.\n    """"""\n    # Start with the dict representation of the namedtuple\n    ret_dict = self._asdict()\n    # Add the class name for reconstruction when deserialized\n    ret_dict[\'__class__\'] = self.__class__.__name__\n    # Convert Split enum instances to their name (string)\n    ret_dict[\'classes_per_split\'] = {\n        split.name: count\n        for split, count in six.iteritems(ret_dict[\'classes_per_split\'])\n    }\n    # Convert binary class names to unicode strings if necessary\n    class_names = {}\n    for class_id, name in six.iteritems(ret_dict[\'class_names\']):\n      if isinstance(name, six.binary_type):\n        name = name.decode()\n      elif isinstance(name, np.integer):\n        name = six.text_type(name)\n      class_names[class_id] = name\n    ret_dict[\'class_names\'] = class_names\n    return ret_dict\n\n\nclass BiLevelDatasetSpecification(\n    collections.namedtuple(\'BiLevelDatasetSpecification\',\n                           (\'name, superclasses_per_split, \'\n                            \'classes_per_superclass, images_per_class, \'\n                            \'superclass_names, class_names, path, \'\n                            \'file_pattern\'))):\n  """"""The specification of a dataset that has a two-level hierarchy.\n\n    Args:\n      name: string, the name of the dataset.\n      superclasses_per_split: a dict specifying the number of superclasses\n        allocated to each split.\n      classes_per_superclass: a dict specifying the number of classes in each\n        superclass.\n      images_per_class: a dict mapping each class id to its number of images.\n      superclass_names: a dict mapping each superclass id to its name.\n      class_names: a dict mapping each class id to the corresponding class name.\n      path: the path to the dataset\'s files.\n      file_pattern: a string representing the naming pattern for each class\'s\n        file. This string should be either \'{}.tfrecords\' or \'{}_{}.tfrecords\'.\n        The first gap will be replaced by the class id in both cases, while in\n        the latter case the second gap will be replaced with by a shard index,\n        or one of \'train\', \'valid\' or \'test\'. This offers support for multiple\n        shards of a class\' images if a class is too large, that will be merged\n        later into a big pool for sampling, as well as different splits that\n        will be treated as disjoint pools for sampling the support versus query\n        examples of an episode.\n  """"""\n\n  def initialize(self, restricted_classes_per_split=None):\n    """"""Initializes a DatasetSpecification.\n\n    Args:\n      restricted_classes_per_split: A dict that specifies for each split, a\n        number to restrict its classes to. This number must be no greater than\n        the total number of classes of that split. By default this is None and\n        no restrictions are applied (all classes are used).\n\n    Raises:\n      ValueError: Invalid file_pattern provided\n    """"""\n    # Check that the file_pattern adheres to one of the allowable forms\n    if self.file_pattern not in [\'{}.tfrecords\', \'{}_{}.tfrecords\']:\n      raise ValueError(\'file_pattern must be either ""{}.tfrecords"" or \'\n                       \'""{}_{}.tfrecords"" to support shards or splits.\')\n    if restricted_classes_per_split is not None:\n      # Create a dict like classes_per_split of DatasetSpecification.\n      classes_per_split = {}\n      for split in self.superclasses_per_split.keys():\n        num_split_classes = self._count_classes_in_superclasses(\n            self.get_superclasses(split))\n        classes_per_split[split] = num_split_classes\n\n      _check_validity_of_restricted_classes_per_split(\n          restricted_classes_per_split, classes_per_split)\n    # The restriction in this case is applied in get_classes() below.\n    self.restricted_classes_per_split = restricted_classes_per_split\n\n  def get_total_images_per_class(self, class_id=None, pool=None):\n    """"""Returns the total number of images for the specified class.\n\n    Args:\n      class_id: The class whose number of images will be returned. If this is\n        None, it is assumed that the dataset has the same number of images for\n        each class.\n      pool: A string (\'train\' or \'test\', optional) indicating which\n        example-level split to select, if the current dataset has them.\n\n    Raises:\n      ValueError: when\n        - no class_id specified and yet there is class imbalance, or\n        - no pool specified when there are example-level splits, or\n        - pool is specified but there are no example-level splits, or\n        - incorrect value for pool.\n      RuntimeError: the DatasetSpecification is out of date (missing info).\n    """"""\n    return get_total_images_per_class(self, class_id, pool=pool)\n\n  def get_superclasses(self, split):\n    """"""Gets the sequence of superclass labels for a split.\n\n    Labels are returned ordered and without gaps.\n\n    Args:\n      split: A Split, the split for which to get the superclasses.\n\n    Returns:\n      The sequence of superclasses for the split.\n\n    Raises:\n      ValueError: An invalid split was specified.\n    """"""\n    return get_classes(split, self.superclasses_per_split)\n\n  def _count_classes_in_superclasses(self, superclass_ids):\n    return sum([\n        self.classes_per_superclass[superclass_id]\n        for superclass_id in superclass_ids\n    ])\n\n  def _get_split_offset(self, split):\n    """"""Returns the starting class id of the contiguous chunk of ids of split.\n\n    Args:\n      split: A Split, the split for which to get classes.\n\n    Raises:\n      ValueError: Invalid dataset split.\n    """"""\n    if split == learning_spec.Split.TRAIN:\n      offset = 0\n    elif split == learning_spec.Split.VALID:\n      previous_superclasses = range(\n          0, self.superclasses_per_split[learning_spec.Split.TRAIN])\n      offset = self._count_classes_in_superclasses(previous_superclasses)\n    elif split == learning_spec.Split.TEST:\n      previous_superclasses = range(\n          0, self.superclasses_per_split[learning_spec.Split.TRAIN] +\n          self.superclasses_per_split[learning_spec.Split.VALID])\n      offset = self._count_classes_in_superclasses(previous_superclasses)\n    else:\n      raise ValueError(\'Invalid dataset split.\')\n    return offset\n\n  def get_classes(self, split):\n    """"""Gets the sequence of class labels for a split.\n\n    Labels are returned ordered and without gaps.\n\n    Args:\n      split: A Split, the split for which to get classes.\n\n    Returns:\n      The sequence of classes for the split.\n    """"""\n    if not hasattr(self, \'restricted_classes_per_split\'):\n      self.initialize()\n    offset = self._get_split_offset(split)\n    if (self.restricted_classes_per_split is not None and\n        split in self.restricted_classes_per_split):\n      num_split_classes = self.restricted_classes_per_split[split]\n    else:\n      # No restriction, so include all classes of the given split.\n      num_split_classes = self._count_classes_in_superclasses(\n          self.get_superclasses(split))\n\n    return range(offset, offset + num_split_classes)\n\n  def get_class_ids_from_superclass_subclass_inds(self, split, superclass_id,\n                                                  class_inds):\n    """"""Gets the class ids of a number of classes of a given superclass.\n\n    Args:\n      split: A Split, the split for which to get classes.\n      superclass_id: An int. The id of a superclass.\n      class_inds: A list or sequence of ints. The indices into the classes of\n        the superclass superclass_id that we wish to return class id\'s for.\n\n    Returns:\n      rel_class_ids: A list of ints of length equal to that of class_inds. The\n        class id\'s relative to the split (between 0 and num classes in split).\n      class_ids: A list of ints of length equal to that of class_inds. The class\n        id\'s relative to the dataset (between 0 and the total num classes).\n    """"""\n    # The number of classes before the start of superclass_id, i.e. the class id\n    # of the first class of the given superclass.\n    superclass_offset = self._count_classes_in_superclasses(\n        range(superclass_id))\n\n    # Absolute class ids (between 0 and the total number of dataset classes).\n    class_ids = [superclass_offset + class_ind for class_ind in class_inds]\n\n    # Relative (between 0 and the total number of classes in the split).\n    # This makes the assumption that the class id\'s are in a contiguous range.\n    rel_class_ids = [\n        class_id - self._get_split_offset(split) for class_id in class_ids\n    ]\n\n    return rel_class_ids, class_ids\n\n  def to_dict(self):\n    """"""Returns a dictionary for serialization to JSON.\n\n    Each member is converted to an elementary type that can be serialized to\n    JSON readily.\n    """"""\n    # Start with the dict representation of the namedtuple\n    ret_dict = self._asdict()\n    # Add the class name for reconstruction when deserialized\n    ret_dict[\'__class__\'] = self.__class__.__name__\n    # Convert Split enum instances to their name (string)\n    ret_dict[\'superclasses_per_split\'] = {\n        split.name: count\n        for split, count in six.iteritems(ret_dict[\'superclasses_per_split\'])\n    }\n    return ret_dict\n\n\nclass HierarchicalDatasetSpecification(\n    collections.namedtuple(\'HierarchicalDatasetSpecification\',\n                           (\'name, split_subgraphs, images_per_class, \'\n                            \'class_names, path, file_pattern\'))):\n  """"""The specification of a hierarchical dataset.\n\n    Args:\n      name: string, the name of the dataset.\n      split_subgraphs: a dict that maps each Split to a set of nodes of its\n        corresponding graph.\n      images_per_class: dict mapping each Split to a dict that maps each node in\n        that split\'s subgraph to the number of images in the subgraph of that\n        node. Note that we can\'t merge these three dicts into a single one, as\n        there are nodes that will appear in more than one of these three\n        subgraphs but will have different connections (parent/child pointers) in\n        each one, therefore \'spanning\' a different number of images.\n      class_names: a dict mapping each class id to the corresponding class name.\n        For ilsvrc_2012, the WordNet id\'s are used in the place of the names.\n      path: the path to the dataset\'s files.\n      file_pattern: a string representing the naming pattern for each class\'s\n        file. The string must contain a placeholder for the class\'s ID (e.g. for\n        ImageNet this is the WordNet id).\n  """"""\n\n  # TODO(etriantafillou): Make this class inherit from object instead\n  # TODO(etriantafillou): Move this method to the __init__ of that revised class\n  def initialize(self, restricted_classes_per_split=None):\n    """"""Initializes a HierarchicalDatasetSpecification.\n\n    Args:\n      restricted_classes_per_split: A dict that specifies for each split, a\n        number to restrict its classes to. This number must be no greater than\n        the total number of classes of that split. By default this is None and\n        no restrictions are applied (all classes are used).\n    """"""\n    # Set self.class_names_to_ids to the inverse dict of self.class_names.\n    self.class_names_to_ids = dict(\n        zip(self.class_names.values(), self.class_names.keys()))\n\n    # Maps each Split enum to the number of its classes.\n    self.classes_per_split = self.get_classes_per_split()\n\n    if restricted_classes_per_split is not None:\n      _check_validity_of_restricted_classes_per_split(\n          restricted_classes_per_split, self.classes_per_split)\n      # Apply the restriction.\n      for split, restricted_num_classes in restricted_classes_per_split.items():\n        self.classes_per_split[split] = restricted_num_classes\n\n  def get_classes_per_split(self):\n    """"""Returns a dict mapping each split enum to the number of its classes.""""""\n\n    def count_split_classes(split):\n      graph = self.split_subgraphs[split]\n      leaves = imagenet_specification.get_leaves(graph)\n      return len(leaves)\n\n    classes_per_split = {}\n    for split in [\n        learning_spec.Split.TRAIN, learning_spec.Split.VALID,\n        learning_spec.Split.TEST\n    ]:\n      classes_per_split[split] = count_split_classes(split)\n    return classes_per_split\n\n  def get_split_subgraph(self, split):\n    """"""Returns the sampling subgraph DAG for the given split.\n\n    Args:\n      split: A Split, the split for which to get classes.\n    """"""\n    return self.split_subgraphs[split]\n\n  def get_classes(self, split):\n    """"""Returns a list of the class id\'s of classes assigned to split.\n\n    Args:\n      split: A Split, the split for which to get classes.\n    """"""\n    # The call to initialize computes self.classes_per_split. Do it only if it\n    # hasn\'t already been done.\n    if not hasattr(self, \'classes_per_split\'):\n      self.initialize()\n    return get_classes(split, self.classes_per_split)\n\n  def get_all_classes_same_example_count(self):\n    """"""If all classes have the same number of images, return that number.\n\n    Returns:\n      An int, representing the common among all dataset classes number of\n      examples, if the classes are balanced, or -1 to indicate class imbalance.\n    """"""\n\n    def list_leaf_num_images(split):\n      return [\n          self.images_per_class[split][n] for n in\n          imagenet_specification.get_leaves(self.split_subgraphs[split])\n      ]\n\n    train_example_counts = set(list_leaf_num_images(learning_spec.Split.TRAIN))\n    valid_example_counts = set(list_leaf_num_images(learning_spec.Split.VALID))\n    test_example_counts = set(list_leaf_num_images(learning_spec.Split.TEST))\n\n    is_class_balanced = (\n        len(train_example_counts) == 1 and len(valid_example_counts) == 1 and\n        len(test_example_counts) == 1 and\n        len(train_example_counts | valid_example_counts\n            | test_example_counts) == 1)\n\n    if is_class_balanced:\n      return list(train_example_counts)[0]\n    else:\n      return -1\n\n  def get_total_images_per_class(self, class_id=None, pool=None):\n    """"""Gets the number of images of class whose id is class_id.\n\n    class_id can only be None in the case where all classes of the dataset have\n    the same number of images.\n\n    Args:\n      class_id: The integer class id of a class.\n      pool: None or string, unused. Should be None because no dataset with a DAG\n        hierarchy supports example-level splits currently.\n\n    Returns:\n      An integer representing the number of images of class with id class_id.\n\n    Raises:\n      ValueError: no class_id specified yet there is class imbalance, or\n        class_id is specified but doesn\'t correspond to any class, or ""pool""\n        is provided.\n    """"""\n    if pool is not None:\n      raise ValueError(\'No dataset with a HierarchicalDataSpecification \'\n                       \'supports example-level splits (pools).\')\n\n    common_num_class_images = self.get_all_classes_same_example_count()\n    if class_id is None:\n      if common_num_class_images < 0:\n        raise ValueError(\'class_id can only be None in the case where all \'\n                         \'dataset classes have the same number of images.\')\n      return common_num_class_images\n\n    # Find the class with class_id in one of the split graphs.\n    for s in learning_spec.Split:\n      for n in self.split_subgraphs[s]:\n        # Only consider leaves, as class_names_to_ids only has keys for them.\n        if n.children:\n          continue\n        if self.class_names_to_ids[n.wn_id] == class_id:\n          return self.images_per_class[s][n]\n    raise ValueError(\'Class id {} not found.\'.format(class_id))\n\n  def to_dict(self):\n    """"""Returns a dictionary for serialization to JSON.\n\n    Each member is converted to an elementary type that can be serialized to\n    JSON readily.\n    """"""\n    # Start with the dict representation of the namedtuple\n    ret_dict = self._asdict()\n    # Add the class name for reconstruction when deserialized\n    ret_dict[\'__class__\'] = self.__class__.__name__\n    # Convert the graph for each split into a serializable form\n    split_subgraphs = {}\n    for split, subgraph in six.iteritems(ret_dict[\'split_subgraphs\']):\n      exported_subgraph = imagenet_specification.export_graph(subgraph)\n      split_subgraphs[split.name] = exported_subgraph\n    ret_dict[\'split_subgraphs\'] = split_subgraphs\n    # WordNet synsets to their WordNet ID as a string in images_per_class.\n    images_per_class = {}\n    for split, synset_counts in six.iteritems(ret_dict[\'images_per_class\']):\n      wn_id_counts = {\n          synset.wn_id: count for synset, count in six.iteritems(synset_counts)\n      }\n      images_per_class[split.name] = wn_id_counts\n    ret_dict[\'images_per_class\'] = images_per_class\n\n    return ret_dict\n\n\ndef as_dataset_spec(dct):\n  """"""Hook to `json.loads` that builds a DatasetSpecification from a dict.\n\n  Args:\n     dct: A dictionary with string keys, corresponding to a JSON file.\n\n  Returns:\n    Depending on the \'__class__\' key of the dictionary, a DatasetSpecification,\n    HierarchicalDatasetSpecification, or BiLevelDatasetSpecification. Defaults\n    to returning `dct`.\n  """"""\n  if \'__class__\' not in dct:\n    return dct\n\n  if dct[\'__class__\'] not in (\'DatasetSpecification\',\n                              \'HierarchicalDatasetSpecification\',\n                              \'BiLevelDatasetSpecification\'):\n    return dct\n\n  def _key_to_int(dct):\n    """"""Returns a new dictionary whith keys converted to ints.""""""\n    return {int(key): value for key, value in six.iteritems(dct)}\n\n  def _key_to_split(dct):\n    """"""Returns a new dictionary whith keys converted to Split enums.""""""\n    return {\n        learning_spec.Split[key]: value for key, value in six.iteritems(dct)\n    }\n\n  if dct[\'__class__\'] == \'DatasetSpecification\':\n    images_per_class = {}\n    for class_id, n_images in six.iteritems(dct[\'images_per_class\']):\n      # If n_images is a dict, it maps each class ID to a string->int\n      # dictionary containing the size of each pool.\n      if isinstance(n_images, dict):\n        # Convert the number of classes in each pool to int.\n        n_images = {\n            pool: int(pool_size) for pool, pool_size in six.iteritems(n_images)\n        }\n      else:\n        n_images = int(n_images)\n      images_per_class[int(class_id)] = n_images\n\n    return DatasetSpecification(\n        name=dct[\'name\'],\n        classes_per_split=_key_to_split(dct[\'classes_per_split\']),\n        images_per_class=images_per_class,\n        class_names=_key_to_int(dct[\'class_names\']),\n        path=dct[\'path\'],\n        file_pattern=dct[\'file_pattern\'])\n\n  elif dct[\'__class__\'] == \'BiLevelDatasetSpecification\':\n    return BiLevelDatasetSpecification(\n        name=dct[\'name\'],\n        superclasses_per_split=_key_to_split(dct[\'superclasses_per_split\']),\n        classes_per_superclass=_key_to_int(dct[\'classes_per_superclass\']),\n        images_per_class=_key_to_int(dct[\'images_per_class\']),\n        superclass_names=_key_to_int(dct[\'superclass_names\']),\n        class_names=_key_to_int(dct[\'class_names\']),\n        path=dct[\'path\'],\n        file_pattern=dct[\'file_pattern\'])\n\n  elif dct[\'__class__\'] == \'HierarchicalDatasetSpecification\':\n    # Load subgraphs associated to each split, and build global mapping from\n    # WordNet ID to Synset objects.\n    split_subgraphs = {}\n    wn_id_to_node = {}\n    for split in learning_spec.Split:\n      split_subgraphs[split] = imagenet_specification.import_graph(\n          dct[\'split_subgraphs\'][split.name])\n      for synset in split_subgraphs[split]:\n        wn_id = synset.wn_id\n        if wn_id in wn_id_to_node:\n          raise ValueError(\n              \'Multiple `Synset` objects associated to the same WordNet ID\')\n        wn_id_to_node[wn_id] = synset\n\n    images_per_class = {}\n    for split_name, wn_id_counts in six.iteritems(dct[\'images_per_class\']):\n      synset_counts = {\n          wn_id_to_node[wn_id]: int(count)\n          for wn_id, count in six.iteritems(wn_id_counts)\n      }\n      images_per_class[learning_spec.Split[split_name]] = synset_counts\n\n    return HierarchicalDatasetSpecification(\n        name=dct[\'name\'],\n        split_subgraphs=split_subgraphs,\n        images_per_class=images_per_class,\n        class_names=_key_to_int(dct[\'class_names\']),\n        path=dct[\'path\'],\n        file_pattern=dct[\'file_pattern\'])\n\n  else:\n    return dct\n\n\ndef load_dataset_spec(dataset_records_path, convert_from_pkl=False):\n  """"""Loads dataset specification from directory containing the dataset records.\n\n  Newly-generated datasets have the dataset specification serialized as JSON,\n  older ones have it as a .pkl file. If no JSON file is present and\n  `convert_from_pkl` is passed, this method will load the .pkl and serialize it\n  to JSON.\n\n  Args:\n    dataset_records_path: A string, the path to the directory containing\n      .tfrecords files and dataset_spec.\n    convert_from_pkl: A boolean (False by default), whether to convert a\n      dataset_spec.pkl file to JSON.\n\n  Returns:\n    A DatasetSpecification, BiLevelDatasetSpecification, or\n      HierarchicalDatasetSpecification, depending on the dataset.\n\n  Raises:\n    RuntimeError: If no suitable dataset_spec file is found in directory\n      (.json or .pkl depending on `convert_from_pkl`).\n  """"""\n  json_path = os.path.join(dataset_records_path, \'dataset_spec.json\')\n  pkl_path = os.path.join(dataset_records_path, \'dataset_spec.pkl\')\n  if tf.io.gfile.exists(json_path):\n    with tf.io.gfile.GFile(json_path, \'r\') as f:\n      data_spec = json.load(f, object_hook=as_dataset_spec)\n  elif tf.io.gfile.exists(pkl_path):\n    if convert_from_pkl:\n      logging.info(\'Loading older dataset_spec.pkl to convert it.\')\n      with tf.io.gfile.GFile(pkl_path, \'rb\') as f:\n        data_spec = pkl.load(f)\n      with tf.io.gfile.GFile(json_path, \'w\') as f:\n        json.dump(data_spec.to_dict(), f, indent=2)\n    else:\n      raise RuntimeError(\n          \'No dataset_spec.json file found in directory %s, but an older \'\n          \'dataset_spec.pkl was found. You can try to pass \'\n          \'`convert_from_pkl=True` to convert it, or you may need to run the \'\n          \'conversion again in order to make sure you have the latest version.\'\n          % dataset_records_path)\n  else:\n    raise RuntimeError(\'No dataset_spec file found in directory %s\' %\n                       dataset_records_path)\n\n  # Replace outdated path of where to find the dataset\'s records.\n  data_spec = data_spec._replace(path=dataset_records_path)\n  return data_spec\n'"
meta_dataset/data/decoder.py,21,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Module responsible for decoding image/feature examples.""""""\nimport gin.tf\nimport tensorflow.compat.v1 as tf\n\n\ndef read_single_example(example_string):\n  """"""Parses the record string.""""""\n  return tf.parse_single_example(\n      example_string,\n      features={\n          \'image\': tf.FixedLenFeature([], dtype=tf.string),\n          \'label\': tf.FixedLenFeature([], tf.int64)\n      })\n\n\ndef read_example_and_parse_image(example_string):\n  """"""Reads the string and decodes the image.""""""\n  parsed_example = read_single_example(example_string)\n  image_decoded = tf.image.decode_image(parsed_example[\'image\'], channels=3)\n  image_decoded.set_shape([None, None, 3])\n  parsed_example[\'image\'] = image_decoded\n  return parsed_example\n\n\n@gin.configurable\nclass ImageDecoder(object):\n  """"""Image decoder.""""""\n  out_type = tf.float32\n\n  def __init__(self, image_size=None, data_augmentation=None):\n    """"""Class constructor.\n\n    Args:\n      image_size: int, desired image size. The extracted image will be resized\n        to `[image_size, image_size]`.\n      data_augmentation: A DataAugmentation object with parameters for\n        perturbing the images.\n    """"""\n    self.image_size = image_size\n    self.data_augmentation = data_augmentation\n\n  def __call__(self, example_string):\n    """"""Processes a single example string.\n\n    Extracts and processes the image, and ignores the label. We assume that the\n    image has three channels.\n\n    Args:\n      example_string: str, an Example protocol buffer.\n\n    Returns:\n      image_rescaled: the image, resized to `image_size x image_size` and\n      rescaled to [-1, 1]. Note that Gaussian data augmentation may cause values\n      to go beyond this range.\n    """"""\n    return self.decode_with_label(example_string)[0]\n\n  def decode_with_label(self, example_string):\n    """"""Processes a single example string.\n\n    Extracts and processes the image, and ignores the label. We assume that the\n    image has three channels.\n\n    Args:\n      example_string: str, an Example protocol buffer.\n\n    Returns:\n      image_rescaled: the image, resized to `image_size x image_size` and\n        rescaled to [-1, 1]. Note that Gaussian data augmentation may cause\n        values to go beyond this range.\n      label: tf.int\n    """"""\n    ex_decoded = read_example_and_parse_image(example_string)\n    image_decoded = ex_decoded[\'image\']\n    image_resized = tf.image.resize_images(\n        image_decoded, [self.image_size, self.image_size],\n        method=tf.image.ResizeMethod.BILINEAR,\n        align_corners=True)\n    image_resized = tf.cast(image_resized, tf.float32)\n    image = 2 * (image_resized / 255.0 - 0.5)  # Rescale to [-1, 1].\n\n    if self.data_augmentation is not None:\n      if self.data_augmentation.enable_gaussian_noise:\n        image = image + tf.random_normal(\n            tf.shape(image)) * self.data_augmentation.gaussian_noise_std\n\n      if self.data_augmentation.enable_jitter:\n        j = self.data_augmentation.jitter_amount\n        paddings = tf.constant([[j, j], [j, j], [0, 0]])\n        image = tf.pad(image, paddings, \'REFLECT\')\n        image = tf.image.random_crop(image,\n                                     [self.image_size, self.image_size, 3])\n    return image, tf.cast(ex_decoded[\'label\'], dtype=tf.int32)\n\n\n@gin.configurable\nclass FeatureDecoder(object):\n  """"""Feature decoder.""""""\n  out_type = tf.float32\n\n  def __init__(self, feat_len):\n    """"""Class constructor.\n\n    Args:\n      feat_len: The expected length of the feature vectors.\n    """"""\n\n    self.feat_len = feat_len\n\n  def __call__(self, example_string):\n    """"""Processes a single example string.\n\n    Extracts and processes the feature, and ignores the label.\n\n    Args:\n      example_string: str, an Example protocol buffer.\n\n    Returns:\n      feat: The feature tensor.\n    """"""\n    feat = tf.parse_single_example(\n        example_string,\n        features={\n            \'image/embedding\':\n                tf.FixedLenFeature([self.feat_len], dtype=tf.float32),\n            \'image/class/label\':\n                tf.FixedLenFeature([], tf.int64)\n        })[\'image/embedding\']\n\n    return feat\n\n\n@gin.configurable\nclass StringDecoder(object):\n  """"""Simple decoder that reads the image without decoding.""""""\n  out_type = tf.string\n\n  def __init__(self):\n    """"""Class constructor.""""""\n\n  def __call__(self, example_string):\n    """"""Processes a single example string.\n\n    Extracts the image as string, and ignores the label.\n\n    Args:\n      example_string: str, an Example protocol buffer.\n\n    Returns:\n      img_string: tf.Tensor of type tf.string.\n    """"""\n    img_string = read_single_example(example_string)[\'image\']\n    return img_string\n'"
meta_dataset/data/decoder_test.py,3,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Tests for meta_dataset.data.decoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom meta_dataset.data import decoder\nfrom meta_dataset.dataset_conversion import dataset_to_records\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\n\nclass DecoderTest(tf.test.TestCase):\n\n  def test_string_decoder(self):\n    # Make random image.\n    image_size = 32\n    image = np.random.randint(\n        low=0, high=255, size=[image_size, image_size, 3]).astype(np.ubyte)\n\n    # Encode\n    image_bytes = dataset_to_records.encode_image(image, image_format=\'PNG\')\n    label = np.zeros(1).astype(np.int64)\n    image_example = dataset_to_records.make_example([\n        (\'image\', \'bytes\', [image_bytes]), (\'label\', \'int64\', [label])\n    ])\n\n    # Decode\n    string_decoder = decoder.StringDecoder()\n    image_string = string_decoder(image_example)\n    decoded_image = tf.image.decode_image(image_string)\n    # Assert perfect reconstruction.\n    with self.session(use_gpu=False) as sess:\n      image_decoded = sess.run(decoded_image)\n    self.assertAllClose(image, image_decoded)\n\n  def test_image_decoder(self):\n    # Make random image.\n    image_size = 84\n    image = np.random.randint(\n        low=0, high=255, size=[image_size, image_size, 3]).astype(np.ubyte)\n\n    # Encode\n    image_bytes = dataset_to_records.encode_image(image, image_format=\'PNG\')\n    label = np.zeros(1).astype(np.int64)\n    image_example = dataset_to_records.make_example([\n        (\'image\', \'bytes\', [image_bytes]), (\'label\', \'int64\', [label])\n    ])\n\n    # Decode\n    image_decoder = decoder.ImageDecoder(image_size=image_size)\n    image_decoded = image_decoder(image_example)\n    # Assert perfect reconstruction.\n    with self.session(use_gpu=False) as sess:\n      image_rec_numpy = sess.run(image_decoded)\n    self.assertAllClose(2 * (image.astype(np.float32) / 255.0 - 0.5),\n                        image_rec_numpy)\n\n  def test_feature_decoder(self):\n    # Make random feature.\n    feat_size = 64\n    feat = np.random.randn(feat_size).astype(np.float32)\n    label = np.zeros(1).astype(np.int64)\n\n    # Encode\n    feat_example = dataset_to_records.make_example([\n        (\'image/embedding\', \'float32\', feat),\n        (\'image/class/label\', \'int64\', [label]),\n    ])\n\n    # Decode\n    feat_decoder = decoder.FeatureDecoder(feat_len=feat_size)\n    feat_decoded = feat_decoder(feat_example)\n\n    # Assert perfect reconstruction.\n    with self.session(use_gpu=False) as sess:\n      feat_rec_numpy = sess.run(feat_decoded)\n    self.assertAllEqual(feat_rec_numpy, feat)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
meta_dataset/data/imagenet_specification.py,12,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Prepares the ILSVRC2012 subset of ImageNet for integration in the benchmark.\n\nThis requires creating a data structure to navigate the subset of the ontology\nof ImageNet that is relevant. This data structure is required both for creating\nclass splits in a hierarchy-aware manner and also for the episode generation\nalgorithm. This algorithm as well as more context about handling hierarchy and\nImageNet\'s ontology in particular is described in the article.\n""""""\n# TODO(manzagop): relocate the code pertaining to imagenet ingestion to\n# dataset_conversion. The code dealing with sampling from a tree should stay.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\n\nfrom absl import logging\nfrom meta_dataset.data import imagenet_stats\nimport numpy as np\nimport six\nimport tensorflow.compat.v1 as tf\n\ntf.flags.DEFINE_string(\n    \'ilsvrc_2012_data_root\',\n    \'\',\n    \'Path to the root of the ImageNet data.\')\n\ntf.flags.DEFINE_string(\n    \'path_to_is_a\',\n    \'\',\n    \'Path to the file containing is-a relationships (parent, child) pairs. \'\n    \'If empty, it defaults to ""wordnet.is_a.txt"" in ilsvrc_2012_data_root.\')\n\ntf.flags.DEFINE_string(\n    \'path_to_words\',\n    \'\',\n    \'Path to the file containing (synset, word description) pairs. \'\n    \'If empty, it defaults to ""words.txt"" in ilsvrc_2012_data_root.\')\n\nFLAGS = tf.flags.FLAGS\n\n\nclass Synset(object):\n  """"""A Synset object.""""""\n\n  def __init__(self, wn_id, words, children, parents):\n    """"""Initialize a Synset.\n\n    Args:\n      wn_id: WordNet id\n      words: word description of the synset\n      children: a set of children Synsets\n      parents: a set of parent Synsets\n    """"""\n    self.wn_id = wn_id\n    self.words = words\n    self.children = children\n    self.parents = parents\n\n\ndef get_node_ancestors(synset):\n  """"""Create a set consisting of all and only the ancestors of synset.\n\n  Args:\n    synset: A Synset.\n\n  Returns:\n    ancestors: A set of Synsets\n  """"""\n  ancestors = set()\n  # In the following line, synset.parents already is a set but we create a copy\n  # of it instead of using synset.parents directly as later we are \'popping\'\n  # elements from this set, which would otherwise result to permanently removing\n  # parents of synset which is undesirable.\n  to_visit = set(synset.parents)\n  visited = set()\n  while to_visit:\n    ancestor = to_visit.pop()\n    ancestors.add(ancestor)\n    visited.add(ancestor)\n    # Same as in the comment above, we create a copy of ancestor.parents\n    to_visit = to_visit | set(ancestor.parents) - visited\n  return ancestors\n\n\ndef get_ancestors(synsets):\n  """"""Create a set consisting of all and only the ancestors of leaves.\n\n  Args:\n    synsets: A list of Synsets.\n\n  Returns:\n    A set of Synsets.\n  """"""\n  all_ancestors = set()\n  for s in synsets:\n    all_ancestors = all_ancestors | get_node_ancestors(s)\n  return all_ancestors\n\n\ndef isolate_graph(nodes):\n  """"""Remove links between Synsets in nodes and Synsets that are not in nodes.\n\n  This effectively isolates the graph defined by nodes from the rest of the\n  Synsets. The resulting set of nodes is such that following any of their child/\n  parent pointers can only lead to other Synsets of the same set of nodes.\n  This requires breaking the necessary parent / child links.\n\n  Args:\n    nodes: A set of Synsets\n  """"""\n  for n in nodes:\n    n.children = list(nodes & set(n.children))\n    n.parents = list(nodes & set(n.parents))\n\n\ndef isolate_node(node):\n  """"""Isolate node from its children and parents by breaking those links.""""""\n  for p in node.parents:\n    p.children.remove(node)\n  for c in node.children:\n    c.parents.remove(node)\n  node.children = []\n  node.parents = []\n\n\ndef collapse(nodes):\n  """"""Collapse any nodes that only have a single child.\n\n  Collapsing of a node is done by removing that node and attaching its child to\n  its parent(s).\n\n  Args:\n    nodes: A set of Synsets.\n\n  Returns:\n    A set containing the Synsets in nodes that were not collapsed, with\n    potentially modified children and parents lists due to collapsing other\n    synsets.\n  """"""\n\n  def collapse_once(nodes):\n    """"""Perform a pass of the collapsing as described above.""""""\n    num_collapsed = 0\n    non_collapsed_nodes = set()\n    for n in nodes:\n      if len(n.children) == 1:\n        # attach the only child to all of n\'s parents\n        n.children[0].parents += n.parents\n        for p in n.parents:\n          p.children.append(n.children[0])\n\n        # Remove all connections to and from n\n        isolate_node(n)\n\n        num_collapsed += 1\n      else:\n        non_collapsed_nodes.add(n)\n\n    assert len(nodes) - len(non_collapsed_nodes) == num_collapsed\n    return non_collapsed_nodes, num_collapsed\n\n  nodes, num_collapsed = collapse_once(nodes)\n  while num_collapsed:\n    nodes, num_collapsed = collapse_once(nodes)\n  return nodes\n\n\ndef get_leaves(nodes):\n  """"""Return a list containing the leaves of the graph defined by nodes.""""""\n  leaves = []\n  for n in nodes:\n    if not n.children:\n      leaves.append(n)\n  return leaves\n\n\ndef get_synsets_from_ids(wn_ids, synsets):\n  """"""Finds the Synsets in synsets whose WordNet id\'s are in wn_ids.\n\n  Args:\n    wn_ids: A list of WordNet id\'s.\n    synsets: A set of Synsets.\n\n  Returns:\n    A dict mapping each WordNet id in wn_ids to the corresponding Synset.\n  """"""\n  wn_ids = set(wn_ids)\n  requested_synsets = {}\n  for s in synsets:\n    if s.wn_id in wn_ids:\n      requested_synsets[s.wn_id] = s\n\n  found = set(requested_synsets.keys())\n  assert found == wn_ids, (\'Did not find synsets for ids: {}.\'.format(wn_ids -\n                                                                      found))\n  return requested_synsets\n\n\ndef get_spanning_leaves(nodes):\n  """"""Get the leaves that each node in nodes can reach.\n\n  The number of leaves that a node can reach, i.e. that a node \'spans\', provides\n  an estimate of how \'high\' in the DAG that node is, with nodes representing\n  more general concepts spanning more leaves and being \'higher\' than nodes\n  representing more specific concepts.\n  A leaf node spans exactly one leaf: itself.\n\n  Args:\n    nodes: A set of Synsets\n\n  Returns:\n    spanning_leaves: a dict mapping Synset instances to the set of leaf Synsets\n      that are their descendants.\n  """"""\n  # First find the leaves\n  leaves = get_leaves(nodes)\n\n  # dict mapping WordNet id\'s to the list of leaf Synsets they span\n  spanning_leaves = {}\n  for n in nodes:\n    spanning_leaves[n] = set()\n    for l in leaves:\n      if is_descendent(l, n) or l == n:\n        spanning_leaves[n].add(l)\n  return spanning_leaves\n\n\ndef get_num_spanning_images(spanning_leaves, num_leaf_images):\n  """"""Create a dict mapping each node to the number of images in its sub-graph.\n\n  This assumes that all images live in the leaves of the graph (as is the case\n  in our graph by construction: we are only interested in using images from the\n  ILSVRC 2012 synsets, and these synsets are all and only the leaves of the\n  sampling graph).\n\n  Args:\n    spanning_leaves: a dict mapping each node to the set of leaves it spans.\n    num_leaf_images: a dict mapping each leaf synset to its number of images.\n\n  Returns:\n    num_images: a dict that maps each node in the sampling graph to the number\n      of images in the leaves that it spans.\n  """"""\n  num_images = {}\n  for node, leaves in spanning_leaves.items():\n    num_images[node] = sum([num_leaf_images[l.wn_id] for l in leaves])\n  return num_images\n\n\ndef create_sampling_graph(synsets, root=None):\n  """"""Create a DAG that only contains synsets and all of their ancestors.\n\n  By construction, the leaves of this graph are all and only the Synsets in the\n  synsets list. The internal nodes of the graph are all and only the ancestors\n  of synsets. All children/parent pointers of the graph nodes are restricted to\n  only lead to other nodes that also belong to the ancestor graph. Finally,\n  appropriate collapsing is performed so that no node has only one child (since\n  it\'s not possible to create an episode from that node).\n\n  Args:\n    synsets: A list of Synsets\n    root: Optionally, a Synset. If provided, it imposes a restriction on which\n      ancestors of the given synsets will be included in the sampling graph.\n      Specifically, an ancestor of a Synset in synsets in will be included only\n      if it is the root or a descendent of the root. This is useful when\n      creating the validation and test sub- graphs, where we want the designated\n      root to indeed have to upward connections in the corresponding subgraph.\n\n  Returns:\n    A set of the Synsets of the DAG.\n  """"""\n  # Get the set of Synsets containing all and only the ancestors of synsets.\n  nodes = get_ancestors(synsets)\n\n  if root is not None:\n    # Remove from the ancestors nodes that aren\'t the root or descendents of it.\n    nodes_to_remove = [\n        n for n in nodes if not (root == n or is_descendent(n, root))\n    ]\n    nodes = nodes - set(nodes_to_remove)\n\n  # The total nodes of the graph are the nodes in synsets, and their ancestors.\n  nodes = nodes | set(synsets)\n\n  # Remove all connections from nodes of the graph to other nodes.\n  isolate_graph(nodes)\n\n  # Remove all nodes with only one child and attach that child to their parents.\n  nodes = collapse(nodes)\n  return nodes\n\n\ndef propose_valid_test_roots(spanning_leaves,\n                             margin=50,\n                             desired_num_valid_classes=150,\n                             desired_num_test_classes=150):\n  """"""Propose roots for the validation and test sub-graphs.\n\n  This is done as follows: each subgraph root will be the Synset that spans the\n  largest number of leaves that still meets the criterion of spanning a number\n  of leaves within its allowable range, which is: desired number of classes for\n  that split +/- margin. We aim to include approx. 70% / 15% / 15% of the\n  classes in the training / validation / testing splits, resp.\n\n  Args:\n    spanning_leaves: A dict mapping each Synset to the leaf Synsets that are\n      reachable from it.\n    margin: The number of additional or fewer leaves that the root of a split\'s\n      subgraph can span compared to the expected number of classes for the\n      corresponding split. This is needed for this splitting method, as there\n      may not be a node in the tree that spans exactly the expected number of\n      classes for some split.\n    desired_num_valid_classes: num classes desirably assigned to the validation\n      split. ILSVRC 2012 has a total of 1000 classes, so 15% corresponds to 150\n      classes, hence the default value of 150.\n    desired_num_test_classes: similarly as above, but for the test split.\n\n  Returns:\n    a dict that maps \'valid\' and \'test\' to the synset that spans the leaves that\n      will desirably be assigned to that split.\n\n  Raises:\n    RuntimeError: When no candidate subgraph roots are available with the given\n      margin value.\n  """"""\n  # Sort in decreasing order of the length of the lists of spanning leaves, so\n  # e.g. the node that spans the most leaves will be the first element.  Ties\n  # are broken by the WordNet ID of the Synset.\n  def _sort_key(synset_and_leaves):\n    synset, leaves = synset_and_leaves\n    return (len(leaves), synset.wn_id)\n\n  spanning_leaves_sorted = sorted(six.iteritems(spanning_leaves), key=_sort_key)\n  spanning_leaves_sorted.reverse()\n\n  # Get the candidate roots for the validation and test sub-graphs, by finding\n  # the nodes whose number of spanning leaves are within the\n  # allowed margin.\n  valid_candidates, test_candidates = [], []\n  for s, leaves in spanning_leaves_sorted:\n    num_leaves = len(leaves)\n    low_limit_valid = desired_num_valid_classes - margin\n    high_limit_valid = desired_num_valid_classes + margin\n    if low_limit_valid < num_leaves and num_leaves < high_limit_valid:\n      valid_candidates.append(s)\n    low_limit_test = desired_num_test_classes - margin\n    high_limit_test = desired_num_test_classes + margin\n    if low_limit_test < num_leaves and num_leaves < high_limit_test:\n      test_candidates.append(s)\n\n  if not valid_candidates or not test_candidates:\n    raise RuntimeError(\'Found no root candidates. Try a different margin.\')\n\n  # For displaying the list of candidates\n  for cand in valid_candidates:\n    logging.info(\'Candidate %s, %s with %d spanning leaves\', cand.words,\n                 cand.wn_id, len(spanning_leaves[cand]))\n\n  # Propose the first possible candidate for each of validation and test\n  valid_root = valid_candidates[0]\n  # Make sure not to choose the same root for testing as for validation\n  test_candidate_ind = 0\n  test_root = test_candidates[test_candidate_ind]\n  while test_root == valid_root:\n    test_candidate_ind += 1\n    if test_candidate_ind == len(test_candidates):\n      raise RuntimeError(\'No candidates for test root. Try a different margin.\')\n    test_root = test_candidates[test_candidate_ind]\n\n  return {\'valid\': valid_root, \'test\': test_root}\n\n\ndef get_class_splits(spanning_leaves, valid_test_roots=None, **kwargs):\n  """"""Gets the assignment of classes (graph leaves) to splits.\n\n  First, if valid_test_roots is not provided, roots for the validation and test\n  sub-graphs are proposed by calling propose_valid_test_roots.\n\n  Then, all classes spanned by the valid root Synset will be assigned to the\n  validation set, all classes spanned by test root Synset will be assigned to\n  the test split, and all remaining classes to the training split. When there\n  are leaves spanned by both validation and test, they are assigned to one of\n  the two randomly (roughly equally).\n\n  Args:\n    spanning_leaves: A dict mapping each Synset to the leaf Synsets that are\n      reachable from it.\n    valid_test_roots: A dict whose keys should be \'valid\' and \'test\' and whose\n      value for a given key is a Synset that spans all and only the leaves that\n      will desirably be assigned to the corresponding split.\n    **kwargs: Keyword arguments for the root proposer that is used if\n      valid_test_roots is None.\n\n  Returns:\n    split_classes: A dict that maps each of \'train\', \'valid\' and \'test\' to the\n      set of WordNet id\'s of the classes for the corresponding split.\n    valid_test_roots: A dict of the same form as the corresponding optional\n      argument.\n\n  Raises:\n    ValueError: when the provided valid_test_roots are invalid.\n  """"""\n  if valid_test_roots is not None:\n    if valid_test_roots[\'valid\'] is None or valid_test_roots[\'test\'] is None:\n      raise ValueError(\'A root cannot be None.\')\n\n  if valid_test_roots is None:\n    valid_test_roots = propose_valid_test_roots(spanning_leaves, **kwargs)\n\n  valid_root, test_root = valid_test_roots[\'valid\'], valid_test_roots[\'test\']\n\n  # The WordNet id\'s of the validation and test classes\n  valid_wn_ids = set([s.wn_id for s in spanning_leaves[valid_root]])\n  test_wn_ids = set([s.wn_id for s in spanning_leaves[test_root]])\n\n  # There may be overlap between the spanning leaves of the chosen roots for\n  # the validation and test subtrees, which would cause overlap between the\n  # classes assigned to these splits. This is addressed below by randomly\n  # assigning each overlapping leaf to either validation or test classes\n  # (roughly equally).\n  overlap = [s for s in valid_wn_ids if s in test_wn_ids]\n  logging.info(\'Size of overlap: %d leaves\', len(overlap))\n  assign_to_valid = True\n  for s in overlap:\n    if assign_to_valid:\n      test_wn_ids.remove(s)\n    else:\n      valid_wn_ids.remove(s)\n    assign_to_valid = not assign_to_valid\n\n  # Training classes are all the remaining ones that are not already assigned\n  leaves = get_leaves(spanning_leaves.keys())\n  train_wn_ids = set([\n      s.wn_id\n      for s in leaves\n      if s.wn_id not in valid_wn_ids and s.wn_id not in test_wn_ids\n  ])\n\n  split_classes = {\n      \'train\': train_wn_ids,\n      \'valid\': valid_wn_ids,\n      \'test\': test_wn_ids\n  }\n  return split_classes, valid_test_roots\n\n\ndef init_split_subgraphs(class_splits, spanning_leaves, valid_test_roots):\n  """"""Gets leaf and root Synsets from different copies of the graph.\n\n  In particular, a new copy is created for each split. For all three splits, the\n  leaf Synsets of the corresponding copy that correspond to split classes are\n  returned. For the validation and test graphs, the corresponding root Synsets\n  are returned as well from the new copies. These will have the same WordNet id\n  and name as those in valid_test_roots but are nodes from the copy of the graph\n  instead of the original one.\n\n  Args:\n    class_splits: a dict whose keys are \'train\', \'valid\' and \'test\' and whose\n      value for a given key is the set of WordNet id\'s of the classes that are\n      assigned to the corresponding split.\n    spanning_leaves: A dict mapping each Synset to the leaf Synsets that are\n      reachable from it.\n    valid_test_roots: A dict whose keys should be \'valid\' and \'test\' and whose\n      value for a given key is a Synset that spans all and only the leaves that\n      will desirably be assigned to the corresponding split.\n\n  Returns:\n    a dict mapping each of \'train\', \'valid\' and \'test\' to the set of Synsets (of\n    the respective copy of the graph) corresponding to the classes that are\n    assigned to that split.\n\n  Raises:\n    ValueError: invalid keys for valid_test_roots, or same synset provided as\n      the root of both valid and test.\n  """"""\n  # Get the wn_id\'s of the train, valid and test classes.\n  train_wn_ids = class_splits[\'train\']\n  valid_wn_ids = class_splits[\'valid\']\n  test_wn_ids = class_splits[\'test\']\n\n  valid_root_wn_id = valid_test_roots[\'valid\'].wn_id\n  test_root_wn_id = valid_test_roots[\'test\'].wn_id\n\n  # Get 3 full copies of the graph that will be modified downstream.\n  graph_copy_train, _ = copy_graph(spanning_leaves.keys())\n  graph_copy_valid, valid_root = copy_graph(spanning_leaves.keys(),\n                                            valid_root_wn_id)\n  graph_copy_test, test_root = copy_graph(spanning_leaves.keys(),\n                                          test_root_wn_id)\n\n  # Get the nodes of each copy that correspond to the splits\' assigned classes.\n  train_classes = set([s for s in graph_copy_train if s.wn_id in train_wn_ids])\n  valid_classes = set([s for s in graph_copy_valid if s.wn_id in valid_wn_ids])\n  test_classes = set([s for s in graph_copy_test if s.wn_id in test_wn_ids])\n  split_leaves = {\n      \'train\': train_classes,\n      \'valid\': valid_classes,\n      \'test\': test_classes\n  }\n  split_roots = {\'valid\': valid_root, \'test\': test_root}\n  return split_leaves, split_roots\n\n\ndef copy_graph(nodes, root_wn_id=None):\n  """"""Create a set of Synsets that are copies of the Synsets in nodes.\n\n  A new Synset is created for each Synset of nodes and then the\n  children/parent relationships of the new Synsets are set to mirror the\n  corresponding ones in the Synsets of nodes.\n\n  This assumes that nodes is an \'isolated\' graph: all parents and\n  children of nodes of nodes also belong to the graph.\n\n  Optionally, if the WordNet id of a node is provided, the copy of that node\n  will be returned.\n\n  Args:\n    nodes: A set of Synsets.\n    root_wn_id: The wn_id field of the Synset that is intended to eventually be\n      the root of the new graph.\n\n  Returns:\n    copy: A set of Synsets of the same size as nodes.\n  """"""\n  root_copy = None\n  copy = {}  # maps wn_id\'s to Synsets\n  parent_child_tuples = set()\n\n  for s in nodes:\n    copy[s.wn_id] = Synset(s.wn_id, s.words, set(), set())\n    if root_wn_id is not None and s.wn_id == root_wn_id:\n      root_copy = copy[s.wn_id]\n    for c in s.children:\n      assert c in nodes\n      parent_child_tuples.add((s.wn_id, c.wn_id))\n\n  # Add the analogous parent/child relations between nodes in\n  # copy as those that existed in nodes\n  for parent, child in parent_child_tuples:\n    copy[parent].children.add(copy[child])\n    copy[child].parents.add(copy[parent])\n\n  return set(copy.values()), root_copy\n\n\ndef create_splits(spanning_leaves, split_enum, valid_test_roots=None, **kwargs):\n  """"""Split the classes of ILSVRC 2012 into train / valid / test.\n\n  Each split will be represented as a sub-graph of the overall sampling graph.\n  The leaves of a split\'s sub-graph are the ILSVRC 2012 synsets that are\n  assigned to that split, and its internal nodes are all and only the ancestors\n  of those leaves. Each split\'s subgraph is \'isolated\' from the rest of the\n  synsets in that following pointers of nodes in that sub-graph is guaranteed\n  to lead to other nodes within in.\n\n  If valid_test_roots is not None, it should contain two Synsets, that are the\n  proposed roots of the validation and test subtrees. Otherwise, a proposal for\n  these two roots is made in get_class_splits.\n\n  Args:\n    spanning_leaves: A dict mapping each Synset to the leaf Synsets that are\n      reachable from it.\n    split_enum: A class that inherits from enum.Enum whose attributes are TRAIN,\n      VALID, and TEST, which are mapped to enumerated constants.\n    valid_test_roots: dict that provides for each of \'valid\' and \'test\' a synset\n      that is the ancestor of all and only the leaves that will be assigned to\n      the corresponding split.\n    **kwargs: keyword args for the function used to propose valid_test_roots,\n      which will be called if split_classes is empty and no valid_test_roots are\n      provided.\n\n  Returns:\n    splits: a dict mapping each Split in split_enum to the set of Synsets in the\n      subgraph of that split. This is different from the split_classes dict,\n      which contained lists of only the leaves of the corresponding graphs.\n    roots: a dict of the same type as valid_test_roots. If it was provided, it\n      is returned unchanged. Otherwise the newly created one is returned.\n  """"""\n  # The classes (leaf Synsets of the overall graph) of each split.\n  split_classes, valid_test_roots = get_class_splits(\n      spanning_leaves, valid_test_roots=valid_test_roots, **kwargs)\n\n  # The copies of the leaf and desired root Synsets for each split. Copies are\n  # needed since in each sub-graph those nodes will have different children /\n  # parent lists.\n  leaves, roots = init_split_subgraphs(split_classes, spanning_leaves,\n                                       valid_test_roots)\n\n  # Create the split sub-graphs as described above.\n  train_graph = create_sampling_graph(leaves[\'train\'])\n  valid_graph = create_sampling_graph(leaves[\'valid\'], root=roots[\'valid\'])\n  test_graph = create_sampling_graph(leaves[\'test\'], root=roots[\'test\'])\n  split_graphs = {\n      split_enum.TRAIN: train_graph,\n      split_enum.VALID: valid_graph,\n      split_enum.TEST: test_graph\n  }\n  return split_graphs, roots\n\n\ndef get_synset_by_wnid(wnid, graph):\n  """"""Return the synset of sampling_graph whose WordNet id is wnid.""""""\n  for n in graph:\n    if n.wn_id == wnid:\n      return n\n  return None\n\n\ndef is_descendent(d, a):\n  """"""Returns whether d is a descendent of a.\n\n  A node is not considered a descendent of itself.\n\n  Args:\n    d: A Synset.\n    a: A Synset.\n  """"""\n  paths = get_upward_paths_from(d, end=a)\n  # The second clause ensures that a node is not a descendent of itself (our\n  # graphs are DAGs so this suffices to enforce this).\n  return len(paths) and not (len(paths) == 1 and len(paths[0]) == 1)\n\n\ndef get_upward_paths_from(start, end=None):\n  """"""Creates a list of paths that go from start either to end or to a root.\n\n  There may be more than one such paths, since the structure we are traversing\n  is a DAG (not strictly a tree). Every path is represented as a list of Synsets\n  whose first elements is a Synset without parents and whose last element is s.\n\n  Args:\n    start: A Synset.\n    end: A Synset. If not provided, the end point will be the first node that is\n      encountered starting from start that does not have parents.\n\n  Returns:\n    A list of lists, containing all paths as described above.\n  """"""\n\n  def is_end_node(n):\n    return (end is not None and n == end) or (end is None and not n.parents)\n\n  if end is not None and not start.parents:\n    # There are no upwards paths from start in which the specified end can be.\n    return []\n\n  if is_end_node(start):\n    return [[start]]\n\n  # If we got here, we haven\'t yet reached the target node and there are upward\n  # paths to explore.\n  parents = start.parents\n\n  # A list of all paths from start to end (or to a root node).\n  paths = []\n\n  # Case where end is a direct parent of start:\n  for p in parents:\n    if is_end_node(p):\n      # Found one path from start to end.\n      paths.append([start, p])\n\n    else:\n      # Get a list of lists corresponding to paths between p and end.\n      p_to_end_paths = get_upward_paths_from(p, end=end)\n      if not p_to_end_paths:  # end not an ancestor of p.\n        continue\n      start_to_end_paths = [[start] + p_path for p_path in p_to_end_paths]\n      paths.extend(start_to_end_paths)\n  return paths\n\n\ndef find_lowest_common_in_paths(path_a, path_b):\n  """"""Find the element with the smallest height that appears in both given lists.\n\n  The height of an element here is defined as the maximum over the indices where\n  it occurs in the two lists. For example if path_a = [2, 3, 5] and\n  path_b = [5, 6, 2] then the height of element 2 is max(0 + 2) = 2 since the\n  element 2 occurs in position 0 in the first list and position 2 in the second.\n\n  Args:\n    path_a: A list.\n    path_b: A list.\n\n  Returns:\n    lowest_common: The element with the smallest \'height\' that is common between\n      path_a and path_b.\n    height: The height of lowest_common, computed as described above.\n  """"""\n  # Maps elements that appear in both lists to their heights.\n  common_elements, heights = [], []\n  for element in path_a:\n    if element in path_b:\n      height = max(path_a.index(element), path_b.index(element))\n      common_elements.append(element)\n      heights.append(height)\n\n  if not heights:\n    raise ValueError(\'No common nodes in given paths {} and {}.\'.format(\n        [n.words for n in path_a], [n.words for n in path_b]))\n\n  # Find the lowest common element.\n  # There may be multiple common ancestors that share the same minimal height.\n  # In that case the first one appearing in common_elements will be returned.\n  min_height = min(heights)\n  argmin_height = heights.index(min_height)\n  lowest_common = common_elements[argmin_height]\n  assert min_height > 0, (\'The lowest common ancestor between two distinct \'\n                          \'leaves cannot be a leaf.\')\n  return lowest_common, min_height\n\n\ndef get_lowest_common_ancestor(leaf_a, leaf_b, path=\'longest\'):\n  """"""Finds the lowest common ancestor of two leaves and its height.\n\n  The height of a node here is defined as the maximum distance between that node\n  and any of the leaves it spans.\n\n  When there are multiple paths starting from a given leaf (due to it possibly\n  having multiple parents), we rely on the value of path to choose which one to\n  use. By default, we use the path whose length to the root is the longest. We\n  find the lowest common ancestor of the two given leaves along the longest such\n  path for each. Alternatively, all paths can be used in which case the minimum\n  LCA over all is returned.\n\n  Args:\n    leaf_a: A Synset.\n    leaf_b: A Synset.\n    path: A str. One of \'longest\', or \'all\'.\n\n  Returns:\n    lca: A Synset. The lowest common ancestor.\n    height_of_lca: An int. The height of the lowest common ancestor.\n\n  Raises:\n    ValueError: Invalid path. Must be \'longest\', or \'all\'.\n  """"""\n  if path not in [\'longest\', \'all\']:\n    raise ValueError(\'Invalid path. Must be ""longest"", or ""all"".\')\n\n  # A list of paths from a each leaf to the root.\n  paths_a = get_upward_paths_from(leaf_a)\n  paths_b = get_upward_paths_from(leaf_b)\n\n  # Each element in paths_a is a path starting from leaf_a and ending at the\n  # root (and analogously for paths_b). We pick the longest path of each list of\n  # paths and find the lowest common ancestor between those two paths.\n  if path == \'longest\':\n    path_a = paths_a[np.argmax([len(path_a) for path_a in paths_a])]\n    path_b = paths_b[np.argmax([len(path_b) for path_b in paths_b])]\n    lca, height_of_lca = find_lowest_common_in_paths(path_a, path_b)\n\n  else:\n    # Search for the LCA across all possible paths from the given leaves.\n    lca, height_of_lca = None, None\n    for path_a in paths_a:\n      for path_b in paths_b:\n        lca_candidate, height = find_lowest_common_in_paths(path_a, path_b)\n        if height_of_lca is None or height < height_of_lca:\n          lca = lca_candidate\n          height_of_lca = height\n\n  return lca, height_of_lca\n\n\ndef get_num_synset_2012_images(path, synsets_2012, files_to_skip=None):\n  """"""Count the number of images of each class in ILSVRC 2012.\n\n  Returns a dict mapping the WordNet of each class of ILSVRC 2012 to the\n  number of its images.\n  This assumes that within FLAGS.ilsvrc_2012_data_root there is a directory for\n  every 2012 synset, named by that synset\'s WordNet ID (e.g. n15075141) and\n  containing all images of that synset.\n\n  If path contains this dict, it is read and returned, otherwise it is computed\n  and stored at path.\n\n  Args:\n    path: An optional path to a cache where the computed dict is / may be\n      stored.\n    synsets_2012: A list of Synsets.\n    files_to_skip: A set with the files that repeat in other datasets.\n\n  Returns:\n    a dict mapping the WordNet id of each ILSVRC 2012 class to its number of\n    images.\n  """"""\n  if path:\n    logging.info(\'Attempting to read number of leaf images from %s...\', path)\n    if tf.io.gfile.exists(path):\n      with tf.io.gfile.GFile(path, \'r\') as f:\n        num_synset_2012_images = json.load(f)\n        logging.info(\'Successful.\')\n        return num_synset_2012_images\n\n  logging.info(\'Unsuccessful. Deriving number of leaf images...\')\n  if files_to_skip is None:\n    files_to_skip = set()\n  num_synset_2012_images = {}\n  for s_2012 in synsets_2012:\n    synset_dir = os.path.join(FLAGS.ilsvrc_2012_data_root, s_2012.wn_id)\n    all_files = set(tf.io.gfile.listdir(synset_dir))\n    img_files = set([f for f in all_files if f.lower().endswith(\'jpeg\')])\n    final_files = img_files - files_to_skip\n    skipped_files = all_files - final_files\n    if skipped_files:\n      logging.info(\'Synset: %s, files_skipped: %s\', s_2012.wn_id, skipped_files)\n    # Size of the set difference (-) between listed files and `files_to_skip`.\n    num_synset_2012_images[s_2012.wn_id] = len(final_files)\n\n  if path:\n    with tf.io.gfile.GFile(path, \'w\') as f:\n      json.dump(num_synset_2012_images, f, indent=2)\n\n  return num_synset_2012_images\n\n\ndef export_graph(nodes):\n  """"""Returns a JSON-serializable representation of a graph.\n\n  Synset objects are represented by a dictionary containing:\n  - their WordNet ID (""wn_id"")\n  - their text description in words (""words"")\n  - the WordNet IDs of their children (""children_ids"")\n  - the WordNet IDs of their parents (""parents_ids"")\n\n  The conversion expects that:\n  - The WordNet ID is a unique identifier for a Synset object.\n  - The parents and children of each Synset in `nodes` is also in `nodes`.\n\n  Args:\n    nodes: A set of Synset objects, representing a complete graph.\n\n  Returns:\n    A list of dictionaries, following the representation described above.\n  """"""\n  node_representations = []\n  wn_ids_to_synsets = {synset.wn_id: synset for synset in nodes}\n  wn_ids = set(wn_ids_to_synsets.keys())\n  if len(wn_ids) != len(nodes):\n    raise ValueError(\'Duplicate WordNet IDs in the same graph\')\n  # Iterate in lexicographic order over the WordNet IDs\n  for wn_id in sorted(wn_ids):\n    synset = wn_ids_to_synsets[wn_id]\n    children_ids = {child.wn_id for child in synset.children}\n    if not children_ids.issubset(wn_ids):\n      raise ValueError(\'Synset has children outside of the graph\')\n    parents_ids = {parent.wn_id for parent in synset.parents}\n    if not parents_ids.issubset(wn_ids):\n      raise ValueError(\'Synset has parents outside of the graph\')\n    node_repr = dict(\n        wn_id=wn_id,\n        words=synset.words,\n        children_ids=sorted(children_ids),\n        parents_ids=sorted(parents_ids))\n    node_representations.append(node_repr)\n  return node_representations\n\n\ndef import_graph(node_representations):\n  """"""Returns a set of Synset nodes from JSON-serializable representation.\n\n  See the documentation of `export_graph` for a description of the format\n  of that representation.\n\n  Args:\n    node_representations: A list of dictionaries, each representing a Synset.\n\n  Returns:\n    A set of Synset objects (nodes), representing a graph.\n  """"""\n  graph = set()\n  # Build one Synset node for each WordNet ID, and keep a mapping.\n  # `children` and `parents` are initialized with empty sets.\n  wn_id_to_node = dict()\n  for node_repr in node_representations:\n    wn_id = node_repr[\'wn_id\']\n    words = node_repr[\'words\']\n    if wn_id in wn_id_to_node:\n      raise ValueError(\'Duplicate Word ID (%s, %s) in the imported graph.\' %\n                       (wn_id, words))\n    node = Synset(wn_id=wn_id, words=words, children=set(), parents=set())\n    wn_id_to_node[wn_id] = node\n\n  # Fill in the `children` and `parents` with the Synset objects.\n  for node_repr in node_representations:\n    wn_id = node_repr[\'wn_id\']\n    node = wn_id_to_node[wn_id]\n    children_ids = node_repr[\'children_ids\']\n    node.children.update(wn_id_to_node[child_id] for child_id in children_ids)\n    parents_ids = node_repr[\'parents_ids\']\n    node.parents.update(wn_id_to_node[parent_id] for parent_id in parents_ids)\n    graph.add(node)\n\n  return graph\n\n\ndef create_imagenet_specification(split_enum,\n                                  files_to_skip,\n                                  path_to_num_leaf_images=None,\n                                  train_split_only=False,\n                                  log_stats=True):\n  """"""Creates the dataset specification of ImageNet.\n\n  This amounts to creating a data structure, a DAG specifically, whose nodes are\n  synsets, and whose leaves are all and only the synsets corresponding to\n  classes of the ILSVRC 2012 subset of ImageNet. Each synset is connected to\n  others via child / parent links that indicate is-a relationships of the\n  corresponding concepts (classes). This DAG will consist of all and only the\n  nodes that are on paths starting from a leaf to a root (i.e. it won\'t contain\n  any nodes that don\'t have leaf descendents) and it will be \'detached\' from the\n  remaining ImageNet synsets (that don\'t belong to the ILSVRC 2012 subset) in\n  that by following a child or parent pointer of any DAG node we are guaranteed\n  to reach another node that also belongs to the DAG.\n\n  Args:\n    split_enum: A class that inherits from enum.Enum whose attributes are TRAIN,\n      VALID, and TEST, which are mapped to enumerated constants.\n    files_to_skip: A set with the files that intersect with other datasets.\n    path_to_num_leaf_images: A string, representing a path to a file containing\n      a dict that maps the WordNet id of each ILSVRC 2012 class to the\n      corresponding number of images. If no file is present, it will be created\n      in order to save on future computation. If None, no attempt at reloading\n      or storing the dict is made.\n    train_split_only: bool, if True, we return the whole Imagenet as our\n      training set.\n    log_stats: whether to print statistics about the sampling graph and the\n      three split subgraphs\n\n  Returns:\n    A tuple of the following:\n    splits: A dict mapping each Split in split_enum to the list of Synsets\n      belonging to the subgraph for that split.\n    split_num_images: A dict mapping each Split in split_enum to a dict for the\n      corresponding split that maps each node in its subgraph to the number of\n      images in the subgraph of that node.\n    sampling_graph: A set of the Synsets that belong to the DAG described above\n    synsets_2012: The list of Synsets of classes of ILSVRC 2012\n    num_synset_2012_images: A dict mapping each WordNet id of ILSVRC 2012 to its\n      number of images\n  """"""\n  # Create Synsets for all ImageNet synsets (82115 in total).\n  data_root = FLAGS.ilsvrc_2012_data_root\n  synsets = {}\n  path_to_words = FLAGS.path_to_words\n  if not path_to_words:\n    path_to_words = os.path.join(data_root, \'words.txt\')\n  with tf.io.gfile.GFile(path_to_words) as f:\n    for line in f:\n      wn_id, words = line.rstrip().split(\'\\t\')\n      synsets[wn_id] = Synset(wn_id, words, set(), set())\n\n  # Populate the parents / children arrays of these Synsets.\n  path_to_is_a = FLAGS.path_to_is_a\n  if not path_to_is_a:\n    path_to_is_a = os.path.join(data_root, \'wordnet.is_a.txt\')\n  with tf.io.gfile.GFile(path_to_is_a, \'r\') as f:\n    for line in f:\n      parent, child = line.rstrip().split(\' \')\n      synsets[parent].children.add(synsets[child])\n      synsets[child].parents.add(synsets[parent])\n\n  # Get the WordNet id\'s of the synsets of ILSVRC 2012.\n  wn_ids_2012 = tf.io.gfile.listdir(data_root)\n  wn_ids_2012 = set(\n      entry for entry in wn_ids_2012\n      if tf.io.gfile.isdir(os.path.join(data_root, entry)))\n  synsets_2012 = [s for s in synsets.values() if s.wn_id in wn_ids_2012]\n  assert len(wn_ids_2012) == len(synsets_2012)\n\n  # Get a dict mapping each WordNet id of ILSVRC 2012 to its number of images.\n  num_synset_2012_images = get_num_synset_2012_images(path_to_num_leaf_images,\n                                                      synsets_2012,\n                                                      files_to_skip)\n\n  # Get the graph of all and only the ancestors of the ILSVRC 2012 classes.\n  sampling_graph = create_sampling_graph(synsets_2012)\n\n  # Create a dict mapping each node to its reachable leaves.\n  spanning_leaves = get_spanning_leaves(sampling_graph)\n\n  # Create a dict mapping each node in sampling graph to the number of images of\n  # ILSVRC 2012 synsets that live in the sub-graph rooted at that node.\n  num_images = get_num_spanning_images(spanning_leaves, num_synset_2012_images)\n\n  if train_split_only:\n    # We are keeping all graph for training.\n    valid_test_roots = None\n    splits = {\n        split_enum.TRAIN: spanning_leaves,\n        split_enum.VALID: set(),\n        split_enum.TEST: set()\n    }\n  else:\n    # Create class splits, each with its own sampling graph.\n    # Choose roots for the validation and test subtrees (see the docstring of\n    # create_splits for more information on how these are used).\n    valid_test_roots = {\n        \'valid\': get_synset_by_wnid(\'n02075296\', sampling_graph),  # \'carnivore\'\n        \'test\':\n            get_synset_by_wnid(\'n03183080\', sampling_graph)  # \'device\'\n    }\n    # The valid_test_roots returned here correspond to the same Synsets as in\n    # the above dict, but are the copied versions of them for each subgraph.\n    splits, valid_test_roots = create_splits(\n        spanning_leaves, split_enum, valid_test_roots=valid_test_roots)\n\n  # Compute num_images for each split.\n  split_num_images = {}\n  split_num_images[split_enum.TRAIN] = get_num_spanning_images(\n      get_spanning_leaves(splits[split_enum.TRAIN]), num_synset_2012_images)\n  split_num_images[split_enum.VALID] = get_num_spanning_images(\n      get_spanning_leaves(splits[split_enum.VALID]), num_synset_2012_images)\n  split_num_images[split_enum.TEST] = get_num_spanning_images(\n      get_spanning_leaves(splits[split_enum.TEST]), num_synset_2012_images)\n\n  # Compute statistics.\n  if log_stats:\n    imagenet_stats.log_graph_stats(\n        sampling_graph,\n        num_images,\n        get_leaves,\n        get_spanning_leaves,\n        graph_name=\'all\')\n    imagenet_stats.log_graph_stats(\n        splits[split_enum.TRAIN],\n        split_num_images[split_enum.TRAIN],\n        get_leaves,\n        get_spanning_leaves,\n        graph_name=\'train\')\n    imagenet_stats.log_graph_stats(\n        splits[split_enum.VALID],\n        split_num_images[split_enum.VALID],\n        get_leaves,\n        get_spanning_leaves,\n        graph_name=\'valid\')\n    imagenet_stats.log_graph_stats(\n        splits[split_enum.TEST],\n        split_num_images[split_enum.TEST],\n        get_leaves,\n        get_spanning_leaves,\n        graph_name=\'test\')\n    # Stats relevant to analysis of fine-graindness.\n    imagenet_stats.log_stats_finegrainedness(\n        splits[split_enum.TRAIN],\n        get_leaves,\n        get_lowest_common_ancestor,\n        graph_name=\'train\',\n        path=\'longest\')\n    imagenet_stats.log_stats_finegrainedness(\n        splits[split_enum.TEST],\n        get_leaves,\n        get_lowest_common_ancestor,\n        graph_name=\'test\',\n        path=\'longest\')\n\n  # Note that spanning_leaves and num_images can easily be created from\n  # sampling_graph if required.\n  return (splits, split_num_images, sampling_graph, synsets_2012,\n          num_synset_2012_images, valid_test_roots)\n'"
meta_dataset/data/imagenet_specification_test.py,5,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Tests for imagenet_specification.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom meta_dataset.data import imagenet_specification as imagenet_spec\nfrom meta_dataset.data import learning_spec\nimport numpy as np\nfrom six.moves import range\nimport tensorflow.compat.v1 as tf\n\nDESIRED_TOY_NUM_VALID_CLASSES = 2\nDESIRED_TOY_NUM_TEST_CLASSES = 1\nTOY_MARGIN = 1\n\n\ndef create_toy_graph():\n  synsets = {}\n  for wn_id, name in enumerate([\'a\', \'b\', \'c\', \'d\', \'e\', \'f\', \'g\', \'h\']):\n    synsets[name] = imagenet_spec.Synset(wn_id, name, set(), set())\n\n  # (parent-child) tuples\n  is_a_relations = [(\'a\', \'b\'), (\'a\', \'c\'), (\'b\', \'g\'), (\'c\', \'d\'), (\'c\', \'e\'),\n                    (\'e\', \'f\'), (\'e\', \'h\')]\n  # The graph is a tree that looks like:\n  #        a\n  #    b       c\n  #  g       d   e\n  #             h f\n  for t in is_a_relations:\n    parent, child = t\n    synsets[parent].children.add(synsets[child])\n    synsets[child].parents.add(synsets[parent])\n\n  subset = [\'f\', \'g\']\n  synsets_subset = [s for s in synsets.values() if s.words in subset]\n\n  # Get the graph of all and only the ancestors of synsets_subset\n  graph_nodes = imagenet_spec.create_sampling_graph(synsets_subset)\n\n  # The created graph should contain all and only the ancestors of subset and\n  # collapses all nodes that have exactly 1 child. It should be:\n  #    a\n  #  g   f\n\n  # Create a data structure mapping graph_nodes to their reachable leaves\n  spanning_leaves = imagenet_spec.get_spanning_leaves(graph_nodes)\n  return graph_nodes, spanning_leaves, synsets_subset\n\n\ndef validate_graph(graph_nodes, subset_synsets, test_instance):\n  """"""Checks that the DAG structure is as expected.""""""\n  # 1) Test that the leaves are all and only the ILSVRC 2012 synsets\n  leaves = imagenet_spec.get_leaves(graph_nodes)\n  test_instance.assertEqual(len(leaves), len(subset_synsets))\n  test_instance.assertEqual(set(leaves), set(subset_synsets))\n\n  # 2) Validate the connectivity\n  # If a node is listed as a child of another, the latter must also be listed as\n  # a parent of the former, and similarly if a node is listed as a parent of\n  # another, the latter must also be listed as a child of the former.\n  for n in graph_nodes:\n    for c in n.children:\n      test_instance.assertIn(n, c.parents)\n    for p in n.parents:\n      test_instance.assertIn(n, p.children)\n\n  # 3) Check that no node has only 1 child, as it\'s not possible to create an\n  # episode from such a node.\n  for n in graph_nodes:\n    test_instance.assertNotEqual(len(n.children), 1)\n\n  # 4) Check that the graph is detached from the remaining non-graph synsets.\n  # We want to guarantee that by following parent or child pointers of graph\n  # nodes we will stay within the graph.\n  for n in graph_nodes:\n    for c in n.children:\n      test_instance.assertIn(c, graph_nodes)\n    for p in n.parents:\n      test_instance.assertIn(p, graph_nodes)\n\n  # 5) Check that every node in graph nodes is either an ILSVRC 2012 synset or\n  # the ancestor of an ILSVRC 2012 synset\n  for n in graph_nodes:\n    if n in subset_synsets:\n      continue\n    has_2012_descendent = False\n    for s in subset_synsets:\n      has_2012_descendent = imagenet_spec.is_descendent(s, n)\n      if has_2012_descendent:\n        break\n    test_instance.assertTrue(has_2012_descendent)\n\n\ndef validate_spanning_leaves(spanning_leaves, subset_synsets, test_instance):\n  """"""Check the correctness of the spanning_leaves dict.""""""\n  # 1) ILSVRC 2012 synsets should span exactly 1 leaf each (themselves)\n  for s in subset_synsets:\n    test_instance.assertEqual(spanning_leaves[s], set([s]))\n\n  # 2) Checks regarding the number of leaves a node can span in relation to\n  # the number of leaves its children span\n  # - The number of leaves spanned by a node must be greater than or equal to\n  #   the number of leaves spanned by any one of its children. It can\'t be\n  #   less than it by definition. It can only be equal under one condition,\n  #   described below.\n  # - The total number of leaves spanned by the children of a node should be\n  #   greater than or equal to the number of leaves spanned by that node\n  #   (it could be greater if some leaf is spanned by more than one child of\n  #   that node, which can happen since a node can have multiple parents.)\n  for s, leaves in spanning_leaves.items():\n    # These checks are not applicable to leafs (since they have no children)\n    if not s.children:\n      continue\n    num_spanning_leaves = len(leaves)\n    children_spanning_leaves = []\n    num_leaf_children = 0\n    for c in s.children:\n      if not c.children:\n        num_leaf_children += 1\n      test_instance.assertGreaterEqual(num_spanning_leaves,\n                                       len(spanning_leaves[c]))\n      if num_spanning_leaves == len(spanning_leaves[c]):\n        # A child c of a synset s may span the same number of leaves as s only\n        # if c is a parent of all other children of s. Otherwise it would be\n        # that s has a child that spans no synsets, which isn\'t allowed.\n        for c_other in s.children:\n          if c == c_other:\n            continue\n          test_instance.assertIn(c, c_other.parents)\n      children_spanning_leaves += spanning_leaves[c]\n\n    sum_of_children_leaves = len(children_spanning_leaves)\n    assert num_spanning_leaves <= sum_of_children_leaves\n    # The strict equality in the above can only hold under 1 condition:\n    # If num_spanning_leaves of node n is less than the sum of spanning leaves\n    # of n\'s children it must be that one of those leaves was double-counted,\n    # in that some node along a path from that leaf to n had two parents,\n    # causing that leaf to be a descendent of two different children of n.\n    if num_spanning_leaves < sum_of_children_leaves:\n      test_instance.assertNotEqual(\n          len(children_spanning_leaves), len(set(children_spanning_leaves)))\n      diff = abs(\n          len(children_spanning_leaves) - len(set(children_spanning_leaves)))\n      test_instance.assertEqual(\n          diff, abs(num_spanning_leaves - sum_of_children_leaves))\n\n\ndef test_lowest_common_ancestor_(lca,\n                                 height,\n                                 leaf_a,\n                                 leaf_b,\n                                 test_instance,\n                                 root=None):\n  """"""Check the correctness of the lowest common ancestor and its height.""""""\n  # First, check that it is a common ancestor of the longest paths.\n  paths_a = imagenet_spec.get_upward_paths_from(leaf_a)\n  longest_path_a = paths_a[np.argmax([len(p) for p in paths_a])]\n  test_instance.assertIn(lca, longest_path_a)\n  paths_b = imagenet_spec.get_upward_paths_from(leaf_b)\n  longest_path_b = paths_b[np.argmax([len(p) for p in paths_b])]\n  test_instance.assertIn(lca, longest_path_b)\n\n  # Check that the LCA is not higher than the root.\n  if root is not None:\n    test_instance.assertFalse(imagenet_spec.is_descendent(root, lca))\n\n  # Assert that there is no lower common ancestor than the given lca.\n  for height_a, node in enumerate(longest_path_a):\n    if node in longest_path_b:\n      height_b = longest_path_b.index(node)\n      node_height = max(height_a, height_b)\n      if node == lca:\n        test_instance.assertEqual(node_height, height)\n      else:\n        # It then must have greater height than the lca\'s height.\n        test_instance.assertGreaterEqual(node_height, height)\n\n\ndef test_lowest_common_ancestor(graph_nodes, test_instance, root=None):\n  # Test the computation of the lowest common ancestor of two nodes.\n  # Randomly sample two leaves a number of times, find their lowest common\n  # ancestor and its height and verify that they are computed correctly.\n  leaves = imagenet_spec.get_leaves(graph_nodes)\n  for _ in range(10000):\n    first_ind = np.random.randint(len(leaves))\n    second_ind = np.random.randint(len(leaves))\n    while first_ind == second_ind:\n      second_ind = np.random.randint(len(leaves))\n    leaf_a = leaves[first_ind]\n    leaf_b = leaves[second_ind]\n    lca, height = imagenet_spec.get_lowest_common_ancestor(leaf_a, leaf_b)\n    test_lowest_common_ancestor_(\n        lca, height, leaf_a, leaf_b, test_instance, root=root)\n\n\ndef test_get_upward_paths(graph_nodes, test_instance, subgraph_root=None):\n  """"""Test the correctness of imagenet_spec.get_upward_paths_from.""""""\n  # Randomly sample a number of start nodes for get_upward_paths. For each, test\n  # the behavior of get_upward_paths when either specifying an end node or not.\n  graph_nodes_list = list(graph_nodes)\n  num_tested = 0\n  while num_tested < 10:\n    start_node = np.random.choice(graph_nodes_list)\n\n    if not start_node.parents:\n      continue\n\n    # Test the behavior of get_upward_paths_from without an end_node specified.\n    paths = imagenet_spec.get_upward_paths_from(start_node)\n    for p in paths:\n      last_node = p[-1]\n      if subgraph_root is not None:\n        test_instance.assertEqual(last_node, subgraph_root)\n      else:\n        # Make sure the last node does not have parents (is a root).\n        test_instance.assertLen(last_node.parents, 0)\n\n    # Now test the case where an end node is given which is a direct parent of\n    # the start node.\n    end_node = np.random.choice(list(start_node.parents))\n    paths = imagenet_spec.get_upward_paths_from(start_node, end_node)\n    # There should be at least one path in paths that contains only\n    # (start_node and end_node).\n    found_direct_path = False\n    for p in paths:\n      if len(p) == 2 and p[0] == start_node and p[1] == end_node:\n        found_direct_path = True\n    test_instance.assertTrue(found_direct_path)\n\n    num_tested += 1\n\n\nclass GraphCopyTest(tf.test.TestCase):\n  """"""Test the correctness of imagenet_spec.copy_graph.""""""\n\n  def validate_copy(self, graph, graph_copy):\n    """"""Make sure graph_copy is a correct copy of graph.""""""\n    # Make sure that for each node in graph, there is exactly one node in\n    # graph_copy with the same WordNet id\n    for n in graph:\n      wn_id = n.wn_id\n      found_wn_in_copy = False\n      for n_copy in graph_copy:\n        if n_copy.wn_id == wn_id:\n          found_wn_in_copy = True\n          break\n      self.assertTrue(found_wn_in_copy)\n\n    # Make sure that for every link in graph there is a corresponding link in\n    # graph copy (correspondence is assessed via the WordNet id\'s of the nodes\n    # that are being connected).\n    graph_parent_child_links = set()\n    for s in graph:\n      for c in s.children:\n        graph_parent_child_links.add((s.wn_id, c.wn_id))\n    for s in graph_copy:\n      for p, c in graph_parent_child_links:\n        # Find the nodes in graph_copy whose wn_id\'s are p and c\n        for n in graph_copy:\n          if n.wn_id == c:\n            c_node = n\n          if n.wn_id == p:\n            p_node = n\n        self.assertIn(c_node, p_node.children)\n        self.assertIn(p_node, c_node.parents)\n\n  def test_toy_graph_copy(self):\n    specification = create_toy_graph()\n    toy_graph, _, _ = specification\n    toy_graph_copy, _ = imagenet_spec.copy_graph(toy_graph)\n    self.validate_copy(toy_graph, toy_graph_copy)\n\n\nclass TestGetSynsetsFromIds(tf.test.TestCase):\n  """"""Test the correctness of imagenet_spec.get_synsets_from_ids().""""""\n\n  def test_on_toy_graph(self):\n    specification = create_toy_graph()\n    toy_graph, _, _ = specification\n    wn_ids = [5, 0, 6]\n    id_to_synset = imagenet_spec.get_synsets_from_ids(wn_ids, toy_graph)\n    self.assertEqual(set(id_to_synset.keys()), set(wn_ids))\n    for wn_id, synset in id_to_synset.items():\n      self.assertEqual(wn_id, synset.wn_id)\n\n\nclass SplitCreationTest(tf.test.TestCase):\n  """"""Test the correctness of imagenet_spec.propose_valid_test_roots.""""""\n\n  def validate_roots(self, valid_test_roots, spanning_leaves):\n    # Make sure that the number of leaves spanned by the proposed valid and test\n    # roots are within the allowable margin.\n    valid_root, test_root = valid_test_roots[\'valid\'], valid_test_roots[\'test\']\n    num_valid_leaves = len(spanning_leaves[valid_root])\n    num_test_leaves = len(spanning_leaves[test_root])\n    self.assertGreaterEqual(num_valid_leaves,\n                            DESIRED_TOY_NUM_VALID_CLASSES - TOY_MARGIN)\n    self.assertLessEqual(num_valid_leaves,\n                         DESIRED_TOY_NUM_VALID_CLASSES + TOY_MARGIN)\n    self.assertGreaterEqual(num_test_leaves,\n                            DESIRED_TOY_NUM_TEST_CLASSES - TOY_MARGIN)\n    self.assertLessEqual(num_test_leaves,\n                         DESIRED_TOY_NUM_TEST_CLASSES + TOY_MARGIN)\n\n  def validate_splits(self, splits, spanning_leaves):\n    # Make sure that the classes assigned to each split cover all the leaves\n    # and no class is assigned to more than one splits\n    train_wn_ids = splits[\'train\']\n    valid_wn_ids = splits[\'valid\']\n    test_wn_ids = splits[\'test\']\n    self.assertFalse(train_wn_ids & valid_wn_ids)\n    self.assertFalse(train_wn_ids & test_wn_ids)\n    self.assertFalse(test_wn_ids & valid_wn_ids)\n    all_wn_ids = train_wn_ids | valid_wn_ids | test_wn_ids\n    leaves = imagenet_spec.get_leaves(spanning_leaves.keys())\n    self.assertLen(all_wn_ids, len(leaves))  # all covered\n\n  def test_toy_root_proposer(self):\n    specification = create_toy_graph()\n    _, toy_span_leaves, _ = specification\n    valid_test_roots = imagenet_spec.propose_valid_test_roots(\n        toy_span_leaves,\n        margin=TOY_MARGIN,\n        desired_num_valid_classes=DESIRED_TOY_NUM_VALID_CLASSES,\n        desired_num_test_classes=DESIRED_TOY_NUM_TEST_CLASSES)\n    self.validate_roots(valid_test_roots, toy_span_leaves)\n\n    # returns the lists of id\'s of classes belonging to each split\n    # unlike create_splits which returns the nodes of the three subgraphs that\n    # are constructed for the different splits\n    splits, _ = imagenet_spec.get_class_splits(\n        toy_span_leaves, valid_test_roots=valid_test_roots)\n    self.validate_splits(splits, toy_span_leaves)\n\n\nclass ImagenetSpecificationTest(tf.test.TestCase):\n\n  def validate_num_span_images(self, span_leaves, num_span_images):\n    # Ensure that the number of images spanned by each node is exactly the\n    # number of images living in the leaves spanned by that node\n    for node, leaves in span_leaves.items():\n      self.assertEqual(num_span_images[node],\n                       sum([num_span_images[l] for l in leaves]))\n\n  def validate_splits(self, splits):\n    """"""Check the correctness of the class splits.""""""\n    train_graph = splits[learning_spec.Split.TRAIN]\n    valid_graph = splits[learning_spec.Split.VALID]\n    test_graph = splits[learning_spec.Split.TEST]\n\n    # Make sure that by following child/parent pointers of nodes of a given\n    # split\'s subgraph, we will reach nodes that also belong to that subgraph.\n    def ensure_isolated(nodes):\n      for n in nodes:\n        for c in n.children:\n          self.assertIn(c, nodes)\n        for p in n.parents:\n          self.assertIn(p, nodes)\n\n    ensure_isolated(train_graph)\n    ensure_isolated(valid_graph)\n    ensure_isolated(test_graph)\n\n    train_classes = imagenet_spec.get_leaves(train_graph)\n    valid_classes = imagenet_spec.get_leaves(valid_graph)\n    test_classes = imagenet_spec.get_leaves(test_graph)\n\n    # Ensure that there is no overlap between classes of different splits\n    # and that combined they cover all ILSVRC 2012 classes\n    all_classes = train_classes + valid_classes + test_classes\n    self.assertLen(set(all_classes), 1000)  # all covered\n    self.assertLen(set(all_classes), len(all_classes))  # no duplicates\n\n  def test_imagenet_specification(self):\n    spec = imagenet_spec.create_imagenet_specification(learning_spec.Split,\n                                                       set())\n    splits, _, graph_nodes, synsets_2012, num_synset_2012_images, roots = spec\n    span_leaves = imagenet_spec.get_spanning_leaves(graph_nodes)\n    num_span_images = imagenet_spec.get_num_spanning_images(\n        span_leaves, num_synset_2012_images)\n\n    validate_graph(graph_nodes, synsets_2012, self)\n    validate_spanning_leaves(span_leaves, synsets_2012, self)\n    self.validate_splits(splits)\n    self.validate_num_span_images(span_leaves, num_span_images)\n\n    test_lowest_common_ancestor(graph_nodes, self)\n    test_get_upward_paths(graph_nodes, self)\n    # Make sure that in no sub-tree can the LCA of two chosen leaves of that\n    # sub-tree be a node that is an ancestor of the sub-tree\'s root.\n    valid_subgraph, test_subgraph = splits[learning_spec.Split.VALID], splits[\n        learning_spec.Split.TEST]\n    valid_root, test_root = roots[\'valid\'], roots[\'test\']\n    test_lowest_common_ancestor(valid_subgraph, self, valid_root)\n    test_get_upward_paths(valid_subgraph, self, valid_root)\n    test_lowest_common_ancestor(test_subgraph, self, test_root)\n    test_get_upward_paths(test_subgraph, self, test_root)\n\n  def test_toy_graph_specification(self):\n    specification = create_toy_graph()\n    toy_graph_nodes, toy_span_leaves, toy_synsets_2012 = specification\n    validate_graph(toy_graph_nodes, toy_synsets_2012, self)\n    validate_spanning_leaves(toy_span_leaves, toy_synsets_2012, self)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
meta_dataset/data/imagenet_stats.py,0,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Computes stats of the graphs created in imagenet_specification.py.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom absl import logging\nimport numpy as np\nfrom six.moves import range\n\n\ndef log_graph_stats(nodes,\n                    num_images,\n                    get_leaves_fn,\n                    get_spanning_leaves_fn,\n                    graph_name=None,\n                    min_way=5,\n                    max_way=50):\n  """"""Compute and display statistics about the graph defined by nodes.\n\n  In particular, the statistics that are computed are:\n  the number of nodes, the numbers of roots and leaves, the min/max/mean number\n  of images living in the leaves, the min/max/mean number of children of\n  internal nodes, the min/max/mean depth of leaves.\n\n  Args:\n    nodes: A set of Synsets representing a graph.\n    num_images: A dict mapping each node\'s WordNet id to the number of images\n      living in the leaves spanned by that node.\n    get_leaves_fn: A function that returns the set of leaves of a graph defined\n      by a given set of nodes, e.g. get_leaves in imagenet_specification.py\n    get_spanning_leaves_fn: A function that returns a dict mapping each node of\n      a given set of nodes to the set of leaf Synsets spanned by that node, e.g.\n      get_spanning_leaves in imagenet_specification.py.\n    graph_name: A name for the graph (for the printed logs).\n    min_way: The smallest allowable way of an episode.\n    max_way: The largest allowable way of an episode.\n  """"""\n  logging.info(\n      \'Graph statistics%s:\',\n      \' of graph {}\'.format(graph_name) if graph_name is not None else \'\')\n  logging.info(\'Number of nodes: %d\', len(nodes))\n  if not nodes:\n    # Empty set\n    return\n\n  # Compute the dict mapping internal nodes to their spanning leaves. Note that\n  # this is different for the different splits since even for nodes that may be\n  # shared across splits, their connectivity will be different.\n  spanning_leaves = get_spanning_leaves_fn(nodes)\n\n  # Compute the number of roots and leaves\n  num_roots = 0\n  for n in nodes:\n    if not n.parents:\n      num_roots += 1\n      logging.info(\'Root: %s\', n.words)\n  logging.info(\'Number of roots: %d\', num_roots)\n  leaves = get_leaves_fn(nodes)\n  logging.info(\'Number of leaves: %d\', len(leaves))\n\n  # Compute the number of images in the leaves\n  num_leaf_images = []\n  for n in nodes:\n    if n.children:\n      continue\n    num_leaf_images.append(num_images[n])\n  logging.info(\'Number of leaf images: min %d, max %d, median %f\',\n               min(num_leaf_images), max(num_leaf_images),\n               np.median(num_leaf_images))\n\n  # Compute the average number of children of internal nodes\n  num_children = []\n  for n in nodes:\n    if not n.children:\n      continue\n    num_children.append(len(n.children))\n  logging.info(\n      \'Number of children of internal nodes: min %d, max %d, mean %f median %f\',\n      min(num_children), max(num_children), np.mean(num_children),\n      np.median(num_children))\n\n  # Compute the average number of leaves spanned by internal nodes.\n  num_span_leaves = []\n  for n in nodes:\n    if not n.children:\n      continue\n    num_span_leaves.append(len(spanning_leaves[n]))\n  logging.info(\n      \'Number of spanning leaves of internal nodes: min %d, max %d, mean %f \'\n      \'median %f\', min(num_span_leaves), max(num_span_leaves),\n      np.mean(num_span_leaves), np.median(num_span_leaves))\n\n  # Log the effects of restricting the allowable \'way\' of episodes.\n  all_reachable_leaves = set()  # leaves reachable under the restriction.\n  possible_ways_in_range = []\n  for v in spanning_leaves.values():\n    way = len(v)\n    if way >= min_way and way <= max_way:\n      possible_ways_in_range.append(way)\n      all_reachable_leaves |= set(v)\n  logging.info(\n      \'When restricting the allowable way to be between %d and %d, \'\n      \'the achievable ways are: %s\', min_way, max_way, possible_ways_in_range)\n  logging.info(\n      \'So there is a total of %d available internal nodes and a \'\n      \'total of %d different ways.\', len(possible_ways_in_range),\n      len(set(possible_ways_in_range)))\n  # Are all leaves reachable when using the restricted way?\n  logging.info(\' %d / %d are reachable.\', len(all_reachable_leaves),\n               len(leaves))\n\n\ndef log_stats_finegrainedness(nodes,\n                              get_leaves_fn,\n                              get_lowest_common_ancestor_fn,\n                              graph_name=None,\n                              num_per_height_to_print=2,\n                              num_leaf_pairs=10000,\n                              path=\'longest\'):\n  """"""Gather some stats relating to the heights of LCA\'s of random leaf pairs.\n\n  Args:\n    nodes: A set of Synsets.\n    get_leaves_fn: A function that returns the set of leaves of a graph defined\n      by a given set of nodes, e.g. get_leaves in imagenet_specification.py\n    get_lowest_common_ancestor_fn: A function that returns the lowest common\n      ancestor node of a given pair of Synsets and its height, e.g. the\n      get_lowest_common_ancestor function in imagenet_specification.py.\n    graph_name: A name for the graph defined by nodes (for logging).\n    num_per_height_to_print: An int. The number of example leaf pairs and\n      corresponding lowest common ancestors to print for each height.\n    num_leaf_pairs: An int. The number of random leaf pairs to sample.\n    path: A str. The \'path\' argument of get_lowest_common_ancestor. Can be\n      either \'longest\' or \'all.\n  """"""\n  if not nodes:\n    # Empty set\n    return\n  logging.info(\n      \'Finegrainedness analysis of %s graph using %s paths in \'\n      \'finding the lowest common ancestor.\', graph_name, path)\n  leaves = get_leaves_fn(nodes)\n  # Maps the height of the lowest common ancestor of two leaves to the \'example\'\n  # in which that height occurred. The example is a tuple of the string words\n  # associated with (first leaf, second leaf, lowest common ancestor).\n  heights_to_examples = collections.defaultdict(list)\n  # Maps the height of the lowest common ancestor of two leaves to the number of\n  # leaf pairs whose LCA has that height and is the root.\n  heights_to_num_lca_root = collections.defaultdict(int)\n  # A list of all observed LCA heights.\n  heights = []\n  # Sample a number of random pairs of leaves, and compute the height of their\n  # lowest common ancestor.\n  for _ in range(num_leaf_pairs):\n    first_ind = np.random.randint(len(leaves))\n    second_ind = np.random.randint(len(leaves))\n    while first_ind == second_ind:\n      second_ind = np.random.randint(len(leaves))\n    leaf_a = leaves[first_ind]\n    leaf_b = leaves[second_ind]\n    lca, height = get_lowest_common_ancestor_fn(leaf_a, leaf_b, path=path)\n    heights.append(height)\n\n    heights_to_examples[height].append((leaf_a.words, leaf_b.words, lca.words))\n    if not lca.parents:\n      heights_to_num_lca_root[height] += 1\n\n  name_message = \' of the {} graph\'.format(\n      graph_name) if graph_name is not None else \'\'\n  stats_message = \'mean: {}, median: {}, max: {}, min: {}\'.format(\n      np.mean(heights), np.median(heights), max(heights), min(heights))\n  logging.info(\n      \'Stats on the height of the Lowest Common Ancestor of random leaf pairs%s\'\n      \': %s\', name_message, stats_message)\n\n  # For each given height, how many pairs of leaves are there?\n  heights_to_num_examples = {}\n  heights_to_proportion_root = {}\n  for h, examples in heights_to_examples.items():\n    heights_to_num_examples[h] = len(examples) / num_leaf_pairs\n    heights_to_proportion_root[h] = heights_to_num_lca_root[h] / float(\n        len(examples))\n  logging.info(\n      \'Proportion of example leaf pairs (out of num_leaf_pairs \'\n      \'random pairs) for each height of the LCA of the leaves: %s\',\n      heights_to_num_examples)\n\n  # What proportion of those have the root as LCA, for each possible height?\n  logging.info(\n      \'Proportion of example leaf pairs per height whose LCA is the root: %s\',\n      heights_to_proportion_root)\n\n  logging.info(\'Examples with different fine-grainedness:\\n\')\n  for height in heights_to_examples.keys():\n    # Get representative examples of this height.\n    for i, example in enumerate(heights_to_examples[height]):\n      if i == num_per_height_to_print:\n        break\n      logging.info(\'Examples with height %s:\\nleafs: %s and %s. LCA: %s\',\n                   height, example[0], example[1], example[2])\n'"
meta_dataset/data/learning_spec.py,0,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Interfaces for learning specifications.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport enum\n\n\nclass Split(enum.Enum):\n  """"""The possible data splits.""""""\n  TRAIN = 0\n  VALID = 1\n  TEST = 2\n\n\nclass BatchSpecification(\n    collections.namedtuple(\'BatchSpecification\', \'split, batch_size\')):\n  """"""The specification of an episode.\n\n    Args:\n      split: the Split from which to pick data.\n      batch_size: an int, the number of (image, label) pairs in the batch.\n  """"""\n  pass\n\n\nclass EpisodeSpecification(\n    collections.namedtuple(\n        \'EpisodeSpecification\',\n        \'split, num_classes, num_train_examples, num_test_examples\')):\n  """"""The specification of an episode.\n\n    Args:\n      split: A Split from which to pick data.\n      num_classes: The number of classes in the episode, or None for variable.\n      num_train_examples: The number of examples to use per class in the train\n        phase, or None for variable.\n      num_test_examples: the number of examples to use per class in the test\n        phase, or None for variable.\n  """"""\n'"
meta_dataset/data/pipeline.py,25,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""This module assembles full input data pipelines.\n\nThe whole pipeline incorporate (potentially) multiple Readers, the logic to\nselect between them, and the common logic to extract support / query sets if\nneeded, decode the example strings, and resize the images.\n""""""\n# TODO(lamblinp): Organize the make_*_pipeline functions into classes, and\n# make them output Batch or EpisodeDataset objects directly.\n# TODO(lamblinp): Update variable names to be more consistent\n# - target, class_idx, label\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nfrom absl import logging\nimport gin.tf\nfrom meta_dataset import data\nfrom meta_dataset.data import decoder\nfrom meta_dataset.data import learning_spec\nfrom meta_dataset.data import reader\nfrom meta_dataset.data import sampling\nfrom six.moves import zip\nimport tensorflow.compat.v1 as tf\n\n\ndef filter_dummy_examples(example_strings, class_ids):\n  """"""Returns tensors with only actual examples, filtering out the dummy ones.\n\n  Actual examples are the first ones in the tensors, and followed by dummy ones,\n  indicated by negative class IDs.\n\n  Args:\n    example_strings: 1-D Tensor of dtype str, Example protocol buffers.\n    class_ids: 1-D Tensor of dtype int, class IDs (absolute wrt the original\n      dataset, except for negative ones, that indicate dummy examples).\n  """"""\n  num_actual = tf.reduce_sum(tf.cast(class_ids >= 0, tf.int32))\n  actual_example_strings = example_strings[:num_actual]\n  actual_class_ids = class_ids[:num_actual]\n  return (actual_example_strings, actual_class_ids)\n\n\ndef log_data_augmentation(data_augmentation, name):\n  """"""Logs the given data augmentation parameters for diagnostic purposes.""""""\n  if not data_augmentation:\n    logging.info(\'No data augmentation provided for %s\', name)\n  else:\n    logging.info(\'%s augmentations:\', name)\n    logging.info(\'enable_jitter: %s\', data_augmentation.enable_jitter)\n    logging.info(\'jitter_amount: %d\', data_augmentation.jitter_amount)\n    logging.info(\'enable_gaussian_noise: %s\',\n                 data_augmentation.enable_gaussian_noise)\n    logging.info(\'gaussian_noise_std: %s\', data_augmentation.gaussian_noise_std)\n\n\ndef flush_and_chunk_episode(example_strings, class_ids, chunk_sizes):\n  """"""Removes flushed examples from an episode and chunks it.\n\n  This function:\n\n  1) splits the batch of examples into a ""flush"" chunk and some number of\n     additional chunks (as determined by `chunk_sizes`),\n  2) throws away the ""flush"" chunk, and\n  3) removes the padded dummy examples from the additional chunks.\n\n  For example, in the context of few-shot learning, where episodes are composed\n  of a support set and a query set, `chunk_size = (150, 100, 50)` would be\n  interpreted as describing a ""flush"" chunk of size 150, a ""support"" chunk of\n  size 100, and a ""query"" chunk of size 50.\n\n  Args:\n    example_strings: 1-D Tensor of dtype str, tf.train.Example protocol buffers.\n    class_ids: 1-D Tensor of dtype int, class IDs (absolute wrt the original\n      dataset).\n    chunk_sizes: tuple of ints representing the sizes of the flush and\n      additional chunks.\n\n  Returns:\n    A tuple of episode chunks of the form `((chunk_0_example_strings,\n    chunk_0_class_ids), (chunk_1_example_strings, chunk_1_class_ids), ...)`.\n  """"""\n  example_strings_chunks = tf.split(\n      example_strings, num_or_size_splits=chunk_sizes)[1:]\n  class_ids_chunks = tf.split(class_ids, num_or_size_splits=chunk_sizes)[1:]\n\n  return tuple(\n      filter_dummy_examples(strings, ids)\n      for strings, ids in zip(example_strings_chunks, class_ids_chunks))\n\n\n@gin.configurable(whitelist=[\'support_decoder\', \'query_decoder\'])\ndef process_dumped_episode(support_strings, query_strings, image_size,\n                           support_decoder, query_decoder):\n  """"""Processes a dumped episode.\n\n  This function is almost like `process_episode()` function, except:\n  - It doesn\'t need to call flush_and_chunk_episode().\n  - And the labels are read from the tf.Example directly. We assume that\n    labels are already mapped in to [0, n_ways - 1].\n\n  Args:\n    support_strings: 1-D Tensor of dtype str, Example protocol buffers of\n      support set.\n    query_strings: 1-D Tensor of dtype str, Example protocol buffers of query\n      set.\n    image_size: int, desired image size used during decoding.\n    support_decoder: ImageDecoder, used to decode support set images.\n    query_decoder: ImageDecoder, used to decode query set images.\n\n  Returns:\n    support_images, support_labels, support_labels, query_images,\n      query_labels, query_labels: Tensors, batches of images, labels, and\n      labels, for the support and query sets (respectively). We return labels\n      twice since dumped datasets doesn\'t have (absolute) class IDs anymore.\n  """"""\n  if isinstance(support_decoder, decoder.ImageDecoder):\n    log_data_augmentation(support_decoder.data_augmentation, \'support\')\n    support_decoder.image_size = image_size\n  else:\n    raise TypeError(\'support_decoder type: %s is not ImageDecoder\' %\n                    type(support_decoder))\n  if isinstance(query_decoder, decoder.ImageDecoder):\n    log_data_augmentation(query_decoder.data_augmentation, \'query\')\n    query_decoder.image_size = image_size\n  else:\n    raise TypeError(\'query_decoder type: %s is not ImageDecoder\' %\n                    type(query_decoder))\n\n  support_decoder.image_size = image_size\n  query_decoder.image_size = image_size\n  support_images, support_labels = tf.map_fn(\n      support_decoder.decode_with_label,\n      support_strings,\n      dtype=(support_decoder.out_type, tf.int32),\n      back_prop=False)\n  query_images, query_labels = tf.map_fn(\n      support_decoder.decode_with_label,\n      query_strings,\n      dtype=(support_decoder.out_type, tf.int32),\n      back_prop=False)\n\n  return (support_images, support_labels, support_labels, query_images,\n          query_labels, query_labels)\n\n\n@gin.configurable(whitelist=[\'support_decoder\', \'query_decoder\'])\ndef process_episode(example_strings, class_ids, chunk_sizes, image_size,\n                    support_decoder, query_decoder):\n  """"""Processes an episode.\n\n  This function:\n\n  1) splits the batch of examples into ""flush"", ""support"", and ""query"" chunks,\n  2) throws away the ""flush"" chunk,\n  3) removes the padded dummy examples from the ""support"" and ""query"" chunks,\n  4) extracts and processes images out of the example strings, and\n  5) builds support and query targets (numbers from 0 to K-1 where K is the\n     number of classes in the episode) from the class IDs.\n\n  Args:\n    example_strings: 1-D Tensor of dtype str, tf.train.Example protocol buffers.\n    class_ids: 1-D Tensor of dtype int, class IDs (absolute wrt the original\n      dataset).\n    chunk_sizes: Tuple of ints representing the sizes the flush and additional\n      chunks.\n    image_size: int, desired image size used during decoding.\n    support_decoder: Decoder, used to decode support set images.\n    query_decoder: Decoder, used to decode query set images.\n\n  Returns:\n    support_images, support_labels, support_class_ids, query_images,\n      query_labels, query_class_ids: Tensors, batches of images, labels, and\n      (absolute) class IDs, for the support and query sets (respectively).\n  """"""\n  # TODO(goroshin): Replace with `support_decoder.log_summary(name=\'support\')`.\n  # TODO(goroshin): Eventually remove setting the image size here and pass it\n  # to the ImageDecoder constructor instead.\n  if isinstance(support_decoder, decoder.ImageDecoder):\n    log_data_augmentation(support_decoder.data_augmentation, \'support\')\n    support_decoder.image_size = image_size\n  if isinstance(query_decoder, decoder.ImageDecoder):\n    log_data_augmentation(query_decoder.data_augmentation, \'query\')\n    query_decoder.image_size = image_size\n\n  (support_strings, support_class_ids), (query_strings, query_class_ids) = \\\n      flush_and_chunk_episode(example_strings, class_ids, chunk_sizes)\n  support_images = tf.map_fn(\n      support_decoder,\n      support_strings,\n      dtype=support_decoder.out_type,\n      back_prop=False)\n  query_images = tf.map_fn(\n      query_decoder,\n      query_strings,\n      dtype=query_decoder.out_type,\n      back_prop=False)\n\n  # Convert class IDs into labels in [0, num_ways).\n  _, support_labels = tf.unique(support_class_ids)\n  _, query_labels = tf.unique(query_class_ids)\n\n  return (support_images, support_labels, support_class_ids, query_images,\n          query_labels, query_class_ids)\n\n\n@gin.configurable(whitelist=[\'batch_decoder\'])\ndef process_batch(example_strings, class_ids, image_size, batch_decoder):\n  """"""Processes a batch.\n\n  This function:\n\n  1) extracts and processes images out of the example strings.\n  2) builds targets from the class ID and offset.\n\n  Args:\n    example_strings: 1-D Tensor of dtype str, Example protocol buffers.\n    class_ids: 1-D Tensor of dtype int, class IDs (absolute wrt the original\n      dataset).\n    image_size: int, desired image size used during decoding.\n    batch_decoder: Decoder class instance for the batch.\n\n  Returns:\n    images, labels: Tensors, a batch of image and labels.\n  """"""\n  # TODO(goroshin): Replace with `batch_decoder.log_summary(name=\'support\')`.\n  if isinstance(batch_decoder, decoder.ImageDecoder):\n    log_data_augmentation(batch_decoder.data_augmentation, \'batch\')\n    batch_decoder.image_size = image_size\n  images = tf.map_fn(\n      batch_decoder,\n      example_strings,\n      dtype=batch_decoder.out_type,\n      back_prop=False)\n  labels = class_ids\n  return (images, labels)\n\n\ndef make_one_source_episode_pipeline(dataset_spec,\n                                     use_dag_ontology,\n                                     use_bilevel_ontology,\n                                     split,\n                                     episode_descr_config,\n                                     pool=None,\n                                     shuffle_buffer_size=None,\n                                     read_buffer_size_bytes=None,\n                                     num_prefetch=0,\n                                     image_size=None,\n                                     num_to_take=None):\n  """"""Returns a pipeline emitting data from one single source as Episodes.\n\n  Args:\n    dataset_spec: A DatasetSpecification object defining what to read from.\n    use_dag_ontology: Whether to use source\'s ontology in the form of a DAG to\n      sample episodes classes.\n    use_bilevel_ontology: Whether to use source\'s bilevel ontology (consisting\n      of superclasses and subclasses) to sample episode classes.\n    split: A learning_spec.Split object identifying the source (meta-)split.\n    episode_descr_config: An instance of EpisodeDescriptionConfig containing\n      parameters relating to sampling shots and ways for episodes.\n    pool: String (optional), for example-split datasets, which example split to\n      use (\'train\', \'valid\', or \'test\'), used at meta-test time only.\n    shuffle_buffer_size: int or None, shuffle buffer size for each Dataset.\n    read_buffer_size_bytes: int or None, buffer size for each TFRecordDataset.\n    num_prefetch: int, the number of examples to prefetch for each class of each\n      dataset. Prefetching occurs just after the class-specific Dataset object\n      is constructed. If < 1, no prefetching occurs.\n    image_size: int, desired image size used during decoding.\n    num_to_take: Optional, an int specifying a number of elements to pick from\n      each class\' tfrecord. If specified, the available images of each class\n      will be restricted to that int. By default no restriction is applied and\n      all data is used.\n\n  Returns:\n    A Dataset instance that outputs tuples of fully-assembled and decoded\n      episodes zipped with the ID of their data source of origin.\n  """"""\n  use_all_classes = False\n  if pool is not None:\n    if not data.POOL_SUPPORTED:\n      raise NotImplementedError(\'Example-level splits or pools not supported.\')\n  if num_to_take is None:\n    num_to_take = -1\n  episode_reader = reader.EpisodeReader(dataset_spec, split,\n                                        shuffle_buffer_size,\n                                        read_buffer_size_bytes, num_prefetch,\n                                        num_to_take)\n  sampler = sampling.EpisodeDescriptionSampler(\n      episode_reader.dataset_spec,\n      split,\n      episode_descr_config,\n      pool=pool,\n      use_dag_hierarchy=use_dag_ontology,\n      use_bilevel_hierarchy=use_bilevel_ontology,\n      use_all_classes=use_all_classes)\n  dataset = episode_reader.create_dataset_input_pipeline(sampler, pool=pool)\n  # Episodes coming out of `dataset` contain flushed examples and are internally\n  # padded with dummy examples. `process_episode` discards flushed examples,\n  # splits the episode into support and query sets, removes the dummy examples\n  # and decodes the example strings.\n  chunk_sizes = sampler.compute_chunk_sizes()\n  map_fn = functools.partial(\n      process_episode, chunk_sizes=chunk_sizes, image_size=image_size)\n  dataset = dataset.map(map_fn)\n  # There is only one data source, so we know that all episodes belong to it,\n  # but for interface consistency, zip with a dataset identifying the source.\n  source_id_dataset = tf.data.Dataset.from_tensors(0).repeat()\n  dataset = tf.data.Dataset.zip((dataset, source_id_dataset))\n\n  # Overlap episode processing and training.\n  dataset = dataset.prefetch(1)\n  return dataset\n\n\ndef make_multisource_episode_pipeline(dataset_spec_list,\n                                      use_dag_ontology_list,\n                                      use_bilevel_ontology_list,\n                                      split,\n                                      episode_descr_config,\n                                      pool=None,\n                                      shuffle_buffer_size=None,\n                                      read_buffer_size_bytes=None,\n                                      num_prefetch=0,\n                                      image_size=None,\n                                      num_to_take=None):\n  """"""Returns a pipeline emitting data from multiple sources as Episodes.\n\n  Each episode only contains data from one single source. For each episode, its\n  source is sampled uniformly across all sources.\n\n  Args:\n    dataset_spec_list: A list of DatasetSpecification, one for each source.\n    use_dag_ontology_list: A list of Booleans, one for each source: whether to\n      use that source\'s DAG-structured ontology to sample episode classes.\n    use_bilevel_ontology_list: A list of Booleans, one for each source: whether\n      to use that source\'s bi-level ontology to sample episode classes.\n    split: A learning_spec.Split object identifying the sources split. It is the\n      same for all datasets.\n    episode_descr_config: An instance of EpisodeDescriptionConfig containing\n      parameters relating to sampling shots and ways for episodes.\n    pool: String (optional), for example-split datasets, which example split to\n      use (\'train\', \'valid\', or \'test\'), used at meta-test time only.\n    shuffle_buffer_size: int or None, shuffle buffer size for each Dataset.\n    read_buffer_size_bytes: int or None, buffer size for each TFRecordDataset.\n    num_prefetch: int, the number of examples to prefetch for each class of each\n      dataset. Prefetching occurs just after the class-specific Dataset object\n      is constructed. If < 1, no prefetching occurs.\n    image_size: int, desired image size used during decoding.\n    num_to_take: Optional, a list specifying for each dataset the number of\n      examples per class to restrict to (for this given split). If provided, its\n      length must be the same as len(dataset_spec). If None, no restrictions are\n      applied to any dataset and all data per class is used.\n\n  Returns:\n    A Dataset instance that outputs tuples of fully-assembled and decoded\n      episodes zipped with the ID of their data source of origin.\n  """"""\n  if pool is not None:\n    if not data.POOL_SUPPORTED:\n      raise NotImplementedError(\'Example-level splits or pools not supported.\')\n  if num_to_take is not None and len(num_to_take) != len(dataset_spec_list):\n    raise ValueError(\'num_to_take does not have the same length as \'\n                     \'dataset_spec_list.\')\n  if num_to_take is None:\n    num_to_take = [-1] * len(dataset_spec_list)\n  sources = []\n  for source_id, (dataset_spec, use_dag_ontology, use_bilevel_ontology,\n                  num_to_take_for_dataset) in enumerate(\n                      zip(dataset_spec_list, use_dag_ontology_list,\n                          use_bilevel_ontology_list, num_to_take)):\n    episode_reader = reader.EpisodeReader(dataset_spec, split,\n                                          shuffle_buffer_size,\n                                          read_buffer_size_bytes, num_prefetch,\n                                          num_to_take_for_dataset)\n    sampler = sampling.EpisodeDescriptionSampler(\n        episode_reader.dataset_spec,\n        split,\n        episode_descr_config,\n        pool=pool,\n        use_dag_hierarchy=use_dag_ontology,\n        use_bilevel_hierarchy=use_bilevel_ontology)\n    dataset = episode_reader.create_dataset_input_pipeline(sampler, pool=pool)\n    # Create a dataset to zip with the above for identifying the source.\n    source_id_dataset = tf.data.Dataset.from_tensors(source_id).repeat()\n    sources.append(tf.data.Dataset.zip((dataset, source_id_dataset)))\n\n  # Sample uniformly among sources.\n  dataset = tf.data.experimental.sample_from_datasets(sources)\n\n  # Episodes coming out of `dataset` contain flushed examples and are internally\n  # padded with dummy examples. `process_episode` discards flushed examples,\n  # splits the episode into support and query sets, removes the dummy examples\n  # and decodes the example strings.\n  chunk_sizes = sampler.compute_chunk_sizes()\n\n  def map_fn(episode, source_id):\n    return process_episode(\n        *episode, chunk_sizes=chunk_sizes, image_size=image_size), source_id\n\n  dataset = dataset.map(map_fn)\n\n  # Overlap episode processing and training.\n  dataset = dataset.prefetch(1)\n  return dataset\n\n\ndef make_one_source_batch_pipeline(dataset_spec,\n                                   split,\n                                   batch_size,\n                                   pool=None,\n                                   shuffle_buffer_size=None,\n                                   read_buffer_size_bytes=None,\n                                   num_prefetch=0,\n                                   image_size=None,\n                                   num_to_take=None):\n  """"""Returns a pipeline emitting data from one single source as Batches.\n\n  Args:\n    dataset_spec: A DatasetSpecification object defining what to read from.\n    split: A learning_spec.Split object identifying the source split.\n    batch_size: An int representing the max number of examples in each batch.\n    pool: String (optional), for example-split datasets, which example split to\n      use (\'valid\', or \'test\'), used at meta-test time only.\n    shuffle_buffer_size: int or None, number of examples in the buffer used for\n      shuffling the examples from different classes, while they are mixed\n      together. There is only one shuffling operation, not one per class.\n    read_buffer_size_bytes: int or None, buffer size for each TFRecordDataset.\n    num_prefetch: int, the number of examples to prefetch for each class of each\n      dataset. Prefetching occurs just after the class-specific Dataset object\n      is constructed. If < 1, no prefetching occurs.\n    image_size: int, desired image size used during decoding.\n    num_to_take: Optional, an int specifying a number of elements to pick from\n      each class\' tfrecord. If specified, the available images of each class\n      will be restricted to that int. By default no restriction is applied and\n      all data is used.\n\n  Returns:\n    A Dataset instance that outputs decoded batches from all classes in the\n    split.\n  """"""\n  if num_to_take is None:\n    num_to_take = -1\n  batch_reader = reader.BatchReader(dataset_spec, split, shuffle_buffer_size,\n                                    read_buffer_size_bytes, num_prefetch,\n                                    num_to_take)\n  dataset = batch_reader.create_dataset_input_pipeline(\n      batch_size=batch_size, pool=pool)\n  map_fn = functools.partial(process_batch, image_size=image_size)\n  dataset = dataset.map(map_fn)\n\n  # There is only one data source, so we know that all batches belong to it,\n  # but for interface consistency, zip with a dataset identifying the source.\n  source_id_dataset = tf.data.Dataset.from_tensors(0).repeat()\n  dataset = tf.data.Dataset.zip((dataset, source_id_dataset))\n\n  # Overlap episode processing and training.\n  dataset = dataset.prefetch(1)\n  return dataset\n\n\n# TODO(lamblinp): Update this option\'s name\n@gin.configurable(\'BatchSplitReaderGetReader\', whitelist=[\'add_dataset_offset\'])\ndef make_multisource_batch_pipeline(dataset_spec_list,\n                                    split,\n                                    batch_size,\n                                    add_dataset_offset,\n                                    pool=None,\n                                    shuffle_buffer_size=None,\n                                    read_buffer_size_bytes=None,\n                                    num_prefetch=0,\n                                    image_size=None,\n                                    num_to_take=None):\n  """"""Returns a pipeline emitting data from multiple source as Batches.\n\n  Args:\n    dataset_spec_list: A list of DatasetSpecification, one for each source.\n    split: A learning_spec.Split object identifying the source split.\n    batch_size: An int representing the max number of examples in each batch.\n    add_dataset_offset: A Boolean, whether to add an offset to each dataset\'s\n      targets, so that each target is unique across all datasets.\n    pool: String (optional), for example-split datasets, which example split to\n      use (\'valid\', or \'test\'), used at meta-test time only.\n    shuffle_buffer_size: int or None, number of examples in the buffer used for\n      shuffling the examples from different classes, while they are mixed\n      together. There is only one shuffling operation, not one per class.\n    read_buffer_size_bytes: int or None, buffer size for each TFRecordDataset.\n    num_prefetch: int, the number of examples to prefetch for each class of each\n      dataset. Prefetching occurs just after the class-specific Dataset object\n      is constructed. If < 1, no prefetching occurs.\n    image_size: int, desired image size used during decoding.\n    num_to_take: Optional, a list specifying for each dataset the number of\n      examples per class to restrict to (for this given split). If provided, its\n      length must be the same as len(dataset_spec). If None, no restrictions are\n      applied to any dataset and all data per class is used.\n\n  Returns:\n    A Dataset instance that outputs decoded batches from all classes in the\n    split.\n  """"""\n  if num_to_take is not None and len(num_to_take) != len(dataset_spec_list):\n    raise ValueError(\'num_to_take does not have the same length as \'\n                     \'dataset_spec_list.\')\n  if num_to_take is None:\n    num_to_take = [-1] * len(dataset_spec_list)\n  sources = []\n  offset = 0\n  for source_id, (dataset_spec, num_to_take_for_dataset) in enumerate(\n      zip(dataset_spec_list, num_to_take)):\n    batch_reader = reader.BatchReader(dataset_spec, split, shuffle_buffer_size,\n                                      read_buffer_size_bytes, num_prefetch,\n                                      num_to_take_for_dataset)\n    dataset = batch_reader.create_dataset_input_pipeline(\n        batch_size=batch_size, pool=pool, offset=offset)\n    # Create a dataset to zip with the above for identifying the source.\n    source_id_dataset = tf.data.Dataset.from_tensors(source_id).repeat()\n    sources.append(tf.data.Dataset.zip((dataset, source_id_dataset)))\n    if add_dataset_offset:\n      offset += len(dataset_spec.get_classes(split))\n\n  # Sample uniformly among sources\n  dataset = tf.data.experimental.sample_from_datasets(sources)\n\n  def map_fn(batch, source_id):\n    return process_batch(*batch, image_size=image_size), source_id\n\n  dataset = dataset.map(map_fn)\n\n  # Overlap episode processing and training.\n  dataset = dataset.prefetch(1)\n  return dataset\n'"
meta_dataset/data/pipeline_test.py,2,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\nr""""""Tests for meta_dataset.data.pipeline.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nimport gin\nfrom meta_dataset.data import config\nfrom meta_dataset.data import learning_spec\nfrom meta_dataset.data import pipeline\nfrom meta_dataset.data import test_utils\nfrom meta_dataset.data.dataset_spec import DatasetSpecification\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\n\nclass PipelineTest(tf.test.TestCase):\n\n  def test_make_multisource_episode_pipeline_feature(self):\n\n    # Create some feature records and write them to a temp directory.\n    feat_size = 64\n    num_examples = 100\n    num_classes = 10\n    output_path = self.get_temp_dir()\n    gin.parse_config_file(\n        \'third_party/py/meta_dataset/learn/gin/setups/data_config_feature.gin\')\n\n    # 1-Write feature records to temp directory.\n    self.rng = np.random.RandomState(0)\n    class_features = []\n    for class_id in range(num_classes):\n      features = self.rng.randn(num_examples, feat_size).astype(np.float32)\n      label = np.array(class_id).astype(np.int64)\n      output_file = os.path.join(output_path, str(class_id) + \'.tfrecords\')\n      test_utils.write_feature_records(features, label, output_file)\n      class_features.append(features)\n    class_features = np.stack(class_features)\n\n    # 2-Read records back using multi-source pipeline.\n    # DatasetSpecification to use in tests\n    dataset_spec = DatasetSpecification(\n        name=None,\n        classes_per_split={\n            learning_spec.Split.TRAIN: 5,\n            learning_spec.Split.VALID: 2,\n            learning_spec.Split.TEST: 3\n        },\n        images_per_class={i: num_examples for i in range(num_classes)},\n        class_names=None,\n        path=output_path,\n        file_pattern=\'{}.tfrecords\')\n\n    # Duplicate the dataset to simulate reading from multiple datasets.\n    use_bilevel_ontology_list = [False] * 2\n    use_dag_ontology_list = [False] * 2\n    all_dataset_specs = [dataset_spec] * 2\n\n    fixed_ways_shots = config.EpisodeDescriptionConfig(\n        num_query=5, num_support=5, num_ways=5)\n\n    dataset_episodic = pipeline.make_multisource_episode_pipeline(\n        dataset_spec_list=all_dataset_specs,\n        use_dag_ontology_list=use_dag_ontology_list,\n        use_bilevel_ontology_list=use_bilevel_ontology_list,\n        episode_descr_config=fixed_ways_shots,\n        split=learning_spec.Split.TRAIN,\n        image_size=None)\n\n    episode, _ = self.evaluate(\n        dataset_episodic.make_one_shot_iterator().get_next())\n\n    # 3-Check that support and query features are in class_features and have\n    # the correct corresponding label.\n    support_features, support_class_ids = episode[0], episode[2]\n    query_features, query_class_ids = episode[3], episode[5]\n\n    for feat, class_id in zip(list(support_features), list(support_class_ids)):\n      abs_err = np.abs(np.sum(class_features - feat[None][None], axis=-1))\n      # Make sure the feature is present in the original data.\n      self.assertEqual(abs_err.min(), 0.0)\n      found_class_id = np.where(abs_err == 0.0)[0][0]\n      self.assertEqual(found_class_id, class_id)\n\n    for feat, class_id in zip(list(query_features), list(query_class_ids)):\n      abs_err = np.abs(np.sum(class_features - feat[None][None], axis=-1))\n      # Make sure the feature is present in the original data.\n      self.assertEqual(abs_err.min(), 0.0)\n      found_class_id = np.where(abs_err == 0.0)[0][0]\n      self.assertEqual(found_class_id, class_id)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
meta_dataset/data/providers.py,10,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Interfaces for data returned by the pipelines.\n\nTODO(lamblinp): Integrate better with pipeline.py.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow.compat.v1 as tf\n\n\ndef compute_shot(way, labels):\n  """"""Computes the `shot` of the episode containing labels.\n\n  Args:\n    way: An int constant tensor. The number of classes in the episode.\n    labels: A Tensor of labels of shape [batch_size].\n\n  Returns:\n    shots: An int 1D tensor: The number of support examples per class.\n  """"""\n  class_ids = tf.reshape(tf.range(way), [way, 1])\n  class_labels = tf.reshape(labels, [1, -1])\n  is_equal = tf.equal(class_labels, class_ids)\n  return tf.reduce_sum(tf.cast(is_equal, tf.int32), axis=1)\n\n\ndef compute_unique_class_ids(class_ids):\n  """"""Computes the unique class IDs of the episode containing `class_ids`.\n\n  Args:\n    class_ids: A 1D tensor representing class IDs, one per example in an\n      episode.\n\n  Returns:\n    A 1D tensor of the unique class IDs whose size is equal to the way of an\n    episode.\n  """"""\n  return tf.unique(class_ids)[0]\n\n\nclass EpisodeDataset(\n    collections.namedtuple(\n        \'EpisodeDataset\', \'train_images, test_images, \'\n        \'train_labels, test_labels, train_class_ids, test_class_ids\')):\n  """"""Wraps an episode\'s data and facilitates creation of feed dict.\n\n    Args:\n      train_images: A Tensor of images for training.\n      test_images: A Tensor of images for testing.\n      train_labels: A 1D Tensor, the matching training labels (numbers between 0\n        and K-1, with K the number of classes involved in the episode).\n      test_labels: A 1D Tensor, the matching testing labels (numbers between 0\n        and K-1, with K the number of classes involved in the episode).\n      train_class_ids: A 1D Tensor, the matching training class ids (numbers\n        between 0 and N-1, with N the number of classes in the full dataset).\n      test_class_ids: A 1D Tensor, the matching testing class ids (numbers\n        between 0 and N-1, with N the number of classes in the full dataset).\n  """"""\n\n  @property\n  def unique_class_ids(self):\n    return compute_unique_class_ids(\n        tf.concat((self.train_class_ids, self.test_class_ids), -1))\n\n  @property\n  def train_shots(self):\n    return compute_shot(self.way, self.train_labels)\n\n  @property\n  def test_shots(self):\n    return compute_shot(self.way, self.test_labels)\n\n  # TODO(evcu) We should probably calculate way from unique labels, not\n  # class_ids.\n  @property\n  def way(self):\n    return tf.size(self.unique_class_ids)\n\n  @property\n  def labels(self):\n    """"""Return query labels to provide an episodic/batch-agnostic API.""""""\n    return self.test_labels\n\n  @property\n  def onehot_labels(self):\n    """"""Return one-hot query labels to provide an episodic/batch-agnostic API.""""""\n    return self.onehot_test_labels\n\n  @property\n  def onehot_train_labels(self):\n    return tf.one_hot(self.train_labels, self.way)\n\n  @property\n  def onehot_test_labels(self):\n    return tf.one_hot(self.test_labels, self.way)\n\n\nclass Batch(collections.namedtuple(\'Batch\', \'images, labels, n_classes\')):\n  """"""Wraps an batch\'s data and facilitates creation of feed dict.\n\n    Args:\n      images: a Tensor of images of shape [self.batch_size] + image shape.\n      labels: a Tensor of labels of shape [self.batch_size].\n      n_classes: a scalar int Tensor, the total number of available classes\n        (labels). Used to express targets as 1-hot vectors.\n  """"""\n\n  @property\n  def way(self):\n    """"""Compute the way of the episode.\n\n    Returns:\n      way: An int, the number of possible classes in the dataset.\n    """"""\n    return self.n_classes\n\n  @property\n  def onehot_labels(self):\n    return tf.one_hot(self.labels, self.way)\n'"
meta_dataset/data/reader.py,22,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Forming the first part of a tf.data pipeline, reading from a source on disk.\n\nThe data output by the Reader consists in episodes or batches (for EpisodeReader\nand BatchReader respectively) from one source (one split of a dataset). They\ncontain strings represented images that have not been decoded yet, and can\ncontain dummy examples and examples to discard.\nSee data/pipeline.py for the next stage of the pipeline.\n""""""\n# TODO(lamblinp): Update variable names to be more consistent\n# - target, class_idx, label\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport itertools\nimport os\n\nfrom meta_dataset import data\nimport numpy as np\nfrom six.moves import range\nimport tensorflow.compat.v1 as tf\n\n# DUMMY_CLASS_ID will be used as the target of examples used for padding only.\nDUMMY_CLASS_ID = -1\n\n\ndef _pad(dataset_indices, chunk_size, dummy_dataset_id):\n  """"""Pads `dataset_indices` with dummy values so it has length `chunk_size`.\n\n  Args:\n    dataset_indices: list of int, dataset indices.\n    chunk_size: int, size to pad to.\n    dummy_dataset_id: int, dummy value to pad with.\n  """"""\n  pad_size = chunk_size - len(dataset_indices)\n  assert pad_size >= 0\n  dataset_indices.extend([dummy_dataset_id] * pad_size)\n\n\ndef dataset_id_generator(dataset_spec, split, pool, sampler):\n  """"""Generates a stream of dataset IDs forming a sequence of episodes.\n\n  Each episode is chunked into:\n\n  * a ""flush"" chunk, which is meant to allow to flush examples, in case we are\n    at the end of an epoch for one or more class in the episode (we want to\n    avoid accidentally repeating an example due to epoch boundaries), and\n  * some number of additional chunks (for example, a ""support"" chunk and a\n    ""query"" chunk).\n\n  To make sure the input pipeline knows where the episode boundary is within the\n  stream (and where the boundary is between chunks in an episode), we enforce\n  that each chunk has a fixed size by padding with dummy dataset IDs (of value\n  `num_classes`) as needed (in some cases it\'s possible that no padding is ever\n  needed). The size of each chunk is prescribed by the `compute_chunk_sizes`\n  method of `sampler`, which also implicitly defines the number of additional\n  chunks (i.e. `len(chunk_sizes) - 1`).\n\n  This generator is meant to be used with\n  `tf.data.experimental.choose_from_datasets` and assumes that the list of\n  tf.data.Dataset objects corresponding to each class in the dataset (there are\n  `num_classes` of them, which is determined by inspecting the `dataset_spec`\n  argument using the `split` argument) is appended with a ""dummy"" Dataset (which\n  has index `num_classes` in the list) which outputs a constant `(b\'\',\n  DUMMY_CLASS_ID)` tuple).\n\n  Note that a dataset ID is different from the (absolute) class ID: the dataset\n  ID refers to the index of the Dataset in the list of Dataset objects, and the\n  class ID (or label) refers to the second element of the tuple that the Dataset\n  outputs.\n\n  Args:\n    dataset_spec: DatasetSpecification, dataset specification.\n    split: one of Split.TRAIN, Split.VALID, or Split.TEST.\n    pool: A string (\'train\' or \'test\') or None, indicating which example-level\n      split to select, if the current dataset has them.\n    sampler: EpisodeDescriptionSampler instance.\n\n  Yields:\n    i: int, dataset ID.\n  """"""\n  chunk_sizes = sampler.compute_chunk_sizes()\n  # An episode always starts with a ""flush"" chunk to allow flushing examples at\n  # class epoch boundaries, and contains `len(chunk_sizes) - 1` additional\n  # chunks.\n  flush_chunk_size, other_chunk_sizes = chunk_sizes[0], chunk_sizes[1:]\n\n  class_set = dataset_spec.get_classes(split)\n  num_classes = len(class_set)\n  dummy_dataset_id = num_classes\n\n  total_images_per_class = dict(\n      (class_idx,\n       dataset_spec.get_total_images_per_class(class_set[class_idx], pool))\n      for class_idx in range(num_classes))\n  cursors = [0] * num_classes\n\n  # Infinite loop over episodes.\n  while True:\n    flushed_dataset_indices = []\n    selected_dataset_indices = [[] for _ in other_chunk_sizes]\n    # Sample an episode description. A description is a tuple of\n    # `(class_idx, ...)` tuples, where `class_idx` indicates the class to sample\n    # from and the remaining `len(chunk_sizes) - 1` elements indicate how many\n    # examples to allocate to each chunk.\n    episode_description = sampler.sample_episode_description()\n    for element in episode_description:\n      class_idx, distribution = element[0], element[1:]\n      total_requested = sum(distribution)\n      if total_requested > total_images_per_class[class_idx]:\n        raise ValueError(""Requesting more images than what\'s available for the ""\n                         \'whole class\')\n      # If the total number of requested examples is greater than the number of\n      # examples remaining for the current pass over class `class_idx`, we flush\n      # the remaining examples and start a new pass over class `class_idx`.\n      # TODO(lamblinp): factor this out into its own tracker class for\n      # readability and testability.\n      remaining = total_images_per_class[class_idx] - cursors[class_idx]\n      if total_requested > remaining:\n        flushed_dataset_indices.extend([class_idx] * remaining)\n        cursors[class_idx] = 0\n      # Elements of `distribution` correspond to how many examples of class\n      # `class_idx` to allocate for each chunk (e.g. in a few-shot learning\n      # context `distribution = [5, 8]` would allocate 5 examples to the\n      # ""support"" chunk and 8 examples to the ""query"" chunk). Elements of\n      # `selected_dataset_indices` correspond to the list of dataset indices\n      # that have so far been requested for each chunk.\n      for num_to_allocate, dataset_indices in zip(distribution,\n                                                  selected_dataset_indices):\n        dataset_indices.extend([class_idx] * num_to_allocate)\n      cursors[class_idx] += total_requested\n\n    # An episode sequence is generated in multiple phases, each padded with an\n    # agreed-upon number of dummy dataset IDs.\n\n    _pad(flushed_dataset_indices, flush_chunk_size, dummy_dataset_id)\n    for dataset_indices, chunk_size in zip(selected_dataset_indices,\n                                           other_chunk_sizes):\n      _pad(dataset_indices, chunk_size, dummy_dataset_id)\n\n    # Yield dataset IDs one by one.\n    # TODO(lamblinp): revisit yielding the whole list at once rather than\n    # element by element for performance reasons.\n    dataset_indices = itertools.chain(flushed_dataset_indices,\n                                      *selected_dataset_indices)\n    for i in dataset_indices:\n      yield i\n\n\nclass Reader(object):\n  """"""Class reading data from one source and assembling examples.\n\n  Specifically, it holds part of a tf.data pipeline (the source-specific part),\n  that reads data from TFRecords and assembles examples from them.\n  """"""\n\n  def __init__(self,\n               dataset_spec,\n               split,\n               shuffle_buffer_size,\n               read_buffer_size_bytes,\n               num_prefetch,\n               num_to_take=-1):\n    """"""Initializes a Reader from a source.\n\n    The source is identified by dataset_spec and split.\n\n    Args:\n      dataset_spec: DatasetSpecification, dataset specification.\n      split: A learning_spec.Split object identifying the source split.\n      shuffle_buffer_size: An integer, the shuffle buffer size for each Dataset\n        object. If 0, no shuffling operation will happen.\n      read_buffer_size_bytes: int or None, buffer size for each TFRecordDataset.\n      num_prefetch: int, the number of examples to prefetch for each class of\n        each dataset. Prefetching occurs just after the class-specific Dataset\n        object is constructed. If < 1, no prefetching occurs.\n      num_to_take: Optional, an int specifying a number of elements to pick from\n        each tfrecord. If specified, the available images of each class will be\n        restricted to that int. By default (-1) no restriction is applied and\n        all data is used.\n    """"""\n    self.dataset_spec = dataset_spec\n    self.split = split\n    self.shuffle_buffer_size = shuffle_buffer_size\n    self.read_buffer_size_bytes = read_buffer_size_bytes\n    self.num_prefetch = num_prefetch\n    self.num_to_take = num_to_take\n\n    self.base_path = self.dataset_spec.path\n    self.class_set = self.dataset_spec.get_classes(self.split)\n    self.num_classes = len(self.class_set)\n\n  def construct_class_datasets(self,\n                               pool=None,\n                               repeat=True,\n                               shuffle=True,\n                               shuffle_seed=None):\n    """"""Constructs the list of class datasets.\n\n    Args:\n      pool: A string (optional) indicating whether to only read examples from a\n        given example-level split.\n      repeat: Boolean indicating whether each of the class datasets should be\n        repeated (to provide an infinite stream) or not.\n      shuffle: Boolean indicating whether each of the class datasets should be\n        shuffled or not.\n      shuffle_seed: Optional, an int containing the seed passed to\n        tf.data.Dataset.shuffle.\n\n    Returns:\n      class_datasets: list of tf.data.Dataset, one for each class.\n    """"""\n    file_pattern = self.dataset_spec.file_pattern\n    # We construct one dataset object per class. Each dataset outputs a stream\n    # of `(example_string, dataset_id)` tuples.\n    class_datasets = []\n    for dataset_id in range(self.num_classes):\n      class_id = self.class_set[dataset_id]\n      if pool:\n        if not data.POOL_SUPPORTED:\n          raise NotImplementedError(\n              \'Example-level splits or pools not supported.\')\n      else:\n        if file_pattern.startswith(\'{}_{}\'):\n          # TODO(lamblinp): Add support for sharded files if needed.\n          raise NotImplementedError(\'Sharded files are not supported yet. \'\n                                    \'The code expects one dataset per class.\')\n        elif file_pattern.startswith(\'{}\'):\n          filename = os.path.join(self.base_path, file_pattern.format(class_id))\n        else:\n          raise ValueError(\'Unsupported file_pattern in DatasetSpec: %s. \'\n                           \'Expected something starting with ""{}"" or ""{}_{}"".\' %\n                           file_pattern)\n\n      example_string_dataset = tf.data.TFRecordDataset(\n          filename, buffer_size=self.read_buffer_size_bytes)\n\n      # Create a dataset containing only num_to_take elements from\n      # example_string_dataset. By default, takes all elements.\n      example_string_dataset = example_string_dataset.take(self.num_to_take)\n\n      if self.num_prefetch > 0:\n        example_string_dataset = example_string_dataset.prefetch(\n            self.num_prefetch)\n      if shuffle:\n        # Do not set a buffer size greater than the number of examples in this\n        # class, as it can result in unnecessary memory being allocated.\n        num_examples = self.dataset_spec.get_total_images_per_class(\n            class_id, pool=pool)\n        shuffle_buffer_size = min(num_examples, self.shuffle_buffer_size)\n        if shuffle_buffer_size > 1:\n          example_string_dataset = example_string_dataset.shuffle(\n              buffer_size=shuffle_buffer_size,\n              seed=shuffle_seed,\n              reshuffle_each_iteration=True)\n      if repeat:\n        example_string_dataset = example_string_dataset.repeat()\n\n      # These are absolute, dataset-specific class IDs (not relative to a given\n      # split). It is okay to have class ID collisions across datasets, since we\n      # don\'t sample multi-dataset episodes.\n      class_id_dataset = tf.data.Dataset.from_tensors(class_id).repeat()\n      dataset = tf.data.Dataset.zip((example_string_dataset, class_id_dataset))\n      class_datasets.append(dataset)\n\n    assert len(class_datasets) == self.num_classes\n    return class_datasets\n\n\nclass EpisodeReaderMixin(object):\n  """"""Mixin class to assemble examples as episodes.""""""\n\n  def create_dataset_input_pipeline(self,\n                                    sampler,\n                                    pool=None,\n                                    shuffle_seed=None):\n    """"""Creates a Dataset encapsulating the input pipeline for one data source.\n\n    Args:\n      sampler: EpisodeDescriptionSampler instance.\n      pool: A string (optional) indicating whether to only read examples from a\n        given example-level split.\n      shuffle_seed: Optional, an int containing the seed passed to\n        tf.data.Dataset.shuffle.\n\n    Returns:\n      dataset: a tf.data.Dataset instance which encapsulates episode creation\n        for the data identified by `dataset_spec` and `split`. These episodes\n        contain flushed examples and are internally padded with dummy examples.\n        A later part of the pipeline, shared across all sources, will extract\n        support and query sets and decode the example strings.\n    """"""\n    # Always shuffle, unless self.shuffle_buffer_size is 0\n    shuffle = (self.shuffle_buffer_size and self.shuffle_buffer_size > 0)\n    class_datasets = self.construct_class_datasets(\n        pool=pool, shuffle=shuffle, shuffle_seed=shuffle_seed)\n\n    # We also construct a dummy dataset which outputs `(b\'\', DUMMY_CLASS_ID)`\n    # tuples.\n    dummy_dataset = tf.data.Dataset.zip(\n        (tf.data.Dataset.from_tensors(b\'\').repeat(),\n         tf.data.Dataset.from_tensors(DUMMY_CLASS_ID).repeat()))\n    class_datasets.append(dummy_dataset)\n\n    # The ""choice"" dataset outputs a stream of dataset IDs which are used to\n    # select which class dataset to sample from. We turn the stream of dataset\n    # IDs into a stream of `(example_string, class_id)` tuples using\n    # `choose_from_datasets`.\n    choice_generator = functools.partial(\n        dataset_id_generator,\n        dataset_spec=self.dataset_spec,\n        split=self.split,\n        pool=pool,\n        sampler=sampler)\n\n    choice_dataset = tf.data.Dataset.from_generator(choice_generator,\n                                                    (tf.int64),\n                                                    tf.TensorShape([]))\n    dataset = tf.data.experimental.choose_from_datasets(class_datasets,\n                                                        choice_dataset)\n\n    # Episodes have a fixed size prescribed by `sampler.compute_chunk_sizes`.\n    dataset = dataset.batch(sum(sampler.compute_chunk_sizes()))\n    # Overlap batching and episode processing.\n    dataset = dataset.prefetch(1)\n\n    return dataset\n\n\nclass EpisodeReader(Reader, EpisodeReaderMixin):\n  """"""Subclass of Reader assembling the examples as Episodes.""""""\n\n\ndef add_offset_to_target(example_strings, targets, offset):\n  """"""Adds offset to the targets.\n\n  This function is intented to be passed to tf.data.Dataset.map.\n\n  Args:\n    example_strings: 1-D Tensor of dtype str, Example protocol buffers.\n    targets: 1-D Tensor of dtype int, targets representing the absolute class\n      IDs.\n    offset: int, optional, number to add to class IDs to get targets.\n\n  Returns:\n    example_strings, labels: Tensors, a batch of examples and labels.\n  """"""\n  labels = targets + offset\n  return (example_strings, labels)\n\n\nclass BatchReaderMixin(object):\n  """"""Mixin class to assemble examples as batches.""""""\n\n  def create_dataset_input_pipeline(self,\n                                    batch_size,\n                                    offset=0,\n                                    pool=None,\n                                    shuffle_seed=None):\n    """"""Creates a Dataset encapsulating the input pipeline for one data source.\n\n    Args:\n      batch_size: An int representing the max number of examples in each batch.\n      offset: An int, that is added to the value of all the targets. This makes\n        it possible to have a unique range of targets for each dataset.\n      pool: A string (optional) indicating whether to only read examples from a\n        given example-level split. If it is provided, these examples will be\n        used as \'real test data\', and used once each for evaluation only. The\n        accepted values are \'valid\' and \'test\'.\n      shuffle_seed: Optional, an int containing the seed passed to\n        tf.data.Dataset.shuffle.\n\n    Returns:\n      dataset: a tf.data.Dataset instance which encapsulates batch creation for\n        the data identified by `dataset_spec` and `split`. These batches contain\n        compressed image representations and (possibly offset) absolute class\n        IDs. A later part of the pipeline, shared across all sources, will\n        decode the example strings.\n\n    Raises:\n      ValueError: Invalid pool provided. The supported values are \'valid\' and\n        \'test\'.\n    """"""\n    if pool and pool not in [\'valid\', \'test\']:\n      raise ValueError(\'Invalid pool provided. The supported values \'\n                       \'are ""valid"" and ""test"".\')\n    # Do not shuffle or repeat each class dataset, to avoid fuzzing epoch\n    # boundaries.\n    class_datasets = self.construct_class_datasets(\n        pool=pool, repeat=False, shuffle=False)\n    num_classes = len(class_datasets)\n\n    if pool:\n      if not data.POOL_SUPPORTED:\n        raise NotImplementedError(\n            \'Example-level splits or pools not supported.\')\n    else:\n      # To have labels start at 0 and be contiguous, subtracting the starting\n      # index from all\n      start_ind = self.class_set[0]\n      class_set = [\n          self.class_set[ds_id] - start_ind for ds_id in range(num_classes)\n      ]\n      if list(class_set) != list(range(num_classes)):\n        raise NotImplementedError(\'Batch training currently assumes the class \'\n                                  \'set is contiguous and starts at 0.\')\n\n      # Sample from each class dataset according to its proportion of examples,\n      # so examples from one class should be spread across the whole epoch.\n      # Then, shuffle and repeat the combined dataset.\n      num_examples_per_class = [\n          self.dataset_spec.get_total_images_per_class(class_id, pool=pool)\n          for class_id in class_set\n      ]\n      num_examples_per_class = np.array(num_examples_per_class, \'float64\')\n      class_proportions = num_examples_per_class / num_examples_per_class.sum()\n\n      dataset = tf.data.experimental.sample_from_datasets(\n          class_datasets, weights=class_proportions, seed=shuffle_seed)\n      if self.shuffle_buffer_size and self.shuffle_buffer_size > 0:\n        dataset = dataset.shuffle(\n            buffer_size=self.shuffle_buffer_size,\n            seed=shuffle_seed,\n            reshuffle_each_iteration=True)\n\n    # Using drop_remainder=False for two reasons:\n    # - Most importantly, during established splits evaluation, we need to\n    #   evaluate on all examples.\n    # - Also during training, if the shuffle buffer does not hold all the data,\n    #   the last examples are more likely to be dropped than the first ones.\n    # In any case, we are handling variable-sized batches just fine, so there\n    # is no real reason to drop data.\n    dataset = dataset.batch(batch_size, drop_remainder=False)\n    if not pool:\n      dataset = dataset.repeat()\n\n    if offset:\n      map_fn = functools.partial(add_offset_to_target, offset=offset)\n      dataset = dataset.map(map_fn)\n\n    # Overlap batching and episode processing.\n    dataset = dataset.prefetch(1)\n\n    return dataset\n\n\nclass BatchReader(Reader, BatchReaderMixin):\n  """"""Subclass of Reader assembling the examples as Batches.""""""\n'"
meta_dataset/data/reader_test.py,9,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Tests for Readers and related functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport itertools\n\nimport gin.tf\nfrom meta_dataset.data import config\nfrom meta_dataset.data import reader\nfrom meta_dataset.data import sampling\nfrom meta_dataset.data.dataset_spec import DatasetSpecification\nfrom meta_dataset.data.learning_spec import Split\nimport numpy as np\nfrom six.moves import range\nfrom six.moves import zip\nimport tensorflow.compat.v1 as tf\n\n# DatasetSpecification to use in tests\nDATASET_SPEC = DatasetSpecification(\n    name=None,\n    classes_per_split={\n        Split.TRAIN: 15,\n        Split.VALID: 5,\n        Split.TEST: 10\n    },\n    images_per_class=dict(enumerate([10, 20, 30] * 10)),\n    class_names=None,\n    path=None,\n    file_pattern=\'{}.tfrecords\')\n\n# Define defaults and set Gin configuration for EpisodeDescriptionConfig\nMIN_WAYS = 5\nMAX_WAYS_UPPER_BOUND = 50\nMAX_NUM_QUERY = 10\nMAX_SUPPORT_SET_SIZE = 500\nMAX_SUPPORT_SIZE_CONTRIB_PER_CLASS = 100\nMIN_LOG_WEIGHT = np.log(0.5)\nMAX_LOG_WEIGHT = np.log(2)\n\ngin.bind_parameter(\'EpisodeDescriptionConfig.num_ways\', None)\ngin.bind_parameter(\'EpisodeDescriptionConfig.num_support\', None)\ngin.bind_parameter(\'EpisodeDescriptionConfig.num_query\', None)\ngin.bind_parameter(\'EpisodeDescriptionConfig.min_ways\', MIN_WAYS)\ngin.bind_parameter(\'EpisodeDescriptionConfig.max_ways_upper_bound\',\n                   MAX_WAYS_UPPER_BOUND)\ngin.bind_parameter(\'EpisodeDescriptionConfig.max_num_query\', MAX_NUM_QUERY)\ngin.bind_parameter(\'EpisodeDescriptionConfig.max_support_set_size\',\n                   MAX_SUPPORT_SET_SIZE)\ngin.bind_parameter(\n    \'EpisodeDescriptionConfig.max_support_size_contrib_per_class\',\n    MAX_SUPPORT_SIZE_CONTRIB_PER_CLASS)\ngin.bind_parameter(\'EpisodeDescriptionConfig.min_log_weight\', MIN_LOG_WEIGHT)\ngin.bind_parameter(\'EpisodeDescriptionConfig.max_log_weight\', MAX_LOG_WEIGHT)\ngin.bind_parameter(\'EpisodeDescriptionConfig.ignore_dag_ontology\', False)\ngin.bind_parameter(\'EpisodeDescriptionConfig.ignore_bilevel_ontology\', False)\n\n\ndef split_into_chunks(batch, chunk_sizes):\n  """"""Returns batch split in 3 according to chunk_sizes.\n\n  Args:\n    batch: A sequence of length sum(chunk_sizes), usually examples or targets.\n    chunk_sizes: A tuple of 3 ints (flush_size, support_size, query_size).\n\n  Returns:\n    A tuple of 3 sequences (flush_chunk, support_chunk, query_chunk).\n  """"""\n  assert sum(chunk_sizes) == len(batch)\n  flush_chunk_size, support_chunk_size, _ = chunk_sizes\n  query_start = flush_chunk_size + support_chunk_size\n\n  flush_chunk = batch[:flush_chunk_size]\n  support_chunk = batch[flush_chunk_size:query_start]\n  query_chunk = batch[query_start:]\n\n  return (flush_chunk, support_chunk, query_chunk)\n\n\nclass DatasetIDGenTest(tf.test.TestCase):\n  """"""Tests `reader.dataset_id_generator`.""""""\n\n  def setUp(self):\n    super(DatasetIDGenTest, self).setUp()\n    self.dataset_spec = DATASET_SPEC\n    self.split = Split.TRAIN\n\n  def check_expected_structure(self, sampler):\n    """"""Checks the stream of dataset indices is as expected.""""""\n    chunk_sizes = sampler.compute_chunk_sizes()\n    batch_size = sum(chunk_sizes)\n    dummy_id = len(self.dataset_spec.get_classes(self.split))\n\n    generator = reader.dataset_id_generator(self.dataset_spec, self.split, None,\n                                            sampler)\n    for _ in range(3):\n      # Re-assemble batch.\n      # TODO(lamblinp): update if we change dataset_id_generator to return\n      # the whole batch at once\n      batch = list(itertools.islice(generator, batch_size))\n\n      self.assertEqual(len(batch), batch_size)\n      flush_chunk, support_chunk, query_chunk = split_into_chunks(\n          batch, chunk_sizes)\n\n      # flush_chunk is slightly oversized: if we actually had support_chunk_size\n      # + query_chunk_size examples remaining, we could have used them.\n      # Therefore, the last element of flush_chunk should be padding.\n      self.assertEqual(flush_chunk[-1], dummy_id)\n      # TODO(lamblinp): check more about the content of flush_chunk\n\n      # The padding should be at the end of each chunk.\n      for chunk in (flush_chunk, support_chunk, query_chunk):\n        num_actual_examples = sum(class_id != dummy_id for class_id in chunk)\n        self.assertNotIn(dummy_id, chunk[:num_actual_examples])\n        self.assertTrue(\n            all(dummy_id == class_id\n                for class_id in chunk[num_actual_examples:]))\n\n  def test_default(self):\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec,\n        self.split,\n        episode_descr_config=config.EpisodeDescriptionConfig())\n    self.check_expected_structure(sampler)\n\n  def test_fixed_query(self):\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec,\n        self.split,\n        episode_descr_config=config.EpisodeDescriptionConfig(num_query=5))\n    self.check_expected_structure(sampler)\n\n  def test_no_query(self):\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec,\n        self.split,\n        episode_descr_config=config.EpisodeDescriptionConfig(num_query=5))\n    self.check_expected_structure(sampler)\n\n  def test_fixed_shots(self):\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec,\n        self.split,\n        episode_descr_config=config.EpisodeDescriptionConfig(\n            num_support=3, num_query=7))\n    self.check_expected_structure(sampler)\n\n  def test_fixed_ways(self):\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec,\n        self.split,\n        episode_descr_config=config.EpisodeDescriptionConfig(num_ways=12))\n    self.check_expected_structure(sampler)\n\n  def test_fixed_episodes(self):\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec,\n        self.split,\n        episode_descr_config=config.EpisodeDescriptionConfig(\n            num_ways=12, num_support=3, num_query=7))\n    self.check_expected_structure(sampler)\n\n\ndef construct_dummy_datasets(class_ids,\n                             examples_per_class,\n                             repeat=True,\n                             shuffle=True,\n                             shuffle_seed=None):\n  """"""Construct a list of in-memory dummy datasets.\n\n  Args:\n    class_ids: A list of ints, one for each dataset to build.\n    examples_per_class: A list of int, how many examples there are in each\n      dataset.\n    repeat: A Boolean indicating whether each of the datasets should be repeated\n      (to provide an infinite stream).\n    shuffle: A Boolean indicating whether each dataset should be shuffled.\n    shuffle_seed: Optional, an int containing the seed passed to\n      tf.data.Dataset.shuffle.\n\n  Returns:\n    A list of tf.data.Dataset. Each one contains a series of pairs:\n    (a string formatted like \'<class_id>.<example_id>\', an int: class_id).\n  """"""\n  datasets = []\n  for i, class_id in enumerate(class_ids):\n    num_examples = examples_per_class[i]\n    example_string_dataset = tf.data.Dataset.from_tensor_slices(\n        [\'{}.{}\'.format(class_id, ex_id) for ex_id in range(num_examples)])\n    if shuffle:\n      example_string_dataset = example_string_dataset.shuffle(\n          buffer_size=num_examples,\n          seed=shuffle_seed,\n          reshuffle_each_iteration=True)\n    if repeat:\n      example_string_dataset = example_string_dataset.repeat()\n\n    class_id_dataset = tf.data.Dataset.from_tensors(class_id).repeat()\n    dataset = tf.data.Dataset.zip((example_string_dataset, class_id_dataset))\n    datasets.append(dataset)\n\n  return datasets\n\n\nclass DummyEpisodeReader(reader.EpisodeReader):\n  """"""Subclass of EpisodeReader that builds class datasets in-memory.""""""\n\n  def construct_class_datasets(self,\n                               pool=None,\n                               repeat=True,\n                               shuffle=True,\n                               shuffle_seed=None):\n    class_ids = [\n        self.class_set[dataset_id] for dataset_id in range(self.num_classes)\n    ]\n    examples_per_class = [\n        self.dataset_spec.get_total_images_per_class(class_id)\n        for class_id in class_ids\n    ]\n    shuffle = self.shuffle_buffer_size > 0\n    return construct_dummy_datasets(class_ids, examples_per_class, repeat,\n                                    shuffle, shuffle_seed)\n\n\nclass EpisodeReaderTest(tf.test.TestCase):\n  """"""Tests behaviour of Reader.\n\n  To avoid reading from the filesystem, we actually test a subclass,\n  DummyEpisodeReader, that overrides Reader.construct_class_datasets,\n  replacing it with a method building small, in-memory datasets instead.\n  """"""\n\n  def setUp(self):\n    super(EpisodeReaderTest, self).setUp()\n    self.dataset_spec = DATASET_SPEC\n    self.split = Split.TRAIN\n    self.shuffle_buffer_size = 30\n    self.read_buffer_size_bytes = None\n    self.num_prefetch = 0\n\n  def generate_episodes(self,\n                        sampler,\n                        num_episodes,\n                        shuffle=True,\n                        shuffle_seed=None):\n    dataset_spec = sampler.dataset_spec\n    split = sampler.split\n    if shuffle:\n      shuffle_buffer_size = self.shuffle_buffer_size\n    else:\n      shuffle_buffer_size = 0\n\n    episode_reader = DummyEpisodeReader(dataset_spec, split,\n                                        shuffle_buffer_size,\n                                        self.read_buffer_size_bytes,\n                                        self.num_prefetch)\n    input_pipeline = episode_reader.create_dataset_input_pipeline(\n        sampler, shuffle_seed=shuffle_seed)\n    iterator = input_pipeline.make_one_shot_iterator()\n    next_element = iterator.get_next()\n    with tf.Session() as sess:\n      episodes = [sess.run(next_element) for _ in range(num_episodes)]\n    return episodes\n\n  def check_episode_consistency(self, examples, targets, chunk_sizes):\n    """"""Tests that a given episode is correctly built and consistent.\n\n    In particular:\n    - test that examples come from the right class\n    - test that the overall ""flush, support, query"" structure is respected\n    - test that within each chunk, the padding is at the end\n\n    Args:\n      examples: A 1D array of strings.\n      targets: A 1D array of ints.\n      chunk_sizes: A tuple of 3 ints, describing the structure of the episode.\n    """"""\n    self.check_consistent_class(examples, targets)\n\n    batch_size = sum(chunk_sizes)\n    self.assertEqual(batch_size, len(examples), len(targets))\n\n    flush_examples, support_examples, query_examples = split_into_chunks(\n        examples, chunk_sizes)\n    flush_targets, support_targets, query_targets = split_into_chunks(\n        targets, chunk_sizes)\n\n    self.check_end_padding(flush_examples, flush_targets)\n    self.check_end_padding(support_examples, support_targets)\n    self.check_end_padding(query_examples, query_targets)\n\n  def check_consistent_class(self, examples, targets):\n    """"""Checks that the content of examples corresponds to the target.\n\n    This assumes the datasets were generated from `construct_dummy_datasets`,\n    with a dummy class of DUMMY_CLASS_ID with empty string examples.\n\n    Args:\n      examples: A 1D array of strings.\n      targets: A 1D array of ints.\n    """"""\n    self.assertEqual(len(examples), len(targets))\n    for (example, target) in zip(examples, targets):\n      if example:\n        expected_target, _ = example.decode().split(\'.\')\n        self.assertEqual(int(expected_target), target)\n      else:\n        self.assertEqual(target, reader.DUMMY_CLASS_ID)\n\n  def check_end_padding(self, examples_chunk, targets_chunk):\n    """"""Checks the padding is at the end of each chunk.\n\n    Args:\n      examples_chunk: A 1D array of strings.\n      targets_chunk: A 1D array of ints.\n    """"""\n    num_actual = sum(\n        class_id != reader.DUMMY_CLASS_ID for class_id in targets_chunk)\n    self.assertNotIn(reader.DUMMY_CLASS_ID, targets_chunk[:num_actual])\n    self.assertNotIn(b\'\', examples_chunk[:num_actual])\n    self.assertTrue(\n        all(reader.DUMMY_CLASS_ID == target\n            for target in targets_chunk[num_actual:]))\n    self.assertAllInSet(examples_chunk[num_actual:], [b\'\'])\n\n  def generate_and_check(self, sampler, num_episodes):\n    chunk_sizes = sampler.compute_chunk_sizes()\n    episodes = self.generate_episodes(sampler, num_episodes)\n    for episode in episodes:\n      examples, targets = episode\n      self.check_episode_consistency(examples, targets, chunk_sizes)\n\n  def test_train(self):\n    """"""Tests that a few episodes are consistent.""""""\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec,\n        Split.TRAIN,\n        episode_descr_config=config.EpisodeDescriptionConfig())\n    self.generate_and_check(sampler, 10)\n\n  def test_valid(self):\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec,\n        Split.VALID,\n        episode_descr_config=config.EpisodeDescriptionConfig())\n    self.generate_and_check(sampler, 10)\n\n  def test_test(self):\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec,\n        Split.TEST,\n        episode_descr_config=config.EpisodeDescriptionConfig())\n    self.generate_and_check(sampler, 10)\n\n  def test_fixed_query(self):\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec,\n        self.split,\n        episode_descr_config=config.EpisodeDescriptionConfig(num_query=5))\n    self.generate_and_check(sampler, 10)\n\n  def test_no_query(self):\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec,\n        self.split,\n        episode_descr_config=config.EpisodeDescriptionConfig(num_query=0))\n    self.generate_and_check(sampler, 10)\n\n  def test_fixed_shots(self):\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec,\n        self.split,\n        episode_descr_config=config.EpisodeDescriptionConfig(\n            num_support=3, num_query=7))\n    self.generate_and_check(sampler, 10)\n\n  def test_fixed_ways(self):\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec,\n        self.split,\n        episode_descr_config=config.EpisodeDescriptionConfig(num_ways=12))\n    self.generate_and_check(sampler, 10)\n\n  def test_fixed_episodes(self):\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec,\n        self.split,\n        episode_descr_config=config.EpisodeDescriptionConfig(\n            num_ways=12, num_support=3, num_query=7))\n    self.generate_and_check(sampler, 10)\n\n  def test_non_deterministic_shuffle(self):\n    """"""Different Readers generate different episode compositions.\n\n    Even with the same episode descriptions, the content should be different.\n    """"""\n    num_episodes = 10\n    init_rng = sampling.RNG\n    seed = 20181120\n    episode_streams = []\n    chunk_sizes = []\n    try:\n      for _ in range(2):\n        sampling.RNG = np.random.RandomState(seed)\n        sampler = sampling.EpisodeDescriptionSampler(\n            self.dataset_spec,\n            self.split,\n            episode_descr_config=config.EpisodeDescriptionConfig())\n        episodes = self.generate_episodes(sampler, num_episodes)\n        episode_streams.append(episodes)\n        chunk_size = sampler.compute_chunk_sizes()\n        chunk_sizes.append(chunk_size)\n        for examples, targets in episodes:\n          self.check_episode_consistency(examples, targets, chunk_size)\n\n    finally:\n      # Restore the original RNG\n      sampling.RNG = init_rng\n\n    self.assertEqual(chunk_sizes[0], chunk_sizes[1])\n\n    # It is unlikely that all episodes will be the same\n    num_identical_episodes = 0\n    for ((examples1, targets1), (examples2, targets2)) in zip(*episode_streams):\n      self.check_episode_consistency(examples1, targets1, chunk_sizes[0])\n      self.check_episode_consistency(examples2, targets2, chunk_sizes[1])\n      self.assertAllEqual(targets1, targets2)\n      if all(examples1 == examples2):\n        num_identical_episodes += 1\n\n    self.assertNotEqual(num_identical_episodes, num_episodes)\n\n  def test_deterministic_noshuffle(self):\n    """"""Tests episode generation determinism when there is noshuffle queue.""""""\n    num_episodes = 10\n    init_rng = sampling.RNG\n    seed = 20181120\n    episode_streams = []\n    chunk_sizes = []\n    try:\n      for _ in range(2):\n        sampling.RNG = np.random.RandomState(seed)\n        sampler = sampling.EpisodeDescriptionSampler(\n            self.dataset_spec,\n            self.split,\n            episode_descr_config=config.EpisodeDescriptionConfig())\n        episodes = self.generate_episodes(sampler, num_episodes, shuffle=False)\n        episode_streams.append(episodes)\n        chunk_size = sampler.compute_chunk_sizes()\n        chunk_sizes.append(chunk_size)\n        for examples, targets in episodes:\n          self.check_episode_consistency(examples, targets, chunk_size)\n\n    finally:\n      # Restore the original RNG\n      sampling.RNG = init_rng\n\n    self.assertEqual(chunk_sizes[0], chunk_sizes[1])\n\n    for ((examples1, targets1), (examples2, targets2)) in zip(*episode_streams):\n      self.assertAllEqual(examples1, examples2)\n      self.assertAllEqual(targets1, targets2)\n\n  def test_deterministic_tfseed(self):\n    """"""Tests episode generation determinism when shuffle queues are seeded.""""""\n    num_episodes = 10\n    seed = 20181120\n    episode_streams = []\n    chunk_sizes = []\n    init_rng = sampling.RNG\n    try:\n      for _ in range(2):\n        sampling.RNG = np.random.RandomState(seed)\n        sampler = sampling.EpisodeDescriptionSampler(\n            self.dataset_spec,\n            self.split,\n            episode_descr_config=config.EpisodeDescriptionConfig())\n        episodes = self.generate_episodes(\n            sampler, num_episodes, shuffle_seed=seed)\n        episode_streams.append(episodes)\n        chunk_size = sampler.compute_chunk_sizes()\n        chunk_sizes.append(chunk_size)\n        for examples, targets in episodes:\n          self.check_episode_consistency(examples, targets, chunk_size)\n\n    finally:\n      # Restore the original RNG\n      sampling.RNG = init_rng\n\n    self.assertEqual(chunk_sizes[0], chunk_sizes[1])\n\n    for ((examples1, targets1), (examples2, targets2)) in zip(*episode_streams):\n      self.check_episode_consistency(examples1, targets1, chunk_sizes[0])\n      self.check_episode_consistency(examples2, targets2, chunk_sizes[1])\n      self.assertAllEqual(examples1, examples2)\n      self.assertAllEqual(targets1, targets2)\n\n  def check_description_vs_target_chunks(\n      self, description, target_support_chunk, target_query_chunk, offset):\n    """"""Checks that target chunks are consistent with the description.\n\n    The number of support and query exampes should correspond to the\n    description, and no other class ID (except DUMMY_CLASS_ID) should be\n    present.\n\n    Args:\n      description: A sequence of (class_id, num_support, num_query) tuples of\n        ints, describing the content of an episode.\n      target_support_chunk: A sequence of ints, padded.\n      target_query_chunk: A sequence of ints, padded.\n      offset: An int, the difference between the absolute class IDs in the\n        target, and the relative class IDs in the episode description.\n    """"""\n    support_cursor = 0\n    query_cursor = 0\n    for class_id, num_support, num_query in description:\n      self.assertAllEqual(\n          target_support_chunk[support_cursor:support_cursor + num_support],\n          [class_id + offset] * num_support)\n      support_cursor += num_support\n      self.assertAllEqual(\n          target_query_chunk[query_cursor:query_cursor + num_query],\n          [class_id + offset] * num_query)\n      query_cursor += num_query\n\n    self.assertTrue(\n        all(target_support_chunk[support_cursor:] == reader.DUMMY_CLASS_ID))\n    self.assertTrue(\n        all(target_query_chunk[query_cursor:] == reader.DUMMY_CLASS_ID))\n\n  def check_same_as_generator(self, split, offset):\n    """"""Tests that the targets are the one requested by the generator.\n\n    Args:\n      split: A value of the Split enum, which split to generate from.\n      offset: An int, the difference between the absolute class IDs in the\n        source, and the relative class IDs in the episodes.\n    """"""\n    num_episodes = 10\n    seed = 20181121\n    init_rng = sampling.RNG\n    try:\n      sampling.RNG = np.random.RandomState(seed)\n      sampler = sampling.EpisodeDescriptionSampler(\n          self.dataset_spec,\n          split,\n          episode_descr_config=config.EpisodeDescriptionConfig())\n      # Each description is a (class_id, num_support, num_query) tuple.\n      descriptions = [\n          sampler.sample_episode_description() for _ in range(num_episodes)\n      ]\n\n      sampling.RNG = np.random.RandomState(seed)\n      sampler = sampling.EpisodeDescriptionSampler(\n          self.dataset_spec,\n          split,\n          episode_descr_config=config.EpisodeDescriptionConfig())\n      episodes = self.generate_episodes(sampler, num_episodes)\n      chunk_sizes = sampler.compute_chunk_sizes()\n      self.assertEqual(len(descriptions), len(episodes))\n      for (description, episode) in zip(descriptions, episodes):\n        examples, targets = episode\n        self.check_episode_consistency(examples, targets, chunk_sizes)\n        _, targets_support_chunk, targets_query_chunk = split_into_chunks(\n            targets, chunk_sizes)\n        self.check_description_vs_target_chunks(\n            description, targets_support_chunk, targets_query_chunk, offset)\n    finally:\n      sampling.RNG = init_rng\n\n  def test_same_as_generator(self):\n    # The offset corresponds to the difference between the absolute class ID as\n    # used in the episode pipeline, and class ID relative to the split (provided\n    # by the episode generator).\n    offset = 0\n    for split in Split:\n      self.check_same_as_generator(split, offset)\n      offset += len(self.dataset_spec.get_classes(split))\n\n  def test_flush_logic(self):\n    """"""Tests the ""flush"" logic avoiding example duplication in an episode.""""""\n    # Generate two episodes from un-shuffled data sources. For classes where\n    # there are enough examples for both, new examples should be used for the\n    # second episodes. Otherwise, the first examples should be re-used.\n    # A data_spec with classes between 10 and 29 examples.\n    num_classes = 30\n    dataset_spec = DatasetSpecification(\n        name=None,\n        classes_per_split={\n            Split.TRAIN: num_classes,\n            Split.VALID: 0,\n            Split.TEST: 0\n        },\n        images_per_class={i: 10 + i for i in range(num_classes)},\n        class_names=None,\n        path=None,\n        file_pattern=\'{}.tfrecords\')\n    # Sample from all train classes, 5 + 5 examples from each episode\n    sampler = sampling.EpisodeDescriptionSampler(\n        dataset_spec,\n        Split.TRAIN,\n        episode_descr_config=config.EpisodeDescriptionConfig(\n            num_ways=num_classes, num_support=5, num_query=5))\n    episodes = self.generate_episodes(sampler, num_episodes=2, shuffle=False)\n\n    # The ""flush"" part of the second episode should contain 0 from class_id 0, 1\n    # for 1, ..., 9 for 9, and then 0 for 10 and the following.\n    chunk_sizes = sampler.compute_chunk_sizes()\n    _, episode2 = episodes\n    examples2, targets2 = episode2\n    flush_target2, _, _ = split_into_chunks(targets2, chunk_sizes)\n    for class_id in range(10):\n      self.assertEqual(\n          sum(target == class_id for target in flush_target2), class_id)\n    for class_id in range(10, num_classes):\n      self.assertEqual(sum(target == class_id for target in flush_target2), 0)\n\n    # The ""support"" part of the second episode should start at example 0 for\n    # class_ids from 0 to 9 (included), and at example 10 for class_id 10 and\n    # higher.\n    _, support_examples2, query_examples2 = split_into_chunks(\n        examples2, chunk_sizes)\n\n    def _build_class_id_to_example_ids(examples):\n      # Build a mapping: class_id -> list of example ids\n      mapping = collections.defaultdict(list)\n      for example in examples:\n        if not example:\n          # Padding is at the end\n          break\n        class_id, example_id = example.decode().split(\'.\')\n        mapping[int(class_id)].append(int(example_id))\n      return mapping\n\n    support2_example_ids = _build_class_id_to_example_ids(support_examples2)\n    query2_example_ids = _build_class_id_to_example_ids(query_examples2)\n\n    for class_id in range(10):\n      self.assertCountEqual(support2_example_ids[class_id], list(range(5)))\n      self.assertCountEqual(query2_example_ids[class_id], list(range(5, 10)))\n\n    for class_id in range(10, num_classes):\n      self.assertCountEqual(support2_example_ids[class_id], list(range(10, 15)))\n      self.assertCountEqual(query2_example_ids[class_id], list(range(15, 20)))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
meta_dataset/data/sampling.py,0,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Sampling the composition of episodes.\n\nThe composition of episodes consists in the number of classes (num_ways), which\nclasses (relative class_ids), and how many examples per class (num_support,\nnum_query).\n\nThis module aims at replacing `sampler.py` in the new data pipeline.\n""""""\n# TODO(lamblinp): Update variable names to be more consistent\n# - target, class_idx, label\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import logging\nfrom meta_dataset.data import dataset_spec as dataset_spec_lib\nfrom meta_dataset.data import imagenet_specification\nimport numpy as np\nfrom six.moves import zip\n\n# Module-level random number generator. Initialized randomly, can be seeded.\nRNG = np.random.RandomState(seed=None)\n\n# How the value of MAX_SPANNING_LEAVES_ELIGIBLE was selected.\n# This controls the upper bound on the number of leaves that an internal node\n# may span in order for it to be eligible for selection. We found that this\n# value is the minimum such value that allows every leaf to be reachable. By\n# decreasing it, not all leaves would be reachable (therefore some classes would\n# never be used). By increasing it, all leaves would still be reachable but we\n# would sacrifice naturalness more than necessary (since when we sample an\n# internal node that has more than MAX_HIERARCHICAL_CLASSES spanned leaves we\n# sub-sample those leaves randomly which is essentially performing class\n# selection without taking the hierarchy into account).\nMAX_SPANNING_LEAVES_ELIGIBLE = 392\n\n\ndef sample_num_ways_uniformly(num_classes, min_ways, max_ways):\n  """"""Samples a number of ways for an episode uniformly and at random.\n\n  The support of the distribution is [min_ways, num_classes], or\n  [min_ways, max_ways] if num_classes > max_ways.\n\n  Args:\n    num_classes: int, number of classes.\n    min_ways: int, minimum number of ways.\n    max_ways: int, maximum number of ways. Only used if num_classes > max_ways.\n\n  Returns:\n    num_ways: int, number of ways for the episode.\n  """"""\n  max_ways = min(max_ways, num_classes)\n  return RNG.randint(low=min_ways, high=max_ways + 1)\n\n\ndef sample_class_ids_uniformly(num_ways, rel_classes):\n  """"""Samples the (relative) class IDs for the episode.\n\n  Args:\n    num_ways: int, number of ways for the episode.\n    rel_classes: list of int, available class IDs to sample from.\n\n  Returns:\n    class_ids: np.array, class IDs for the episode, with values in rel_classes.\n  """"""\n  return RNG.choice(rel_classes, num_ways, replace=False)\n\n\ndef compute_num_query(images_per_class, max_num_query):\n  """"""Computes the number of query examples per class in the episode.\n\n  Query sets are balanced, i.e., contain the same number of examples for each\n  class in the episode.\n\n  That number is such that the number of query examples corresponds to at most\n  half of the examples for any of the class in the episode, and is no greater\n  than `max_num_query`.\n\n  Args:\n    images_per_class: np.array, number of images for each class.\n    max_num_query: int, number of images for each class.\n\n  Returns:\n    num_query: int, number of query examples per class in the episode.\n  """"""\n  if images_per_class.min() < 2:\n    raise ValueError(\'Expected at least 2 images per class.\')\n  return np.minimum(max_num_query, (images_per_class // 2).min())\n\n\ndef sample_support_set_size(num_remaining_per_class,\n                            max_support_size_contrib_per_class,\n                            max_support_set_size):\n  """"""Samples the size of the support set in the episode.\n\n  That number is such that:\n\n  * The contribution of each class to the number is no greater than\n    `max_support_size_contrib_per_class`.\n  * It is no greater than `max_support_set_size`.\n  * The support set size is greater than or equal to the number of ways.\n\n  Args:\n    num_remaining_per_class: np.array, number of images available for each class\n      after taking into account the number of query images.\n    max_support_size_contrib_per_class: int, maximum contribution for any given\n      class to the support set size. Note that this is not a limit on the number\n      of examples of that class in the support set; this is a limit on its\n      contribution to computing the support set _size_.\n    max_support_set_size: int, maximum size of the support set.\n\n  Returns:\n    support_set_size: int, size of the support set in the episode.\n  """"""\n  if max_support_set_size < len(num_remaining_per_class):\n    raise ValueError(\'max_support_set_size is too small to have at least one \'\n                     \'support example per class.\')\n  beta = RNG.uniform()\n  support_size_contributions = np.minimum(max_support_size_contrib_per_class,\n                                          num_remaining_per_class)\n  return np.minimum(\n      # Taking the floor and adding one is equivalent to sampling beta uniformly\n      # in the (0, 1] interval and taking the ceiling of its product with\n      # `support_size_contributions`. This ensures that the support set size is\n      # at least as big as the number of ways.\n      np.floor(beta * support_size_contributions + 1).sum(),\n      max_support_set_size)\n\n\ndef sample_num_support_per_class(images_per_class, num_remaining_per_class,\n                                 support_set_size, min_log_weight,\n                                 max_log_weight):\n  """"""Samples the number of support examples per class.\n\n  At a high level, we wish the composition to loosely match class frequencies.\n  Sampling is done such that:\n\n  * The number of support examples per class is no greater than\n    `support_set_size`.\n  * The number of support examples per class is no greater than the number of\n    remaining examples per class after the query set has been taken into\n    account.\n\n  Args:\n    images_per_class: np.array, number of images for each class.\n    num_remaining_per_class: np.array, number of images available for each class\n      after taking into account the number of query images.\n    support_set_size: int, size of the support set in the episode.\n    min_log_weight: float, minimum log-weight to give to any particular class.\n    max_log_weight: float, maximum log-weight to give to any particular class.\n\n  Returns:\n    num_support_per_class: np.array, number of support examples for each class.\n  """"""\n  if support_set_size < len(num_remaining_per_class):\n    raise ValueError(\'Requesting smaller support set than the number of ways.\')\n  if np.min(num_remaining_per_class) < 1:\n    raise ValueError(\'Some classes have no remaining examples.\')\n\n  # Remaining number of support examples to sample after we guarantee one\n  # support example per class.\n  remaining_support_set_size = support_set_size - len(num_remaining_per_class)\n\n  unnormalized_proportions = images_per_class * np.exp(\n      RNG.uniform(min_log_weight, max_log_weight, size=images_per_class.shape))\n  support_set_proportions = (\n      unnormalized_proportions / unnormalized_proportions.sum())\n\n  # This guarantees that there is at least one support example per class.\n  num_desired_per_class = np.floor(\n      support_set_proportions * remaining_support_set_size).astype(\'int32\') + 1\n\n  return np.minimum(num_desired_per_class, num_remaining_per_class)\n\n\nclass EpisodeDescriptionSampler(object):\n  """"""Generates descriptions of Episode composition.\n\n  In particular, for each Episode, it will generate the class IDs (relative to\n  the selected split of the dataset) to include, as well as the number of\n  support and query examples for each class ID.\n  """"""\n\n  def __init__(self,\n               dataset_spec,\n               split,\n               episode_descr_config,\n               pool=None,\n               use_dag_hierarchy=False,\n               use_bilevel_hierarchy=False,\n               use_all_classes=False):\n    """"""Initializes an EpisodeDescriptionSampler.episode_config.\n\n    Args:\n      dataset_spec: DatasetSpecification, dataset specification.\n      split: one of Split.TRAIN, Split.VALID, or Split.TEST.\n      episode_descr_config: An instance of EpisodeDescriptionConfig containing\n        parameters relating to sampling shots and ways for episodes.\n      pool: A string (\'train\' or \'test\') or None, indicating which example-level\n        split to select, if the current dataset has them.\n      use_dag_hierarchy: Boolean, defaults to False. If a DAG-structured\n        ontology is defined in dataset_spec, use it to choose related classes.\n      use_bilevel_hierarchy: Boolean, defaults to False. If a bi-level ontology\n        is defined in dataset_spec, use it for sampling classes.\n      use_all_classes: Boolean, defaults to False. Uses all available classes,\n        in order, instead of sampling. Overrides `num_ways` to the number of\n        classes in `split`.\n\n    Raises:\n      RuntimeError: if required parameters are missing.\n      ValueError: Inconsistent parameters.\n    """"""\n    self.dataset_spec = dataset_spec\n    self.split = split\n    self.pool = pool\n    self.use_dag_hierarchy = use_dag_hierarchy\n    self.use_bilevel_hierarchy = use_bilevel_hierarchy\n    self.use_all_classes = use_all_classes\n    self.num_ways = episode_descr_config.num_ways\n    self.num_support = episode_descr_config.num_support\n    self.num_query = episode_descr_config.num_query\n    self.min_ways = episode_descr_config.min_ways\n    self.max_ways_upper_bound = episode_descr_config.max_ways_upper_bound\n    self.max_num_query = episode_descr_config.max_num_query\n    self.max_support_set_size = episode_descr_config.max_support_set_size\n    self.max_support_size_contrib_per_class = episode_descr_config.max_support_size_contrib_per_class\n    self.min_log_weight = episode_descr_config.min_log_weight\n    self.max_log_weight = episode_descr_config.max_log_weight\n    self.min_examples_in_class = episode_descr_config.min_examples_in_class\n\n    self.class_set = dataset_spec.get_classes(self.split)\n    self.num_classes = len(self.class_set)\n    # Filter out classes with too few examples\n    self._filtered_class_set = []\n    # Store (class_id, n_examples) of skipped classes for logging.\n    skipped_classes = []\n    for class_id in self.class_set:\n      n_examples = dataset_spec.get_total_images_per_class(class_id, pool=pool)\n      if n_examples < self.min_examples_in_class:\n        skipped_classes.append((class_id, n_examples))\n      else:\n        self._filtered_class_set.append(class_id)\n    self.num_filtered_classes = len(self._filtered_class_set)\n\n    if skipped_classes:\n      logging.info(\n          \'Skipping the following classes, which do not have at least \'\n          \'%d examples\', self.min_examples_in_class)\n    for class_id, n_examples in skipped_classes:\n      logging.info(\'%s (ID=%d, %d examples)\',\n                   dataset_spec.class_names[class_id], class_id, n_examples)\n\n    if self.min_ways and self.num_filtered_classes < self.min_ways:\n      raise ValueError(\n          \'""min_ways"" is set to {}, but split {} of dataset {} only has {} \'\n          \'classes with at least {} examples ({} total), so it is not possible \'\n          \'to create an episode for it. This may have resulted from applying a \'\n          \'restriction on this split of this dataset by specifying \'\n          \'benchmark.restrict_classes or benchmark.min_examples_in_class.\'\n          .format(self.min_ways, split, dataset_spec.name,\n                  self.num_filtered_classes, self.min_examples_in_class,\n                  self.num_classes))\n\n    if self.use_all_classes:\n      if self.num_classes != self.num_filtered_classes:\n        raise ValueError(\'""use_all_classes"" is not compatible with a value of \'\n                         \'""min_examples_in_class"" ({}) that results in some \'\n                         \'classes being excluded.\'.format(\n                             self.min_examples_in_class))\n      self.num_ways = self.num_classes\n\n    # Maybe overwrite use_dag_hierarchy or use_bilevel_hierarchy if requested.\n    if episode_descr_config.ignore_dag_ontology:\n      self.use_dag_hierarchy = False\n    if episode_descr_config.ignore_bilevel_ontology:\n      self.use_bilevel_hierarchy = False\n\n    # For Omniglot.\n    if self.use_bilevel_hierarchy:\n      if self.num_ways is not None:\n        raise ValueError(\'""use_bilevel_hierarchy"" is incompatible with \'\n                         \'""num_ways"".\')\n      if self.min_examples_in_class > 0:\n        raise ValueError(\'""use_bilevel_hierarchy"" is incompatible with \'\n                         \'""min_examples_in_class"".\')\n\n      if not isinstance(dataset_spec,\n                        dataset_spec_lib.BiLevelDatasetSpecification):\n        raise ValueError(\'Only applicable to datasets with a bi-level \'\n                         \'dataset specification.\')\n      # The id\'s of the superclasses of the split (a contiguous range of ints).\n      self.superclass_set = dataset_spec.get_superclasses(self.split)\n\n    # For ImageNet.\n    elif self.use_dag_hierarchy:\n      if self.num_ways is not None:\n        raise ValueError(\'""use_dag_hierarchy"" is incompatible with ""num_ways"".\')\n\n      if not isinstance(dataset_spec,\n                        dataset_spec_lib.HierarchicalDatasetSpecification):\n        raise ValueError(\'Only applicable to datasets with a hierarchical \'\n                         \'dataset specification.\')\n\n      # A DAG for navigating the ontology for the given split.\n      graph = dataset_spec.get_split_subgraph(self.split)\n\n      # Map the absolute class IDs in the split\'s class set to IDs relative to\n      # the split.\n      class_set = self.class_set\n      abs_to_rel_ids = dict((abs_id, i) for i, abs_id in enumerate(class_set))\n\n      # Extract the sets of leaves and internal nodes in the DAG.\n      leaves = set(imagenet_specification.get_leaves(graph))\n      internal_nodes = graph - leaves  # set difference\n\n      # Map each node of the DAG to the Synsets of the leaves it spans.\n      spanning_leaves_dict = imagenet_specification.get_spanning_leaves(graph)\n\n      # Build a list of lists storing the relative class IDs of the spanning\n      # leaves for each eligible internal node.\n      self.span_leaves_rel = []\n      for node in internal_nodes:\n        node_leaves = spanning_leaves_dict[node]\n        # Build a list of relative class IDs of leaves that have at least\n        # min_examples_in_class examples.\n        ids_rel = []\n        for leaf in node_leaves:\n          abs_id = dataset_spec.class_names_to_ids[leaf.wn_id]\n          if abs_id in self._filtered_class_set:\n            ids_rel.append(abs_to_rel_ids[abs_id])\n\n        # Internal nodes are eligible if they span at least\n        # `min_allowed_classes` and at most `max_eligible` leaves.\n        if self.min_ways <= len(ids_rel) <= MAX_SPANNING_LEAVES_ELIGIBLE:\n          self.span_leaves_rel.append(ids_rel)\n\n      num_eligible_nodes = len(self.span_leaves_rel)\n      if num_eligible_nodes < 1:\n        raise ValueError(\'There are no classes eligible for participating in \'\n                         \'episodes. Consider changing the value of \'\n                         \'`EpisodeDescriptionSampler.min_ways` in gin, or \'\n                         \'or MAX_SPANNING_LEAVES_ELIGIBLE in data.py.\')\n\n  def sample_class_ids(self):\n    """"""Returns the (relative) class IDs for an episode.\n\n    If self.use_dag_hierarchy, it samples them according to a procedure\n    informed by the dataset\'s ontology, otherwise randomly.\n    If self.min_examples_in_class > 0, classes with too few examples will not\n    be selected.\n    """"""\n    if self.use_dag_hierarchy:\n      # Retrieve the list of relative class IDs for an internal node sampled\n      # uniformly at random.\n      episode_classes_rel = RNG.choice(self.span_leaves_rel)\n\n      # If the number of chosen classes is larger than desired, sub-sample them.\n      if len(episode_classes_rel) > self.max_ways_upper_bound:\n        episode_classes_rel = RNG.choice(\n            episode_classes_rel,\n            size=[self.max_ways_upper_bound],\n            replace=False)\n\n      # Light check to make sure the chosen number of classes is valid.\n      assert len(episode_classes_rel) >= self.min_ways\n      assert len(episode_classes_rel) <= self.max_ways_upper_bound\n    elif self.use_bilevel_hierarchy:\n      # First sample a coarse category uniformly. Then randomly sample the way\n      # uniformly, but taking care not to sample more than the number of classes\n      # of the chosen supercategory.\n      episode_superclass = RNG.choice(self.superclass_set, 1)[0]\n      num_superclass_classes = self.dataset_spec.classes_per_superclass[\n          episode_superclass]\n\n      num_ways = sample_num_ways_uniformly(\n          num_superclass_classes,\n          min_ways=self.min_ways,\n          max_ways=self.max_ways_upper_bound)\n\n      # e.g. if these are [3, 1] then the 4\'th and the 2\'nd of the subclasses\n      # that belong to the chosen superclass will be used. If the class id\'s\n      # that belong to this superclass are [23, 24, 25, 26] then the returned\n      # episode_classes_rel will be [26, 24] which as usual are number relative\n      # to the split.\n      episode_subclass_ids = sample_class_ids_uniformly(num_ways,\n                                                        num_superclass_classes)\n      (episode_classes_rel,\n       _) = self.dataset_spec.get_class_ids_from_superclass_subclass_inds(\n           self.split, episode_superclass, episode_subclass_ids)\n    elif self.use_all_classes:\n      episode_classes_rel = np.arange(self.num_classes)\n    else:  # No type of hierarchy is used. Classes are randomly sampled.\n      if self.num_ways is not None:\n        num_ways = self.num_ways\n      else:\n        num_ways = sample_num_ways_uniformly(\n            self.num_filtered_classes,\n            min_ways=self.min_ways,\n            max_ways=self.max_ways_upper_bound)\n      # Filtered class IDs relative to the selected split\n      ids_rel = [\n          class_id - self.class_set[0] for class_id in self._filtered_class_set\n      ]\n      episode_classes_rel = sample_class_ids_uniformly(num_ways, ids_rel)\n\n    return episode_classes_rel\n\n  def sample_episode_description(self):\n    """"""Returns the composition of an episode.\n\n    Returns:\n      A sequence of `(class_id, num_support, num_query)` tuples, where\n        relative `class_id` is an integer in [0, self.num_classes).\n    """"""\n    class_ids = self.sample_class_ids()\n    images_per_class = np.array([\n        self.dataset_spec.get_total_images_per_class(\n            self.class_set[cid], pool=self.pool) for cid in class_ids\n    ])\n\n    if self.num_query is not None:\n      num_query = self.num_query\n    else:\n      num_query = compute_num_query(\n          images_per_class, max_num_query=self.max_num_query)\n\n    if self.num_support is not None:\n      if any(self.num_support + num_query > images_per_class):\n        raise ValueError(\'Some classes have not enough examples.\')\n      num_support_per_class = [self.num_support for _ in class_ids]\n    else:\n      num_remaining_per_class = images_per_class - num_query\n      support_set_size = sample_support_set_size(\n          num_remaining_per_class,\n          self.max_support_size_contrib_per_class,\n          max_support_set_size=self.max_support_set_size)\n      num_support_per_class = sample_num_support_per_class(\n          images_per_class,\n          num_remaining_per_class,\n          support_set_size,\n          min_log_weight=self.min_log_weight,\n          max_log_weight=self.max_log_weight)\n\n    return tuple(\n        (class_id, num_support, num_query)\n        for class_id, num_support in zip(class_ids, num_support_per_class))\n\n  def compute_chunk_sizes(self):\n    """"""Computes the maximal sizes for the flush, support, and query chunks.\n\n    Sequences of dataset IDs are padded with dummy IDs to make sure they can be\n    batched into episodes of equal sizes.\n\n    The ""flush"" part of the sequence has a size that is upper-bounded by the\n    size of the ""support"" and ""query"" parts.\n\n    If variable, the size of the ""support"" part is in the worst case\n\n        max_support_set_size,\n\n    and the size of the ""query"" part is in the worst case\n\n        max_ways_upper_bound * max_num_query.\n\n    Returns:\n      The sizes of the flush, support, and query chunks.\n    """"""\n    if self.num_ways is None:\n      max_num_ways = self.max_ways_upper_bound\n    else:\n      max_num_ways = self.num_ways\n\n    if self.num_support is None:\n      support_chunk_size = self.max_support_set_size\n    else:\n      support_chunk_size = max_num_ways * self.num_support\n\n    if self.num_query is None:\n      max_num_query = self.max_num_query\n    else:\n      max_num_query = self.num_query\n    query_chunk_size = max_num_ways * max_num_query\n\n    flush_chunk_size = support_chunk_size + query_chunk_size\n    return (flush_chunk_size, support_chunk_size, query_chunk_size)\n'"
meta_dataset/data/sampling_test.py,9,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Tests for `sampling` module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport gin.tf\nfrom meta_dataset.data import config\nfrom meta_dataset.data import sampling\nfrom meta_dataset.data import test_utils\nfrom meta_dataset.data.dataset_spec import DatasetSpecification\nfrom meta_dataset.data.learning_spec import Split\nimport numpy as np\nfrom six.moves import range\nfrom six.moves import zip\nimport tensorflow.compat.v1 as tf\n\ntest_utils.set_episode_descr_config_defaults()\n\n\nclass SampleNumWaysUniformlyTest(tf.test.TestCase):\n  """"""Tests for the `sample_num_ways_uniformly` function.""""""\n\n  def test_min_ways_respected(self):\n    for _ in range(10):\n      num_ways = sampling.sample_num_ways_uniformly(\n          10,\n          min_ways=test_utils.MIN_WAYS,\n          max_ways=test_utils.MAX_WAYS_UPPER_BOUND)\n      self.assertGreaterEqual(num_ways, test_utils.MIN_WAYS)\n\n  def test_num_classes_respected(self):\n    num_classes = 10\n    for _ in range(10):\n      num_ways = sampling.sample_num_ways_uniformly(\n          num_classes,\n          min_ways=test_utils.MIN_WAYS,\n          max_ways=test_utils.MAX_WAYS_UPPER_BOUND)\n      self.assertLessEqual(num_ways, num_classes)\n\n  def test_max_ways_upper_bound_respected(self):\n    num_classes = 2 * test_utils.MAX_WAYS_UPPER_BOUND\n    for _ in range(10):\n      num_ways = sampling.sample_num_ways_uniformly(\n          num_classes,\n          min_ways=test_utils.MIN_WAYS,\n          max_ways=test_utils.MAX_WAYS_UPPER_BOUND)\n      self.assertLessEqual(num_ways, test_utils.MAX_WAYS_UPPER_BOUND)\n\n\nclass SampleClassIDsUniformlyTest(tf.test.TestCase):\n  """"""Tests for the `sample_class_ids_uniformly` function.""""""\n\n  def test_num_ways_respected(self):\n    num_classes = test_utils.MAX_WAYS_UPPER_BOUND\n    num_ways = test_utils.MIN_WAYS\n    for _ in range(10):\n      class_ids = sampling.sample_class_ids_uniformly(num_ways, num_classes)\n      self.assertLen(set(class_ids), num_ways)\n      self.assertLen(class_ids, num_ways)\n\n  def test_num_classes_respected(self):\n    num_classes = test_utils.MAX_WAYS_UPPER_BOUND\n    num_ways = test_utils.MIN_WAYS\n    for _ in range(10):\n      class_ids = sampling.sample_class_ids_uniformly(num_ways, num_classes)\n      self.assertContainsSubset(class_ids, list(range(num_classes)))\n\n  def test_unique_class_ids(self):\n    num_classes = test_utils.MAX_WAYS_UPPER_BOUND\n    num_ways = test_utils.MIN_WAYS\n    for _ in range(10):\n      class_ids = sampling.sample_class_ids_uniformly(num_ways, num_classes)\n      self.assertCountEqual(class_ids, set(class_ids))\n\n\nclass ComputeNumQueryTest(tf.test.TestCase):\n  """"""Tests for the `compute_num_query` function.""""""\n\n  def test_max_num_query_respected(self):\n    images_per_class = np.array([30, 45, 35, 50])\n    num_query = sampling.compute_num_query(\n        images_per_class, max_num_query=test_utils.MAX_NUM_QUERY)\n    self.assertEqual(num_query, test_utils.MAX_NUM_QUERY)\n\n  def test_at_most_half(self):\n    images_per_class = np.array([10, 9, 20, 21])\n    num_query = sampling.compute_num_query(\n        images_per_class, max_num_query=test_utils.MAX_NUM_QUERY)\n    self.assertEqual(num_query, 4)\n\n  def test_raises_error_on_one_image_per_class(self):\n    images_per_class = np.array([1, 3, 8, 8])\n    with self.assertRaises(ValueError):\n      sampling.compute_num_query(\n          images_per_class, max_num_query=test_utils.MAX_NUM_QUERY)\n\n\nclass SampleSupportSetSizeTest(tf.test.TestCase):\n  """"""Tests for the `sample_support_set_size` function.""""""\n\n  def test_max_support_set_size_respected(self):\n    num_remaining_per_class = np.array([test_utils.MAX_SUPPORT_SET_SIZE] * 10)\n    for _ in range(10):\n      support_set_size = sampling.sample_support_set_size(\n          num_remaining_per_class,\n          max_support_size_contrib_per_class=(\n              test_utils.MAX_SUPPORT_SIZE_CONTRIB_PER_CLASS),\n          max_support_set_size=test_utils.MAX_SUPPORT_SET_SIZE)\n      self.assertLessEqual(support_set_size, test_utils.MAX_SUPPORT_SET_SIZE)\n\n  def test_raises_error_max_support_too_small(self):\n    num_remaining_per_class = np.array([5] * 10)\n    with self.assertRaises(ValueError):\n      sampling.sample_support_set_size(\n          num_remaining_per_class,\n          max_support_size_contrib_per_class=(\n              test_utils.MAX_SUPPORT_SIZE_CONTRIB_PER_CLASS),\n          max_support_set_size=len(num_remaining_per_class) - 1)\n\n\nclass SampleNumSupportPerClassTest(tf.test.TestCase):\n  """"""Tests for the `sample_num_support_per_class` function.""""""\n\n  def test_support_set_size_respected(self):\n    num_images_per_class = np.array([50, 40, 30, 20])\n    num_remaining_per_class = np.array([40, 30, 20, 10])\n    support_set_size = 50\n    for _ in range(10):\n      num_support_per_class = sampling.sample_num_support_per_class(\n          num_images_per_class,\n          num_remaining_per_class,\n          support_set_size,\n          min_log_weight=test_utils.MIN_LOG_WEIGHT,\n          max_log_weight=test_utils.MAX_LOG_WEIGHT)\n      self.assertLessEqual(num_support_per_class.sum(), support_set_size)\n\n  def test_at_least_one_example_per_class(self):\n    num_images_per_class = np.array([10, 10, 10, 10, 10])\n    num_remaining_per_class = np.array([5, 5, 5, 5, 5])\n    support_set_size = 5\n    for _ in range(10):\n      num_support_per_class = sampling.sample_num_support_per_class(\n          num_images_per_class,\n          num_remaining_per_class,\n          support_set_size,\n          min_log_weight=test_utils.MIN_LOG_WEIGHT,\n          max_log_weight=test_utils.MAX_LOG_WEIGHT)\n      self.assertTrue((num_support_per_class > 0).any())\n\n  def test_complains_on_too_small_support_set_size(self):\n    num_images_per_class = np.array([10, 10, 10, 10, 10])\n    num_remaining_per_class = np.array([5, 5, 5, 5, 5])\n    support_set_size = 3\n    with self.assertRaises(ValueError):\n      sampling.sample_num_support_per_class(\n          num_images_per_class,\n          num_remaining_per_class,\n          support_set_size,\n          min_log_weight=test_utils.MIN_LOG_WEIGHT,\n          max_log_weight=test_utils.MAX_LOG_WEIGHT)\n\n  def test_complains_on_zero_remaining(self):\n    num_images_per_class = np.array([10, 10, 10, 10, 10])\n    num_remaining_per_class = np.array([5, 0, 5, 5, 5])\n    support_set_size = 5\n    with self.assertRaises(ValueError):\n      sampling.sample_num_support_per_class(\n          num_images_per_class,\n          num_remaining_per_class,\n          support_set_size,\n          min_log_weight=test_utils.MIN_LOG_WEIGHT,\n          max_log_weight=test_utils.MAX_LOG_WEIGHT)\n\n\n# TODO(vdumoulin): move this class into `config_test.py`.\nclass EpisodeDescrSamplerErrorTest(parameterized.TestCase, tf.test.TestCase):\n  """"""Episode sampler should verify args when ways/shots are sampled.""""""\n  dataset_spec = test_utils.DATASET_SPEC\n  split = Split.VALID\n\n  @parameterized.named_parameters((\'num_ways_none\', None, 5, 10, {}),\n                                  (\'num_ways_none2\', None, 5, 10, {\n                                      \'min_ways\': 3\n                                  }), (\'num_support_none\', 5, None, 10, {}),\n                                  (\'num_support_none2\', 5, None, 10, {\n                                      \'max_support_set_size\': 3\n                                  }), (\'num_query_none\', 5, 5, None, {}))\n  def test_runtime_errors(self, num_ways, num_support, num_query, kwargs):\n    """"""Testing run-time errors thrown when arguments are not set correctly.""""""\n    # The following scope removes the gin-config set.\n    with gin.config_scope(\'none\'):\n      with self.assertRaises(RuntimeError):\n        _ = sampling.EpisodeDescriptionSampler(\n            self.dataset_spec,\n            self.split,\n            episode_descr_config=config.EpisodeDescriptionConfig(\n                num_ways=num_ways,\n                num_support=num_support,\n                num_query=num_query,\n                **kwargs))\n\n  @parameterized.named_parameters((\'num_ways_none\', None, 5, 10, {\n      \'min_ways\': 3,\n      \'max_ways_upper_bound\': 5\n  }), (\'num_support_none\', 5, None, 10, {\n      \'max_support_set_size\': 3,\n      \'max_support_size_contrib_per_class\': 5,\n      \'min_log_weight\': 0.5,\n      \'max_log_weight\': 0.5\n  }), (\'num_query_none\', 5, 5, None, {\n      \'max_num_query\': 3\n  }))\n  def test_runtime_no_error(self, num_ways, num_support, num_query, kwargs):\n    """"""Testing run-time errors thrown when arguments are not set correctly.""""""\n    # The following scope removes the gin-config set.\n    with gin.config_scope(\'none\'):\n      # No error thrown\n      _ = sampling.EpisodeDescriptionSampler(\n          self.dataset_spec,\n          self.split,\n          episode_descr_config=config.EpisodeDescriptionConfig(\n              num_ways=num_ways,\n              num_support=num_support,\n              num_query=num_query,\n              **kwargs))\n\n\nclass EpisodeDescrSamplerTest(tf.test.TestCase):\n  """"""Tests EpisodeDescriptionSampler defaults.\n\n  This class provides some tests to be run by inherited classes.\n  """"""\n\n  dataset_spec = test_utils.DATASET_SPEC\n  split = Split.VALID\n\n  def setUp(self):\n    super(EpisodeDescrSamplerTest, self).setUp()\n    self.sampler = self.make_sampler()\n\n  def make_sampler(self):\n    """"""Helper function to make a new instance of the tested sampler.""""""\n    return sampling.EpisodeDescriptionSampler(self.dataset_spec, self.split,\n                                              config.EpisodeDescriptionConfig())\n\n  def test_max_examples(self):\n    """"""The number of requested examples per class should not be too large.""""""\n    class_set = self.dataset_spec.get_classes(self.split)\n    for _ in range(10):\n      episode_description = self.sampler.sample_episode_description()\n      self.assertTrue(\n          all(s +\n              q <= self.dataset_spec.get_total_images_per_class(class_set[cid])\n              for cid, s, q in episode_description))\n\n  def test_min_examples(self):\n    """"""There should be at least 1 support and query example per class.""""""\n    for _ in range(10):\n      episode_description = self.sampler.sample_episode_description()\n      self.assertTrue(\n          all(s >= 1 and q >= 1 for cid, s, q in episode_description))\n\n  def test_non_deterministic(self):\n    """"""By default, generated episodes should be different across Samplers.""""""\n    reference_sample = self.sampler.sample_episode_description()\n    for _ in range(10):\n      sampler = self.make_sampler()\n      sample = sampler.sample_episode_description()\n      if sample != reference_sample:\n        # Test should pass\n        break\n    else:\n      # The end of the loop was reached with no ""break"" triggered.\n      # If it generated the same description 11 times, this is an error.\n      raise AssertionError(\'Different EpisodeDescriptionSamplers generate \'\n                           \'the same sequence of episode descriptions.\')\n\n  def test_setting_randomstate(self):\n    """"""Setting the RNG state should make episode generation deterministic.""""""\n    init_rng = sampling.RNG\n    seed = 20181113\n    try:\n      sampling.RNG = np.random.RandomState(seed)\n      sampler = self.make_sampler()\n      reference_sample = sampler.sample_episode_description()\n      for _ in range(10):\n        sampling.RNG = np.random.RandomState(seed)\n        sampler = self.make_sampler()\n        sample = sampler.sample_episode_description()\n        self.assertEqual(reference_sample, sample)\n\n    finally:\n      # Restore the original RNG\n      sampling.RNG = init_rng\n\n  def assert_expected_chunk_sizes(self, expected_support_chunk_size,\n                                  expected_query_chunk_size):\n    rval = self.sampler.compute_chunk_sizes()\n    flush_chunk_size, support_chunk_size, query_chunk_size = rval\n\n    expected_flush_chunk_size = (\n        expected_support_chunk_size + expected_query_chunk_size)\n    self.assertEqual(flush_chunk_size, expected_flush_chunk_size)\n    self.assertEqual(support_chunk_size, expected_support_chunk_size)\n    self.assertEqual(query_chunk_size, expected_query_chunk_size)\n\n  def test_correct_chunk_sizes(self):\n    self.assert_expected_chunk_sizes(\n        test_utils.MAX_SUPPORT_SET_SIZE,\n        test_utils.MAX_WAYS_UPPER_BOUND * test_utils.MAX_NUM_QUERY)\n\n\nclass FixedQueryEpisodeDescrSamplerTest(EpisodeDescrSamplerTest):\n  """"""Tests EpisodeDescriptionSampler with fixed query set.\n\n  Inherits from EpisodeDescrSamplerTest so:\n    - Tests defined in the parent class will be run\n    - parent setUp method will be called\n    - make_sampler is overridden.\n  """"""\n\n  split = Split.TRAIN\n  num_query = 5\n\n  def make_sampler(self):\n    return sampling.EpisodeDescriptionSampler(\n        self.dataset_spec, self.split,\n        config.EpisodeDescriptionConfig(num_query=self.num_query))\n\n  def test_num_query_examples(self):\n    class_set = self.dataset_spec.get_classes(self.split)\n    for _ in range(10):\n      episode_description = self.sampler.sample_episode_description()\n      for cid, _, q in episode_description:\n        self.assertIn(cid, class_set)\n        self.assertEqual(q, self.num_query)\n\n  def test_query_too_big(self):\n    """"""Asserts failure if all examples of a class are selected for query.""""""\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec, self.split,\n        config.EpisodeDescriptionConfig(num_query=10))\n    with self.assertRaises(ValueError):\n      # Sample enough times that we encounter a class with only 10 examples.\n      for _ in range(10):\n        sampler.sample_episode_description()\n\n  def test_correct_chunk_sizes(self):\n    self.assert_expected_chunk_sizes(\n        test_utils.MAX_SUPPORT_SET_SIZE,\n        test_utils.MAX_WAYS_UPPER_BOUND * self.num_query)\n\n\nclass NoQueryEpisodeDescrSamplerTest(FixedQueryEpisodeDescrSamplerTest):\n  """"""Tests EpisodeDescriptionSampler with no query set.\n\n  Special case of FixedQueryEpisodeDescrSamplerTest with num_query = 0.\n  """"""\n  num_query = 0\n\n  def test_min_examples(self):\n    """"""Overrides base class because 0 query examples is actually expected.""""""\n    for _ in range(10):\n      episode_description = self.sampler.sample_episode_description()\n      self.assertTrue(all(s >= 1 for cid, s, q in episode_description))\n\n\nclass FixedShotsEpisodeDescrSamplerTest(FixedQueryEpisodeDescrSamplerTest):\n  """"""Tests EpisodeDescriptionSampler with fixed support and query size.\n\n  Inherits form FixedQueryEpisodeDescrSamplerTest, so parent tests, including\n  test_num_query_examples will be run as well.\n  """"""\n  # Chosen so num_support + num_query <= 10, since some classes have 10 ex.\n  num_support = 3\n  num_query = 7\n\n  def make_sampler(self):\n    return sampling.EpisodeDescriptionSampler(\n        self.dataset_spec, self.split,\n        config.EpisodeDescriptionConfig(\n            num_support=self.num_support, num_query=self.num_query))\n\n  def test_num_support_examples(self):\n    for _ in range(10):\n      episode_description = self.sampler.sample_episode_description()\n      for _, s, _ in episode_description:\n        self.assertEqual(s, self.num_support)\n\n  def test_shots_too_big(self):\n    """"""Asserts failure if not enough examples to fulfill support and query.""""""\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec, self.split,\n        config.EpisodeDescriptionConfig(num_support=5, num_query=15))\n    with self.assertRaises(ValueError):\n      sampler.sample_episode_description()\n\n  def test_correct_chunk_sizes(self):\n    self.assert_expected_chunk_sizes(\n        test_utils.MAX_WAYS_UPPER_BOUND * self.num_support,\n        test_utils.MAX_WAYS_UPPER_BOUND * self.num_query)\n\n\nclass FixedWaysEpisodeDescrSamplerTest(EpisodeDescrSamplerTest):\n  """"""Tests EpisodeDescriptionSampler with fixed number of classes.""""""\n  split = Split.TRAIN\n  num_ways = 12\n\n  def make_sampler(self):\n    return sampling.EpisodeDescriptionSampler(\n        self.dataset_spec, self.split,\n        config.EpisodeDescriptionConfig(num_ways=self.num_ways))\n\n  def test_num_ways(self):\n    for _ in range(10):\n      episode_description = self.sampler.sample_episode_description()\n      self.assertLen((episode_description), self.num_ways)\n\n  def test_ways_too_big(self):\n    """"""Asserts failure if more ways than classes are available.""""""\n    # Use Split.VALID as it only has 10 classes.\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec, Split.VALID,\n        config.EpisodeDescriptionConfig(num_ways=self.num_ways))\n    with self.assertRaises(ValueError):\n      sampler.sample_episode_description()\n\n  def test_correct_chunk_sizes(self):\n    self.assert_expected_chunk_sizes(test_utils.MAX_SUPPORT_SET_SIZE,\n                                     self.num_ways * test_utils.MAX_NUM_QUERY)\n\n\nclass FixedEpisodeDescrSamplerTest(FixedShotsEpisodeDescrSamplerTest,\n                                   FixedWaysEpisodeDescrSamplerTest):\n  """"""Tests EpisodeDescriptionSampler with fixed shots and ways.""""""\n\n  def make_sampler(self):\n    return sampling.EpisodeDescriptionSampler(\n        self.dataset_spec, self.split,\n        config.EpisodeDescriptionConfig(\n            num_ways=self.num_ways,\n            num_support=self.num_support,\n            num_query=self.num_query))\n\n  def test_correct_chunk_sizes(self):\n    self.assert_expected_chunk_sizes(self.num_ways * self.num_support,\n                                     self.num_ways * self.num_query)\n\n\nclass NotEnoughExamplesSamplerTest(EpisodeDescrSamplerTest):\n  """"""Tests skipping classes with too few examples.""""""\n  # Skip classes 0, 3, ..., 27 of test_utils.DATASET_SPEC, as they only have 10\n  # examples each.\n  split = Split.TRAIN\n  min_examples_in_class = 11\n\n  def make_sampler(self):\n    return sampling.EpisodeDescriptionSampler(\n        self.dataset_spec, self.split,\n        config.EpisodeDescriptionConfig(\n            min_examples_in_class=self.min_examples_in_class))\n\n  def test_skip_classes(self):\n    expected_classes = [i for i in range(30) if i % 3]\n    for _ in range(10):\n      episode_description = self.sampler.sample_episode_description()\n      print(\'episode_description:\', episode_description)\n      self.assertAllInSet([cid for cid, _, _ in episode_description],\n                          expected_classes)\n\n  def test_noskip_at_min(self):\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec, self.split,\n        config.EpisodeDescriptionConfig(min_examples_in_class=10))\n    # We expect 10-example classes to be sampled at least some times\n    for _ in range(10):\n      episode_description = sampler.sample_episode_description()\n      if any(cid % 3 == 0 for cid, _, _ in episode_description):\n        # Test should pass\n        break\n      else:\n        # The end of the loop was reached with no ""break"" triggered.\n        # If no 10-example class is sampled after 10 iterations, it is an error.\n        raise AssertionError(\'Classes with exactly `min_examples_in_class` \'\n                             \'were not sampled.\')\n\n  def test_skip_too_many(self):\n    # The ""valid"" split does not have MIN_WAYS (5) classes left if we skip some.\n    with self.assertRaises(ValueError):\n      sampling.EpisodeDescriptionSampler(\n          self.dataset_spec, Split.VALID,\n          config.EpisodeDescriptionConfig(\n              min_examples_in_class=self.min_examples_in_class))\n\n\nclass ChunkSizesTest(tf.test.TestCase):\n  """"""Tests the boundaries of compute_chunk_sizes.""""""\n\n  def setUp(self):\n    super(ChunkSizesTest, self).setUp()\n    # Set up a DatasetSpecification with lots of classes and samples.\n    self.dataset_spec = DatasetSpecification(\n        name=None,\n        classes_per_split=dict(zip(Split, [1000, 0, 0])),\n        images_per_class={i: 1000 for i in range(1000)},\n        class_names=None,\n        path=None,\n        file_pattern=\'{}.tfrecords\')\n\n  def test_large_support(self):\n    """"""Support set larger than MAX_SUPPORT_SET_SIZE with fixed shots.""""""\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec, Split.TRAIN,\n        config.EpisodeDescriptionConfig(num_ways=30, num_support=20))\n    _, support_chunk_size, _ = sampler.compute_chunk_sizes()\n    self.assertGreater(support_chunk_size, test_utils.MAX_SUPPORT_SET_SIZE)\n    sampler.sample_episode_description()\n\n  def test_large_ways(self):\n    """"""Fixed num_ways above MAX_WAYS_UPPER_BOUND.""""""\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec, Split.TRAIN,\n        config.EpisodeDescriptionConfig(num_ways=60, num_support=10))\n    _, support_chunk_size, query_chunk_size = sampler.compute_chunk_sizes()\n    self.assertGreater(support_chunk_size, test_utils.MAX_SUPPORT_SET_SIZE)\n    self.assertGreater(\n        query_chunk_size,\n        test_utils.MAX_WAYS_UPPER_BOUND * test_utils.MAX_NUM_QUERY)\n    sampler.sample_episode_description()\n\n  def test_large_query(self):\n    """"""Query set larger than MAX_NUM_QUERY per class.""""""\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec, Split.TRAIN,\n        config.EpisodeDescriptionConfig(num_query=60))\n    _, _, query_chunk_size = sampler.compute_chunk_sizes()\n    self.assertGreater(\n        query_chunk_size,\n        test_utils.MAX_WAYS_UPPER_BOUND * test_utils.MAX_NUM_QUERY)\n    sampler.sample_episode_description()\n\n  def test_too_many_ways(self):\n    """"""Too many ways to have 1 example per class with default variable shots.""""""\n    sampler = sampling.EpisodeDescriptionSampler(\n        self.dataset_spec, Split.TRAIN,\n        config.EpisodeDescriptionConfig(num_ways=600))\n    with self.assertRaises(ValueError):\n      sampler.sample_episode_description()\n\n\n# TODO(lamblinp)\n# - test with use_hierarchy=True\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
meta_dataset/data/test_utils.py,1,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Utility functions for input pipeline tests.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin.tf\nfrom meta_dataset.data.dataset_spec import DatasetSpecification\nfrom meta_dataset.data.learning_spec import Split\nfrom meta_dataset.dataset_conversion import dataset_to_records\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\n# DatasetSpecification to use in tests\nDATASET_SPEC = DatasetSpecification(\n    name=None,\n    classes_per_split={\n        Split.TRAIN: 15,\n        Split.VALID: 5,\n        Split.TEST: 10\n    },\n    images_per_class=dict(enumerate([10, 20, 30] * 10)),\n    class_names=[\'%d\' % i for i in range(30)],\n    path=None,\n    file_pattern=\'{}.tfrecords\')\n\n# Define defaults for the input pipeline.\nMIN_WAYS = 5\nMAX_WAYS_UPPER_BOUND = 50\nMAX_NUM_QUERY = 10\nMAX_SUPPORT_SET_SIZE = 500\nMAX_SUPPORT_SIZE_CONTRIB_PER_CLASS = 100\nMIN_LOG_WEIGHT = np.log(0.5)\nMAX_LOG_WEIGHT = np.log(2)\n\n\ndef set_episode_descr_config_defaults():\n  """"""Sets default values for EpisodeDescriptionConfig using gin.""""""\n  gin.parse_config(\'import meta_dataset.data.config\')\n\n  gin.bind_parameter(\'EpisodeDescriptionConfig.num_ways\', None)\n  gin.bind_parameter(\'EpisodeDescriptionConfig.num_support\', None)\n  gin.bind_parameter(\'EpisodeDescriptionConfig.num_query\', None)\n  gin.bind_parameter(\'EpisodeDescriptionConfig.min_ways\', MIN_WAYS)\n  gin.bind_parameter(\'EpisodeDescriptionConfig.max_ways_upper_bound\',\n                     MAX_WAYS_UPPER_BOUND)\n  gin.bind_parameter(\'EpisodeDescriptionConfig.max_num_query\', MAX_NUM_QUERY)\n  gin.bind_parameter(\'EpisodeDescriptionConfig.max_support_set_size\',\n                     MAX_SUPPORT_SET_SIZE)\n  gin.bind_parameter(\n      \'EpisodeDescriptionConfig.max_support_size_contrib_per_class\',\n      MAX_SUPPORT_SIZE_CONTRIB_PER_CLASS)\n  gin.bind_parameter(\'EpisodeDescriptionConfig.min_log_weight\', MIN_LOG_WEIGHT)\n  gin.bind_parameter(\'EpisodeDescriptionConfig.max_log_weight\', MAX_LOG_WEIGHT)\n  gin.bind_parameter(\'EpisodeDescriptionConfig.ignore_dag_ontology\', False)\n  gin.bind_parameter(\'EpisodeDescriptionConfig.ignore_bilevel_ontology\', False)\n\n  # Following is set in a different scope.\n  gin.bind_parameter(\'none/EpisodeDescriptionConfig.min_ways\', None)\n  gin.bind_parameter(\'none/EpisodeDescriptionConfig.max_ways_upper_bound\', None)\n  gin.bind_parameter(\'none/EpisodeDescriptionConfig.max_num_query\', None)\n  gin.bind_parameter(\'none/EpisodeDescriptionConfig.max_support_set_size\', None)\n  gin.bind_parameter(\n      \'none/EpisodeDescriptionConfig.max_support_size_contrib_per_class\', None)\n  gin.bind_parameter(\'none/EpisodeDescriptionConfig.min_log_weight\', None)\n  gin.bind_parameter(\'none/EpisodeDescriptionConfig.max_log_weight\', None)\n\n\ndef write_feature_records(features, label, output_path):\n  """"""Creates a record file from features and labels.\n\n  Args:\n    features: An [n, m] numpy array of features.\n    label: An integer, the label common to all records.\n    output_path: A string specifying the location of the record.\n  """"""\n  writer = tf.python_io.TFRecordWriter(output_path)\n  for feat in list(features):\n    # Write the example.\n    serialized_example = dataset_to_records.make_example([\n        (\'image/embedding\', \'float32\', feat.tolist()),\n        (\'image/class/label\', \'int64\', [label])\n    ])\n    writer.write(serialized_example)\n  writer.close()\n'"
meta_dataset/dataset_conversion/__init__.py,0,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n'"
meta_dataset/dataset_conversion/convert_datasets_to_records.py,6,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n# pyformat: disable\nr""""""Main file for converting the datasets used in the benchmark into records.\n\nExample command to convert dataset omniglot:\n# pylint: disable=line-too-long\npython -m meta_dataset.dataset_conversion.convert_datasets_to_records \\\n  --dataset=omniglot \\\n  --omniglot_data_root=<path/to/omniglot> \\\n  --records_root=<path/to/records> \\\n  --splits_root=<path/to/splits>\n# pylint: enable=line-too-long\n""""""\n# pyformat: enable\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom absl import logging\nfrom meta_dataset.dataset_conversion import dataset_to_records\nimport tensorflow.compat.v1 as tf\n\ntf.flags.DEFINE_string(\n    \'mini_imagenet_records_dir\',\n    # This dataset is for diagnostic purposes only, which is why we want to\n    # store it in a different location than the other datasets.\n    \'\',\n    \'The path to store the tf.Records of MiniImageNet.\')\n\ntf.flags.DEFINE_string(\'dataset\', \'omniglot\',\n                       \'The name of the dataset to convert to records.\')\n\nFLAGS = tf.flags.FLAGS\n\n\nclass ConverterArgs(\n    collections.namedtuple(\'ConverterArgs\', \'data_root, long_name\')):\n  """"""Arguments to be passed to a DatasetConverter\'s constructor.\n\n  Attributes:\n    data_root: string, path to the root of the dataset.\n    long_name: string, dataset name in longer or capitalized form.\n  """"""\n\n\ndef _dataset_name_to_converter_and_args(flags=FLAGS):\n  """"""Returns a dict mapping dataset name to (converter class, arguments).\n\n  This (converter class, arguments) pair will be used to build the corresponding\n  DatasetConverter object.\n\n  Args:\n    flags: A tf.flags.FlagValues object, by default tf.flags.FLAGS, containing\n      the data_root of the datasets.\n  """"""\n  # The dictionary is built inside a function, rather than at the module\n  # top-level, because the FLAGS are not available at import time.\n  return {\n      # Datasets in the same order as reported in the article.\n      \'ilsvrc_2012\': (dataset_to_records.ImageNetConverter,\n                      ConverterArgs(\n                          data_root=flags.ilsvrc_2012_data_root,\n                          long_name=\'ImageNet ILSVRC-2012\')),\n      \'omniglot\': (dataset_to_records.OmniglotConverter,\n                   ConverterArgs(\n                       data_root=flags.omniglot_data_root,\n                       long_name=\'Omniglot\')),\n      \'aircraft\': (dataset_to_records.AircraftConverter,\n                   ConverterArgs(\n                       data_root=flags.aircraft_data_root,\n                       long_name=\'FGVC-Aircraft Benchmark\')),\n      \'cu_birds\': (dataset_to_records.CUBirdsConverter,\n                   ConverterArgs(\n                       data_root=flags.cu_birds_data_root,\n                       long_name=\'CU Birds\')),\n      \'dtd\': (dataset_to_records.DTDConverter,\n              ConverterArgs(\n                  data_root=flags.dtd_data_root,\n                  long_name=\'Describable Textures Dataset\')),\n      \'quickdraw\': (dataset_to_records.QuickdrawConverter,\n                    ConverterArgs(\n                        data_root=flags.quickdraw_data_root,\n                        long_name=\'Quick, Draw!\')),\n      \'fungi\': (dataset_to_records.FungiConverter,\n                ConverterArgs(\n                    data_root=flags.fungi_data_root,\n                    long_name=\'fungi 2018 FGVCx\')),\n      \'vgg_flower\': (dataset_to_records.VGGFlowerConverter,\n                     ConverterArgs(\n                         data_root=flags.vgg_flower_data_root,\n                         long_name=\'VGG Flower\')),\n      \'traffic_sign\': (dataset_to_records.TrafficSignConverter,\n                       ConverterArgs(\n                           data_root=flags.traffic_sign_data_root,\n                           long_name=\'Traffic Sign\')),\n      \'mscoco\':\n          (dataset_to_records.MSCOCOConverter,\n           ConverterArgs(data_root=flags.mscoco_data_root, long_name=\'MSCOCO\')),\n      # Diagnostics-only dataset\n      \'mini_imagenet\': (dataset_to_records.MiniImageNetConverter,\n                        ConverterArgs(\n                            data_root=flags.mini_imagenet_data_root,\n                            long_name=\'MiniImageNet\')),\n  }\n\n\ndef main(argv):\n  del argv\n\n  dataset_name_to_converter_and_args = _dataset_name_to_converter_and_args(\n      flags=FLAGS)\n  if FLAGS.dataset not in dataset_name_to_converter_and_args:\n    raise NotImplementedError(\n        \'Dataset {} not supported. Supported datasets are {}\'.format(\n            FLAGS.dataset, sorted(dataset_name_to_converter_and_args.keys())))\n\n  converter_class, converter_args = dataset_name_to_converter_and_args[\n      FLAGS.dataset]\n  if FLAGS.dataset == \'mini_imagenet\':\n    # MiniImagenet is for diagnostics purposes only, do not use the default\n    # records_path to avoid confusion.\n    records_path = FLAGS.mini_imagenet_records_dir\n  else:\n    records_path = None\n  converter = converter_class(\n      name=FLAGS.dataset,\n      data_root=converter_args.data_root,\n      records_path=records_path)\n  logging.info(\'Creating %s specification and records in directory %s...\',\n               converter_args.long_name, converter.records_path)\n  converter.convert_dataset()\n\n\nif __name__ == \'__main__\':\n  tf.app.run(main)\n'"
meta_dataset/dataset_conversion/dataset_to_records.py,73,"b'# coding=utf-8\n# Copyright 2020 The Meta-Dataset Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Tools for preparing datasets for integration in the benchmark.\n\nSpecifically, the DatasetConverter class is used to perform the conversion of a\ndataset to the format necessary for its addition in the benchmark. This involves\ncreating a DatasetSpecification for the dataset in question, and creating (and\nstoring) a tf.record for every one of its classes.\n\nSome subclasses make use of a ""split file"", which is a JSON file file that\nstores a dictionary whose keys are \'train\', \'valid\', and \'test\' and whose values\nindicate the corresponding classes assigned to these splits. Note that not all\ndatasets require a split file. For example it may be the case that a dataset\nindicates the intended assignment of classes to splits via their structure (e.g.\nall train classes live in a \'train\' folder etc).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport binascii\nimport collections\nimport io\nimport itertools\nimport json\nimport operator\nimport os\nimport traceback\n\nfrom absl import logging\nfrom meta_dataset.data import dataset_spec as ds_spec\nfrom meta_dataset.data import imagenet_specification\nfrom meta_dataset.data import learning_spec\nimport numpy as np\nfrom PIL import Image\nfrom PIL import ImageOps\nfrom scipy.io import loadmat\nimport six\nfrom six.moves import range\nimport six.moves.cPickle as pkl\nimport tensorflow.compat.v1 as tf\n\n# Datasets in the same order as reported in the article.\n# \'ilsvrc_2012_data_root\' is already defined in imagenet_specification.py.\ntf.flags.DEFINE_string(\n    \'ilsvrc_2012_num_leaf_images_path\', \'\',\n    \'A path used as a cache for a dict mapping the WordNet id of each Synset \'\n    \'of a ILSVRC 2012 class to its number of images. If empty, it defaults to \'\n    \'""ilsvrc_2012/num_leaf_images.json"" inside records_root.\')\n\ntf.flags.DEFINE_string(\n    \'omniglot_data_root\',\n    \'\',\n    \'Path to the root of the omniglot data.\')\n\ntf.flags.DEFINE_string(\n    \'aircraft_data_root\',\n    \'\',\n    \'Path to the root of the FGVC-Aircraft Benchmark.\')\n\ntf.flags.DEFINE_string(\n    \'cu_birds_data_root\',\n    \'\',\n    \'Path to the root of the CU-Birds dataset.\')\n\ntf.flags.DEFINE_string(\n    \'dtd_data_root\',\n    \'\',\n    \'Path to the root of the Describable Textures Dataset.\')\n\ntf.flags.DEFINE_string(\n    \'quickdraw_data_root\',\n    \'\',\n    \'Path to the root of the quickdraw data.\')\n\ntf.flags.DEFINE_string(\n    \'fungi_data_root\',\n    \'\',\n    \'Path to the root of the fungi data.\')\n\ntf.flags.DEFINE_string(\n    \'vgg_flower_data_root\',\n    \'\',\n    \'Path to the root of the VGG Flower data.\')\n\ntf.flags.DEFINE_string(\n    \'traffic_sign_data_root\',\n    \'\',\n    \'Path to the root of the Traffic Sign dataset.\')\n\ntf.flags.DEFINE_string(\n    \'mscoco_data_root\',\n    \'\',\n    \'Path to the root of the MSCOCO images and annotations. The root directory \'\n    \'should have a subdirectory `train2017` and an annotation JSON file \'\n    \'`instances_train2017.json`. Both can be downloaded from MSCOCO website: \'\n    \'http://cocodataset.org/#download and unzipped into the root directory.\')\n\n# Diagnostics-only dataset.\ntf.flags.DEFINE_string(\n    \'mini_imagenet_data_root\',\n    \'\',\n    \'Path to the root of the MiniImageNet data.\')\n\n# Output flags.\ntf.flags.DEFINE_string(\n    \'records_root\', \'\',\n    \'The root directory storing all tf.Records of datasets.\')\n\ntf.flags.DEFINE_string(\'splits_root\', \'\',\n                       \'The root directory storing the splits of datasets.\')\n\nFLAGS = tf.flags.FLAGS\nDEFAULT_FILE_PATTERN = \'{}.tfrecords\'\nTRAIN_TEST_FILE_PATTERN = \'{}_{}.tfrecords\'\nAUX_DATA_PATH = os.path.dirname(os.path.realpath(__file__))\nVGGFLOWER_LABELS_PATH = os.path.join(AUX_DATA_PATH,\n                                     \'VggFlower_labels.txt\')\nTRAFFICSIGN_LABELS_PATH = os.path.join(AUX_DATA_PATH, \'TrafficSign_labels.txt\')\n\n\ndef make_example(features):\n  """"""Creates an Example protocol buffer.\n\n  Create a protocol buffer with an integer feature for the class label, and a\n  bytes feature for the input (image or feature)\n\n  Args:\n    features: sequence of (key, feature_type, value) tuples. Features to encode\n      in the Example. `key` corresponds to the feature name, `feature_type` can\n      either be \'int64\', \'float32\', or \'bytes\', and `value` corresponds to the\n      feature itself.\n\n  Returns:\n    example_serial: A string corresponding to the serialized example.\n\n  """"""\n\n  def _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n  def _float32_feature(value):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n  def _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\n  feature_fns = {\n      \'int64\': _int64_feature,\n      \'float32\': _float32_feature,\n      \'bytes\': _bytes_feature\n  }\n\n  feature_dict = dict((key, feature_fns[feature_type](value))\n                      for key, feature_type, value in features)\n\n  # Create an example protocol buffer.\n  example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n  example_serial = example.SerializeToString()\n  return example_serial\n\n\ndef write_example(data_bytes,\n                  class_label,\n                  writer,\n                  input_key=\'image\',\n                  label_key=\'label\'):\n  """"""Create and write an Example protocol buffer for the given image.\n\n  Create a protocol buffer with an integer feature for the class label, and a\n  bytes feature for the image.\n\n  Args:\n    data_bytes: bytes, an encoded image representation or serialized feature.\n      For images, the usual encoding is JPEG, but could be different\n      as long as the DataProvider\'s record_decoder accepts it.\n    class_label: the integer class label of the image.\n    writer: a TFRecordWriter\n    input_key: String used as key for the input (image of feature).\n    label_key: String used as key for the label.\n  """"""\n  example = make_example([(input_key, \'bytes\', [data_bytes]),\n                          (label_key, \'int64\', [class_label])])\n  writer.write(example)\n\n\ndef gen_rand_split_inds(num_train_classes, num_valid_classes, num_test_classes):\n  """"""Generates a random set of indices corresponding to dataset splits.\n\n  It assumes the indices go from [0, num_classes), where the num_classes =\n  num_train_classes + num_val_classes + num_test_classes. The returned indices\n  are non-overlapping and cover the entire range.\n\n  Note that in the current implementation, valid_inds and test_inds are sorted,\n  but train_inds is in random order.\n\n  Args:\n    num_train_classes : int, number of (meta)-training classes.\n    num_valid_classes : int, number of (meta)-valid classes.\n    num_test_classes : int, number of (meta)-test classes.\n\n  Returns:\n    train_inds : np array of training inds.\n    valid_inds : np array of valid inds.\n    test_inds  : np array of test inds.\n  """"""\n  num_trainval_classes = num_train_classes + num_valid_classes\n  num_classes = num_trainval_classes + num_test_classes\n\n  # First split into trainval and test splits.\n  trainval_inds = np.random.choice(\n      num_classes, num_trainval_classes, replace=False)\n  test_inds = np.setdiff1d(np.arange(num_classes), trainval_inds)\n  # Now further split trainval into train and val.\n  train_inds = np.random.choice(trainval_inds, num_train_classes, replace=False)\n  valid_inds = np.setdiff1d(trainval_inds, train_inds)\n\n  logging.info(\n      \'Created splits with %d train, %d validation and %d test classes.\',\n      len(train_inds), len(valid_inds), len(test_inds))\n  return train_inds.tolist(), valid_inds.tolist(), test_inds.tolist()\n\n\ndef write_tfrecord_from_npy_single_channel(class_npy_file, class_label,\n                                           output_path):\n  """"""Create and write a tf.record file for the data of a class.\n\n  This assumes that the provided .npy file stores the data of a given class in\n  an array of shape [num_images_of_given_class, side**2].\n  In the case of the Quickdraw dataset for example, side = 28.\n  Each row of that array is interpreted as a single-channel side x side image,\n  read into a PIL.Image, converted to RGB and then written into a record.\n  Args:\n    class_npy_file: the .npy file of the images of class class_label.\n    class_label: the label of the class that a Record is being made for.\n    output_path: the location to write the Record.\n\n  Returns:\n    The number of images in the .npy file for class class_label.\n  """"""\n\n  def load_image(img):\n    """"""Load image img.\n\n    Args:\n      img: a 1D numpy array of shape [side**2]\n\n    Returns:\n      a PIL Image\n    """"""\n    # We make the assumption that the images are square.\n    side = int(np.sqrt(img.shape[0]))\n    # To load an array as a PIL.Image we must first reshape it to 2D.\n    img = Image.fromarray(img.reshape((side, side)))\n    img = img.convert(\'RGB\')\n    return img\n\n  with tf.io.gfile.GFile(class_npy_file, \'rb\') as f:\n    imgs = np.load(f)\n\n  # If the values are in the range 0-1, bring them to the range 0-255.\n  if imgs.dtype == np.bool:\n    imgs = imgs.astype(np.uint8)\n    imgs *= 255\n\n  writer = tf.python_io.TFRecordWriter(output_path)\n  # Takes a row each time, i.e. a different image (of the same class_label).\n  for image in imgs:\n    img = load_image(image)\n    # Compress to JPEG before writing\n    buf = io.BytesIO()\n    img.save(buf, format=\'JPEG\')\n    buf.seek(0)\n    write_example(buf.getvalue(), class_label, writer)\n\n  writer.close()\n  return len(imgs)\n\n\ndef write_tfrecord_from_image_files(class_files,\n                                    class_label,\n                                    output_path,\n                                    invert_img=False,\n                                    bboxes=None,\n                                    output_format=\'JPEG\',\n                                    skip_on_error=False):\n  """"""Create and write a tf.record file for the images corresponding to a class.\n\n  Args:\n    class_files: the list of paths to images of class class_label.\n    class_label: the label of the class that a record is being made for.\n    output_path: the location to write the record.\n    invert_img: change black pixels to white ones and vice versa. Used for\n      Omniglot for example to change the black-background-white-digit images\n      into more conventional-looking white-background-black-digit ones.\n    bboxes: list of bounding boxes, one for each filename passed as input. If\n      provided, images are cropped to those bounding box values.\n    output_format: a string representing a PIL.Image encoding type: how the\n      image data is encoded inside the tf.record. This needs to be consistent\n      with the record_decoder of the DataProvider that will read the file.\n    skip_on_error: whether to skip an image if there is an issue in reading it.\n      The default it to crash and report the original exception.\n\n  Returns:\n    The number of images written into the records file.\n  """"""\n\n  def load_and_process_image(path, bbox=None):\n    """"""Process the image living at path if necessary.\n\n    If the image does not need any processing (inverting, converting to RGB\n    for instance), and is in the desired output_format, then the original\n    byte representation is returned.\n\n    If that is not the case, the resulting image is encoded to output_format.\n\n    Args:\n      path: the path to an image file (e.g. a .png file).\n      bbox: bounding box to crop the image to.\n\n    Returns:\n      A bytes representation of the encoded image.\n    """"""\n    with tf.io.gfile.GFile(path, \'rb\') as f:\n      image_bytes = f.read()\n    try:\n      img = Image.open(io.BytesIO(image_bytes))\n    except:\n      logging.warn(\'Failed to open image: %s\', path)\n      raise\n\n    img_needs_encoding = False\n\n    if img.format != output_format:\n      img_needs_encoding = True\n    if img.mode != \'RGB\':\n      img = img.convert(\'RGB\')\n      img_needs_encoding = True\n    if bbox is not None:\n      img = img.crop(bbox)\n      img_needs_encoding = True\n    if invert_img:\n      img = ImageOps.invert(img)\n      img_needs_encoding = True\n\n    if img_needs_encoding:\n      # Convert the image into output_format\n      buf = io.BytesIO()\n      img.save(buf, format=output_format)\n      buf.seek(0)\n      image_bytes = buf.getvalue()\n    return image_bytes\n\n  writer = tf.python_io.TFRecordWriter(output_path)\n  written_images_count = 0\n  for i, path in enumerate(class_files):\n    bbox = bboxes[i] if bboxes is not None else None\n    try:\n      img = load_and_process_image(path, bbox)\n    except (IOError, tf.errors.PermissionDeniedError) as e:\n      if skip_on_error:\n        logging.warn(\'While trying to load file %s, got error: %s\', path, e)\n      else:\n        raise\n    else:\n      # This gets executed only if no Exception was raised\n      write_example(img, class_label, writer)\n      written_images_count += 1\n\n  writer.close()\n  return written_images_count\n\n\ndef write_tfrecord_from_directory(class_directory,\n                                  class_label,\n                                  output_path,\n                                  invert_img=False,\n                                  files_to_skip=None,\n                                  skip_on_error=False):\n  """"""Create and write a tf.record file for the images corresponding to a class.\n\n  Args:\n    class_directory: the home of the images of class class_label.\n    class_label: the label of the class that a record is being made for.\n    output_path: the location to write the record.\n    invert_img: change black pixels to white ones and vice versa. Used for\n      Omniglot for example to change the black-background-white-digit images\n      into more conventional-looking white-background-black-digit ones.\n    files_to_skip: a set containing names of files that should be skipped if\n      present in class_directory.\n    skip_on_error: whether to skip an image if there is an issue in reading it.\n      The default it to crash and report the original exception.\n\n  Returns:\n    The number of images written into the records file.\n  """"""\n  if files_to_skip is None:\n    files_to_skip = set()\n  class_files = []\n  filenames = sorted(tf.io.gfile.listdir(class_directory))\n  for filename in filenames:\n    if filename in files_to_skip:\n      logging.info(\'skipping file %s\', filename)\n      continue\n    filepath = os.path.join(class_directory, filename)\n    if tf.io.gfile.isdir(filepath):\n      continue\n    class_files.append(filepath)\n\n  written_images_count = write_tfrecord_from_image_files(\n      class_files,\n      class_label,\n      output_path,\n      invert_img,\n      skip_on_error=skip_on_error)\n\n  if not skip_on_error:\n    assert len(class_files) == written_images_count\n  return written_images_count\n\n\n# TODO(goroshin): Make sure to use this function where appropriate.\ndef encode_image(img, image_format):\n  """"""Get image encoded bytes from numpy array.\n\n     Note: use lossless PNG compression to test perfect reconstruction.\n  Args:\n    img: A numpy array of uint8 with shape [image_size, image_size, 3].\n    image_format: A string describing the image compression format.\n\n  Returns:\n    contents: The compressed image serialized to a string of bytes.\n  """"""\n  img = Image.fromarray(img)\n  buf = io.BytesIO()\n  img.save(buf, image_format)\n  buf.seek(0)\n  img_bytes = buf.getvalue()\n  buf.close()\n  return img_bytes\nclass DatasetConverter(object):\n  """"""Converts a dataset to the format required to integrate it in the benchmark.\n\n  In particular, this involves:\n  1) Creating a tf.record file for each class of the dataset.\n  2) Creating an instance of DatasetSpecification or BiLevelDatasetSpecification\n    (as appropriate) for the dataset. This includes information about the\n    splits, classes, super-classes if applicable, etc that is required for\n    creating episodes from the dataset.\n\n  1) and 2) are accomplished by calling the convert_dataset() method.\n  This will create and write the dataset specification and records in\n  self.records_path.\n  """"""\n\n  def __init__(self,\n               name,\n               data_root,\n               has_superclasses=False,\n               records_path=None,\n               split_file=None,\n               random_seed=22):\n    """"""Initialize a DatasetConverter.\n\n    Args:\n      name: the name of the dataset\n      data_root: the root of the dataset\n      has_superclasses: Whether the dataset\'s classes are organized in a two\n        level hierarchy of coarse and fine classes. In that case, a\n        BiLevelDatasetSpecification will be created.\n      records_path: optional path to store the created records. If it\'s not\n        provided, the default path for the dataset will be used.\n      split_file: optional path to a file storing the training, validation and\n        testing splits of the dataset\'s classes. If provided, it\'s a JSON file\n        that stores a dictionary whose keys are \'train\', \'valid\', and \'test\' and\n        whose values indicate the corresponding classes assigned to these\n        splits. Note that not all datasets require a split file. For example it\n        may be the case that a dataset indicates the intended assignment of\n        classes to splits via their structure (e.g. all train classes live in a\n        \'train\' folder etc).\n      random_seed: a random seed used for creating splits (when applicable) in a\n        reproducible way.\n    """"""\n    self.name = name\n    self.data_root = data_root\n    self.has_superclasses = has_superclasses\n    self.seed = random_seed\n    if records_path is None:\n      records_path = os.path.join(FLAGS.records_root, name)\n    tf.io.gfile.makedirs(records_path)\n    self.records_path = records_path\n\n    # Where to write the DatasetSpecification instance.\n    self.dataset_spec_path = os.path.join(self.records_path,\n                                          \'dataset_spec.json\')\n\n    self.split_file = split_file\n    if self.split_file is None:\n      self.split_file = os.path.join(FLAGS.splits_root,\n                                     \'{}_splits.json\'.format(self.name))\n      tf.io.gfile.makedirs(FLAGS.splits_root)\n\n    # Sets self.dataset_spec to an initial DatasetSpecification or\n    # BiLevelDatasetSpecification.\n    self._init_specification()\n\n  def _init_data_specification(self):\n    """"""Sets self.dataset_spec to an initial DatasetSpecification.""""""\n    # Maps each Split to the number of classes assigned to it.\n    self.classes_per_split = {\n        learning_spec.Split.TRAIN: 0,\n        learning_spec.Split.VALID: 0,\n        learning_spec.Split.TEST: 0\n    }\n\n    self._create_data_spec()\n\n  def _init_bilevel_data_specification(self):\n    """"""Sets self.dataset_spec to an initial BiLevelDatasetSpecification.""""""\n    # Maps each Split to the number of superclasses assigned to it.\n    self.superclasses_per_split = {\n        learning_spec.Split.TRAIN: 0,\n        learning_spec.Split.VALID: 0,\n        learning_spec.Split.TEST: 0\n    }\n\n    # Maps each superclass id to the number of classes it contains.\n    self.classes_per_superclass = collections.defaultdict(int)\n\n    # Maps each superclass id to the name of its class.\n    self.superclass_names = {}\n\n    self._create_data_spec()\n\n  def _init_specification(self):\n    """"""Returns an initial DatasetSpecification or BiLevelDatasetSpecification.\n\n    Creates this instance using initial values that need to be overwritten in\n    every sub-class implementing the converter for a different dataset. In\n    particular, in the case of a DatasetSpecification, each sub-class must\n    overwrite the 3 following fields accordingly: classes_per_split,\n    images_per_class, and class_names. In the case of its bi-level counterpart,\n    each sub-class must overwrite: superclasses_per_split,\n    classes_per_superclass, images_per_class, superclass_names, and class_names.\n    In both cases, this happens in create_dataset_specification_and_records().\n    Note that if other, non-mutable fields are updated, or if these objects are\n    replaced with other ones, see self._create_data_spec() to create a new spec.\n    """"""\n    # First initialize the fields that are common to both types of data specs.\n    # Maps each class id to its number of images.\n    self.images_per_class = collections.defaultdict(int)\n\n    # Maps each class id to the name of its class.\n    self.class_names = {}\n\n    # Pattern that each class\' filenames should adhere to.\n    self.file_pattern = DEFAULT_FILE_PATTERN\n\n    if self.has_superclasses:\n      self._init_bilevel_data_specification()\n    else:\n      self._init_data_specification()\n\n  def _create_data_spec(self):\n    """"""Create a new [BiLevel]DatasetSpecification given the fields in self.\n\n    Set self.dataset_spec to that new object. After the initial creation,\n    this is needed in the case of datasets with example-level splits, since\n    file_pattern and images_per_class have to be replaced by new objects.\n    """"""\n    if self.has_superclasses:\n      self.dataset_spec = ds_spec.BiLevelDatasetSpecification(\n          self.name, self.superclasses_per_split, self.classes_per_superclass,\n          self.images_per_class, self.superclass_names, self.class_names,\n          self.records_path, self.file_pattern)\n    else:\n      self.dataset_spec = ds_spec.DatasetSpecification(\n          self.name, self.classes_per_split, self.images_per_class,\n          self.class_names, self.records_path, self.file_pattern)\n\n  def convert_dataset(self):\n    """"""Converts dataset as required to integrate it in the benchmark.\n\n    Wrapper for self.create_dataset_specification_and_records() which does most\n    of the work. This method additionally handles writing the finalized\n    DatasetSpecification to the designated location.\n    """"""\n    self.create_dataset_specification_and_records()\n\n    # Write the DatasetSpecification to the designated location.\n    self.write_data_spec()\n\n  def create_dataset_specification_and_records(self):\n    """"""Creates a DatasetSpecification and records for the dataset.\n\n    Specifically, the work that needs to be done here is twofold:\n    Firstly, the initial values of the following attributes need to be updated:\n    1) self.classes_per_split: a dict mapping each split to the number of\n      classes assigned to it\n    2) self.images_per_class: a dict mapping each class to its number of images\n    3) self.class_names: a dict mapping each class (e.g. 0) to its (string) name\n      if available.\n    This automatically results to updating self.dataset_spec as required.\n\n    Important note: Must assign class ids in a certain order:\n    lowest ones for training classes, then for validation classes and highest\n    ones for testing classes.\n    The reader data sources operate under this assumption.\n\n    Secondly, a tf.record needs to be created and written for each class. There\n    are some general functions at the top of this file that may be useful for\n    this (e.g. write_tfrecord_from_npy_single_channel,\n    write_tfrecord_from_image_files).\n    """"""\n    raise NotImplementedError(\'Must be implemented in each sub-class.\')\n\n  def read_splits(self):\n    """"""Reads the splits for the dataset from self.split_file.\n\n    This will not always be used (as we noted earlier there are datasets that\n    define the splits in other ways, e.g. via structure of their directories).\n\n    Returns:\n      A splits dictionary mapping each split to a list of class names belonging\n      to it, or False upon failure (e.g. the splits do not exist).\n    """"""\n    logging.info(\'Attempting to read splits from %s...\', self.split_file)\n    if tf.io.gfile.exists(self.split_file):\n      with tf.io.gfile.GFile(self.split_file, \'r\') as f:\n        try:\n          splits = json.load(f)\n        except json.decoder.JSONDecodeError:\n          logging.info(\'Unsuccessful: file exists, but loading failed. %s\',\n                       traceback.format_exc())\n          return False\n        logging.info(\'Successful.\')\n        return splits\n    else:\n      logging.info(\'Unsuccessful.\')\n      return False\n\n  def write_data_spec(self):\n    """"""Write the dataset\'s specification to a JSON file.""""""\n    with tf.io.gfile.GFile(self.dataset_spec_path, \'w\') as f:\n      # Use 2-space indentation (which also add newlines) for legibility.\n      json.dump(self.dataset_spec.to_dict(), f, indent=2)\n\n  def get_splits(self, force_create=False):\n    """"""Returns the class splits.\n\n    If the splits already exist in the designated location, they are simply\n    read. Otherwise, they are created. For this, first reset the random seed to\n    self.seed for reproducibility, then create the splits and finally writes\n    them to the designated location.\n    The actual split creation takes place in self.create_splits() which each\n    sub-class must override.\n\n    Args:\n      force_create: bool. if True, the splits will be created even if they\n        already exist.\n\n    Returns:\n      splits: a dictionary whose keys are \'train\', \'valid\', and \'test\', and\n      whose values are lists of the corresponding class names.\n    """"""\n    # Check if the splits already exist.\n    if not force_create:\n      splits = self.read_splits()\n      if splits:\n        return splits\n\n    # First, re-set numpy\'s random seed, for reproducibility.\n    np.random.seed(self.seed)\n\n    # Create the dataset-specific splits.\n    splits = self.create_splits()\n\n    # Finally, write the splits in the designated location.\n    logging.info(\'Saving new splits for dataset %s at %s...\', self.name,\n                 self.split_file)\n    with tf.io.gfile.GFile(self.split_file, \'w\') as f:\n      json.dump(splits, f, indent=2)\n    logging.info(\'Done.\')\n\n    return splits\n\n  def create_splits(self):\n    """"""Create class splits.\n\n    Specifically, create a dictionary whose keys are \'train\', \'valid\', and\n    \'test\', and whose values are lists of the corresponding classes.\n    """"""\n    raise NotImplementedError(\'Must be implemented in each sub-class.\')\n\n\nclass OmniglotConverter(DatasetConverter):\n  """"""Prepares Omniglot as required for integrating it in the benchmark.\n\n  Omniglot is organized into two high-level directories, referred to as\n  the background and evaluation sets, respectively, with the former\n  intended for training and the latter for testing. Each of these contains a\n  number of sub-directories, corresponding to different alphabets.\n  Each alphabet directory in turn has a number of sub-folders, each\n  corresponding to a character, which stores 20 images of that character, each\n  drawn by a different person.\n  We consider each character to be a different class for our purposes.\n  The following diagram illustrates this struture.\n\n  omniglot_root\n  |- images_background\n     |- alphabet\n        |- character\n           |- images of character\n        ...\n  |- images_evaluation\n    |- alphabet\n        |- character\n           |- images of character\n        ...\n  """"""\n\n  def __init__(self, *args, **kwargs):\n    """"""Initialize an OmniglotConverter.""""""\n    # Make has_superclasses default to True for the Omniglot dataset.\n    if \'has_superclasses\' not in kwargs:\n      kwargs[\'has_superclasses\'] = True\n    super(OmniglotConverter, self).__init__(*args, **kwargs)\n\n  def parse_split_data(self, split, alphabets, alphabets_path):\n    """"""Parse the data of the given split.\n\n    Specifically, update self.class_names, self.images_per_class, and\n    self.classes_per_split with the information for the given split, and\n    create and write records of the classes of the given split.\n\n    Args:\n      split: an instance of learning_spec.Split\n      alphabets: the list of names of alphabets belonging to split\n      alphabets_path: the directory with the folders corresponding to alphabets.\n    """"""\n    # Each alphabet is a superclass.\n    for alphabet_folder_name in alphabets:\n      alphabet_path = os.path.join(alphabets_path, alphabet_folder_name)\n      # Each character is a class.\n      for char_folder_name in sorted(tf.io.gfile.listdir(alphabet_path)):\n        class_path = os.path.join(alphabet_path, char_folder_name)\n        class_label = len(self.class_names)\n        class_records_path = os.path.join(\n            self.records_path,\n            self.dataset_spec.file_pattern.format(class_label))\n        self.class_names[class_label] = \'{}-{}\'.format(alphabet_folder_name,\n                                                       char_folder_name)\n        self.images_per_class[class_label] = len(\n            tf.io.gfile.listdir(class_path))\n\n        # Create and write the tf.Record of the examples of this class.\n        write_tfrecord_from_directory(\n            class_path, class_label, class_records_path, invert_img=True)\n\n        # Add this character to the count of subclasses of this superclass.\n        superclass_label = len(self.superclass_names)\n        self.classes_per_superclass[superclass_label] += 1\n\n      # Add this alphabet as a superclass.\n      self.superclasses_per_split[split] += 1\n      self.superclass_names[superclass_label] = alphabet_folder_name\n\n  def create_dataset_specification_and_records(self):\n    """"""Implements DatasetConverter.create_dataset_specification_and_records().\n\n    We use Lake\'s original train/test splits as we believe this is a more\n    challenging setup and because we like that it\'s hierarchically structured.\n    We also held out a subset of that train split to act as our validation set.\n    Specifically, the 5 alphabets from that set with the least number of\n    characters were chosen for this purpose.\n    """"""\n\n    # We chose the 5 smallest alphabets (i.e. those with the least characters)\n    # out of the \'background\' set of alphabets that are intended for train/val\n    # We keep the \'evaluation\' set of alphabets for testing exclusively\n    # The chosen alphabets have 14, 14, 16, 17, and 20 characters, respectively.\n    validation_alphabets = [\n        \'Blackfoot_(Canadian_Aboriginal_Syllabics)\',\n        \'Ojibwe_(Canadian_Aboriginal_Syllabics)\',\n        \'Inuktitut_(Canadian_Aboriginal_Syllabics)\', \'Tagalog\',\n        \'Alphabet_of_the_Magi\'\n    ]\n\n    training_alphabets = []\n    data_path_trainval = os.path.join(self.data_root, \'images_background\')\n    for alphabet_name in sorted(tf.io.gfile.listdir(data_path_trainval)):\n      if alphabet_name not in validation_alphabets:\n        training_alphabets.append(alphabet_name)\n    assert len(training_alphabets) + len(validation_alphabets) == 30\n\n    data_path_test = os.path.join(self.data_root, \'images_evaluation\')\n    test_alphabets = sorted(tf.io.gfile.listdir(data_path_test))\n    assert len(test_alphabets) == 20\n\n    self.parse_split_data(learning_spec.Split.TRAIN, training_alphabets,\n                          data_path_trainval)\n    self.parse_split_data(learning_spec.Split.VALID, validation_alphabets,\n                          data_path_trainval)\n    self.parse_split_data(learning_spec.Split.TEST, test_alphabets,\n                          data_path_test)\n\n\nclass QuickdrawConverter(DatasetConverter):\n  """"""Prepares Quickdraw as required to integrate it in the benchmark.""""""\n\n  def create_splits(self):\n    """"""Create splits for Quickdraw and store them in the default path.""""""\n    # Quickdraw is stored in a number of .npy files, one for every class\n    # with each .npy file storing an array containing the images of that class.\n    class_npy_files = sorted(tf.io.gfile.listdir(self.data_root))\n    class_names = [fname[:fname.find(\'.\')] for fname in class_npy_files]\n    # Sort the class names, for reproducibility.\n    class_names.sort()\n    num_classes = len(class_npy_files)\n    # Split into train, validation and test splits that have 70% / 15% / 15%\n    # of the data, respectively.\n    num_trainval_classes = int(0.85 * num_classes)\n    num_train_classes = int(0.7 * num_classes)\n    num_valid_classes = num_trainval_classes - num_train_classes\n    num_test_classes = num_classes - num_trainval_classes\n\n    train_inds, valid_inds, test_inds = gen_rand_split_inds(\n        num_train_classes, num_valid_classes, num_test_classes)\n    splits = {\n        \'train\': [class_names[i] for i in train_inds],\n        \'valid\': [class_names[i] for i in valid_inds],\n        \'test\': [class_names[i] for i in test_inds]\n    }\n    return splits\n\n  def parse_split_data(self, split, split_class_names):\n    """"""Parse the data of the given split.\n\n    Specifically, update self.class_names, self.images_per_class, and\n    self.classes_per_split with the information for the given split, and\n    create and write records of the classes of the given split.\n\n    Args:\n      split: an instance of learning_spec.Split\n      split_class_names: the list of names of classes belonging to split\n    """"""\n    for class_name in split_class_names:\n      self.classes_per_split[split] += 1\n      class_label = len(self.class_names)\n      class_records_path = os.path.join(\n          self.records_path, self.dataset_spec.file_pattern.format(class_label))\n\n      # The names of the files in self.data_root for Quickdraw are of the form\n      # class_name.npy, for example airplane.npy.\n      class_npy_fname = class_name + \'.npy\'\n      self.class_names[class_label] = class_name\n      class_path = os.path.join(self.data_root, class_npy_fname)\n\n      # Create and write the tf.Record of the examples of this class.\n      num_imgs = write_tfrecord_from_npy_single_channel(class_path, class_label,\n                                                        class_records_path)\n      self.images_per_class[class_label] = num_imgs\n\n  def create_dataset_specification_and_records(self):\n    """"""Implements DatasetConverter.create_dataset_specification_and_records.\n\n    If no split file is provided, and the default location for Quickdraw splits\n    does not contain a split file, splits are randomly created in this\n    function using 70%, 15%, and 15% of the data for training, validation and\n    testing, respectively, and then stored in that default location.\n\n    The splits for this dataset are represented as a dictionary mapping each of\n    \'train\', \'valid\', and \'test\' to a list of class names. For example the value\n    associated with the key \'train\' may be [\'angel\', \'clock\', ...].\n    """"""\n\n    splits = self.get_splits()\n    # Get the names of the classes assigned to each split.\n    train_classes = splits[\'train\']\n    valid_classes = splits[\'valid\']\n    test_classes = splits[\'test\']\n\n    self.parse_split_data(learning_spec.Split.TRAIN, train_classes)\n    self.parse_split_data(learning_spec.Split.VALID, valid_classes)\n    self.parse_split_data(learning_spec.Split.TEST, test_classes)\n\n\nclass CUBirdsConverter(DatasetConverter):\n  """"""Prepares CU-Birds dataset as required to integrate it in the benchmark.""""""\n  # There are 200 classes in CU-Birds.\n  NUM_TRAIN_CLASSES = 140\n  NUM_VALID_CLASSES = 30\n  NUM_TEST_CLASSES = 30\n  NUM_TOTAL_CLASSES = NUM_TRAIN_CLASSES + NUM_VALID_CLASSES + NUM_TEST_CLASSES\n\n  def create_splits(self):\n    """"""Create splits for CU-Birds and store them in the default path.\n\n    If no split file is provided, and the default location for CU-Birds splits\n    does not contain a split file, splits are randomly created in this\n    function using 70%, 15%, and 15% of the data for training, validation and\n    testing, respectively, and then stored in that default location.\n\n    Returns:\n      The splits for this dataset, represented as a dictionary mapping each of\n      \'train\', \'valid\', and \'test\' to a list of class names.\n    """"""\n\n    with tf.io.gfile.GFile(os.path.join(self.data_root, \'classes.txt\'),\n                           \'r\') as f:\n      class_names = []\n      for lines in f:\n        _, class_name = lines.strip().split(\' \')\n        class_names.append(class_name)\n\n    err_msg = \'number of classes in dataset does not match split specification\'\n    assert len(class_names) == self.NUM_TOTAL_CLASSES, err_msg\n\n    train_inds, valid_inds, test_inds = gen_rand_split_inds(\n        self.NUM_TRAIN_CLASSES, self.NUM_VALID_CLASSES, self.NUM_TEST_CLASSES)\n    splits = {\n        \'train\': [class_names[i] for i in train_inds],\n        \'valid\': [class_names[i] for i in valid_inds],\n        \'test\': [class_names[i] for i in test_inds]\n    }\n    return splits\n\n  def create_dataset_specification_and_records(self):\n    """"""Implements DatasetConverter.create_dataset_specification_and_records.""""""\n\n    splits = self.get_splits()\n    # Get the names of the classes assigned to each split.\n    train_classes = splits[\'train\']\n    valid_classes = splits[\'valid\']\n    test_classes = splits[\'test\']\n\n    self.classes_per_split[learning_spec.Split.TRAIN] = len(train_classes)\n    self.classes_per_split[learning_spec.Split.VALID] = len(valid_classes)\n    self.classes_per_split[learning_spec.Split.TEST] = len(test_classes)\n\n    image_root_folder = os.path.join(self.data_root, \'images\')\n    all_classes = list(\n        itertools.chain(train_classes, valid_classes, test_classes))\n    for class_id, class_label in enumerate(all_classes):\n      logging.info(\'Creating record for class ID %d (%s)...\', class_id,\n                   class_label)\n      class_records_path = os.path.join(\n          self.records_path, self.dataset_spec.file_pattern.format(class_id))\n      self.class_names[class_id] = class_label\n      class_directory = os.path.join(image_root_folder, class_label)\n      self.images_per_class[class_id] = len(\n          tf.io.gfile.listdir(class_directory))\n      write_tfrecord_from_directory(class_directory, class_id,\n                                    class_records_path)\n\n\nclass VGGFlowerConverter(DatasetConverter):\n  """"""Prepares VGG Flower as required to integrate it in the benchmark.""""""\n  # There are 102 classes in the VGG Flower dataset. A 70% / 15% / 15% split\n  # between train, validation and test maps to roughly 71 / 15 / 16 classes,\n  # respectively.\n  NUM_TRAIN_CLASSES = 71\n  NUM_VALID_CLASSES = 15\n  NUM_TEST_CLASSES = 16\n  NUM_TOTAL_CLASSES = NUM_TRAIN_CLASSES + NUM_VALID_CLASSES + NUM_TEST_CLASSES\n  ID_LEN = 3\n\n  def create_splits(self):\n    """"""Create splits for VGG Flower and store them in the default path.\n\n    If no split file is provided, and the default location for VGG Flower splits\n    does not contain a split file, splits are randomly created in this\n    function using 70%, 15%, and 15% of the data for training, validation and\n    testing, respectively, and then stored in that default location.\n\n    Returns:\n      The splits for this dataset, represented as a dictionary mapping each of\n      \'train\', \'valid\', and \'test\' to a list of class integers.\n    """"""\n    # Load class names from the text file\n    file_path = VGGFLOWER_LABELS_PATH\n    with tf.io.gfile.GFile(file_path) as fd:\n      all_lines = fd.read()\n    # First line is expected to be a comment.\n    class_names = all_lines.splitlines()[1:]\n    err_msg = \'number of classes in dataset does not match split specification\'\n    assert len(class_names) == self.NUM_TOTAL_CLASSES, err_msg\n\n    # Provided class labels are numbers started at 1.\n    train_inds, valid_inds, test_inds = gen_rand_split_inds(\n        self.NUM_TRAIN_CLASSES, self.NUM_VALID_CLASSES, self.NUM_TEST_CLASSES)\n    format_str = \'%%0%dd.%%s\' % self.ID_LEN\n    splits = {\n        \'train\': [format_str % (i + 1, class_names[i]) for i in train_inds],\n        \'valid\': [format_str % (i + 1, class_names[i]) for i in valid_inds],\n        \'test\': [format_str % (i + 1, class_names[i]) for i in test_inds]\n    }\n    return splits\n\n  def create_dataset_specification_and_records(self):\n    """"""Implements DatasetConverter.create_dataset_specification_and_records.""""""\n    splits = self.get_splits()\n    # Get the names of the classes assigned to each split.\n    train_classes = splits[\'train\']\n    valid_classes = splits[\'valid\']\n    test_classes = splits[\'test\']\n\n    self.classes_per_split[learning_spec.Split.TRAIN] = len(train_classes)\n    self.classes_per_split[learning_spec.Split.VALID] = len(valid_classes)\n    self.classes_per_split[learning_spec.Split.TEST] = len(test_classes)\n\n    imagelabels_path = os.path.join(self.data_root, \'imagelabels.mat\')\n    with tf.io.gfile.GFile(imagelabels_path, \'rb\') as f:\n      labels = loadmat(f)[\'labels\'][0]\n    filepaths = collections.defaultdict(list)\n    for i, label in enumerate(labels):\n      filepaths[label].append(\n          os.path.join(self.data_root, \'jpg\', \'image_{:05d}.jpg\'.format(i + 1)))\n\n    all_classes = list(\n        itertools.chain(train_classes, valid_classes, test_classes))\n    # Class IDs are constructed in such a way that\n    #   - training class IDs lie in [0, num_train_classes),\n    #   - validation class IDs lie in\n    #     [num_train_classes, num_train_classes + num_validation_classes), and\n    #   - test class IDs lie in\n    #     [num_train_classes + num_validation_classes, num_classes).\n    for class_id, class_label in enumerate(all_classes):\n      logging.info(\'Creating record for class ID %d (%s)...\', class_id,\n                   class_label)\n      # We encode the original ID\'s in the label.\n      original_id = int(class_label[:self.ID_LEN])\n      class_paths = filepaths[original_id]\n      class_records_path = os.path.join(\n          self.records_path, self.dataset_spec.file_pattern.format(class_id))\n      self.class_names[class_id] = class_label\n      self.images_per_class[class_id] = len(class_paths)\n      # Create and write the tf.Record of the examples of this class.\n      write_tfrecord_from_image_files(class_paths, class_id, class_records_path)\n\n\nclass DTDConverter(DatasetConverter):\n  """"""Prepares DTD as required to integrate it in the benchmark.""""""\n  # There are 47 classes in the Describable Textures Dataset. A 70% / 15% / 15%\n  # split between train, validation and test maps to roughly 33 / 7 / 7 classes,\n  # respectively.\n  NUM_TRAIN_CLASSES = 33\n  NUM_VALID_CLASSES = 7\n  NUM_TEST_CLASSES = 7\n\n  def create_splits(self):\n    """"""Create splits for DTD and store them in the default path.\n\n    If no split file is provided, and the default location for DTD splits\n    does not contain a split file, splits are randomly created in this\n    function using 70%, 15%, and 15% of the data for training, validation and\n    testing, respectively, and then stored in that default location.\n\n    Returns:\n      The splits for this dataset, represented as a dictionary mapping each of\n      \'train\', \'valid\', and \'test\' to a list of strings (class names).\n    """"""\n    train_inds, valid_inds, test_inds = gen_rand_split_inds(\n        self.NUM_TRAIN_CLASSES, self.NUM_VALID_CLASSES, self.NUM_TEST_CLASSES)\n    class_names = sorted(\n        tf.io.gfile.listdir(os.path.join(self.data_root, \'images\')))\n    splits = {\n        \'train\': [class_names[i] for i in train_inds],\n        \'valid\': [class_names[i] for i in valid_inds],\n        \'test\': [class_names[i] for i in test_inds]\n    }\n    return splits\n\n  def create_dataset_specification_and_records(self):\n    """"""Implements DatasetConverter.create_dataset_specification_and_records.""""""\n\n    splits = self.get_splits()\n    # Get the names of the classes assigned to each split.\n    train_classes = splits[\'train\']\n    valid_classes = splits[\'valid\']\n    test_classes = splits[\'test\']\n\n    self.classes_per_split[learning_spec.Split.TRAIN] = len(train_classes)\n    self.classes_per_split[learning_spec.Split.VALID] = len(valid_classes)\n    self.classes_per_split[learning_spec.Split.TEST] = len(test_classes)\n\n    all_classes = list(\n        itertools.chain(train_classes, valid_classes, test_classes))\n\n    for class_id, class_name in enumerate(all_classes):\n      logging.info(\'Creating record for class ID %d (%s)...\', class_id,\n                   class_name)\n      class_directory = os.path.join(self.data_root, \'images\', class_name)\n      class_records_path = os.path.join(\n          self.records_path, self.dataset_spec.file_pattern.format(class_id))\n      self.class_names[class_id] = class_name\n      # \'waffled\' class directory has a leftover \'.directory\' file.\n      files_to_skip = set()\n      if class_name == \'waffled\':\n        files_to_skip.add(\'.directory\')\n      self.images_per_class[class_id] = write_tfrecord_from_directory(\n          class_directory,\n          class_id,\n          class_records_path,\n          files_to_skip=files_to_skip)\n\n\nclass AircraftConverter(DatasetConverter):\n  """"""Prepares Aircraft as required to integrate it in the benchmark.""""""\n  # There are 100 classes in the Aircraft dataset. A 70% / 15% / 15%\n  # split between train, validation and test maps to 70 / 15 / 15\n  # classes, respectively.\n  NUM_TRAIN_CLASSES = 70\n  NUM_VALID_CLASSES = 15\n  NUM_TEST_CLASSES = 15\n\n  def create_splits(self):\n    """"""Create splits for Aircraft and store them in the default path.\n\n    If no split file is provided, and the default location for Aircraft splits\n    does not contain a split file, splits are randomly created in this\n    function using 70%, 15%, and 15% of the data for training, validation and\n    testing, respectively, and then stored in that default location.\n\n    Returns:\n      The splits for this dataset, represented as a dictionary mapping each of\n      \'train\', \'valid\', and \'test\' to a list of strings (class names).\n    """"""\n    train_inds, valid_inds, test_inds = gen_rand_split_inds(\n        self.NUM_TRAIN_CLASSES, self.NUM_VALID_CLASSES, self.NUM_TEST_CLASSES)\n    # ""Variant"" refers to the aircraft model variant (e.g., A330-200) and is\n    # used as the class name in the dataset.\n    variants_path = os.path.join(self.data_root, \'data\', \'variants.txt\')\n    with tf.io.gfile.GFile(variants_path, \'r\') as f:\n      variants = [line.strip() for line in f.readlines() if line]\n    variants = sorted(variants)\n    assert len(variants) == (\n        self.NUM_TRAIN_CLASSES + self.NUM_VALID_CLASSES + self.NUM_TEST_CLASSES)\n\n    splits = {\n        \'train\': [variants[i] for i in train_inds],\n        \'valid\': [variants[i] for i in valid_inds],\n        \'test\': [variants[i] for i in test_inds]\n    }\n    return splits\n\n  def create_dataset_specification_and_records(self):\n    """"""Implements DatasetConverter.create_dataset_specification_and_records.""""""\n\n    splits = self.get_splits()\n    # Get the names of the classes assigned to each split\n    train_classes = splits[\'train\']\n    valid_classes = splits[\'valid\']\n    test_classes = splits[\'test\']\n\n    self.classes_per_split[learning_spec.Split.TRAIN] = len(train_classes)\n    self.classes_per_split[learning_spec.Split.VALID] = len(valid_classes)\n    self.classes_per_split[learning_spec.Split.TEST] = len(test_classes)\n\n    # Retrieve mapping from filename to bounding box.\n    # Cropping to the bounding boxes is important for two reasons:\n    # 1) The dataset documentation mentions that ""[the] (main) aircraft in each\n    #    image is annotated with a tight bounding box [...]"", which suggests\n    #    that there may be more than one aircraft in some images. Cropping to\n    #    the bounding boxes removes ambiguity as to which airplane the label\n    #    refers to.\n    # 2) Raw images have a 20-pixel border at the bottom with copyright\n    #    information which needs to be removed. Cropping to the bounding boxes\n    #    has the side-effect that it removes the border.\n    bboxes_path = os.path.join(self.data_root, \'data\', \'images_box.txt\')\n    with tf.io.gfile.GFile(bboxes_path, \'r\') as f:\n      names_to_bboxes = [\n          line.split(\'\\n\')[0].split(\' \') for line in f.readlines()\n      ]\n      names_to_bboxes = dict(\n          (name, map(int, (xmin, ymin, xmax, ymax)))\n          for name, xmin, ymin, xmax, ymax in names_to_bboxes)\n\n    # Retrieve mapping from filename to variant\n    variant_trainval_path = os.path.join(self.data_root, \'data\',\n                                         \'images_variant_trainval.txt\')\n    with tf.io.gfile.GFile(variant_trainval_path, \'r\') as f:\n      names_to_variants = [\n          line.split(\'\\n\')[0].split(\' \', 1) for line in f.readlines()\n      ]\n\n    variant_test_path = os.path.join(self.data_root, \'data\',\n                                     \'images_variant_test.txt\')\n    with tf.io.gfile.GFile(variant_test_path, \'r\') as f:\n      names_to_variants += [\n          line.split(\'\\n\')[0].split(\' \', 1) for line in f.readlines()\n      ]\n\n    names_to_variants = dict(names_to_variants)\n\n    # Build mapping from variant to filenames. ""Variant"" refers to the aircraft\n    # model variant (e.g., A330-200) and is used as the class name in the\n    # dataset. The position of the class name in the concatenated list of\n    # training, validation, and test class name constitutes its class ID.\n    variants_to_names = collections.defaultdict(list)\n    for name, variant in names_to_variants.items():\n      variants_to_names[variant].append(name)\n\n    all_classes = list(\n        itertools.chain(train_classes, valid_classes, test_classes))\n    assert set(variants_to_names.keys()) == set(all_classes)\n\n    for class_id, class_name in enumerate(all_classes):\n      logging.info(\'Creating record for class ID %d (%s)...\', class_id,\n                   class_name)\n      class_files = [\n          os.path.join(self.data_root, \'data\', \'images\',\n                       \'{}.jpg\'.format(filename))\n          for filename in sorted(variants_to_names[class_name])\n      ]\n      bboxes = [\n          names_to_bboxes[name]\n          for name in sorted(variants_to_names[class_name])\n      ]\n      class_records_path = os.path.join(\n          self.records_path, self.dataset_spec.file_pattern.format(class_id))\n      self.class_names[class_id] = class_name\n      self.images_per_class[class_id] = len(class_files)\n\n      write_tfrecord_from_image_files(\n          class_files, class_id, class_records_path, bboxes=bboxes)\n\n\nclass TrafficSignConverter(DatasetConverter):\n  """"""Prepares Traffic Sign as required to integrate it in the benchmark.""""""\n  # There are 43 classes in the Traffic Sign dataset, all of which are used for\n  # test episodes.\n  NUM_TRAIN_CLASSES = 0\n  NUM_VALID_CLASSES = 0\n  NUM_TEST_CLASSES = 43\n  NUM_TOTAL_CLASSES = NUM_TRAIN_CLASSES + NUM_VALID_CLASSES + NUM_TEST_CLASSES\n\n  def create_splits(self):\n    """"""Create splits for Traffic Sign and store them in the default path.\n\n    If no split file is provided, and the default location for Traffic Sign\n    splits does not contain a split file, a\n    self.NUM_TRAIN_CLASSES / self.NUM_VALID_CLASSES / self.NUM_TEST_CLASSES\n    split is created and stored in that default location.\n\n    Returns:\n      The splits for this dataset, represented as a dictionary mapping each of\n      \'train\', \'valid\', and \'test\' to a list of class names.\n    """"""\n    # Load class names from the text file\n    file_path = TRAFFICSIGN_LABELS_PATH\n    with tf.io.gfile.GFile(file_path) as fd:\n      all_lines = fd.read()\n    # First line is expected to be a comment.\n    class_names = all_lines.splitlines()[1:]\n\n    err_msg = \'number of classes in dataset does not match split specification\'\n    assert len(class_names) == self.NUM_TOTAL_CLASSES, err_msg\n\n    splits = {\n        \'train\': [],\n        \'valid\': [],\n        \'test\': [\n            \'%02d.%s\' % (i, class_names[i])\n            for i in range(self.NUM_TEST_CLASSES)\n        ]\n    }\n    return splits\n\n  def create_dataset_specification_and_records(self):\n    """"""Implements DatasetConverter.create_dataset_specification_and_records.""""""\n\n    splits = self.get_splits()\n    # Get the names of the classes assigned to each split\n    train_classes = splits[\'train\']\n    valid_classes = splits[\'valid\']\n    test_classes = splits[\'test\']\n\n    self.classes_per_split[learning_spec.Split.TRAIN] = len(train_classes)\n    self.classes_per_split[learning_spec.Split.VALID] = len(valid_classes)\n    self.classes_per_split[learning_spec.Split.TEST] = len(test_classes)\n\n    all_classes = list(\n        itertools.chain(train_classes, valid_classes, test_classes))\n    for class_id, class_label in enumerate(all_classes):\n      logging.info(\'Creating record for class ID %d (%s)...\', class_id,\n                   class_label)\n      # The raw dataset file uncompresses to `GTSRB/Final_Training/Images/`.\n      # The `Images` subdirectory contains 43 subdirectories (one for each\n      # class) whose names are zero-padded, 5-digit strings representing the\n      # class number. data_root should be the path to the GTSRB directory.\n      class_directory = os.path.join(self.data_root, \'Final_Training\', \'Images\',\n                                     \'{:05d}\'.format(class_id))\n      class_records_path = os.path.join(\n          self.records_path, self.dataset_spec.file_pattern.format(class_id))\n      self.class_names[class_id] = class_label\n      # We skip `GT-?????.csv` files, which contain addditional annotations.\n      self.images_per_class[class_id] = write_tfrecord_from_directory(\n          class_directory,\n          class_id,\n          class_records_path,\n          files_to_skip=set([\'GT-{:05d}.csv\'.format(class_id)]))\n\n\nclass MSCOCOConverter(DatasetConverter):\n  """"""Prepares MSCOCO as required to integrate it in the benchmark.""""""\n\n  # There are 80 classes in the MSCOCO dataset. A 0% / 50% / 50% split\n  # between train, validation and test maps to roughly 0 / 40 / 40 classes,\n  # respectively.\n  NUM_TRAIN_CLASSES = 0\n  NUM_VALID_CLASSES = 40\n  NUM_TEST_CLASSES = 40\n\n  def __init__(self,\n               name,\n               data_root,\n               records_path=None,\n               split_file=None,\n               image_subdir_name=\'train2017\',\n               annotation_json_name=\'instances_train2017.json\',\n               box_scale_ratio=1.2):\n    self.num_all_classes = (\n        self.NUM_TRAIN_CLASSES + self.NUM_VALID_CLASSES + self.NUM_TEST_CLASSES)\n    image_dir = os.path.join(data_root, image_subdir_name)\n    if not tf.io.gfile.isdir(image_dir):\n      raise ValueError(\'Directory %s does not exist\' % image_dir)\n    self.image_dir = image_dir\n\n    annotation_path = os.path.join(data_root, annotation_json_name)\n    if not tf.io.gfile.exists(annotation_path):\n      raise ValueError(\'Annotation file %s does not exist\' % annotation_path)\n    with tf.io.gfile.GFile(annotation_path, \'r\') as json_file:\n      annotations = json.load(json_file)\n      instance_annotations = annotations[\'annotations\']\n      if not instance_annotations:\n        raise ValueError(\'Instance annotations is empty.\')\n      self.coco_instance_annotations = instance_annotations\n      categories = annotations[\'categories\']\n      if len(categories) != self.num_all_classes:\n        raise ValueError(\n            \'Total number of MSCOCO classes %d should be equal to the sum of \'\n            \'train, val, test classes %d.\' %\n            (len(categories), self.num_all_classes))\n      self.coco_categories = categories\n    self.coco_name_to_category = {cat[\'name\']: cat for cat in categories}\n\n    if box_scale_ratio < 1.0:\n      raise ValueError(\'Box scale ratio must be greater or equal to 1.0.\')\n    self.box_scale_ratio = box_scale_ratio\n\n    super(MSCOCOConverter, self).__init__(name, data_root, records_path,\n                                          split_file)\n\n  def create_splits(self):\n    """"""Create splits for MSCOCO and store them in the default path.\n\n    Returns:\n      The splits for this dataset, represented as a dictionary mapping each of\n      \'train\', \'valid\', and \'test\' to a list of class names.\n    """"""\n    train_inds, valid_inds, test_inds = gen_rand_split_inds(\n        self.NUM_TRAIN_CLASSES, self.NUM_VALID_CLASSES, self.NUM_TEST_CLASSES)\n\n    splits = {\n        \'train\': [self.coco_categories[i][\'name\'] for i in train_inds],\n        \'valid\': [self.coco_categories[i][\'name\'] for i in valid_inds],\n        \'test\': [self.coco_categories[i][\'name\'] for i in test_inds]\n    }\n    return splits\n\n  def create_dataset_specification_and_records(self):\n    """"""Implements DatasetConverter.create_dataset_specification_and_records.""""""\n    splits = self.get_splits()\n    self.classes_per_split[learning_spec.Split.TRAIN] = len(splits[\'train\'])\n    self.classes_per_split[learning_spec.Split.VALID] = len(splits[\'valid\'])\n    self.classes_per_split[learning_spec.Split.TEST] = len(splits[\'test\'])\n    all_classes = list(\n        itertools.chain(splits[\'train\'], splits[\'valid\'], splits[\'test\']))\n\n    # Map original COCO ""id"" to class ids that conform to DatasetConverter\'s\n    # contract.\n    coco_id_to_class_id = {}\n    for class_id, class_name in enumerate(all_classes):\n      self.class_names[class_id] = class_name\n      category = self.coco_name_to_category[class_name]\n      coco_id_to_class_id[category[\'id\']] = class_id\n\n    def get_image_crop_and_class_id(annotation):\n      """"""Gets image crop and its class label.""""""\n      image_id = annotation[\'image_id\']\n      image_path = os.path.join(self.image_dir, \'%012d.jpg\' % image_id)\n      # The bounding box is represented as (x_topleft, y_topleft, width, height)\n      bbox = annotation[\'bbox\']\n      coco_class_id = annotation[\'category_id\']\n      class_id = coco_id_to_class_id[coco_class_id]\n\n      with tf.io.gfile.GFile(image_path, \'rb\') as f:\n        # The image shape is [?, ?, 3] and the type is uint8.\n        image = Image.open(f)\n        image = image.convert(mode=\'RGB\')\n        image_w, image_h = image.size\n\n        def scale_box(bbox, scale_ratio):\n          x, y, w, h = bbox\n          x = x - 0.5 * w * (scale_ratio - 1.0)\n          y = y - 0.5 * h * (scale_ratio - 1.0)\n          w = w * scale_ratio\n          h = h * scale_ratio\n          return [x, y, w, h]\n\n        x, y, w, h = scale_box(bbox, self.box_scale_ratio)\n        # Convert half-integer to full-integer representation.\n        # The Python Imaging Library uses a Cartesian pixel coordinate system,\n        # with (0,0) in the upper left corner. Note that the coordinates refer\n        # to the implied pixel corners; the centre of a pixel addressed as\n        # (0, 0) actually lies at (0.5, 0.5). Since COCO uses the later\n        # convention and we use PIL to crop the image, we need to convert from\n        # half-integer to full-integer representation.\n        xmin = max(int(round(x - 0.5)), 0)\n        ymin = max(int(round(y - 0.5)), 0)\n        xmax = min(int(round(x + w - 0.5)) + 1, image_w)\n        ymax = min(int(round(y + h - 0.5)) + 1, image_h)\n        image_crop = image.crop((xmin, ymin, xmax, ymax))\n        crop_width, crop_height = image_crop.size\n        if crop_width <= 0 or crop_height <= 0:\n          raise ValueError(\'crops are not valid.\')\n      return image_crop, class_id\n\n    class_tf_record_writers = []\n    for class_id in range(self.num_all_classes):\n      output_path = os.path.join(\n          self.records_path, self.dataset_spec.file_pattern.format(class_id))\n      class_tf_record_writers.append(tf.python_io.TFRecordWriter(output_path))\n\n    for i, annotation in enumerate(self.coco_instance_annotations):\n      try:\n        image_crop, class_id = get_image_crop_and_class_id(annotation)\n      except IOError:\n        logging.warning(\'Image can not be opened and will be skipped.\')\n        continue\n      except ValueError:\n        logging.warning(\'Image can not be cropped and will be skipped.\')\n        continue\n\n      logging.info(\'writing image %d/%d\', i,\n                   len(self.coco_instance_annotations))\n\n      # TODO(manzagop): refactor this, e.g. use write_tfrecord_from_image_files.\n      image_crop_bytes = io.BytesIO()\n      image_crop.save(image_crop_bytes, format=\'JPEG\')\n      image_crop_bytes.seek(0)\n\n      write_example(image_crop_bytes.getvalue(), class_id,\n                    class_tf_record_writers[class_id])\n      self.images_per_class[class_id] += 1\n\n    for writer in class_tf_record_writers:\n      writer.close()\n\n\nclass ImageNetConverter(DatasetConverter):\n  """"""Prepares ImageNet for integration in the benchmark.\n\n  Different from most datasets that are getting converted here, for\n  ImageNet we define a HierarchicalDatasetSpecification which has different\n  attributes from a standard DatasetSpecification.\n\n  Only the ""training"" split of the original ImageNet dataset will be used.\n\n  Images that are shared with other datasets (Caltech for instance) are\n  skipped, so that examples from the test sets are not inadvertently\n  used during training.\n  """"""\n\n  def _create_data_spec(self, train_split_only=False):\n    """"""Initializes the HierarchicalDatasetSpecification instance for ImageNet.\n\n    See HierarchicalDatasetSpecification for details.\n    Args:\n      train_split_only: bool, if True the entire dataset is assigned to the\n        training split.\n    """"""\n    # Load lists of image names that are duplicates with images in other\n    # datasets. They will be skipped from ImageNet.\n    self.files_to_skip = set()\n    for other_dataset in (\'Caltech101\', \'Caltech256\', \'CUBirds\'):\n      duplicates_file = os.path.join(\n          AUX_DATA_PATH,\n          \'ImageNet_{}_duplicates.txt\'.format(other_dataset))\n\n      with tf.io.gfile.GFile(duplicates_file) as fd:\n        duplicates = fd.read()\n      lines = duplicates.splitlines()\n\n      for l in lines:\n        # Skip comment lines\n        l = l.strip()\n        if l.startswith(\'#\'):\n          continue\n        # Lines look like:\n        # \'synset/synset_imgnumber.JPEG  # original_file_name.jpg\\n\'.\n        # Extract only the \'synset_imgnumber.JPG\' part.\n        file_path = l.split(\'#\')[0].strip()\n        file_name = os.path.basename(file_path)\n        self.files_to_skip.add(file_name)\n    ilsvrc_2012_num_leaf_images_path = FLAGS.ilsvrc_2012_num_leaf_images_path\n    if not ilsvrc_2012_num_leaf_images_path:\n      ilsvrc_2012_num_leaf_images_path = os.path.join(self.records_path,\n                                                      \'num_leaf_images.json\')\n    specification = imagenet_specification.create_imagenet_specification(\n        learning_spec.Split,\n        self.files_to_skip,\n        ilsvrc_2012_num_leaf_images_path,\n        train_split_only=train_split_only)\n    split_subgraphs, images_per_class, _, _, _, _ = specification\n\n    # Maps each class id to the name of its class.\n    self.class_names = {}\n\n    self.dataset_spec = ds_spec.HierarchicalDatasetSpecification(\n        self.name, split_subgraphs, images_per_class, self.class_names,\n        self.records_path, \'{}.tfrecords\')\n\n  def _get_synset_ids(self, split):\n    """"""Returns a list of synset id\'s of the classes assigned to split.""""""\n    return sorted([\n        synset.wn_id for synset in imagenet_specification.get_leaves(\n            self.dataset_spec.split_subgraphs[split])\n    ])\n\n  def create_dataset_specification_and_records(self):\n    """"""Create Records for the ILSVRC 2012 classes.\n\n    The field that requires modification in this case is only self.class_names.\n    """"""\n    # Get a list of synset id\'s assigned to each split.\n    train_synset_ids = self._get_synset_ids(learning_spec.Split.TRAIN)\n    valid_synset_ids = self._get_synset_ids(learning_spec.Split.VALID)\n    test_synset_ids = self._get_synset_ids(learning_spec.Split.TEST)\n    all_synset_ids = train_synset_ids + valid_synset_ids + test_synset_ids\n\n    # It is expected that within self.data_root there is a directory\n    # for every ILSVRC 2012 synset, named by that synset\'s WordNet ID\n    # (e.g. n15075141) and containing all images of that synset.\n    set_of_directories = set(\n        entry for entry in tf.io.gfile.listdir(self.data_root)\n        if tf.io.gfile.isdir(os.path.join(self.data_root, entry)))\n    assert set_of_directories == set(all_synset_ids), (\n        \'self.data_root should contain a directory whose name is the WordNet \'\n        ""id of each synset that is a leaf of any split\'s subgraph."")\n\n    # By construction of all_synset_ids, we are guaranteed to get train synsets\n    # before validation synsets, and validation synsets before test synsets.\n    # Therefore the assigned class_labels will respect that partial order.\n    for class_label, synset_id in enumerate(all_synset_ids):\n      self.class_names[class_label] = synset_id\n      class_path = os.path.join(self.data_root, synset_id)\n      class_records_path = os.path.join(\n          self.records_path, self.dataset_spec.file_pattern.format(class_label))\n\n      # Create and write the tf.Record of the examples of this class.\n      # Image files for ImageNet do not necessarily come from a canonical\n      # source, so pass \'skip_on_error\' to be more resilient and avoid crashes\n      write_tfrecord_from_directory(\n          class_path,\n          class_label,\n          class_records_path,\n          files_to_skip=self.files_to_skip,\n          skip_on_error=True)\n\n\nclass FungiConverter(DatasetConverter):\n  """"""Prepares Fungi as required to integrate it in the benchmark.\n\n  From https://github.com/visipedia/fgvcx_fungi_comp  download:\n    -Training and validation images [13GB]\n    -Training and validation annotations [2.9MB]\n  and untar the files in the directory passed to initializer as data_root.\n  """"""\n  NUM_TRAIN_CLASSES = 994\n  NUM_VALID_CLASSES = 200\n  NUM_TEST_CLASSES = 200\n\n  def create_splits(self):\n    """"""Create splits for Fungi and store them in the default path.\n\n    If no split file is provided, and the default location for Fungi Identity\n    splits does not contain a split file, splits are randomly created in this\n    function using 70%, 15%, and 15% of the data for training, validation and\n    testing, respectively, and then stored in that default location.\n\n    Returns:\n      The splits for this dataset, represented as a dictionary mapping each of\n      \'train\', \'valid\', and \'test\' to a list of class names.\n    """"""\n    # We ignore the original train and validation splits (the test set cannot be\n    # used since it is not labeled).\n    with tf.io.gfile.GFile(os.path.join(self.data_root, \'train.json\')) as f:\n      original_train = json.load(f)\n    with tf.io.gfile.GFile(os.path.join(self.data_root, \'val.json\')) as f:\n      original_val = json.load(f)\n\n    # The categories (classes) for train and validation should be the same.\n    assert original_train[\'categories\'] == original_val[\'categories\']\n    # Sort by category ID for reproducibility.\n    categories = sorted(\n        original_train[\'categories\'], key=operator.itemgetter(\'id\'))\n\n    # Assert contiguous range [0:category_number]\n    assert ([category[\'id\'] for category in categories\n            ] == list(range(len(categories))))\n\n    # Some categories share the same name (see\n    # https://github.com/visipedia/fgvcx_fungi_comp/issues/1)\n    # so we include the category id in the label.\n    labels = [\n        \'{:04d}.{}\'.format(category[\'id\'], category[\'name\'])\n        for category in categories\n    ]\n\n    train_inds, valid_inds, test_inds = gen_rand_split_inds(\n        self.NUM_TRAIN_CLASSES, self.NUM_VALID_CLASSES, self.NUM_TEST_CLASSES)\n    splits = {\n        \'train\': [labels[i] for i in train_inds],\n        \'valid\': [labels[i] for i in valid_inds],\n        \'test\': [labels[i] for i in test_inds]\n    }\n    return splits\n\n  def create_dataset_specification_and_records(self):\n    """"""Implements DatasetConverter.create_dataset_specification_and_records.""""""\n\n    splits = self.get_splits()\n    # Get the names of the classes assigned to each split\n    train_classes = splits[\'train\']\n    valid_classes = splits[\'valid\']\n    test_classes = splits[\'test\']\n\n    self.classes_per_split[learning_spec.Split.TRAIN] = len(train_classes)\n    self.classes_per_split[learning_spec.Split.VALID] = len(valid_classes)\n    self.classes_per_split[learning_spec.Split.TEST] = len(test_classes)\n\n    with tf.io.gfile.GFile(os.path.join(self.data_root, \'train.json\')) as f:\n      original_train = json.load(f)\n    with tf.io.gfile.GFile(os.path.join(self.data_root, \'val.json\')) as f:\n      original_val = json.load(f)\n\n    image_list = original_train[\'images\'] + original_val[\'images\']\n    image_id_dict = {}\n    for image in image_list:\n      # assert this image_id was not previously added\n      assert image[\'id\'] not in image_id_dict\n      image_id_dict[image[\'id\']] = image\n\n    # Add a class annotation to every image in image_id_dict.\n    annotations = original_train[\'annotations\'] + original_val[\'annotations\']\n    for annotation in annotations:\n      # assert this images_id was not previously annotated\n      assert \'class\' not in image_id_dict[annotation[\'image_id\']]\n      image_id_dict[annotation[\'image_id\']][\'class\'] = annotation[\'category_id\']\n\n    # dict where the class is the key.\n    class_filepaths = collections.defaultdict(list)\n    for image in image_list:\n      class_filepaths[image[\'class\']].append(\n          os.path.join(self.data_root, image[\'file_name\']))\n\n    all_classes = list(\n        itertools.chain(train_classes, valid_classes, test_classes))\n    for class_id, class_label in enumerate(all_classes):\n      logging.info(\'Creating record for class ID %d (%s)...\', class_id,\n                   class_label)\n      # Extract the ""category_id"" information from the class label\n      category_id = int(class_label[:4])\n      # Check that the key is actually in `class_filepaths`, so that an empty\n      # list is not accidentally used.\n      if category_id not in class_filepaths:\n        raise ValueError(\'class_filepaths does not contain paths to any \'\n                         \'image for category %d. Existing categories are: %s.\' %\n                         (category_id, class_filepaths.keys()))\n      class_paths = class_filepaths[category_id]\n      class_records_path = os.path.join(\n          self.records_path, self.dataset_spec.file_pattern.format(class_id))\n      self.class_names[class_id] = class_label\n      self.images_per_class[class_id] = len(class_paths)\n\n      # Create and write the tf.Record of the examples of this class\n      write_tfrecord_from_image_files(class_paths, class_id, class_records_path)\n\n\nclass MiniImageNetConverter(DatasetConverter):\n  """"""Prepares MiniImageNet as required to integrate it in the benchmark.\n\n  From https://github.com/renmengye/few-shot-ssl-public download and untar the\n  miniImageNet file in the directory passed to init as data_root.\n  """"""\n  NUM_TRAIN_CLASSES = 64\n  NUM_VALID_CLASSES = 16\n  NUM_TEST_CLASSES = 20\n\n  def create_splits(self):\n    """"""Create splits for MiniImageNet and store them in the default path.\n\n    If no split file is provided, and the default location for MiniImageNet\n    splits does not contain a split file, splits are created in this function\n    according to the Ravi & Larochelle specification and then stored in that\n    default location.\n\n    Returns:\n      The splits for this dataset, represented as a dictionary mapping each of\n      \'train\', \'valid\', and \'test\' to a list of class names.\n    """"""\n    start_stop = np.cumsum([\n        0, self.NUM_TRAIN_CLASSES, self.NUM_VALID_CLASSES, self.NUM_TEST_CLASSES\n    ])\n    train_inds = list(range(start_stop[0], start_stop[1]))\n    valid_inds = list(range(start_stop[1], start_stop[2]))\n    test_inds = list(range(start_stop[2], start_stop[3]))\n    splits = {\'train\': train_inds, \'valid\': valid_inds, \'test\': test_inds}\n    return splits\n\n  def create_dataset_specification_and_records(self):\n    """"""Implements DatasetConverter.create_dataset_specification_and_records.""""""\n\n    splits = self.get_splits()\n    # Get the names of the classes assigned to each split\n    train_classes = splits[\'train\']\n    valid_classes = splits[\'valid\']\n    test_classes = splits[\'test\']\n\n    self.classes_per_split[learning_spec.Split.TRAIN] = len(train_classes)\n    self.classes_per_split[learning_spec.Split.VALID] = len(valid_classes)\n    self.classes_per_split[learning_spec.Split.TEST] = len(test_classes)\n\n    for classes, split in zip([train_classes, valid_classes, test_classes],\n                              [\'train\', \'val\', \'test\']):\n      path = os.path.join(self.data_root,\n                          \'mini-imagenet-cache-{}.pkl\'.format(split))\n      with tf.io.gfile.GFile(path, \'rb\') as f:\n        data = pkl.load(f)\n      # We sort class names to make the dataset creation deterministic\n      names = sorted(data[\'class_dict\'].keys())\n      for class_id, class_name in zip(classes, names):\n        logging.info(\'Creating record class %d\', class_id)\n        class_records_path = os.path.join(self.records_path,\n                                          self.file_pattern.format(class_id))\n        self.class_names[class_id] = class_name\n        indices = data[\'class_dict\'][class_name]\n        self.images_per_class[class_id] = len(indices)\n\n        writer = tf.python_io.TFRecordWriter(class_records_path)\n        for image in data[\'image_data\'][indices]:\n          img = Image.fromarray(image)\n          buf = io.BytesIO()\n          img.save(buf, format=\'JPEG\')\n          buf.seek(0)\n          write_example(buf.getvalue(), class_id, writer)\n        writer.close()\n'"
