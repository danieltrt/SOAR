file_path,api_count,code
setup.py,0,"b'""""""Setup for morph-net package.""""""\n\nimport setuptools\n\nwith open(""README.md"", ""r"") as fh:\n  long_description = fh.read()\n\nsetuptools.setup(\n    name=""morph_net"",\n    version=""0.2.1"",\n    author=""Google LLC"",\n    author_email=""morphnet@google.com"",\n    description=""A library for learning deep network structure during training"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/google-research/morph-net"",\n    packages=setuptools.find_packages(),\n    classifiers=[\n        ""Programming Language :: Python :: 2"",\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Operating System :: OS Independent"",\n    ],\n)\n'"
morph_net/__init__.py,0,b''
examples/keras/main.py,3,"b'""""""\nMorphNet Model Zoo\n\nLei Mao\nNVIDIA\nhttps://github.com/leimao\n\nMain script to start MorphNet training for selected models.\n""""""\n\nimport argparse\nimport tensorflow as tf\n\nfrom model import MorphNetModel\nfrom utils import set_reproducible_environment, select_keras_base_model, train_epoch, validate_epoch\n\n\ndef main():\n\n    parser = argparse.ArgumentParser(\n        description=""Run MorphNet Algorithm on Image Classification Model Zoo."")\n\n    num_epochs_default = 1000\n    num_classes_default = 10\n    batch_size_default = 1024\n    base_model_name_default = ""ResNet50""\n    learning_rate_default = 0.0001\n    morphnet_regularizer_algorithm_default = ""GroupLasso""\n    morphnet_target_cost_default = ""FLOPs""\n    morphnet_hardware_default = ""V100""\n    morphnet_regularizer_threshold_default = 1e-2\n    morphnet_regularization_multiplier_default = 1000.0\n    log_dir_default = ""./morphnet_log""\n    main_train_device_default = ""/cpu:0""\n    main_eval_device_default = ""/gpu:0""\n    num_cuda_device_default = 4\n    random_seed_default = 0\n    base_model_choices = [\n        ""ResNet50"", ""ResNet101"", ""ResNet152"", ""ResNet50V2"", ""ResNet101V2"",\n        ""ResNet101V2"", ""ResNet152V2"", ""VGG16"", ""VGG19"", ""Xception"",\n        ""InceptionV3"", ""InceptionResNetV2"", ""MobileNet"", ""MobileNetV2"",\n        ""DenseNet121"", ""DenseNet169"", ""DenseNet201"", ""NASNetLarge"",\n        ""NASNetMobile""\n    ]\n    morphnet_regularizer_algorithm_choices = [""GroupLasso"", ""Gamma""]\n    morphnet_target_cost_choices = [""FLOPs"", ""Latency"", ""ModelSize""]\n    morphnet_hardware_choices = [""V100"", ""P100"", ""Others""]\n\n    parser.add_argument(""--num-epochs"",\n                        type=int,\n                        help=""The number of epochs for training."",\n                        default=num_epochs_default)\n    parser.add_argument(""--num-classes"",\n                        type=int,\n                        help=""The number of classes for image classification."",\n                        default=num_classes_default)\n    parser.add_argument(""--batch-size"",\n                        type=int,\n                        help=""Batch size."",\n                        default=batch_size_default)\n    parser.add_argument(""--learning-rate"",\n                        type=float,\n                        help=""Learning rate."",\n                        default=learning_rate_default)\n    parser.add_argument(""--base-model-name"",\n                        type=str,\n                        choices=base_model_choices,\n                        help=""Select base model for image classification."",\n                        default=base_model_name_default)\n    parser.add_argument(""--morphnet-regularizer-algorithm"",\n                        type=str,\n                        choices=morphnet_regularizer_algorithm_choices,\n                        help=""Select MorphNet regularization algorithm."",\n                        default=morphnet_regularizer_algorithm_default)\n    parser.add_argument(""--morphnet-target-cost"",\n                        type=str,\n                        choices=morphnet_target_cost_choices,\n                        help=""Select MorphNet target cost."",\n                        default=morphnet_target_cost_default)\n    parser.add_argument(""--morphnet-hardware"",\n                        type=str,\n                        choices=morphnet_hardware_choices,\n                        help=""Select MorphNet hardware."",\n                        default=morphnet_hardware_default)\n    parser.add_argument(\n        ""--morphnet-regularizer-threshold"",\n        type=float,\n        help=""Set the threshold [0, 1] for killing neuron layers."",\n        default=morphnet_regularizer_threshold_default)\n    parser.add_argument(\n        ""--morphnet-regularization-multiplier"",\n        type=float,\n        help=\n        ""Set MorphNet regularization multiplier for regularization strength. The regularization strength for training equals the regularization multiplier divided by the initial cost of the model. Set this value to zero turns of MorphNet regularization."",\n        default=morphnet_regularization_multiplier_default)\n    parser.add_argument(\n        ""--log-dir"",\n        type=str,\n        help=""Log directory for TensorBoard and optimized model architectures."",\n        default=log_dir_default)\n    parser.add_argument(""--num-cuda-device"",\n                        type=int,\n                        help=""Number of CUDA device to use."",\n                        default=num_cuda_device_default)\n    parser.add_argument(""--random-seed"",\n                        type=int,\n                        help=""Random seed."",\n                        default=random_seed_default)\n    parser.add_argument(\n        ""--main-train-device"",\n        type=str,\n        help=""The device where the model parameters were located."",\n        default=main_train_device_default)\n    parser.add_argument(""--main-eval-device"",\n                        type=str,\n                        help=""The device used for model evaluation"",\n                        default=main_eval_device_default)\n\n    argv = parser.parse_args()\n\n    num_epochs = argv.num_epochs\n    num_classes = argv.num_classes\n    batch_size = argv.batch_size\n    base_model_name = argv.base_model_name\n    learning_rate = argv.learning_rate\n    morphnet_regularizer_algorithm = argv.morphnet_regularizer_algorithm\n    morphnet_target_cost = argv.morphnet_target_cost\n    morphnet_hardware = argv.morphnet_hardware\n    morphnet_regularizer_threshold = argv.morphnet_regularizer_threshold\n    morphnet_regularization_multiplier = argv.morphnet_regularization_multiplier\n    log_dir = argv.log_dir\n    num_cuda_device = argv.num_cuda_device\n    random_seed = argv.random_seed\n    main_train_device = argv.main_train_device\n    main_eval_device = argv.main_eval_device\n\n    set_reproducible_environment(random_seed=random_seed)\n\n    (x_train, y_train), (x_valid,\n                         y_valid) = tf.keras.datasets.cifar10.load_data()\n    # Convert class vectors to binary class matrices.\n    y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes)\n    y_valid_onehot = tf.keras.utils.to_categorical(y_valid, num_classes)\n    image_shape = x_train[1:]\n    # Normalize image inputs\n    x_train = x_train.astype(""float32"") / 255.0\n    x_valid = x_valid.astype(""float32"") / 255.0\n\n    base_model = select_keras_base_model(base_model_name=base_model_name)\n    morphnet_regularization_strength_dummy = 1e-9\n    model = MorphNetModel(\n        base_model=base_model,\n        num_classes=num_classes,\n        learning_rate=learning_rate,\n        batch_size=batch_size,\n        num_gpus=num_cuda_device,\n        main_train_device=main_train_device,\n        main_eval_device=main_eval_device,\n        morphnet_regularizer_algorithm=morphnet_regularizer_algorithm,\n        morphnet_target_cost=morphnet_target_cost,\n        morphnet_hardware=morphnet_hardware,\n        morphnet_regularizer_threshold=morphnet_regularizer_threshold,\n        morphnet_regularization_strength=morphnet_regularization_strength_dummy,\n        log_dir=log_dir)\n\n    # Export the unmodified model configures.\n    initial_cost = model.get_model_cost(inputs=x_train[:batch_size])\n    print(""*"" * 100)\n    print(""Initial Model Cost: {:.1f}"".format(initial_cost))\n    morphnet_regularization_strength = 1.0 / initial_cost * morphnet_regularization_multiplier\n    print(""Use Regularization Strength: {}"".format(\n        morphnet_regularization_strength))\n    model.set_morphnet_regularization_strength(\n        morphnet_regularization_strength=morphnet_regularization_strength)\n    print(""*"" * 100)\n    # Export the unmodified model configures.\n    model.export_model_config_with_inputs(inputs=x_train[:batch_size])\n\n    for epoch in range(num_epochs):\n        validate_epoch(epoch=epoch,\n                       model=model,\n                       x_valid=x_valid,\n                       y_valid_onehot=y_valid_onehot,\n                       batch_size=batch_size)\n        train_epoch(epoch=epoch,\n                    model=model,\n                    x_train=x_train,\n                    y_train_onehot=y_train_onehot,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    print_batch_info=False)\n        # Export the model configure routinely.\n        model.export_model_config_with_inputs(inputs=x_train[:batch_size])\n\n    validate_epoch(epoch=num_epochs,\n                   model=model,\n                   x_valid=x_valid,\n                   y_valid_onehot=y_valid_onehot,\n                   batch_size=batch_size)\n\n    model.close()\n\n    return 0\n\n\nif __name__ == ""__main__"":\n\n    main()\n'"
examples/keras/model.py,44,"b'""""""\nMorphNet Model Zoo\n\nLei Mao\nNVIDIA\nhttps://github.com/leimao\n\nImplementation of MorphNetModel class.\n""""""\n\nimport os\nimport tensorflow as tf\nfrom datetime import datetime\nfrom morph_net.network_regularizers import flop_regularizer, latency_regularizer, model_size_regularizer\nfrom morph_net.tools import structure_exporter\n\n\nclass MorphNetModel(object):\n\n    def __init__(self,\n                 base_model,\n                 num_classes,\n                 learning_rate=1e-3,\n                 batch_size=256,\n                 num_gpus=4,\n                 main_train_device=""/cpu:0"",\n                 main_eval_device=""/gpu:0"",\n                 morphnet_regularizer_algorithm=""GroupLasso"",\n                 morphnet_target_cost=""FLOPs"",\n                 morphnet_hardware=""V100"",\n                 morphnet_regularizer_threshold=1e-2,\n                 morphnet_regularization_strength=1e-9,\n                 log_dir=""./morphnet_log""):\n        """"""\n        Initialize MorphNetModel instance.\n        Args:\n            base_model: Keras model class.\n            num_classes: Number of classes for classification. Integer.\n            learning_rate: Learning rate. Float.\n            batch_size: Batch size for multi-GPU training. The batch size would be further divided across multiple GPUs. Integer.\n            num_gpus: Number of GPU devices used for model training. Integer.\n            main_train_device: The GPU device used for computing the average of gradients collected from all the GPU devices. For multi-GPU training, on my DGX-station (E5-2698, 4x V100), it seems setting it to ""/cpu:0"" is the fastest. String.\n            main_eval_device: The GPU device used for evaluation and inference. String.\n            morphnet_regularizer_algorithm: MorphNet regularization algorithm name. Currently we support ""GroupLasso"" and ""Gamma"". String.\n            morphnet_target_cost: The pptimization target cost for MorphNet regularization algorithms. Currently we support ""FLOPs"", ""Latency"", and ""ModelSize"". String.\n            morphnet_hardware: The hardware for using ""Latency"" as the optimization target cost. Currently we support ""V100"" and ""P100"". String.\n            morphnet_regularizer_threshold: The threshold determines which output channels can be eliminated. Float.\n            morphnet_regularization_strength: The regularization strength for MorphNet as MorphNet is a regularization technique. Float.\n            log_dir: The log directory for TensorBoard, intermediate files, and model architecture files.\n        """"""\n        self.base_model = base_model\n        self.num_classes = num_classes\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.num_gpus = num_gpus\n        self.main_train_device = main_train_device\n        self.main_eval_device = main_eval_device\n        self.morphnet_regularizer_algorithm = morphnet_regularizer_algorithm\n        self.morphnet_target_cost = morphnet_target_cost\n        self.morphnet_hardware = morphnet_hardware\n\n        self.morphnet_regularizer_threshold = morphnet_regularizer_threshold\n        # Setting regularization strength to zero removes MorphNet architecture search.\n        self.morphnet_regularization_strength = morphnet_regularization_strength\n\n        self.log_dir = log_dir\n        self.global_step = 0\n\n        self.initialize_model()\n        self.initialize_training_instance()\n        self.initialize_evaluation_instance()\n\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n\n        self.log_dir = os.path.join(self.log_dir,\n                                    datetime.now().strftime(""%Y%m%d-%H%M%S""))\n        self.writer = tf.summary.FileWriter(self.log_dir,\n                                            tf.get_default_graph())\n        self.morphnet_summary = self.summary()\n\n    def initialize_model(self, input_tensor=None):\n        """"""\n        Initialize the model.\n        Args:\n            input_tensor: Input tensor to the model. tf.placeholder variable or tf.Keras.Input instance. If not provided, use the default inputs from the the base model.\n        """"""\n        with tf.device(self.main_train_device):\n\n            base_model = self.base_model(weights=None,\n                                         include_top=False,\n                                         input_tensor=input_tensor)\n            x = base_model.output\n            # Add a global spatial average pooling layer since MorphNet does not support Flatten/Reshape OPs.\n            x = tf.keras.layers.GlobalAveragePooling2D()(x)\n            x = tf.keras.layers.Dense(1024, activation=""relu"")(x)\n            logits = tf.keras.layers.Dense(self.num_classes)(x)\n\n            self.model = tf.keras.Model(inputs=base_model.input, outputs=logits)\n\n            self.inputs = self.model.input\n            self.labels = tf.placeholder(tf.float32, [None, self.num_classes])\n\n            self.morphnet_regularization_strength_placeholder = tf.placeholder(\n                tf.float32, shape=[])\n\n    def initialize_morphnet(self, input_boundary, output_boundary,\n                            morphnet_regularization_strength):\n        """"""\n        Initialize MorphNet components.\n        Args:\n            input_boundary: The input boundary for MorphNet regularization. A list of TensorFlow operators.\n            output_boundary: The output boundary for MorphNet regularization. A list of TensorFlow operators.\n            morphnet_regularization_strength: The regularization strength for MorphNet as MorphNet is a regularization technique. Float.\n        Returns:\n            network_regularizer: MorphNet regularizer handler.\n            regularizer_loss: MorphNet regularization loss. Tensor.\n            exporter: MorphNet model architecture exporter.\n            cost: Target cost. Tensor.\n        """"""\n        if self.morphnet_regularizer_algorithm == ""GroupLasso"":\n            if self.morphnet_target_cost == ""FLOPs"":\n                regularizer_fn = flop_regularizer.GroupLassoFlopsRegularizer\n                network_regularizer = regularizer_fn(\n                    output_boundary=output_boundary,\n                    input_boundary=input_boundary,\n                    threshold=self.morphnet_regularizer_threshold)\n            elif self.morphnet_target_cost == ""Latency"":\n                if self.morphnet_hardware not in (""V100"", ""P100""):\n                    raise Exception(\n                        ""Unsupported MorphNet Hardware For Latency Regularizer!""\n                    )\n                regularizer_fn = latency_regularizer.GroupLassoLatencyRegularizer\n                network_regularizer = regularizer_fn(\n                    output_boundary=output_boundary,\n                    input_boundary=input_boundary,\n                    threshold=self.morphnet_regularizer_threshold,\n                    hardware=self.morphnet_hardware)\n            elif self.morphnet_target_cost == ""ModelSize"":\n                regularizer_fn = model_size_regularizer.GroupLassoModelSizeRegularizer\n                network_regularizer = regularizer_fn(\n                    output_boundary=output_boundary,\n                    input_boundary=input_boundary,\n                    threshold=self.morphnet_regularizer_threshold)\n            else:\n                raise Exception(""Unsupported MorphNet Regularizer Target Cost!"")\n        elif self.morphnet_regularizer_algorithm == ""Gamma"":\n            if self.morphnet_target_cost == ""FLOPs"":\n                regularizer_fn = flop_regularizer.GammaFlopsRegularizer\n                network_regularizer = regularizer_fn(\n                    output_boundary=output_boundary,\n                    input_boundary=input_boundary,\n                    gamma_threshold=self.morphnet_regularizer_threshold)\n            elif self.morphnet_target_cost == ""Latency"":\n                if self.morphnet_hardware not in (""V100"", ""P100""):\n                    raise Exception(\n                        ""Unsupported MorphNet Hardware For Latency Regularizer!""\n                    )\n                regularizer_fn = latency_regularizer.GammaLatencyRegularizer\n                network_regularizer = regularizer_fn(\n                    output_boundary=output_boundary,\n                    input_boundary=input_boundary,\n                    gamma_threshold=self.morphnet_regularizer_threshold,\n                    hardware=self.morphnet_hardware)\n            elif self.morphnet_target_cost == ""ModelSize"":\n                regularizer_fn = model_size_regularizer.GammaModelSizeRegularizer\n                network_regularizer = regularizer_fn(\n                    output_boundary=output_boundary,\n                    input_boundary=input_boundary,\n                    gamma_threshold=self.morphnet_regularizer_threshold)\n            else:\n                raise Exception(""Unsupported MorphNet Regularizer Target Cost!"")\n        else:\n            raise Exception(""Unsupported MorphNet Regularizer Algorithm!"")\n\n        regularizer_loss = network_regularizer.get_regularization_term(\n        ) * morphnet_regularization_strength\n        exporter = structure_exporter.StructureExporter(\n            network_regularizer.op_regularizer_manager)\n        cost = network_regularizer.get_cost()\n\n        return network_regularizer, regularizer_loss, exporter, cost\n\n    def initialize_loss(self, logits, labels):\n        """"""\n        Initialize the loss of the original model. The MorphNet regularization loss is not included.\n        Args:\n            logits: Logits output tensor from the model. Tensor.\n            labels: One-hot encoded ground-truth label tensor. Tensor.\n        Returns:\n            model_loss: The loss of original model. Tensor.\n        """"""\n        model_loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels,\n                                                       logits=logits))\n\n        return model_loss\n\n    def average_gradients(self, tower_grads):\n        """"""\n        Calculate the average gradient for each shared variable across all towers.\n        Note that this function provides a synchronization point across all towers.\n        Args:\n            tower_grads: List of lists of (gradient, variable) tuples. The outer list\n            is over individual gradients. The inner list is over the gradient\n            calculation for each tower.\n        Returns:\n            List of pairs of (gradient, variable) where the gradient has been averaged\n            across all towers.\n        """"""\n        average_grads = []\n        for grad_and_vars in zip(*tower_grads):\n            # Note that each grad_and_vars looks like the following:\n            # ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n            grads = []\n            for g, _ in grad_and_vars:\n                # Add 0 dimension to the gradients to represent the tower.\n                expanded_g = tf.expand_dims(g, 0)\n\n                # Append on a \'tower\' dimension which we will average over below.\n                grads.append(expanded_g)\n\n            # Average over the \'tower\' dimension.\n            grad = tf.concat(axis=0, values=grads)\n            grad = tf.reduce_mean(grad, 0)\n\n            # Keep in mind that the Variables are redundant because they are shared\n            # across towers. So .. we will just return the first tower\'s pointer to\n            # the Variable.\n            v = grad_and_vars[0][1]\n            grad_and_var = (grad, v)\n            average_grads.append(grad_and_var)\n\n        return average_grads\n\n    def initialize_training_instance(self):\n        """"""\n        Initialize the single-node multi-GPU training instance.\n        """"""\n        # Compute the average of the gradients main_train_device\n        tower_grads = []\n\n        # Distribute the model onto available GPUs\n        for i in range(self.num_gpus):\n            with tf.device(""/gpu:{}"".format(i)):\n\n                optimizer = tf.train.AdamOptimizer(\n                    learning_rate=self.learning_rate)\n                batch_size_instance = self.batch_size // self.num_gpus\n\n                # Split data between GPUs\n                inputs_instance = self.inputs[i * batch_size_instance:(i + 1) *\n                                              batch_size_instance]\n                labels_instance = self.labels[i * batch_size_instance:(i + 1) *\n                                              batch_size_instance]\n\n                logits = self.model(inputs_instance)\n                trainable_variables = self.model.trainable_variables\n                model_loss = self.initialize_loss(logits=logits,\n                                                  labels=labels_instance)\n                network_regularizer, regularizer_loss, exporter, cost = self.initialize_morphnet(\n                    input_boundary=[inputs_instance.op],\n                    output_boundary=[logits.op],\n                    morphnet_regularization_strength=self.\n                    morphnet_regularization_strength_placeholder)\n                total_loss = model_loss + regularizer_loss\n\n                grads = optimizer.compute_gradients(\n                    total_loss, var_list=trainable_variables)\n                tower_grads.append(grads)\n\n                # Specify a GPU to compute for testing purpose\n                # Usually we would use the first GPU\n                if i == 0:\n                    # Evaluate model (with test logits, for dropout to be disabled)\n                    self.logits_train_instance = logits\n                    self.model_loss_train_instance = model_loss\n                    self.probs_train_instance = tf.nn.softmax(logits)\n                    self.correct_pred_train_instance = tf.equal(\n                        tf.argmax(logits, 1), tf.argmax(labels_instance, 1))\n                    self.accuracy_train_instance = tf.reduce_mean(\n                        tf.cast(self.correct_pred_train_instance, tf.float32))\n\n                    self.network_regularizer_train_instance = network_regularizer\n                    self.regularizer_loss_train_instance = regularizer_loss\n                    self.total_loss_train_instance = total_loss\n                    self.exporter_train_instance = exporter\n                    self.cost_train_instance = cost\n\n        # Compute the average of the gradients main_train_device\n        with tf.device(self.main_train_device):\n            grads = self.average_gradients(tower_grads)\n            self.train_op = optimizer.apply_gradients(grads, global_step=None)\n\n    def initialize_evaluation_instance(self):\n        """"""\n        Initialize the single-GPU evaluation instance.\n        """"""\n        with tf.device(self.main_eval_device):\n\n            self.inputs_eval = self.model.input\n            self.labels_eval = tf.placeholder(tf.float32,\n                                              [None, self.num_classes])\n            self.logits_eval = self.model(self.inputs_eval)\n            self.probs_eval = tf.nn.softmax(self.logits_eval)\n            self.correct_pred_eval = tf.equal(tf.argmax(self.logits_eval, 1),\n                                              tf.argmax(self.labels_eval, 1))\n            self.accuracy_eval = tf.reduce_mean(\n                tf.cast(self.correct_pred_eval, tf.float32))\n            # Created for checking the MorphNet statistics only\n            _, _, self.exporter_eval, self.cost_eval = self.initialize_morphnet(\n                input_boundary=[self.inputs_eval.op],\n                output_boundary=[self.logits_eval.op],\n                morphnet_regularization_strength=self.\n                morphnet_regularization_strength_placeholder)\n\n    def train(self, inputs, labels):\n        """"""\n        Train one batch of the training data using tf.Session().\n        Args:\n            inputs: RGB Image. Numpy array.\n            labels: One-hot encoded labels. Numpy array.\n        Returns:\n            loss: Training loss for this batch.\n            accuracy: Training accuracy for this batch.\n            cost: The target cost of the model with the current architecture after MorphNet optimization.\n        """"""\n        # Set the phase to training.\n        tf.keras.backend.set_learning_phase(1)\n        self.global_step += 1\n\n        _, structure_exporter_tensors, loss, accuracy, cost, morphnet_summary = self.sess.run(\n            [\n                self.train_op, self.exporter_train_instance.tensors,\n                self.total_loss_train_instance, self.accuracy_train_instance,\n                self.cost_train_instance, self.morphnet_summary\n            ],\n            feed_dict={\n                self.inputs:\n                    inputs,\n                self.labels:\n                    labels,\n                self.morphnet_regularization_strength_placeholder:\n                    self.morphnet_regularization_strength\n            })\n\n        self.writer.add_summary(morphnet_summary, self.global_step)\n        self.exporter_train_instance.populate_tensor_values(\n            structure_exporter_tensors)\n\n        return loss, accuracy, cost\n\n    def validate(self, inputs, labels):\n        """"""\n        Validate one batch of the validation data using tf.Session().\n        Args:\n            inputs: RGB Image. Numpy array.\n            labels: One-hot encoded labels. Numpy array.\n        Returns:\n            accuracy: Validation accuracy for this batch. Float.\n            cost: The target cost of the model with the current architecture after MorphNet optimization. Float.\n        """"""\n        # Set the phase to test.\n        tf.keras.backend.set_learning_phase(0)\n        accuracy, cost = self.sess.run([self.accuracy_eval, self.cost_eval],\n                                       feed_dict={\n                                           self.inputs_eval: inputs,\n                                           self.labels_eval: labels\n                                       })\n\n        return accuracy, cost\n\n    def test(self, inputs):\n        """"""\n        Test one batch of the test data using tf.Session().\n        Args:\n            inputs: RGB Image. Numpy array.\n        Returns:\n            probs: The predicted probability vectors for the inputs.\n        """"""\n        # Set the phase to test.\n        tf.keras.backend.set_learning_phase(0)\n        probs = self.sess.run(self.probs_eval,\n                              feed_dict={self.inputs_eval: inputs})\n\n        return probs\n\n    def export_model_config_with_inputs(self, inputs):\n        """"""\n        Export the model architecture after MorphNet optimization to JSON file. Require inputs to compute the model architecture.\n        Args:\n            inputs: RGB Image. Numpy array.\n        """"""\n        structure_exporter_tensors = self.sess.run(\n            self.exporter_eval.tensors, feed_dict={self.inputs: inputs})\n        self.exporter_eval.populate_tensor_values(structure_exporter_tensors)\n        self.exporter_eval.create_file_and_save_alive_counts(\n            self.log_dir, self.global_step)\n\n    def get_model_cost(self, inputs):\n        """"""\n        Get the target cost. Require inputs to compute the target cost.\n        Args:\n            inputs: RGB Image. Numpy array.\n        Returns:\n            cost: The target cost of the model with the current architecture after MorphNet optimization. Float.\n        """"""\n        cost = self.sess.run(self.cost_eval, feed_dict={self.inputs: inputs})\n\n        return cost\n\n    def set_morphnet_regularization_strength(self,\n                                             morphnet_regularization_strength):\n        """"""\n        Set the MorphNet regularization strength. We allow the regularization strength to be changed dynamically during training.\n        Args:\n            morphnet_regularization_strength: The regularization strength for MorphNet as MorphNet is a regularization technique. Float.\n        """"""\n        self.morphnet_regularization_strength = morphnet_regularization_strength\n\n    def summary(self):\n        """"""\n        Create TensorFlow summaries for TensorBoard.\n        Returns: \n            morphnet_summary: A TensorFlow summary handler for regularization loss and target cost.\n        """"""\n        regularization_loss_summary = tf.summary.scalar(\n            ""RegularizationLoss"", self.regularizer_loss_train_instance)\n        network_cost_summary = tf.summary.scalar(\n            self.network_regularizer_train_instance.cost_name,\n            self.cost_train_instance)\n        morphnet_summary = tf.summary.merge(\n            [regularization_loss_summary, network_cost_summary])\n\n        return morphnet_summary\n\n    def close(self):\n        """"""\n        Close the MorphNetModel model instance.\n        """"""\n        tf.keras.backend.clear_session()\n        tf.reset_default_graph()\n'"
examples/keras/test_configs.py,0,"b'""""""\nMorphNet Model Zoo\n\nLei Mao\nNVIDIA\nhttps://github.com/leimao\n\nTest script for MorphNetModel training configurations and model zoo.\n""""""\n\nimport subprocess\nimport os\nimport itertools\n\n\ndef test_combinations(log_dir=""./morphnet_log"",\n                      csv_filename=""morphnet_test_results.csv"",\n                      num_cuda_device=4):\n    """"""\n    Test the MorphNet model zoo configuration combinations. Most of the networks and MorphNet regularization configurations have been supported.\n    Args: \n        log_dir: Directory to log the test results. String.\n        csv_filename: CSV filename to save the test results. String.\n        num_cuda_device: Number of the CUDA device used for testing. Integer.\n    Returns:\n        successes: A list of the tuples of the configurations which MorphNet model zoo ran successfully.\n        failures: A list of the tuples of the configurations which MorphNet model zoo failed to run.\n    """"""\n\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n\n    fhand = open(os.path.join(log_dir, csv_filename), ""w"")\n    fhand.write(\n        ""base_model,morphnet_regularizer_algorithm,morphnet_target_cost,morphnet_hardware,success\\n""\n    )\n    fhand.close()\n\n    successes = []\n    failures = []\n\n    num_epochs_default = 1\n    num_classes_default = 10\n    batch_size_default = 1024\n    learning_rate_default = 0.0001\n    morphnet_regularizer_threshold_default = 1e-2\n    morphnet_regularization_multiplier_default = 1000.0\n    log_dir_default = ""./morphnet_log""\n    num_cuda_device_default = 4\n    base_model_choices = [\n        ""ResNet50"", ""ResNet101"", ""ResNet152"", ""ResNet50V2"", ""ResNet101V2"",\n        ""ResNet101V2"", ""ResNet152V2"", ""VGG16"", ""VGG19"", ""Xception"",\n        ""InceptionV3"", ""InceptionResNetV2"", ""MobileNet"", ""MobileNetV2"",\n        ""DenseNet121"", ""DenseNet169"", ""DenseNet201"", ""NASNetLarge"",\n        ""NASNetMobile""\n    ]\n    morphnet_regularizer_algorithm_choices = [""GroupLasso"", ""Gamma""]\n    morphnet_target_cost_choices = [""FLOPs"", ""Latency"", ""ModelSize""]\n    morphnet_hardware_choices = [""V100"", ""P100""]\n\n    with open(os.path.join(log_dir, csv_filename), ""a"", buffering=1) as fhand:\n\n        for base_model, morphnet_regularizer_algorithm, morphnet_target_cost, morphnet_hardware in itertools.product(\n                base_model_choices, morphnet_regularizer_algorithm_choices,\n                morphnet_target_cost_choices, morphnet_hardware_choices):\n\n            print(\n                ""Testing MorphNet Algorithm Combinations [{}, {}, {}, {}] ..."".\n                format(base_model, morphnet_regularizer_algorithm,\n                       morphnet_target_cost, morphnet_hardware))\n            shell_command = ""python morphnet_model_zoo.py --num-epochs {num_epoch} --num-classes {num_classes} --batch-size {batch_size} --learning-rate {learning_rate} --base-model-name {base_model_name} --morphnet-regularizer-algorithm {morphnet_regularizer_algorithm} --morphnet-target-cost {morphnet_target_cost} --morphnet-hardware {morphnet_hardware} --morphnet-regularizer-threshold {morphnet_regularizer_threshold} --morphnet-regularization-multiplier {morphnet_regularization_multiplier} --log-dir {log_dir} --num-cuda-device {num_cuda_device}"".format(\n                num_epoch=num_epochs_default,\n                num_classes=num_classes_default,\n                batch_size=batch_size_default,\n                base_model_name=base_model,\n                learning_rate=learning_rate_default,\n                morphnet_regularizer_algorithm=morphnet_regularizer_algorithm,\n                morphnet_target_cost=morphnet_target_cost,\n                morphnet_hardware=morphnet_hardware,\n                morphnet_regularizer_threshold=\n                morphnet_regularizer_threshold_default,\n                morphnet_regularization_multiplier=\n                morphnet_regularization_multiplier_default,\n                log_dir=log_dir_default,\n                num_cuda_device=num_cuda_device_default)\n\n            try:\n                process = subprocess.run(shell_command,\n                                         shell=True,\n                                         check=True,\n                                         stdout=subprocess.PIPE,\n                                         stderr=subprocess.PIPE)\n                successes.append((base_model, morphnet_regularizer_algorithm,\n                                  morphnet_target_cost, morphnet_hardware))\n                fhand.write(""{},{},{},{},{}\\n"".format(\n                    base_model, morphnet_regularizer_algorithm,\n                    morphnet_target_cost, morphnet_hardware, ""True""))\n                print(""Trial Successful!"")\n            except subprocess.CalledProcessError as exception:\n                failures.append((base_model, morphnet_regularizer_algorithm,\n                                 morphnet_target_cost, morphnet_hardware))\n                fhand.write(""{},{},{},{},{}\\n"".format(\n                    base_model, morphnet_regularizer_algorithm,\n                    morphnet_target_cost, morphnet_hardware, ""False""))\n                print(""Trial Failed!"")\n                print(exception.stderr)\n\n    return successes, failures\n\n\nif __name__ == ""__main__"":\n\n    test_combinations(log_dir=""./morphnet_log"",\n                      csv_filename=""morphnet_test_results.csv"",\n                      num_cuda_device=4)\n'"
examples/keras/utils.py,20,"b'""""""\nMorphNet Model Zoo\n\nLei Mao\nNVIDIA\nhttps://github.com/leimao\n\nUtility functions and classes for MorphNet model zoo.\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm, trange\n\n\ndef select_keras_base_model(base_model_name):\n    """"""\n    Select base model architecture from Keras model zoo.\n    https://keras.io/applications/\n    Args:\n        base_model_name: The base model name. String.\n    Returns:\n        Keras model class.\n    """"""\n    # This has to be set before the Keras model was created.\n    # Otherwise the Batch normalization layer in the model might not be compatible with the MorphNet library.\n    tf.keras.backend.set_learning_phase(1)\n\n    if base_model_name == ""ResNet50"":\n        base_model = tf.keras.applications.resnet.ResNet50\n    elif base_model_name == ""ResNet101"":\n        base_model = tf.keras.applications.resnet.ResNet101\n    elif base_model_name == ""ResNet152"":\n        base_model = tf.keras.applications.resnet.ResNet152\n    elif base_model_name == ""ResNet50V2"":\n        base_model = tf.keras.applications.resnet_v2.ResNet50V2\n    elif base_model_name == ""ResNet101V2"":\n        base_model = tf.keras.applications.resnet_v2.ResNet101V2\n    elif base_model_name == ""ResNet152V2"":\n        base_model = tf.keras.applications.resnet_v2.ResNet152V2\n    elif base_model_name == ""VGG16"":\n        base_model = tf.keras.applications.vgg16.VGG16\n    elif base_model_name == ""VGG19"":\n        base_model = tf.keras.applications.vgg19.VGG19\n    elif base_model_name == ""Xception"":\n        base_model = tf.keras.applications.xception.Xception\n    elif base_model_name == ""InceptionV3"":\n        base_model = tf.keras.applications.inception_v3.InceptionV3\n    elif base_model_name == ""InceptionResNetV2"":\n        base_model = tf.keras.applications.inception_resnet_v2.InceptionResNetV2\n    elif base_model_name == ""MobileNet"":\n        base_model = tf.keras.applications.mobilenet.MobileNet\n    elif base_model_name == ""MobileNetV2"":\n        base_model = tf.keras.applications.mobilenet_v2.MobileNetV2\n    elif base_model_name == ""DenseNet121"":\n        base_model = tf.keras.applications.densenet.DenseNet121\n    elif base_model_name == ""DenseNet169"":\n        base_model = tf.keras.applications.densenet.DenseNet169\n    elif base_model_name == ""DenseNet201"":\n        base_model = tf.keras.applications.densenet.DenseNet201\n    elif base_model_name == ""NASNetLarge"":\n        base_model = tf.keras.applications.nasnet.NASNetLarge\n    elif base_model_name == ""NASNetMobile"":\n        base_model = tf.keras.applications.nasnet.NASNetMobile\n    else:\n        raise Exception(""Unsupported Base Model!"")\n\n    return base_model\n\n\ndef set_reproducible_environment(random_seed=None):\n    """"""\n    Set a reproducible environment using random see.\n    Args:\n        random_seed: Random seed. Integer.\n    """"""\n    if random_seed is not None:\n        np.random.seed(random_seed)\n        tf.set_random_seed(random_seed)\n\n\ndef validate_epoch(epoch, model, x_valid, y_valid_onehot, batch_size):\n    """"""\n    Validating the model using an epoch of the validation dataset.\n    Args:\n        epoch: Epoch ID. Interger.\n        model: MorphNetModel instance.\n        x_valid: Input RGB images from validation dataset. Numpy array.\n        y_valid_onehot: One-hot encoded labels from validation dataset. Numpy array.\n        batch_size: Batch size. Integer.\n    """"""\n    validation_idx = np.arange(len(x_valid))\n    mini_batch_idx = [\n        validation_idx[k:k + batch_size]\n        for k in range(0, len(x_valid), batch_size)\n    ]\n    tqdm_iterator = trange(len(mini_batch_idx),\n                           desc=""Validation Epoch: {}"".format(epoch))\n    num_validation_samples = 0\n    num_correct_predictions = 0\n    total_cost = 0\n\n    for i in tqdm_iterator:\n        idx = mini_batch_idx[i]\n        accuracy, cost = model.validate(inputs=x_valid[idx],\n                                        labels=y_valid_onehot[idx])\n        num_correct_predictions += accuracy * len(idx)\n        num_validation_samples += len(idx)\n        total_cost += cost\n\n    validation_acc = num_correct_predictions / num_validation_samples\n    cost_avg = total_cost / len(mini_batch_idx)\n    print(""Epoch: {}, Validation Acc: {:.4f}, Cost Avg: {:.4f}"".format(\n        epoch, validation_acc, cost_avg))\n\n\ndef train_epoch(epoch,\n                model,\n                x_train,\n                y_train_onehot,\n                batch_size,\n                shuffle=True,\n                print_batch_info=False):\n    """"""\n    Training the model using an epoch of the training dataset.\n    Args:\n        epoch: Epoch ID. Interger.\n        model: MorphNetModel instance.\n        x_train: Input RGB images from training dataset. Numpy array.\n        y_train_onehot: One-hot encoded labels from training dataset. Numpy array.\n        batch_size: Batch size. Integer.\n        shuffle: Whether to shuffle the training dataset before each epoch. Boolean.\n        print_batch_info: Whether to print the training statistics for each batch. Boolean.\n    """"""\n    train_idx = np.arange(len(x_train))\n    if shuffle == True:\n        np.random.shuffle(train_idx)\n    mini_batch_idx = [\n        train_idx[k:k + batch_size] for k in range(0, len(x_train), batch_size)\n    ]\n    tqdm_iterator = trange(len(mini_batch_idx),\n                           desc=""Train Epoch: {}"".format(epoch))\n    num_train_samples = 0\n    num_correct_predictions = 0\n    total_cost = 0\n    total_loss = 0\n\n    for i in tqdm_iterator:\n        idx = mini_batch_idx[i]\n        loss, accuracy, cost = model.train(inputs=x_train[idx],\n                                           labels=y_train_onehot[idx])\n        num_correct_predictions += accuracy * len(idx)\n        num_train_samples += len(idx)\n        total_cost += cost\n        total_loss += loss * len(idx)\n        if print_batch_info == True:\n            train_batch_acc = accuracy\n            tqdm.write(\n                ""Epoch: {}, Batch: {}, Train Acc: {:.4f}, Loss: {:.4f}, Cost: {:.4f}""\n                .format(epoch, i, train_batch_acc, loss, cost))\n\n    train_acc = num_correct_predictions / num_train_samples\n    cost_avg = total_cost / len(mini_batch_idx)\n    loss_avg = total_loss / len(x_train)\n    print(""Epoch: {}, Train Acc: {:.4f}, Loss Avg: {:.4f}, Cost Avg: {:.4f}"".\n          format(epoch, train_acc, loss_avg, cost_avg))\n'"
examples/slim/__init__.py,0,b''
examples/slim/download_and_convert_data.py,4,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts a particular dataset.\n\nUsage:\n```shell\n\n$ python download_and_convert_data.py \\\n    --dataset_name=mnist \\\n    --dataset_dir=/tmp/mnist\n\n$ python download_and_convert_data.py \\\n    --dataset_name=cifar10 \\\n    --dataset_dir=/tmp/cifar10\n\n$ python download_and_convert_data.py \\\n    --dataset_name=flowers \\\n    --dataset_dir=/tmp/flowers\n```\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom datasets import download_and_convert_cifar10\nfrom datasets import download_and_convert_flowers\nfrom datasets import download_and_convert_mnist\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\',\n    None,\n    \'The name of the dataset to convert, one of ""cifar10"", ""flowers"", ""mnist"".\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\',\n    None,\n    \'The directory where the output TFRecords and temporary files are saved.\')\n\n\ndef main(_):\n  if not FLAGS.dataset_name:\n    raise ValueError(\'You must supply the dataset name with --dataset_name\')\n  if not FLAGS.dataset_dir:\n    raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n  if FLAGS.dataset_name == \'cifar10\':\n    download_and_convert_cifar10.run(FLAGS.dataset_dir)\n  elif FLAGS.dataset_name == \'flowers\':\n    download_and_convert_flowers.run(FLAGS.dataset_dir)\n  elif FLAGS.dataset_name == \'mnist\':\n    download_and_convert_mnist.run(FLAGS.dataset_dir)\n  else:\n    raise ValueError(\n        \'dataset_name [%s] was not recognized.\' % FLAGS.dataset_name)\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
examples/slim/eval_image_classifier.py,31,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic evaluation script that evaluates a model using a given dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport tensorflow as tf\n\nfrom datasets import dataset_factory\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\n\nslim = tf.contrib.slim\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 100, \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'max_num_batches\', None,\n    \'Max number of batches to evaluate by default use all.\')\n\ntf.app.flags.DEFINE_string(\n    \'master\', \'\', \'The address of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'/tmp/tfmodel/\',\n    \'The directory where the model was written to or an absolute path to a \'\n    \'checkpoint file.\')\n\ntf.app.flags.DEFINE_string(\n    \'eval_dir\', \'/tmp/tfmodel/\', \'Directory where the results are saved to.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_split_name\', \'test\', \'The name of the train/test split.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n    \'labels_offset\', 0,\n    \'An offset for the labels in the dataset. This flag is primarily used to \'\n    \'evaluate the VGG and ResNet architectures which do not use a background \'\n    \'class for the ImageNet dataset.\')\n\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'inception_v3\', \'The name of the architecture to evaluate.\')\n\ntf.app.flags.DEFINE_string(\n    \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n    \'as `None`, then the model_name flag is used.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\ntf.app.flags.DEFINE_integer(\n    \'eval_image_size\', None, \'Eval image size\')\n\ntf.app.flags.DEFINE_bool(\n    \'quantize\', False, \'whether to use quantized graph or not.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n  if not FLAGS.dataset_dir:\n    raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  with tf.Graph().as_default():\n    tf_global_step = slim.get_or_create_global_step()\n\n    ######################\n    # Select the dataset #\n    ######################\n    dataset = dataset_factory.get_dataset(\n        FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n\n    ####################\n    # Select the model #\n    ####################\n    network_fn = nets_factory.get_network_fn(\n        FLAGS.model_name,\n        num_classes=(dataset.num_classes - FLAGS.labels_offset),\n        is_training=False)\n\n    ##############################################################\n    # Create a dataset provider that loads data from the dataset #\n    ##############################################################\n    provider = slim.dataset_data_provider.DatasetDataProvider(\n        dataset,\n        shuffle=False,\n        common_queue_capacity=2 * FLAGS.batch_size,\n        common_queue_min=FLAGS.batch_size)\n    [image, label] = provider.get([\'image\', \'label\'])\n    label -= FLAGS.labels_offset\n\n    #####################################\n    # Select the preprocessing function #\n    #####################################\n    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n        preprocessing_name,\n        is_training=False)\n\n    eval_image_size = FLAGS.eval_image_size or network_fn.default_image_size\n\n    image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\n\n    images, labels = tf.train.batch(\n        [image, label],\n        batch_size=FLAGS.batch_size,\n        num_threads=FLAGS.num_preprocessing_threads,\n        capacity=5 * FLAGS.batch_size)\n\n    ####################\n    # Define the model #\n    ####################\n    logits, _ = network_fn(images)\n\n    if FLAGS.quantize:\n      tf.contrib.quantize.create_eval_graph()\n\n    if FLAGS.moving_average_decay:\n      variable_averages = tf.train.ExponentialMovingAverage(\n          FLAGS.moving_average_decay, tf_global_step)\n      variables_to_restore = variable_averages.variables_to_restore(\n          slim.get_model_variables())\n      variables_to_restore[tf_global_step.op.name] = tf_global_step\n    else:\n      variables_to_restore = slim.get_variables_to_restore()\n\n    predictions = tf.argmax(logits, 1)\n    labels = tf.squeeze(labels)\n\n    # Define the metrics:\n    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n        \'Accuracy\': slim.metrics.streaming_accuracy(predictions, labels),\n        \'Recall_5\': slim.metrics.streaming_recall_at_k(\n            logits, labels, 5),\n    })\n\n    # Print the summaries to screen.\n    for name, value in names_to_values.items():\n      summary_name = \'eval/%s\' % name\n      op = tf.summary.scalar(summary_name, value, collections=[])\n      op = tf.Print(op, [value], summary_name)\n      tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n\n    # TODO(sguada) use num_epochs=1\n    if FLAGS.max_num_batches:\n      num_batches = FLAGS.max_num_batches\n    else:\n      # This ensures that we make a single pass over all of the data.\n      num_batches = math.ceil(dataset.num_samples / float(FLAGS.batch_size))\n\n    if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n      checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n    else:\n      checkpoint_path = FLAGS.checkpoint_path\n\n    tf.logging.info(\'Evaluating %s\' % checkpoint_path)\n\n    slim.evaluation.evaluate_once(\n        master=FLAGS.master,\n        checkpoint_path=checkpoint_path,\n        logdir=FLAGS.eval_dir,\n        num_evals=num_batches,\n        eval_op=list(names_to_updates.values()),\n        variables_to_restore=variables_to_restore)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
examples/slim/export_inference_graph.py,20,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Saves out a GraphDef containing the architecture of the model.\n\nTo use it, run something like this, with a model name defined by slim:\n\nbazel build tensorflow_models/research/slim:export_inference_graph\nbazel-bin/tensorflow_models/research/slim/export_inference_graph \\\n--model_name=inception_v3 --output_file=/tmp/inception_v3_inf_graph.pb\n\nIf you then want to use the resulting model with your own or pretrained\ncheckpoints as part of a mobile model, you can run freeze_graph to get a graph\ndef with the variables inlined as constants using:\n\nbazel build tensorflow/python/tools:freeze_graph\nbazel-bin/tensorflow/python/tools/freeze_graph \\\n--input_graph=/tmp/inception_v3_inf_graph.pb \\\n--input_checkpoint=/tmp/checkpoints/inception_v3.ckpt \\\n--input_binary=true --output_graph=/tmp/frozen_inception_v3.pb \\\n--output_node_names=InceptionV3/Predictions/Reshape_1\n\nThe output node names will vary depending on the model, but you can inspect and\nestimate them using the summarize_graph tool:\n\nbazel build tensorflow/tools/graph_transforms:summarize_graph\nbazel-bin/tensorflow/tools/graph_transforms/summarize_graph \\\n--in_graph=/tmp/inception_v3_inf_graph.pb\n\nTo run the resulting graph in C++, you can look at the label_image sample code:\n\nbazel build tensorflow/examples/label_image:label_image\nbazel-bin/tensorflow/examples/label_image/label_image \\\n--image=${HOME}/Pictures/flowers.jpg \\\n--input_layer=input \\\n--output_layer=InceptionV3/Predictions/Reshape_1 \\\n--graph=/tmp/frozen_inception_v3.pb \\\n--labels=/tmp/imagenet_slim_labels.txt \\\n--input_mean=0 \\\n--input_std=255\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\n\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import gfile\nfrom datasets import dataset_factory\nfrom nets import nets_factory\n\n\nslim = tf.contrib.slim\n\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'inception_v3\', \'The name of the architecture to save.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'is_training\', False,\n    \'Whether to save out a training-focused version of the model.\')\n\ntf.app.flags.DEFINE_integer(\n    \'image_size\', None,\n    \'The image size to use, otherwise use the model default_image_size.\')\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', None,\n    \'Batch size for the exported model. Defaulted to ""None"" so batch size can \'\n    \'be specified at model runtime.\')\n\ntf.app.flags.DEFINE_string(\'dataset_name\', \'imagenet\',\n                           \'The name of the dataset to use with the model.\')\n\ntf.app.flags.DEFINE_integer(\n    \'labels_offset\', 0,\n    \'An offset for the labels in the dataset. This flag is primarily used to \'\n    \'evaluate the VGG and ResNet architectures which do not use a background \'\n    \'class for the ImageNet dataset.\')\n\ntf.app.flags.DEFINE_string(\n    \'output_file\', \'\', \'Where to save the resulting file to.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', \'\', \'Directory to save intermediate dataset files to\')\n\ntf.app.flags.DEFINE_bool(\n    \'quantize\', False, \'whether to use quantized graph or not.\')\n\ntf.app.flags.DEFINE_bool(\n    \'is_video_model\', False, \'whether to use 5-D inputs for video model.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_frames\', None,\n    \'The number of frames to use. Only used if is_video_model is True.\')\n\ntf.app.flags.DEFINE_bool(\'write_text_graphdef\', False,\n                         \'Whether to write a text version of graphdef.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n  if not FLAGS.output_file:\n    raise ValueError(\'You must supply the path to save to with --output_file\')\n  if FLAGS.is_video_model and not FLAGS.num_frames:\n    raise ValueError(\n        \'Number of frames must be specified for video models with --num_frames\')\n  tf.logging.set_verbosity(tf.logging.INFO)\n  with tf.Graph().as_default() as graph:\n    dataset = dataset_factory.get_dataset(FLAGS.dataset_name, \'train\',\n                                          FLAGS.dataset_dir)\n    network_fn = nets_factory.get_network_fn(\n        FLAGS.model_name,\n        num_classes=(dataset.num_classes - FLAGS.labels_offset),\n        is_training=FLAGS.is_training)\n    image_size = FLAGS.image_size or network_fn.default_image_size\n    if FLAGS.is_video_model:\n      input_shape = [FLAGS.batch_size, FLAGS.num_frames,\n                     image_size, image_size, 3]\n    else:\n      input_shape = [FLAGS.batch_size, image_size, image_size, 3]\n    placeholder = tf.placeholder(name=\'input\', dtype=tf.float32,\n                                 shape=input_shape)\n    network_fn(placeholder)\n\n    if FLAGS.quantize:\n      tf.contrib.quantize.create_eval_graph()\n\n    graph_def = graph.as_graph_def()\n    if FLAGS.write_text_graphdef:\n      tf.io.write_graph(\n          graph_def,\n          os.path.dirname(FLAGS.output_file),\n          os.path.basename(FLAGS.output_file),\n          as_text=True)\n    else:\n      with gfile.GFile(FLAGS.output_file, \'wb\') as f:\n        f.write(graph_def.SerializeToString())\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
examples/slim/export_inference_graph_test.py,3,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for export_inference_graph.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\n\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import gfile\nimport export_inference_graph\n\n\nclass ExportInferenceGraphTest(tf.test.TestCase):\n\n  def testExportInferenceGraph(self):\n    tmpdir = self.get_temp_dir()\n    output_file = os.path.join(tmpdir, \'inception_v3.pb\')\n    flags = tf.app.flags.FLAGS\n    flags.output_file = output_file\n    flags.model_name = \'inception_v3\'\n    flags.dataset_dir = tmpdir\n    export_inference_graph.main(None)\n    self.assertTrue(gfile.Exists(output_file))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
examples/slim/setup.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Setup script for slim.""""""\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n\nsetup(\n    name=\'slim\',\n    version=\'0.1\',\n    include_package_data=True,\n    packages=find_packages(),\n    description=\'tf-slim\',\n)\n'"
examples/slim/train_image_classifier.py,95,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic training script that trains a model using a given dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom datasets import dataset_factory\nfrom deployment import model_deploy\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\n\nfrom morph_net.network_regularizers import flop_regularizer, latency_regularizer\nfrom morph_net.network_regularizers import model_size_regularizer\nfrom morph_net.tools import structure_exporter\n\n################################################################################\n# Morph-Net Parameters from lines 546-552\n################################################################################\n\nslim = tf.contrib.slim\n\ntf.app.flags.DEFINE_string(\n    \'master\', \'\', \'The address of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\n    \'train_dir\', \'/tmp/tfmodel/\',\n    \'Directory where checkpoints and event logs are written to.\')\n\ntf.app.flags.DEFINE_integer(\'num_clones\', 1,\n                            \'Number of model clones to deploy. Note For \'\n                            \'historical reasons loss from all clones averaged \'\n                            \'out and learning rate decay happen per clone \'\n                            \'epochs\')\n\ntf.app.flags.DEFINE_boolean(\'clone_on_cpu\', False,\n                            \'Use CPUs to deploy clones.\')\n\ntf.app.flags.DEFINE_integer(\'worker_replicas\', 1, \'Number of worker replicas.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_ps_tasks\', 0,\n    \'The number of parameter servers. If the value is 0, then the parameters \'\n    \'are handled locally by the worker.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 4,\n    \'The number of parallel readers that read data from the dataset.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_summaries_secs\', 600,\n    \'The frequency with which summaries are saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_interval_secs\', 600,\n    \'The frequency with which the model is saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n    \'task\', 0, \'Task id of the replica running the training.\')\n\n######################\n# Optimization Flags #\n######################\n\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 0.00004, \'The weight decay on the model weights.\')\n\ntf.app.flags.DEFINE_string(\n    \'optimizer\', \'rmsprop\',\n    \'The name of the optimizer, one of ""adadelta"", ""adagrad"", ""adam"",\'\n    \'""ftrl"", ""momentum"", ""sgd"" or ""rmsprop"".\')\n\ntf.app.flags.DEFINE_float(\n    \'adadelta_rho\', 0.95,\n    \'The decay rate for adadelta.\')\n\ntf.app.flags.DEFINE_float(\n    \'adagrad_initial_accumulator_value\', 0.1,\n    \'Starting value for the AdaGrad accumulators.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta1\', 0.9,\n    \'The exponential decay rate for the 1st moment estimates.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta2\', 0.999,\n    \'The exponential decay rate for the 2nd moment estimates.\')\n\ntf.app.flags.DEFINE_float(\'opt_epsilon\', 1.0, \'Epsilon term for the optimizer.\')\n\ntf.app.flags.DEFINE_float(\'ftrl_learning_rate_power\', -0.5,\n                          \'The learning rate power.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_initial_accumulator_value\', 0.1,\n    \'Starting value for the FTRL accumulators.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_l1\', 0.0, \'The FTRL l1 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_l2\', 0.0, \'The FTRL l2 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_momentum\', 0.9, \'Momentum.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_decay\', 0.9, \'Decay term for RMSProp.\')\n\ntf.app.flags.DEFINE_integer(\n    \'quantize_delay\', -1,\n    \'Number of steps to start quantized training. Set to -1 would disable \'\n    \'quantized training.\')\n\n#######################\n# Learning Rate Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'learning_rate_decay_type\',\n    \'exponential\',\n    \'Specifies how the learning rate is decayed. One of ""fixed"", ""exponential"",\'\n    \' or ""polynomial""\')\n\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.0001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'label_smoothing\', 0.0, \'The amount of label smoothing.\')\n\ntf.app.flags.DEFINE_float(\n    \'learning_rate_decay_factor\', 0.94, \'Learning rate decay factor.\')\n\ntf.app.flags.DEFINE_float(\n    \'num_epochs_per_decay\', 2.0,\n    \'Number of epochs after which learning rate decays. Note: this flag counts \'\n    \'epochs per clone but aggregates per sync replicas. So 1.0 means that \'\n    \'each clone will go over full epoch individually, but replicas will go \'\n    \'once across all replicas.\')\n\ntf.app.flags.DEFINE_bool(\n    \'sync_replicas\', False,\n    \'Whether or not to synchronize the replicas during training.\')\n\ntf.app.flags.DEFINE_integer(\n    \'replicas_to_aggregate\', 1,\n    \'The Number of gradients to collect before updating params.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\n#######################\n# Dataset Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_split_name\', \'train\', \'The name of the train/test split.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n    \'labels_offset\', 0,\n    \'An offset for the labels in the dataset. This flag is primarily used to \'\n    \'evaluate the VGG and ResNet architectures which do not use a background \'\n    \'class for the ImageNet dataset.\')\n\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'inception_v3\', \'The name of the architecture to train.\')\n\ntf.app.flags.DEFINE_string(\n    \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n    \'as `None`, then the model_name flag is used.\')\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 32, \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', None, \'Train image size\')\n\ntf.app.flags.DEFINE_integer(\'max_number_of_steps\', None,\n                            \'The maximum number of training steps.\')\n\n#####################\n# Fine-Tuning Flags #\n#####################\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring \'\n    \'from a checkpoint.\')\n\ntf.app.flags.DEFINE_string(\n    \'trainable_scopes\', None,\n    \'Comma-separated list of scopes to filter the set of variables to train.\'\n    \'By default, None would train all the variables.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', False,\n    \'When restoring a checkpoint would ignore missing variables.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _configure_learning_rate(num_samples_per_epoch, global_step):\n  """"""Configures the learning rate.\n\n  Args:\n    num_samples_per_epoch: The number of samples in each epoch of training.\n    global_step: The global_step tensor.\n\n  Returns:\n    A `Tensor` representing the learning rate.\n\n  Raises:\n    ValueError: if\n  """"""\n  # Note: when num_clones is > 1, this will actually have each clone to go\n  # over each epoch FLAGS.num_epochs_per_decay times. This is different\n  # behavior from sync replicas and is expected to produce different results.\n  decay_steps = int(num_samples_per_epoch * FLAGS.num_epochs_per_decay /\n                    FLAGS.batch_size)\n\n  if FLAGS.sync_replicas:\n    decay_steps /= FLAGS.replicas_to_aggregate\n\n  if FLAGS.learning_rate_decay_type == \'exponential\':\n    return tf.train.exponential_decay(FLAGS.learning_rate,\n                                      global_step,\n                                      decay_steps,\n                                      FLAGS.learning_rate_decay_factor,\n                                      staircase=True,\n                                      name=\'exponential_decay_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'fixed\':\n    return tf.constant(FLAGS.learning_rate, name=\'fixed_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'polynomial\':\n    return tf.train.polynomial_decay(FLAGS.learning_rate,\n                                     global_step,\n                                     decay_steps,\n                                     FLAGS.end_learning_rate,\n                                     power=1.0,\n                                     cycle=False,\n                                     name=\'polynomial_decay_learning_rate\')\n  else:\n    raise ValueError(\'learning_rate_decay_type [%s] was not recognized\' %\n                     FLAGS.learning_rate_decay_type)\n\n\ndef _configure_optimizer(learning_rate):\n  """"""Configures the optimizer used for training.\n\n  Args:\n    learning_rate: A scalar or `Tensor` learning rate.\n\n  Returns:\n    An instance of an optimizer.\n\n  Raises:\n    ValueError: if FLAGS.optimizer is not recognized.\n  """"""\n  if FLAGS.optimizer == \'adadelta\':\n    optimizer = tf.train.AdadeltaOptimizer(\n        learning_rate,\n        rho=FLAGS.adadelta_rho,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'adagrad\':\n    optimizer = tf.train.AdagradOptimizer(\n        learning_rate,\n        initial_accumulator_value=FLAGS.adagrad_initial_accumulator_value)\n  elif FLAGS.optimizer == \'adam\':\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate,\n        beta1=FLAGS.adam_beta1,\n        beta2=FLAGS.adam_beta2,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'ftrl\':\n    optimizer = tf.train.FtrlOptimizer(\n        learning_rate,\n        learning_rate_power=FLAGS.ftrl_learning_rate_power,\n        initial_accumulator_value=FLAGS.ftrl_initial_accumulator_value,\n        l1_regularization_strength=FLAGS.ftrl_l1,\n        l2_regularization_strength=FLAGS.ftrl_l2)\n  elif FLAGS.optimizer == \'momentum\':\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate,\n        momentum=FLAGS.momentum,\n        name=\'Momentum\')\n  elif FLAGS.optimizer == \'rmsprop\':\n    optimizer = tf.train.RMSPropOptimizer(\n        learning_rate,\n        decay=FLAGS.rmsprop_decay,\n        momentum=FLAGS.rmsprop_momentum,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'sgd\':\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  else:\n    raise ValueError(\'Optimizer [%s] was not recognized\' % FLAGS.optimizer)\n  return optimizer\n\n\ndef _get_init_fn():\n  """"""Returns a function run by the chief worker to warm-start the training.\n\n  Note that the init_fn is only run when initializing the model during the very\n  first global step.\n\n  Returns:\n    An init function run by the supervisor.\n  """"""\n  if FLAGS.checkpoint_path is None:\n    return None\n\n  # Warn the user if a checkpoint exists in the train_dir. Then we\'ll be\n  # ignoring the checkpoint anyway.\n  if tf.train.latest_checkpoint(FLAGS.train_dir):\n    tf.logging.info(\n        \'Ignoring --checkpoint_path because a checkpoint already exists in %s\'\n        % FLAGS.train_dir)\n    return None\n\n  exclusions = []\n  if FLAGS.checkpoint_exclude_scopes:\n    exclusions = [scope.strip()\n                  for scope in FLAGS.checkpoint_exclude_scopes.split(\',\')]\n\n  # TODO(sguada) variables.filter_variables()\n  variables_to_restore = []\n  for var in slim.get_model_variables():\n    for exclusion in exclusions:\n      if var.op.name.startswith(exclusion):\n        break\n    else:\n      variables_to_restore.append(var)\n\n  if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n    checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n  else:\n    checkpoint_path = FLAGS.checkpoint_path\n\n  tf.logging.info(\'Fine-tuning from %s\' % checkpoint_path)\n\n  return slim.assign_from_checkpoint_fn(\n      checkpoint_path,\n      variables_to_restore,\n      ignore_missing_vars=FLAGS.ignore_missing_vars)\n\n\ndef _get_variables_to_train():\n  """"""Returns a list of variables to train.\n\n  Returns:\n    A list of variables to train by the optimizer.\n  """"""\n  if FLAGS.trainable_scopes is None:\n    return tf.trainable_variables()\n  else:\n    scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(\',\')]\n\n  variables_to_train = []\n  for scope in scopes:\n    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n    variables_to_train.extend(variables)\n  return variables_to_train\n\n\ndef mn_train_step(sess, train_op, global_step, train_step_kwargs):\n  """"""Function that takes a gradient step and specifies whether to stop.\n  Args:\n    sess: The current session.\n    train_op: An `Operation` that evaluates the gradients and returns the total\n      loss.\n    global_step: A `Tensor` representing the global training step.\n    train_step_kwargs: A dictionary of keyword arguments.\n  Returns:\n    The total loss and a boolean indicating whether or not to stop training.\n  Raises:\n    ValueError: if \'should_trace\' is in `train_step_kwargs` but `logdir` is not.\n  """"""\n  import time\n  from tensorflow.core.protobuf import config_pb2\n  from tensorflow.python.client import timeline\n  from tensorflow.python.platform import tf_logging as logging\n  from tensorflow.python.lib.io import file_io\n\n  start_time = time.time()\n\n  trace_run_options = None\n  run_metadata = None\n  if \'should_trace\' in train_step_kwargs:\n    if \'logdir\' not in train_step_kwargs:\n      raise ValueError(\'logdir must be present in train_step_kwargs when \'\n                       \'should_trace is present\')\n    if sess.run(train_step_kwargs[\'should_trace\']):\n      trace_run_options = config_pb2.RunOptions(\n          trace_level=config_pb2.RunOptions.FULL_TRACE)\n      run_metadata = config_pb2.RunMetadata()\n\n  total_loss, np_global_step, structure_exporter_tensors = sess.run([train_op[0], \n                                        global_step, train_op[1].tensors],\n                                        options=trace_run_options,\n                                        run_metadata=run_metadata)\n  time_elapsed = time.time() - start_time\n\n  if (np_global_step % 1000 == 0):\n    train_op[1].populate_tensor_values(structure_exporter_tensors)\n    train_op[1].create_file_and_save_alive_counts(FLAGS.train_dir+""/ee"", \n                                                    np_global_step)\n\n  if run_metadata is not None:\n    tl = timeline.Timeline(run_metadata.step_stats)\n    trace = tl.generate_chrome_trace_format()\n    trace_filename = os.path.join(train_step_kwargs[\'logdir\'],\n                                  \'tf_trace-%d.json\' % np_global_step)\n    logging.info(\'Writing trace to %s\', trace_filename)\n    file_io.write_string_to_file(trace_filename, trace)\n    if \'summary_writer\' in train_step_kwargs:\n      train_step_kwargs[\'summary_writer\'].add_run_metadata(\n          run_metadata, \'run_metadata-%d\' % np_global_step)\n\n  if \'should_log\' in train_step_kwargs:\n    if sess.run(train_step_kwargs[\'should_log\']):\n      logging.info(\'global step %d: loss = %.4f (%.3f sec/step)\',\n                   np_global_step, total_loss, time_elapsed)\n\n  if \'should_stop\' in train_step_kwargs:\n    should_stop = sess.run(train_step_kwargs[\'should_stop\'])\n  else:\n    should_stop = False\n\n  return total_loss, should_stop\n\n\ndef main(_):\n  if not FLAGS.dataset_dir:\n    raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  with tf.Graph().as_default():\n    #######################\n    # Config model_deploy #\n    #######################\n    deploy_config = model_deploy.DeploymentConfig(\n        num_clones=FLAGS.num_clones,\n        clone_on_cpu=FLAGS.clone_on_cpu,\n        replica_id=FLAGS.task,\n        num_replicas=FLAGS.worker_replicas,\n        num_ps_tasks=FLAGS.num_ps_tasks)\n\n    # Create global_step\n    with tf.device(deploy_config.variables_device()):\n      global_step = slim.create_global_step()\n\n    ######################\n    # Select the dataset #\n    ######################\n    dataset = dataset_factory.get_dataset(\n        FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n\n    ######################\n    # Select the network #\n    ######################\n    network_fn = nets_factory.get_network_fn(\n        FLAGS.model_name,\n        num_classes=(dataset.num_classes - FLAGS.labels_offset),\n        weight_decay=FLAGS.weight_decay,\n        is_training=True)\n\n    #####################################\n    # Select the preprocessing function #\n    #####################################\n    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n        preprocessing_name,\n        is_training=True)\n\n    ##############################################################\n    # Create a dataset provider that loads data from the dataset #\n    ##############################################################\n    with tf.device(deploy_config.inputs_device()):\n      provider = slim.dataset_data_provider.DatasetDataProvider(\n          dataset,\n          num_readers=FLAGS.num_readers,\n          common_queue_capacity=20 * FLAGS.batch_size,\n          common_queue_min=10 * FLAGS.batch_size)\n      [image, label] = provider.get([\'image\', \'label\'])\n      label -= FLAGS.labels_offset\n\n      train_image_size = FLAGS.train_image_size or network_fn.default_image_size\n\n      image = image_preprocessing_fn(image, train_image_size, train_image_size)\n\n      images, labels = tf.train.batch(\n          [image, label],\n          batch_size=FLAGS.batch_size,\n          num_threads=FLAGS.num_preprocessing_threads,\n          capacity=5 * FLAGS.batch_size)\n      labels = slim.one_hot_encoding(\n          labels, dataset.num_classes - FLAGS.labels_offset)\n      batch_queue = slim.prefetch_queue.prefetch_queue(\n          [images, labels], capacity=2 * deploy_config.num_clones)\n\n    ####################\n    # Define the model #\n    ####################\n    def clone_fn(batch_queue):\n      """"""Allows data parallelism by creating multiple clones of network_fn.""""""\n      images, labels = batch_queue.dequeue()\n      logits, end_points = network_fn(images)\n\n      network_regularizer = flop_regularizer.GammaFlopsRegularizer(\n          output_boundary=[logits.op],\n          input_boundary=[images.op, labels.op],\n          gamma_threshold=1e-3\n      )\n      regularization_strength = 7e-9\n      regularizer_loss = (network_regularizer.get_regularization_term() \n                            * regularization_strength)\n\n      #############################\n      # Specify the loss function #\n      #############################\n      if \'AuxLogits\' in end_points:\n        slim.losses.softmax_cross_entropy(\n            end_points[\'AuxLogits\'], labels,\n            label_smoothing=FLAGS.label_smoothing, weights=0.4,\n            scope=\'aux_loss\')\n      slim.losses.softmax_cross_entropy(\n          logits, labels, label_smoothing=FLAGS.label_smoothing, weights=1.0)\n      return end_points, network_regularizer, regularizer_loss\n\n    # Gather initial summaries.\n    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n    first_clone_scope = deploy_config.clone_scope(0)\n    # Gather update_ops from the first clone. These contain, for example,\n    # the updates for the batch_norm variables created by network_fn.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n\n    # Add summaries for end_points.\n    end_points = clones[0].outputs\n    for end_point in end_points:\n      x = end_points[end_point]\n      summaries.add(tf.summary.histogram(\'activations/\' + end_point, x))\n      summaries.add(tf.summary.scalar(\'sparsity/\' + end_point,\n                                      tf.nn.zero_fraction(x)))\n\n    summaries.add(tf.summary.scalar(\'RegularizationLoss\', clones[0].reg_loss))\n    summaries.add(tf.summary.scalar(clones[0].network_regularizer.cost_name, \n                                    clones[0].network_regularizer.get_cost()))\n\n    # Add summaries for losses.\n    for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n      summaries.add(tf.summary.scalar(\'losses/%s\' % loss.op.name, loss))\n\n    # Add summaries for variables.\n    for variable in slim.get_model_variables():\n      summaries.add(tf.summary.histogram(variable.op.name, variable))\n\n    #################################\n    # Configure the moving averages #\n    #################################\n    if FLAGS.moving_average_decay:\n      moving_average_variables = slim.get_model_variables()\n      variable_averages = tf.train.ExponentialMovingAverage(\n          FLAGS.moving_average_decay, global_step)\n    else:\n      moving_average_variables, variable_averages = None, None\n\n    if FLAGS.quantize_delay >= 0:\n      tf.contrib.quantize.create_training_graph(\n          quant_delay=FLAGS.quantize_delay)\n\n    #########################################\n    # Configure the optimization procedure. #\n    #########################################\n    with tf.device(deploy_config.optimizer_device()):\n      learning_rate = _configure_learning_rate(dataset.num_samples, global_step)\n      optimizer = _configure_optimizer(learning_rate)\n      summaries.add(tf.summary.scalar(\'learning_rate\', learning_rate))\n\n    if FLAGS.sync_replicas:\n      # If sync_replicas is enabled, the averaging will be done in the chief\n      # queue runner.\n      optimizer = tf.train.SyncReplicasOptimizer(\n          opt=optimizer,\n          replicas_to_aggregate=FLAGS.replicas_to_aggregate,\n          total_num_replicas=FLAGS.worker_replicas,\n          variable_averages=variable_averages,\n          variables_to_average=moving_average_variables)\n    elif FLAGS.moving_average_decay:\n      # Update ops executed locally by trainer.\n      update_ops.append(variable_averages.apply(moving_average_variables))\n\n    # Variables to train.\n    variables_to_train = _get_variables_to_train()\n\n    #  and returns a train_tensor and summary_op\n    total_loss, clones_gradients = model_deploy.optimize_clones(\n        clones,\n        optimizer,\n        var_list=variables_to_train)\n    # Add total_loss to summary.\n    summaries.add(tf.summary.scalar(\'total_loss\', total_loss))\n\n    # Create gradient updates.\n    grad_updates = optimizer.apply_gradients(clones_gradients,\n                                             global_step=global_step)\n    update_ops.append(grad_updates)\n\n    ###########################################\n    # Morph-Net exporter\n    exporter = structure_exporter.StructureExporter(\n                clones[0].network_regularizer.op_regularizer_manager)\n\n    ###########################################\n\n    update_op = tf.group(*update_ops)\n    with tf.control_dependencies([update_op]):\n      train_tensor = tf.identity(total_loss, name=\'train_op\')\n    \n    train_list = [train_tensor, exporter]\n\n    # Add the summaries from the first clone. These contain the summaries\n    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                       first_clone_scope))\n\n    # Merge all summaries together.\n    summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n\n    ###########################\n    # Kicks off the training. #\n    ###########################\n    slim.learning.train(\n        train_list,\n        train_step_fn=mn_train_step,\n        logdir=FLAGS.train_dir,\n        master=FLAGS.master,\n        is_chief=(FLAGS.task == 0),\n        init_fn=_get_init_fn(),\n        summary_op=summary_op,\n        number_of_steps=FLAGS.max_number_of_steps,\n        log_every_n_steps=FLAGS.log_every_n_steps,\n        save_summaries_secs=FLAGS.save_summaries_secs,\n        save_interval_secs=FLAGS.save_interval_secs,\n        sync_optimizer=optimizer if FLAGS.sync_replicas else None)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
morph_net/framework/__init__.py,0,b''
morph_net/framework/batch_norm_source_op_handler.py,1,"b'""""""OpHandler implementation for batch norm ops that are regularizer sources.\n\nThis OpHandler is used when batch norm gammas are considered regularizers.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom morph_net.framework import grouping_op_handler\nfrom morph_net.framework import tpu_util\nfrom morph_net.op_regularizers import gamma_l1_regularizer\n\n\nclass BatchNormSourceOpHandler(grouping_op_handler.GroupingOpHandler):\n  """"""OpHandler implementation for batch norm source operations.""""""\n\n  def __init__(self, gamma_threshold):\n    """"""Instantiate an instance.\n\n    Args:\n      gamma_threshold: Float scalar, the threshold above which a gamma is\n        considered alive.\n    """"""\n    super(BatchNormSourceOpHandler, self).__init__()\n    self._gamma_threshold = gamma_threshold\n\n  @property\n  def is_source_op(self):\n    return True\n\n  def assign_grouping(self, op, op_reg_manager):\n    """"""Assign grouping to the given op and updates the manager.\n\n    Args:\n      op: tf.Operation to assign grouping to.\n      op_reg_manager: OpRegularizerManager to keep track of the grouping.\n    """"""\n    # This is a source op so begin by getting the OpGroup or creating one.\n    op_slices = op_reg_manager.get_op_slices(op)\n    for op_slice in op_slices:\n      op_group = op_reg_manager.get_op_group(op_slice)\n      if op_group is None:\n        op_reg_manager.create_op_group_for_op_slice(op_slice)\n\n    super(BatchNormSourceOpHandler, self).assign_grouping(op, op_reg_manager)\n\n  def create_regularizer(self, op_slice):\n    """"""Create a regularizer for this batch norm OpSlice.\n\n    Args:\n      op_slice: op_regularizer_manager.OpSlice that is a batch norm OpSlice.\n\n    Returns:\n      OpRegularizer for this batch norm op.\n    """"""\n    start_index = op_slice.slice.start_index\n    size = op_slice.slice.size\n    gamma = op_slice.op.inputs[1]  # Input 1 is gamma.\n\n    # If OpSlice size matches tensor size, use the entire tensor.  Otherwise,\n    # slice the tensor accordingly.\n    if start_index == 0 and size == gamma.shape.as_list()[-1]:\n      return gamma_l1_regularizer.GammaL1Regularizer(\n          gamma, self._gamma_threshold)\n    else:\n      # Note: this conversion is also attempted inside GammaL1Regularizer\n      # because it may be invoked from another call site.\n      gamma = tpu_util.maybe_convert_to_variable(gamma)\n      return gamma_l1_regularizer.GammaL1Regularizer(\n          gamma[start_index:start_index + size], self._gamma_threshold)\n'"
morph_net/framework/batch_norm_source_op_handler_test.py,8,"b'""""""Tests for BatchNormSourceOpHandler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport mock\nfrom morph_net.framework import batch_norm_source_op_handler\nfrom morph_net.framework import op_regularizer_manager as orm\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import framework\nfrom tensorflow.contrib import layers\n\narg_scope = framework.arg_scope\n\n_GAMMA_THRESHOLD = 0.001\n\n\nclass BatchNormSourceOpHandlerTest(tf.test.TestCase):\n\n  def _batch_norm_scope(self):\n    params = {\n        \'trainable\': True,\n        \'normalizer_fn\': layers.batch_norm,\n        \'normalizer_params\': {\n            \'scale\': True,\n        },\n    }\n\n    with arg_scope([layers.conv2d], **params) as sc:\n      return sc\n\n  def setUp(self):\n    super(BatchNormSourceOpHandlerTest, self).setUp()\n\n    # This tests a Conv2D -> BatchNorm -> ReLU chain of ops.\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      layers.conv2d(inputs, num_outputs=5, kernel_size=3, scope=\'conv1\')\n\n    g = tf.get_default_graph()\n\n    # Declare OpSlice and OpGroup for ops that are created in the test network.\n    self.batch_norm_op = g.get_operation_by_name(\n        \'conv1/BatchNorm/FusedBatchNormV3\')\n    self.batch_norm_op_slice = orm.OpSlice(self.batch_norm_op, orm.Slice(0, 5))\n    self.batch_norm_op_group = orm.OpGroup(self.batch_norm_op_slice)\n\n    self.conv_op = g.get_operation_by_name(\'conv1/Conv2D\')\n    self.conv_op_slice = orm.OpSlice(self.conv_op, orm.Slice(0, 5))\n    self.conv_op_group = orm.OpGroup(\n        self.conv_op_slice, omit_source_op_slices=[self.conv_op_slice])\n\n    self.relu_op = g.get_operation_by_name(\'conv1/Relu\')\n    self.relu_op_slice = orm.OpSlice(self.relu_op, orm.Slice(0, 5))\n    self.relu_op_group = orm.OpGroup(self.relu_op_slice)\n\n    self.gamma_op = g.get_operation_by_name(\'conv1/BatchNorm/gamma/read\')\n    self.gamma_op_slice = orm.OpSlice(self.gamma_op, orm.Slice(0, 5))\n    self.gamma_op_group = orm.OpGroup(\n        self.gamma_op_slice, omit_source_op_slices=[self.gamma_op_slice])\n\n    self.beta_op = g.get_operation_by_name(\'conv1/BatchNorm/beta/read\')\n    self.beta_op_slice = orm.OpSlice(self.beta_op, orm.Slice(0, 5))\n    self.beta_op_group = orm.OpGroup(\n        self.beta_op_slice, omit_source_op_slices=[self.beta_op_slice])\n\n    # Create custom mapping of OpSlice and OpGroup in manager.\n    self.mock_op_reg_manager = mock.create_autospec(orm.OpRegularizerManager)\n\n    def get_op_slices(op):\n      return self.op_slice_dict.get(op, [])\n\n    def get_op_group(op_slice):\n      return self.op_group_dict.get(op_slice)\n\n    self.mock_op_reg_manager.get_op_slices.side_effect = get_op_slices\n    self.mock_op_reg_manager.get_op_group.side_effect = get_op_group\n    self.mock_op_reg_manager.is_source_op.return_value = False\n    self.mock_op_reg_manager.ops = [\n        self.batch_norm_op, self.conv_op, self.relu_op, self.gamma_op,\n        self.beta_op]\n\n  def testAssignGrouping_NoNeighborGroups(self):\n    self.op_slice_dict = {\n        self.batch_norm_op: [self.batch_norm_op_slice],\n        self.conv_op: [self.conv_op_slice],\n        self.relu_op: [self.relu_op_slice],\n        self.gamma_op: [self.gamma_op_slice],\n        self.beta_op: [self.beta_op_slice],\n    }\n\n    # No neighbor ops have groups.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = batch_norm_source_op_handler.BatchNormSourceOpHandler(\n        _GAMMA_THRESHOLD)\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up op slice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.batch_norm_op)\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.conv_op)\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.relu_op)\n\n    # Verify manager creates OpGroup for batch norm op.\n    self.mock_op_reg_manager.create_op_group_for_op_slice(\n        self.batch_norm_op_slice)\n\n    # Verify manager groups batch norm with outputs and inputs.\n    self.mock_op_reg_manager.group_op_slices.assert_not_called()\n\n    # Verify manager processes grouping for input and output ops.\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.relu_op, self.conv_op, self.gamma_op,\n         self.beta_op])\n    self.mock_op_reg_manager.process_ops_last.assert_called_once_with(\n        [self.batch_norm_op])\n\n  def testAssignGrouping_ProcessNeighborGroups(self):\n    self.op_slice_dict = {\n        self.batch_norm_op: [self.batch_norm_op_slice],\n        self.conv_op: [self.conv_op_slice],\n        self.relu_op: [self.relu_op_slice],\n        self.gamma_op: [self.gamma_op_slice],\n        self.beta_op: [self.beta_op_slice],\n    }\n\n    # All ops have groups.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.conv_op_group,\n        self.relu_op_slice: self.relu_op_group,\n        self.gamma_op_slice: self.gamma_op_group,\n        self.beta_op_slice: self.beta_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = batch_norm_source_op_handler.BatchNormSourceOpHandler(\n        _GAMMA_THRESHOLD)\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up op slice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.batch_norm_op)\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.conv_op)\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.relu_op)\n\n    # Verify manager groups batch norm with outputs and inputs.\n    self.mock_op_reg_manager.group_op_slices.assert_has_calls(\n        [mock.call([self.batch_norm_op_slice, self.relu_op_slice]),\n         mock.call([self.batch_norm_op_slice, self.conv_op_slice,\n                    self.gamma_op_slice, self.beta_op_slice])])\n\n    # Verify manager does not reprocess any ops.\n    self.mock_op_reg_manager.process_ops.assert_not_called()\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n  def testAssignGrouping_ProcessNeighborGroupsWithSlices(self):\n    batch_norm_op_slice_0_2 = orm.OpSlice(self.batch_norm_op, orm.Slice(0, 2))\n    batch_norm_op_slice_2_3 = orm.OpSlice(self.batch_norm_op, orm.Slice(2, 1))\n    batch_norm_op_group1 = orm.OpGroup(batch_norm_op_slice_0_2)\n    batch_norm_op_group2 = orm.OpGroup(batch_norm_op_slice_2_3)\n\n    conv_op_slice_0_2 = orm.OpSlice(self.conv_op, orm.Slice(0, 2))\n    conv_op_slice_2_3 = orm.OpSlice(self.conv_op, orm.Slice(2, 1))\n    conv_op_group1 = orm.OpGroup(\n        conv_op_slice_0_2, omit_source_op_slices=[conv_op_slice_0_2])\n    conv_op_group2 = orm.OpGroup(\n        conv_op_slice_2_3, omit_source_op_slices=[conv_op_slice_2_3])\n\n    relu_op_slice_0_2 = orm.OpSlice(self.relu_op, orm.Slice(0, 2))\n    relu_op_slice_2_3 = orm.OpSlice(self.relu_op, orm.Slice(2, 1))\n    relu_op_group1 = orm.OpGroup(relu_op_slice_0_2)\n    relu_op_group2 = orm.OpGroup(relu_op_slice_2_3)\n\n    gamma_op_slice_0_2 = orm.OpSlice(self.gamma_op, orm.Slice(0, 2))\n    gamma_op_slice_2_3 = orm.OpSlice(self.gamma_op, orm.Slice(2, 1))\n    gamma_op_group1 = orm.OpGroup(\n        gamma_op_slice_0_2, omit_source_op_slices=[gamma_op_slice_0_2])\n    gamma_op_group2 = orm.OpGroup(\n        gamma_op_slice_2_3, omit_source_op_slices=[gamma_op_slice_2_3])\n\n    beta_op_slice_0_2 = orm.OpSlice(self.beta_op, orm.Slice(0, 2))\n    beta_op_slice_2_3 = orm.OpSlice(self.beta_op, orm.Slice(2, 1))\n    beta_op_group1 = orm.OpGroup(\n        beta_op_slice_0_2, omit_source_op_slices=[beta_op_slice_0_2])\n    beta_op_group2 = orm.OpGroup(\n        beta_op_slice_2_3, omit_source_op_slices=[beta_op_slice_2_3])\n\n    self.op_slice_dict = {\n        self.batch_norm_op: [batch_norm_op_slice_0_2, batch_norm_op_slice_2_3],\n        self.conv_op: [conv_op_slice_0_2, conv_op_slice_2_3],\n        self.relu_op: [relu_op_slice_0_2, relu_op_slice_2_3],\n        self.gamma_op: [gamma_op_slice_0_2, gamma_op_slice_2_3],\n        self.beta_op: [beta_op_slice_0_2, beta_op_slice_2_3],\n    }\n\n    # All OpSlice have groups.\n    self.op_group_dict = {\n        batch_norm_op_slice_0_2: batch_norm_op_group1,\n        batch_norm_op_slice_2_3: batch_norm_op_group2,\n        conv_op_slice_0_2: conv_op_group1,\n        conv_op_slice_2_3: conv_op_group2,\n        relu_op_slice_0_2: relu_op_group1,\n        relu_op_slice_2_3: relu_op_group2,\n        gamma_op_slice_0_2: gamma_op_group1,\n        gamma_op_slice_2_3: gamma_op_group2,\n        beta_op_slice_0_2: beta_op_group1,\n        beta_op_slice_2_3: beta_op_group2,\n    }\n\n    # Call handler to assign grouping.\n    handler = batch_norm_source_op_handler.BatchNormSourceOpHandler(\n        _GAMMA_THRESHOLD)\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up op slice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.batch_norm_op)\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.conv_op)\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.relu_op)\n\n    # Verify manager groups batch norm with outputs and inputs by slice.\n    self.mock_op_reg_manager.group_op_slices.assert_has_calls(\n        [mock.call([batch_norm_op_slice_0_2, relu_op_slice_0_2]),\n         mock.call([batch_norm_op_slice_0_2, conv_op_slice_0_2,\n                    gamma_op_slice_0_2, beta_op_slice_0_2]),\n         mock.call([batch_norm_op_slice_2_3, relu_op_slice_2_3]),\n         mock.call([batch_norm_op_slice_2_3, conv_op_slice_2_3,\n                    gamma_op_slice_2_3, beta_op_slice_2_3])])\n\n    # Verify manager does not reprocess any ops.\n    self.mock_op_reg_manager.process_ops.assert_not_called()\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n  def testAssignGrouping_NeighborsHaveSameGroup(self):\n    self.op_slice_dict = {\n        self.batch_norm_op: [self.batch_norm_op_slice],\n        self.conv_op: [self.batch_norm_op_slice],\n        self.relu_op: [self.batch_norm_op_slice],\n        self.gamma_op: [self.batch_norm_op_slice],\n        self.beta_op: [self.batch_norm_op_slice],\n    }\n\n    # All ops have the same group.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.batch_norm_op_group,\n        self.relu_op_slice: self.batch_norm_op_group,\n        self.gamma_op_slice: self.batch_norm_op_group,\n        self.beta_op_slice: self.batch_norm_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = batch_norm_source_op_handler.BatchNormSourceOpHandler(\n        _GAMMA_THRESHOLD)\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up op slice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.batch_norm_op)\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.conv_op)\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.relu_op)\n\n    # Verify manager does not group any ops.\n    self.mock_op_reg_manager.group_op_slices.assert_not_called()\n\n    # Verify manager does not process additional ops.\n    self.mock_op_reg_manager.process_ops.assert_not_called()\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n  def testAssignGrouping_NeighborsHaveSameGroup_ReprocessSources(self):\n    source_conv_op_group = orm.OpGroup(self.conv_op_slice)\n    self.op_slice_dict = {\n        self.batch_norm_op: [self.batch_norm_op_slice],\n        self.conv_op: [self.conv_op_slice],\n        self.relu_op: [self.relu_op_slice],\n        self.gamma_op: [self.gamma_op_slice],\n        self.beta_op: [self.beta_op_slice],\n    }\n\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: source_conv_op_group,\n        self.relu_op_slice: self.batch_norm_op_group,\n        self.gamma_op_slice: self.batch_norm_op_group,\n        self.beta_op_slice: self.batch_norm_op_group,\n    }\n\n    source_ops = (self.conv_op,)\n    def is_source_op(op):\n      return op in source_ops\n\n    self.mock_op_reg_manager.is_source_op.side_effect = is_source_op\n\n    # Call handler to assign grouping.\n    handler = batch_norm_source_op_handler.BatchNormSourceOpHandler(\n        _GAMMA_THRESHOLD)\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up op slice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.batch_norm_op)\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.conv_op)\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.relu_op)\n\n    # Verify manager groups batch norm with inputs and overrides source.\n    self.mock_op_reg_manager.group_op_slices.assert_called_once_with(\n        [self.batch_norm_op_slice, self.conv_op_slice, self.gamma_op_slice,\n         self.beta_op_slice])\n\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n  def testCreateRegularizer(self):\n    # Call handler to create regularizer.\n    handler = batch_norm_source_op_handler.BatchNormSourceOpHandler(\n        _GAMMA_THRESHOLD)\n    regularizer = handler.create_regularizer(self.batch_norm_op_slice)\n\n    # Verify regularizer is the gamma tensor.\n    g = tf.get_default_graph()\n    gamma_tensor = g.get_tensor_by_name(\'conv1/BatchNorm/gamma/read:0\')\n    self.assertEqual(gamma_tensor, regularizer._gamma)\n\n  def testCreateRegularizer_Sliced(self):\n    # Call handler to create regularizer.\n    handler = batch_norm_source_op_handler.BatchNormSourceOpHandler(\n        _GAMMA_THRESHOLD)\n    batch_norm_op_slice = orm.OpSlice(self.batch_norm_op, orm.Slice(0, 3))\n    regularizer = handler.create_regularizer(batch_norm_op_slice)\n\n    # Verify regularizer is the gamma tensor.\n    with self.cached_session():\n      # Initialize the gamma tensor to check value equality.\n      with tf.variable_scope(\'\', reuse=tf.AUTO_REUSE):\n        gamma_tensor = tf.get_variable(\'conv1/BatchNorm/gamma\')\n      init = tf.variables_initializer([gamma_tensor])\n      init.run()\n\n      # Verify regularizer is the sliced gamma tensor.\n      self.assertAllEqual(gamma_tensor.eval()[0:3],\n                          regularizer._gamma.eval())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/concat_and_slice_regularizers.py,4,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""OpRegularizers that concatenate and slice other OpRegularizers.\n\nWhen we have a concatenation op in the network, which concatenates several\ntensors, the regularizers of the concatenated ops (that is, the\nregularization_vector and the alive_vector) should be concatenated as well.\n\nSlicing is the complementary op - if regularizers Ra and Rb were concatenated\ninto a regularizer Rc, Ra and Rb can be obtained from Rc by slicing.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import generic_regularizers\nimport tensorflow.compat.v1 as tf\n\n\nclass ConcatRegularizer(generic_regularizers.OpRegularizer):\n  """"""An OpRegularizer that concatenates others, to reflect a Concat op.""""""\n\n  def __init__(self, regularizers_to_concatenate):\n    for r in regularizers_to_concatenate:\n      if not generic_regularizers.dimensions_are_compatible(r):\n        raise ValueError(\'Bad regularizer: dimensions are not compatible\')\n\n    self._alive_vector = tf.concat(\n        [r.alive_vector for r in regularizers_to_concatenate], 0)\n    self._regularization_vector = tf.concat(\n        [r.regularization_vector for r in regularizers_to_concatenate], 0)\n\n  @property\n  def regularization_vector(self):\n    return self._regularization_vector\n\n  @property\n  def alive_vector(self):\n    return self._alive_vector\n\n\nclass SlicingReferenceRegularizer(generic_regularizers.OpRegularizer):\n  """"""An OpRegularizer that slices a segment of another regularizer.\n\n  This is useful to complement the ConcatRegularizer. For example, suppose that\n  we have two ops, one with 3 outputs (Op1) and the other with 4 outputs (Op2).\n  Each has own regularizer, Reg1 and Reg2.\n\n  Now suppose that a concat op concatenated Op1 and Op2 into OpC. Reg1 and Reg2\n  should be concatenated to RegC. To make the situation more complicated, RegC\n  was grouped in a group lasso with another op in the graph, resulting in RegG.\n\n  Whan happens next? All references to RegC should obviously be replaced by\n  RegG. But what about Reg1? The latter could be the first 3 outputs of RegG,\n  and Reg2 would be the 4 last outputs of RegG.\n\n  SlicingReferenceRegularizer is a regularizer that picks a segment of outputs\n  form an existing OpRegularizer. When OpRegularizers are concatenated, they\n  are replaced by SlicingReferenceRegularizer.\n  """"""\n\n  def __init__(self, get_regularizer_to_slice, begin, size):\n    """"""Creates an instance.\n\n    Args:\n      get_regularizer_to_slice: A callable, such that get_regularizer_to_slice()\n        returns an OpRegularizer that has to be sliced.\n      begin: An integer, where to begin the slice.\n      size: An integer, the length of the slice (so the slice ends at\n        begin + size).\n    """"""\n    self._get_regularizer_to_slice = get_regularizer_to_slice\n    self._begin = begin\n    self._size = size\n    self._alive_vector = None\n    self._regularization_vector = None\n\n  @property\n  def regularization_vector(self):\n    if self._regularization_vector is None:\n      regularizer_to_slice = self._get_regularizer_to_slice()\n      self._regularization_vector = tf.slice(\n          regularizer_to_slice.regularization_vector, [self._begin],\n          [self._size])\n    return self._regularization_vector\n\n  @property\n  def alive_vector(self):\n    if self._alive_vector is None:\n      regularizer_to_slice = self._get_regularizer_to_slice()\n      assert regularizer_to_slice is not self\n      self._alive_vector = tf.slice(regularizer_to_slice.alive_vector,\n                                    [self._begin], [self._size])\n    return self._alive_vector\n'"
morph_net/framework/concat_and_slice_regularizers_test.py,2,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for framework.concat_and_slice_regularizers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import concat_and_slice_regularizers\nfrom morph_net.testing import op_regularizer_stub\nimport tensorflow.compat.v1 as tf\n\n\nclass ConcatAndSliceRegularizersTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(ConcatAndSliceRegularizersTest, self).setUp()\n    self._reg_vec1 = [0.1, 0.3, 0.6, 0.2]\n    self._alive_vec1 = [False, True, True, False]\n    self._reg_vec2 = [0.2, 0.4, 0.5]\n    self._alive_vec2 = [False, True, False]\n    self._reg1 = op_regularizer_stub.OpRegularizerStub(self._reg_vec1,\n                                                       self._alive_vec1)\n    self._reg2 = op_regularizer_stub.OpRegularizerStub(self._reg_vec2,\n                                                       self._alive_vec2)\n\n  def testConcatRegularizer(self):\n    concat_reg = concat_and_slice_regularizers.ConcatRegularizer(\n        [self._reg1, self._reg2])\n    with self.cached_session():\n      self.assertAllEqual(self._alive_vec1 + self._alive_vec2,\n                          concat_reg.alive_vector.eval())\n      self.assertAllClose(self._reg_vec1 + self._reg_vec2,\n                          concat_reg.regularization_vector.eval(), 1e-5)\n\n  def testSliceRegularizer(self):\n    concat_reg = concat_and_slice_regularizers.SlicingReferenceRegularizer(\n        lambda: self._reg1, 1, 2)\n    with self.cached_session():\n      self.assertAllEqual(self._alive_vec1[1:3],\n                          concat_reg.alive_vector.eval())\n      self.assertAllClose(self._reg_vec1[1:3],\n                          concat_reg.regularization_vector.eval(), 1e-5)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/concat_op_handler.py,4,"b'""""""OpHandler implementation for concat operations.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import grouping_op_handler\nfrom morph_net.framework import op_handler\nfrom morph_net.framework import op_handler_util\n\n\n# The axis arg of tf.concat is a constant tensor stored in the last element of\n# op.inputs. This function access the value of that tensor.\ndef _get_concat_op_axis(op):\n  return op.inputs[-1].op.get_attr(\'value\').int_val[0]\n\n\nclass ConcatOpHandler(op_handler.OpHandler):\n  """"""OpHandler implementation for concat operations.""""""\n\n  @property\n  def is_source_op(self):\n    return False\n\n  @property\n  def is_passthrough(self):\n    return True\n\n  def assign_grouping(self, op, op_reg_manager):\n    """"""Assign grouping to the given op and updates the manager.\n\n    Args:\n      op: tf.Operation to assign grouping to.\n      op_reg_manager: OpRegularizerManager to keep track of the grouping.\n    """"""\n    concat_axis = _get_concat_op_axis(op)\n    # Need to figure out the rank to know if axis is last.\n    rank = len(op.inputs[0].shape)  # Rank of the first input.\n\n    if concat_axis != -1 and concat_axis != rank - 1:\n      # Concat is actually grouping inputs!\n      handler = grouping_op_handler.GroupingOpHandler()\n      handler.assign_grouping(op, op_reg_manager)\n      return\n\n    # If concat is of the last dimension, this is a `standard` concat.\n    # TODO(a1): Consider refactoring this duplicated logic.\n    # Check if all input ops have groups, or tell the manager to process them.\n    input_ops = op_handler_util.get_input_ops(op, op_reg_manager)\n    input_ops_without_group = op_handler_util.get_ops_without_groups(\n        input_ops, op_reg_manager)\n\n    # Check if all output ops have groups, or tell the manager to process them.\n    output_ops = op_handler_util.get_output_ops(op, op_reg_manager)\n    output_ops_without_group = op_handler_util.get_ops_without_groups(\n        output_ops, op_reg_manager)\n\n    # Remove non-passthrough ops from outputs ops to group with.\n    output_ops = op_handler_util.remove_non_passthrough_ops(\n        output_ops, op_reg_manager)\n\n    # Only group with output ops that have the same size.  Process the ops that\n    # have mismatched size.\n    input_ops_to_group = input_ops\n    input_ops_to_process = input_ops_without_group\n    output_ops_to_group, output_ops_to_process = (\n        op_handler_util.separate_same_size_ops(op, output_ops))\n\n    # Also process ungrouped ops.\n    output_ops_to_process.extend(output_ops_without_group)\n\n    # Populate OpSlice data for all relevant ops.\n    concat_op_slices = op_reg_manager.get_op_slices(op)\n    input_op_slices, output_op_slices = self._get_input_output_op_slices(\n        input_ops_to_group, output_ops_to_group, op_reg_manager)\n\n    # Align op slices sizes if needed.\n    aligned_op_slice_sizes = op_handler_util.get_aligned_op_slice_sizes(\n        concat_op_slices, input_op_slices, output_op_slices)\n    op_handler_util.reslice_concat_ops(\n        input_ops_to_group, aligned_op_slice_sizes, op_reg_manager)\n    op_handler_util.reslice_ops(\n        output_ops_to_group + [op], aligned_op_slice_sizes, op_reg_manager)\n\n    # Repopulate OpSlice data, as ops may have been resliced.\n    input_op_slices, output_op_slices = self._get_input_output_op_slices(\n        input_ops_to_group, output_ops_to_group, op_reg_manager)\n\n    # Group aligned OpSlice.\n    op_handler_util.group_aligned_input_output_slices(\n        op, input_ops_to_process, output_ops_to_process, input_op_slices,\n        output_op_slices, aligned_op_slice_sizes, op_reg_manager)\n\n  def create_regularizer(self, _):\n    raise NotImplementedError(\'Not a source op.\')\n\n  def _get_input_output_op_slices(self, input_ops, output_ops, op_reg_manager):\n    """"""Returns op slices for inputs and outputs.\n\n    Args:\n      input_ops: List of tf.Operation.\n      output_ops: List of tf.Operation.\n      op_reg_manager: OpRegularizerManager to keep track of the grouping.\n\n    Returns:\n      Tuple of (input_op_slices, output_op_slices), where each element is a list\n      of list of OpSlice with a list per op.\n    """"""\n    input_op_slices = op_handler_util.get_concat_input_op_slices(\n        input_ops, op_reg_manager)\n    output_op_slices = op_handler_util.get_op_slices(\n        [output_op for output_op in output_ops\n         if op_reg_manager.is_passthrough(output_op)], op_reg_manager)\n    return (input_op_slices, output_op_slices)\n'"
morph_net/framework/concat_op_handler_test.py,13,"b'""""""Tests for concat_op_handler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport mock\nfrom morph_net.framework import concat_op_handler\nfrom morph_net.framework import op_regularizer_manager as orm\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import framework\nfrom tensorflow.contrib import layers\n\narg_scope = framework.arg_scope\n\n\nclass ConcatOpHandlerTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(ConcatOpHandlerTest, self).setUp()\n\n    # This tests 3 Conv2D ops being concatenated.\n    inputs = tf.zeros([2, 4, 4, 3])\n    c1 = layers.conv2d(inputs, num_outputs=5, kernel_size=3, scope=\'conv1\')\n    c2 = layers.conv2d(inputs, num_outputs=6, kernel_size=3, scope=\'conv2\')\n    c3 = layers.conv2d(inputs, num_outputs=7, kernel_size=3, scope=\'conv3\')\n    net = tf.concat([c1, c2, c3], axis=3)\n    layers.batch_norm(net)\n\n    g = tf.get_default_graph()\n\n    # Declare OpSlice and OpGroup for ops of interest.\n    self.concat_op = g.get_operation_by_name(\'concat\')\n    self.concat_op_slice = orm.OpSlice(self.concat_op, orm.Slice(0, 18))\n    self.concat_op_slice_0_5 = orm.OpSlice(self.concat_op, orm.Slice(0, 5))\n    self.concat_op_slice_5_11 = orm.OpSlice(self.concat_op, orm.Slice(5, 6))\n    self.concat_op_slice_11_18 = orm.OpSlice(self.concat_op, orm.Slice(11, 7))\n    self.concat_op_group1 = orm.OpGroup(\n        self.concat_op_slice_0_5,\n        omit_source_op_slices=[self.concat_op_slice_0_5])\n    self.concat_op_group2 = orm.OpGroup(\n        self.concat_op_slice_5_11,\n        omit_source_op_slices=[self.concat_op_slice_5_11])\n    self.concat_op_group3 = orm.OpGroup(\n        self.concat_op_slice_11_18,\n        omit_source_op_slices=[self.concat_op_slice_11_18])\n\n    self.relu1_op = g.get_operation_by_name(\'conv1/Relu\')\n    self.relu1_op_slice = orm.OpSlice(self.relu1_op, orm.Slice(0, 5))\n    self.relu1_op_group = orm.OpGroup(\n        self.relu1_op_slice, omit_source_op_slices=[self.relu1_op_slice])\n\n    self.relu2_op = g.get_operation_by_name(\'conv2/Relu\')\n    self.relu2_op_slice = orm.OpSlice(self.relu2_op, orm.Slice(0, 6))\n    self.relu2_op_group = orm.OpGroup(\n        self.relu2_op_slice, omit_source_op_slices=[self.relu2_op_slice])\n\n    self.relu3_op = g.get_operation_by_name(\'conv3/Relu\')\n    self.relu3_op_slice = orm.OpSlice(self.relu3_op, orm.Slice(0, 7))\n    self.relu3_op_group = orm.OpGroup(\n        self.relu3_op_slice, omit_source_op_slices=[self.relu3_op_slice])\n\n    self.axis_op = g.get_operation_by_name(\'concat/axis\')\n\n    self.batch_norm_op = g.get_operation_by_name(\'BatchNorm/FusedBatchNormV3\')\n    self.batch_norm_op_slice = orm.OpSlice(self.batch_norm_op, orm.Slice(0, 18))\n    self.batch_norm_op_group = orm.OpGroup(\n        self.batch_norm_op_slice,\n        omit_source_op_slices=[self.batch_norm_op_slice])\n    self.batch_norm_op_slice_0_5 = orm.OpSlice(\n        self.batch_norm_op, orm.Slice(0, 5))\n    self.batch_norm_op_slice_5_11 = orm.OpSlice(\n        self.batch_norm_op, orm.Slice(5, 6))\n    self.batch_norm_op_slice_11_18 = orm.OpSlice(\n        self.batch_norm_op, orm.Slice(11, 7))\n    self.batch_norm_op_group1 = orm.OpGroup(\n        self.batch_norm_op_slice_0_5,\n        omit_source_op_slices=[self.batch_norm_op_slice_0_5])\n    self.batch_norm_op_group2 = orm.OpGroup(\n        self.batch_norm_op_slice_5_11,\n        omit_source_op_slices=[self.batch_norm_op_slice_5_11])\n    self.batch_norm_op_group3 = orm.OpGroup(\n        self.batch_norm_op_slice_11_18,\n        omit_source_op_slices=[self.batch_norm_op_slice_11_18])\n\n    # Create mock OpRegularizerManager with custom mapping of OpSlice and\n    # OpGroup.\n    self.mock_op_reg_manager = mock.create_autospec(orm.OpRegularizerManager)\n\n    def get_op_slices(op):\n      return self.op_slice_dict.get(op, [])\n\n    def get_op_group(op_slice):\n      return self.op_group_dict.get(op_slice)\n\n    # Update op_slice_dict when an op is sliced.\n    def slice_op(op, _):\n      if op == self.batch_norm_op:\n        self.op_slice_dict[self.batch_norm_op] = [\n            self.batch_norm_op_slice_0_5,\n            self.batch_norm_op_slice_5_11,\n            self.batch_norm_op_slice_11_18]\n      if op == self.concat_op:\n        self.op_slice_dict[self.concat_op] = [\n            self.concat_op_slice_0_5,\n            self.concat_op_slice_5_11,\n            self.concat_op_slice_11_18]\n\n    self.mock_op_reg_manager.get_op_slices.side_effect = get_op_slices\n    self.mock_op_reg_manager.get_op_group.side_effect = get_op_group\n    self.mock_op_reg_manager.is_source_op.return_value = False\n    self.mock_op_reg_manager.slice_op.side_effect = slice_op\n    self.mock_op_reg_manager.is_passthrough.return_value = True\n    self.mock_op_reg_manager.ops = [\n        self.concat_op, self.relu1_op, self.relu2_op, self.relu3_op,\n        self.batch_norm_op]\n\n  def testAssignGrouping_AllNeighborsGrouped_SlicesAligned(self):\n    # In this test, the output op (batch norm) has size 18 and is sliced into\n    # sizes [5, 6, 7] which matches the Conv2D sizes which are [5, 6, 7].\n\n    # Map ops to slices.  Batch norm op is composed of multiple slices.\n    self.op_slice_dict = {\n        self.relu1_op: [self.relu1_op_slice],\n        self.relu2_op: [self.relu2_op_slice],\n        self.relu3_op: [self.relu3_op_slice],\n        self.concat_op: [self.concat_op_slice],\n        self.batch_norm_op: [self.batch_norm_op_slice_0_5,\n                             self.batch_norm_op_slice_5_11,\n                             self.batch_norm_op_slice_11_18],\n    }\n\n    # Map each slice to a group.\n    self.op_group_dict = {\n        self.relu1_op_slice: self.relu1_op_group,\n        self.relu2_op_slice: self.relu2_op_group,\n        self.relu3_op_slice: self.relu3_op_group,\n        self.batch_norm_op_slice_0_5: self.batch_norm_op_group1,\n        self.batch_norm_op_slice_5_11: self.batch_norm_op_group2,\n        self.batch_norm_op_slice_11_18: self.batch_norm_op_group3,\n    }\n\n    # Call handler to assign grouping.\n    handler = concat_op_handler.ConcatOpHandler()\n    handler.assign_grouping(self.concat_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Initial slice data.\n         mock.call(self.concat_op),\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Reslicing.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         mock.call(self.concat_op),\n         # Refreshing slice data.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Group concat op.\n         mock.call(self.concat_op)])\n\n    # Verify manager only slices the concat op.\n    self.mock_op_reg_manager.slice_op.assert_called_once_with(\n        self.concat_op, [5, 6, 7])\n\n    # Verify manager groups the new slices.\n    self.mock_op_reg_manager.group_op_slices.assert_has_calls(\n        [mock.call([self.concat_op_slice_0_5, self.relu1_op_slice,\n                    self.batch_norm_op_slice_0_5]),\n         mock.call([self.concat_op_slice_5_11, self.relu2_op_slice,\n                    self.batch_norm_op_slice_5_11]),\n         mock.call([self.concat_op_slice_11_18, self.relu3_op_slice,\n                    self.batch_norm_op_slice_11_18])])\n\n  def testAssignGrouping_AllNeighborsGrouped_SlicesAligned_SameGroup(self):\n    # This test verifies that no slicing or grouping occurs.\n\n    # Map ops to slices.  Batch norm op is composed of multiple slices.\n    self.op_slice_dict = {\n        self.relu1_op: [self.relu1_op_slice],\n        self.relu2_op: [self.relu2_op_slice],\n        self.relu3_op: [self.relu3_op_slice],\n        self.concat_op: [self.concat_op_slice_0_5, self.concat_op_slice_5_11,\n                         self.concat_op_slice_11_18],\n        self.batch_norm_op: [self.batch_norm_op_slice_0_5,\n                             self.batch_norm_op_slice_5_11,\n                             self.batch_norm_op_slice_11_18],\n    }\n\n    # Map each slice to a group.  Corresponding op slices have the same group.\n    self.op_group_dict = {\n        self.relu1_op_slice: self.batch_norm_op_group1,\n        self.relu2_op_slice: self.batch_norm_op_group2,\n        self.relu3_op_slice: self.batch_norm_op_group3,\n        self.concat_op_slice_0_5: self.batch_norm_op_group1,\n        self.concat_op_slice_5_11: self.batch_norm_op_group2,\n        self.concat_op_slice_11_18: self.batch_norm_op_group3,\n        self.batch_norm_op_slice_0_5: self.batch_norm_op_group1,\n        self.batch_norm_op_slice_5_11: self.batch_norm_op_group2,\n        self.batch_norm_op_slice_11_18: self.batch_norm_op_group3,\n    }\n\n    # Call handler to assign grouping.\n    handler = concat_op_handler.ConcatOpHandler()\n    handler.assign_grouping(self.concat_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Initial slice data.\n         mock.call(self.concat_op),\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Reslicing.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         mock.call(self.concat_op),\n         # Refreshing slice data.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Group concat op.\n         mock.call(self.concat_op)])\n\n    # Verify manager does not slice any ops.\n    self.mock_op_reg_manager.slice_op.assert_not_called()\n\n    # Verify manager does not group any ops.\n    self.mock_op_reg_manager.group_op_slices.assert_not_called()\n\n  def testAssignGrouping_AllNeighborsGrouped_OutputSlicesNotAligned(self):\n    # The output (batch norm) has sizes [9, 4, 5] which are not aligned.  This\n    # test verifies that the concat, batch norm, and Conv2D ops are sliced in\n    # alignment.\n    concat_op_slice_0_5 = orm.OpSlice(self.concat_op, orm.Slice(0, 5))\n    concat_op_slice_5_9 = orm.OpSlice(self.concat_op, orm.Slice(5, 4))\n    concat_op_slice_9_11 = orm.OpSlice(self.concat_op, orm.Slice(9, 2))\n    concat_op_slice_11_13 = orm.OpSlice(self.concat_op, orm.Slice(11, 2))\n    concat_op_slice_13_18 = orm.OpSlice(self.concat_op, orm.Slice(13, 5))\n\n    relu2_op_slice_0_4 = orm.OpSlice(self.relu2_op, orm.Slice(0, 4))\n    relu2_op_slice_4_6 = orm.OpSlice(self.relu2_op, orm.Slice(4, 2))\n\n    relu3_op_slice_0_2 = orm.OpSlice(self.relu3_op, orm.Slice(0, 2))\n    relu3_op_slice_2_7 = orm.OpSlice(self.relu3_op, orm.Slice(2, 5))\n\n    batch_norm_op_slice_0_9 = orm.OpSlice(self.batch_norm_op, orm.Slice(0, 9))\n    batch_norm_op_group1 = orm.OpGroup(\n        batch_norm_op_slice_0_9,\n        omit_source_op_slices=[batch_norm_op_slice_0_9])\n    batch_norm_op_slice_9_13 = orm.OpSlice(self.batch_norm_op, orm.Slice(9, 4))\n    batch_norm_op_group2 = orm.OpGroup(\n        batch_norm_op_slice_9_13,\n        omit_source_op_slices=[batch_norm_op_slice_9_13])\n    batch_norm_op_slice_13_18 = orm.OpSlice(\n        self.batch_norm_op, orm.Slice(13, 5))\n    batch_norm_op_group3 = orm.OpGroup(\n        batch_norm_op_slice_13_18,\n        omit_source_op_slices=[batch_norm_op_slice_13_18])\n    batch_norm_op_slice_0_5 = orm.OpSlice(self.batch_norm_op, orm.Slice(0, 5))\n    batch_norm_op_group4 = orm.OpGroup(\n        batch_norm_op_slice_0_5,\n        omit_source_op_slices=[batch_norm_op_slice_0_5])\n    batch_norm_op_slice_5_9 = orm.OpSlice(self.batch_norm_op, orm.Slice(5, 4))\n    batch_norm_op_group5 = orm.OpGroup(\n        batch_norm_op_slice_5_9,\n        omit_source_op_slices=[batch_norm_op_slice_5_9])\n    batch_norm_op_slice_9_11 = orm.OpSlice(self.batch_norm_op, orm.Slice(9, 2))\n    batch_norm_op_group6 = orm.OpGroup(\n        batch_norm_op_slice_9_11,\n        omit_source_op_slices=[batch_norm_op_slice_9_11])\n    batch_norm_op_slice_11_13 = orm.OpSlice(\n        self.batch_norm_op, orm.Slice(11, 2))\n    batch_norm_op_group7 = orm.OpGroup(\n        batch_norm_op_slice_11_13,\n        omit_source_op_slices=[batch_norm_op_slice_11_13])\n    batch_norm_op_slice_13_18 = orm.OpSlice(\n        self.batch_norm_op, orm.Slice(11, 5))\n    batch_norm_op_group8 = orm.OpGroup(\n        batch_norm_op_slice_13_18,\n        omit_source_op_slices=[batch_norm_op_slice_13_18])\n\n    # Map ops to slices.  Batch norm op is composed of multiple slices.\n    self.op_slice_dict = {\n        self.relu1_op: [self.relu1_op_slice],\n        self.relu2_op: [self.relu2_op_slice],\n        self.relu3_op: [self.relu3_op_slice],\n        self.concat_op: [self.concat_op_slice],\n        self.batch_norm_op: [batch_norm_op_slice_0_9, batch_norm_op_slice_9_13,\n                             batch_norm_op_slice_13_18],\n    }\n\n    # Map each slice to a group.\n    self.op_group_dict = {\n        self.relu1_op_slice: self.relu1_op_group,\n        self.relu2_op_slice: self.relu2_op_group,\n        self.relu3_op_slice: self.relu3_op_group,\n        batch_norm_op_slice_0_9: batch_norm_op_group1,\n        batch_norm_op_slice_9_13: batch_norm_op_group2,\n        batch_norm_op_slice_13_18: batch_norm_op_group3,\n        batch_norm_op_slice_0_5: batch_norm_op_group4,\n        batch_norm_op_slice_5_9: batch_norm_op_group5,\n        batch_norm_op_slice_9_11: batch_norm_op_group6,\n        batch_norm_op_slice_11_13: batch_norm_op_group7,\n        batch_norm_op_slice_13_18: batch_norm_op_group8,\n    }\n\n    # Update op_slice_dict when an op is sliced.\n    def slice_op(op, _):\n      if op == self.batch_norm_op:\n        self.op_slice_dict[self.batch_norm_op] = [\n            batch_norm_op_slice_0_5,\n            batch_norm_op_slice_5_9,\n            batch_norm_op_slice_9_11,\n            batch_norm_op_slice_11_13,\n            batch_norm_op_slice_13_18]\n      if op == self.concat_op:\n        self.op_slice_dict[self.concat_op] = [\n            concat_op_slice_0_5,\n            concat_op_slice_5_9,\n            concat_op_slice_9_11,\n            concat_op_slice_11_13,\n            concat_op_slice_13_18]\n      if op == self.relu2_op:\n        self.op_slice_dict[self.relu2_op] = [\n            relu2_op_slice_0_4,\n            relu2_op_slice_4_6]\n      if op == self.relu3_op:\n        self.op_slice_dict[self.relu3_op] = [\n            relu3_op_slice_0_2,\n            relu3_op_slice_2_7]\n\n    self.mock_op_reg_manager.slice_op.side_effect = slice_op\n\n    # Call handler to assign grouping.\n    handler = concat_op_handler.ConcatOpHandler()\n    handler.assign_grouping(self.concat_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Initial slice data.\n         mock.call(self.concat_op),\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Reslicing.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         mock.call(self.concat_op),\n         # Refreshing slice data.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Group concat op.\n         mock.call(self.concat_op)])\n\n    # Verify manager slices ops that do not have aligned OpSlice sizes.\n    self.mock_op_reg_manager.slice_op.assert_has_calls(\n        [mock.call(self.relu2_op, [4, 2]),\n         mock.call(self.relu3_op, [2, 5]),\n         mock.call(self.batch_norm_op, [5, 4, 2, 2, 5]),\n         mock.call(self.concat_op, [5, 4, 2, 2, 5])])\n\n    # Verify manager groups the new slices.\n    self.mock_op_reg_manager.group_op_slices.assert_has_calls(\n        [mock.call([concat_op_slice_0_5, self.relu1_op_slice,\n                    batch_norm_op_slice_0_5]),\n         mock.call([concat_op_slice_5_9, relu2_op_slice_0_4,\n                    batch_norm_op_slice_5_9]),\n         mock.call([concat_op_slice_9_11, relu2_op_slice_4_6,\n                    batch_norm_op_slice_9_11]),\n         mock.call([concat_op_slice_11_13, relu3_op_slice_0_2,\n                    batch_norm_op_slice_11_13]),\n         mock.call([concat_op_slice_13_18, relu3_op_slice_2_7,\n                    batch_norm_op_slice_13_18])])\n\n  def testAssignGrouping_AllNeighborsGrouped_InputSlicesNotAligned(self):\n    # In this test, the op c2 has size 6 but is split into 2 slices of size 3.\n    # The concat op (and its output, the batch norm) both have size 18.  This\n    # test verifies that the concat and batch norm are sliced according to the\n    # sizes of c1, c2, and c3, and takes into account that c2 is also composed\n    # of multiple slices.\n    concat_op_slice_0_5 = orm.OpSlice(self.concat_op, orm.Slice(0, 5))\n    concat_op_slice_5_8 = orm.OpSlice(self.concat_op, orm.Slice(5, 3))\n    concat_op_slice_8_11 = orm.OpSlice(self.concat_op, orm.Slice(8, 3))\n    concat_op_slice_11_18 = orm.OpSlice(self.concat_op, orm.Slice(11, 7))\n\n    relu2_op_slice_0_3 = orm.OpSlice(self.relu2_op, orm.Slice(0, 3))\n    relu2_op_slice_3_6 = orm.OpSlice(self.relu2_op, orm.Slice(3, 3))\n    relu2_op_group1 = orm.OpGroup(\n        relu2_op_slice_0_3, omit_source_op_slices=[relu2_op_slice_0_3])\n    relu2_op_group2 = orm.OpGroup(\n        relu2_op_slice_3_6, omit_source_op_slices=[relu2_op_slice_3_6])\n\n    batch_norm_op_slice = orm.OpSlice(self.batch_norm_op, orm.Slice(0, 18))\n    batch_norm_op_group = orm.OpGroup(\n        batch_norm_op_slice, omit_source_op_slices=[batch_norm_op_slice])\n    batch_norm_op_slice_0_5 = orm.OpSlice(self.batch_norm_op, orm.Slice(0, 5))\n    batch_norm_op_group1 = orm.OpGroup(\n        batch_norm_op_slice_0_5,\n        omit_source_op_slices=[batch_norm_op_slice_0_5])\n    batch_norm_op_slice_5_8 = orm.OpSlice(self.batch_norm_op, orm.Slice(5, 3))\n    batch_norm_op_group2 = orm.OpGroup(\n        batch_norm_op_slice_5_8,\n        omit_source_op_slices=[batch_norm_op_slice_5_8])\n    batch_norm_op_slice_8_11 = orm.OpSlice(self.batch_norm_op, orm.Slice(8, 3))\n    batch_norm_op_group3 = orm.OpGroup(\n        batch_norm_op_slice_8_11,\n        omit_source_op_slices=[batch_norm_op_slice_8_11])\n    batch_norm_op_slice_11_18 = orm.OpSlice(\n        self.batch_norm_op, orm.Slice(11, 7))\n    batch_norm_op_group4 = orm.OpGroup(\n        batch_norm_op_slice_11_18,\n        omit_source_op_slices=[batch_norm_op_slice_11_18])\n\n    # Map ops to slices.  The op c2 is composed of multiple slices.\n    self.op_slice_dict = {\n        self.relu1_op: [self.relu1_op_slice],\n        self.relu2_op: [relu2_op_slice_0_3, relu2_op_slice_3_6],\n        self.relu3_op: [self.relu3_op_slice],\n        self.concat_op: [self.concat_op_slice],\n        self.batch_norm_op: [batch_norm_op_slice],\n    }\n\n    # Map each slice to a group.\n    self.op_group_dict = {\n        self.relu1_op_slice: self.relu1_op_group,\n        relu2_op_slice_0_3: relu2_op_group1,\n        relu2_op_slice_3_6: relu2_op_group2,\n        self.relu3_op_slice: self.relu3_op_group,\n        batch_norm_op_slice: batch_norm_op_group,\n        batch_norm_op_slice_0_5: batch_norm_op_group1,\n        batch_norm_op_slice_5_8: batch_norm_op_group2,\n        batch_norm_op_slice_8_11: batch_norm_op_group3,\n        batch_norm_op_slice_11_18: batch_norm_op_group4,\n    }\n\n    # Update op_slice_dict when an op is sliced.\n    def slice_op(op, _):\n      if op == self.batch_norm_op:\n        self.op_slice_dict[self.batch_norm_op] = [\n            batch_norm_op_slice_0_5,\n            batch_norm_op_slice_5_8,\n            batch_norm_op_slice_8_11,\n            batch_norm_op_slice_11_18]\n      if op == self.concat_op:\n        self.op_slice_dict[self.concat_op] = [\n            concat_op_slice_0_5,\n            concat_op_slice_5_8,\n            concat_op_slice_8_11,\n            concat_op_slice_11_18]\n\n    self.mock_op_reg_manager.slice_op.side_effect = slice_op\n\n    # Call handler to assign grouping.\n    handler = concat_op_handler.ConcatOpHandler()\n    handler.assign_grouping(self.concat_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Initial slice data.\n         mock.call(self.concat_op),\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Reslicing.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         mock.call(self.concat_op),\n         # Refreshing slice data.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Group concat op.\n         mock.call(self.concat_op)])\n\n    # Verify manager slices ops that do not have aligned OpSlice sizes.\n    self.mock_op_reg_manager.slice_op.assert_has_calls(\n        [mock.call(self.batch_norm_op, [5, 3, 3, 7]),\n         mock.call(self.concat_op, [5, 3, 3, 7])])\n\n    # Verify manager groups the new slices.\n    self.mock_op_reg_manager.group_op_slices.assert_has_calls(\n        [mock.call([concat_op_slice_0_5, self.relu1_op_slice,\n                    batch_norm_op_slice_0_5]),\n         mock.call([concat_op_slice_5_8, relu2_op_slice_0_3,\n                    batch_norm_op_slice_5_8]),\n         mock.call([concat_op_slice_8_11, relu2_op_slice_3_6,\n                    batch_norm_op_slice_8_11]),\n         mock.call([concat_op_slice_11_18, self.relu3_op_slice,\n                    batch_norm_op_slice_11_18])])\n\n  def testAssignGrouping_InputsGrouped(self):\n    # In this test, only the input ops are grouped.  The concat and batch norm\n    # ops will be sliced according to the input sizes.\n\n    # Map ops to slices.\n    self.op_slice_dict = {\n        self.relu1_op: [self.relu1_op_slice],\n        self.relu2_op: [self.relu2_op_slice],\n        self.relu3_op: [self.relu3_op_slice],\n        self.concat_op: [self.concat_op_slice],\n        self.batch_norm_op: [self.batch_norm_op_slice],\n    }\n\n    # Map each slice to a group.  Batch norm (output) is not grouped.\n    self.op_group_dict = {\n        self.relu1_op_slice: self.relu1_op_group,\n        self.relu2_op_slice: self.relu2_op_group,\n        self.relu3_op_slice: self.relu3_op_group,\n        self.concat_op_slice_0_5: self.concat_op_group1,\n        self.concat_op_slice_5_11: self.concat_op_group2,\n        self.concat_op_slice_11_18: self.concat_op_group3,\n    }\n\n    # Call handler to assign grouping.\n    handler = concat_op_handler.ConcatOpHandler()\n    handler.assign_grouping(self.concat_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Initial slice data.\n         mock.call(self.concat_op),\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Reslicing.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         mock.call(self.concat_op),\n         # Refreshing slice data.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Group concat op.\n         mock.call(self.concat_op)])\n\n    # Verify manager slices ops that do not have aligned OpSlice sizes.\n    self.mock_op_reg_manager.slice_op.assert_has_calls(\n        [mock.call(self.batch_norm_op, [5, 6, 7]),\n         mock.call(self.concat_op, [5, 6, 7])])\n\n    # Verify manager groups the new slices.\n    self.mock_op_reg_manager.group_op_slices.assert_has_calls(\n        [mock.call([self.concat_op_slice_0_5, self.relu1_op_slice]),\n         mock.call([self.concat_op_slice_5_11, self.relu2_op_slice]),\n         mock.call([self.concat_op_slice_11_18, self.relu3_op_slice])])\n\n    # Verify manager adds ops to processing queue.\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.batch_norm_op])\n\n  def testAssignGrouping_OutputsGrouped(self):\n    # In this test, only the output ops are grouped.  The concat and batch norm\n    # ops will be sliced according to the input sizes.\n\n    # Map ops to slices.\n    self.op_slice_dict = {\n        self.relu1_op: [self.relu1_op_slice],\n        self.relu2_op: [self.relu2_op_slice],\n        self.relu3_op: [self.relu3_op_slice],\n        self.concat_op: [self.concat_op_slice],\n        self.batch_norm_op: [self.batch_norm_op_slice],\n    }\n\n    # Map each slice to a group.  Input ops (ReLU) are not grouped.\n    self.op_group_dict = {\n        self.concat_op_slice_0_5: self.concat_op_group1,\n        self.concat_op_slice_5_11: self.concat_op_group2,\n        self.concat_op_slice_11_18: self.concat_op_group3,\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.batch_norm_op_slice_0_5: self.batch_norm_op_group1,\n        self.batch_norm_op_slice_5_11: self.batch_norm_op_group2,\n        self.batch_norm_op_slice_11_18: self.batch_norm_op_group3,\n    }\n\n    # Call handler to assign grouping.\n    handler = concat_op_handler.ConcatOpHandler()\n    handler.assign_grouping(self.concat_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Initial slice data.\n         mock.call(self.concat_op),\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Reslicing.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         mock.call(self.concat_op),\n         # Refreshing slice data.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op)])\n\n    # Verify manager slices ops that do not have aligned OpSlice sizes.\n    self.mock_op_reg_manager.slice_op.assert_has_calls(\n        [mock.call(self.batch_norm_op, [5, 6, 7]),\n         mock.call(self.concat_op, [5, 6, 7])])\n\n    # Verify manager does not group ops.\n    self.mock_op_reg_manager.group_op_slices.assert_not_called()\n\n    # Verify manager adds ops to processing queue.\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.relu1_op, self.relu2_op, self.relu3_op])\n    self.mock_op_reg_manager.process_ops_last.assert_called_once_with(\n        [self.concat_op])\n\n  def testAssignGrouping_NoNeighborGroups(self):\n    # In this test, both the inputs and outputs are missing groups.  The concat\n    # and batch norm are sliced, but grouping does not happen until the inputs\n    # and outputs are grouped.\n\n    # Map ops to slices.\n    self.op_slice_dict = {\n        self.relu1_op: [self.relu1_op_slice],\n        self.relu2_op: [self.relu2_op_slice],\n        self.relu3_op: [self.relu3_op_slice],\n        self.concat_op: [self.concat_op_slice],\n        self.batch_norm_op: [self.batch_norm_op_slice],\n    }\n\n    # No neighbor slices are grouped.\n    self.op_group_dict = {\n        self.concat_op_slice_0_5: self.concat_op_group1,\n        self.concat_op_slice_5_11: self.concat_op_group2,\n        self.concat_op_slice_11_18: self.concat_op_group3,\n    }\n\n    # Call handler to assign grouping.\n    handler = concat_op_handler.ConcatOpHandler()\n    handler.assign_grouping(self.concat_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Initial slice data.\n         mock.call(self.concat_op),\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Reslicing.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         mock.call(self.concat_op),\n         # Refreshing slice data.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op)])\n\n    # Verify manager slices ops that do not have aligned OpSlice sizes.\n    self.mock_op_reg_manager.slice_op.assert_has_calls(\n        [mock.call(self.batch_norm_op, [5, 6, 7]),\n         mock.call(self.concat_op, [5, 6, 7])])\n\n    # Verify manager doesn\'t group anything.\n    self.mock_op_reg_manager.group_op_slices.assert_not_called()\n\n    # Verify manager adds ops to processing queue.\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.batch_norm_op, self.relu1_op, self.relu2_op, self.relu3_op])\n    self.mock_op_reg_manager.process_ops_last.assert_called_once_with(\n        [self.concat_op])\n\n  def testGetInputOutputOpSlices(self):\n    # Map ops to slices.\n    self.op_slice_dict = {\n        self.relu1_op: [self.relu1_op_slice],\n        self.relu2_op: [self.relu2_op_slice],\n        self.relu3_op: [self.relu3_op_slice],\n        self.concat_op: [self.concat_op_slice],\n        self.batch_norm_op: [self.batch_norm_op_slice],\n    }\n\n    input_ops = [self.relu1_op, self.relu2_op, self.relu3_op, self.axis_op]\n    output_ops = [self.batch_norm_op]\n\n    expected_input_op_slices = [\n        [self.relu1_op_slice, self.relu2_op_slice, self.relu3_op_slice]]\n    expected_output_op_slices = [\n        [self.batch_norm_op_slice]]\n\n    # Instantiate handler.\n    handler = concat_op_handler.ConcatOpHandler()\n\n    self.assertEqual(\n        (expected_input_op_slices, expected_output_op_slices),\n        handler._get_input_output_op_slices(input_ops, output_ops,\n                                            self.mock_op_reg_manager))\n\n\nclass GroupingConcatOpHandlerTest(tf.test.TestCase):\n\n  def _get_scope(self):\n    params = {\n        \'trainable\': True,\n        \'normalizer_fn\': layers.batch_norm,\n        \'normalizer_params\': {\n            \'scale\': True,\n        },\n    }\n\n    with arg_scope([layers.conv2d], **params) as sc:\n      return sc\n\n  def setUp(self):\n    super(GroupingConcatOpHandlerTest, self).setUp()\n\n    # This tests 3 Conv2D ops being concatenated.\n    inputs = tf.zeros([2, 4, 4, 3])\n    with framework.arg_scope(self._get_scope()):\n      c1 = layers.conv2d(inputs, num_outputs=6, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(inputs, num_outputs=6, kernel_size=3, scope=\'conv2\')\n      c3 = layers.conv2d(inputs, num_outputs=6, kernel_size=3, scope=\'conv3\')\n      net = tf.concat([c1, c2, c3], axis=2)\n      layers.batch_norm(net)\n\n    g = tf.get_default_graph()\n\n    # Declare OpSlice and OpGroup for ops of interest.\n    self.concat_op = g.get_operation_by_name(\'concat\')\n    self.concat_op_slice = orm.OpSlice(self.concat_op, orm.Slice(0, 6))\n    self.concat_op_group = orm.OpGroup(\n        self.concat_op_slice,\n        omit_source_op_slices=[self.concat_op_slice])\n\n    self.relu1_op = g.get_operation_by_name(\'conv1/Relu\')\n    self.relu1_op_slice = orm.OpSlice(self.relu1_op, orm.Slice(0, 6))\n    self.relu1_op_group = orm.OpGroup(\n        self.relu1_op_slice, omit_source_op_slices=[self.relu1_op_slice])\n\n    self.relu2_op = g.get_operation_by_name(\'conv2/Relu\')\n    self.relu2_op_slice = orm.OpSlice(self.relu2_op, orm.Slice(0, 6))\n    self.relu2_op_group = orm.OpGroup(\n        self.relu2_op_slice, omit_source_op_slices=[self.relu2_op_slice])\n\n    self.relu3_op = g.get_operation_by_name(\'conv3/Relu\')\n    self.relu3_op_slice = orm.OpSlice(self.relu3_op, orm.Slice(0, 6))\n    self.relu3_op_group = orm.OpGroup(\n        self.relu3_op_slice, omit_source_op_slices=[self.relu3_op_slice])\n\n    self.batch_norm_op = g.get_operation_by_name(\'BatchNorm/FusedBatchNormV3\')\n    self.batch_norm_op_slice = orm.OpSlice(self.batch_norm_op, orm.Slice(0, 6))\n    self.batch_norm_op_group = orm.OpGroup(\n        self.batch_norm_op_slice,\n        omit_source_op_slices=[self.batch_norm_op_slice])\n\n    self.concat_group = orm.OpGroup(\n        op_slice=None,\n        op_groups=[\n            self.batch_norm_op_group, self.concat_op_group, self.relu1_op_group,\n            self.relu2_op_group, self.relu3_op_group\n        ])\n\n    # Create mock OpRegularizerManager with custom mapping of OpSlice and\n    # OpGroup.\n    self.mock_op_reg_manager = mock.create_autospec(orm.OpRegularizerManager)\n\n    def get_op_slices(op):\n      return self.op_slice_dict.get(op, [])\n\n    def get_op_group(op_slice):\n      return self.op_group_dict.get(op_slice)\n\n    self.mock_op_reg_manager.get_op_slices.side_effect = get_op_slices\n    self.mock_op_reg_manager.get_op_group.side_effect = get_op_group\n    self.mock_op_reg_manager.is_source_op.return_value = False\n    self.mock_op_reg_manager.is_passthrough.return_value = True\n    self.mock_op_reg_manager.ops = [\n        self.concat_op, self.relu1_op, self.relu2_op, self.relu3_op,\n        self.batch_norm_op]\n\n  def test_AssignGroupingOfGroupingConcatNoSlicing(self):\n    # In this test, the output op (batch norm) has size 6 and is not sliced.\n    # and that input Conv2Ds are all of size 6, and are grouped.\n\n    # Map ops to slices.  Batch norm op is composed of multiple slices.\n    self.op_slice_dict = {\n        self.relu1_op: [self.relu1_op_slice],\n        self.relu2_op: [self.relu2_op_slice],\n        self.relu3_op: [self.relu3_op_slice],\n        self.concat_op: [self.concat_op_slice],\n        self.batch_norm_op: [self.batch_norm_op_slice],\n    }\n\n    # Map each slice to a group.\n    self.op_group_dict = {\n        self.relu1_op_slice: self.relu1_op_group,\n        self.relu2_op_slice: self.relu2_op_group,\n        self.relu3_op_slice: self.relu3_op_group,\n        self.batch_norm_op_slice: self.batch_norm_op_group\n    }\n\n    # Call handler to assign grouping.\n    handler = concat_op_handler.ConcatOpHandler()\n    handler.assign_grouping(self.concat_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Initial slice data.\n         mock.call(self.concat_op),\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Reslicing.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.concat_op),\n         mock.call(self.batch_norm_op),\n         # Refreshing slice data.\n         mock.call(self.relu1_op),\n         mock.call(self.relu2_op),\n         mock.call(self.relu3_op),\n         mock.call(self.batch_norm_op),\n         # Group concat op.\n         mock.call(self.concat_op)])\n\n    # Verify manager does not slices the concat op.\n    self.mock_op_reg_manager.slice_op.assert_not_called()\n\n    # Verify manager groups the new slices.\n    self.mock_op_reg_manager.group_op_slices.assert_called_once_with([\n        self.concat_op_slice, self.relu1_op_slice, self.relu2_op_slice,\n        self.relu3_op_slice, self.batch_norm_op_slice\n    ])\n\n  def testGetConcatOpAxis(self):\n    x = tf.zeros([7, 12, 12, 3])\n    self.assertEqual(\n        concat_op_handler._get_concat_op_axis(tf.concat([x, x], 3).op), 3)\n    self.assertEqual(\n        concat_op_handler._get_concat_op_axis(tf.concat([x, x, x], 1).op), 1)\n    self.assertEqual(\n        concat_op_handler._get_concat_op_axis(tf.concat([x, x, x], 2).op), 2)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/constant_op_regularizer.py,2,"b'""""""An OpRegularizer with 0 regularization and always alive.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom morph_net.framework import generic_regularizers\nimport tensorflow.compat.v1 as tf\n\n\nclass ConstantOpRegularizer(generic_regularizers.OpRegularizer):\n  """"""An OpRegularizer with 0 regularization and always alive.""""""\n\n  def __init__(self, size):\n    """"""Creates an instance.\n\n    Args:\n      size: Integer size of the regularizer.\n    """"""\n    self._regularization_vector = tf.zeros(size)\n    self._alive_vector = tf.ones(size, dtype=tf.bool)\n\n  @property\n  def regularization_vector(self):\n    return self._regularization_vector\n\n  @property\n  def alive_vector(self):\n    return self._alive_vector\n'"
morph_net/framework/conv2d_transpose_source_op_handler.py,0,"b'""""""OpHandler implementation for regularizer source conv2d_transpose ops.\n\nOpHandler for Conv2DBackpropInput (conv2d_transpose) source ops that use\ngroup lasso regularization.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import group_lasso_base_op_handler\n\n\nclass Conv2DTransposeSourceOpHandler(\n    group_lasso_base_op_handler.GroupLassoBaseSourceOpHandler):\n  """"""OpHandler for Conv2DBackpropInput (conv2d_transpose) source operations.""""""\n\n  def _reduce_dims(self, op):\n    del op  # Unused.\n    # Reduction dimensions for Group Lasso.\n    return (0, 1, 3)\n'"
morph_net/framework/conv_source_op_handler.py,0,"b'""""""OpHandler implementation for conv2d ops that are regularizer sources.\n\nThis OpHandler is used when a conv2d op does not have an associated batch norm\ngamma. When a path in the model graph has a group lasso based source op which is\nfollowed downstream by a batch norm source, the batch norm takes precedence.\nFor example: conv2d -> relu -> {bn, foo}.\nRelu is passthrough so conv2d is grouped with the last op.\nIf the last op is batch norm, then the source op for the group is the batch\nnorm gamma. If not, the source op for the group is the conv2d group lasso.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import group_lasso_base_op_handler\n\n\nclass ConvSourceOpHandler(\n    group_lasso_base_op_handler.GroupLassoBaseSourceOpHandler):\n  """"""OpHandler implementation for Conv2D, Conv3D source operations.""""""\n\n  def _reduce_dims(self, op):\n    # Reduction dimensions for Group Lasso.\n    if op.type == \'Conv2D\':\n      return (0, 1, 2)\n    elif op.type == \'Conv3D\':\n      return (0, 1, 2, 3)\n    else:\n      raise ValueError(\'Unsupported op type %s\' % op.type)\n\nConv2DSourceOpHandler = ConvSourceOpHandler\n'"
morph_net/framework/conv_source_op_handler_test.py,5,"b'""""""Tests for regularizers.framework.conv_source_op_handler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom absl.testing import parameterized\n\nimport mock\nfrom morph_net.framework import conv_source_op_handler\nfrom morph_net.framework import op_regularizer_manager as orm\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import layers\n\n\n_DEFAULT_THRESHOLD = 0.001\n\n\nclass ConvsSourceOpHandlerTest(parameterized.TestCase, tf.test.TestCase):\n\n  def _build(self, conv_type):\n    assert conv_type in [\'Conv2D\', \'Conv3D\']\n    if conv_type == \'Conv2D\':\n      inputs = tf.zeros([2, 4, 4, 3])\n      conv_fn = layers.conv2d\n    else:\n      inputs = tf.zeros([2, 4, 4, 4, 3])\n      conv_fn = layers.conv3d\n\n    c1 = conv_fn(\n        inputs, num_outputs=5, kernel_size=3, scope=\'conv1\', normalizer_fn=None)\n    conv_fn(c1, num_outputs=6, kernel_size=3, scope=\'conv2\', normalizer_fn=None)\n\n    g = tf.get_default_graph()\n\n    # Declare OpSlice and OpGroup for ops of interest.\n    self.conv1_op = g.get_operation_by_name(\'conv1/\' + conv_type)\n    self.conv1_op_slice = orm.OpSlice(self.conv1_op, orm.Slice(0, 5))\n    self.conv1_op_group = orm.OpGroup(\n        self.conv1_op_slice, omit_source_op_slices=[self.conv1_op_slice])\n\n    self.relu1_op = g.get_operation_by_name(\'conv1/Relu\')\n    self.relu1_op_slice = orm.OpSlice(self.relu1_op, orm.Slice(0, 5))\n    self.relu1_op_group = orm.OpGroup(\n        self.relu1_op_slice, omit_source_op_slices=[self.relu1_op_slice])\n\n    self.conv2_op = g.get_operation_by_name(\'conv2/\' + conv_type)\n    self.conv2_op_slice = orm.OpSlice(self.conv2_op, orm.Slice(0, 6))\n    self.conv2_op_group = orm.OpGroup(\n        self.conv2_op_slice, omit_source_op_slices=[self.conv2_op_slice])\n\n    self.conv2_weights_op = g.get_operation_by_name(\'conv2/weights/read\')\n    self.conv2_weights_op_slice = orm.OpSlice(\n        self.conv2_weights_op, orm.Slice(0, 6))\n    self.conv2_weights_op_group = orm.OpGroup(\n        self.conv2_weights_op_slice,\n        omit_source_op_slices=[self.conv2_weights_op_slice])\n\n    self.relu2_op = g.get_operation_by_name(\'conv2/Relu\')\n    self.relu2_op_slice = orm.OpSlice(self.relu2_op, orm.Slice(0, 6))\n    self.relu2_op_group = orm.OpGroup(\n        self.relu2_op_slice, omit_source_op_slices=[self.relu2_op_slice])\n\n    # Create mock OpRegularizerManager with custom mapping of OpSlice and\n    # OpGroup.\n    self.mock_op_reg_manager = mock.create_autospec(orm.OpRegularizerManager)\n\n    def get_op_slices(op):\n      return self.op_slice_dict.get(op, [])\n\n    def get_op_group(op_slice):\n      return self.op_group_dict.get(op_slice)\n\n    def is_passthrough(op):\n      if op in [self.conv1_op, self.conv2_op]:\n        h = conv_source_op_handler.ConvSourceOpHandler(_DEFAULT_THRESHOLD)\n        return h.is_passthrough\n      else:\n        return False\n\n    self.mock_op_reg_manager.get_op_slices.side_effect = get_op_slices\n    self.mock_op_reg_manager.get_op_group.side_effect = get_op_group\n    self.mock_op_reg_manager.is_source_op.return_value = False\n    self.mock_op_reg_manager.is_passthrough.side_effect = is_passthrough\n    self.mock_op_reg_manager.ops = [\n        self.conv1_op, self.relu1_op, self.conv2_op, self.relu2_op,\n        self.conv2_weights_op]\n\n  @parameterized.named_parameters((\'_conv2d\', \'Conv2D\'), (\'_conv3d\', \'Conv3D\'))\n  def testAssignGrouping_GroupWithOutputOnly(self, conv_type):\n    self._build(conv_type)\n    # Map ops to slices.\n    self.op_slice_dict = {\n        self.conv1_op: [self.conv1_op_slice],\n        self.relu1_op: [self.relu1_op_slice],\n        self.conv2_op: [self.conv2_op_slice],\n        self.relu2_op: [self.relu2_op_slice],\n    }\n\n    # Map each slice to a group. Corresponding op slices have the same group.\n    self.op_group_dict = {\n        self.conv2_op_slice: self.conv2_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = conv_source_op_handler.ConvSourceOpHandler(_DEFAULT_THRESHOLD)\n    handler.assign_grouping(self.conv2_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up op slice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.conv2_op)\n\n    # Verify manager does not slice any ops.\n    self.mock_op_reg_manager.slice_op.assert_not_called()\n\n    # Verify manager adds inputs to process queue.\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.relu1_op])\n\n  @parameterized.named_parameters((\'_conv2d\', \'Conv2D\'), (\'_conv3d\', \'Conv3D\'))\n  def testCreateRegularizer(self, conv_type):\n    self._build(conv_type)\n    # Call handler to create regularizer.\n    handler = conv_source_op_handler.ConvSourceOpHandler(_DEFAULT_THRESHOLD)\n    regularizer = handler.create_regularizer(self.conv2_op_slice)\n\n    # Verify regularizer produces correctly shaped tensors.\n    # Most of the regularizer testing is in group_lasso_regularizer_test.py\n    expected_norm_dim = self.conv2_op.inputs[1].shape.as_list()[-1]\n    self.assertEqual(expected_norm_dim,\n                     regularizer.regularization_vector.shape.as_list()[0])\n\n  @parameterized.named_parameters((\'_conv2d\', \'Conv2D\'), (\'_conv3d\', \'Conv3D\'))\n  def testCreateRegularizer_Sliced(self, conv_type):\n    self._build(conv_type)\n    # Call handler to create regularizer.\n    handler = conv_source_op_handler.ConvSourceOpHandler(_DEFAULT_THRESHOLD)\n    conv2_op_slice = orm.OpSlice(self.conv2_op, orm.Slice(0, 3))\n    regularizer = handler.create_regularizer(conv2_op_slice)\n\n    # Verify regularizer produces correctly shaped tensors.\n    # Most of the regularizer testing is in group_lasso_regularizer_test.py\n    expected_norm_dim = 3\n    self.assertEqual(expected_norm_dim,\n                     regularizer.regularization_vector.shape.as_list()[0])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/depth_to_space_op_handler.py,1,"b'""""""OpHandler implementation for DepthToSpace operations.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import op_handler\nfrom morph_net.framework import op_handler_util\n\n\nclass DepthToSpaceOpHandler(op_handler.OpHandler):\n  """"""OpHandler implementation for DepthToSpace operations.""""""\n\n  @property\n  def is_source_op(self):\n    return False\n\n  @property\n  def is_passthrough(self):\n    return False\n\n  def assign_grouping(self, op, op_reg_manager):\n    """"""Assign grouping to the given op and updates the manager.\n\n    Args:\n      op: tf.Operation to assign grouping to.\n      op_reg_manager: OpRegularizerManager to keep track of the grouping.\n    """"""\n    # Check if all input ops have groups, or tell the manager to process them.\n    input_ops = op_handler_util.get_input_ops(op, op_reg_manager)\n    input_ops_without_group = op_handler_util.get_ops_without_groups(\n        input_ops, op_reg_manager)\n\n    # Check if all output ops have groups, or tell the manager to process them.\n    output_ops = op_handler_util.get_output_ops(op, op_reg_manager)\n    output_ops_without_group = op_handler_util.get_ops_without_groups(\n        output_ops, op_reg_manager)\n\n    # Remove non-passthrough ops from outputs ops to group with.\n    output_ops = op_handler_util.remove_non_passthrough_ops(\n        output_ops, op_reg_manager)\n\n    # Only group with ops that have the same size.  Defer the ops that have\n    # mismatched size.\n    input_ops_to_group = input_ops\n    output_ops_to_group, output_ops_to_defer = (\n        op_handler_util.separate_same_size_ops(op, output_ops))\n\n    # Also defer ungrouped ops.\n    input_ops_to_defer = input_ops_without_group\n    for output_op_without_group in output_ops_without_group:\n      if output_op_without_group not in output_ops_to_defer:\n        output_ops_to_defer.append(output_op_without_group)\n\n    # Only slice and merge if all inputs are grouped.\n    if input_ops_to_defer:\n      op_reg_manager.process_ops(input_ops_to_defer)\n      return\n\n    block_size = op.get_attr(\'block_size\')\n    block_group = block_size * block_size\n\n    # For DepthToSpace, slice ops into individual channels before mapping.  For\n    # example, this op might reshape a tensor [N, H, W, 4] -> [N, 2H, 2W, 1]\n    # where the 4 input channels are mapped to 1 output channel.  Thus, slice\n    # the input into individual OpSlice in order to group.\n    assert len(input_ops_to_group) == 1\n    input_op = input_ops_to_group[0]\n    op_handler_util.reslice_ops(\n        input_ops, [1] * op_handler_util.get_op_size(input_op),\n        op_reg_manager)\n    op_handler_util.reslice_ops(\n        [op] + output_ops_to_group, [1] * op_handler_util.get_op_size(op),\n        op_reg_manager)\n\n    # Repopulate OpSlice data.\n    op_slices = op_reg_manager.get_op_slices(op)\n    input_op_slices = op_handler_util.get_op_slices(input_ops, op_reg_manager)\n\n    # Group blocks of input channels with output channels based on block group.\n    # For block_size B, the block group is B * B.  For example, if the input\n    # tensor is [N, H, W, 18] with block_size 3, the output tensor is\n    # [N, 3H, 3W, 2] where block_size * block_size number of channels are mapped\n    # to space values (i.e. 3H and 3W).  See Tensorflow documentation for\n    # additional details.\n    for i, op_slice in enumerate(op_slices):\n      for input_op_slice in input_op_slices:\n        op_reg_manager.group_op_slices(\n            input_op_slice[i * block_group:(i + 1) * block_group] + [op_slice])\n\n    # Process deferred ops.\n    if input_ops_to_defer or output_ops_to_defer:\n      op_reg_manager.process_ops(output_ops_to_defer + input_ops_to_defer)\n\n  def create_regularizer(self, _):\n    raise NotImplementedError(\'Not a source op.\')\n'"
morph_net/framework/depth_to_space_op_handler_test.py,7,"b'""""""Tests for depth_to_space_op_handler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport mock\n\nfrom morph_net.framework import depth_to_space_op_handler\nfrom morph_net.framework import op_regularizer_manager as orm\nimport tensorflow.compat.v1 as tf\n\n\nclass DepthToSpaceOpHandlerTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(DepthToSpaceOpHandlerTest, self).setUp()\n    # Test a Identity -> DepthToSpace -> Identity chain of ops.\n    inputs = tf.zeros([2, 4, 4, 4])\n    id1 = tf.identity(inputs)\n    dts = tf.depth_to_space(id1, 2)\n    tf.identity(dts)\n\n    g = tf.get_default_graph()\n\n    # Declare OpSlice and OpGroup for ops of interest.\n    self.id1_op = g.get_operation_by_name(\'Identity\')\n    self.id1_op_slice = orm.OpSlice(self.id1_op, orm.Slice(0, 4))\n    self.id1_op_group = orm.OpGroup(self.id1_op_slice,\n                                    omit_source_op_slices=[self.id1_op_slice])\n    self.id1_op_slice0 = orm.OpSlice(self.id1_op, orm.Slice(0, 1))\n    self.id1_op_slice1 = orm.OpSlice(self.id1_op, orm.Slice(1, 1))\n    self.id1_op_slice2 = orm.OpSlice(self.id1_op, orm.Slice(2, 1))\n    self.id1_op_slice3 = orm.OpSlice(self.id1_op, orm.Slice(3, 1))\n\n    self.dts_op = g.get_operation_by_name(\'DepthToSpace\')\n    self.dts_op_slice = orm.OpSlice(self.dts_op, orm.Slice(0, 1))\n    self.dts_op_group = orm.OpGroup(self.dts_op_slice,\n                                    omit_source_op_slices=[self.dts_op_slice])\n\n    self.id2_op = g.get_operation_by_name(\'Identity_1\')\n    self.id2_op_slice = orm.OpSlice(self.id2_op, orm.Slice(0, 1))\n    self.id2_op_group = orm.OpGroup(self.id2_op_slice,\n                                    omit_source_op_slices=[self.id2_op_slice])\n\n    # Create mock OpRegularizerManager with custom mapping of OpSlice and\n    # OpGroup.\n    self.mock_op_reg_manager = mock.create_autospec(orm.OpRegularizerManager)\n\n    self.op_slice_dict = {\n        self.id1_op: [self.id1_op_slice],\n        self.dts_op: [self.dts_op_slice],\n        self.id2_op: [self.id2_op_slice],\n    }\n    def get_op_slices(op):\n      return self.op_slice_dict.get(op)\n\n    def get_op_group(op_slice):\n      return self.op_group_dict.get(op_slice)\n\n    self.mock_op_reg_manager.get_op_slices.side_effect = get_op_slices\n    self.mock_op_reg_manager.get_op_group.side_effect = get_op_group\n    self.mock_op_reg_manager.is_source_op.return_value = False\n    self.mock_op_reg_manager.ops = [self.id1_op, self.dts_op, self.id2_op]\n\n  def test_assign_grouping_no_neighbor_groups(self):\n    # No ops have groups.\n    self.op_group_dict = {}\n\n    # Call handler to assign grouping.\n    handler = depth_to_space_op_handler.DepthToSpaceOpHandler()\n    handler.assign_grouping(self.dts_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        [mock.call(self.id1_op),\n         mock.call(self.id2_op)])\n\n    # Verify manager does not group.\n    self.mock_op_reg_manager.group_op_slices.assert_not_called()\n\n    # Verify manager processes grouping for identity ops.\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.id1_op])\n\n  def test_assign_grouping_all_inputs_grouped(self):\n    # Map ops to slices.\n    self.op_slice_dict[self.id1_op] = [\n        self.id1_op_slice0,\n        self.id1_op_slice1,\n        self.id1_op_slice2,\n        self.id1_op_slice3]\n\n    # All inputs have groups.\n    self.op_group_dict = {\n        self.id1_op_slice0: self.id1_op_group,\n        self.id1_op_slice1: self.id1_op_group,\n        self.id1_op_slice2: self.id1_op_group,\n        self.id1_op_slice3: self.id1_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = depth_to_space_op_handler.DepthToSpaceOpHandler()\n    handler.assign_grouping(self.dts_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.id1_op),\n         mock.call(self.id2_op),\n         # Reslicing.\n         mock.call(self.id1_op),\n         mock.call(self.dts_op),\n         mock.call(self.id2_op),\n         # Refreshing slice data.\n         mock.call(self.dts_op),\n         mock.call(self.id1_op)])\n\n    # Verify manager groups DepthToSpace channel with individual input channels.\n    self.mock_op_reg_manager.group_op_slices.assert_called_once_with(\n        [self.id1_op_slice0, self.id1_op_slice1, self.id1_op_slice2,\n         self.id1_op_slice3, self.dts_op_slice])\n\n    # Verify manager processes grouping for identity ops.\n    self.mock_op_reg_manager.process_ops.assert_called_once_with([self.id2_op])\n\n  def test_assign_grouping_all_outputs_grouped(self):\n    # All outputs have groups.\n    self.op_group_dict = {\n        self.id2_op_slice: self.id2_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = depth_to_space_op_handler.DepthToSpaceOpHandler()\n    handler.assign_grouping(self.dts_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.id1_op),\n         mock.call(self.id2_op)])\n\n    # Verify manager does not group.\n    self.mock_op_reg_manager.group_op_slices.assert_not_called()\n\n    # Verify manager processes grouping for identity ops.\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.id1_op])\n\n  def test_assign_grouping_all_neighbors_grouped(self):\n    # Map ops to slices.\n    self.op_slice_dict[self.id1_op] = [\n        self.id1_op_slice0,\n        self.id1_op_slice1,\n        self.id1_op_slice2,\n        self.id1_op_slice3]\n\n    # All neighbors have groups.\n    self.op_group_dict = {\n        self.id1_op_slice0: self.id1_op_group,\n        self.id1_op_slice1: self.id1_op_group,\n        self.id1_op_slice2: self.id1_op_group,\n        self.id1_op_slice3: self.id1_op_group,\n        self.id2_op_slice: self.id2_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = depth_to_space_op_handler.DepthToSpaceOpHandler()\n    handler.assign_grouping(self.dts_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.id1_op),\n         mock.call(self.id2_op),\n         # Reslicing.\n         mock.call(self.id1_op),\n         mock.call(self.dts_op),\n         mock.call(self.id2_op),\n         # Refreshing slice data.\n         mock.call(self.dts_op),\n         mock.call(self.id1_op)])\n\n    # Verify manager groups DepthToSpace channel with individual input channels.\n    self.mock_op_reg_manager.group_op_slices.assert_called_once_with(\n        [self.id1_op_slice0, self.id1_op_slice1, self.id1_op_slice2,\n         self.id1_op_slice3, self.dts_op_slice])\n\n    # Verify manager processes grouping for identity ops.\n    self.mock_op_reg_manager.process_ops.assert_not_called()\n\n  def test_assign_grouping_all_neighbors_grouped_same_group(self):\n    # Map ops to slices.\n    self.op_slice_dict[self.id1_op] = [\n        self.id1_op_slice0,\n        self.id1_op_slice1,\n        self.id1_op_slice2,\n        self.id1_op_slice3]\n\n    # All neighbors have the same group.\n    self.op_group_dict = {\n        self.id1_op_slice0: self.id1_op_group,\n        self.id1_op_slice1: self.id1_op_group,\n        self.id1_op_slice2: self.id1_op_group,\n        self.id1_op_slice3: self.id1_op_group,\n        self.id2_op_slice: self.id1_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = depth_to_space_op_handler.DepthToSpaceOpHandler()\n    handler.assign_grouping(self.dts_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.id1_op),\n         mock.call(self.id2_op),\n         # Reslicing.\n         mock.call(self.id1_op),\n         mock.call(self.dts_op),\n         mock.call(self.id2_op),\n         # Refreshing slice data.\n         mock.call(self.dts_op),\n         mock.call(self.id1_op)])\n\n    # Verify manager groups DepthToSpace channel with individual input channels.\n    self.mock_op_reg_manager.group_op_slices.assert_called_once_with(\n        [self.id1_op_slice0, self.id1_op_slice1, self.id1_op_slice2,\n         self.id1_op_slice3, self.dts_op_slice])\n\n    # Verify manager processes grouping for identity ops.\n    self.mock_op_reg_manager.process_ops.assert_not_called()\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/depthwise_convolution_op_handler.py,3,"b'""""""OpHandler implementation for depthwise convolution.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import grouping_op_handler\nfrom morph_net.framework import op_handler_util\n\n\nclass DepthwiseConvolutionOpHandler(grouping_op_handler.GroupingOpHandler):\n  """"""OpHandler implementation for depthwise convolution.""""""\n\n  def assign_grouping(self, op, op_reg_manager):\n    """"""Assign grouping to the given op and updates the manager.\n\n    Args:\n      op: tf.Operation to assign grouping to.\n      op_reg_manager: OpRegularizerManager to keep track of the grouping.\n    """"""\n    assert op.type == \'DepthwiseConv2dNative\'\n\n    # Get output size.\n    output_size = op_handler_util.get_op_size(op)\n\n    # Get input size.\n    input_size = op_handler_util.get_op_size(op.inputs[0].op)\n\n    # Take depth_multiplier from size of weight tensor.\n    depth_multiplier = op.inputs[1].shape.as_list()[-1]\n\n    if depth_multiplier == 1:\n      super(DepthwiseConvolutionOpHandler, self).assign_grouping(\n          op, op_reg_manager)\n      return\n\n    # Check if all input ops have groups, or tell the manager to process them.\n    input_ops = op_handler_util.get_input_ops(op, op_reg_manager)\n    input_ops_without_group = op_handler_util.get_ops_without_groups(\n        input_ops, op_reg_manager)\n\n    # Check if all output ops have groups, or tell the manager to process them.\n    output_ops = op_handler_util.get_output_ops(op, op_reg_manager)\n    output_ops_without_group = op_handler_util.get_ops_without_groups(\n        output_ops, op_reg_manager)\n\n    # Remove non-passthrough ops from outputs ops to group with.\n    output_ops = op_handler_util.remove_non_passthrough_ops(\n        output_ops, op_reg_manager)\n\n    # Only group with ops that have the same size.  Process the ops that have\n    # mismatched size.  For the input, we hardcode that inputs[0] is a normal\n    # input while inputs[1] is the depthwise filter.\n    input_ops_to_group = [input_ops[0]]\n    input_ops_to_process = input_ops_without_group\n    output_ops_to_group, output_ops_to_process = (\n        op_handler_util.separate_same_size_ops(op, output_ops))\n\n    # Also process ungrouped ops.\n    for output_op_without_group in output_ops_without_group:\n      if output_op_without_group not in output_ops_to_process:\n        output_ops_to_process.append(output_op_without_group)\n\n    # Slice ops into individual channels.  For example, consider 3 input\n    # channels and depth_multiplier = 2.  Let the input channels be [0, 1, 2]\n    # and the output channels be [3, 4, 5, 6, 7, 8].  The channels should be\n    # individually sliced and grouped with consecutive groups of size\n    # depth_multiplier.  Thus, this would end up grouping [0, 0, 1, 1, 2, 2] and\n    # [3, 4, 5, 6, 7, 8] into groups (0, 3, 4), (1, 5, 6), and (2, 7, 8).\n    aligned_op_slice_sizes = [1] * output_size\n    op_handler_util.reslice_ops(\n        input_ops_to_group, [1] * input_size, op_reg_manager)\n    op_handler_util.reslice_ops(\n        [op] + output_ops_to_group, aligned_op_slice_sizes, op_reg_manager)\n\n    # Rearrange OpSlice to align input and output.\n    input_op_slices, output_op_slices = (\n        self._get_depth_multiplier_input_output_op_slices(\n            input_ops_to_group, input_size, output_ops_to_group,\n            op_reg_manager, depth_multiplier))\n\n    # Group with inputs and outputs.\n    op_handler_util.group_aligned_input_output_slices(\n        op, input_ops_to_process, output_ops_to_process, input_op_slices,\n        output_op_slices, aligned_op_slice_sizes, op_reg_manager)\n\n  def _get_depth_multiplier_input_output_op_slices(\n      self, input_ops, input_size, output_ops, op_reg_manager,\n      depth_multiplier):\n    """"""Returns op slices for inputs and outputs.\n\n    Args:\n      input_ops: List of tf.Operation.\n      input_size: Integer number of input channels.\n      output_ops: List of tf.Operation.\n      op_reg_manager: OpRegularizerManager to keep track of the grouping.\n      depth_multiplier: Integer indicating how many times each input channel\n        should be replicated.  Must be positive.\n\n    Returns:\n      Tuple of (input_op_slices, output_op_slices), where each element is a list\n      of list of OpSlice with a list per op.\n    """"""\n    input_op_slices = op_handler_util.get_op_slices(input_ops, op_reg_manager)\n\n    # Each input OpSlice needs to be replicated N times where N is\n    # depth_multiplier.\n    depth_multiplier_input_op_slices = []\n    for input_op in input_op_slices:\n      slices = []\n      for op_slice in input_op:\n        slices.extend([op_slice] * depth_multiplier)\n      depth_multiplier_input_op_slices.append(slices)\n\n    output_op_slices = op_handler_util.get_op_slices(output_ops, op_reg_manager)\n\n    return (depth_multiplier_input_op_slices, output_op_slices)\n'"
morph_net/framework/depthwise_convolution_op_handler_test.py,25,"b'""""""Tests for depthwise_convolution_op_handler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport mock\nfrom morph_net.framework import depthwise_convolution_op_handler\nfrom morph_net.framework import op_regularizer_manager as orm\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import framework as framework\nfrom tensorflow.contrib import layers\n\narg_scope = framework.arg_scope\n\n\nclass DepthwiseConvolutionOpHandlerTest(tf.test.TestCase):\n\n  def _batch_norm_scope(self):\n    params = {\n        \'trainable\': True,\n        \'normalizer_fn\': layers.batch_norm,\n        \'normalizer_params\': {\n            \'scale\': True,\n        },\n    }\n\n    with arg_scope([layers.conv2d], **params) as sc:\n      return sc\n\n  def setUp(self):\n    super(DepthwiseConvolutionOpHandlerTest, self).setUp()\n    tf.reset_default_graph()\n\n    # This tests a Conv2D -> SeparableConv2D -> Conv2D chain of ops.\n    with framework.arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=5, kernel_size=3, scope=\'conv1\')\n      c2 = layers.separable_conv2d(c1, num_outputs=8, kernel_size=3,\n                                   depth_multiplier=2, scope=\'conv2\')\n      layers.conv2d(c2, num_outputs=6, kernel_size=3, scope=\'conv3\')\n\n    g = tf.get_default_graph()\n\n    # Declare OpSlice and OpGroup for ops of interest.\n    self.dwise_conv2_op = g.get_operation_by_name(\n        \'conv2/separable_conv2d/depthwise\')\n    self.dwise_conv2_op_slice = orm.OpSlice(\n        self.dwise_conv2_op, orm.Slice(0, 10))\n    self.dwise_conv2_op_slice_0_1 = orm.OpSlice(\n        self.dwise_conv2_op, orm.Slice(0, 1))\n    self.dwise_conv2_op_slice_1_2 = orm.OpSlice(\n        self.dwise_conv2_op, orm.Slice(1, 1))\n    self.dwise_conv2_op_slice_2_3 = orm.OpSlice(\n        self.dwise_conv2_op, orm.Slice(2, 1))\n    self.dwise_conv2_op_slice_3_4 = orm.OpSlice(\n        self.dwise_conv2_op, orm.Slice(3, 1))\n    self.dwise_conv2_op_slice_4_5 = orm.OpSlice(\n        self.dwise_conv2_op, orm.Slice(4, 1))\n    self.dwise_conv2_op_slice_5_6 = orm.OpSlice(\n        self.dwise_conv2_op, orm.Slice(5, 1))\n    self.dwise_conv2_op_slice_6_7 = orm.OpSlice(\n        self.dwise_conv2_op, orm.Slice(6, 1))\n    self.dwise_conv2_op_slice_7_8 = orm.OpSlice(\n        self.dwise_conv2_op, orm.Slice(7, 1))\n    self.dwise_conv2_op_slice_8_9 = orm.OpSlice(\n        self.dwise_conv2_op, orm.Slice(8, 1))\n    self.dwise_conv2_op_slice_9_10 = orm.OpSlice(\n        self.dwise_conv2_op, orm.Slice(9, 1))\n\n    self.conv2_op = g.get_operation_by_name(\'conv2/separable_conv2d\')\n    self.conv2_op_slice = orm.OpSlice(self.conv2_op, orm.Slice(0, 8))\n\n    self.relu1_op = g.get_operation_by_name(\'conv1/Relu\')\n    self.relu1_op_slice = orm.OpSlice(self.relu1_op, orm.Slice(0, 5))\n    self.relu1_op_slice_0_1 = orm.OpSlice(self.relu1_op, orm.Slice(0, 1))\n    self.relu1_op_slice_1_2 = orm.OpSlice(self.relu1_op, orm.Slice(1, 1))\n    self.relu1_op_slice_2_3 = orm.OpSlice(self.relu1_op, orm.Slice(2, 1))\n    self.relu1_op_slice_3_4 = orm.OpSlice(self.relu1_op, orm.Slice(3, 1))\n    self.relu1_op_slice_4_5 = orm.OpSlice(self.relu1_op, orm.Slice(4, 1))\n    self.relu1_op_group = orm.OpGroup(self.relu1_op_slice)\n\n    self.conv3_op = g.get_operation_by_name(\'conv3/Conv2D\')\n    self.conv3_op_slice = orm.OpSlice(self.conv3_op, orm.Slice(0, 6))\n\n    # Create mock OpRegularizerManager with custom mapping of OpSlice and\n    # OpGroup.\n    self.mock_op_reg_manager = mock.create_autospec(orm.OpRegularizerManager)\n\n    self.op_slice_dict = {\n        self.dwise_conv2_op: [self.dwise_conv2_op_slice],\n        self.conv2_op: [self.conv2_op_slice],\n        self.relu1_op: [self.relu1_op_slice],\n        self.conv3_op: [self.conv3_op_slice],\n    }\n    def get_op_slices(op):\n      return self.op_slice_dict.get(op)\n\n    def get_op_group(op_slice):\n      return self.op_group_dict.get(op_slice)\n\n    # Update op_slice_dict when an op is sliced.\n    def slice_op(op, _):\n      if op == self.dwise_conv2_op:\n        self.op_slice_dict[self.dwise_conv2_op] = [\n            self.dwise_conv2_op_slice_0_1,\n            self.dwise_conv2_op_slice_1_2,\n            self.dwise_conv2_op_slice_2_3,\n            self.dwise_conv2_op_slice_3_4,\n            self.dwise_conv2_op_slice_4_5,\n            self.dwise_conv2_op_slice_5_6,\n            self.dwise_conv2_op_slice_6_7,\n            self.dwise_conv2_op_slice_7_8,\n            self.dwise_conv2_op_slice_8_9,\n            self.dwise_conv2_op_slice_9_10]\n      if op == self.relu1_op:\n        self.op_slice_dict[self.relu1_op] = [\n            self.relu1_op_slice_0_1,\n            self.relu1_op_slice_1_2,\n            self.relu1_op_slice_2_3,\n            self.relu1_op_slice_3_4,\n            self.relu1_op_slice_4_5]\n\n    self.mock_op_reg_manager.get_op_slices.side_effect = get_op_slices\n    self.mock_op_reg_manager.get_op_group.side_effect = get_op_group\n    self.mock_op_reg_manager.is_source_op.return_value = False\n    self.mock_op_reg_manager.slice_op.side_effect = slice_op\n    self.mock_op_reg_manager.ops = [\n        self.relu1_op, self.dwise_conv2_op, self.conv2_op, self.conv3_op]\n\n  def testAssignGrouping_DepthMultiplier(self):\n    # All neighbor ops have groups.\n    self.op_group_dict = {\n        self.relu1_op_slice: self.relu1_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = depthwise_convolution_op_handler.DepthwiseConvolutionOpHandler()\n    handler.assign_grouping(self.dwise_conv2_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu1_op),\n         mock.call(self.conv2_op),\n         # Reslicing.\n         mock.call(self.relu1_op),\n         mock.call(self.dwise_conv2_op),\n         # Refreshing slice data.\n         mock.call(self.relu1_op),\n         # Group depthwise convolution.\n         mock.call(self.dwise_conv2_op)])\n\n    # Verify manager groups batch norm with inputs and outputs.\n    self.mock_op_reg_manager.group_op_slices.assert_has_calls(\n        [mock.call([self.dwise_conv2_op_slice_0_1, self.relu1_op_slice_0_1]),\n         mock.call([self.dwise_conv2_op_slice_1_2, self.relu1_op_slice_0_1]),\n         mock.call([self.dwise_conv2_op_slice_2_3, self.relu1_op_slice_1_2]),\n         mock.call([self.dwise_conv2_op_slice_3_4, self.relu1_op_slice_1_2]),\n         mock.call([self.dwise_conv2_op_slice_4_5, self.relu1_op_slice_2_3]),\n         mock.call([self.dwise_conv2_op_slice_5_6, self.relu1_op_slice_2_3]),\n         mock.call([self.dwise_conv2_op_slice_6_7, self.relu1_op_slice_3_4]),\n         mock.call([self.dwise_conv2_op_slice_7_8, self.relu1_op_slice_3_4]),\n         mock.call([self.dwise_conv2_op_slice_8_9, self.relu1_op_slice_4_5]),\n         mock.call([self.dwise_conv2_op_slice_9_10, self.relu1_op_slice_4_5])])\n\n    # Verify manager does not process any additional ops.\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.conv2_op])\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n  def testAssignGrouping_NoDepthMultiplier(self):\n    # Repeat setUp, but with depth_multiplier=1.  Unfortunately, this involves\n    # rebuilding the graph from scratch.\n    tf.reset_default_graph()\n\n    # This tests a Conv2D -> SeparableConv2D -> Conv2D chain of ops.\n    with framework.arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=5, kernel_size=3, scope=\'conv1\')\n      c2 = layers.separable_conv2d(c1, num_outputs=8, kernel_size=3,\n                                   depth_multiplier=1, scope=\'conv2\')\n      layers.conv2d(c2, num_outputs=6, kernel_size=3, scope=\'conv3\')\n\n    g = tf.get_default_graph()\n\n    # Declare OpSlice and OpGroup for ops of interest.\n    self.dwise_conv2_op = g.get_operation_by_name(\n        \'conv2/separable_conv2d/depthwise\')\n    self.dwise_conv2_op_slice = orm.OpSlice(\n        self.dwise_conv2_op, orm.Slice(0, 5))\n\n    self.conv2_op = g.get_operation_by_name(\'conv2/separable_conv2d\')\n    self.conv2_op_slice = orm.OpSlice(self.conv2_op, orm.Slice(0, 8))\n\n    self.relu1_op = g.get_operation_by_name(\'conv1/Relu\')\n    self.relu1_op_slice = orm.OpSlice(self.relu1_op, orm.Slice(0, 5))\n    self.relu1_op_group = orm.OpGroup(self.relu1_op_slice)\n\n    self.conv3_op = g.get_operation_by_name(\'conv3/Conv2D\')\n    self.conv3_op_slice = orm.OpSlice(self.conv3_op, orm.Slice(0, 6))\n\n    # Create mock OpRegularizerManager with custom mapping of OpSlice and\n    # OpGroup.\n    self.mock_op_reg_manager = mock.create_autospec(orm.OpRegularizerManager)\n\n    self.op_slice_dict = {\n        self.dwise_conv2_op: [self.dwise_conv2_op_slice],\n        self.conv2_op: [self.conv2_op_slice],\n        self.relu1_op: [self.relu1_op_slice],\n        self.conv3_op: [self.conv3_op_slice],\n    }\n    def get_op_slices(op):\n      return self.op_slice_dict.get(op)\n\n    def get_op_group(op_slice):\n      return self.op_group_dict.get(op_slice)\n\n    self.mock_op_reg_manager.get_op_slices.side_effect = get_op_slices\n    self.mock_op_reg_manager.get_op_group.side_effect = get_op_group\n    self.mock_op_reg_manager.is_source_op.return_value = False\n    self.mock_op_reg_manager.ops = [\n        self.relu1_op, self.dwise_conv2_op, self.conv2_op, self.conv3_op]\n\n    # All neighbor ops have groups.\n    self.op_group_dict = {\n        self.relu1_op_slice: self.relu1_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = depthwise_convolution_op_handler.DepthwiseConvolutionOpHandler()\n    handler.assign_grouping(self.dwise_conv2_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu1_op),\n         mock.call(self.conv2_op),\n         # Initial slice data.\n         mock.call(self.dwise_conv2_op),\n         mock.call(self.relu1_op),\n         # Reslicing.\n         mock.call(self.relu1_op),\n         mock.call(self.dwise_conv2_op),\n         # Refreshing slice data.\n         mock.call(self.relu1_op),\n         # Group depthwise convolution.\n         mock.call(self.dwise_conv2_op)])\n\n    # Verify manager groups batch norm with inputs and outputs.\n    self.mock_op_reg_manager.group_op_slices.assert_called_once_with(\n        [self.dwise_conv2_op_slice, self.relu1_op_slice])\n\n    # Verify manager does not process any additional ops.\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.conv2_op])\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n  def testDepthwiseChannelMapping(self):\n    """"""Verify depth multiplier maps input to output as expected.""""""\n    tf.reset_default_graph()\n\n    # Construct input tensor with shape [1, 4, 4, 5].  There are 5 channels\n    # where each channel has values corresponding to the channel index.\n    channel0 = tf.ones([1, 4, 4, 1]) * 0\n    channel1 = tf.ones([1, 4, 4, 1]) * 1\n    channel2 = tf.ones([1, 4, 4, 1]) * 2\n    channel3 = tf.ones([1, 4, 4, 1]) * 3\n    channel4 = tf.ones([1, 4, 4, 1]) * 4\n    inputs = tf.concat(\n        [channel0, channel1, channel2, channel3, channel4], axis=3)\n    # Sanity check that input tensor is the right shape.\n    self.assertAllEqual([1, 4, 4, 5], inputs.shape.as_list())\n\n    conv = layers.separable_conv2d(\n        inputs, num_outputs=None, kernel_size=3, depth_multiplier=2,\n        weights_initializer=identity_initializer, scope=\'depthwise_conv\')\n\n    with self.cached_session():\n      with tf.variable_scope(\'\', reuse=tf.AUTO_REUSE):\n        weights = tf.get_variable(\'depthwise_conv/depthwise_weights\')\n        biases = tf.get_variable(\'depthwise_conv/biases\', [10],\n                                 initializer=tf.zeros_initializer)\n      init = tf.variables_initializer([weights, biases])\n      init.run()\n\n      # The depth_multiplier replicates channels with [a, a, b, b, c, c, ...]\n      # pattern.  Expected output has shape [1, 4, 4, 10].\n      expected_output = tf.concat(\n          [channel0, channel0,\n           channel1, channel1,\n           channel2, channel2,\n           channel3, channel3,\n           channel4, channel4],\n          axis=3)\n      # Sanity check that output tensor is the right shape.\n      self.assertAllEqual([1, 4, 4, 10], expected_output.shape.as_list())\n\n      self.assertAllEqual(expected_output.eval(), conv.eval())\n\n\ndef identity_initializer(shape, dtype=None, partition_info=None):\n  """"""Fake weight initializer to initialize a 3x3 identity kernel.""""""\n  del shape  # Unused.\n  del dtype  # Unused.\n  del partition_info  # Unused.\n\n  # Start with a 3x3 kernel identity kernel.\n  kernel = [[0, 0, 0],\n            [0, 1, 0],\n            [0, 0, 0]]\n\n  # Expand and tile kernel to get a tensor with shape [3, 3, 5, 2].\n  kernel = tf.expand_dims(kernel, axis=-1)\n  kernel = tf.expand_dims(kernel, axis=-1)\n  tensor = tf.tile(kernel, [1, 1, 5, 2])\n  return tf.cast(tensor, dtype=tf.float32)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/generic_regularizers.py,5,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Interface for MorphNet regularizers framework.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\n\nclass NetworkRegularizer(object):  # pytype: disable=ignored-metaclass\n  """"""An interface for Network Regularizers.""""""\n  __metaclass__ = abc.ABCMeta\n\n  @abc.abstractmethod\n  def get_regularization_term(self, ops=None):\n    """"""Compute the regularization term.\n\n    Args:\n      ops: A list of tf.Operation. If not specified, all ops that the\n        NetworkRegularizer is aware of are implied.\n\n    Returns:\n      A tf.Tensor scalar of floating point type that evaluates to the\n        regularization term that should be added to the total loss, with a\n        suitable coefficient.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def get_cost(self, ops=None):\n    """"""Calculates the cost targeted by the Regularizer.\n\n    Args:\n      ops: A list of tf.Operation objects. Same as get_regularization_term, but\n        returns total cost implied by the regularization term.\n\n    Returns:\n      A tf.Tensor scalar that evaluates to the cost.\n    """"""\n    pass\n\n  @property\n  def op_regularizer_manager(self):\n    """"""Returns the OpRegularizerManager managing the graph\'s OpRegularizers.\n\n    If the NetworkRegularizer subclass is not using an OpRegularizerManager,\n    None is returned.\n    """"""\n    return None\n\n  @property\n  def name(self):\n    """"""Name of network regularizer..""""""\n    return \'\'\n\n  @property\n  def cost_name(self):\n    """"""Name of the cost targeted by network regularizer.""""""\n    return \'\'\n\n\nclass OpRegularizer(object):  # pytype: disable=ignored-metaclass\n  """"""An interface for Op Regularizers.\n\n  An OpRegularizer object corresponds to a tf.Operation, and provides\n  a regularizer for the output of the op (we assume that the op has one output\n  of interest in the context of MorphNet).\n  """"""\n  __metaclass__ = abc.ABCMeta\n\n  @abc.abstractproperty\n  def regularization_vector(self):\n    """"""Returns a vector of floats, with regularizers.\n\n    The length of the vector is the number of ""output activations"" (call them\n    neurons, nodes, filters, etc.) of the op. For a convolutional network, it\'s\n    the number of filters (aka ""depth""). For a fully-connected layer, it\'s\n    usually the second (and last) dimension - assuming the first one is the\n    batch size.\n    """"""\n    pass\n\n  @abc.abstractproperty\n  def alive_vector(self):\n    """"""Returns a vector of booleans, indicating which activations are alive.\n\n    Call them activations, neurons, nodes, filters, etc. This vector is of the\n    same length as the regularization_vector.\n    """"""\n    pass\n\n\ndef dimensions_are_compatible(op_regularizer):\n  """"""Checks if op_regularizer\'s alive_vector matches regularization_vector.""""""\n  return op_regularizer.alive_vector.shape.with_rank(1).dims[\n      0].is_compatible_with(\n          op_regularizer.regularization_vector.shape.with_rank(1).dims[0])\n'"
morph_net/framework/group_lasso_base_op_handler.py,1,"b'""""""Base OpHandler for ops that use group lasso regularizer.\n\nThis OpHandler should not be called directly. It is a virtual base class\nfor regularization source OpHandlers that use Group Lasso as their regularizer.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\nfrom morph_net.framework import op_handler\nfrom morph_net.framework import op_handler_util\nfrom morph_net.framework import tpu_util\nfrom morph_net.op_regularizers import group_lasso_regularizer\n\n\nclass GroupLassoBaseSourceOpHandler(op_handler.OpHandler):\n  """"""Base OpHandler for source ops that use Group Lasso.""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, threshold, l1_fraction=0.0):\n    """"""Instantiate an instance.\n\n    Args:\n      threshold: Float scalar used as threshold for GroupLassoRegularizer.\n      l1_fraction: Float scalar used as l1_fraction for GroupLassoRegularizer.\n    """"""\n    self._threshold = threshold\n    self._l1_fraction = l1_fraction\n\n  @abc.abstractmethod\n  def _reduce_dims(self, op):\n    # Reduction dimensions for Group Lasso.\n    pass\n\n  @property\n  def is_source_op(self):\n    return True\n\n  @property\n  def is_passthrough(self):\n    return False\n\n  def assign_grouping(self, op, op_reg_manager):\n    """"""Assign grouping to the given op and updates the manager.\n\n    Args:\n      op: tf.Operation to assign grouping to.\n      op_reg_manager: OpRegularizerManager to keep track of the grouping.\n    """"""\n    # This is a source op so begin by getting the OpGroup or creating one.\n    op_slices = op_reg_manager.get_op_slices(op)\n    for op_slice in op_slices:\n      op_group = op_reg_manager.get_op_group(op_slice)\n      if op_group is None:\n        op_reg_manager.create_op_group_for_op_slice(op_slice)\n\n    # Check if all input ops have groups, or tell the manager to process them.\n    input_ops = op_handler_util.get_input_ops(op, op_reg_manager)\n    input_ops_without_group = op_handler_util.get_ops_without_groups(\n        input_ops, op_reg_manager)\n\n    # Check if all output ops have groups, or tell the manager to process them.\n    output_ops = op_handler_util.get_output_ops(op, op_reg_manager)\n    output_ops_without_group = op_handler_util.get_ops_without_groups(\n        output_ops, op_reg_manager)\n\n    # Remove non-passthrough ops from outputs ops to group with.\n    output_ops = op_handler_util.remove_non_passthrough_ops(\n        output_ops, op_reg_manager)\n\n    # Only group with ops that have the same size.  Process the ops that have\n    # mismatched size.\n    output_ops_to_group, output_ops_to_process = (\n        op_handler_util.separate_same_size_ops(op, output_ops))\n\n    # Also process ungrouped ops.\n    input_ops_to_process = input_ops_without_group\n    output_ops_to_process.extend(output_ops_without_group)\n\n    # Align op slice sizes if needed.\n    output_op_slices = op_handler_util.get_op_slices(\n        output_ops_to_group, op_reg_manager)\n    aligned_op_slice_sizes = op_handler_util.get_aligned_op_slice_sizes(\n        op_slices, [], output_op_slices)\n    op_handler_util.reslice_ops([op] + output_ops_to_group,\n                                aligned_op_slice_sizes, op_reg_manager)\n\n    # Repopulate OpSlice data, as ops may have been resliced.\n    output_op_slices = op_handler_util.get_op_slices(\n        output_ops_to_group, op_reg_manager)\n\n    # Group with outputs.\n    op_handler_util.group_op_with_inputs_and_outputs(\n        op, [], output_op_slices, aligned_op_slice_sizes,\n        op_reg_manager)\n\n    # Reprocess ops.\n    op_reg_manager.process_ops(output_ops_to_process + input_ops_to_process)\n\n  def create_regularizer(self, op_slice):\n    """"""Create a regularizer for this conv2d OpSlice.\n\n    Args:\n      op_slice: op_regularizer_manager.OpSlice that is a conv2d OpSlice.\n\n    Returns:\n      OpRegularizer for this conv2d op.\n    """"""\n    start_index = op_slice.slice.start_index\n    size = op_slice.slice.size\n    weights = op_slice.op.inputs[1]  # Input 1 are the weights.\n    weights = tpu_util.maybe_convert_to_variable(weights)\n    reduce_dims = self._reduce_dims(op_slice.op)\n    rank = len(weights.shape.as_list())\n    if rank != len(reduce_dims) + 1:\n      raise ValueError(\'Rank %d incompatible with reduce_dims %s for op %s\' %\n                       (rank, reduce_dims, op_slice.op.name))\n\n    def _slice_weights():\n      """"""Slices the weight tensor according to op_slice information.""""""\n      if rank == 2:\n        if reduce_dims[0] == 0:\n          return weights[:, start_index:start_index + size]\n        else:\n          return weights[start_index:start_index + size, :]\n      if rank == 3:\n        if 2 not in reduce_dims:\n          return weights[:, :, start_index:start_index + size]\n        if 1 not in reduce_dims:\n          return weights[:, start_index:start_index + size, :]\n        if 0 not in reduce_dims:\n          return weights[start_index:start_index + size, :, :]\n      if rank == 4:\n        if 3 not in reduce_dims:\n          return weights[:, :, :, start_index:start_index + size]\n        if 2 not in reduce_dims:\n          return weights[:, :, start_index:start_index + size, :]\n        if 1 not in reduce_dims:\n          return weights[:, start_index:start_index + size, :, :]\n        if 0 not in reduce_dims:\n          return weights[start_index:start_index + size, :, :, :]\n      if rank == 5:\n        if 4 not in reduce_dims:\n          return weights[:, :, :, :, start_index:start_index + size]\n        raise ValueError(\'Unsupported reduce_dim for rank 5 tensors (Conv3D)\')\n\n      raise ValueError(\'Unsupported rank or bad reduce_dim\')\n\n    weight_tensor = _slice_weights()\n\n    # If OpSlice size matches tensor size, use the entire tensor.  Otherwise,\n    # slice the tensor accordingly.\n    return group_lasso_regularizer.GroupLassoRegularizer(\n        weight_tensor=weight_tensor,\n        reduce_dims=self._reduce_dims(op_slice.op),\n        threshold=self._threshold,\n        l1_fraction=self._l1_fraction)\n'"
morph_net/framework/grouping_op_handler.py,4,"b'""""""OpHandler implementation for grouping operations.\n\nThis is the default OpHandler for ops without a specifically assigned OpHandler.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import op_handler\nfrom morph_net.framework import op_handler_util\n\n\nclass GroupingOpHandler(op_handler.OpHandler):\n  """"""OpHandler implementation for grouping operations.""""""\n\n  def __init__(self, grouping_indices=None):\n    """"""Creates a GroupingOpHandler.\n\n    Args:\n      grouping_indices: A list of indices which define which of the inputs the\n        handler should group. The goal is to allow the handler to ignore indices\n        which hold tensors that should not be grouped, e.g. The kernel size of a\n        convolution should not be grouped with the input tensor.\n    """"""\n    self._grouping_indices = grouping_indices\n\n  @property\n  def is_source_op(self):\n    return False\n\n  @property\n  def is_passthrough(self):\n    return True\n\n  def assign_grouping(self, op, op_reg_manager):\n    """"""Assign grouping to the given op and updates the manager.\n\n    Args:\n      op: tf.Operation to assign grouping to.\n      op_reg_manager: OpRegularizerManager to keep track of the grouping.\n    """"""\n    # Check if all input ops have groups, or tell the manager to process them.\n    input_ops = op_handler_util.get_input_ops(op, op_reg_manager,\n                                              self._grouping_indices)\n    input_ops_without_group = op_handler_util.get_ops_without_groups(\n        input_ops, op_reg_manager)\n\n    # Check if all output ops have groups, or tell the manager to process them.\n    output_ops = op_handler_util.get_output_ops(op, op_reg_manager)\n    output_ops_without_group = op_handler_util.get_ops_without_groups(\n        output_ops, op_reg_manager)\n\n    # Remove non-passthrough ops from outputs ops to group with.\n    output_ops = op_handler_util.remove_non_passthrough_ops(\n        output_ops, op_reg_manager)\n\n    # Only group with ops that have the same size.  Process the ops that have\n    # mismatched size.\n    input_ops_to_group, input_ops_to_process = (\n        op_handler_util.separate_same_size_ops(op, input_ops))\n    output_ops_to_group, output_ops_to_process = (\n        op_handler_util.separate_same_size_ops(op, output_ops))\n\n    # Remove broadcast ops.\n    input_ops_to_process = [input_op for input_op in input_ops_to_process\n                            if not self._is_broadcast(input_op, op_reg_manager)]\n\n    # Also process ungrouped ops.\n    for input_op_without_group in input_ops_without_group:\n      if input_op_without_group not in input_ops_to_process:\n        input_ops_to_process.append(input_op_without_group)\n    for output_op_without_group in output_ops_without_group:\n      if output_op_without_group not in output_ops_to_process:\n        output_ops_to_process.append(output_op_without_group)\n\n    # Align op slice sizes if needed.\n    op_slices = op_reg_manager.get_op_slices(op)\n    input_op_slices = op_handler_util.get_op_slices(\n        input_ops_to_group, op_reg_manager)\n    output_op_slices = op_handler_util.get_op_slices(\n        output_ops_to_group, op_reg_manager)\n    aligned_op_slice_sizes = op_handler_util.get_aligned_op_slice_sizes(\n        op_slices, input_op_slices, output_op_slices)\n    op_handler_util.reslice_ops(input_ops_to_group + [op] + output_ops_to_group,\n                                aligned_op_slice_sizes, op_reg_manager)\n\n    # Repopulate OpSlice data, as ops may have been resliced.\n    input_op_slices, output_op_slices = self._get_input_output_op_slices(\n        input_ops_to_group, output_ops_to_group, op_reg_manager)\n\n    # Group with inputs and outputs.\n    op_handler_util.group_aligned_input_output_slices(\n        op, input_ops_to_process, output_ops_to_process, input_op_slices,\n        output_op_slices, aligned_op_slice_sizes, op_reg_manager)\n\n  def create_regularizer(self, _):\n    raise NotImplementedError(\'Not a source op.\')\n\n  def _get_input_output_op_slices(self, input_ops, output_ops, op_reg_manager):\n    """"""Returns op slices for inputs and outputs.\n\n    Args:\n      input_ops: List of tf.Operation.\n      output_ops: List of tf.Operation.\n      op_reg_manager: OpRegularizerManager to keep track of the grouping.\n\n    Returns:\n      Tuple of (input_op_slices, output_op_slices), where each element is a list\n      of list of OpSlice with a list per op.\n    """"""\n    input_op_slices = op_handler_util.get_op_slices(input_ops, op_reg_manager)\n    output_op_slices = op_handler_util.get_op_slices(output_ops, op_reg_manager)\n    return (input_op_slices, output_op_slices)\n\n  def _is_broadcast(self, op, op_reg_manager):\n    """"""Returns True if op is broadcast.\n\n    Args:\n      op: A tf.Operation.\n      op_reg_manager: OpRegularizerManager to keep track of the grouping.\n\n    Returns:\n      A boolean indicating if op is broadcast.\n    """"""\n    op_slices = op_reg_manager.get_op_slices(op)\n    op_groups = [op_reg_manager.get_op_group(op_slice)\n                 for op_slice in op_slices]\n    return op_handler_util.get_op_size(op) == 1 and all(op_groups)\n'"
morph_net/framework/grouping_op_handler_test.py,7,"b'""""""Tests for grouping_op_handler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport mock\nfrom morph_net.framework import grouping_op_handler\nfrom morph_net.framework import op_regularizer_manager as orm\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import framework\nfrom tensorflow.contrib import layers\n\narg_scope = framework.arg_scope\n\n\nclass GroupingOpHandlerTest(tf.test.TestCase):\n\n  def _batch_norm_scope(self):\n    params = {\n        \'trainable\': True,\n        \'normalizer_fn\': layers.batch_norm,\n        \'normalizer_params\': {\n            \'scale\': True,\n        },\n    }\n\n    with arg_scope([layers.conv2d], **params) as sc:\n      return sc\n\n  def setUp(self):\n    super(GroupingOpHandlerTest, self).setUp()\n    tf.reset_default_graph()\n\n    # This tests a Conv2D -> BatchNorm -> ReLU chain of ops.\n    with framework.arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      layers.conv2d(inputs, num_outputs=5, kernel_size=3, scope=\'conv1\')\n\n    g = tf.get_default_graph()\n\n    # Declare OpSlice and OpGroup for ops of interest.\n    self.batch_norm_op = g.get_operation_by_name(\n        \'conv1/BatchNorm/FusedBatchNormV3\')\n    self.batch_norm_op_slice = orm.OpSlice(self.batch_norm_op, orm.Slice(0, 5))\n    self.batch_norm_op_group = orm.OpGroup(self.batch_norm_op_slice)\n\n    self.conv_op = g.get_operation_by_name(\'conv1/Conv2D\')\n    self.conv_op_slice = orm.OpSlice(self.conv_op, orm.Slice(0, 5))\n    self.conv_op_group = orm.OpGroup(\n        self.conv_op_slice, omit_source_op_slices=[self.conv_op_slice])\n\n    self.relu_op = g.get_operation_by_name(\'conv1/Relu\')\n    self.relu_op_slice = orm.OpSlice(self.relu_op, orm.Slice(0, 5))\n    self.relu_op_group = orm.OpGroup(self.relu_op_slice)\n\n    self.gamma_op = g.get_operation_by_name(\'conv1/BatchNorm/gamma/read\')\n    self.gamma_op_slice = orm.OpSlice(self.gamma_op, orm.Slice(0, 5))\n\n    self.beta_op = g.get_operation_by_name(\'conv1/BatchNorm/beta/read\')\n    self.beta_op_slice = orm.OpSlice(self.beta_op, orm.Slice(0, 5))\n\n    # Create mock OpRegularizerManager with custom mapping of OpSlice and\n    # OpGroup.\n    self.mock_op_reg_manager = mock.create_autospec(orm.OpRegularizerManager)\n\n    self.op_slice_dict = {\n        self.batch_norm_op: [self.batch_norm_op_slice],\n        self.conv_op: [self.conv_op_slice],\n        self.relu_op: [self.relu_op_slice],\n        self.gamma_op: [self.gamma_op_slice],\n        self.beta_op: [self.beta_op_slice],\n    }\n    def get_op_slices(op):\n      return self.op_slice_dict.get(op)\n\n    def get_op_group(op_slice):\n      return self.op_group_dict.get(op_slice)\n\n    self.mock_op_reg_manager.get_op_slices.side_effect = get_op_slices\n    self.mock_op_reg_manager.get_op_group.side_effect = get_op_group\n    self.mock_op_reg_manager.is_source_op.return_value = False\n    self.mock_op_reg_manager.ops = [\n        self.batch_norm_op, self.conv_op, self.relu_op, self.gamma_op,\n        self.beta_op]\n\n  def testAssignGrouping_NoNeighborGroups(self):\n    # No ops have groups.\n    self.op_group_dict = {}\n\n    # Call handler to assign grouping.\n    handler = grouping_op_handler.GroupingOpHandler()\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op),\n         # Initial slice data.\n         mock.call(self.batch_norm_op),\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op),\n         # Reslicing.\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.batch_norm_op),\n         mock.call(self.relu_op),\n         # Refreshing slice data.\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op)])\n\n    # Verify manager does not group.\n    self.mock_op_reg_manager.group_op_slices.assert_not_called()\n\n    # Verify manager processes grouping for Conv2D, ReLU, and batch norm ops.\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.relu_op, self.conv_op, self.gamma_op, self.beta_op])\n    self.mock_op_reg_manager.process_ops_last.assert_called_once_with(\n        [self.batch_norm_op])\n\n  def testAssignGrouping_AllInputsGrouped(self):\n    # All inputs have groups.  Some output ops (mean_op and std_op) do not have\n    # groups.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.conv_op_group,\n        self.relu_op_slice: self.relu_op_group,\n        self.gamma_op_slice: self.conv_op_group,\n        self.beta_op_slice: self.conv_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = grouping_op_handler.GroupingOpHandler()\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op),\n         # Initial slice data.\n         mock.call(self.batch_norm_op),\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op),\n         # Reslicing.\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.batch_norm_op),\n         mock.call(self.relu_op),\n         # Refreshing slice data.\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op),\n         # Group batch norm op.\n         mock.call(self.batch_norm_op)])\n\n    # Verify manager groups batch norm with input ops.\n    self.mock_op_reg_manager.group_op_slices.assert_has_calls(\n        [mock.call([self.batch_norm_op_slice, self.relu_op_slice]),\n         mock.call([self.batch_norm_op_slice, self.conv_op_slice,\n                    self.gamma_op_slice, self.beta_op_slice])])\n\n    # Verify manager processes grouping for mean_op and std_op which do not have\n    # groups.\n    self.mock_op_reg_manager.process_ops.assert_not_called()\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n  def testAssignGrouping_AllOutputsGrouped(self):\n    # All outputs have groups.  Input beta_op does not have a group.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.conv_op_group,\n        self.relu_op_slice: self.relu_op_group,\n        self.gamma_op_slice: self.conv_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = grouping_op_handler.GroupingOpHandler()\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op),\n         # Initial slice data.\n         mock.call(self.batch_norm_op),\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op),\n         # Reslicing.\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.batch_norm_op),\n         mock.call(self.relu_op),\n         # Refreshing slice data.\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op)])\n\n    # Verify manager does not group.\n    self.mock_op_reg_manager.group_op_slices.assert_not_called()\n\n    # Verify manager processes all neighbors.\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.beta_op])\n    self.mock_op_reg_manager.process_ops_last.assert_called_once_with(\n        [self.batch_norm_op])\n\n  def testAssignGrouping_AllNeighborsGrouped(self):\n    # All neighbor ops have groups.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.conv_op_group,\n        self.relu_op_slice: self.relu_op_group,\n        self.gamma_op_slice: self.conv_op_group,\n        self.beta_op_slice: self.conv_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = grouping_op_handler.GroupingOpHandler()\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op),\n         # Initial slice data.\n         mock.call(self.batch_norm_op),\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op),\n         # Reslicing.\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.batch_norm_op),\n         mock.call(self.relu_op),\n         # Refreshing slice data.\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op),\n         # Group batch norm op.\n         mock.call(self.batch_norm_op)])\n\n    # Verify manager groups batch norm with inputs and outputs.\n    self.mock_op_reg_manager.group_op_slices.assert_has_calls(\n        [mock.call([self.batch_norm_op_slice, self.relu_op_slice]),\n         mock.call([self.batch_norm_op_slice, self.conv_op_slice,\n                    self.gamma_op_slice, self.beta_op_slice])])\n\n    # Verify manager does not process any additional ops.\n    self.mock_op_reg_manager.process_ops.assert_not_called()\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n  def testAssignGrouping_AllNeighborsGroupedSameGroup(self):\n    # All neighbor ops have same group as batch norm.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.batch_norm_op_group,\n        self.relu_op_slice: self.batch_norm_op_group,\n        self.gamma_op_slice: self.batch_norm_op_group,\n        self.beta_op_slice: self.batch_norm_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = grouping_op_handler.GroupingOpHandler()\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op),\n         # Initial slice data.\n         mock.call(self.batch_norm_op),\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op),\n         # Reslicing.\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.batch_norm_op),\n         mock.call(self.relu_op),\n         # Refreshing slice data.\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op),\n         # Group batch norm op.\n         mock.call(self.batch_norm_op)])\n\n    # Verify manager doesn\'t perform any additional grouping.\n    self.mock_op_reg_manager.group_op_slices.assert_not_called()\n\n    # Verify manager does not process any additional ops.\n    self.mock_op_reg_manager.process_ops.assert_not_called()\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n  def testAssignGrouping_NonPassthroughOutputsSkipped(self):\n    # Designate ReLU as non-passthrough for this test to demonstrate that batch\n    # norm op does not group with ReLU.\n    def is_passthrough(op):\n      if op == self.relu_op:\n        return False\n      return True\n\n    self.mock_op_reg_manager.is_passthrough.side_effect = is_passthrough\n\n    # All neighbor ops have groups.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.conv_op_group,\n        self.relu_op_slice: self.relu_op_group,\n        self.gamma_op_slice: self.conv_op_group,\n        self.beta_op_slice: self.conv_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = grouping_op_handler.GroupingOpHandler()\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.relu_op),\n         # Initial slice data.\n         mock.call(self.batch_norm_op),\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         # Reslicing.\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         mock.call(self.batch_norm_op),\n         # Refreshing slice data.\n         mock.call(self.conv_op),\n         mock.call(self.gamma_op),\n         mock.call(self.beta_op),\n         # Group batch norm op.\n         mock.call(self.batch_norm_op)])\n\n    # Verify manager groups batch norm with inputs and outputs.  ReLU is not\n    # part of the grouping.\n    self.mock_op_reg_manager.group_op_slices.assert_has_calls(\n        [mock.call([self.batch_norm_op_slice, self.conv_op_slice,\n                    self.gamma_op_slice, self.beta_op_slice])])\n\n    # Verify manager does not process any additional ops.\n    self.mock_op_reg_manager.process_ops.assert_not_called()\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n  def testGetInputOutputOpSlices(self):\n    input_ops = [self.conv_op, self.gamma_op, self.beta_op]\n    output_ops = [self.relu_op]\n\n    expected_input_op_slices = [\n        [self.conv_op_slice], [self.gamma_op_slice], [self.beta_op_slice]]\n    expected_output_op_slices = [[self.relu_op_slice]]\n\n    # Instantiate handler.\n    handler = grouping_op_handler.GroupingOpHandler()\n\n    self.assertEqual(\n        (expected_input_op_slices, expected_output_op_slices),\n        handler._get_input_output_op_slices(input_ops, output_ops,\n                                            self.mock_op_reg_manager))\n\n  def testIsBroadcast(self):\n    handler = grouping_op_handler.GroupingOpHandler()\n    self.op_group_dict = {}\n\n    # Size is not 1.\n    self.assertFalse(handler._is_broadcast(self.batch_norm_op,\n                                           self.mock_op_reg_manager))\n\n    # Size is 1 but op is not grouped.\n    ungrouped_broadcast_input = tf.zeros([2, 4, 4, 1])\n    ungrouped_broadcast_input_slice = orm.OpSlice(ungrouped_broadcast_input,\n                                                  orm.Slice(0, 1))\n    self.op_slice_dict[ungrouped_broadcast_input.op] = [\n        ungrouped_broadcast_input_slice]\n    self.assertFalse(handler._is_broadcast(ungrouped_broadcast_input.op,\n                                           self.mock_op_reg_manager))\n\n    # Size is 1 and op is grouped.\n    broadcast_input = tf.zeros([2, 4, 4, 1])\n    broadcast_input_slice = orm.OpSlice(broadcast_input.op, orm.Slice(0, 1))\n    self.op_slice_dict[broadcast_input.op] = [broadcast_input_slice]\n    broadcast_input_group = orm.OpGroup(broadcast_input_slice)\n    self.op_group_dict[broadcast_input_slice] = broadcast_input_group\n    self.assertTrue(handler._is_broadcast(broadcast_input.op,\n                                          self.mock_op_reg_manager))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/grouping_regularizers.py,7,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Regularizers that group other regularizers for residual connections.\n\nAn element-wise operation between two tensors (addition, multiplication, maximum\netc.) imposes a constraint of equality on the shapes of the constituents. For\nexample, if A, B are convolutions, and another op in the network\nreceives A + B as input, it means that the i-th output of A is tied to the i-th\noutput of B. Only if the i-th output was regularized away by the regularizer in\nboth A and B can we discard the i-th activation in both.\n\nTherefore we group the i-th output of A and the i-th output of B in a group\nLASSO, a group for each i. The grouping methods can vary, and this file offers\nseveral variants.\n\nResidual connections, in ResNet or in RNNs, are examples where this kind of\ngrouping is needed.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import generic_regularizers\nimport tensorflow.compat.v1 as tf\n\n\nDEFAULT_THRESHOLD = 0.01\n\n\nclass MaxGroupingRegularizer(generic_regularizers.OpRegularizer):\n  """"""A regularizer that groups others by taking their maximum.""""""\n\n  def __init__(self, regularizers_to_group):\n    """"""Creates an instance.\n\n    Args:\n      regularizers_to_group: A list of generic_regularizers.OpRegularizer\n        objects.Their regularization_vector (alive_vector) are expected to be of\n        the same length.\n\n    Raises:\n      ValueError: regularizers_to_group is not of length at least 2.\n    """"""\n    if len(regularizers_to_group) < 2:\n      raise ValueError(\'Groups must be of at least size 2.\')\n\n    first = regularizers_to_group[0]\n    regularization_vector = first.regularization_vector\n    alive_vector = first.alive_vector\n    for index in range(1, len(regularizers_to_group)):\n      regularizer = regularizers_to_group[index]\n      regularization_vector = tf.maximum(regularization_vector,\n                                         regularizer.regularization_vector)\n      alive_vector = tf.logical_or(alive_vector, regularizer.alive_vector)\n    self._regularization_vector = regularization_vector\n    self._alive_vector = alive_vector\n\n  @property\n  def regularization_vector(self):\n    return self._regularization_vector\n\n  @property\n  def alive_vector(self):\n    return self._alive_vector\n\n\nclass L2GroupingRegularizer(generic_regularizers.OpRegularizer):\n  r""""""A regularizer that groups others by taking their L2 norm.\n\n  R_j = sqrt((\\sum_i r_{ij}^2))\n\n  Where r_i is the i-th regularization vector, r_{ij} is its j-th element, and\n  R_j is the j-th element of the resulting regularization vector.\n  """"""\n\n  def __init__(self, regularizers_to_group, threshold=DEFAULT_THRESHOLD):\n    """"""Creates an instance.\n\n    Args:\n      regularizers_to_group: A list of generic_regularizers.OpRegularizer\n        objects.Their regularization_vector (alive_vector) are expected to be of\n        the same length.\n      threshold: A float. An group of activations will be considered alive if\n        its L2 norm is greater than `threshold`.\n\n    Raises:\n      ValueError: regularizers_to_group is not of length at least 2.\n    """"""\n    if len(regularizers_to_group) < 2:\n      raise ValueError(\'Groups must be of at least size 2.\')\n    self._regularization_vector = tf.sqrt(\n        tf.add_n([\n            lazy_square(r.regularization_vector)\n            for r in regularizers_to_group\n        ]))\n    self._alive_vector = self._regularization_vector > threshold\n\n  @property\n  def regularization_vector(self):\n    return self._regularization_vector\n\n  @property\n  def alive_vector(self):\n    return self._alive_vector\n\n\ndef lazy_square(tensor):\n  """"""Computes the square of a tensor in a lazy way.\n\n  This function is lazy in the following sense, for:\n    tensor = tf.sqrt(input)\n  will return input (and not tf.square(tensor)).\n\n  Args:\n    tensor: A `Tensor` of floats to compute the square of.\n\n  Returns:\n    The square of the input tensor.\n  """"""\n  if tensor.op.type == \'Sqrt\':\n    return tensor.op.inputs[0]\n  else:\n    return tf.square(tensor)\n'"
morph_net/framework/grouping_regularizers_test.py,2,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for framework.grouping_regularizers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom morph_net.framework import grouping_regularizers\nfrom morph_net.testing import op_regularizer_stub\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\n\ndef _l2_reg_with_025_threshold(regularizers_to_group):\n  return grouping_regularizers.L2GroupingRegularizer(regularizers_to_group,\n                                                     0.25)\n\n\ndef _l2_reg_with_05_threshold(regularizers_to_group):\n  return grouping_regularizers.L2GroupingRegularizer(regularizers_to_group, 0.5)\n\n\nclass GroupingRegularizersTest(parameterized.TestCase, tf.test.TestCase):\n\n  # TODO(e1): Add parametrized tests.\n  def setUp(self):\n    super(GroupingRegularizersTest, self).setUp()\n    self._reg_vec1 = [0.1, 0.3, 0.6, 0.2]\n    self._alive_vec1 = [False, True, True, False]\n    self._reg_vec2 = [0.2, 0.4, 0.5, 0.1]\n    self._alive_vec2 = [False, True, False, True]\n    self._reg_vec3 = [0.3, 0.2, 0.0, 0.25]\n    self._alive_vec3 = [False, True, False, True]\n\n    self._reg1 = op_regularizer_stub.OpRegularizerStub(self._reg_vec1,\n                                                       self._alive_vec1)\n    self._reg2 = op_regularizer_stub.OpRegularizerStub(self._reg_vec2,\n                                                       self._alive_vec2)\n    self._reg3 = op_regularizer_stub.OpRegularizerStub(self._reg_vec3,\n                                                       self._alive_vec3)\n\n  def testMaxGroupingRegularizer(self):\n    group_reg = grouping_regularizers.MaxGroupingRegularizer(\n        [self._reg1, self._reg2])\n    with self.cached_session():\n      self.assertAllEqual(\n          [x or y for x, y in zip(self._alive_vec1, self._alive_vec2)],\n          group_reg.alive_vector.eval())\n      self.assertAllClose(\n          [max(x, y) for x, y in zip(self._reg_vec1, self._reg_vec2)],\n          group_reg.regularization_vector.eval(), 1e-5)\n\n  def testL2GroupingRegularizer(self):\n    group_reg = grouping_regularizers.L2GroupingRegularizer(\n        [self._reg1, self._reg2], 0.25)\n    expcted_reg_vec = [\n        np.sqrt((x**2 + y**2))\n        for x, y in zip(self._reg_vec1, self._reg_vec2)\n    ]\n    with self.cached_session():\n      self.assertAllEqual([x > 0.25 for x in expcted_reg_vec],\n                          group_reg.alive_vector.eval())\n      self.assertAllClose(expcted_reg_vec,\n                          group_reg.regularization_vector.eval(), 1e-5)\n\n  @parameterized.named_parameters(\n      (\'Max\', grouping_regularizers.MaxGroupingRegularizer),\n      (\'L2\', _l2_reg_with_025_threshold))\n  def testOrderDoesNotMatter(self, create_reg):\n    group12 = create_reg([self._reg1, self._reg2])\n    group13 = create_reg([self._reg1, self._reg3])\n    group23 = create_reg([self._reg2, self._reg3])\n\n    group123 = create_reg([group12, self._reg3])\n    group132 = create_reg([group13, self._reg2])\n    group231 = create_reg([group23, self._reg1])\n\n    with self.cached_session():\n      self.assertAllEqual(group123.alive_vector.eval(),\n                          group132.alive_vector.eval())\n      self.assertAllEqual(group123.alive_vector.eval(),\n                          group231.alive_vector.eval())\n\n      self.assertAllClose(group123.regularization_vector.eval(),\n                          group132.regularization_vector.eval())\n      self.assertAllClose(group123.regularization_vector.eval(),\n                          group231.regularization_vector.eval())\n\n  @parameterized.named_parameters(\n      (\'Max\', grouping_regularizers.MaxGroupingRegularizer,\n       [0.3, 0.4, 0.6, 0.25], [False, True, True, True]),\n      (\'L2\', _l2_reg_with_025_threshold,\n       [0.374165, 0.538516, 0.781024, 0.335410], [True, True, True, True]),\n      (\'L2_0.5\', _l2_reg_with_05_threshold,\n       [0.374165, 0.538516, 0.781024, 0.335410], [False, True, True, False]))\n  def testThreeWayGroups(self, create_reg, expected_vector, expected_alive):\n    group = create_reg([self._reg1, self._reg2, self._reg3])\n    with self.cached_session():\n      self.assertAllEqual(group.alive_vector.eval(), expected_alive)\n      self.assertAllClose(group.regularization_vector.eval(), expected_vector)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/leaf_op_handler.py,1,"b'""""""OpHandler implementation for leaf operations.\n\nA leaf operation terminates the OpRegularizerManager graph traversal.  This is\ntypically network inputs, constants, variables, etc.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import op_handler\nfrom morph_net.framework import op_handler_util\n\n\nclass LeafOpHandler(op_handler.OpHandler):\n  """"""OpHandler implementation for leaf operations.""""""\n\n  @property\n  def is_source_op(self):\n    return False\n\n  @property\n  def is_passthrough(self):\n    return False\n\n  def assign_grouping(self, op, op_reg_manager):\n    """"""Assign grouping to the given op and updates the manager.\n\n    Args:\n      op: tf.Operation to assign grouping to.\n      op_reg_manager: OpRegularizerManager to keep track of the grouping.\n    """"""\n    # Check if all output ops have groups, or tell the manager to process them.\n    output_ops = op_handler_util.get_output_ops(op, op_reg_manager)\n    output_ops_without_group = op_handler_util.get_ops_without_groups(\n        output_ops, op_reg_manager)\n\n    # Remove non-passthrough ops from outputs ops to group with.\n    output_ops = op_handler_util.remove_non_passthrough_ops(\n        output_ops, op_reg_manager)\n\n    # Only group with ops that have the same size.  Process the ops that have\n    # mismatched size.\n    output_ops_to_group, output_ops_to_process = (\n        op_handler_util.separate_same_size_ops(op, output_ops))\n\n    # Also process ungrouped ops.\n    for output_op_without_group in output_ops_without_group:\n      if output_op_without_group not in output_ops_to_process:\n        output_ops_to_process.append(output_op_without_group)\n\n    # Align op slice sizes if needed.\n    op_slices = op_reg_manager.get_op_slices(op)\n    output_op_slices = op_handler_util.get_op_slices(\n        output_ops_to_group, op_reg_manager)\n    aligned_op_slice_sizes = op_handler_util.get_aligned_op_slice_sizes(\n        op_slices, [], output_op_slices)\n    op_handler_util.reslice_ops([op] + output_ops_to_group,\n                                aligned_op_slice_sizes, op_reg_manager)\n\n    # Repopulate OpSlice data, as ops may have been resliced.\n    output_op_slices = op_handler_util.get_op_slices(\n        output_ops_to_group, op_reg_manager)\n\n    # Group with outputs.\n    op_handler_util.group_aligned_input_output_slices(\n        op, [], output_ops_to_process, [], output_op_slices,\n        aligned_op_slice_sizes, op_reg_manager)\n\n  def create_regularizer(self, _):\n    raise NotImplementedError(\'Not a source op.\')\n'"
morph_net/framework/leaf_op_handler_test.py,4,"b'""""""Tests for leaf_op_handler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport mock\nfrom morph_net.framework import leaf_op_handler\nfrom morph_net.framework import op_regularizer_manager as orm\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import framework\nfrom tensorflow.contrib import layers\n\narg_scope = framework.arg_scope\n\n\nclass LeafOpHandlerTest(tf.test.TestCase):\n\n  def _batch_norm_scope(self):\n    params = {\n        \'trainable\': True,\n        \'normalizer_fn\': layers.batch_norm,\n        \'normalizer_params\': {\n            \'scale\': True,\n        },\n    }\n\n    with arg_scope([layers.conv2d], **params) as sc:\n      return sc\n\n  def setUp(self):\n    super(LeafOpHandlerTest, self).setUp()\n\n    # This tests a Conv2D -> BatchNorm -> ReLU chain of ops.\n    with framework.arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      layers.conv2d(inputs, num_outputs=5, kernel_size=3, scope=\'conv1\')\n\n    g = tf.get_default_graph()\n\n    # Declare OpSlice and OpGroup for ops of interest.\n    self.batch_norm_op = g.get_operation_by_name(\n        \'conv1/BatchNorm/FusedBatchNormV3\')\n    self.batch_norm_op_slice = orm.OpSlice(self.batch_norm_op, orm.Slice(0, 5))\n    self.batch_norm_op_group = orm.OpGroup(self.batch_norm_op_slice)\n\n    self.conv_op = g.get_operation_by_name(\'conv1/Conv2D\')\n    self.conv_op_slice = orm.OpSlice(self.conv_op, orm.Slice(0, 5))\n    self.conv_op_group = orm.OpGroup(\n        self.conv_op_slice, omit_source_op_slices=[self.conv_op_slice])\n\n    self.relu_op = g.get_operation_by_name(\'conv1/Relu\')\n    self.relu_op_slice = orm.OpSlice(self.relu_op, orm.Slice(0, 5))\n    self.relu_op_group = orm.OpGroup(self.relu_op_slice)\n\n    self.gamma_op = g.get_operation_by_name(\'conv1/BatchNorm/gamma/read\')\n    self.gamma_op_slice = orm.OpSlice(self.gamma_op, orm.Slice(0, 5))\n\n    self.beta_op = g.get_operation_by_name(\'conv1/BatchNorm/beta/read\')\n    self.beta_op_slice = orm.OpSlice(self.beta_op, orm.Slice(0, 5))\n\n    # Create mock OpRegularizerManager with custom mapping of OpSlice and\n    # OpGroup.\n    self.mock_op_reg_manager = mock.create_autospec(orm.OpRegularizerManager)\n\n    self.op_slice_dict = {\n        self.batch_norm_op: [self.batch_norm_op_slice],\n        self.conv_op: [self.conv_op_slice],\n        self.relu_op: [self.relu_op_slice],\n        self.gamma_op: [self.gamma_op_slice],\n        self.beta_op: [self.beta_op_slice],\n    }\n    def get_op_slices(op):\n      return self.op_slice_dict.get(op)\n\n    def get_op_group(op_slice):\n      return self.op_group_dict.get(op_slice)\n\n    self.mock_op_reg_manager.get_op_slices.side_effect = get_op_slices\n    self.mock_op_reg_manager.get_op_group.side_effect = get_op_group\n    self.mock_op_reg_manager.is_source_op.return_value = False\n    self.mock_op_reg_manager.ops = [\n        self.batch_norm_op, self.conv_op, self.relu_op, self.gamma_op,\n        self.beta_op]\n\n  def testAssignGrouping_NoNeighborGroups(self):\n    # No ops have groups.\n    self.op_group_dict = {}\n\n    # Call handler to assign grouping.\n    handler = leaf_op_handler.LeafOpHandler()\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu_op),\n         # Initial slice data.\n         mock.call(self.batch_norm_op),\n         mock.call(self.relu_op),\n         # Reslicing.\n         mock.call(self.batch_norm_op),\n         mock.call(self.relu_op),\n         # Refreshing slice data.\n         mock.call(self.relu_op)])\n\n    # Verify manager groups leaf op.\n    self.mock_op_reg_manager.group_op_slices.assert_called_once_with(\n        [self.batch_norm_op_slice])\n\n    # Verify manager processes grouping for batch norm and output ops.\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.relu_op])\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n  def testAssignGrouping_AllOutputsGrouped(self):\n    # All outputs have groups.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.relu_op_slice: self.relu_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = leaf_op_handler.LeafOpHandler()\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu_op),\n         # Initial slice data.\n         mock.call(self.batch_norm_op),\n         mock.call(self.relu_op),\n         # Reslicing.\n         mock.call(self.batch_norm_op),\n         mock.call(self.relu_op),\n         # Refreshing slice data.\n         mock.call(self.relu_op)])\n\n    # Verify manager groups batch norm and outputs.\n    self.mock_op_reg_manager.group_op_slices.assert_called_once_with(\n        [self.batch_norm_op_slice, self.relu_op_slice])\n\n    # Verify manager processes all neighbors.\n    self.mock_op_reg_manager.process_ops.assert_not_called()\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n  def testAssignGrouping_AllNeighborsGrouped(self):\n    # All neighbor ops have groups.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.conv_op_group,\n        self.relu_op_slice: self.relu_op_group,\n        self.gamma_op_slice: self.conv_op_group,\n        self.beta_op_slice: self.conv_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = leaf_op_handler.LeafOpHandler()\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu_op),\n         # Initial slice data.\n         mock.call(self.batch_norm_op),\n         mock.call(self.relu_op),\n         # Reslicing.\n         mock.call(self.batch_norm_op),\n         mock.call(self.relu_op),\n         # Refreshing slice data.\n         mock.call(self.relu_op),\n         # Group batch norm op.\n         mock.call(self.batch_norm_op)])\n\n    # Verify manager groups batch norm and outputs.\n    self.mock_op_reg_manager.group_op_slices.assert_called_once_with(\n        [self.batch_norm_op_slice, self.relu_op_slice])\n\n    # Verify manager does not process any additional ops.\n    self.mock_op_reg_manager.process_ops.assert_not_called()\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n  def testAssignGrouping_AllNeighborsGroupedSameGroup(self):\n    # All neighbor ops have same group as batch norm.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.batch_norm_op_group,\n        self.relu_op_slice: self.batch_norm_op_group,\n        self.gamma_op_slice: self.batch_norm_op_group,\n        self.beta_op_slice: self.batch_norm_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = leaf_op_handler.LeafOpHandler()\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu_op),\n         # Initial slice data.\n         mock.call(self.batch_norm_op),\n         mock.call(self.relu_op),\n         # Reslicing.\n         mock.call(self.batch_norm_op),\n         mock.call(self.relu_op),\n         # Refreshing slice data.\n         mock.call(self.relu_op),\n         # Group batch norm op.\n         mock.call(self.batch_norm_op)])\n\n    # Verify manager doesn\'t perform any additional grouping.\n    self.mock_op_reg_manager.group_op_slices.assert_not_called()\n\n    # Verify manager does not process any additional ops.\n    self.mock_op_reg_manager.process_ops.assert_not_called()\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n  def testAssignGrouping_NonPassthroughOutputsSkipped(self):\n    # Designate ReLU as non-passthrough for this test to demonstrate that batch\n    # norm op does not group with ReLU.\n    def is_passthrough(op):\n      if op == self.relu_op:\n        return False\n      return True\n\n    self.mock_op_reg_manager.is_passthrough.side_effect = is_passthrough\n\n    # All neighbor ops have groups.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.conv_op_group,\n        self.relu_op_slice: self.relu_op_group,\n        self.gamma_op_slice: self.conv_op_group,\n        self.beta_op_slice: self.conv_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = leaf_op_handler.LeafOpHandler()\n    handler.assign_grouping(self.batch_norm_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu_op),\n         # Initial slice data.\n         mock.call(self.batch_norm_op),\n         # Reslicing.\n         mock.call(self.batch_norm_op),\n         # Group batch norm op.\n         mock.call(self.batch_norm_op)])\n\n    # Verify ReLU is not part of the grouping due to being non-passthrough.\n    self.mock_op_reg_manager.group_op_slices.assert_not_called()\n\n    # Verify manager does not process any additional ops.\n    self.mock_op_reg_manager.process_ops.assert_not_called()\n    self.mock_op_reg_manager.process_ops_last.assert_not_called()\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/matmul_source_op_handler.py,2,"b'""""""OpHandler implementation for MatMul ops that are regularizer sources.\n\nOpHandler for MatMul ops source ops that use group lasso regularization.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom morph_net.framework import group_lasso_base_op_handler\nimport tensorflow.compat.v1 as tf\n\n\nclass MatMulSourceOpHandler(\n    group_lasso_base_op_handler.GroupLassoBaseSourceOpHandler):\n  """"""OpHandler for MatMul source operations.""""""\n  # TODO(a1): Sometimes this should not be skipped (b/123778611)\n\n  def _reduce_dims(self, op):\n    tf.logging.info(\'MatMulSourceOpHandler: found kernel = %s for op %s\',\n                    op.inputs[0], op.type)\n    # Reduction dimensions for Group Lasso.\n    try:\n      if op.get_attr(\'transpose_b\'):\n        return (1,)\n    except ValueError:\n      tf.logging.warning(\n          \'MatMulSourceOpHandler: used on op.type %s with no transpose_b attr\',\n          op.type)\n    return (0,)\n'"
morph_net/framework/matmul_source_op_handler_test.py,7,"b'""""""Tests for morph_net.framework.matmul_source_op_handler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom absl.testing import parameterized\nfrom morph_net.framework import matmul_source_op_handler\nfrom morph_net.framework import op_regularizer_manager as orm\nimport tensorflow.compat.v1 as tf\n\n\nclass MatmulSourceOpHandlerTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters((\'_slice_all\', 3), (\'_part_slice\', 2))\n  def testMatMul2D(self, size):\n    inputs = tf.zeros((13, 2))\n    handler = matmul_source_op_handler.MatMulSourceOpHandler(0.1)\n\n    kernel = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)\n    x = tf.matmul(inputs, kernel, transpose_b=False, name=\'MatMul\')\n    op_slice = orm.OpSlice(x.op, orm.Slice(0, size))\n\n    transpose_kernel = tf.constant([[1, 4], [2, 5], [3, 6]], dtype=tf.float32)\n    x_other = tf.matmul(\n        inputs,\n        transpose_kernel,\n        transpose_b=True,\n        name=\'MatMulTransposedKernel\')\n    op_slice_other = orm.OpSlice(x_other.op, orm.Slice(0, size))\n\n    self.assertAllClose(\n        handler.create_regularizer(op_slice).regularization_vector,\n        handler.create_regularizer(op_slice_other).regularization_vector)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/op_handler.py,1,"b'""""""Interface for OpHandler classes.\n\nAn OpHandler contains the logic for assigning a regularizer to an op type.  The\nOpRegularizerManager uses a dictionary of {op type: OpHandler} to assign\nregularizers to ops in the network.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\n\nclass OpHandler(object):\n  """"""An interface for OpHandler classes.\n\n  An OpHandler contains the logic for assigning a regularizer to an op type.\n  The OpHandler denotes if the op type is considered a source of regularization,\n  or how the regularization is derived from neighboring ops.  Ops are first\n  assigned grouping if they share a regularizer.  Then, regularizers are created\n  for the groups.\n  """"""\n  __metaclass__ = abc.ABCMeta\n\n  @abc.abstractproperty\n  def is_source_op(self):\n    """"""Returns True if this op type is a regularizer source.""""""\n\n  @abc.abstractproperty\n  def is_passthrough(self):\n    """"""Returns True if this op type is considered passthrough.\n\n    Neighboring ops may query this property before deciding to group with this\n    op.  For example, consider OpX followed by convolution followed by batch\n    norm (source op).  The batch norm creates its own group and adds the\n    convolution to the processing queue.  Then the convolution groups itself\n    with the output op (batch norm) and adds OpX to the processing queue.  OpX\n    would examine its output (the convolution) but see that\n    is_passthrough=False.  Thus, OpX would NOT group itself with the convolution\n    and would group with the input instead.\n    """"""\n\n  @abc.abstractmethod\n  def assign_grouping(self, op, op_reg_manager):\n    """"""Assign grouping to the given op and updates the manager.\n\n    Each OpHandler includes custom logic for how to assign op grouping.  This\n    logic could consider: if this op type is a source, if neighboring ops have\n    groupings and are passthrough, if all inputs or outputs have groupings, if\n    channels need to be concat/sliced into groups, etc.\n\n    For example, generic passthrough ops can group with input or output\n    neighbors, but Conv2D ops would not group with input ops.\n\n    Once the OpHandler determines how to group the op, OpRegularizerManager\n    should be updated with the resulting grouping.  Finally, OpHandler should\n    also decide which neighboring ops should be put into the queue for\n    OpRegularizerManager to process.\n\n    Args:\n      op: tf.Operation to assign grouping to.\n      op_reg_manager: OpRegularizerManager to keep track of the grouping.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def create_regularizer(self, op_slice):\n    """"""Create a regularizer for a source OpSlice.\n\n    Args:\n      op_slice: op_regularizer_manager.OpSlice that is a source OpSlice.\n\n    Returns:\n      OpRegularizer for the source OpSlice.\n    """"""\n    pass\n'"
morph_net/framework/op_handler_decorator.py,0,"b'""""""Support for overriding op regularization penalty.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import op_handler\n\n\nclass OpHandlerDecorator(op_handler.OpHandler):\n  """"""A decorator for OpHandler implementations.\n\n  This decorator overrides the create_regularizer method, allowing customization\n  of the regularization penalty used.  Other members of the original OpHandler\n  are unchanged.\n  """"""\n\n  def __init__(self, handler, regularizer_decorator=None,\n               decorator_parameters=None):\n    """"""Creates an instance.\n\n    Args:\n      handler: OpHandler to be decorated.\n      regularizer_decorator: OpRegularizer decorator to apply to OpRegularizer\n        returned by create_regularizer method of handler.  If None, the\n        OpRegularizer is unchanged.\n      decorator_parameters: Dictionary of regularizer decorator parameters.\n        None or {} will pass no parameters.\n    """"""\n    self._op_handler = handler\n    self._regularization_decorator = regularizer_decorator\n    self._decorator_parameters = decorator_parameters or {}\n\n  @property\n  def is_source_op(self):\n    return self._op_handler.is_source_op\n\n  @property\n  def is_passthrough(self):\n    return self._op_handler.is_passthrough\n\n  def assign_grouping(self, op, op_reg_manager):\n    self._op_handler.assign_grouping(op, op_reg_manager)\n\n  def create_regularizer(self, op_slice):\n    """"""Creates a decorated OpRegularizer for the given OpSlice.\n\n    Args:\n      op_slice: op_regularizer_manager.OpSlice to create a regularizer for.\n\n    Returns:\n      A decorated OpRegularizer for the given OpSlice.\n    """"""\n    regularizer = self._op_handler.create_regularizer(op_slice)\n    if regularizer and self._regularization_decorator:\n      regularizer = self._regularization_decorator(\n          regularizer, **(self._decorator_parameters))\n\n    return regularizer\n\n'"
morph_net/framework/op_handler_decorator_test.py,5,"b'""""""Tests for morph_net.framework.op_regularizer_decorator.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import conv_source_op_handler\nfrom morph_net.framework import generic_regularizers\nfrom morph_net.framework import op_handler_decorator\nfrom morph_net.framework import op_regularizer_manager as orm\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\n\nclass DummyDecorator(generic_regularizers.OpRegularizer):\n  """"""A dummy decorator that multiply the regularization vector by 0.5.\n\n  """"""\n\n  def __init__(self, regularizer_object):\n    """"""Creates an instance.\n\n    Accept an OpRegularizer that is decorated by this class.\n\n    Args:\n      regularizer_object: OpRegularizer to decorate.\n    """"""\n\n    self._regularization_vector = regularizer_object.regularization_vector * 0.5\n    self._alive_vector = regularizer_object.alive_vector\n\n  @property\n  def regularization_vector(self):\n    return self._regularization_vector\n\n  @property\n  def alive_vector(self):\n    return self._alive_vector\n\n\nclass OpHandlerDecoratorTest(tf.test.TestCase):\n  """"""Test class for OpHandlerDecorator.""""""\n\n  def testOpHandlerDecorator(self):\n    image = tf.constant(0.0, shape=[1, 17, 19, 3])\n    kernel = tf.ones([5, 5, 3, 3])\n\n    output = tf.nn.conv2d(image, kernel, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n    decorated_op_handler = op_handler_decorator.OpHandlerDecorator(\n        conv_source_op_handler.ConvSourceOpHandler(1e-3, 0), DummyDecorator)\n    op_slice = orm.OpSlice(output.op, orm.Slice(0, 3))\n    regularizer = decorated_op_handler.create_regularizer(op_slice)\n\n    self.assertAllClose(0.5 * np.ones(3), regularizer.regularization_vector)\n    self.assertAllClose(np.ones(3), regularizer.alive_vector)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/op_handler_util.py,29,"b'""""""Utility methods for working with OpHandler and tf.Operation.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport tensorflow.compat.v1 as tf\n\nOP_TYPES_WITH_MULTIPLE_OUTPUTS = (\'SplitV\',)\n\n# Dictionary mapping op type to input index of weights.\nWEIGHTS_INDEX_DICT = {\n    \'Conv2D\': 1,\n    \'Conv2DBackpropInput\': 1,\n    \'DepthwiseConv2dNative\': 1,\n    \'MatMul\': 1\n}\n\n\ndef get_input_ops(op, op_reg_manager, whitelist_indices=None):\n  """"""Returns input ops for a given op.\n\n  Filters constants and weight tensors.\n\n  Args:\n    op: tf.Operation to get inputs of.\n    op_reg_manager: OpRegularizerManager to keep track of the grouping.\n    whitelist_indices: Optional, indices of op.inputs that should be considered.\n\n  Returns:\n    List of tf.Operation that are the inputs to op.\n  """"""\n  # Ignore scalar or 1-D constant inputs.\n  def is_const(tensor):\n    return tensor.op.type == \'Const\'\n  def is_weight_tensor(i, op_type):\n    return i == WEIGHTS_INDEX_DICT.get(op_type, -666)  # If op_type not in dict.\n\n  # If op has a weight tensor as an input, remove it.\n  inputs = list(op.inputs)\n\n  whitelist_indices = whitelist_indices or range(len(inputs))\n  filted_input_ops = []\n  for i, tensor in enumerate(inputs):\n    if (i not in whitelist_indices\n        or is_weight_tensor(i, op.type)\n        or is_const(tensor)\n        or tensor.op not in op_reg_manager.ops):\n      continue\n    filted_input_ops.append(tensor.op)\n  return filted_input_ops\n\n\ndef get_output_ops(op, op_reg_manager):\n  """"""Returns output ops for a given op.\n\n  Args:\n    op: tf.Operation to get outputs of.\n    op_reg_manager: OpRegularizerManager to keep track of the grouping.\n\n  Returns:\n    List of tf.Operation that are the outputs of op.\n  """"""\n  output_ops = []\n  for output_tensor in op.outputs:\n    for output_op in output_tensor.consumers():\n      if output_op not in output_ops and output_op in op_reg_manager.ops:\n        output_ops.append(output_op)\n  return output_ops\n\n\ndef get_ops_without_groups(ops, op_reg_manager):\n  """"""Returns ops without OpGroup.\n\n  Args:\n    ops: List of tf.Operation.\n    op_reg_manager: OpRegularizerManager to keep track of the grouping.\n\n  Returns:\n    List of tf.Operation that do not have OpGroup assigned.\n  """"""\n  ops_without_groups = []\n  for op in ops:\n    op_slices = op_reg_manager.get_op_slices(op)\n    for op_slice in op_slices:\n      op_group = op_reg_manager.get_op_group(op_slice)\n      if op_group is None:\n        ops_without_groups.append(op)\n        break\n\n  return ops_without_groups\n\n\ndef remove_non_passthrough_ops(ops, op_reg_manager):\n  """"""Removes non-passthrough ops from ops.\n\n  Args:\n    ops: List of tf.Operation.\n    op_reg_manager: OpRegularizerManager to keep track of the grouping.\n\n  Returns:\n    List of tf.Operation of only passthrough ops in ops.\n  """"""\n  return [op for op in ops if op_reg_manager.is_passthrough(op)]\n\n\ndef group_op_with_inputs_and_outputs(op, input_op_slices, output_op_slices,\n                                     aligned_op_slice_sizes, op_reg_manager):\n  """"""Groups op with inputs and outputs if grouping is inconsistent.\n\n  Args:\n    op: tf.Operation.\n    input_op_slices: List of list of OpSlice, with a list per input op.\n    output_op_slices: List of list of OpSlice, with a list per output op.\n    aligned_op_slice_sizes: List of integer OpSlice sizes.\n    op_reg_manager: OpRegularizerManager to keep track of the grouping.\n\n  Returns:\n    Boolean indicating if grouping was inconsistent.\n  """"""\n  op_slices = op_reg_manager.get_op_slices(op)\n\n  inconsistent_grouping = False\n  # Group aligned OpSlice by iterating along each slice.\n  for slice_index in range(len(aligned_op_slice_sizes)):\n    op_group = op_reg_manager.get_op_group(op_slices[slice_index])\n    output_op_slices_at_index = [output_op_slice[slice_index]\n                                 for output_op_slice in output_op_slices]\n    input_op_slices_at_index = [input_op_slice[slice_index]\n                                for input_op_slice in input_op_slices]\n\n    if op_group is None:\n      # The current op does not have a group.  Group with inputs and outputs.\n      op_reg_manager.group_op_slices(\n          [op_slices[slice_index]] + input_op_slices_at_index\n          + output_op_slices_at_index)\n      continue\n\n    if any([op_group != op_reg_manager.get_op_group(output_op_slice)\n            for output_op_slice in output_op_slices_at_index]):\n      # Some output OpSlice have different grouping.\n      op_reg_manager.group_op_slices(\n          [op_slices[slice_index]] + output_op_slices_at_index)\n      # Refesh OpGroup before comparing with input groups.\n      op_group = op_reg_manager.get_op_group(op_slices[slice_index])\n      inconsistent_grouping = True\n\n    if any([op_group != op_reg_manager.get_op_group(input_op_slice)\n            for input_op_slice in input_op_slices_at_index]):\n      # Some input OpSlice have different grouping.\n      op_slice = op_slices[slice_index]\n\n      # Check if inputs have source ops. The default behavior is to regularize\n      # all sources in the group; however, depending on the local structure, it\n      # may be unnecessary to regularize these input sources. Flag this as a\n      # potential issue.\n      source_op_slices = _get_source_op_slices([op_slice], op_reg_manager)\n      input_source_op_slices = _get_source_op_slices(\n          input_op_slices_at_index, op_reg_manager)\n      input_source_op_slices_to_be_merged = [s for s in input_source_op_slices\n                                             if s not in source_op_slices]\n      if source_op_slices and input_source_op_slices_to_be_merged:\n        tf.logging.warn(\'Potential overregularization near {}.\'.format(op.name))\n        tf.logging.warn(\'Downstream source slices:\')\n        for ss in source_op_slices:\n          tf.logging.warn(\'  {}\'.format(ss))\n        tf.logging.warn(\'...to be merged with upstream source slices:\')\n        for ss in input_source_op_slices_to_be_merged:\n          tf.logging.warn(\'  {}\'.format(ss))\n        tf.logging.warn(\'\')\n\n      op_reg_manager.group_op_slices([op_slice] + input_op_slices_at_index)\n      inconsistent_grouping = True\n\n  return inconsistent_grouping\n\n\ndef get_concat_input_op_slices(concat_ops, op_reg_manager):\n  """"""Returns OpSlice for concat input ops to concatenate.\n\n  For concat, all input OpSlice should be stacked to align with the concat\n  OpSlice.  Also, the last input is the axis which should be omitted.\n\n  Args:\n    concat_ops: List of tf.Operation which provide inputs to the concat op.\n    op_reg_manager: OpRegularizerManager that tracks the slicing.\n\n  Returns:\n    List of list of OpSlice, where the outer list only has 1 element, and the\n    inner list is the concatenation of input OpSlice.\n  """"""\n  concat_input_op_slices = []\n  for concat_op in concat_ops:\n    concat_input_op_slices.extend(op_reg_manager.get_op_slices(concat_op))\n\n  return [concat_input_op_slices]\n\n\ndef get_op_slices(ops, op_reg_manager):\n  """"""Returns list of OpSlice per op in a list of ops.\n\n  Args:\n    ops: List of tf.Operation.\n    op_reg_manager: OpRegularizerManager that tracks the slicing.\n\n  Returns:\n    List of list of OpSlice, where the outer list has a list per op, and the\n    inner list is a list of OpSlice that compose the op.\n  """"""\n  op_slices = []\n  for op in ops:\n    op_slices.append(op_reg_manager.get_op_slices(op))\n  return list(filter(None, op_slices))\n\n\ndef get_op_slice_sizes(op_slices):\n  """"""Returns OpSlice sizes for a list of list of OpSlice.\n\n  The outer list has an element per op, while the inner list is the list of\n  OpSlice that compose the op.\n\n  Args:\n    op_slices: List of list of OpSlice.\n\n  Returns:\n    List of list of OpSlice sizes where the outer list has an entry per op.\n  """"""\n  op_slice_sizes = []\n  for op in op_slices:\n    op_slice_sizes.append([op_slice.slice.size for op_slice in op])\n\n  return op_slice_sizes\n\n\ndef get_aligned_op_slice_sizes(op_slices, input_op_slices, output_op_slices):\n  """"""Returns list of OpSlice sizes with aligned sizes.\n\n  Given lists of OpSlice for an op and its inputs and outputs, returns the\n  smallest list of slice sizes that aligns the slices.  For example, given an\n  input of [[1, 2], [3]] representing a first op with slice sizes [1, 2] and a\n  second op with op slice size [3], then the aligned slice sizes is [1, 2] to be\n  compatible.  This means the second op would need to be sliced to match the\n  aligned slice sizes.  As another example, given an input of [[2, 5], [3, 4]],\n  both ops would need to be resliced.  The smallest list of slice sizes that\n  aligns the 2 ops is [2, 1, 4].  Finally, consider the example\n  [[5, 6, 7], [9, 4, 5], [18]], which returns [5, 4, 2, 2, 5].  Once the slice\n  sizes are aligned, the corresponding slices are of matching size and can be\n  grouped for the purpose of regularization.\n\n  Given lists of OpSlice for an op and its inputs and outputs, returns the\n  smallest list of slice sizes that aligns the slices.\n\n  Args:\n    op_slices: List of OpSlice for an op.\n    input_op_slices: List of list of OpSlice, with a list per input op.\n    output_op_slices: List of list of OpSlice, with a list per output op.\n\n  Returns:\n    List of integer slice sizes which is the smallest list of aligned sizes.\n  """"""\n  # TODO(a1): Create a helper class to manage list of list of OpSlice.\n  input_op_slice_sizes = get_op_slice_sizes(input_op_slices)\n  output_op_slices_sizes = get_op_slice_sizes(output_op_slices)\n  op_slice_sizes = get_op_slice_sizes([op_slices])\n\n  all_op_slice_sizes = (input_op_slice_sizes + output_op_slices_sizes\n                        + op_slice_sizes)\n  return get_aligned_sizes(all_op_slice_sizes)\n\n\ndef get_aligned_sizes(op_slice_sizes):\n  """"""Returns list of OpSlice sizes with aligned sizes.\n\n  Given a list of OpSlice sizes, returns the smallest list of slice sizes that\n  aligns the slices.\n\n  Args:\n    op_slice_sizes: List of list of slice sizes, where the outer list has a list\n      per op and the inner list is the integer slice sizes of the op.\n\n  Returns:\n    List of integer slice sizes which is the smallest list of aligned sizes.\n\n  Raises:\n    ValueError: If op_slice_sizes is empty.\n    ValueError: If slice size lists do not have the same total size.\n  """"""\n  # Check for empty list.\n  if not op_slice_sizes:\n    raise ValueError(\'Cannot align empty op slice lists.\')\n\n  # Check that all ops have the same total size.\n  total_slice_sizes = [\n      get_total_slice_size(op_slice_size, 0, len(op_slice_size))\n      for op_slice_size in op_slice_sizes]\n  if total_slice_sizes.count(total_slice_sizes[0]) != len(total_slice_sizes):\n    raise ValueError(\n        \'Total size for op slices do not match: %s\' % op_slice_sizes)\n\n  # Make local copy of op_slice_sizes for destruction.\n  aligned_op_slice_sizes = [list(op_slice_size)\n                            for op_slice_size in op_slice_sizes]\n  slice_index = 0\n  num_slices = _get_num_slices(op_slice_sizes)\n  # Iterate slice by slice to check if slice sizes match across ops, or if they\n  # need to be split further.\n  while slice_index < num_slices:\n    op_slices_at_index = [slice_size[slice_index]\n                          for slice_size in aligned_op_slice_sizes]\n    min_slice_size = min(op_slices_at_index)\n    for op_index in range(len(aligned_op_slice_sizes)):\n      old_size = aligned_op_slice_sizes[op_index][slice_index]\n      if old_size != min_slice_size:\n        # This OpSlice is bigger than the minimum, meaning this op needs to be\n        # sliced again to match sizes.\n        aligned_op_slice_sizes[op_index][slice_index] = min_slice_size\n        aligned_op_slice_sizes[op_index].insert(\n            slice_index + 1, old_size - min_slice_size)\n    num_slices = _get_num_slices(aligned_op_slice_sizes)\n    slice_index += 1\n  return aligned_op_slice_sizes[0]\n\n\ndef _get_num_slices(op_slice_sizes):\n  """"""Returns the number of slices in a list of OpSlice sizes.\n\n  Args:\n    op_slice_sizes: List of list of slice sizes, where the outer list has a list\n      per op and the inner list is the slice sizes of the op.\n\n  Returns:\n    Integer max number of slices in the list of ops.\n  """"""\n  return max([len(slices) for slices in op_slice_sizes])\n\n\ndef reslice_concat_ops(concat_ops, aligned_op_slice_sizes, op_reg_manager):\n  """"""Reslices concat ops according to aligned sizes.\n\n  For concat, the input ops are concatenated which means the input op slice\n  sizes must be concatenated when comparing to aligned slice sizes.  This is\n  different from the output, where the output op slices can be directly compared\n  to the aligned sizes.\n\n  For example, consider a concatenation of OpA (size 3) and OpB (size 5) which\n  is input into OpC (size 8, but slices of size [3, 3, 2] perhaps due to another\n  downstream concat).  To group these ops, the input op slices need to be\n  concatenated before aligning with the output op slices, which requires\n  aligning ops slice sizes [[3, 5], [3, 3, 2]] which results in [3, 3, 2].\n  Thus, OpB needs to be sliced into sizes [3, 2] in order to makes slice sizes\n  compatible for grouping.\n\n  Args:\n    concat_ops: List of tf.Operation to slice.\n    aligned_op_slice_sizes: List of integer slice sizes.\n    op_reg_manager: OpRegularizerManager to keep track of slicing.\n\n  Raises:\n    ValueError: If concat op slice sizes do not match aligned op slice sizes.\n  """"""\n  concat_slice_index = 0\n  for concat_op in concat_ops:\n    concat_op_slices = op_reg_manager.get_op_slices(concat_op)\n    concat_op_slice_sizes = get_op_slice_sizes([concat_op_slices])[0]\n    if concat_op_slice_sizes == aligned_op_slice_sizes[\n        concat_slice_index:concat_slice_index + len(concat_op_slice_sizes)]:\n      # Slice sizes match so move on to the next op.\n      concat_slice_index += len(concat_op_slice_sizes)\n      continue\n    else:\n      # Slice sizes do not match.  The concat op needs to be resliced to match\n      # the aligned sizes.\n      slice_count = 1\n      concat_op_size = sum(concat_op_slice_sizes)\n      slice_size = get_total_slice_size(\n          aligned_op_slice_sizes, concat_slice_index, slice_count)\n\n      # Accumulate aligned slices until the sizes match the input op size.\n      while concat_op_size > slice_size:\n        slice_count += 1\n        slice_size = get_total_slice_size(\n            aligned_op_slice_sizes, concat_slice_index, slice_count)\n\n      if concat_op_size != slice_size:\n        raise ValueError(\'Could not properly slice op: %s\' % concat_op)\n      else:\n        # Now concat_slice_index and slice_count specify the sublist of aligned\n        # op slice sizes that match the current concat op.  Reslice the concat\n        # op using the aligned sizes.\n        op_reg_manager.slice_op(\n            concat_op,\n            aligned_op_slice_sizes[\n                concat_slice_index:concat_slice_index + slice_count])\n        concat_slice_index += slice_count\n\n\ndef get_total_slice_size(op_slice_sizes, index, slice_count):\n  """"""Returns total size of a sublist of slices.\n\n  Args:\n    op_slice_sizes: List of integer slice sizes.\n    index: Integer index specifying the start of the sublist.\n    slice_count: Integer number of slices to include in the total size.\n\n  Returns:\n    Integer total size of the sublist of slices.\n  """"""\n  return sum(op_slice_sizes[index:index + slice_count])\n\n\ndef reslice_ops(ops, aligned_op_slice_sizes, op_reg_manager):\n  """"""Reslices ops according to aligned sizes.\n\n  Args:\n    ops: List of tf.Operation to slice.\n    aligned_op_slice_sizes: List of integer slice sizes.\n    op_reg_manager: OpRegularizerManager to keep track of slicing.\n  """"""\n  for op_to_slice in ops:\n    op_slice_sizes = [\n        op_slice.slice.size\n        for op_slice in op_reg_manager.get_op_slices(op_to_slice)]\n    if op_slice_sizes and op_slice_sizes != aligned_op_slice_sizes:\n      op_reg_manager.slice_op(op_to_slice, aligned_op_slice_sizes)\n\n\ndef _get_source_op_slices(op_slices, op_reg_manager):\n  """"""Returns list of OpSlice that are sources.\n\n  Args:\n    op_slices: List of OpSlice.\n    op_reg_manager: OpRegularizerManager to keep track of slicing.\n\n  Returns:\n    List of OpSlice that are sources.\n  """"""\n  op_groups = [op_reg_manager.get_op_group(op_slice)\n               for op_slice in op_slices\n               if op_reg_manager.get_op_group(op_slice) is not None]\n  # pylint: disable=g-complex-comprehension\n  return list(set([source_op_slice for op_group in op_groups\n                   for source_op_slice in op_group.source_op_slices]))\n  # pylint: enable=g-complex-comprehension\n\n\ndef group_aligned_input_output_slices(\n    op, input_ops_to_process, output_ops_to_process, input_op_slices,\n    output_op_slices, aligned_op_slice_sizes, op_reg_manager):\n  """"""Groups aligned OpSlice and reprocesses ungrouped ops.\n\n  Assuming OpSlice of op have been aligned with input and output, groups the\n  corresponding OpSlice based on whether all inputs or all outputs are grouped.\n  Ungrouped ops are put into the queue for processing.\n\n  1. If all inputs and outputs have groups, op is also grouped with them for\n     consistency.\n  2. If all inputs are grouped, op is grouped with inputs while ungrouped\n     outputs are queued for processing.\n  3. If all outputs are grouped and there is only 1 input, op is grouped with\n     outputs while ungrouped inputs are queued for processing.\n  4. If neither inputs or outputs are grouped, then all ungrouped ops are queued\n     for processing as grouping for op currently cannot be resolved.\n\n  Args:\n    op: tf.Operation to determine grouping for.\n    input_ops_to_process: List of tf.Operation of ungrouped input ops.\n    output_ops_to_process: List of tf.Operation of ungrouped output ops.\n    input_op_slices: List of list of OpSlice, with a list per input op.\n    output_op_slices: List of list of OpSlice, with a list per output op.\n    aligned_op_slice_sizes: List of integer slice sizes.\n    op_reg_manager: OpRegularizerManager to keep track of grouping.\n  """"""\n  # If all inputs and outputs have groups, group slices with op for consistency.\n  if not input_ops_to_process and not output_ops_to_process:\n    group_op_with_inputs_and_outputs(\n        op, input_op_slices, output_op_slices, aligned_op_slice_sizes,\n        op_reg_manager)\n  elif not input_ops_to_process:\n    # All inputs are grouped, so group with inputs and process outputs.\n    group_op_with_inputs_and_outputs(\n        op, input_op_slices, [], aligned_op_slice_sizes, op_reg_manager)\n    op_reg_manager.process_ops(output_ops_to_process)\n  else:\n    # Both inputs and outputs need to be grouped first.\n    op_reg_manager.process_ops(output_ops_to_process + input_ops_to_process)\n    op_reg_manager.process_ops_last([op])\n\n\ndef get_op_size(op):\n  """"""Returns output size of an op.\n\n  The output size of an op is typically the last dimension of the output tensor.\n  For example, this is the number of output channels of a convolution.  If the\n  op has no shape (i.e. a constant), then return 0.\n\n  Args:\n    op: A tf.Operation.\n\n  Returns:\n    Integer output size of the op.\n  """"""\n  if op.type in OP_TYPES_WITH_MULTIPLE_OUTPUTS:\n    return sum([output_tensor.shape.as_list()[-1]\n                for output_tensor in op.outputs])\n  # For regular ops, return the size of the first output tensor.\n  shape = op.outputs[0].shape.as_list()\n  if shape:\n    return shape[-1]\n  return 0\n\n\ndef separate_same_size_ops(reference_op, ops):\n  """"""Separate ops by comparing to size of op.\n\n  Ops of size 0 are dropped.\n\n  Args:\n    reference_op: tf.Operation which is the reference size.\n    ops: List of tf.Operation to compare to the reference op size.\n\n  Returns:\n    A 2-tuple of lists of tf.Operation.  The first element is a list of\n    tf.Operation which match the size of the reference op.  The second element\n    is a list of tf.Operation that do not match the size of the reference op.\n  """"""\n  same_size_ops = []\n  different_size_ops = []\n  reference_op_size = get_op_size(reference_op)\n  for op in ops:\n    op_size = get_op_size(op)\n    if op_size == reference_op_size:\n      same_size_ops.append(op)\n    elif op_size > 0:\n      different_size_ops.append(op)\n\n  return (same_size_ops, different_size_ops)\n\n\ndef group_match(regex, op_slices):\n  """"""Returns True if the regex is found in the op name of any Opslice.\n\n  Args:\n    regex: A string regex.\n    op_slices: List of OpRegularizerManager.OpSlice.\n\n  Returns:\n    True if the regex is found in the op name of any op in op_slices.\n  """"""\n  # If no regex, then group does not match.\n  if not regex:\n    return False\n\n  # Check if any OpSlice in the group matches the regex.\n  matches = [re.search(regex, op_slice.op.name) for op_slice in op_slices]\n  return any(matches)\n'"
morph_net/framework/op_handler_util_test.py,15,"b'""""""Tests for op_handler_util.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport mock\nfrom morph_net.framework import op_handler_util\nfrom morph_net.framework import op_regularizer_manager as orm\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import framework\nfrom tensorflow.contrib import layers\n\narg_scope = framework.arg_scope\n\n\nclass OpHandlerUtilTest(tf.test.TestCase):\n\n  def _batch_norm_scope(self):\n    params = {\n        \'trainable\': True,\n        \'normalizer_fn\': layers.batch_norm,\n        \'normalizer_params\': {\n            \'scale\': True,\n        },\n    }\n\n    with arg_scope([layers.conv2d], **params) as sc:\n      return sc\n\n  def setUp(self):\n    super(OpHandlerUtilTest, self).setUp()\n    tf.reset_default_graph()\n\n    # This tests a Conv2D -> BatchNorm -> ReLU chain of ops.\n    with framework.arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      layers.conv2d(inputs, num_outputs=5, kernel_size=3, scope=\'conv1\')\n\n    # This tests 3 Conv2D ops being concatenated before a batch normalization.\n    c2 = layers.conv2d(inputs, num_outputs=5, kernel_size=3, scope=\'conv2\')\n    c3 = layers.conv2d(inputs, num_outputs=6, kernel_size=3, scope=\'conv3\')\n    c4 = layers.conv2d(inputs, num_outputs=7, kernel_size=3, scope=\'conv4\')\n    net = tf.concat([c2, c3, c4], axis=3)\n    layers.batch_norm(net)\n\n    g = tf.get_default_graph()\n\n    # Declare OpSlice and OpGroup for ops in the first test network.\n    self.batch_norm_op = g.get_operation_by_name(\n        \'conv1/BatchNorm/FusedBatchNormV3\')\n    self.batch_norm_op_slice = orm.OpSlice(self.batch_norm_op, None)\n    self.batch_norm_op_group = orm.OpGroup(self.batch_norm_op_slice)\n\n    self.conv_op = g.get_operation_by_name(\'conv1/Conv2D\')\n    self.conv_op_slice = orm.OpSlice(self.conv_op, None)\n    self.conv_op_group = orm.OpGroup(\n        self.conv_op_slice, omit_source_op_slices=[self.conv_op_slice])\n\n    self.gamma_op = g.get_operation_by_name(\'conv1/BatchNorm/gamma/read\')\n    self.beta_op = g.get_operation_by_name(\'conv1/BatchNorm/beta/read\')\n\n    self.relu_op = g.get_operation_by_name(\'conv1/Relu\')\n    self.relu_op_slice = orm.OpSlice(self.relu_op, None)\n    self.relu_op_group = orm.OpGroup(\n        self.relu_op_slice, omit_source_op_slices=[self.relu_op_slice])\n\n    # Declare OpSlice and OpGroup for ops in the second test network.\n    self.relu2_op = g.get_operation_by_name(\'conv2/Relu\')\n    self.relu2_op_slice = orm.OpSlice(self.relu2_op, orm.Slice(0, 5))\n    self.relu2_op_group = orm.OpGroup(\n        self.relu2_op_slice, omit_source_op_slices=[self.relu2_op_slice])\n\n    self.relu3_op = g.get_operation_by_name(\'conv3/Relu\')\n    self.relu3_op_slice = orm.OpSlice(self.relu3_op, orm.Slice(0, 6))\n    self.relu3_op_group = orm.OpGroup(\n        self.relu3_op_slice, omit_source_op_slices=[self.relu3_op_slice])\n\n    self.relu4_op = g.get_operation_by_name(\'conv4/Relu\')\n    self.relu4_op_slice = orm.OpSlice(self.relu4_op, orm.Slice(0, 7))\n    self.relu4_op_group = orm.OpGroup(\n        self.relu4_op_slice, omit_source_op_slices=[self.relu4_op_slice])\n\n    self.unfused_batch_norm_op = g.get_operation_by_name(\n        \'BatchNorm/FusedBatchNormV3\')\n    self.unfused_batch_norm_op_slice = orm.OpSlice(\n        self.unfused_batch_norm_op, orm.Slice(0, 18))\n\n    self.concat_op = g.get_operation_by_name(\'concat\')\n    self.concat_op_slice = orm.OpSlice(self.concat_op, orm.Slice(0, 18))\n    self.concat_op_group = orm.OpGroup(\n        self.concat_op_slice, omit_source_op_slices=[self.concat_op_slice])\n\n    # Create mock OpRegularizerManager with custom mapping of OpSlice and\n    # OpGroup.\n    self.mock_op_reg_manager = mock.create_autospec(orm.OpRegularizerManager)\n\n    def get_op_slices(op):\n      return self.op_slice_dict.get(op, [])\n\n    def get_op_group(op_slice):\n      return self.op_group_dict.get(op_slice)\n\n    def is_passthrough(op):\n      return op in self._passthrough_ops\n\n    self.mock_op_reg_manager.get_op_slices.side_effect = get_op_slices\n    self.mock_op_reg_manager.get_op_group.side_effect = get_op_group\n    self.mock_op_reg_manager.is_passthrough.side_effect = is_passthrough\n    self.mock_op_reg_manager.ops = [\n        self.batch_norm_op, self.gamma_op, self.beta_op,\n        self.conv_op, self.relu_op,\n        self.relu2_op, self.relu3_op, self.relu4_op, self.unfused_batch_norm_op,\n        self.concat_op]\n\n  def testGetInputOps(self):\n    # For batch norm, the expected inputs are Conv2D, gamma, and beta.  The\n    # decay and epsilon are excluded because they are scalars.\n    expected_inputs = [self.conv_op, self.gamma_op, self.beta_op]\n\n    # Check for expected input ops.\n    input_ops = op_handler_util.get_input_ops(self.batch_norm_op,\n                                              self.mock_op_reg_manager)\n    self.assertEqual(expected_inputs, input_ops)\n\n  def testGetOutputOps(self):\n    # For batch norm, the expected outputs are mean, std, and ReLU.\n    expected_outputs = [self.relu_op]\n\n    # Check for expected output ops.\n    self.assertEqual(\n        expected_outputs,\n        op_handler_util.get_output_ops(self.batch_norm_op,\n                                       self.mock_op_reg_manager))\n\n  def testGetOpsWithoutGroups(self):\n    # For a list of ops, verify that ops without groups are returned.\n    self.op_slice_dict = {\n        self.batch_norm_op: [self.batch_norm_op_slice],\n        self.conv_op: [self.conv_op_slice],\n        self.gamma_op: [orm.OpSlice(self.gamma_op, None)],\n        self.beta_op: [orm.OpSlice(self.beta_op, None)],\n    }\n\n    # Only batch norm and conv ops have groups.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.conv_op_group\n    }\n\n    all_ops = [self.batch_norm_op, self.conv_op, self.gamma_op, self.beta_op]\n    # Batch norm and conv ops have groups.  The other ops do not have groups.\n    expected_ops = [self.gamma_op, self.beta_op]\n    self.assertEqual(\n        expected_ops,\n        op_handler_util.get_ops_without_groups(\n            all_ops, self.mock_op_reg_manager))\n\n  def testRemoveNonPassthroughOps(self):\n    self._passthrough_ops = (self.gamma_op,)\n\n    all_ops = [self.batch_norm_op, self.conv_op, self.gamma_op, self.beta_op]\n    expected_ops = [self.gamma_op]\n\n    self.assertListEqual(\n        expected_ops,\n        op_handler_util.remove_non_passthrough_ops(all_ops,\n                                                   self.mock_op_reg_manager))\n\n  def testGroupOpWithInputsAndOutputs_SingleSlice(self):\n    # For the single slice case, verify that batch norm is grouped with its\n    # output (ReLU) and its input (Conv2D).\n    aligned_op_slice_sizes = [5]\n\n    self.op_slice_dict = {\n        self.batch_norm_op: [self.batch_norm_op_slice],\n        self.conv_op: [self.conv_op_slice],\n        self.relu_op: [self.relu_op_slice]\n    }\n    # All ops have groups.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.conv_op_group,\n        self.relu_op_slice: self.relu_op_group\n    }\n\n    ops_grouped = op_handler_util.group_op_with_inputs_and_outputs(\n        self.batch_norm_op, [[self.conv_op_slice]], [[self.relu_op_slice]],\n        aligned_op_slice_sizes, self.mock_op_reg_manager)\n\n    # Verify manager looks up op slice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.batch_norm_op)\n\n    # Verify manager groups batch norm with Conv2D and ReLU ops.\n    self.assertTrue(ops_grouped)\n    self.mock_op_reg_manager.group_op_slices.assert_has_calls(\n        [mock.call([self.batch_norm_op_slice, self.relu_op_slice]),\n         mock.call([self.batch_norm_op_slice, self.conv_op_slice])])\n\n  def testGroupOpWithInputsAndOutputs_MultipleSlices(self):\n    # For the multiple slice case, verify that batch norm slices are grouped\n    # with output slices (ReLU) and input slices (Conv2D).\n    batch_norm_op_slice_0_2 = orm.OpSlice(\n        self.batch_norm_op, orm.OpSlice(0, 2))\n    batch_norm_op_slice_2_5 = orm.OpSlice(\n        self.batch_norm_op, orm.OpSlice(2, 3))\n    batch_norm_op_group1 = orm.OpGroup(\n        batch_norm_op_slice_0_2)\n    batch_norm_op_group2 = orm.OpGroup(\n        batch_norm_op_slice_2_5)\n\n    conv_op_slice_0_2 = orm.OpSlice(\n        self.conv_op, orm.OpSlice(0, 2))\n    conv_op_slice_2_5 = orm.OpSlice(\n        self.conv_op, orm.OpSlice(2, 3))\n    conv_op_group1 = orm.OpGroup(\n        conv_op_slice_0_2, omit_source_op_slices=[conv_op_slice_0_2])\n    conv_op_group2 = orm.OpGroup(\n        conv_op_slice_2_5, omit_source_op_slices=[conv_op_slice_2_5])\n\n    relu_op_slice_0_2 = orm.OpSlice(\n        self.relu_op, orm.OpSlice(0, 2))\n    relu_op_slice_2_5 = orm.OpSlice(\n        self.relu_op, orm.OpSlice(2, 3))\n    relu_op_group1 = orm.OpGroup(relu_op_slice_0_2)\n    relu_op_group2 = orm.OpGroup(relu_op_slice_2_5)\n\n    aligned_op_slice_sizes = [2, 3]\n\n    self.op_slice_dict = {\n        self.batch_norm_op: [batch_norm_op_slice_0_2, batch_norm_op_slice_2_5],\n        self.conv_op: [conv_op_slice_0_2, conv_op_slice_2_5],\n        self.relu_op: [relu_op_slice_0_2, relu_op_slice_2_5],\n    }\n\n    # All ops have groups.\n    self.op_group_dict = {\n        batch_norm_op_slice_0_2: batch_norm_op_group1,\n        batch_norm_op_slice_2_5: batch_norm_op_group2,\n        conv_op_slice_0_2: conv_op_group1,\n        conv_op_slice_2_5: conv_op_group2,\n        relu_op_slice_0_2: relu_op_group1,\n        relu_op_slice_2_5: relu_op_group2,\n    }\n\n    ops_grouped = op_handler_util.group_op_with_inputs_and_outputs(\n        self.batch_norm_op, [[conv_op_slice_0_2, conv_op_slice_2_5]],\n        [[relu_op_slice_0_2, relu_op_slice_2_5]], aligned_op_slice_sizes,\n        self.mock_op_reg_manager)\n\n    # Verify manager looks up op slice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_any_call(self.batch_norm_op)\n\n    # Verify manager groups batch norm with Conv2D and ReLU ops.\n    self.assertTrue(ops_grouped)\n    self.mock_op_reg_manager.group_op_slices.assert_has_calls(\n        [mock.call([batch_norm_op_slice_0_2, relu_op_slice_0_2]),\n         mock.call([batch_norm_op_slice_0_2, conv_op_slice_0_2]),\n         mock.call([batch_norm_op_slice_2_5, relu_op_slice_2_5]),\n         mock.call([batch_norm_op_slice_2_5, conv_op_slice_2_5])])\n\n  def testGetConcatInputOpSlices(self):\n    # For concat, the input op slices are the concatenation of op slices of each\n    # input op.\n\n    # Map ops to slices.\n    self.op_slice_dict = {\n        self.relu2_op: [self.relu2_op_slice],\n        self.relu3_op: [self.relu3_op_slice],\n        self.relu4_op: [self.relu4_op_slice],\n    }\n\n    # The concat input is relu2, relu3, and relu4.\n    expected_input_op_slices = [\n        [self.relu2_op_slice, self.relu3_op_slice, self.relu4_op_slice]]\n\n    input_ops = op_handler_util.get_input_ops(\n        self.concat_op, self.mock_op_reg_manager)\n    self.assertEqual(\n        expected_input_op_slices,\n        op_handler_util.get_concat_input_op_slices(\n            input_ops, self.mock_op_reg_manager))\n\n  def testGetOpSlices(self):\n    # Generic ops are treated as a concatenation of their constituent OpSlice.\n    batch_norm_op_slice_0_5 = orm.OpSlice(\n        self.unfused_batch_norm_op, orm.Slice(0, 5))\n    batch_norm_op_slice_5_11 = orm.OpSlice(\n        self.unfused_batch_norm_op, orm.Slice(5, 6))\n    batch_norm_op_slice_11_18 = orm.OpSlice(\n        self.unfused_batch_norm_op, orm.Slice(11, 7))\n\n    # Map ops to slices.\n    self.op_slice_dict = {\n        self.unfused_batch_norm_op: [\n            batch_norm_op_slice_0_5, batch_norm_op_slice_5_11,\n            batch_norm_op_slice_11_18],\n    }\n\n    # A nested list composed of a list of OpSlice for each output op.  In this\n    # case, there is just one output op (i.e. batch norm).\n    expected_output_op_slices = [[\n        batch_norm_op_slice_0_5,\n        batch_norm_op_slice_5_11,\n        batch_norm_op_slice_11_18]]\n\n    output_ops = op_handler_util.get_output_ops(\n        self.concat_op, self.mock_op_reg_manager)\n    self.assertEqual(\n        expected_output_op_slices,\n        op_handler_util.get_op_slices(output_ops, self.mock_op_reg_manager))\n\n  def testGetOpSlices_FilterEmptySlices(self):\n    # No slices are mapped to ops.\n    self.op_slice_dict = {}\n\n    # Verify that empty slices are removed.\n    input_ops = op_handler_util.get_input_ops(\n        self.batch_norm_op, self.mock_op_reg_manager)\n    self.assertListEqual([], op_handler_util.get_op_slices(\n        input_ops, self.mock_op_reg_manager))\n\n  def testGetOpSliceSizes(self):\n    relu3_op_slice_0_3 = orm.OpSlice(\n        self.relu2_op, orm.Slice(0, 3))\n    relu3_op_slice_3_6 = orm.OpSlice(\n        self.relu2_op, orm.Slice(3, 3))\n\n    batch_norm_op_slice_0_5 = orm.OpSlice(\n        self.unfused_batch_norm_op, orm.Slice(0, 5))\n    batch_norm_op_slice_5_8 = orm.OpSlice(\n        self.unfused_batch_norm_op, orm.Slice(5, 3))\n    batch_norm_op_slice_8_11 = orm.OpSlice(\n        self.unfused_batch_norm_op, orm.Slice(8, 3))\n    batch_norm_op_slice_11_18 = orm.OpSlice(\n        self.unfused_batch_norm_op, orm.Slice(11, 7))\n\n    # Map ops to slices.\n    self.op_slice_dict = {\n        self.relu2_op: [self.relu2_op_slice],\n        self.relu3_op: [relu3_op_slice_0_3, relu3_op_slice_3_6],\n        self.relu4_op: [self.relu4_op_slice],\n        self.unfused_batch_norm_op: [\n            batch_norm_op_slice_0_5, batch_norm_op_slice_5_8,\n            batch_norm_op_slice_8_11, batch_norm_op_slice_11_18],\n    }\n\n    expected_op_slice_sizes = [\n        [5],  # c2 has size 5.\n        [3, 3],  # c3 has size 6, but in 2 slices of size 3.\n        [7],  # c4 has size 7.\n        [5, 3, 3, 7]]  # batch norm has size 18, but slice sizes of c1, c2, c3.\n\n    self.assertEqual(\n        expected_op_slice_sizes,\n        op_handler_util.get_op_slice_sizes([\n            [self.relu2_op_slice],\n            [relu3_op_slice_0_3, relu3_op_slice_3_6],\n            [self.relu4_op_slice],\n            [batch_norm_op_slice_0_5, batch_norm_op_slice_5_8,\n             batch_norm_op_slice_8_11, batch_norm_op_slice_11_18]]))\n\n  def testGetAlignedOpSliceSizes(self):\n    expected_op_slice_sizes = [5, 4, 2, 2, 5]\n    self.assertEqual(\n        expected_op_slice_sizes,\n        op_handler_util.get_aligned_sizes([\n            [5, 4, 2, 7],\n            [9, 4, 5],\n            [18]]))\n\n    expected_op_slice_sizes = [1, 2, 2, 1, 3, 1, 2, 2, 1]\n    self.assertEqual(\n        expected_op_slice_sizes,\n        op_handler_util.get_aligned_sizes([\n            [1, 2, 3, 4, 5],\n            [5, 4, 3, 2, 1]]))\n\n    expected_op_slice_sizes = [1, 1, 1, 1, 1]\n    self.assertEqual(\n        expected_op_slice_sizes,\n        op_handler_util.get_aligned_sizes([\n            [5],\n            [1, 1, 1, 1, 1]]))\n\n    expected_op_slice_sizes = [10]\n    self.assertEqual(\n        expected_op_slice_sizes,\n        op_handler_util.get_aligned_sizes([[10]]))\n\n    # Raise exception for empty input.\n    with self.assertRaises(ValueError):\n      op_handler_util.get_aligned_sizes([])\n\n    # Raise exception if total sizes do not match.\n    with self.assertRaises(ValueError):\n      op_handler_util.get_aligned_sizes([[1, 2], [4]])\n\n  def testGetNumSlices(self):\n    self.assertEqual(\n        5, op_handler_util._get_num_slices([[1, 2, 3, 4, 5], [6, 7], [8]]))\n    self.assertEqual(\n        2, op_handler_util._get_num_slices([[6, 7], [8]]))\n    self.assertEqual(\n        1, op_handler_util._get_num_slices([[8]]))\n\n  def testResliceConcatOps_Aligned(self):\n    # Map ops to slices.\n    self.op_slice_dict = {\n        self.relu2_op: [self.relu2_op_slice],\n        self.relu3_op: [self.relu3_op_slice],\n        self.relu4_op: [self.relu4_op_slice],\n    }\n\n    op_handler_util.reslice_concat_ops(\n        [self.relu2_op, self.relu3_op, self.relu4_op],\n        [5, 6, 7], self.mock_op_reg_manager)\n\n    # Verify manager does not slice any ops.\n    self.mock_op_reg_manager.slice_op.assert_not_called()\n\n  def testResliceConcatOps_NotAligned(self):\n    relu3_op_slice_0_3 = orm.OpSlice(\n        self.relu3_op, orm.Slice(0, 3))\n    relu3_op_slice_3_6 = orm.OpSlice(\n        self.relu3_op, orm.Slice(3, 3))\n\n    # Map ops to slices.  The op c3 is composed of multiple slices.\n    self.op_slice_dict = {\n        self.relu2_op: [self.relu2_op_slice],\n        self.relu3_op: [relu3_op_slice_0_3, relu3_op_slice_3_6],\n        self.relu4_op: [self.relu4_op_slice],\n    }\n\n    op_handler_util.reslice_concat_ops(\n        [self.relu2_op, self.relu3_op, self.relu4_op],\n        [5, 4, 2, 2, 5], self.mock_op_reg_manager)\n\n    # Verify manager slices input ops.\n    self.mock_op_reg_manager.slice_op.assert_has_calls(\n        [mock.call(self.relu3_op, [4, 2]),\n         mock.call(self.relu4_op, [2, 5])])\n\n  def testGetTotalSliceSize(self):\n    op_slice_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n    self.assertEqual(\n        15, op_handler_util.get_total_slice_size(op_slice_sizes, 0, 5))\n    self.assertEqual(\n        15, op_handler_util.get_total_slice_size(op_slice_sizes, 3, 3))\n    self.assertEqual(\n        30, op_handler_util.get_total_slice_size(op_slice_sizes, 5, 4))\n    self.assertEqual(\n        3, op_handler_util.get_total_slice_size(op_slice_sizes, 2, 1))\n\n  def testResliceOps(self):\n    # Map ops to slices\n    self.op_slice_dict = {\n        self.concat_op: [self.concat_op_slice],\n        self.unfused_batch_norm_op: [self.unfused_batch_norm_op_slice],\n    }\n\n    op_handler_util.reslice_ops(\n        [self.concat_op, self.unfused_batch_norm_op],\n        [5, 4, 2, 2, 5], self.mock_op_reg_manager)\n\n    # Verify manager slices input ops.\n    self.mock_op_reg_manager.slice_op.assert_has_calls(\n        [mock.call(self.concat_op, [5, 4, 2, 2, 5]),\n         mock.call(self.unfused_batch_norm_op, [5, 4, 2, 2, 5])])\n\n  def testGetSourceOpSlices(self):\n    op_slices = [self.batch_norm_op_slice, self.conv_op_slice,\n                 self.relu_op_slice]\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.conv_op_group,\n        self.relu_op_slice: self.relu_op_group,\n    }\n    expected_source_op_slices = [self.batch_norm_op_slice]\n\n    self.assertEqual(\n        expected_source_op_slices,\n        op_handler_util._get_source_op_slices(\n            op_slices, self.mock_op_reg_manager))\n\n  def testGroupAlignedInputOutputSlices_InputsOutputsGrouped(self):\n    self.op_slice_dict = {\n        self.batch_norm_op: [self.batch_norm_op_slice],\n        self.conv_op: [self.conv_op_slice],\n        self.relu_op: [self.relu_op_slice]\n    }\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.conv_op_group,\n        self.relu_op_slice: self.relu_op_group\n    }\n    input_op_slices = [[self.conv_op_slice]]\n    output_op_slices = [[self.relu_op_slice]]\n    aligned_op_slice_sizes = [5]\n\n    op_handler_util.group_aligned_input_output_slices(\n        self.batch_norm_op, [], [], input_op_slices, output_op_slices,\n        aligned_op_slice_sizes, self.mock_op_reg_manager)\n\n    self.mock_op_reg_manager.group_op_slices.assert_has_calls(\n        [mock.call([self.batch_norm_op_slice, self.relu_op_slice]),\n         mock.call([self.batch_norm_op_slice, self.conv_op_slice])])\n    self.mock_op_reg_manager.process_ops.assert_not_called()\n\n  def testGroupAlignedInputOutputSlices_InputsGrouped(self):\n    self.op_slice_dict = {\n        self.batch_norm_op: [self.batch_norm_op_slice],\n        self.conv_op: [self.conv_op_slice],\n        self.relu_op: [self.relu_op_slice]\n    }\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.conv_op_group,\n        self.relu_op_slice: self.relu_op_group\n    }\n    input_op_slices = [[self.conv_op_slice]]\n    output_op_slices = [[self.relu_op_slice]]\n    aligned_op_slice_sizes = [5]\n\n    op_handler_util.group_aligned_input_output_slices(\n        self.batch_norm_op, [], [self.relu_op], input_op_slices,\n        output_op_slices, aligned_op_slice_sizes, self.mock_op_reg_manager)\n\n    self.mock_op_reg_manager.group_op_slices.assert_called_once_with(\n        [self.batch_norm_op_slice, self.conv_op_slice])\n    self.mock_op_reg_manager.process_ops.assert_called_once_with([self.relu_op])\n\n  def testGroupAlignedInputOutputSlices_OutputsGrouped(self):\n    self.op_slice_dict = {\n        self.batch_norm_op: [self.batch_norm_op_slice],\n        self.conv_op: [self.conv_op_slice],\n        self.relu_op: [self.relu_op_slice]\n    }\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.conv_op_group,\n        self.relu_op_slice: self.relu_op_group\n    }\n    input_op_slices = [[self.conv_op_slice]]\n    output_op_slices = [[self.relu_op_slice]]\n    aligned_op_slice_sizes = [5]\n\n    op_handler_util.group_aligned_input_output_slices(\n        self.batch_norm_op, [self.conv_op], [], input_op_slices,\n        output_op_slices, aligned_op_slice_sizes, self.mock_op_reg_manager)\n\n    self.mock_op_reg_manager.group_op_slices.assert_not_called()\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.conv_op])\n    self.mock_op_reg_manager.process_ops_last.assert_called_once_with(\n        [self.batch_norm_op])\n\n  def testGroupAlignedInputOutputSlices_NoGroups(self):\n    self.op_slice_dict = {\n        self.batch_norm_op: [self.batch_norm_op_slice],\n        self.conv_op: [self.conv_op_slice],\n        self.relu_op: [self.relu_op_slice]\n    }\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n        self.conv_op_slice: self.conv_op_group,\n        self.relu_op_slice: self.relu_op_group\n    }\n    input_op_slices = [[self.conv_op_slice]]\n    output_op_slices = [[self.relu_op_slice]]\n    aligned_op_slice_sizes = [5]\n\n    op_handler_util.group_aligned_input_output_slices(\n        self.batch_norm_op, [self.conv_op], [self.relu_op], input_op_slices,\n        output_op_slices, aligned_op_slice_sizes, self.mock_op_reg_manager)\n\n    self.mock_op_reg_manager.group_op_slices.assert_not_called()\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.relu_op, self.conv_op])\n    self.mock_op_reg_manager.process_ops_last.assert_called_once_with(\n        [self.batch_norm_op])\n\n  def testGetOpSize(self):\n    # Verify correct size for regular ops.\n    self.assertEqual(5, op_handler_util.get_op_size(self.relu2_op))\n    self.assertEqual(6, op_handler_util.get_op_size(self.relu3_op))\n    self.assertEqual(7, op_handler_util.get_op_size(self.relu4_op))\n\n    # Verify correct size for ops with multiple outputs.\n    split = tf.split(self.conv_op.outputs[0], [2, 3], axis=3)\n    self.assertEqual(5, op_handler_util.get_op_size(split[0].op))\n\n  def testSeparateSameSizeOps(self):\n    op1 = tf.zeros([2, 4, 4, 3])\n    op2 = tf.zeros([2, 4, 4, 3])\n    op3 = tf.zeros([2, 4, 4, 5])\n    op4 = tf.zeros([])\n    op5 = tf.zeros([2, 4, 4, 3])\n    op6 = tf.zeros([2, 4, 4, 2])\n    all_ops = [op2.op, op3.op, op4.op, op5.op, op6.op]\n\n    # Op2 and Op5 have matching sizes.  Op3 and Op6 have different sizes.  Op4\n    # has size 0 and is dropped.\n    expected_same_size_ops = [op2.op, op5.op]\n    expected_different_size_ops = [op3.op, op6.op]\n\n    same_size_ops, different_size_ops = (\n        op_handler_util.separate_same_size_ops(op1.op, all_ops))\n\n    # Verify lists of same size ops and different size ops.\n    self.assertListEqual(expected_same_size_ops, same_size_ops)\n    self.assertListEqual(expected_different_size_ops, different_size_ops)\n\n  def testOpAssumptions(self):\n    # Verify that op assumptions are true.  For example, verify that specific\n    # inputs are at expected indices.\n    conv_transpose = layers.conv2d_transpose(\n        self.batch_norm_op.outputs[0], num_outputs=8, kernel_size=3,\n        scope=\'conv_transpose\')\n    layers.separable_conv2d(\n        conv_transpose, num_outputs=9, kernel_size=3, scope=\'dwise_conv\')\n    layers.fully_connected(tf.zeros([1, 7]), 10, scope=\'fc\')\n\n    g = tf.get_default_graph()\n\n    # Verify that FusedBatchNormV3 has gamma as inputs[1].\n    self.assertEqual(\'conv1/BatchNorm/gamma/read:0\',\n                     self.batch_norm_op.inputs[1].name)\n\n    # Verify that Conv2D has weights at expected index.\n    index = op_handler_util.WEIGHTS_INDEX_DICT[self.conv_op.type]\n    self.assertEqual(\'conv1/weights/read:0\',\n                     self.conv_op.inputs[index].name)\n\n    # Verify that Conv2DBackpropInput has weights at expected index.\n    conv_transpose_op = g.get_operation_by_name(\n        \'conv_transpose/conv2d_transpose\')\n    index = op_handler_util.WEIGHTS_INDEX_DICT[conv_transpose_op.type]\n    self.assertEqual(\'conv_transpose/weights/read:0\',\n                     conv_transpose_op.inputs[index].name)\n\n    # Verify that DepthwiseConv2dNative has weights at expected index.\n    depthwise_conv_op = g.get_operation_by_name(\n        \'dwise_conv/separable_conv2d/depthwise\')\n    index = op_handler_util.WEIGHTS_INDEX_DICT[depthwise_conv_op.type]\n    self.assertEqual(\'dwise_conv/depthwise_weights/read:0\',\n                     depthwise_conv_op.inputs[index].name)\n\n    # Verify that MatMul has weights at expected index.\n    matmul_op = g.get_operation_by_name(\'fc/MatMul\')\n    index = op_handler_util.WEIGHTS_INDEX_DICT[matmul_op.type]\n    self.assertEqual(\'fc/weights/read:0\',\n                     matmul_op.inputs[index].name)\n\n  def testGroupMatch(self):\n    # Verify that regex matches an op in the group.\n    regex = \'BatchNorm\'\n    op_slices = [self.batch_norm_op_slice, self.conv_op_slice,\n                 self.relu_op_slice]\n\n    # Regex matches the batch norm.\n    self.assertTrue(op_handler_util.group_match(regex, op_slices))\n\n    # Remove the matching batch norm op.\n    op_slices.pop(0)\n    self.assertFalse(op_handler_util.group_match(regex, op_slices))\n\n  def testGroupMatch_EmptyRegex(self):\n    # Verify that empty regex does not match.\n    regex = \'\'\n    op_slices = [self.batch_norm_op_slice, self.conv_op_slice,\n                 self.relu_op_slice]\n\n    self.assertFalse(op_handler_util.group_match(regex, op_slices))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/op_handlers.py,0,"b'""""""Op Handlers for use with different NetworkRegularizers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# [internal] enable type annotations\nfrom __future__ import print_function\n\nimport collections\n\nfrom morph_net.framework import concat_op_handler\nfrom morph_net.framework import depthwise_convolution_op_handler\nfrom morph_net.framework import grouping_op_handler\nfrom morph_net.framework import leaf_op_handler\nfrom morph_net.framework import output_non_passthrough_op_handler\n\nRESIZE_OP_NAMES = [\n    \'ResizeArea\', \'ResizeBicubic\', \'ResizeBilinear\', \'ResizeNearestNeighbor\'\n]\n\n\ndef _get_base_op_hander_dicts():\n  """"""Returns the base op_hander_dict for all regularizers.""""""\n  base_dict = collections.defaultdict(\n      grouping_op_handler.GroupingOpHandler, {\n          \'ConcatV2\':\n              concat_op_handler.ConcatOpHandler(),\n          \'DepthToSpace\':\n              output_non_passthrough_op_handler.OutputNonPassthroughOpHandler(),\n          \'DepthwiseConv2dNative\':\n              depthwise_convolution_op_handler.DepthwiseConvolutionOpHandler(),\n          \'ExpandDims\':\n              output_non_passthrough_op_handler.OutputNonPassthroughOpHandler(),\n          \'RandomUniform\':\n              leaf_op_handler.LeafOpHandler(),\n          \'Reshape\':\n              leaf_op_handler.LeafOpHandler(),\n          \'Shape\':\n              leaf_op_handler.LeafOpHandler(),\n          \'SpaceToDepth\':\n              leaf_op_handler.LeafOpHandler(),\n          \'StridedSlice\':\n              leaf_op_handler.LeafOpHandler(),\n          \'TensorArrayGatherV3\':\n              leaf_op_handler.LeafOpHandler(),\n          \'Transpose\':\n              output_non_passthrough_op_handler.OutputNonPassthroughOpHandler(),\n      })\n  for resize_method in RESIZE_OP_NAMES:\n    # Resize* ops, second input might be a tensor which will result in an error.\n    base_dict[resize_method] = grouping_op_handler.GroupingOpHandler([0])\n  return base_dict\n\n\ndef get_gamma_op_handler_dict():\n  """"""Returns the base op_hander_dict for gamma based regularizers.""""""\n  op_handler_dict = _get_base_op_hander_dicts()\n  op_handler_dict.update({\n      \'Conv3D\':\n          output_non_passthrough_op_handler.OutputNonPassthroughOpHandler(),\n      \'Conv2D\':\n          output_non_passthrough_op_handler.OutputNonPassthroughOpHandler(),\n      \'MatMul\':\n          output_non_passthrough_op_handler.OutputNonPassthroughOpHandler(),\n      \'Conv2DBackpropInput\':\n          output_non_passthrough_op_handler.OutputNonPassthroughOpHandler(),\n  })\n  return op_handler_dict\n\n\ndef get_group_lasso_op_handler_dict():\n  """"""Returns the base op_hander_dict for group-lasso based regularizers.""""""\n  return _get_base_op_hander_dicts()\n'"
morph_net/framework/op_handlers_test.py,2,"b'""""""Tests for framework.op_handlers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import op_handlers\n\nimport tensorflow.compat.v1 as tf\n\n\nclass OpHandlersTest(tf.test.TestCase):\n\n  def test_dict_logic(self):\n    gamma_dict = op_handlers.get_gamma_op_handler_dict()\n    self.assertIn(\'Conv2D\', gamma_dict)\n    self.assertIn(\'MatMul\', gamma_dict)\n    group_lasso_dict = op_handlers.get_group_lasso_op_handler_dict()\n    self.assertNotIn(\'Conv2D\', group_lasso_dict)\n    self.assertNotIn(\'MatMul\', group_lasso_dict)\n    for op in group_lasso_dict:\n      self.assertIn(op, gamma_dict)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/op_regularizer_manager.py,18,"b'""""""A class for managing OpRegularizers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# [internal] enable type annotations\nfrom __future__ import print_function\n\nimport collections\n\nfrom morph_net.framework import concat_and_slice_regularizers\nfrom morph_net.framework import constant_op_regularizer\nfrom morph_net.framework import grouping_regularizers\nfrom morph_net.framework import op_handler_util\nimport tensorflow.compat.v1 as tf\n\nfrom typing import List\n\n# Hardcoded limit for OpRegularizerManager to finish analyzing the network.\nITERATION_LIMIT = 1000000\n\n\n# OpSlice represents a slice of a tf.Operation.\n# op: A tf.Operation.\n# slice: A Slice tuple containing the index and size of the slice.  If None, or\n#   part of the tuple is None, then the OpSlice represents the entire op.\nclass OpSlice(collections.namedtuple(\'OpSlice\', [\'op\', \'slice\'])):\n\n  def __str__(self):\n    return \'{} {}\'.format(self.op.name, self.slice)\n\n  __repr__ = __str__\n\n\n# Slice represents the index and size of a slice.\n# start_index: Integer specifying start index of the slice.\n# size: Integer specifying number of elements in the slice.\nclass Slice(collections.namedtuple(\'Slice\', [\'start_index\', \'size\'])):\n\n  def __str__(self):\n    return \'({}, {})\'.format(self.start_index, self.size)\n\n  __repr__ = __str__\n\n\nclass OpRegularizerManager(object):\n  """"""A class for managing OpRegularizers.""""""\n\n  def __init__(\n      self,\n      output_boundary: List[tf.Operation],\n      op_handler_dict=None,\n      create_grouping_regularizer=grouping_regularizers.MaxGroupingRegularizer,\n      force_group=None,\n      regularizer_blacklist=None,\n      input_boundary: List[tf.Operation] = None,\n      iteration_limit=ITERATION_LIMIT):\n    """"""Creates an instance of OpRegularizerManager.\n\n    Several internal data structures are initialized which are used to track ops\n    and their grouping.  A DFS is performed starting from `output_boundary` and\n    following data dependencies that do not involve ops in `input_boundary`. The\n    source ops found in this DFS are placed into a queue for processing. The\n    OpRegularizerManager then loops over ops in the queue, using the associated\n    OpHandler to determine the grouping of the op.  Once all ops have been\n    grouped, regularizers for the groups can be created.\n\n    If a group has multiple sources of regularization, the\n    create_grouping_regularizer function is used to create an OpRegularizer that\n    combines the multiple sources.\n\n    If force_group is specified, ops that would not normally be grouped are\n    force-grouped.  Ops matching the regex will be grouped together, along with\n    all ops that were grouped with the matching ops.  Basically, the groups\n    would be merged.  Each regex specifies a separate force-grouping.\n\n    If regularizer_blacklist is specified, then ops matching any of the regex\n    (and ops in the same group) do not get regularized.  The\n    OpRegularizerManager will instead create a None regularizer for the group.\n\n    Args:\n      output_boundary: A list of ops to start regularization from.\n      op_handler_dict: Dictionary mapping tf.Operation type to OpHandler.\n      create_grouping_regularizer: Function that creates an OpRegularizer given\n        a list of OpRegularizer.\n      force_group: List of regex for ops that should be force-grouped.  Each\n        regex corresponds to a separate group.  Use \'|\' operator to specify\n        multiple patterns in a single regex.\n      regularizer_blacklist: List of regex for ops that should not be\n        regularized.\n      input_boundary: A list of ops that should be excluded from regularization.\n      iteration_limit: Integer iteration limit for OpRegularizerManager to\n        finish analyzing the network.  If the limit is reached, it is assumed\n        that OpRegularizerManager got stuck in a loop.\n\n    Raises:\n      RuntimeError: If OpRegularizerManager cannot analyze the entire network\n        within ITERATION_LIMIT.\n      TypeError: If force_group argument is not a list.\n      TypeError: If regularizer_blacklist argument is not a list.\n    """"""\n    # Dictionary mapping op to list of OpSlice.  The op is the concatenation of\n    # its OpSlice list.\n    self._op_slice_dict = {}\n\n    # Dictionary mapping OpSlice to OpGroup.\n    self._op_group_dict = {}\n\n    # Dictionary mapping op type to OpHandler class.\n    self._op_handler_dict = op_handler_dict or {}\n\n    # Dictionary mapping OpSlice to OpRegularizer.\n    self._op_regularizer_dict = {}\n\n    # Queue of ops to process.\n    self._op_deque = collections.deque()\n\n    # Set of all ops to regularize.\n    self._all_ops = set()\n\n    # Start DFS from outputs to find all source ops.\n    tf.logging.info(\'OpRegularizerManager starting analysis from: %s.\',\n                    output_boundary)\n    self._dfs_for_source_ops(output_boundary, input_boundary)\n    tf.logging.info(\'OpRegularizerManager found %d ops and %d sources.\',\n                    len(self._all_ops), len(self._op_deque))\n\n    # Process grouping for all ops.\n    iteration_count = 0\n    while self._op_deque and iteration_count < iteration_limit:\n      op = self._op_deque.pop()\n      self._op_handler_dict[op.type].assign_grouping(op, self)\n      iteration_count += 1\n    if iteration_count >= iteration_limit:\n      # OpRegularizerManager got stuck in a loop.  Report the ops still in the\n      # processing queue.\n      raise RuntimeError(\'OpRegularizerManager could not handle ops: %s\' %\n                         [\'%s (%s)\' % (o.name, o.type) for o in self._op_deque])\n\n    # Force-group ops.\n    force_group = force_group or []\n    if not isinstance(force_group, list):\n      raise TypeError(\'force_group must be a list of regex.\')\n    self._force_group_ops(force_group)\n\n    # Create blacklist regex.\n    blacklist_regex = \'\'\n    if regularizer_blacklist:\n      if not isinstance(regularizer_blacklist, list):\n        raise TypeError(\'regularizer_blacklist must be a list of regex.\')\n      blacklist_regex = \'|\'.join(regularizer_blacklist)\n\n    # Instantiate regularizers for all groups that have sources.\n    groups = set(self._op_group_dict.values())\n    blacklist_used = False\n    for group in groups:\n      # Collect regularizer for every source OpSlice in the OpGroup.\n      source_op_slices = []\n      regularizers = []\n\n      # If group is blacklisted, then no regularizers are created and all\n      # OpSlice will be assigned a None regularizer.\n      if op_handler_util.group_match(blacklist_regex, group.op_slices):\n        tf.logging.info(\'OpGroup not regularized due to blacklist: %s.\',\n                        group.op_slices)\n        blacklist_used = True\n      else:\n        for source_op_slice in group.source_op_slices:\n          handler = self._op_handler_dict[source_op_slice.op.type]\n          source_op_slices.append(source_op_slice)\n          regularizers.append(handler.create_regularizer(source_op_slice))\n\n      # Create a group regularizer and assign to all OpSlice in the OpGroup.  If\n      # there are no regularizers, assign None.\n      if regularizers:\n        if len(regularizers) > 1:\n          group_regularizer = create_grouping_regularizer(regularizers)\n        else:\n          group_regularizer = regularizers[0]\n      else:\n        group_regularizer = None\n      for op_slice in group.op_slices:\n        self._op_regularizer_dict[op_slice] = group_regularizer\n      tf.logging.info(\'Source OpSlice %s for OpGroup: %s.\', source_op_slices,\n                      group.op_slices)\n\n    if blacklist_regex and not blacklist_used:\n      raise ValueError(\'Blacklist regex never used: \\\'%s\\\'.\' % blacklist_regex)\n\n    tf.logging.info(\'OpRegularizerManager regularizing %d groups.\',\n                    len(set(self._op_group_dict.values())))\n\n    # Set scope of all ops to be ops that were analyzed.\n    self._all_ops = set(self._op_slice_dict.keys())\n\n  @property\n  def ops(self):\n    """"""Returns all ops discovered by OpRegularizerManager.""""""\n    return self._all_ops\n\n  def get_regularizer(self, op):\n    """"""Returns an OpRegularizer for the specified op.\n\n    If no OpRegularizer exists for any slices in the op, returns None.\n    Otherwise, create a ConstantOpRegularizer for any slices that are missing a\n    regularizer.\n\n    Args:\n      op: A tf.Operation.\n\n    Returns:\n      An OpRegularizer for op, or None if no OpRegularizer exists.\n    """"""\n    op_slices = self.get_op_slices(op)\n    regularizers = [\n        self._op_regularizer_dict.get(op_slice) for op_slice in op_slices\n    ]\n    # If all OpSlice have None regularizer, return None.\n    if not any(regularizers):\n      return None\n\n    regularizers = []\n    for op_slice in op_slices:\n      regularizer = self._op_regularizer_dict.get(op_slice)\n      if regularizer is None:\n        regularizer = constant_op_regularizer.ConstantOpRegularizer(\n            op_slice.slice.size)\n        self._op_regularizer_dict[op_slice] = regularizer\n      regularizers.append(regularizer)\n\n    # If op only has 1 OpSlice, return the regularizer for that OpSlice.\n    # Otherwise, return the concatenation of regularizers for the constituent\n    # OpSlice.\n    if len(regularizers) == 1:\n      return regularizers[0]\n    else:\n      return concat_and_slice_regularizers.ConcatRegularizer(regularizers)\n\n  def create_op_group_for_op_slice(self, op_slice, is_source=True):\n    """"""Creates an OpGroup for an OpSlice.\n\n    Args:\n      op_slice: OpSlice to create an OpGroup for.\n      is_source: Boolean indicating if the OpSlice is a source.\n\n    Returns:\n      OpGroup for the OpSlice.\n    """"""\n    # If OpSlice is not a source, then omit it from list of source OpSlice.\n    omit_source_op_slices = [] if is_source else [op_slice]\n\n    # Create OpGroup for the OpSlice.\n    op_group = OpGroup(op_slice, omit_source_op_slices=omit_source_op_slices)\n\n    # Update mapping of OpSlice to new OpGroup.\n    self._op_group_dict[op_slice] = op_group\n\n    return self.get_op_group(op_slice)\n\n  def group_op_slices(self, op_slices, omit_source_op_slices=None):\n    """"""Group op slices.\n\n    Each OpSlice in op_slices gets mapped to the same group.  Additionally, the\n    new group is also mapped to the list of OpSlice.  Note that this is\n    transitive, meaning that if group_op_slices([A, B]) is called when B is\n    grouped with C, then all 3 OpSlice [A, B, C] will be grouped together.\n\n    Args:\n      op_slices: List of OpSlice to group.\n      omit_source_op_slices: List of OpSlice to not track as sources in the new\n        OpGroup.\n    """"""\n    # Find groups that op slices are already a part of.\n    existing_op_groups = []\n    for op_slice in op_slices:\n      op_group = self.get_op_group(op_slice)\n      if op_group and op_group not in existing_op_groups:\n        existing_op_groups.append(op_group)\n\n    # Find OpSlice that will change group.\n    # pylint: disable=g-complex-comprehension\n    op_slices_to_update = [\n        os for og in existing_op_groups for os in og.op_slices\n    ]\n    for op_slice in op_slices:\n      if op_slice not in op_slices_to_update:\n        # This OpSlice does not have an OpGroup, so create a temporary one.\n        temp_op_group = self.create_op_group_for_op_slice(\n            op_slice, is_source=self.is_source_op(op_slice.op))\n        existing_op_groups.append(temp_op_group)\n        op_slices_to_update.append(op_slice)\n\n    # Create new OpGroup.\n    new_op_group = OpGroup(\n        op_groups=existing_op_groups,\n        omit_source_op_slices=omit_source_op_slices)\n\n    # Update mapping.\n    for op_slice in op_slices_to_update:\n      self._op_group_dict[op_slice] = new_op_group\n\n  def slice_op(self, op, sizes):\n    """"""Slice an op into specified sizes.\n\n    Creates OpSlice objects to represent slices of op.  The op is mapped to its\n    constituent OpSlice and reformed by concatenating the OpSlice.  For example,\n    if op has 10 channels and sizes is [3, 7], then this method returns\n    [OpSlice(op, (0, 3)), OpSlice(op, (3, 7))].\n\n    Note that sizes must be able to be aligned with the original op slice sizes.\n    An original slice can be partitioned into smaller slices, but the original\n    slice boundaries cannot be changed.  For example, if the original sizes are\n    [3, 7], the op cannot be sliced into sizes [2, 8].  However, slicing into\n    sizes [1, 2, 3, 4] is okay because the original slices are being sliced\n    (3 -> [1, 2] and 7 -> [3, 4]).\n\n    Also note that ops that are grouped with op will also be sliced accordingly,\n    with respective slices grouped.  For example, if OpA is grouped with OpB and\n    OpC, and OpA is sliced into OpA1 and OpA2, then the result will be groups\n    (OpA1, OpB1, OpC1) and (OpA2, OpB2, OpC2).\n\n    Args:\n      op: A tf.Operation to slice for the purpose of grouping.\n      sizes: List of Integer sizes to slice op into.  Sizes must sum up to the\n        number of output channels for op.\n\n    Raises:\n      ValueError: If sizes cannot be aligned with original op slice sizes.\n    """"""\n    old_op_slices = self.get_op_slices(op)\n    old_op_slice_sizes = op_handler_util.get_op_slice_sizes([old_op_slices])[0]\n\n    # If sizes already match, then nothing happens.\n    if old_op_slice_sizes == sizes:\n      return\n\n    # If sizes cannot be aligned with original sizes, raise exception.\n    try:\n      aligned_op_slice_sizes = op_handler_util.get_aligned_sizes(\n          [old_op_slice_sizes, sizes])\n    except ValueError as e:\n      raise ValueError(\'Error with op: %s: %s\' % (op.name, e.args[0]))\n\n    if sizes != aligned_op_slice_sizes:\n      raise ValueError(\'Cannot slice op %s from sizes %s to %s\' %\n                       (op.name, old_op_slice_sizes, sizes))\n\n    # Iterate through slices to find old slices that need to be resliced.\n    old_slice_index = 0\n    new_slice_index = 0\n    new_slice_count = 1\n    while (new_slice_index + new_slice_count <= len(aligned_op_slice_sizes) and\n           old_slice_index < len(old_op_slice_sizes)):\n      old_size = old_op_slice_sizes[old_slice_index]\n      new_size = op_handler_util.get_total_slice_size(sizes, new_slice_index,\n                                                      new_slice_count)\n      if old_size == new_size:\n        if new_slice_count > 1:\n          # If sizes match then this old slice is sliced into new_slice_count\n          # smaller slices.  Find the group of the old slice because all OpSlice\n          # in the group will need to be sliced similarly.\n          op_group = self.get_op_group(old_op_slices[old_slice_index])\n          if op_group:\n            group_op_slices = op_group.op_slices\n          else:\n            # If OpSlice has no group, just use the OpSlice itself.\n            group_op_slices = [old_op_slices[old_slice_index]]\n          new_op_slice_group = [list() for _ in range(new_slice_count)]\n          for group_op_slice in group_op_slices:\n            self._slice_op_slice(group_op_slice, sizes, new_slice_index,\n                                 new_slice_count, new_op_slice_group)\n\n          if op_group:\n            # Group all new OpSlice along each index.\n            for i in range(new_slice_count):\n              self.group_op_slices(new_op_slice_group[i])\n\n        # Update indices for the next slice.\n        old_slice_index += 1\n        new_slice_index += new_slice_count\n        new_slice_count = 1\n      else:\n        # If sizes do not match, then more new slices are needed to match the\n        # old slice.\n        new_slice_count += 1\n\n  def process_ops(self, ops):\n    """"""Add ops to processing queue.\n\n    Args:\n      ops: List of tf.Operation to put into the processing queue.\n    """"""\n    new_ops = [\n        op for op in ops if op not in self._op_deque and op in self._all_ops\n    ]\n    self._op_deque.extend(new_ops)\n\n  def process_ops_last(self, ops):\n    """"""Add ops to the end of the processing queue.\n\n    Used to avoid infinite looping if an OpHandler decides to defer processing\n    of itself.\n\n    Args:\n      ops: List of tf.Operation to put at the end of the processing queue.\n    """"""\n    new_ops = [op for op in ops if op not in self._op_deque]\n    self._op_deque.extendleft(new_ops)\n\n  def is_source_op(self, op):\n    """"""Returns True if op is a source op.\n\n    Args:\n      op: tf.Operation to check whether it is a source op.\n\n    Returns:\n      Boolean indicating if op is a source op.\n    """"""\n    op_handler = self._op_handler_dict[op.type]\n    return op_handler.is_source_op\n\n  def is_passthrough(self, op):\n    """"""Returns True if op is passthrough.\n\n    Args:\n      op: tf.Operation to check whether it is passthrough.\n\n    Returns:\n      Boolean indicating if op is passthrough.\n    """"""\n    op_handler = self._op_handler_dict[op.type]\n    return op_handler.is_passthrough\n\n  def get_op_slices(self, op):\n    """"""Returns OpSlice objects that are mapped to op.\n\n    If no mapping exists, a new OpSlice object will be created and mapped to op.\n\n    Args:\n      op: A tf.Operation to get OpSlice for.\n\n    Returns:\n      List of OpSlice that constitute op.\n    """"""\n    if op not in self._op_slice_dict:\n      # No OpSlice exists for op so create a new one.\n      size = op_handler_util.get_op_size(op)\n      if size > 0:\n        new_op_slice = OpSlice(op, Slice(0, size))\n        self._op_slice_dict[op] = [new_op_slice]\n      else:\n        self._op_slice_dict[op] = []\n    return self._op_slice_dict[op]\n\n  def get_op_group(self, op_slice):\n    """"""Returns the OpGroup that contains op_slice.\n\n    Returns None if no mapping exists.\n\n    Args:\n      op_slice: An OpSlice to find OpGroup for.\n\n    Returns:\n      OpGroup that contains op_slice, or None if no mapping exists.\n    """"""\n    return self._op_group_dict.get(op_slice)\n\n  def _slice_op_slice(self, op_slice, sizes, size_index, size_count,\n                      new_op_slice_group):\n    """"""Slices an OpSlice according to new sizes.\n\n    During reslicing, any OpSlice of an op could be resliced.  Given the new\n    sizes, this method finds the index where the old OpSlice matches, and\n    reslices the OpSlice according to the new sizes.  The new OpSlice are added\n    to new_op_slice_group by index, so that matching OpSlice can be grouped\n    together later.\n\n    Args:\n      op_slice: OpSlice that should be sliced.\n      sizes: List of integers specifying the new slice sizes.\n      size_index: Integer specifying which index in sizes corresponds to\n        op_slice.\n      size_count: Integer specifying how many slices op_slice will be sliced\n        into.\n      new_op_slice_group: List of list of new OpSlice that should be grouped\n        together.\n    """"""\n    op = op_slice.op\n    op_slices = self.get_op_slices(op)\n\n    # Get slice sizes for op.\n    op_slice_sizes = op_handler_util.get_op_slice_sizes([op_slices])[0]\n\n    # Find the slice index that needs to be resliced.\n    op_slice_index = op_slices.index(op_slice)\n\n    # Clear old OpSlice to OpGroup mapping.\n    if op_slice in self._op_group_dict:\n      del self._op_group_dict[op_slice]\n\n    # Calculate the new op slice sizes for the op.\n    op_slice_sizes.pop(op_slice_index)\n    # Keep track of which OpSlice were resliced.\n    is_resliced = [False] * len(op_slice_sizes)\n    for i in range(size_count):\n      op_slice_sizes.insert(op_slice_index + i, sizes[size_index + i])\n      is_resliced.insert(op_slice_index + i, True)\n\n    # Find source slices and slice the op.\n    is_source = self._get_source_slices(op_slice_sizes, op_slices)\n    slices = self._slice_op_with_sizes(op, op_slice_sizes, is_source,\n                                       is_resliced)\n\n    # Accumulate new OpSlice at the corresonding index.\n    for i in range(size_count):\n      new_op_slice_group[i].append(slices[op_slice_index + i])\n\n  def _slice_op_with_sizes(self, op, sizes, is_source, is_resliced):\n    """"""Slices the op according to sizes.\n\n    Args:\n      op: tf.Operation to slice.\n      sizes: List of integers of slice sizes.\n      is_source: List of booleans indicating which new slices are sources.\n      is_resliced: List of booleans indicating which slices are new.\n\n    Returns:\n      List of OpSlice for the newly sliced op.\n    """"""\n    old_slices = self.get_op_slices(op)\n    slices = []\n    for i, size in enumerate(sizes):\n      if is_resliced[i]:\n        # Sum up previous slice sizes to find start index of next slice.\n        index = sum(sizes[:i])\n        # Create new OpSlice for new slices.\n        new_slice = OpSlice(op, Slice(index, size))\n        # Create new OpGroup for OpSlice that should be sources.\n        if is_source[i]:\n          self.create_op_group_for_op_slice(new_slice)\n      else:\n        # If OpSlice is not new, reuse existing OpSlice.  Calculate the index of\n        # the old OpSlice by subtracting the count of new slices.\n        offset = max(is_resliced[:i].count(True) - 1, 0)\n        new_slice = old_slices[i - offset]\n      slices.append(new_slice)\n\n    # Update OpSlice for the op.\n    self._op_slice_dict[op] = slices\n\n    return slices\n\n  def _get_source_slices(self, sizes, op_slices):\n    """"""Returns list of booleans indicating which slices are sources.\n\n    If an OpSlice is a source, then its slices are also sources.  For example,\n    if an op consists of slices size [3, 7] where only the first slice is a\n    source, but is resliced into sizes [1, 2, 3, 4], then only the first 2\n    slices are sources.  Then this method would return\n    [True, True, False, False].\n\n    Args:\n      sizes: List of integers indicating new slice sizes.\n      op_slices: List of OpSlice before slicing.\n\n    Returns:\n      List of booleans indicating which slices are sources.\n    """"""\n    size_index = 0\n    slice_index = 0\n    is_source = []\n    while size_index < len(sizes):\n      # Get the OpGroup for the OpSlice to see if it is a source.\n      op_slice = op_slices[slice_index]\n      op_group = self.get_op_group(op_slice)\n      if op_group and op_slice in op_group.source_op_slices:\n        is_source.append(True)\n      else:\n        is_source.append(False)\n\n      # Check end indices of op_slices and sizes.  If they match, then current\n      # slice is done and slice index should be incremented.\n      end_index = sum(sizes[:size_index + 1])\n      slice_end_index = op_slice.slice.start_index + op_slice.slice.size\n      if end_index == slice_end_index:\n        slice_index += 1\n      size_index += 1\n    return is_source\n\n  def _dfs_for_source_ops(self, output_boundary, input_boundary=None):\n    """"""Performs DFS from ops and finds source ops to process.\n\n    Args:\n      output_boundary: An OpRegularizer will be created for all these\n        operations, and recursively for all ops they depend on via data\n        dependency that does not involve ops from input_boundary.\n      input_boundary: A list of ops where traversal should terminate.\n    """"""\n    if input_boundary:\n      input_boundary = set(input_boundary)\n    else:\n      input_boundary = set()\n    to_visit = list(output_boundary)\n    visited = set()\n    while to_visit:\n      # Get next op and mark as visited.\n      op = to_visit.pop()\n      visited.add(op)\n      if op in input_boundary:\n        continue\n      self._all_ops.add(op)\n\n      # Check if op is a source by querying OpHandler.\n      if self._op_handler_dict[op.type].is_source_op:\n        self.process_ops([op])\n\n      # Add op inputs to to_visit.\n      for tensor in op.inputs:\n        next_op = tensor.op\n        if next_op not in visited:\n          to_visit.append(next_op)\n\n  def _force_group_ops(self, force_group):\n    """"""Force-groups ops that match a regex.\n\n    Args:\n      force_group: List of regex.  For each regex, all matching ops will have\n        their groups merged.\n    """"""\n    for regex in force_group:\n      force_group_ops = []\n      for op, op_slices in self._op_slice_dict.items():\n        if op_handler_util.group_match(regex, op_slices):\n          force_group_ops.append(op)\n\n      # If no ops match, continue to the next force-group.\n      if not force_group_ops:\n        raise ValueError(\'Regex \\\'%s\\\' did not match any ops.\')\n\n      # Assert all ops to force-group have only 1 OpSlice.\n      if ([len(self._op_slice_dict[op]) for op in force_group_ops] !=\n          [1] * len(force_group_ops)):\n        multiple_slice_ops = []\n        for op in force_group_ops:\n          if len(self._op_slice_dict[op]) != 1:\n            multiple_slice_ops.append(op.name)\n        raise ValueError(\'Cannot force-group ops with more than 1 OpSlice: %s\' %\n                         multiple_slice_ops)\n\n      # Assert all ops to force-group have the same size.\n      target_op_size = self._op_slice_dict[force_group_ops[0]][0].slice.size\n      if ([self._op_slice_dict[op][0].slice.size for op in force_group_ops] !=\n          [target_op_size] * len(force_group_ops)):\n        op_names = [op.name for op in force_group_ops]\n        raise ValueError(\n            \'Cannot force-group ops with different sizes: %s\' % op_names)\n\n      # Group the ops.\n      self.group_op_slices(\n          [self._op_slice_dict[op][0] for op in force_group_ops])\n\n\nclass OpGroup(object):\n  """"""Helper class to keep track of OpSlice grouping.""""""\n\n  _static_index = 0\n\n  def __init__(self, op_slice=None, op_groups=None, omit_source_op_slices=None):\n    """"""Create OpGroup with self-incrementing index.\n\n    The OpGroup keeps a list of OpSlice that belong to the group.  The OpGroup\n    also keeps a separate list of source OpSlice.  If op_slice is specified, it\n    is assumed to be a source.  All OpGroup in op_groups will be merged together\n    to form a new OpGroup.  OpSlice listed in omit_source_op_slices will not\n    be tracked as sources in the new OpGroup.\n\n    Args:\n      op_slice: OpSlice to include in the group and track as a source.\n      op_groups: List of OpGroup to merge together into a new OpGroup.\n      omit_source_op_slices: List of OpSlice to not track as sources in the new\n        OpGroup.\n    """"""\n    omit_source_op_slices = omit_source_op_slices or []\n\n    # Add op_slice to the OpGroup.\n    self._op_slices = []\n    if op_slice:\n      self._op_slices.append(op_slice)\n    self._source_op_slices = []\n    if op_slice is not None and op_slice not in omit_source_op_slices:\n      self._source_op_slices.append(op_slice)\n\n    # Merge op_groups into a combined OpGroup.\n    if op_groups:\n      for op_group in op_groups:\n        # Collect OpSlice from each OpGroup.\n        for op_slice in op_group.op_slices:\n          if op_slice not in self._op_slices:\n            self._op_slices.append(op_slice)\n        # Collect source OpSlice from each OpGroup.\n        for source_op_slice in op_group.source_op_slices:\n          if (source_op_slice not in omit_source_op_slices and\n              source_op_slice not in self._source_op_slices):\n            self._source_op_slices.append(source_op_slice)\n\n    # Increment OpGroup index.\n    self._index = OpGroup._static_index\n    OpGroup._static_index += 1\n\n  @property\n  def op_slices(self):\n    """"""Return a list of OpSlice belonging to the OpGroup.""""""\n    return self._op_slices\n\n  @property\n  def source_op_slices(self):\n    """"""Return a list of OpSlice that are regularizer sources.""""""\n    return self._source_op_slices\n'"
morph_net/framework/op_regularizer_manager_test.py,187,"b'""""""Tests for op_regularizer_manager.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nfrom absl.testing import parameterized\nfrom morph_net.framework import batch_norm_source_op_handler\nfrom morph_net.framework import concat_op_handler\nfrom morph_net.framework import conv_source_op_handler\nfrom morph_net.framework import depthwise_convolution_op_handler\nfrom morph_net.framework import generic_regularizers\nfrom morph_net.framework import grouping_op_handler\nfrom morph_net.framework import op_regularizer_manager as orm\nfrom morph_net.framework import output_non_passthrough_op_handler\nfrom morph_net.testing import add_concat_model_stub\nfrom morph_net.testing import grouping_concat_model_stub\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import framework\nfrom tensorflow.contrib import layers\n\narg_scope = framework.arg_scope\n\nDEBUG_PRINTS = False\n\n\ndef _get_op(name):\n  return tf.get_default_graph().get_operation_by_name(name)\n\n\nclass OpRegularizerManagerTest(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    super(OpRegularizerManagerTest, self).setUp()\n    tf.set_random_seed(12)\n    np.random.seed(665544)\n    IndexOpRegularizer.reset_index()\n\n    # Create default OpHandler dict for testing.\n    self._default_op_handler_dict = collections.defaultdict(\n        grouping_op_handler.GroupingOpHandler)\n    self._default_op_handler_dict.update({\n        \'FusedBatchNormV3\':\n            IndexBatchNormSourceOpHandler(),\n        \'Conv2D\':\n            output_non_passthrough_op_handler.OutputNonPassthroughOpHandler(),\n        \'ConcatV2\':\n            concat_op_handler.ConcatOpHandler(),\n        \'DepthwiseConv2dNative\':\n            depthwise_convolution_op_handler.DepthwiseConvolutionOpHandler(),\n    })\n\n  def _batch_norm_scope(self):\n    params = {\n        \'trainable\': True,\n        \'normalizer_fn\': layers.batch_norm,\n        \'normalizer_params\': {\n            \'scale\': True\n        }\n    }\n\n    with arg_scope([layers.conv2d], **params) as sc:\n      return sc\n\n  @parameterized.named_parameters((\'Batch_no_par1\', True, False, \'conv1\'),\n                                  (\'Batch_par1\', True, True, \'conv1\'),\n                                  (\'NoBatch_no_par1\', False, False, \'conv1\'),\n                                  (\'NoBatch_par2\', False, True, \'conv2\'),\n                                  (\'Batch_no_par2\', True, False, \'conv2\'),\n                                  (\'Batch_par2\', True, True, \'conv2\'),\n                                  (\'Batch_par3\', True, True, \'conv3\'),\n                                  (\'NoBatch_par3\', False, True, \'conv3\'),\n                                  (\'NoBatch_no_par3\', False, False, \'conv3\'))\n  def testSimpleOpGetRegularizer(self, use_batch_norm, use_partitioner, scope):\n    # Tests the alive pattern of the conv and relu ops.\n    # use_batch_norm: A Boolean. Indicates if batch norm should be used.\n    # use_partitioner: A Boolean. Indicates if a fixed_size_partitioner should\n    #   be used.\n    # scope: A String with the scope to test.\n    sc = self._batch_norm_scope() if use_batch_norm else []\n    partitioner = tf.fixed_size_partitioner(2) if use_partitioner else None\n    model_stub = add_concat_model_stub\n    with arg_scope(sc):\n      with tf.variable_scope(tf.get_variable_scope(), partitioner=partitioner):\n        final_op = add_concat_model_stub.build_model()\n\n    # Instantiate OpRegularizerManager.\n    op_handler_dict = self._default_op_handler_dict\n    op_handler_dict[\'FusedBatchNormV3\'] = StubBatchNormSourceOpHandler(\n        model_stub)\n    if not use_batch_norm:\n      op_handler_dict[\'Conv2D\'] = StubConvSourceOpHandler(model_stub)\n    op_reg_manager = orm.OpRegularizerManager([final_op], op_handler_dict)\n\n    expected_alive = model_stub.expected_alive()\n    conv_reg = op_reg_manager.get_regularizer(_get_op(scope + \'/Conv2D\'))\n    self.assertAllEqual(expected_alive[scope], conv_reg.alive_vector)\n\n    relu_reg = op_reg_manager.get_regularizer(_get_op(scope + \'/Relu\'))\n    self.assertAllEqual(expected_alive[scope], relu_reg.alive_vector)\n\n  @parameterized.named_parameters((\'Batch_no_par\', True, False),\n                                  (\'Batch_par\', True, True),\n                                  (\'NoBatch_no_par\', False, False),\n                                  (\'NoBatch_par\', False, True))\n  def testConcatOpGetRegularizer(self, use_batch_norm, use_partitioner):\n    sc = self._batch_norm_scope() if use_batch_norm else []\n    partitioner = tf.fixed_size_partitioner(2) if use_partitioner else None\n    model_stub = add_concat_model_stub\n    with arg_scope(sc):\n      with tf.variable_scope(tf.get_variable_scope(), partitioner=partitioner):\n        final_op = add_concat_model_stub.build_model()\n\n    # Instantiate OpRegularizerManager.\n    op_handler_dict = self._default_op_handler_dict\n    op_handler_dict[\'FusedBatchNormV3\'] = StubBatchNormSourceOpHandler(\n        model_stub)\n    if not use_batch_norm:\n      op_handler_dict[\'Conv2D\'] = StubConvSourceOpHandler(model_stub)\n    op_reg_manager = orm.OpRegularizerManager([final_op], op_handler_dict)\n\n    expected_alive = model_stub.expected_alive()\n    expected = np.logical_or(expected_alive[\'conv4\'], expected_alive[\'concat\'])\n    conv_reg = op_reg_manager.get_regularizer(_get_op(\'conv4/Conv2D\'))\n    self.assertAllEqual(expected, conv_reg.alive_vector)\n\n    relu_reg = op_reg_manager.get_regularizer(_get_op(\'conv4/Relu\'))\n    self.assertAllEqual(expected, relu_reg.alive_vector)\n\n  @parameterized.named_parameters(\n      (\'_conv1\', \'conv1/Conv2D\', \'conv1\'),\n      (\'_conv2\', \'conv2/Conv2D\', \'conv2\'),\n      (\'_conv3\', \'conv3/Conv2D\', \'conv3\'),\n      (\'_conv4\', \'conv4/Conv2D\', \'conv4\'),\n  )\n  def testGroupConcatOpGetRegularizerValues(self, op_name, short_name):\n    model_stub = grouping_concat_model_stub\n    with arg_scope(self._batch_norm_scope()):\n      with tf.variable_scope(tf.get_variable_scope()):\n        final_op = model_stub.build_model()\n\n    # Instantiate OpRegularizerManager.\n    op_handler_dict = self._default_op_handler_dict\n    op_handler_dict[\'FusedBatchNormV3\'] = StubBatchNormSourceOpHandler(\n        model_stub)\n\n    op_reg_manager = orm.OpRegularizerManager([final_op], op_handler_dict)\n\n    expected_alive = model_stub.expected_alive()\n    expected_reg = model_stub.expected_regularization()\n\n    reg = op_reg_manager.get_regularizer(_get_op(op_name))\n    self.assertAllEqual(expected_alive[short_name], reg.alive_vector)\n    self.assertAllClose(expected_reg[short_name], reg.regularization_vector)\n\n  def testGroupConcatOpGetRegularizerObjects(self):\n    model_stub = grouping_concat_model_stub\n    with arg_scope(self._batch_norm_scope()):\n      with tf.variable_scope(tf.get_variable_scope()):\n        final_op = model_stub.build_model()\n\n    # Instantiate OpRegularizerManager.\n    op_handler_dict = self._default_op_handler_dict\n    op_handler_dict[\'FusedBatchNormV3\'] = StubBatchNormSourceOpHandler(\n        model_stub)\n\n    op_reg_manager = orm.OpRegularizerManager([final_op], op_handler_dict)\n    self.assertEqual(\n        op_reg_manager.get_regularizer(_get_op(\'conv1/Conv2D\')),\n        op_reg_manager.get_regularizer(_get_op(\'conv2/Conv2D\')))\n    self.assertEqual(\n        op_reg_manager.get_regularizer(_get_op(\'conv3/Conv2D\')),\n        op_reg_manager.get_regularizer(_get_op(\'conv4/Conv2D\')))\n\n  @parameterized.named_parameters((\'Concat_5\', True, 5),\n                                  (\'Concat_7\', True, 7),\n                                  (\'Add_6\', False, 6))\n  def testGetRegularizerForConcatWithNone(self, test_concat, depth):\n    image = tf.constant(0.0, shape=[1, 17, 19, 3])\n    conv2 = layers.conv2d(image, 5, [1, 1], padding=\'SAME\', scope=\'conv2\')\n    other_input = tf.add(\n        tf.identity(tf.constant(3.0, shape=[1, 17, 19, depth])), 3.0)\n    # other_input has None as regularizer.\n    concat = tf.concat([other_input, conv2], 3)\n    output = tf.add(concat, concat, name=\'output_out\')\n    op = concat.op if test_concat else output.op\n\n    # Instantiate OpRegularizerManager.\n    op_handler_dict = self._default_op_handler_dict\n    op_handler_dict[\'Conv2D\'] = StubConvSourceOpHandler(add_concat_model_stub)\n    op_reg_manager = orm.OpRegularizerManager([output.op], op_handler_dict)\n\n    expected_alive = add_concat_model_stub.expected_alive()\n    alive = op_reg_manager.get_regularizer(op).alive_vector\n    self.assertAllEqual([True] * depth, alive[:depth])\n    self.assertAllEqual(expected_alive[\'conv2\'], alive[depth:])\n\n  @parameterized.named_parameters((\'add\', tf.add),\n                                  (\'div\', tf.divide),\n                                  (\'mul\', tf.multiply),\n                                  (\'max\', tf.maximum),\n                                  (\'min\', tf.minimum),\n                                  (\'l2\', tf.squared_difference))\n  def testGroupingOps(self, tested_op):\n    th = 0.5\n    image = tf.constant(0.5, shape=[1, 17, 19, 3])\n\n    conv1 = layers.conv2d(image, 5, [1, 1], padding=\'SAME\', scope=\'conv1\')\n    conv2 = layers.conv2d(image, 5, [1, 1], padding=\'SAME\', scope=\'conv2\')\n    res = tested_op(conv1, conv2)\n\n    # Instantiate OpRegularizerManager.\n    op_handler_dict = self._default_op_handler_dict\n    op_handler_dict[\'Conv2D\'] = RandomConvSourceOpHandler(th)\n    op_reg_manager = orm.OpRegularizerManager([res.op], op_handler_dict)\n\n    alive = op_reg_manager.get_regularizer(res.op).alive_vector\n    conv1_reg = op_reg_manager.get_regularizer(conv1.op).regularization_vector\n    conv2_reg = op_reg_manager.get_regularizer(conv2.op).regularization_vector\n    with self.session():\n      self.assertAllEqual(alive, np.logical_or(conv1_reg.eval() > th,\n                                               conv2_reg.eval() > th))\n\n  def testCascadedGrouping(self):\n    inputs = tf.zeros([6, 8, 8, 10], name=\'prev\')\n    with arg_scope(\n        [layers.conv2d, layers.max_pool2d],\n        kernel_size=1,\n        stride=1,\n        padding=\'SAME\'):\n      net = layers.conv2d(inputs, 17, scope=\'conv/input\')\n\n      first = layers.conv2d(net, num_outputs=17, scope=\'conv/first\')\n      add_0 = tf.add(first, net, \'Add/first\')  # So conv/first must be 17.\n      second = layers.conv2d(add_0, num_outputs=17, scope=\'conv/second\')\n      out = tf.add(net, second, \'Add/second\')  # So conv/second must be 17.\n\n    # Instantiate OpRegularizerManager.\n    op_handler_dict = self._default_op_handler_dict\n    op_handler_dict[\'Conv2D\'] = IndexConvSourceOpHandler()\n    op_reg_manager = orm.OpRegularizerManager([out.op], op_handler_dict)\n\n    grouped_names = [\n        [op_slice.op.name for op_slice in group.op_slices]\n        for group in op_reg_manager._op_group_dict.values()]\n    expected = set([\n        \'conv/second/Conv2D\', \'Add/second\', \'conv/first/Conv2D\',\n        \'conv/input/Conv2D\', \'Add/first\'\n    ])\n    groups = []\n    for group in grouped_names:\n      filtered = []\n      for op_name in group:\n        if \'/Conv2D\' in op_name or \'Add/\' in op_name:\n          filtered.append(op_name)\n      if filtered:\n        groups.append(set(filtered))\n        if DEBUG_PRINTS:\n          print(\'Group Found = \', filtered)\n    self.assertIn(expected, groups)\n\n  def testBroadcast(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=1, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv2\')\n      tmp = c1 + c2\n      final_op = layers.conv2d(\n          tmp, num_outputs=13, kernel_size=3, scope=\'conv3\')\n\n    manager = orm.OpRegularizerManager(\n        [final_op.op], self._default_op_handler_dict)\n\n    c1_reg = manager.get_regularizer(_get_op(\'conv1/Conv2D\'))\n    c2_reg = manager.get_regularizer(_get_op(\'conv2/Conv2D\'))\n    add_reg = manager.get_regularizer(_get_op(\'add\'))\n    c3_reg = manager.get_regularizer(_get_op(\'conv3/Conv2D\'))\n\n    expected_c1_reg_size = 1\n    self.assertEqual(expected_c1_reg_size, c1_reg.regularization_vector.shape)\n    self.assertEqual(10, c2_reg.regularization_vector.shape)\n    self.assertEqual(10, add_reg.regularization_vector.shape)\n    self.assertEqual(13, c3_reg.regularization_vector.shape)\n\n    c1_slice = manager.get_op_slices(c1.op)[0]\n    c1_group = manager.get_op_group(c1_slice)\n    c2_slice = manager.get_op_slices(c2.op)[0]\n    c2_group = manager.get_op_group(c2_slice)\n    add_slice = manager.get_op_slices(tmp.op)[0]\n    add_group = manager.get_op_group(add_slice)\n    c3_slice = manager.get_op_slices(final_op.op)[0]\n    c3_group = manager.get_op_group(c3_slice)\n\n    # Verify all OpSlice grouped with c1 have size 1.\n    for op_slice in c1_group.op_slices:\n      self.assertEqual(1, op_slice.slice.size)\n\n    # Verify all OpSlice grouped with c2 have size 10.\n    for op_slice in c2_group.op_slices:\n      self.assertEqual(10, op_slice.slice.size)\n\n    # Verify c1 is not grouped with c2, add, or c3.\n    self.assertNotEqual(c1_group, c2_group)\n    self.assertNotEqual(c1_group, add_group)\n    self.assertNotEqual(c1_group, c3_group)\n\n    # Verify c2 and add are grouped to each other, but not c3.\n    self.assertEqual(c2_group, add_group)\n    self.assertNotEqual(c2_group, c3_group)\n\n  def testReuse(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    num_outputs = 3\n    kernel_size = 1\n    with arg_scope([layers.conv2d], normalizer_fn=layers.batch_norm):\n      with tf.variable_scope(\'parallel\', reuse=tf.AUTO_REUSE):\n        mul0 = layers.conv2d(inputs, num_outputs, kernel_size, scope=\'conv1\')\n        mul1 = layers.conv2d(inputs, num_outputs, kernel_size,\n                             activation_fn=tf.nn.sigmoid, scope=\'conv2\')\n        prev1 = np.prod([mul0, mul1])\n      with tf.variable_scope(\'parallel\', reuse=tf.AUTO_REUSE):\n        mul0_1 = layers.conv2d(prev1, num_outputs, kernel_size, scope=\'conv1\')\n        mul1_1 = layers.conv2d(prev1, num_outputs, kernel_size,\n                               activation_fn=tf.nn.sigmoid, scope=\'conv2\')\n      prev2 = np.prod([mul0_1, mul1_1])\n      prev3 = prev2 + 0.0\n      # This hack produces the desired grouping due to variable reuse.\n      # prev3 = prev2 + 0.0 * (mul0 + mul1 + mul0_1 + mul1_1)\n\n    manager = orm.OpRegularizerManager(\n        [prev3.op], self._default_op_handler_dict)\n\n    mul0_reg = manager.get_regularizer(_get_op(\'parallel/conv1/Conv2D\'))\n    mul1_reg = manager.get_regularizer(_get_op(\'parallel/conv2/Conv2D\'))\n    mul0_1_reg = manager.get_regularizer(_get_op(\'parallel_1/conv1/Conv2D\'))\n    mul1_1_reg = manager.get_regularizer(_get_op(\'parallel_1/conv2/Conv2D\'))\n\n    # Check that regularizers were grouped properly.\n    self.assertEqual(mul0_reg, mul1_reg)\n    self.assertEqual(mul0_1_reg, mul1_1_reg)\n    # These regularizers should be grouped due to reused variables.\n    # self.assertEqual(mul0_reg, mul0_1_reg)\n    # self.assertEqual(mul1_reg, mul1_1_reg)\n\n  def testGather(self):\n    gather_index = [5, 6, 7, 8, 9, 0, 1, 2, 3, 4]\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv1\')\n      gather = tf.gather(c1, gather_index, axis=3)\n\n    manager = orm.OpRegularizerManager(\n        [gather.op], self._default_op_handler_dict)\n\n    c1_reg = manager.get_regularizer(_get_op(\'conv1/Conv2D\'))\n    gather_reg = manager.get_regularizer(_get_op(\'GatherV2\'))\n\n    # Check regularizer indices.\n    self.assertAllEqual(list(range(10)), c1_reg.regularization_vector)\n    # This fails due to gather not being supported.  Once gather is supported,\n    # this test can be enabled to verify that the regularization vector is\n    # gathered in the same ordering as the tensor.\n    # self.assertAllEqual(\n    #     gather_index, gather_reg.regularization_vector)\n\n    # This test shows that gather is not supported.  The regularization vector\n    # has the same initial ordering after the gather op scrambled the\n    # channels.  Remove this once gather is supported.\n    self.assertAllEqual(list(range(10)), gather_reg.regularization_vector)\n\n  def testConcat(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv2\')\n      concat = tf.concat([c1, c2], axis=3)\n      tmp = c1 + c2\n\n    manager = orm.OpRegularizerManager(\n        [concat.op, tmp.op], self._default_op_handler_dict)\n\n    # Fetch OpSlice to verify grouping.\n    inputs_op_slice = manager.get_op_slices(inputs.op)[0]\n    c1_op_slice = manager.get_op_slices(c1.op)[0]\n    c2_op_slice = manager.get_op_slices(c2.op)[0]\n    tmp_op_slice = manager.get_op_slices(tmp.op)[0]\n    concat_op_slice0 = manager.get_op_slices(concat.op)[0]\n    concat_op_slice1 = manager.get_op_slices(concat.op)[1]\n\n    # Verify inputs and c1 have different group.\n    self.assertNotEqual(manager.get_op_group(inputs_op_slice),\n                        manager.get_op_group(c1_op_slice))\n\n    # Verify inputs and c2 have different group.\n    self.assertNotEqual(manager.get_op_group(inputs_op_slice),\n                        manager.get_op_group(c2_op_slice))\n\n    # Verify c1, c2, and add have the same group.\n    self.assertEqual(manager.get_op_group(c1_op_slice),\n                     manager.get_op_group(c2_op_slice))\n    self.assertEqual(manager.get_op_group(c1_op_slice),\n                     manager.get_op_group(tmp_op_slice))\n\n    # Verify concat slices are grouped with c1, c2, and add.\n    self.assertEqual(manager.get_op_group(c1_op_slice),\n                     manager.get_op_group(concat_op_slice0))\n    self.assertEqual(manager.get_op_group(c1_op_slice),\n                     manager.get_op_group(concat_op_slice1))\n\n  def testGroupingConcat(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=5, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(inputs, num_outputs=5, kernel_size=3, scope=\'conv2\')\n      concat = tf.concat([c1, c2], axis=2)\n\n    manager = orm.OpRegularizerManager([concat.op],\n                                       self._default_op_handler_dict)\n\n    # Fetch OpSlice to verify grouping.\n    inputs_op_slice = manager.get_op_slices(inputs.op)[0]\n    c1_op_slice = manager.get_op_slices(c1.op)[0]\n    c2_op_slice = manager.get_op_slices(c2.op)[0]\n    concat_op_slice = manager.get_op_slices(concat.op)[0]\n\n    # Verify inputs and c1 have different group.\n    self.assertNotEqual(\n        manager.get_op_group(inputs_op_slice),\n        manager.get_op_group(c1_op_slice))\n\n    # Verify inputs and c2 have different group.\n    self.assertNotEqual(\n        manager.get_op_group(inputs_op_slice),\n        manager.get_op_group(c2_op_slice))\n\n    # Verify c1, c2, and concat have the same group.\n    self.assertEqual(\n        manager.get_op_group(c1_op_slice), manager.get_op_group(c2_op_slice))\n    self.assertEqual(\n        manager.get_op_group(c1_op_slice),\n        manager.get_op_group(concat_op_slice))\n\n  def testBatchNormAfterConcat(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    # BN before concat - one per conv.\n    with arg_scope(\n        [layers.conv2d],\n        normalizer_fn=layers.batch_norm,\n        normalizer_params={\'fused\': True, \'scale\': True}):\n      left = layers.conv2d(inputs, 2, kernel_size=3, scope=\'left\')\n      right = layers.conv2d(inputs, 3, kernel_size=3, scope=\'right\')\n      concat = tf.concat([left, right], -1)\n\n    manager = orm.OpRegularizerManager(\n        [concat.op], self._default_op_handler_dict)\n\n    # Fetch OpSlice to verify grouping.\n    left_op_slice = manager.get_op_slices(left.op)[0]\n    right_op_slice = manager.get_op_slices(right.op)[0]\n    concat_op_slice0 = manager.get_op_slices(concat.op)[0]\n    concat_op_slice1 = manager.get_op_slices(concat.op)[1]\n\n    # Verify that left op is grouped with left part of concat.\n    self.assertEqual(manager.get_op_group(left_op_slice),\n                     manager.get_op_group(concat_op_slice0))\n\n    # Verify that right op is grouped with right part of concat.\n    self.assertEqual(manager.get_op_group(right_op_slice),\n                     manager.get_op_group(concat_op_slice1))\n\n    # BN after concat\n    tf.reset_default_graph()\n    inputs = tf.zeros([2, 4, 4, 3])\n    left = layers.conv2d(inputs, 3, kernel_size=3, scope=\'left_after\')\n    right = layers.conv2d(inputs, 4, kernel_size=3, scope=\'right_after\')\n    concat = tf.concat([left, right], -1)\n    batch_norm = layers.batch_norm(concat, fused=True, scale=True)\n\n    manager = orm.OpRegularizerManager(\n        [batch_norm.op], self._default_op_handler_dict)\n\n    # Fetch OpSlice to verify grouping.\n    left_op_slice = manager.get_op_slices(left.op)[0]\n    right_op_slice = manager.get_op_slices(right.op)[0]\n    concat_op_slice0 = manager.get_op_slices(concat.op)[0]\n    concat_op_slice1 = manager.get_op_slices(concat.op)[1]\n    batch_norm_op_slice0 = manager.get_op_slices(batch_norm.op)[0]\n    batch_norm_op_slice1 = manager.get_op_slices(batch_norm.op)[1]\n\n    # Verify that left op is grouped with left part of concat and batch norm.\n    self.assertEqual(manager.get_op_group(left_op_slice),\n                     manager.get_op_group(concat_op_slice0))\n    self.assertEqual(manager.get_op_group(left_op_slice),\n                     manager.get_op_group(batch_norm_op_slice0))\n\n    # Verify that right op is grouped with right part of concat and batch norm.\n    self.assertEqual(manager.get_op_group(right_op_slice),\n                     manager.get_op_group(concat_op_slice1))\n    self.assertEqual(manager.get_op_group(right_op_slice),\n                     manager.get_op_group(batch_norm_op_slice1))\n\n    # Verify that original concat OpSlice is removed.\n    old_concat_op_slice = orm.OpSlice(concat.op, orm.Slice(0, 7))\n    self.assertIsNone(manager.get_op_group(old_concat_op_slice))\n\n  def testNestedConcat(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    conv1 = layers.conv2d(inputs, num_outputs=1, kernel_size=3, scope=\'conv1\')\n    conv2 = layers.conv2d(inputs, num_outputs=1, kernel_size=3, scope=\'conv2\')\n    conv3 = layers.conv2d(inputs, num_outputs=1, kernel_size=3, scope=\'conv3\')\n    conv4 = layers.conv2d(inputs, num_outputs=1, kernel_size=3, scope=\'conv4\')\n    conv5 = layers.conv2d(inputs, num_outputs=1, kernel_size=3, scope=\'conv5\')\n    conv6 = layers.conv2d(inputs, num_outputs=1, kernel_size=3, scope=\'conv6\')\n    conv7 = layers.conv2d(inputs, num_outputs=1, kernel_size=3, scope=\'conv7\')\n    conv8 = layers.conv2d(inputs, num_outputs=1, kernel_size=3, scope=\'conv8\')\n    conv9 = layers.conv2d(inputs, num_outputs=1, kernel_size=3, scope=\'conv9\')\n    conv10 = layers.conv2d(inputs, num_outputs=1, kernel_size=3, scope=\'conv10\')\n\n    concat1 = tf.concat([conv1, conv2, conv3], axis=3)\n    concat2 = tf.concat([conv4, conv5, conv6, conv7], axis=3)\n    concat3 = tf.concat([conv8, conv9], axis=3)\n    concat4 = tf.concat([conv10], axis=3)\n\n    concat5 = tf.concat([concat1, concat2], axis=3)\n    concat6 = tf.concat([concat3, concat4], axis=3)\n\n    # This looks like [[[1, 2, 3], [4, 5, 6, 7]], [[8, 9], [10]]].\n    concat7 = tf.concat([concat5, concat6], axis=3)\n    batch_norm = layers.batch_norm(concat7)\n\n    manager = orm.OpRegularizerManager(\n        [batch_norm.op], self._default_op_handler_dict)\n\n    # Verify that batch norm gets sliced into individual channels due to\n    # concatenation of all the convolutions.\n    batch_norm_op_slices = manager.get_op_slices(batch_norm.op)\n    self.assertLen(batch_norm_op_slices, 10)\n    for i in range(10):\n      op_slice = batch_norm_op_slices[i]\n      self.assertEqual(i, op_slice.slice.start_index)\n      self.assertEqual(1, op_slice.slice.size)\n\n      # Verify other OpSlice are not grouped with this one.\n      group_op_slices = manager.get_op_group(op_slice).op_slices\n      for j in range(10):\n        if i != j:\n          self.assertNotIn(batch_norm_op_slices[j], group_op_slices)\n\n  def testSplit(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv1\')\n      split = tf.split(c1, [5, 5], axis=3)\n      c2 = layers.conv2d(inputs, num_outputs=5, kernel_size=3, scope=\'conv2\')\n      c3 = layers.conv2d(inputs, num_outputs=5, kernel_size=3, scope=\'conv3\')\n      out1 = split[0] + c2\n      out2 = split[1] + c3\n\n    with self.assertRaises(RuntimeError):\n      # Regularizer assignment fails because c2/c3 have size 5 while split has\n      # size 10, so regularizer grouping fails.\n      orm.OpRegularizerManager(\n          [out1.op, out2.op], self._default_op_handler_dict,\n          iteration_limit=100)\n\n  @parameterized.named_parameters((\'DepthMultiplier_1\', 8, 1),\n                                  (\'DepthMultiplier_2\', 8, 2),\n                                  (\'DepthMultiplier_7\', 8, 7),\n                                  (\'DepthMultiplier_1_no_pointwise\', None, 1),\n                                  (\'DepthMultiplier_2_no_pointwise\', None, 2),\n                                  (\'DepthMultiplier_7_no_pointwise\', None, 7))\n  def testSeparableConv2D_DepthMultiplier(\n      self, pointwise_outputs, depth_multiplier):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      num_outputs = 5\n      c1 = layers.conv2d(\n          inputs, num_outputs=num_outputs, kernel_size=3, scope=\'conv1\')\n      c2 = layers.separable_conv2d(\n          c1, num_outputs=pointwise_outputs, kernel_size=3,\n          depth_multiplier=depth_multiplier, scope=\'conv2\')\n      identity = tf.identity(c2)\n\n    manager = orm.OpRegularizerManager(\n        [identity.op], self._default_op_handler_dict)\n\n    # If separable_conv2d is passed num_outputs=None, the name of the depthwise\n    # convolution changes.\n    depthwise_conv_name = \'conv2/separable_conv2d/depthwise\'\n    if pointwise_outputs is None:\n      depthwise_conv_name = \'conv2/depthwise\'\n\n    dwise_op = _get_op(depthwise_conv_name)\n    dwise_reg = manager.get_regularizer(dwise_op)\n\n    # Verify that depthwise convolution has output tensor and regularization\n    # vector of size 5 * depth_multiplier channels where 5 is the number of\n    # input channels from c1.\n    self.assertEqual(num_outputs * depth_multiplier,\n                     dwise_op.outputs[0].shape[-1])\n    self.assertEqual(num_outputs * depth_multiplier,\n                     dwise_reg.regularization_vector.shape[-1])\n\n    # Verify OpSlice in the depthwise convolution has the correct grouping.\n    relu1_op_slices = manager.get_op_slices(c1.op)\n    dwise_op_slices = manager.get_op_slices(dwise_op)\n    relu2_op_slices = manager.get_op_slices(c2.op)\n\n    # Expected input grouping has a pattern like [0, 0, 1, 1, 2, 2, ...].\n    # pylint: disable=g-complex-comprehension\n    expected_input_grouping = [j\n                               for j in range(num_outputs)\n                               for i in range(depth_multiplier)]\n    # pylint: enable=g-complex-comprehension\n    # Expected output grouping is just linear, but with\n    # num_outputs * depth_multiplier channels (e.g. [0, 1, 2, 3, ...]).\n    expected_output_grouping = range(num_outputs * depth_multiplier)\n\n    for i, op_slice in enumerate(dwise_op_slices):\n      group = manager.get_op_group(op_slice)\n      group_op_slices = group.op_slices\n      self.assertIn(relu1_op_slices[expected_input_grouping[i]],\n                    group_op_slices)\n      self.assertIn(dwise_op_slices[expected_output_grouping[i]],\n                    group_op_slices)\n      if pointwise_outputs is None:\n        # When pointwise_outputs is None, the pointwise convolution is omitted\n        # and the depthwise convolution is immediately followed by\n        # BiasAdd -> Relu ops.  In that case, verify that input channels of the\n        # depthwise convolution are correctly grouped with the output (relu2)\n        # channels.  Otherwise, the depthwise convolution is immediately\n        # followed by a pointwise convolution which is non-passthrough, so there\n        # is no output grouping to verify.\n        self.assertIn(relu2_op_slices[expected_output_grouping[i]],\n                      group_op_slices)\n\n  def testAddN(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity1 = tf.identity(inputs)\n    identity2 = tf.identity(inputs)\n    identity3 = tf.identity(inputs)\n    identity4 = tf.identity(inputs)\n    add_n = tf.add_n([identity1, identity2, identity3, identity4])\n    batch_norm = layers.batch_norm(add_n)\n\n    manager = orm.OpRegularizerManager(\n        [batch_norm.op], op_handler_dict=self._default_op_handler_dict)\n\n    op_slices = manager.get_op_slices(identity1.op)\n    self.assertLen(op_slices, 1)\n    op_group = manager.get_op_group(op_slices[0]).op_slices\n\n    # Verify all ops are in the same group.\n    for test_op in (identity1.op, identity2.op, identity3.op, identity4.op,\n                    add_n.op, batch_norm.op):\n      test_op_slices = manager.get_op_slices(test_op)\n      self.assertLen(test_op_slices, 1)\n      self.assertIn(test_op_slices[0], op_group)\n\n  def testAddN_Duplicates(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity = tf.identity(inputs)\n    add_n = tf.add_n([identity, identity, identity, identity])\n    batch_norm = layers.batch_norm(add_n)\n\n    manager = orm.OpRegularizerManager(\n        [batch_norm.op], op_handler_dict=self._default_op_handler_dict)\n\n    op_slices = manager.get_op_slices(identity.op)\n    self.assertLen(op_slices, 1)\n    op_group = manager.get_op_group(op_slices[0]).op_slices\n\n    # Verify all ops are in the same group.\n    for test_op in (identity.op, add_n.op, batch_norm.op):\n      test_op_slices = manager.get_op_slices(test_op)\n      self.assertLen(test_op_slices, 1)\n      self.assertIn(test_op_slices[0], op_group)\n\n  def testInit_Add(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv2\')\n      add = c1 + c2\n      c3 = layers.conv2d(add, num_outputs=10, kernel_size=3, scope=\'conv3\')\n      out = tf.identity(c3)\n\n    manager = orm.OpRegularizerManager(\n        [out.op], self._default_op_handler_dict, SumGroupingRegularizer)\n\n    # Fetch OpSlice to verify grouping.\n    inputs_op_slice = manager.get_op_slices(inputs.op)[0]\n    c1_op_slice = manager.get_op_slices(c1.op)[0]\n    c2_op_slice = manager.get_op_slices(c2.op)[0]\n    add_op_slice = manager.get_op_slices(add.op)[0]\n    c3_op_slice = manager.get_op_slices(c3.op)[0]\n    out_op_slice = manager.get_op_slices(out.op)[0]\n\n    # Verify inputs and c1 have different group.\n    self.assertNotEqual(manager.get_op_group(inputs_op_slice),\n                        manager.get_op_group(c1_op_slice))\n    self.assertNotEqual(manager.get_regularizer(inputs.op),\n                        manager.get_regularizer(c1.op))\n\n    # Verify inputs and c2 have different group.\n    self.assertNotEqual(manager.get_op_group(inputs_op_slice),\n                        manager.get_op_group(c2_op_slice))\n    self.assertNotEqual(manager.get_regularizer(inputs.op),\n                        manager.get_regularizer(c2.op))\n\n    # Verify c1, c2, and add have the same group.\n    self.assertEqual(manager.get_op_group(c1_op_slice),\n                     manager.get_op_group(c2_op_slice))\n    self.assertEqual(manager.get_op_group(c1_op_slice),\n                     manager.get_op_group(add_op_slice))\n    self.assertEqual(manager.get_regularizer(c1.op),\n                     manager.get_regularizer(c2.op))\n    self.assertEqual(manager.get_regularizer(c1.op),\n                     manager.get_regularizer(add.op))\n\n    # Verify c3 and out have the same group, which differs from c1 and c2.\n    self.assertEqual(manager.get_op_group(c3_op_slice),\n                     manager.get_op_group(out_op_slice))\n    self.assertNotEqual(manager.get_op_group(c3_op_slice),\n                        manager.get_op_group(c1_op_slice))\n    self.assertEqual(manager.get_regularizer(c3.op),\n                     manager.get_regularizer(out.op))\n    self.assertNotEqual(manager.get_regularizer(c3.op),\n                        manager.get_regularizer(c1.op))\n\n  def testInit_Concat(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv2\')\n      concat = tf.concat([c1, c2], axis=3)\n      out = tf.identity(concat)\n\n    manager = orm.OpRegularizerManager(\n        [out.op], self._default_op_handler_dict, SumGroupingRegularizer)\n\n    # Fetch OpSlice to verify grouping.\n    inputs_op_slice = manager.get_op_slices(inputs.op)[0]\n    c1_op_slice = manager.get_op_slices(c1.op)[0]\n    c2_op_slice = manager.get_op_slices(c2.op)[0]\n    out_op_slice0 = manager.get_op_slices(out.op)[0]\n    out_op_slice1 = manager.get_op_slices(out.op)[1]\n\n    # Verify inputs and c1 have different group and OpRegularizer.\n    self.assertNotEqual(manager.get_op_group(inputs_op_slice),\n                        manager.get_op_group(c1_op_slice))\n    self.assertNotEqual(manager.get_regularizer(inputs.op),\n                        manager.get_regularizer(c1.op))\n\n    # Verify inputs and c2 have different group and OpRegularizer.\n    self.assertNotEqual(manager.get_op_group(inputs_op_slice),\n                        manager.get_op_group(c2_op_slice))\n    self.assertNotEqual(manager.get_regularizer(inputs.op),\n                        manager.get_regularizer(c2.op))\n\n    # Verify c1 and c2 have different group and OpRegularizer.\n    self.assertNotEqual(manager.get_op_group(c1_op_slice),\n                        manager.get_op_group(c2_op_slice))\n    self.assertNotEqual(manager.get_regularizer(c1.op),\n                        manager.get_regularizer(c2.op))\n\n    # Verify c1 is grouped with first slice of out.\n    self.assertEqual(manager.get_op_group(c1_op_slice),\n                     manager.get_op_group(out_op_slice0))\n\n    # Verify c2 is grouped with second slice of out.\n    self.assertEqual(manager.get_op_group(c2_op_slice),\n                     manager.get_op_group(out_op_slice1))\n\n    # Verify out regularization_vector is the concat of c1 and c2\n    # regularizertion_vector.\n    self.assertAllEqual(\n        manager.get_regularizer(c1.op).regularization_vector,\n        manager.get_regularizer(out.op).regularization_vector[0:10])\n    self.assertAllEqual(\n        manager.get_regularizer(c2.op).regularization_vector,\n        manager.get_regularizer(out.op).regularization_vector[10:20])\n\n  def testInit_AddConcat(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv2\')\n      add = c1 + c2\n      c3 = layers.conv2d(add, num_outputs=10, kernel_size=3, scope=\'conv3\')\n      out = tf.identity(c3)\n      concat = tf.concat([c1, c2], axis=3)\n      c4 = layers.conv2d(concat, num_outputs=10, kernel_size=3, scope=\'conv4\')\n\n    manager = orm.OpRegularizerManager(\n        [out.op, c4.op], self._default_op_handler_dict, SumGroupingRegularizer)\n\n    # Fetch OpSlice to verify grouping.\n    inputs_op_slice = manager.get_op_slices(inputs.op)[0]\n    c1_op_slice = manager.get_op_slices(c1.op)[0]\n    c2_op_slice = manager.get_op_slices(c2.op)[0]\n    add_op_slice = manager.get_op_slices(add.op)[0]\n    c3_op_slice = manager.get_op_slices(c3.op)[0]\n    out_op_slice = manager.get_op_slices(out.op)[0]\n    concat_op_slice0 = manager.get_op_slices(concat.op)[0]\n    concat_op_slice1 = manager.get_op_slices(concat.op)[1]\n    c4_op_slice = manager.get_op_slices(c4.op)[0]\n\n    # Verify inputs and c1 have different group.\n    self.assertNotEqual(manager.get_op_group(inputs_op_slice),\n                        manager.get_op_group(c1_op_slice))\n    self.assertNotEqual(manager.get_regularizer(inputs.op),\n                        manager.get_regularizer(c1.op))\n\n    # Verify inputs and c2 have different group.\n    self.assertNotEqual(manager.get_op_group(inputs_op_slice),\n                        manager.get_op_group(c2_op_slice))\n    self.assertNotEqual(manager.get_regularizer(inputs.op),\n                        manager.get_regularizer(c2.op))\n\n    # Verify c1, c2, and add have the same group.\n    self.assertEqual(manager.get_op_group(c1_op_slice),\n                     manager.get_op_group(c2_op_slice))\n    self.assertEqual(manager.get_op_group(c1_op_slice),\n                     manager.get_op_group(add_op_slice))\n    self.assertEqual(manager.get_regularizer(c1.op),\n                     manager.get_regularizer(c2.op))\n    self.assertEqual(manager.get_regularizer(c1.op),\n                     manager.get_regularizer(add.op))\n\n    # Verify c3 and out have the same group, which differs from c1 and c2.\n    self.assertEqual(manager.get_op_group(c3_op_slice),\n                     manager.get_op_group(out_op_slice))\n    self.assertNotEqual(manager.get_op_group(c3_op_slice),\n                        manager.get_op_group(c1_op_slice))\n    self.assertEqual(manager.get_regularizer(c3.op),\n                     manager.get_regularizer(out.op))\n    self.assertNotEqual(manager.get_regularizer(c3.op),\n                        manager.get_regularizer(c1.op))\n\n    # Verify concat slices are grouped with c1, c2, and add.\n    self.assertEqual(manager.get_op_group(c1_op_slice),\n                     manager.get_op_group(concat_op_slice0))\n    self.assertEqual(manager.get_op_group(c1_op_slice),\n                     manager.get_op_group(concat_op_slice1))\n\n    # Verify concat regularization_vector is the concat of c1 and c2\n    # regularizertion_vector.\n    self.assertAllEqual(\n        manager.get_regularizer(c1.op).regularization_vector,\n        manager.get_regularizer(concat.op).regularization_vector[0:10])\n    self.assertAllEqual(\n        manager.get_regularizer(c2.op).regularization_vector,\n        manager.get_regularizer(concat.op).regularization_vector[10:20])\n\n    # Verify c4 has a different group than c1, c2, and add.\n    self.assertNotEqual(manager.get_op_group(c1_op_slice),\n                        manager.get_op_group(c4_op_slice))\n    self.assertNotEqual(manager.get_regularizer(c1.op),\n                        manager.get_regularizer(c4.op))\n\n  def testInit_AddConcat_AllOps(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv2\')\n      add = c1 + c2\n      c3 = layers.conv2d(add, num_outputs=10, kernel_size=3, scope=\'conv3\')\n      out = tf.identity(c3)\n      concat = tf.concat([c1, c2], axis=3)\n      c4 = layers.conv2d(concat, num_outputs=10, kernel_size=3, scope=\'conv4\')\n\n    manager = orm.OpRegularizerManager(\n        [out.op], self._default_op_handler_dict, SumGroupingRegularizer)\n\n    # Op c4 is not in the DFS path of out.  Verify that OpRegularizerManager\n    # does not process c4.\n    self.assertNotIn(c4.op, manager.ops)\n    self.assertNotIn(concat.op, manager.ops)\n\n  def testInit_ForceGroup(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(c1, num_outputs=10, kernel_size=3, scope=\'conv2\')\n      c3 = layers.conv2d(c2, num_outputs=10, kernel_size=3, scope=\'conv3\')\n\n    # Initialize OpRegularizerManager with no force-grouping.\n    manager = orm.OpRegularizerManager(\n        [c3.op], self._default_op_handler_dict, SumGroupingRegularizer)\n\n    # Verify that c2 is not grouped with c3.\n    c2_op_slices = manager.get_op_slices(c2.op)\n    self.assertLen(c2_op_slices, 1)\n    c2_op_slice = c2_op_slices[0]\n    c2_group = manager.get_op_group(c2_op_slice)\n    c3_op_slices = manager.get_op_slices(c3.op)\n    self.assertLen(c3_op_slices, 1)\n    c3_op_slice = c3_op_slices[0]\n    self.assertNotIn(c3_op_slice, c2_group.op_slices)\n\n    # Force-group c2 and c3.\n    manager = orm.OpRegularizerManager(\n        [c3.op], self._default_op_handler_dict, SumGroupingRegularizer,\n        force_group=[\'conv2|conv3\'])\n\n    # Verify that c2 is grouped with c3.\n    c2_op_slices = manager.get_op_slices(c2.op)\n    self.assertLen(c2_op_slices, 1)\n    c2_op_slice = c2_op_slices[0]\n    c2_group = manager.get_op_group(c2_op_slice)\n    c3_op_slices = manager.get_op_slices(c3.op)\n    self.assertLen(c3_op_slices, 1)\n    c3_op_slice = c3_op_slices[0]\n    self.assertIn(c3_op_slice, c2_group.op_slices)\n\n  def testInit_ForceGroup_MultipleOpSlice(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=5, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(inputs, num_outputs=5, kernel_size=3, scope=\'conv2\')\n      concat = tf.concat([c1, c2], axis=3)\n      c3 = layers.conv2d(concat, num_outputs=10, kernel_size=3, scope=\'conv3\')\n\n    # Verify force-group with multiple OpSlice raises ValueError.\n    self.assertRaisesRegexp(\n        ValueError,\n        r\'Cannot force-group ops with more than 1 OpSlice: \\[u?\\\'concat\\\'\\]\',\n        orm.OpRegularizerManager, [c3.op], self._default_op_handler_dict,\n        SumGroupingRegularizer, force_group=[\'conv3|concat\'])\n\n  def testInit_ForceGroup_SizeMismatch(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(c1, num_outputs=10, kernel_size=3, scope=\'conv2\')\n      # c3 has size 9 instead of 10.\n      c3 = layers.conv2d(c2, num_outputs=9, kernel_size=3, scope=\'conv3\')\n\n    # Verify size mismatch raises ValueError.\n    self.assertRaisesRegexp(\n        ValueError,\n        r\'Cannot force-group ops with different sizes: \\[.*\\]\',\n        orm.OpRegularizerManager, [c3.op], self._default_op_handler_dict,\n        SumGroupingRegularizer, force_group=[\'conv2|conv3\'])\n\n  def testInit_ForceGroup_NotList(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n\n    # Verify that force_group string instead of a list raises exception.\n    self.assertRaisesRegexp(\n        TypeError,\n        r\'force_group must be a list of regex.\',\n        orm.OpRegularizerManager, [inputs.op], self._default_op_handler_dict,\n        SumGroupingRegularizer, force_group=\'conv\')\n\n  def testInit_Blacklist(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=3, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(c1, num_outputs=4, kernel_size=3, scope=\'conv2\')\n      c3 = layers.conv2d(c2, num_outputs=5, kernel_size=3, scope=\'conv3\')\n\n    # Verify c2 has a regularizer.\n    manager = orm.OpRegularizerManager(\n        [c3.op], self._default_op_handler_dict, SumGroupingRegularizer)\n    self.assertIsNotNone(manager.get_regularizer(c2.op))\n\n    # Verify c2 has None regularizer after blacklisting.\n    manager = orm.OpRegularizerManager(\n        [c3.op], self._default_op_handler_dict, SumGroupingRegularizer,\n        regularizer_blacklist=[\'conv2\'])\n    self.assertIsNone(manager.get_regularizer(c2.op))\n\n  def testInit_BlacklistGroup(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv2\')\n      add = c1 + c2\n      c3 = layers.conv2d(add, num_outputs=10, kernel_size=3, scope=\'conv3\')\n\n    # Verify c2 has a regularizer.\n    manager = orm.OpRegularizerManager(\n        [c3.op], self._default_op_handler_dict, SumGroupingRegularizer)\n    self.assertIsNotNone(manager.get_regularizer(c2.op))\n\n    # Verify c2 has None regularizer after blacklisting c1 which is grouped.\n    manager = orm.OpRegularizerManager(\n        [c3.op], self._default_op_handler_dict, SumGroupingRegularizer,\n        regularizer_blacklist=[\'conv1\'])\n    self.assertIsNone(manager.get_regularizer(c2.op))\n\n  def testInit_BlacklistGroup_NoMatch(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv2\')\n      add = c1 + c2\n      c3 = layers.conv2d(add, num_outputs=10, kernel_size=3, scope=\'conv3\')\n\n    # Verify blacklist regex without match raises ValueError\n    self.assertRaisesWithLiteralMatch(\n        ValueError,\n        \'Blacklist regex never used: \\\'oops\\\'.\',\n        orm.OpRegularizerManager, [c3.op], self._default_op_handler_dict,\n        SumGroupingRegularizer, regularizer_blacklist=[\'oops\'])\n\n  def testInit_BlacklistGroup_NotList(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n\n    # Verify that regularizer_blacklist string instead of a list raises\n    # exception.\n    self.assertRaisesRegexp(\n        TypeError,\n        r\'regularizer_blacklist must be a list of regex.\',\n        orm.OpRegularizerManager, [inputs.op], self._default_op_handler_dict,\n        SumGroupingRegularizer, regularizer_blacklist=\'conv\')\n\n  def testInit_IterationLimit(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n\n    # Verify that reaching iteration limit raises exception.\n    self.assertRaisesRegexp(\n        RuntimeError,\n        r\'OpRegularizerManager could not handle ops:\',\n        orm.OpRegularizerManager, [inputs.op], self._default_op_handler_dict,\n        SumGroupingRegularizer, iteration_limit=0)\n\n  def testGetRegularizer(self):\n    op1 = tf.zeros([2, 4, 4, 3])\n    op2 = tf.zeros([2, 4, 4, 3])\n    op3 = tf.zeros([2, 4, 4, 10])\n\n    manager = orm.OpRegularizerManager([], self._default_op_handler_dict)\n    manager.slice_op(op3.op, [1, 2, 3, 4])\n\n    # op2 has 1 OpSlice and op3 has 4 OpSlice of size [1, 2, 3, 4].\n    op2_slices = manager.get_op_slices(op2.op)\n    op3_slices = manager.get_op_slices(op3.op)\n\n    op2_reg = IndexOpRegularizer(op2_slices[0], manager)\n    op3_reg0 = IndexOpRegularizer(op3_slices[0], manager)\n    op3_reg1 = IndexOpRegularizer(op3_slices[1], manager)\n    op3_reg2 = IndexOpRegularizer(op3_slices[2], manager)\n    op3_reg3 = IndexOpRegularizer(op3_slices[3], manager)\n\n    # Map OpSlice to OpRegularizer.\n    manager._op_regularizer_dict = {\n        op2_slices[0]: op2_reg,\n        op3_slices[0]: op3_reg0,\n        op3_slices[1]: op3_reg1,\n        op3_slices[2]: op3_reg2,\n        op3_slices[3]: op3_reg3,\n    }\n\n    # Verify None is returned if OpSlice does not have OpRegularizer.\n    self.assertIsNone(manager.get_regularizer(op1.op))\n\n    # Verify OpRegularizer for op with single OpSlice.\n    self.assertAllEqual([0, 1, 2],\n                        manager.get_regularizer(op2.op).regularization_vector)\n\n    # Verify OpRegularizer for op with multiple OpSlice.\n    self.assertAllEqual(list(range(3, 13)),\n                        manager.get_regularizer(op3.op).regularization_vector)\n\n    # Verify OpRegularzier for op with multiple OpSlice but not all slices have\n    # a regularizer.\n    del manager._op_regularizer_dict[op3_slices[2]]\n    expected_regularization_vector = [3, 4, 5, 0, 0, 0, 9, 10, 11, 12]\n    self.assertAllEqual(expected_regularization_vector,\n                        manager.get_regularizer(op3.op).regularization_vector)\n\n  def testCreateOpGroupForOpSlice_Source(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity = tf.identity(inputs)\n\n    manager = orm.OpRegularizerManager([])\n\n    # Create OpSlice for each identity op.\n    op_slice = manager.get_op_slices(identity.op)[0]\n\n    # Create OpGroup for each OpSlice.\n    op_group = manager.create_op_group_for_op_slice(op_slice)\n\n    self.assertListEqual([op_slice], op_group.op_slices)\n    self.assertListEqual([op_slice], op_group.source_op_slices)\n    self.assertEqual(op_group, manager.get_op_group(op_slice))\n\n  def testCreateOpGroupForOpSlice_NotSource(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity = tf.identity(inputs)\n\n    manager = orm.OpRegularizerManager([])\n\n    # Create OpSlice for each identity op.\n    op_slice = manager.get_op_slices(identity.op)[0]\n\n    # Create OpGroup for each OpSlice.\n    op_group = manager.create_op_group_for_op_slice(op_slice, is_source=False)\n\n    self.assertListEqual([op_slice], op_group.op_slices)\n    self.assertListEqual([], op_group.source_op_slices)\n    self.assertEqual(op_group, manager.get_op_group(op_slice))\n\n  def testGroupOpSlices(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity1 = tf.identity(inputs)\n    identity2 = tf.identity(inputs)\n    identity3 = tf.identity(inputs)\n    identity4 = tf.identity(inputs)\n    identity5 = tf.identity(inputs)\n    identity6 = tf.identity(inputs)\n    identity7 = tf.identity(inputs)\n    identity8 = tf.identity(inputs)\n\n    manager = orm.OpRegularizerManager([])\n\n    # Create OpSlice for each identity op.\n    op_slice1 = manager.get_op_slices(identity1.op)[0]\n    op_slice2 = manager.get_op_slices(identity2.op)[0]\n    op_slice3 = manager.get_op_slices(identity3.op)[0]\n    op_slice4 = manager.get_op_slices(identity4.op)[0]\n    op_slice5 = manager.get_op_slices(identity5.op)[0]\n    op_slice6 = manager.get_op_slices(identity6.op)[0]\n    op_slice7 = manager.get_op_slices(identity7.op)[0]\n    op_slice8 = manager.get_op_slices(identity8.op)[0]\n\n    # Create OpGroup for each OpSlice.\n    op_group1 = manager.create_op_group_for_op_slice(op_slice1)\n    op_group2 = manager.create_op_group_for_op_slice(op_slice2)\n    op_group3 = manager.create_op_group_for_op_slice(op_slice3)\n    op_group4 = manager.create_op_group_for_op_slice(op_slice4)\n    op_group5 = manager.create_op_group_for_op_slice(op_slice5)\n    op_group6 = manager.create_op_group_for_op_slice(op_slice6)\n    op_group7 = manager.create_op_group_for_op_slice(op_slice7)\n    op_group8 = manager.create_op_group_for_op_slice(op_slice8)\n\n    # Group all OpGroup together by grouping their OpSlice.\n    manager.group_op_slices([op_slice1, op_slice2, op_slice3, op_slice4,\n                             op_slice5, op_slice6, op_slice7, op_slice8])\n\n    expected_group = orm.OpGroup(\n        op_groups=[op_group1, op_group2, op_group3, op_group4, op_group5,\n                   op_group6, op_group7, op_group8])\n\n    # Check all OpSlice are in one big group.\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice1).op_slices)\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice2).op_slices)\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice3).op_slices)\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice4).op_slices)\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice5).op_slices)\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice6).op_slices)\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice7).op_slices)\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice8).op_slices)\n\n  def testGroupOpSlices_TransitiveGrouping(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity1 = tf.identity(inputs)\n    identity2 = tf.identity(inputs)\n    identity3 = tf.identity(inputs)\n    identity4 = tf.identity(inputs)\n    identity5 = tf.identity(inputs)\n    identity6 = tf.identity(inputs)\n    identity7 = tf.identity(inputs)\n    identity8 = tf.identity(inputs)\n\n    manager = orm.OpRegularizerManager([])\n\n    # Create OpSlice for each identity op.\n    op_slice1 = manager.get_op_slices(identity1.op)[0]\n    op_slice2 = manager.get_op_slices(identity2.op)[0]\n    op_slice3 = manager.get_op_slices(identity3.op)[0]\n    op_slice4 = manager.get_op_slices(identity4.op)[0]\n    op_slice5 = manager.get_op_slices(identity5.op)[0]\n    op_slice6 = manager.get_op_slices(identity6.op)[0]\n    op_slice7 = manager.get_op_slices(identity7.op)[0]\n    op_slice8 = manager.get_op_slices(identity8.op)[0]\n\n    # Create OpGroup for each OpSlice.\n    op_group1 = manager.create_op_group_for_op_slice(op_slice1)\n    op_group2 = manager.create_op_group_for_op_slice(op_slice2)\n    op_group3 = manager.create_op_group_for_op_slice(op_slice3)\n    op_group4 = manager.create_op_group_for_op_slice(op_slice4)\n    op_group5 = manager.create_op_group_for_op_slice(op_slice5)\n    op_group6 = manager.create_op_group_for_op_slice(op_slice6)\n    op_group7 = manager.create_op_group_for_op_slice(op_slice7)\n    op_group8 = manager.create_op_group_for_op_slice(op_slice8)\n\n    # Group all OpGroup together by grouping their OpSlice.\n    manager.group_op_slices([op_slice1, op_slice2, op_slice3, op_slice4])\n    manager.group_op_slices([op_slice5, op_slice6, op_slice7, op_slice8])\n    # Transitively create one large group by grouping one OpSlice from each\n    # group.\n    manager.group_op_slices([op_slice3, op_slice6])\n\n    expected_group = orm.OpGroup(\n        op_groups=[op_group1, op_group2, op_group3, op_group4, op_group5,\n                   op_group6, op_group7, op_group8])\n\n    # Check all OpSlice are in one big group.\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice1).op_slices)\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice2).op_slices)\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice3).op_slices)\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice4).op_slices)\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice5).op_slices)\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice6).op_slices)\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice7).op_slices)\n    self.assertListEqual(\n        expected_group.op_slices,\n        manager.get_op_group(op_slice8).op_slices)\n\n  def testSliceOp_SingleSlice(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity1 = tf.identity(inputs)\n    identity2 = tf.identity(inputs)\n    identity3 = tf.identity(inputs)\n    identity4 = tf.identity(inputs)\n    identity5 = tf.identity(inputs)\n    identity6 = tf.identity(inputs)\n    identity7 = tf.identity(inputs)\n    identity8 = tf.identity(inputs)\n\n    manager = orm.OpRegularizerManager([], self._default_op_handler_dict)\n\n    # Create OpSlice for each identity op.\n    op_slice1 = manager.get_op_slices(identity1.op)[0]\n    op_slice2 = manager.get_op_slices(identity2.op)[0]\n    op_slice3 = manager.get_op_slices(identity3.op)[0]\n    op_slice4 = manager.get_op_slices(identity4.op)[0]\n    op_slice5 = manager.get_op_slices(identity5.op)[0]\n    op_slice6 = manager.get_op_slices(identity6.op)[0]\n    op_slice7 = manager.get_op_slices(identity7.op)[0]\n    op_slice8 = manager.get_op_slices(identity8.op)[0]\n\n    # Create OpGroup for each OpSlice.\n    manager.create_op_group_for_op_slice(op_slice1)\n    manager.create_op_group_for_op_slice(op_slice2)\n    manager.create_op_group_for_op_slice(op_slice3)\n    manager.create_op_group_for_op_slice(op_slice4)\n    manager.create_op_group_for_op_slice(op_slice5)\n    manager.create_op_group_for_op_slice(op_slice6)\n    manager.create_op_group_for_op_slice(op_slice7)\n    manager.create_op_group_for_op_slice(op_slice8)\n\n    # Group all OpGroup together by grouping their OpSlice.\n    manager.group_op_slices([op_slice1, op_slice2, op_slice3, op_slice4])\n    manager.group_op_slices([op_slice5, op_slice6, op_slice7, op_slice8])\n\n    # Only slice identity1 op.  This will also slice identity2, identity3, and\n    # identity4 because the slices are grouped.  The ops identity5, identity6,\n    # identity7, and identity8 are unaffected.\n    manager.slice_op(identity1.op, [1, 2])\n\n    # Verify ops grouped with identity1 are sliced, while other ops are not.\n    self.assertLen(manager.get_op_slices(identity1.op), 2)\n    self.assertLen(manager.get_op_slices(identity2.op), 2)\n    self.assertLen(manager.get_op_slices(identity3.op), 2)\n    self.assertLen(manager.get_op_slices(identity4.op), 2)\n    self.assertLen(manager.get_op_slices(identity5.op), 1)\n    self.assertLen(manager.get_op_slices(identity6.op), 1)\n    self.assertLen(manager.get_op_slices(identity7.op), 1)\n    self.assertLen(manager.get_op_slices(identity8.op), 1)\n\n    # Verify sliced ops have sizes [1, 2].\n    for op in (identity1.op, identity2.op, identity3.op, identity4.op):\n      op_slices = manager.get_op_slices(op)\n      self.assertEqual(0, op_slices[0].slice.start_index)\n      self.assertEqual(1, op_slices[0].slice.size)\n      self.assertEqual(1, op_slices[1].slice.start_index)\n      self.assertEqual(2, op_slices[1].slice.size)\n\n  def testSliceOp_SingleSlice_Ungrouped(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity1 = tf.identity(inputs)\n\n    manager = orm.OpRegularizerManager([], self._default_op_handler_dict)\n\n    # Only slice identity1 op which is ungrouped.\n    manager.slice_op(identity1.op, [1, 2])\n\n    # Verify identity1 op is sliced.\n    self.assertLen(manager.get_op_slices(identity1.op), 2)\n\n    # Verify sliced op has size [1, 2].\n    op_slices = manager.get_op_slices(identity1.op)\n    self.assertEqual(0, op_slices[0].slice.start_index)\n    self.assertEqual(1, op_slices[0].slice.size)\n    self.assertEqual(1, op_slices[1].slice.start_index)\n    self.assertEqual(2, op_slices[1].slice.size)\n\n  def testSliceOp_MultipleSlices(self):\n    inputs = tf.zeros([2, 4, 4, 20])\n    identity1 = tf.identity(inputs)\n    identity2 = tf.identity(inputs)\n    identity3 = tf.identity(inputs)\n\n    manager = orm.OpRegularizerManager([], self._default_op_handler_dict)\n\n    # First op has sizes [4, 3, 7, 6].\n    op_slice1_0_4 = orm.OpSlice(identity1.op, orm.Slice(0, 4))\n    op_slice1_4_7 = orm.OpSlice(identity1.op, orm.Slice(4, 3))\n    op_slice1_7_14 = orm.OpSlice(identity1.op, orm.Slice(7, 7))\n    op_slice1_14_20 = orm.OpSlice(identity1.op, orm.Slice(14, 6))\n\n    # Second op has sizes [3, 7, 10].\n    op_slice2_0_3 = orm.OpSlice(identity2.op, orm.Slice(0, 3))\n    op_slice2_3_10 = orm.OpSlice(identity2.op, orm.Slice(3, 7))\n    op_slice2_10_20 = orm.OpSlice(identity2.op, orm.Slice(10, 10))\n\n    # Third op has sizes [2, 2, 2, 2, 3, 7, 2].\n    op_slice3_0_2 = orm.OpSlice(identity3.op, orm.Slice(0, 2))\n    op_slice3_2_4 = orm.OpSlice(identity3.op, orm.Slice(2, 2))\n    op_slice3_4_6 = orm.OpSlice(identity3.op, orm.Slice(4, 2))\n    op_slice3_6_8 = orm.OpSlice(identity3.op, orm.Slice(6, 2))\n    op_slice3_8_11 = orm.OpSlice(identity3.op, orm.Slice(8, 3))\n    op_slice3_11_18 = orm.OpSlice(identity3.op, orm.Slice(11, 7))\n    op_slice3_18_20 = orm.OpSlice(identity3.op, orm.Slice(18, 2))\n\n    manager._op_slice_dict = {\n        identity1.op: [op_slice1_0_4, op_slice1_4_7, op_slice1_7_14,\n                       op_slice1_14_20],\n        identity2.op: [op_slice2_0_3, op_slice2_3_10, op_slice2_10_20],\n        identity3.op: [op_slice3_0_2, op_slice3_2_4, op_slice3_4_6,\n                       op_slice3_6_8, op_slice3_8_11, op_slice3_11_18,\n                       op_slice3_18_20],\n    }\n\n    # Only the [3, 7] slices of the ops are grouped.  Only the first op is a\n    # source.\n    manager.group_op_slices(\n        [op_slice1_4_7, op_slice2_0_3, op_slice3_8_11],\n        omit_source_op_slices=[op_slice2_0_3, op_slice3_8_11])\n    manager.group_op_slices(\n        [op_slice1_7_14, op_slice2_3_10, op_slice3_11_18],\n        omit_source_op_slices=[op_slice2_3_10, op_slice3_11_18])\n\n    # Slice the [3, 7] grouped slices into [1, 2, 3, 4].\n    manager.slice_op(identity1.op, [4, 1, 2, 3, 4, 6])\n\n    # Verify grouped ops are sliced into the correct sizes.\n    op_slices1 = manager.get_op_slices(identity1.op)\n    op_slices2 = manager.get_op_slices(identity2.op)\n    op_slices3 = manager.get_op_slices(identity3.op)\n\n    expected_sizes1 = [4, 1, 2, 3, 4, 6]\n    expected_sizes2 = [1, 2, 3, 4, 10]\n    expected_sizes3 = [2, 2, 2, 2, 1, 2, 3, 4, 2]\n\n    self.assertListEqual(\n        expected_sizes1, [s.slice.size for s in op_slices1])\n    self.assertListEqual(\n        expected_sizes2, [s.slice.size for s in op_slices2])\n    self.assertListEqual(\n        expected_sizes3, [s.slice.size for s in op_slices3])\n\n    # Verify new slices are grouped.\n    op_slice1_4_5 = orm.OpSlice(identity1.op, orm.Slice(4, 1))\n    op_slice1_5_7 = orm.OpSlice(identity1.op, orm.Slice(5, 2))\n    op_slice1_7_10 = orm.OpSlice(identity1.op, orm.Slice(7, 3))\n    op_slice1_10_14 = orm.OpSlice(identity1.op, orm.Slice(10, 4))\n\n    op_slice2_0_1 = orm.OpSlice(identity2.op, orm.Slice(0, 1))\n    op_slice2_1_3 = orm.OpSlice(identity2.op, orm.Slice(1, 2))\n    op_slice2_3_6 = orm.OpSlice(identity2.op, orm.Slice(3, 3))\n    op_slice2_6_10 = orm.OpSlice(identity2.op, orm.Slice(6, 4))\n\n    op_slice3_8_9 = orm.OpSlice(identity3.op, orm.Slice(8, 1))\n    op_slice3_9_11 = orm.OpSlice(identity3.op, orm.Slice(9, 2))\n    op_slice3_11_14 = orm.OpSlice(identity3.op, orm.Slice(11, 3))\n    op_slice3_14_18 = orm.OpSlice(identity3.op, orm.Slice(14, 4))\n\n    expected_group1 = [op_slice1_4_5, op_slice2_0_1, op_slice3_8_9]\n    expected_group2 = [op_slice1_5_7, op_slice2_1_3, op_slice3_9_11]\n    expected_group3 = [op_slice1_7_10, op_slice2_3_6, op_slice3_11_14]\n    expected_group4 = [op_slice1_10_14, op_slice2_6_10, op_slice3_14_18]\n\n    self.assertListEqual(\n        expected_group1, manager.get_op_group(op_slice1_4_5).op_slices)\n    self.assertListEqual(\n        expected_group1, manager.get_op_group(op_slice2_0_1).op_slices)\n    self.assertListEqual(\n        expected_group1, manager.get_op_group(op_slice3_8_9).op_slices)\n\n    self.assertListEqual(\n        expected_group2, manager.get_op_group(op_slice1_5_7).op_slices)\n    self.assertListEqual(\n        expected_group2, manager.get_op_group(op_slice2_1_3).op_slices)\n    self.assertListEqual(\n        expected_group2, manager.get_op_group(op_slice3_9_11).op_slices)\n\n    self.assertListEqual(\n        expected_group3, manager.get_op_group(op_slice1_7_10).op_slices)\n    self.assertListEqual(\n        expected_group3, manager.get_op_group(op_slice2_3_6).op_slices)\n    self.assertListEqual(\n        expected_group3, manager.get_op_group(op_slice3_11_14).op_slices)\n\n    self.assertListEqual(\n        expected_group4, manager.get_op_group(op_slice1_10_14).op_slices)\n    self.assertListEqual(\n        expected_group4, manager.get_op_group(op_slice2_6_10).op_slices)\n    self.assertListEqual(\n        expected_group4, manager.get_op_group(op_slice3_14_18).op_slices)\n\n  def testProcessOps(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    batch_norm = layers.batch_norm(inputs)\n    identity1 = tf.identity(batch_norm)\n    identity2 = tf.identity(batch_norm)\n\n    manager = orm.OpRegularizerManager(\n        [identity1.op, identity2.op],\n        op_handler_dict=self._default_op_handler_dict)\n    manager.process_ops([identity1.op, identity2.op, batch_norm.op])\n\n    self.assertLen(manager._op_deque, 3)\n    self.assertEqual(batch_norm.op, manager._op_deque.pop())\n    self.assertEqual(identity2.op, manager._op_deque.pop())\n    self.assertEqual(identity1.op, manager._op_deque.pop())\n\n  def testProcessOps_DuplicatesRemoved(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    batch_norm = layers.batch_norm(inputs)\n    identity1 = tf.identity(batch_norm)\n    identity2 = tf.identity(batch_norm)\n\n    manager = orm.OpRegularizerManager(\n        [identity1.op, identity2.op],\n        op_handler_dict=self._default_op_handler_dict)\n    manager.process_ops([identity1.op, identity2.op, batch_norm.op])\n    # Try to process the same ops again.\n    manager.process_ops([identity1.op, identity2.op, batch_norm.op])\n\n    self.assertLen(manager._op_deque, 3)\n    self.assertEqual(batch_norm.op, manager._op_deque.pop())\n    self.assertEqual(identity2.op, manager._op_deque.pop())\n    self.assertEqual(identity1.op, manager._op_deque.pop())\n\n  def testProcessOpsLast(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    batch_norm = layers.batch_norm(inputs)\n    identity1 = tf.identity(batch_norm)\n    identity2 = tf.identity(batch_norm)\n\n    manager = orm.OpRegularizerManager(\n        [identity1.op, identity2.op],\n        op_handler_dict=self._default_op_handler_dict)\n    manager.process_ops([identity1.op])\n    manager.process_ops_last([identity2.op, batch_norm.op])\n\n    self.assertLen(manager._op_deque, 3)\n    self.assertEqual(identity1.op, manager._op_deque.pop())\n    self.assertEqual(identity2.op, manager._op_deque.pop())\n    self.assertEqual(batch_norm.op, manager._op_deque.pop())\n\n  def testProcessOpsLast_DuplicatesRemoved(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    batch_norm = layers.batch_norm(inputs)\n    identity1 = tf.identity(batch_norm)\n    identity2 = tf.identity(batch_norm)\n\n    manager = orm.OpRegularizerManager(\n        [identity1.op, identity2.op],\n        op_handler_dict=self._default_op_handler_dict)\n    manager.process_ops([identity1.op])\n    manager.process_ops_last([identity2.op, batch_norm.op])\n    # Try to process the same ops again.\n    manager.process_ops_last([identity2.op, batch_norm.op])\n\n    self.assertLen(manager._op_deque, 3)\n    self.assertEqual(identity1.op, manager._op_deque.pop())\n    self.assertEqual(identity2.op, manager._op_deque.pop())\n    self.assertEqual(batch_norm.op, manager._op_deque.pop())\n\n  def testIsSourceOp(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity = tf.identity(inputs)\n    batch_norm = layers.batch_norm(identity)\n\n    manager = orm.OpRegularizerManager([], self._default_op_handler_dict)\n\n    self.assertFalse(manager.is_source_op(identity.op))\n    self.assertTrue(manager.is_source_op(batch_norm.op))\n\n  def testIsPassthrough(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity = tf.identity(inputs)\n    layers.conv2d(identity, 5, 3, scope=\'conv1\')\n\n    manager = orm.OpRegularizerManager([], self._default_op_handler_dict)\n\n    self.assertTrue(manager.is_passthrough(identity.op))\n    # TODO(a1): Verify OutputNonPassthrough OpHandler returns False.\n\n  def testGetOpSlices(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity = tf.identity(inputs)\n\n    # Create OpRegularizerManager with OpSlice mapping.\n    manager = orm.OpRegularizerManager([])\n    op_slice = orm.OpSlice(identity.op, orm.Slice(0, 3))\n    manager._op_slice_dict[identity.op] = [op_slice]\n\n    op_slices = manager.get_op_slices(identity.op)\n\n    self.assertLen(op_slices, 1)\n    self.assertEqual(op_slice, op_slices[0])\n\n  def testGetOpSlices_CreateNew(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity = tf.identity(inputs)\n\n    # Create OpRegularizerManager with empty OpSlice dictionary.\n    manager = orm.OpRegularizerManager([])\n    manager._op_slice_dict = {}\n\n    op_slices = manager.get_op_slices(identity.op)\n\n    # Verify OpSlice is created correctly.\n    self.assertLen(op_slices, 1)\n    op_slice = op_slices[0]\n    self.assertEqual(identity.op, op_slice.op)\n    self.assertEqual(0, op_slice.slice.start_index)\n    self.assertEqual(3, op_slice.slice.size)\n\n  def testGetOpSlices_CreateNew_MultipleOutputs(self):\n    inputs = tf.zeros([2, 4, 4, 10])\n    split = tf.split(inputs, [3, 7], axis=3)\n    split_op = split[0].op\n\n    # Create OpRegularizerManager with empty OpSlice dictionary.\n    manager = orm.OpRegularizerManager([])\n    manager._op_slice_dict = {}\n\n    op_slices = manager.get_op_slices(split_op)\n\n    # Verify OpSlice is created correctly.\n    self.assertLen(op_slices, 1)\n    op_slice = op_slices[0]\n    self.assertEqual(split_op, op_slice.op)\n    self.assertEqual(0, op_slice.slice.start_index)\n    self.assertEqual(10, op_slice.slice.size)\n\n  def testGetOpSlices_ZeroSize(self):\n    constant = tf.constant(123)\n\n    # Create OpRegularizerManager with empty OpSlice dictionary.\n    manager = orm.OpRegularizerManager([])\n    manager._op_slice_dict = {}\n\n    op_slices = manager.get_op_slices(constant.op)\n\n    # Verify zero-size op has no slices.\n    self.assertListEqual([], op_slices)\n\n  def testSliceOpSlice(self):\n    inputs = tf.zeros([2, 4, 4, 10])\n    identity = tf.identity(inputs)\n\n    op_slice1 = orm.OpSlice(identity.op, orm.Slice(0, 2))\n    op_slice2 = orm.OpSlice(identity.op, orm.Slice(2, 6))\n    op_slice3 = orm.OpSlice(identity.op, orm.Slice(8, 2))\n\n    manager = orm.OpRegularizerManager([])\n    manager._op_slice_dict[identity.op] = [op_slice1, op_slice2, op_slice3]\n\n    # Original op has slice sizes [2, 6, 2].  The middle op is being sliced into\n    # [1, 3, 2], so the new slice sizes are [2, 1, 3, 2, 2].\n    sizes = [2, 1, 3, 2, 2]\n    size_index = 1\n    size_count = 3\n    new_op_slice_group = [list() for _ in range(size_count)]\n    manager._slice_op_slice(op_slice2, sizes, size_index, size_count,\n                            new_op_slice_group)\n\n    # Verify new slices are created.\n    self.assertLen(new_op_slice_group, size_count)\n    for i in range(size_count):\n      self.assertLen(new_op_slice_group[i], 1)\n\n    # Verify new slices are correct.\n    new_slice1 = new_op_slice_group[0][0]\n    self.assertEqual(2, new_slice1.slice.start_index)\n    self.assertEqual(1, new_slice1.slice.size)\n\n    new_slice2 = new_op_slice_group[1][0]\n    self.assertEqual(3, new_slice2.slice.start_index)\n    self.assertEqual(3, new_slice2.slice.size)\n\n    new_slice3 = new_op_slice_group[2][0]\n    self.assertEqual(6, new_slice3.slice.start_index)\n    self.assertEqual(2, new_slice3.slice.size)\n\n  def testSliceOpWithSizes(self):\n    inputs = tf.zeros([2, 4, 4, 10])\n    identity = tf.identity(inputs)\n\n    manager = orm.OpRegularizerManager([])\n\n    sizes = [1, 2, 3, 4]\n    is_source = [True, False, True, False]\n    is_resliced = [True, True, True, True]\n    op_slices = manager._slice_op_with_sizes(identity.op, sizes, is_source,\n                                             is_resliced)\n\n    # Verify OpSlice count and whether they are sources.\n    self.assertLen(op_slices, 4)\n\n    slice1 = op_slices[0]\n    op_group1 = manager.get_op_group(slice1)\n    self.assertIn(slice1, op_group1.source_op_slices)\n\n    slice2 = op_slices[1]\n    op_group2 = manager.get_op_group(slice2)\n    self.assertIsNone(op_group2)\n\n    slice3 = op_slices[2]\n    op_group3 = manager.get_op_group(slice3)\n    self.assertIn(slice3, op_group3.source_op_slices)\n\n    slice4 = op_slices[3]\n    op_group4 = manager.get_op_group(slice4)\n    self.assertIsNone(op_group4)\n\n  def testGetSourceSlices(self):\n    inputs = tf.zeros([2, 4, 4, 10])\n    identity = tf.identity(inputs)\n\n    manager = orm.OpRegularizerManager([])\n\n    # Create OpSlices with size [3, 7].\n    identity_slice1 = orm.OpSlice(identity.op, orm.Slice(0, 3))\n    identity_slice2 = orm.OpSlice(identity.op, orm.Slice(3, 7))\n\n    # Create OpGroup where only first group has source OpSlice.\n    manager.create_op_group_for_op_slice(identity_slice1)\n    manager.create_op_group_for_op_slice(identity_slice2,\n                                         is_source=False)\n\n    # First slice of size 3 is sliced into [1, 2], so these are sources.  Second\n    # slice of size 7 is sliced into [3, 4], which are not sources.\n    sizes = [1, 2, 3, 4]\n    expected_sources = [True, True, False, False]\n    self.assertListEqual(\n        expected_sources,\n        manager._get_source_slices(sizes, [identity_slice1, identity_slice2]))\n\n  def testDfsForSourceOps(self):\n    with arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv1\')\n      c2 = layers.conv2d(inputs, num_outputs=10, kernel_size=3, scope=\'conv2\')\n      tmp = c1 + c2\n      c3 = layers.conv2d(tmp, num_outputs=10, kernel_size=3, scope=\'conv3\')\n      out = tf.identity(c3)\n      # Extra branch that is not a dependency of out.\n      concat = tf.concat([c1, c2], axis=3)\n      layers.conv2d(concat, num_outputs=10, kernel_size=3, scope=\'conv4\')\n\n    manager = orm.OpRegularizerManager([], self._default_op_handler_dict)\n    manager._dfs_for_source_ops([out.op])\n\n    # Verify source ops were found.\n    expected_queue = collections.deque([\n        _get_op(\'conv3/BatchNorm/FusedBatchNormV3\'),\n        _get_op(\'conv2/BatchNorm/FusedBatchNormV3\'),\n        _get_op(\'conv1/BatchNorm/FusedBatchNormV3\')\n    ])\n    self.assertEqual(expected_queue, manager._op_deque)\n\n    # Verify extra branch was not included.\n    self.assertNotIn(\n        _get_op(\'conv4/BatchNorm/FusedBatchNormV3\'), manager._op_deque)\n\n  def testOpGroup_NewSourceGroup(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity = tf.identity(inputs)\n    op_slice = orm.OpSlice(identity.op, None)\n    op_group = orm.OpGroup(op_slice)\n\n    self.assertListEqual([op_slice], op_group.op_slices)\n    self.assertListEqual([op_slice], op_group.source_op_slices)\n\n  def testOpGroup_NewGroupNoSource(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity = tf.identity(inputs)\n    op_slice = orm.OpSlice(identity.op, None)\n    op_group = orm.OpGroup(op_slice, omit_source_op_slices=[op_slice])\n\n    self.assertListEqual([op_slice], op_group.op_slices)\n    self.assertListEqual([], op_group.source_op_slices)\n\n  def testOpGroup_NewSourceGroup_DuplicateOpSlice(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity1 = tf.identity(inputs)\n    identity2 = tf.identity(inputs)\n    op_slice1 = orm.OpSlice(identity1.op, None)\n    op_slice2 = orm.OpSlice(identity2.op, None)\n    op_group1 = orm.OpGroup(op_slice1)\n    op_group2 = orm.OpGroup(\n        op_slice2, [op_group1], omit_source_op_slices=[op_slice2])\n    op_group3 = orm.OpGroup(op_groups=[op_group1, op_group2])\n\n    self.assertListEqual([op_slice1, op_slice2], op_group3.op_slices)\n    self.assertListEqual([op_slice1], op_group3.source_op_slices)\n\n  def testOpGroup_MergeGroups(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity1 = tf.identity(inputs)\n    identity2 = tf.identity(inputs)\n    identity3 = tf.identity(inputs)\n    identity4 = tf.identity(inputs)\n    identity5 = tf.identity(inputs)\n    identity6 = tf.identity(inputs)\n    identity7 = tf.identity(inputs)\n    identity8 = tf.identity(inputs)\n\n    # Reset OpGroup counter.\n    orm.OpGroup._static_index = 0\n\n    # Create OpGroup where only identity3, identity6, and identity7 are sources.\n    op_slice1 = orm.OpSlice(identity1.op, None)\n    op_group1 = orm.OpGroup(op_slice1, omit_source_op_slices=[op_slice1])\n    op_slice2 = orm.OpSlice(identity2.op, None)\n    op_group2 = orm.OpGroup(op_slice2, omit_source_op_slices=[op_slice2])\n    op_slice3 = orm.OpSlice(identity3.op, None)\n    op_group3 = orm.OpGroup(op_slice3)\n    op_slice4 = orm.OpSlice(identity4.op, None)\n    op_group4 = orm.OpGroup(op_slice4, omit_source_op_slices=[op_slice4])\n    op_slice5 = orm.OpSlice(identity5.op, None)\n    op_group5 = orm.OpGroup(op_slice5, omit_source_op_slices=[op_slice5])\n    op_slice6 = orm.OpSlice(identity6.op, None)\n    op_group6 = orm.OpGroup(op_slice6)\n    op_slice7 = orm.OpSlice(identity7.op, None)\n    op_group7 = orm.OpGroup(op_slice7)\n    op_slice8 = orm.OpSlice(identity8.op, None)\n    op_group8 = orm.OpGroup(op_slice8, omit_source_op_slices=[op_slice8])\n\n    # Merge group1 and group2 into group9.\n    op_group9 = orm.OpGroup(op_groups=[op_group1, op_group2])\n    self.assertListEqual([op_slice1, op_slice2], op_group9.op_slices)\n    self.assertListEqual([], op_group9.source_op_slices)\n    self.assertEqual(8, op_group9._index)  # OpGroup is zero-indexed.\n\n    # Merge group3 and group4 into group10.\n    op_group10 = orm.OpGroup(op_groups=[op_group3, op_group4])\n    self.assertListEqual([op_slice3, op_slice4], op_group10.op_slices)\n    self.assertListEqual([op_slice3], op_group10.source_op_slices)\n    self.assertEqual(9, op_group10._index)  # OpGroup is zero-indexed.\n\n    # Merge group5, group6, group7, and group8 into group 11.\n    op_group11 = orm.OpGroup(\n        op_groups=[op_group5, op_group6, op_group7, op_group8])\n    self.assertListEqual(\n        [op_slice5, op_slice6, op_slice7, op_slice8], op_group11.op_slices)\n    self.assertListEqual([op_slice6, op_slice7], op_group11.source_op_slices)\n    self.assertEqual(10, op_group11._index)  # OpGroup is zero-indexed.\n\n    # Merge group9 and group10 into group12.\n    op_group12 = orm.OpGroup(op_groups=[op_group9, op_group10])\n    self.assertListEqual(\n        [op_slice1, op_slice2, op_slice3, op_slice4], op_group12.op_slices)\n    self.assertListEqual([op_slice3], op_group12.source_op_slices)\n    self.assertEqual(11, op_group12._index)  # OpGroup is zero-indexed.\n\n    # Merge group11 and group12 into group13.\n    op_group13 = orm.OpGroup(op_groups=[op_group11, op_group12])\n    self.assertListEqual(\n        [op_slice5, op_slice6, op_slice7, op_slice8, op_slice1, op_slice2,\n         op_slice3, op_slice4],\n        op_group13.op_slices)\n    self.assertListEqual([op_slice6, op_slice7, op_slice3],\n                         op_group13.source_op_slices)\n    self.assertEqual(12, op_group13._index)  # OpGroup is zero-indexed.\n\n  def testCorrectSourceOpsWithSkipConnection(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    x0 = layers.conv2d(\n        inputs, num_outputs=8, kernel_size=3, activation_fn=None, scope=\'conv0\')\n    x1 = tf.nn.relu(layers.batch_norm(x0, scale=True, scope=\'bn0\'))\n    x1 = layers.conv2d(\n        x1, num_outputs=8, kernel_size=3, activation_fn=None, scope=\'conv1\')\n    x2 = tf.add_n([x0, x1], name=\'add\')\n    final_op = tf.nn.relu(layers.batch_norm(x2, scale=True, scope=\'bn1\'))\n\n    op_handler_dict = self._default_op_handler_dict\n    op_reg_manager = orm.OpRegularizerManager([final_op.op], op_handler_dict)\n\n    # All ops are in the same group\n    group = list(op_reg_manager._op_group_dict.values())[0]\n    source_op_names = [s.op.name for s in group.source_op_slices]\n    self.assertSetEqual(set([\'bn0/FusedBatchNormV3\', \'bn1/FusedBatchNormV3\']),\n                        set(source_op_names))\n\n  def testPrintOpSlices(self):\n    inputs = tf.zeros([2, 4, 4, 3])\n    identity1 = tf.identity(inputs)\n    identity2 = tf.identity(inputs)\n\n    manager = orm.OpRegularizerManager(\n        [identity1.op, identity2.op],\n        op_handler_dict=self._default_op_handler_dict)\n    op_slices1 = manager.get_op_slices(identity1.op)\n    op_slices2 = manager.get_op_slices(identity2.op)\n    all_slices = op_slices1 + op_slices2\n\n    self.assertEqual(\'[Identity (0, 3), Identity_1 (0, 3)]\',\n                     str(all_slices))\n\n\nclass IndexOpRegularizer(generic_regularizers.OpRegularizer):\n  """"""A test OpRegularizer with a self-incrementing index.\n\n  This class creates a regularizer where the regularization vector contains\n  self-incrementing values (e.g. [0, 1, 2, ...]).  The index continues to\n  increment as regularizers are created.  This is convenient for testing in\n  order to track individual elements of the regularization vector (e.g. gather).\n\n  For example, creating 2 regularizers of size 3 results in\n  r1 = [0, 1, 2] and r2 = [3, 4, 5].\n  """"""\n\n  index = 0\n\n  def __init__(self, op_slice, op_reg_manager):\n    size = op_slice.slice.size\n    self._alive_vector = tf.cast(tf.ones(size), tf.bool)\n    self._regularization_vector = tf.constant(\n        list(range(IndexOpRegularizer.index, IndexOpRegularizer.index + size)),\n        tf.float32)\n    IndexOpRegularizer.index += size\n\n  @classmethod\n  def reset_index(cls):\n    IndexOpRegularizer.index = 0\n\n  @property\n  def regularization_vector(self):\n    return self._regularization_vector\n\n  @property\n  def alive_vector(self):\n    return self._alive_vector\n\n\nclass SumGroupingRegularizer(generic_regularizers.OpRegularizer):\n  """"""A regularizer that groups others by summing their regularization values.""""""\n\n  def __init__(self, regularizers_to_group):\n    """"""Creates an instance.\n\n    Args:\n      regularizers_to_group: A list of generic_regularizers.OpRegularizer\n        objects.Their regularization_vector (alive_vector) are expected to be of\n        the same length.\n\n    Raises:\n      ValueError: regularizers_to_group is not of length at least 2.\n    """"""\n    if len(regularizers_to_group) < 2:\n      raise ValueError(\'Groups must be of at least size 2.\')\n    self._regularization_vector = tf.add_n(\n        [r.regularization_vector for r in regularizers_to_group])\n    self._alive_vector = tf.cast(\n        tf.ones(self._regularization_vector.get_shape()[-1]), tf.bool)\n\n  @property\n  def regularization_vector(self):\n    return self._regularization_vector\n\n  @property\n  def alive_vector(self):\n    return self._alive_vector\n\n\nclass IndexBatchNormSourceOpHandler(\n    batch_norm_source_op_handler.BatchNormSourceOpHandler):\n  """"""An OpHandler that creates OpRegularizer using IndexOpRegularizer.\n\n  A wrapper around BatchNormSourceOpHandler that overrides the\n  create_regularizer method to use IndexOpRegularizer for testing.\n  """"""\n\n  def __init__(self):\n    super(IndexBatchNormSourceOpHandler, self).__init__(0.0)\n\n  def create_regularizer(self, op_slice):\n    return IndexOpRegularizer(op_slice, None)\n\n\nclass StubBatchNormSourceOpHandler(\n    batch_norm_source_op_handler.BatchNormSourceOpHandler):\n  """"""An OpHandler that creates OpRegularizer using stub values.\n\n  A wrapper around BatchNormSourceOpHandler that overrides the\n  create_regularizer method to use stub values for testing.\n  """"""\n\n  def __init__(self, model_stub):\n    super(StubBatchNormSourceOpHandler, self).__init__(0.0)\n    self._model_stub = model_stub\n\n  def create_regularizer(self, op_slice):\n    return _stub_create_regularizer(op_slice, self._model_stub)\n\n\nclass IndexConvSourceOpHandler(\n    conv_source_op_handler.ConvSourceOpHandler):\n  """"""An OpHandler that creates OpRegularizer using IndexOpRegularizer.\n\n  A wrapper around ConvSourceOpHandler that overrides the create_regularizer\n  method to use IndexOpRegularizer for testing.\n  """"""\n\n  def __init__(self):\n    pass\n\n  def create_regularizer(self, op_slice):\n    return IndexOpRegularizer(op_slice, None)\n\n\nclass StubConvSourceOpHandler(conv_source_op_handler.ConvSourceOpHandler):\n  """"""An OpHandler that creates OpRegularizer using stub values.\n\n  A wrapper around ConvSourceOpHandler that overrides the create_regularizer\n  method to use stub values for testing.\n  """"""\n\n  def __init__(self, model_stub):\n    super(StubConvSourceOpHandler, self).__init__(0.1)\n    self._model_stub = model_stub\n\n  def create_regularizer(self, op_slice):\n    return _stub_create_regularizer(op_slice, self._model_stub)\n\n\nclass RandomConvSourceOpHandler(\n    conv_source_op_handler.ConvSourceOpHandler):\n  """"""An OpHandler that creates OpRegularizer using random values.\n\n  A wrapper around ConvSourceOpHandler that overrides the create_regularizer\n  method to use random values for testing.\n  """"""\n\n  def create_regularizer(self, op_slice):\n    regularization_vector = np.random.random(op_slice.slice.size)\n    return StubOpRegularizer(regularization_vector,\n                             regularization_vector > self._threshold)\n\n\ndef _stub_create_regularizer(op_slice, model_stub):\n  """"""Create a StubOpRegularizer for a given OpSlice.\n\n  Args:\n    op_slice: A op_regularizer_manager.OpSlice.\n    model_stub: Module name where REG_STUB and ALIVE_STUB will be found.\n\n  Returns:\n    StubOpRegularizer with stubbed regularization and alive vectors.\n  """"""\n  op = op_slice.op\n  start_index = op_slice.slice.start_index\n  size = op_slice.slice.size\n  for key in model_stub.REG_STUB:\n    if op.name.startswith(key):\n      return StubOpRegularizer(\n          model_stub.REG_STUB[key][start_index:start_index + size],\n          model_stub.ALIVE_STUB[key][start_index:start_index + size])\n  raise ValueError(\'No regularizer for %s\' % op.name)\n\n\nclass StubOpRegularizer(generic_regularizers.OpRegularizer):\n  """"""A test OpRegularizer with configured regularization vectors.\n\n  Regularization values are stored in a dict and keyed on op name prefix.\n  """"""\n\n  def __init__(self, regularization_vector, alive_vector):\n    self._regularization_vector = tf.constant(regularization_vector)\n    self._alive_vector = tf.constant(alive_vector, dtype=tf.bool)\n\n  @property\n  def regularization_vector(self):\n    return self._regularization_vector\n\n  @property\n  def alive_vector(self):\n    return self._alive_vector\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/output_non_passthrough_op_handler.py,3,"b'""""""OpHandler for OutputNonPassthrough ops.\n\nOutputNonPassthrough ops take their regularizer from the output and do not\npassthrough the regularizer to their input. This is the default OpHandler for\nops like Conv2D and MatMul when L1-gamma regularization is used.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import op_handler\nfrom morph_net.framework import op_handler_util\n\n\nclass OutputNonPassthroughOpHandler(op_handler.OpHandler):\n  """"""OpHandler implementation for OutputNonPassthrough operations.\n\n   These ops take their regularizer from the output and do not\n   passthrough the regularizer to their input.\n  """"""\n\n  @property\n  def is_source_op(self):\n    return False\n\n  @property\n  def is_passthrough(self):\n    return False\n\n  def assign_grouping(self, op, op_reg_manager):\n    """"""Assign grouping to the given op and updates the manager.\n\n    Args:\n      op: tf.Operation to assign grouping to.\n      op_reg_manager: OpRegularizerManager to keep track of the grouping.\n    """"""\n    # Check if all input ops have groups, or tell the manager to process them.\n    input_ops = op_handler_util.get_input_ops(op, op_reg_manager)\n    input_ops_without_group = op_handler_util.get_ops_without_groups(\n        input_ops, op_reg_manager)\n\n    # Check if all output ops have groups, or tell the manager to process them.\n    output_ops = op_handler_util.get_output_ops(op, op_reg_manager)\n    output_ops_without_group = op_handler_util.get_ops_without_groups(\n        output_ops, op_reg_manager)\n\n    # Remove non-passthrough ops from outputs ops to group with.\n    output_ops = op_handler_util.remove_non_passthrough_ops(\n        output_ops, op_reg_manager)\n\n    # Only group with ops that have the same size.  Process the ops that have\n    # mismatched size.\n    output_ops_to_group, output_ops_to_process = (\n        op_handler_util.separate_same_size_ops(op, output_ops))\n\n    # Also process ungrouped ops.\n    input_ops_to_process = input_ops_without_group\n    output_ops_to_process.extend(output_ops_without_group)\n\n    # Align op slice sizes if needed.\n    op_slices = op_reg_manager.get_op_slices(op)\n    output_op_slices = op_handler_util.get_op_slices(\n        output_ops_to_group, op_reg_manager)\n    aligned_op_slice_sizes = op_handler_util.get_aligned_op_slice_sizes(\n        op_slices, [], output_op_slices)\n    op_handler_util.reslice_ops([op] + output_ops_to_group,\n                                aligned_op_slice_sizes, op_reg_manager)\n\n    # TODO(a1): Consider refactoring this method.\n    # Repopulate OpSlice data, as ops may have been resliced.\n    output_op_slices = self._get_output_op_slices(\n        output_ops_to_group, op_reg_manager)\n\n    # Group with inputs and outputs.\n    op_handler_util.group_op_with_inputs_and_outputs(\n        op, [], output_op_slices, aligned_op_slice_sizes,\n        op_reg_manager)\n\n    # Reprocess ops.\n    op_reg_manager.process_ops(output_ops_to_process + input_ops_to_process)\n\n  def _group_with_output_slices(\n      self, op, output_op_slices, op_slices, op_reg_manager):\n    """"""Groups OpSlice of current op with output ops.\n\n    Assuming OpSlice of op have been aligned with output, groups the\n    corresponding OpSlice.\n\n    Args:\n      op: tf.Operation to determine grouping for.\n      output_op_slices: List of list of OpSlice, with a list per output op.\n      op_slices: List of OpSlice for current op.\n      op_reg_manager: OpRegularizerManager to keep track of grouping.\n\n    Raises:\n      ValueError: If sizes for current and output op slices are not the same.\n    """"""\n    # Assert that op slices for output and current op are aligned.\n    output_op_slices_sizes = op_handler_util.get_op_slice_sizes(\n        output_op_slices)\n    op_slice_sizes = op_handler_util.get_op_slice_sizes([op_slices])\n\n    if op_slice_sizes != output_op_slices_sizes:\n      raise ValueError(\'Current op and output op have differing slice \'\n                       \'sizes: {}, {}\'.format(\n                           op_slice_sizes, output_op_slices_sizes))\n\n    op_handler_util.group_op_with_inputs_and_outputs(\n        op, [], output_op_slices, op_slice_sizes, op_reg_manager)\n\n  def _get_output_op_slices(self, output_ops, op_reg_manager):\n    """"""Returns op slices for outputs.\n\n    Args:\n      output_ops: List of tf.Operation.\n      op_reg_manager: OpRegularizerManager to keep track of the grouping.\n\n    Returns:\n      A list of list of OpSlice with a list per output op.\n    """"""\n    return op_handler_util.get_op_slices(output_ops, op_reg_manager)\n\n  def create_regularizer(self, _):\n    raise NotImplementedError(\'Not a source op.\')\n'"
morph_net/framework/output_non_passthrough_op_handler_test.py,4,"b'""""""Tests for output_non_passthrough_op_handler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport mock\nfrom morph_net.framework import op_regularizer_manager as orm\nfrom morph_net.framework import output_non_passthrough_op_handler\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import framework\nfrom tensorflow.contrib import layers\n\n\nclass OutputNonPassthroughOpHandlerTest(tf.test.TestCase):\n\n  def _batch_norm_scope(self):\n    params = {\n        \'trainable\': True,\n        \'normalizer_fn\': layers.batch_norm,\n        \'normalizer_params\': {\n            \'scale\': True,\n        },\n    }\n\n    with framework.arg_scope([layers.conv2d], **params) as sc:\n      return sc\n\n  def setUp(self):\n    super(OutputNonPassthroughOpHandlerTest, self).setUp()\n\n    # This tests 2 Conv2D ops with batch norm at the top.\n    with framework.arg_scope(self._batch_norm_scope()):\n      inputs = tf.zeros([2, 4, 4, 3])\n      c1 = layers.conv2d(inputs, num_outputs=5,\n                         kernel_size=3, scope=\'conv1\', normalizer_fn=None)\n      layers.conv2d(c1, num_outputs=6, kernel_size=3, scope=\'conv2\')\n\n    g = tf.get_default_graph()\n\n    # Declare OpSlice and OpGroup for ops of interest.\n    self.conv1_op = g.get_operation_by_name(\'conv1/Conv2D\')\n    self.conv1_op_slice = orm.OpSlice(self.conv1_op, orm.Slice(0, 5))\n    self.conv1_op_group = orm.OpGroup(\n        self.conv1_op_slice, omit_source_op_slices=[self.conv1_op_slice])\n\n    self.relu1_op = g.get_operation_by_name(\'conv1/Relu\')\n    self.relu1_op_slice = orm.OpSlice(self.relu1_op, orm.Slice(0, 5))\n    self.relu1_op_group = orm.OpGroup(\n        self.relu1_op_slice, omit_source_op_slices=[self.relu1_op_slice])\n\n    self.conv2_op = g.get_operation_by_name(\'conv2/Conv2D\')\n    self.conv2_op_slice = orm.OpSlice(self.conv2_op, orm.Slice(0, 6))\n    self.conv2_op_group = orm.OpGroup(\n        self.conv2_op_slice, omit_source_op_slices=[self.conv2_op_slice])\n\n    self.relu2_op = g.get_operation_by_name(\'conv2/Relu\')\n    self.relu2_op_slice = orm.OpSlice(self.relu2_op, orm.Slice(0, 6))\n    self.relu2_op_group = orm.OpGroup(\n        self.relu2_op_slice, omit_source_op_slices=[self.relu2_op_slice])\n\n    self.batch_norm_op = g.get_operation_by_name(\n        \'conv2/BatchNorm/FusedBatchNormV3\')\n    self.batch_norm_op_slice = orm.OpSlice(self.batch_norm_op, orm.Slice(0, 6))\n    self.batch_norm_op_group = orm.OpGroup(self.batch_norm_op_slice)\n\n    # Create mock OpRegularizerManager with custom mapping of OpSlice and\n    # OpGroup.\n    self.mock_op_reg_manager = mock.create_autospec(orm.OpRegularizerManager)\n\n    def get_op_slices(op):\n      return self.op_slice_dict.get(op, [])\n\n    def get_op_group(op_slice):\n      return self.op_group_dict.get(op_slice)\n\n    def is_passthrough(op):\n      if op in [self.conv1_op, self.conv2_op]:\n        h = output_non_passthrough_op_handler.OutputNonPassthroughOpHandler()\n        return h.is_passthrough\n      if op == self.batch_norm_op:\n        return True\n      else:\n        return False\n\n    self.mock_op_reg_manager.get_op_slices.side_effect = get_op_slices\n    self.mock_op_reg_manager.get_op_group.side_effect = get_op_group\n    self.mock_op_reg_manager.is_source_op.return_value = False\n    self.mock_op_reg_manager.is_passthrough.side_effect = is_passthrough\n    self.mock_op_reg_manager.ops = [\n        self.conv1_op, self.relu1_op, self.conv2_op, self.relu2_op,\n        self.batch_norm_op]\n\n  def testAssignGrouping_GroupWithOutputOnly(self):\n    # Map ops to slices.\n    self.op_slice_dict = {\n        self.conv1_op: [self.conv1_op_slice],\n        self.relu1_op: [self.relu1_op_slice],\n        self.conv2_op: [self.conv2_op_slice],\n        self.relu2_op: [self.relu2_op_slice],\n        self.batch_norm_op: [self.batch_norm_op_slice],\n    }\n\n    # Map each slice to a group. Corresponding op slices have the same group.\n    self.op_group_dict = {\n        self.batch_norm_op_slice: self.batch_norm_op_group,\n    }\n\n    # Call handler to assign grouping.\n    handler = output_non_passthrough_op_handler.OutputNonPassthroughOpHandler()\n    handler.assign_grouping(self.conv2_op, self.mock_op_reg_manager)\n\n    # Verify manager looks up OpSlice for ops of interest.\n    self.mock_op_reg_manager.get_op_slices.assert_has_calls(\n        # Checking for ops to process.\n        [mock.call(self.relu1_op),\n         mock.call(self.batch_norm_op),\n         # Align.\n         mock.call(self.conv2_op),\n         mock.call(self.batch_norm_op),\n         # Slice.\n         mock.call(self.conv2_op),\n         mock.call(self.batch_norm_op),\n         # Update after align.\n         mock.call(self.batch_norm_op),\n         # Grouping.\n         mock.call(self.conv2_op)])\n\n    # Verify manager does not slice any ops.\n    self.mock_op_reg_manager.slice_op.assert_not_called()\n\n    # Verify manager adds inputs to process queue.\n    self.mock_op_reg_manager.process_ops.assert_called_once_with(\n        [self.relu1_op])\n\n    # Verify manager groups c2 with bn.\n    self.mock_op_reg_manager.group_op_slices.assert_called_once_with(\n        [self.conv2_op_slice, self.batch_norm_op_slice])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/framework/tpu_util.py,15,"b'""""""Utility functions for handling TPU graphs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib2\nimport tensorflow.compat.v1 as tf\n\n_run_on_cpu = False\n\n\n@contextlib2.contextmanager\ndef run_on_cpu():\n  """"""Provide a context for the code that needs to run on CPU.\n\n  Not thread-safe.\n\n  Yields:\n    None.\n  """"""\n  global _run_on_cpu\n  original_run_on_cpu = _run_on_cpu\n  _run_on_cpu = True\n  try:\n    yield\n  finally:\n    _run_on_cpu = original_run_on_cpu\n\n\ndef is_on_cpu():\n  return _run_on_cpu\n\n\ndef get_variable_name(read_variable_op):\n  assert read_variable_op.type == \'ReadVariableOp\'\n  op = read_variable_op\n  while op.type != \'VarHandleOp\':\n    assert len(op.inputs) == 1\n    op = op.inputs[0].op\n  return op.name\n\n\ndef maybe_convert_to_variable(tensor):\n  """"""Read value of a tensor from a variable when possible.\n\n  This function is intended to make tensors from inside the TPU while loop\n  available on the CPU by reading it from the variable to which the tensor was\n  written earlier. Note that the read may not reflect any writes that happened\n  in the same session.run(), unless control dependencies are added.\n\n  Args:\n    tensor: A tf.Tensor.\n\n  Returns:\n    A tf.Tensor. If input tensor is an output of reading a ResourceVariable, we\n    return an equivalent tensor produced in the current context. Otherwise, we\n    return the original input tensor.\n  """"""\n  op = tensor.op\n  if is_on_cpu() and tensor in var_store:\n    return var_store[tensor]\n  while op.type == \'Identity\':\n    assert len(op.inputs) == 1\n    op = op.inputs[0].op\n  if op.type != \'ReadVariableOp\':\n    # No need to convert.\n    return tensor\n  with tf.variable_scope(\n      # Reset the scope because variable_name contains all the scopes we need.\n      name_or_scope=tf.VariableScope(\'\'),\n      # We are looking for a reference to an existing variable, so we want to\n      # raise an exception if variable is not found.\n      reuse=True,\n  ):\n    variable_name = get_variable_name(op)\n    tf.logging.info(\'Converting tensor %s --> variable %s\',\n                    tensor, variable_name)\n    try:\n      return tf.get_variable(variable_name)\n    except ValueError:\n      tf.logging.info(\n          \'Variable %s was not created with tf.get_variable(). \'\n          \'Attempting to find it in GLOBAL_VARIABLES collection.\',\n          variable_name)\n    global_vars = tensor.graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    matched_vars = [v for v in global_vars if v.name == variable_name + \':0\']\n    if not matched_vars:\n      raise ValueError(\'Variable %s is in GraphDef but not in the live graph.\')\n    assert len(matched_vars) == 1\n    return matched_vars[0]\n\n\nvar_store = {}\ntop_level_scope = tf.get_variable_scope()\n\n\ndef write_to_variable(tensor, fail_if_exists=True):\n  """"""Saves a tensor for later retrieval on CPU.""""""\n  # Only relevant for debugging.\n  debug_name = \'tpu_util__\' + tensor.name.split(\':\')[0]\n\n  reuse = False if fail_if_exists else tf.compat.v1.AUTO_REUSE\n  with tf.variable_scope(top_level_scope, reuse=reuse):\n    variable = tf.get_variable(\n        name=debug_name,\n        shape=tensor.shape,\n        dtype=tensor.dtype,\n        trainable=False,\n        use_resource=True)\n\n  var_store[tensor] = variable\n  with tf.control_dependencies([variable.assign(tensor)]):\n    tensor_copy = tf.identity(tensor)\n  var_store[tensor_copy] = variable\n  return tensor_copy\n\n\ndef read_from_variable(tensor):\n  """"""Retrieves (a possibly stale copy of) the previously stored tensor.""""""\n  if is_on_cpu():\n    # Stale read, but on CPU that\'s all we can do without adding to loop vars.\n    return var_store[tensor]\n  else:\n    # Current read, but only works on TPU.\n    return tensor\n\n\ndef is_intermediate_var(v):\n  """"""Returns True if `v` was created by `write_to_variable` above.""""""\n  return v in var_store.values()\n'"
morph_net/framework/tpu_util_test.py,12,"b'# Lint as: python3\n""""""Tests for morph_net.framework.tpu_util.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom morph_net.framework import tpu_util\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import slim\n\n\nclass TpuUtilTest(parameterized.TestCase, tf.test.TestCase):\n\n  def build_model(self):\n    return slim.conv2d(\n        tf.zeros([64, 10, 10, 3]),\n        32, [5, 5],\n        scope=\'conv1\',\n        trainable=True,\n        normalizer_fn=slim.batch_norm,\n        normalizer_params={\n            \'scale\': True,\n            \'fused\': True\n        })\n\n  def get_gamma(self, activation_tensor):\n    # The input to the activation tensor is a FusedBatchNorm tensor; its name\n    # should be conv1/BatchNorm/FusedBatchNormV3:0, but the version may change.\n    batch_norm_tensor, = activation_tensor.op.inputs\n    assert \'FusedBatchNorm\' in batch_norm_tensor.name\n\n    # gamma tensor is used by MorphNet regularizer. It is:\n    # conv1/BatchNorm/ReadVariableOp:0 for ResourceVariable,\n    # conv1/BatchNorm/gamma/read:0 for VariableV2.\n    (unused_input_tensor, gamma_tensor, unused_beta_tensor,\n     unused_population_mean_tensor,\n     unused_population_variance_tensor) = batch_norm_tensor.op.inputs\n\n    # gamma_source is the op that drives the value of the gamma:\n    # \'conv1/BatchNorm/gamma\' of type VarHandleOp for ResourceVariable,\n    # \'conv1/BatchNorm/gamma\' of type VariableV2 for VariableV2.\n    gamma_source_op = gamma_tensor.op.inputs[0].op\n\n    return gamma_tensor, gamma_source_op\n\n  def test_variable_v2(self):\n    with tf.variable_scope(\'\', use_resource=False):\n      relu = self.build_model()\n    gamma_tensor, _ = self.get_gamma(relu)\n    # Check that maybe_convert_to_variable ignores VariableV2 (i.e., is no op).\n    self.assertEqual(\n        tpu_util.maybe_convert_to_variable(gamma_tensor), gamma_tensor)\n\n  def test_resource_variable(self):\n    with tf.variable_scope(\'\', use_resource=True):\n      relu = self.build_model()\n    gamma_tensor, gamma_source_op = self.get_gamma(relu)\n    variable = tpu_util.maybe_convert_to_variable(gamma_tensor)\n\n    # First assert that we didn\'t return the original tensor\n    self.assertNotEqual(variable, gamma_tensor)\n\n    # Now check that the variable created by maybe_convert_to_variable is\n    # driven by the same op as the tensor passed as input.\n    self.assertEqual(variable.op, gamma_source_op)\n\n    # If input tensor is separated from a variable by an extra hop of Identity,\n    # maybe_read_variable pretends the Identity op isn\'t there.\n    identity_tensor = tf.identity(gamma_tensor)\n    self.assertEqual(\n        tpu_util.maybe_convert_to_variable(identity_tensor), variable)\n\n  def test_noop(self):\n    with tf.variable_scope(\'\', use_resource=True):\n      relu = self.build_model()\n    # Check tensors that are not variable reads are ignored.\n    self.assertEqual(tpu_util.maybe_convert_to_variable(relu), relu)\n\n  def test_write_to_variable(self):\n    foo = tf.constant(0., name=\'foo\')\n    tpu_util.write_to_variable(foo)\n\n    with self.assertRaises(ValueError):\n      tpu_util.write_to_variable(foo, fail_if_exists=True)\n\n    # Variable sharing behavior should be dictated by `fail_if_exists` which\n    # overrides the effect of outer scopes.\n    with tf.variable_scope(\'\', reuse=True):\n      # should fail to return existing variable even though reuse=True\n      with self.assertRaises(ValueError):\n        tpu_util.write_to_variable(foo, fail_if_exists=True)\n\n    with tf.variable_scope(\'\', reuse=False):\n      # should return existing variable even though reuse=False\n      foo_copy = tpu_util.write_to_variable(foo, fail_if_exists=False)\n      self.assertEqual(tpu_util.var_store[foo], tpu_util.var_store[foo_copy])\n      self.assertLen(set(tpu_util.var_store.values()), 1)\n\n    with tf.variable_scope(\'\', reuse=True):\n      # should create new variable even though reuse=True\n      bar = tf.constant(0., name=\'bar\')\n      tpu_util.write_to_variable(bar)\n      self.assertLen(set(tpu_util.var_store.values()), 2)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/network_regularizers/__init__.py,0,b''
morph_net/network_regularizers/activation_regularizer.py,4,"b'""""""A L1 loss on BatchNorm gamma that targets the number of activations.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# [internal] enable type annotations\nfrom __future__ import print_function\n\nfrom morph_net.framework import batch_norm_source_op_handler\nfrom morph_net.framework import conv2d_transpose_source_op_handler\nfrom morph_net.framework import conv_source_op_handler\nfrom morph_net.framework import generic_regularizers\nfrom morph_net.framework import matmul_source_op_handler\nfrom morph_net.framework import op_handler_decorator\nfrom morph_net.framework import op_handlers\nfrom morph_net.framework import op_regularizer_manager as orm\nfrom morph_net.network_regularizers import cost_calculator\nfrom morph_net.network_regularizers import resource_function\nimport tensorflow.compat.v1 as tf\nfrom typing import Type, List\n\n\nclass GammaActivationRegularizer(generic_regularizers.NetworkRegularizer):\n  """"""A NetworkRegularizer that targets activation count using Gamma L1.""""""\n\n  def __init__(\n      self,\n      output_boundary: List[tf.Operation],\n      gamma_threshold,\n      regularizer_decorator: Type[generic_regularizers.OpRegularizer] = None,\n      decorator_parameters=None,\n      input_boundary: List[tf.Operation] = None,\n      force_group=None,\n      regularizer_blacklist=None):\n    """"""Creates a GammaActivationRegularizer object.\n\n    Args:\n      output_boundary: An OpRegularizer will be created for all these\n        operations, and recursively for all ops they depend on via data\n        dependency that does not involve ops from input_boundary.\n      gamma_threshold: A float scalar, will be used as a \'gamma_threshold\' for\n        all instances GammaL1Regularizer created by this class.\n      regularizer_decorator: A class of OpRegularizer decorator to use.\n      decorator_parameters: A dictionary of parameters to pass to the decorator\n        factory. To be used only with decorators that requires parameters,\n        otherwise use None.\n      input_boundary: A list of ops that represent the input boundary of the\n        subgraph being regularized (input boundary is not regularized).\n      force_group: List of regex for ops that should be force-grouped.  Each\n        regex corresponds to a separate group.  Use \'|\' operator to specify\n        multiple patterns in a single regex. See op_regularizer_manager for more\n        detail.\n      regularizer_blacklist: List of regex for ops that should not be\n        regularized. See op_regularizer_manager for more detail.\n    """"""\n    source_op_handler = batch_norm_source_op_handler.BatchNormSourceOpHandler(\n        gamma_threshold)\n    if regularizer_decorator:\n      source_op_handler = op_handler_decorator.OpHandlerDecorator(\n          source_op_handler, regularizer_decorator, decorator_parameters)\n    op_handler_dict = op_handlers.get_gamma_op_handler_dict()\n    op_handler_dict.update({\n        \'FusedBatchNorm\': source_op_handler,\n        \'FusedBatchNormV2\': source_op_handler,\n        \'FusedBatchNormV3\': source_op_handler,\n    })\n\n    self._manager = orm.OpRegularizerManager(\n        output_boundary,\n        op_handler_dict,\n        input_boundary=input_boundary,\n        force_group=force_group,\n        regularizer_blacklist=regularizer_blacklist)\n    self._calculator = cost_calculator.CostCalculator(\n        self._manager, resource_function.activation_count_function)\n\n  def get_regularization_term(self, ops=None):\n    return self._calculator.get_regularization_term(ops)\n\n  def get_cost(self, ops=None):\n    return self._calculator.get_cost(ops)\n\n  @property\n  def op_regularizer_manager(self):\n    return self._manager\n\n  @property\n  def name(self):\n    return \'GammaActivationCount\'\n\n  @property\n  def cost_name(self):\n    return \'Activations\'\n\n\nclass GroupLassoActivationRegularizer(generic_regularizers.NetworkRegularizer):\n  """"""A NetworkRegularizer that targets activation count using L1 group lasso.""""""\n\n  def __init__(\n      self,\n      output_boundary: List[tf.Operation],\n      threshold,\n      l1_fraction=0,\n      regularizer_decorator: Type[generic_regularizers.OpRegularizer] = None,\n      decorator_parameters=None,\n      input_boundary: List[tf.Operation] = None,\n      force_group=None,\n      regularizer_blacklist=None):\n    """"""Creates a GroupLassoActivationRegularizer object.\n\n    Args:\n      output_boundary: An OpRegularizer will be created for all these\n        operations, and recursively for all ops they depend on via data\n        dependency that does not involve ops from input_boundary.\n      threshold: A float scalar, will be used as a \'threshold\' for all\n        regularizer instances created by this class.\n      l1_fraction: Relative weight of L1 in L1 + L2 regularization.\n      regularizer_decorator: A class of OpRegularizer decorator to use.\n      decorator_parameters: A dictionary of parameters to pass to the decorator\n        factory. To be used only with decorators that requires parameters,\n        otherwise use None.\n      input_boundary: A list of ops that represent the input boundary of the\n        subgraph being regularized (input boundary is not regularized).\n      force_group: List of regex for ops that should be force-grouped.  Each\n        regex corresponds to a separate group.  Use \'|\' operator to specify\n        multiple patterns in a single regex. See op_regularizer_manager for more\n        detail.\n      regularizer_blacklist: List of regex for ops that should not be\n        regularized. See op_regularizer_manager for more detail.\n    """"""\n    conv2d_handler = conv_source_op_handler.Conv2DSourceOpHandler(\n        threshold, l1_fraction)\n    conv2d_transpose_handler = (\n        conv2d_transpose_source_op_handler.Conv2DTransposeSourceOpHandler(\n            threshold, l1_fraction))\n    matmul_handler = matmul_source_op_handler.MatMulSourceOpHandler(\n        threshold, l1_fraction)\n    if regularizer_decorator:\n      conv2d_handler = op_handler_decorator.OpHandlerDecorator(\n          conv2d_handler, regularizer_decorator, decorator_parameters)\n      conv2d_transpose_handler = op_handler_decorator.OpHandlerDecorator(\n          conv2d_transpose_handler, regularizer_decorator, decorator_parameters)\n      matmul_handler = op_handler_decorator.OpHandlerDecorator(\n          matmul_handler, regularizer_decorator, decorator_parameters)\n\n    op_handler_dict = op_handlers.get_group_lasso_op_handler_dict()\n    op_handler_dict.update({\n        \'Conv2D\': conv2d_handler,\n        \'Conv2DBackpropInput\': conv2d_transpose_handler,\n        \'MatMul\': matmul_handler,\n    })\n\n    self._manager = orm.OpRegularizerManager(\n        output_boundary,\n        op_handler_dict,\n        input_boundary=input_boundary,\n        force_group=force_group,\n        regularizer_blacklist=regularizer_blacklist)\n    self._calculator = cost_calculator.CostCalculator(\n        self._manager, resource_function.activation_count_function)\n\n  def get_regularization_term(self, ops=None):\n    return self._calculator.get_regularization_term(ops)\n\n  def get_cost(self, ops=None):\n    return self._calculator.get_cost(ops)\n\n  @property\n  def op_regularizer_manager(self):\n    return self._manager\n\n  @property\n  def name(self):\n    return \'GroupLassoActivationCount\'\n\n  @property\n  def cost_name(self):\n    return \'Activations\'\n'"
morph_net/network_regularizers/activation_regularizer_test.py,9,"b'""""""Tests for morph_net.network_regularizers.activation_regularizer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom morph_net.network_regularizers import activation_regularizer\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import slim as contrib_slim\n\nslim = contrib_slim\n\n\nclass ActivationLossTest(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    super(ActivationLossTest, self).setUp()\n    tf.reset_default_graph()\n    self.build_with_batch_norm()\n    with self.cached_session():\n      self.init()\n\n  def build_with_batch_norm(self):\n    params = {\n        \'trainable\': True,\n        \'normalizer_fn\': slim.batch_norm,\n        \'normalizer_params\': {\n            \'scale\': True\n        }\n    }\n\n    with slim.arg_scope([slim.layers.conv2d], **params):\n      self.build_model()\n\n  def build_model(self):\n    image = tf.constant(0.0, shape=[1, 17, 19, 3])\n    conv1 = slim.layers.conv2d(image, 13, [7, 5], padding=\'SAME\', scope=\'conv1\')\n    conv2 = slim.layers.conv2d(image, 23, [1, 1], padding=\'SAME\', scope=\'conv2\')\n    concat = tf.concat([conv1, conv2], 3)\n    self.conv3 = slim.layers.conv2d(\n        concat, 29, [3, 3], stride=2, padding=\'SAME\', scope=\'conv3\')\n    self.conv4 = slim.layers.conv2d(\n        concat, 31, [1, 1], stride=1, padding=\'SAME\', scope=\'conv4\')\n    self.name_to_var = {v.op.name: v for v in tf.global_variables()}\n\n    self.gamma_activation_reg = (\n        activation_regularizer.GammaActivationRegularizer(\n            [self.conv3.op, self.conv4.op], gamma_threshold=0.45))\n\n  def get_conv(self, name):\n    return tf.get_default_graph().get_operation_by_name(name +  \'/Conv2D\')\n\n  def init(self):\n    tf.global_variables_initializer().run()\n    gamma1 = self.name_to_var[\'conv1/BatchNorm/gamma\']\n    gamma1.assign([0.8] * 7 + [0.2] * 6).eval()\n    gamma2 = self.name_to_var[\'conv2/BatchNorm/gamma\']\n    gamma2.assign([-0.7] * 11 + [0.1] * 12).eval()\n    gamma3 = self.name_to_var[\'conv3/BatchNorm/gamma\']\n    gamma3.assign([0.6] * 10 + [-0.3] * 19).eval()\n    gamma4 = self.name_to_var[\'conv4/BatchNorm/gamma\']\n    gamma4.assign([-0.5] * 17 + [-0.4] * 14).eval()\n\n  def cost(self, conv):\n    with self.cached_session():\n      return self.gamma_activation_reg.get_cost(conv).eval()\n\n  def loss(self, conv):\n    with self.cached_session():\n      try:\n        return self.gamma_activation_reg.get_regularization_term(conv).eval()\n      except AttributeError:\n        print (str(conv))\n        print (\'getloss\', self.gamma_activation_reg.get_regularization_term(\n            conv))\n\n  @parameterized.named_parameters(\n      (\'_1\', \'conv1\', 7),  # 7 is the number of values > 0.45 in conv1.\n      (\'_2\', \'conv2\', 11),  # 11 is the number of values > 0.45 in conv2.\n      (\'_3\', \'conv3\', 10),  # 10 is the number of values > 0.45 in conv3.\n      (\'_4\', \'conv4\', 17))  # 17 is the number of values > 0.45 in conv4.\n  def test_cost(self, conv_name, expected_cost):\n    conv = self.get_conv(conv_name)\n    self.assertEqual(expected_cost, self.cost([conv]))\n\n  def test_list_cost(self):\n    # Test that passing a list of convs sums their contributions:\n    convs = [self.get_conv(\'conv3\'), self.get_conv(\'conv4\')]\n    self.assertEqual(\n        self.cost(convs[1:]) + self.cost(convs[:1]), self.cost(convs))\n\n  @parameterized.named_parameters(  # Expected values are sum(abs(gamma)).\n      (\'_1\', \'conv1\', 6.8),\n      (\'_2\', \'conv2\', 8.9),\n      (\'_3\', \'conv3\', 11.7),\n      (\'_4\', \'conv4\', 14.1))\n  def test_loss(self, conv_name, expected_loss):\n    self.assertAllClose(expected_loss, self.loss([self.get_conv(conv_name)]))\n\n  def test_loss_gradient_conv2(self):\n    loss = self.gamma_activation_reg.get_regularization_term(\n        [self.get_conv(\'conv2\')])\n    expected_grad = np.array([-1.0] * 11 + [1.0] * 12)\n    gammas = [\n        self.name_to_var[\'conv%d/BatchNorm/gamma\' % i] for i in range(1, 5)\n    ]\n\n    # Although the loss associated with conv2 depends on the gammas of conv2,\n    # conv3 and conv4, only gamma2 should receive graients. The other gammas are\n    # compared to the threshold to see if they are alive or not, but this should\n    # not send gradients to them.\n    grads = tf.gradients(loss, gammas)\n    self.assertEqual(None, grads[0])\n    self.assertEqual(None, grads[2])\n    self.assertEqual(None, grads[3])\n\n    # Regarding gamma2, it receives a -1 or a +1 gradient, depending on whether\n    # the gamma is negative or positive, since the loss is |gamma|. This is\n    # multiplied by expected_coeff.\n    with self.cached_session():\n      self.assertAllClose(expected_grad, grads[1].eval())\n\n  def test_conv_has_no_gamma(self):\n    conv5 = slim.layers.conv2d(\n        self.conv3, 11, [1, 1], stride=1, padding=\'SAME\', scope=\'conv5\')\n    self.gamma_activation_reg = (\n        activation_regularizer.GammaActivationRegularizer(\n            [conv5.op], gamma_threshold=0.45))\n\n    # Sanity check regarding conv3.\n    self.assertAllClose(11.7, self.loss([self.get_conv(\'conv3\')]))\n\n    # Conv5 has 11 outputs.\n    conv = self.get_conv(\'conv5\')\n    self.assertEqual(11, self.cost([conv]))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/network_regularizers/cost_calculator.py,13,"b'""""""CostCalculator that computes network cost or regularization loss.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf\n\n\nCONV2D_OPS = (\'Conv2D\', \'Conv2DBackpropInput\', \'DepthwiseConv2dNative\')\nCONV3D_OPS = (\'Conv3D\',)\nCONV_OPS = CONV2D_OPS + CONV3D_OPS\nFLOP_OPS = CONV_OPS + (\'MatMul\',)\nSUPPORTED_OPS = FLOP_OPS + (\'Add\', \'AddV2\', \'AddN\', \'ConcatV2\',\n                            \'FusedBatchNorm\', \'FusedBatchNormV2\',\n                            \'FusedBatchNormV3\', \'Mul\', \'Relu\', \'Relu6\', \'Sum\')\n\n\nclass CostCalculator(object):\n  """"""CostCalculator that calculates resource cost/loss for a network.""""""\n\n  def __init__(self, op_regularizer_manager, resource_function):\n    """"""Creates an instance.\n\n    Args:\n      op_regularizer_manager: OpRegularizerManager that contains the\n        OpRegularizer for each op in the network.\n      resource_function: Callable that returns the resource (e.g. FLOP) cost or\n        loss for an op.  The function signature is:\n          op; A tf.Operation.\n          is_regularization; Boolean indicating whether to calculate\n            regularization loss.  If False, calculate cost instead.\n          num_alive_inputs; Scalar Tensor indicating how many input channels are\n            considered alive.\n          num_alive_outputs; Scalar Tensor indicating how many output channels\n            are considered alive.\n          reg_inputs; Scalar Tensor which is the sum over the input\n            regularization vector.\n          reg_outputs; Scalar Tensor which is the sum over the output\n            regularization vector.\n          batch_size; Integer batch size to calculate cost/loss for.\n    """"""\n    self._manager = op_regularizer_manager\n    self._resource_function = resource_function\n\n  def _get_cost_or_regularization_term(self, is_regularization, ops=None):\n    """"""Returns cost or regularization term for ops.\n\n    Args:\n      is_regularization: Boolean indicating whether to calculate regularization\n        loss.  If False, calculate cost instead.\n      ops: List of tf.Operation.  If None, calculates cost/regularization for\n        all ops found by OpRegularizerManager.\n\n    Returns:\n      Cost or regularization term for ops as a tensor or float.\n    """"""\n    total = 0.0\n    if not ops:\n      ops = self._manager.ops\n    for op in ops:\n      if op.type not in SUPPORTED_OPS:\n        continue\n\n      # Get regularization and alive terms for input and output.\n      input_tensor = get_input_activation(op)\n      if op.type == \'ConcatV2\':\n        # For concat, the input and output regularization are identical but the\n        # input is composed of multiple concatenated regularizers.  Thus, just\n        # use the output regularizer as the input regularizer for simpler cost\n        # calculation.\n        input_tensor = op.outputs[0]\n      input_op_reg = self._manager.get_regularizer(input_tensor.op)\n      output_op_reg = self._manager.get_regularizer(op)\n      num_alive_inputs = _count_alive(input_tensor, input_op_reg)\n      num_alive_outputs = _count_alive(op.outputs[0], output_op_reg)\n      reg_inputs = _sum_of_reg_vector(input_op_reg)\n      reg_outputs = _sum_of_reg_vector(output_op_reg)\n\n      total += self._resource_function(\n          op, is_regularization, num_alive_inputs, num_alive_outputs,\n          reg_inputs, reg_outputs)\n\n    # If at least one supported op is present, type would be tensor, not float.\n    if isinstance(total, float):\n      # Tests rely on this function not raising exception in this case.\n      tf.logging.warning(\'No supported ops found.\')\n    return total\n\n  def get_cost(self, ops=None):\n    """"""Returns cost for ops.\n\n    Args:\n      ops: List of tf.Operation.  If None, calculates cost/regularization for\n        all ops found by OpRegularizerManager.\n\n    Returns:\n      Cost of ops as a tensor for float.\n    """"""\n\n    return self._get_cost_or_regularization_term(False, ops)\n\n  def get_regularization_term(self, ops=None):\n    """"""Returns regularization for ops.\n\n    Args:\n      ops: List of tf.Operation.  If None, calculates cost/regularization for\n        all ops found by OpRegularizerManager.\n\n    Returns:\n      Regularization term of ops as a tensor or float.\n    """"""\n    return self._get_cost_or_regularization_term(True, ops)\n\n\ndef get_input_activation(op):\n  """"""Returns the input to `op` that represents the activations.\n\n  (as opposed to e.g. weights.)\n\n  Args:\n    op: A tf.Operation object with type in SUPPORTED_OPS.\n\n  Returns:\n    A tf.Tensor representing the input activations.\n\n  Raises:\n    ValueError: op type not supported.).\n    ValueError: MatMul is used with transposition (unsupported).\n  """"""\n  if op.type not in SUPPORTED_OPS:\n    raise ValueError(\'Op type %s is not supported.\' % op.type)\n  if op.type in (\'Conv3D\', \'Conv2D\', \'DepthwiseConv2dNative\'):\n    return op.inputs[0]\n  if op.type == \'Conv2DBackpropInput\':\n    return op.inputs[2]\n  if op.type == \'MatMul\':\n    if op.get_attr(\'transpose_a\') or op.get_attr(\'transpose_b\'):\n      raise ValueError(\'MatMul with transposition is not yet supported.\')\n    return op.inputs[0]\n  return op.inputs[0]\n\n\ndef _count_alive(tensor, opreg):\n  if opreg:\n    return tf.reduce_sum(tf.cast(opreg.alive_vector, tf.float32))\n  shape = tensor.shape.as_list()\n  if shape:\n    num_outputs = tensor.shape.as_list()[-1]\n    if num_outputs is not None:\n      return tf.constant(num_outputs, tf.float32)\n  tf.logging.info(\'Unknown channel count in tensor %s\', tensor)\n  return tf.constant(0, tf.float32)\n\n\ndef _sum_of_reg_vector(opreg):\n  if opreg:\n    return tf.reduce_sum(opreg.regularization_vector)\n  else:\n    return tf.constant(0.0, tf.float32)\n'"
morph_net/network_regularizers/cost_calculator_test.py,11,"b'""""""Tests for network_regularizers.cost_calculator.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nfrom absl.testing import parameterized\nfrom morph_net.framework import batch_norm_source_op_handler\nfrom morph_net.framework import concat_op_handler\nfrom morph_net.framework import grouping_op_handler\nfrom morph_net.framework import op_regularizer_manager as orm\nfrom morph_net.framework import output_non_passthrough_op_handler\nfrom morph_net.network_regularizers import cost_calculator as cc\nfrom morph_net.network_regularizers import resource_function\nfrom morph_net.testing import add_concat_model_stub\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow.contrib import layers\nfrom tensorflow.contrib.framework import arg_scope\n\n\nclass CostCalculatorTest(parameterized.TestCase, tf.test.TestCase):\n\n  def _batch_norm_scope(self):\n    params = {\n        \'trainable\': True,\n        \'normalizer_fn\': layers.batch_norm,\n        \'normalizer_params\': {\n            \'scale\': True\n        }\n    }\n\n    with arg_scope([layers.conv2d], **params) as sc:\n      return sc\n\n  def testImageIsNotZerothOutputOfOp(self):\n    # Throughout the framework, we assume that the 0th output of each op is the\n    # only one of interest. One exception that often happens is when the input\n    # image comes from a queue or from a staging op. Then the image is one of\n    # the outputs of the dequeue (or staging) op, not necessarily the 0th one.\n    # Here we test that the BilinearNetworkRegularizer deals correctly with this\n    # case.\n\n    # Create an input op where the image is output number 1, not 0.\n    # TODO(g1) Move this mechanism to add_concat_model_stub, possibly using\n    # tf.split to produce an op where the image is not the 0th output image\n    # (instead of FIFOQueue).\n    image = add_concat_model_stub.image_stub()\n    non_image_tensor = tf.zeros(shape=(41,))\n    queue = tf.FIFOQueue(\n        capacity=1,\n        dtypes=(tf.float32,) * 2,\n        shapes=(non_image_tensor.shape, image.shape))\n\n    # Pass the image (output[1]) to the network.\n    with arg_scope(self._batch_norm_scope()):\n      output_op = add_concat_model_stub.build_model(queue.dequeue()[1])\n\n    # Create OpHandler dict for test.\n    op_handler_dict = collections.defaultdict(\n        grouping_op_handler.GroupingOpHandler)\n    op_handler_dict.update({\n        \'FusedBatchNormV3\':\n            batch_norm_source_op_handler.BatchNormSourceOpHandler(0.1),\n        \'Conv2D\':\n            output_non_passthrough_op_handler.OutputNonPassthroughOpHandler(),\n        \'ConcatV2\':\n            concat_op_handler.ConcatOpHandler(),\n    })\n\n    # Create OpRegularizerManager and NetworkRegularizer for test.\n    manager = orm.OpRegularizerManager([output_op], op_handler_dict)\n    calculator = cc.CostCalculator(manager, resource_function.flop_function)\n\n    # Calculate expected FLOP cost.\n    expected_alive_conv1 = sum(add_concat_model_stub.expected_alive()[\'conv1\'])\n    conv1_op = tf.get_default_graph().get_operation_by_name(\'conv1/Conv2D\')\n    conv1_coeff = resource_function.flop_coeff(conv1_op)\n    num_channels = 3\n    expected_cost = conv1_coeff * num_channels * expected_alive_conv1\n\n    with self.session():\n      tf.global_variables_initializer().run()\n      # Set gamma values to replicate aliveness in add_concat_model_stub.\n      name_to_var = {v.op.name: v for v in tf.global_variables()}\n      gamma1 = name_to_var[\'conv1/BatchNorm/gamma\']\n      gamma1.assign([0, 1, 1, 0, 1, 0, 1]).eval()\n      gamma4 = name_to_var[\'conv4/BatchNorm/gamma\']\n      gamma4.assign([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0]).eval()\n\n      queue.enqueue((non_image_tensor, image)).run()\n      self.assertEqual(expected_cost,\n                       calculator.get_cost([conv1_op]).eval())\n      # for 0/1 assigments cost and reg_term are equal:\n      self.assertEqual(expected_cost,\n                       calculator.get_regularization_term([conv1_op]).eval())\n\n  @parameterized.named_parameters(\n      (\'_conv2d\', 4, lambda x: layers.conv2d(x, 16, 3), \'Conv2D\'),\n      (\'_convt\', 4, lambda x: layers.conv2d_transpose(x, 16, 3),\n       \'conv2d_transpose\'),\n      (\'_conv2s\', 4, lambda x: layers.separable_conv2d(x, None, 3),\n       \'depthwise\'),\n      (\'_conv3d\', 5, lambda x: layers.conv3d(x, 16, 3), \'Conv3D\'))\n  def test_get_input_activation2(self, rank, fn, op_name):\n    g = tf.get_default_graph()\n    inputs = tf.zeros([6] * rank)\n    with arg_scope([\n        layers.conv2d, layers.conv2d_transpose, layers.separable_conv2d,\n        layers.conv3d\n    ],\n                   scope=\'test_layer\'):\n      _ = fn(inputs)\n    for op in g.get_operations():\n      print(op.name)\n    self.assertEqual(\n        inputs,\n        cc.get_input_activation(\n            g.get_operation_by_name(\'test_layer/\' + op_name)))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/network_regularizers/flop_regularizer.py,4,"b'""""""A NetworkRegularizer that targets the number of FLOPs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# [internal] enable type annotations\nfrom __future__ import print_function\n\nfrom morph_net.framework import batch_norm_source_op_handler\nfrom morph_net.framework import conv2d_transpose_source_op_handler as conv2d_transpose_handler\nfrom morph_net.framework import conv_source_op_handler as conv_handler\nfrom morph_net.framework import generic_regularizers\nfrom morph_net.framework import matmul_source_op_handler as matmul_handler\nfrom morph_net.framework import op_handler_decorator\nfrom morph_net.framework import op_handlers\nfrom morph_net.framework import op_regularizer_manager as orm\nfrom morph_net.network_regularizers import cost_calculator\nfrom morph_net.network_regularizers import resource_function\nimport tensorflow.compat.v1 as tf\nfrom typing import Type, List\n\n\nclass GammaFlopsRegularizer(generic_regularizers.NetworkRegularizer):\n  """"""A NetworkRegularizer that targets FLOPs using Gamma L1 as OpRegularizer.""""""\n\n  def __init__(\n      self,\n      output_boundary: List[tf.Operation],\n      gamma_threshold,\n      regularizer_decorator: Type[generic_regularizers.OpRegularizer] = None,\n      decorator_parameters=None,\n      input_boundary: List[tf.Operation] = None,\n      force_group=None,\n      regularizer_blacklist=None):\n    """"""Creates a GammaFlopsRegularizer object.\n\n    Args:\n      output_boundary: An OpRegularizer will be created for all these\n        operations, and recursively for all ops they depend on via data\n        dependency that does not involve ops from input_boundary.\n      gamma_threshold: A float scalar, will be used as a \'gamma_threshold\' for\n        all instances GammaL1Regularizer created by this class.\n      regularizer_decorator: A class of OpRegularizer decorator to use.\n      decorator_parameters: A dictionary of parameters to pass to the decorator\n        factory. To be used only with decorators that requires parameters,\n        otherwise use None.\n      input_boundary: A list of ops that represent the input boundary of the\n        subgraph being regularized (input boundary is not regularized).\n      force_group: List of regex for ops that should be force-grouped.  Each\n        regex corresponds to a separate group.  Use \'|\' operator to specify\n        multiple patterns in a single regex. See op_regularizer_manager for more\n        detail.\n      regularizer_blacklist: List of regex for ops that should not be\n        regularized. See op_regularizer_manager for more detail.\n    """"""\n    source_op_handler = batch_norm_source_op_handler.BatchNormSourceOpHandler(\n        gamma_threshold)\n    if regularizer_decorator:\n      source_op_handler = op_handler_decorator.OpHandlerDecorator(\n          source_op_handler, regularizer_decorator, decorator_parameters)\n    op_handler_dict = op_handlers.get_gamma_op_handler_dict()\n    op_handler_dict.update({\n        \'FusedBatchNorm\': source_op_handler,\n        \'FusedBatchNormV2\': source_op_handler,\n        \'FusedBatchNormV3\': source_op_handler,\n    })\n\n    self._manager = orm.OpRegularizerManager(\n        output_boundary,\n        op_handler_dict,\n        input_boundary=input_boundary,\n        force_group=force_group,\n        regularizer_blacklist=regularizer_blacklist)\n    self._calculator = cost_calculator.CostCalculator(\n        self._manager, resource_function.flop_function)\n\n  def get_regularization_term(self, ops=None):\n    return self._calculator.get_regularization_term(ops)\n\n  def get_cost(self, ops=None):\n    return self._calculator.get_cost(ops)\n\n  @property\n  def op_regularizer_manager(self):\n    return self._manager\n\n  @property\n  def name(self):\n    return \'GammaFlops\'\n\n  @property\n  def cost_name(self):\n    return \'FLOPs\'\n\n\nclass GroupLassoFlopsRegularizer(generic_regularizers.NetworkRegularizer):\n  """"""A NetworkRegularizer that targets FLOPs using L1 group lasso.""""""\n\n  def __init__(\n      self,\n      output_boundary: List[tf.Operation],\n      threshold,\n      l1_fraction=0,\n      regularizer_decorator: Type[generic_regularizers.OpRegularizer] = None,\n      decorator_parameters=None,\n      input_boundary: List[tf.Operation] = None,\n      force_group=None,\n      regularizer_blacklist=None):\n    """"""Creates a GroupLassoFlopsRegularizer object.\n\n    Args:\n      output_boundary: An OpRegularizer will be created for all these\n        operations, and recursively for all ops they depend on via data\n        dependency that does not involve ops from input_boundary.\n      threshold: A float scalar, will be used as a \'threshold\' for all\n        regularizer instances created by this class.\n      l1_fraction: Relative weight of L1 in L1 + L2 regularization.\n      regularizer_decorator: A class of OpRegularizer decorator to use.\n      decorator_parameters: A dictionary of parameters to pass to the decorator\n        factory. To be used only with decorators that requires parameters,\n        otherwise use None.\n      input_boundary: A list of ops that represent the input boundary of the\n        subgraph being regularized (input boundary is not regularized).\n      force_group: List of regex for ops that should be force-grouped.  Each\n        regex corresponds to a separate group.  Use \'|\' operator to specify\n        multiple patterns in a single regex. See op_regularizer_manager for more\n        detail.\n      regularizer_blacklist: List of regex for ops that should not be\n        regularized. See op_regularizer_manager for more detail.\n    """"""\n    custom_handlers = {\n        \'Conv2D\':\n            conv_handler.ConvSourceOpHandler(threshold, l1_fraction),\n        \'Conv3D\':\n            conv_handler.ConvSourceOpHandler(threshold, l1_fraction),\n        \'Conv2DBackpropInput\':\n            conv2d_transpose_handler.Conv2DTransposeSourceOpHandler(\n                threshold, l1_fraction),\n        \'MatMul\':\n            matmul_handler.MatMulSourceOpHandler(threshold, l1_fraction)\n    }\n    if regularizer_decorator:\n      for key in custom_handlers:\n        custom_handlers[key] = op_handler_decorator.OpHandlerDecorator(\n            custom_handlers[key], regularizer_decorator, decorator_parameters)\n\n    op_handler_dict = op_handlers.get_group_lasso_op_handler_dict()\n    op_handler_dict.update(custom_handlers)\n\n    self._manager = orm.OpRegularizerManager(\n        output_boundary,\n        op_handler_dict,\n        input_boundary=input_boundary,\n        force_group=force_group,\n        regularizer_blacklist=regularizer_blacklist)\n    self._calculator = cost_calculator.CostCalculator(\n        self._manager, resource_function.flop_function)\n\n  def get_regularization_term(self, ops=None):\n    return self._calculator.get_regularization_term(ops)\n\n  def get_cost(self, ops=None):\n    return self._calculator.get_cost(ops)\n\n  @property\n  def op_regularizer_manager(self):\n    return self._manager\n\n  @property\n  def name(self):\n    return \'GroupLassoFlops\'\n\n  @property\n  def cost_name(self):\n    return \'FLOPs\'\n'"
morph_net/network_regularizers/flop_regularizer_test.py,76,"b'# Lint as: python3\n""""""Tests for network_regularizers.flop_regularizer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom morph_net.network_regularizers import flop_regularizer\nfrom morph_net.network_regularizers import resource_function\nfrom morph_net.testing import dummy_decorator\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow.contrib import layers as contrib_layers\nfrom tensorflow.contrib import slim as contrib_slim\nfrom tensorflow.contrib.slim.nets import resnet_v1\n\nslim = contrib_slim\n\n_coeff = resource_function.flop_coeff\nNUM_CHANNELS = 3\nSKIP_GAMMA_CONV3D = True  # TODO(e1) remove when gamma is supported.\n\n\nclass GammaFlopsTest(parameterized.TestCase, tf.test.TestCase):\n\n  def BuildWithBatchNorm(self, fused):\n    params = {\n        \'trainable\': True,\n        \'normalizer_fn\': slim.batch_norm,\n        \'normalizer_params\': {\n            \'scale\': True,\n            \'fused\': fused,\n        }\n    }\n\n    with slim.arg_scope([slim.layers.conv2d], **params):\n      self.BuildModel()\n    with self.cached_session():\n      self.Init()\n\n  def BuildModel(self):\n    # Our test model is:\n    #\n    #         -> conv1 --+     -> conv3 -->\n    #        /           |    /\n    #  image          [concat]\n    #        \\           |    \\\n    #         -> conv2 --+     -> conv4 -->\n    #\n    # (the model has two ""outputs"", conv3 and conv4).\n    #\n\n    # op.name: \'Const\'\n    image = tf.constant(0.0, shape=[1, 17, 19, NUM_CHANNELS])\n    # op.name: \'conv1/Conv2D\'\n    self.conv1 = slim.layers.conv2d(\n        image, 13, [7, 5], padding=\'SAME\', scope=\'conv1\')\n    self.conv2 = slim.layers.conv2d(\n        image, 23, [1, 1], padding=\'SAME\', scope=\'conv2\')\n    self.concat = tf.concat([self.conv1, self.conv2], 3)\n    self.conv3 = slim.layers.conv2d(\n        self.concat, 29, [3, 3], stride=2, padding=\'SAME\', scope=\'conv3\')\n    self.conv4 = slim.layers.conv2d(\n        self.concat, 31, [1, 1], stride=1, padding=\'SAME\', scope=\'conv4\')\n    self.name_to_var = {v.op.name: v for v in tf.global_variables()}\n\n  def AddRegularizer(self, input_boundary=None):\n    self.gamma_flop_reg = flop_regularizer.GammaFlopsRegularizer(\n        [self.conv3.op, self.conv4.op],\n        gamma_threshold=0.45,\n        input_boundary=input_boundary)\n\n  def GetConv(self, name):\n    return tf.get_default_graph().get_operation_by_name(name + \'/Conv2D\')\n\n  def Init(self):\n    tf.global_variables_initializer().run()\n    gamma1 = self.name_to_var[\'conv1/BatchNorm/gamma\']\n    gamma1.assign([0.8] * 7 + [0.2] * 6).eval()\n    gamma2 = self.name_to_var[\'conv2/BatchNorm/gamma\']\n    gamma2.assign([-0.7] * 11 + [0.1] * 12).eval()\n    gamma3 = self.name_to_var[\'conv3/BatchNorm/gamma\']\n    gamma3.assign([0.6] * 10 + [-0.3] * 19).eval()\n    gamma4 = self.name_to_var[\'conv4/BatchNorm/gamma\']\n    gamma4.assign([-0.5] * 17 + [-0.4] * 14).eval()\n\n  def GetCost(self, conv):\n    with self.cached_session():\n      return self.gamma_flop_reg.get_cost(conv).eval()\n\n  def GetLoss(self, conv):\n    with self.cached_session():\n      return self.gamma_flop_reg.get_regularization_term(conv).eval()\n\n  def GetSourceOps(self):\n    op_regularizer_manager = self.gamma_flop_reg.op_regularizer_manager\n    return [\n        op.name\n        for op in op_regularizer_manager.ops\n        if op_regularizer_manager.is_source_op(op)\n    ]\n\n  def testCost(self):\n    self.BuildWithBatchNorm(fused=True)\n    self.AddRegularizer(input_boundary=None)\n\n    # Conv1 has 7 gammas above 0.45, and NUM_CHANNELS inputs (from the image).\n    conv = self.GetConv(\'conv1\')\n    self.assertEqual(_coeff(conv) * 7 * NUM_CHANNELS, self.GetCost([conv]))\n\n    # Conv2 has 11 gammas above 0.45, and NUM_CHANNELS inputs (from the image).\n    conv = self.GetConv(\'conv2\')\n    self.assertEqual(_coeff(conv) * 11 * NUM_CHANNELS, self.GetCost([conv]))\n\n    # Conv3 has 10 gammas above 0.45, and 7 + 11 inputs from conv1 and conv2.\n    conv = self.GetConv(\'conv3\')\n    self.assertEqual(_coeff(conv) * 10 * 18, self.GetCost([conv]))\n\n    # Conv4 has 17 gammas above 0.45, and 7 + 11 inputs from conv1 and conv2.\n    conv = self.GetConv(\'conv4\')\n    self.assertEqual(_coeff(conv) * 17 * 18, self.GetCost([conv]))\n\n    # Test that passing a list of convs sums their contributions:\n    convs = [self.GetConv(\'conv3\'), self.GetConv(\'conv4\')]\n    self.assertEqual(\n        self.GetCost(convs[:1]) + self.GetCost(convs[1:]), self.GetCost(convs))\n\n  def testInputBoundaryNone(self):\n    self.BuildWithBatchNorm(fused=True)\n    self.AddRegularizer(input_boundary=None)\n    self.assertCountEqual(self.GetSourceOps(), [\n        \'conv1/BatchNorm/FusedBatchNormV3\', \'conv2/BatchNorm/FusedBatchNormV3\',\n        \'conv3/BatchNorm/FusedBatchNormV3\', \'conv4/BatchNorm/FusedBatchNormV3\'\n    ])\n\n  def testInputBoundaryConv3(self):\n    # Only block one path, can still reach all other convolutions.\n    self.BuildWithBatchNorm(fused=True)\n    self.AddRegularizer(input_boundary=[self.conv3.op])\n    self.assertCountEqual(self.GetSourceOps(), [\n        \'conv1/BatchNorm/FusedBatchNormV3\', \'conv2/BatchNorm/FusedBatchNormV3\',\n        \'conv4/BatchNorm/FusedBatchNormV3\'\n    ])\n\n  def testInputBoundaryConv3And4(self):\n    # Block both paths, can no longer reach Concat and earlier convolutions.\n    self.BuildWithBatchNorm(fused=True)\n    self.AddRegularizer(input_boundary=[self.conv3.op, self.conv4.op])\n    self.assertCountEqual(self.GetSourceOps(), [])\n\n  def testInputBoundaryConcat(self):\n    # Block concat, can only see conv3 and conv4.\n    self.BuildWithBatchNorm(fused=True)\n    self.AddRegularizer(input_boundary=[self.concat.op])\n    self.assertCountEqual(self.GetSourceOps(), [\n        \'conv3/BatchNorm/FusedBatchNormV3\', \'conv4/BatchNorm/FusedBatchNormV3\'\n    ])\n\n  def testLossDecorated(self):\n    self.BuildWithBatchNorm(True)\n    self.AddRegularizer()\n    # Create network regularizer with DummyDecorator op regularization.\n    self.gamma_flop_reg = flop_regularizer.GammaFlopsRegularizer(\n        [self.conv3.op, self.conv4.op],\n        gamma_threshold=0.45,\n        regularizer_decorator=dummy_decorator.DummyDecorator,\n        decorator_parameters={\'scale\': 0.5})\n\n    all_convs = [\n        o for o in tf.get_default_graph().get_operations() if o.type == \'Conv2D\'\n    ]\n    total_reg_term = 1410376.375\n    self.assertAllClose(total_reg_term * 0.5, self.GetLoss(all_convs))\n    self.assertAllClose(total_reg_term * 0.5, self.GetLoss([]))\n\n\nclass GammaFlopDecoratedTest(parameterized.TestCase, tf.test.TestCase):\n  """"""A simple test to check the op regularizer decorator with flop regularizer.\n  """"""\n\n  def testLossCostDecorated(self):\n    params = {\'trainable\': True, \'normalizer_fn\': slim.batch_norm,\n              \'normalizer_params\': {\'scale\': True}}\n\n    with slim.arg_scope([slim.layers.conv2d], **params):\n      image = tf.constant(0.0, shape=[1, 3, 3, NUM_CHANNELS])\n      conv1 = slim.layers.conv2d(\n          image, 2, kernel_size=1, padding=\'SAME\', scope=\'conv1\')\n    with self.cached_session():\n      tf.global_variables_initializer().run()\n      name_to_var = {v.op.name: v for v in tf.global_variables()}\n      gamma1 = name_to_var[\'conv1/BatchNorm/gamma\']\n      gamma1.assign([1] * 2).eval()\n\n    self.gamma_flop_reg = flop_regularizer.GammaFlopsRegularizer(\n        [conv1.op],\n        gamma_threshold=0.1,\n        regularizer_decorator=dummy_decorator.DummyDecorator,\n        decorator_parameters={\'scale\': 0.5})\n\n    conv = tf.get_default_graph().get_operation_by_name(\'conv1/Conv2D\')\n    # we compare the computed cost and regularization calculated as follows:\n    # reg_term = op_coeff * (number_of_inputs * (regularization=2 * 0.5) +\n    # number_of_outputs * (input_regularization=0))\n    # number_of_flops = coeff * number_of_inputs * number_of_outputs.\n    with self.cached_session():\n      predicted_reg = self.gamma_flop_reg.get_regularization_term([conv]).eval()\n      self.assertEqual(_coeff(conv) * NUM_CHANNELS * 1, predicted_reg)\n      predicted_cost = self.gamma_flop_reg.get_cost([conv]).eval()\n      self.assertEqual(_coeff(conv) * 2 * NUM_CHANNELS, predicted_cost)\n\n\nclass GroupLassoFlopDecoratedTest(parameterized.TestCase, tf.test.TestCase):\n  """"""A test to check the op regularizer decorator for group lasso regularizer.\n  """"""\n\n  def testLossCostDecorated(self):\n    image = tf.constant(0.0, shape=[1, 3, 3, 3])\n    kernel = tf.ones([1, 1, 3, 2])\n\n    pred = tf.nn.conv2d(image, kernel, strides=[1, 1, 1, 1], padding=\'SAME\')\n    conv = pred.op\n\n    self.group_lasso_reg = flop_regularizer.GroupLassoFlopsRegularizer(\n        [conv],\n        0.1,\n        l1_fraction=0,\n        regularizer_decorator=dummy_decorator.DummyDecorator,\n        decorator_parameters={\'scale\': 0.5})\n    # we compare the computed cost and regularization calculated as follows:\n    # reg_term = op_coeff * (number_of_inputs * (regularization=2 * 0.5) +\n    # number_of_outputs * (input_regularization=0))\n    # number_of_flops = coeff * number_of_inputs * number_of_outputs.\n    with self.cached_session():\n      pred_reg = self.group_lasso_reg.get_regularization_term([conv]).eval()\n      self.assertEqual(_coeff(conv) * 3 * 1, pred_reg)\n      pred_cost = self.group_lasso_reg.get_cost([conv]).eval()\n      self.assertEqual(_coeff(conv) * 2 * NUM_CHANNELS, pred_cost)\n\n\nclass GammaFlopsWithDepthwiseConvTestBase(tf.test.TestCase):\n  """"""Test flop_regularizer for a network with depthwise convolutions.""""""\n\n  def BuildWithBatchNorm(self):\n    params = {\n        \'trainable\': True,\n        \'normalizer_fn\': slim.batch_norm,\n        \'normalizer_params\': {\n            \'scale\': True\n        }\n    }\n    ops_with_batchnorm = [slim.layers.conv2d]\n    if self._depthwise_use_batchnorm:\n      ops_with_batchnorm.append(slim.layers.separable_conv2d)\n\n    with slim.arg_scope(ops_with_batchnorm, **params):\n      self.BuildModel()\n\n  def BuildModel(self):\n    # Our test model is:\n    #\n    #         -> dw1 --> conv1 --+\n    #        /                   |\n    #  image                     [concat] --> conv3\n    #        \\                   |\n    #         -> conv2 --> dw2 --+\n    #\n    # (the model has two ""outputs"", conv3).\n    #\n    image = tf.constant(0.0, shape=[1, 17, 19, NUM_CHANNELS])\n    dw1 = slim.layers.separable_conv2d(\n        image, None, [3, 3], depth_multiplier=1, stride=1, scope=\'dw1\')\n    conv1 = slim.layers.conv2d(dw1, 13, [7, 5], padding=\'SAME\', scope=\'conv1\')\n    conv2 = slim.layers.conv2d(image, 23, [1, 1], padding=\'SAME\', scope=\'conv2\')\n    dw2 = slim.layers.separable_conv2d(\n        conv2, None, [5, 5], depth_multiplier=1, stride=1, scope=\'dw2\')\n    concat = tf.concat([conv1, dw2], 3)\n    self.conv3 = slim.layers.conv2d(\n        concat, 29, [3, 3], stride=2, padding=\'SAME\', scope=\'conv3\')\n    self.name_to_var = {v.op.name: v for v in tf.global_variables()}\n\n    regularizer_blacklist = None\n    if self._depthwise_use_batchnorm:\n      regularizer_blacklist = [\'dw1\']\n    self.gamma_flop_reg = flop_regularizer.GammaFlopsRegularizer(\n        [self.conv3.op], gamma_threshold=0.45,\n        regularizer_blacklist=regularizer_blacklist)\n\n  def GetConv(self, name):\n    return tf.get_default_graph().get_operation_by_name(\n        name + (\'/Conv2D\' if \'conv\' in name else \'/depthwise\'))\n\n  def GetGammaAbsValue(self, name):\n    gamma_op = tf.get_default_graph().get_operation_by_name(name +\n                                                            \'/BatchNorm/gamma\')\n    with self.cached_session():\n      gamma = gamma_op.outputs[0].eval()\n    return np.abs(gamma)\n\n  def Init(self):\n    tf.global_variables_initializer().run()\n    gamma1 = self.name_to_var[\'conv1/BatchNorm/gamma\']\n    gamma1.assign([0.8] * 7 + [0.2] * 6).eval()\n    gamma2 = self.name_to_var[\'conv2/BatchNorm/gamma\']\n    gamma2.assign([-0.7] * 11 + [0.1] * 12).eval()\n    gamma3 = self.name_to_var[\'conv3/BatchNorm/gamma\']\n    gamma3.assign([0.6] * 10 + [-0.3] * 19).eval()\n    # Initialize gamma for depthwise convs only if there are Batchnorm for them.\n    if self._depthwise_use_batchnorm:\n      gammad1 = self.name_to_var[\'dw1/BatchNorm/gamma\']\n      gammad1.assign([-0.3] * 1 + [-0.9] * 2).eval()\n      gammad2 = self.name_to_var[\'dw2/BatchNorm/gamma\']\n      gammad2.assign([0.3] * 5 + [0.9] * 10 + [-0.1] * 8).eval()\n\n  def cost(self, conv):  # pylint: disable=invalid-name\n    with self.cached_session():\n      cost = self.gamma_flop_reg.get_cost(conv)\n      return cost.eval() if isinstance(cost, tf.Tensor) else cost\n\n  def loss(self, conv):  # pylint: disable=invalid-name\n    with self.cached_session():\n      reg = self.gamma_flop_reg.get_regularization_term(conv)\n      return reg.eval() if isinstance(reg, tf.Tensor) else reg\n\n\nclass GammaFlopsWithDepthwiseConvTest(GammaFlopsWithDepthwiseConvTestBase):\n  """"""Test flop_regularizer for a network with depthwise convolutions.""""""\n\n  def setUp(self):\n    self._depthwise_use_batchnorm = True\n    super(GammaFlopsWithDepthwiseConvTest, self).setUp()\n    self.BuildWithBatchNorm()\n    with self.cached_session():\n      self.Init()\n\n  def testCost(self):\n    # Dw1 has 2 gammas above 0.45 out of NUM_CHANNELS inputs (from the image),\n    # but because the input doesn\'t have a regularizer, it has no way of\n    # removing the channels, so the channel count is still NUM_CHANNELS.\n    conv = self.GetConv(\'dw1\')\n    self.assertEqual(_coeff(conv) * NUM_CHANNELS, self.cost([conv]))\n\n    # Conv1 has 7 gammas above 0.45, and NUM_CHANNELS inputs (from dw1).\n    conv = self.GetConv(\'conv1\')\n    self.assertEqual(_coeff(conv) * 7 * NUM_CHANNELS, self.cost([conv]))\n\n    # Conv2 has 11 active + 12 inactive, while Dw2 has 5 inactive, 10 active and\n    # 8 active. Their max (or) has 15 active and 8 inactive.\n    # Conv2 has NUM_CHANNELS inputs (from the image).\n    conv = self.GetConv(\'conv2\')\n    self.assertEqual(_coeff(conv) * 15 * NUM_CHANNELS, self.cost([conv]))\n\n    # Dw2 has 15 out of 23 inputs (from the Conv2).\n    conv = self.GetConv(\'dw2\')\n    self.assertEqual(_coeff(conv) * 15, self.cost([conv]))\n\n    # Conv3 has 10 gammas above 0.45, and 7 + 15 inputs from conv1 and dw2.\n    conv = self.GetConv(\'conv3\')\n    self.assertEqual(_coeff(conv) * 10 * 22, self.cost([conv]))\n\n  def testRegularizer(self):\n    # Dw1 depthwise convolution is connected to the input (no regularizer).\n    conv = self.GetConv(\'dw1\')\n    # Although the effective regularizer for dw is computed as below:\n    # gamma = self.GetGammaAbsValue(\'dw1\')\n    # expected_loss = _coeff(conv) * gamma.sum()\n    # Since the input is not regularized, dw does not return a regularizer.\n    expected_loss = 0.0\n    self.assertNear(expected_loss, self.loss([conv]), expected_loss * 1e-5)\n\n    # Conv1 takes Dw1 as input, its input regularizer is from dw1.\n    conv = self.GetConv(\'conv1\')\n    gamma = self.GetGammaAbsValue(\'conv1\')\n    # The effective size for dw can be computed from its gamma, and\n    # the loss may be computed as follows:\n    # gamma_dw = self.GetGammaAbsValue(\'dw1\')\n    # expected_loss = _coeff(conv) * (\n    #     gamma.sum() * (gamma_dw > 0.45).sum() + gamma_dw.sum() *\n    #     (gamma > 0.45).sum())\n    # However, since dw cannot change shape because its input doesn\'t have a\n    # regularizer, the real loss we expect should be:\n    expected_loss = _coeff(conv) * (gamma.sum() * NUM_CHANNELS)\n    self.assertNear(expected_loss, self.loss([conv]), expected_loss * 1e-5)\n\n    # Dw2 depthwise convolution is connected to conv2 (grouped regularizer).\n    conv = self.GetConv(\'conv2\')\n    gamma_conv = self.GetGammaAbsValue(\'conv2\')\n    dw = self.GetConv(\'dw2\')\n    gamma_dw = self.GetGammaAbsValue(\'dw2\')\n    gamma = np.maximum(gamma_dw, gamma_conv).sum()\n    expected_loss = _coeff(conv) * (gamma * 3 + (gamma > 0.45).sum() * 0)\n    self.assertNear(expected_loss, self.loss([conv]), expected_loss * 1e-5)\n    expected_loss = _coeff(dw) * gamma * 2\n    self.assertNear(expected_loss, self.loss([dw]), expected_loss * 1e-5)\n\n\nclass GammaFlopsWithDepthwiseConvNoBatchNormTest(\n    GammaFlopsWithDepthwiseConvTestBase):\n  """"""Test flop_regularizer for un-batchnormed depthwise convolutions.\n\n  This test is used to confirm that when depthwise convolution is not BNed, it\n  will not be considered towards the regularizer, but it will be counted towards\n  the cost.\n  This design choice is for backward compatibility for users who did not\n  regularize depthwise convolutions. However, the cost will be reported\n  regardless in order to be faithful to the real computation complexity.\n  """"""\n\n  def setUp(self):\n    self._depthwise_use_batchnorm = False\n    super(GammaFlopsWithDepthwiseConvNoBatchNormTest, self).setUp()\n    self.BuildWithBatchNorm()\n    with self.cached_session():\n      self.Init()\n\n  def testCost(self):\n    # Dw1 has NUM_CHANNELS inputs (from the image).\n    conv = self.GetConv(\'dw1\')\n    self.assertEqual(_coeff(conv) * 3, self.cost([conv]))\n\n    # Conv1 has 7 gammas above 0.45, and 3 inputs (from dw1).\n    conv = self.GetConv(\'conv1\')\n    self.assertEqual(_coeff(conv) * 7 * 3, self.cost([conv]))\n\n    # Conv2 has 11 active outputs and NUM_CHANNELS inputs (from the image).\n    conv = self.GetConv(\'conv2\')\n    self.assertEqual(_coeff(conv) * 11 * NUM_CHANNELS, self.cost([conv]))\n\n    # Dw2 has 11 inputs (pass-through from the Conv2).\n    conv = self.GetConv(\'dw2\')\n    self.assertEqual(_coeff(conv) * 11, self.cost([conv]))\n\n    # Conv3 has 10 gammas above 0.45, and 7 + 11 inputs from conv1 and dw2.\n    conv = self.GetConv(\'conv3\')\n    self.assertEqual(_coeff(conv) * 10 * 18, self.cost([conv]))\n\n  def testRegularizer(self):\n    # Dw1 depthwise convolution is connected to the input (no regularizer).\n    conv = self.GetConv(\'dw1\')\n    expected_loss = 0.0\n    self.assertNear(expected_loss, self.loss([conv]), expected_loss * 1e-5)\n\n    # Conv1 takes Dw1 as input, but it\'s not affected by dw1 because depthwise\n    # is not BNed.\n    conv = self.GetConv(\'conv1\')\n    gamma = self.GetGammaAbsValue(\'conv1\')\n    expected_loss = _coeff(conv) * (gamma.sum() * NUM_CHANNELS)\n    self.assertNear(expected_loss, self.loss([conv]), expected_loss * 1e-5)\n\n    # Dw2 depthwise convolution is connected to conv2 (pass through).\n    dw = self.GetConv(\'dw2\')\n    gamma = self.GetGammaAbsValue(\'conv2\')\n    expected_loss = _coeff(dw) * gamma.sum() * 2\n    self.assertNear(expected_loss, self.loss([dw]), expected_loss * 1e-5)\n\n\nclass GammaFlopResidualConnectionsLossTest(tf.test.TestCase):\n  """"""Tests flop_regularizer for a network with residual connections.""""""\n\n  def setUp(self):\n    super(GammaFlopResidualConnectionsLossTest, self).setUp()\n    tf.set_random_seed(7)\n    self._threshold = 0.6\n\n  def BuildModel(self, resnet_fn, block_fn):\n    # We use this model as a test case because the slim.nets.resnet module is\n    # used in some production.\n    #\n    # The model looks as follows:\n    #\n    # Image --> unit_1/shortcut\n    # Image --> unit_1/conv1 --> unit_1/conv2 --> unit_1/conv3\n    #\n    # unit_1/shortcut + unit_1/conv3 --> unit_1 (residual connection)\n    #\n    # unit_1 --> unit_2/conv1  -> unit_2/conv2 --> unit_2/conv3\n    #\n    # unit_1 + unit_2/conv3 --> unit_2 (residual connection)\n    #\n    # In between, there are strided convolutions and pooling ops, but these\n    # should not affect the regularizer.\n    blocks = [\n        block_fn(\'block1\', base_depth=7, num_units=2, stride=2),\n    ]\n    image = tf.constant(0.0, shape=[1, 2, 2, NUM_CHANNELS])\n    net = resnet_fn(\n        image, blocks, include_root_block=False, is_training=False)[0]\n    net = tf.reduce_mean(net, axis=(1, 2))\n    return slim.layers.fully_connected(net, 23, scope=\'FC\')\n\n  def BuildGraphWithBatchNorm(self, resnet_fn, block_fn):\n    params = {\n        \'trainable\': True,\n        \'normalizer_fn\': slim.batch_norm,\n        \'normalizer_params\': {\n            \'scale\': True\n        }\n    }\n\n    with slim.arg_scope([slim.layers.conv2d, slim.layers.separable_conv2d],\n                        **params):\n      self.net = self.BuildModel(resnet_fn, block_fn)\n\n  def InitGamma(self):\n    assignments = []\n    gammas = {}\n    for v in tf.global_variables():\n      if v.op.name.endswith(\'/gamma\'):\n        assignments.append(v.assign(tf.random_uniform(v.shape)))\n        gammas[v.op.name] = v\n    with self.cached_session() as s:\n      s.run(assignments)\n      self._gammas = s.run(gammas)\n\n  def GetGamma(self, short_name):\n    tokens = short_name.split(\'/\')\n    name = (\'resnet_v1/block1/\' + tokens[0] + \'/bottleneck_v1/\' + tokens[1] +\n            \'/BatchNorm/gamma\')\n    return self._gammas[name]\n\n  def GetOp(self, short_name):\n    if short_name == \'FC\':\n      return tf.get_default_graph().get_operation_by_name(\'FC/MatMul\')\n    tokens = short_name.split(\'/\')\n    name = (\'resnet_v1/block1/\' + tokens[0] + \'/bottleneck_v1/\' + tokens[1] +\n            \'/Conv2D\')\n    return tf.get_default_graph().get_operation_by_name(name)\n\n  def NumAlive(self, short_name):\n    return np.sum(self.GetGamma(short_name) > self._threshold)\n\n  def GetCoeff(self, short_name):\n    return _coeff(self.GetOp(short_name))\n\n  def testCost(self):\n    self.BuildGraphWithBatchNorm(resnet_v1.resnet_v1, resnet_v1.resnet_v1_block)\n    self.InitGamma()\n    res_alive = np.logical_or(\n        np.logical_or(\n            self.GetGamma(\'unit_1/shortcut\') > self._threshold,\n            self.GetGamma(\'unit_1/conv3\') > self._threshold),\n        self.GetGamma(\'unit_2/conv3\') > self._threshold)\n\n    self.gamma_flop_reg = flop_regularizer.GammaFlopsRegularizer(\n        [self.net.op], self._threshold)\n\n    expected = {}\n    expected[\'unit_1/shortcut\'] = (\n        self.GetCoeff(\'unit_1/shortcut\') * np.sum(res_alive) * NUM_CHANNELS)\n    expected[\'unit_1/conv1\'] = (\n        self.GetCoeff(\'unit_1/conv1\') * self.NumAlive(\'unit_1/conv1\') *\n        NUM_CHANNELS)\n    expected[\'unit_1/conv2\'] = (\n        self.GetCoeff(\'unit_1/conv2\') * self.NumAlive(\'unit_1/conv2\') *\n        self.NumAlive(\'unit_1/conv1\'))\n    expected[\'unit_1/conv3\'] = (\n        self.GetCoeff(\'unit_1/conv3\') * np.sum(res_alive) *\n        self.NumAlive(\'unit_1/conv2\'))\n    expected[\'unit_2/conv1\'] = (\n        self.GetCoeff(\'unit_2/conv1\') * self.NumAlive(\'unit_2/conv1\') *\n        np.sum(res_alive))\n    expected[\'unit_2/conv2\'] = (\n        self.GetCoeff(\'unit_2/conv2\') * self.NumAlive(\'unit_2/conv2\') *\n        self.NumAlive(\'unit_2/conv1\'))\n    expected[\'unit_2/conv3\'] = (\n        self.GetCoeff(\'unit_2/conv3\') * np.sum(res_alive) *\n        self.NumAlive(\'unit_2/conv2\'))\n    expected[\'FC\'] = 2.0 * np.sum(res_alive) * 23.0\n\n    # TODO(e1): Is there a way to use Parametrized Tests to make this more\n    # elegant?\n    with self.cached_session():\n      for short_name in expected:\n        cost = self.gamma_flop_reg.get_cost([self.GetOp(short_name)]).eval()\n        self.assertEqual(expected[short_name], cost)\n\n      self.assertEqual(\n          sum(expected.values()),\n          self.gamma_flop_reg.get_cost().eval())\n\n\nclass GammaConv3DTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters(\n      (\'_all_alive\', 1e-3, 3),\n      (\'_one_alive\', .35, 1),\n      (\'_all_dead\', .99, 0),\n  )\n  def test_simple_conv3d(self, threshold, expected_alive):\n    # TODO(e1) remove when gamma is supported.\n    # This test works if reshape not set to be a handled by\n    # leaf_op_handler.LeafOpHandler() in op_handlers.py. However this changes\n    # brakes other tests for reasons to be investigated.\n    if SKIP_GAMMA_CONV3D:\n      return\n\n    def fused_batch_norm3d(*args, **kwargs):\n      if args:\n        inputs = args[0]\n        args = args[1:]\n      else:\n        inputs = kwargs.pop(\'inputs\')\n      shape = inputs.shape\n      # inputs is assumed to be NHWTC (T is for time).\n      batch_size = shape[0]\n      # space x time cube is reshaped to be 2D with dims:\n      # H, W, T --> H, W * T\n      # the idea is that batch norm only needs this to collect spacial stats.\n      target_shape = [batch_size, shape[1], shape[2] * shape[3], shape[4]]\n      inputs = tf.reshape(inputs, target_shape, name=\'Reshape/to2d\')\n      normalized = slim.batch_norm(inputs, *args, **kwargs)\n      return tf.reshape(normalized, shape, name=\'Reshape/to3d\')\n\n    gamma_val = [0.5, 0.3, 0.2]\n    num_inputs = 4\n    batch_size = 2\n    video = tf.zeros([batch_size, 8, 8, 8, num_inputs])\n    kernel = [5, 5, 5]\n    num_outputs = 3\n    net = slim.conv3d(\n        video,\n        num_outputs,\n        kernel,\n        padding=\'SAME\',\n        normalizer_fn=fused_batch_norm3d,\n        normalizer_params={\n            \'scale\': True,\n            \'fused\': True\n        },\n        scope=\'vconv1\')\n    self.assertLen(net.shape.as_list(), 5)\n    shape = net.shape.as_list()\n    # The number of applications is the number of elements in the [HWT] tensor.\n    num_applications = shape[1] * shape[2] * shape[3]\n    application_cost = num_inputs * kernel[0] * kernel[1] * kernel[2]\n    name_to_var = {v.op.name: v for v in tf.global_variables()}\n    flop_reg = flop_regularizer.GammaFlopsRegularizer(\n        [net.op,\n         tf.get_default_graph().get_operation_by_name(\'vconv1/Conv3D\')],\n        threshold,\n        force_group=[\'vconv1/Reshape/to3d|vconv1/Reshape/to2d|vconv1/Conv3D\'])\n    gamma = name_to_var[\'vconv1/BatchNorm/gamma\']\n    with self.session():\n      tf.global_variables_initializer().run()\n      gamma.assign(gamma_val).eval()\n\n      self.assertAllClose(\n          flop_reg.get_cost(),\n          2 * expected_alive * num_applications * application_cost)\n\n      raw_cost = 2 * num_outputs * num_applications * application_cost\n      self.assertAllClose(flop_reg.get_regularization_term(),\n                          raw_cost * np.mean(gamma_val))\n\n\nclass GroupLassoFlopRegTest(parameterized.TestCase, tf.test.TestCase):\n\n  def assertNearRelatively(self, expected, actual):\n    self.assertNear(expected, actual, expected * 1e-6)\n\n  def testFlopRegularizer(self):\n    tf.reset_default_graph()\n    tf.set_random_seed(7907)\n    with slim.arg_scope(\n        [slim.layers.conv2d, slim.layers.conv2d_transpose],\n        weights_initializer=tf.random_normal_initializer):\n      # Our test model is:\n      #\n      #         -> conv1 --+\n      #        /           |--[concat]\n      #  image --> conv2 --+\n      #        \\\n      #         -> convt\n      #\n      # (the model has two ""outputs"", convt and concat).\n      #\n      image = tf.constant(0.0, shape=[1, 17, 19, NUM_CHANNELS])\n      conv1 = slim.layers.conv2d(\n          image, 13, [7, 5], padding=\'SAME\', scope=\'conv1\')\n      conv2 = slim.layers.conv2d(\n          image, 23, [1, 1], padding=\'SAME\', scope=\'conv2\')\n      self.concat = tf.concat([conv1, conv2], 3)\n      self.convt = slim.layers.conv2d_transpose(\n          image, 29, [7, 5], stride=3, padding=\'SAME\', scope=\'convt\')\n      self.name_to_var = {v.op.name: v for v in tf.global_variables()}\n    with self.cached_session():\n      tf.global_variables_initializer().run()\n\n    threshold = 1.0\n    flop_reg = flop_regularizer.GroupLassoFlopsRegularizer(\n        [self.concat.op, self.convt.op], threshold=threshold, l1_fraction=0)\n\n    with self.cached_session() as s:\n      evaluated_vars = s.run(self.name_to_var)\n\n    def group_norm(weights, axis=(0, 1, 2)):  # pylint: disable=invalid-name\n      return np.sqrt(np.mean(weights**2, axis=axis))\n\n    reg_vectors = {\n        \'conv1\': group_norm(evaluated_vars[\'conv1/weights\'], (0, 1, 2)),\n        \'conv2\': group_norm(evaluated_vars[\'conv2/weights\'], (0, 1, 2)),\n        \'convt\': group_norm(evaluated_vars[\'convt/weights\'], (0, 1, 3))\n    }\n\n    num_alive = {k: np.sum(r > threshold) for k, r in reg_vectors.items()}\n    total_outputs = (\n        reg_vectors[\'conv1\'].shape[0] + reg_vectors[\'conv2\'].shape[0])\n    total_alive_outputs = sum(num_alive.values())\n    assert total_alive_outputs > 0, (\n        \'All outputs are dead - test is trivial. Decrease the threshold.\')\n    assert total_alive_outputs < total_outputs, (\n        \'All outputs are alive - test is trivial. Increase the threshold.\')\n\n    coeff1 = _coeff(_get_op(\'conv1/Conv2D\'))\n    coeff2 = _coeff(_get_op(\'conv2/Conv2D\'))\n    coefft = _coeff(_get_op(\'convt/conv2d_transpose\'))\n\n    expected_flop_cost = NUM_CHANNELS * (\n        coeff1 * num_alive[\'conv1\'] + coeff2 * num_alive[\'conv2\'] +\n        coefft * num_alive[\'convt\'])\n    expected_reg_term = NUM_CHANNELS * (\n        coeff1 * np.sum(reg_vectors[\'conv1\']) + coeff2 * np.sum(\n            reg_vectors[\'conv2\']) + coefft * np.sum(reg_vectors[\'convt\']))\n    with self.cached_session():\n      self.assertEqual(\n          round(expected_flop_cost), round(flop_reg.get_cost().eval()))\n      self.assertNearRelatively(expected_reg_term,\n                                flop_reg.get_regularization_term().eval())\n\n  @parameterized.named_parameters(\n      (\'_matmul\', False),\n      (\'_slim\', True),\n  )\n  def testFlopRegularizerWithMatMul(self, use_contrib):\n    """"""Test the MatMul op regularizer with FLOP network regularizer.""""""\n\n    # Set up a two layer fully connected network.\n    tf.reset_default_graph()\n    # Create the variables, and corresponding values.\n    x = tf.constant(1.0, shape=[2, 6], name=\'x\', dtype=tf.float32)\n    w = tf.get_variable(\'w\', shape=(6, 4), dtype=tf.float32)\n    b = tf.get_variable(\'b\', shape=(4), dtype=tf.float32)\n    w2 = tf.get_variable(\'w2\', shape=(4, 1), dtype=tf.float32)\n    b2 = tf.get_variable(\'b2\', shape=(1), dtype=tf.float32)\n    w_value = np.arange(24).reshape((6, 4)).astype(\'float32\')\n    b_value = np.arange(4).reshape(4).astype(\'float32\')\n    w2_value = np.arange(21, 25).reshape((4, 1)).astype(\'float32\')\n    b2_value = np.arange(1).astype(\'float32\')\n    fc1_name = \'matmul1/MatMul\'\n    fc2_name = \'matmul2/MatMul\'\n\n    # Build the test network model.\n    if use_contrib:\n      net = contrib_layers.fully_connected(\n          x,\n          4,\n          scope=\'matmul1\',\n          weights_initializer=tf.constant_initializer(w_value),\n          biases_initializer=tf.constant_initializer(b_value))\n      output = contrib_layers.fully_connected(\n          net,\n          1,\n          scope=\'matmul2\',\n          weights_initializer=tf.constant_initializer(w2_value),\n          biases_initializer=tf.constant_initializer(b2_value))\n      with self.cached_session():\n        tf.global_variables_initializer().run()\n    else:\n      net = tf.nn.relu(tf.matmul(x, w, name=fc1_name) + b)\n      output = tf.nn.relu(tf.matmul(net, w2, name=fc2_name) + b2)\n      # Assign values to network parameters.\n      with self.cached_session() as session:\n        session.run([\n            w.assign(w_value),\n            b.assign(b_value),\n            w2.assign(w2_value),\n            b2.assign(b2_value)\n        ])\n\n    # Create FLOPs network regularizer.\n    threshold = 32.0\n    flop_reg = flop_regularizer.GroupLassoFlopsRegularizer([output.op],\n                                                           threshold, 0)\n\n    # Compute expected regularization vector and alive vector.\n    def group_norm(weights, axis=(0, 1, 2)):  # pylint: disable=invalid-name\n      return np.sqrt(np.mean(weights**2, axis=axis))\n    expected_reg_vector1 = group_norm(w_value, axis=(0,))\n    expected_reg_vector2 = group_norm(w2_value, axis=(0,))\n    # Since the threshold is 32, and the L2 norm of columns in matrix w is\n    # (29.66479301, 31.71750259, 33.82307053, 35.97220993). Thus, the alive\n    # vector for w should be (0, 0, 1, 1). The alive vector is [1] since the L2\n    # norm for w2_value is 45.055521 > 32.\n    # Compute the expected FLOPs cost and expected regularization term.\n    matmul1_live_input = 6\n    matmul1_live_output = sum(expected_reg_vector1 > threshold)\n    matmul2_live_output = sum(expected_reg_vector2 > threshold)\n    expected_flop_cost = (\n        _coeff(_get_op(fc1_name)) * matmul1_live_input * matmul1_live_output +\n        _coeff(_get_op(fc2_name)) * matmul1_live_output * matmul2_live_output)\n    regularizer1 = np.sum(expected_reg_vector1)\n    regularizer2 = np.sum(expected_reg_vector2)\n    expected_reg_term = (\n        _coeff(_get_op(fc1_name)) * matmul1_live_input * regularizer1 +\n        _coeff(_get_op(fc2_name)) * (matmul1_live_output * regularizer2 +\n                                     matmul2_live_output * regularizer1))\n    with self.cached_session() as session:\n      self.assertEqual(\n          round(flop_reg.get_cost().eval()), round(expected_flop_cost))\n      self.assertNearRelatively(flop_reg.get_regularization_term().eval(),\n                                expected_reg_term)\n\n  def testFlopRegularizerDontConvertToVariable(self):\n    tf.reset_default_graph()\n    tf.set_random_seed(1234)\n\n    x = tf.constant(1.0, shape=[2, 6], name=\'x\', dtype=tf.float32)\n    w = tf.Variable(tf.truncated_normal([6, 4], stddev=1.0), use_resource=True)\n    net = tf.matmul(x, w)\n\n    # Create FLOPs network regularizer.\n    threshold = 0.9\n    flop_reg = flop_regularizer.GroupLassoFlopsRegularizer([net.op], threshold,\n                                                           0)\n\n    with self.cached_session():\n      tf.global_variables_initializer().run()\n      flop_reg.get_regularization_term().eval()\n\n  def test_group_lasso_conv3d(self):\n    shape = [3, 3, 3]\n    video = tf.zeros([2, 3, 3, 3, 1])\n    net = slim.conv3d(\n        video,\n        5,\n        shape,\n        padding=\'VALID\',\n        weights_initializer=tf.glorot_normal_initializer(),\n        scope=\'vconv1\')\n    conv3d_op = tf.get_default_graph().get_operation_by_name(\'vconv1/Conv3D\')\n    conv3d_weights = conv3d_op.inputs[1]\n\n    threshold = 0.09\n    flop_reg = flop_regularizer.GroupLassoFlopsRegularizer([net.op],\n                                                           threshold=threshold)\n    norm = tf.sqrt(tf.reduce_mean(tf.square(conv3d_weights), [0, 1, 2, 3]))\n    alive = tf.reduce_sum(tf.cast(norm > threshold, tf.float32))\n    with self.session():\n      flop_coeff = 2 * shape[0] * shape[1] * shape[2]\n      tf.compat.v1.global_variables_initializer().run()\n      self.assertAllClose(flop_reg.get_cost(), flop_coeff * alive)\n      self.assertAllClose(flop_reg.get_regularization_term(),\n                          flop_coeff * tf.reduce_sum(norm))\n\n\ndef _get_op(name):  # pylint: disable=invalid-name\n  return tf.get_default_graph().get_operation_by_name(name)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/network_regularizers/latency_regularizer.py,2,"b'""""""A NetworkRegularizer that targets inference latency.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# [internal] enable type annotations\nfrom __future__ import print_function\n\nfrom morph_net.framework import batch_norm_source_op_handler\nfrom morph_net.framework import conv2d_transpose_source_op_handler as conv2d_transpose_handler\nfrom morph_net.framework import conv_source_op_handler as conv_handler\nfrom morph_net.framework import generic_regularizers\nfrom morph_net.framework import matmul_source_op_handler as matmul_handler\n\nfrom morph_net.framework import op_handler_decorator\nfrom morph_net.framework import op_handlers\nfrom morph_net.framework import op_regularizer_manager as orm\nfrom morph_net.network_regularizers import cost_calculator\nfrom morph_net.network_regularizers import resource_function\nimport tensorflow.compat.v1 as tf\nfrom typing import Type, List\n\n\nclass GammaLatencyRegularizer(generic_regularizers.NetworkRegularizer):\n  """"""A NetworkRegularizer that targets latency using Gamma L1.""""""\n\n  def __init__(\n      self,\n      output_boundary: List[tf.Operation],\n      gamma_threshold,\n      hardware,\n      batch_size=1,\n      regularizer_decorator: Type[generic_regularizers.OpRegularizer] = None,\n      decorator_parameters=None,\n      input_boundary: List[tf.Operation] = None,\n      force_group=None,\n      regularizer_blacklist=None) -> None:\n    """"""Creates a GammaLatencyRegularizer object.\n\n    Latency cost and regularization loss is calculated for a specified hardware\n    platform.\n\n    Args:\n      output_boundary: An OpRegularizer will be created for all these\n        operations, and recursively for all ops they depend on via data\n        dependency that does not involve ops from input_boundary.\n      gamma_threshold: A float scalar, will be used as a \'gamma_threshold\' for\n        all instances GammaL1Regularizer created by this class.\n      hardware: String name of hardware platform to target.  Must be a key from\n        resource_function.PEAK_COMPUTE.\n      batch_size: Integer batch size to calculate cost/loss for.\n      regularizer_decorator: A string, the name of the regularizer decorators\n        to use. Supported decorators are listed in\n        op_regularizer_decorator.SUPPORTED_DECORATORS.\n      decorator_parameters: A dictionary of parameters to pass to the decorator\n        factory. To be used only with decorators that requires parameters,\n        otherwise use None.\n      input_boundary: A list of ops that represent the input boundary of the\n        subgraph being regularized (input boundary is not regularized).\n      force_group: List of regex for ops that should be force-grouped.  Each\n        regex corresponds to a separate group.  Use \'|\' operator to specify\n        multiple patterns in a single regex. See op_regularizer_manager for\n        more detail.\n      regularizer_blacklist: List of regex for ops that should not be\n        regularized. See op_regularizer_manager for more detail.\n    """"""\n    source_op_handler = batch_norm_source_op_handler.BatchNormSourceOpHandler(\n        gamma_threshold)\n    if regularizer_decorator:\n      source_op_handler = op_handler_decorator.OpHandlerDecorator(\n          source_op_handler, regularizer_decorator,\n          decorator_parameters)\n    op_handler_dict = op_handlers.get_gamma_op_handler_dict()\n    op_handler_dict.update({\n        \'FusedBatchNorm\': source_op_handler,\n        \'FusedBatchNormV2\': source_op_handler,\n        \'FusedBatchNormV3\': source_op_handler,\n    })\n\n    self._manager = orm.OpRegularizerManager(\n        output_boundary, op_handler_dict, input_boundary=input_boundary,\n        force_group=force_group, regularizer_blacklist=regularizer_blacklist)\n    self._calculator = cost_calculator.CostCalculator(\n        self._manager,\n        resource_function.latency_function_factory(hardware, batch_size))\n    self._hardware = hardware\n\n  def get_regularization_term(self, ops=None):\n    return self._calculator.get_regularization_term(ops)\n\n  def get_cost(self, ops=None):\n    return self._calculator.get_cost(ops)\n\n  @property\n  def op_regularizer_manager(self):\n    return self._manager\n\n  @property\n  def name(self):\n    return \'Latency\'\n\n  @property\n  def cost_name(self):\n    return self._hardware + \' Latency\'\n\n\nclass GroupLassoLatencyRegularizer(generic_regularizers.NetworkRegularizer):\n  """"""A NetworkRegularizer that targets Latency using L1 group lasso.""""""\n\n  def __init__(self,\n               output_boundary,\n               threshold,\n               hardware,\n               batch_size=1,\n               l1_fraction=0,\n               regularizer_decorator=None,\n               decorator_parameters=None,\n               input_boundary=None,\n               force_group=None,\n               regularizer_blacklist=None):\n    """"""Creates a GroupLassoFlopsRegularizer object.\n\n    Args:\n      output_boundary: An OpRegularizer will be created for all these\n        operations, and recursively for all ops they depend on via data\n        dependency that does not involve ops from input_boundary.\n      threshold: A float scalar, will be used as a \'threshold\' for all\n        regularizer instances created by this class.\n      hardware: String name of hardware platform to target. Must be a key from\n        resource_function.PEAK_COMPUTE.\n      batch_size: Integer batch size to calculate cost/loss for.\n      l1_fraction: Relative weight of L1 in L1 + L2 regularization.\n      regularizer_decorator: A class of OpRegularizer decorator to use.\n      decorator_parameters: A dictionary of parameters to pass to the decorator\n        factory. To be used only with decorators that requires parameters,\n        otherwise use None.\n      input_boundary: A list of ops that represent the input boundary of the\n        subgraph being regularized (input boundary is not regularized).\n      force_group: List of regex for ops that should be force-grouped.  Each\n        regex corresponds to a separate group.  Use \'|\' operator to specify\n        multiple patterns in a single regex. See op_regularizer_manager for more\n        detail.\n      regularizer_blacklist: List of regex for ops that should not be\n        regularized. See op_regularizer_manager for more detail.\n    """"""\n    custom_handlers = {\n        \'Conv2D\':\n            conv_handler.ConvSourceOpHandler(threshold, l1_fraction),\n        \'Conv3D\':\n            conv_handler.ConvSourceOpHandler(threshold, l1_fraction),\n        \'Conv2DBackpropInput\':\n            conv2d_transpose_handler.Conv2DTransposeSourceOpHandler(\n                threshold, l1_fraction),\n        \'MatMul\':\n            matmul_handler.MatMulSourceOpHandler(threshold, l1_fraction)\n    }\n    if regularizer_decorator:\n      for key in custom_handlers:\n        custom_handlers[key] = op_handler_decorator.OpHandlerDecorator(\n            custom_handlers[key], regularizer_decorator, decorator_parameters)\n\n    op_handler_dict = op_handlers.get_group_lasso_op_handler_dict()\n    op_handler_dict.update(custom_handlers)\n\n    self._manager = orm.OpRegularizerManager(\n        output_boundary,\n        op_handler_dict,\n        input_boundary=input_boundary,\n        force_group=force_group,\n        regularizer_blacklist=regularizer_blacklist)\n    self._calculator = cost_calculator.CostCalculator(\n        self._manager,\n        resource_function.latency_function_factory(hardware, batch_size))\n    self._hardware = hardware\n\n  def get_regularization_term(self, ops=None):\n    return self._calculator.get_regularization_term(ops)\n\n  def get_cost(self, ops=None):\n    return self._calculator.get_cost(ops)\n\n  @property\n  def op_regularizer_manager(self):\n    return self._manager\n\n  @property\n  def name(self):\n    return \'Latency\'\n\n  @property\n  def cost_name(self):\n    return self._hardware + \' Latency\'\n'"
morph_net/network_regularizers/latency_regularizer_test.py,18,"b'""""""Tests for network_regularizers.latency_regularizer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom absl.testing import parameterized\nfrom morph_net.network_regularizers import flop_regularizer\nfrom morph_net.network_regularizers import latency_regularizer\nfrom morph_net.network_regularizers import model_size_regularizer\nfrom morph_net.network_regularizers import resource_function\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import layers as contrib_layers\nfrom tensorflow.contrib import slim as contrib_slim\n\n\nfrom tensorflow.contrib.slim.nets import inception\n\nslim = contrib_slim\n\nNUM_CHANNELS = 3\nHARDWARE = \'P100\'\n\n\nclass LatencyRegularizerTest(parameterized.TestCase, tf.test.TestCase):\n\n  def build_with_batch_norm(self, fused):\n    params = {\n        \'trainable\': True,\n        \'normalizer_fn\': slim.batch_norm,\n        \'normalizer_params\': {\n            \'scale\': True,\n            \'fused\': fused,\n        }\n    }\n\n    with slim.arg_scope([slim.layers.conv2d], **params):\n      self.build_model()\n    with self.cached_session():\n      self.init()\n\n  def build_model(self):\n    # Our test model is:\n    #\n    #         -> conv1 --+     -> conv3 -->\n    #        /           |    /\n    #  image          [concat]\n    #        \\           |    \\\n    #         -> conv2 --+     -> conv4 -->\n    #\n    # (the model has two ""outputs"", conv3 and conv4).\n    #\n    image = tf.constant(0.0, shape=[1, 17, 19, NUM_CHANNELS])\n    conv1 = slim.layers.conv2d(image, 13, [7, 5], padding=\'SAME\', scope=\'conv1\')\n    conv2 = slim.layers.conv2d(image, 23, [1, 1], padding=\'SAME\', scope=\'conv2\')\n    concat = tf.concat([conv1, conv2], 3)\n    self.conv3 = slim.layers.conv2d(\n        concat, 29, [3, 3], stride=2, padding=\'SAME\', scope=\'conv3\')\n    self.conv4 = slim.layers.conv2d(\n        concat, 31, [1, 1], stride=1, padding=\'SAME\', scope=\'conv4\')\n    self.name_to_var = {v.op.name: v for v in tf.global_variables()}\n\n    self.regularizer = latency_regularizer.GammaLatencyRegularizer(\n        [self.conv3.op, self.conv4.op],\n        gamma_threshold=0.45, hardware=HARDWARE)\n\n  def get_conv(self, name):\n    return tf.get_default_graph().get_operation_by_name(name + \'/Conv2D\')\n\n  def init(self):\n    tf.global_variables_initializer().run()\n    gamma1 = self.name_to_var[\'conv1/BatchNorm/gamma\']\n    gamma1.assign([0.8] * 7 + [0.2] * 6).eval()\n    gamma2 = self.name_to_var[\'conv2/BatchNorm/gamma\']\n    gamma2.assign([-0.7] * 11 + [0.1] * 12).eval()\n    gamma3 = self.name_to_var[\'conv3/BatchNorm/gamma\']\n    gamma3.assign([0.6] * 10 + [-0.3] * 19).eval()\n    gamma4 = self.name_to_var[\'conv4/BatchNorm/gamma\']\n    gamma4.assign([-0.5] * 17 + [-0.4] * 14).eval()\n\n  def get_cost(self, conv):\n    with self.cached_session():\n      return self.regularizer.get_cost(conv).eval()\n\n  def get_loss(self, conv):\n    with self.cached_session():\n      return self.regularizer.get_regularization_term(conv).eval()\n\n  def test_cost(self):\n    self.build_with_batch_norm(True)\n    # Conv1 has 7 gammas above 0.45, and NUM_CHANNELS inputs (from the image).\n    conv = self.get_conv(\'conv1\')\n    # FLOPs = 2 * NHWRSCK\n    expected_flops = 2 * 17 * 19 * 7 * 5 * 3 * 7\n    expected_cost = (\n        float(expected_flops) / resource_function.PEAK_COMPUTE[HARDWARE])\n    self.assertAllClose(expected_cost, self.get_cost([conv]))\n    self.assertAllClose(51.0548400879, self.get_cost([conv]))\n\n    # Conv2 has 11 gammas above 0.45, and NUM_CHANNELS inputs (from the image).\n    conv = self.get_conv(\'conv2\')\n    # Memory = (input_size + weight_size + output_size) * dtype_size\n    #        = (NHWC + RSCK + NHWK) * 4\n    expected_memory = (17 * 19 * 3 + 3 * 11 + 17 * 19 * 11) * 4\n    expected_cost = (\n        float(expected_memory) / resource_function.MEMORY_BANDWIDTH[HARDWARE])\n    self.assertAllClose(expected_cost, self.get_cost([conv]))\n    self.assertAllClose(24.8907108307, self.get_cost([conv]))\n\n    # Conv3 has 10 gammas above 0.45, and 7 + 11 inputs from conv1 and conv2.\n    conv = self.get_conv(\'conv3\')\n    # Memory = (input_size + weight_size + output_size) * dtype_size\n    #        = (NHWC + RSCK + NHWK) * 4\n    expected_memory = (17 * 19 * 18 + 3 * 3 * 18 * 10 + 9 * 10 * 10) * 4\n    expected_cost3 = (\n        float(expected_memory) / resource_function.MEMORY_BANDWIDTH[HARDWARE])\n    self.assertAllClose(expected_cost3, self.get_cost([conv]))\n    self.assertAllClose(45.5409851074, self.get_cost([conv]))\n\n    # Conv4 has 17 gammas above 0.45, and 7 + 11 inputs from conv1 and conv2.\n    conv = self.get_conv(\'conv4\')\n    # Memory = (input_size + weight_size + output_size) * dtype_size\n    #        = (NHWC + RSCK + NHWK) * 4\n    expected_memory = (17 * 19 * 18 + 18 * 17 + 17 * 19 * 17) * 4\n    expected_cost4 = (\n        float(expected_memory) / resource_function.MEMORY_BANDWIDTH[HARDWARE])\n    self.assertAllClose(expected_cost4, self.get_cost([conv]))\n    self.assertAllClose(63.4480857849, self.get_cost([conv]))\n\n    # Test that passing a list of convs sums their contributions:\n    convs = [self.get_conv(\'conv3\'), self.get_conv(\'conv4\')]\n    self.assertAllClose(expected_cost3 + expected_cost4, self.get_cost(convs))\n    self.assertAllClose(108.989074707, self.get_cost(convs))\n\n  def testLossRegression(self):\n    self.build_with_batch_norm(True)\n    g = tf.get_default_graph()\n    all_convs = [o for o in g.get_operations() if o.type == \'Conv2D\']\n\n    # FLOP loss = 2 * NHWRS * (alive_in * reg_out + alive_out * reg_in)\n    conv1_loss = (\n        2 * 17 * 19 * 7 * 5 * (3 * 6.8 + 7 * 0)\n        / resource_function.PEAK_COMPUTE[HARDWARE])\n    self.assertAllClose(conv1_loss, self.get_loss([self.get_conv(\'conv1\')]))\n\n    # Memory loss = (HW * reg_in + HW * reg_out) * dtype_size\n    # Note that reg_in = reg_out for pass-through ops such as Relu.\n    relu1_loss = ((17 * 19 * 6.8 + 17 * 19 * 6.8) * 4\n                  / resource_function.MEMORY_BANDWIDTH[HARDWARE])\n    batch_norm1_loss = relu1_loss\n    relu1_op = g.get_operation_by_name(\'conv1/Relu\')\n    self.assertAllClose(relu1_loss, self.get_loss([relu1_op]))\n\n    # Memory loss = input_tensor + weight_tensor + output_tensor\n    #             = (HW * reg_in\n    #                + RS * (alive_in * reg_out + alive_out * reg_in)\n    #                + HW * reg_out) * dtype_size\n    conv2_loss = ((17 * 19 * 0 + (3 * 8.9 + 11 * 0) + 17 * 19 * 8.9) * 4\n                  / resource_function.MEMORY_BANDWIDTH[HARDWARE])\n    self.assertAllClose(conv2_loss, self.get_loss([self.get_conv(\'conv2\')]))\n\n    relu2_loss = ((17 * 19 * 8.9) * 2 * 4\n                  / resource_function.MEMORY_BANDWIDTH[HARDWARE])\n    batch_norm2_loss = relu2_loss\n    relu2_op = g.get_operation_by_name(\'conv2/Relu\')\n    self.assertAllClose(relu2_loss, self.get_loss([relu2_op]))\n\n    # Memory loss = input_tensor + output_tensor\n    #             = HW * reg_in + HW * reg_out\n    # Note that reg_in = reg_out for concat.\n    concat_loss = ((17 * 19 * 15.7 + 17 * 19 * 15.7) * 4\n                   / resource_function.MEMORY_BANDWIDTH[HARDWARE])\n    concat_op = g.get_operation_by_name(\'concat\')\n    self.assertAllClose(concat_loss, self.get_loss([concat_op]))\n\n    conv3_loss = ((17 * 19 * 15.7\n                   + 3 * 3 * (18 * 11.7 + 10 * 15.7)\n                   + 9 * 10 * 11.7) * 4\n                  / resource_function.MEMORY_BANDWIDTH[HARDWARE])\n    self.assertAllClose(conv3_loss, self.get_loss([self.get_conv(\'conv3\')]))\n\n    relu3_loss = ((9 * 10 * 11.7) * 2 * 4\n                  / resource_function.MEMORY_BANDWIDTH[HARDWARE])\n    batch_norm3_loss = relu3_loss\n    relu3_op = g.get_operation_by_name(\'conv3/Relu\')\n    self.assertAllClose(relu3_loss, self.get_loss([relu3_op]))\n\n    conv4_loss = ((17 * 19 * 15.7\n                   + (18 * 14.1 + 17 * 15.7)\n                   + 17 * 19 * 14.1) * 4\n                  / resource_function.MEMORY_BANDWIDTH[HARDWARE])\n    self.assertAllClose(conv4_loss, self.get_loss([self.get_conv(\'conv4\')]))\n\n    relu4_loss = ((17 * 19 * 14.1) * 2 * 4\n                  / resource_function.MEMORY_BANDWIDTH[HARDWARE])\n    batch_norm4_loss = relu4_loss\n    relu4_op = g.get_operation_by_name(\'conv4/Relu\')\n    self.assertAllClose(relu4_loss, self.get_loss([relu4_op]))\n\n    conv_losses = conv1_loss + conv2_loss + conv3_loss + conv4_loss\n    other_losses = (concat_loss\n                    + relu1_loss + batch_norm1_loss\n                    + relu2_loss + batch_norm2_loss\n                    + relu3_loss + batch_norm3_loss\n                    + relu4_loss + batch_norm4_loss)\n    self.assertAllClose(conv_losses, self.get_loss(all_convs))\n    self.assertAllClose(conv_losses + other_losses, self.get_loss([]))\n\n  @parameterized.named_parameters(\n      (\'_P100\', \'P100\'),\n      (\'_V100\', \'V100\'))\n  def testInceptionV2(self, hardware):\n    image = tf.zeros([1, 224, 224, 3])\n    net, _ = inception.inception_v2_base(image)\n    g = tf.get_default_graph()\n    self.regularizer = latency_regularizer.GammaLatencyRegularizer(\n        [net.op], gamma_threshold=0.5, hardware=hardware)\n\n    # Compute-bound convolution.\n    op = g.get_operation_by_name(\n        \'InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/Conv2D\')\n    # FLOP cost = 2 * NHWRSCK\n    expected_cost = (2 * 28 * 28 * 3 * 3 * 96 * 96\n                     / resource_function.PEAK_COMPUTE[hardware])\n    self.assertAllClose(expected_cost, self.get_cost([op]))\n\n    # Memory-bound convolution.\n    op = g.get_operation_by_name(\n        \'InceptionV2/Conv2d_1a_7x7/separable_conv2d\')\n    # Memory cost = input_tensor + weight_tensor + output_tensor\n    #             = NHWC + RSCK + NHWK\n    # Note that this is a pointwise convolution with kernel 1x1.\n    expected_cost = ((112 * 112 * 24 + 24 * 64 + 112 * 112 * 64) * 4\n                     / resource_function.MEMORY_BANDWIDTH[hardware])\n    self.assertAllClose(expected_cost, self.get_cost([op]))\n\n  def testInceptionV2_TotalCost(self):\n    conv_params = {\n        \'activation_fn\': tf.nn.relu6,\n        \'weights_regularizer\': contrib_layers.l2_regularizer(0.00004),\n        \'weights_initializer\': tf.random_normal_initializer(stddev=0.03),\n        \'trainable\': True,\n        \'biases_initializer\': tf.constant_initializer(0.0),\n        \'normalizer_fn\': contrib_layers.batch_norm,\n        \'normalizer_params\': {\n            \'is_training\': False,\n            \'decay\': 0.9997,\n            \'scale\': True,\n            \'epsilon\': 0.001,\n        }\n    }\n\n    tf.reset_default_graph()\n    with slim.arg_scope([slim.layers.conv2d, slim.layers.separable_conv2d],\n                        **conv_params):\n      # Build model.\n      image = tf.zeros([1, 224, 224, 3])\n      net, _ = inception.inception_v2_base(image)\n      logits = slim.layers.fully_connected(\n          net,\n          1001,\n          activation_fn=None,\n          scope=\'logits\',\n          weights_initializer=tf.random_normal_initializer(stddev=1e-3),\n          biases_initializer=tf.constant_initializer(0.0))\n\n    # Instantiate regularizers.\n    flop_reg = flop_regularizer.GammaFlopsRegularizer(\n        [logits.op], gamma_threshold=0.5)\n    p100_reg = latency_regularizer.GammaLatencyRegularizer(\n        [logits.op], gamma_threshold=0.5, hardware=\'P100\')\n    v100_reg = latency_regularizer.GammaLatencyRegularizer(\n        [logits.op], gamma_threshold=0.5, hardware=\'V100\')\n    model_size_reg = model_size_regularizer.GammaModelSizeRegularizer(\n        [logits.op], gamma_threshold=0.5)\n\n    with self.cached_session():\n      tf.global_variables_initializer().run()\n\n    # Verify costs are expected.\n    self.assertAllClose(3.86972e+09, flop_reg.get_cost())\n    self.assertAllClose(517536.0, p100_reg.get_cost())\n    self.assertAllClose(173330.453125, v100_reg.get_cost())\n    self.assertAllClose(1.11684e+07, model_size_reg.get_cost())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/network_regularizers/model_size_regularizer.py,4,"b'""""""A NetworkRegularizer that targets the number of weights in the model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# [internal] enable type annotations\nfrom __future__ import print_function\n\nfrom morph_net.framework import batch_norm_source_op_handler\nfrom morph_net.framework import conv2d_transpose_source_op_handler as conv2d_transpose_handler\nfrom morph_net.framework import conv_source_op_handler as conv_handler\nfrom morph_net.framework import generic_regularizers\nfrom morph_net.framework import matmul_source_op_handler as matmul_handler\nfrom morph_net.framework import op_handler_decorator\nfrom morph_net.framework import op_handlers\nfrom morph_net.framework import op_regularizer_manager as orm\nfrom morph_net.network_regularizers import cost_calculator\nfrom morph_net.network_regularizers import resource_function\nimport tensorflow.compat.v1 as tf\nfrom typing import Text, Type, List\n\n\nclass GammaModelSizeRegularizer(generic_regularizers.NetworkRegularizer):\n  """"""A NetworkRegularizer that targets model size using Gamma L1.""""""\n\n  def __init__(\n      self,\n      output_boundary: List[tf.Operation],\n      gamma_threshold,\n      regularizer_decorator: Type[generic_regularizers.OpRegularizer] = None,\n      decorator_parameters=None,\n      input_boundary: List[tf.Operation] = None,\n      force_group=None,\n      regularizer_blacklist=None):\n    """"""Creates a GammaModelSizeRegularizer object.\n\n    Args:\n      output_boundary: An OpRegularizer will be created for all these\n        operations, and recursively for all ops they depend on via data\n        dependency that does not involve ops from input_boundary.\n      gamma_threshold: A float scalar, will be used as a \'gamma_threshold\' for\n        all instances GammaL1Regularizer created by this class.\n      regularizer_decorator: A string, the name of the regularizer decorators\n        to use. Supported decorators are listed in\n        op_regularizer_decorator.SUPPORTED_DECORATORS.\n      decorator_parameters: A dictionary of parameters to pass to the decorator\n        factory. To be used only with decorators that requires parameters,\n        otherwise use None.\n      input_boundary: A list of ops that represent the input boundary of the\n        subgraph being regularized (input boundary is not regularized).\n      force_group: List of regex for ops that should be force-grouped.  Each\n        regex corresponds to a separate group.  Use \'|\' operator to specify\n        multiple patterns in a single regex. See op_regularizer_manager for\n        more detail.\n      regularizer_blacklist: List of regex for ops that should not be\n        regularized. See op_regularizer_manager for more detail.\n    """"""\n    source_op_handler = batch_norm_source_op_handler.BatchNormSourceOpHandler(\n        gamma_threshold)\n    if regularizer_decorator:\n      source_op_handler = op_handler_decorator.OpHandlerDecorator(\n          source_op_handler, regularizer_decorator, decorator_parameters)\n    op_handler_dict = op_handlers.get_gamma_op_handler_dict()\n    op_handler_dict.update({\n        \'FusedBatchNorm\': source_op_handler,\n        \'FusedBatchNormV2\': source_op_handler,\n        \'FusedBatchNormV3\': source_op_handler,\n    })\n\n    self._manager = orm.OpRegularizerManager(\n        output_boundary, op_handler_dict, input_boundary=input_boundary,\n        force_group=force_group, regularizer_blacklist=regularizer_blacklist)\n    self._calculator = cost_calculator.CostCalculator(\n        self._manager, resource_function.model_size_function)\n\n  def get_regularization_term(self, ops=None):\n    return self._calculator.get_regularization_term(ops)\n\n  def get_cost(self, ops=None):\n    return self._calculator.get_cost(ops)\n\n  @property\n  def op_regularizer_manager(self):\n    return self._manager\n\n  @property\n  def name(self):\n    return \'GammaModelSize\'\n\n  @property\n  def cost_name(self):\n    return \'ModelSize\'\n\n\nclass GroupLassoModelSizeRegularizer(generic_regularizers.NetworkRegularizer):\n  """"""A NetworkRegularizer that targets model size using L1 group lasso.""""""\n\n  def __init__(\n      self,\n      output_boundary: List[tf.Operation],\n      threshold,\n      l1_fraction=0.0,\n      regularizer_decorator: Type[generic_regularizers.OpRegularizer] = None,\n      decorator_parameters=None,\n      input_boundary: List[tf.Operation] = None,\n      force_group: List[Text] = None,\n      regularizer_blacklist: List[Text] = None):\n    """"""Creates a GroupLassoModelSizeRegularizer object.\n\n    Args:\n      output_boundary: An OpRegularizer will be created for all these\n        operations, and recursively for all ops they depend on via data\n        dependency that does not involve ops from input_boundary.\n      threshold: A float scalar, will be used as a \'threshold\' for all\n        regularizer instances created by this class.\n      l1_fraction: A float scalar.  The relative weight of L1 in L1 + L2\n        regularization.\n      regularizer_decorator: A class of OpRegularizer decorator to use.\n      decorator_parameters: A dictionary of parameters to pass to the decorator\n        factory. To be used only with decorators that requires parameters,\n        otherwise use None.\n      input_boundary: A list of ops that represent the input boundary of the\n        subgraph being regularized (input boundary is not regularized).\n      force_group: List of regex for ops that should be force-grouped.  Each\n        regex corresponds to a separate group.  Use \'|\' operator to specify\n        multiple patterns in a single regex. See op_regularizer_manager for more\n        detail.\n      regularizer_blacklist: List of regex for ops that should not be\n        regularized. See op_regularizer_manager for more detail.\n    """"""\n    custom_handlers = {\n        \'Conv2D\':\n            conv_handler.ConvSourceOpHandler(threshold, l1_fraction),\n        \'Conv3D\':\n            conv_handler.ConvSourceOpHandler(threshold, l1_fraction),\n        \'Conv2DBackpropInput\':\n            conv2d_transpose_handler.Conv2DTransposeSourceOpHandler(\n                threshold, l1_fraction),\n        \'MatMul\':\n            matmul_handler.MatMulSourceOpHandler(threshold, l1_fraction)\n    }\n    if regularizer_decorator:\n      for key in custom_handlers:\n        custom_handlers[key] = op_handler_decorator.OpHandlerDecorator(\n            custom_handlers[key], regularizer_decorator, decorator_parameters)\n\n    op_handler_dict = op_handlers.get_group_lasso_op_handler_dict()\n    op_handler_dict.update(custom_handlers)\n\n    self._manager = orm.OpRegularizerManager(\n        output_boundary,\n        op_handler_dict,\n        input_boundary=input_boundary,\n        force_group=force_group,\n        regularizer_blacklist=regularizer_blacklist)\n    self._calculator = cost_calculator.CostCalculator(\n        self._manager, resource_function.model_size_function)\n\n  def get_regularization_term(self, ops=None):\n    return self._calculator.get_regularization_term(ops)\n\n  def get_cost(self, ops=None):\n    return self._calculator.get_cost(ops)\n\n  @property\n  def op_regularizer_manager(self):\n    return self._manager\n\n  @property\n  def name(self):\n    return \'GroupLassoModelSize\'\n\n  @property\n  def cost_name(self):\n    return \'ModelSize\'\n'"
morph_net/network_regularizers/model_size_regularizer_test.py,6,"b'""""""Tests for network_regularizers.model_size_regularizer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom absl.testing import parameterized\nfrom morph_net.network_regularizers import model_size_regularizer\nfrom morph_net.network_regularizers import resource_function\nfrom morph_net.testing import dummy_decorator\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import slim as contrib_slim\n\nslim = contrib_slim\n\n_coeff = resource_function.num_weights_coeff\nNUM_CHANNELS = 3\n\n\nclass GammaModelSizeDecoratedTest(parameterized.TestCase, tf.test.TestCase):\n  """"""Test op regularizer decorator with model size regularizer.""""""\n\n  def cost(self, conv):\n    with self.cached_session():\n      return self.gamma_flop_reg.get_cost(conv).eval()\n\n  def loss(self, conv):\n    with self.cached_session():\n      return self.gamma_flop_reg.get_regularization_term(conv).eval()\n\n  def get_conv(self, name):\n    return tf.get_default_graph().get_operation_by_name(name + \'/Conv2D\')\n\n  def testLossCostDecorated(self):\n    params = {\'trainable\': True, \'normalizer_fn\': slim.batch_norm,\n              \'normalizer_params\': {\'scale\': True}}\n\n    with slim.arg_scope([slim.layers.conv2d], **params):\n      image = tf.constant(0.0, shape=[1, 1, 1, NUM_CHANNELS])\n      conv1 = slim.layers.conv2d(\n          image, 2, [1, 1], padding=\'SAME\', scope=\'conv1\')\n    with self.cached_session():\n      tf.global_variables_initializer().run()\n      name_to_var = {v.op.name: v for v in tf.global_variables()}\n      gamma1 = name_to_var[\'conv1/BatchNorm/gamma\']\n      gamma1.assign([1] * 2).eval()\n\n    self.gamma_flop_reg = model_size_regularizer.GammaModelSizeRegularizer(\n        [conv1.op],\n        gamma_threshold=0.1,\n        regularizer_decorator=dummy_decorator.DummyDecorator,\n        decorator_parameters={\'scale\': 0.5})\n\n    conv = self.get_conv(\'conv1\')\n    self.assertEqual(_coeff(conv) * 3 * 1, self.loss([conv]))\n    self.assertEqual(_coeff(conv) * 2 * NUM_CHANNELS, self.cost([conv]))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/network_regularizers/resource_function.py,25,"b'""""""Resource functions for various resources (e.g. FLOPs, latency).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom morph_net.framework import op_handler_util\nfrom morph_net.network_regularizers import cost_calculator\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\n# Data sheet for K80:\n# https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/nvidia-tesla-k80-overview.pdf\n\n# Data sheet for P4:\n# https://images.nvidia.com/content/pdf/tesla/184457-Tesla-P4-Datasheet-NV-Final-Letter-Web.pdf\n\n# Data sheet for T4:\n# https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/t4-tensor-core-datasheet.pdf\n\n# Data sheet for P100:\n# https://images.nvidia.com/content/tesla/pdf/nvidia-tesla-p100-PCIe-datasheet.pdf\n\n# Data sheet for V100:\n# https://images.nvidia.com/content/technologies/volta/pdf/tesla-volta-v100-datasheet-letter-fnl-web.pdf\n\n# Data sheet for TPUv2:\n# http://learningsys.org/nips17/assets/slides/dean-nips17.pdf\nPEAK_COMPUTE = {  # GFLOP/s\n    \'K80\': 8740,\n    \'P4\': 5500,\n    \'T4\': 8100,\n    \'P100\': 9300,\n    \'V100\': 125000,\n    \'TPUv2\': 22500,\n    \'FLOP_LATENCY\': 1,  # Simulate a device with infinite memory bandwidth.\n    \'MEMORY_LATENCY\': 1e20,  # Simulate a device with infinite peak compute.\n}\nMEMORY_BANDWIDTH = {  # GB/s\n    \'K80\': 480,\n    \'P4\': 192,\n    \'T4\': 300,\n    \'P100\': 732,\n    \'V100\': 900,\n    \'TPUv2\': 300,\n    \'FLOP_LATENCY\': 1e20,\n    \'MEMORY_LATENCY\': 1,\n}\n\n\ndef flop_coeff(op):\n  """"""Computes the coefficient of number of flops associated with a convolution.\n\n  The FLOPs cost of a convolution is given by C * output_depth * input_depth,\n  where C = 2 * output_width * output_height * filter_size. The 2 is because we\n  have one multiplication and one addition for each convolution weight and\n  pixel. This function returns C.\n\n  Supported operations names are listed in cost_calculator.FLOP_OPS.\n\n  Args:\n    op: A tf.Operation of supported types.\n\n  Returns:\n    A float, the coefficient that when multiplied by the input depth and by the\n    output depth gives the number of flops needed to compute the convolution.\n\n  Raises:\n    ValueError: conv_op is not a supported tf.Operation.\n  """"""\n  if not is_flop_op(op):\n    return 0.0\n  if op.type == \'MatMul\':\n    # A MatMul is like a 1x1 conv with an output size of 1x1, so from the factor\n    # below only the 2.0 remains.\n    return 2.0\n  # Looking at the output shape makes it easy to automatically take into\n  # account strides and the type of padding.\n  def kernel_num_elements(tensor):\n    """"""Returns the number of elements of a kernel.\n\n    Args:\n      tensor: The weight tensor.\n\n    Returns:\n      Number of elements of the kernel (either float or tf.float).\n    """"""\n    num_elements = np.prod(tensor.shape.dims[1:-1]).value\n    if num_elements:\n      return num_elements\n    return tf.to_float(tf.reduce_prod(tf.shape(tensor)[1:-1]))\n\n  if op.type in (\'Conv2D\', \'DepthwiseConv2dNative\', \'Conv3D\'):\n    num_elements = kernel_num_elements(op.outputs[0])\n  elif op.type == \'Conv2DBackpropInput\':\n    # For a transposed convolution, the input and the output are swapped (as\n    # far as shapes are concerned). In other words, for a given filter shape\n    # and stride, if Conv2D maps from shapeX to shapeY, Conv2DBackpropInput\n    # maps from shapeY to shapeX. Therefore wherever we use the output shape\n    # for Conv2D, we use the input shape for Conv2DBackpropInput.\n    num_elements = kernel_num_elements(cost_calculator.get_input_activation(op))\n  else:\n    # Can only happen if elements are added to FLOP_OPS and not taken care of.\n    assert False, \'%s in cost_calculator.FLOP_OPS but not handled\' % op.type\n  # Handle dynamic shaping while keeping old code path to not break\n  # other clients.\n  return 2.0 * num_elements * _get_conv_filter_size(op)\n\n\ndef num_weights_coeff(op):\n  """"""The number of weights of a conv is C * output_depth * input_depth. Finds C.\n\n  Args:\n    op: A tf.Operation of type \'Conv2D\' or \'MatMul\'\n\n  Returns:\n    A float, the coefficient that when multiplied by the input depth and by the\n    output depth gives the number of flops needed to compute the convolution.\n\n  Raises:\n    ValueError: conv_op is not a tf.Operation of type Conv2D.\n  """"""\n  if not is_flop_op(op):\n    return 0.0\n  return (_get_conv_filter_size(op) if op.type in cost_calculator.CONV_OPS\n          else 1.0)\n\n\ndef flop_function(op, is_regularization, num_alive_inputs, num_alive_outputs,\n                  reg_inputs, reg_outputs, batch_size=1):\n  """"""Calculates FLOP cost or regularization loss for an op.\n\n  Args:\n    op: A tf.Operation.\n    is_regularization: Boolean indicating whether to calculate regularization\n      loss.  If False, calculate cost instead.\n    num_alive_inputs: Scalar Tensor indicating how many input channels are\n      considered alive.\n    num_alive_outputs: Scalar Tensor indicating how many output channels are\n      considered alive.\n    reg_inputs: Scalar Tensor which is the sum over the input regularization\n      vector.\n    reg_outputs: Scalar Tensor which is the sum over the output regularization\n      vector.\n    batch_size: Integer batch size to calculate cost/loss for.\n\n  Returns:\n    Tensor with the cost or regularization loss of the op in terms of FLOPs.\n  """"""\n  coeff = flop_coeff(op)\n  if is_regularization:\n    return _calculate_bilinear_regularization(\n        op, coeff, num_alive_inputs, num_alive_outputs, reg_inputs, reg_outputs,\n        batch_size)\n  return _calculate_bilinear_cost(\n      op, coeff, num_alive_inputs, num_alive_outputs, batch_size)\n\n\ndef memory_function(op, is_regularization, num_alive_inputs, num_alive_outputs,\n                    reg_inputs, reg_outputs, batch_size=1):\n  """"""Calculates memory cost or regularization loss for an op.\n\n  Args:\n    op: A tf.Operation.\n    is_regularization: Boolean indicating whether to calculate regularization\n      loss.  If False, calculate cost instead.\n    num_alive_inputs: Scalar Tensor indicating how many input channels are\n      considered alive.\n    num_alive_outputs: Scalar Tensor indicating how many output channels are\n      considered alive.\n    reg_inputs: Scalar Tensor which is the sum over the input regularization\n      vector.\n    reg_outputs: Scalar Tensor which is the sum over the output regularization\n      vector.\n    batch_size: Integer batch size to calculate cost/loss for.\n\n  Returns:\n    Tensor with the cost or regularization loss of the op in terms of memory.\n  """"""\n  # Separate tensors based on how their cost depends on input and output\n  # channels.\n  # 1. Input tensors: Memory size scales with input channels and batch size.\n  # 2. Output tensors: Memory size scales with output channels and batch size.\n  # 3. Weight tensors: Memory size scales with input and output channels\n  #    (bilinear).\n  input_tensors = []\n  output_tensors = []\n  bilinear_tensors = []\n  weight_tensor_index = op_handler_util.WEIGHTS_INDEX_DICT.get(op.type)\n  for i, tensor in enumerate(op.inputs):\n    if weight_tensor_index is not None and i == weight_tensor_index:\n      bilinear_tensors.append(tensor)\n    else:\n      if op.type == \'Conv2DBackpropInput\' and i == 0:\n        # This is tensor <scope>/stack:0 which just holds the output shape.\n        continue\n      if \'FusedBatchNorm\' in op.type and i > 0:\n        # Skip the gamma, beta, mean, and std.\n        continue\n      if op.type == \'Sum\' and i == 1:\n        # Skip the reduction_indices tensor for tf.reduction_sum op.\n        continue\n      input_tensors.append(tensor)\n  for i, tensor in enumerate(op.outputs):\n    if \'FusedBatchNorm\' in op.type and i > 0:\n      # Skip other batch norm outputs.\n      continue\n    output_tensors.append(tensor)\n\n  if op.type == \'ConcatV2\':\n    # For concat, the alive/regularization of the input is the same as the\n    # output, but split into multiple tensors.  For simplicity, treat the input\n    # as the output rather than using the alive/regularization of individual\n    # inputs.\n    input_tensors = output_tensors\n\n  # Normalize memory payload to batch size 1 and depth 1.  Rescale by target\n  # batch size and input/output depth to calculate actual cost/loss.\n  normalized_input_payloads = []\n  for input_tensor in input_tensors:\n    shape = _shape_with_dtype(input_tensor)\n    # Divide by batch size and input channels.\n    normalized_input_payloads.append(\n        tf.reduce_prod(shape[1:-1]) * input_tensor.dtype.size)\n  normalized_output_payloads = []\n  for output_tensor in output_tensors:\n    shape = _shape_with_dtype(output_tensor)\n    # Divide by batch size and output channels.\n    normalized_output_payloads.append(\n        tf.reduce_prod(shape[1:-1]) * output_tensor.dtype.size)\n  normalized_bilinear_payloads = []\n  for bilinear_tensor in bilinear_tensors:\n    shape = _shape_with_dtype(bilinear_tensor)\n    # Divide by input and output channels.\n    normalized_bilinear_payloads.append(\n        tf.reduce_prod(shape[:-2]) * bilinear_tensor.dtype.size)\n\n  # Rescale normalized payload to calculate cost/loss.\n  input_payloads = []\n  output_payloads = []\n  bilinear_payloads = []\n  if is_regularization:\n    for input_payload in normalized_input_payloads:\n      input_payloads.append(input_payload * batch_size * reg_inputs)\n    for output_payload in normalized_output_payloads:\n      output_payloads.append(output_payload * batch_size * reg_outputs)\n    for bilinear_payload in normalized_bilinear_payloads:\n      bilinear_payloads.append(\n          bilinear_payload * (\n              num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs))\n  else:\n    for input_payload in normalized_input_payloads:\n      input_payloads.append(\n          input_payload * batch_size * num_alive_inputs)\n    for output_payload in normalized_output_payloads:\n      output_payloads.append(\n          output_payload * batch_size * num_alive_outputs)\n    for bilinear_payload in normalized_bilinear_payloads:\n      bilinear_payloads.append(\n          bilinear_payload * num_alive_inputs * num_alive_outputs)\n\n  return tf.reduce_sum(input_payloads + output_payloads + bilinear_payloads)\n\n\ndef latency_function(op, is_regularization, num_alive_inputs, num_alive_outputs,\n                     reg_inputs, reg_outputs, peak_compute, memory_bandwidth,\n                     batch_size=1):\n  """"""Calculates latency cost or regularization loss for an op.\n\n  Calculates the compute and memory cost of the op and returns the max.  This\n  assumes ops can overlap compute and memory such that latency results from the\n  slower of the 2 constraints.\n\n  Args:\n    op: A tf.Operation.\n    is_regularization: Boolean indicating whether to calculate regularization\n      loss.  If False, calculate cost instead.\n    num_alive_inputs: Scalar Tensor indicating how many input channels are\n      considered alive.\n    num_alive_outputs: Scalar Tensor indicating how many output channels are\n      considered alive.\n    reg_inputs: Scalar Tensor which is the sum over the input regularization\n      vector.\n    reg_outputs: Scalar Tensor which is the sum over the output regularization\n      vector.\n    peak_compute: Integer peak compute of the target hardware in GFLOP/s.\n    memory_bandwidth: Integer memory bandwidth of the target hardware in GB/s.\n    batch_size: Integer batch size to calculate cost/loss for.\n\n  Returns:\n    Tensor with the cost or regularization loss of the op in terms of latency.\n  """"""\n  # Calculate compute cost for FLOP-expensive ops.\n  flop_cost = 0\n  if is_flop_op(op):\n    # This op has non-trivial compute.\n    flop_cost = flop_function(\n        op, False, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n\n  # Convert FLOP cost to compute time cost.\n  compute_cost = flop_cost / peak_compute\n\n  memory_payload = memory_function(\n      op, False, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs, batch_size)\n  # Convert memory payload cost to memory time cost.\n  memory_cost = memory_payload / memory_bandwidth\n\n  if is_regularization:\n    compute_loss = flop_function(\n        op, True, num_alive_inputs, num_alive_outputs, reg_inputs, reg_outputs,\n        batch_size) / peak_compute\n    memory_loss = memory_function(\n        op, True, num_alive_inputs, num_alive_outputs, reg_inputs, reg_outputs,\n        batch_size) / memory_bandwidth\n    return tf.cond(memory_cost > compute_cost,\n                   lambda: memory_loss,\n                   lambda: compute_loss)\n  else:\n    return tf.maximum(compute_cost, memory_cost)\n\n\ndef latency_function_factory(hardware, batch_size):\n  """"""Return latency_function with appropriate hardware platform specs.\n\n  Args:\n    hardware: String hardware platform to target for latency. Must be a key\n      from PEAK_COMPUTE and MEMORY_BANDWIDTH.\n    batch_size: Integer batch size to calculate cost/loss for.\n\n  Returns:\n    Function latency_function with target hardware specs.\n\n  Raises:\n    ValueError: If hardware not supported.\n  """"""\n  assert batch_size > 0\n  if hardware not in PEAK_COMPUTE:\n    raise ValueError(\n        \'Hardware %s must be in %s\' % (hardware, PEAK_COMPUTE.keys()))\n  # Create latency_function with hardware specifications.\n  def latency_function_for_hardware(\n      op, is_regularization, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    return latency_function(\n        op, is_regularization, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, PEAK_COMPUTE[hardware], MEMORY_BANDWIDTH[hardware],\n        batch_size)\n\n  return latency_function_for_hardware\n\n\ndef model_size_function(op, is_regularization, num_alive_inputs,\n                        num_alive_outputs, reg_inputs, reg_outputs,\n                        batch_size=1):\n  """"""Calculates model size cost or regularization loss for an op.\n\n  Args:\n    op: A tf.Operation.\n    is_regularization: Boolean indicating whether to calculate regularization\n      loss.  If False, calculate cost instead.\n    num_alive_inputs: Scalar Tensor indicating how many input channels are\n      considered alive.\n    num_alive_outputs: Scalar Tensor indicating how many output channels are\n      considered alive.\n    reg_inputs: Scalar Tensor which is the sum over the input regularization\n      vector.\n    reg_outputs: Scalar Tensor which is the sum over the output regularization\n      vector.\n    batch_size: Integer batch size to calculate cost/loss for.  Unused.\n\n  Returns:\n    Tensor with the cost or regularization loss of the op in terms of FLOPs.\n  """"""\n  del batch_size  # Unused.\n  coeff = num_weights_coeff(op)\n  if is_regularization:\n    return _calculate_bilinear_regularization(\n        op, coeff, num_alive_inputs, num_alive_outputs, reg_inputs, reg_outputs,\n        1)\n  return _calculate_bilinear_cost(\n      op, coeff, num_alive_inputs, num_alive_outputs, 1)\n\n\ndef activation_count_function(op, is_regularization, num_alive_inputs,\n                              num_alive_outputs, reg_inputs, reg_outputs,\n                              batch_size=1):\n  """"""Calculates activation cost or regularization loss for an op.\n\n  Args:\n    op: A tf.Operation.\n    is_regularization: Boolean indicating whether to calculate regularization\n      loss.  If False, calculate cost instead.\n    num_alive_inputs: Scalar Tensor indicating how many input channels are\n      considered alive.\n    num_alive_outputs: Scalar Tensor indicating how many output channels are\n      considered alive.\n    reg_inputs: Scalar Tensor which is the sum over the input regularization\n      vector.\n    reg_outputs: Scalar Tensor which is the sum over the output regularization\n      vector.\n    batch_size: Integer batch size to calculate cost/loss for.  Unused.\n\n  Returns:\n    Tensor with the cost or regularization loss of the op in terms of FLOPs.\n  """"""\n  del num_alive_inputs  # Unused.\n  del reg_inputs  # Unused.\n  del batch_size  # Unused.\n  if not is_flop_op(op):\n    return 0.0\n  if is_regularization:\n    return reg_outputs\n  return num_alive_outputs\n\n\ndef _shape_with_dtype(tensor):\n  """"""Returns the tensor shape with the same dtype as the tensor.\n\n  Args:\n    tensor: A tf.Tensor.\n\n  Returns:\n    A tf.Tensor of the tensor shape with the same dtype as tensor.\n  """"""\n  return tf.cast(tf.shape(tensor), tensor.dtype)\n\n\ndef is_flop_op(op):\n  """"""Returns True if op consumes significant FLOPs to evaluate.""""""\n  if not isinstance(op, tf.Operation):\n    raise ValueError(\'conv_op must be a tf.Operation, not %s\' % type(op))\n  return op.type in cost_calculator.FLOP_OPS\n\n\ndef _get_conv_filter_size(conv_op):\n  # Works for 2D and 3D convs where sizes of weight matrix are:\n  # 4D or 5D tensors: [kernel_size[:], inputs, outputs]\n  assert conv_op.type in cost_calculator.CONV_OPS\n  conv_weights = conv_op.inputs[1]\n  filter_shape = conv_weights.shape.as_list()[:-2]\n  return np.prod(filter_shape)\n\n\ndef _calculate_bilinear_regularization(\n    op, coeff, num_alive_inputs, num_alive_outputs, reg_inputs, reg_outputs,\n    batch_size):\n  """"""Calculates bilinear regularization term for an op.\n\n  Args:\n    op: A tf.Operation.\n    coeff: A float coefficient for the bilinear function.\n    num_alive_inputs: Scalar Tensor indicating how many input channels are\n      considered alive.\n    num_alive_outputs: Scalar Tensor indicating how many output channels are\n      considered alive.\n    reg_inputs: Scalar Tensor which is the sum over the input regularization\n      vector.\n    reg_outputs: Scalar Tensor which is the sum over the output regularization\n      vector.\n    batch_size: Integer batch size to calculate cost/loss for.\n\n  Returns:\n    Tensor with the regularization loss of the op.\n  """"""\n  if op.type == \'DepthwiseConv2dNative\':\n    # reg_inputs and reg_outputs are often identical since they should\n    # come from the same regularizer. Duplicate them for symmetry.\n    # When the input doesn\'t have a regularizer (e.g. input), only the\n    # second term is used.\n    # TODO(b1): revisit this expression after experiments.\n    return batch_size * coeff * (reg_inputs + reg_outputs)\n  else:\n    # Handle normal ops.\n    return batch_size * coeff * (\n        num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n\n\ndef _calculate_bilinear_cost(\n    op, coeff, num_alive_inputs, num_alive_outputs, batch_size):\n  """"""Calculates bilinear cost for an op.\n\n  Args:\n    op: A tf.Operation.\n    coeff: A float coefficient for the bilinear function.\n    num_alive_inputs: Scalar Tensor indicating how many input channels are\n      considered alive.\n    num_alive_outputs: Scalar Tensor indicating how many output channels are\n      considered alive.\n    batch_size: Integer batch size to calculate cost/loss for.\n\n  Returns:\n    Tensor with the cost of the op.\n  """"""\n  if op.type == \'DepthwiseConv2dNative\':\n    # num_alive_inputs may not always equals num_alive_outputs because the\n    # input (e.g. the image) may not have a gamma regularizer. In this\n    # case the computation is proportional only to num_alive_outputs.\n    return batch_size * coeff * num_alive_outputs\n  else:\n    return batch_size * coeff * num_alive_inputs * num_alive_outputs\n'"
morph_net/network_regularizers/resource_function_test.py,56,"b'""""""Tests for network_regularizers.resource_function.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom absl.testing import parameterized\nfrom morph_net.network_regularizers import resource_function\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import layers as contrib_layers\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.framework import ops\n\nlayers = contrib_layers\n\n\nclass ResourceFunctionTest(parameterized.TestCase, tf.test.TestCase):\n\n  def assertNearRelatively(self, expected, actual):\n    self.assertNear(expected, actual, expected * 1e-6)\n\n  def setUp(self):\n    super(ResourceFunctionTest, self).setUp()\n\n    self.image_shape = (1, 11, 13, 17)\n    self.image = tf.placeholder(tf.float32, shape=[1, None, None, 17])\n    net = layers.conv2d(\n        self.image, 19, [7, 5], stride=2, padding=\'SAME\', scope=\'conv1\')\n    layers.conv2d_transpose(\n        self.image, 29, [7, 5], stride=2, padding=\'SAME\', scope=\'convt2\')\n    net = tf.reduce_mean(net, axis=(1, 2))\n    layers.fully_connected(net, 23, scope=\'FC\')\n    net = layers.conv2d(\n        self.image, 10, [7, 5], stride=2, padding=\'SAME\', scope=\'conv2\')\n    layers.separable_conv2d(\n        net, None, [3, 2], depth_multiplier=1, padding=\'SAME\', scope=\'dw1\')\n\n    self.video_shape = (1, 11, 9, 13, 17)\n    self.video = tf.placeholder(tf.float32, shape=[1, None, None, None, 17])\n    net = layers.conv3d(\n        self.video, 19, [7, 3, 5], stride=2, padding=\'SAME\', scope=\'vconv1\')\n    g = tf.get_default_graph()\n    self.conv_op = g.get_operation_by_name(\'conv1/Conv2D\')\n    self.convt_op = g.get_operation_by_name(\n        \'convt2/conv2d_transpose\')\n    self.matmul_op = g.get_operation_by_name(\'FC/MatMul\')\n    self.dw_op = g.get_operation_by_name(\'dw1/depthwise\')\n    self.conv3d_op = g.get_operation_by_name(\n        \'vconv1/Conv3D\')\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19\', 1, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testConvFlopFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    flop_cost_tensor = resource_function.flop_function(\n        self.conv_op, False, num_alive_inputs, num_alive_outputs, 17, 19,\n        batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      flop_cost, _ = sess.run(\n          [flop_cost_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected FLOP cost =\n    # 2 * batch_size * feature_map_width * feature_map_height\n    # * kernel_width * kernel_height * input_depth * output_depth\n    expected_flop_cost = (\n        2 * batch_size * 6 * 7 * 7 * 5 * num_alive_inputs * num_alive_outputs)\n    self.assertEqual(expected_flop_cost, flop_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut19\',\n       1, 17, 19, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testConvFlopFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    flop_loss_tensor = resource_function.flop_function(\n        self.conv_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      flop_loss, _ = sess.run(\n          [flop_loss_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected FLOP regularization loss =\n    # 2 * batch_size * feature_map_width * feature_map_height\n    # * kernel_width * kernel_height\n    # * (num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n    expected_flop_loss = (\n        2 * batch_size * 6 * 7 * 7 * 5 * (\n            num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs))\n    self.assertEqual(expected_flop_loss, flop_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19\', 1, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testConvMemoryFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    memory_cost_tensor = resource_function.memory_function(\n        self.conv_op, False, num_alive_inputs, num_alive_outputs, 17, 19,\n        batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      memory_cost, _ = sess.run(\n          [memory_cost_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected memory cost = input_feature + weights + output_feature =\n    # (batch_size * feature_map_width * feature_map_height * num_alive_inputs\n    # + kernel_width * kernel_height * num_alive_inputs * num_alive_outputs\n    # + batch_size * feature_map_width * feature_map_height * num_alive_outputs)\n    # * dtype.size\n    expected_memory_cost = (\n        batch_size * 11 * 13 * num_alive_inputs\n        + 7 * 5 * num_alive_inputs * num_alive_outputs\n        + batch_size * 6 * 7 * num_alive_outputs) * self.image.dtype.size\n    self.assertEqual(expected_memory_cost, memory_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut19\',\n       1, 17, 19, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testConvMemoryFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    memory_loss_tensor = resource_function.memory_function(\n        self.conv_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      memory_loss, _ = sess.run(\n          [memory_loss_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected memory loss = input_feature + weights + output_feature =\n    # (batch_size * feature_map_width * feature_map_height * num_alive_inputs\n    # + kernel_width * kernel_height * (\n    #   num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n    # + batch_size * feature_map_width * feature_map_height * num_alive_outputs)\n    # * dtype.size\n    expected_memory_loss = (\n        batch_size * 11 * 13 * reg_inputs\n        + 7 * 5 * (\n            num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n        + batch_size * 6 * 7 * reg_outputs) * self.image.dtype.size\n    self.assertEqual(expected_memory_loss, memory_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19\', 1, 17, 19, 1, 1),\n      (\'_BatchSize32_AliveIn4_AliveOut9_ComputeBound\', 32, 4, 9, 1000, 2000),\n      (\'_BatchSize32_AliveIn4_AliveOut9_MemoryBound\', 32, 4, 9, 1000, 20))\n  def testConvLatencyFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs, peak_compute,\n      memory_bandwidth):\n    latency_cost_tensor = resource_function.latency_function(\n        self.conv_op, False, num_alive_inputs, num_alive_outputs, 17, 19,\n        peak_compute, memory_bandwidth, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      latency_cost, _ = sess.run(\n          [latency_cost_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected latency cost = max(compute_cost, memory_cost)\n    expected_compute_cost = (\n        2 * batch_size * 6 * 7 * 7 * 5 * num_alive_inputs * num_alive_outputs\n        / peak_compute)\n    expected_memory_cost = (\n        (batch_size * 11 * 13 * num_alive_inputs\n         + 7 * 5 * num_alive_inputs * num_alive_outputs\n         + batch_size * 6 * 7 * num_alive_outputs)\n        * self.image.dtype.size / memory_bandwidth)\n    expected_latency_cost = max(expected_compute_cost, expected_memory_cost)\n    self.assertNearRelatively(expected_latency_cost, latency_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut19\',\n       1, 17, 19, 17, 19, 1, 1),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7_ComputeBound\',\n       32, 4, 9, 3, 7, 1000, 2000),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7_MemoryBound\',\n       32, 4, 9, 3, 7, 1000, 20))\n  def testConvLatencyFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs, peak_compute, memory_bandwidth):\n    latency_loss_tensor = resource_function.latency_function(\n        self.conv_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, peak_compute, memory_bandwidth, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      latency_loss, _ = sess.run(\n          [latency_loss_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected latency loss = max(compute_loss, memory_loss)\n    expected_compute_cost = (\n        2 * batch_size * 6 * 7 * 7 * 5 * num_alive_inputs * num_alive_outputs\n        / peak_compute)\n    expected_memory_cost = (\n        (batch_size * 11 * 13 * num_alive_inputs\n         + 7 * 5 * num_alive_inputs * num_alive_outputs\n         + batch_size * 6 * 7 * num_alive_outputs)\n        * self.image.dtype.size / memory_bandwidth)\n    expected_compute_loss = (\n        2 * batch_size * 6 * 7 * 7 * 5 * (\n            num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n        / peak_compute)\n    expected_memory_loss = (\n        (batch_size * 11 * 13 * reg_inputs\n         + 7 * 5 * (\n             num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n         + batch_size * 6 * 7 * reg_outputs)\n        * self.image.dtype.size / memory_bandwidth)\n    if expected_memory_cost > expected_compute_cost:\n      expected_latency_loss = expected_memory_loss\n    else:\n      expected_latency_loss = expected_compute_loss\n    self.assertNearRelatively(expected_latency_loss, latency_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19\', 1, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testConvModelSizeFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    model_size_cost = resource_function.model_size_function(\n        self.conv_op, False, num_alive_inputs, num_alive_outputs, 17, 19,\n        batch_size)\n\n    # Expected model size cost =\n    # kernel_width * kernel_height * input_depth * output_depth\n    expected_model_size_cost = 7 * 5 * num_alive_inputs * num_alive_outputs\n    self.assertEqual(expected_model_size_cost, model_size_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut19\',\n       1, 17, 19, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testConvModelSizeFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    model_size_loss = resource_function.model_size_function(\n        self.conv_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n\n    # Expected model size regularization loss =\n    # kernel_width * kernel_height\n    # * (num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n    expected_model_size_loss = (\n        7 * 5 * (\n            num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs))\n    self.assertEqual(expected_model_size_loss, model_size_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19\', 1, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testConvActivationCountFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    activation_count_cost = resource_function.activation_count_function(\n        self.conv_op, False, num_alive_inputs, num_alive_outputs, 17, 19,\n        batch_size)\n\n    # Expected activation count cost = output_depth\n    expected_activation_count_cost = num_alive_outputs\n    self.assertEqual(expected_activation_count_cost, activation_count_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut19\',\n       1, 17, 19, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testConvActivationCountFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    activation_count_loss = resource_function.activation_count_function(\n        self.conv_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n\n    # Expected model size regularization loss = reg_outputs\n    expected_activation_count_loss = reg_outputs\n    self.assertEqual(expected_activation_count_loss, activation_count_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut29\', 1, 17, 29),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testConvTransposeFlopFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    flop_cost_tensor = resource_function.flop_function(\n        self.convt_op, False, num_alive_inputs, num_alive_outputs, 17, 29,\n        batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      flop_cost, _ = sess.run(\n          [flop_cost_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected FLOP cost =\n    # 2 * batch_size * feature_map_width * feature_map_height\n    # * kernel_width * kernel_height * input_depth * output_depth\n    expected_flop_cost = (\n        2 * batch_size * 11 * 13 * 7 * 5 * num_alive_inputs * num_alive_outputs)\n    self.assertEqual(expected_flop_cost, flop_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut29\',\n       1, 17, 19, 17, 29),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testConvTransposeFlopFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    flop_loss_tensor = resource_function.flop_function(\n        self.convt_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      flop_loss, _ = sess.run(\n          [flop_loss_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected FLOP regularization loss =\n    # 2 * batch_size * feature_map_width * feature_map_height\n    # * kernel_width * kernel_height\n    # * (num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n    expected_flop_loss = (\n        2 * batch_size * 11 * 13 * 7 * 5 * (\n            num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs))\n    self.assertEqual(expected_flop_loss, flop_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut29\', 1, 17, 29),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testConvTransposeMemoryFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    memory_cost_tensor = resource_function.memory_function(\n        self.convt_op, False, num_alive_inputs, num_alive_outputs, 17, 29,\n        batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      memory_cost, _ = sess.run(\n          [memory_cost_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected memory cost = input_feature + weights + output_feature =\n    # (batch_size * feature_map_width * feature_map_height * num_alive_inputs\n    # + kernel_width * kernel_height * num_alive_inputs * num_alive_outputs\n    # + batch_size * feature_map_width * feature_map_height * num_alive_outputs)\n    # * dtype.size\n    expected_memory_cost = (\n        batch_size * 11 * 13 * num_alive_inputs\n        + 7 * 5 * num_alive_inputs * num_alive_outputs\n        + batch_size * 22 * 26 * num_alive_outputs) * self.image.dtype.size\n    self.assertEqual(expected_memory_cost, memory_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut29\',\n       1, 17, 29, 17, 29),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testConvTransposeMemoryFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    memory_loss_tensor = resource_function.memory_function(\n        self.convt_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      memory_loss, _ = sess.run(\n          [memory_loss_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected memory loss = input_feature + weights + output_feature =\n    # (batch_size * feature_map_width * feature_map_height * num_alive_inputs\n    # + kernel_width * kernel_height * (\n    #   num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n    # + batch_size * feature_map_width * feature_map_height * num_alive_outputs)\n    # * dtype.size\n    expected_memory_loss = (\n        batch_size * 11 * 13 * reg_inputs\n        + 7 * 5 * (\n            num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n        + batch_size * 22 * 26 * reg_outputs) * self.image.dtype.size\n    self.assertEqual(expected_memory_loss, memory_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut29\', 1, 17, 29, 1, 1),\n      (\'_BatchSize32_AliveIn4_AliveOut9_ComputeBound\', 32, 4, 9, 1000, 2000),\n      (\'_BatchSize32_AliveIn4_AliveOut9_MemoryBound\', 32, 4, 9, 1000, 20))\n  def testConvTransposeLatencyFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs, peak_compute,\n      memory_bandwidth):\n    latency_cost_tensor = resource_function.latency_function(\n        self.convt_op, False, num_alive_inputs, num_alive_outputs, 17, 29,\n        peak_compute, memory_bandwidth, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      latency_cost, _ = sess.run(\n          [latency_cost_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected latency cost = max(compute_cost, memory_cost)\n    expected_compute_cost = (\n        2 * batch_size * 11 * 13 * 7 * 5 * num_alive_inputs * num_alive_outputs\n        / peak_compute)\n    expected_memory_cost = (\n        (batch_size * 11 * 13 * num_alive_inputs\n         + 7 * 5 * num_alive_inputs * num_alive_outputs\n         + batch_size * 22 * 26 * num_alive_outputs)\n        * self.image.dtype.size / memory_bandwidth)\n    expected_latency_cost = max(expected_compute_cost, expected_memory_cost)\n    self.assertNearRelatively(expected_latency_cost, latency_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut29\',\n       1, 17, 19, 17, 29, 1, 1),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7_ComputeBound\',\n       32, 4, 9, 3, 7, 1000, 2000),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7_MemoryBound\',\n       32, 4, 9, 3, 7, 1000, 20))\n  def testConvTransposeLatencyFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs, peak_compute, memory_bandwidth):\n    latency_loss_tensor = resource_function.latency_function(\n        self.convt_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, peak_compute, memory_bandwidth, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      latency_loss, _ = sess.run(\n          [latency_loss_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected latency loss = max(compute_loss, memory_loss)\n    expected_compute_cost = (\n        2 * batch_size * 11 * 13 * 7 * 5 * num_alive_inputs * num_alive_outputs\n        / peak_compute)\n    expected_memory_cost = (\n        (batch_size * 11 * 13 * num_alive_inputs\n         + 7 * 5 * num_alive_inputs * num_alive_outputs\n         + batch_size * 22 * 26 * num_alive_outputs)\n        * self.image.dtype.size / memory_bandwidth)\n    expected_compute_loss = (\n        2 * batch_size * 11 * 13 * 7 * 5 * (\n            num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n        / peak_compute)\n    expected_memory_loss = (\n        (batch_size * 11 * 13 * reg_inputs\n         + 7 * 5 * (\n             num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n         + batch_size * 22 * 26 * reg_outputs)\n        * self.image.dtype.size / memory_bandwidth)\n    if expected_memory_cost > expected_compute_cost:\n      expected_latency_loss = expected_memory_loss\n    else:\n      expected_latency_loss = expected_compute_loss\n    self.assertNearRelatively(expected_latency_loss, latency_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut29\', 1, 17, 29),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testConvTransposeModelSizeFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    model_size_cost = resource_function.model_size_function(\n        self.convt_op, False, num_alive_inputs, num_alive_outputs, 17, 29,\n        batch_size)\n\n    # Expected model size cost =\n    # kernel_width * kernel_height * input_depth * output_depth\n    expected_model_size_cost = 7 * 5 * num_alive_inputs * num_alive_outputs\n    self.assertEqual(expected_model_size_cost, model_size_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut29\',\n       1, 17, 19, 17, 29),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testConvTransposeModelSizeFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    model_size_loss = resource_function.model_size_function(\n        self.convt_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n\n    # Expected FLOP regularization loss =\n    # kernel_width * kernel_height\n    # * (num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n    expected_model_size_loss = (\n        7 * 5 * (\n            num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs))\n    self.assertEqual(expected_model_size_loss, model_size_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut29\', 1, 17, 29),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testConvTransposeActivationCountFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    activation_count_cost = resource_function.activation_count_function(\n        self.convt_op, False, num_alive_inputs, num_alive_outputs, 17, 29,\n        batch_size)\n\n    # Expected model size cost = output_depth\n    expected_activation_count_cost = num_alive_outputs\n    self.assertEqual(expected_activation_count_cost, activation_count_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut29\',\n       1, 17, 19, 17, 29),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testConvTransposeActivationCountFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    activation_count_loss = resource_function.activation_count_function(\n        self.convt_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n\n    # Expected FLOP regularization loss = reg_outputs\n    expected_activation_count_loss = reg_outputs\n    self.assertEqual(expected_activation_count_loss, activation_count_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19\', 1, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testMatMulFlopFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    flop_cost = resource_function.flop_function(\n        self.matmul_op, False, num_alive_inputs, num_alive_outputs, 17, 19,\n        batch_size)\n\n    # Expected FLOP cost =\n    # 2 * batch_size * input_depth * output_depth\n    expected_flop_cost = 2 * batch_size * num_alive_inputs * num_alive_outputs\n    self.assertEqual(expected_flop_cost, flop_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut19\',\n       1, 17, 19, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testMatMulFlopFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    flop_loss = resource_function.flop_function(\n        self.matmul_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n\n    # Expected FLOP regularization loss =\n    # 2 * batch_size\n    # * (num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n    expected_flop_loss = (\n        2 * batch_size * (\n            num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs))\n    self.assertEqual(expected_flop_loss, flop_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19\', 1, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testMatMulMemoryFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    memory_cost_tensor = resource_function.memory_function(\n        self.matmul_op, False, num_alive_inputs, num_alive_outputs, 17, 19,\n        batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      memory_cost, _ = sess.run(\n          [memory_cost_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected memory cost = input_feature + weights + output_feature =\n    # (batch_size * num_alive_inputs\n    # + num_alive_inputs * num_alive_outputs\n    # + batch_size * num_alive_outputs) * dtype.size\n    expected_memory_cost = (\n        batch_size * num_alive_inputs\n        + num_alive_inputs * num_alive_outputs\n        + batch_size * num_alive_outputs) * self.image.dtype.size\n    self.assertEqual(expected_memory_cost, memory_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut19\',\n       1, 17, 19, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testMatMulMemoryFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    memory_loss_tensor = resource_function.memory_function(\n        self.matmul_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      memory_loss, _ = sess.run(\n          [memory_loss_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected memory loss = input_feature + weights + output_feature =\n    # (batch_size * feature_map_width * feature_map_height * num_alive_inputs\n    # + kernel_width * kernel_height * (\n    #   num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n    # + batch_size * feature_map_width * feature_map_height * num_alive_outputs)\n    # * dtype.size\n    expected_memory_loss = (\n        batch_size * reg_inputs\n        + (num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n        + batch_size * reg_outputs) * self.image.dtype.size\n    self.assertEqual(expected_memory_loss, memory_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19\', 1, 17, 19, 1, 1),\n      (\'_BatchSize32_AliveIn4_AliveOut9_ComputeBound\', 32, 4, 9, 1000, 2000),\n      (\'_BatchSize32_AliveIn4_AliveOut9_MemoryBound\', 32, 4, 9, 1000, 20))\n  def testMatMulLatencyFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs, peak_compute,\n      memory_bandwidth):\n    latency_cost_tensor = resource_function.latency_function(\n        self.matmul_op, False, num_alive_inputs, num_alive_outputs, 17, 19,\n        peak_compute, memory_bandwidth, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      latency_cost, _ = sess.run(\n          [latency_cost_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected latency cost = max(compute_cost, memory_cost)\n    expected_compute_cost = (\n        2 * batch_size * num_alive_inputs * num_alive_outputs / peak_compute)\n    expected_memory_cost = (\n        (batch_size * num_alive_inputs\n         + num_alive_inputs * num_alive_outputs\n         + batch_size * num_alive_outputs)\n        * self.image.dtype.size / memory_bandwidth)\n    expected_latency_cost = max(expected_compute_cost, expected_memory_cost)\n    self.assertNearRelatively(expected_latency_cost, latency_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut19\',\n       1, 17, 19, 17, 19, 1, 1),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7_ComputeBound\',\n       32, 4, 9, 3, 7, 1000, 2000),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7_MemoryBound\',\n       32, 4, 9, 3, 7, 1000, 20))\n  def testMatMulLatencyFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs, peak_compute, memory_bandwidth):\n    latency_loss_tensor = resource_function.latency_function(\n        self.matmul_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, peak_compute, memory_bandwidth, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      latency_loss, _ = sess.run(\n          [latency_loss_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected latency loss = max(compute_loss, memory_loss)\n    expected_compute_cost = (\n        2 * batch_size * num_alive_inputs * num_alive_outputs / peak_compute)\n    expected_memory_cost = (\n        (batch_size * num_alive_inputs\n         + num_alive_inputs * num_alive_outputs\n         + batch_size * num_alive_outputs)\n        * self.image.dtype.size / memory_bandwidth)\n    expected_compute_loss = (\n        2 * batch_size * (\n            num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n        / peak_compute)\n    expected_memory_loss = (\n        (batch_size * reg_inputs\n         + (num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n         + batch_size * reg_outputs)\n        * self.image.dtype.size / memory_bandwidth)\n    if expected_memory_cost > expected_compute_cost:\n      expected_latency_loss = expected_memory_loss\n    else:\n      expected_latency_loss = expected_compute_loss\n    self.assertNearRelatively(expected_latency_loss, latency_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19\', 1, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testMatMulModelSizeFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    model_size_cost = resource_function.model_size_function(\n        self.matmul_op, False, num_alive_inputs, num_alive_outputs, 17, 19,\n        batch_size)\n\n    # Expected model size cost = input_depth * output_depth\n    expected_model_size_cost = num_alive_inputs * num_alive_outputs\n    self.assertEqual(expected_model_size_cost, model_size_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut19\',\n       1, 17, 19, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testMatMulModelSizeFlopFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    model_size_loss = resource_function.model_size_function(\n        self.matmul_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n\n    # Expected model size regularization loss =\n    # num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs\n    expected_model_size_loss = (\n        num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n    self.assertEqual(expected_model_size_loss, model_size_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19\', 1, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testMatMulActivationCountFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    activation_count_cost = resource_function.activation_count_function(\n        self.matmul_op, False, num_alive_inputs, num_alive_outputs, 17, 19,\n        batch_size)\n\n    # Expected model size cost = output_depth\n    expected_activation_count_cost = num_alive_outputs\n    self.assertEqual(expected_activation_count_cost, activation_count_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveOut19_RegIn17_RegOut19\',\n       1, 17, 19, 17, 19),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testMatMulActivationCountFlopFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    activation_count_loss = resource_function.activation_count_function(\n        self.matmul_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n\n    # Expected model size regularization loss = reg_outputs\n    expected_activation_count_loss = reg_outputs\n    self.assertEqual(expected_activation_count_loss, activation_count_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn10_AliveOut10\', 1, 10, 10),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testDepthwiseConvFlopFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    flop_cost_tensor = resource_function.flop_function(\n        self.dw_op, False, num_alive_inputs, num_alive_outputs, 10, 10,\n        batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      flop_cost, _ = sess.run(\n          [flop_cost_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected FLOP cost =\n    # 2 * batch_size * feature_map_width * feature_map_height\n    # * kernel_width * kernel_height * output_depth\n    expected_flop_cost = (\n        2 * batch_size * 6 * 7 * 3 * 2 * num_alive_outputs)\n    self.assertEqual(expected_flop_cost, flop_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn10_AliveOut10_RegIn10_RegOut10\',\n       1, 10, 10, 10, 10),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testDepthwiseConvFlopFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    flop_loss_tensor = resource_function.flop_function(\n        self.dw_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      flop_loss, _ = sess.run(\n          [flop_loss_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected FLOP regularization loss =\n    # 2 * batch_size * feature_map_width * feature_map_height\n    # * kernel_width * kernel_height * (reg_inputs + reg_outputs)\n    expected_flop_loss = (\n        2 * batch_size * 6 * 7 * 3 * 2 * (reg_inputs + reg_outputs))\n    self.assertEqual(expected_flop_loss, flop_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn10_AliveOut10\', 1, 10, 10),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testDepthwiseConvMemoryFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    memory_cost_tensor = resource_function.memory_function(\n        self.conv_op, False, num_alive_inputs, num_alive_outputs, 10, 10,\n        batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      memory_cost, _ = sess.run(\n          [memory_cost_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected memory cost = input_feature + weights + output_feature =\n    # (batch_size * feature_map_width * feature_map_height * num_alive_inputs\n    # + kernel_width * kernel_height * num_alive_inputs * num_alive_outputs\n    # + batch_size * feature_map_width * feature_map_height * num_alive_outputs)\n    # * dtype.size\n    expected_memory_cost = (\n        batch_size * 11 * 13 * num_alive_inputs\n        + 7 * 5 * num_alive_inputs * num_alive_outputs\n        + batch_size * 6 * 7 * num_alive_outputs) * self.image.dtype.size\n    self.assertEqual(expected_memory_cost, memory_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn10_AliveOut10_RegIn10_RegOut10\',\n       1, 10, 10, 10, 10),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testDepthwiseConvMemoryFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    memory_loss_tensor = resource_function.memory_function(\n        self.conv_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      memory_loss, _ = sess.run(\n          [memory_loss_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected memory loss = input_feature + weights + output_feature =\n    # (batch_size * feature_map_width * feature_map_height * num_alive_inputs\n    # + kernel_width * kernel_height * (\n    #   num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n    # + batch_size * feature_map_width * feature_map_height * num_alive_outputs)\n    # * dtype.size\n    expected_memory_loss = (\n        batch_size * 11 * 13 * reg_inputs\n        + 7 * 5 * (\n            num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n        + batch_size * 6 * 7 * reg_outputs) * self.image.dtype.size\n    self.assertEqual(expected_memory_loss, memory_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn10_AliveOut10\', 1, 10, 10, 1, 1),\n      (\'_BatchSize32_AliveIn4_AliveOut9_ComputeBound\', 32, 4, 9, 1000, 2000),\n      (\'_BatchSize32_AliveIn4_AliveOut9_MemoryBound\', 32, 4, 9, 1000, 20))\n  def testDepthwiseConvLatencyFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs, peak_compute,\n      memory_bandwidth):\n    latency_cost_tensor = resource_function.latency_function(\n        self.conv_op, False, num_alive_inputs, num_alive_outputs, 10, 10,\n        peak_compute, memory_bandwidth, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      latency_cost, _ = sess.run(\n          [latency_cost_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected latency cost = max(compute_cost, memory_cost)\n    expected_compute_cost = (\n        2 * batch_size * 6 * 7 * 7 * 5 * num_alive_inputs * num_alive_outputs\n        / peak_compute)\n    expected_memory_cost = (\n        (batch_size * 11 * 13 * num_alive_inputs\n         + 7 * 5 * num_alive_inputs * num_alive_outputs\n         + batch_size * 6 * 7 * num_alive_outputs)\n        * self.image.dtype.size / memory_bandwidth)\n    expected_latency_cost = max(expected_compute_cost, expected_memory_cost)\n    self.assertNearRelatively(expected_latency_cost, latency_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn10_AliveOut10_RegIn10_RegOut10\',\n       1, 10, 10, 10, 10, 1, 1),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7_ComputeBound\',\n       32, 4, 9, 3, 7, 1000, 2000),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7_MemoryBound\',\n       32, 4, 9, 3, 7, 1000, 20))\n  def testDepthwiseConvLatencyFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs, peak_compute, memory_bandwidth):\n    latency_loss_tensor = resource_function.latency_function(\n        self.conv_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, peak_compute, memory_bandwidth, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      latency_loss, _ = sess.run(\n          [latency_loss_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected latency loss = max(compute_loss, memory_loss)\n    expected_compute_cost = (\n        2 * batch_size * 6 * 7 * 7 * 5 * num_alive_inputs * num_alive_outputs\n        / peak_compute)\n    expected_memory_cost = (\n        (batch_size * 11 * 13 * num_alive_inputs\n         + 7 * 5 * num_alive_inputs * num_alive_outputs\n         + batch_size * 6 * 7 * num_alive_outputs)\n        * self.image.dtype.size / memory_bandwidth)\n    expected_compute_loss = (\n        2 * batch_size * 6 * 7 * 7 * 5 * (\n            num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n        / peak_compute)\n    expected_memory_loss = (\n        (batch_size * 11 * 13 * reg_inputs\n         + 7 * 5 * (\n             num_alive_inputs * reg_outputs + num_alive_outputs * reg_inputs)\n         + batch_size * 6 * 7 * reg_outputs)\n        * self.image.dtype.size / memory_bandwidth)\n    if expected_memory_cost > expected_compute_cost:\n      expected_latency_loss = expected_memory_loss\n    else:\n      expected_latency_loss = expected_compute_loss\n    self.assertNearRelatively(expected_latency_loss, latency_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn10_AliveOut10\', 1, 10, 10),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testDepthwiseConvModelSizeFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    model_size_cost = resource_function.model_size_function(\n        self.dw_op, False, num_alive_inputs, num_alive_outputs, 10, 10,\n        batch_size)\n\n    # Expected model size cost =\n    # kernel_width * kernel_height * output_depth\n    expected_model_size_cost = 3 * 2 * num_alive_outputs\n    self.assertEqual(expected_model_size_cost, model_size_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn10_AliveOut10_RegIn10_RegOut10\',\n       1, 10, 10, 10, 10),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testDepthwiseConvModelSizeFlopFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    model_size_loss = resource_function.model_size_function(\n        self.dw_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n\n    # Expected model size regularization loss =\n    # kernel_width * kernel_height * (reg_inputs + reg_outputs)\n    expected_model_size_loss = 3 * 2 * (reg_inputs + reg_outputs)\n    self.assertEqual(expected_model_size_loss, model_size_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn10_AliveOut10\', 1, 10, 10),\n      (\'_BatchSize32_AliveIn4_AliveOut9\', 32, 4, 9))\n  def testDepthwiseConvActivationCountFunction_Cost(\n      self, batch_size, num_alive_inputs, num_alive_outputs):\n    activation_count_cost = resource_function.activation_count_function(\n        self.dw_op, False, num_alive_inputs, num_alive_outputs, 10, 10,\n        batch_size)\n\n    # Expected model size cost = output_depth\n    expected_activation_count_cost = num_alive_outputs\n    self.assertEqual(expected_activation_count_cost, activation_count_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn10_AliveOut10_RegIn10_RegOut10\',\n       1, 10, 10, 10, 10),\n      (\'_BatchSize32_AliveIn4_AliveOut9_RegIn3_RegOut7\',\n       32, 4, 9, 3, 7))\n  def testDepthwiseConvActivationCountFlopFunction_Regularization(\n      self, batch_size, num_alive_inputs, num_alive_outputs, reg_inputs,\n      reg_outputs):\n    activation_count_loss = resource_function.activation_count_function(\n        self.dw_op, True, num_alive_inputs, num_alive_outputs, reg_inputs,\n        reg_outputs, batch_size)\n\n    # Expected model size regularization loss = reg_outputs\n    expected_activation_count_loss = reg_outputs\n    self.assertEqual(expected_activation_count_loss, activation_count_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn19_AliveIn11\', 1, 19, 11),\n      (\'_BatchSize32_AliveIn15_AliveIn7\', 32, 15, 7))\n  def testConcatFlopFunction_Cost(\n      self, batch_size, num_alive_inputs3, num_alive_inputs4):\n    conv3 = layers.conv2d(\n        self.image, 19, [7, 5], stride=2, padding=\'SAME\', scope=\'conv3\')\n    conv4 = layers.conv2d(\n        self.image, 11, [3, 7], stride=2, padding=\'SAME\', scope=\'conv4\')\n    concat = tf.concat([conv3, conv4], axis=3)\n    flop_cost = resource_function.flop_function(\n        concat.op, False, num_alive_inputs3 + num_alive_inputs4,\n        num_alive_inputs3 + num_alive_inputs4, 19, 11, batch_size)\n\n    expected_flop_cost = 0\n    self.assertEqual(expected_flop_cost, flop_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveIn19_RegIn17_RegIn19\',\n       1, 19, 11, 19, 11),\n      (\'_BatchSize32_AliveIn4_AliveIn9_RegIn3_RegIn7\',\n       32, 15, 7, 12, 6))\n  def testConcatFlopFunction_Regularization(\n      self, batch_size, num_alive_inputs3, num_alive_inputs4, reg_inputs3,\n      reg_inputs4):\n    conv3 = layers.conv2d(\n        self.image, 19, [7, 5], stride=2, padding=\'SAME\', scope=\'conv3\')\n    conv4 = layers.conv2d(\n        self.image, 11, [3, 7], stride=2, padding=\'SAME\', scope=\'conv4\')\n    concat = tf.concat([conv3, conv4], axis=3)\n    flop_loss = resource_function.flop_function(\n        concat.op, True, num_alive_inputs3 + num_alive_inputs4,\n        num_alive_inputs3 + num_alive_inputs4, reg_inputs3 + reg_inputs4,\n        reg_inputs3 + reg_inputs4, batch_size)\n\n    expected_flop_loss = 0\n    self.assertEqual(expected_flop_loss, flop_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn19_AliveIn11\', 1, 19, 11),\n      (\'_BatchSize32_AliveIn15_AliveIn7\', 32, 15, 7))\n  def testConcatMemoryFunction_Cost(\n      self, batch_size, num_alive_inputs3, num_alive_inputs4):\n    conv3 = layers.conv2d(\n        self.image, 19, [7, 5], stride=2, padding=\'SAME\', scope=\'conv3\')\n    conv4 = layers.conv2d(\n        self.image, 11, [3, 7], stride=2, padding=\'SAME\', scope=\'conv4\')\n    concat = tf.concat([conv3, conv4], axis=3)\n    memory_cost_tensor = resource_function.memory_function(\n        concat.op, False, num_alive_inputs3 + num_alive_inputs4,\n        num_alive_inputs3 + num_alive_inputs4, 19, 11, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      memory_cost, _ = sess.run(\n          [memory_cost_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected memory cost = input_feature3 + input_feature4 + output_feature =\n    # (batch_size * feature_map_width * feature_map_height * num_alive_inputs3\n    # + batch_size * feature_map_width * feature_map_height * num_alive_inputs4\n    # + batch_size * feature_map_width * feature_map_height * num_alive_outputs)\n    # * dtype.size\n    expected_memory_cost = (\n        (batch_size * 6 * 7 * num_alive_inputs3\n         + batch_size * 6 * 7 * num_alive_inputs4\n         + batch_size * 6 * 7 * (num_alive_inputs3 + num_alive_inputs4))\n        * self.image.dtype.size)\n    self.assertEqual(expected_memory_cost, memory_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn19_AliveIn11_RegIn19_RegIn11\', 1, 19, 11, 19, 11),\n      (\'_BatchSize32_AliveIn15_AliveIn7_RegIn12_RegIn6\', 32, 15, 7, 12, 6))\n  def testConcatMemoryFunction_Regularization(\n      self, batch_size, num_alive_inputs3, num_alive_inputs4, reg_inputs3,\n      reg_inputs4):\n    conv3 = layers.conv2d(\n        self.image, 19, [7, 5], stride=2, padding=\'SAME\', scope=\'conv3\')\n    conv4 = layers.conv2d(\n        self.image, 11, [3, 7], stride=2, padding=\'SAME\', scope=\'conv4\')\n    concat = tf.concat([conv3, conv4], axis=3)\n    memory_loss_tensor = resource_function.memory_function(\n        concat.op, True, num_alive_inputs3 + num_alive_inputs4,\n        num_alive_inputs3 + num_alive_inputs4, reg_inputs3 + reg_inputs4,\n        reg_inputs3 + reg_inputs4, batch_size)\n    with self.session() as sess:\n      sess.run(tf.global_variables_initializer())\n      memory_loss, _ = sess.run(\n          [memory_loss_tensor, self.image],\n          feed_dict={self.image: np.zeros(self.image_shape)})\n\n    # Expected memory cost = input_feature3 + input_feature4 + output_feature =\n    # (batch_size * feature_map_width * feature_map_height * reg_inputs3\n    # + batch_size * feature_map_width * feature_map_height * reg_inputs4\n    # + batch_size * feature_map_width * feature_map_height *\n    # (reg_inputs3 + reg_inputs4)) * dtype.size\n    expected_memory_loss = (\n        (batch_size * 6 * 7 * reg_inputs3\n         + batch_size * 6 * 7 * reg_inputs4\n         + batch_size * 6 * 7 * (reg_inputs3 + reg_inputs4))\n        * self.image.dtype.size)\n    self.assertEqual(expected_memory_loss, memory_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn19_AliveIn11\', 1, 19, 11),\n      (\'_BatchSize32_AliveIn15_AliveIn7\', 32, 15, 7))\n  def testConcatModelSizeFunction_Cost(\n      self, batch_size, num_alive_inputs3, num_alive_inputs4):\n    conv3 = layers.conv2d(\n        self.image, 19, [7, 5], stride=2, padding=\'SAME\', scope=\'conv3\')\n    conv4 = layers.conv2d(\n        self.image, 11, [3, 7], stride=2, padding=\'SAME\', scope=\'conv4\')\n    concat = tf.concat([conv3, conv4], axis=3)\n    model_size_cost = resource_function.model_size_function(\n        concat.op, False, num_alive_inputs3 + num_alive_inputs4,\n        num_alive_inputs3 + num_alive_inputs4, 19, 11, batch_size)\n\n    expected_model_size_cost = 0\n    self.assertEqual(expected_model_size_cost, model_size_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveIn19_RegIn17_RegIn19\',\n       1, 19, 11, 19, 11),\n      (\'_BatchSize32_AliveIn4_AliveIn9_RegIn3_RegIn7\',\n       32, 15, 7, 12, 6))\n  def testConcatModelSizeFunction_Regularization(\n      self, batch_size, num_alive_inputs3, num_alive_inputs4, reg_inputs3,\n      reg_inputs4):\n    conv3 = layers.conv2d(\n        self.image, 19, [7, 5], stride=2, padding=\'SAME\', scope=\'conv3\')\n    conv4 = layers.conv2d(\n        self.image, 11, [3, 7], stride=2, padding=\'SAME\', scope=\'conv4\')\n    concat = tf.concat([conv3, conv4], axis=3)\n    model_size_loss = resource_function.model_size_function(\n        concat.op, True, num_alive_inputs3 + num_alive_inputs4,\n        num_alive_inputs3 + num_alive_inputs4, reg_inputs3 + reg_inputs4,\n        reg_inputs3 + reg_inputs4, batch_size)\n\n    expected_model_size_loss = 0\n    self.assertEqual(expected_model_size_loss, model_size_loss)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn19_AliveIn11\', 1, 19, 11),\n      (\'_BatchSize32_AliveIn15_AliveIn7\', 32, 15, 7))\n  def testConcatActivationCountFunction_Cost(\n      self, batch_size, num_alive_inputs3, num_alive_inputs4):\n    conv3 = layers.conv2d(\n        self.image, 19, [7, 5], stride=2, padding=\'SAME\', scope=\'conv3\')\n    conv4 = layers.conv2d(\n        self.image, 11, [3, 7], stride=2, padding=\'SAME\', scope=\'conv4\')\n    concat = tf.concat([conv3, conv4], axis=3)\n    activation_count_cost = resource_function.activation_count_function(\n        concat.op, False, num_alive_inputs3 + num_alive_inputs4,\n        num_alive_inputs3 + num_alive_inputs4, 19, 11, batch_size)\n\n    expected_activation_count_cost = 0\n    self.assertEqual(expected_activation_count_cost, activation_count_cost)\n\n  @parameterized.named_parameters(\n      (\'_BatchSize1_AliveIn17_AliveIn19_RegIn17_RegIn19\',\n       1, 19, 11, 19, 11),\n      (\'_BatchSize32_AliveIn4_AliveIn9_RegIn3_RegIn7\',\n       32, 15, 7, 12, 6))\n  def testConcatActivationCountFunction_Regularization(\n      self, batch_size, num_alive_inputs3, num_alive_inputs4, reg_inputs3,\n      reg_inputs4):\n    conv3 = layers.conv2d(\n        self.image, 19, [7, 5], stride=2, padding=\'SAME\', scope=\'conv3\')\n    conv4 = layers.conv2d(\n        self.image, 11, [3, 7], stride=2, padding=\'SAME\', scope=\'conv4\')\n    concat = tf.concat([conv3, conv4], axis=3)\n    activation_count_loss = resource_function.activation_count_function(\n        concat.op, True, num_alive_inputs3 + num_alive_inputs4,\n        num_alive_inputs3 + num_alive_inputs4, reg_inputs3 + reg_inputs4,\n        reg_inputs3 + reg_inputs4, batch_size)\n\n    expected_activation_count_loss = 0\n    self.assertEqual(expected_activation_count_loss, activation_count_loss)\n\n  def testBadHardware(self):\n    with self.assertRaises(ValueError):\n      _ = resource_function.latency_function_factory(\'apple\', 66)\n    with self.assertRaises(ValueError):\n      _ = resource_function.latency_function_factory(None, 11)\n\n  def testConvFlopsCoeff(self):\n    tf.compat.v1.reset_default_graph()\n    image = tf.constant(0.0, shape=[1, 11, 13, 17])\n    layers.conv2d(image, 19, [7, 5], stride=2, padding=\'SAME\', scope=\'conv1\')\n    conv_op = tf.get_default_graph().get_operation_by_name(\'conv1/Conv2D\')\n    # Divide by the input depth and the output depth to get the coefficient.\n    expected_coeff = _flops(conv_op) / (17.0 * 19.0)\n    actual_coeff = resource_function.flop_coeff(conv_op)\n    self.assertNearRelatively(expected_coeff, actual_coeff)\n\n  def testConvFlopsCoeffUnknownShape(self):\n    tf.compat.v1.reset_default_graph()\n    image = tf.placeholder(tf.float32, shape=[1, None, None, 17])\n    net = layers.conv2d(\n        image, 19, [7, 5], stride=2, padding=\'SAME\', scope=\'conv1\')\n    self.conv_op = tf.get_default_graph().get_operation_by_name(\n        \'conv1/Conv2D\')\n    actual_coeff_tensor = resource_function.flop_coeff(self.conv_op)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      actual_coeff, _ = sess.run([actual_coeff_tensor, net],\n                                 feed_dict={image: np.zeros((1, 11, 13, 17))})\n    # We cannot use the _flops function above because the shapes are not all\n    # known in the graph.\n    expected_coeff = 2940.0\n    self.assertNearRelatively(expected_coeff, actual_coeff)\n\n  def testConvTransposeFlopsCoeff(self):\n    tf.compat.v1.reset_default_graph()\n    image = tf.constant(0.0, shape=[1, 11, 13, 17])\n    layers.conv2d_transpose(\n        image, 29, [7, 5], stride=2, padding=\'SAME\', scope=\'convt2\')\n    convt_op = tf.get_default_graph().get_operation_by_name(\n        \'convt2/conv2d_transpose\')\n\n    # Divide by the input depth and the output depth to get the coefficient.\n    expected_coeff = _flops(convt_op) / (17.0 * 29.0)\n    actual_coeff = resource_function.flop_coeff(convt_op)\n    self.assertNearRelatively(expected_coeff, actual_coeff)\n\n  def testFcFlopsCoeff(self):\n    expected_coeff = _flops(self.matmul_op) / (19.0 * 23.0)\n    actual_coeff = resource_function.flop_coeff(self.matmul_op)\n    self.assertNearRelatively(expected_coeff, actual_coeff)\n\n  def testConvNumWeightsCoeff(self):\n    actual_coeff = resource_function.num_weights_coeff(self.conv_op)\n    # The coefficient is just the filter size - 7 * 5 = 35:\n    self.assertNearRelatively(35, actual_coeff)\n\n  def testFcNumWeightsCoeff(self):\n    actual_coeff = resource_function.num_weights_coeff(self.matmul_op)\n    # The coefficient is 1.0, the number of weights is just inputs x outputs.\n    self.assertNearRelatively(1.0, actual_coeff)\n\n  def testDepthwiseConvFlopsCoeff(self):\n    tf.compat.v1.reset_default_graph()\n    image = tf.constant(0.0, shape=[1, 11, 13, 17])\n    net = layers.conv2d(\n        image, 10, [7, 5], stride=2, padding=\'SAME\', scope=\'conv2\')\n    layers.separable_conv2d(\n        net, None, [3, 2], depth_multiplier=1, padding=\'SAME\', scope=\'dw1\')\n    dw_op = tf.get_default_graph().get_operation_by_name(\'dw1/depthwise\')\n\n    # Divide by the input depth (which is also the output depth) to get the\n    # coefficient.\n    expected_coeff = _flops(dw_op) / (10.0)\n    actual_coeff = resource_function.flop_coeff(dw_op)\n    self.assertNearRelatively(expected_coeff, actual_coeff)\n\n  def test_conv3d_flops_coeff(self):\n    tf.compat.v1.reset_default_graph()\n    input_depth = 17\n    output_depth = 10\n    video = tf.zeros([1, 15, 12, 13, input_depth])\n    _ = layers.conv3d(\n        video, output_depth, [7, 5, 3], stride=2, padding=\'SAME\', scope=\'conv\')\n    conv_op = tf.get_default_graph().get_operation_by_name(\'conv/Conv3D\')\n    # Divide by the input depth and the output depth to get the coefficient.\n    expected_coeff = _flops(conv_op) / (input_depth * output_depth)\n    actual_coeff = resource_function.flop_coeff(conv_op)\n    self.assertNearRelatively(expected_coeff, actual_coeff)\n\n\ndef _flops(op):\n  """"""Get the number of flops of a convolution, from the ops stats registry.\n\n  Args:\n    op: A tf.Operation object.\n\n  Returns:\n    The number os flops needed to evaluate conv_op.\n  """"""\n  return ops.get_stats_for_node_def(\n      tf.get_default_graph(), op.node_def, \'flops\').value\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/op_regularizers/__init__.py,0,b''
morph_net/op_regularizers/gamma_l1_regularizer.py,3,"b'""""""An OpRegularizer that applies L1 regularization on batch-norm gammas.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import generic_regularizers\nfrom morph_net.framework import tpu_util\nimport tensorflow.compat.v1 as tf\n\n\nclass GammaL1Regularizer(generic_regularizers.OpRegularizer):\n  """"""An OpRegularizer that L1-regularizes batch-norm gamma.""""""\n\n  def __init__(self, gamma, gamma_threshold):\n    """"""Creates an instance.\n\n    Args:\n      gamma: A tf.Tensor of shape (n_channels,) with the gammas.\n      gamma_threshold: A float scalar, the threshold above which a gamma is\n        considered \'alive\'.\n    """"""\n    self._gamma = tpu_util.maybe_convert_to_variable(gamma)\n    self._gamma_threshold = gamma_threshold\n    abs_gamma = tf.abs(self._gamma)\n    self._alive_vector = abs_gamma > gamma_threshold\n    self._regularization_vector = abs_gamma\n\n  @property\n  def regularization_vector(self):\n    return self._regularization_vector\n\n  @property\n  def alive_vector(self):\n    """"""Returns a tf.Tensor of shape (n_channels,) with alive bits.""""""\n    return self._alive_vector\n'"
morph_net/op_regularizers/group_lasso_regularizer.py,3,"b'""""""A regularizer based on group-lasso.\n\nAll the weights that are related to a single output are grouped into one LASSO\ngroup (https://arxiv.org/pdf/1611.06321.pdf).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom morph_net.framework import generic_regularizers\nfrom morph_net.framework import tpu_util\nimport tensorflow.compat.v1 as tf\n\n\nclass GroupLassoRegularizer(generic_regularizers.OpRegularizer):\n  """"""A regularizer for convolutions and matmul operations, based on group-lasso.\n\n  Supported ops: Conv2D, Conv2DBackpropInput (transposed Conv2D), and MatMul\n  are supported. The grouping is done according to the formula:\n\n  (1 - l1_fraction) * L2(weights) / sqrt(dim) + l1_fraction * L1(weights) / dim,\n\n  where `dim` is the number of weights associated with an activation, L2 and L1\n  are the respective norms, and l1_fraction controls the balance between L1 and\n  L2 grouping. The paper cited above experiments with 0.0 and 0.5 for\n  l1_fraction.\n  """"""\n\n  def __init__(self, weight_tensor, reduce_dims, threshold, l1_fraction=0.0):\n    """"""Creates an instance.\n\n    Args:\n      weight_tensor: A tensor with the weights of the op (potentially sliced).\n      reduce_dims: A tuple indictaing the dimensions of `weight_tensor`\n        to reduce over. Most often it will include all dimensions except\n        the output size.\n      threshold: A float. When the norm of the group associated with an\n        activation is below the threshold, it will be considered dead.\n      l1_fraction: A float, controls the balance between L1 and L2 grouping (see\n        above).\n    """"""\n    weight_tensor = tpu_util.maybe_convert_to_variable(weight_tensor)\n    if l1_fraction < 0.0 or l1_fraction > 1.0:\n      raise ValueError(\n          \'l1_fraction should be in [0.0, 1.0], not %e.\' % l1_fraction)\n\n    self._threshold = threshold\n    l2_norm = tf.sqrt(\n        tf.reduce_mean(tf.square(weight_tensor), axis=reduce_dims))\n    if l1_fraction > 0.0:\n      l1_norm = tf.reduce_mean(tf.abs(weight_tensor), axis=reduce_dims)\n      norm = l1_fraction * l1_norm + (1.0 - l1_fraction) * l2_norm\n    else:\n      norm = l2_norm\n\n    self._regularization_vector = norm\n    self._alive_vector = norm > threshold\n\n  @property\n  def regularization_vector(self):\n    return self._regularization_vector\n\n  @property\n  def alive_vector(self):\n    return self._alive_vector\n'"
morph_net/op_regularizers/group_lasso_regularizer_test.py,15,"b'""""""Tests for regularizers.framework.group_lasso_regularizer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom morph_net.op_regularizers import group_lasso_regularizer\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import framework as contrib_framework\nfrom tensorflow.contrib import layers as contrib_layers\n\nlayers = contrib_layers\n\nALIVE_THRESHOLD = 1.0\n\n\ndef assert_not_all_are_alive_or_dead(alive_vector):\n  assert not all(alive_vector), (\n      \'All activations are alive, test case is trivial. Increase threshold\')\n  assert any(alive_vector), (\n      \'All activations are dead, test case is trivial. Decrease threshold\')\n\n\nclass GroupLassoRegularizerTest(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    tf.reset_default_graph()\n    tf.set_random_seed(7907)\n    with contrib_framework.arg_scope(\n        [layers.conv2d, layers.conv2d_transpose],\n        weights_initializer=tf.random_normal_initializer):\n      self.BuildModel()\n    with self.cached_session():\n      tf.global_variables_initializer().run()\n\n  def BuildModel(self):\n    image = tf.constant(0.0, shape=[1, 17, 19, 3])\n    conv = layers.conv2d(image, 13, [7, 5], padding=\'SAME\', scope=\'conv\')\n    layers.conv2d_transpose(conv, 11, [5, 5], scope=\'convt\')\n\n  # For Conv2D the reduction indices for group lasso are (0, 1, 2).\n  # For Conv2DBackpropInput (aka conv2d transpose) they are (0, 1, 3).\n  @parameterized.named_parameters(\n      (\'_regular_conv\', \'conv/Conv2D\', (0, 1, 2), 0.0),\n      (\'_transpose_conv\', \'convt/conv2d_transpose\', (0, 1, 3), 0.0),\n      (\'_regular_conv_l10.5\', \'conv/Conv2D\', (0, 1, 2), 0.5))\n  def testOp(self, op_name, reduce_dims, l1_fraction):\n    op = tf.get_default_graph().get_operation_by_name(op_name)\n    with self.cached_session():\n      weights = op.inputs[1].eval()\n    l1_reg_vector = np.mean(np.abs(weights), axis=reduce_dims)\n    l2_reg_vector = np.sqrt(np.mean(weights**2, axis=reduce_dims))\n    expected_reg_vector = (\n        l1_fraction * l1_reg_vector + (1.0 - l1_fraction) * l2_reg_vector)\n\n    # We choose the threshold at the expectation value, so that some activations\n    # end up above threshold and others end up below. The weights are normally\n    # distributed, so the L2 norm is 1.0, and the L1 norm is sqrt(2/pi).\n    # With a general l1_fraction, we compute a weighted average of the two:\n    threshold = (1.0 - l1_fraction) + l1_fraction * np.sqrt(2 / np.pi)\n    expected_alive = expected_reg_vector > threshold\n    assert_not_all_are_alive_or_dead(expected_alive)\n\n    conv_reg = (\n        group_lasso_regularizer.GroupLassoRegularizer(\n            weight_tensor=op.inputs[1], reduce_dims=reduce_dims,\n            threshold=threshold, l1_fraction=l1_fraction))\n\n    with self.cached_session():\n      actual_reg_vector = conv_reg.regularization_vector.eval()\n      actual_alive = conv_reg.alive_vector.eval()\n\n    self.assertAllClose(expected_reg_vector, actual_reg_vector)\n    self.assertAllEqual(expected_alive, actual_alive)\n\n\nclass GroupLassoRegularizerMatMulTest(parameterized.TestCase, tf.test.TestCase):\n  """"""Unit tests for MatMul op with GroupLassoRegularizer.""""""\n\n  def setUp(self):\n    tf.reset_default_graph()\n    tf.set_random_seed(7907)\n    with contrib_framework.arg_scope(\n        [layers.fully_connected],\n        weights_initializer=tf.random_normal_initializer):\n      # Build the model.\n      input_data = tf.constant(0.0, shape=[1, 3])\n      layers.fully_connected(input_data, 4, scope=\'fc\')\n    with self.cached_session():\n      tf.global_variables_initializer().run()\n\n  @parameterized.named_parameters((\'_fully_connected_no_l1\', 0.0),\n                                  (\'_fully_connected_l1_0.5\', 0.5))\n  def testMatMulOp(self, l1_fraction):\n    op = tf.get_default_graph().get_operation_by_name(\'fc/MatMul\')\n    with self.cached_session():\n      weights = op.inputs[1].eval()\n    l1_reg_vector = np.mean(np.abs(weights), axis=0)\n    l2_reg_vector = np.sqrt(np.mean(weights**2, axis=0))\n    expected_reg_vector = (\n        l1_fraction * l1_reg_vector + (1.0 - l1_fraction) * l2_reg_vector)\n    # We choose the threshold at the expectation value, so that some activations\n    # end up above threshold and others end up below. The weights are normally\n    # distributed, so the L2 norm is 1.0, and the L1 norm is sqrt(2/pi).\n    # With a general l1_fraction, we compute a weighted average of the two:\n    threshold = (1.0 - l1_fraction) + l1_fraction * np.sqrt(2 / np.pi)\n    expected_alive = expected_reg_vector > threshold\n    assert_not_all_are_alive_or_dead(expected_alive)\n\n    matmul_reg = (\n        group_lasso_regularizer.GroupLassoRegularizer(\n            weight_tensor=op.inputs[1], reduce_dims=(0,),\n            threshold=threshold, l1_fraction=l1_fraction))\n    with self.cached_session():\n      actual_reg_vector = matmul_reg.regularization_vector.eval()\n      actual_alive = matmul_reg.alive_vector.eval()\n\n    self.assertAllClose(actual_reg_vector, expected_reg_vector)\n    self.assertAllEqual(actual_alive, expected_alive)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/testing/add_concat_model_stub.py,1,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""A model stub with convolutions and addition/concat residual connections.\n\nModel:\n\n             -> conv1 --+     -> conv3 -->  conv4 --\n            /           |    /                      \\\n      image          [concat]                      (add) -- > conv5 --> output\n            \\           |    \\                      /\n             -> conv2 --+     -> -------------------\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.testing import op_regularizer_stub\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import layers as contrib_layers\n\nlayers = contrib_layers\n\nALIVE_STUB = {\n    \'conv1\': [False, True, True, False, True, False, True],\n    \'conv2\': [True, False, True, False, False],\n    \'conv3\': [False, False, True, True],\n    \'conv4\': [False, True, False, True, True, False, True,\n              False, False, True, False, False],\n    \'conv5\': [False, True, False],\n}\n\n\nREG_STUB = {\n    \'conv1\': [0.1, 0.3, 0.6, 0.2, 0.4, 0.0, 0.8],\n    \'conv2\': [0.15, 0.25, 0.05, 0.55, 0.45],\n    \'conv3\': [0.07, 0.27, 0.17, 0.37],\n    \'conv4\': [\n        0.07, 0.27, 0.17, 0.37, 0.28, 0.32, 0.12, 0.22, 0.19, 0.11, 0.02, 0.06\n    ],\n    \'conv5\': [0.24, 0.34, 0.29],\n}\n\n\nMOCK_REG_DICT = {\n    \'Conv2D\':\n        op_regularizer_stub.OpRegularizerStubFactory(ALIVE_STUB, REG_STUB)\n        .create_conv2d_regularizer\n}\n\n\ndef image_stub():\n  return op_regularizer_stub.image_stub()\n\n\ndef build_model(image=None):\n  """"""Builds the model network described at the top of the file.\n\n  Args:\n    image: A 4D tensor to be used as image. If None, image_stub will be used.\n\n  Returns:\n    The output op of the network.\n  """"""\n  if image is None:\n    image = image_stub()\n  conv1 = layers.conv2d(image, 7, [7, 5], padding=\'SAME\', scope=\'conv1\')\n  conv2 = layers.conv2d(image, 5, [1, 1], padding=\'SAME\', scope=\'conv2\')\n  concat = tf.concat([conv1, conv2], 3)\n  conv3 = layers.conv2d(concat, 4, [1, 1], padding=\'SAME\', scope=\'conv3\')\n  conv4 = layers.conv2d(conv3, 12, [3, 3], padding=\'SAME\', scope=\'conv4\')\n  conv5 = layers.conv2d(\n      concat + conv4, 3, [3, 3], stride=2, padding=\'SAME\', scope=\'conv5\')\n  return conv5.op\n\n\ndef expected_regularization():\n  """"""Build the expected regularization vectors.""""""\n  concat = REG_STUB[\'conv1\'] + REG_STUB[\'conv2\']\n  # Grouping: Regularization grouping is the max of the constituents.\n  grouped = [max(a, b) for a, b in zip(concat, REG_STUB[\'conv4\'])]\n  conv1_length = len(REG_STUB[\'conv1\'])\n  return {\n      \'conv1\': grouped[:conv1_length],\n      \'conv2\': grouped[conv1_length:],\n      \'conv3\': REG_STUB[\'conv3\'],\n      \'conv4\': grouped,\n      \'conv5\': REG_STUB[\'conv5\'],\n      \'add\': grouped,\n      \'concat\': grouped\n  }\n\n\ndef expected_alive():\n  """"""Build the expected alive vectors.""""""\n  concat = ALIVE_STUB[\'conv1\'] + ALIVE_STUB[\'conv2\']\n  # Grouping: Activation is alive after grouping if one of the constituents is\n  # alive.\n  grouped = [a or b for a, b in zip(concat, ALIVE_STUB[\'conv4\'])]\n  conv1_length = len(ALIVE_STUB[\'conv1\'])\n  return {\n      \'conv1\': grouped[:conv1_length],\n      \'conv2\': grouped[conv1_length:],\n      \'conv3\': ALIVE_STUB[\'conv3\'],\n      \'conv4\': grouped,\n      \'conv5\': ALIVE_STUB[\'conv5\'],\n      \'add\': grouped,\n      \'concat\': grouped\n  }\n'"
morph_net/testing/dummy_decorator.py,0,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""A dummy decorator used for testing.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import generic_regularizers\n\n\nclass DummyDecorator(generic_regularizers.OpRegularizer):\n  """"""A dummy decorator that multiply the regularization vector by a scale.""""""\n\n  def __init__(self, regularizer_object, scale):\n    """"""Creates an instance.\n\n    Accept an OpRegularizer that is decorated by this class.\n\n    Args:\n      regularizer_object: OpRegularizer to decorate.\n      scale: A float to scale regularization_vector.\n    """"""\n\n    self._regularization_vector = (\n        regularizer_object.regularization_vector * scale)\n    self._alive_vector = regularizer_object.alive_vector\n\n  @property\n  def regularization_vector(self):\n    return self._regularization_vector\n\n  @property\n  def alive_vector(self):\n    return self._alive_vector\n\n'"
morph_net/testing/grouping_concat_model_stub.py,2,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""A model stub with convolutions and non-channal concats residual connections.\n\nModel:\n\n             -> conv1 -\\                      conv4 --\n            /           |                     /        \\\n      image          [concat axis=1]  -> conv3   -->   [concat axis=2]\n            \\           |\n             -> conv2 -/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.testing import op_regularizer_stub\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import layers as contrib_layers\n\nlayers = contrib_layers\n\nALIVE_STUB = {\n    \'conv1\': [False, True, True],\n    \'conv2\': [True, False, True],\n    \'conv3\': [False, False, True, True],\n    \'conv4\': [False, True, False, True],\n}\n\nREG_STUB = {\n    \'conv1\': [0.1, 0.3, 0.6],\n    \'conv2\': [0.15, 0.25, 0.05],\n    \'conv3\': [0.07, 0.17, 0.57, 0.37],\n    \'conv4\': [0.06, 100.87, 0.17, 1.57],\n}\n\n\nMOCK_REG_DICT = {\n    \'Conv2D\':\n        op_regularizer_stub.OpRegularizerStubFactory(ALIVE_STUB, REG_STUB)\n        .create_conv2d_regularizer\n}\n\n\ndef image_stub():\n  return op_regularizer_stub.image_stub()\n\n\ndef build_model(image=None):\n  """"""Builds the model network described at the top of the file.\n\n  Args:\n    image: A 4D tensor to be used as image. If None, image_stub will be used.\n\n  Returns:\n    The output op of the network.\n  """"""\n  if image is None:\n    image = image_stub()\n  conv1 = layers.conv2d(image, 3, [7, 5], padding=\'SAME\', scope=\'conv1\')\n  conv2 = layers.conv2d(image, 3, [1, 1], padding=\'SAME\', scope=\'conv2\')\n  concat = tf.concat([conv1, conv2], 1, \'concat1\')\n  conv3 = layers.conv2d(concat, 4, [1, 1], padding=\'SAME\', scope=\'conv3\')\n  conv4 = layers.conv2d(conv3, 4, [3, 3], padding=\'SAME\', scope=\'conv4\')\n  concat2 = tf.concat([conv4, conv3], 2, \'concat2\')\n  return concat2.op\n\n\ndef expected_regularization():\n  """"""Build the expected regularization vectors.""""""\n  # Grouping: Regularization grouping is the max of the constituents.\n  reg1 = [max(a, b) for a, b in zip(REG_STUB[\'conv2\'], REG_STUB[\'conv1\'])]\n  reg2 = [max(a, b) for a, b in zip(REG_STUB[\'conv3\'], REG_STUB[\'conv4\'])]\n  return {\n      \'conv1\': reg1,\n      \'conv2\': reg1,\n      \'conv3\': reg2,\n      \'conv4\': reg2,\n      \'concat1\': reg1,\n      \'concat2\': reg2\n  }\n\n\ndef expected_alive():\n  """"""Build the expected alive vectors.""""""\n  alive1 = [a or b for a, b in zip(ALIVE_STUB[\'conv2\'], ALIVE_STUB[\'conv1\'])]\n  alive2 = [a or b for a, b in zip(ALIVE_STUB[\'conv3\'], ALIVE_STUB[\'conv4\'])]\n  return {\n      \'conv1\': alive1,\n      \'conv2\': alive1,\n      \'conv3\': alive2,\n      \'conv4\': alive2,\n      \'concat1\': alive1,\n      \'concat2\': alive2\n  }\n'"
morph_net/testing/ladder_model_stub.py,7,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""A model stub with residual connections and spatial upscaling.\n\nModel:\n\n              -> ----------\n             /            |\n              -> conv2--(add)---                   -------------\n            /                   |                  |           |\nimage--conv1                     --(add)-> upscale -> conv4--(add) --> output\n            \\                   |\n              -> conv3--(add)---\n              \\           |\n              -> ----------\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.testing import op_regularizer_stub\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import layers as contrib_layers\n\nlayers = contrib_layers\n\nALIVE_STUB = {\n    \'conv1\': [False, True, True],\n    \'conv2\': [False, True, True],\n    \'conv3\': [False, True, True],\n    \'conv4\': [False, True, True],\n}\n\n\nREG_STUB = {\n    \'conv1\': [0.1, 0.3, 0.6],\n    \'conv2\': [0.1, 0.3, 0.6],\n    \'conv3\': [0.1, 0.3, 0.6],\n    \'conv4\': [0.1, 0.3, 0.6],\n}\n\n\nMOCK_REG_DICT = {\n    \'Conv2D\':\n        op_regularizer_stub.OpRegularizerStubFactory(ALIVE_STUB, REG_STUB)\n        .create_conv2d_regularizer\n}\n\n\ndef image_stub():\n  return op_regularizer_stub.image_stub()\n\n\ndef build_model(image=None):\n  """"""Builds the network described at the top.\n\n  Args:\n    image: A 4D tensor to be used as image. If None, image_stub will be used.\n\n  Returns:\n    The output op of the network.\n  """"""\n  if image is None:\n    image = image_stub()\n\n  conv1 = layers.conv2d(image, 3, [3, 3], padding=\'SAME\', scope=\'conv1\')\n  conv2 = layers.conv2d(conv1, 3, [3, 3], padding=\'SAME\', scope=\'conv2\')\n  conv3 = layers.conv2d(conv1, 3, [3, 3], padding=\'SAME\', scope=\'conv3\')\n\n  res1 = tf.add(conv1, conv2, name=\'add_1\')\n  res2 = tf.add(conv1, conv3, name=\'add_2\')\n  merged_towers = tf.add(res1, res2, name=\'add_3\')\n\n  upscale = tf.nn.conv2d_transpose(\n      merged_towers,\n      tf.zeros([2, 2, merged_towers.shape[-1], merged_towers.shape[-1]]),\n      tf.shape(merged_towers) * tf.constant([1, 2, 2, 1]),\n      [1, 2, 2, 1], \'SAME\', name=\'conv2d_transpose1\')\n  conv4 = layers.conv2d(\n      upscale, 3, [3, 3], padding=\'SAME\', scope=\'conv4\')\n\n  res_top = tf.add(upscale, conv4, name=\'add_4\')\n\n  return res_top.op\n\n\ndef expected_regularization():\n  """"""Build the expected regularization vectors.""""""\n  return {\n      \'conv1\': REG_STUB[\'conv1\'],\n      \'conv2\': REG_STUB[\'conv2\'],\n      \'conv3\': REG_STUB[\'conv3\'],\n      \'add1\': REG_STUB[\'conv1\'],\n      \'add2\': REG_STUB[\'conv1\'],\n      \'add3\': REG_STUB[\'conv2\'],\n      \'conv2d_transpose1\': REG_STUB[\'conv2\'],\n      \'conv4\': REG_STUB[\'conv4\'],\n      \'add4\': REG_STUB[\'conv4\'],\n  }\n\n\ndef expected_alive():\n  """"""Build the expected alive vectors.""""""\n  return {\n      \'conv1\': ALIVE_STUB[\'conv1\'],\n      \'conv2\': ALIVE_STUB[\'conv2\'],\n      \'conv3\': ALIVE_STUB[\'conv3\'],\n      \'add1\': ALIVE_STUB[\'conv1\'],\n      \'add2\': ALIVE_STUB[\'conv1\'],\n      \'add3\': ALIVE_STUB[\'conv2\'],\n      \'conv2d_transpose1\': ALIVE_STUB[\'conv2\'],\n      \'conv4\': ALIVE_STUB[\'conv4\'],\n      \'add4\': ALIVE_STUB[\'conv4\'],\n  }\n'"
morph_net/testing/op_regularizer_stub.py,4,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Helpers for testing the regularizers framework.\n\nContains logic for creating Stubs for OpRegularizers for the\nconvolutions in a model.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom morph_net.framework import generic_regularizers\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import layers as contrib_layers\n\nlayers = contrib_layers\n\n\nclass OpRegularizerStub(generic_regularizers.OpRegularizer):\n  """"""A stub that exponses a constant regularization_vector and alive_vector.""""""\n\n  def __init__(self, regularization_vector, alive_vector):\n    self._regularization_vector = tf.constant(\n        regularization_vector, dtype=tf.float32)\n    self._alive_vector = tf.constant(alive_vector, dtype=tf.bool)\n\n  @property\n  def regularization_vector(self):\n    return self._regularization_vector\n\n  @property\n  def alive_vector(self):\n    return self._alive_vector\n\n\nclass OpRegularizerStubFactory(object):\n  """"""OpRegularizerStub factory to be used in testing.""""""\n\n  def __init__(self, alive_stub_dict, reg_stub_dict):\n    self._alive_stub = alive_stub_dict\n    self._reg_stub = reg_stub_dict\n\n  def _create_stub(self, key):\n    return OpRegularizerStub(self._reg_stub[key], self._alive_stub[key])\n\n  def create_conv2d_regularizer(self, conv_op, manager=None):\n    del manager  # unused\n    for key in self._reg_stub:\n      if conv_op.name.startswith(key):\n        return self._create_stub(key)\n    raise ValueError(\'No regularizer for %s\' % conv_op.name)\n\n\ndef image_stub():\n  return tf.constant(0.0, shape=[1, 17, 19, 3])\n'"
morph_net/tools/__init__.py,0,b''
morph_net/tools/configurable_ops.py,13,"b'""""""A module that facilitates creation of configurable networks.\n\nThe goal of this module is to allow centralized parameterization of a (possibly)\ncomplex deep network.\n\nAn important detail in this implementation is about the behaviour of function\ngiven a trivial parameterization. By trivial we mean the case where\nNUM_OUTPUTS is 0. We define the output of a trivial parameterization to be the\nspecial value VANISHED, which is recognized by supported ops. We use 0.0 for its\nvalue, so that it\'s treated as a regular 0.0 by supported Tensorflow ops.\nThis choice implies that:\n  * For a vanished input, functions such as \'conv2d\', or \'fully_connected\' will\n    also produce vanished output.\n  * The \'concat\' function will ignore VANISHED inputs. If all inputs are\n    VANISHED, then the output is also VANISHED.\n\nThis edge-case behaviour achieves two goals:\n  * It minimizes creation of ops in the graph which are not used.\n  * It allows seamless use of the class in networks where some elements\n    might be ""turned off"" by the parameterization.\n\nFor instance the following code will work for any parameterization of\nthe first and second convolutions, including 0.\n```\n# input.shape[-1] == 128\nops = ConfigurableOps(parameterization)\nnet_1 = ops.conv2d(input, num_outputs=256, kernel_size=1, scope=\'conv1\')\nnet_2 = ops.conv2d(net_1, num_outputs=64, kernel_size=3, scope=\'conv2\')\nnet_3 = ops.conv2d(net_2, num_outputs=128, kernel_size=1, scope=\'conv3\')\n\noutput = net_3 + input\n```\nFor `parameterization = \'{\'conv1\': 0}\'`\nthe values of `net_1`, `net_2`, and `net_3` will be all vanished sentinels, and\nthe bypass of this block will essentially vanish.\n\nNote that the VANISHED functionality will save downsteam ops from being created\nbut not upstream ops.  For example, with `parameterization = \'{\'conv2\': 0}\'`,\nthen `net_1` will still be created.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport json\nfrom enum import Enum\n\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.compat.v1 import layers as tf_layers\nfrom tensorflow.contrib import framework\nfrom tensorflow.contrib import layers as contrib_layers\nfrom tensorflow.contrib import slim as slim_layers\n# gfile = tf.gfile  # Aliase needed for mock.\n\nVANISHED = 0.0\n_DEFAULT_NUM_OUTPUTS_KWARG = \'num_outputs\'\n\n_DEFAULT_FUNCTION_DICT = {\n    \'fully_connected\': contrib_layers.fully_connected,\n    \'conv2d\': contrib_layers.conv2d,\n    \'separable_conv2d\': contrib_layers.separable_conv2d,\n    \'concat\': tf.concat,\n    \'add_n\': tf.add_n,\n    \'avg_pool2d\': contrib_layers.avg_pool2d,\n    \'max_pool2d\': contrib_layers.max_pool2d,\n    \'batch_norm\': contrib_layers.batch_norm,\n}\n\n_OP_SCOPE_DEFAULTS = {\n    tf_layers.conv2d: \'conv2d\',\n    slim_layers.conv2d: \'Conv\',\n    contrib_layers.conv2d: \'Conv\',\n\n    tf_layers.separable_conv2d: \'separable_conv2d\',\n    slim_layers.separable_conv2d: \'SeparableConv2d\',\n    contrib_layers.separable_conv2d: \'SeparableConv2d\',\n\n    tf_layers.dense: \'dense\',\n    slim_layers.fully_connected: \'fully_connected\',\n    contrib_layers.fully_connected: \'fully_connected\',\n}\n\n# Maps function names to the suffix of the name of the regularized ops.\n_SUFFIX_DICT = {\n    \'fully_connected\': \'MatMul\',\n    \'conv2d\': \'Conv2D\',\n    \'separable_conv2d\': \'separable_conv2d\',\n}\n\n\ndef get_function_dict(overrides=None):\n  """"""Get mapping from function name to function for ConfigurableOps.\n\n  Args:\n    overrides: Dict: str -> function. Optionally replace entries in\n      `_DEFAULT_FUNCTION_DICT`.\n\n  Returns:\n    Dict: function name (str) to function.\n  """"""\n  overrides = overrides or {}\n  function_dict = copy.deepcopy(_DEFAULT_FUNCTION_DICT)\n  function_dict.update(overrides)\n  return function_dict\n\n\ndef is_vanished(maybe_tensor):\n  """"""Checks if the argument represents a real tensor or None/vanished sentinel.\n\n  For example:\n    `is_vanished(ConfigurableOps({\'conv1\': 0}).conv2d(...,scope=\'conv1\'))`\n  returns True, since 0 channels in a conv2d produces vanished output.\n\n  Args:\n    maybe_tensor: A tensor or None or the vanished sentinel.\n\n  Returns:\n    A boolean, whether maybe_tensor is a tensor.\n  """"""\n  return maybe_tensor == VANISHED or maybe_tensor is None\n\n\nclass FallbackRule(Enum):\n  """"""Fallback rules for the ConfigurableOps class.""""""\n  pass_through = \'pass_through\'\n  strict = \'strict\'\n  zero = \'zero\'\n\n\nclass ConfigurableOps(object):\n  """"""A class that facilitates structure modification of a Tensorflow graph.\n\n  The ConfigurableOps allows modifications of the NUM_OUTPUTS argument ops.\n  The functionality is determined by a \'parameterization\' and by modifiers.\n  The \'parameterization\' is a map between scope names and new NUM_OUTPUTS\n  values. If the scope of an op matches a key from the parameterization, the\n  decorator will override the NUM_OUTPUTS argument.\n\n  Another feature of the ConfigurableOps is support for vanishing input sizes\n  that arise when an the NUM_OUTPUTS argument of a downstream op is set to\n  zero. Specifically, the functions decorated by the class adhere to the\n  following input/output logic:\n    * If NUM_OUTPUTS is set to zero, then the output of the op will be the\n      vanished sentinel, and will return False when checked with is_vanished().\n    * If the input is vanished, the output will be the same.\n    * The concatenation (configurable_ops.concat) of an vanished element with\n      other tensors ignores the vanished elements. The result of concatenating\n      only vanished elements is also vanished.\n\n  In addition the object collects and report the actual NUM_OUTPUTS argument\n  that was used in every context.\n  """"""\n\n  def __init__(self,\n               parameterization=None,\n               function_dict=None,\n               fallback_rule=FallbackRule.pass_through):\n    """"""Constructs a ConfigurableOps.\n\n    Args:\n      parameterization: A dictionary between scope name to be overridden and a\n        integer which is the target NUM_OUTPUTS.\n      function_dict: A dict between names of ops (strings) and functions\n        which accept num_outputs as the second argument. If None defaults to\n        _DEFAULT_FUNCTION_DICT.\n      fallback_rule: A `FallbackRule` enum which controls fallback behavior:\n          * \'pass_through\' provided NUM_OUTPUTS is passed to decorated\n            function (default).\n          * \'strict\' requires the scope name appear in parameterization or else\n            throws an error.\n          * \'zero\' uses `num_outputs` equal to zero if scope name is not in the\n            parameterization.\n    Raises:\n      ValueError: If fallback_rule is not a string or a FallbackRule enum.\n    """"""\n\n    self._parameterization = parameterization or {}\n\n    if not (isinstance(fallback_rule, FallbackRule) or\n            isinstance(fallback_rule, str)):\n      raise ValueError(\'fallback_rule must be a string or FallbackRule Enum\')\n\n    self._function_dict = function_dict or _DEFAULT_FUNCTION_DICT\n    self._suffix_dict = _SUFFIX_DICT\n    self._constructed_ops = collections.OrderedDict()\n    if isinstance(fallback_rule, str):\n      fallback_rule = FallbackRule[fallback_rule]  # Converts from string.\n    self._default_to_zero = fallback_rule == FallbackRule.zero\n    self._strict = fallback_rule == FallbackRule.strict\n    self.default_scope_to_counts_map = {}\n\n    # To keep track of the number of identical scopes encountered\n    self._scope_counts = {}\n\n  @property\n  def parameterization(self):\n    """"""Returns the parameterization dict mapping op names to num_outputs.""""""\n    return self._parameterization\n\n  @framework.add_arg_scope\n  def conv2d(self, *args, **kwargs):\n    """"""Masks num_outputs from the function pointed to by \'conv2d\'.\n\n    The object\'s parameterization has precedence over the given NUM_OUTPUTS\n    argument. The resolution of the op names uses\n    tf.contrib.framework.get_name_scope() and kwargs[\'scope\'].\n\n    Args:\n      *args: Arguments for the operation.\n      **kwargs: Key arguments for the operation.\n\n    Returns:\n      The result of the application of the function_dict[\'conv2d\'] to the given\n      \'inputs\', \'*args\' and \'**kwargs\' while possibly overriding NUM_OUTPUTS\n      according the parameterization.\n\n    Raises:\n      ValueError: If kwargs does not contain a key named \'scope\'.\n    """"""\n    fn, suffix = self._get_function_and_suffix(\'conv2d\')\n    return self._mask(fn, suffix, *args, **kwargs)\n\n  @framework.add_arg_scope\n  def fully_connected(self, *args, **kwargs):\n    """"""Masks NUM_OUTPUTS from the function pointed to by \'fully_connected\'.\n\n    The object\'s parameterization has precedence over the given NUM_OUTPUTS\n    argument. The resolution of the op names uses\n    tf.contrib.framework.get_name_scope() and kwargs[\'scope\'].\n\n    Args:\n      *args: Arguments for the operation.\n      **kwargs: Key arguments for the operation.\n\n    Returns:\n      The result of the application of the function_map[\'fully_connected\'] to\n      the given \'inputs\', \'*args\' and \'**kwargs\' while possibly overriding\n      NUM_OUTPUTS according the parameterization.\n\n    Raises:\n      ValueError: If kwargs does not contain a key named \'scope\'.\n    """"""\n    inputs = _get_from_args_or_kwargs(\'inputs\', 0, args, kwargs)\n    if inputs.shape.ndims != 2:\n      raise ValueError(\n          \'ConfigurableOps does not suport fully_connected with rank != 2\')\n    fn, suffix = self._get_function_and_suffix(\'fully_connected\')\n    return self._mask(fn, suffix, *args, **kwargs)\n\n  @framework.add_arg_scope\n  def separable_conv2d(self, *args, **kwargs):\n    """"""Masks NUM_OUTPUTS from the function pointed to by \'separable_conv2d\'.\n\n    The object\'s parameterization has precedence over the given NUM_OUTPUTS\n    argument. The resolution of the op names uses\n    tf.contrib.framework.get_name_scope() and kwargs[\'scope\'].\n\n    Args:\n      *args: Arguments for the operation.\n      **kwargs: Key arguments for the operation.\n\n    Returns:\n      The result of the application of the function_map[\'separable_conv2d\'] to\n      the given \'inputs\', \'*args\', and \'**kwargs\' while possibly overriding\n      NUM_OUTPUTS according the parameterization.\n\n    Raises:\n      ValueError: If kwargs does not contain a key named \'scope\'.\n    """"""\n    # This function actually only decorates the num_outputs of the Conv2D after\n    # the depthwise convolution, as the former does not have any free params.\n    fn, suffix = self._get_function_and_suffix(\'separable_conv2d\')\n    num_outputs_kwarg_name = self._get_num_outputs_kwarg_name(fn)\n    num_outputs = _get_from_args_or_kwargs(\n        num_outputs_kwarg_name, 1, args, kwargs, False)\n    if num_outputs is None:\n      tf.logging.warning(\n          \'Trying to decorate separable_conv2d with num_outputs = None\')\n      kwargs[num_outputs_kwarg_name] = None\n\n    return self._mask(fn, suffix, *args, **kwargs)\n\n  def _mask(self, function, suffix, *args, **kwargs):\n    """"""Masks num_outputs from the given function.\n\n    The object\'s parameterization has precedence over the given NUM_OUTPUTS\n    argument. The resolution of the op names uses\n      `tf.contrib.framework.get_name_scope()` and `kwargs[\'scope\']`.\n\n    The NUM_OUTPUTS argument is assumed to be either in **kwargs or held in\n    *args[1].\n\n    In case the `inputs` argument is VANISHED or that `num_outputs` is 0,\n    returns VANISHED without adding ops to the graph.\n\n    Args:\n      function: A callable function to mask the NUM_OUTPUTS parameter from.\n        Examples for functions are in _DEFAULT_FUNCTION_DICT.\n        The callable function must accept a NUM_OUTPUTS parameter as the\n        second argument.\n      suffix: A string with the suffix of the op name.\n      *args: Arguments for the operation.\n      **kwargs: Key arguments for the operation.\n\n    Returns:\n      The result of the application of the function to the given \'inputs\',\n      \'*args\', and \'**kwargs\' while possibly overriding NUM_OUTPUTS according\n      to the parameterization.\n\n    Raises:\n      ValueError: If kwargs does not contain a key named \'scope\'.\n    """"""\n    inputs = args[0] if args else kwargs.pop(\'inputs\')\n    if is_vanished(inputs):\n      return VANISHED\n\n    # Support for tf.contrib.layers and tf.layers API.\n    op_scope = kwargs.get(\'scope\')\n    current_scope = framework.get_name_scope() or \'\'\n    if current_scope and not current_scope.endswith(\'/\'):\n      current_scope += \'/\'\n\n    op_scope = kwargs.get(\'scope\') or kwargs.get(\'name\')\n    if op_scope:\n      if op_scope.endswith(\'/\'):\n        raise ValueError(\n            \'Scope `{}` ends with `/` which leads to unexpected \'\n            \'behavior.\'.format(op_scope))\n      full_scope = current_scope + op_scope\n    else:\n      # Use default scope, optionally appending a unique ID if scope exists\n      if function not in _OP_SCOPE_DEFAULTS:\n        raise ValueError(\n            \'No `scope` or `name` found in kwargs, and no default scope \'\n            \'defined for {}\'.format(_get_function_name(function)))\n      op_scope = _OP_SCOPE_DEFAULTS[function]\n      full_scope = current_scope + op_scope\n      if full_scope in self._scope_counts:\n        new_scope = full_scope + \'_\' + str(self._scope_counts[full_scope])\n        self._scope_counts[full_scope] += 1\n        full_scope = new_scope\n      else:\n        self._scope_counts[full_scope] = 1\n\n    op_name = full_scope + \'/\' + suffix\n\n    # Assumes `inputs` is the first argument and `num_outputs` is the second\n    # argument.\n    num_outputs = self._parse_num_outputs(\n        op_name, self._get_num_outputs_kwarg_name(function), args, kwargs)\n    args = args[2:]  # Possibly and empty list of < 3 positional args are used.\n\n    self._insert_to_parameterization_log(op_name, num_outputs)\n    if num_outputs == 0:\n      return VANISHED\n\n    return function(inputs, num_outputs, *args, **kwargs)\n\n  @property\n  def constructed_ops(self):\n    """"""Returns a dictionary between op names built to their NUM_OUTPUTS.\n\n       The dictionary will contain an op.name: NUM_OUTPUTS pair for each op\n       constructed by the decorator. The dictionary is ordered according to the\n       order items were added.\n       The parameterization is accumulated during all the calls to the object\'s\n       members, such as `conv2d`, `fully_connected` and `separable_conv2d`.\n       The values used are either the values from the parameterization set for\n       the object, or the values that where passed to the members.\n    """"""\n    return self._constructed_ops\n\n  def concat(self, *args, **kwargs):\n    return self._pass_through_mask_list(\'concat\', \'values\', *args, **kwargs)\n\n  def add_n(self, *args, **kwargs):\n    return self._pass_through_mask_list(\'add_n\', \'inputs\', *args, **kwargs)\n\n  @framework.add_arg_scope\n  def avg_pool2d(self, *args, **kwargs):\n    return self._pass_through_mask(\n        self._function_dict[\'avg_pool2d\'], *args, **kwargs)\n\n  @framework.add_arg_scope\n  def max_pool2d(self, *args, **kwargs):\n    return self._pass_through_mask(\n        self._function_dict[\'max_pool2d\'], *args, **kwargs)\n\n  @framework.add_arg_scope\n  def batch_norm(self, *args, **kwargs):\n    return self._pass_through_mask(\n        self._function_dict[\'batch_norm\'], *args, **kwargs)\n\n  def _get_num_outputs_kwarg_name(self, function):\n    """"""Gets the `num_outputs`-equivalent kwarg for a supported function.""""""\n    alt_num_outputs_kwarg = {\n        tf_layers.conv2d: \'filters\',\n        tf_layers.separable_conv2d: \'filters\',\n        tf_layers.dense: \'units\',\n    }\n    return alt_num_outputs_kwarg.get(function, _DEFAULT_NUM_OUTPUTS_KWARG)\n\n  def _parse_num_outputs(self, op_name, num_outputs_kwarg_name, args, kwargs):\n    """"""Computes the target NUM_OUTPUTS and adjusts kwargs in place.\n\n    Will try to extract the number of outputs from the op_name\'s\n    parameterization. If that\'s not possible, it will default to 0 when\n    _default_to_zero is set, otherwise defaulting to the NUM_OUTPUTS argument\n    that is either in kwargs or args[1].\n\n    Args:\n      op_name: A string, the name of the op to get NUM_OUTPUTS for.\n      num_outputs_kwarg_name: A string, the name of the `num_outputs`-equivalent\n        kwarg.\n      args: Position arguments for the callable. Assumes that NUM_OUTPUTS\n      position is 1.\n      kwargs: key word arguments for the callable.\n\n    Returns:\n      The target value.\n\n    Raises:\n      KeyError: If strict and op_name not found in parameterization.\n    """"""\n    if self._strict and op_name not in self._parameterization:\n      # If strict and op_name not found in parameterization, throw an error.\n      raise KeyError(\'op_name \\""%s\\"" not found in parameterization\' % op_name)\n\n    # Assumes that the position of num_outputs is 1.\n    base_num_outputs = _get_from_args_or_kwargs(\n        num_outputs_kwarg_name, 1, args, kwargs)\n    kwargs.pop(num_outputs_kwarg_name, None)  # Removes num_outputs from kwargs.\n\n    default_num_outputs = 0 if self._default_to_zero else base_num_outputs\n    return self._parameterization.get(op_name, default_num_outputs)\n\n  def _get_function_and_suffix(self, key):\n    """"""Returns the function and suffix associated with key.""""""\n    if key not in self._function_dict:\n      raise KeyError(\'Function ""%s"" not supported by function_dict\' % key)\n    return self._function_dict[key], self._suffix_dict[key]\n\n  def _insert_to_parameterization_log(self, name, num_outputs):\n    """"""Logs the NUM_OUTPUTS for scope \'name\' into _constructed_ops.""""""\n    if name in self._constructed_ops:\n      tf.logging.warning(\'Function called more than once with scope %s.\', name)\n    self._constructed_ops[name] = num_outputs\n\n  def _pass_through_mask_list(self, fn_name, inputs_name, *args, **kwargs):\n    """"""Drops any tensors that are None or vanished and applies `fn` to result.\n\n    Assumes the first argument to `fn` is the list of tensors.\n\n    Args:\n      fn_name: Function name to apply on filtered inputs, must be a key of\n        \'function_dict\'.\n      inputs_name: Name of the input argument (in case it\'s passed as a kwarg).\n      *args: Args for the function defined by `fn_name`.\n      **kwargs: Kwargs for he function defined by `fn_name`.\n\n    Returns:\n      Output of function on filtered inputs, or vanished if all inputs are\n        vanished.\n    """"""\n    if fn_name not in self._function_dict:\n      raise ValueError(\'Unrecognized function name %s\' % fn_name)\n    fn = self._function_dict[fn_name]\n    if args:\n      inputs = args[0]\n      args = args[1:]\n    else:\n      if inputs_name not in kwargs:\n        raise ValueError(\'Missing `{}` argument.\'.format(inputs_name))\n      inputs = kwargs.pop(inputs_name)\n\n    inputs = [t for t in inputs if not is_vanished(t)]\n    return fn(inputs, *args, **kwargs) if inputs else VANISHED\n\n  def _pass_through_mask(self, layer_fn, *args, **kwargs):\n    inputs = args[0] if args else kwargs[\'inputs\']\n    return VANISHED if is_vanished(inputs) else layer_fn(*args, **kwargs)\n\n\ndef _get_from_args_or_kwargs(name, index, args, kwargs, is_required=True):\n  try:\n    return kwargs[name] if name in kwargs else args[index]\n  except IndexError:\n    if is_required:\n      raise ValueError(\'Argument `{}` is required.\'.format(name))\n    return None\n\n\ndef _get_function_name(function):\n  """"""Get a descriptive identifier for `function`.""""""\n  return \'{}.{}\'.format(function.__module__, function.__name__)\n\n\ndef hijack_module_functions(configurable_ops, module):\n  """"""Hijacks the functions from module using configurable_ops.\n\n  Overrides globally declared function reference in module with configurable_ops\n  member functions.\n\n  If a module defines global aliases in the form:\n\n  example_module.py\n    ```\n    conv2d = tr.contrib.layers.conv2d\n    fully_connected = tr.contrib.layers.fully_connected\n\n    def build_layer(inputs):\n      return conv2d(inputs, 64, 3, scope=\'demo\')\n    ```\n\n  Then this function provides the possibility to replace these aliases with\n  the members of the given `configurable_ops` object.\n\n  So after a call to `hijack_module_functions(configurable_ops, example_module)`\n  the call `example_module.build_layer(net)` will under the hood use\n  `configurable_ops.conv2d` rather than `tf.contrib.layers.conv2d`.\n\n  Note: This function could be unsafe as it depends on aliases defined in a\n  possibly external module. In addition, a function in that module that calls\n  directly, will not be affected by the hijacking, for instance:\n\n  ```\n    def build_layer_not_affected(inputs):\n      return tf.contrib.layers.conv2d(inputs, 64, 3, scope=\'bad\')\n  ```\n\n  Args:\n    configurable_ops: An ConfigurableOps object, to use functions as defined in\n    \'_DEFAULT_FUNCTION_DICT\'.\n    module: A module name to override its functions.\n\n  Returns:\n    A dict of the function pointers before the hijacking.\n  """"""\n  originals = {}\n\n  def maybe_setattr(attr):\n    """"""Sets module.attr = configurable_ops.attr if module has attr.\n\n    Overrides module.\'attr\' with configurable_ops.\'attr\', if module already has\n    an attribute name \'attr\'.\n\n    Args:\n\n      attr: Name of the attribute to override.\n    """"""\n    if hasattr(module, attr):\n      originals[attr] = getattr(module, attr)\n      setattr(module, attr, getattr(configurable_ops, attr))\n\n  for fn in _DEFAULT_FUNCTION_DICT:\n    maybe_setattr(fn)\n  return originals\n\n\ndef recover_module_functions(originals, module):\n  """"""Recovers the functions hijacked to from module.\n\n  Args:\n    originals: Dict of functions to recover. Assumes keys are a contained in\n    \'_DEFAULT_FUNCTION_DICT\'.\n    module: A module name to recover its functions.\n\n  """"""\n  for attr, original in originals.items():\n    setattr(module, attr, original)\n\n\ndef decorator_from_parameterization_file(\n    filename, fallback_rule=FallbackRule.pass_through, **kwargs):\n  """"""Create a ConfigurableOps from a parameterization file.\n\n    Loads a json parameterization file from disk\n    (as saved by tools.structure_exporter) and creates an ConfigurableOps from\n    it.\n\n  Args:\n    filename: Path to a parameterization file in json format.\n    fallback_rule: A `FallbackRule` enum which controls fallback behavior\n      (see __init__ for more detail.)\n    **kwargs: Miscellaneous args for ConfigurableOps.\n\n  Returns:\n    An ConfigurableOps instance with the parameterization from `filename`.\n  """"""\n  with tf.gfile.Open(filename, \'r\') as f:\n    parameterization = json.loads(f.read())\n    return ConfigurableOps(\n        parameterization=parameterization, fallback_rule=fallback_rule,\n        **kwargs)\n'"
morph_net/tools/configurable_ops_test.py,35,"b'""""""Tests for morph_net.tools.configurable_ops.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\n\nfrom absl import flags\n\nfrom absl.testing import parameterized\n\nfrom morph_net.tools import configurable_ops as ops\n\nimport tensorflow.compat.v1 as tf\nimport tensorflow.contrib as tf_contrib\n\nfrom tensorflow.contrib import layers\nfrom tensorflow.contrib.framework import add_arg_scope\nfrom tensorflow.contrib.framework import arg_scope\nfrom tensorflow.contrib.slim.nets import resnet_utils\nfrom tensorflow.contrib.slim.nets import resnet_v1\nfrom tensorflow.contrib.slim.nets import resnet_v2\n\n\nFLAGS = flags.FLAGS\n\n\n@add_arg_scope\ndef mock_fully_connected(*args, **kwargs):\n  return {\'mock_name\': \'myfully_connected\', \'args\': args, \'kwargs\': kwargs}\n\n\n@add_arg_scope\ndef mock_conv2d(*args, **kwargs):\n  return {\'mock_name\': \'myconv2d\', \'args\': args, \'kwargs\': kwargs}\n\n\n@add_arg_scope\ndef mock_separable_conv2d(*args, **kwargs):\n  return {\'mock_name\': \'myseparable_conv2d\', \'args\': args, \'kwargs\': kwargs}\n\n\n@add_arg_scope\ndef mock_concat(*args, **kwargs):\n  return {\'mock_name\': \'myconcat\', \'args\': args, \'kwargs\': kwargs}\n\n\n@add_arg_scope\ndef mock_add_n(*args, **kwargs):\n  return {\'mock_name\': \'myaddn\', \'args\': args, \'kwargs\': kwargs}\n\n\nclass ConfigurableOpsTest(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    super(ConfigurableOpsTest, self).setUp()\n    tf.reset_default_graph()\n    self.inputs_shape = [2, 4, 4, 3]\n    self.inputs = tf.ones(self.inputs_shape, dtype=tf.float32)\n    self.fc_inputs = tf.ones([3, 12])\n\n  def testMapBinding(self):\n    # TODO(e1): Clean up this file/test. Split to different tests\n    function_dict = {\n        \'fully_connected\': mock_fully_connected,\n        \'conv2d\': mock_conv2d,\n        \'separable_conv2d\': mock_separable_conv2d,\n        \'concat\': mock_concat,\n        \'add_n\': mock_add_n,\n    }\n    parameterization = {\n        \'fc/MatMul\': 13,\n        \'conv/Conv2D\': 15,\n        \'sep/separable_conv2d\': 17\n    }\n    num_outputs = lambda res: res[\'args\'][1]\n    decorator = ops.ConfigurableOps(\n        parameterization=parameterization, function_dict=function_dict)\n    fc = decorator.fully_connected(self.fc_inputs, num_outputs=88, scope=\'fc\')\n    self.assertEqual(\'myfully_connected\', fc[\'mock_name\'])\n    self.assertEqual(parameterization[\'fc/MatMul\'], num_outputs(fc))\n\n    conv2d = decorator.conv2d(\n        self.inputs, num_outputs=11, kernel_size=3, scope=\'conv\')\n    self.assertEqual(\'myconv2d\', conv2d[\'mock_name\'])\n    self.assertEqual(parameterization[\'conv/Conv2D\'], num_outputs(conv2d))\n\n    separable_conv2d = decorator.separable_conv2d(\n        self.inputs, num_outputs=88, kernel_size=3, scope=\'sep\')\n    self.assertEqual(\'myseparable_conv2d\', separable_conv2d[\'mock_name\'])\n    self.assertEqual(parameterization[\'sep/separable_conv2d\'],\n                     num_outputs(separable_conv2d))\n\n    concat = decorator.concat(axis=1, values=[1, None, 2])\n    self.assertEqual(concat[\'args\'][0], [1, 2])\n    self.assertEqual(concat[\'kwargs\'][\'axis\'], 1)\n    with self.assertRaises(ValueError):\n      _ = decorator.concat(inputs=[1, None, 2])\n\n    add_n = decorator.add_n(name=\'add_n\', inputs=[1, None, 2])\n    self.assertEqual(add_n[\'args\'][0], [1, 2])\n\n  def testScopeAndNameKwargs(self):\n    function_dict = {\n        \'fully_connected\': mock_fully_connected,\n        \'conv2d\': mock_conv2d,\n        \'separable_conv2d\': mock_separable_conv2d,\n        \'concat\': mock_concat,\n        \'add_n\': mock_add_n,\n    }\n    parameterization = {\n        \'fc/MatMul\': 13,\n        \'conv/Conv2D\': 15,\n        \'sep/separable_conv2d\': 17\n    }\n    num_outputs = lambda res: res[\'args\'][1]\n    decorator = ops.ConfigurableOps(\n        parameterization=parameterization, function_dict=function_dict)\n\n    conv2d = decorator.conv2d(\n        self.inputs, num_outputs=11, kernel_size=3, scope=\'conv\')\n    self.assertEqual(\'myconv2d\', conv2d[\'mock_name\'])\n    self.assertEqual(parameterization[\'conv/Conv2D\'], num_outputs(conv2d))\n\n    conv2d = decorator.conv2d(\n        self.inputs, num_outputs=11, kernel_size=3, name=\'conv\')\n    self.assertEqual(\'myconv2d\', conv2d[\'mock_name\'])\n    self.assertEqual(parameterization[\'conv/Conv2D\'], num_outputs(conv2d))\n\n  def testFullyConnectedOpAllKwargs(self):\n    decorator = ops.ConfigurableOps(parameterization={\'test/MatMul\': 13})\n    output = decorator.fully_connected(\n        inputs=self.fc_inputs, num_outputs=88, scope=\'test\')\n    self.assertEqual(13, output.shape.as_list()[-1])\n\n  def testFullyConnectedOpInputArgs(self):\n    decorator = ops.ConfigurableOps(parameterization={\'test/MatMul\': 14})\n    output = decorator.fully_connected(\n        self.fc_inputs, num_outputs=87, scope=\'test\')\n    self.assertEqual(14, output.shape.as_list()[-1])\n\n  def testFullyConnectedOpAllArgs(self):\n    decorator = ops.ConfigurableOps(parameterization={\'test/MatMul\': 15})\n    output = decorator.fully_connected(self.fc_inputs, 86, scope=\'test\')\n    self.assertEqual(15, output.shape.as_list()[-1])\n\n  def testSeparableConv2dOp(self):\n    parameterization = {\'test/separable_conv2d\': 12}\n    decorator = ops.ConfigurableOps(parameterization=parameterization)\n    output = decorator.separable_conv2d(\n        self.inputs,\n        num_outputs=88,\n        kernel_size=3,\n        depth_multiplier=1,\n        scope=\'test\')\n    self.assertEqual(12, output.shape.as_list()[-1])\n\n  def testComplexNet(self):\n    parameterization = {\'Branch0/Conv_1x1/Conv2D\': 13, \'Conv3_1x1/Conv2D\': 77}\n    decorator = ops.ConfigurableOps(parameterization=parameterization)\n\n    def conv2d(inputs, num_outputs, kernel_size, scope):\n      return decorator.conv2d(\n          inputs, num_outputs=num_outputs, kernel_size=kernel_size, scope=scope)\n\n    net = self.inputs\n\n    with tf.variable_scope(\'Branch0\'):\n      branch_0 = conv2d(net, 1, 1, scope=\'Conv_1x1\')\n    with tf.variable_scope(\'Branch1\'):\n      branch_1 = conv2d(net, 2, 1, scope=\'Conv_1x1\')\n      out_2 = conv2d(branch_1, 3, 3, scope=\'Conv_3x3\')\n    net = conv2d(net, 1, 1, scope=\'Conv3_1x1\')\n    output = tf.concat([net, branch_0, branch_1, out_2], -1)\n    expected_output_shape = self.inputs_shape\n    expected_output_shape[-1] = 95\n    self.assertEqual(expected_output_shape, output.shape.as_list())\n    self.assertEqual(2, decorator.constructed_ops[\'Branch1/Conv_1x1/Conv2D\'])\n    self.assertEqual(13, decorator.constructed_ops[\'Branch0/Conv_1x1/Conv2D\'])\n    self.assertEqual(77, decorator.constructed_ops[\'Conv3_1x1/Conv2D\'])\n    self.assertEqual(3, decorator.constructed_ops[\'Branch1/Conv_3x3/Conv2D\'])\n\n  @parameterized.named_parameters(\n      (\'_first_only\', {\n          \'first/Conv2D\': 3\n      }, (2, 2, 2, 3), (2, 4, 4, 7)),\n      (\'_second_only\', {\n          \'second/Conv2D\': 13\n      }, (2, 2, 2, 7), (2, 4, 4, 13)),\n      (\'_both\', {\n          \'first/Conv2D\': 9,\n          \'second/Conv2D\': 5\n      }, (2, 2, 2, 9), (2, 4, 4, 5)),\n  )\n  def testDifferentParameterization(self, parameterization,\n                                    expected_first_shape, expected_conv2_shape):\n    alternate_num_outputs = 7\n    decorator = ops.ConfigurableOps(parameterization=parameterization)\n    with arg_scope([layers.conv2d], padding=\'VALID\'):\n      first_out = decorator.conv2d(\n          self.inputs,\n          num_outputs=alternate_num_outputs,\n          kernel_size=3,\n          scope=\'first\')\n      conv2_out = decorator.conv2d(\n          self.inputs,\n          num_outputs=alternate_num_outputs,\n          kernel_size=1,\n          scope=\'second\')\n      self.assertAllEqual(expected_first_shape, first_out.shape.as_list())\n      self.assertAllEqual(expected_conv2_shape, conv2_out.shape.as_list())\n\n  def testShareParams(self):\n    # Tests reuse option.\n    first_outputs = 2\n    alternate_num_outputs = 12\n    parameterization = {\'first/Conv2D\': first_outputs}\n    decorator = ops.ConfigurableOps(parameterization=parameterization)\n    explicit = layers.conv2d(\n        self.inputs, first_outputs, 3, scope=\'first\')\n    with arg_scope([layers.conv2d], reuse=True):\n      decorated = decorator.conv2d(\n          self.inputs,\n          num_outputs=alternate_num_outputs,\n          kernel_size=3,\n          scope=\'first\')\n    with self.cached_session():\n      tf.global_variables_initializer().run()\n      # verifies that parameters are shared.\n      self.assertAllClose(explicit.eval(), decorated.eval())\n    conv_ops = sorted([\n        op.name\n        for op in tf.get_default_graph().get_operations()\n        if op.type == \'Conv2D\'\n    ])\n    self.assertAllEqual([\'first/Conv2D\', \'first_1/Conv2D\'], conv_ops)\n\n  def testOneDConcat(self):\n    # A 1-D tensor.\n    tensor1 = tf.constant([1])\n    # Another 1-D tensor.\n    tensor2 = tf.constant([100])\n    decorator = ops.ConfigurableOps()\n    result = decorator.concat([tensor1, tensor2], axis=0)\n    expected_tensor = tf.constant([1, 100])\n\n    self.assertAllEqual(expected_tensor, result)\n    self.assertAllEqual(\n        expected_tensor,\n        decorator.concat([None, ops.VANISHED, tensor1, tensor2], axis=0))\n\n  def testFourDConcat(self):\n    # A 4-D tensor.\n    tensor1 = tf.constant([[[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]],\n                           [[[13, 14, 15], [16, 17, 18]],\n                            [[19, 20, 21], [22, 23, 24]]]])\n    # Another 4-D tensor.\n    tensor2 = tf.constant([[[[25, 26, 27], [28, 29, 30]],\n                            [[31, 32, 33], [34, 35, 36]]],\n                           [[[37, 38, 39], [40, 41, 42]],\n                            [[43, 44, 45], [46, 47, 48]]]])\n    decorator = ops.ConfigurableOps()\n\n    result = decorator.concat([tensor1, tensor2], 3)\n    expected_tensor = tf.constant([[[[1, 2, 3, 25, 26, 27],\n                                     [4, 5, 6, 28, 29, 30]],\n                                    [[7, 8, 9, 31, 32, 33],\n                                     [10, 11, 12, 34, 35, 36]]],\n                                   [[[13, 14, 15, 37, 38, 39],\n                                     [16, 17, 18, 40, 41, 42]],\n                                    [[19, 20, 21, 43, 44, 45],\n                                     [22, 23, 24, 46, 47, 48]]]])\n\n    self.assertAllEqual(expected_tensor, result)\n    self.assertAllEqual(\n        expected_tensor,\n        decorator.concat([None, tensor1, None, tensor2, None], 3))\n\n  def testNoneConcat(self):\n    self.assertEqual(ops.ConfigurableOps().concat([None, None], 3),\n                     ops.VANISHED)\n\n  def testAddN(self):\n    t1 = tf.constant([1, 2, 3])\n    t2 = tf.constant([4, 2, 3])\n    decorator = ops.ConfigurableOps()\n    results = decorator.add_n([t1, None, t2])\n    self.assertAllEqual(results, [5, 4, 6])\n\n  def testNoneAddN(self):\n    empty = ops.VANISHED\n    self.assertEqual(ops.ConfigurableOps().add_n([None, empty]), empty)\n\n  @parameterized.named_parameters((\'_first_to_zero\', {\n      \'first/Conv2D\': 0\n  }), (\'_conv2_to_zero\', {\n      \'second/Conv2D\': 0\n  }), (\'_both_conv_to_zero\', {\n      \'first/Conv2D\': 0,\n      \'second/Conv2D\': 0\n  }))\n  def testTowerVanishes(self, parameterization):\n    depth = self.inputs.shape.as_list()[3]\n    decorator = ops.ConfigurableOps(parameterization=parameterization)\n\n    net = decorator.conv2d(\n        self.inputs, num_outputs=12, kernel_size=3, scope=\'first\')\n    net = decorator.conv2d(\n        net, num_outputs=depth, kernel_size=1, scope=\'second\')\n    self.assertTrue(ops.is_vanished(net))\n\n  def testStrict_PartialParameterizationFails(self):\n    partial_parameterization = {\'first/Conv2D\': 3}\n    default_num_outputs = 7\n    decorator = ops.ConfigurableOps(\n        parameterization=partial_parameterization, fallback_rule=\'strict\')\n    decorator.conv2d(\n        self.inputs,\n        num_outputs=default_num_outputs,\n        kernel_size=3,\n        scope=\'first\')\n    with self.assertRaisesRegexp(\n        KeyError, \'op_name \\""second/Conv2D\\"" not found in parameterization\'):\n      decorator.conv2d(\n          self.inputs,\n          num_outputs=default_num_outputs,\n          kernel_size=1,\n          scope=\'second\')\n\n  def testDefaultToZero(self):\n    parameterization = {\'first/Conv2D\': 3}\n    decorator = ops.ConfigurableOps(\n        parameterization=parameterization, fallback_rule=\'zero\')\n    first = decorator.conv2d(\n        self.inputs, num_outputs=12, kernel_size=3, scope=\'first\')\n    second = decorator.conv2d(self.inputs, 13, kernel_size=1, scope=\'second\')\n    self.assertEqual(3, first.shape.as_list()[3])\n    self.assertTrue(ops.is_vanished(second))\n    self.assertEqual(0, decorator.constructed_ops[\'second/Conv2D\'])\n\n  @parameterized.named_parameters(\n      (\'_string_fallback_rule\', \'strict\'),\n      (\'enum_fallback_rule\', ops.FallbackRule.strict))\n  def testStrict_FullParameterizationPasses(self, fallback_rule):\n    full_parameterization = {\'first/Conv2D\': 3, \'second/Conv2D\': 13}\n    default_num_outputs = 7\n    decorator = ops.ConfigurableOps(\n        parameterization=full_parameterization, fallback_rule=fallback_rule)\n    first = decorator.conv2d(\n        self.inputs,\n        num_outputs=default_num_outputs,\n        kernel_size=3,\n        scope=\'first\')\n    second = decorator.conv2d(\n        self.inputs,\n        num_outputs=default_num_outputs,\n        kernel_size=1,\n        scope=\'second\')\n\n    self.assertAllEqual(3, first.shape.as_list()[3])\n    self.assertAllEqual(13, second.shape.as_list()[3])\n\n  def testBadFallbackRule(self):\n    with self.assertRaises(KeyError):\n      ops.ConfigurableOps(fallback_rule=\'bad bad rule\')\n\n  def testWrongTypeFallbackRule(self):\n    with self.assertRaises(ValueError):\n      ops.ConfigurableOps(fallback_rule=20180207)\n\n  def testDecoratorFromParamFile(self):\n    parameterization = {\'first/Conv2D\': 3, \'second/Conv2D\': 13}\n    filename = os.path.join(FLAGS.test_tmpdir, \'parameterization_file\')\n    with tf.gfile.Open(filename, \'w\') as f:\n      json.dump(parameterization, f)\n\n    expected_decorator = ops.ConfigurableOps(\n        parameterization=parameterization,\n        fallback_rule=ops.FallbackRule.pass_through)\n\n    default_num_outputs = 7\n    _ = expected_decorator.conv2d(\n        self.inputs,\n        num_outputs=default_num_outputs,\n        kernel_size=3,\n        scope=\'first\')\n    _ = expected_decorator.conv2d(\n        self.inputs,\n        num_outputs=default_num_outputs,\n        kernel_size=1,\n        scope=\'second\')\n    decorator_from_file = ops.decorator_from_parameterization_file(\n        filename, fallback_rule=ops.FallbackRule.pass_through)\n\n    self.assertEqual(expected_decorator.constructed_ops,\n                     decorator_from_file._parameterization)\n\n  def testPool(self):\n    decorator = ops.ConfigurableOps()\n    empty = ops.VANISHED\n    pool_kwargs = dict(kernel_size=2, stride=2, padding=\'same\', scope=\'pool\')\n\n    for fn_name in [\'max_pool2d\', \'avg_pool2d\']:\n      decorator_pool_fn = getattr(decorator, fn_name)\n      decorator_regular_output = decorator_pool_fn(self.inputs, **pool_kwargs)\n      decorator_zero_output = decorator_pool_fn(empty, **pool_kwargs)\n\n      tf_pool_fn = getattr(layers, fn_name)\n      tf_output = tf_pool_fn(self.inputs, **pool_kwargs)\n\n      self.assertAllEqual(decorator_regular_output, tf_output)\n      self.assertTrue(ops.is_vanished(decorator_zero_output))\n\n  def testBatchNorm(self):\n    decorator = ops.ConfigurableOps()\n    kwargs = dict(center=False, scale=False)\n    decorator_regular_output = decorator.batch_norm(self.inputs, **kwargs)\n    decorator_zero_output = decorator.batch_norm(ops.VANISHED, **kwargs)\n    tf_output = layers.batch_norm(self.inputs, **kwargs)\n\n    with self.cached_session():\n      tf.global_variables_initializer().run()\n      self.assertAllEqual(decorator_regular_output, tf_output)\n    self.assertTrue(ops.is_vanished(decorator_zero_output))\n\n  @parameterized.named_parameters(\n      (\'_SlimLayers\', tf_contrib.slim.conv2d, \'num_outputs\', \'Conv\'),\n      (\'_ContribLayers\', tf_contrib.layers.conv2d, \'num_outputs\', \'Conv\'),\n      (\'_TfLayer\', tf.layers.conv2d, \'filters\', \'conv2d\'))\n  def testDefaultScopes_Conv(\n      self, conv_fn, num_outputs_kwarg, expected_op_scope):\n    inputs = tf.ones([1, 3, 3, 2])\n    parameterization = {\n        \'{}/Conv2D\'.format(expected_op_scope): 5\n    }\n    decorator = ops.ConfigurableOps(\n        parameterization=parameterization, function_dict={\'conv2d\': conv_fn})\n    _ = decorator.conv2d(inputs, **{num_outputs_kwarg: 8, \'kernel_size\': 2})\n    self.assertDictEqual(parameterization, decorator.constructed_ops)\n\n  @parameterized.named_parameters(\n      (\'_SlimLayers\',\n       tf_contrib.slim.fully_connected, \'num_outputs\', \'fully_connected\'),\n      (\'_ContribLayers\',\n       tf_contrib.layers.fully_connected, \'num_outputs\', \'fully_connected\'),\n      (\'_TfLayer\',\n       tf.layers.dense, \'units\', \'dense\'))\n  def testDefaultScopes_Dense(\n      self, dense_fn, num_outputs_kwarg, expected_op_scope):\n    inputs = tf.ones([1, 2])\n    parameterization = {\n        \'{}/MatMul\'.format(expected_op_scope): 5\n    }\n    decorator = ops.ConfigurableOps(\n        parameterization=parameterization,\n        function_dict={\'fully_connected\': dense_fn})\n    _ = decorator.fully_connected(inputs, **{num_outputs_kwarg: 8})\n    self.assertDictEqual(parameterization, decorator.constructed_ops)\n\n  def testDefaultScopesRepeated(self):\n    inputs = tf.ones([1, 3, 3, 2])\n    parameterization = {\n        \'s1/SeparableConv2d/separable_conv2d\': 1,\n        \'s1/SeparableConv2d_1/separable_conv2d\': 2,\n        \'s1/s2/SeparableConv2d/separable_conv2d\': 3,\n        \'s1/s2/SeparableConv2d_1/separable_conv2d\': 4,\n    }\n    decorator = ops.ConfigurableOps(\n        parameterization=parameterization,\n        function_dict={\'separable_conv2d\': tf_contrib.slim.separable_conv2d})\n\n    with tf.variable_scope(\'s1\'):\n      # first call in s1: op scope should be `s1/SeparableConv2d`\n      _ = decorator.separable_conv2d(inputs, num_outputs=8, kernel_size=2)\n\n      with tf.variable_scope(\'s2\'):\n        # first call in s2: op scope should be `s1/s2/SeparableConv2d`\n        _ = decorator.separable_conv2d(inputs, num_outputs=8, kernel_size=2)\n\n        # second call in s2: op scope should be `s1/s2/SeparableConv2d_1`\n        _ = decorator.separable_conv2d(inputs, num_outputs=8, kernel_size=2)\n\n      # second call in s1: op scope should be `s1/SeparableConv2d_1`\n      _ = decorator.separable_conv2d(inputs, num_outputs=8, kernel_size=2)\n\n    conv_op_names = [op.name for op in tf.get_default_graph().get_operations()\n                     if op.name.endswith(\'separable_conv2d\')]\n    self.assertCountEqual(parameterization, conv_op_names)\n    self.assertDictEqual(parameterization, decorator.constructed_ops)\n\n\nclass Fake(object):\n  # This Class is a cheap simulation of a module.\n  # TODO(e1): Replace with an actual test module.\n\n  def __init__(self):\n    self.conv2d = layers.conv2d\n    self.fully_connected = layers.fully_connected\n    self.separable_conv2d = layers.separable_conv2d\n    self.concat = tf.concat\n\n\nclass FakeConv2DMissing(object):\n  # This class is a cheap simulation of a module, with \'second\' missing.\n  # TODO(e1): Replace with an actual test module.\n\n  def __init__(self):\n    self.fully_connected = layers.fully_connected\n    self.separable_conv2d = layers.separable_conv2d\n\n\nclass HijackerTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters(\n      (\'Normal\', Fake(), True, True, True),\n      (\'MissingConv2d\', FakeConv2DMissing(), False, True, True))\n  def testHijack(self, fake_module, has_conv2d, has_separable_conv2d,\n                 has_fully_connected):\n    # This test verifies that hijacking works with arg scope.\n    # TODO(e1): Test that all is correct when hijacking a real module.\n    def name_and_output_fn(name):\n      # By design there is no add arg_scope here.\n      def fn(*args, **kwargs):\n        return (name, args[1], kwargs[\'scope\'])\n\n      return fn\n\n    function_dict = {\n        \'fully_connected\': name_and_output_fn(\'testing_fully_connected\'),\n        \'conv2d\': name_and_output_fn(\'testing_conv2d\'),\n        \'separable_conv2d\': name_and_output_fn(\'testing_separable_conv2d\')\n    }\n\n    decorator = ops.ConfigurableOps(function_dict=function_dict)\n    originals = ops.hijack_module_functions(decorator, fake_module)\n\n    self.assertEqual(\'conv2d\' in originals, has_conv2d)\n    self.assertEqual(\'separable_conv2d\' in originals, has_separable_conv2d)\n    self.assertEqual(\'fully_connected\' in originals, has_fully_connected)\n\n    if has_conv2d:\n      with arg_scope([fake_module.conv2d], num_outputs=2):\n        out = fake_module.conv2d(\n            inputs=tf.zeros([10, 3, 3, 4]), scope=\'test_conv2d\')\n      self.assertAllEqual([\'testing_conv2d\', 2, \'test_conv2d\'], out)\n\n    if has_fully_connected:\n      with arg_scope([fake_module.fully_connected], num_outputs=3):\n        out = fake_module.fully_connected(\n            inputs=tf.zeros([10, 4]), scope=\'test_fc\')\n      self.assertAllEqual([\'testing_fully_connected\', 3, \'test_fc\'], out)\n\n    if has_separable_conv2d:\n      with arg_scope([fake_module.separable_conv2d], num_outputs=4):\n        out = fake_module.separable_conv2d(\n            inputs=tf.zeros([10, 3, 3, 4]), scope=\'test_sep\')\n      self.assertAllEqual([\'testing_separable_conv2d\', 4, \'test_sep\'], out)\n\n  def testConcatHijack(self):\n    decorator = ops.ConfigurableOps()\n    module = Fake()\n    inputs = tf.ones([2, 3, 3, 5])\n    empty = ops.VANISHED\n    with self.assertRaises(ValueError):\n      # empty will generate an error before the hijack.\n      _ = module.concat([inputs, empty], 3).shape.as_list()\n\n    # hijacking:\n    ops.hijack_module_functions(decorator, module)\n    # Verifying success of hijack.\n    self.assertAllEqual(\n        module.concat([inputs, empty], 3).shape.as_list(), [2, 3, 3, 5])\n    self.assertTrue(ops.is_vanished(module.concat([empty, empty], 3)))\n    self.assertAllEqual(\n        module.concat([inputs, empty, inputs], 3).shape.as_list(),\n        [2, 3, 3, 10])\n\n  def testRecover(self):\n    # If this test does not work well, then it might have some bizarre effect on\n    # other tests as it changes the functions in layers\n    decorator = ops.ConfigurableOps()\n    true_separable_conv2d = layers.separable_conv2d\n    original_dict = ops.hijack_module_functions(decorator, layers)\n\n    self.assertEqual(true_separable_conv2d, original_dict[\'separable_conv2d\'])\n    # Makes sure hijacking worked.\n    self.assertNotEqual(true_separable_conv2d,\n                        layers.separable_conv2d)\n    # Recovers original ops\n    ops.recover_module_functions(original_dict, layers)\n    self.assertEqual(true_separable_conv2d, layers.separable_conv2d)\n\n  @parameterized.named_parameters((\'_v1\', \'resnet_v1\'), (\'_v2\', \'resnet_v2\'))\n  def testResnet(self, resnet_version):\n    resnets = {\'resnet_v1\': (resnet_v1, \'v1\'), \'resnet_v2\': (resnet_v2, \'v2\')}\n    resnet_module = resnets[resnet_version][0]\n\n    decorator = ops.ConfigurableOps()\n    hijacked_from_layers_lib = ops.hijack_module_functions(\n        decorator, resnet_module.layers_lib)\n    hijacked_from_utils = ops.hijack_module_functions(decorator,\n                                                      resnet_utils.layers)\n    hijacked_from_module = ops.hijack_module_functions(decorator,\n                                                       resnet_module.layers)\n    print(\'hijacked_from_layers_lib\', hijacked_from_layers_lib)\n    print(\'hijacked_from_utils\', hijacked_from_utils)\n    print(\'hijacked_from_module\', hijacked_from_module)\n    inputs = tf.ones([3, 16, 16, 5])\n    _ = resnet_module.bottleneck(\n        inputs, depth=64, depth_bottleneck=16, stride=1)\n\n    self.assertLen(decorator.constructed_ops, 4)\n\n    base_name = \'bottleneck_\' + resnets[resnet_version][1]\n    expected_decorated_ops = sorted([\n        base_name + \'/conv1/Conv2D\',\n        base_name + \'/conv2/Conv2D\',\n        base_name + \'/conv3/Conv2D\',\n        base_name + \'/shortcut/Conv2D\',\n    ])\n\n    self.assertAllEqual(expected_decorated_ops,\n                        sorted(decorator.constructed_ops.keys()))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
morph_net/tools/structure_exporter.py,9,"b'""""""Module for calculating and saving learned structures.\n\nWhen training with a network regularizer, the emerging structure of the\nnetwork is encoded in the `alive_vector`s and `regularization_vector`s of the\n`OpRegularizerManager`.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# [internal] enable type annotations\nfrom __future__ import print_function\n\nimport json\nimport os\nfrom morph_net.framework import op_regularizer_manager as orm\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom typing import Text, Sequence, Dict, Optional, IO, Iterable, Callable\n\nSUPPORTED_OPS = [\'Conv2D\', \'Conv2DBackpropInput\', \'Conv3D\']\nALIVE_FILENAME = \'alive\'\n\n\nclass StructureExporter(object):\n  """"""Reports statistics about the current state of regularization.\n\n  Obtains live activation counts for supported ops: a map from each op name\n  to its count of alive activations (filters).\n\n  Usage:\n    1. Build model.\n      `logits = build_model(parmas)`\n    2. Create network regularizer.\n      `network_regularizer = flop_regularizer.GammaFlopsRegularizer([logits.op])\n    3. Create StructureExporter:\n      `exporter = StructureExporter(net_reg.op_regularizer_manager)`\n    4. Gather tensors to eval:\n      `tensor_to_eval_dict = exporter.tensors`\n    5. Within a `tf.Session()` eval and populate tensors:\n      `exporter.populate_tensor_values(tensor_to_eval_dict.eval())`\n    6. Export structure:\n      `exporter.save_alive_counts(tf.gfile.Open(...))`\n  """"""\n\n  def __init__(self,\n               op_regularizer_manager: orm.OpRegularizerManager,\n               remove_common_prefix: bool = False) -> None:\n    """"""Build a StructureExporter object.\n\n    Args:\n      op_regularizer_manager: An OpRegularizerManager, an object that contains\n        info about every op we care about and its corresponding regularizer.\n      remove_common_prefix: A bool. If True, determine if all op names start\n        with the same prefix (up to and including the first \'/\'), and if so,\n        skip that prefix in exported data.\n    """"""\n    self._alive_vectors_as_tensors = {}  # type: Dict[Text, tf.Tensor]\n    self._alive_vectors_as_values = None  # type: Optional[Dict[Text, Sequence[bool]]]\n\n    for op in op_regularizer_manager.ops:\n      if op.type not in SUPPORTED_OPS:\n        continue\n      op_regularizer = op_regularizer_manager.get_regularizer(op)\n      if not op_regularizer:\n        tf.logging.warning(\'No regularizer found for: %s\', op.name)\n        continue\n      self._alive_vectors_as_tensors[op.name] = op_regularizer.alive_vector\n\n    if remove_common_prefix:\n      rename_op = get_remove_common_prefix_fn(self._alive_vectors_as_tensors)\n      self._alive_vectors_as_tensors = {\n          rename_op(k): v for k, v in self._alive_vectors_as_tensors.items()\n      }\n\n  @property\n  def tensors(self):\n    """"""A dictionary between op names and alive vectors.\n\n    Alive vectors are `tf.Tensor`s of type tf.int32.\n\n    Returns:\n      Dict: op name -> alive vector tensor\n    """"""\n    return self._alive_vectors_as_tensors\n\n  def populate_tensor_values(self, values: Dict[Text, Sequence[bool]]) -> None:\n    """"""Records alive values for ops regularized by op_regularizer_manager.\n\n    The given mapping must match op names from `self.tensor`.\n\n    Args:\n      values: A dict mapping op names to a boolean alive status.\n\n    Raises:\n      ValueError: If keys of input do not match keys of `self.tensor`.\n    """"""\n    # TODO(p1): Rename values to something better. values is a dict!\n    if sorted(values) != sorted(self.tensors):\n      raise ValueError(\n          \'`values` and `self.tensors` must have the same keys but are %s and %s\'\n          % (sorted(values), sorted(self.tensors)))\n    self._alive_vectors_as_values = values\n\n  def get_alive_counts(self) -> Dict[Text, int]:\n    """"""Computes alive counts.\n\n    populate_tensor_values() must have been called earlier.\n\n    Returns:\n      A dict {op_name: alive_count}, alive_count is a scalar integer tf.Tensor.\n\n    Raises:\n      RuntimeError: tensor values not populated.\n    """"""\n\n    if self._alive_vectors_as_values is None:\n      raise RuntimeError(\'Tensor values not populated.\')\n    return _compute_alive_counts(self._alive_vectors_as_values)\n\n  def save_alive_counts(self, f: IO[bytes]) -> None:\n    """"""Saves live counts to a file.\n\n    Args:\n      f: a file object where alive counts are saved.\n    """"""\n    f.write(format_structure(self.get_alive_counts()))  # pytype: disable=wrong-arg-types\n\n  def create_file_and_save_alive_counts(self, base_dir: Text,\n                                        global_step: int) -> None:\n    """"""Creates and updates files with alive counts.\n\n    Creates the directory `{base_dir}/learned_structure/` and saves the current\n    alive counts to:\n      `{base_dir}/learned_structure/{ALIVE_FILENAME}_{global_step}`.\n\n    Args:\n      base_dir: where to export the alive counts.\n      global_step: current value of global step, used as a suffix in filename.\n    """"""\n    current_filename = \'%s_%s\' % (ALIVE_FILENAME, global_step)\n    directory = os.path.join(base_dir, \'learned_structure\')\n    try:\n      tf.gfile.MakeDirs(directory)\n    except tf.errors.OpError:\n      # Probably already exists. If not, we\'ll see the error in the next line.\n      pass\n    with tf.gfile.Open(os.path.join(directory, current_filename), \'w\') as f:\n      self.save_alive_counts(f)  # pytype: disable=wrong-arg-types\n\n\n# TODO(p1): maybe check that we still end up with unique names after prefix\n# removal, and do nothing if that\'s not the case?\ndef get_remove_common_prefix_fn(iterable: Iterable[Text]\n                               ) -> Callable[[Text], Text]:\n  """"""Obtains a function that removes common prefix.\n\n  Determines if all items in iterable start with the same substring (up to and\n  including the first \'/\'). If so, returns a function str->str that removes\n  the prefix of matching length. Otherwise returns identity function.\n\n  Args:\n    iterable: strings to process.\n\n  Returns:\n    A function that removes the common prefix from a string.\n  """"""\n  try:\n    first = next(iter(iterable))\n  except StopIteration:\n    return lambda x: x\n  separator_index = first.find(\'/\')\n  if separator_index == -1:\n    return lambda x: x\n  prefix = first[:separator_index + 1]\n  if not all(k.startswith(prefix) for k in iterable):\n    return lambda x: x\n  return lambda item: item[len(prefix):]\n\n\ndef _compute_alive_counts(\n    alive_vectors: Dict[Text, Sequence[bool]]) -> Dict[Text, int]:\n  """"""Computes alive counts.\n\n  Args:\n    alive_vectors: A mapping from op_name to a vector where each element\n      indicates whether the corresponding output activation is alive.\n\n  Returns:\n    Mapping from op_name to the number of its alive output activations.\n  """"""\n  return {\n      op_name: int(np.sum(alive_vector))\n      for op_name, alive_vector in alive_vectors.items()\n  }\n\n\ndef format_structure(structure: Dict[Text, int]) -> Text:\n  return json.dumps(structure, indent=2, sort_keys=True, default=str)\n'"
morph_net/tools/structure_exporter_test.py,8,"b'""""""Tests for structure_exporter.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\n\nfrom absl import flags\nfrom absl.testing import parameterized\n\nfrom morph_net.framework import generic_regularizers\nfrom morph_net.framework import op_regularizer_manager as orm\nfrom morph_net.tools import structure_exporter as se\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import layers as contrib_layers\n\n\nFLAGS = flags.FLAGS\nlayers = contrib_layers\n\n\ndef _alive_from_file(filename):\n  with tf.gfile.Open(os.path.join(FLAGS.test_tmpdir, filename)) as f:\n    return json.loads(f.read())\n\n\nclass FakeOpReg(generic_regularizers.OpRegularizer):\n\n  def __init__(self, alive):\n    self.alive = alive\n\n  @property\n  def alive_vector(self):\n    return self.alive\n\n  @property\n  def regularization_vector(self):\n    assert False\n    return 0\n\n\nclass FakeORM(orm.OpRegularizerManager):\n\n  def __init__(self):\n    image = tf.constant(0.0, shape=[1, 17, 19, 3])\n    layers.conv2d(image, 3, 2, scope=\'X/c1\')\n    layers.conv2d(image, 4, 3, scope=\'X/c2\')\n    layers.conv2d_transpose(image, 5, 1, scope=\'X/c3\')\n\n    layers.conv3d(\n        tf.zeros(shape=[1, 4, 4, 6, 3]),\n        num_outputs=2,\n        kernel_size=3,\n        scope=\'X/c4\')\n    self.regularizer = {\n        \'X/c1/Conv2D\': FakeOpReg([True, False, True]),\n        \'X/c2/Conv2D\': FakeOpReg([True, False, True, False]),\n        \'X/c4/Conv3D\': FakeOpReg([True, False]),\n        \'X/c3/conv2d_transpose\': FakeOpReg([True, True, False, True, False])\n    }\n    self.ops = [\n        tf.get_default_graph().get_operation_by_name(op_name)\n        for op_name in self.regularizer\n    ]\n\n  def ops(self):\n    return self.ops\n\n  def get_regularizer(self, op):\n    return self.regularizer[op.name]\n\n\nclass TestStructureExporter(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    super(TestStructureExporter, self).setUp()\n    self.exporter = se.StructureExporter(\n        op_regularizer_manager=FakeORM(), remove_common_prefix=False)\n    self.tensor_value_1 = {\n        \'X/c1/Conv2D\': [True] * 3,\n        \'X/c2/Conv2D\': [False] * 4,\n        \'X/c4/Conv3D\': [False, True],\n        \'X/c3/conv2d_transpose\': [True] * 5\n    }\n    self.expected_alive_1 = {\n        \'X/c1/Conv2D\': 3,\n        \'X/c2/Conv2D\': 0,\n        \'X/c4/Conv3D\': 1,\n        \'X/c3/conv2d_transpose\': 5\n    }\n\n    self.tensor_value_2 = {\n        \'X/c1/Conv2D\': [True, False, False],\n        \'X/c4/Conv3D\': [True, True],\n        \'X/c2/Conv2D\': [True] * 4,\n        \'X/c3/conv2d_transpose\': [False, False, False, False, True]\n    }\n    self.expected_alive_2 = {\n        \'X/c1/Conv2D\': 1,\n        \'X/c2/Conv2D\': 4,\n        \'X/c4/Conv3D\': 2,\n        \'X/c3/conv2d_transpose\': 1\n    }\n\n  def test_tensors(self):\n    expected = {\n        \'X/c1/Conv2D\': [1, 0, 1],\n        \'X/c2/Conv2D\': [1, 0, 1, 0],\n        \'X/c4/Conv3D\': [1, 0],\n        \'X/c3/conv2d_transpose\': [1, 1, 0, 1, 0]\n    }\n    self.assertAllEqual(sorted(self.exporter.tensors), sorted(expected))\n    for name in self.exporter.tensors:\n      self.assertAllEqual(self.exporter.tensors[name], expected[name])\n\n  def test_populate_tensor_values(self):\n    self.exporter.populate_tensor_values(self.tensor_value_1)\n    self.assertAllEqual(self.exporter.get_alive_counts(), self.expected_alive_1)\n    self.exporter.populate_tensor_values(self.tensor_value_2)\n    self.assertAllEqual(self.exporter.get_alive_counts(), self.expected_alive_2)\n\n  def test_compute_alive_count(self):\n    self.assertAllEqual(\n        se._compute_alive_counts({\'a\': [True, False, False]}), {\'a\': 1})\n    self.assertAllEqual(\n        se._compute_alive_counts({\'b\': [False, False]}), {\'b\': 0})\n    self.assertAllEqual(\n        se._compute_alive_counts(self.tensor_value_1), self.expected_alive_1)\n    self.assertAllEqual(\n        se._compute_alive_counts(self.tensor_value_2), self.expected_alive_2)\n\n  def test_save_alive_counts(self):\n    filename = \'alive007\'\n    self.exporter.populate_tensor_values(self.tensor_value_1)\n    with tf.gfile.Open(os.path.join(FLAGS.test_tmpdir, filename), \'w\') as f:\n      self.exporter.save_alive_counts(f)\n    self.assertAllEqual(_alive_from_file(filename), self.expected_alive_1)\n\n  def test_create_file_and_save_alive_counts(self):\n    base_dir = os.path.join(FLAGS.test_tmpdir, \'ee\')\n\n    self.exporter.populate_tensor_values(self.tensor_value_1)\n    self.exporter.create_file_and_save_alive_counts(base_dir, 19)\n    self.assertAllEqual(\n        _alive_from_file(\'ee/learned_structure/alive_19\'),\n        self.expected_alive_1)\n\n    self.exporter.populate_tensor_values(self.tensor_value_2)\n    self.exporter.create_file_and_save_alive_counts(base_dir, 1009)\n    self.assertAllEqual(\n        _alive_from_file(\'ee/learned_structure/alive_1009\'),\n        self.expected_alive_2)\n    self.assertAllEqual(\n        _alive_from_file(\'ee/learned_structure/alive_19\'),\n        self.expected_alive_1)\n\n  @parameterized.parameters(\n      ([], []),\n      ([\'\', \'\'], [\'\', \'\']),\n      ([\'a\', \'a\'], [\'a\', \'a\']),  # No / present\n      ([\'/\', \'/\'], [\'\', \'\']),\n      ([\'/x\', \'/x\'], [\'x\', \'x\']),\n      ([\'a/x\', \'a/x\'], [\'x\', \'x\']),\n      ([\'abc/\', \'abc/\', \'abc/\'], [\'\', \'\', \'\']),\n      ([\'abc/x\', \'abc/x\', \'abd/x\'], [\'abc/x\', \'abc/x\', \'abd/x\']),\n      ([\'abc/x\', \'abc/y\', \'abc/z\'], [\'x\', \'y\', \'z\']),\n      ([\'abc/x/\', \'abc/y\', \'abc/z/\'], [\'x/\', \'y\', \'z/\']),\n      ([\'abc/x/\', \'abc/y/\', \'abc/z/\'], [\'x/\', \'y/\', \'z/\']),\n  )\n  def test_find_common_prefix_size(self, iterable, expected_result):\n    rename_op = se.get_remove_common_prefix_fn(iterable)\n    self.assertEqual(expected_result, list(map(rename_op, iterable)))\n\n\nclass TestStructureExporterRemovePrefix(tf.test.TestCase):\n\n  def test_removes_prefix(self):\n    exporter = se.StructureExporter(\n        op_regularizer_manager=FakeORM(), remove_common_prefix=True)\n    expected = [\'c1/Conv2D\', \'c2/Conv2D\', \'c4/Conv3D\', \'c3/conv2d_transpose\']\n    self.assertAllEqual(sorted(exporter.tensors), sorted(expected))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
examples/slim/datasets/__init__.py,0,b'\n'
examples/slim/datasets/build_imagenet_data.py,31,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts ImageNet data to TFRecords file format with Example protos.\n\nThe raw ImageNet data set is expected to reside in JPEG files located in the\nfollowing directory structure.\n\n  data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n  data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n  ...\n\nwhere \'n01440764\' is the unique synset label associated with\nthese images.\n\nThe training data set consists of 1000 sub-directories (i.e. labels)\neach containing 1200 JPEG images for a total of 1.2M JPEG images.\n\nThe evaluation data set consists of 1000 sub-directories (i.e. labels)\neach containing 50 JPEG images for a total of 50K JPEG images.\n\nThis TensorFlow script converts the training and evaluation data into\na sharded data set consisting of 1024 and 128 TFRecord files, respectively.\n\n  train_directory/train-00000-of-01024\n  train_directory/train-00001-of-01024\n  ...\n  train_directory/train-00127-of-01024\n\nand\n\n  validation_directory/validation-00000-of-00128\n  validation_directory/validation-00001-of-00128\n  ...\n  validation_directory/validation-00127-of-00128\n\nEach validation TFRecord file contains ~390 records. Each training TFREcord\nfile contains ~1250 records. Each record within the TFRecord file is a\nserialized Example proto. The Example proto contains the following fields:\n\n  image/encoded: string containing JPEG encoded image in RGB colorspace\n  image/height: integer, image height in pixels\n  image/width: integer, image width in pixels\n  image/colorspace: string, specifying the colorspace, always \'RGB\'\n  image/channels: integer, specifying the number of channels, always 3\n  image/format: string, specifying the format, always\'JPEG\'\n\n  image/filename: string containing the basename of the image file\n            e.g. \'n01440764_10026.JPEG\' or \'ILSVRC2012_val_00000293.JPEG\'\n  image/class/label: integer specifying the index in a classification layer.\n    The label ranges from [1, 1000] where 0 is not used.\n  image/class/synset: string specifying the unique ID of the label,\n    e.g. \'n01440764\'\n  image/class/text: string specifying the human-readable version of the label\n    e.g. \'red fox, Vulpes vulpes\'\n\n  image/object/bbox/xmin: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/xmax: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/ymin: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/ymax: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/label: integer specifying the index in a classification\n    layer. The label ranges from [1, 1000] where 0 is not used. Note this is\n    always identical to the image label.\n\nNote that the length of xmin is identical to the length of xmax, ymin and ymax\nfor each example.\n\nRunning this script using 16 threads may take around ~2.5 hours on a HP Z420.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os\nimport random\nimport sys\nimport threading\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow.compat.v1 as tf\n\n\ntf.app.flags.DEFINE_string(\'train_directory\', \'/tmp/\',\n                           \'Training data directory\')\ntf.app.flags.DEFINE_string(\'validation_directory\', \'/tmp/\',\n                           \'Validation data directory\')\ntf.app.flags.DEFINE_string(\'output_directory\', \'/tmp/\',\n                           \'Output data directory\')\n\ntf.app.flags.DEFINE_integer(\'train_shards\', 1024,\n                            \'Number of shards in training TFRecord files.\')\ntf.app.flags.DEFINE_integer(\'validation_shards\', 128,\n                            \'Number of shards in validation TFRecord files.\')\n\ntf.app.flags.DEFINE_integer(\'num_threads\', 8,\n                            \'Number of threads to preprocess the images.\')\n\n# The labels file contains a list of valid labels are held in this file.\n# Assumes that the file contains entries as such:\n#   n01440764\n#   n01443537\n#   n01484850\n# where each line corresponds to a label expressed as a synset. We map\n# each synset contained in the file to an integer (based on the alphabetical\n# ordering). See below for details.\ntf.app.flags.DEFINE_string(\'labels_file\',\n                           \'imagenet_lsvrc_2015_synsets.txt\',\n                           \'Labels file\')\n\n# This file containing mapping from synset to human-readable label.\n# Assumes each line of the file looks like:\n#\n#   n02119247    black fox\n#   n02119359    silver fox\n#   n02119477    red fox, Vulpes fulva\n#\n# where each line corresponds to a unique mapping. Note that each line is\n# formatted as <synset>\\t<human readable label>.\ntf.app.flags.DEFINE_string(\'imagenet_metadata_file\',\n                           \'imagenet_metadata.txt\',\n                           \'ImageNet metadata file\')\n\n# This file is the output of process_bounding_box.py\n# Assumes each line of the file looks like:\n#\n#   n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n#\n# where each line corresponds to one bounding box annotation associated\n# with an image. Each line can be parsed as:\n#\n#   <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n#\n# Note that there might exist mulitple bounding box annotations associated\n# with an image file.\ntf.app.flags.DEFINE_string(\'bounding_box_file\',\n                           \'./imagenet_2012_bounding_boxes.csv\',\n                           \'Bounding box file\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _int64_feature(value):\n  """"""Wrapper for inserting int64 features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef _float_feature(value):\n  """"""Wrapper for inserting float features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef _bytes_feature(value):\n  """"""Wrapper for inserting bytes features into Example proto.""""""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _convert_to_example(filename, image_buffer, label, synset, human, bbox,\n                        height, width):\n  """"""Build an Example proto for an example.\n\n  Args:\n    filename: string, path to an image file, e.g., \'/path/to/example.JPG\'\n    image_buffer: string, JPEG encoding of RGB image\n    label: integer, identifier for the ground truth for the network\n    synset: string, unique WordNet ID specifying the label, e.g., \'n02323233\'\n    human: string, human-readable label, e.g., \'red fox, Vulpes vulpes\'\n    bbox: list of bounding boxes; each box is a list of integers\n      specifying [xmin, ymin, xmax, ymax]. All boxes are assumed to belong to\n      the same label as the image label.\n    height: integer, image height in pixels\n    width: integer, image width in pixels\n  Returns:\n    Example proto\n  """"""\n  xmin = []\n  ymin = []\n  xmax = []\n  ymax = []\n  for b in bbox:\n    assert len(b) == 4\n    # pylint: disable=expression-not-assigned\n    [l.append(point) for l, point in zip([xmin, ymin, xmax, ymax], b)]\n    # pylint: enable=expression-not-assigned\n\n  colorspace = \'RGB\'\n  channels = 3\n  image_format = \'JPEG\'\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      \'image/height\': _int64_feature(height),\n      \'image/width\': _int64_feature(width),\n      \'image/colorspace\': _bytes_feature(colorspace),\n      \'image/channels\': _int64_feature(channels),\n      \'image/class/label\': _int64_feature(label),\n      \'image/class/synset\': _bytes_feature(synset),\n      \'image/class/text\': _bytes_feature(human),\n      \'image/object/bbox/xmin\': _float_feature(xmin),\n      \'image/object/bbox/xmax\': _float_feature(xmax),\n      \'image/object/bbox/ymin\': _float_feature(ymin),\n      \'image/object/bbox/ymax\': _float_feature(ymax),\n      \'image/object/bbox/label\': _int64_feature([label] * len(xmin)),\n      \'image/format\': _bytes_feature(image_format),\n      \'image/filename\': _bytes_feature(os.path.basename(filename)),\n      \'image/encoded\': _bytes_feature(image_buffer)}))\n  return example\n\n\nclass ImageCoder(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Create a single Session to run all image coding calls.\n    self._sess = tf.Session()\n\n    # Initializes function that converts PNG to JPEG data.\n    self._png_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_png(self._png_data, channels=3)\n    self._png_to_jpeg = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that converts CMYK JPEG data to RGB JPEG data.\n    self._cmyk_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_jpeg(self._cmyk_data, channels=0)\n    self._cmyk_to_rgb = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def png_to_jpeg(self, image_data):\n    return self._sess.run(self._png_to_jpeg,\n                          feed_dict={self._png_data: image_data})\n\n  def cmyk_to_rgb(self, image_data):\n    return self._sess.run(self._cmyk_to_rgb,\n                          feed_dict={self._cmyk_data: image_data})\n\n  def decode_jpeg(self, image_data):\n    image = self._sess.run(self._decode_jpeg,\n                           feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _is_png(filename):\n  """"""Determine if a file contains a PNG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a PNG.\n  """"""\n  # File list from:\n  # https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU\n  return \'n02105855_2933.JPEG\' in filename\n\n\ndef _is_cmyk(filename):\n  """"""Determine if file contains a CMYK JPEG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a JPEG encoded with CMYK color space.\n  """"""\n  # File list from:\n  # https://github.com/cytsai/ilsvrc-cmyk-image-list\n  blacklist = [\'n01739381_1309.JPEG\', \'n02077923_14822.JPEG\',\n               \'n02447366_23489.JPEG\', \'n02492035_15739.JPEG\',\n               \'n02747177_10752.JPEG\', \'n03018349_4028.JPEG\',\n               \'n03062245_4620.JPEG\', \'n03347037_9675.JPEG\',\n               \'n03467068_12171.JPEG\', \'n03529860_11437.JPEG\',\n               \'n03544143_17228.JPEG\', \'n03633091_5218.JPEG\',\n               \'n03710637_5125.JPEG\', \'n03961711_5286.JPEG\',\n               \'n04033995_2932.JPEG\', \'n04258138_17003.JPEG\',\n               \'n04264628_27969.JPEG\', \'n04336792_7448.JPEG\',\n               \'n04371774_5854.JPEG\', \'n04596742_4225.JPEG\',\n               \'n07583066_647.JPEG\', \'n13037406_4650.JPEG\']\n  return filename.split(\'/\')[-1] in blacklist\n\n\ndef _process_image(filename, coder):\n  """"""Process a single image file.\n\n  Args:\n    filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n  Returns:\n    image_buffer: string, JPEG encoding of RGB image.\n    height: integer, image height in pixels.\n    width: integer, image width in pixels.\n  """"""\n  # Read the image file.\n  image_data = tf.gfile.GFile(filename, \'r\').read()\n\n  # Clean the dirty data.\n  if _is_png(filename):\n    # 1 image is a PNG.\n    print(\'Converting PNG to JPEG for %s\' % filename)\n    image_data = coder.png_to_jpeg(image_data)\n  elif _is_cmyk(filename):\n    # 22 JPEG images are in CMYK colorspace.\n    print(\'Converting CMYK to RGB for %s\' % filename)\n    image_data = coder.cmyk_to_rgb(image_data)\n\n  # Decode the RGB JPEG.\n  image = coder.decode_jpeg(image_data)\n\n  # Check that image converted to RGB\n  assert len(image.shape) == 3\n  height = image.shape[0]\n  width = image.shape[1]\n  assert image.shape[2] == 3\n\n  return image_data, height, width\n\n\ndef _process_image_files_batch(coder, thread_index, ranges, name, filenames,\n                               synsets, labels, humans, bboxes, num_shards):\n  """"""Processes and saves list of images as TFRecord in 1 thread.\n\n  Args:\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    thread_index: integer, unique batch to run index is within [0, len(ranges)).\n    ranges: list of pairs of integers specifying ranges of each batches to\n      analyze in parallel.\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    synsets: list of strings; each string is a unique WordNet ID\n    labels: list of integer; each integer identifies the ground truth\n    humans: list of strings; each string is a human-readable label\n    bboxes: list of bounding boxes for each image. Note that each entry in this\n      list might contain from 0+ entries corresponding to the number of bounding\n      box annotations for the image.\n    num_shards: integer number of shards for this data set.\n  """"""\n  # Each thread produces N shards where N = int(num_shards / num_threads).\n  # For instance, if num_shards = 128, and the num_threads = 2, then the first\n  # thread would produce shards [0, 64).\n  num_threads = len(ranges)\n  assert not num_shards % num_threads\n  num_shards_per_batch = int(num_shards / num_threads)\n\n  shard_ranges = np.linspace(ranges[thread_index][0],\n                             ranges[thread_index][1],\n                             num_shards_per_batch + 1).astype(int)\n  num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n\n  counter = 0\n  for s in xrange(num_shards_per_batch):\n    # Generate a sharded version of the file name, e.g. \'train-00002-of-00010\'\n    shard = thread_index * num_shards_per_batch + s\n    output_filename = \'%s-%.5d-of-%.5d\' % (name, shard, num_shards)\n    output_file = os.path.join(FLAGS.output_directory, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    shard_counter = 0\n    files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n      synset = synsets[i]\n      human = humans[i]\n      bbox = bboxes[i]\n\n      image_buffer, height, width = _process_image(filename, coder)\n\n      example = _convert_to_example(filename, image_buffer, label,\n                                    synset, human, bbox,\n                                    height, width)\n      writer.write(example.SerializeToString())\n      shard_counter += 1\n      counter += 1\n\n      if not counter % 1000:\n        print(\'%s [thread %d]: Processed %d of %d images in thread batch.\' %\n              (datetime.now(), thread_index, counter, num_files_in_thread))\n        sys.stdout.flush()\n\n    writer.close()\n    print(\'%s [thread %d]: Wrote %d images to %s\' %\n          (datetime.now(), thread_index, shard_counter, output_file))\n    sys.stdout.flush()\n    shard_counter = 0\n  print(\'%s [thread %d]: Wrote %d images to %d shards.\' %\n        (datetime.now(), thread_index, counter, num_files_in_thread))\n  sys.stdout.flush()\n\n\ndef _process_image_files(name, filenames, synsets, labels, humans,\n                         bboxes, num_shards):\n  """"""Process and save list of images as TFRecord of Example protos.\n\n  Args:\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    synsets: list of strings; each string is a unique WordNet ID\n    labels: list of integer; each integer identifies the ground truth\n    humans: list of strings; each string is a human-readable label\n    bboxes: list of bounding boxes for each image. Note that each entry in this\n      list might contain from 0+ entries corresponding to the number of bounding\n      box annotations for the image.\n    num_shards: integer number of shards for this data set.\n  """"""\n  assert len(filenames) == len(synsets)\n  assert len(filenames) == len(labels)\n  assert len(filenames) == len(humans)\n  assert len(filenames) == len(bboxes)\n\n  # Break all images into batches with a [ranges[i][0], ranges[i][1]].\n  spacing = np.linspace(0, len(filenames), FLAGS.num_threads + 1).astype(np.int)\n  ranges = []\n  threads = []\n  for i in xrange(len(spacing) - 1):\n    ranges.append([spacing[i], spacing[i+1]])\n\n  # Launch a thread for each batch.\n  print(\'Launching %d threads for spacings: %s\' % (FLAGS.num_threads, ranges))\n  sys.stdout.flush()\n\n  # Create a mechanism for monitoring when all threads are finished.\n  coord = tf.train.Coordinator()\n\n  # Create a generic TensorFlow-based utility for converting all image codings.\n  coder = ImageCoder()\n\n  threads = []\n  for thread_index in xrange(len(ranges)):\n    args = (coder, thread_index, ranges, name, filenames,\n            synsets, labels, humans, bboxes, num_shards)\n    t = threading.Thread(target=_process_image_files_batch, args=args)\n    t.start()\n    threads.append(t)\n\n  # Wait for all the threads to terminate.\n  coord.join(threads)\n  print(\'%s: Finished writing all %d images in data set.\' %\n        (datetime.now(), len(filenames)))\n  sys.stdout.flush()\n\n\ndef _find_image_files(data_dir, labels_file):\n  """"""Build a list of all images files and labels in the data set.\n\n  Args:\n    data_dir: string, path to the root directory of images.\n\n      Assumes that the ImageNet data set resides in JPEG files located in\n      the following directory structure.\n\n        data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n        data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n\n      where \'n01440764\' is the unique synset label associated with these images.\n\n    labels_file: string, path to the labels file.\n\n      The list of valid labels are held in this file. Assumes that the file\n      contains entries as such:\n        n01440764\n        n01443537\n        n01484850\n      where each line corresponds to a label expressed as a synset. We map\n      each synset contained in the file to an integer (based on the alphabetical\n      ordering) starting with the integer 1 corresponding to the synset\n      contained in the first line.\n\n      The reason we start the integer labels at 1 is to reserve label 0 as an\n      unused background class.\n\n  Returns:\n    filenames: list of strings; each string is a path to an image file.\n    synsets: list of strings; each string is a unique WordNet ID.\n    labels: list of integer; each integer identifies the ground truth.\n  """"""\n  print(\'Determining list of input files and labels from %s.\' % data_dir)\n  challenge_synsets = [\n      l.strip() for l in tf.gfile.GFile(labels_file, \'r\').readlines()\n  ]\n\n  labels = []\n  filenames = []\n  synsets = []\n\n  # Leave label index 0 empty as a background class.\n  label_index = 1\n\n  # Construct the list of JPEG files and labels.\n  for synset in challenge_synsets:\n    jpeg_file_path = \'%s/%s/*.JPEG\' % (data_dir, synset)\n    matching_files = tf.gfile.Glob(jpeg_file_path)\n\n    labels.extend([label_index] * len(matching_files))\n    synsets.extend([synset] * len(matching_files))\n    filenames.extend(matching_files)\n\n    if not label_index % 100:\n      print(\'Finished finding files in %d of %d classes.\' % (\n          label_index, len(challenge_synsets)))\n    label_index += 1\n\n  # Shuffle the ordering of all image files in order to guarantee\n  # random ordering of the images with respect to label in the\n  # saved TFRecord files. Make the randomization repeatable.\n  shuffled_index = range(len(filenames))\n  random.seed(12345)\n  random.shuffle(shuffled_index)\n\n  filenames = [filenames[i] for i in shuffled_index]\n  synsets = [synsets[i] for i in shuffled_index]\n  labels = [labels[i] for i in shuffled_index]\n\n  print(\'Found %d JPEG files across %d labels inside %s.\' %\n        (len(filenames), len(challenge_synsets), data_dir))\n  return filenames, synsets, labels\n\n\ndef _find_human_readable_labels(synsets, synset_to_human):\n  """"""Build a list of human-readable labels.\n\n  Args:\n    synsets: list of strings; each string is a unique WordNet ID.\n    synset_to_human: dict of synset to human labels, e.g.,\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n\n  Returns:\n    List of human-readable strings corresponding to each synset.\n  """"""\n  humans = []\n  for s in synsets:\n    assert s in synset_to_human, (\'Failed to find: %s\' % s)\n    humans.append(synset_to_human[s])\n  return humans\n\n\ndef _find_image_bounding_boxes(filenames, image_to_bboxes):\n  """"""Find the bounding boxes for a given image file.\n\n  Args:\n    filenames: list of strings; each string is a path to an image file.\n    image_to_bboxes: dictionary mapping image file names to a list of\n      bounding boxes. This list contains 0+ bounding boxes.\n  Returns:\n    List of bounding boxes for each image. Note that each entry in this\n    list might contain from 0+ entries corresponding to the number of bounding\n    box annotations for the image.\n  """"""\n  num_image_bbox = 0\n  bboxes = []\n  for f in filenames:\n    basename = os.path.basename(f)\n    if basename in image_to_bboxes:\n      bboxes.append(image_to_bboxes[basename])\n      num_image_bbox += 1\n    else:\n      bboxes.append([])\n  print(\'Found %d images with bboxes out of %d images\' % (\n      num_image_bbox, len(filenames)))\n  return bboxes\n\n\ndef _process_dataset(name, directory, num_shards, synset_to_human,\n                     image_to_bboxes):\n  """"""Process a complete data set and save it as a TFRecord.\n\n  Args:\n    name: string, unique identifier specifying the data set.\n    directory: string, root path to the data set.\n    num_shards: integer number of shards for this data set.\n    synset_to_human: dict of synset to human labels, e.g.,\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n    image_to_bboxes: dictionary mapping image file names to a list of\n      bounding boxes. This list contains 0+ bounding boxes.\n  """"""\n  filenames, synsets, labels = _find_image_files(directory, FLAGS.labels_file)\n  humans = _find_human_readable_labels(synsets, synset_to_human)\n  bboxes = _find_image_bounding_boxes(filenames, image_to_bboxes)\n  _process_image_files(name, filenames, synsets, labels,\n                       humans, bboxes, num_shards)\n\n\ndef _build_synset_lookup(imagenet_metadata_file):\n  """"""Build lookup for synset to human-readable label.\n\n  Args:\n    imagenet_metadata_file: string, path to file containing mapping from\n      synset to human-readable label.\n\n      Assumes each line of the file looks like:\n\n        n02119247    black fox\n        n02119359    silver fox\n        n02119477    red fox, Vulpes fulva\n\n      where each line corresponds to a unique mapping. Note that each line is\n      formatted as <synset>\\t<human readable label>.\n\n  Returns:\n    Dictionary of synset to human labels, such as:\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n  """"""\n  lines = tf.gfile.GFile(imagenet_metadata_file, \'r\').readlines()\n  synset_to_human = {}\n  for l in lines:\n    if l:\n      parts = l.strip().split(\'\\t\')\n      assert len(parts) == 2\n      synset = parts[0]\n      human = parts[1]\n      synset_to_human[synset] = human\n  return synset_to_human\n\n\ndef _build_bounding_box_lookup(bounding_box_file):\n  """"""Build a lookup from image file to bounding boxes.\n\n  Args:\n    bounding_box_file: string, path to file with bounding boxes annotations.\n\n      Assumes each line of the file looks like:\n\n        n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n\n      where each line corresponds to one bounding box annotation associated\n      with an image. Each line can be parsed as:\n\n        <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n\n      Note that there might exist mulitple bounding box annotations associated\n      with an image file. This file is the output of process_bounding_boxes.py.\n\n  Returns:\n    Dictionary mapping image file names to a list of bounding boxes. This list\n    contains 0+ bounding boxes.\n  """"""\n  lines = tf.gfile.GFile(bounding_box_file, \'r\').readlines()\n  images_to_bboxes = {}\n  num_bbox = 0\n  num_image = 0\n  for l in lines:\n    if l:\n      parts = l.split(\',\')\n      assert len(parts) == 5, (\'Failed to parse: %s\' % l)\n      filename = parts[0]\n      xmin = float(parts[1])\n      ymin = float(parts[2])\n      xmax = float(parts[3])\n      ymax = float(parts[4])\n      box = [xmin, ymin, xmax, ymax]\n\n      if filename not in images_to_bboxes:\n        images_to_bboxes[filename] = []\n        num_image += 1\n      images_to_bboxes[filename].append(box)\n      num_bbox += 1\n\n  print(\'Successfully read %d bounding boxes \'\n        \'across %d images.\' % (num_bbox, num_image))\n  return images_to_bboxes\n\n\ndef main(unused_argv):\n  assert not FLAGS.train_shards % FLAGS.num_threads, (\n      \'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards\')\n  assert not FLAGS.validation_shards % FLAGS.num_threads, (\n      \'Please make the FLAGS.num_threads commensurate with \'\n      \'FLAGS.validation_shards\')\n  print(\'Saving results to %s\' % FLAGS.output_directory)\n\n  # Build a map from synset to human-readable label.\n  synset_to_human = _build_synset_lookup(FLAGS.imagenet_metadata_file)\n  image_to_bboxes = _build_bounding_box_lookup(FLAGS.bounding_box_file)\n\n  # Run it!\n  _process_dataset(\'validation\', FLAGS.validation_directory,\n                   FLAGS.validation_shards, synset_to_human, image_to_bboxes)\n  _process_dataset(\'train\', FLAGS.train_directory, FLAGS.train_shards,\n                   synset_to_human, image_to_bboxes)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
examples/slim/datasets/build_visualwakewords_data.py,14,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Build Visual WakeWords Dataset with images and labels for person/not-person.\n\nThis script generates the Visual WakeWords dataset annotations from\nthe raw COCO dataset and converts them to TFRecord.\nVisual WakeWords Dataset derives from the COCO dataset to design tiny models\nclassifying two classes, such as person/not-person. The COCO annotations\nare filtered to two classes: foreground_class_of_interest and background\n( for e.g. person and not-person). Bounding boxes for small objects\nwith area less than 5% of the image area are filtered out.\n\nThe resulting annotations file has the following fields, where\nthe image and categories fields are same as COCO dataset, while the annotation\nfield corresponds to the foreground_class_of_interest/background class and\nbounding boxes for the foreground_class_of_interest class.\n\n  images{""id"", ""width"", ""height"", ""file_name"", ""license"", ""flickr_url"",\n  ""coco_url"", ""date_captured"",}\n\n  annotations{\n  ""image_id"", object[{""category_id"", ""area"", ""bbox"" : [x,y,width,height],}]\n  ""count"",\n  ""label""\n  }\n\n  categories[{\n  ""id"", ""name"", ""supercategory"",\n  }]\n\n\nThe TFRecord file contains the following features:\n{ image/height, image/width, image/source_id, image/encoded,\n  image/class/label_text, image/class/label,\n  image/object/class/text,\n  image/object/bbox/ymin, image/object/bbox/xmin, image/object/bbox/ymax,\n  image/object/bbox/xmax, image/object/area\n  image/filename, image/format, image/key/sha256}\nFor classification models, you need the image/encoded and image/class/label.\nPlease note that this tool creates sharded output files.\n\nExample usage:\nAdd folder tensorflow/models/research/slim to your PYTHONPATH,\nand from this folder, run the following commands:\n\n    bash download_mscoco.sh path-to-mscoco-dataset\n    TRAIN_IMAGE_DIR=""path-to-mscoco-dataset/train2014""\n    VAL_IMAGE_DIR=""path-to-mscoco-dataset/val2014""\n\n    TRAIN_ANNOTATIONS_FILE=""path-to-mscoco-dataset/annotations/instances_train2014.json""\n    VAL_ANNOTATIONS_FILE=""path-to-mscoco-dataset/annotations/instances_val2014.json""\n\n    python datasets/build_visualwakewords_data.py --logtostderr \\\n      --train_image_dir=""${TRAIN_IMAGE_DIR}"" \\\n      --val_image_dir=""${VAL_IMAGE_DIR}"" \\\n      --train_annotations_file=""${TRAIN_ANNOTATIONS_FILE}"" \\\n      --val_annotations_file=""${VAL_ANNOTATIONS_FILE}"" \\\n      --output_dir=""${OUTPUT_DIR}"" \\\n      --small_object_area_threshold=0.005 \\\n      --foreground_class_of_interest=\'person\'\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow.compat.v1 as tf\nfrom datasets import build_visualwakewords_data_lib\n\nflags = tf.app.flags\ntf.flags.DEFINE_string(\'train_image_dir\', \'\', \'Training image directory.\')\ntf.flags.DEFINE_string(\'val_image_dir\', \'\', \'Validation image directory.\')\ntf.flags.DEFINE_string(\'train_annotations_file\', \'\',\n                       \'Training annotations JSON file.\')\ntf.flags.DEFINE_string(\'val_annotations_file\', \'\',\n                       \'Validation annotations JSON file.\')\ntf.flags.DEFINE_string(\'output_dir\', \'/tmp/\', \'Output data directory.\')\ntf.flags.DEFINE_float(\n    \'small_object_area_threshold\', 0.005,\n    \'Threshold of fraction of image area below which small\'\n    \'objects are filtered\')\ntf.flags.DEFINE_string(\n    \'foreground_class_of_interest\', \'person\',\n    \'Build a binary classifier based on the presence or absence\'\n    \'of this object in the scene (default is person/not-person)\')\n\nFLAGS = flags.FLAGS\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef main(unused_argv):\n  # Path to COCO dataset images and annotations\n  assert FLAGS.train_image_dir, \'`train_image_dir` missing.\'\n  assert FLAGS.val_image_dir, \'`val_image_dir` missing.\'\n  assert FLAGS.train_annotations_file, \'`train_annotations_file` missing.\'\n  assert FLAGS.val_annotations_file, \'`val_annotations_file` missing.\'\n  visualwakewords_annotations_train = os.path.join(\n      FLAGS.output_dir, \'instances_visualwakewords_train2014.json\')\n  visualwakewords_annotations_val = os.path.join(\n      FLAGS.output_dir, \'instances_visualwakewords_val2014.json\')\n  visualwakewords_labels_filename = os.path.join(FLAGS.output_dir,\n                                                 \'labels.txt\')\n  small_object_area_threshold = FLAGS.small_object_area_threshold\n  foreground_class_of_interest = FLAGS.foreground_class_of_interest\n  # Create the Visual WakeWords annotations from COCO annotations\n  if not tf.gfile.IsDirectory(FLAGS.output_dir):\n    tf.gfile.MakeDirs(FLAGS.output_dir)\n  build_visualwakewords_data_lib.create_visual_wakeword_annotations(\n      FLAGS.train_annotations_file, visualwakewords_annotations_train,\n      small_object_area_threshold, foreground_class_of_interest,\n      visualwakewords_labels_filename)\n  build_visualwakewords_data_lib.create_visual_wakeword_annotations(\n      FLAGS.val_annotations_file, visualwakewords_annotations_val,\n      small_object_area_threshold, foreground_class_of_interest,\n      visualwakewords_labels_filename)\n\n  # Create the TF Records for Visual WakeWords Dataset\n  if not tf.gfile.IsDirectory(FLAGS.output_dir):\n    tf.gfile.MakeDirs(FLAGS.output_dir)\n  train_output_path = os.path.join(FLAGS.output_dir, \'train.record\')\n  val_output_path = os.path.join(FLAGS.output_dir, \'val.record\')\n  build_visualwakewords_data_lib.create_tf_record_for_visualwakewords_dataset(\n      visualwakewords_annotations_train,\n      FLAGS.train_image_dir,\n      train_output_path,\n      num_shards=100)\n  build_visualwakewords_data_lib.create_tf_record_for_visualwakewords_dataset(\n      visualwakewords_annotations_val,\n      FLAGS.val_image_dir,\n      val_output_path,\n      num_shards=10)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
examples/slim/datasets/build_visualwakewords_data_lib.py,16,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Generate Visual Wakewords Dataset.\n\n    Helper functions to generate the Visual WakeWords dataset. It filters raw\n    COCO annotations file to Visual WakeWords Dataset annotations.\n    The resulting annotations and COCO images are then\n    converted to TF records.\n    See build_visualwakewords_data.py for the sample usage.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport hashlib\nimport io\nimport json\nimport os\nimport contextlib2\n\nimport PIL.Image\n\nimport tensorflow.compat.v1 as tf\n\nfrom datasets import dataset_utils\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef create_visual_wakeword_annotations(annotations_file,\n                                       visualwakewords_annotations_path,\n                                       small_object_area_threshold,\n                                       foreground_class_of_interest,\n                                       visualwakewords_labels_filename):\n  """"""Generate visual wakewords annotations file.\n\n  Loads COCO annotation json files and filters to person/not-person\n  class (or user-specified class) to generate visual wakewords annotations file.\n  Each image is assigned a label 1 or 0. The label 1 is assigned as long\n  as it has at least one foreground_class_of_interest (e.g. person)\n  bounding box greater than 5% of the image area.\n\n  Args:\n    annotations_file: JSON file containing COCO bounding box annotations\n    visualwakewords_annotations_path: output path to annotations file\n    small_object_area_threshold: threshold on fraction of image area below which\n      small object bounding boxes are filtered\n    foreground_class_of_interest: category from COCO dataset that is filtered by\n      the visual wakewords dataset\n    visualwakewords_labels_filename: The filename to write the visual wakewords\n      label file\n  """"""\n  # default object of interest is person\n  foreground_class_of_interest_id = 1\n  with tf.gfile.GFile(annotations_file, \'r\') as fid:\n    groundtruth_data = json.load(fid)\n    images = groundtruth_data[\'images\']\n    # Create category index\n    category_index = {}\n    for category in groundtruth_data[\'categories\']:\n      if category[\'name\'] == foreground_class_of_interest:\n        foreground_class_of_interest_id = category[\'id\']\n        category_index[category[\'id\']] = category\n\n    # Create annotations index\n    annotations_index = {}\n    annotations_index_filtered = {}\n    if \'annotations\' in groundtruth_data:\n      tf.logging.info(\n          \'Found groundtruth annotations. Building annotations index.\')\n      for annotation in groundtruth_data[\'annotations\']:\n        image_id = annotation[\'image_id\']\n        if image_id not in annotations_index:\n          annotations_index[image_id] = []\n          annotations_index_filtered[image_id] = []\n        annotations_index[image_id].append(annotation)\n      missing_annotation_count = 0\n      for image in images:\n        image_id = image[\'id\']\n        if image_id not in annotations_index:\n          missing_annotation_count += 1\n          annotations_index[image_id] = []\n          annotations_index_filtered[image_id] = []\n      tf.logging.info(\'%d images are missing annotations.\',\n                      missing_annotation_count)\n    # Create filtered annotations index\n    for idx, image in enumerate(images):\n      if idx % 100 == 0:\n        tf.logging.info(\'On image %d of %d\', idx, len(images))\n      annotations_list = annotations_index[image[\'id\']]\n      annotations_list_filtered = _filter_annotations_list(\n          annotations_list, image, small_object_area_threshold,\n          foreground_class_of_interest_id)\n      annotations_index_filtered[image[\'id\']].append(annotations_list_filtered)\n    # Output Visual WakeWords annotations and labels\n    labels_to_class_names = {0: \'background\', 1: foreground_class_of_interest}\n    with open(visualwakewords_labels_filename, \'w\') as fp:\n      for label in labels_to_class_names:\n        fp.write(str(label) + \':\' + str(labels_to_class_names[label]) + \'\\n\')\n    with open(visualwakewords_annotations_path, \'w\') as fp:\n      json.dump(\n          {\n              \'images\': images,\n              \'annotations\': annotations_index_filtered,\n              \'categories\': category_index\n          }, fp)\n\n\ndef _filter_annotations_list(annotations_list, image,\n                             small_object_area_threshold,\n                             foreground_class_of_interest_id):\n  """"""Filters COCO annotations_list to visual wakewords annotations_list.\n\n  Each image is assigned a label 1 or 0. The label 1 is assigned as long\n  as it has at least one foreground_class_of_interest (e.g. person)\n  bounding box greater than 5% of the image area.\n\n  Args:\n    annotations_list: list of dicts with keys: [ u\'id\', u\'image_id\',\n    u\'category_id\', u\'segmentation\', u\'area\', u\'bbox\' : [x,y,width,height],\n      u\'iscrowd\']. Notice that bounding box coordinates in the official COCO\n      dataset are given as [x, y, width, height] tuples using absolute\n      coordinates where x, y represent the top-left (0-indexed) corner.\n    image: dict with keys: [u\'license\', u\'file_name\', u\'coco_url\', u\'height\',\n      u\'width\', u\'date_captured\', u\'flickr_url\', u\'id\']\n    small_object_area_threshold: threshold on fraction of image area below which\n      small objects are filtered\n    foreground_class_of_interest_id: category of COCO dataset which visual\n      wakewords filters\n\n  Returns:\n    filtered_annotations_list: list of dicts with keys: [ u\'image_id\',\n    u\'label\', u\'category_id\', u\'count\',\n    u\'object\':[{""category_id"", ""area"", ""bbox"" : [x,y,width,height],}]\n  """"""\n  category_ids = []\n  area = []\n  flag_small_object = []\n  num_ann = 0\n  image_height = image[\'height\']\n  image_width = image[\'width\']\n  image_area = image_height * image_width\n  bbox = []\n  # count of filtered object\n  count = 0\n  for object_annotations in annotations_list:\n    (x, y, width, height) = tuple(object_annotations[\'bbox\'])\n    category_id = int(object_annotations[\'category_id\'])\n    category_ids.append(category_id)\n    obj_area = object_annotations[\'area\']\n    normalized_object_area = obj_area / image_area\n    # Filter small object bounding boxes\n    if category_id == foreground_class_of_interest_id:\n      if normalized_object_area < small_object_area_threshold:\n        flag_small_object.append(True)\n      else:\n        flag_small_object.append(False)\n        bbox.append({\n            u\'bbox\': [x, y, width, height],\n            u\'area\': obj_area,\n            u\'category_id\': category_id\n        })\n        count = count + 1\n    area.append(obj_area)\n    num_ann = num_ann + 1\n  # Filtered annotations_list with two classes corresponding to\n  # foreground_class_of_interest_id (e.g. person) and\n  # background (e.g. not-person)\n  if (foreground_class_of_interest_id in category_ids) and (\n      False in flag_small_object):\n    return {\n        u\'image_id\': image[\'id\'],\n        u\'label\': 1,\n        u\'object\': bbox,\n        u\'count\': count\n    }\n  else:\n    return {u\'image_id\': image[\'id\'], u\'label\': 0, u\'object\': [], u\'count\': 0}\n\n\ndef create_tf_record_for_visualwakewords_dataset(annotations_file, image_dir,\n                                                 output_path, num_shards):\n  """"""Loads Visual WakeWords annotations/images and converts to tf.Record format.\n\n  Args:\n    annotations_file: JSON file containing bounding box annotations.\n    image_dir: Directory containing the image files.\n    output_path: Path to output tf.Record file.\n    num_shards: number of output file shards.\n  """"""\n  with contextlib2.ExitStack() as tf_record_close_stack, \\\n      tf.gfile.GFile(annotations_file, \'r\') as fid:\n    output_tfrecords = dataset_utils.open_sharded_output_tfrecords(\n        tf_record_close_stack, output_path, num_shards)\n    groundtruth_data = json.load(fid)\n    images = groundtruth_data[\'images\']\n\n    category_index = {}\n    for category in groundtruth_data[\'categories\'].values():\n      # if not background class\n      if category[\'id\'] != 0:\n        category_index[category[\'id\']] = category\n\n    annotations_index = {}\n    if \'annotations\' in groundtruth_data:\n      tf.logging.info(\n          \'Found groundtruth annotations. Building annotations index.\')\n      for annotation in groundtruth_data[\'annotations\'].values():\n        image_id = annotation[0][\'image_id\']\n        if image_id not in annotations_index:\n          annotations_index[image_id] = []\n        annotations_index[image_id].append(annotation[0])\n    missing_annotation_count = 0\n    for image in images:\n      image_id = image[\'id\']\n      if image_id not in annotations_index:\n        missing_annotation_count += 1\n        annotations_index[image_id] = []\n    tf.logging.info(\'%d images are missing annotations.\',\n                    missing_annotation_count)\n\n    total_num_annotations_skipped = 0\n    for idx, image in enumerate(images):\n      if idx % 100 == 0:\n        tf.logging.info(\'On image %d of %d\', idx, len(images))\n      annotations_list = annotations_index[image[\'id\']]\n      _, tf_example, num_annotations_skipped = _create_tf_example(\n          image, annotations_list[0], image_dir)\n      total_num_annotations_skipped += num_annotations_skipped\n      shard_idx = idx % num_shards\n      output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n    tf.logging.info(\'Finished writing, skipped %d annotations.\',\n                    total_num_annotations_skipped)\n\n\ndef _create_tf_example(image, annotations_list, image_dir):\n  """"""Converts image and annotations to a tf.Example proto.\n\n  Args:\n    image: dict with keys: [u\'license\', u\'file_name\', u\'coco_url\', u\'height\',\n      u\'width\', u\'date_captured\', u\'flickr_url\', u\'id\']\n    annotations_list:\n      list of dicts with keys: [u\'image_id\', u\'bbox\', u\'label\',\n      object[{""category_id"", ""area"", ""bbox"" : [x,y,width,height],}]]. Notice\n        that bounding box coordinates in the COCO dataset are given as [x, y,\n        width, height] tuples using absolute coordinates where x, y represent\n        the top-left (0-indexed) corner. This function converts to the format\n        that can be used by the Tensorflow Object Detection API (which is [ymin,\n        xmin, ymax, xmax] with coordinates normalized relative to image size).\n    image_dir: directory containing the image files.\n\n  Returns:\n    example: The converted tf.Example\n    num_annotations_skipped: Number of (invalid) annotations that were ignored.\n\n  Raises:\n    ValueError: if the image pointed to by data[\'filename\'] is not a valid JPEG\n  """"""\n  image_height = image[\'height\']\n  image_width = image[\'width\']\n  filename = image[\'file_name\']\n  image_id = image[\'id\']\n\n  full_path = os.path.join(image_dir, filename)\n  with tf.gfile.GFile(full_path, \'rb\') as fid:\n    encoded_jpg = fid.read()\n  encoded_jpg_io = io.BytesIO(encoded_jpg)\n  image = PIL.Image.open(encoded_jpg_io)\n  key = hashlib.sha256(encoded_jpg).hexdigest()\n\n  xmin = []\n  xmax = []\n  ymin = []\n  ymax = []\n  category_ids = []\n  area = []\n  num_annotations_skipped = 0\n  label = annotations_list[\'label\']\n  for object_annotations in annotations_list[\'object\']:\n    (x, y, width, height) = tuple(object_annotations[\'bbox\'])\n    if width <= 0 or height <= 0:\n      num_annotations_skipped += 1\n      continue\n    if x + width > image_width or y + height > image_height:\n      num_annotations_skipped += 1\n      continue\n    xmin.append(float(x) / image_width)\n    xmax.append(float(x + width) / image_width)\n    ymin.append(float(y) / image_height)\n    ymax.append(float(y + height) / image_height)\n    category_id = int(object_annotations[\'category_id\'])\n    category_ids.append(category_id)\n    area.append(object_annotations[\'area\'])\n\n  feature_dict = {\n      \'image/height\':\n          dataset_utils.int64_feature(image_height),\n      \'image/width\':\n          dataset_utils.int64_feature(image_width),\n      \'image/filename\':\n          dataset_utils.bytes_feature(filename.encode(\'utf8\')),\n      \'image/source_id\':\n          dataset_utils.bytes_feature(str(image_id).encode(\'utf8\')),\n      \'image/key/sha256\':\n          dataset_utils.bytes_feature(key.encode(\'utf8\')),\n      \'image/encoded\':\n          dataset_utils.bytes_feature(encoded_jpg),\n      \'image/format\':\n          dataset_utils.bytes_feature(\'jpeg\'.encode(\'utf8\')),\n      \'image/class/label\':\n          dataset_utils.int64_feature(label),\n      \'image/object/bbox/xmin\':\n          dataset_utils.float_list_feature(xmin),\n      \'image/object/bbox/xmax\':\n          dataset_utils.float_list_feature(xmax),\n      \'image/object/bbox/ymin\':\n          dataset_utils.float_list_feature(ymin),\n      \'image/object/bbox/ymax\':\n          dataset_utils.float_list_feature(ymax),\n      \'image/object/class/label\':\n          dataset_utils.int64_feature(label),\n      \'image/object/area\':\n          dataset_utils.float_list_feature(area),\n  }\n  example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n  return key, example, num_annotations_skipped\n'"
examples/slim/datasets/cifar10.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the Cifar10 dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/research/slim/datasets/download_and_convert_cifar10.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow.compat.v1 as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'cifar10_%s.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 50000, \'test\': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A [32 x 32 x 3] color image.\',\n    \'label\': \'A single integer between 0 and 9\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading cifar10.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if not reader:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(shape=[32, 32, 3]),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
examples/slim/datasets/dataset_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A factory-pattern class which returns classification image/label pairs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets import cifar10\nfrom datasets import flowers\nfrom datasets import imagenet\nfrom datasets import mnist\nfrom datasets import visualwakewords\n\ndatasets_map = {\n    \'cifar10\': cifar10,\n    \'flowers\': flowers,\n    \'imagenet\': imagenet,\n    \'mnist\': mnist,\n    \'visualwakewords\': visualwakewords,\n}\n\n\ndef get_dataset(name, split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Given a dataset name and a split_name returns a Dataset.\n\n  Args:\n    name: String, the name of the dataset.\n    split_name: A train/test split name.\n    dataset_dir: The directory where the dataset files are stored.\n    file_pattern: The file pattern to use for matching the dataset source files.\n    reader: The subclass of tf.ReaderBase. If left as `None`, then the default\n      reader defined by each dataset is used.\n\n  Returns:\n    A `Dataset` class.\n\n  Raises:\n    ValueError: If the dataset `name` is unknown.\n  """"""\n  if name not in datasets_map:\n    raise ValueError(\'Name of dataset unknown %s\' % name)\n  return datasets_map[name].get_split(\n      split_name,\n      dataset_dir,\n      file_pattern,\n      reader)\n'"
examples/slim/datasets/dataset_utils.py,10,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains utilities for downloading and converting datasets.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow.compat.v1 as tf\n\nLABELS_FILENAME = \'labels.txt\'\n\n\ndef int64_feature(values):\n  """"""Returns a TF-Feature of int64s.\n\n  Args:\n    values: A scalar or list of values.\n\n  Returns:\n    A TF-Feature.\n  """"""\n  if not isinstance(values, (tuple, list)):\n    values = [values]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\n\ndef bytes_list_feature(values):\n  """"""Returns a TF-Feature of list of bytes.\n\n  Args:\n    values: A string or list of strings.\n\n  Returns:\n    A TF-Feature.\n  """"""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=values))\n\n\ndef float_list_feature(values):\n  """"""Returns a TF-Feature of list of floats.\n\n  Args:\n    values: A float or list of floats.\n\n  Returns:\n    A TF-Feature.\n  """"""\n  return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n\n\ndef bytes_feature(values):\n  """"""Returns a TF-Feature of bytes.\n\n  Args:\n    values: A string.\n\n  Returns:\n    A TF-Feature.\n  """"""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n\n\ndef float_feature(values):\n  """"""Returns a TF-Feature of floats.\n\n  Args:\n    values: A scalar of list of values.\n\n  Returns:\n    A TF-Feature.\n  """"""\n  if not isinstance(values, (tuple, list)):\n    values = [values]\n  return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n\n\ndef image_to_tfexample(image_data, image_format, height, width, class_id):\n  return tf.train.Example(features=tf.train.Features(feature={\n      \'image/encoded\': bytes_feature(image_data),\n      \'image/format\': bytes_feature(image_format),\n      \'image/class/label\': int64_feature(class_id),\n      \'image/height\': int64_feature(height),\n      \'image/width\': int64_feature(width),\n  }))\n\n\ndef download_and_uncompress_tarball(tarball_url, dataset_dir):\n  """"""Downloads the `tarball_url` and uncompresses it locally.\n\n  Args:\n    tarball_url: The URL of a tarball file.\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = tarball_url.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n\n  def _progress(count, block_size, total_size):\n    sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n        filename, float(count * block_size) / float(total_size) * 100.0))\n    sys.stdout.flush()\n  filepath, _ = urllib.request.urlretrieve(tarball_url, filepath, _progress)\n  print()\n  statinfo = os.stat(filepath)\n  print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef write_label_file(labels_to_class_names, dataset_dir,\n                     filename=LABELS_FILENAME):\n  """"""Writes a file with the list of class names.\n\n  Args:\n    labels_to_class_names: A map of (integer) labels to class names.\n    dataset_dir: The directory in which the labels file should be written.\n    filename: The filename where the class names are written.\n  """"""\n  labels_filename = os.path.join(dataset_dir, filename)\n  with tf.gfile.Open(labels_filename, \'w\') as f:\n    for label in labels_to_class_names:\n      class_name = labels_to_class_names[label]\n      f.write(\'%d:%s\\n\' % (label, class_name))\n\n\ndef has_labels(dataset_dir, filename=LABELS_FILENAME):\n  """"""Specifies whether or not the dataset directory contains a label map file.\n\n  Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n  Returns:\n    `True` if the labels file exists and `False` otherwise.\n  """"""\n  return tf.gfile.Exists(os.path.join(dataset_dir, filename))\n\n\ndef read_label_file(dataset_dir, filename=LABELS_FILENAME):\n  """"""Reads the labels file and returns a mapping from ID to class name.\n\n  Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n  Returns:\n    A map from a label (integer) to class name.\n  """"""\n  labels_filename = os.path.join(dataset_dir, filename)\n  with tf.gfile.Open(labels_filename, \'rb\') as f:\n    lines = f.read().decode()\n  lines = lines.split(\'\\n\')\n  lines = filter(None, lines)\n\n  labels_to_class_names = {}\n  for line in lines:\n    index = line.index(\':\')\n    labels_to_class_names[int(line[:index])] = line[index+1:]\n  return labels_to_class_names\n\n\ndef open_sharded_output_tfrecords(exit_stack, base_path, num_shards):\n  """"""Opens all TFRecord shards for writing and adds them to an exit stack.\n\n  Args:\n    exit_stack: A context2.ExitStack used to automatically closed the TFRecords\n      opened in this function.\n    base_path: The base path for all shards\n    num_shards: The number of shards\n\n  Returns:\n    The list of opened TFRecords. Position k in the list corresponds to shard k.\n  """"""\n  tf_record_output_filenames = [\n      \'{}-{:05d}-of-{:05d}\'.format(base_path, idx, num_shards)\n      for idx in range(num_shards)\n  ]\n\n  tfrecords = [\n      exit_stack.enter_context(tf.python_io.TFRecordWriter(file_name))\n      for file_name in tf_record_output_filenames\n  ]\n\n  return tfrecords\n'"
examples/slim/datasets/download_and_convert_cifar10.py,12,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts cifar10 data to TFRecords of TF-Example protos.\n\nThis module downloads the cifar10 data, uncompresses it, reads the files\nthat make up the cifar10 data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take several minutes to run.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tarfile\n\nimport numpy as np\nfrom six.moves import cPickle\nfrom six.moves import urllib\nimport tensorflow.compat.v1 as tf\n\nfrom datasets import dataset_utils\n\n# The URL where the CIFAR data can be downloaded.\n_DATA_URL = \'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'\n\n# The number of training files.\n_NUM_TRAIN_FILES = 5\n\n# The height and width of each image.\n_IMAGE_SIZE = 32\n\n# The names of the classes.\n_CLASS_NAMES = [\n    \'airplane\',\n    \'automobile\',\n    \'bird\',\n    \'cat\',\n    \'deer\',\n    \'dog\',\n    \'frog\',\n    \'horse\',\n    \'ship\',\n    \'truck\',\n]\n\n\ndef _add_to_tfrecord(filename, tfrecord_writer, offset=0):\n  """"""Loads data from the cifar10 pickle files and writes files to a TFRecord.\n\n  Args:\n    filename: The filename of the cifar10 pickle file.\n    tfrecord_writer: The TFRecord writer to use for writing.\n    offset: An offset into the absolute number of images previously written.\n\n  Returns:\n    The new offset.\n  """"""\n  with tf.gfile.Open(filename, \'rb\') as f:\n    if sys.version_info < (3,):\n      data = cPickle.load(f)\n    else:\n      data = cPickle.load(f, encoding=\'bytes\')\n\n  images = data[b\'data\']\n  num_images = images.shape[0]\n\n  images = images.reshape((num_images, 3, 32, 32))\n  labels = data[b\'labels\']\n\n  with tf.Graph().as_default():\n    image_placeholder = tf.placeholder(dtype=tf.uint8)\n    encoded_image = tf.image.encode_png(image_placeholder)\n\n    with tf.Session(\'\') as sess:\n\n      for j in range(num_images):\n        sys.stdout.write(\'\\r>> Reading file [%s] image %d/%d\' % (\n            filename, offset + j + 1, offset + num_images))\n        sys.stdout.flush()\n\n        image = np.squeeze(images[j]).transpose((1, 2, 0))\n        label = labels[j]\n\n        png_string = sess.run(encoded_image,\n                              feed_dict={image_placeholder: image})\n\n        example = dataset_utils.image_to_tfexample(\n            png_string, b\'png\', _IMAGE_SIZE, _IMAGE_SIZE, label)\n        tfrecord_writer.write(example.SerializeToString())\n\n  return offset + num_images\n\n\ndef _get_output_filename(dataset_dir, split_name):\n  """"""Creates the output filename.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n    split_name: The name of the train/test split.\n\n  Returns:\n    An absolute file path.\n  """"""\n  return \'%s/cifar10_%s.tfrecord\' % (dataset_dir, split_name)\n\n\ndef _download_and_uncompress_dataset(dataset_dir):\n  """"""Downloads cifar10 and uncompresses it locally.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n\n  if not os.path.exists(filepath):\n    def _progress(count, block_size, total_size):\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n          filename, float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(_DATA_URL, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n    tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n  tf.gfile.Remove(filepath)\n\n  tmp_dir = os.path.join(dataset_dir, \'cifar-10-batches-py\')\n  tf.gfile.DeleteRecursively(tmp_dir)\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  training_filename = _get_output_filename(dataset_dir, \'train\')\n  testing_filename = _get_output_filename(dataset_dir, \'test\')\n\n  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n\n  # First, process the training data:\n  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:\n    offset = 0\n    for i in range(_NUM_TRAIN_FILES):\n      filename = os.path.join(dataset_dir,\n                              \'cifar-10-batches-py\',\n                              \'data_batch_%d\' % (i + 1))  # 1-indexed.\n      offset = _add_to_tfrecord(filename, tfrecord_writer, offset)\n\n  # Next, process the testing data:\n  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:\n    filename = os.path.join(dataset_dir,\n                            \'cifar-10-batches-py\',\n                            \'test_batch\')\n    _add_to_tfrecord(filename, tfrecord_writer)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the Cifar10 dataset!\')\n'"
examples/slim/datasets/download_and_convert_flowers.py,11,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts Flowers data to TFRecords of TF-Example protos.\n\nThis module downloads the Flowers data, uncompresses it, reads the files\nthat make up the Flowers data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take about a minute to run.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\nimport random\nimport sys\n\nimport tensorflow.compat.v1 as tf\n\nfrom datasets import dataset_utils\n\n# The URL where the Flowers data can be downloaded.\n_DATA_URL = \'http://download.tensorflow.org/example_images/flower_photos.tgz\'\n\n# The number of images in the validation set.\n_NUM_VALIDATION = 350\n\n# Seed for repeatability.\n_RANDOM_SEED = 0\n\n# The number of shards per dataset split.\n_NUM_SHARDS = 5\n\n\nclass ImageReader(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def read_image_dims(self, sess, image_data):\n    image = self.decode_jpeg(sess, image_data)\n    return image.shape[0], image.shape[1]\n\n  def decode_jpeg(self, sess, image_data):\n    image = sess.run(self._decode_jpeg,\n                     feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _get_filenames_and_classes(dataset_dir):\n  """"""Returns a list of filenames and inferred class names.\n\n  Args:\n    dataset_dir: A directory containing a set of subdirectories representing\n      class names. Each subdirectory should contain PNG or JPG encoded images.\n\n  Returns:\n    A list of image file paths, relative to `dataset_dir` and the list of\n    subdirectories, representing class names.\n  """"""\n  flower_root = os.path.join(dataset_dir, \'flower_photos\')\n  directories = []\n  class_names = []\n  for filename in os.listdir(flower_root):\n    path = os.path.join(flower_root, filename)\n    if os.path.isdir(path):\n      directories.append(path)\n      class_names.append(filename)\n\n  photo_filenames = []\n  for directory in directories:\n    for filename in os.listdir(directory):\n      path = os.path.join(directory, filename)\n      photo_filenames.append(path)\n\n  return photo_filenames, sorted(class_names)\n\n\ndef _get_dataset_filename(dataset_dir, split_name, shard_id):\n  output_filename = \'flowers_%s_%05d-of-%05d.tfrecord\' % (\n      split_name, shard_id, _NUM_SHARDS)\n  return os.path.join(dataset_dir, output_filename)\n\n\ndef _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir):\n  """"""Converts the given filenames to a TFRecord dataset.\n\n  Args:\n    split_name: The name of the dataset, either \'train\' or \'validation\'.\n    filenames: A list of absolute paths to png or jpg images.\n    class_names_to_ids: A dictionary from class names (strings) to ids\n      (integers).\n    dataset_dir: The directory where the converted datasets are stored.\n  """"""\n  assert split_name in [\'train\', \'validation\']\n\n  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n\n  with tf.Graph().as_default():\n    image_reader = ImageReader()\n\n    with tf.Session(\'\') as sess:\n\n      for shard_id in range(_NUM_SHARDS):\n        output_filename = _get_dataset_filename(\n            dataset_dir, split_name, shard_id)\n\n        with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n          start_ndx = shard_id * num_per_shard\n          end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n          for i in range(start_ndx, end_ndx):\n            sys.stdout.write(\'\\r>> Converting image %d/%d shard %d\' % (\n                i+1, len(filenames), shard_id))\n            sys.stdout.flush()\n\n            # Read the filename:\n            image_data = tf.gfile.GFile(filenames[i], \'rb\').read()\n            height, width = image_reader.read_image_dims(sess, image_data)\n\n            class_name = os.path.basename(os.path.dirname(filenames[i]))\n            class_id = class_names_to_ids[class_name]\n\n            example = dataset_utils.image_to_tfexample(\n                image_data, b\'jpg\', height, width, class_id)\n            tfrecord_writer.write(example.SerializeToString())\n\n  sys.stdout.write(\'\\n\')\n  sys.stdout.flush()\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n  tf.gfile.Remove(filepath)\n\n  tmp_dir = os.path.join(dataset_dir, \'flower_photos\')\n  tf.gfile.DeleteRecursively(tmp_dir)\n\n\ndef _dataset_exists(dataset_dir):\n  for split_name in [\'train\', \'validation\']:\n    for shard_id in range(_NUM_SHARDS):\n      output_filename = _get_dataset_filename(\n          dataset_dir, split_name, shard_id)\n      if not tf.gfile.Exists(output_filename):\n        return False\n  return True\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  if _dataset_exists(dataset_dir):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n  photo_filenames, class_names = _get_filenames_and_classes(dataset_dir)\n  class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n\n  # Divide into train and test:\n  random.seed(_RANDOM_SEED)\n  random.shuffle(photo_filenames)\n  training_filenames = photo_filenames[_NUM_VALIDATION:]\n  validation_filenames = photo_filenames[:_NUM_VALIDATION]\n\n  # First, convert the training and validation sets.\n  _convert_dataset(\'train\', training_filenames, class_names_to_ids,\n                   dataset_dir)\n  _convert_dataset(\'validation\', validation_filenames, class_names_to_ids,\n                   dataset_dir)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the Flowers dataset!\')\n'"
examples/slim/datasets/download_and_convert_mnist.py,11,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts MNIST data to TFRecords of TF-Example protos.\n\nThis module downloads the MNIST data, uncompresses it, reads the files\nthat make up the MNIST data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take about a minute to run.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport sys\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow.compat.v1 as tf\n\nfrom datasets import dataset_utils\n\n# The URLs where the MNIST data can be downloaded.\n_DATA_URL = \'http://yann.lecun.com/exdb/mnist/\'\n_TRAIN_DATA_FILENAME = \'train-images-idx3-ubyte.gz\'\n_TRAIN_LABELS_FILENAME = \'train-labels-idx1-ubyte.gz\'\n_TEST_DATA_FILENAME = \'t10k-images-idx3-ubyte.gz\'\n_TEST_LABELS_FILENAME = \'t10k-labels-idx1-ubyte.gz\'\n\n_IMAGE_SIZE = 28\n_NUM_CHANNELS = 1\n\n# The names of the classes.\n_CLASS_NAMES = [\n    \'zero\',\n    \'one\',\n    \'two\',\n    \'three\',\n    \'four\',\n    \'five\',\n    \'size\',\n    \'seven\',\n    \'eight\',\n    \'nine\',\n]\n\n\ndef _extract_images(filename, num_images):\n  """"""Extract the images into a numpy array.\n\n  Args:\n    filename: The path to an MNIST images file.\n    num_images: The number of images in the file.\n\n  Returns:\n    A numpy array of shape [number_of_images, height, width, channels].\n  """"""\n  print(\'Extracting images from: \', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(16)\n    buf = bytestream.read(\n        _IMAGE_SIZE * _IMAGE_SIZE * num_images * _NUM_CHANNELS)\n    data = np.frombuffer(buf, dtype=np.uint8)\n    data = data.reshape(num_images, _IMAGE_SIZE, _IMAGE_SIZE, _NUM_CHANNELS)\n  return data\n\n\ndef _extract_labels(filename, num_labels):\n  """"""Extract the labels into a vector of int64 label IDs.\n\n  Args:\n    filename: The path to an MNIST labels file.\n    num_labels: The number of labels in the file.\n\n  Returns:\n    A numpy array of shape [number_of_labels]\n  """"""\n  print(\'Extracting labels from: \', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(8)\n    buf = bytestream.read(1 * num_labels)\n    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n  return labels\n\n\ndef _add_to_tfrecord(data_filename, labels_filename, num_images,\n                     tfrecord_writer):\n  """"""Loads data from the binary MNIST files and writes files to a TFRecord.\n\n  Args:\n    data_filename: The filename of the MNIST images.\n    labels_filename: The filename of the MNIST labels.\n    num_images: The number of images in the dataset.\n    tfrecord_writer: The TFRecord writer to use for writing.\n  """"""\n  images = _extract_images(data_filename, num_images)\n  labels = _extract_labels(labels_filename, num_images)\n\n  shape = (_IMAGE_SIZE, _IMAGE_SIZE, _NUM_CHANNELS)\n  with tf.Graph().as_default():\n    image = tf.placeholder(dtype=tf.uint8, shape=shape)\n    encoded_png = tf.image.encode_png(image)\n\n    with tf.Session(\'\') as sess:\n      for j in range(num_images):\n        sys.stdout.write(\'\\r>> Converting image %d/%d\' % (j + 1, num_images))\n        sys.stdout.flush()\n\n        png_string = sess.run(encoded_png, feed_dict={image: images[j]})\n\n        example = dataset_utils.image_to_tfexample(\n            png_string, \'png\'.encode(), _IMAGE_SIZE, _IMAGE_SIZE, labels[j])\n        tfrecord_writer.write(example.SerializeToString())\n\n\ndef _get_output_filename(dataset_dir, split_name):\n  """"""Creates the output filename.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n    split_name: The name of the train/test split.\n\n  Returns:\n    An absolute file path.\n  """"""\n  return \'%s/mnist_%s.tfrecord\' % (dataset_dir, split_name)\n\n\ndef _download_dataset(dataset_dir):\n  """"""Downloads MNIST locally.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  for filename in [_TRAIN_DATA_FILENAME,\n                   _TRAIN_LABELS_FILENAME,\n                   _TEST_DATA_FILENAME,\n                   _TEST_LABELS_FILENAME]:\n    filepath = os.path.join(dataset_dir, filename)\n\n    if not os.path.exists(filepath):\n      print(\'Downloading file %s...\' % filename)\n      def _progress(count, block_size, total_size):\n        sys.stdout.write(\'\\r>> Downloading %.1f%%\' % (\n            float(count * block_size) / float(total_size) * 100.0))\n        sys.stdout.flush()\n      filepath, _ = urllib.request.urlretrieve(_DATA_URL + filename,\n                                               filepath,\n                                               _progress)\n      print()\n      with tf.gfile.GFile(filepath) as f:\n        size = f.size()\n      print(\'Successfully downloaded\', filename, size, \'bytes.\')\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  for filename in [_TRAIN_DATA_FILENAME,\n                   _TRAIN_LABELS_FILENAME,\n                   _TEST_DATA_FILENAME,\n                   _TEST_LABELS_FILENAME]:\n    filepath = os.path.join(dataset_dir, filename)\n    tf.gfile.Remove(filepath)\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  training_filename = _get_output_filename(dataset_dir, \'train\')\n  testing_filename = _get_output_filename(dataset_dir, \'test\')\n\n  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  _download_dataset(dataset_dir)\n\n  # First, process the training data:\n  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:\n    data_filename = os.path.join(dataset_dir, _TRAIN_DATA_FILENAME)\n    labels_filename = os.path.join(dataset_dir, _TRAIN_LABELS_FILENAME)\n    _add_to_tfrecord(data_filename, labels_filename, 60000, tfrecord_writer)\n\n  # Next, process the testing data:\n  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:\n    data_filename = os.path.join(dataset_dir, _TEST_DATA_FILENAME)\n    labels_filename = os.path.join(dataset_dir, _TEST_LABELS_FILENAME)\n    _add_to_tfrecord(data_filename, labels_filename, 10000, tfrecord_writer)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the MNIST dataset!\')\n'"
examples/slim/datasets/flowers.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the flowers dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/research/slim/datasets/download_and_convert_flowers.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow.compat.v1 as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'flowers_%s_*.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 3320, \'validation\': 350}\n\n_NUM_CLASSES = 5\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying size.\',\n    \'label\': \'A single integer between 0 and 4\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading flowers.\n\n  Args:\n    split_name: A train/validation split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/validation split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
examples/slim/datasets/imagenet.py,20,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the ImageNet ILSVRC 2012 Dataset plus some bounding boxes.\n\nSome images have one or more bounding boxes associated with the label of the\nimage. See details here: http://image-net.org/download-bboxes\n\nImageNet is based upon WordNet 3.0. To uniquely identify a synset, we use\n""WordNet ID"" (wnid), which is a concatenation of POS ( i.e. part of speech )\nand SYNSET OFFSET of WordNet. For more information, please refer to the\nWordNet documentation[http://wordnet.princeton.edu/wordnet/documentation/].\n\n""There are bounding boxes for over 3000 popular synsets available.\nFor each synset, there are on average 150 images with bounding boxes.""\n\nWARNING: Don\'t use for object detection, in this case all the bounding boxes\nof the image belong to just one class.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom six.moves import urllib\nimport tensorflow.compat.v1 as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n# TODO(nsilberman): Add tfrecord file type once the script is updated.\n_FILE_PATTERN = \'%s-*\'\n\n_SPLITS_TO_SIZES = {\n    \'train\': 1281167,\n    \'validation\': 50000,\n}\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying height and width.\',\n    \'label\': \'The label id of the image, integer between 0 and 999\',\n    \'label_text\': \'The text of the label.\',\n    \'object/bbox\': \'A list of bounding boxes.\',\n    \'object/label\': \'A list of labels, one per each object.\',\n}\n\n_NUM_CLASSES = 1001\n\n# If set to false, will not try to set label_to_names in dataset\n# by reading them from labels.txt or github.\nLOAD_READABLE_NAMES = True\n\n\ndef create_readable_names_for_imagenet_labels():\n  """"""Create a dict mapping label id to human readable string.\n\n  Returns:\n      labels_to_names: dictionary where keys are integers from to 1000\n      and values are human-readable names.\n\n  We retrieve a synset file, which contains a list of valid synset labels used\n  by ILSVRC competition. There is one synset one per line, eg.\n          #   n01440764\n          #   n01443537\n  We also retrieve a synset_to_human_file, which contains a mapping from synsets\n  to human-readable names for every synset in Imagenet. These are stored in a\n  tsv format, as follows:\n          #   n02119247    black fox\n          #   n02119359    silver fox\n  We assign each synset (in alphabetical order) an integer, starting from 1\n  (since 0 is reserved for the background class).\n\n  Code is based on\n  https://github.com/tensorflow/models/blob/master/research/inception/inception/data/build_imagenet_data.py#L463\n  """"""\n\n  # pylint: disable=g-line-too-long\n  base_url = \'https://raw.githubusercontent.com/tensorflow/models/master/research/inception/inception/data/\'\n  synset_url = \'{}/imagenet_lsvrc_2015_synsets.txt\'.format(base_url)\n  synset_to_human_url = \'{}/imagenet_metadata.txt\'.format(base_url)\n\n  filename, _ = urllib.request.urlretrieve(synset_url)\n  synset_list = [s.strip() for s in open(filename).readlines()]\n  num_synsets_in_ilsvrc = len(synset_list)\n  assert num_synsets_in_ilsvrc == 1000\n\n  filename, _ = urllib.request.urlretrieve(synset_to_human_url)\n  synset_to_human_list = open(filename).readlines()\n  num_synsets_in_all_imagenet = len(synset_to_human_list)\n  assert num_synsets_in_all_imagenet == 21842\n\n  synset_to_human = {}\n  for s in synset_to_human_list:\n    parts = s.strip().split(\'\\t\')\n    assert len(parts) == 2\n    synset = parts[0]\n    human = parts[1]\n    synset_to_human[synset] = human\n\n  label_index = 1\n  labels_to_names = {0: \'background\'}\n  for synset in synset_list:\n    name = synset_to_human[synset]\n    labels_to_names[label_index] = name\n    label_index += 1\n\n  return labels_to_names\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading ImageNet.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature(\n          (), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature(\n          (), tf.string, default_value=\'jpeg\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], dtype=tf.int64, default_value=-1),\n      \'image/class/text\': tf.FixedLenFeature(\n          [], dtype=tf.string, default_value=\'\'),\n      \'image/object/bbox/xmin\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/ymin\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/xmax\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/ymax\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/class/label\': tf.VarLenFeature(\n          dtype=tf.int64),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n      \'label_text\': slim.tfexample_decoder.Tensor(\'image/class/text\'),\n      \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n          [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n      \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if LOAD_READABLE_NAMES:\n    if dataset_utils.has_labels(dataset_dir):\n      labels_to_names = dataset_utils.read_label_file(dataset_dir)\n    else:\n      labels_to_names = create_readable_names_for_imagenet_labels()\n      dataset_utils.write_label_file(labels_to_names, dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
examples/slim/datasets/mnist.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the MNIST dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/research/slim/datasets/download_and_convert_mnist.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow.compat.v1 as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'mnist_%s.tfrecord\'\n\n_SPLITS_TO_SIZES = {\'train\': 60000, \'test\': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A [28 x 28 x 1] grayscale image.\',\n    \'label\': \'A single integer between 0 and 9\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading MNIST.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'raw\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [1], tf.int64, default_value=tf.zeros([1], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(shape=[28, 28, 1], channels=1),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\', shape=[]),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      num_classes=_NUM_CLASSES,\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      labels_to_names=labels_to_names)\n'"
examples/slim/datasets/preprocess_imagenet_validation_data.py,0,"b'#!/usr/bin/python\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Process the ImageNet Challenge bounding boxes for TensorFlow model training.\n\nAssociate the ImageNet 2012 Challenge validation data set with labels.\n\nThe raw ImageNet validation data set is expected to reside in JPEG files\nlocated in the following directory structure.\n\n data_dir/ILSVRC2012_val_00000001.JPEG\n data_dir/ILSVRC2012_val_00000002.JPEG\n ...\n data_dir/ILSVRC2012_val_00050000.JPEG\n\nThis script moves the files into a directory structure like such:\n data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n ...\nwhere \'n01440764\' is the unique synset label associated with\nthese images.\n\nThis directory reorganization requires a mapping from validation image\nnumber (i.e. suffix of the original file) to the associated label. This\nis provided in the ImageNet development kit via a Matlab file.\n\nIn order to make life easier and divorce ourselves from Matlab, we instead\nsupply a custom text file that provides this mapping for us.\n\nSample usage:\n  ./preprocess_imagenet_validation_data.py ILSVRC2012_img_val \\\n  imagenet_2012_validation_synset_labels.txt\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\n\nif __name__ == \'__main__\':\n  if len(sys.argv) < 3:\n    print(\'Invalid usage\\n\'\n          \'usage: preprocess_imagenet_validation_data.py \'\n          \'<validation data dir> <validation labels file>\')\n    sys.exit(-1)\n  data_dir = sys.argv[1]\n  validation_labels_file = sys.argv[2]\n\n  # Read in the 50000 synsets associated with the validation data set.\n  labels = [l.strip() for l in open(validation_labels_file).readlines()]\n  unique_labels = set(labels)\n\n  # Make all sub-directories in the validation data dir.\n  for label in unique_labels:\n    labeled_data_dir = os.path.join(data_dir, label)\n    os.makedirs(labeled_data_dir)\n\n  # Move all of the image to the appropriate sub-directory.\n  for i in xrange(len(labels)):\n    basename = \'ILSVRC2012_val_000%.5d.JPEG\' % (i + 1)\n    original_filename = os.path.join(data_dir, basename)\n    if not os.path.exists(original_filename):\n      print(\'Failed to find: \', original_filename)\n      sys.exit(-1)\n    new_filename = os.path.join(data_dir, labels[i], basename)\n    os.rename(original_filename, new_filename)\n'"
examples/slim/datasets/process_bounding_boxes.py,0,"b'#!/usr/bin/python\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Process the ImageNet Challenge bounding boxes for TensorFlow model training.\n\nThis script is called as\n\nprocess_bounding_boxes.py <dir> [synsets-file]\n\nWhere <dir> is a directory containing the downloaded and unpacked bounding box\ndata. If [synsets-file] is supplied, then only the bounding boxes whose\nsynstes are contained within this file are returned. Note that the\n[synsets-file] file contains synset ids, one per line.\n\nThe script dumps out a CSV text file in which each line contains an entry.\n  n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n\nThe entry can be read as:\n  <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n\nThe bounding box for <JPEG file name> contains two points (xmin, ymin) and\n(xmax, ymax) specifying the lower-left corner and upper-right corner of a\nbounding box in *relative* coordinates.\n\nThe user supplies a directory where the XML files reside. The directory\nstructure in the directory <dir> is assumed to look like this:\n\n<dir>/nXXXXXXXX/nXXXXXXXX_YYYY.xml\n\nEach XML file contains a bounding box annotation. The script:\n\n (1) Parses the XML file and extracts the filename, label and bounding box info.\n\n (2) The bounding box is specified in the XML files as integer (xmin, ymin) and\n    (xmax, ymax) *relative* to image size displayed to the human annotator. The\n    size of the image displayed to the human annotator is stored in the XML file\n    as integer (height, width).\n\n    Note that the displayed size will differ from the actual size of the image\n    downloaded from image-net.org. To make the bounding box annotation useable,\n    we convert bounding box to floating point numbers relative to displayed\n    height and width of the image.\n\n    Note that each XML file might contain N bounding box annotations.\n\n    Note that the points are all clamped at a range of [0.0, 1.0] because some\n    human annotations extend outside the range of the supplied image.\n\n    See details here: http://image-net.org/download-bboxes\n\n(3) By default, the script outputs all valid bounding boxes. If a\n    [synsets-file] is supplied, only the subset of bounding boxes associated\n    with those synsets are outputted. Importantly, one can supply a list of\n    synsets in the ImageNet Challenge and output the list of bounding boxes\n    associated with the training images of the ILSVRC.\n\n    We use these bounding boxes to inform the random distortion of images\n    supplied to the network.\n\nIf you run this script successfully, you will see the following output\nto stderr:\n> Finished processing 544546 XML files.\n> Skipped 0 XML files not in ImageNet Challenge.\n> Skipped 0 bounding boxes not in ImageNet Challenge.\n> Wrote 615299 bounding boxes from 544546 annotated images.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport os.path\nimport sys\nimport xml.etree.ElementTree as ET\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\n\nclass BoundingBox(object):\n  pass\n\n\ndef GetItem(name, root, index=0):\n  count = 0\n  for item in root.iter(name):\n    if count == index:\n      return item.text\n    count += 1\n  # Failed to find ""index"" occurrence of item.\n  return -1\n\n\ndef GetInt(name, root, index=0):\n  return int(GetItem(name, root, index))\n\n\ndef FindNumberBoundingBoxes(root):\n  index = 0\n  while True:\n    if GetInt(\'xmin\', root, index) == -1:\n      break\n    index += 1\n  return index\n\n\ndef ProcessXMLAnnotation(xml_file):\n  """"""Process a single XML file containing a bounding box.""""""\n  # pylint: disable=broad-except\n  try:\n    tree = ET.parse(xml_file)\n  except Exception:\n    print(\'Failed to parse: \' + xml_file, file=sys.stderr)\n    return None\n  # pylint: enable=broad-except\n  root = tree.getroot()\n\n  num_boxes = FindNumberBoundingBoxes(root)\n  boxes = []\n\n  for index in xrange(num_boxes):\n    box = BoundingBox()\n    # Grab the \'index\' annotation.\n    box.xmin = GetInt(\'xmin\', root, index)\n    box.ymin = GetInt(\'ymin\', root, index)\n    box.xmax = GetInt(\'xmax\', root, index)\n    box.ymax = GetInt(\'ymax\', root, index)\n\n    box.width = GetInt(\'width\', root)\n    box.height = GetInt(\'height\', root)\n    box.filename = GetItem(\'filename\', root) + \'.JPEG\'\n    box.label = GetItem(\'name\', root)\n\n    xmin = float(box.xmin) / float(box.width)\n    xmax = float(box.xmax) / float(box.width)\n    ymin = float(box.ymin) / float(box.height)\n    ymax = float(box.ymax) / float(box.height)\n\n    # Some images contain bounding box annotations that\n    # extend outside of the supplied image. See, e.g.\n    # n03127925/n03127925_147.xml\n    # Additionally, for some bounding boxes, the min > max\n    # or the box is entirely outside of the image.\n    min_x = min(xmin, xmax)\n    max_x = max(xmin, xmax)\n    box.xmin_scaled = min(max(min_x, 0.0), 1.0)\n    box.xmax_scaled = min(max(max_x, 0.0), 1.0)\n\n    min_y = min(ymin, ymax)\n    max_y = max(ymin, ymax)\n    box.ymin_scaled = min(max(min_y, 0.0), 1.0)\n    box.ymax_scaled = min(max(max_y, 0.0), 1.0)\n\n    boxes.append(box)\n\n  return boxes\n\nif __name__ == \'__main__\':\n  if len(sys.argv) < 2 or len(sys.argv) > 3:\n    print(\'Invalid usage\\n\'\n          \'usage: process_bounding_boxes.py <dir> [synsets-file]\',\n          file=sys.stderr)\n    sys.exit(-1)\n\n  xml_files = glob.glob(sys.argv[1] + \'/*/*.xml\')\n  print(\'Identified %d XML files in %s\' % (len(xml_files), sys.argv[1]),\n        file=sys.stderr)\n\n  if len(sys.argv) == 3:\n    labels = set([l.strip() for l in open(sys.argv[2]).readlines()])\n    print(\'Identified %d synset IDs in %s\' % (len(labels), sys.argv[2]),\n          file=sys.stderr)\n  else:\n    labels = None\n\n  skipped_boxes = 0\n  skipped_files = 0\n  saved_boxes = 0\n  saved_files = 0\n  for file_index, one_file in enumerate(xml_files):\n    # Example: <...>/n06470073/n00141669_6790.xml\n    label = os.path.basename(os.path.dirname(one_file))\n\n    # Determine if the annotation is from an ImageNet Challenge label.\n    if labels is not None and label not in labels:\n      skipped_files += 1\n      continue\n\n    bboxes = ProcessXMLAnnotation(one_file)\n    assert bboxes is not None, \'No bounding boxes found in \' + one_file\n\n    found_box = False\n    for bbox in bboxes:\n      if labels is not None:\n        if bbox.label != label:\n          # Note: There is a slight bug in the bounding box annotation data.\n          # Many of the dog labels have the human label \'Scottish_deerhound\'\n          # instead of the synset ID \'n02092002\' in the bbox.label field. As a\n          # simple hack to overcome this issue, we only exclude bbox labels\n          # *which are synset ID\'s* that do not match original synset label for\n          # the XML file.\n          if bbox.label in labels:\n            skipped_boxes += 1\n            continue\n\n      # Guard against improperly specified boxes.\n      if (bbox.xmin_scaled >= bbox.xmax_scaled or\n          bbox.ymin_scaled >= bbox.ymax_scaled):\n        skipped_boxes += 1\n        continue\n\n      # Note bbox.filename occasionally contains \'%s\' in the name. This is\n      # data set noise that is fixed by just using the basename of the XML file.\n      image_filename = os.path.splitext(os.path.basename(one_file))[0]\n      print(\'%s.JPEG,%.4f,%.4f,%.4f,%.4f\' %\n            (image_filename,\n             bbox.xmin_scaled, bbox.ymin_scaled,\n             bbox.xmax_scaled, bbox.ymax_scaled))\n\n      saved_boxes += 1\n      found_box = True\n    if found_box:\n      saved_files += 1\n    else:\n      skipped_files += 1\n\n    if not file_index % 5000:\n      print(\'--> processed %d of %d XML files.\' %\n            (file_index + 1, len(xml_files)),\n            file=sys.stderr)\n      print(\'--> skipped %d boxes and %d XML files.\' %\n            (skipped_boxes, skipped_files), file=sys.stderr)\n\n  print(\'Finished processing %d XML files.\' % len(xml_files), file=sys.stderr)\n  print(\'Skipped %d XML files not in ImageNet Challenge.\' % skipped_files,\n        file=sys.stderr)\n  print(\'Skipped %d bounding boxes not in ImageNet Challenge.\' % skipped_boxes,\n        file=sys.stderr)\n  print(\'Wrote %d bounding boxes from %d annotated images.\' %\n        (saved_boxes, saved_files),\n        file=sys.stderr)\n  print(\'Finished.\', file=sys.stderr)\n'"
examples/slim/datasets/visualwakewords.py,11,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for Visual WakeWords Dataset with images+labels.\n\nVisual WakeWords Dataset derives from the COCO dataset to design tiny models\nclassifying two classes, such as person/not-person. The COCO annotations\nare filtered to two classes: person and not-person (or another user-defined\ncategory). Bounding boxes for small objects with area less than 5% of the image\narea are filtered out.\nSee build_visualwakewords_data.py which generates the Visual WakeWords dataset\nannotations from the raw COCO dataset and converts them to TFRecord.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow.compat.v1 as tf\n\nfrom datasets import dataset_utils\n\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'%s.record-*\'\n\n_SPLITS_TO_SIZES = {\n    \'train\': 82783,\n    \'validation\': 40504,\n}\n\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying height and width.\',\n    \'label\': \'The label id of the image, an integer in {0, 1}\',\n    \'object/bbox\': \'A list of bounding boxes.\',\n    \'object/label\': \'A list of labels, all objects belong to the same class.\',\n}\n\n_NUM_CLASSES = 2\n\n# labels file\nLABELS_FILENAME = \'labels.txt\'\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading ImageNet.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources. It\n      is assumed that the pattern contains a \'%s\' string so that the split name\n      can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\':\n          tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\':\n          tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n      \'image/class/label\':\n          tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),\n      \'image/object/bbox/xmin\':\n          tf.VarLenFeature(dtype=tf.float32),\n      \'image/object/bbox/ymin\':\n          tf.VarLenFeature(dtype=tf.float32),\n      \'image/object/bbox/xmax\':\n          tf.VarLenFeature(dtype=tf.float32),\n      \'image/object/bbox/ymax\':\n          tf.VarLenFeature(dtype=tf.float32),\n      \'image/object/class/label\':\n          tf.VarLenFeature(dtype=tf.int64),\n  }\n\n  items_to_handlers = {\n      \'image\':\n          slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n      \'label\':\n          slim.tfexample_decoder.Tensor(\'image/class/label\'),\n      \'object/bbox\':\n          slim.tfexample_decoder.BoundingBox([\'ymin\', \'xmin\', \'ymax\', \'xmax\'],\n                                             \'image/object/bbox/\'),\n      \'object/label\':\n          slim.tfexample_decoder.Tensor(\'image/object/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features,\n                                                    items_to_handlers)\n\n  labels_to_names = None\n  labels_file = os.path.join(dataset_dir, LABELS_FILENAME)\n  if tf.gfile.Exists(labels_file):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
examples/slim/deployment/__init__.py,0,b'\n'
examples/slim/deployment/model_deploy.py,54,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Deploy Slim models across multiple clones and replicas.\n\n# TODO(sguada) docstring paragraph by (a) motivating the need for the file and\n# (b) defining clones.\n\n# TODO(sguada) describe the high-level components of model deployment.\n# E.g. ""each model deployment is composed of several parts: a DeploymentConfig,\n# which captures A, B and C, an input_fn which loads data.. etc\n\nTo easily train a model on multiple GPUs or across multiple machines this\nmodule provides a set of helper functions: `create_clones`,\n`optimize_clones` and `deploy`.\n\nUsage:\n\n  g = tf.Graph()\n\n  # Set up DeploymentConfig\n  config = model_deploy.DeploymentConfig(num_clones=2, clone_on_cpu=True)\n\n  # Create the global step on the device storing the variables.\n  with tf.device(config.variables_device()):\n    global_step = slim.create_global_step()\n\n  # Define the inputs\n  with tf.device(config.inputs_device()):\n    images, labels = LoadData(...)\n    inputs_queue = slim.data.prefetch_queue((images, labels))\n\n  # Define the optimizer.\n  with tf.device(config.optimizer_device()):\n    optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate, FLAGS.momentum)\n\n  # Define the model including the loss.\n  def model_fn(inputs_queue):\n    images, labels = inputs_queue.dequeue()\n    predictions = CreateNetwork(images)\n    slim.losses.log_loss(predictions, labels)\n\n  model_dp = model_deploy.deploy(config, model_fn, [inputs_queue],\n                                 optimizer=optimizer)\n\n  # Run training.\n  slim.learning.train(model_dp.train_op, my_log_dir,\n                      summary_op=model_dp.summary_op)\n\nThe Clone namedtuple holds together the values associated with each call to\nmodel_fn:\n  * outputs: The return values of the calls to `model_fn()`.\n  * scope: The scope used to create the clone.\n  * device: The device used to create the clone.\n\nDeployedModel namedtuple, holds together the values needed to train multiple\nclones:\n  * train_op: An operation that run the optimizer training op and include\n    all the update ops created by `model_fn`. Present only if an optimizer\n    was specified.\n  * summary_op: An operation that run the summaries created by `model_fn`\n    and process_gradients.\n  * total_loss: A `Tensor` that contains the sum of all losses created by\n    `model_fn` plus the regularization losses.\n  * clones: List of `Clone` tuples returned by `create_clones()`.\n\nDeploymentConfig parameters:\n  * num_clones: Number of model clones to deploy in each replica.\n  * clone_on_cpu: True if clones should be placed on CPU.\n  * replica_id: Integer.  Index of the replica for which the model is\n      deployed.  Usually 0 for the chief replica.\n  * num_replicas: Number of replicas to use.\n  * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n  * worker_job_name: A name for the worker job.\n  * ps_job_name: A name for the parameter server job.\n\nTODO(sguada):\n  - describe side effect to the graph.\n  - what happens to summaries and update_ops.\n  - which graph collections are altered.\n  - write a tutorial on how to use this.\n  - analyze the possibility of calling deploy more than once.\n\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\n__all__ = [\'create_clones\',\n           \'deploy\',\n           \'optimize_clones\',\n           \'DeployedModel\',\n           \'DeploymentConfig\',\n           \'Clone\',\n          ]\n\n\n# Namedtuple used to represent a clone during deployment.\nClone = collections.namedtuple(\'Clone\',\n                               [\'outputs\',  # Whatever model_fn() returned.\n                                \'scope\',  # The scope used to create it.\n                                \'device\',  # The device used to create.\n                                \'network_regularizer\', # MN network regularizer.\n                                \'reg_loss\', # morph_net regularizer_loss.\n                               ])\n\n# Namedtuple used to represent a DeployedModel, returned by deploy().\nDeployedModel = collections.namedtuple(\'DeployedModel\',\n                                       [\'train_op\',  # The `train_op`\n                                        \'summary_op\',  # The `summary_op`\n                                        \'total_loss\',  # The loss `Tensor`\n                                        \'clones\',  # A list of `Clones` tuples.\n                                       ])\n\n# Default parameters for DeploymentConfig\n_deployment_params = {\'num_clones\': 1,\n                      \'clone_on_cpu\': False,\n                      \'replica_id\': 0,\n                      \'num_replicas\': 1,\n                      \'num_ps_tasks\': 0,\n                      \'worker_job_name\': \'worker\',\n                      \'ps_job_name\': \'ps\'}\n\n\ndef create_clones(config, model_fn, args=None, kwargs=None):\n  """"""Creates multiple clones according to config using a `model_fn`.\n\n  The returned values of `model_fn(*args, **kwargs)` are collected along with\n  the scope and device used to created it in a namedtuple\n  `Clone(outputs, scope, device)`\n\n  Note: it is assumed that any loss created by `model_fn` is collected at\n  the tf.GraphKeys.LOSSES collection.\n\n  To recover the losses, summaries or update_ops created by the clone use:\n  ```python\n    losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n    summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, clone.scope)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, clone.scope)\n  ```\n\n  The deployment options are specified by the config object and support\n  deploying one or several clones on different GPUs and one or several replicas\n  of such clones.\n\n  The argument `model_fn` is called `config.num_clones` times to create the\n  model clones as `model_fn(*args, **kwargs)`.\n\n  If `config` specifies deployment on multiple replicas then the default\n  tensorflow device is set appropriatly for each call to `model_fn` and for the\n  slim variable creation functions: model and global variables will be created\n  on the `ps` device, the clone operations will be on the `worker` device.\n\n  Args:\n    config: A DeploymentConfig object.\n    model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n    args: Optional list of arguments to pass to `model_fn`.\n    kwargs: Optional list of keyword arguments to pass to `model_fn`.\n\n  Returns:\n    A list of namedtuples `Clone`.\n  """"""\n  clones = []\n  args = args or []\n  kwargs = kwargs or {}\n  with slim.arg_scope([slim.model_variable, slim.variable],\n                      device=config.variables_device()):\n    # Create clones.\n    for i in range(0, config.num_clones):\n      with tf.name_scope(config.clone_scope(i)) as clone_scope:\n        clone_device = config.clone_device(i)\n        with tf.device(clone_device):\n          with tf.variable_scope(tf.get_variable_scope(),\n                                 reuse=True if i > 0 else None):\n            outputs, net_regularizer, clone_reg_loss = model_fn(*args, **kwargs)\n          clones.append(Clone(outputs, clone_scope, clone_device, \n                                net_regularizer, clone_reg_loss))\n  return clones\n\n\ndef _gather_clone_loss(clone, num_clones, regularization_losses):\n  """"""Gather the loss for a single clone.\n\n  Args:\n    clone: A Clone namedtuple.\n    num_clones: The number of clones being deployed.\n    regularization_losses: Possibly empty list of regularization_losses\n      to add to the clone losses.\n\n  Returns:\n    A tensor for the total loss for the clone.  Can be None.\n  """"""\n  # The return value.\n  sum_loss = None\n  # Individual components of the loss that will need summaries.\n  clone_loss = None\n  regularization_loss = None\n  # Compute and aggregate losses on the clone device.\n  with tf.device(clone.device):\n    all_losses = []\n    clone_losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n    if clone_losses:\n      clone_loss = tf.add_n(clone_losses, name=\'clone_loss\')\n      if num_clones > 1:\n        clone_loss = tf.div(clone_loss, 1.0 * num_clones,\n                            name=\'scaled_clone_loss\')\n      all_losses.append(clone_loss)\n\n    if regularization_losses:\n      regularization_loss = tf.add_n(regularization_losses,\n                                     name=\'regularization_loss\')\n      all_losses.append(regularization_loss)\n    if all_losses:\n      sum_loss = tf.add_n(all_losses)\n  # Add the summaries out of the clone device block.\n  if clone_loss is not None:\n    tf.summary.scalar(\'/\'.join(filter(None,\n                                      [\'Losses\', clone.scope, \'clone_loss\'])),\n                      clone_loss)\n  if regularization_loss is not None:\n    tf.summary.scalar(\'Losses/regularization_loss\', regularization_loss)\n  return sum_loss\n\n\ndef _optimize_clone(optimizer, clone, num_clones, regularization_losses,\n                    **kwargs):\n  """"""Compute losses and gradients for a single clone.\n\n  Args:\n    optimizer: A tf.Optimizer  object.\n    clone: A Clone namedtuple.\n    num_clones: The number of clones being deployed.\n    regularization_losses: Possibly empty list of regularization_losses\n      to add to the clone losses.\n    **kwargs: Dict of kwarg to pass to compute_gradients().\n\n  Returns:\n    A tuple (clone_loss, clone_grads_and_vars).\n      - clone_loss: A tensor for the total loss for the clone.  Can be None.\n      - clone_grads_and_vars: List of (gradient, variable) for the clone.\n        Can be empty.\n  """"""\n  sum_loss = _gather_clone_loss(clone, num_clones, regularization_losses)\n  clone_grad = None\n  if sum_loss is not None:\n    with tf.device(clone.device):\n      clone_grad = optimizer.compute_gradients(sum_loss+clone.reg_loss, **kwargs)\n  return sum_loss, clone_grad\n\n\ndef optimize_clones(clones, optimizer,\n                    regularization_losses=None,\n                    **kwargs):\n  """"""Compute clone losses and gradients for the given list of `Clones`.\n\n  Note: The regularization_losses are added to the first clone losses.\n\n  Args:\n   clones: List of `Clones` created by `create_clones()`.\n   optimizer: An `Optimizer` object.\n   regularization_losses: Optional list of regularization losses. If None it\n     will gather them from tf.GraphKeys.REGULARIZATION_LOSSES. Pass `[]` to\n     exclude them.\n   **kwargs: Optional list of keyword arguments to pass to `compute_gradients`.\n\n  Returns:\n   A tuple (total_loss, grads_and_vars).\n     - total_loss: A Tensor containing the average of the clone losses including\n       the regularization loss.\n     - grads_and_vars: A List of tuples (gradient, variable) containing the sum\n       of the gradients for each variable.\n\n  """"""\n  grads_and_vars = []\n  clones_losses = []\n  num_clones = len(clones)\n  if regularization_losses is None:\n    regularization_losses = tf.get_collection(\n        tf.GraphKeys.REGULARIZATION_LOSSES)\n  for clone in clones:\n    with tf.name_scope(clone.scope):\n      clone_loss, clone_grad = _optimize_clone(\n          optimizer, clone, num_clones, regularization_losses, **kwargs)\n      if clone_loss is not None:\n        clones_losses.append(clone_loss)\n        grads_and_vars.append(clone_grad)\n      # Only use regularization_losses for the first clone\n      regularization_losses = None\n  # Compute the total_loss summing all the clones_losses.\n  total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n  # Sum the gradients across clones.\n  grads_and_vars = _sum_clones_gradients(grads_and_vars)\n  return total_loss, grads_and_vars\n\n\ndef deploy(config,\n           model_fn,\n           args=None,\n           kwargs=None,\n           optimizer=None,\n           summarize_gradients=False):\n  """"""Deploys a Slim-constructed model across multiple clones.\n\n  The deployment options are specified by the config object and support\n  deploying one or several clones on different GPUs and one or several replicas\n  of such clones.\n\n  The argument `model_fn` is called `config.num_clones` times to create the\n  model clones as `model_fn(*args, **kwargs)`.\n\n  The optional argument `optimizer` is an `Optimizer` object.  If not `None`,\n  the deployed model is configured for training with that optimizer.\n\n  If `config` specifies deployment on multiple replicas then the default\n  tensorflow device is set appropriatly for each call to `model_fn` and for the\n  slim variable creation functions: model and global variables will be created\n  on the `ps` device, the clone operations will be on the `worker` device.\n\n  Args:\n    config: A `DeploymentConfig` object.\n    model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n    args: Optional list of arguments to pass to `model_fn`.\n    kwargs: Optional list of keyword arguments to pass to `model_fn`.\n    optimizer: Optional `Optimizer` object.  If passed the model is deployed\n      for training with that optimizer.\n    summarize_gradients: Whether or not add summaries to the gradients.\n\n  Returns:\n    A `DeployedModel` namedtuple.\n\n  """"""\n  # Gather initial summaries.\n  summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n  # Create Clones.\n  clones = create_clones(config, model_fn, args, kwargs)\n  first_clone = clones[0]\n\n  # Gather update_ops from the first clone. These contain, for example,\n  # the updates for the batch_norm variables created by model_fn.\n  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone.scope)\n\n  train_op = None\n  total_loss = None\n  with tf.device(config.optimizer_device()):\n    if optimizer:\n      # Place the global step on the device storing the variables.\n      with tf.device(config.variables_device()):\n        global_step = slim.get_or_create_global_step()\n\n      # Compute the gradients for the clones.\n      total_loss, clones_gradients = optimize_clones(clones, optimizer)\n\n      if clones_gradients:\n        if summarize_gradients:\n          # Add summaries to the gradients.\n          summaries |= set(_add_gradients_summaries(clones_gradients))\n\n        # Create gradient updates.\n        grad_updates = optimizer.apply_gradients(clones_gradients,\n                                                 global_step=global_step)\n        update_ops.append(grad_updates)\n\n        update_op = tf.group(*update_ops)\n        with tf.control_dependencies([update_op]):\n          train_op = tf.identity(total_loss, name=\'train_op\')\n    else:\n      clones_losses = []\n      regularization_losses = tf.get_collection(\n          tf.GraphKeys.REGULARIZATION_LOSSES)\n      for clone in clones:\n        with tf.name_scope(clone.scope):\n          clone_loss = _gather_clone_loss(clone, len(clones),\n                                          regularization_losses)\n          if clone_loss is not None:\n            clones_losses.append(clone_loss)\n          # Only use regularization_losses for the first clone\n          regularization_losses = None\n      if clones_losses:\n        total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n\n    # Add the summaries from the first clone. These contain the summaries\n    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                       first_clone.scope))\n\n    if total_loss is not None:\n      # Add total_loss to summary.\n      summaries.add(tf.summary.scalar(\'total_loss\', total_loss))\n\n    if summaries:\n      # Merge all summaries together.\n      summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n    else:\n      summary_op = None\n\n  return DeployedModel(train_op, summary_op, total_loss, clones)\n\n\ndef _sum_clones_gradients(clone_grads):\n  """"""Calculate the sum gradient for each shared variable across all clones.\n\n  This function assumes that the clone_grads has been scaled appropriately by\n  1 / num_clones.\n\n  Args:\n    clone_grads: A List of List of tuples (gradient, variable), one list per\n    `Clone`.\n\n  Returns:\n     List of tuples of (gradient, variable) where the gradient has been summed\n     across all clones.\n  """"""\n  sum_grads = []\n  for grad_and_vars in zip(*clone_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))\n    grads = []\n    var = grad_and_vars[0][1]\n    for g, v in grad_and_vars:\n      assert v == var\n      if g is not None:\n        grads.append(g)\n    if grads:\n      if len(grads) > 1:\n        sum_grad = tf.add_n(grads, name=var.op.name + \'/sum_grads\')\n      else:\n        sum_grad = grads[0]\n      sum_grads.append((sum_grad, var))\n  return sum_grads\n\n\ndef _add_gradients_summaries(grads_and_vars):\n  """"""Add histogram summaries to gradients.\n\n  Note: The summaries are also added to the SUMMARIES collection.\n\n  Args:\n    grads_and_vars: A list of gradient to variable pairs (tuples).\n\n  Returns:\n    The _list_ of the added summaries for grads_and_vars.\n  """"""\n  summaries = []\n  for grad, var in grads_and_vars:\n    if grad is not None:\n      if isinstance(grad, tf.IndexedSlices):\n        grad_values = grad.values\n      else:\n        grad_values = grad\n      summaries.append(tf.summary.histogram(var.op.name + \':gradient\',\n                                            grad_values))\n      summaries.append(tf.summary.histogram(var.op.name + \':gradient_norm\',\n                                            tf.global_norm([grad_values])))\n    else:\n      tf.logging.info(\'Var %s has no gradient\', var.op.name)\n  return summaries\n\n\nclass DeploymentConfig(object):\n  """"""Configuration for deploying a model with `deploy()`.\n\n  You can pass an instance of this class to `deploy()` to specify exactly\n  how to deploy the model to build.  If you do not pass one, an instance built\n  from the default deployment_hparams will be used.\n  """"""\n\n  def __init__(self,\n               num_clones=1,\n               clone_on_cpu=False,\n               replica_id=0,\n               num_replicas=1,\n               num_ps_tasks=0,\n               worker_job_name=\'worker\',\n               ps_job_name=\'ps\'):\n    """"""Create a DeploymentConfig.\n\n    The config describes how to deploy a model across multiple clones and\n    replicas.  The model will be replicated `num_clones` times in each replica.\n    If `clone_on_cpu` is True, each clone will placed on CPU.\n\n    If `num_replicas` is 1, the model is deployed via a single process.  In that\n    case `worker_device`, `num_ps_tasks`, and `ps_device` are ignored.\n\n    If `num_replicas` is greater than 1, then `worker_device` and `ps_device`\n    must specify TensorFlow devices for the `worker` and `ps` jobs and\n    `num_ps_tasks` must be positive.\n\n    Args:\n      num_clones: Number of model clones to deploy in each replica.\n      clone_on_cpu: If True clones would be placed on CPU.\n      replica_id: Integer.  Index of the replica for which the model is\n        deployed.  Usually 0 for the chief replica.\n      num_replicas: Number of replicas to use.\n      num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n      worker_job_name: A name for the worker job.\n      ps_job_name: A name for the parameter server job.\n\n    Raises:\n      ValueError: If the arguments are invalid.\n    """"""\n    if num_replicas > 1:\n      if num_ps_tasks < 1:\n        raise ValueError(\'When using replicas num_ps_tasks must be positive\')\n    if num_replicas > 1 or num_ps_tasks > 0:\n      if not worker_job_name:\n        raise ValueError(\'Must specify worker_job_name when using replicas\')\n      if not ps_job_name:\n        raise ValueError(\'Must specify ps_job_name when using parameter server\')\n    if replica_id >= num_replicas:\n      raise ValueError(\'replica_id must be less than num_replicas\')\n    self._num_clones = num_clones\n    self._clone_on_cpu = clone_on_cpu\n    self._replica_id = replica_id\n    self._num_replicas = num_replicas\n    self._num_ps_tasks = num_ps_tasks\n    self._ps_device = \'/job:\' + ps_job_name if num_ps_tasks > 0 else \'\'\n    self._worker_device = \'/job:\' + worker_job_name if num_ps_tasks > 0 else \'\'\n\n  @property\n  def num_clones(self):\n    return self._num_clones\n\n  @property\n  def clone_on_cpu(self):\n    return self._clone_on_cpu\n\n  @property\n  def replica_id(self):\n    return self._replica_id\n\n  @property\n  def num_replicas(self):\n    return self._num_replicas\n\n  @property\n  def num_ps_tasks(self):\n    return self._num_ps_tasks\n\n  @property\n  def ps_device(self):\n    return self._ps_device\n\n  @property\n  def worker_device(self):\n    return self._worker_device\n\n  def caching_device(self):\n    """"""Returns the device to use for caching variables.\n\n    Variables are cached on the worker CPU when using replicas.\n\n    Returns:\n      A device string or None if the variables do not need to be cached.\n    """"""\n    if self._num_ps_tasks > 0:\n      return lambda op: op.device\n    else:\n      return None\n\n  def clone_device(self, clone_index):\n    """"""Device used to create the clone and all the ops inside the clone.\n\n    Args:\n      clone_index: Int, representing the clone_index.\n\n    Returns:\n      A value suitable for `tf.device()`.\n\n    Raises:\n      ValueError: if `clone_index` is greater or equal to the number of clones"".\n    """"""\n    if clone_index >= self._num_clones:\n      raise ValueError(\'clone_index must be less than num_clones\')\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._worker_device\n    if self._clone_on_cpu:\n      device += \'/device:CPU:0\'\n    else:\n      device += \'/device:GPU:%d\' % clone_index\n    return device\n\n  def clone_scope(self, clone_index):\n    """"""Name scope to create the clone.\n\n    Args:\n      clone_index: Int, representing the clone_index.\n\n    Returns:\n      A name_scope suitable for `tf.name_scope()`.\n\n    Raises:\n      ValueError: if `clone_index` is greater or equal to the number of clones"".\n    """"""\n    if clone_index >= self._num_clones:\n      raise ValueError(\'clone_index must be less than num_clones\')\n    scope = \'\'\n    if self._num_clones > 1:\n      scope = \'clone_%d\' % clone_index\n    return scope\n\n  def optimizer_device(self):\n    """"""Device to use with the optimizer.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    if self._num_ps_tasks > 0 or self._num_clones > 0:\n      return self._worker_device + \'/device:CPU:0\'\n    else:\n      return \'\'\n\n  def inputs_device(self):\n    """"""Device to use to build the inputs.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._worker_device\n    device += \'/device:CPU:0\'\n    return device\n\n  def variables_device(self):\n    """"""Returns the device to use for variables created inside the clone.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._ps_device\n    device += \'/device:CPU:0\'\n\n    class _PSDeviceChooser(object):\n      """"""Slim device chooser for variables when using PS.""""""\n\n      def __init__(self, device, tasks):\n        self._device = device\n        self._tasks = tasks\n        self._task = 0\n\n      def choose(self, op):\n        if op.device:\n          return op.device\n        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n        if node_def.op.startswith(\'Variable\'):\n          t = self._task\n          self._task = (self._task + 1) % self._tasks\n          d = \'%s/task:%d\' % (self._device, t)\n          return d\n        else:\n          return op.device\n\n    if not self._num_ps_tasks:\n      return device\n    else:\n      chooser = _PSDeviceChooser(device, self._num_ps_tasks)\n      return chooser.choose\n'"
examples/slim/deployment/model_deploy_test.py,96,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for model_deploy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom deployment import model_deploy\n\nslim = tf.contrib.slim\n\n\nclass DeploymentConfigTest(tf.test.TestCase):\n\n  def testDefaults(self):\n    deploy_config = model_deploy.DeploymentConfig()\n\n    self.assertEqual(slim.get_variables(), [])\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'GPU:0\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testCPUonly(self):\n    deploy_config = model_deploy.DeploymentConfig(clone_on_cpu=True)\n\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'CPU:0\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testMultiGPU(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1), \'GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=1, num_ps_tasks=1)\n\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker/device:GPU:0\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n    with tf.device(deploy_config.variables_device()):\n      a = tf.Variable(0)\n      b = tf.Variable(0)\n      c = tf.no_op()\n      d = slim.variable(\'a\', [],\n                        caching_device=deploy_config.caching_device())\n    self.assertDeviceEqual(a.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(a.device, a.value().device)\n    self.assertDeviceEqual(b.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(b.device, b.value().device)\n    self.assertDeviceEqual(c.device, \'\')\n    self.assertDeviceEqual(d.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(d.value().device, \'\')\n\n  def testMultiGPUPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=2, num_ps_tasks=1)\n\n    self.assertEqual(deploy_config.caching_device()(tf.no_op()), \'\')\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker/device:GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1),\n                           \'/job:worker/device:GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testReplicasPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_replicas=2,\n                                                  num_ps_tasks=2)\n\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker/device:GPU:0\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testReplicasMultiGPUPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_replicas=2,\n                                                  num_clones=2,\n                                                  num_ps_tasks=2)\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker/device:GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1),\n                           \'/job:worker/device:GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testVariablesPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_ps_tasks=2)\n\n    with tf.device(deploy_config.variables_device()):\n      a = tf.Variable(0)\n      b = tf.Variable(0)\n      c = tf.no_op()\n      d = slim.variable(\'a\', [],\n                        caching_device=deploy_config.caching_device())\n\n    self.assertDeviceEqual(a.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(a.device, a.value().device)\n    self.assertDeviceEqual(b.device, \'/job:ps/task:1/device:CPU:0\')\n    self.assertDeviceEqual(b.device, b.value().device)\n    self.assertDeviceEqual(c.device, \'\')\n    self.assertDeviceEqual(d.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(d.value().device, \'\')\n\n\ndef LogisticClassifier(inputs, labels, scope=None, reuse=None):\n  with tf.variable_scope(scope, \'LogisticClassifier\', [inputs, labels],\n                         reuse=reuse):\n    predictions = slim.fully_connected(inputs, 1, activation_fn=tf.sigmoid,\n                                       scope=\'fully_connected\')\n    slim.losses.log_loss(predictions, labels)\n    return predictions\n\n\ndef BatchNormClassifier(inputs, labels, scope=None, reuse=None):\n  with tf.variable_scope(scope, \'BatchNormClassifier\', [inputs, labels],\n                         reuse=reuse):\n    inputs = slim.batch_norm(inputs, decay=0.1, fused=True)\n    predictions = slim.fully_connected(inputs, 1,\n                                       activation_fn=tf.sigmoid,\n                                       scope=\'fully_connected\')\n    slim.losses.log_loss(predictions, labels)\n    return predictions\n\n\nclass CreatecloneTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def testCreateLogisticClassifier(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = LogisticClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      clone = clones[0]\n      self.assertEqual(len(slim.get_variables()), 2)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(clone.outputs.op.name,\n                       \'LogisticClassifier/fully_connected/Sigmoid\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertDeviceEqual(clone.device, \'GPU:0\')\n      self.assertEqual(len(slim.losses.get_losses()), 1)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(update_ops, [])\n\n  def testCreateSingleclone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      clone = clones[0]\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(clone.outputs.op.name,\n                       \'BatchNormClassifier/fully_connected/Sigmoid\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertDeviceEqual(clone.device, \'GPU:0\')\n      self.assertEqual(len(slim.losses.get_losses()), 1)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n  def testCreateMulticlone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(len(clones), num_clones)\n      for i, clone in enumerate(clones):\n        self.assertEqual(\n            clone.outputs.op.name,\n            \'clone_%d/BatchNormClassifier/fully_connected/Sigmoid\' % i)\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, clone.scope)\n        self.assertEqual(len(update_ops), 2)\n        self.assertEqual(clone.scope, \'clone_%d/\' % i)\n        self.assertDeviceEqual(clone.device, \'GPU:%d\' % i)\n\n  def testCreateOnecloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1,\n                                                    num_ps_tasks=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(clones), 1)\n      clone = clones[0]\n      self.assertEqual(clone.outputs.op.name,\n                       \'BatchNormClassifier/fully_connected/Sigmoid\')\n      self.assertDeviceEqual(clone.device, \'/job:worker/device:GPU:0\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'/job:ps/task:0/CPU:0\')\n        self.assertDeviceEqual(v.device, v.value().device)\n\n  def testCreateMulticloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2,\n                                                    num_ps_tasks=2)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      for i, v in enumerate(slim.get_variables()):\n        t = i % 2\n        self.assertDeviceEqual(v.device, \'/job:ps/task:%d/device:CPU:0\' % t)\n        self.assertDeviceEqual(v.device, v.value().device)\n      self.assertEqual(len(clones), 2)\n      for i, clone in enumerate(clones):\n        self.assertEqual(\n            clone.outputs.op.name,\n            \'clone_%d/BatchNormClassifier/fully_connected/Sigmoid\' % i)\n        self.assertEqual(clone.scope, \'clone_%d/\' % i)\n        self.assertDeviceEqual(clone.device, \'/job:worker/device:GPU:%d\' % i)\n\n\nclass OptimizeclonesTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def testCreateLogisticClassifier(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = LogisticClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 2)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(update_ops, [])\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'GPU:0\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateSingleclone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'GPU:0\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateMulticlone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), num_clones * 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateMulticloneCPU(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones,\n                                                    clone_on_cpu=True)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, model_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), num_clones * 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateOnecloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1,\n                                                    num_ps_tasks=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, model_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'/job:worker/device:GPU:0\')\n        self.assertDeviceEqual(v.device, \'/job:ps/task:0/CPU:0\')\n\n\nclass DeployTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def _addBesselsCorrection(self, sample_size, expected_var):\n    correction_factor = sample_size / (sample_size - 1)\n    expected_var *= correction_factor\n    return expected_var\n\n  def testLocalTrainOp(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2,\n                                                    clone_on_cpu=True)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n\n      self.assertEqual(slim.get_variables(), [])\n      model = model_deploy.deploy(deploy_config, model_fn, model_args,\n                                  optimizer=optimizer)\n\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 4)\n      self.assertEqual(len(model.clones), 2)\n      self.assertEqual(model.total_loss.op.name, \'total_loss\')\n      self.assertEqual(model.summary_op.op.name, \'summary_op/summary_op\')\n      self.assertEqual(model.train_op.op.name, \'train_op\')\n\n      with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        moving_mean = tf.contrib.framework.get_variables_by_name(\n            \'moving_mean\')[0]\n        moving_variance = tf.contrib.framework.get_variables_by_name(\n            \'moving_variance\')[0]\n        initial_loss = sess.run(model.total_loss)\n        initial_mean, initial_variance = sess.run([moving_mean,\n                                                   moving_variance])\n        self.assertAllClose(initial_mean, [0.0, 0.0, 0.0, 0.0])\n        self.assertAllClose(initial_variance, [1.0, 1.0, 1.0, 1.0])\n        for _ in range(10):\n          sess.run(model.train_op)\n        final_loss = sess.run(model.total_loss)\n        self.assertLess(final_loss, initial_loss / 5.0)\n\n        final_mean, final_variance = sess.run([moving_mean,\n                                               moving_variance])\n        expected_mean = np.array([0.125, 0.25, 0.375, 0.25])\n        expected_var = np.array([0.109375, 0.1875, 0.234375, 0.1875])\n        expected_var = self._addBesselsCorrection(16, expected_var)\n        self.assertAllClose(final_mean, expected_mean)\n        self.assertAllClose(final_variance, expected_var)\n\n  def testNoSummariesOnGPU(self):\n    with tf.Graph().as_default():\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n      # clone function creates a fully_connected layer with a regularizer loss.\n      def ModelFn():\n        inputs = tf.constant(1.0, shape=(10, 20), dtype=tf.float32)\n        reg = tf.contrib.layers.l2_regularizer(0.001)\n        tf.contrib.layers.fully_connected(inputs, 30, weights_regularizer=reg)\n\n      model = model_deploy.deploy(\n          deploy_config, ModelFn,\n          optimizer=tf.train.GradientDescentOptimizer(1.0))\n      # The model summary op should have a few summary inputs and all of them\n      # should be on the CPU.\n      self.assertTrue(model.summary_op.op.inputs)\n      for inp in  model.summary_op.op.inputs:\n        self.assertEqual(\'/device:CPU:0\', inp.device)\n\n  def testNoSummariesOnGPUForEvals(self):\n    with tf.Graph().as_default():\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n      # clone function creates a fully_connected layer with a regularizer loss.\n      def ModelFn():\n        inputs = tf.constant(1.0, shape=(10, 20), dtype=tf.float32)\n        reg = tf.contrib.layers.l2_regularizer(0.001)\n        tf.contrib.layers.fully_connected(inputs, 30, weights_regularizer=reg)\n\n      # No optimizer here, it\'s an eval.\n      model = model_deploy.deploy(deploy_config, ModelFn)\n      # The model summary op should have a few summary inputs and all of them\n      # should be on the CPU.\n      self.assertTrue(model.summary_op.op.inputs)\n      for inp in  model.summary_op.op.inputs:\n        self.assertEqual(\'/device:CPU:0\', inp.device)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
examples/slim/nets/__init__.py,0,b'\n'
examples/slim/nets/nets_factory.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\n\nfrom nets import resnet_v1\nfrom nets import resnet_v2\n\n\nslim = tf.contrib.slim\n\nnetworks_map = {\'resnet_v1_50\': resnet_v1.resnet_v1_50,\n                \'resnet_v1_101\': resnet_v1.resnet_v1_101,\n                \'resnet_v1_152\': resnet_v1.resnet_v1_152,\n                \'resnet_v1_200\': resnet_v1.resnet_v1_200,\n                \'resnet_v2_50\': resnet_v2.resnet_v2_50,\n                \'resnet_v2_101\': resnet_v2.resnet_v2_101,\n                \'resnet_v2_152\': resnet_v2.resnet_v2_152,\n                \'resnet_v2_200\': resnet_v2.resnet_v2_200\n               }\n\narg_scopes_map = {\'resnet_v1_50\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_101\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_152\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_200\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v2_50\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_101\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_152\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_200\': resnet_v2.resnet_arg_scope\n                 }\n\n\ndef get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):\n  """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n\n  Args:\n    name: The name of the network.\n    num_classes: The number of classes to use for classification. If 0 or None,\n      the logits layer is omitted and its input features are returned instead.\n    weight_decay: The l2 coefficient for the model weights.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    network_fn: A function that applies the model to a batch of images. It has\n      the following signature:\n          net, end_points = network_fn(images)\n      The `images` input is a tensor of shape [batch_size, height, width, 3]\n      with height = width = network_fn.default_image_size. (The permissibility\n      and treatment of other sizes depends on the network_fn.)\n      The returned `end_points` are a dictionary of intermediate activations.\n      The returned `net` is the topmost layer, depending on `num_classes`:\n      If `num_classes` was a non-zero integer, `net` is a logits tensor\n      of shape [batch_size, num_classes].\n      If `num_classes` was 0 or `None`, `net` is a tensor with the input\n      to the logits layer of shape [batch_size, 1, 1, num_features] or\n      [batch_size, num_features]. Dropout has not been applied to this\n      (even if the network\'s original classification does); it remains for\n      the caller to do this or not.\n\n  Raises:\n    ValueError: If network `name` is not recognized.\n  """"""\n  if name not in networks_map:\n    raise ValueError(\'Name of network unknown %s\' % name)\n  func = networks_map[name]\n  @functools.wraps(func)\n  def network_fn(images, **kwargs):\n    arg_scope = arg_scopes_map[name](weight_decay=weight_decay)\n    with slim.arg_scope(arg_scope):\n      return func(images, num_classes=num_classes, is_training=is_training,\n                  **kwargs)\n  if hasattr(func, \'default_image_size\'):\n    network_fn.default_image_size = func.default_image_size\n\n  return network_fn\n'"
examples/slim/nets/nets_factory_test.py,11,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for slim.inception.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nfrom nets import nets_factory\n\n\nclass NetworksTest(tf.test.TestCase):\n\n  def testGetNetworkFnFirstHalf(self):\n    batch_size = 5\n    num_classes = 1000\n    for net in list(nets_factory.networks_map.keys())[:10]:\n      with tf.Graph().as_default() as g, self.test_session(g):\n        net_fn = nets_factory.get_network_fn(net, num_classes=num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224)\n        if net not in [\'i3d\', \'s3dg\']:\n          inputs = tf.random_uniform(\n              (batch_size, image_size, image_size, 3))\n          logits, end_points = net_fn(inputs)\n          self.assertTrue(isinstance(logits, tf.Tensor))\n          self.assertTrue(isinstance(end_points, dict))\n          self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n          self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\n  def testGetNetworkFnSecondHalf(self):\n    batch_size = 5\n    num_classes = 1000\n    for net in list(nets_factory.networks_map.keys())[10:]:\n      with tf.Graph().as_default() as g, self.test_session(g):\n        net_fn = nets_factory.get_network_fn(net, num_classes=num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224)\n        if net not in [\'i3d\', \'s3dg\']:\n          inputs = tf.random_uniform(\n              (batch_size, image_size, image_size, 3))\n          logits, end_points = net_fn(inputs)\n          self.assertTrue(isinstance(logits, tf.Tensor))\n          self.assertTrue(isinstance(end_points, dict))\n          self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n          self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\n  def testGetNetworkFnVideoModels(self):\n    batch_size = 5\n    num_classes = 400\n    for net in [\'i3d\', \'s3dg\']:\n      with tf.Graph().as_default() as g, self.test_session(g):\n        net_fn = nets_factory.get_network_fn(net, num_classes=num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224) // 2\n        inputs = tf.random_uniform(\n            (batch_size, 10, image_size, image_size, 3))\n        logits, end_points = net_fn(inputs)\n        self.assertTrue(isinstance(logits, tf.Tensor))\n        self.assertTrue(isinstance(end_points, dict))\n        self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n        self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
examples/slim/nets/resnet_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                       padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                       rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None,\n                       store_non_strided_activations=False,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    store_non_strided_activations: If True, we compute non-strided (undecimated)\n      activations at the last unit of each block and store them in the\n      `outputs_collections` before subsampling them. This gives us access to\n      higher resolution intermediate activations which are useful in some\n      dense prediction problems but increases 4x the computation and memory cost\n      at the last unit of each block.\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n      block_stride = 1\n      for i, unit in enumerate(block.args):\n        if store_non_strided_activations and i == len(block.args) - 1:\n          # Move stride from the block\'s last unit to the end of the block.\n          block_stride = unit.get(\'stride\', 1)\n          unit = dict(unit, stride=1)\n\n        with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))\n            rate *= unit.get(\'stride\', 1)\n\n          else:\n            net = block.unit_fn(net, rate=1, **unit)\n            current_stride *= unit.get(\'stride\', 1)\n            if output_stride is not None and current_stride > output_stride:\n              raise ValueError(\'The target output_stride cannot be reached.\')\n\n      # Collect activations at the block\'s end before performing subsampling.\n      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n      # Subsampling of the block\'s output activations.\n      if output_stride is not None and current_stride == output_stride:\n        rate *= block_stride\n      else:\n        net = subsample(net, block_stride)\n        current_stride *= block_stride\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True,\n                     activation_fn=tf.nn.relu,\n                     use_batch_norm=True,\n                     batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n    activation_fn: The activation function which is used in ResNet.\n    use_batch_norm: Whether or not to use batch normalization.\n    batch_norm_updates_collections: Collection for the update ops for\n      batch norm.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': batch_norm_updates_collections,\n      \'fused\': None,  # Use fused batch norm if possible.\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=activation_fn,\n      normalizer_fn=slim.batch_norm if use_batch_norm else None,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n'"
examples/slim/nets/resnet_v1.py,9,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\nslim = tf.contrib.slim\n\n\nclass NoOpScope(object):\n  """"""No-op context manager.""""""\n\n  def __enter__(self):\n    return None\n\n  def __exit__(self, exc_type, exc_value, traceback):\n    return False\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs,\n               depth,\n               depth_bottleneck,\n               stride,\n               rate=1,\n               outputs_collections=None,\n               scope=None,\n               use_bounded_activations=False):\n  """"""Bottleneck residual unit variant with BN after convolutions.\n\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n  its definition. Note that we use here the bottleneck variant which has an\n  extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n    use_bounded_activations: Whether or not to use bounded activations. Bounded\n      activations better lend themselves to quantized inference.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(\n          inputs,\n          depth, [1, 1],\n          stride=stride,\n          activation_fn=tf.nn.relu6 if use_bounded_activations else None,\n          scope=\'shortcut\')\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           activation_fn=None, scope=\'conv3\')\n\n    if use_bounded_activations:\n      # Use clip_by_value to simulate bandpass activation.\n      residual = tf.clip_by_value(residual, -6.0, 6.0)\n      output = tf.nn.relu6(shortcut + residual)\n    else:\n      output = tf.nn.relu(shortcut + residual)\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.name,\n                                            output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              store_non_strided_activations=False,\n              reuse=None,\n              scope=None):\n  """"""Generator for v1 ResNet models.\n\n  This function generates a family of ResNet v1 models. See the resnet_v1_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks.\n      If 0 or None, we return the features before the logit layer.\n    is_training: whether batch_norm layers are in training mode. If this is set\n      to None, the callers can specify slim.batch_norm\'s is_training parameter\n      from an outer slim.arg_scope.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n        To use this parameter, the input images must be smaller than 300x300\n        pixels, in which case the output logit layer does not contain spatial\n        information and can be removed.\n    store_non_strided_activations: If True, we compute non-strided (undecimated)\n      activations at the last unit of each block and store them in the\n      `outputs_collections` before subsampling them. This gives us access to\n      higher resolution intermediate activations which are useful in some\n      dense prediction problems but increases 4x the computation and memory cost\n      at the last unit of each block.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is 0 or None,\n      then net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes a non-zero integer, net contains the\n      pre-softmax activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.original_name_scope + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with (slim.arg_scope([slim.batch_norm], is_training=is_training)\n            if is_training is not None else NoOpScope()):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride,\n                                              store_non_strided_activations)\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n            end_points_collection)\n\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n          end_points[\'global_pool\'] = net\n        if num_classes:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n          end_points[sc.name + \'/logits\'] = net\n          if spatial_squeeze:\n            net = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n            end_points[sc.name + \'/spatial_squeeze\'] = net\n          end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n        return net, end_points\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_block(scope, base_depth, num_units, stride):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n  }])\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 store_non_strided_activations=False,\n                 min_base_depth=8,\n                 depth_multiplier=1,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n  depth_func = lambda d: max(int(d * depth_multiplier), min_base_depth)\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=depth_func(64), num_units=3,\n                      stride=2),\n      resnet_v1_block(\'block2\', base_depth=depth_func(128), num_units=4,\n                      stride=2),\n      resnet_v1_block(\'block3\', base_depth=depth_func(256), num_units=6,\n                      stride=2),\n      resnet_v1_block(\'block4\', base_depth=depth_func(512), num_units=3,\n                      stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   store_non_strided_activations=store_non_strided_activations,\n                   reuse=reuse, scope=scope)\nresnet_v1_50.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  store_non_strided_activations=False,\n                  min_base_depth=8,\n                  depth_multiplier=1,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n  depth_func = lambda d: max(int(d * depth_multiplier), min_base_depth)\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=depth_func(64), num_units=3,\n                      stride=2),\n      resnet_v1_block(\'block2\', base_depth=depth_func(128), num_units=4,\n                      stride=2),\n      resnet_v1_block(\'block3\', base_depth=depth_func(256), num_units=23,\n                      stride=2),\n      resnet_v1_block(\'block4\', base_depth=depth_func(512), num_units=3,\n                      stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   store_non_strided_activations=store_non_strided_activations,\n                   reuse=reuse, scope=scope)\nresnet_v1_101.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  store_non_strided_activations=False,\n                  spatial_squeeze=True,\n                  min_base_depth=8,\n                  depth_multiplier=1,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n  depth_func = lambda d: max(int(d * depth_multiplier), min_base_depth)\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=depth_func(64), num_units=3,\n                      stride=2),\n      resnet_v1_block(\'block2\', base_depth=depth_func(128), num_units=8,\n                      stride=2),\n      resnet_v1_block(\'block3\', base_depth=depth_func(256), num_units=36,\n                      stride=2),\n      resnet_v1_block(\'block4\', base_depth=depth_func(512), num_units=3,\n                      stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   store_non_strided_activations=store_non_strided_activations,\n                   reuse=reuse, scope=scope)\nresnet_v1_152.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  store_non_strided_activations=False,\n                  spatial_squeeze=True,\n                  min_base_depth=8,\n                  depth_multiplier=1,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n  depth_func = lambda d: max(int(d * depth_multiplier), min_base_depth)\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=depth_func(64), num_units=3,\n                      stride=2),\n      resnet_v1_block(\'block2\', base_depth=depth_func(128), num_units=24,\n                      stride=2),\n      resnet_v1_block(\'block3\', base_depth=depth_func(256), num_units=36,\n                      stride=2),\n      resnet_v1_block(\'block4\', base_depth=depth_func(512), num_units=3,\n                      stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   store_non_strided_activations=store_non_strided_activations,\n                   reuse=reuse, scope=scope)\nresnet_v1_200.default_image_size = resnet_v1.default_image_size\n'"
examples/slim/nets/resnet_v1_test.py,48,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.resnet_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom nets import resnet_utils\nfrom nets import resnet_v1\nimport numpy as np\nfrom six.moves import range\nfrom six.moves import zip\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = slim.utils.convert_collection_to_dict(\'end_points\')\n        return net, end_points\n\n  def testEndPointsV1(self):\n    """"""Test the end points of a tiny v1 bottleneck network.""""""\n    blocks = [\n        resnet_v1.resnet_v1_block(\n            \'block1\', base_depth=1, num_units=2, stride=2),\n        resnet_v1.resnet_v1_block(\n            \'block2\', base_depth=2, num_units=2, stride=1),\n    ]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv3\']\n    self.assertItemsEqual(expected, list(end_points.keys()))\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net, rate=1, **unit)\n    return net\n\n  def testAtrousValuesBottleneck(self):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n    """"""\n    block = resnet_v1.resnet_v1_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=2, stride=2),\n        block(\'block2\', base_depth=2, num_units=2, stride=2),\n        block(\'block3\', base_depth=4, num_units=2, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n  def testStridingLastUnitVsSubsampleBlockEnd(self):\n    """"""Compares subsampling at the block\'s last unit or block\'s end.\n\n    Makes sure that the final output is the same when we use a stride at the\n    last unit of a block vs. we subsample activations at the end of a block.\n    """"""\n    block = resnet_v1.resnet_v1_block\n\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=2, stride=2),\n        block(\'block2\', base_depth=2, num_units=2, stride=2),\n        block(\'block3\', base_depth=4, num_units=2, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n\n              # Subsampling at the last unit of the block.\n              output = resnet_utils.stack_blocks_dense(\n                  inputs, blocks, output_stride,\n                  store_non_strided_activations=False,\n                  outputs_collections=\'output\')\n              output_end_points = slim.utils.convert_collection_to_dict(\n                  \'output\')\n\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n\n              # Subsample activations at the end of the blocks.\n              expected = resnet_utils.stack_blocks_dense(\n                  inputs, blocks, output_stride,\n                  store_non_strided_activations=True,\n                  outputs_collections=\'expected\')\n              expected_end_points = slim.utils.convert_collection_to_dict(\n                  \'expected\')\n\n              sess.run(tf.global_variables_initializer())\n\n              # Make sure that the final output is the same.\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n              # Make sure that intermediate block activations in\n              # output_end_points are subsampled versions of the corresponding\n              # ones in expected_end_points.\n              for i, block in enumerate(blocks[:-1:]):\n                output = output_end_points[block.scope]\n                expected = expected_end_points[block.scope]\n                atrous_activated = (output_stride is not None and\n                                    2 ** i >= output_stride)\n                if not atrous_activated:\n                  expected = resnet_utils.subsample(expected, 2)\n                output, expected = sess.run([output, expected])\n                self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v1 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    spatial_squeeze=True,\n                    reuse=None,\n                    scope=\'resnet_v1_small\'):\n    """"""A shallow and thin ResNet v1 for faster tests.""""""\n    block = resnet_v1.resnet_v1_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=3, stride=2),\n        block(\'block2\', base_depth=2, num_units=3, stride=2),\n        block(\'block3\', base_depth=4, num_units=3, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    return resnet_v1.resnet_v1(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               spatial_squeeze=spatial_squeeze,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              spatial_squeeze=False,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n    self.assertTrue(\'global_pool\' in end_points)\n    self.assertListEqual(end_points[\'global_pool\'].get_shape().as_list(),\n                         [2, 1, 1, 32])\n\n  def testClassificationEndPointsWithNoBatchNormArgscope(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              spatial_squeeze=False,\n                                              is_training=None,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n    self.assertTrue(\'global_pool\' in end_points)\n    self.assertListEqual(end_points[\'global_pool\'].get_shape().as_list(),\n                         [2, 1, 1, 32])\n\n  def testEndpointNames(self):\n    # Like ResnetUtilsTest.testEndPointsV1(), but for the public API.\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n    expected = [\'resnet/conv1\']\n    for block in range(1, 5):\n      for unit in range(1, 4 if block < 4 else 3):\n        for conv in range(1, 4):\n          expected.append(\'resnet/block%d/unit_%d/bottleneck_v1/conv%d\' %\n                          (block, unit, conv))\n        expected.append(\'resnet/block%d/unit_%d/bottleneck_v1\' % (block, unit))\n      expected.append(\'resnet/block%d/unit_1/bottleneck_v1/shortcut\' % block)\n      expected.append(\'resnet/block%d\' % block)\n    expected.extend([\'global_pool\', \'resnet/logits\', \'resnet/spatial_squeeze\',\n                     \'predictions\'])\n    self.assertItemsEqual(list(end_points.keys()), expected)\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         spatial_squeeze=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         spatial_squeeze=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         spatial_squeeze=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None, is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None, is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     spatial_squeeze=False,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None, global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n  def testDepthMultiplier(self):\n    resnets = [\n        resnet_v1.resnet_v1_50, resnet_v1.resnet_v1_101,\n        resnet_v1.resnet_v1_152, resnet_v1.resnet_v1_200\n    ]\n    resnet_names = [\n        \'resnet_v1_50\', \'resnet_v1_101\', \'resnet_v1_152\', \'resnet_v1_200\'\n    ]\n    for resnet, resnet_name in zip(resnets, resnet_names):\n      depth_multiplier = 0.25\n      global_pool = True\n      num_classes = 10\n      inputs = create_test_input(2, 224, 224, 3)\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        scope_base = resnet_name + \'_base\'\n        _, end_points_base = resnet(\n            inputs,\n            num_classes,\n            global_pool=global_pool,\n            min_base_depth=1,\n            scope=scope_base)\n        scope_test = resnet_name + \'_test\'\n        _, end_points_test = resnet(\n            inputs,\n            num_classes,\n            global_pool=global_pool,\n            min_base_depth=1,\n            depth_multiplier=depth_multiplier,\n            scope=scope_test)\n        for block in [\'block1\', \'block2\', \'block3\', \'block4\']:\n          block_name_base = scope_base + \'/\' + block\n          block_name_test = scope_test + \'/\' + block\n          self.assertTrue(block_name_base in end_points_base)\n          self.assertTrue(block_name_test in end_points_test)\n          self.assertEqual(\n              len(end_points_base[block_name_base].get_shape().as_list()), 4)\n          self.assertEqual(\n              len(end_points_test[block_name_test].get_shape().as_list()), 4)\n          self.assertListEqual(\n              end_points_base[block_name_base].get_shape().as_list()[:3],\n              end_points_test[block_name_test].get_shape().as_list()[:3])\n          self.assertEqual(\n              int(depth_multiplier *\n                  end_points_base[block_name_base].get_shape().as_list()[3]),\n              end_points_test[block_name_test].get_shape().as_list()[3])\n\n  def testMinBaseDepth(self):\n    resnets = [\n        resnet_v1.resnet_v1_50, resnet_v1.resnet_v1_101,\n        resnet_v1.resnet_v1_152, resnet_v1.resnet_v1_200\n    ]\n    resnet_names = [\n        \'resnet_v1_50\', \'resnet_v1_101\', \'resnet_v1_152\', \'resnet_v1_200\'\n    ]\n    for resnet, resnet_name in zip(resnets, resnet_names):\n      min_base_depth = 5\n      global_pool = True\n      num_classes = 10\n      inputs = create_test_input(2, 224, 224, 3)\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        _, end_points = resnet(\n            inputs,\n            num_classes,\n            global_pool=global_pool,\n            min_base_depth=min_base_depth,\n            depth_multiplier=0,\n            scope=resnet_name)\n        for block in [\'block1\', \'block2\', \'block3\', \'block4\']:\n          block_name = resnet_name + \'/\' + block\n          self.assertTrue(block_name in end_points)\n          self.assertEqual(\n              len(end_points[block_name].get_shape().as_list()), 4)\n          # The output depth is 4 times base_depth.\n          depth_expected = min_base_depth * 4\n          self.assertEqual(\n              end_points[block_name].get_shape().as_list()[3], depth_expected)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
examples/slim/nets/resnet_v2.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the preactivation form of Residual Networks.\n\nResidual networks (ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant implemented in this module was\nintroduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer.\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v2\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\nslim = tf.contrib.slim\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN before convolutions.\n\n  This is the full preactivation residual unit variant proposed in [2]. See\n  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n  variant which has an extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\'preact\')\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n                             normalizer_fn=None, activation_fn=None,\n                             scope=\'shortcut\')\n\n    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           normalizer_fn=None, activation_fn=None,\n                           scope=\'conv3\')\n\n    output = shortcut + residual\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.name,\n                                            output)\n\n\ndef resnet_v2(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              reuse=None,\n              scope=None):\n  """"""Generator for v2 (preactivation) ResNet models.\n\n  This function generates a family of ResNet v2 models. See the resnet_v2_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks.\n      If 0 or None, we return the features before the logit layer.\n    is_training: whether batch_norm layers are in training mode.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it. If excluded, `inputs` should be the\n      results of an activation-less convolution.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n        To use this parameter, the input images must be smaller than 300x300\n        pixels, in which case the output logit layer does not contain spatial\n        information and can be removed.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is 0 or None,\n      then net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is a non-zero integer, net contains the\n      pre-softmax activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.original_name_scope + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          # We do not include batch normalization or activation functions in\n          # conv1 because the first ResNet unit will perform these. Cf.\n          # Appendix of [2].\n          with slim.arg_scope([slim.conv2d],\n                              activation_fn=None, normalizer_fn=None):\n            net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        # This is needed because the pre-activation variant does not have batch\n        # normalization or activation functions in the residual unit output. See\n        # Appendix of [2].\n        net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n            end_points_collection)\n\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n          end_points[\'global_pool\'] = net\n        if num_classes:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n          end_points[sc.name + \'/logits\'] = net\n          if spatial_squeeze:\n            net = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n            end_points[sc.name + \'/spatial_squeeze\'] = net\n          end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n        return net, end_points\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_block(scope, base_depth, num_units, stride):\n  """"""Helper function for creating a resnet_v2 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v2 bottleneck block.\n  """"""\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n  }])\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'resnet_v2_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=6, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_50.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=23, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_101.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=8, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_152.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=24, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_200.default_image_size = resnet_v2.default_image_size\n'"
examples/slim/nets/resnet_v2_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.resnet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v2\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = slim.utils.convert_collection_to_dict(\'end_points\')\n        return net, end_points\n\n  def testEndPointsV2(self):\n    """"""Test the end points of a tiny v2 bottleneck network.""""""\n    blocks = [\n        resnet_v2.resnet_v2_block(\n            \'block1\', base_depth=1, num_units=2, stride=2),\n        resnet_v2.resnet_v2_block(\n            \'block2\', base_depth=2, num_units=2, stride=1),\n    ]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv3\']\n    self.assertItemsEqual(expected, end_points.keys())\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net, rate=1, **unit)\n    return net\n\n  def testAtrousValuesBottleneck(self):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n    """"""\n    block = resnet_v2.resnet_v2_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=2, stride=2),\n        block(\'block2\', base_depth=2, num_units=2, stride=2),\n        block(\'block3\', base_depth=4, num_units=2, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v2 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    spatial_squeeze=True,\n                    reuse=None,\n                    scope=\'resnet_v2_small\'):\n    """"""A shallow and thin ResNet v2 for faster tests.""""""\n    block = resnet_v2.resnet_v2_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=3, stride=2),\n        block(\'block2\', base_depth=2, num_units=3, stride=2),\n        block(\'block3\', base_depth=4, num_units=3, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    return resnet_v2.resnet_v2(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               spatial_squeeze=spatial_squeeze,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              spatial_squeeze=False,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n    self.assertTrue(\'global_pool\' in end_points)\n    self.assertListEqual(end_points[\'global_pool\'].get_shape().as_list(),\n                         [2, 1, 1, 32])\n\n  def testEndpointNames(self):\n    # Like ResnetUtilsTest.testEndPointsV2(), but for the public API.\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n    expected = [\'resnet/conv1\']\n    for block in range(1, 5):\n      for unit in range(1, 4 if block < 4 else 3):\n        for conv in range(1, 4):\n          expected.append(\'resnet/block%d/unit_%d/bottleneck_v2/conv%d\' %\n                          (block, unit, conv))\n        expected.append(\'resnet/block%d/unit_%d/bottleneck_v2\' % (block, unit))\n      expected.append(\'resnet/block%d/unit_1/bottleneck_v2/shortcut\' % block)\n      expected.append(\'resnet/block%d\' % block)\n    expected.extend([\'global_pool\', \'resnet/logits\', \'resnet/spatial_squeeze\',\n                     \'predictions\'])\n    self.assertItemsEqual(end_points.keys(), expected)\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         spatial_squeeze=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         spatial_squeeze=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         spatial_squeeze=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None,\n                                           is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None,\n                                             is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     spatial_squeeze=False,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None,\n                                     global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
examples/slim/preprocessing/__init__.py,0,b'\n'
examples/slim/preprocessing/preprocessing_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom preprocessing import vgg_preprocessing\n\nslim = tf.contrib.slim\n\n\ndef get_preprocessing(name, is_training=False):\n  """"""Returns preprocessing_fn(image, height, width, **kwargs).\n\n  Args:\n    name: The name of the preprocessing function.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    preprocessing_fn: A function that preprocessing a single image (pre-batch).\n      It has the following signature:\n        image = preprocessing_fn(image, output_height, output_width, ...).\n\n  Raises:\n    ValueError: If Preprocessing `name` is not recognized.\n  """"""\n  preprocessing_fn_map = {\n      \'resnet_v1_50\': vgg_preprocessing,\n      \'resnet_v1_101\': vgg_preprocessing,\n      \'resnet_v1_152\': vgg_preprocessing,\n      \'resnet_v1_200\': vgg_preprocessing,\n      \'resnet_v2_50\': vgg_preprocessing,\n      \'resnet_v2_101\': vgg_preprocessing,\n      \'resnet_v2_152\': vgg_preprocessing,\n      \'resnet_v2_200\': vgg_preprocessing\n  }\n\n  if name not in preprocessing_fn_map:\n    raise ValueError(\'Preprocessing name [%s] was not recognized\' % name)\n\n  def preprocessing_fn(image, output_height, output_width, **kwargs):\n    return preprocessing_fn_map[name].preprocess_image(\n        image, output_height, output_width, is_training=is_training, **kwargs)\n\n  return preprocessing_fn\n'"
examples/slim/preprocessing/vgg_preprocessing.py,60,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nThe preprocessing steps for VGG were introduced in the following technical\nreport:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n  """"""Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn\'t assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: an image of shape [height, width, channels].\n    offset_height: a scalar tensor indicating the height offset.\n    offset_width: a scalar tensor indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.\n  """"""\n  original_shape = tf.shape(image)\n\n  rank_assertion = tf.Assert(\n      tf.equal(tf.rank(image), 3),\n      [\'Rank of image must be equal to 3.\'])\n  with tf.control_dependencies([rank_assertion]):\n    cropped_shape = tf.stack([crop_height, crop_width, original_shape[2]])\n\n  size_assertion = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(original_shape[0], crop_height),\n          tf.greater_equal(original_shape[1], crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  with tf.control_dependencies([size_assertion]):\n    image = tf.slice(image, offsets, cropped_shape)\n  return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n\n  Returns:\n    the image_list with cropped images.\n\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(\'Empty image_list.\')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n        tf.equal(image_rank, 3),\n        [\'Wrong rank for tensor  %s [expected] [actual]\',\n         image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  with tf.control_dependencies([rank_assertions[0]]):\n    image_shape = tf.shape(image_list[0])\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(image_height, crop_height),\n          tf.greater_equal(image_width, crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    with tf.control_dependencies([rank_assertions[i]]):\n      shape = tf.shape(image)\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n        tf.equal(height, image_height),\n        [\'Wrong height for tensor %s [expected][actual]\',\n         image.name, height, image_height])\n    width_assert = tf.Assert(\n        tf.equal(width, image_width),\n        [\'Wrong width for tensor %s [expected][actual]\',\n         image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  with tf.control_dependencies(asserts):\n    max_offset_height = tf.reshape(image_height - crop_height + 1, [])\n  with tf.control_dependencies(asserts):\n    max_offset_width = tf.reshape(image_width - crop_width + 1, [])\n  offset_height = tf.random_uniform(\n      [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random_uniform(\n      [], maxval=max_offset_width, dtype=tf.int32)\n\n  return [_crop(image, offset_height, offset_width,\n                crop_height, crop_width) for image in image_list]\n\n\ndef _central_crop(image_list, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    the list of cropped images.\n  """"""\n  outputs = []\n  for image in image_list:\n    image_height = tf.shape(image)[0]\n    image_width = tf.shape(image)[1]\n\n    offset_height = (image_height - crop_height) / 2\n    offset_width = (image_width - crop_width) / 2\n\n    outputs.append(_crop(image, offset_height, offset_width,\n                         crop_height, crop_width))\n  return outputs\n\n\ndef _mean_image_subtraction(image, means):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(axis=2, values=channels)\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: and int32 scalar tensor indicating the new width.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  height = tf.to_float(height)\n  width = tf.to_float(width)\n  smallest_side = tf.to_float(smallest_side)\n\n  scale = tf.cond(tf.greater(height, width),\n                  lambda: smallest_side / width,\n                  lambda: smallest_side / height)\n  new_height = tf.to_int32(tf.rint(height * scale))\n  new_width = tf.to_int32(tf.rint(width * scale))\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, smallest_side):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  shape = tf.shape(image)\n  height = shape[0]\n  width = shape[1]\n  new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n  image = tf.expand_dims(image, 0)\n  resized_image = tf.image.resize_bilinear(image, [new_height, new_width],\n                                           align_corners=False)\n  resized_image = tf.squeeze(resized_image)\n  resized_image.set_shape([None, None, 3])\n  return resized_image\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         resize_side_min=_RESIZE_SIDE_MIN,\n                         resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  resize_side = tf.random_uniform(\n      [], minval=resize_side_min, maxval=resize_side_max+1, dtype=tf.int32)\n\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _random_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  image = tf.image.random_flip_left_right(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_for_eval(image, output_height, output_width, resize_side):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side: The smallest side of the image for aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _central_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False,\n                     resize_side_min=_RESIZE_SIDE_MIN,\n                     resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, then this value\n      is used for rescaling.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, this value is\n      ignored. Otherwise, the resize side is sampled from\n        [resize_size_min, resize_size_max].\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    return preprocess_for_train(image, output_height, output_width,\n                                resize_side_min, resize_side_max)\n  else:\n    return preprocess_for_eval(image, output_height, output_width,\n                               resize_side_min)\n'"
