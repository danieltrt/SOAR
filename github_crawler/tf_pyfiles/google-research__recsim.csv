file_path,api_count,code
setup.py,0,"b'# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Setup script for RecSim.\n\nThis script will install RecSim as a Python module.\n\nSee: https://github.com/google-research/recsim\n\n""""""\n\nfrom os import path\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nhere = path.abspath(path.dirname(__file__))\n\ninstall_requires = [\n    \'absl-py\',\n    \'dopamine-rl >= 2.0.5\',\n    \'gin-config\',\n    \'gym\',\n    \'numpy\',\n    \'scipy\',\n    \'tensorflow\',\n]\n\nrecsim_description = (\n    \'RecSim: A Configurable Recommender Systems Simulation Platform\')\n\nwith open(\'README.md\', \'r\') as fh:\n  long_description = fh.read()\n\nsetup(\n    name=\'recsim\',\n    version=\'0.2.3\',\n    author=\'The RecSim Team\',\n    author_email=\'no-reply@google.com\',\n    description=recsim_description,\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    url=\'https://github.com/google-research/recsim\',\n    packages=find_packages(exclude=[\'docs\']),\n    classifiers=[  # Optional\n        \'Development Status :: 3 - Alpha\',\n\n        # Indicate who your project is intended for\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n\n        # Pick your license as you wish\n        \'License :: OSI Approved :: Apache Software License\',\n\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 3\',\n\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Mathematics\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n\n    ],\n    install_requires=install_requires,\n    project_urls={  # Optional\n        \'Documentation\': \'https://github.com/google-research/recsim\',\n        \'Bug Reports\': \'https://github.com/google-research/recsim/issues\',\n        \'Source\': \'https://github.com/google-research/recsim\',\n    },\n    license=\'Apache 2.0\',\n    keywords=\'recsim reinforcement-learning recommender-system simulation\'\n)\n'"
recsim/__init__.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Module importing the core library.""""""\nfrom recsim import agent\nfrom recsim import choice_model\nfrom recsim import document\nfrom recsim import user\nfrom recsim import utils\n'"
recsim/agent.py,4,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Abstract interface for recommender system agents.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nfrom absl import logging\nimport six\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass AbstractRecommenderAgent(object):\n  """"""Abstract class to model a recommender system agent.""""""\n  _multi_user = False\n\n  def __init__(self, action_space):\n    """"""Initializes AbstractRecommenderAgent.\n\n    Args:\n      action_space: A gym.spaces object that specifies the format of actions.\n    """"""\n    self._slate_size = action_space.nvec.shape[0]\n\n  @property\n  def multi_user(self):\n    """"""Returns boolean indicating whether this agent serves multiple users.""""""\n    return self._multi_user\n\n  @abc.abstractmethod\n  def step(self, reward, observation):\n    """"""Records the most recent transition and returns the agent\'s next action.\n\n    We store the observation of the last time step since we want to store it\n    with the reward.\n\n    Args:\n      reward: The reward received from the agent\'s most recent action as a\n        float.\n      observation: A dictionary that includes the most recent observations.\n\n    Returns:\n      slate: An integer array of size _slate_size, where each element is an\n        index into the list of doc_obs\n    """"""\n\n  @abc.abstractmethod\n  def bundle_and_checkpoint(self, checkpoint_dir, iteration_number):\n    """"""Returns a self-contained bundle of the agent\'s state.\n\n    This is used for checkpointing. It will return a dictionary containing all\n    non-TensorFlow objects (to be saved into a file by the caller), and it saves\n    all TensorFlow objects into a checkpoint file.\n\n    Args:\n      checkpoint_dir: A string for the directory where objects will be saved.\n      iteration_number: An integer of iteration number to use for naming the\n        checkpoint file.\n\n    Returns:\n      A dictionary containing additional Python objects to be checkpointed by\n        the experiment. Each key is a string for the object name and the value\n        is actual object. If the checkpoint directory does not exist, returns\n        empty dictionary.\n    """"""\n\n  @abc.abstractmethod\n  def unbundle(self, checkpoint_dir, iteration_number, bundle_dict):\n    """"""Restores the agent from a checkpoint.\n\n    Restores the agent\'s Python objects to those specified in bundle_dict,\n    and restores the TensorFlow objects to those specified in the\n    checkpoint_dir. If the checkpoint_dir does not exist, will not reset the\n    agent\'s state.\n\n    Args:\n      checkpoint_dir: A string that represents the path to the checkpoint saved\n        by tf.Save.\n      iteration_number: An integer that represents the checkpoint version and is\n        used when restoring replay buffer.\n      bundle_dict: A dict containing additional Python objects owned by the\n        agent. Each key is an object name and the value is the actual object.\n\n    Returns:\n      bool, True if unbundling was successful.\n    """"""\n\n\nclass AbstractEpisodicRecommenderAgent(AbstractRecommenderAgent):\n  """"""Abstract class for recommender systems that solves episodic tasks.""""""\n\n  def __init__(self, action_space, summary_writer=None):\n    """"""Initializes AbstractEpisodicRecommenderAgent.\n\n    Args:\n      action_space: A gym.spaces object that specifies the format of actions.\n      summary_writer: A Tensorflow summary writer to pass to the agent\n        for in-agent training statistics in Tensorboard.\n    """"""\n    super(AbstractEpisodicRecommenderAgent, self).__init__(action_space)\n    self._episode_num = 0\n    self._summary_writer = summary_writer\n\n  def begin_episode(self, observation=None):\n    """"""Returns the agent\'s first action for this episode.\n\n    Args:\n      observation: numpy array, the environment\'s initial observation.\n\n    Returns:\n      slate: An integer array of size _slate_size, where each element is an\n        index into the list of doc_obs\n    """"""\n    self._episode_num += 1\n    return self.step(0, observation)\n\n  def end_episode(self, reward, observation=None):\n    """"""Signals the end of the episode to the agent.\n\n    Args:\n      reward: An float that is the last reward from the environment.\n      observation: numpy array that represents the last observation of the\n        episode.\n    """"""\n    pass\n\n  def bundle_and_checkpoint(self, checkpoint_dir, iteration_number):\n    """"""Returns a self-contained bundle of the agent\'s state.\n\n    Args:\n      checkpoint_dir: A string that represents the path to the checkpoint and is\n        used when we save TensorFlow objects by tf.Save.\n      iteration_number: An integer that represents the checkpoint version and is\n        used when restoring replay buffer.\n\n    Returns:\n      A dictionary containing additional Python objects to be checkpointed by\n        the experiment. Each key is a string for the object name and the value\n        is actual object. If the checkpoint directory does not exist, returns\n        empty dictionary.\n    """"""\n    del checkpoint_dir  # Unused.\n    del iteration_number  # Unused.\n    bundle_dict = {}\n    bundle_dict[\'episode_num\'] = self._episode_num\n    return bundle_dict\n\n  def unbundle(self, checkpoint_dir, iteration_number, bundle_dict):\n    """"""Restores the agent from a checkpoint.\n\n    Args:\n      checkpoint_dir: A string that represents the path to the checkpoint and is\n        used when we save TensorFlow objects by tf.Save.\n      iteration_number: An integer that represents the checkpoint version and is\n        used when restoring replay buffer.\n      bundle_dict: A dict containing additional Python objects owned by the\n        agent. Each key is an object name and the value is the actual object.\n\n    Returns:\n      bool, True if unbundling was successful.\n    """"""\n    del checkpoint_dir  # Unused.\n    del iteration_number  # Unused.\n    if \'episode_num\' not in bundle_dict:\n      logging.warning(\n          \'Could not unbundle from checkpoint files with exception.\')\n      return False\n    self._episode_num = bundle_dict[\'episode_num\']\n    return True\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass AbstractMultiUserEpisodicRecommenderAgent(\n    AbstractEpisodicRecommenderAgent):\n  """"""Abstract class to model a recommender agent handling multiple users.""""""\n  _multi_user = True\n\n  def __init__(self, action_space):\n    """"""Initializes AbstractMultiUserEpisodicRecommenderAgent.\n\n    Args:\n      action_space: A gym.spaces object that specifies the format of actions.\n    """"""\n    self._num_users = len(action_space)\n    if not self._num_users > 0:\n      raise ValueError(\'Multi-user agent must have at least 1 user.\')\n    super(AbstractMultiUserEpisodicRecommenderAgent, self).__init__(\n        action_space[0]\n    )\n\n\nclass AbstractHierarchicalAgentLayer(AbstractRecommenderAgent):\n  """"""Parent class for stackable agent layers.""""""\n\n  def __init__(self, action_space, *base_agent_ctors):\n    super(AbstractHierarchicalAgentLayer, self).__init__(action_space)\n    self._base_agent_ctors = base_agent_ctors\n    self._base_agents = None\n\n  def _preprocess_reward_observation(self, reward, observation):\n    r""""""Modifies the reward and observation before passing to base agent.\n\n    This function is used to modify the observation and reward before\n    propagating it downward to the base agent. For example, it can\n    inject additional features like sufficient statistics by inserting fields\n    to observation[\\\'user\\\'], or, to implement regularization schemes by\n    subtracting penalties from the reward.\n\n    Args:\n      reward: float number.\n      observation: gym space in recsim format.\n\n    Returns:\n      reward: float number.\n      observation: gym space in recsim format.\n    """"""\n    return reward, observation\n\n  @abc.abstractmethod\n  def _postprocess_actions(self, action_list):\n    r""""""Aggregates (possibly abstract) base agent actions into a valid slate.""""""\n\n  def begin_episode(self, observation=None):\n    if observation is not None:\n      _, observation = self._preprocess_reward_observation(0, observation)\n    action_list = [\n        base_agent.begin_episode(observation=observation)\n        for base_agent in self._base_agents\n    ]\n    return self._postprocess_actions(action_list)\n\n  def end_episode(self, reward, observation):\n    reward, observation = self._preprocess_reward_observation(\n        reward, observation)\n    action_list = [\n        base_agent.end_episode(reward, observation=observation)\n        for base_agent in self._base_agents\n    ]\n    return self._postprocess_actions(action_list)\n\n  def bundle_and_checkpoint(self, checkpoint_dir, iteration_number):\n    """"""Returns a self-contained bundle of the agent\'s state.\n\n    Args:\n      checkpoint_dir: A string for the directory where objects will be saved.\n      iteration_number: An integer of iteration number to use for naming the\n        checkpoint file.\n\n    Returns:\n      A dictionary containing additional Python objects to be checkpointed by\n        the experiment. Each key is a string for the object name and the value\n        is actual object. If the checkpoint directory does not exist, returns\n        empty dictionary.\n    """"""\n    bundle_dict = {}\n    for i, base_agent in enumerate(self._base_agents):\n      base_bundle_dict = base_agent.bundle_and_checkpoint(\n          checkpoint_dir, iteration_number)\n      bundle_dict[\'base_agent_bundle_{}\'.format(i)] = base_bundle_dict\n    return bundle_dict\n\n  def unbundle(self, checkpoint_dir, iteration_number, bundle_dict):\n    """"""Restores the agent from a checkpoint.\n\n    Args:\n      checkpoint_dir: A string that represents the path to the checkpoint saved\n        by tf.Save.\n      iteration_number: An integer that represents the checkpoint version and is\n        used when restoring replay buffer.\n      bundle_dict: A dict containing additional Python objects owned by the\n        agent. Each key is an object name and the value is the actual object.\n\n    Returns:\n      bool, True if unbundling was successful.\n    """"""\n    success = True\n    for i, base_agent in enumerate(self._base_agents):\n      if \'base_agent_bundle_{}\'.format(i) not in bundle_dict:\n        logging.warning(\'Base agent bundle not found in bundle.\')\n        return False\n      success &= base_agent.unbundle(\n          checkpoint_dir, iteration_number,\n          bundle_dict[\'base_agent_bundle_{}\'.format(i)])\n    return success\n'"
recsim/agent_test.py,1,"b'# coding=utf-8\n# Lint as: python3\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\n\nfrom absl.testing import parameterized\nimport gym\nimport numpy as np\nfrom recsim import agent\nfrom recsim.simulator import environment\nfrom recsim.simulator import recsim_gym\nfrom recsim.simulator import runner_lib\nimport tensorflow.compat.v1 as tf\n\n\nclass DummySingleUserAgent(agent.AbstractEpisodicRecommenderAgent):\n\n  def step(self, reward, observation):\n    pass\n\n  def bundle_and_checkpoint(self, checkpoint_dir, iteration_number):\n    pass\n\n  def unbundle(self, checkpoint_dir, iteration_number, bundle_dict):\n    pass\n\n\nclass DummyMultiUserAgent(agent.AbstractMultiUserEpisodicRecommenderAgent):\n\n  def step(self, reward, observation):\n    pass\n\n  def bundle_and_checkpoint(self, checkpoint_dir, iteration_number):\n    pass\n\n  def unbundle(self, checkpoint_dir, iteration_number, bundle_dict):\n    pass\n\n\nclass DummySingleUserEnvironment(environment.SingleUserEnvironment):\n\n  def __init__(self):\n    pass\n\n\nclass DummyMultiUserEnvironment(environment.MultiUserEnvironment):\n\n  def __init__(self):\n    pass\n\n\ndef create_environment(env_config):\n  if env_config[\'multiuser_env\']:\n    env = DummyMultiUserEnvironment()\n  else:\n    env = DummySingleUserEnvironment()\n  reward_aggregator = lambda x: x\n  return recsim_gym.RecSimGymEnv(env, reward_aggregator)\n\n\nclass AgentTest(parameterized.TestCase):\n  num_users = 100\n  num_candidates = 10\n  slate_size = 8\n\n  @parameterized.named_parameters(\n      (\'multiuser_env_multiuser_agent\', True, True, True),\n      (\'singleuser_env_multiuser_agent\', False, True, False),\n      (\'multiuser_env_singleuser_agent\', True, False, False),\n      (\'singleuser_env_singleuser_agent\', False, False, True),\n  )\n  def test_multi_and_single_user_consistency(\n      self, multiuser_env, multiuser_agent, should_succeed):\n\n    def create_agent(\n        sess, env, summary_writer, eval_mode, multiuser_agent=True):\n      del sess, env, summary_writer, eval_mode  # unused\n      action_space = gym.spaces.MultiDiscrete(np.ones((self.slate_size,)))\n      if multiuser_agent:\n        action_space = gym.spaces.Tuple([action_space] * self.num_users)\n        AgentClass = DummyMultiUserAgent  # pylint:disable=invalid-name\n      else:\n        AgentClass = DummySingleUserAgent  # pylint:disable=invalid-name\n      return AgentClass(action_space)\n\n    env_config = dict(multiuser_env=multiuser_env)\n    base_dir = \'/tmp/Env%sAgent%s\' % (multiuser_env, multiuser_agent)\n    create_agent_fn = functools.partial(create_agent,\n                                        multiuser_agent=multiuser_agent)\n    if not os.path.exists(base_dir):\n      os.makedirs(base_dir)\n    if should_succeed:  # constructors should work\n      _ = runner_lib.TrainRunner(\n          base_dir=base_dir,\n          create_agent_fn=create_agent_fn,\n          env=create_environment(env_config),\n          max_training_steps=1,\n          max_steps_per_episode=1,\n          num_iterations=1)\n    else:  # agent constructor should raise error\n      with self.assertRaises(ValueError):\n        _ = runner_lib.TrainRunner(\n            base_dir=base_dir,\n            create_agent_fn=create_agent_fn,\n            env=create_environment(env_config),\n            max_training_steps=1,\n            max_steps_per_episode=1,\n            num_iterations=1)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
recsim/choice_model.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Abstract classes that encode a user\'s state and dynamics.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport numpy as np\nimport six\n\n\ndef softmax(vector):\n  """"""Computes the softmax of a vector.""""""\n  normalized_vector = np.array(vector) - np.max(\n      vector)  # For numerical stability\n  return np.exp(normalized_vector) / np.sum(np.exp(normalized_vector))\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass AbstractChoiceModel(object):\n  """"""Abstract class to represent the user choice model.\n\n  Each user has a choice model.\n  """"""\n  _scores = None\n  _score_no_click = None\n\n  @abc.abstractmethod\n  def score_documents(self, user_state, doc_obs):\n    """"""Computes unnormalized scores of documents in the slate given user state.\n\n    Args:\n      user_state: An instance of AbstractUserState.\n      doc_obs: A numpy array that represents the observation of all documents in\n        the slate.\n    Attributes:\n      scores: A numpy array that stores the scores of all documents.\n      score_no_click: A float that represents the score for the action of\n        picking no document.\n    """"""\n\n  @property\n  def scores(self):\n    return self._scores\n\n  @property\n  def score_no_click(self):\n    return self._score_no_click\n\n  @abc.abstractmethod\n  def choose_item(self):\n    """"""Returns selected index of document in the slate.\n\n    Returns:\n      selected_index: a integer indicating which item was chosen, or None if\n        none were selected.\n    """"""\n\n\nclass NormalizableChoiceModel(AbstractChoiceModel):\n  """"""A normalizable choice model.""""""\n\n  @staticmethod\n  def _score_documents_helper(user_state, doc_obs):\n    scores = np.array([])\n    for doc in doc_obs:\n      scores = np.append(scores, user_state.score_document(doc))\n    return scores\n\n  def choose_item(self):\n    all_scores = np.append(self._scores, self._score_no_click)\n    all_probs = all_scores / np.sum(all_scores)\n    selected_index = np.random.choice(len(all_probs), p=all_probs)\n    if selected_index == len(all_probs) - 1:\n      selected_index = None\n    return selected_index\n\n\nclass MultinomialLogitChoiceModel(NormalizableChoiceModel):\n  """"""A multinomial logit choice model.\n\n   Samples item x in scores according to\n     p(x) = exp(x) / Sum_{y in scores} exp(y)\n\n   Args:\n     choice_features: a dict that stores the features used in choice model:\n       `no_click_mass`: a float indicating the mass given to a no click option.\n  """"""\n\n  def __init__(self, choice_features):\n    self._no_click_mass = choice_features.get(\'no_click_mass\', -float(\'Inf\'))\n\n  def score_documents(self, user_state, doc_obs):\n    logits = self._score_documents_helper(user_state, doc_obs)\n    logits = np.append(logits, self._no_click_mass)\n    # Use softmax scores instead of exponential scores to avoid overflow.\n    all_scores = softmax(logits)\n    self._scores = all_scores[:-1]\n    self._score_no_click = all_scores[-1]\n\n\nclass MultinomialProportionalChoiceModel(NormalizableChoiceModel):\n  """"""A multinomial proportional choice function.\n\n  Samples item x in scores according to\n    p(x) = x - min_normalizer / sum(x - min_normalizer)\n\n  Attributes:\n    min_normalizer: A float (<= 0) used to offset the scores to be positive.\n      Specifically, if the scores have negative elements, then they do not\n      form a valid probability distribution for sampling. Subtracting the\n      least expected element is one heuristic for normalization.\n    no_click_mass: An optional float indicating the mass given to a no click\n      option\n  """"""\n\n  def __init__(self, choice_features):\n    self._min_normalizer = choice_features.get(\'min_normalizer\')\n    self._no_click_mass = choice_features.get(\'no_click_mass\', 0)\n\n  def score_documents(self, user_state, doc_obs):\n    scores = self._score_documents_helper(user_state, doc_obs)\n    all_scores = np.append(scores, self._no_click_mass)\n    all_scores = all_scores - self._min_normalizer\n    assert all_scores[\n        all_scores <\n        0.0].size == 0, \'Normalized scores have non-positive elements.\'\n    self._scores = all_scores[:-1]\n    self._score_no_click = all_scores[-1]\n\n\nclass CascadeChoiceModel(NormalizableChoiceModel):\n  """"""The base class for cascade choice models.\n\n  Attributes:\n    attention_prob: The probability of examining a document i given document i -\n      1 not clicked.\n    score_scaling: A multiplicative factor to convert score of document i to the\n      click probability of examined document i.\n\n  Raises:\n    ValueError: if either attention_prob or base_attention_prob is invalid.\n  """"""\n\n  def __init__(self, choice_features):\n    self._attention_prob = choice_features.get(\'attention_prob\', 1.0)\n    self._score_scaling = choice_features.get(\'score_scaling\')\n    if self._attention_prob < 0.0 or self._attention_prob > 1.0:\n      raise ValueError(\'attention_prob must be in [0,1].\')\n    if self._score_scaling < 0.0:\n      raise ValueError(\'score_scaling must be positive.\')\n\n  def _positional_normalization(self, scores):\n    """"""Computes the click probability of each document in _scores.\n\n    The probability to click item i conditioned on unclicked item i - 1 is:\n      attention_prob * score_scaling * score(i)\n    We also compute the probability of not clicking any items in _score_no_click\n    Because they are already probabilities, the normlaization in choose_item\n    is no-op but we utilize random choice there.\n\n    Args:\n      scores: normalizable scores.\n    """"""\n    self._score_no_click = 1.0\n    for i in range(len(scores)):\n      s = self._score_scaling * scores[i]\n      assert s <= 1.0, (\'score_scaling cannot convert score %f into a \'\n                        \'probability\') % scores[i]\n      scores[i] = self._score_no_click * self._attention_prob * s\n      self._score_no_click *= (1.0 - self._attention_prob * s)\n    self._scores = scores\n\n\nclass ExponentialCascadeChoiceModel(CascadeChoiceModel):\n  """"""An exponential cascade choice model.\n\n  Clicks the item at position i according to\n    p(i) = attention_prob * score_scaling * exp(score(i))\n  by going through the slate in order, and stopping once an item has been\n  clicked.\n  """"""\n\n  def score_documents(self, user_state, doc_obs):\n    scores = self._score_documents_helper(user_state, doc_obs)\n    scores = np.exp(scores)\n    self._positional_normalization(scores)\n\n\nclass ProportionalCascadeChoiceModel(CascadeChoiceModel):\n  """"""A proportional cascade choice model.\n\n  Clicks the item at position i according to\n    attention_prob * score_scaling * (score(i) - min_normalizer)\n  by going through the slate in order, and stopping once an item has been\n  clicked.\n  """"""\n\n  def __init__(self, choice_features):\n    self._min_normalizer = choice_features.get(\'min_normalizer\')\n    super(ProportionalCascadeChoiceModel, self).__init__(choice_features)\n\n  def score_documents(self, user_state, doc_obs):\n    scores = self._score_documents_helper(user_state, doc_obs)\n    scores = scores - self._min_normalizer\n    assert not scores[\n        scores < 0.0], \'Normalized scores have non-positive elements.\'\n    self._positional_normalization(scores)\n'"
recsim/choice_model_test.py,4,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.choice_model.""""""\n\nimport numpy as np\nfrom recsim import choice_model\nfrom recsim.environments import interest_evolution as evolution\nimport tensorflow.compat.v1 as tf\n\n\nclass SoftmaxTest(tf.test.TestCase):\n\n  def test_softmax_single_int(self):\n    self.assertAllClose(choice_model.softmax([0]), [1.0])\n\n  def test_softmax_equal_ints(self):\n    self.assertAllClose(\n        choice_model.softmax(np.ones(4)), np.array([0.25, 0.25, 0.25, 0.25]))\n\n  def test_softmax_positive_floats(self):\n    self.assertAllClose(\n        choice_model.softmax(np.log(np.arange(1, 5))),\n        np.array([0.1, 0.2, 0.3, 0.4]))\n\n  def test_softmax_negative_floats(self):\n    self.assertAllClose(\n        choice_model.softmax(-1.0 * np.log(np.arange(1, 5))),\n        np.array([0.48, 0.24, 0.16, 0.12]))\n\n\nclass MultinomialChoiceModelTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(MultinomialChoiceModelTest, self).setUp()\n    np.random.seed(0)\n    self._user_state = evolution.IEvUserState(np.array([0.8, 0.6]))\n\n  def test_multinomial_logit_default(self):\n    mnl_model = choice_model.MultinomialLogitChoiceModel(choice_features={})\n    mnl_model.score_documents(\n        self._user_state, np.array([[0.8, 0.6], [0.6, 0.8]]))\n    # The logits for two documents are 1 and 0.96 respectively. When computing\n    # softmax logits, we subtract the largest value, which is 1 here. So the\n    # score is softmax([0, -0.04]) = [0.51, 0.49]\n    self.assertAlmostEqual(mnl_model._scores[0], 0.510, delta=0.001)\n    self.assertAlmostEqual(mnl_model._scores[1], 0.490, delta=0.001)\n    self.assertEqual(mnl_model._score_no_click, 0)\n\n  def test_multinomial_logit_no_click_mass(self):\n    choice_features = dict(no_click_mass=1.0)\n    mnl_model = choice_model.MultinomialLogitChoiceModel(\n        choice_features=choice_features)\n    # The logits for two documents are 1 and 0.96 respectively. No click mass\n    # is 1.0. When computing\n    # softmax logits, we subtract the largest value, which is 1 here. So the\n    # score is softmax([0, -0.04, 0]) = [0.337, 0.325, 0.338]\n    mnl_model.score_documents(\n        self._user_state, np.array([[0.8, 0.6], [0.6, 0.8]]))\n    self.assertAlmostEqual(mnl_model._scores[0], 0.338, delta=0.001)\n    self.assertAlmostEqual(mnl_model._scores[1], 0.325, delta=0.001)\n    self.assertAlmostEqual(mnl_model._score_no_click, 0.338, delta=0.001)\n\n  def test_multinomial_proportion_choice_model_default(self):\n    choice_features = dict(min_normalizer=0)\n    mnp_model = choice_model.MultinomialProportionalChoiceModel(\n        choice_features=choice_features)\n    mnp_model.score_documents(\n        self._user_state, np.array([[0.8, 0.6], [0.6, 0.8]]))\n    # The scores are the dot product between user features and doc features.\n    self.assertAlmostEqual(mnp_model._scores[0], 1, delta=0.001)\n    self.assertAlmostEqual(mnp_model._scores[1], 0.96, delta=0.001)\n    self.assertEqual(mnp_model._score_no_click, 0)\n\n  def test_multinomial_proportion_min_normalizer(self):\n    choice_features = dict(min_normalizer=0.5, no_click_mass=0.5)\n    mnp_model = choice_model.MultinomialProportionalChoiceModel(\n        choice_features=choice_features)\n    mnp_model.score_documents(\n        self._user_state, np.array([[0.8, 0.6], [0.6, 0.8]]))\n    # The scores the dot product user features and doc features minus the\n    # min_normalizer.\n    self.assertAlmostEqual(mnp_model._scores[0], 0.5, delta=0.001)\n    self.assertAlmostEqual(mnp_model._scores[1], 0.46, delta=0.001)\n    self.assertAlmostEqual(mnp_model._score_no_click, 0, delta=0.001)\n\n\nclass CascadeChoiceModelTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(CascadeChoiceModelTest, self).setUp()\n    np.random.seed(0)\n    self._user_state = evolution.IEvUserState(np.array([1.0]))\n\n  def test_exponential_cascade_invalid_score_scaling(self):\n    with self.assertRaises(ValueError):\n      choice_features = {\'attention_prob\': 0.8, \'score_scaling\': -1.0}\n      choice_model.ExponentialCascadeChoiceModel(choice_features)\n\n  def test_exponential_cascade_invalid_attenion_prob(self):\n    with self.assertRaises(ValueError):\n      choice_features = {\'attention_prob\': 2.0}\n      choice_model.ExponentialCascadeChoiceModel(choice_features)\n\n  def test_exponential_cascade(self):\n    choice_features = {\'score_scaling\': 0.04}\n    model = choice_model.ExponentialCascadeChoiceModel(choice_features)\n    model.score_documents(self._user_state, np.array([[3.0], [2.0], [1.0]]))\n    self.assertEqual(model.choose_item(), 0)\n\n  def test_exponential_cascade_with_no_click(self):\n    choice_features = {\'attention_prob\': 1.0, \'score_scaling\': 0.0}\n    model = choice_model.ExponentialCascadeChoiceModel(choice_features)\n    model.score_documents(self._user_state, np.array([[3.0], [2.0], [1.0]]))\n    self.assertEqual(model.choose_item(), None)\n\n  def test_proportional_cascade_invalid_attenion_prob(self):\n    with self.assertRaises(ValueError):\n      choice_features = {\n          \'attention_prob\': 2.0,\n          \'min_normalizer\': -2.0,\n          \'score_scaling\': 0.1\n      }\n      choice_model.ProportionalCascadeChoiceModel(choice_features)\n\n  def test_proportional_cascade_invalid_score_scaling(self):\n    with self.assertRaises(ValueError):\n      choice_features = {\n          \'attention_prob\': 0.5,\n          \'min_normalizer\': -2.0,\n          \'score_scaling\': -1.0\n      }\n      choice_model.ProportionalCascadeChoiceModel(choice_features)\n\n  def test_proportional_cascade(self):\n    choice_features = {\n        \'attention_prob\': 1.0,\n        \'min_normalizer\': -4.0,\n        \'score_scaling\': 0.07\n    }\n    model = choice_model.ProportionalCascadeChoiceModel(choice_features)\n    model.score_documents(self._user_state,\n                          np.array([[-3.0], [-3.0], [10.0], [1.0], [-4.0]]))\n    self.assertEqual(model.choose_item(), 2)\n\n  def test_proportional_cascade_with_no_click(self):\n    choice_features = {\n        \'attention_prob\': 0.5,\n        \'min_normalizer\': -1.0,\n        \'score_scaling\': 0.1\n    }\n    model = choice_model.ProportionalCascadeChoiceModel(choice_features)\n    model.score_documents(self._user_state, np.array([[0.0], [0.0], [0.0]]))\n    self.assertEqual(model.choose_item(), None)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
recsim/document.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Classes to represent and interface with documents.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nfrom gym import spaces\nimport numpy as np\nimport six\n\n# Some notes:\n#\n#   These represent properties of the documents. Presumably we can add more or\n#   remove documents from the candidate set. But these do not include features\n#   that depend on both the user and the document.\n#\n\n\nclass CandidateSet(object):\n  """"""Class to represent a collection of AbstractDocuments.\n\n     The candidate set is represented as a hashmap (dictionary), with documents\n     indexed by their document ID.\n  """"""\n\n  def __init__(self):\n    """"""Initializes a document candidate set with 0 documents.""""""\n    self._documents = {}\n\n  def size(self):\n    """"""Returns an integer, the number of documents in this candidate set.""""""\n    return len(self._documents)\n\n  def get_all_documents(self):\n    """"""Returns all documents.""""""\n    return self.get_documents(self._documents.keys())\n\n  def get_documents(self, document_ids):\n    """"""Gets the documents associated with the specified document IDs.\n\n    Args:\n      document_ids: an array representing indices into the candidate set.\n        Indices can be integers or string-encoded integers.\n\n    Returns:\n      (documents) an ordered list of AbstractDocuments associated with the\n        document ids.\n    """"""\n    return [self._documents[int(k)] for k in document_ids]\n\n  def add_document(self, document):\n    """"""Adds a document to the candidate set.""""""\n    self._documents[document.doc_id()] = document\n\n  def remove_document(self, document):\n    """"""Removes a document from the set (to simulate a changing corpus).""""""\n    del self._documents[document.doc_id()]\n\n  def create_observation(self):\n    """"""Returns a dictionary of observable features of documents.""""""\n    return {\n        str(k): self._documents[k].create_observation()\n        for k in self._documents.keys()\n    }\n\n  def observation_space(self):\n    return spaces.Dict({\n        str(k): self._documents[k].observation_space()\n        for k in self._documents.keys()\n    })\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass AbstractDocumentSampler(object):\n  """"""Abstract class to sample documents.""""""\n\n  def __init__(self, doc_ctor, seed=0):\n    self._doc_ctor = doc_ctor\n    self._seed = seed\n    self.reset_sampler()\n\n  def reset_sampler(self):\n    self._rng = np.random.RandomState(self._seed)\n\n  @abc.abstractmethod\n  def sample_document(self):\n    """"""Samples and return an instantiation of AbstractDocument.""""""\n\n  def get_doc_ctor(self):\n    """"""Returns the constructor/class of the documents that will be sampled.""""""\n    return self._doc_ctor\n\n  @property\n  def num_clusters(self):\n    """"""Returns the number of document clusters. Returns 0 if not applicable.""""""\n    return 0\n\n  def update_state(self, documents, responses):\n    """"""Update document state (if needed) given user\'s (or users\') responses.""""""\n    pass\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass AbstractDocument(object):\n  """"""Abstract class to represent a document and its properties.""""""\n\n  # Number of features to represent the document.\n  NUM_FEATURES = None\n\n  def __init__(self, doc_id):\n    self._doc_id = doc_id  # Unique identifier for the document\n\n  def doc_id(self):\n    """"""Returns the document ID.""""""\n    return self._doc_id\n\n  @abc.abstractmethod\n  def create_observation(self):\n    """"""Returns observable properties of this document as a float array.""""""\n\n  @classmethod\n  @abc.abstractmethod\n  def observation_space(cls):\n    """"""Gym space that defines how documents are represented.""""""\n'"
recsim/main.py,1,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nr""""""An example of main function for training in RecSim.\n\nUse the interest evolution environment and a slateQ agent for illustration.\n\nTo run locally:\n\npython main.py --base_dir=/tmp/interest_evolution \\\n  --gin_bindings=simulator.runner_lib.Runner.max_steps_per_episode=50\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\nfrom absl import flags\nimport numpy as np\nfrom recsim.agents import full_slate_q_agent\nfrom recsim.environments import interest_evolution\nfrom recsim.simulator import runner_lib\n\n\nFLAGS = flags.FLAGS\n\n\ndef create_agent(sess, environment, eval_mode, summary_writer=None):\n  """"""Creates an instance of FullSlateQAgent.\n\n  Args:\n    sess: A `tf.Session` object for running associated ops.\n    environment: A recsim Gym environment.\n    eval_mode: A bool for whether the agent is in training or evaluation mode.\n    summary_writer: A Tensorflow summary writer to pass to the agent for\n      in-agent training statistics in Tensorboard.\n\n  Returns:\n    An instance of FullSlateQAgent.\n  """"""\n  kwargs = {\n      \'observation_space\': environment.observation_space,\n      \'action_space\': environment.action_space,\n      \'summary_writer\': summary_writer,\n      \'eval_mode\': eval_mode,\n  }\n  return full_slate_q_agent.FullSlateQAgent(sess, **kwargs)\n\n\ndef main(argv):\n  if len(argv) > 1:\n    raise app.UsageError(\'Too many command-line arguments.\')\n\n  runner_lib.load_gin_configs(FLAGS.gin_files, FLAGS.gin_bindings)\n  seed = 0\n  slate_size = 2\n  np.random.seed(seed)\n  env_config = {\n      \'num_candidates\': 5,\n      \'slate_size\': slate_size,\n      \'resample_documents\': True,\n      \'seed\': seed,\n  }\n\n  runner = runner_lib.TrainRunner(\n      base_dir=FLAGS.base_dir,\n      create_agent_fn=create_agent,\n      env=interest_evolution.create_environment(env_config),\n      episode_log_file=FLAGS.episode_log_file,\n      max_training_steps=50,\n      num_iterations=10)\n  runner.run_experiment()\n\n  runner = runner_lib.EvalRunner(\n      base_dir=FLAGS.base_dir,\n      create_agent_fn=create_agent,\n      env=interest_evolution.create_environment(env_config),\n      max_eval_episodes=5,\n      test_mode=True)\n  runner.run_experiment()\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'base_dir\')\n  app.run(main)\n'"
recsim/user.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Abstract classes that encode a user\'s state and dynamics.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nfrom gym import spaces\nimport numpy as np\nimport six\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass AbstractResponse(object):\n  """"""Abstract class to model a user response.""""""\n\n  @staticmethod\n  @abc.abstractmethod\n  def response_space():\n    """"""ArraySpec that defines how a single response is represented.""""""\n\n  @abc.abstractmethod\n  def create_observation(self):\n    """"""Creates a tensor observation of this response.""""""\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass AbstractUserState(object):\n  """"""Abstract class to represent a user\'s state.""""""\n\n  # Number of features to represent the user\'s interests.\n  NUM_FEATURES = None\n\n  @abc.abstractmethod\n  def create_observation(self):\n    """"""Generates obs of underlying state to simulate partial observability.\n\n    Returns:\n      obs: A float array of the observed user features.\n    """"""\n\n  @staticmethod\n  @abc.abstractmethod\n  def observation_space():\n    """"""Gym.spaces object that defines how user states are represented.""""""\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass AbstractUserSampler(object):\n  """"""Abstract class to sample users.""""""\n\n  def __init__(self, user_ctor, seed=0):\n    """"""Creates a new user state sampler.\n\n    User states of the type user_ctor are sampled.\n\n    Args:\n      user_ctor: A class/constructor for the type of user states that will be\n        sampled.\n      seed: An integer for a random seed.\n    """"""\n    self._user_ctor = user_ctor\n    self._seed = seed\n    self.reset_sampler()\n\n  def reset_sampler(self):\n    self._rng = np.random.RandomState(self._seed)\n\n  @abc.abstractmethod\n  def sample_user(self):\n    """"""Creates a new instantiation of this user\'s hidden state parameters.""""""\n\n  def get_user_ctor(self):\n    """"""Returns the constructor/class of the user states that will be sampled.""""""\n    return self._user_ctor\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass AbstractUserModel(object):\n  """"""Abstract class to represent an encoding of a user\'s dynamics.""""""\n\n  def __init__(self, response_model_ctor, user_sampler, slate_size):\n    """"""Initializes a new user model.\n\n    Args:\n      response_model_ctor: A class/constructor representing the type of\n        responses this model will generate.\n      user_sampler: An instance of AbstractUserSampler that can generate\n        initial user states from an inital state distribution.\n      slate_size: integer number of documents that can be served to the user at\n        any interaction.\n    """"""\n    if not response_model_ctor:\n      raise TypeError(\'response_model_ctor is a required callable\')\n\n    self._user_sampler = user_sampler\n    self._user_state = self._user_sampler.sample_user()\n    self._response_model_ctor = response_model_ctor\n    self._slate_size = slate_size\n\n  ## Transition model\n  @abc.abstractmethod\n  def update_state(self, slate_documents, responses):\n    """"""Updates the user\'s state based on the slate and document selected.\n\n    Args:\n      slate_documents: A list of AbstractDocuments for items in the slate.\n      responses: A list of AbstractResponses for each item in the slate.\n    Updates: The user\'s hidden state.\n    """"""\n\n  def reset(self):\n    """"""Resets the user.""""""\n    self._user_state = self._user_sampler.sample_user()\n\n  def reset_sampler(self):\n    """"""Resets the sampler.""""""\n    self._user_sampler.reset_sampler()\n\n  @abc.abstractmethod\n  def is_terminal(self):\n    """"""Returns a boolean indicating whether this session is over.""""""\n\n  ## Choice model\n  @abc.abstractmethod\n  def simulate_response(self, documents):\n    """"""Simulates the user\'s response to a slate of documents.\n\n    This could involve simulating models of attention, as well as random\n    sampling for selection from scored documents.\n\n    Args:\n      documents: a list of AbstractDocuments\n\n    Returns:\n      (response) a list of AbstractResponse objects for each slate item\n    """"""\n\n  def response_space(self):\n    res_space = self._response_model_ctor.response_space()\n    return spaces.Tuple(tuple([\n        res_space,\n    ] * self._slate_size))\n\n  def get_response_model_ctor(self):\n    """"""Returns a constructor for the type of response this model will create.""""""\n    return self._response_model_ctor\n\n  def observation_space(self):\n    """"""A Gym.spaces object that describes possible user observations.""""""\n    return self._user_state.observation_space()\n\n  def create_observation(self):\n    """"""Emits obesrvation about user\'s state.""""""\n    return self._user_state.create_observation()\n'"
recsim/utils.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utility functions for RecSim environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\ndef aggregate_video_cluster_metrics(responses, metrics, info=None):\n  """"""Aggregates the video cluster metrics with one step responses.\n\n  Args:\n    responses: a dictionary of names, observed responses.\n    metrics: A dictionary mapping from metric_name to its value in float.\n    info: Additional info for computing metrics (ignored here)\n\n  Returns:\n    A dictionary storing metrics after aggregation.\n  """"""\n  del info  # Unused.\n  is_clicked = False\n  metrics[\'impression\'] += 1\n\n  for response in responses:\n    if not response[\'click\']:\n      continue\n    is_clicked = True\n    metrics[\'click\'] += 1\n    metrics[\'quality\'] += response[\'quality\']\n    cluster_id = response[\'cluster_id\']\n    metrics[\'cluster_watch_count_cluster_%d\' % cluster_id] += 1\n\n  if not is_clicked:\n    metrics[\'cluster_watch_count_no_click\'] += 1\n  return metrics\n\n\ndef write_video_cluster_metrics(metrics, add_summary_fn):\n  """"""Writes average video cluster metrics using add_summary_fn.""""""\n  add_summary_fn(\'CTR\', metrics[\'click\'] / metrics[\'impression\'])\n  if metrics[\'click\'] > 0:\n    add_summary_fn(\'AverageQuality\', metrics[\'quality\'] / metrics[\'click\'])\n  for k in metrics:\n    prefix = \'cluster_watch_count_cluster_\'\n    if k.startswith(prefix):\n      add_summary_fn(\'cluster_watch_count_frac/cluster_%s\' % k[len(prefix):],\n                     metrics[k] / metrics[\'impression\'])\n  add_summary_fn(\n      \'cluster_watch_count_frac/no_click\',\n      metrics[\'cluster_watch_count_no_click\'] / metrics[\'impression\'])\n'"
recsim/utils_test.py,2,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.utils.""""""\n\nimport collections\nimport numpy as np\nfrom recsim import utils\nimport tensorflow.compat.v1 as tf\n\n\nclass UtilsTest(tf.test.TestCase):\n\n  def test_aggregate_video_cluster_metrics(self):\n    metrics = collections.defaultdict(float)\n    metrics[\'impression\'] = 10\n    metrics[\'cluster_watch_count_cluster_0\'] = 1\n    metrics[\'cluster_watch_count_no_click\'] = 9\n    metrics[\'quality\'] = 0.7\n    metrics[\'click\'] = 1\n    responses = ({\n        \'click\': 1,\n        \'quality\': np.array(0.5),\n        \'cluster_id\': np.array(1)\n    }, {\n        \'click\': 0,\n        \'quality\': np.array(0.8),\n        \'cluster_id\': np.array(2)\n    })\n    metrics = utils.aggregate_video_cluster_metrics(responses, metrics)\n    self.assertEqual(\n        metrics, {\n            \'impression\': 11.0,\n            \'cluster_watch_count_cluster_0\': 1.0,\n            \'cluster_watch_count_cluster_1\': 1.0,\n            \'cluster_watch_count_no_click\': 9.0,\n            \'quality\': 1.2,\n            \'click\': 2.0\n        })\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
recsim/agents/__init__.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Module importing all agents.""""""\nfrom recsim.agents import agent_utils\nfrom recsim.agents import cluster_bandit_agent\nfrom recsim.agents import full_slate_q_agent\nfrom recsim.agents import greedy_pctr_agent\nfrom recsim.agents import random_agent\nfrom recsim.agents import slate_decomp_q_agent\nfrom recsim.agents import tabular_q_agent\n'"
recsim/agents/agent_utils.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Convenience primitives relating to the implementation of agents.""""""\nfrom gym import spaces\nimport numpy as np\n\n\nclass GymSpaceWalker(object):\n  """"""Class for recursively applying a given function to a gym space.\n\n  Gym spaces have nested structure in terms of container spaces (e.g. Dict and\n  Tuple) containing basic spaces such as Discrete and Box. This class consumes a\n  gym observation definition and a leaf operator is used to produce a flat list\n  of the contents of the gym space, apply the leaf operator to all basic spaces\n  in the proces. E.g., given a gym space of the form Tuple((Box(1), Box(1)) and\n  a leaf operator f, this class can is used to transform an observation (a, b)\n  to [f(a), f(b)].\n\n  Args:\n  gym_space: An instance of an OpenAI Gym space.\n  leaf_op: A function taking as arguments an OpenAI Gym space and an observation\n    conforming to that space. There are no requirements on its output.\n  """"""\n\n  def __init__(self, gym_space, leaf_op):\n    self._gym_space = gym_space\n    self._leaf_op = leaf_op\n\n  def apply_and_flatten(self, gym_observations):\n    return self._descend_and_flatten(self._gym_space, gym_observations)\n\n  def _descend_and_flatten(self, gym_space, gym_observations):\n    """"""Recursive implementation of flattening and leaf op application.\n\n    Args:\n      gym_space: An instance of an OpenAI Gym space.\n      gym_observations: A list of observation conforming to the format\n        of gym_space.\n\n    Returns:\n      flattened_apply: a list of the applications of leaf_op to the leaves of\n        the gym space, as encountered in post-order traversal.\n    """"""\n    if isinstance(gym_space, spaces.dict.Dict):\n      flattened_apply = []\n      for key, space in gym_space.spaces.items():\n        flattened_apply += self._descend_and_flatten(\n            space,\n            [gym_observation[key] for gym_observation in gym_observations])\n    elif isinstance(gym_space, spaces.tuple.Tuple):\n      flattened_apply = []\n      for i, space in enumerate(gym_space.spaces):\n        flattened_apply += self._descend_and_flatten(\n            space, [gym_observation[i] for gym_observation in gym_observations])\n    elif isinstance(gym_space, spaces.box.Box) or isinstance(\n        gym_space, spaces.discrete.Discrete):\n      return self._leaf_op(gym_space, gym_observations)\n    else:\n      raise NotImplementedError(\'Gym space type \' + str(type(gym_space)) +\n                                \' not implemented yet.\')\n    return flattened_apply\n\n\ndef epsilon_greedy_exploration(state_action_iterator, q_function, epsilon):\n  """"""Epsilon greedy exploration.\n\n  Either picks a slate uniformly at random with probability epsilon, or returns\n  a slate with maximal Q-value. TODO(mmladenov): more verbose doc.\n  Args:\n    state_action_iterator: an iterator over slate, state_action_index tuples.\n    q_function: a container holding Q-values of state-action pairs.\n    epsilon: probability of random action.\n  Returns:\n    slate: the picked slate.\n    sa_index: the index of the picked slate in the Q-value table.\n  """"""\n  max_q_next = -np.Inf\n  max_state_action_index = None\n  max_slate = []\n  random_state_action_index = None\n  random_slate = []\n  sa_index = None\n  for slate_count, (slate, state_action_index) in enumerate(\n      state_action_iterator, start=1):\n    q_value = q_function.get(state_action_index, 0)\n    if q_value > max_q_next:\n      max_q_next = q_value\n      max_state_action_index = state_action_index\n      max_slate = slate\n    # Pick a random action by reservoir sampling in order to avoid materializing\n    # all possible slates.\n    if slate_count == 1 or np.random.random() < 1.0 / (1.0 * slate_count):\n      random_state_action_index = state_action_index\n      random_slate = slate\n  if np.random.random() <= epsilon:\n    slate = random_slate\n    sa_index = random_state_action_index\n  else:\n    slate = max_slate\n    sa_index = max_state_action_index\n  return slate, sa_index\n\n\ndef min_count_exploration(state_action_iterator, counts_function):\n  """"""Minimum count exploration.\n\n  Picks the state-action pair with minimum counts.\n  Args:\n    state_action_iterator: an iterator over slate, state_action_index tuples.\n    counts_function: a container holding the number of times a state-action pair\n      has been executed so far.\n  Returns:\n    slate: the picked slate.\n    sa_index: the index of the picked slate in the counts table.\n  """"""\n  min_sa_count = np.Inf\n  min_sa_count_slate = []\n  min_sa_count_index = None\n  for slate, state_action_index in state_action_iterator:\n    sa_count = counts_function.get(state_action_index, 0)\n    if sa_count < min_sa_count:\n      min_sa_count_slate = slate\n      min_sa_count_index = state_action_index\n      min_sa_count = sa_count\n    if sa_count == 0:\n      break\n  return min_sa_count_slate, min_sa_count_index\n'"
recsim/agents/cluster_bandit_agent.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Agent that picks topics based on the UCB1 algorithm given past responses.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nimport gin\nimport numpy as np\nfrom recsim import agent\nfrom recsim.agents.bandits import algorithms\nfrom recsim.agents.layers import abstract_click_bandit\n\n\n@gin.configurable\nclass ClusterBanditAgent(abstract_click_bandit.AbstractClickBanditLayer):\n  """"""An agent that recommends items with the highest UCBs of topic affinities.\n\n  This agent assumes no knowledge of user\'s affinity for each topic but receives\n  observations of user\'s past responses for each topic. When creating a slate,\n  it utilizes a bandit algorithm to pick the best topics. Within the same best\n  topic, we pick documents with the best document quality scores.\n  """"""\n\n  def __init__(self,\n               observation_space,\n               action_space,\n               alg_ctor=algorithms.UCB1,\n               ci_scaling=1.0,\n               random_seed=0,\n               **kwargs):\n    """"""Initializes a new bandit agent for clustered arm exploration.\n\n    Args:\n      observation_space: Instance of a gym space corresponding to the\n        observation format.\n      action_space: A gym.spaces object that specifies the format of actions.\n      alg_ctor: A class of an MABAlgorithm for exploration, default to UCB1.\n      ci_scaling: A floating number specifying the scaling of confidence bound.\n      random_seed: An integer for random seed.\n      **kwargs: currently unused arguments.\n    """"""\n    num_topics = list(observation_space.spaces[\'doc\'].spaces.values()\n                     )[0].spaces[\'cluster_id\'].n\n    base_agent_ctors = [\n        functools.partial(GreedyClusterAgent, cluster_id=i)\n        for i in range(num_topics)\n    ]\n    super(ClusterBanditAgent, self).__init__(\n        observation_space,\n        action_space,\n        base_agent_ctors,\n        alg_ctor=alg_ctor,\n        ci_scaling=ci_scaling,\n        random_seed=random_seed,\n        **kwargs)\n\n\nclass GreedyClusterAgent(agent.AbstractEpisodicRecommenderAgent):\n  """"""Simple agent sorting all documents of a topic according to quality.""""""\n\n  def __init__(self, observation_space, action_space, cluster_id, **kwargs):\n    del observation_space\n    super(GreedyClusterAgent, self).__init__(action_space)\n    self._cluster_id = cluster_id\n\n  def step(self, reward, observation):\n    del reward\n    my_docs = []\n    my_doc_quality = []\n    for i, doc in enumerate(observation[\'doc\'].values()):\n      if doc[\'cluster_id\'] == self._cluster_id:\n        my_docs.append(i)\n        my_doc_quality.append(doc[\'quality\'])\n    if not bool(my_docs):\n      return []\n    sorted_indices = np.argsort(my_doc_quality)[::-1]\n    return list(np.array(my_docs)[sorted_indices])\n'"
recsim/agents/cluster_bandit_agent_test.py,2,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.agents.cluster_bandit_agent.""""""\n\nfrom gym import spaces\nimport numpy as np\nfrom recsim.agents import cluster_bandit_agent\nfrom recsim.environments import interest_exploration as ie\nimport tensorflow.compat.v1 as tf\n\n\nclass ClusterBanditAgentTest(tf.test.TestCase):\n\n  def dummy_observation_space(self):\n    single_response_space = spaces.Dict({\n        \'cluster_id\': spaces.Discrete(2),\n        \'click\': spaces.Discrete(2)\n    })\n    doc_space = spaces.Dict(\n        {0: spaces.Dict({\'cluster_id\': spaces.Discrete(2)})})\n    user_space = spaces.Dict({\n        \'sufficient_statistics\':\n            spaces.Dict({\n                \'impression_count\':\n                    spaces.Box(np.array([0] * 2), np.array([np.inf] * 2)),\n                \'click_count\':\n                    spaces.Box(np.array([0] * 2), np.array([np.inf] * 2))\n            })\n    })\n    return spaces.Dict({\n        \'user\': user_space,\n        \'doc\': doc_space,\n        \'response\': spaces.Tuple([\n            single_response_space,\n        ])\n    })\n\n  def doc_user_to_sufficient_stats(self, docs, observation):\n    sufficient_stats_observation = {\'user\': {\'sufficient_statistics\': {}}}\n    sufficient_stats_observation[\'user\'][\'sufficient_statistics\'][\n        \'impression_count\'] = observation[:2]\n    sufficient_stats_observation[\'user\'][\'sufficient_statistics\'][\n        \'click_count\'] = observation[2:]\n    sufficient_stats_observation[\'doc\'] = docs\n    return sufficient_stats_observation\n\n  def test_step_with_bigger_slate(self):\n    # Initialize agent.\n    slate_size = 5\n    num_candidates = 5\n    action_space = spaces.MultiDiscrete(num_candidates * np.ones((slate_size,)))\n    agent = cluster_bandit_agent.ClusterBanditAgent(\n        self.dummy_observation_space(), action_space)\n\n    # Create a set of documents\n    document_sampler = ie.IETopicDocumentSampler(seed=1)\n    documents = {}\n    for i in range(num_candidates):\n      video = document_sampler.sample_document()\n      documents[i] = video.create_observation()\n\n    # Past observation shows Topic 1 is better.\n    user_obs = np.array([1, 1, 0, 1])\n    sufficient_stats_observation = self.doc_user_to_sufficient_stats(\n        documents, user_obs)\n    slate = agent.step(0, sufficient_stats_observation)\n    # Documents in Topic 0 sorted by quality: 1, 2.\n    # Documents in Topic 1 sorted by quality: 0, 4, 3.\n    self.assertAllEqual(slate, [0, 4, 3, 1, 2])\n\n  def test_bundle_and_unbundle_trivial(self):\n    action_space = spaces.MultiDiscrete(2 * np.ones((2,)))\n    agent = cluster_bandit_agent.ClusterBanditAgent(\n        self.dummy_observation_space(), action_space)\n    self.assertFalse(agent.unbundle(\'\', 0, {}))\n    self.assertEqual(\n        {\n            \'base_agent_bundle_0\': {\n                \'episode_num\': 0\n            },\n            \'base_agent_bundle_1\': {\n                \'episode_num\': 0\n            }\n        }, agent.bundle_and_checkpoint(\'\', 0))\n\n  def test_bundle_and_unbundle(self):\n    # Initialize agent\n    slate_size = 2\n    num_candidates = 5\n    action_space = spaces.MultiDiscrete(num_candidates * np.ones((slate_size,)))\n\n    agent = cluster_bandit_agent.ClusterBanditAgent(\n        self.dummy_observation_space(), action_space)\n\n    # Create a set of documents\n    document_sampler = ie.IETopicDocumentSampler()\n    documents = {}\n    for i in range(num_candidates):\n      video = document_sampler.sample_document()\n      documents[i] = video.create_observation()\n\n    # Test that slate indices in correct range and length is correct\n    sufficient_stats_observation = self.doc_user_to_sufficient_stats(\n        documents, np.array([0, 0, 0, 0]))\n\n    agent.step(1, sufficient_stats_observation)\n\n    bundle_dict = agent.bundle_and_checkpoint(\'\', 0)\n    self.assertTrue(agent.unbundle(\'\', 0, bundle_dict))\n    self.assertEqual(bundle_dict, agent.bundle_and_checkpoint(\'\', 0))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
recsim/agents/full_slate_q_agent.py,7,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Agent that implements the Slate-Q algorithms.""""""\n\nimport itertools\nimport gin.tf\nfrom gym import spaces\nfrom recsim import agent as abstract_agent\nfrom recsim.agents.dopamine import dqn_agent\nimport tensorflow.compat.v1 as tf\n\n\n@gin.configurable\nclass FullSlateQAgent(dqn_agent.DQNAgentRecSim,\n                      abstract_agent.AbstractEpisodicRecommenderAgent):\n  """"""A recommender agent implements full slate Q-learning based on DQN agent.\n\n  This is a standard, nondecomposed Q-learning method that treats each slate\n  atomically (i.e., holistically) as a single action.\n  """"""\n\n  def __init__(self,\n               sess,\n               observation_space,\n               action_space,\n               optimizer_name=\'\',\n               eval_mode=False,\n               **kwargs):\n    """"""Initializes a FullSlateQAgent.\n\n    Args:\n      sess: a Tensorflow session.\n      observation_space: A gym.spaces object that specifies the format of\n        observations.\n      action_space: A gym.spaces object that specifies the format of actions.\n      optimizer_name: The name of the optimizer.\n      eval_mode: A bool for whether the agent is in training or evaluation mode.\n      **kwargs: Keyword arguments to the DQNAgent.\n    """"""\n    self._num_candidates = int(action_space.nvec[0])\n    abstract_agent.AbstractEpisodicRecommenderAgent.__init__(self, action_space)\n    # Each slate is a single action. Assume ordering of items matters.\n    self._all_possible_slates = [\n        x for x in itertools.permutations(\n            range(self._num_candidates), action_space.nvec.shape[0])\n    ]\n    num_actions = len(self._all_possible_slates)\n    self._env_action_space = spaces.Discrete(num_actions)\n\n    dqn_agent.DQNAgentRecSim.__init__(\n        self,\n        sess,\n        observation_space,\n        num_actions=num_actions,\n        stack_size=1,\n        optimizer_name=\'\',\n        eval_mode=eval_mode,\n        **kwargs)\n\n  # Builds a tower to compute Q-value for each possible slate.\n  def _network_adapter(self, states, scope):\n    self._validate_states(states)\n\n    with tf.name_scope(\'network\'):\n      q_value_list = []\n      for slate in self._all_possible_slates:\n        user = tf.squeeze(states[:, 0, :, :], axis=2)\n        docs = []\n        for i in slate:\n          docs.append(tf.squeeze(states[:, i + 1, :, :], axis=2))\n        q_value_list.append(self.network(user, tf.concat(docs, axis=1), scope))\n      q_values = tf.concat(q_value_list, axis=1)\n\n    return dqn_agent.DQNNetworkType(q_values)\n\n  def _build_networks(self):\n    with tf.name_scope(\'networks\'):\n      self._replay_net_outputs = self._network_adapter(self._replay.states,\n                                                       \'Online\')\n      self._replay_next_target_net_outputs = self._network_adapter(\n          self._replay.states, \'Target\')\n      self._net_outputs = self._network_adapter(self.state_ph, \'Online\')\n      self._q_argmax = tf.argmax(input=self._net_outputs.q_values, axis=1)[0]\n\n  def step(self, reward, observation):\n    """"""Receives observations of environment and returns a slate.\n\n    Args:\n      reward: A double representing the overall reward to the recommended slate.\n      observation: A dictionary that stores all the observations including:\n        - user: A list of floats representing the user\'s observed state\n        - doc: A list of observations of document features\n        - response: A vector valued response signal that represent user\'s\n          response to each document\n\n    Returns:\n      slate: An integer array of size _slate_size, where each element is an\n        index in the list of document observvations.\n    """"""\n    return self._all_possible_slates[super(FullSlateQAgent, self).step(\n        reward, self._obs_adapter.encode(observation))]\n\n  def _build_replay_buffer(self, use_staging):\n    """"""Creates the replay buffer used by the agent.\n\n    Args:\n      use_staging: bool, if True, uses a staging area to prefetch data for\n        faster training.\n\n    Returns:\n      A WrapperReplayBuffer object.\n    """"""\n    return dqn_agent.wrapped_replay_buffer(\n        observation_shape=self.observation_shape,\n        stack_size=self.stack_size,\n        use_staging=use_staging,\n        update_horizon=self.update_horizon,\n        gamma=self.gamma,\n        observation_dtype=self.observation_dtype)\n\n  def begin_episode(self, observation):\n    """"""Returns the agent\'s first action for this episode.\n\n    Args:\n      observation: numpy array, the environment\'s initial observation.\n\n    Returns:\n      An integer array of size _slate_size, the selected slated, each\n      element of which is an index in the list of doc_obs.\n    """"""\n    return self._all_possible_slates[super(FullSlateQAgent, self).begin_episode(\n        self._obs_adapter.encode(observation))]\n\n  def end_episode(self, reward, observation):\n    """"""Signals the end of the episode to the agent.\n\n    We store the observation of the current time step, which is the last\n    observation of the episode.\n\n    Args:\n      reward: float, the last reward from the environment.\n      observation: numpy array, the environment\'s initial observation.\n    """"""\n    super(FullSlateQAgent, self).end_episode(reward)\n'"
recsim/agents/greedy_pctr_agent.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Agent that picks items with highest pCTR given the true user choice model.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nfrom absl import logging\n\nimport numpy as np\n\nfrom recsim import agent\nfrom recsim import choice_model as cm\n\n\nclass GreedyPCTRAgent(agent.AbstractEpisodicRecommenderAgent):\n  """"""An agent that recommends slates with the highest pCTR items.\n\n  This agent assumes knowledge of the true underlying choice model. Note that\n  this implicitly means it receives observations of the true user and document\n  states. This agent myopically creates slates with items that have the highest\n  probability of being clicked under the given choice model.\n  """"""\n\n  def __init__(self,\n               action_space,\n               belief_state,\n               choice_model=cm.MultinomialLogitChoiceModel({\'no_click_mass\': 5\n                                                           })):\n    """"""Initializes a new greedy pCTR agent.\n\n    Args:\n      action_space: A gym.spaces object that specifies the format of actions\n      belief_state: An instantiation of AbstractUserState assumed by the agent\n      choice_model: An instantiation of AbstractChoiceModel assumed by the agent\n        Default to a multinomial logit choice model with no_click_mass = 5.\n    """"""\n\n    super(GreedyPCTRAgent, self).__init__(action_space)\n    self._choice_model = choice_model\n    self._belief_state = belief_state\n\n  def step(self, reward, observation):\n    """"""Records the most recent transition and returns the agent\'s next action.\n\n    We store the observation of the last time step since we want to store it\n    with the reward.\n\n    Args:\n      reward: Unused.\n      observation: A dictionary that includes the most recent observations and\n        should have the following fields:\n        - user: A list of floats representing the user\'s observed state\n        - doc: A list of observations of document features\n\n    Returns:\n      slate: An integer array of size _slate_size, where each element is an\n        index into the list of doc_obs\n    """"""\n    del reward  # Unused argument.\n    doc_obs = observation[\'doc\']\n\n    # Score the documents without knowing the latent user state.\n    self._choice_model.score_documents(self._belief_state, doc_obs.values())\n\n    # Find the indices of the top scoring documents\n    slate = self.findBestDocuments(self._choice_model.scores)\n\n    # Return the slate of those documents\n    logging.debug(\'Recommended slate: %s\', slate)\n    return slate\n\n  def findBestDocuments(self, scores):\n    """"""Returns the indices of the highest scores in sorted order.\n\n    Args:\n      scores: A list of floats representing unnormalized document scores\n\n    Returns:\n      sorted_indices: A list of integers indexing the highest scores, in sorted\n      order\n    """"""\n    # Chose the k = slate_size best ones\n    scores = np.array(scores)\n    indices = np.argpartition(scores, -self._slate_size)[-self._slate_size:]\n\n    # Sort them so the best appear first\n    sorted_indices = indices[np.argsort(-scores[indices])]\n    return sorted_indices\n'"
recsim/agents/greedy_pctr_agent_test.py,2,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.agents.greedy_pctr_agent.""""""\n\nfrom gym import spaces\nimport numpy as np\n\nfrom recsim.agents import greedy_pctr_agent\nfrom recsim.environments import interest_exploration as ie\nfrom recsim.simulator import environment\nimport tensorflow.compat.v1 as tf\n\n\nclass GreedyPCTRAgentTest(tf.test.TestCase):\n\n  def test_find_best_documents(self):\n    action_space = spaces.MultiDiscrete(4 * np.ones((4,)))\n    agent = greedy_pctr_agent.GreedyPCTRAgent(action_space, None)\n    scores = [-1, -2, 4.32, 0, 15, -6, 4.32]\n    indices = agent.findBestDocuments(scores)\n    self.assertAllEqual(indices, [4, 2, 6, 3])\n\n  def test_step(self):\n    # Create a simple user\n    slate_size = 2\n    num_candidates = 5\n    action_space = spaces.MultiDiscrete(num_candidates * np.ones((slate_size,)))\n    user_model = ie.IEUserModel(\n        slate_size,\n        user_state_ctor=ie.IEUserState,\n        response_model_ctor=ie.IEResponse)\n\n    # Create a set of documents\n    document_sampler = ie.IETopicDocumentSampler(seed=1)\n    ieenv = environment.Environment(\n        user_model,\n        document_sampler,\n        num_candidates,\n        slate_size,\n        resample_documents=True)\n\n    # Create agent\n    agent = greedy_pctr_agent.GreedyPCTRAgent(action_space,\n                                              user_model.avg_user_state)\n\n    # This agent doesn\'t use the previous user response\n    observation, documents = ieenv.reset()\n    slate = agent.step(1, dict(user=observation, doc=documents))\n    scores = [\n        user_model.avg_user_state.score_document(doc_obs)\n        for doc_obs in list(documents.values())\n    ]\n    expected_slate = sorted(np.argsort(scores)[-2:])\n    self.assertAllEqual(sorted(slate), expected_slate)\n\n  def test_bundle_and_unbundle_trivial(self):\n    action_space = spaces.MultiDiscrete(np.ones((1,)))\n    agent = greedy_pctr_agent.GreedyPCTRAgent(action_space, None)\n    self.assertFalse(agent.unbundle(\'\', 0, {}))\n    self.assertEqual({\n        \'episode_num\': 0\n    }, agent.bundle_and_checkpoint(\'\', 0))\n\n  def test_bundle_and_unbundle(self):\n    # Initialize agent\n    slate_size = 1\n    num_candidates = 3\n    action_space = spaces.MultiDiscrete(num_candidates * np.ones((slate_size,)))\n\n    user_model = ie.IEUserModel(\n        slate_size,\n        user_state_ctor=ie.IEUserState,\n        response_model_ctor=ie.IEResponse)\n    agent = greedy_pctr_agent.GreedyPCTRAgent(action_space,\n                                              user_model.avg_user_state)\n\n    # Create a set of documents\n    document_sampler = ie.IETopicDocumentSampler()\n    documents = {}\n    for i in range(num_candidates):\n      video = document_sampler.sample_document()\n      documents[i] = video.create_observation()\n\n    # Test that slate indices in correct range and length is correct\n    observation = dict(user=user_model.create_observation(), doc=documents)\n    agent.step(1, observation)\n\n    bundle_dict = agent.bundle_and_checkpoint(\'\', 0)\n    self.assertTrue(agent.unbundle(\'\', 0, bundle_dict))\n    self.assertEqual(bundle_dict, agent.bundle_and_checkpoint(\'\', 0))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
recsim/agents/random_agent.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A simple recommender system agent that recommends random slates.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nfrom absl import logging\n\nimport numpy as np\n\nfrom recsim import agent\n\n\nclass RandomAgent(agent.AbstractEpisodicRecommenderAgent):\n  """"""An agent that recommends a random slate of documents.""""""\n\n  def __init__(self, action_space, random_seed=0):\n    super(RandomAgent, self).__init__(action_space)\n    self._rng = np.random.RandomState(random_seed)\n\n  def step(self, reward, observation):\n    """"""Records the most recent transition and returns the agent\'s next action.\n\n    We store the observation of the last time step since we want to store it\n    with the reward.\n\n    Args:\n      reward: Unused.\n      observation: A dictionary that includes the most recent observation.\n        Should include \'doc\' field that includes observation of all candidates.\n\n    Returns:\n      slate: An integer array of size _slate_size, where each element is an\n        index into the list of doc_obs\n    """"""\n    del reward  # Unused argument.\n    doc_obs = observation[\'doc\']\n\n    # Simulate a random slate\n    doc_ids = list(range(len(doc_obs)))\n    self._rng.shuffle(doc_ids)\n    slate = doc_ids[:self._slate_size]\n    logging.debug(\'Recommended slate: %s\', slate)\n    return slate\n'"
recsim/agents/random_agent_test.py,2,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.agents.random_agent.""""""\n\nfrom gym import spaces\nimport numpy as np\nfrom recsim import choice_model\nfrom recsim.agents import random_agent\nfrom recsim.environments import interest_evolution as iev\nfrom recsim.environments import interest_exploration as ie\nfrom recsim.simulator import environment\nimport tensorflow.compat.v1 as tf\n\n\nclass RandomAgentTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(RandomAgentTest, self).setUp()\n    # The maximum length of videos in response\n    iev.IEvResponse.MAX_VIDEO_LENGTH = 100.0\n\n    # The number of features used to represent user state.\n    iev.IEvUserState.NUM_FEATURES = 10\n\n    # The number of features used to represent video.\n    iev.IEvVideo.NUM_FEATURES = 10\n    # The maximum length of videos\n    iev.IEvVideo.MAX_VIDEO_LENGTH = 100.0\n\n  def test_step(self):\n    # Create a simple user\n    slate_size = 2\n    user_model = iev.IEvUserModel(\n        slate_size,\n        choice_model_ctor=choice_model.MultinomialLogitChoiceModel,\n        response_model_ctor=iev.IEvResponse)\n\n    # Create a candidate_set with 5 items\n    num_candidates = 5\n    document_sampler = iev.IEvVideoSampler()\n    ievsim = environment.Environment(user_model, document_sampler,\n                                     num_candidates, slate_size)\n\n    # Create agent\n    action_space = spaces.MultiDiscrete(num_candidates * np.ones((slate_size,)))\n    agent = random_agent.RandomAgent(action_space, random_seed=0)\n\n    # This agent doesn\'t use the previous user response\n    observation, documents = ievsim.reset()\n    slate = agent.step(1, dict(user=observation, doc=documents))\n    self.assertAllEqual(slate, [2, 0])\n\n  def test_slate_indices_and_length(self):\n    # Initialize agent\n    slate_size = 2\n    num_candidates = 100\n    action_space = spaces.MultiDiscrete(num_candidates * np.ones((slate_size,)))\n\n    user_model = iev.IEvUserModel(\n        slate_size,\n        choice_model_ctor=choice_model.MultinomialLogitChoiceModel,\n        response_model_ctor=iev.IEvResponse)\n    agent = random_agent.RandomAgent(action_space, random_seed=0)\n\n    # Create a set of documents\n    document_sampler = iev.IEvVideoSampler()\n    ievenv = environment.Environment(user_model, document_sampler,\n                                     num_candidates, slate_size)\n\n    # Test that slate indices in correct range and length is correct\n    observation, documents = ievenv.reset()\n    slate = agent.step(1, dict(user=observation, doc=documents))\n    self.assertLen(slate, slate_size)\n    self.assertAllInSet(slate, range(num_candidates))\n\n  def test_bundle_and_unbundle_trivial(self):\n    action_space = spaces.MultiDiscrete(np.ones((1,)))\n    agent = random_agent.RandomAgent(action_space, random_seed=0)\n    self.assertFalse(agent.unbundle(\'\', 0, {}))\n    self.assertEqual({\n        \'episode_num\': 0\n    }, agent.bundle_and_checkpoint(\'\', 0))\n\n  def test_bundle_and_unbundle(self):\n    # Initialize agent\n    slate_size = 1\n    num_candidates = 3\n    action_space = spaces.MultiDiscrete(num_candidates * np.ones((slate_size,)))\n\n    user_model = ie.IEUserModel(\n        slate_size,\n        user_state_ctor=ie.IEUserState,\n        response_model_ctor=ie.IEResponse)\n    agent = random_agent.RandomAgent(action_space, random_seed=0)\n\n    # Create a set of documents\n    document_sampler = ie.IETopicDocumentSampler()\n    documents = {}\n    for i in range(num_candidates):\n      video = document_sampler.sample_document()\n      documents[i] = video.create_observation()\n\n    # Test that slate indices in correct range and length is correct\n    observation = dict(user=user_model.create_observation(), doc=documents)\n    agent.step(1, observation)\n\n    bundle_dict = agent.bundle_and_checkpoint(\'\', 0)\n    self.assertTrue(agent.unbundle(\'\', 0, bundle_dict))\n    self.assertEqual(bundle_dict, agent.bundle_and_checkpoint(\'\', 0))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
recsim/agents/slate_decomp_q_agent.py,106,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Agent that implements the Slate-Q algorithms.""""""\nimport gin.tf\nimport numpy as np\nfrom recsim import agent as abstract_agent\nfrom recsim import choice_model\nfrom recsim.agents.dopamine import dqn_agent\nimport tensorflow.compat.v1 as tf\n\n\ndef compute_probs_tf(slate, scores_tf, score_no_click_tf):\n  """"""Computes the selection probability and returns selected index.\n\n  This assumes scores are normalizable, e.g., scores cannot be negative.\n\n  Args:\n    slate: a list of integers that represents the video slate.\n    scores_tf: a float tensor that stores the scores of all documents.\n    score_no_click_tf: a float tensor that represents the score for the action\n      of picking no document.\n\n  Returns:\n    A float tensor that represents the probabilities of selecting each document\n      in the slate.\n  """"""\n  all_scores = tf.concat([\n      tf.gather(scores_tf, slate),\n      tf.reshape(score_no_click_tf, (1, 1))\n  ], axis=0)  # pyformat: disable\n  all_probs = all_scores / tf.reduce_sum(input_tensor=all_scores)\n  return all_probs[:-1]\n\n\ndef score_documents_tf(user_obs,\n                       doc_obs,\n                       no_click_mass=1.0,\n                       is_mnl=False,\n                       min_normalizer=-1.0):\n  """"""Computes unnormalized scores given both user and document observations.\n\n  This implements both multinomial proportional model and multinormial logit\n    model given some parameters. We also assume scores are based on inner\n    products of user_obs and doc_obs.\n\n  Args:\n    user_obs: An instance of AbstractUserState.\n    doc_obs: A numpy array that represents the observation of all documents in\n      the candidate set.\n    no_click_mass: a float indicating the mass given to a no click option\n    is_mnl: whether to use a multinomial logit model instead of a multinomial\n      proportional model.\n    min_normalizer: A float (<= 0) used to offset the scores to be positive when\n      using multinomial proportional model.\n\n  Returns:\n    A float tensor that stores unnormalzied scores of documents and a float\n      tensor that represents the score for the action of picking no document.\n  """"""\n  user_obs = tf.reshape(user_obs, [1, -1])\n  scores = tf.reduce_sum(input_tensor=tf.multiply(user_obs, doc_obs), axis=1)\n  all_scores = tf.concat([scores, tf.constant([no_click_mass])], axis=0)\n  if is_mnl:\n    all_scores = tf.nn.softmax(all_scores)\n  else:\n    all_scores = all_scores - min_normalizer\n  return all_scores[:-1], all_scores[-1]\n\n\ndef score_documents(user_obs,\n                    doc_obs,\n                    no_click_mass=1.0,\n                    is_mnl=False,\n                    min_normalizer=-1.0):\n  """"""Computes unnormalized scores given both user and document observations.\n\n  Similar to score_documents_tf but works on NumPy objects.\n\n  Args:\n    user_obs: An instance of AbstractUserState.\n    doc_obs: A numpy array that represents the observation of all documents in\n      the candidate set.\n    no_click_mass: a float indicating the mass given to a no click option\n    is_mnl: whether to use a multinomial logit model instead of a multinomial\n      proportional model.\n    min_normalizer: A float (<= 0) used to offset the scores to be positive when\n      using multinomial proportional model.\n\n  Returns:\n    A float array that stores unnormalzied scores of documents and a float\n      number that represents the score for the action of picking no document.\n  """"""\n  scores = np.array([])\n  for doc in doc_obs:\n    scores = np.append(scores, np.dot(user_obs, doc))\n\n  all_scores = np.append(scores, no_click_mass)\n  if is_mnl:\n    all_scores = choice_model.softmax(all_scores)\n  else:\n    all_scores = all_scores - min_normalizer\n  assert not all_scores[\n      all_scores < 0.0], \'Normalized scores have non-positive elements.\'\n  return all_scores[:-1], all_scores[-1]\n\n\ndef select_slate_topk(slate_size, s_no_click, s, q):\n  """"""Selects the slate using the top-K algorithm.\n\n  This algorithm corresponds to the method ""TS"" in\n  Ie et al. https://arxiv.org/abs/1905.12767.\n\n  Args:\n    slate_size: int, the size of the recommendation slate.\n    s_no_click: float tensor, the score for not clicking any document.\n    s: [num_of_documents] tensor, the scores for clicking documents.\n    q: [num_of_documents] tensor, the predicted q values for documents.\n\n  Returns:\n    [slate_size] tensor, the selected slate.\n  """"""\n  del s_no_click  # Unused argument.\n  _, output_slate = tf.math.top_k(s * q, k=slate_size)\n  return output_slate\n\n\ndef select_slate_greedy(slate_size, s_no_click, s, q):\n  """"""Selects the slate using the adaptive greedy algorithm.\n\n  This algorithm corresponds to the method ""GS"" in\n  Ie et al. https://arxiv.org/abs/1905.12767.\n\n  Args:\n    slate_size: int, the size of the recommendation slate.\n    s_no_click: float tensor, the score for not clicking any document.\n    s: [num_of_documents] tensor, the scores for clicking documents.\n    q: [num_of_documents] tensor, the predicted q values for documents.\n\n  Returns:\n    [slate_size] tensor, the selected slate.\n  """"""\n\n  def argmax(v, mask):\n    return tf.argmax(\n        input=(v - tf.reduce_min(input_tensor=v) + 1) * mask, axis=0)\n\n  numerator = tf.constant(0.)\n  denominator = tf.constant(0.) + s_no_click\n  mask = tf.ones(tf.shape(input=q)[0])\n\n  def set_element(v, i, x):\n    mask = tf.one_hot(i, tf.shape(input=v)[0])\n    v_new = tf.ones_like(v) * x\n    return tf.where(tf.equal(mask, 1), v_new, v)\n\n  for _ in range(slate_size):\n    k = argmax((numerator + s * q) / (denominator + s), mask)\n    mask = set_element(mask, k, 0)\n    numerator = numerator + tf.gather(s * q, k)\n    denominator = denominator + tf.gather(s, k)\n\n  output_slate = tf.compat.v1.where(tf.equal(mask, 0))\n  return output_slate\n\n\ndef select_slate_optimal(slate_size, s_no_click, s, q):\n  """"""Selects the slate using exhaustive search.\n\n  This algorithm corresponds to the method ""OS"" in\n  Ie et al. https://arxiv.org/abs/1905.12767.\n\n  Args:\n    slate_size: int, the size of the recommendation slate.\n    s_no_click: float tensor, the score for not clicking any document.\n    s: [num_of_documents] tensor, the scores for clicking documents.\n    q: [num_of_documents] tensor, the predicted q values for documents.\n\n  Returns:\n    [slate_size] tensor, the selected slate.\n  """"""\n\n  num_candidates = s.shape.as_list()[0]\n\n  # Obtain all possible slates given current docs in the candidate set.\n  mesh_args = [list(range(num_candidates))] * slate_size\n  slates = tf.stack(tf.meshgrid(*mesh_args), axis=-1)\n  slates = tf.reshape(slates, shape=(-1, slate_size))\n\n  # Filter slates that include duplicates to ensure each document is picked\n  # at most once.\n  unique_mask = tf.map_fn(\n      lambda x: tf.equal(tf.size(input=x), tf.size(input=tf.unique(x)[0])),\n      slates,\n      dtype=tf.bool)\n  slates = tf.boolean_mask(tensor=slates, mask=unique_mask)\n\n  slate_q_values = tf.gather(s * q, slates)\n  slate_scores = tf.gather(s, slates)\n  slate_normalizer = tf.reduce_sum(\n      input_tensor=slate_scores, axis=1) + s_no_click\n\n  slate_q_values = slate_q_values / tf.expand_dims(slate_normalizer, 1)\n  slate_sum_q_values = tf.reduce_sum(input_tensor=slate_q_values, axis=1)\n  max_q_slate_index = tf.argmax(input=slate_sum_q_values)\n  return tf.gather(slates, max_q_slate_index, axis=0)\n\n\ndef compute_target_sarsa(reward, gamma, next_actions, next_q_values,\n                         next_states, terminals):\n  """"""Computes the SARSA target Q value.\n\n  Args:\n    reward: [batch_size] tensor, the immediate reward.\n    gamma: float, discount factor with the usual RL meaning.\n    next_actions: [batch_size, slate_size] tensor, the next slate.\n    next_q_values: [batch_size, num_of_documents] tensor, the q values of the\n      documents in the next step.\n    next_states: [batch_size, 1 + num_of_documents] tensor, the features for the\n      user and the docuemnts in the next step.\n    terminals: [batch_size] tensor, indicating if this is a terminal step.\n\n  Returns:\n    [batch_size] tensor, the target q values.\n  """"""\n  stack_number = -1\n  user_obs = next_states[:, 0, :, stack_number]\n  doc_obs = next_states[:, 1:, :, stack_number]\n\n  batch_size = next_q_values.get_shape().as_list()[0]\n  next_sarsa_q_list = []\n  for i in range(batch_size):\n    s, s_no_click = score_documents_tf(user_obs[i], doc_obs[i])\n    q = next_q_values[i]\n\n    slate = tf.expand_dims(next_actions[i], 1)\n    p_selected = compute_probs_tf(slate, s, s_no_click)\n    q_selected = tf.gather(q, slate)\n    next_sarsa_q_list.append(\n        tf.reduce_sum(input_tensor=p_selected * q_selected))\n\n  next_sarsa_q_values = tf.stack(next_sarsa_q_list)\n\n  return reward + gamma * next_sarsa_q_values * (1. -\n                                                 tf.cast(terminals, tf.float32))\n\n\n# The top-K and greedy algorithms rely on the fact that the\n# probs[slate] = normalize(scores[slate], score_no_click)\ndef compute_target_greedy_q(reward, gamma, next_actions, next_q_values,\n                            next_states, terminals):\n  """"""Computes the optimal target Q value with the adaptive greedy algorithm.\n\n  This algorithm corresponds to the method ""GT"" in\n  Ie et al. https://arxiv.org/abs/1905.12767..\n\n  Args:\n    reward: [batch_size] tensor, the immediate reward.\n    gamma: float, discount factor with the usual RL meaning.\n    next_actions: [batch_size, slate_size] tensor, the next slate.\n    next_q_values: [batch_size, num_of_documents] tensor, the q values of the\n      documents in the next step.\n    next_states: [batch_size, 1 + num_of_documents] tensor, the features for the\n      user and the docuemnts in the next step.\n    terminals: [batch_size] tensor, indicating if this is a terminal step.\n\n  Returns:\n    [batch_size] tensor, the target q values.\n  """"""\n  slate_size = next_actions.get_shape().as_list()[1]\n  stack_number = -1\n  user_obs = next_states[:, 0, :, stack_number]\n  doc_obs = next_states[:, 1:, :, stack_number]\n\n  batch_size = next_q_values.get_shape().as_list()[0]\n  next_greedy_q_list = []\n  for i in range(batch_size):\n    s, s_no_click = score_documents_tf(user_obs[i], doc_obs[i])\n    q = next_q_values[i]\n\n    slate = select_slate_greedy(slate_size, s_no_click, s, q)\n    p_selected = compute_probs_tf(slate, s, s_no_click)\n    q_selected = tf.gather(q, slate)\n    next_greedy_q_list.append(\n        tf.reduce_sum(input_tensor=p_selected * q_selected))\n\n  next_greedy_q_values = tf.stack(next_greedy_q_list)\n\n  return reward + gamma * next_greedy_q_values * (\n      1. - tf.cast(terminals, tf.float32))\n\n\ndef _get_unnormalized_scores(states):\n  """"""Computes the unnormalized scores for the docs.""""""\n  stack_number = -1\n  user_obs = states[:, 0, :, stack_number]\n  doc_obs = states[:, 1:, :, stack_number]\n\n  batch_size = states.get_shape().as_list()[0]\n  scores_list = []\n  score_no_click_list = []\n  for i in range(batch_size):\n    scores_tf, score_no_click_tf = score_documents_tf(user_obs[i], doc_obs[i])\n    scores_list.append(scores_tf)\n    score_no_click_list.append(score_no_click_tf)\n  scores = tf.stack(scores_list)\n  score_no_click = tf.stack(score_no_click_list)\n\n  return scores, score_no_click\n\n\ndef compute_target_topk_q(reward, gamma, next_actions, next_q_values,\n                          next_states, terminals):\n  """"""Computes the optimal target Q value with the greedy algorithm.\n\n  This algorithm corresponds to the method ""TT"" in\n  Ie et al. https://arxiv.org/abs/1905.12767.\n\n  Args:\n    reward: [batch_size] tensor, the immediate reward.\n    gamma: float, discount factor with the usual RL meaning.\n    next_actions: [batch_size, slate_size] tensor, the next slate.\n    next_q_values: [batch_size, num_of_documents] tensor, the q values of the\n      documents in the next step.\n    next_states: [batch_size, 1 + num_of_documents] tensor, the features for the\n      user and the docuemnts in the next step.\n    terminals: [batch_size] tensor, indicating if this is a terminal step.\n\n  Returns:\n    [batch_size] tensor, the target q values.\n  """"""\n  slate_size = next_actions.get_shape().as_list()[1]\n  scores, score_no_click = _get_unnormalized_scores(next_states)\n\n  # Choose the documents with top affinity_scores * Q values to fill a slate and\n  # treat it as if it is the optimal slate.\n  unnormalized_next_q_target = next_q_values * scores\n  _, topk_optimal_slate = tf.math.top_k(\n      unnormalized_next_q_target, k=slate_size)\n\n  # Get the expected Q-value of the slate containing top-K items.\n  # [batch_size, slate_size]\n  next_q_values_selected = tf.compat.v1.batch_gather(\n      next_q_values, tf.cast(topk_optimal_slate, dtype=tf.int32))\n\n  # Get normalized affinity scores on the slate.\n  # [batch_size, slate_size]\n  scores_selected = tf.compat.v1.batch_gather(\n      scores, tf.cast(topk_optimal_slate, dtype=tf.int32))\n\n  next_q_target_topk = tf.reduce_sum(\n      input_tensor=next_q_values_selected * scores_selected, axis=1) / (\n          tf.reduce_sum(input_tensor=scores_selected, axis=1) + score_no_click)\n\n  return reward + gamma * next_q_target_topk * (\n      1. - tf.cast(terminals, tf.float32))\n\n\ndef compute_target_optimal_q(reward, gamma, next_actions, next_q_values,\n                             next_states, terminals):\n  """"""Builds an op used as a target for the Q-value.\n\n  This algorithm corresponds to the method ""OT"" in\n  Ie et al. https://arxiv.org/abs/1905.12767..\n\n  Args:\n    reward: [batch_size] tensor, the immediate reward.\n    gamma: float, discount factor with the usual RL meaning.\n    next_actions: [batch_size, slate_size] tensor, the next slate.\n    next_q_values: [batch_size, num_of_documents] tensor, the q values of the\n      documents in the next step.\n    next_states: [batch_size, 1 + num_of_documents] tensor, the features for the\n      user and the docuemnts in the next step.\n    terminals: [batch_size] tensor, indicating if this is a terminal step.\n\n  Returns:\n    [batch_size] tensor, the target q values.\n  """"""\n  scores, score_no_click = _get_unnormalized_scores(next_states)\n\n  # Obtain all possible slates given current docs in the candidate set.\n  slate_size = next_actions.get_shape().as_list()[1]\n  num_candidates = next_q_values.get_shape().as_list()[1]\n  mesh_args = [list(range(num_candidates))] * slate_size\n  slates = tf.stack(tf.meshgrid(*mesh_args), axis=-1)\n  slates = tf.reshape(slates, shape=(-1, slate_size))\n  # Filter slates that include duplicates to ensure each document is picked\n  # at most once.\n  unique_mask = tf.map_fn(\n      lambda x: tf.equal(tf.size(input=x), tf.size(input=tf.unique(x)[0])),\n      slates,\n      dtype=tf.bool)\n  # [num_of_slates, slate_size]\n  slates = tf.boolean_mask(tensor=slates, mask=unique_mask)\n\n  # [batch_size, num_of_slates, slate_size]\n  next_q_values_slate = tf.gather(next_q_values, slates, axis=1)\n  # [batch_size, num_of_slates, slate_size]\n  scores_slate = tf.gather(scores, slates, axis=1)\n  # [batch_size, num_of_slates]\n  batch_size = next_states.get_shape().as_list()[0]\n  score_no_click_slate = tf.reshape(\n      tf.tile(score_no_click,\n              tf.shape(input=slates)[:1]), [batch_size, -1])\n\n  # [batch_size, num_of_slates]\n  next_q_target_slate = tf.reduce_sum(\n      input_tensor=next_q_values_slate * scores_slate, axis=2) / (\n          tf.reduce_sum(input_tensor=scores_slate, axis=2) +\n          score_no_click_slate)\n\n  next_q_target_max = tf.reduce_max(input_tensor=next_q_target_slate, axis=1)\n\n  return reward + gamma * next_q_target_max * (1. -\n                                               tf.cast(terminals, tf.float32))\n\n\n@gin.configurable\nclass SlateDecompQAgent(dqn_agent.DQNAgentRecSim,\n                        abstract_agent.AbstractEpisodicRecommenderAgent):\n  """"""A recommender agent implements DQN using slate decomposition techniques.""""""\n\n  def __init__(self,\n               sess,\n               observation_space,\n               action_space,\n               optimizer_name=\'\',\n               select_slate_fn=None,\n               compute_target_fn=None,\n               stack_size=1,\n               eval_mode=False,\n               **kwargs):\n    """"""Initializes SlateDecompQAgent.\n\n    Args:\n      sess: a Tensorflow session.\n      observation_space: A gym.spaces object that specifies the format of\n        observations.\n      action_space: A gym.spaces object that specifies the format of actions.\n      optimizer_name: The name of the optimizer.\n      select_slate_fn: A function that selects the slate.\n      compute_target_fn: A function that omputes the target q value.\n      stack_size: The stack size for the replay buffer.\n      eval_mode: A bool for whether the agent is in training or evaluation mode.\n      **kwargs: Keyword arguments to the DQNAgent.\n    """"""\n    self._response_adapter = dqn_agent.ResponseAdapter(\n        observation_space.spaces[\'response\'])\n    response_names = self._response_adapter.response_names\n    expected_response_names = [\'click\', \'watch_time\']\n    if not all(key in response_names for key in expected_response_names):\n      raise ValueError(\n          ""Couldn\'t find all fields needed for the decomposition: %r"" %\n          expected_response_names)\n\n    self._click_response_index = response_names.index(\'click\')\n    self._reward_response_index = response_names.index(\'watch_time\')\n    self._quality_response_index = response_names.index(\'quality\')\n    self._cluster_id_response_index = response_names.index(\'cluster_id\')\n\n    self._env_action_space = action_space\n    self._num_candidates = int(action_space.nvec[0])\n    abstract_agent.AbstractEpisodicRecommenderAgent.__init__(self, action_space)\n\n    # The doc score is a [num_candidates] vector.\n    self._doc_affinity_scores_ph = tf.compat.v1.placeholder(\n        tf.float32, (self._num_candidates,), name=\'doc_affinity_scores_ph\')\n    self._prob_no_click_ph = tf.compat.v1.placeholder(\n        tf.float32, (), name=\'prob_no_click_ph\')\n\n    self._select_slate_fn = select_slate_fn\n    self._compute_target_fn = compute_target_fn\n\n    dqn_agent.DQNAgentRecSim.__init__(\n        self,\n        sess,\n        observation_space,\n        num_actions=0,  # Unused.\n        stack_size=1,\n        optimizer_name=optimizer_name,\n        eval_mode=eval_mode,\n        **kwargs)\n\n  def _network_adapter(self, states, scope):\n    self._validate_states(states)\n\n    with tf.compat.v1.name_scope(\'network\'):\n      # Since we decompose the slate optimization into an item-level\n      # optimization problem, the observation space is the user state\n      # observation plus all documents\' observations. In the Dopamine DQN agent\n      # implementation, there is one head for each possible action value, which\n      # is designed for computing the argmax operation in the action space.\n      # In our implementation, we generate one output for each document.\n      q_value_list = []\n      for i in range(self._num_candidates):\n        user = tf.squeeze(states[:, 0, :, :], axis=2)\n        doc = tf.squeeze(states[:, i + 1, :, :], axis=2)\n        q_value_list.append(self.network(user, doc, scope))\n      q_values = tf.concat(q_value_list, axis=1)\n\n    return dqn_agent.DQNNetworkType(q_values)\n\n  def _build_networks(self):\n    with tf.compat.v1.name_scope(\'networks\'):\n      self._replay_net_outputs = self._network_adapter(self._replay.states,\n                                                       \'Online\')\n      self._replay_next_target_net_outputs = self._network_adapter(\n          self._replay.states, \'Target\')\n      self._net_outputs = self._network_adapter(self.state_ph, \'Online\')\n      self._build_select_slate_op()\n\n  def _build_train_op(self):\n    """"""Builds a training op.\n\n    Returns:\n      An op performing one step of training from replay data.\n    """"""\n    # click_indicator: [B, S]\n    # q_values: [B, A]\n    # actions: [B, S]\n    # slate_q_values: [B, S]\n    # replay_click_q: [B]\n    click_indicator = self._replay.rewards[:, :, self._click_response_index]\n    slate_q_values = tf.compat.v1.batch_gather(\n        self._replay_net_outputs.q_values,\n        tf.cast(self._replay.actions, dtype=tf.int32))\n    # Only get the Q from the clicked document.\n    replay_click_q = tf.reduce_sum(\n        input_tensor=slate_q_values * click_indicator,\n        axis=1,\n        name=\'replay_click_q\')\n\n    target = tf.stop_gradient(self._build_target_q_op())\n\n    clicked = tf.reduce_sum(input_tensor=click_indicator, axis=1)\n    clicked_indices = tf.squeeze(\n        tf.compat.v1.where(tf.equal(clicked, 1)), axis=1)\n    # clicked_indices is a vector and tf.gather selects the batch dimension.\n    q_clicked = tf.gather(replay_click_q, clicked_indices)\n    target_clicked = tf.gather(target, clicked_indices)\n\n    def get_train_op():\n      loss = tf.reduce_mean(input_tensor=tf.square(q_clicked - target_clicked))\n      if self.summary_writer is not None:\n        with tf.compat.v1.variable_scope(\'Losses\'):\n          tf.compat.v1.summary.scalar(\'Loss\', loss)\n\n      return loss\n\n    loss = tf.cond(\n        pred=tf.greater(tf.reduce_sum(input_tensor=clicked), 0),\n        true_fn=get_train_op,\n        false_fn=lambda: tf.constant(0.),\n        name=\'\')\n\n    return self.optimizer.minimize(loss)\n\n  def _build_target_q_op(self):\n    """"""Builds an op used as a target for the Q-value.\n\n    Returns:\n      An op calculating the Q-value.\n    """"""\n    item_reward = self._replay.rewards[:, :, self._reward_response_index]\n    click_indicator = self._replay.rewards[:, :, self._click_response_index]\n    # Only compute the watch time reward of the clicked item.\n    reward = tf.reduce_sum(input_tensor=item_reward * click_indicator, axis=1)\n\n    return self._compute_target_fn(\n        reward=reward,\n        gamma=self.gamma,\n        next_actions=self._replay.next_actions,\n        next_q_values=self._replay_next_target_net_outputs.q_values,\n        next_states=self._replay.next_states,\n        terminals=self._replay.terminals)\n\n  # The following functions defines how the agent takes actions.\n  def step(self, reward, observation):\n    """"""Records the transition and returns the agent\'s next action.\n\n    It uses document-level user response instead of overral reward as the reward\n    of the problem.\n\n    Args:\n      reward: unused.\n      observation: a space.Dict that includes observation of the user state\n        observation, documents and user responses.\n\n    Returns:\n      Array, the selected action.\n    """"""\n    del reward  # Unused argument.\n\n    responses = observation[\'response\']\n    self._raw_observation = observation\n    return super(SlateDecompQAgent,\n                 self).step(self._response_adapter.encode(responses),\n                            self._obs_adapter.encode(observation))\n\n  def _build_select_slate_op(self):\n    p_no_click = self._prob_no_click_ph\n    p = self._doc_affinity_scores_ph\n    q = self._net_outputs.q_values[0]\n    with tf.compat.v1.name_scope(\'select_slate\'):\n      self._output_slate = self._select_slate_fn(self._slate_size, p_no_click,\n                                                 p, q)\n\n    self._output_slate = tf.compat.v1.Print(\n        self._output_slate, [tf.constant(\'cp 1\'), self._output_slate, p, q],\n        summarize=10000)\n    self._output_slate = tf.reshape(self._output_slate, (self._slate_size,))\n\n    self._action_counts = tf.compat.v1.get_variable(\n        \'action_counts\',\n        shape=[self._num_candidates],\n        initializer=tf.compat.v1.zeros_initializer())\n    output_slate = tf.reshape(self._output_slate, [-1])\n    output_one_hot = tf.one_hot(output_slate, self._num_candidates)\n    update_ops = []\n    for i in range(self._slate_size):\n      update_ops.append(\n          tf.compat.v1.assign_add(self._action_counts, output_one_hot[i]))\n    self._select_action_update_op = tf.group(*update_ops)\n\n  def _select_action(self):\n    """"""Selects an slate based on the trained model.\n\n    Chooses an action randomly with probability self._calculate_epsilon(), and\n    otherwise acts greedily according to the current Q-value estimates. It will\n    pick the top slate_size documents with highest Q values and return them as a\n    slate.\n\n    Returns:\n       Array, the selected action.\n    """"""\n    if self.eval_mode:\n      epsilon = self.epsilon_eval\n    else:\n      epsilon = self.epsilon_fn(self.epsilon_decay_period, self.training_steps,\n                                self.min_replay_history, self.epsilon_train)\n      self._add_summary(\'epsilon\', epsilon)\n\n    if np.random.random() <= epsilon:\n      # Sample without replacement.\n      return np.random.choice(\n          self._num_candidates, self._slate_size, replace=False)\n    else:\n      observation = self._raw_observation\n      user_obs = observation[\'user\']\n      doc_obs = np.array(list(observation[\'doc\'].values()))\n      tf.compat.v1.logging.debug(\'cp 1: %s, %s\', doc_obs, observation)\n      # TODO(cwhsu): Use score_documents_tf() and remove score_documents().\n      scores, score_no_click = score_documents(user_obs, doc_obs)\n      output_slate, _ = self._sess.run(\n          [self._output_slate, self._select_action_update_op], {\n              self.state_ph: self.state,\n              self._doc_affinity_scores_ph: scores,\n              self._prob_no_click_ph: score_no_click,\n          })\n\n      return output_slate\n\n  # Other functions.\n  def _build_replay_buffer(self, use_staging):\n    """"""Creates the replay buffer used by the agent.\n\n    Args:\n      use_staging: bool, if True, uses a staging area to prefetch data for\n        faster training.\n\n    Returns:\n      A WrapperReplayBuffer object.\n    """"""\n    return dqn_agent.wrapped_replay_buffer(\n        observation_shape=self.observation_shape,\n        stack_size=self.stack_size,\n        use_staging=use_staging,\n        update_horizon=self.update_horizon,\n        gamma=self.gamma,\n        observation_dtype=self.observation_dtype,\n        action_shape=self._env_action_space.shape,\n        action_dtype=self._env_action_space.dtype,\n        reward_shape=self._response_adapter.response_shape,\n        reward_dtype=self._response_adapter.response_dtype)\n\n  def _add_summary(self, tag, value):\n    if self.summary_writer:\n      summary = tf.compat.v1.Summary(\n          value=[tf.compat.v1.Summary.Value(tag=tag, simple_value=value)])\n      self.summary_writer.add_summary(summary, self.training_steps)\n\n  def begin_episode(self, observation):\n    """"""Returns the agent\'s first action for this episode.\n\n    Args:\n      observation: numpy array, the environment\'s initial observation.\n\n    Returns:\n      An integer array of size _slate_size, the selected slated, each\n      element of which is an index in the list of doc_obs.\n    """"""\n    self._raw_observation = observation\n    return super(SlateDecompQAgent,\n                 self).begin_episode(self._obs_adapter.encode(observation))\n\n  def end_episode(self, reward, observation):\n    """"""Signals the end of the episode to the agent.\n\n    We store the observation of the current time step, which is the last\n    observation of the episode.\n\n    Args:\n      reward: float, the last reward from the environment.\n      observation: numpy array, the environment\'s initial observation.\n    """"""\n    del reward  # Unused argument.\n    super(SlateDecompQAgent, self).end_episode(\n        self._response_adapter.encode(observation[\'response\']))\n\n\ndef create_agent(agent_name, sess, **kwargs):\n  """"""Creates a slate decomposition agent given agent name.""""""\n  if agent_name == \'dp_random\':\n    return SlateDecompQAgent(\n        sess,\n        epsilon_train=1.0,\n        epsilon_eval=1.0,\n        select_slate_fn=select_slate_greedy,\n        compute_target_fn=compute_target_sarsa,\n        **kwargs)\n  elif agent_name == \'slate_topk_sarsa\':\n    return SlateDecompQAgent(\n        sess,\n        select_slate_fn=select_slate_topk,\n        compute_target_fn=compute_target_sarsa,\n        **kwargs)\n  elif agent_name == \'myopic_slate_topk_sarsa\':\n    return SlateDecompQAgent(\n        sess,\n        gamma=0,\n        select_slate_fn=select_slate_topk,\n        compute_target_fn=compute_target_sarsa,\n        **kwargs)\n  elif agent_name == \'slate_greedy_sarsa\':\n    return SlateDecompQAgent(\n        sess,\n        select_slate_fn=select_slate_greedy,\n        compute_target_fn=compute_target_sarsa,\n        **kwargs)\n  elif agent_name == \'myopic_slate_greedy_sarsa\':\n    return SlateDecompQAgent(\n        sess,\n        gamma=0,\n        select_slate_fn=select_slate_greedy,\n        compute_target_fn=compute_target_sarsa,\n        **kwargs)\n  elif agent_name == \'slate_greedy_optimal_q\':\n    return SlateDecompQAgent(\n        sess,\n        select_slate_fn=select_slate_greedy,\n        compute_target_fn=compute_target_optimal_q,\n        **kwargs)\n  elif agent_name == \'slate_topk_topk_q\':\n    return SlateDecompQAgent(\n        sess,\n        select_slate_fn=select_slate_topk,\n        compute_target_fn=compute_target_topk_q,\n        **kwargs)\n  elif agent_name == \'slate_topk_optimal_q\':\n    return SlateDecompQAgent(\n        sess,\n        select_slate_fn=select_slate_topk,\n        compute_target_fn=compute_target_optimal_q,\n        **kwargs)\n  elif agent_name == \'slate_optimal_optimal_q\':\n    return SlateDecompQAgent(\n        sess,\n        select_slate_fn=select_slate_optimal,\n        compute_target_fn=compute_target_optimal_q,\n        **kwargs)\n  elif agent_name == \'slate_greedy_greedy_q\':\n    return SlateDecompQAgent(\n        sess,\n        select_slate_fn=select_slate_greedy,\n        compute_target_fn=compute_target_greedy_q,\n        **kwargs)\n  else:\n    raise ValueError(\'Unknown agent: {}\'.format(agent_name))\n'"
recsim/agents/tabular_q_agent.py,1,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A Tabular Q-learning implementation.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nfrom absl import logging\nfrom gym import spaces\nimport numpy as np\n\nfrom recsim import agent\nfrom recsim.agents import agent_utils\n\n\nclass TabularQAgent(agent.AbstractEpisodicRecommenderAgent):\n  """"""Tabular Q-learning agent with universal function approximation.\n\n  This agent provides a tabular implementation of the Q-learning algorithm.\n  To construct a tabular representation of the state-action space, the agent\n  does the following:\n    1. the action space consists of all ordered k-tuples of document features\n    available in observation[\'doc\'];\n    2. the state space consists of observation[\'user\']\n    and observation[\'response\'];\n    3. the observation and action space are joined and flattened;\n    4. all continuoius values in the flattened state-action vector are\n    discretized into a predefined number of bins.\n  In the tabularized state-action space, the agent applies the standard\n  Q-learning update of\n    Q_{t+1}(s,a) = (1-a) * Q_t(s,a) + a * (R(s,a) + g * max_a(Q_t(s\', a))).\n  Assuming that the discretization of countinous attributes is fine enough, and\n  the problem itself is Markovian given the observations, the output of this\n  agent can be assumed to converge to a close approximation of the ground truth\n  Q-function. Producing ground truth Q-functions is the main intended use of\n  this agent, since discretization is prohibitively expensive in\n  high-dimensional environments.\n  """"""\n\n  def __init__(self,\n               observation_space,\n               action_space,\n               eval_mode=False,\n               ignore_response=True,\n               discretization_bounds=(0.0, 10.0),\n               number_bins=100,\n               exploration_policy=\'epsilon_greedy\',\n               exploration_temperature=0.99,\n               learning_rate=0.1,\n               gamma=0.99,\n               ordinal_slates=False,\n               **kwargs):\n    """"""TabularQAgent init.\n\n    Args:\n      observation_space: a gym.spaces object specifying the format of\n        observations.\n      action_space: a gym.spaces object that specifies the format of actions.\n      eval_mode: Boolean indicating whether the agent is in training or eval\n        mode.\n      ignore_response: Boolean indicating whether the agent should ignore the\n        response part of the observation.\n      discretization_bounds: pair of real numbers indicating the min and max\n        value for continuous attributes discretization. Values below the min\n        will all be grouped in the first bin, while values above the max will\n        all be grouped in the last bin. See the documentation of numpy.digitize\n        for further details.\n      number_bins: positive integer number of bins used to discretize continuous\n        attributes.\n      exploration_policy: either one of [\'epsilon_greedy\', \'min_count\'] or a\n        custom function. TODO(mmladenov): formalize requirements of this\n          function.\n      exploration_temperature: a real number passed as parameter to the\n        exploration policy.\n      learning_rate: a real number between 0 and 1 indicating how much to update\n        Q-values, i.e. Q_t+1(s,a) = (1 - learning_rate) * Q_t(s, a)\n                                     + learning_rate * (R(s,a) + ...).\n      gamma: real value between 0 and 1 indicating the discount factor of the\n        MDP.\n      ordinal_slates: boolean indicating whether slate ordering matters, e.g.\n        whether the slates (1, 2) and (2, 1) should be considered different\n        actions. Using ordinal slates increases complexity factorially.\n      **kwargs: additional arguments like eval_mode.\n    """"""\n    self._kwargs = kwargs\n    super(TabularQAgent, self).__init__(action_space)\n    # hard params\n    self._gamma = gamma\n    self._eval_mode = eval_mode\n    self._previous_slate = None\n    self._ordinal_slates = ordinal_slates\n    self._learning_rate = learning_rate\n    # storage\n    self._q_value_table = {}\n    self._state_action_counts = {}\n    self._previous_state_action_index = None\n    # discretization and spaces\n    self._discretization_bins = np.linspace(\n        discretization_bounds[0], discretization_bounds[1], num=number_bins)\n    single_doc_space = list(observation_space.spaces[\'doc\'].spaces.values())[0]\n    slate_tuple = tuple([single_doc_space] * self._slate_size)\n    action_space = spaces.Tuple(slate_tuple)\n    self._ignore_response = ignore_response\n    state_action_space = {\n        \'user\': observation_space.spaces[\'user\'],\n        \'action\': action_space\n    }\n    if not self._ignore_response:\n      state_action_space[\'response\'] = observation_space.spaces[\'response\']\n    self._state_action_space = spaces.Dict(state_action_space)\n    self._observation_featurizer = agent_utils.GymSpaceWalker(\n        self._state_action_space, self._discretize_gym_leaf)\n    # exploration\n    self._exploration_policy = exploration_policy\n    self._exploration_temperature = exploration_temperature\n    self._base_exploration_temperature = self._exploration_temperature\n    self._exploration_functions = {\n        \'epsilon_greedy\':\n            lambda observation: agent_utils.epsilon_greedy_exploration(  # pylint: disable=g-long-lambda\n                self._enumerate_state_action_indices(observation), self.\n                _q_value_table, self._exploration_temperature),\n        \'min_count\':\n            lambda observation: agent_utils.min_count_exploration(  # pylint: disable=g-long-lambda\n                self._enumerate_state_action_indices(observation),\n                self._state_action_counts)\n    }\n\n  def _discretize_gym_leaf(self, gym_space, gym_observations):\n\n    index = []\n    for gym_observation in gym_observations:\n      gym_observation = gym_observations[0]\n      if isinstance(gym_space, spaces.box.Box):\n        gym_observation = np.array(gym_observation)\n        dis_obs = np.digitize(gym_observation.flatten(),\n                              self._discretization_bins)\n        index += list(dis_obs)\n      elif isinstance(gym_space, spaces.discrete.Discrete):\n        index.append(gym_observation)\n      else:\n        raise NotImplementedError(\'Gym space type \' + str(type(gym_space)) +\n                                  \' not implemented yet.\')\n    return index\n\n  def _enumerate_slates(self, doc_dict):\n    documents = list(doc_dict.values())\n    num_documents = len(documents)\n    if self._ordinal_slates:\n      generator_fn = itertools.permutations\n    else:\n      generator_fn = itertools.combinations\n    for slate in generator_fn(range(num_documents), self._slate_size):\n      yield slate, tuple([documents[i] for i in slate])\n\n  def _enumerate_state_action_indices(self, observation):\n    for (slate, slate_features) in self._enumerate_slates(observation[\'doc\']):\n      state_action_pair = {\n          \'user\': observation[\'user\'],\n          \'action\': slate_features\n      }\n      if not self._ignore_response:\n        state_action_pair[\'response\'] = observation[\'response\']\n      state_action_index = self._observation_featurizer.apply_and_flatten([\n          state_action_pair,\n      ])\n      state_action_index = tuple(state_action_index)\n      yield slate, state_action_index\n\n  def step(self, reward, observation):\n    """"""Records the most recent transition and returns the agent\'s next action.\n\n    We store the observation of the last time step since we want to store it\n    with the reward.\n\n    Args:\n      reward: The reward received from the agent\'s most recent action as a\n        float.\n      observation: A dictionary that includes the most recent observations and\n        should have the following fields:\n        - user: A NumPy array representing user\'s observed state. Assumes it is\n          a concatenation of topic pull counts and topic click counts.\n        - doc: A NumPy array representing observations of document features.\n          Assumes it is a concatenation of one-hot encoding of topic_id and\n          document quality.\n\n    Returns:\n      slate: An integer array of size _slate_size, where each element is an\n        index into the list of doc_obs\n    Raises:\n      ValueError: if reward is not in [0, 1].\n    """"""\n    # Find max-Q action given the current state and Q-table.\n    max_q_state_action = max(\n        self._enumerate_state_action_indices(observation),\n        key=lambda sa: self._q_value_table.get(sa[1], 0))\n    max_q_next = self._q_value_table.get(max_q_state_action[1], 0)\n    # Update the Q-table.\n    if self._previous_state_action_index is not None:\n      old_q = self._q_value_table.get(self._previous_state_action_index, 0.)\n      self._q_value_table[self._previous_state_action_index] = (\n          self._learning_rate * (reward + self._gamma * max_q_next) +\n          (1. - self._learning_rate) * old_q)\n      self._state_action_counts[\n          self._previous_state_action_index] = self._state_action_counts.get(\n              self._previous_state_action_index, 0) + 1\n    # Pick next action.\n    if not self._eval_mode:\n      slate, state_action_index = self._exploration_functions[\n          self._exploration_policy](\n              observation)\n      self._previous_state_action_index = state_action_index\n    else:\n      slate, state_action_index = max_q_state_action\n    return slate\n\n  def end_episode(self, reward, observation):\n    self._exploration_temperature *= self._base_exploration_temperature\n    self._exploration_functions = {\n        \'epsilon_greedy\':\n            lambda observation: agent_utils.epsilon_greedy_exploration(  # pylint: disable=g-long-lambda\n                self._enumerate_state_action_indices(observation), self.\n                _q_value_table, self._exploration_temperature),\n        \'min_count\':\n            lambda observation: agent_utils.min_count_exploration(  # pylint: disable=g-long-lambda\n                self._enumerate_state_action_indices(observation),\n                self._state_action_counts)\n    }\n    self._previous_state_action_index = None\n\n  def bundle_and_checkpoint(self, checkpoint_dir, iteration_number):\n    """"""Returns a self-contained bundle of the agent\'s state.\n\n    Args:\n      checkpoint_dir: A string for the directory where objects will be saved.\n      iteration_number: An integer of iteration number to use for naming the\n        checkpoint file.\n\n    Returns:\n      A dictionary containing additional Python objects to be checkpointed by\n        the experiment. Each key is a string for the object name and the value\n        is actual object. If the checkpoint directory does not exist, returns\n        empty dictionary.\n    """"""\n    del checkpoint_dir  # Unused.\n    del iteration_number  # Unused.\n    bundle_dict = {\'q_value_table\': self._q_value_table}\n    bundle_dict[\'sa_count\'] = self._state_action_counts\n    return bundle_dict\n\n  def unbundle(self, checkpoint_dir, iteration_number, bundle_dict):\n    """"""Restores the agent from a checkpoint.\n\n    Args:\n      checkpoint_dir: A string that represents the path to the checkpoint saved\n        by tf.Save.\n      iteration_number: An integer that represents the checkpoint version and is\n        used when restoring replay buffer.\n      bundle_dict: A dict containing additional Python objects owned by the\n        agent. Each key is an object name and the value is the actual object.\n\n    Returns:\n      bool, True if unbundling was successful.\n    """"""\n    del checkpoint_dir  # Unused.\n    del iteration_number  # Unused.\n    if \'q_value_table\' not in bundle_dict:\n      logging.warning(\n          \'Could not unbundle from checkpoint files with exception.\')\n      return False\n    self._q_value_table = bundle_dict[\'q_value_table\']\n    self._state_action_counts = bundle_dict.get(\'sa_count\', {})\n    return True\n'"
recsim/agents/tabular_q_agent_test.py,2,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.agents.tabular_q_agent.""""""\n\nfrom gym import spaces\nimport numpy as np\nfrom recsim.agents import tabular_q_agent\nfrom recsim.testing import test_environment as te\nimport tensorflow.compat.v1 as tf\n\n\nclass TabularQAgentTest(tf.test.TestCase):\n\n  def init_agent_and_env(self,\n                         slate_size=1,\n                         num_candidates=10,\n                         learning_rate=0.8,\n                         gamma=0.0,\n                         policy=\'epsilon_greedy\',\n                         ordinal_slates=False,\n                         starting_probs=(1.0, 0.0, 0.0, 0.0, 0.0, 0.0)):\n    env_config = {\n        \'num_candidates\': num_candidates,\n        \'slate_size\': slate_size,\n        \'resample_documents\': False,\n        \'seed\': 42,\n        \'starting_probs\': starting_probs\n    }\n    te_sim = te.create_environment(env_config)\n    agent = tabular_q_agent.TabularQAgent(\n        te_sim.observation_space,\n        te_sim.action_space,\n        gamma=gamma,\n        exploration_policy=policy,\n        learning_rate=learning_rate,\n        ordinal_slates=ordinal_slates)\n    return te_sim, agent\n\n  def test_step(self):\n    te_sim, agent = self.init_agent_and_env()\n    observation0 = te_sim.reset()\n    slate1 = agent.step(0, observation0)\n    selected_doc0 = list(observation0[\'doc\'].values())[slate1[0]]\n    # Environment always starts at state 0.\n    self.assertEqual(agent._previous_state_action_index, (selected_doc0, 0))\n    observation1, reward1, _, _ = te_sim.step(slate1)\n    slate2 = agent.step(reward1, observation1)\n    selected_doc1 = list(observation1[\'doc\'].values())[slate2[0]]\n    observed_state = observation1[\'user\']\n    self.assertEqual(agent._previous_state_action_index,\n                     (selected_doc1, observed_state))\n    self.assertEqual(agent._q_value_table,\n                     {(selected_doc0, 0): agent._learning_rate * -10.0})\n    self.assertEqual(agent._state_action_counts, {(selected_doc0, 0): 1})\n\n  def test_myopic_value_estimation(self):\n    te_sim, agent = self.init_agent_and_env()\n    observation0 = te_sim.reset()\n    slate = agent.step(0, observation0)\n    for _ in range(1000):\n      observation, reward, _, _ = te_sim.step(slate)\n      slate = agent.step(reward, observation)\n    for state in range(6):\n      for action in range(4):\n        self.assertAlmostEqual(agent._q_value_table[(action, state)],\n                               te.QVALUES0[state][action])\n\n  def test_gamma05_value_estimation(self):\n    te_sim, agent = self.init_agent_and_env(gamma=0.5)\n    observation = te_sim.reset()\n    reward = 0\n    for i in range(100, 50100):\n      slate = agent.step(reward, observation)\n      observation, reward, _, _ = te_sim.step(slate)\n      agent._learning_rate = 100.0 / float(i)\n    for state in range(6):\n      for action in range(4):\n        self.assertAlmostEqual(\n            agent._q_value_table[(action, state)],\n            te.QVALUES05[state][action],\n            delta=0.25)\n\n  def test_dicretize_gym_leaf(self):\n    _, agent = self.init_agent_and_env()\n    self.assertEqual(\n        agent._discretize_gym_leaf(spaces.Discrete(5), [\n            4,\n        ]), [\n            4,\n        ])\n    box = spaces.Box(\n        low=agent._discretization_bins[0],\n        high=agent._discretization_bins[-1],\n        shape=(1, 1),\n        dtype=np.float32)\n    # Some corner cases in 1d and 2x2d.\n    self.assertEqual(\n        agent._discretize_gym_leaf(box, [\n            agent._discretization_bins[0] - 10E-5,\n        ]), [\n            0,\n        ])\n    self.assertEqual(\n        agent._discretize_gym_leaf(box, [\n            agent._discretization_bins[0],\n        ]), [\n            1,\n        ])\n    self.assertEqual(\n        agent._discretize_gym_leaf(box, [\n            agent._discretization_bins[-1] - 10E-5,\n        ]), [\n            len(agent._discretization_bins) - 1,\n        ])\n    self.assertEqual(\n        agent._discretize_gym_leaf(box, [\n            agent._discretization_bins[-1] + 1.0,\n        ]), [\n            len(agent._discretization_bins),\n        ])\n    box2x2 = spaces.Box(\n        low=agent._discretization_bins[0],\n        high=agent._discretization_bins[-1],\n        shape=(2, 2),\n        dtype=np.float32)\n    self.assertEqual(\n        agent._discretize_gym_leaf(box2x2, [\n            np.array([[\n                agent._discretization_bins[0] - 10E-6,\n                agent._discretization_bins[-1] - 10E-6\n            ],\n                      [\n                          agent._discretization_bins[-1] + 1.0,\n                          agent._discretization_bins[0]\n                      ]]),\n        ]), [0, 99, 100, 1])\n\n  def test_slate_enumeration(self):\n    te_sim, agent = self.init_agent_and_env(slate_size=2, num_candidates=4)\n    observation0 = te_sim.reset()\n    non_ordinal_slates = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n    enumerated_slates = [\n        slate for slate, _ in agent._enumerate_slates(observation0[\'doc\'])\n    ]\n    self.assertCountEqual(non_ordinal_slates, enumerated_slates)\n    te_sim, agent = self.init_agent_and_env(slate_size=2, num_candidates=4,\n                                            ordinal_slates=True)\n    ordinal_slates = non_ordinal_slates + [\n        (1, 0), (2, 0), (3, 0), (2, 1), (3, 1), (3, 2)\n    ]\n    enumerated_slates = [\n        slate for slate, _ in agent._enumerate_slates(observation0[\'doc\'])\n    ]\n    self.assertCountEqual(ordinal_slates, enumerated_slates)\n\n  def test_bundle_and_unbundle(self):\n    te_sim, agent = self.init_agent_and_env(\n        slate_size=1, num_candidates=4, policy=\'min_count\')\n    # Make a few steps to populate counts and Q-table.\n    observation0 = te_sim.reset()\n    slate1 = agent.step(0, observation0)\n    observation1, reward1, _, _ = te_sim.step(slate1)\n    agent.step(reward1, observation1)\n    bundle_dict = {\n        \'q_value_table\': agent._q_value_table,\n        \'sa_count\': agent._state_action_counts\n    }\n    self.assertEqual(bundle_dict, agent.bundle_and_checkpoint(\'\', 0))\n    _, new_agent = self.init_agent_and_env(slate_size=1, num_candidates=4)\n    self.assertTrue(new_agent.unbundle(\'\', 0, bundle_dict))\n    self.assertEqual(bundle_dict[\'q_value_table\'], new_agent._q_value_table)\n    self.assertEqual(bundle_dict[\'sa_count\'], new_agent._state_action_counts)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
recsim/agents/ucb1_agent_test.py,2,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.agents.ucb1_agent.""""""\n\nimport numpy as np\nfrom recsim.agents import cluster_bandit_agent\nfrom recsim.agents.layers import cluster_click_statistics\nfrom recsim.environments import interest_exploration as ie\nimport tensorflow.compat.v1 as tf\n\n\nclass UCB1AgentTest(tf.test.TestCase):\n\n  def test_step(self):\n    # Initialize agent.\n    env_config = {\n        \'slate_size\': 1,\n        \'num_candidates\': 5,\n        \'resample_documents\': True,\n        \'seed\': 1,\n    }\n    env = ie.create_environment(env_config)\n    kwargs = {\n        \'observation_space\': env.observation_space,\n        \'action_space\': env.action_space,\n    }\n    agent = cluster_click_statistics.ClusterClickStatsLayer(\n        cluster_bandit_agent.ClusterBanditAgent, **kwargs)\n\n    observation1, documents1 = env.environment.reset()\n    slate1 = agent.step(0,\n                        dict(user=observation1, doc=documents1, response=None))\n    # Pick the document with the best quality in Topic 0.\n    scores_c0 = [(features[\'quality\'] if features[\'cluster_id\'] == 0 else 0)\n                 for _, features in documents1.items()]\n    scores_c1 = [(features[\'quality\'] if features[\'cluster_id\'] == 1 else 0)\n                 for _, features in documents1.items()]\n    self.assertIn(slate1[0], [np.argmax(scores_c0), np.argmax(scores_c1)])\n    picked_cluster = list(documents1.values())[slate1[0]][\'cluster_id\']\n\n    observation2, documents, response1, _ = env.environment.step(slate1)\n    response1_obs = [response.create_observation() for response in response1]\n    response1_obs[0][\'cluster_id\'] = picked_cluster\n    slate2 = agent.step(\n        ie.total_clicks_reward(response1),\n        dict(user=observation2, doc=documents, response=response1_obs))\n    # Pick Topic 1 because we have no observation about it.\n    # Pick the document with the best quality there.\n    doc_qualities = [\n        (features[\'quality\'] if features[\'cluster_id\'] != picked_cluster else 0)\n        for _, features in documents.items()\n    ]\n    self.assertAllEqual(slate2, [\n        np.argmax(doc_qualities),\n    ])\n\n    self.assertNotEqual(\n        list(documents.values())[slate2[0]][\'cluster_id\'], picked_cluster)\n\n    observation3, documents, response2, _ = env.environment.step(slate2)\n    response2_obs = [response.create_observation() for response in response2]\n    # Make a clicked response.\n    response2_obs[0][\'click\'] = 1\n    response2_obs[0][\'cluster_id\'] = 1 - picked_cluster\n    slate3 = agent.step(\n        ie.total_clicks_reward(response2),\n        dict(user=observation3, doc=documents, response=response2_obs))\n    # Pick the first topic which has the best UCB and then pick the document\n    # with the best quality in it.\n    pulls = np.array([1, 1], dtype=np.float)\n    rewards = np.array([0, 0], dtype=np.float)\n    rewards[1 - picked_cluster] = 1\n    ct = np.sqrt(2.0 * np.log(2.0))\n    topic_index = rewards / pulls + ct * np.sqrt(1.0 / pulls)\n    doc_qualities = [(features[\'quality\'] if\n                      features[\'cluster_id\'] == np.argmax(topic_index) else 0)\n                     for _, features in documents.items()]\n    self.assertAllEqual(slate3, [np.argmax(doc_qualities)])\n\n    agent.end_episode(\n        ie.total_clicks_reward(response2),\n        dict(user=observation3, doc=documents, response=response2_obs))\n    slate4 = agent.step(0,\n                        dict(user=observation1, doc=documents1, response=None))\n    self.assertAllEqual(slate4, slate1)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
recsim/environments/__init__.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Module importing all environments.""""""\nfrom recsim.environments import interest_evolution\nfrom recsim.environments import interest_exploration\nfrom recsim.environments import long_term_satisfaction\n'"
recsim/environments/interest_evolution.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Classes to represent the interest evolution documents and users.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nfrom absl import logging\nimport gin.tf\nfrom gym import spaces\nimport numpy as np\nfrom recsim import choice_model\nfrom recsim import document\nfrom recsim import user\nfrom recsim import utils\nfrom recsim.simulator import environment\nfrom recsim.simulator import recsim_gym\n\nFLAGS = flags.FLAGS\n\n\nclass IEvResponse(user.AbstractResponse):\n  """"""Class to represent a user\'s response to a video.\n\n  Attributes:\n    clicked: A boolean indicating whether the video was clicked.\n    watch_time: A float for fraction of the video watched.\n    liked: A boolean indicating whether the video was liked.\n    quality: A float indicating the quality of the video.\n    cluster_id: A integer representing the cluster ID of the video.\n  """"""\n\n  # The min quality score.\n  MIN_QUALITY_SCORE = -100\n  # The max quality score.\n  MAX_QUALITY_SCORE = 100\n\n  def __init__(self,\n               clicked=False,\n               watch_time=0.0,\n               liked=False,\n               quality=0.0,\n               cluster_id=0.0):\n    """"""Creates a new user response for a video.\n\n    Args:\n      clicked: A boolean indicating whether the video was clicked\n      watch_time: A float for fraction of the video watched\n      liked: A boolean indicating whether the video was liked\n      quality: A float for document quality\n      cluster_id: a integer for the cluster ID of the document.\n    """"""\n    self.clicked = clicked\n    self.watch_time = watch_time\n    self.liked = liked\n    self.quality = quality\n    self.cluster_id = cluster_id\n\n  def create_observation(self):\n    return {\n        \'click\': int(self.clicked),\n        \'watch_time\': np.array(self.watch_time),\n        \'liked\': int(self.liked),\n        \'quality\': np.array(self.quality),\n        \'cluster_id\': int(self.cluster_id)\n    }\n\n  @classmethod\n  def response_space(cls):\n    # `clicked` feature range is [0, 1]\n    # `watch_time` feature range is [0, MAX_VIDEO_LENGTH]\n    # `liked` feature range is [0, 1]\n    # `quality`: the quality of the document and range is specified by\n    #    [MIN_QUALITY_SCORE, MAX_QUALITY_SCORE].\n    # `cluster_id`: the cluster the document belongs to and its range is\n    # .  [0, IEvVideo.NUM_FEATURES].\n    return spaces.Dict({\n        \'click\':\n            spaces.Discrete(2),\n        \'watch_time\':\n            spaces.Box(\n                low=0.0,\n                high=IEvVideo.MAX_VIDEO_LENGTH,\n                shape=tuple(),\n                dtype=np.float32),\n        \'liked\':\n            spaces.Discrete(2),\n        \'quality\':\n            spaces.Box(\n                low=cls.MIN_QUALITY_SCORE,\n                high=cls.MAX_QUALITY_SCORE,\n                shape=tuple(),\n                dtype=np.float32),\n        \'cluster_id\':\n            spaces.Discrete(IEvVideo.NUM_FEATURES)\n    })\n\n\nclass IEvVideo(document.AbstractDocument):\n  """"""Class to represent a interest evolution Video.\n\n  Attributes:\n    features: A numpy array that stores video features.\n    cluster_id: An integer that represents.\n    video_length : A float for video length.\n    quality: a float the represents document quality.\n  """"""\n\n  # The maximum length of videos.\n  MAX_VIDEO_LENGTH = 100.0\n\n  # The number of features to represent each video.\n  NUM_FEATURES = 20\n\n  def __init__(self,\n               doc_id,\n               features,\n               cluster_id=None,\n               video_length=None,\n               quality=None):\n    """"""Generates a random set of features for this interest evolution Video.""""""\n\n    # Document features (i.e. distribution over topics)\n    self.features = features\n\n    # Cluster ID\n    self.cluster_id = cluster_id\n\n    # Length of video\n    self.video_length = video_length\n\n    # Document quality (i.e. trashiness/nutritiousness)\n    self.quality = quality\n\n    # doc_id is an integer representing the unique ID of this document\n    super(IEvVideo, self).__init__(doc_id)\n\n  def create_observation(self):\n    """"""Returns observable properties of this document as a float array.""""""\n    return self.features\n\n  @classmethod\n  def observation_space(cls):\n    return spaces.Box(\n        shape=(cls.NUM_FEATURES,), dtype=np.float32, low=-1.0, high=1.0)\n\n\nclass IEvVideoSampler(document.AbstractDocumentSampler):\n  """"""Class to sample interest_evolution videos.""""""\n\n  def __init__(self,\n               doc_ctor=IEvVideo,\n               min_feature_value=-1.0,\n               max_feature_value=1.0,\n               video_length_mean=4.3,\n               video_length_std=1.0,\n               **kwargs):\n    """"""Creates a new interest evolution video sampler.\n\n    Args:\n      doc_ctor: A class/constructor for the type of videos that will be sampled\n        by this sampler.\n      min_feature_value: A float for the min feature value.\n      max_feature_value: A float for the max feature value.\n      video_length_mean: A float for the mean of the video length.\n      video_length_std: A float for the std deviation of video length.\n      **kwargs: other keyword parameters for the video sampler.\n    """"""\n    super(IEvVideoSampler, self).__init__(doc_ctor, **kwargs)\n    self._doc_count = 0\n    self._min_feature_value = min_feature_value\n    self._max_feature_value = max_feature_value\n    self._video_length_mean = video_length_mean\n    self._video_length_std = video_length_std\n\n  def sample_document(self):\n    doc_features = {}\n    doc_features[\'doc_id\'] = self._doc_count\n    # For now, assume the document properties are uniform random.\n    # It will probably make more sense to concentrate the interests around a few\n    # (e.g. 5?) categories or have a more sophisticated generative process?\n    doc_features[\'features\'] = self._rng.uniform(\n        self._min_feature_value, self._max_feature_value,\n        self.get_doc_ctor().NUM_FEATURES)\n    doc_features[\'video_length\'] = min(\n        self._rng.normal(self._video_length_mean, self._video_length_std),\n        self.get_doc_ctor().MAX_VIDEO_LENGTH)\n    doc_features[\'quality\'] = 1.0\n    self._doc_count += 1\n    return self._doc_ctor(**doc_features)\n\n\nclass UtilityModelVideoSampler(document.AbstractDocumentSampler):\n  """"""Class that samples videos for utility model experiment.""""""\n\n  def __init__(self,\n               doc_ctor=IEvVideo,\n               min_utility=-3.0,\n               max_utility=3.0,\n               video_length=4.0,\n               **kwargs):\n    """"""Creates a new utility model video sampler.\n\n    Args:\n      doc_ctor: A class/constructor for the type of videos that will be sampled\n        by this sampler.\n      min_utility: A float for the min utility score.\n      max_utility: A float for the max utility score.\n      video_length: A float for the video_length in minutes.\n      **kwargs: other keyword parameters for the video sampler.\n    """"""\n    super(UtilityModelVideoSampler, self).__init__(doc_ctor, **kwargs)\n    self._doc_count = 0\n    self._num_clusters = self.get_doc_ctor().NUM_FEATURES\n    self._min_utility = min_utility\n    self._max_utility = max_utility\n    self._video_length = video_length\n\n    # Linearly space utility according to cluster ID\n    # cluster 0 will get min_utility. cluster\n    # NUM_FEATURES - 1 will get max_utility\n    # In between will be spaced as follows\n    trashy = np.linspace(self._min_utility, 0, int(self._num_clusters * 0.7))\n    nutritious = np.linspace(0, self._max_utility,\n                             int(self._num_clusters * 0.3))\n    self.cluster_means = np.concatenate((trashy, nutritious))\n\n  def sample_document(self):\n    doc_features = {}\n    doc_features[\'doc_id\'] = self._doc_count\n\n    # Sample a cluster_id. Assumes there are NUM_FEATURE clusters.\n    cluster_id = self._rng.randint(0, self._num_clusters)\n    doc_features[\'cluster_id\'] = cluster_id\n\n    # Features are a 1-hot encoding of cluster id\n    features = np.zeros(self._num_clusters)\n    features[cluster_id] = 1.0\n    doc_features[\'features\'] = features\n\n    # Fixed video lengths (in minutes)\n    doc_features[\'video_length\'] = self._video_length\n\n    # Quality\n    quality_mean = self.cluster_means[cluster_id]\n\n    # Variance fixed\n    quality_variance = 0.1\n    doc_features[\'quality\'] = self._rng.normal(quality_mean, quality_variance)\n\n    self._doc_count += 1\n    return self._doc_ctor(**doc_features)\n\n\nclass IEvUserState(user.AbstractUserState):\n  """"""Class to represent interest evolution users.""""""\n\n  # Number of features in the user state representation.\n  NUM_FEATURES = 20\n\n  def __init__(self,\n               user_interests,\n               time_budget=None,\n               score_scaling=None,\n               attention_prob=None,\n               no_click_mass=None,\n               keep_interact_prob=None,\n               min_doc_utility=None,\n               user_update_alpha=None,\n               watched_videos=None,\n               impressed_videos=None,\n               liked_videos=None,\n               step_penalty=None,\n               min_normalizer=None,\n               user_quality_factor=None,\n               document_quality_factor=None):\n    """"""Initializes a new user.""""""\n\n    # Only user_interests is required, since it is needed to create an\n    # observation. It is the responsibility of the designer to make sure any\n    # other variables that are needed in the user choice/transition model are\n    # also provided.\n\n    ## User features\n    #######################\n\n    # The user\'s interests (1 = very interested, -1 = disgust)\n    # Another option could be to represent in [0,1] e.g. by dirichlet\n    self.user_interests = user_interests\n\n    # Amount of time in minutes this user has left in session.\n    self.time_budget = time_budget\n\n    # Probability of interacting with another element on the same slate\n    self.keep_interact_prob = keep_interact_prob\n\n    # Min utility to interact with a document\n    self.min_doc_utility = min_doc_utility\n\n    # Convenience wrapper\n    self.choice_features = {\n        \'score_scaling\': score_scaling,\n        # Factor of attention to give for subsequent items on slate\n        # Item i on a slate will get attention (attention_prob)^i\n        \'attention_prob\': attention_prob,\n        # Mass that user does not click on any item in the slate\n        \'no_click_mass\': no_click_mass,\n        # If using the multinomial proportion model with negative scores, this\n        # negative value will be subtracted from all scores to make a valid\n        # distribution for sampling.\n        \'min_normalizer\': min_normalizer\n    }\n\n    ## Transition model parameters\n    ##############################\n\n    # Step size for updating user interests based on watched videos (small!)\n    # We may want to have different values for different interests\n    # to represent how malleable those interests are (e.g. strong dislikes may\n    # be less malleable).\n    self.user_update_alpha = user_update_alpha\n\n    # A step penalty applied when no item is selected (e.g. the time wasted\n    # looking through a slate but not clicking, and any loss of interest)\n    self.step_penalty = step_penalty\n\n    # How much to weigh the user quality when updating budget\n    self.user_quality_factor = user_quality_factor\n    # How much to weigh the document quality when updating budget\n    self.document_quality_factor = document_quality_factor\n\n    # Observable user features (these are just examples for now)\n    ###########################\n\n    # Video IDs of videos that have been watched\n    self.watched_videos = watched_videos\n\n    # Video IDs of videos that have been impressed\n    self.impressed_videos = impressed_videos\n\n    # Video IDs of liked videos\n    self.liked_videos = liked_videos\n\n  def score_document(self, doc_obs):\n    if self.user_interests.shape != doc_obs.shape:\n      raise ValueError(\'User and document feature dimension mismatch!\')\n    return np.dot(self.user_interests, doc_obs)\n\n  def create_observation(self):\n    """"""Return an observation of this user\'s observable state.""""""\n    return self.user_interests\n\n  @classmethod\n  def observation_space(cls):\n    return spaces.Box(\n        shape=(cls.NUM_FEATURES,), dtype=np.float32, low=-1.0, high=1.0)\n\n\nclass IEvUserDistributionSampler(user.AbstractUserSampler):\n  """"""Class to sample users by a hardcoded distribution.""""""\n\n  def __init__(self, user_ctor=IEvUserState, **kwargs):\n    """"""Creates a new user state sampler.""""""\n    logging.debug(\'Initialized IEvUserDistributionSampler\')\n    super(IEvUserDistributionSampler, self).__init__(user_ctor, **kwargs)\n\n  def sample_user(self):\n    """"""Samples a new user, with a new set of features.""""""\n\n    features = {}\n    features[\'user_interests\'] = self._rng.uniform(\n        -1.0, 1.0,\n        self.get_user_ctor().NUM_FEATURES)\n    features[\'time_budget\'] = 30\n    features[\'score_scaling\'] = 0.05\n    features[\'attention_prob\'] = 0.9\n    features[\'no_click_mass\'] = 1\n    features[\'keep_interact_prob\'] = self._rng.beta(1, 3, 1)\n    features[\'min_doc_utility\'] = 0.1\n    features[\'user_update_alpha\'] = 0\n    features[\'watched_videos\'] = set()\n    features[\'impressed_videos\'] = set()\n    features[\'liked_videos\'] = set()\n    features[\'step_penalty\'] = 1.0\n    features[\'min_normalizer\'] = -1.0\n    features[\'user_quality_factor\'] = 1.0\n    features[\'document_quality_factor\'] = 1.0\n    return self._user_ctor(**features)\n\n\n@gin.configurable\nclass UtilityModelUserSampler(user.AbstractUserSampler):\n  """"""Class that samples users for utility model experiment.""""""\n\n  def __init__(self,\n               user_ctor=IEvUserState,\n               document_quality_factor=1.0,\n               no_click_mass=1.0,\n               min_normalizer=-1.0,\n               **kwargs):\n    """"""Creates a new user state sampler.""""""\n    logging.debug(\'Initialized UtilityModelUserSampler\')\n    self._no_click_mass = no_click_mass\n    self._min_normalizer = min_normalizer\n    self._document_quality_factor = document_quality_factor\n    super(UtilityModelUserSampler, self).__init__(user_ctor, **kwargs)\n\n  def sample_user(self):\n    features = {}\n    # Interests are distributed uniformly randomly\n    features[\'user_interests\'] = self._rng.uniform(\n        -1.0, 1.0,\n        self.get_user_ctor().NUM_FEATURES)\n    # Assume all users have fixed amount of time\n    features[\'time_budget\'] = 200.0  # 120.0\n    features[\'no_click_mass\'] = self._no_click_mass\n    features[\'step_penalty\'] = 0.5\n    features[\'score_scaling\'] = 0.05\n    features[\'attention_prob\'] = 0.65\n    features[\'min_normalizer\'] = self._min_normalizer\n    features[\'user_quality_factor\'] = 0.0\n    features[\'document_quality_factor\'] = self._document_quality_factor\n\n    # Fraction of video length we can extend (or cut) budget by\n    # Maybe this should be a parameter that varies by user?\n    alpha = 0.9\n    # In our setup, utility is just doc_quality as user_quality_factor is 0.\n    # doc_quality is distributed normally ~ N([-3,3], 0.1) for a 3 sigma range\n    # of [-3.3,3.3]. Therefore, we normalize doc_quality by 3.4 (adding a little\n    # extra in case) to get in [-1,1].\n    utility_range = 1.0 / 3.4\n    features[\'user_update_alpha\'] = alpha * utility_range\n    return self._user_ctor(**features)\n\n\nclass IEvUserModel(user.AbstractUserModel):\n  """"""Class to model an interest evolution user.\n\n  Assumes the user state contains:\n    - user_interests\n    - time_budget\n    - no_click_mass\n  """"""\n\n  def __init__(self,\n               slate_size,\n               choice_model_ctor=None,\n               response_model_ctor=IEvResponse,\n               user_state_ctor=IEvUserState,\n               no_click_mass=1.0,\n               seed=0,\n               alpha_x_intercept=1.0,\n               alpha_y_intercept=0.3):\n    """"""Initializes a new user model.\n\n    Args:\n      slate_size: An integer representing the size of the slate\n      choice_model_ctor: A contructor function to create user choice model.\n      response_model_ctor: A constructor function to create response. The\n        function should take a string of doc ID as input and returns a\n        IEvResponse object.\n      user_state_ctor: A constructor to create user state\n      no_click_mass: A float that will be passed to compute probability of no\n        click.\n      seed: A integer used as the seed of the choice model.\n      alpha_x_intercept: A float for the x intercept of the line used to compute\n        interests update factor.\n      alpha_y_intercept: A float for the y intercept of the line used to compute\n        interests update factor.\n\n    Raises:\n      Exception: if choice_model_ctor is not specified.\n    """"""\n    super(IEvUserModel, self).__init__(\n        response_model_ctor,\n        UtilityModelUserSampler(\n            user_ctor=user_state_ctor, no_click_mass=no_click_mass, seed=seed),\n        slate_size)\n    if choice_model_ctor is None:\n      raise Exception(\'A choice model needs to be specified!\')\n    self.choice_model = choice_model_ctor(self._user_state.choice_features)\n\n    self._alpha_x_intercept = alpha_x_intercept\n    self._alpha_y_intercept = alpha_y_intercept\n\n  def is_terminal(self):\n    """"""Returns a boolean indicating if the session is over.""""""\n    return self._user_state.time_budget <= 0\n\n  def update_state(self, slate_documents, responses):\n    """"""Updates the user state based on responses to the slate.\n\n    This function assumes only 1 response per slate. If a video is watched, we\n    update the user\'s interests some small step size alpha based on the\n    user\'s interest in that topic. The update is either towards the\n    video\'s features or away, and is determined stochastically by the user\'s\n    interest in that document.\n\n    Args:\n      slate_documents: a list of IEvVideos representing the slate\n      responses: a list of IEvResponses representing the user\'s response to each\n        video in the slate.\n    """"""\n\n    user_state = self._user_state\n\n    # Step size should vary based on interest.\n    def compute_alpha(x, x_intercept, y_intercept):\n      return (-y_intercept / x_intercept) * np.absolute(x) + y_intercept\n\n    for doc, response in zip(slate_documents, responses):\n      if response.clicked:\n        self.choice_model.score_documents(\n            user_state, [doc.create_observation()])\n        # scores is a list of length 1 since only one doc observation is set.\n        expected_utility = self.choice_model.scores[0]\n        ## Update interests\n        target = doc.features - user_state.user_interests\n        mask = doc.features\n        alpha = compute_alpha(user_state.user_interests,\n                              self._alpha_x_intercept, self._alpha_y_intercept)\n\n        update = alpha * mask * target\n        positive_update_prob = np.dot((user_state.user_interests + 1.0) / 2,\n                                      mask)\n        flip = np.random.rand(1)\n        if flip < positive_update_prob:\n          user_state.user_interests += update\n        else:\n          user_state.user_interests -= update\n        user_state.user_interests = np.clip(user_state.user_interests, -1.0,\n                                            1.0)\n        ## Update budget\n        received_utility = (\n            user_state.user_quality_factor * expected_utility) + (\n                user_state.document_quality_factor * doc.quality)\n        user_state.time_budget -= response.watch_time\n        user_state.time_budget += (\n            user_state.user_update_alpha * response.watch_time *\n            received_utility)\n        return\n\n    # Step penalty if no selection\n    user_state.time_budget -= user_state.step_penalty\n\n  def simulate_response(self, documents):\n    """"""Simulates the user\'s response to a slate of documents with choice model.\n\n    Args:\n      documents: a list of IEvVideo objects\n\n    Returns:\n      responses: a list of IEvResponse objects, one for each document\n    """"""\n    # List of empty responses\n    responses = [self._response_model_ctor() for _ in documents]\n\n    # Sample some clicked responses using user\'s choice model and populate\n    # responses.\n    doc_obs = [doc.create_observation() for doc in documents]\n    self.choice_model.score_documents(self._user_state, doc_obs)\n    selected_index = self.choice_model.choose_item()\n\n    for i, response in enumerate(responses):\n      response.quality = documents[i].quality\n      response.cluster_id = documents[i].cluster_id\n\n    if selected_index is None:\n      return responses\n    self._generate_click_response(documents[selected_index],\n                                  responses[selected_index])\n\n    return responses\n\n  def _generate_click_response(self, doc, response):\n    """"""Generates a response to a clicked document.\n\n    Right now we assume watch_time is a fixed value that is the minium value of\n    time_budget and video_length. In the future, we may want to try more\n    variations of watch_time definition.\n\n    Args:\n      doc: an IEvVideo object\n      response: am IEvResponse for the document\n    Updates: response, with whether the document was clicked, liked, and how\n      much of it was watched\n    """"""\n    user_state = self._user_state\n    response.clicked = True\n    response.watch_time = min(user_state.time_budget, doc.video_length)\n\n\ndef clicked_watchtime_reward(responses):\n  """"""Calculates the total clicked watchtime from a list of responses.\n\n  Args:\n    responses: A list of IEvResponse objects\n\n  Returns:\n    reward: A float representing the total watch time from the responses\n  """"""\n  reward = 0.0\n  for response in responses:\n    if response.clicked:\n      reward += response.watch_time\n  return reward\n\n\ndef total_clicks_reward(responses):\n  """"""Calculates the total number of clicks from a list of responses.\n\n  Args:\n     responses: A list of IEvResponse objects\n\n  Returns:\n    reward: A float representing the total clicks from the responses\n  """"""\n  reward = 0.0\n  for r in responses:\n    reward += r.clicked\n  return reward\n\n\ndef create_environment(env_config):\n  """"""Creates an interest evolution environment.""""""\n\n  user_model = IEvUserModel(\n      env_config[\'slate_size\'],\n      choice_model_ctor=choice_model.MultinomialProportionalChoiceModel,\n      response_model_ctor=IEvResponse,\n      user_state_ctor=IEvUserState,\n      seed=env_config[\'seed\'])\n\n  document_sampler = UtilityModelVideoSampler(\n      doc_ctor=IEvVideo, seed=env_config[\'seed\'])\n\n  ievenv = environment.Environment(\n      user_model,\n      document_sampler,\n      env_config[\'num_candidates\'],\n      env_config[\'slate_size\'],\n      resample_documents=env_config[\'resample_documents\'])\n\n  return recsim_gym.RecSimGymEnv(ievenv, clicked_watchtime_reward,\n                                 utils.aggregate_video_cluster_metrics,\n                                 utils.write_video_cluster_metrics)\n'"
recsim/environments/interest_evolution_test.py,2,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.environments.interest_evolution.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom recsim import choice_model\nfrom recsim.environments import interest_evolution\nimport tensorflow.compat.v1 as tf\n\n\nclass FakeChoiceModel(choice_model.AbstractChoiceModel):\n  """"""A fake choice model that returns fixed index and scores.""""""\n\n  def __init__(self, scores, score_no_click, select_index):\n    self._scores = scores\n    self._score_no_click = score_no_click\n    self._select_index = select_index\n\n  def score_documents(self, user_obs, doc_obs):\n    """"""Skip set user and docs features.""""""\n    pass\n\n  def choose_item(self):\n    return self._select_index\n\n\nclass InterestEvolutionTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(InterestEvolutionTest, self).setUp()\n    self._user_model = interest_evolution.IEvUserModel(\n        slate_size=1,\n        choice_model_ctor=choice_model.MultinomialLogitChoiceModel,\n        seed=0,\n        no_click_mass=0)\n    self._user_model.choice_model = FakeChoiceModel(\n        scores=[2], score_no_click=0, select_index=0)\n\n  def test_user_model_simulate_response(self):\n    """"""Test the simulate_response function.""""""\n    self._user_model._user_state.time_budget = 0.5\n    cluster_id = 3\n    quality = 2.0\n    document = interest_evolution.IEvVideo(\n        doc_id=1,\n        features=np.ones((20,)),\n        cluster_id=cluster_id,\n        video_length=1,\n        quality=quality)\n    responses = self._user_model.simulate_response([document])\n\n    self.assertEqual(responses[0].cluster_id, 3)\n    self.assertEqual(responses[0].quality, quality)\n    # The clicked is true since our choice model wil select the first document.\n    self.assertEqual(responses[0].clicked, True)\n    # The watch time is 0.5 because time_budget < video_length and\n    # time_budget = 0.5.\n    self.assertEqual(responses[0].watch_time, 0.5)\n\n  def test_user_model_update_state(self):\n    """"""Test the update_state function.""""""\n    self._user_model._user_state.time_budget = 0.5\n    self._user_model._user_state.user_quality_factor = 0.3\n    self._user_model._user_state.document_quality_factor = 0.7\n    self._user_model._user_state.user_update_alpha = 1\n\n    cluster_id = 3\n    quality = 3.0\n    document = interest_evolution.IEvVideo(\n        doc_id=1,\n        features=np.ones((20,)),\n        cluster_id=cluster_id,\n        video_length=1,\n        quality=quality)\n    response = interest_evolution.IEvResponse(clicked=True, watch_time=0.3)\n    self._user_model.update_state([document], [response])\n\n    # The expected_utility is 2 (the score of the choice model), the document\n    # quality is 3.0, the user_quality_factor = 0.3 and document_quality_factor\n    # = 0.7, so received_utility = 0.3 * 2 * 0.7 * 3 = 2.7. Besides,\n    # user_update_alpha = 1 so the final time budget is\n    # 0.5 - 0.3 + 1 * 0.3 * 2.7 = 1.01.\n    self.assertAlmostEqual(self._user_model._user_state.time_budget, 1.01)\n\n  def test_user_model_is_terminal(self):\n    # is_terminal is False is time_budget is > 0.\n    self._user_model._user_state.time_budget = 0.5\n    self.assertFalse(self._user_model.is_terminal())\n\n    # is_terminal is True is time_budget is > 0.\n    self._user_model._user_state.time_budget = 0\n    self.assertTrue(self._user_model.is_terminal())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
recsim/environments/interest_exploration.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Correlated interest exploration environment.\n\nThis environment models the problem of active exploration of user interests. It\nis meant to illustrate popularity bias in recommender systems, where myopic\nmaximization of engagement leads to bias towards documents that have wider\nappeal, whereas niche user interests remain unexplored.\n\nIn this setting, documents are generated from M topics (types) such that each\ndocument belongs to exactly one topic. Furthermore, there are N types (types)\nof users. Each document d has a production quality f_D(d) score drawn from a\ndistribution associated with its type D (e.g. more mass-appeal types tend\nto have higher production values). On the other hand, each user u has an\naffinity score g_U(u,d) towards each document type (drawn from a distribution\nassociated with the user\'s type). The final affinity of user u to document\nd is thus g_U(u,d) + f_D(d). When faced with a slate of documents, the user\nclicks on a document based on a multinomial logistic choice model with the\naffinity scores as parameters.\n\nA myopic agent will favor types with high production value, as they have\na high apriori probability of getting clicked across all user types. This leads\nthe agent to ignore niche interests, producing a suboptimal policy. This\nscenario can be seen as a correlated arms bandit problem.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nfrom absl import logging\nimport gin.tf\nfrom gym import spaces\nimport numpy as np\nfrom recsim import choice_model\nfrom recsim import document\nfrom recsim import user\nfrom recsim import utils\nfrom recsim.simulator import environment\nfrom recsim.simulator import recsim_gym\n\nFLAGS = flags.FLAGS\n\n\nclass IEUserModel(user.AbstractUserModel):\n  """"""Class to model a user.\n\n  The user in this scenario is completely characterized by a vector g\n  of affinity scores of dimension |D| (the number of document topics). When\n  presented with a slate of documents, the user scores each document as g(d) +\n  f(d), where f(d) is the document\'s quality score, and then chooses according\n  to a choice model based on these scores.\n\n  The state space consists of a vector of affinity scores which is unique to the\n  user and static but not observable.\n\n  Args:\n  slate_size: An integer representing the size of the slate.\n  no_click_mass: A float indicating the mass given to a no-click option.\n    Must be positive, otherwise CTR is always 1.\n  choice_model_ctor: A contructor function to create user choice model.\n  user_state_ctor: A constructor to create user state.\n  response_model_ctor: A constructor function to create response. The\n    function should take a string of doc ID as input and returns a\n    IEResponse object.\n  seed: an integer used as the seed in random sampling.\n  """"""\n\n  def __init__(self,\n               slate_size,\n               no_click_mass=5,\n               choice_model_ctor=choice_model.MultinomialLogitChoiceModel,\n               user_state_ctor=None,\n               response_model_ctor=None,\n               seed=0):\n    if no_click_mass < 0:\n      raise ValueError(\'no_click_mass must be positive.\')\n\n    super(IEUserModel, self).__init__(response_model_ctor, IEClusterUserSampler(\n        user_ctor=user_state_ctor, seed=seed), slate_size)\n    self._user_state_ctor = user_state_ctor\n    if choice_model_ctor is None:\n      raise Exception(\'A choice model needs to be specified!\')\n    self.choice_model = choice_model_ctor({\n        \'no_click_mass\': no_click_mass,\n    })\n\n  @property\n  def avg_user_state(self):\n    """"""Returns the prior of user state.""""""\n    return self._user_state_ctor(self._user_sampler.avg_affinity_given_topic())\n\n  def is_terminal(self):\n    """"""Returns a boolean indicating if the session is over.""""""\n    return False\n\n  # No state transitions.\n  def update_state(self, slate_documents, responses):\n    del slate_documents  # Unused\n    del responses  # Unused\n    return\n\n  def simulate_response(self, documents):\n    """"""Simulates the user\'s response to a slate of documents with choice model.\n\n    Args:\n      documents: a list of IEDocument objects in the slate.\n\n    Returns:\n      responses: a list of IEResponse objects, one for each document.\n    """"""\n    # List of empty responses\n    responses = [self._response_model_ctor() for _ in documents]\n\n    # Sample the response for selected slate items.\n\n    self.choice_model.score_documents(\n        self._user_state, [doc.create_observation() for doc in documents])\n    selected_index = self.choice_model.choose_item()\n    for i, response in enumerate(responses):\n      response.quality = documents[i].quality\n      response.cluster_id = documents[i].cluster_id\n    if selected_index is None:\n      return responses\n    self._generate_response(documents[selected_index],\n                            responses[selected_index])\n    return responses\n\n  def _generate_response(self, doc, response):\n    """"""Trivial response: sets the clicked property of a clicked document.\n\n    Args:\n      doc: a IEDocument object.\n      response: a IEResponse for the document.\n    Updates: response, with whether the document was clicked.\n    """"""\n    response.clicked = True\n\n\nclass IEUserState(user.AbstractUserState):\n  """"""Class to represent users.\n\n  Attributes:\n    topic_affinity: a nonnegative vector holds document type affinities which\n      are not temporal dynamics and hidden.\n  """"""\n\n  def __init__(self, topic_affinity):\n    """"""Initializes a new user.""""""\n    self.topic_affinity = topic_affinity\n\n  def score_document(self, doc_obs):\n    """"""Returns user document affinity plus document quality.""""""\n    return self.topic_affinity[doc_obs[\'cluster_id\']] + doc_obs[\'quality\']\n\n  def create_observation(self):\n    """"""User\'s topic_affinity is not observable.""""""\n    return np.array([])\n\n  @staticmethod\n  def observation_space():\n    return spaces.Box(shape=(0,), dtype=np.float32, low=0.0, high=np.inf)\n\n\n@gin.configurable\nclass IEClusterUserSampler(user.AbstractUserSampler):\n  """"""Samples users from predetermined types with type-specific parameters.\n\n    This sampler consumes a distribution over user types and type-specific\n    parameters for the user\'s affinity towards content types. It first samples\n    a user type, then using that user type generates affinities according to\n    the type-specific parameters. In this case, these are the mean and scale\n    of a lognormal distribution, i.e. the affinity of user u of type U towards\n    an document of type D is drawn according to\n    lognormal(mean(U,D), scale(U,D)).\n\n    Args:\n      user_type_distribution: a non-negative array of dimension equal to the\n        number of user types, whose entries sum to one.\n      user_document_mean_affinity_matrix: a non-negative two-dimensional array\n        with dimensions number of user types by number of document topics.\n        Represents the mean of the affinity score of a user type to a topic.\n      user_document_stddev_affinity_matrix: a non-negative two-dimensional array\n        with dimensions number of user types by number of document topics.\n        Represents the scale of the affinity score of a user type to a topic.\n      user_ctor: constructor for a user state.\n  """"""\n\n  def __init__(self,\n               user_type_distribution=(0.3, 0.7),\n               user_document_mean_affinity_matrix=((.1, .7), (.7, .1)),\n               user_document_stddev_affinity_matrix=((.1, .1), (.1, .1)),\n               user_ctor=IEUserState,\n               **kwargs):\n    self._number_of_user_types = len(user_type_distribution)\n    self._user_type_dist = user_type_distribution\n    if len(user_document_mean_affinity_matrix) != len(user_type_distribution):\n      raise ValueError(\'The dimensions of user_type_distribution and \'\n                       \'user_document_mean_affinity_matrix do not match.\')\n    if len(user_document_stddev_affinity_matrix) != len(user_type_distribution):\n      raise ValueError(\'The dimensions of user_type_distribution and \'\n                       \'user_document_stddev_affinity_matrix do not match.\')\n    self._user_doc_means = user_document_mean_affinity_matrix\n    self._user_doc_stddev = user_document_stddev_affinity_matrix\n    logging.debug(\'Initialized IEClusterUserSampler\')\n    super(IEClusterUserSampler, self).__init__(user_ctor, **kwargs)\n\n  def sample_user(self):\n    # 1. Pick user type.\n    user_type = self._rng.choice(\n        self._number_of_user_types, p=self._user_type_dist)\n    # 2. Sample user-document affinity given type.\n    user_doc_affinity = (\n        self._rng.lognormal(\n            mean=self._user_doc_means[user_type],\n            sigma=self._user_doc_stddev[user_type]))\n    return self._user_ctor(user_doc_affinity)\n\n  def avg_affinity_given_topic(self):\n    # Returns the prior of document affinity.\n    return np.matmul(self._user_type_dist, self._user_doc_means)\n\n\nclass IEResponse(user.AbstractResponse):\n  """"""Class to represent a user\'s response to a document.\n\n  Attributes:\n    clicked: boolean indicating whether the item was clicked or not.\n    quality: a float indicating the quality of the document.\n    cluster_id: an integer representing the topic ID of the document.\n  """"""\n\n  NUM_CLUSTERS = 0\n\n  def __init__(self,\n               clicked=False,\n               quality=0.0,\n               cluster_id=0):\n    self.clicked = clicked\n    self.quality = quality\n    self.cluster_id = cluster_id\n\n  def __str__(self):\n    return str(self.clicked)\n\n  def __repr__(self):\n    return self.__str__()\n\n  def create_observation(self):\n    return {\n        \'click\': int(self.clicked),\n        \'quality\': np.array(self.quality),\n        \'cluster_id\': self.cluster_id\n    }\n\n  @classmethod\n  def response_space(cls):\n    return spaces.Dict({\n        \'click\':\n            spaces.Discrete(2),\n        \'quality\':\n            spaces.Box(\n                low=0.0, high=np.inf, shape=tuple(), dtype=np.float32),\n        \'cluster_id\':\n            spaces.Discrete(cls.NUM_CLUSTERS)\n    })\n\n\nclass IEDocument(document.AbstractDocument):\n  """"""Class to represent an IE Document.\n\n  Attributes:\n    cluster_id: an integer representing the document cluster.\n    quality: non-negative real number representing the quality of the document.\n  """"""\n\n  NUM_CLUSTERS = 0\n\n  def __init__(self, doc_id, cluster_id, quality):\n    self.cluster_id = cluster_id\n    self.quality = quality\n    # doc_id is an integer representing the unique ID of this document\n    super(IEDocument, self).__init__(doc_id)\n\n  def create_observation(self):\n    return {\'quality\': np.array(self.quality), \'cluster_id\': self.cluster_id}\n\n  @classmethod\n  def observation_space(cls):\n    return spaces.Dict({\n        \'quality\':\n            spaces.Box(\n                low=0.0, high=np.inf, shape=tuple(), dtype=np.float32),\n        \'cluster_id\':\n            spaces.Discrete(cls.NUM_CLUSTERS)\n    })\n\n\n@gin.configurable\nclass IETopicDocumentSampler(document.AbstractDocumentSampler):\n  """"""Samples documents with topic-specific quality distribution.\n\n     Consumes a distribution over document topics and topic-specific parameters\n     for generating a quality score (according to a lognormal distribution).\n\n     Args:\n       topic_distribution: a non-negative array of dimension equal to the\n         number of topics, whose entries sum to one.\n       topic_quality_mean: a non-negative array of dimension equal to the\n         number of topics, representing the mean of the topic quality score.\n       topic_quality_stddev: a non-negative array of dimension equal to the\n         number of topics, representing the scale of the topic quality score.\n       doc_ctor: A class/constructor for the type of videos that will be sampled\n        by this sampler.\n  """"""\n\n  def __init__(self,\n               topic_distribution=(.2, .8),\n               topic_quality_mean=(.8, .2),\n               topic_quality_stddev=(.1, .1),\n               doc_ctor=IEDocument,\n               **kwargs):\n    self._number_of_topics = len(topic_distribution)\n    self._topic_dist = topic_distribution\n    if len(topic_quality_mean) != len(topic_distribution):\n      raise ValueError(\'The dimensions of topic_quality_mean and \'\n                       \'topic_distribution do not match.\')\n    if len(topic_quality_stddev) != len(topic_distribution):\n      raise ValueError(\'The dimensions of topic_quality_stddev and \'\n                       \'topic_distribution do not match.\')\n    self._topic_quality_mean = topic_quality_mean\n    self._topic_quality_stddev = topic_quality_stddev\n    super(IETopicDocumentSampler, self).__init__(doc_ctor, **kwargs)\n    self._doc_count = 0\n\n  @property\n  def num_clusters(self):\n    return self._number_of_topics\n\n  def sample_document(self):\n    """"""Samples the topic and then samples document features given the topic.""""""\n    doc_features = {}\n    doc_features[\'doc_id\'] = self._doc_count\n    self._doc_count += 1\n    topic_id = self._rng.choice(self._number_of_topics, p=self._topic_dist)\n    doc_quality = (\n        self._rng.lognormal(\n            mean=self._topic_quality_mean[topic_id],\n            sigma=self._topic_quality_stddev[topic_id]))\n    doc_features[\'cluster_id\'] = topic_id\n    doc_features[\'quality\'] = doc_quality\n    return self._doc_ctor(**doc_features)\n\n\ndef total_clicks_reward(responses):\n  """"""Calculates the total number of clicks from a list of responses.\n\n  Args:\n     responses: A list of IEResponse objects\n\n  Returns:\n    reward: A float representing the total clicks from the responses\n  """"""\n  reward = 0.0\n  for r in responses:\n    reward += r.clicked\n  return reward\n\n\ndef create_environment(env_config):\n  """"""Creates an interest exploration environment.""""""\n\n  document_sampler = IETopicDocumentSampler(seed=env_config[\'seed\'])\n  IEDocument.NUM_CLUSTERS = document_sampler.num_clusters\n  IEResponse.NUM_CLUSTERS = document_sampler.num_clusters\n\n  user_model = IEUserModel(\n      env_config[\'slate_size\'],\n      user_state_ctor=IEUserState,\n      response_model_ctor=IEResponse,\n      seed=env_config[\'seed\'])\n\n  ieenv = environment.Environment(\n      user_model,\n      document_sampler,\n      env_config[\'num_candidates\'],\n      env_config[\'slate_size\'],\n      resample_documents=env_config[\'resample_documents\'])\n\n  return recsim_gym.RecSimGymEnv(ieenv, total_clicks_reward,\n                                 utils.aggregate_video_cluster_metrics,\n                                 utils.write_video_cluster_metrics)\n'"
recsim/environments/interest_exploration_test.py,2,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.environments.interest_exploration.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom recsim.environments import interest_exploration\nimport tensorflow.compat.v1 as tf\n\n\nclass InterestExplorationTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(InterestExplorationTest, self).setUp()\n    self._num_topics = 2\n    env_config = {\n        \'num_candidates\': 20,\n        \'slate_size\': 2,\n        \'resample_documents\': False,\n        \'seed\': 1,\n    }\n    self._env = interest_exploration.create_environment(env_config)\n\n  def test_step(self):\n    self._env.seed(0)\n    obs0 = self._env.reset()\n    self.assertEqual((0,), obs0[\'user\'].shape)\n    slate = np.array([0, 1])\n    obs, reward, done, _ = self._env.step(slate)\n    doc_obs0 = list(obs0[\'doc\'].values())\n    doc_obs = list(obs[\'doc\'].values())\n    for i, resp in enumerate(obs[\'response\']):\n      self.assertFalse(resp[\'click\'])\n      self.assertEqual(doc_obs0[i][\'cluster_id\'], resp[\'cluster_id\'])\n      self.assertEqual(doc_obs[i][\'cluster_id\'], resp[\'cluster_id\'])\n    self.assertEqual(0, reward)\n    self.assertFalse(done)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
recsim/environments/long_term_satisfaction.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Long term satisfaction (Choc/Kale) environment.\n\nThis environment depicts a situation in which a user of an online service\ninteracts with items of content, which are characterized by their level of\nclickbaitiness (on a scale of 0 to 1). In particular, clickbaity items (choc)\ngenerate engagement, but lead to decrease in long-term satisfaction.\nNon-clickbaity items (kale) increase satisfaction but do not generate as much\nengagement. The challenge is to balance the two in order to achieve some long-\nterm optimal trade-off.\nThe dynamics of this system are partially observable, as satisfaction is a\nlatent variable. It has to be inferred through the increase/decrease in\nengagement.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nfrom absl import logging\nimport gin.tf\nfrom gym import spaces\nimport numpy as np\nfrom recsim import document\nfrom recsim import user\nfrom recsim.simulator import environment\nfrom recsim.simulator import recsim_gym\n\nFLAGS = flags.FLAGS\n\n\nclass LTSUserModel(user.AbstractUserModel):\n  """"""Class to model a user with long-term satisfaction dynamics.\n\n  Implements a controlled continuous Hidden Markov Model of the user having\n  the following components.\n    * State space: one dimensional real number, termed net_positive_exposure\n      (abbreviated NPE);\n    * controls: one dimensional control signal in [0, 1], representing the\n      clickbait score of the item of content;\n    * transition dynamics: net_positive_exposure is updated according to:\n      NPE_(t+1) := memory_discount * NPE_t\n                   + 2 * (clickbait_score - .5)\n                   + N(0, innovation_stddev);\n    * observation space: a nonnegative real number, representing the degree of\n      engagement, e.g. econds watched from a recommended video. An observation\n      is drawn from a log-normal distribution with mean\n\n      (clickbait_score * choc_mean\n                      + (1 - clickbait_score) * kale_mean) * SAT_t,\n\n      where SAT_t = sigmoid(sensitivity * NPE_t). The observation standard\n      standard deviation is similarly given by\n\n      (clickbait_score * choc_stddev + ((1 - clickbait_score) * kale_stddev)).\n\n      An individual user is thus represented by the combination of parameters\n      (memory_discount, innovation_stddev, choc_mean, choc_stddev, kale_mean,\n      kale_stddev, sensitivity), which are encapsulated in LTSUserState.\n\n    Args:\n      slate_size: An integer representing the size of the slate\n      user_state_ctor: A constructor to create user state.\n      response_model_ctor: A constructor function to create response. The\n        function should take a string of doc ID as input and returns a\n        LTSResponse object.\n      seed: an integer as the seed in random sampling.\n  """"""\n\n  def __init__(self,\n               slate_size,\n               user_state_ctor=None,\n               response_model_ctor=None,\n               seed=0):\n    if not response_model_ctor:\n      raise TypeError(\'response_model_ctor is a required callable.\')\n\n    super(LTSUserModel, self).__init__(\n        response_model_ctor,\n        LTSStaticUserSampler(user_ctor=user_state_ctor, seed=seed), slate_size)\n\n  def is_terminal(self):\n    """"""Returns a boolean indicating if the session is over.""""""\n    return self._user_state.time_budget <= 0\n\n  def update_state(self, slate_documents, responses):\n    """"""Updates the user\'s latent state based on responses to the slate.\n\n    Args:\n      slate_documents: a list of LTSDocuments representing the slate\n      responses: a list of LTSResponses representing the user\'s response to each\n        document in the slate.\n    """"""\n\n    for doc, response in zip(slate_documents, responses):\n      if response.clicked:\n        innovation = np.random.normal(scale=self._user_state.innovation_stddev)\n        net_positive_exposure = (self._user_state.memory_discount\n                                 * self._user_state.net_positive_exposure\n                                 - 2.0 * (doc.clickbait_score - 0.5)\n                                 + innovation\n                                )\n        self._user_state.net_positive_exposure = net_positive_exposure\n        satisfaction = 1 / (1.0 + np.exp(-self._user_state.sensitivity\n                                         * net_positive_exposure)\n                           )\n        self._user_state.satisfaction = satisfaction\n        self._user_state.time_budget -= 1\n        return\n\n  def simulate_response(self, documents):\n    """"""Simulates the user\'s response to a slate of documents with choice model.\n\n    Args:\n      documents: a list of LTSDocument objects.\n\n    Returns:\n      responses: a list of LTSResponse objects, one for each document.\n    """"""\n    # List of empty responses\n    responses = [self._response_model_ctor() for _ in documents]\n    # User always clicks the first item.\n    selected_index = 0\n    self.generate_response(documents[selected_index], responses[selected_index])\n    return responses\n\n  def generate_response(self, doc, response):\n    """"""Generates a response to a clicked document.\n\n    Args:\n      doc: an LTSDocument object.\n      response: an LTSResponse for the document.\n    Updates: response, with whether the document was clicked, liked, and how\n      much of it was watched.\n    """"""\n    response.clicked = True\n    # linear interpolation between choc and kale.\n    engagement_loc = (doc.clickbait_score * self._user_state.choc_mean\n                      + (1 - doc.clickbait_score) * self._user_state.kale_mean)\n    engagement_loc *= self._user_state.satisfaction\n    engagement_scale = (doc.clickbait_score * self._user_state.choc_stddev\n                        + ((1 - doc.clickbait_score)\n                           * self._user_state.kale_stddev))\n    log_engagement = np.random.normal(loc=engagement_loc,\n                                      scale=engagement_scale)\n    response.engagement = np.exp(log_engagement)\n\n\nclass LTSUserState(user.AbstractUserState):\n  """"""Class to represent users.\n\n  See the LTSUserModel class documentation for precise information about how the\n  parameters influence user dynamics.\n  Attributes:\n    memory_discount: rate of forgetting of latent state.\n    sensitivity: magnitude of the dependence between latent state and\n      engagement.\n    innovation_stddev: noise standard deviation in latent state transitions.\n    choc_mean: mean of engagement with clickbaity content.\n    choc_stddev: standard deviation of engagement with clickbaity content.\n    kale_mean: mean of engagement with non-clickbaity content.\n    kale_stddev: standard deviation of engagement with non-clickbaity content.\n    net_positive_exposure: starting value for NPE (NPE_0).\n    time_budget: length of a user session.\n  """"""\n\n  def __init__(self, memory_discount, sensitivity, innovation_stddev,\n               choc_mean, choc_stddev, kale_mean, kale_stddev,\n               net_positive_exposure, time_budget\n              ):\n    """"""Initializes a new user.""""""\n    ## Transition model parameters\n    ##############################\n    self.memory_discount = memory_discount\n    self.sensitivity = sensitivity\n    self.innovation_stddev = innovation_stddev\n\n    ## Engagement parameters\n    self.choc_mean = choc_mean\n    self.choc_stddev = choc_stddev\n    self.kale_mean = kale_mean\n    self.kale_stddev = kale_stddev\n\n    ## State variables\n    ##############################\n    self.net_positive_exposure = net_positive_exposure\n    self.satisfaction = 1 / (1 + np.exp(-sensitivity * net_positive_exposure))\n    self.time_budget = time_budget\n\n  def create_observation(self):\n    """"""User\'s state is not observable.""""""\n    return np.array([])\n\n  # No choice model.\n  def score_document(self, doc_obs):\n    return 1\n\n  @staticmethod\n  def observation_space():\n    return spaces.Box(shape=(0,), dtype=np.float32, low=0.0, high=np.inf)\n\n\n@gin.configurable\nclass LTSStaticUserSampler(user.AbstractUserSampler):\n  """"""Generates user with identical predetermined parameters.""""""\n  _state_parameters = None\n\n  def __init__(self,\n               user_ctor=LTSUserState,\n               memory_discount=0.7,\n               sensitivity=0.01,\n               innovation_stddev=0.05,\n               choc_mean=5.0,\n               choc_stddev=1.0,\n               kale_mean=4.0,\n               kale_stddev=1.0,\n               time_budget=60,\n               **kwargs):\n    """"""Creates a new user state sampler.""""""\n    logging.debug(\'Initialized LTSStaticUserSampler\')\n    self._state_parameters = {\'memory_discount\': memory_discount,\n                              \'sensitivity\': sensitivity,\n                              \'innovation_stddev\': innovation_stddev,\n                              \'choc_mean\': choc_mean,\n                              \'choc_stddev\': choc_stddev,\n                              \'kale_mean\': kale_mean,\n                              \'kale_stddev\': kale_stddev,\n                              \'time_budget\': time_budget\n                             }\n    super(LTSStaticUserSampler, self).__init__(user_ctor, **kwargs)\n\n  def sample_user(self):\n    starting_npe = ((self._rng.random_sample() - .5) *\n                    (1 / (1.0 - self._state_parameters[\'memory_discount\'])))\n    self._state_parameters[\'net_positive_exposure\'] = starting_npe\n    return self._user_ctor(**self._state_parameters)\n\n\nclass LTSResponse(user.AbstractResponse):\n  """"""Class to represent a user\'s response to a document.\n\n  Attributes:\n    engagement: real number representing the degree of engagement with a\n      document (e.g. watch time).\n    clicked: boolean indicating whether the item was clicked or not.\n  """"""\n\n  # The maximum degree of engagement.\n  MAX_ENGAGEMENT_MAGNITUDE = 100.0\n\n  def __init__(self, clicked=False, engagement=0.0):\n    """"""Creates a new user response for a document.\n\n    Args:\n      clicked: boolean indicating whether the item was clicked or not.\n      engagement: real number representing the degree of engagement with a\n        document (e.g. watch time).\n    """"""\n    self.clicked = clicked\n    self.engagement = engagement\n\n  def __str__(self):\n    return \'[\' + self.engagement + \']\'\n\n  def __repr__(self):\n    return self.__str__()\n\n  def create_observation(self):\n    return {\n        \'click\':\n            int(self.clicked),\n        \'engagement\':\n            np.clip(self.engagement, 0, LTSResponse.MAX_ENGAGEMENT_MAGNITUDE)\n    }\n\n  @classmethod\n  def response_space(cls):\n    # `engagement` feature range is [0, MAX_ENGAGEMENT_MAGNITUDE]\n    return spaces.Dict({\n        \'click\':\n            spaces.Discrete(2),\n        \'engagement\':\n            spaces.Box(\n                low=0.0,\n                high=LTSResponse.MAX_ENGAGEMENT_MAGNITUDE,\n                shape=tuple(),\n                dtype=np.float32)\n    })\n\n\nclass LTSDocument(document.AbstractDocument):\n  """"""Class to represent an LTS Document.\n\n  Attributes:\n    clickbait_score: real number in [0,1] representing the clickbaitiness of a\n      document.\n  """"""\n\n  def __init__(self, doc_id, clickbait_score):\n    self.clickbait_score = clickbait_score\n    # doc_id is an integer representing the unique ID of this document\n    super(LTSDocument, self).__init__(doc_id)\n\n  def create_observation(self):\n    return np.array([self.clickbait_score])\n\n  @staticmethod\n  def observation_space():\n    return spaces.Box(shape=(1,), dtype=np.float32, low=0.0, high=1.0)\n\n\nclass LTSDocumentSampler(document.AbstractDocumentSampler):\n  """"""Class to sample LTSDocument documents.\n\n    Args:\n    doc_ctor: A class/constructor for the type of documents that will be sampled\n      by this sampler.\n  """"""\n\n  def __init__(self, doc_ctor=LTSDocument, **kwargs):\n    super(LTSDocumentSampler, self).__init__(doc_ctor, **kwargs)\n    self._doc_count = 0\n\n  def sample_document(self):\n    doc_features = {}\n    doc_features[\'doc_id\'] = self._doc_count\n    doc_features[\'clickbait_score\'] = self._rng.random_sample()\n    self._doc_count += 1\n    return self._doc_ctor(**doc_features)\n\n\ndef clicked_engagement_reward(responses):\n  """"""Calculates the total clicked watchtime from a list of responses.\n\n  Args:\n    responses: A list of LTSResponse objects\n\n  Returns:\n    reward: A float representing the total watch time from the responses\n  """"""\n  reward = 0.0\n  for response in responses:\n    if response.clicked:\n      reward += response.engagement\n  return reward\n\n\ndef create_environment(env_config):\n  """"""Creates a long-term satisfaction environment.""""""\n\n  user_model = LTSUserModel(\n      env_config[\'slate_size\'],\n      user_state_ctor=LTSUserState,\n      response_model_ctor=LTSResponse)\n\n  document_sampler = LTSDocumentSampler()\n\n  ltsenv = environment.Environment(\n      user_model,\n      document_sampler,\n      env_config[\'num_candidates\'],\n      env_config[\'slate_size\'],\n      resample_documents=env_config[\'resample_documents\'])\n\n  return recsim_gym.RecSimGymEnv(ltsenv, clicked_engagement_reward)\n'"
recsim/simulator/__init__.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Module of the simulator.""""""\nfrom recsim.simulator import environment\nfrom recsim.simulator import recsim_gym\nfrom recsim.simulator import runner_lib\n'"
recsim/simulator/environment.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Class to represent the environment in the recommender system setting.\n\n   Thus, it models things such as (1) the user\'s state, for example his/her\n   interests and circumstances, (2) the documents available to suggest from and\n   their properties, (3) simulates the selection of an item in the slate (or a\n   no-op/quit), and (4) models the change in a user\'s state based on the slate\n   presented and the document selected.\n\n   The agent interacting with the environment is the recommender system.  The\n   agent receives the state, which is an observation of the user\'s state and\n   observations of the candidate documents. The agent then provides an action,\n   which is a slate (an array of indices into the candidate set).\n\n   The goal of the agent is to learn a recommendation policy: a policy that\n   serves the user a slate (action) based on user and document features (state)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport collections\nimport itertools\n\nfrom recsim import document\nimport six\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass AbstractEnvironment(object):\n  """"""Abstract class representing the recommender system environment.\n\n  Attributes:\n    user_model: An list or single instantiation of AbstractUserModel\n      representing the user/users.\n    document_sampler: An instantiation of AbstractDocumentSampler.\n    num_candidates: An integer representing the size of the candidate_set.\n    slate_size: An integer representing the slate size.\n    candidate_set: An instantiation of CandidateSet.\n    num_clusters: An integer representing the number of document clusters.\n  """"""\n\n  def __init__(self,\n               user_model,\n               document_sampler,\n               num_candidates,\n               slate_size,\n               resample_documents=True):\n    """"""Initializes a new simulation environment.\n\n    Args:\n      user_model: An instantiation of AbstractUserModel or list of such\n        instantiations\n      document_sampler: An instantiation of AbstractDocumentSampler\n      num_candidates: An integer representing the size of the candidate_set\n      slate_size: An integer representing the slate size\n      resample_documents: A boolean indicating whether to resample the candidate\n        set every step\n    """"""\n    self._user_model = user_model\n    self._document_sampler = document_sampler\n    self._slate_size = slate_size\n    self._num_candidates = num_candidates\n    self._resample_documents = resample_documents\n\n    # Create a candidate set.\n    self._do_resample_documents()\n    assert (slate_size <= num_candidates\n           ), \'Slate size %d cannot be larger than number of candidates %d\' % (\n               slate_size, num_candidates)\n\n  def _do_resample_documents(self):\n    # TODO(sanmit): eventually model this creation with content creators.\n    self._candidate_set = document.CandidateSet()\n    for _ in range(self._num_candidates):\n      self._candidate_set.add_document(self._document_sampler.sample_document())\n\n  @abc.abstractmethod\n  def reset(self):\n    """"""Resets the environment and return the first observation.\n\n    Returns:\n      user_obs: An array of floats representing observations of the user\'s\n        current state\n      doc_obs: An OrderedDict of document observations keyed by document ids\n    """"""\n\n  @abc.abstractmethod\n  def reset_sampler(self):\n    """"""Resets the relevant samplers of documents and user/users.""""""\n\n  @property\n  def num_candidates(self):\n    return self._num_candidates\n\n  @property\n  def slate_size(self):\n    return self._slate_size\n\n  @property\n  def candidate_set(self):\n    return self._candidate_set\n\n  @property\n  def user_model(self):\n    return self._user_model\n\n  @abc.abstractmethod\n  def step(self, slate):\n    """"""Executes the action, returns next state observation and reward.\n\n    Args:\n      slate: An integer array of size slate_size (or list of such arrays), where\n      each element is an index into the set of current_documents presented.\n\n    Returns:\n      user_obs: A gym observation representing the user\'s next state\n      doc_obs: A list of observations of the documents\n      responses: A list of AbstractResponse objects for each item in the slate\n      done: A boolean indicating whether the episode has terminated\n    """"""\n\n\nclass SingleUserEnvironment(AbstractEnvironment):\n  """"""Class to represent the environment with one user.\n\n  Attributes:\n    user_model: An instantiation of AbstractUserModel that represents a user.\n    document_sampler: An instantiation of AbstractDocumentSampler.\n    num_candidates: An integer representing the size of the candidate_set.\n    slate_size: An integer representing the slate size.\n    candidate_set: An instantiation of CandidateSet.\n    num_clusters: An integer representing the number of document clusters.\n  """"""\n\n  def reset(self):\n    """"""Resets the environment and return the first observation.\n\n    Returns:\n      user_obs: An array of floats representing observations of the user\'s\n        current state\n      doc_obs: An OrderedDict of document observations keyed by document ids\n    """"""\n    self._user_model.reset()\n    user_obs = self._user_model.create_observation()\n    if self._resample_documents:\n      self._do_resample_documents()\n    self._current_documents = collections.OrderedDict(\n        self._candidate_set.create_observation())\n    return (user_obs, self._current_documents)\n\n  def reset_sampler(self):\n    """"""Resets the relevant samplers of documents and user/users.""""""\n    self._document_sampler.reset_sampler()\n    self._user_model.reset_sampler()\n\n  def step(self, slate):\n    """"""Executes the action, returns next state observation and reward.\n\n    Args:\n      slate: An integer array of size slate_size, where each element is an index\n        into the set of current_documents presented\n\n    Returns:\n      user_obs: A gym observation representing the user\'s next state\n      doc_obs: A list of observations of the documents\n      responses: A list of AbstractResponse objects for each item in the slate\n      done: A boolean indicating whether the episode has terminated\n    """"""\n\n    assert (len(slate) <= self._slate_size\n           ), \'Received unexpectedly large slate size: expecting %s, got %s\' % (\n               self._slate_size, len(slate))\n\n    # Get the documents associated with the slate\n    doc_ids = list(self._current_documents)  # pytype: disable=attribute-error\n    mapped_slate = [doc_ids[x] for x in slate]\n    documents = self._candidate_set.get_documents(mapped_slate)\n    # Simulate the user\'s response\n    responses = self._user_model.simulate_response(documents)\n\n    # Update the user\'s state.\n    self._user_model.update_state(documents, responses)\n\n    # Update the documents\' state.\n    self._document_sampler.update_state(documents, responses)\n\n    # Obtain next user state observation.\n    user_obs = self._user_model.create_observation()\n\n    # Check if reaches a terminal state and return.\n    done = self._user_model.is_terminal()\n\n    # Optionally, recreate the candidate set to simulate candidate\n    # generators for the next query.\n    if self._resample_documents:\n      self._do_resample_documents()\n\n    # Create observation of candidate set.\n    self._current_documents = collections.OrderedDict(\n        self._candidate_set.create_observation())\n\n    return (user_obs, self._current_documents, responses, done)\n\n\nEnvironment = SingleUserEnvironment  # for backwards compatability\n\n\nclass MultiUserEnvironment(AbstractEnvironment):\n  """"""Class to represent environment with multiple users.\n\n  Attributes:\n    user_model: A list of AbstractUserModel instances that represent users.\n    num_users: An integer representing the number of users.\n    document_sampler: An instantiation of AbstractDocumentSampler.\n    num_candidates: An integer representing the size of the candidate_set.\n    slate_size: An integer representing the slate size.\n    candidate_set: An instantiation of CandidateSet.\n    num_clusters: An integer representing the number of document clusters.\n  """"""\n\n  def reset(self):\n    """"""Resets the environment and return the first observation.\n\n    Returns:\n      user_obs: An array of floats representing observations of the user\'s\n        current state\n      doc_obs: An OrderedDict of document observations keyed by document ids\n    """"""\n    for user_model in self.user_model:\n      user_model.reset()\n    user_obs = [\n        user_model.create_observation() for user_model in self.user_model\n    ]\n    if self._resample_documents:\n      self._do_resample_documents()\n    self._current_documents = collections.OrderedDict(\n        self._candidate_set.create_observation())\n    return (user_obs, self._current_documents)\n\n  def reset_sampler(self):\n    self._document_sampler.reset_sampler()\n    for user_model in self.user_model:\n      user_model.reset_sampler()\n\n  @property\n  def num_users(self):\n    return len(self.user_model)\n\n  def step(self, slates):\n    """"""Executes the action, returns next state observation and reward.\n\n    Args:\n      slates: A list of slates, where each slate is an integer array of size\n        slate_size, where each element is an index into the set of\n        current_documents presented\n\n    Returns:\n      user_obs: A list of gym observation representing all users\' next state\n      doc_obs: A list of observations of the documents\n      responses: A list of AbstractResponse objects for each item in the slate\n      done: A boolean indicating whether the episode has terminated\n    """"""\n\n    assert (len(slates) == self.num_users\n           ), \'Received unexpected number of slates: expecting %s, got %s\' % (\n               self._slate_size, len(slates))\n    for i, slate in enumerate(slates):\n      assert (len(slate) <= self._slate_size\n             ), \'Slate %s is too large : expecting size %s, got %s\' % (\n                 i, self._slate_size, len(slate))\n\n    all_user_obs = []\n    all_documents = []  # Accumulate documents served to each user.\n    all_responses = []  # Accumulate each user\'s responses to served documents.\n    for user_model, slate in zip(self.user_model, slates):\n      # Get the documents associated with the slate\n      doc_ids = list(self._current_documents)  # pytype: disable=attribute-error\n      mapped_slate = [doc_ids[x] for x in slate]\n      documents = self._candidate_set.get_documents(mapped_slate)\n      if user_model.is_terminal():\n        responses = []\n      else:\n        # Simulate the user\'s response\n        responses = user_model.simulate_response(documents)\n\n        # Update the user\'s state.\n        user_model.update_state(documents, responses)\n\n      # Obtain next user state observation.\n      all_user_obs.append(user_model.create_observation())\n      all_documents.append(documents)\n      all_responses.append(responses)\n\n    def flatten(list_):\n      return list(itertools.chain(*list_))\n\n    # Update the documents\' state.\n    self._document_sampler.update_state(\n        flatten(all_documents), flatten(all_responses))\n\n    # Check if reaches a terminal state and return.\n    done = all([user_model.is_terminal() for user_model in self.user_model])\n\n    # Optionally, recreate the candidate set to simulate candidate\n    # generators for the next query.\n    if self._resample_documents:\n      self._do_resample_documents()\n\n    # Create observation of candidate set.\n    self._current_documents = collections.OrderedDict(\n        self._candidate_set.create_observation())\n\n    return (all_user_obs, self._current_documents, all_responses, done)\n'"
recsim/simulator/environment_test.py,3,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.environment.""""""\n\nimport numpy as np\nfrom recsim.environments import interest_exploration as ie\nfrom recsim.simulator import environment\nimport tensorflow.compat.v1 as tf\n\n\nclass EnvironmentTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(EnvironmentTest, self).setUp()\n    self._slate_size = 2\n    self._num_candidates = 20\n    user_model = ie.IEUserModel(\n        self._slate_size,\n        user_state_ctor=ie.IEUserState,\n        response_model_ctor=ie.IEResponse)\n    document_sampler = ie.IETopicDocumentSampler()\n    self._environment = environment.Environment(user_model, document_sampler,\n                                                self._num_candidates,\n                                                self._slate_size)\n\n  def test_environment(self):\n    user_obs, documents = self._environment.reset()\n    self.assertAllEqual(np.array([]), user_obs)\n    self.assertAllEqual([\n        str(doc)\n        for doc in range(self._num_candidates, 2 * self._num_candidates)\n    ], sorted(documents.keys()))\n    slate = [0, 0]\n    for i, doc_id in enumerate(list(documents)):\n      if documents[doc_id][\'cluster_id\'] > 0.5:\n        slate[0] = i\n      else:\n        slate[1] = i\n      if slate[0] != 0 and slate[1] != 0:\n        break\n    user_obs, documents, _, done = self._environment.step(slate)\n    self.assertAllEqual(np.array([]), user_obs)\n    self.assertAllEqual([\n        str(doc)\n        for doc in range(2 * self._num_candidates, 3 * self._num_candidates)\n    ], sorted(documents.keys()))\n    self.assertFalse(done)\n\n\nclass MultiUserEnvironmentTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(MultiUserEnvironmentTest, self).setUp()\n    self._slate_size = 2\n    self._num_candidates = 20\n    self._num_users = 100\n    user_models = []\n    for _ in range(self._num_users):\n      user_models.append(ie.IEUserModel(self._slate_size,\n                                        user_state_ctor=ie.IEUserState,\n                                        response_model_ctor=ie.IEResponse))\n    document_sampler = ie.IETopicDocumentSampler()\n    self._environment = environment.MultiUserEnvironment(\n        user_models, document_sampler, self._num_candidates, self._slate_size)\n\n  def test_multi_user_environment(self):\n    user_obs_list, documents = self._environment.reset()\n    for user_obs in user_obs_list:\n      self.assertAllEqual(np.array([]), user_obs)\n    self.assertAllEqual([\n        str(doc)\n        for doc in range(self._num_candidates, 2 * self._num_candidates)\n    ], sorted(documents.keys()))\n    slate = [0, 0]\n    for i, doc_id in enumerate(list(documents)):\n      if documents[doc_id][\'cluster_id\'] > 0.5:\n        slate[0] = i\n      else:\n        slate[1] = i\n      if slate[0] != 0 and slate[1] != 0:\n        break\n    slates = [slate for _ in range(self._num_users)]\n    user_obs_list, documents, _, done = self._environment.step(slates)\n    for user_obs in user_obs_list:\n      self.assertAllEqual(np.array([]), user_obs)\n    self.assertAllEqual([\n        str(doc)\n        for doc in range(2 * self._num_candidates, 3 * self._num_candidates)\n    ], sorted(documents.keys()))\n    self.assertFalse(done)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n\n'"
recsim/simulator/recsim_gym.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A wrapper for using Gym environment.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport gym\nfrom gym import spaces\nimport numpy as np\nfrom recsim.simulator import environment\n\n\ndef _dummy_metrics_aggregator(responses, metrics, info):\n  del responses  # Unused.\n  del metrics  # Unused.\n  del info  # Unused.\n  return\n\n\ndef _dummy_metrics_writer(metrics, add_summary_fn):\n  del metrics  # Unused.\n  del add_summary_fn  # Unused.\n  return\n\n\nclass RecSimGymEnv(gym.Env):\n  """"""Class to wrap recommender system environment to gym.Env.\n\n  Attributes:\n    game_over: A boolean indicating whether the current game has finished\n    action_space: A gym.spaces object that specifies the space for possible\n      actions.\n    observation_space: A gym.spaces object that specifies the space for possible\n      observations.\n  """"""\n\n  def __init__(self,\n               raw_environment,\n               reward_aggregator,\n               metrics_aggregator=_dummy_metrics_aggregator,\n               metrics_writer=_dummy_metrics_writer):\n    """"""Initializes a RecSim environment conforming to gym.Env.\n\n    Args:\n      raw_environment: A recsim recommender system environment.\n      reward_aggregator: A function mapping a list of responses to a number.\n      metrics_aggregator: A function aggregating metrics over all steps given\n        responses and response_names.\n      metrics_writer:  A function writing final metrics to TensorBoard.\n    """"""\n    self._environment = raw_environment\n    self._reward_aggregator = reward_aggregator\n    self._metrics_aggregator = metrics_aggregator\n    self._metrics_writer = metrics_writer\n    self.reset_metrics()\n\n  @property\n  def environment(self):\n    """"""Returns the recsim recommender system environment.""""""\n    return self._environment\n\n  @property\n  def game_over(self):\n    return False\n\n  @property\n  def action_space(self):\n    """"""Returns the action space of the environment.\n\n    Each action is a vector that specified document slate. Each element in the\n    vector corresponds to the index of the document in the candidate set.\n    """"""\n    action_space = spaces.MultiDiscrete(\n        self._environment.num_candidates * np.ones(\n            (self._environment.slate_size,)\n        ))\n    if isinstance(self._environment, environment.MultiUserEnvironment):\n      action_space = spaces.Tuple([action_space] * self._environment.num_users)\n    return action_space\n\n  @property\n  def observation_space(self):\n    """"""Returns the observation space of the environment.\n\n    Each observation is a dictionary with three keys `user`, `doc` and\n    `response` that includes observation about user state, document and user\n    response, respectively.\n    """"""\n    if isinstance(self._environment, environment.MultiUserEnvironment):\n      user_obs_space = self._environment.user_model[0].observation_space()\n      resp_obs_space = self._environment.user_model[0].response_space()\n      user_obs_space = spaces.Tuple(\n          [user_obs_space] * self._environment.num_users)\n      resp_obs_space = spaces.Tuple(\n          [resp_obs_space] * self._environment.num_users)\n\n    if isinstance(self._environment, environment.SingleUserEnvironment):\n      user_obs_space = self._environment.user_model.observation_space()\n      resp_obs_space = self._environment.user_model.response_space()\n\n    return spaces.Dict({\n        \'user\': user_obs_space,\n        \'doc\': self._environment.candidate_set.observation_space(),\n        \'response\': resp_obs_space,\n    })\n\n  def step(self, action):\n    """"""Runs one timestep of the environment\'s dynamics.\n\n    When end of episode is reached, you are responsible for calling `reset()`\n    to reset this environment\'s state. Accepts an action and returns a tuple\n    (observation, reward, done, info).\n\n    Args:\n      action (object): An action provided by the environment\n\n    Returns:\n      A four-tuple of (observation, reward, done, info) where:\n        observation (object): agent\'s observation that include\n          1. User\'s state features\n          2. Document\'s observation\n          3. Observation about user\'s slate responses.\n        reward (float) : The amount of reward returned after previous action\n        done (boolean): Whether the episode has ended, in which case further\n          step() calls will return undefined results\n        info (dict): Contains responses for the full slate for\n          debugging/learning.\n    """"""\n    user_obs, doc_obs, responses, done = self._environment.step(action)\n    if isinstance(self._environment, environment.MultiUserEnvironment):\n      all_responses = tuple(\n          tuple(\n              response.create_observation() for response in single_user_resps\n              ) for single_user_resps in responses\n          )\n    else:  # single user environment\n      all_responses = tuple(\n          response.create_observation() for response in responses\n          )\n    obs = dict(\n        user=user_obs,\n        doc=doc_obs,\n        response=all_responses)\n\n    # extract rewards from responses\n    reward = self._reward_aggregator(responses)\n    info = self.extract_env_info()\n    return obs, reward, done, info\n\n  def reset(self):\n    user_obs, doc_obs = self._environment.reset()\n    return dict(user=user_obs, doc=doc_obs, response=None)\n\n  def reset_sampler(self):\n    self._environment.reset_sampler()\n\n  def render(self, mode=\'human\'):\n    raise NotImplementedError\n\n  def close(self):\n    raise NotImplementedError\n\n  def seed(self, seed=None):\n    np.random.seed(seed=seed)\n\n  def extract_env_info(self):\n    info = {\'env\': self._environment}\n    return info\n\n  def reset_metrics(self):\n    """"""Resets every metric to zero.\n\n    We reset metrics for every iteration but not every episode. On the other\n    hand, reset() gets called for every episode.\n    """"""\n    self._metrics = collections.defaultdict(float)\n\n  def update_metrics(self, responses, info=None):\n    """"""Updates metrics with one step responses.""""""\n    self._metrics = self._metrics_aggregator(\n        responses, self._metrics, info)\n\n  def write_metrics(self, add_summary_fn):\n    """"""Writes metrics to TensorBoard by calling add_summary_fn.""""""\n    self._metrics_writer(self._metrics, add_summary_fn)\n'"
recsim/simulator/runner_lib.py,24,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nr""""""An executable class to run agents in the simulator.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import flags\nfrom dopamine.discrete_domains import checkpointer\nimport gin.tf\nfrom gym import spaces\nimport numpy as np\nfrom recsim.simulator import environment\nimport tensorflow.compat.v1 as tf\n\n\nflags.DEFINE_bool(\n    \'debug_mode\', False,\n    \'If set to true, the agent will output in-episode statistics \'\n    \'to Tensorboard. Disabled by default as this results in \'\n    \'slower training.\')\nflags.DEFINE_string(\'agent_name\', None, \'Name of the agent.\')\nflags.DEFINE_string(\'base_dir\', None,\n                    \'Base directory to host all required sub-directories.\')\nflags.DEFINE_string(\n    \'environment_name\', \'interest_evolution\',\n    \'The environment with which to run the experiment. Supported choices are \'\n    \'{interest_evolution, interest_exploration}.\')\nflags.DEFINE_string(\n    \'episode_log_file\', \'\',\n    \'Filename under base_dir to output simulated episodes in SequenceExample.\')\nflags.DEFINE_multi_string(\n    \'gin_files\', [], \'List of paths to gin configuration files (e.g.\'\n    \'""third_party/py/dopamine/agents/dqn/dqn.gin"").\')\nflags.DEFINE_multi_string(\n    \'gin_bindings\', [],\n    \'Gin bindings to override the values set in the config files \'\n    \'(e.g. ""runner_lib.Runner.max_steps_per_episode=100\')\n\n\nFLAGS = flags.FLAGS\n\n\ndef load_gin_configs(gin_files, gin_bindings):\n  """"""Loads gin configuration files.\n\n  Args:\n    gin_files: list, of paths to the gin configuration files for this\n      experiment.\n    gin_bindings: list, of gin parameter bindings to override the values in the\n      config files.\n  """"""\n  gin.parse_config_files_and_bindings(\n      gin_files, bindings=gin_bindings, skip_unknown=False)\n\n\n@gin.configurable\nclass Runner(object):\n  """"""Object that handles running experiments.\n\n  Here we use the term \'experiment\' to mean simulating interactions between the\n  agent and the environment and reporting some statistics pertaining to these\n  interactions.\n  """"""\n\n  _output_dir = None\n  _checkpoint_dir = None\n  _agent = None\n  _checkpointer = None\n\n  def __init__(self,\n               base_dir,\n               create_agent_fn,\n               env,\n               episode_log_file=\'\',\n               checkpoint_file_prefix=\'ckpt\',\n               max_steps_per_episode=27000):\n    """"""Initializes the Runner object in charge of running a full experiment.\n\n    Args:\n      base_dir: str, the base directory to host all required sub-directories.\n      create_agent_fn: A function that takes as args a Tensorflow session and an\n        environment, and returns an agent.\n      env: A Gym environment for running the experiments.\n      episode_log_file: Path to output simulated episodes in tf.SequenceExample.\n        Disable logging if episode_log_file is an empty string.\n      checkpoint_file_prefix: str, the prefix to use for checkpoint files.\n      max_steps_per_episode: int, maximum number of steps after which an episode\n        terminates.\n    """"""\n    tf.logging.info(\'max_steps_per_episode = %s\', max_steps_per_episode)\n\n    if base_dir is None:\n      raise ValueError(\'Missing base_dir.\')\n\n    self._base_dir = base_dir\n    self._create_agent_fn = create_agent_fn\n    self._env = env\n    self._checkpoint_file_prefix = checkpoint_file_prefix\n    self._max_steps_per_episode = max_steps_per_episode\n    self._episode_log_file = episode_log_file\n    self._episode_writer = None\n\n  def _set_up(self, eval_mode):\n    """"""Sets up the runner by creating and initializing the agent.""""""\n    # Reset the tf default graph to avoid name collisions from previous runs\n    # before doing anything else.\n    tf.reset_default_graph()\n    self._summary_writer = tf.summary.FileWriter(self._output_dir)\n    if self._episode_log_file:\n      self._episode_writer = tf.io.TFRecordWriter(\n          os.path.join(self._output_dir, self._episode_log_file))\n    # Set up a session and initialize variables.\n    self._sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n    self._agent = self._create_agent_fn(\n        self._sess,\n        self._env,\n        summary_writer=self._summary_writer,\n        eval_mode=eval_mode)\n    # type check: env/agent must both be multi- or single-user\n    if self._agent.multi_user and not isinstance(\n        self._env.environment, environment.MultiUserEnvironment):\n      raise ValueError(\'Multi-user agent requires multi-user environment.\')\n    if not self._agent.multi_user and isinstance(\n        self._env.environment, environment.MultiUserEnvironment):\n      raise ValueError(\'Single-user agent requires single-user environment.\')\n    self._summary_writer.add_graph(graph=tf.get_default_graph())\n    self._sess.run(tf.global_variables_initializer())\n    self._sess.run(tf.local_variables_initializer())\n\n  def _initialize_checkpointer_and_maybe_resume(self, checkpoint_file_prefix):\n    """"""Reloads the latest checkpoint if it exists.\n\n    This method will first create a `Checkpointer` object and then call\n    `checkpointer.get_latest_checkpoint_number` to determine if there is a valid\n    checkpoint in self._checkpoint_dir, and what the largest file number is.\n    If a valid checkpoint file is found, it will load the bundled data from this\n    file and will pass it to the agent for it to reload its data.\n    If the agent is able to successfully unbundle, this method will increase and\n    return the iteration number keyed by \'current_iteration\' and the step number\n    keyed by \'total_steps\' as the return values.\n\n    Args:\n      checkpoint_file_prefix: str, the checkpoint file prefix.\n    Returns:\n      start_iteration: The iteration number to be continued after the latest\n        checkpoint.\n      start_step: The step number to be continued after the latest checkpoint.\n    """"""\n    self._checkpointer = checkpointer.Checkpointer(self._checkpoint_dir,\n                                                   checkpoint_file_prefix)\n    start_iteration = 0\n    start_step = 0\n    # Check if checkpoint exists.\n    # Note that the existence of checkpoint 0 means that we have finished\n    # iteration 0 (so we will start from iteration 1).\n    latest_checkpoint_version = checkpointer.get_latest_checkpoint_number(\n        self._checkpoint_dir)\n    if latest_checkpoint_version >= 0:\n      assert not self._episode_writer, \'Can only log episodes from scratch.\'\n      experiment_data = self._checkpointer.load_checkpoint(\n          latest_checkpoint_version)\n      start_iteration = experiment_data[\'current_iteration\'] + 1\n      del experiment_data[\'current_iteration\']\n      start_step = experiment_data[\'total_steps\'] + 1\n      del experiment_data[\'total_steps\']\n      if self._agent.unbundle(self._checkpoint_dir, latest_checkpoint_version,\n                              experiment_data):\n        tf.logging.info(\n            \'Reloaded checkpoint and will start from \'\n            \'iteration %d\', start_iteration)\n    return start_iteration, start_step\n\n  def _log_one_step(self, user_obs, doc_obs, slate, responses, reward,\n                    is_terminal, sequence_example):\n    """"""Adds one step of agent-environment interaction into SequenceExample.\n\n    Args:\n      user_obs: An array of floats representing user state observations\n      doc_obs: A list of observations of the documents\n      slate: An array of indices to doc_obs\n      responses: A list of observations of responses for items in the slate\n      reward: A float for the reward returned after this step\n      is_terminal: A boolean for whether a terminal state has been reached\n      sequence_example: A SequenceExample proto for logging current episode\n    """"""\n\n    def _add_float_feature(feature, values):\n      feature.feature.add(float_list=tf.train.FloatList(value=values))\n\n    def _add_int64_feature(feature, values):\n      feature.feature.add(int64_list=tf.train.Int64List(value=values))\n\n    if self._episode_writer is None:\n      return\n    fl = sequence_example.feature_lists.feature_list\n\n    if isinstance(self._env.environment, environment.MultiUserEnvironment):\n      for i, (single_user,\n              single_slate,\n              single_user_responses,\n              single_reward) in enumerate(zip(user_obs,\n                                              slate,\n                                              responses,\n                                              reward)):\n        user_space = list(self._env.observation_space.spaces[\'user\'].spaces)[i]\n        _add_float_feature(fl[\'user_%d\' % i], spaces.flatten(\n            user_space, single_user))\n        _add_int64_feature(fl[\'slate_%d\' % i], single_slate)\n        _add_float_feature(fl[\'reward_%d\' % i], [single_reward])\n        for j, response in enumerate(single_user_responses):\n          resp_space = self._env.observation_space.spaces[\'response\'][i][0]\n          for k in response:\n            _add_float_feature(fl[\'response_%d_%d_%s\' % (i, j, k)],\n                               spaces.flatten(resp_space, response))\n    else:  # single-user environment\n      _add_float_feature(\n          fl[\'user\'],\n          spaces.flatten(self._env.observation_space.spaces[\'user\'], user_obs))\n      _add_int64_feature(fl[\'slate\'], slate)\n      for i, response in enumerate(responses):\n        resp_space = self._env.observation_space.spaces[\'response\'][0]\n        for k in response:\n          _add_float_feature(fl[\'response_%d_%s\' % (i, k)],\n                             spaces.flatten(resp_space, response))\n      _add_float_feature(fl[\'reward\'], [reward])\n\n    for i, doc in enumerate(list(doc_obs.values())):\n      doc_space = list(\n          self._env.observation_space.spaces[\'doc\'].spaces.values())[i]\n      _add_float_feature(fl[\'doc_%d\' % i], spaces.flatten(doc_space, doc))\n\n    _add_int64_feature(fl[\'is_terminal\'], [is_terminal])\n\n  def _run_one_episode(self):\n    """"""Executes a full trajectory of the agent interacting with the environment.\n\n    Returns:\n      The number of steps taken and the total reward.\n    """"""\n    step_number = 0\n    total_reward = 0.\n\n    start_time = time.time()\n\n    sequence_example = tf.train.SequenceExample()\n    observation = self._env.reset()\n    action = self._agent.begin_episode(observation)\n\n    # Keep interacting until we reach a terminal state.\n    while True:\n      last_observation = observation\n      observation, reward, done, info = self._env.step(action)\n      self._log_one_step(last_observation[\'user\'], last_observation[\'doc\'],\n                         action, observation[\'response\'], reward, done,\n                         sequence_example)\n      # Update environment-specific metrics with responses to the slate.\n      self._env.update_metrics(observation[\'response\'], info)\n\n      total_reward += reward\n      step_number += 1\n\n      if done:\n        break\n      elif step_number == self._max_steps_per_episode:\n        # Stop the run loop once we reach the true end of episode.\n        break\n      else:\n        action = self._agent.step(reward, observation)\n\n    self._agent.end_episode(reward, observation)\n    if self._episode_writer is not None:\n      self._episode_writer.write(sequence_example.SerializeToString())\n\n    time_diff = time.time() - start_time\n    self._update_episode_metrics(\n        episode_length=step_number,\n        episode_time=time_diff,\n        episode_reward=total_reward)\n\n    return step_number, total_reward\n\n  def _initialize_metrics(self):\n    """"""Initializes the metrics.""""""\n    self._stats = {\n        \'episode_length\': [],\n        \'episode_time\': [],\n        \'episode_reward\': [],\n    }\n    # Initialize environment-specific metrics.\n    self._env.reset_metrics()\n\n  def _update_episode_metrics(self, episode_length, episode_time,\n                              episode_reward):\n    """"""Updates the episode metrics with one episode.""""""\n\n    self._stats[\'episode_length\'].append(episode_length)\n    self._stats[\'episode_time\'].append(episode_time)\n    self._stats[\'episode_reward\'].append(episode_reward)\n\n  def _write_metrics(self, step, suffix):\n    """"""Writes the metrics to Tensorboard summaries.""""""\n\n    def add_summary(tag, value):\n      summary = tf.Summary(\n          value=[tf.Summary.Value(tag=tag + \'/\' + suffix, simple_value=value)])\n      self._summary_writer.add_summary(summary, step)\n\n    num_steps = np.sum(self._stats[\'episode_length\'])\n    time_per_step = np.sum(self._stats[\'episode_time\']) / num_steps\n\n    add_summary(\'TimePerStep\', time_per_step)\n    add_summary(\'AverageEpisodeLength\', np.mean(self._stats[\'episode_length\']))\n    add_summary(\'AverageEpisodeRewards\', np.mean(self._stats[\'episode_reward\']))\n    add_summary(\'StdEpisodeRewards\', np.std(self._stats[\'episode_reward\']))\n\n    # Environment-specific Tensorboard summaries.\n    self._env.write_metrics(add_summary)\n\n    self._summary_writer.flush()\n\n  def _checkpoint_experiment(self, iteration, total_steps):\n    """"""Checkpoints experiment data.\n\n    Args:\n      iteration: int, iteration number for checkpointing.\n      total_steps: int, total number of steps for all iterations so far.\n    """"""\n    experiment_data = self._agent.bundle_and_checkpoint(self._checkpoint_dir,\n                                                        iteration)\n    if experiment_data:\n      experiment_data[\'current_iteration\'] = iteration\n      experiment_data[\'total_steps\'] = total_steps\n      self._checkpointer.save_checkpoint(iteration, experiment_data)\n\n\n@gin.configurable\nclass TrainRunner(Runner):\n  """"""Object that handles running the training.\n\n  See main.py for a simple example to train an agent.\n  """"""\n\n  def __init__(self, max_training_steps=250000, num_iterations=100,\n               checkpoint_frequency=1, **kwargs):\n    tf.logging.info(\n        \'max_training_steps = %s, number_iterations = %s,\'\n        \'checkpoint frequency = %s iterations.\', max_training_steps,\n        num_iterations, checkpoint_frequency)\n\n    super(TrainRunner, self).__init__(**kwargs)\n    self._max_training_steps = max_training_steps\n    self._num_iterations = num_iterations\n    self._checkpoint_frequency = checkpoint_frequency\n\n    self._output_dir = os.path.join(self._base_dir, \'train\')\n    self._checkpoint_dir = os.path.join(self._output_dir, \'checkpoints\')\n\n    self._set_up(eval_mode=False)\n\n  def run_experiment(self):\n    """"""Runs a full experiment, spread over multiple iterations.""""""\n    tf.logging.info(\'Beginning training...\')\n    start_iter, total_steps = self._initialize_checkpointer_and_maybe_resume(\n        self._checkpoint_file_prefix)\n    if self._num_iterations <= start_iter:\n      tf.logging.warning(\'num_iterations (%d) < start_iteration(%d)\',\n                         self._num_iterations, start_iter)\n      return\n\n    for iteration in range(start_iter, self._num_iterations):\n      tf.logging.info(\'Starting iteration %d\', iteration)\n      total_steps = self._run_train_phase(total_steps)\n      if iteration % self._checkpoint_frequency == 0:\n        self._checkpoint_experiment(iteration, total_steps)\n\n  def _run_train_phase(self, total_steps):\n    """"""Runs training phase and updates total_steps.""""""\n\n    self._initialize_metrics()\n\n    num_steps = 0\n\n    while num_steps < self._max_training_steps:\n      episode_length, _ = self._run_one_episode()\n      num_steps += episode_length\n\n    total_steps += num_steps\n    self._write_metrics(total_steps, suffix=\'train\')\n    return total_steps\n\n\n@gin.configurable\nclass EvalRunner(Runner):\n  """"""Object that handles running the evaluation.\n\n  See main.py for a simple example to evaluate an agent.\n  """"""\n\n  def __init__(self,\n               max_eval_episodes=125000,\n               test_mode=False,\n               min_interval_secs=30,\n               train_base_dir=None,\n               **kwargs):\n    tf.logging.info(\'max_eval_episodes = %s\', max_eval_episodes)\n    super(EvalRunner, self).__init__(**kwargs)\n    self._max_eval_episodes = max_eval_episodes\n    self._test_mode = test_mode\n    self._min_interval_secs = min_interval_secs\n\n    self._output_dir = os.path.join(self._base_dir,\n                                    \'eval_%s\' % max_eval_episodes)\n    tf.io.gfile.makedirs(self._output_dir)\n    if train_base_dir is None:\n      train_base_dir = self._base_dir\n    self._checkpoint_dir = os.path.join(train_base_dir, \'train\', \'checkpoints\')\n\n    self._set_up(eval_mode=True)\n\n  def run_experiment(self):\n    """"""Runs a full experiment, spread over multiple iterations.""""""\n    tf.logging.info(\'Beginning evaluation...\')\n    # Use the checkpointer class.\n    self._checkpointer = checkpointer.Checkpointer(\n        self._checkpoint_dir, self._checkpoint_file_prefix)\n    checkpoint_version = -1\n    # Check new checkpoints in a loop.\n    while True:\n      # Check if checkpoint exists.\n      # Note that the existence of checkpoint 0 means that we have finished\n      # iteration 0 (so we will start from iteration 1).\n      latest_checkpoint_version = checkpointer.get_latest_checkpoint_number(\n          self._checkpoint_dir)\n      # checkpoints_iterator already makes sure a new checkpoint exists.\n      if latest_checkpoint_version <= checkpoint_version:\n        time.sleep(self._min_interval_secs)\n        continue\n      checkpoint_version = latest_checkpoint_version\n      experiment_data = self._checkpointer.load_checkpoint(\n          latest_checkpoint_version)\n      assert self._agent.unbundle(self._checkpoint_dir,\n                                  latest_checkpoint_version, experiment_data)\n\n      self._run_eval_phase(experiment_data[\'total_steps\'])\n      if self._test_mode:\n        break\n\n  def _run_eval_phase(self, total_steps):\n    """"""Runs evaluation phase given model has been trained for total_steps.""""""\n\n    self._env.reset_sampler()\n    self._initialize_metrics()\n\n    num_episodes = 0\n    episode_rewards = []\n\n    while num_episodes < self._max_eval_episodes:\n      _, episode_reward = self._run_one_episode()\n      episode_rewards.append(episode_reward)\n      num_episodes += 1\n\n    self._write_metrics(total_steps, suffix=\'eval\')\n\n    output_file = os.path.join(self._output_dir, \'returns_%s\' % total_steps)\n    tf.logging.info(\'eval_file: %s\', output_file)\n    with tf.io.gfile.GFile(output_file, \'w+\') as f:\n      f.write(str(episode_rewards))\n'"
recsim/testing/test_environment.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Simple sequential environment with known Q-function for testing purposes.\n\nThe user dynamics in this environment follow a 6 state x 4  action MDP with\nthe following specification:\n\n0  # action\n0 (1 0.1) (2 0.1) (3 0.8)\n1 (2 0.1) (3 0.1) (4 0.8)\n2 (3 0.1) (4 0.1) (5 0.8)\n3 (4 0.1) (5 0.1) (0 0.8)\n4 (5 1.0)\n5 (0 1.0)\nreward (0 -10.0) (1 0.0) (2 0.0) (3 4.0) (4 0.0) (5 5.0)\n\n1  # action\n0 (2 0.1) (3 0.1) (4 0.8)\n1 (3 0.1) (4 0.1) (5 0.8)\n2 (4 0.1) (5 0.1) (0 0.8)\n3 (5 0.1) (0 0.1) (1 0.8)\n4 (0 1.0)\n5 (1 1.0)\nreward (0 -10.0) (1 0.0) (2 1.0) (3 0.0) (4 0.0) (5 0.0)\n\n2  # action\n0 (3 0.1) (4 0.1) (5 0.8)\n1 (4 0.1) (5 0.1) (0 0.8)\n2 (5 0.1) (0 0.1) (1 0.8)\n3 (0 0.1) (1 0.1) (2 0.8)\n4 (1 1.0)\n5 (2 1.0)\nreward (0 -10.0) (1 1.0) (2 0.0) (3 2.0) (4 0.0) (5 2.0)\n\n3  # action\n0 (4 0.1) (5 0.1) (0 0.8)\n1 (5 0.1) (0 0.1) (1 0.8)\n2 (0 0.1) (1 0.1) (2 0.8)\n3 (1 0.1) (2 0.1) (3 0.8)\n4 (2 1.0)\n5 (3 1.0)\nreward (0 -10.0) (1 0.0) (2 0.0) (3 0.0) (4 0.0) (5 5.0)\n\nKnown Q and value functions for:\n\n* gamma = 0\n  Q-function:\n            Action\n  State        0          1          2          3\n    0         -10        -10        -10        -10\n    1                                1\n    2                     1\n    3          4                     2\n    4\n    5          5                     2          5\n\n  Value function:\n  V[0] = -10, V[1] = 1, V[2] = 1, V[3] = 4, V[4] = 0, V[5] = 5\n\n* gamma = 0.5\n  Q-function:\n            Action\n  State        0          1          2          3\n    0         -8.53022   -8.41259   -7.10072   -12.3547\n    1          1.58741    2.89928   -1.35468    1.12842\n    2          2.89928   -1.35468    1.12842    0.94964\n    3          1.64532    1.12842    2.94964    1.46978\n    4          3.23741   -3.55036    1.44964    1.44964\n    5          1.44964    1.44964    3.44964    6.47482\n\n  Value function:\n  V[0] = -7.10072, V[1] = 2.89928, V[2] = 2.89928, V[3] = 2.94964,\n  V[4] = 3.23741, V[5] = 6.47482\n\n* gamma = 0.9\n  Q-function:\n            Action\n  State        0          1          2          3\n    0          5.79888    6.59282    8.12425   -0.615864\n    1          16.5928    18.1242    10.3841    15.641\n    2          18.1242    10.3841    15.641     15.4118\n    3          13.3841    15.641     17.4118    15.7989\n    4          18.6036    7.31182    16.3118    16.3118\n    5          12.3118    16.3118    18.3118    20.6706\n\n  Value function:\n  V[0] = 8.12425, V[1] = 18.1242, V[2] = 18.1242, V[3] = 17.4118,\n  V[4] = 18.6036, V[5] = 20.6706\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport gin.tf\nfrom gym import spaces\nimport numpy as np\n\nfrom recsim import document\nfrom recsim import user\nfrom recsim.simulator import environment\nfrom recsim.simulator import recsim_gym\n\nFLAGS = flags.FLAGS\nQVALUES0 = [[-10.0, -10.0, -10.0, -10.0], [0.0, 0.0, 1.0, 0.0],\n            [0.0, 1.0, 0.0, 0.0], [4.0, 0.0, 2.0, 0.0], [0.0, 0.0, 0.0, 0.0],\n            [5.0, 0.0, 2.0, 5.0]]\nQVALUES05 = [[-8.53022, -8.41259, -7.10072, -12.3547],\n             [1.58741, 2.89928, -1.35468, 1.12842],\n             [2.89928, -1.35468, 1.12842, 0.94964],\n             [1.64532, 1.12842, 2.94964, 1.46978],\n             [3.23741, -3.55036, 1.44964, 1.44964],\n             [1.44964, 1.44964, 3.44964, 6.47482]]\nQVALUES09 = [[5.79888, 6.59282, 8.12425, -0.615864],\n             [16.5928, 18.1242, 10.3841, 15.641],\n             [18.1242, 10.3841, 15.641, 15.4118],\n             [13.3841, 15.641, 17.4118, 15.7989],\n             [18.6036, 7.31182, 16.3118, 16.3118],\n             [12.3118, 16.3118, 18.3118, 20.6706]]\n\n\nclass SimpleSequentialUserModel(user.AbstractUserModel):\n  """"""Class to model a simple sequential user for testing.\n\n  This is a 6-state user with dynamics described above. It can consume one of 4\n  document types and transition according to a fixed transition matrix. To\n  facilitate testing of Q-estimation, the entire state is emitted, i.e. the\n  environment is fully observed.\n\n  Args:\n  seed: random seed.\n  """"""\n\n  def __init__(self,\n               slate_size,\n               seed=0,\n               starting_probs=(1.0, 0.0, 0.0, 0.0, 0.0, 0.0)):\n    super(SimpleSequentialUserModel, self).__init__(\n        SimpleSequentialResponse,\n        SimpleSequentialUserSampler(seed=seed, starting_probs=starting_probs),\n        slate_size)\n    self._transition_matrix = np.zeros((4, 6, 6))\n    self._transition_matrix[0, :, :] = np.array([[0, .1, .1, .8, 0, 0],\n                                                 [0, 0, .1, .1, .8, 0],\n                                                 [0, 0, 0, .1, .1, .8],\n                                                 [.8, 0, 0, 0, .1, .1],\n                                                 [0, 0, 0, 0, 0, 1.0],\n                                                 [1.0, 0, 0, 0, 0, 0]])\n    self._transition_matrix[1, :, :] = np.array([[0, 0, .1, .1, .8, 0],\n                                                 [0, 0, 0, .1, .1, .8],\n                                                 [.8, 0, 0, 0, .1, .1],\n                                                 [.1, .8, 0, 0, 0, .1],\n                                                 [1.0, 0, 0, 0, 0, 0],\n                                                 [0, 1.0, 0, 0, 0, 0]])\n    self._transition_matrix[2, :, :] = np.array([[0, 0, 0, .1, .1, .8],\n                                                 [.8, 0, 0, 0, .1, .1],\n                                                 [.1, .8, 0, 0, 0, .1],\n                                                 [.1, .1, .8, 0, 0, 0],\n                                                 [0, 1.0, 0, 0, 0, 0],\n                                                 [0, 0, 1.0, 0, 0, 0]])\n    self._transition_matrix[3, :, :] = np.array([[.8, 0, 0, 0, .1, .1],\n                                                 [.1, .8, 0, 0, 0, .1],\n                                                 [.1, .1, .8, 0, 0, 0],\n                                                 [0, .1, .1, .8, 0, 0],\n                                                 [0, 0, 1.0, 0, 0, 0],\n                                                 [0, 0, 0, 1.0, 0, 0]])\n    self._reward_vector = np.zeros((4, 6))\n    self._reward_vector[0, :] = np.array([-10.0, 0.0, 0.0, 4.0, 0.0, 5.0])\n    self._reward_vector[1, :] = np.array([-10.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n    self._reward_vector[2, :] = np.array([-10.0, 1.0, 0.0, 2.0, 0.0, 2.0])\n    self._reward_vector[3, :] = np.array([-10.0, 0.0, 0.0, 0.0, 0.0, 5.0])\n\n  def is_terminal(self):\n    """"""Returns a boolean indicating if the session is over.""""""\n    return False\n\n  def update_state(self, slate_documents, responses):\n    doc = slate_documents[0]\n    next_state = np.random.choice(\n        6, p=self._transition_matrix[doc.action_id, self._user_state.state])\n    self._user_state = SimpleSequentialUserState(next_state)\n    return\n\n  def simulate_response(self, documents):\n    """"""Simulates the user\'s response to a slate of documents with choice model.\n\n    Args:\n      documents: a list of SimpleSequentialDocument objects in the slate.\n\n    Returns:\n      responses: a list of SimpleSequentialResponse objects,\n        one for each document.\n    """"""\n    # List of empty responses\n    responses = [self._response_model_ctor() for _ in documents]\n    # Always pick the first document in the slate\n    selected_index = 0\n    self._generate_response(documents[selected_index],\n                            responses[selected_index])\n    return responses\n\n  def _generate_response(self, doc, response):\n    """"""Trivial response: sets the clicked property of a clicked document.\n\n    Args:\n      doc: a SimpleSequentialDocument object.\n      response: a SimpleSequentialResponse for the document.\n    Updates: response, with whether the document was clicked.\n    """"""\n    response.reward = self._reward_vector[doc.action_id, self._user_state.state]\n\n\nclass SimpleSequentialUserState(user.AbstractUserState):\n  """"""Class to represent user state for testing. Fully observed.\n\n  Attributes:\n    state: integer in [0...5] representing the state of the user Markov Chain.\n  """"""\n\n  def __init__(self, state):\n    """"""Initializes a new user.""""""\n    self.state = state\n\n  def create_observation(self):\n    return self.state\n\n  def observation_space(self):\n    return spaces.Discrete(6)\n\n  def score_document(self, doc_obs):\n    del doc_obs  # unused\n    return 1.0\n\n\n@gin.configurable\nclass SimpleSequentialUserSampler(user.AbstractUserSampler):\n  """"""Samples initial user state from a multinomial distribution.\n\n\n    Args:\n      probs: 6-outcome probability mass function for sampling initial state.\n  """"""\n\n  def __init__(self, starting_probs=(1.0, 0, 0, 0, 0, 0), **kwargs):\n    self._probs = starting_probs\n    super(SimpleSequentialUserSampler, self).__init__(SimpleSequentialUserState,\n                                                      **kwargs)\n\n  def sample_user(self):\n    starting_state = np.random.choice(6, p=self._probs)\n    return SimpleSequentialUserState(starting_state)\n\n\nclass SimpleSequentialResponse(user.AbstractResponse):\n  """"""Class to represent a user\'s response to a document.\n\n  Attributes:\n    reward: a real number representing the state reward of the action executed\n      by the document.\n  """"""\n\n  # The max possible doc ID. We assume the doc ID is in range [0, MAX_DOC_ID].\n  MAX_DOC_ID = None\n\n  def __init__(self, reward=0.0):\n    self.reward = reward\n\n  def __str__(self):\n    return str(self.reward)\n\n  def __repr__(self):\n    return self.__str__()\n\n  def create_observation(self):\n    return {\'reward\': np.array(self.reward)}\n\n  @classmethod\n  def response_space(cls):\n    return spaces.Dict({\n        \'reward\':\n            spaces.Box(low=-10.0, high=5.0, shape=tuple(), dtype=np.float32)\n    })\n\n\nclass SimpleSequentialDocument(document.AbstractDocument):\n  """"""Class to represent an Simple Sequential Document.\n\n  Attributes:\n    doc_id: integer represents the document id.\n    action_id: integer represents one of the 4 available actions.\n  """"""\n\n  def __init__(self, doc_id, action_id):\n    self.action_id = action_id\n    super(SimpleSequentialDocument, self).__init__(doc_id)\n\n  def create_observation(self):\n    return self.action_id\n\n  def observation_space(self):\n    return spaces.Discrete(4)\n\n\n@gin.configurable\nclass SimpleSequentialDocumentSampler(document.AbstractDocumentSampler):\n  """"""Round robin a selection of all 4 actions.\n\n  As long as the number of candidates is more than 4, this guarantees that all\n  actions will be available.\n  """"""\n\n  def __init__(self, **kwargs):\n    self._last_action_id = -1\n    self._doc_count = 0\n\n    super(SimpleSequentialDocumentSampler,\n          self).__init__(SimpleSequentialDocument, **kwargs)\n\n  def sample_document(self):\n    self._last_action_id += 1\n    self._last_action_id %= 4\n    self._doc_count += 1\n    return self._doc_ctor(self._doc_count, self._last_action_id)\n\n\ndef total_reward(responses):\n  """"""Calculates the total reward from a list of responses.\n\n  Args:\n     responses: A list of SimpleSequentialResponse objects\n\n  Returns:\n    reward: A float representing the total clicks from the responses\n  """"""\n  reward = 0.0\n  for r in responses:\n    reward += r.reward\n  return reward\n\n\ndef create_environment(env_config):\n  """"""Creates an simple sequential testing environment.""""""\n  if env_config[\'num_candidates\'] < 4:\n    raise ValueError(\'num_candidates must be at least 4.\')\n\n  SimpleSequentialResponse.MAX_DOC_ID = env_config[\'num_candidates\'] - 1\n  user_model = SimpleSequentialUserModel(\n      env_config[\'slate_size\'],\n      seed=env_config[\'seed\'],\n      starting_probs=env_config[\'starting_probs\'])\n  document_sampler = SimpleSequentialDocumentSampler(seed=env_config[\'seed\'])\n  simple_seq_env = environment.Environment(\n      user_model,\n      document_sampler,\n      env_config[\'num_candidates\'],\n      env_config[\'slate_size\'],\n      resample_documents=env_config[\'resample_documents\'])\n\n  return recsim_gym.RecSimGymEnv(simple_seq_env, total_reward,\n                                 lambda _, __, ___: None, lambda _, __: None)\n'"
recsim/agents/bandits/__init__.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
recsim/agents/bandits/algorithms.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Classes for Bandit Algorithms.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\n\n\nclass MABAlgorithm(object):\n  """"""Base class for Multi-armed bandit (MAB) algorithms.\n\n  We implement multi-armed bandit algorithms with confidence width tuning\n  proposed in Hsu et al. https://arxiv.org/abs/1904.02664.\n\n  Attributes:\n    pulls: A numpy array which counts number of pulls of each arm\n    reward: A numpy array which sums up reward of each arm\n    optimism_scaling: A float specifying the confidence level. Default value\n      (1.0) corresponds to the exploration strategy presented in the literature.\n      A smaller number means less exploration and more exploitation.\n    _rng: An instance of random.RandomState for random number generation\n  """"""\n\n  def __init__(self, num_arms, params, seed=0):\n    """"""Initializes MABAlgorithm.\n\n    Args:\n      num_arms: Number of arms. Must be greater than one.\n      params: A dictionary which includes additional parameters like\n        optimism_scaling. Default is an empty dictionary.\n      seed: Random seed for this object. Default is zero.\n    """"""\n    if num_arms < 2:\n      raise ValueError(\'num_arms must be greater than one.\')\n    self.pulls = np.zeros(num_arms)\n    self.reward = np.zeros(num_arms)\n    self._rng = np.random.RandomState(seed)\n\n    self.optimism_scaling = 1.0\n    for attr, val in params.items():\n      setattr(self, attr, val)\n\n  def set_state(self, pulls, reward):\n    if len(pulls) != len(self.pulls) or len(reward) != len(self.reward):\n      raise ValueError(\'Cannot set state with a different number of arms.\')\n    self.pulls[:] = pulls\n    self.reward[:] = reward\n\n  def update(self, arm, reward):\n    if reward < 0 or reward > 1:\n      raise ValueError(\'reward must be in [0, 1].\')\n    self.pulls[arm] += 1\n    self.reward[arm] += reward\n\n\nclass UCB1(MABAlgorithm):\n  """"""UCB1 algorithm.\n\n  See ""Finite-time Analysis of the Multiarmed Bandit Problem"" by Auer,\n  Cesa-Bianchi, and Fischer.\n  """"""\n\n  def get_score(self, t):\n    """"""Computes upper confidence bounds of reward / pulls at round t.""""""\n    # Pull any arm that we haven\'t pulled.\n    if not all(self.pulls):\n      return np.where(self.pulls > 0, 0, np.Inf)\n    ct = self.optimism_scaling * np.sqrt(2 * np.log(t))\n    return self.reward / self.pulls + ct * np.sqrt(1 / self.pulls)\n\n  def get_arm(self, t):\n    return np.argmax(self.get_score(t))\n\n  @staticmethod\n  def print():\n    return \'UCB1\'\n\n\nclass KLUCB(MABAlgorithm):\n  """"""Kullback-Leibler Upper Confidence Bounds (KL-UCB) algorithm.\n\n  See ""The KL-UCB algorithm for bounded stochastic bandits and beyond"" by\n  Garivier and Cappe.\n  """"""\n\n  def get_score(self, t):\n    """"""Computes upper confidence bounds of reward / pulls at round t.""""""\n    # Pull any arm that we haven\'t pulled.\n    if not all(self.pulls):\n      return np.where(self.pulls > 0, 0, np.Inf)\n    c = self.optimism_scaling**2 * (np.log(t) +\n                                    3 * np.log(np.log(t))) / self.pulls\n    p = self.reward / self.pulls\n\n    # KL-divergence d(p, q) is strictly increasing over [p, 1].\n    # Use binary search to find q such that d(p, q) <= c.\n    qmin = p\n    qmax = np.ones(p.size)\n    for _ in range(16):  # Error bounded by 2^-16.\n      q = (qmax + qmin) / 2\n      ndx = (np.where(p > 0, p * np.log(p / q), 0) +\n             np.where(p < 1, (1 - p) * np.log((1 - p) / (1 - q)), 0)) < c\n      qmin[ndx] = q[ndx]\n      qmax[~ndx] = q[~ndx]\n\n    return q\n\n  def get_arm(self, t):\n    return np.argmax(self.get_score(t))\n\n  @staticmethod\n  def print():\n    return \'KL-UCB\'\n\n\nclass ThompsonSampling(MABAlgorithm):\n  """"""Thompson Sampling algorithm for the Bernoulli bandit.\n\n  See ""Further Optimal Regret Bounds for Thompson Sampling"" by Agrawal and\n    Goyal.\n  """"""\n\n  def update(self, arm, reward):\n    if reward > 0 and reward < 1:\n      reward = float(self._rng.rand() < reward)\n    MABAlgorithm.update(self, arm, reward)\n\n  def get_score(self, t):\n    """"""Samples scores from the posterior distribution.""""""\n    del t\n    # Generate a beta distribution based on the expectation of reward.\n    alpha = 1 + self.reward / self.optimism_scaling**2\n    beta = 1 + (self.pulls - self.reward) / self.optimism_scaling**2\n    return self._rng.beta(alpha, beta)\n\n  def get_arm(self, t):\n    return np.argmax(self.get_score(t))\n\n  @staticmethod\n  def print():\n    return \'ThompsonSampling\'\n'"
recsim/agents/bandits/algorithms_test.py,4,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.agents.bandits.algorithm.""""""\n\nimport numpy as np\nfrom recsim.agents.bandits import algorithms\nimport tensorflow.compat.v1 as tf\n\n\nclass UCB1Test(tf.test.TestCase):\n\n  def setUp(self):\n    super(UCB1Test, self).setUp()\n    self._alg = algorithms.UCB1(2, {})\n    self._alg.set_state(np.array([10, 10]), np.array([10, 0]))\n\n  def test_get_score(self):\n    ucb = self._alg.get_score(20)\n    self.assertAlmostEqual(1.7740455, ucb[0])\n    self.assertAlmostEqual(0.7740455, ucb[1])\n\n  def test_get_arm(self):\n    # Arm 0 is clearly the best.\n    self.assertEqual(0, self._alg.get_arm(20))\n\n  def test_get_arm_initial_stage(self):\n    alg = algorithms.UCB1(3, {})\n    alg.update(0, 1)\n    alg.update(2, 1)\n    # Pull Arm 1 which has not been pulled.\n    self.assertEqual(1, alg.get_arm(1))\n\n\nclass KLUCBTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(KLUCBTest, self).setUp()\n    self._alg = algorithms.KLUCB(2, {})\n    self._alg.set_state(np.array([10, 10]), np.array([10, 0]))\n\n  def test_get_score(self):\n    ucb = self._alg.get_score(20)\n    self.assertAlmostEqual(1, ucb[0])\n    self.assertAlmostEqual(0.5 - 2**-16, ucb[1])\n\n  def test_get_arm(self):\n    # Arm 0 is clearly the best.\n    self.assertEqual(0, self._alg.get_arm(20))\n\n  def test_get_arm_initial_stage(self):\n    alg = algorithms.KLUCB(3, {})\n    alg.update(0, 1)\n    alg.update(2, 1)\n    # Pull Arm 1 which has not been pulled.\n    self.assertEqual(1, alg.get_arm(1))\n\n\nclass ThompsonSamplingTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(ThompsonSamplingTest, self).setUp()\n    self._alg = algorithms.ThompsonSampling(2, {})\n    self._alg.set_state(np.array([10, 10]), np.array([10, 0]))\n\n  def test_update(self):\n    self._alg.update(0, 0.9)\n    self._alg.update(1, 0)\n    self.assertAllEqual(np.array([11, 11]), self._alg.pulls)\n    self.assertAllEqual(np.array([11, 0]), self._alg.reward)\n\n  def test_get_score(self):\n    mu = self._alg.get_score(20)\n    self.assertAlmostEqual(0.9570183, mu[0])\n    self.assertAlmostEqual(0.0438080, mu[1])\n\n  def test_get_arm(self):\n    # Arm 0 is clearly the best.\n    self.assertEqual(0, self._alg.get_arm(20))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
recsim/agents/bandits/glm_algorithms.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Classes for Bandit Algorithms for Generalized Linear Models.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport abc\nimport numpy as np\nfrom scipy import special\nimport six\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass GLMAlgorithm(object):\n  """"""Base class for Generalized Linear Models (GLM) bandit algorithms.\n\n  In this setting each arm is represented by a feature vector x and there exists\n  an unknown weight vector w*. In each round the algorithm pulls an arm and gets\n  a noisy reward, which is assumed to be the result of composing the dot product\n  x.w* with a link function, plus random noise. For example,\n  sigmoid(x.w*) + eps, where eps is sub-Gaussian noise.\n\n  Attributes:\n    arms: the arms pulled so far\n    rewards: the rewards observed so far\n    dim: the dimension of the feature/weight vectors\n    outer: the sum of outer products of the arms pulled so far (x.x^T)\n    sigma0: a parameter scaling the identity matrix used for making the\n      resulting Gram matrix positive definite.\n    optimism_scaling: A float specifying the confidence level. Default value\n      (1.0) corresponds to the exploration strategy presented in the literature.\n      A smaller number means less exploration and more exploitation.\n    _rng: An instance of random.RandomState for random number generation\n  """"""\n\n  def __init__(self, dim, sigma0=1., optimism_scaling=1.):\n    self._rewards = np.array([])  # stores all rewards\n    self._arms = np.ndarray([0, dim])  # stores arm differences\n    self._dim = dim\n    self._outer = np.zeros([dim, dim])\n    self._sigma0 = sigma0\n    self._optimism_scaling = optimism_scaling\n\n  def update(self, reward, arm):\n    """"""Updates state with arm and reward.\n\n    Args:\n      reward: the reward received\n      arm: the arm that was pulled\n    """"""\n    assert len(arm) == self._dim, \'Expected dimension {}, got {}\'.format(\n        self._dim, len(arm))\n    self._rewards = np.append(self._rewards, reward)\n    self._arms = np.concatenate((self._arms, [arm]), axis=0)\n    self._outer += np.outer(arm, arm)\n\n  def solve_logistic_bandit(self, init_iters=10, num_iters=20, tol=1e-3):\n    """"""Solves the maximum-likelihood problem.\n\n    Implements iterative reweighted least squares for Bayesian logistic\n    regression. See sections 4.3.3 and 4.5.1 in Pattern Recognition and Machine\n    Learning, Bishop (2006)\n\n    Args:\n      init_iters: number of initial iterations to skip (returns zeros)\n      num_iters: number of least squares iterations\n      tol: tolerance level of change in solution between iterations before\n        terminating\n    Returns:\n      w: maximum likelihood solution\n      gram: Gram matrix\n    """"""\n\n    arms = self._arms\n    w = np.zeros(self._dim)\n    gram = np.eye(self._dim) / np.square(self._sigma0)\n    if len(self._arms) > init_iters:\n      for _ in range(num_iters):\n        prev_w = np.copy(w)\n        arms_w = arms.dot(w)\n        sig_arms_w = special.expit(arms_w)\n        r = np.diag(sig_arms_w * (1 - sig_arms_w))\n        gram = (((arms.T).dot(r)).dot(arms) +\n                np.eye(self._dim) / np.square(self._sigma0))\n        rz = r.dot(arms_w) - (sig_arms_w - self._rewards)\n        w = np.linalg.solve(gram, (arms.T).dot(rz))\n        if np.linalg.norm(w - prev_w) < tol:\n          break\n\n    return w, gram\n\n  def get_arm_matrix(self, arms):\n    """"""Puts all arms into a matrix.""""""\n    return np.stack(arms, axis=0)\n\n  @abc.abstractmethod\n  def get_arm(self, arms):\n    """"""Computes which arm to pull next.\n\n    Args:\n      arms: a list of feature vectors, one for each arm\n\n    Returns:\n      arm: the chosen arm\n      arm_ind: index of the chosen arm\n      scores: an array with arm scores\n    """"""\n\n\nclass UCB_GLM(GLMAlgorithm):  # pylint: disable=invalid-name\n  """"""UCB-GLM algorithm.\n\n  See ""Provably Optimal Algorithms for Generalized Linear Contextual Bandits"",\n  by Li et al. (2017).\n  """"""\n\n  def __init__(self, dim, horizon, sigma0=1., optimism_scaling=1.):\n    super(UCB_GLM, self).__init__(dim, sigma0, optimism_scaling)\n    # Set confidence interval scaling, by\n    # Theorem 2 in Li (2017)\n    # Provably Optimal Algorithms for Generalized Linear Contextual Bandits\n    crs = optimism_scaling  # confidence region scaling\n    delta = 1. / float(horizon)\n    sigma = 0.5\n    kappa = 0.25\n    # Confidence ellipsoid width (cew):\n    cew = (sigma / kappa) * (np.sqrt((self._dim / 2) *\n                                     np.log(1. + 2. * horizon / self._dim) +\n                                     np.log(1 / delta)))\n    self._ci_scaling = crs * cew\n\n  def get_arm(self, arms):\n    """"""Computes which arm to pull next.\n\n    Args:\n      arms: a list of feature vectors, one for each arm\n    Returns:\n      The selected arm, its index in arms, and the computed scores\n    """"""\n    arm_matrix = self.get_arm_matrix(arms)\n    gram = self._outer + np.eye(self._dim) / np.square(self._sigma0)\n    gram_inv = np.linalg.inv(gram)\n    ucbs = np.sqrt((np.matmul(arm_matrix, gram_inv) * arm_matrix).sum(axis=1))\n    # Estimate w\n    w, _ = self.solve_logistic_bandit()\n    # Compute UCB\n    mu = np.matmul(arm_matrix, w) + self._ci_scaling * ucbs\n    arm = np.random.choice(np.flatnonzero(mu == mu.max()))\n\n    return arms[arm], arm, mu\n\n  @staticmethod\n  def print():\n    return \'GLM-UCB\'\n\n\nclass GLM_TS(GLMAlgorithm):  # pylint: disable=invalid-name\n  """"""Thompson sampling algorithm for generalized linear models.\n\n  See ""Linear Thompson Sampling Revisited"" by Abeille and Lazaric (2017).\n  """"""\n\n  def get_arm(self, arms):\n    """"""Computes which arm to pull next.\n\n    Args:\n      arms: a list of feature vectors, one for each arm\n    Returns:\n      The selected arm, its index in arms, and the computed scores\n    """"""\n    arm_matrix = self.get_arm_matrix(arms)\n    w, gram = self.solve_logistic_bandit()\n    gram_inv = np.square(self._optimism_scaling) * np.linalg.inv(gram)\n\n    # Posterior sampling\n    w_tilde = np.random.multivariate_normal(w, gram_inv)\n    mu = np.matmul(arm_matrix, w_tilde)\n    # Argmax breaking ties randomly\n    arm = np.random.choice(np.flatnonzero(mu == mu.max()))\n\n    return arms[arm], arm, mu\n\n  @staticmethod\n  def print():\n    return \'GLM-TS\'\n'"
recsim/agents/bandits/glm_algorithms_test.py,3,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.agents.bandits.glm_algorithm.""""""\n\nimport numpy as np\nfrom recsim.agents.bandits import glm_algorithms\nfrom scipy import special\nimport tensorflow.compat.v1 as tf\n\n\nclass UCB_GLMTest(tf.test.TestCase):  # pylint: disable=invalid-name\n\n  def setUp(self):\n    super(UCB_GLMTest, self).setUp()\n    self._dim = 3\n    self._alg = glm_algorithms.UCB_GLM(self._dim, horizon=100)\n\n  def add_random_arms(self, n_arms):\n    # Add some random arms with rewards\n    for _ in range(n_arms):\n      arm = np.random.uniform(size=self._alg._dim)\n      reward = np.random.binomial(1, 0.5)  # Random reward\n      self._alg.update(reward, arm)\n\n  def test_update(self):\n    n_arms = 10\n    self.add_random_arms(n_arms)\n    self.assertLen(self._alg._arms, n_arms)\n\n  def test_arm_matrix(self):\n    n_arms = 10\n    arms = [np.random.uniform(size=self._dim) for _ in range(n_arms)]\n    arm_matrix = self._alg.get_arm_matrix(arms)\n    self.assertEqual(np.shape(arm_matrix), (n_arms, self._alg._dim))\n\n  def test_solve_logistic_bandit(self):\n    n_arms = 10\n    self.add_random_arms(n_arms)\n    w, gram = self._alg.solve_logistic_bandit()\n    self.assertEqual(np.shape(w), (self._dim,))\n    self.assertEqual(np.shape(gram), (self._dim, self._dim))\n\n  def test_learning(self):\n    # Construct a set of arms\n    n_arms = 10\n    arms = [np.random.uniform(size=self._dim) for _ in range(n_arms)]\n    # Sample weight vector\n    w_star = np.random.normal(size=self._dim)\n    rounds = 20\n    for _ in range(rounds):\n      arm, arm_id, scores = self._alg.get_arm(arms)\n      self.assertLess(arm_id, len(arms))\n      self.assertLen(scores, len(arms))\n      reward = np.random.binomial(1, special.expit(np.dot(arm, w_star)))\n      self._alg.update(reward, arm)\n\n\nclass GLM_TSTest(tf.test.TestCase):  # pylint: disable=invalid-name\n\n  def setUp(self):\n    super(GLM_TSTest, self).setUp()\n    self._dim = 3\n    self._alg = glm_algorithms.GLM_TS(self._dim)\n\n  def test_learning(self):\n    # Construct a set of arms\n    n_arms = 20\n    arms = [np.random.uniform(size=self._dim) for _ in range(n_arms)]\n    # Sample weight vector\n    w_star = np.random.normal(size=self._dim)\n    rounds = 10\n    for _ in range(rounds):\n      arm, arm_id, scores = self._alg.get_arm(arms)\n      self.assertLess(arm_id, len(arms))\n      self.assertLen(scores, len(arms))\n      reward = np.random.binomial(1, special.expit(np.dot(arm, w_star)))\n      self._alg.update(reward, arm)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
recsim/agents/dopamine/__init__.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
recsim/agents/dopamine/dqn_agent.py,8,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Recsim-specific Dopamine DQN agent and related utilities.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom dopamine.agents.dqn import dqn_agent\nfrom dopamine.replay_memory import circular_replay_buffer\nimport gin.tf\nfrom gym import spaces\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nDQNNetworkType = collections.namedtuple(\'dqn_network\', [\'q_values\'])\n\n\nclass ResponseAdapter(object):\n  """"""Custom flattening of responses to accommodate dopamine replay buffer.""""""\n\n  def __init__(self, input_response_space):\n    """"""Init function for ResponseAdapter.\n\n    Args:\n      input_response_space: this is assumed to be an instance of\n        gym.spaces.Tuple; each element of the tuple is has to be an instance\n        of gym.spaces.Dict consisting of feature_name: 0-d gym.spaces.Box\n          (single float) key-value pairs.\n    """"""\n    self._input_response_space = input_response_space\n    self._single_response_space = input_response_space.spaces[0]\n    self._response_names = list(self._single_response_space.spaces.keys())\n    self._response_shape = (len(input_response_space.spaces),\n                            len(self._response_names))\n    self._response_dtype = np.float32\n\n  @property\n  def response_names(self):\n    return self._response_names\n\n  @property\n  def response_shape(self):\n    return self._response_shape\n\n  @property\n  def response_dtype(self):\n    return self._response_dtype\n\n  def encode(self, responses):\n    response_tensor = np.zeros(self._response_shape, dtype=self._response_dtype)\n    for i, response in enumerate(responses):\n      # Note: the order of dictionary keys in the self._input_response_space is\n      # not necessarily the same as the order of the keys in the observation\n      # dictionary itself. To guarrantee the position of elements are consistent\n      # with the order in self._response_names, we iterate over keys explicitly.\n      for j, key in enumerate(self.response_names):\n        response_tensor[i, j] = response[key]\n    return response_tensor\n\n\n@gin.configurable\nclass ObservationAdapter(object):\n  """"""An adapter to convert between user/doc observation and images.""""""\n\n  def __init__(self, input_observation_space, stack_size=1):\n    self._input_observation_space = input_observation_space\n    user_space = input_observation_space.spaces[\'user\']\n    doc_space = input_observation_space.spaces[\'doc\']\n    self._num_candidates = len(doc_space.spaces)\n\n    doc_space_shape = spaces.flatdim(list(doc_space.spaces.values())[0])\n    # Use the longer of user_space and doc_space as the shape of each row.\n    obs_shape = (np.max([spaces.flatdim(user_space), doc_space_shape]),)\n    self._observation_shape = (self._num_candidates + 1,) + obs_shape\n    self._observation_dtype = user_space.dtype\n    self._stack_size = stack_size\n\n  # Pads an array with zeros to make its shape identical to  _observation_shape.\n  def _pad_with_zeros(self, array):\n    width = self._observation_shape[1] - len(array)\n    return np.pad(array, (0, width), mode=\'constant\')\n\n  @property\n  def output_observation_space(self):\n    """"""The output observation space of the adapter.""""""\n    user_space = self._input_observation_space.spaces[\'user\']\n    doc_space = self._input_observation_space.spaces[\'doc\']\n    user_dim = spaces.flatdim(user_space)\n    low = np.concatenate(\n        [self._pad_with_zeros(np.ones(user_dim) * -np.inf).reshape(1, -1)] + [\n            self._pad_with_zeros(np.ones(spaces.flatdim(d)) *\n                                 -np.inf).reshape(1, -1)\n            for d in doc_space.spaces.values()\n        ])\n    high = np.concatenate(\n        [self._pad_with_zeros(np.ones(user_dim) * np.inf).reshape(1, -1)] + [\n            self._pad_with_zeros(np.ones(spaces.flatdim(d)) *\n                                 np.inf).reshape(1, -1)\n            for d in doc_space.spaces.values()\n        ])\n    return spaces.Box(low=low, high=high, dtype=np.float32)\n\n  def encode(self, observation):\n    """"""Encode user observation and document observations to an image.""""""\n    # It converts the observation from the simulator to a numpy array to be\n    # consumed by DQN agent, which assume the input is a ""image"".\n    # The first row is user\'s observation. The remaining rows are documents\'\n    # observation, one row for each document.\n    image = np.zeros(\n        self._observation_shape + (self._stack_size,),\n        dtype=self._observation_dtype)\n    image[0, :, 0] = self._pad_with_zeros(\n        spaces.flatten(self._input_observation_space.spaces[\'user\'],\n                       observation[\'user\']))\n    doc_space = zip(self._input_observation_space.spaces[\'doc\'].spaces.values(),\n                    observation[\'doc\'].values())\n    image[1:, :, 0] = np.array([\n        self._pad_with_zeros(spaces.flatten(doc_space, d))\n        for doc_space, d in doc_space\n    ])\n\n    return image\n\n\n# The following functions creates the DQN network for RecSim.\ndef recsim_dqn_network(user, doc, scope):\n  inputs = tf.concat([user, doc], axis=1)\n  with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n    hidden = tf.keras.layers.Dense(256, activation=tf.nn.relu)(inputs)\n    hidden = tf.keras.layers.Dense(32, activation=tf.nn.relu)(hidden)\n    q_value = tf.keras.layers.Dense(1, name=\'output\')(hidden)\n  return q_value\n\n\nclass DQNAgentRecSim(dqn_agent.DQNAgent):\n  """"""RecSim-specific Dopamine DQN agent that converts the observation space.""""""\n\n  def __init__(self, sess, observation_space, num_actions, stack_size,\n               optimizer_name, eval_mode, **kwargs):\n    if stack_size != 1:\n      raise ValueError(\n          \'Invalid stack_size: %s. Only stack_size=1 is supported for now.\' %\n          stack_size)\n\n    self._env_observation_space = observation_space\n    # In our case, the observation is a data structure that stores observation\n    # of the user and candidate documents. We uses an observation adapter to\n    # convert it to an ""image"", which is required by dopamine DQNAgent.\n    self._obs_adapter = ObservationAdapter(self._env_observation_space)\n\n    if optimizer_name == \'adam\':\n      optimizer = tf.train.AdamOptimizer()\n    elif optimizer_name == \'sgd\':\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-3)\n    else:\n      optimizer = tf.train.RMSPropOptimizer(\n          learning_rate=0.00025,\n          decay=0.95,\n          momentum=0.0,\n          epsilon=0.00001,\n          centered=True)\n\n    dqn_agent.DQNAgent.__init__(\n        self,\n        sess,\n        num_actions,\n        observation_shape=self._obs_adapter.output_observation_space.shape,\n        observation_dtype=self._obs_adapter.output_observation_space.dtype,\n        stack_size=stack_size,\n        network=recsim_dqn_network,\n        optimizer=optimizer,\n        eval_mode=eval_mode,\n        **kwargs)\n\n  def _validate_states(self, states):\n    shape = states.get_shape()\n    if len(shape) != 4 or shape[1] != self._num_candidates + 1:\n      raise ValueError(\'Invalid states shape: %s. \'\n                       \'Expecting [batch_size, %s, num_features, stack_size].\' %\n                       (shape, self._num_candidates + 1))\n\n\ndef wrapped_replay_buffer(**kwargs):\n  return circular_replay_buffer.WrappedReplayBuffer(**kwargs)\n'"
recsim/agents/layers/__init__.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Module importing all layered agents.""""""\nfrom recsim.agents.layers import cluster_click_statistics\nfrom recsim.agents.layers import fixed_length_history\nfrom recsim.agents.layers import sufficient_statistics\nfrom recsim.agents.layers import temporal_aggregation\n'"
recsim/agents/layers/abstract_click_bandit.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Agent that picks topics based on the UCB1 algorithm given past responses.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport gin\nimport numpy as np\n\nfrom recsim import agent\nfrom recsim.agents.bandits import algorithms\n\n\n@gin.configurable\nclass AbstractClickBanditLayer(agent.AbstractHierarchicalAgentLayer):\n  """"""A hierarchical bandit layer which treats a set of base agents as arms.\n\n  This layer consumes a list of base agents with apriori unknown mean payoffs\n  and has the job of mixing them in a way that minimizes the regret relative to\n  the best among them. Each agent is assumed to output a slate of size up to\n  slate_size. If an agent outputs an incomplete slate, the AbstractClickBandit\n  will use other agent\'s outputs to complete it, packing them in decreasing\n  order according to the index of the bandit policy. E.g. if using the upper\n  confidence bound as index, the AbstractClickBandit will put the partial slate\n  of the highest-UCB base agent in first place, then the second, until the slate\n  is complete.\n  """"""\n\n  def __init__(self,\n               observation_space,\n               action_space,\n               arm_base_agent_ctors,\n               alg_ctor=algorithms.UCB1,\n               ci_scaling=1.0,\n               random_seed=0,\n               **kwargs):\n    """"""Initializes a new bandit agent for clustered arm exploration.\n\n    Args:\n      observation_space: Instance of a gym space corresponding to the\n        observation format.\n      action_space: A gym.spaces object that specifies the format of actions.\n      arm_base_agent_ctors: a list of agent constructors, each agent corresponds\n        to a bandit arm.\n      alg_ctor: A class of an MABAlgorithm for exploration, default to UCB1.\n      ci_scaling: A floating number specifying the scaling of confidence bound.\n      random_seed: An integer for random seed.\n      **kwargs: arguments for base agents.\n    """"""\n\n    super(AbstractClickBanditLayer, self).__init__(action_space,\n                                                   *arm_base_agent_ctors)\n    self._alg_ctor = alg_ctor\n    self._random_seed = random_seed\n    self._params = {\'optimism_scaling\': ci_scaling}\n    kwargs[\'observation_space\'] = observation_space\n    kwargs[\'action_space\'] = action_space\n    self._num_arms = len(self._base_agent_ctors)\n    user_observation_space = observation_space.spaces[\'user\'].spaces\n    if \'sufficient_statistics\' not in user_observation_space:\n      ValueError(\'observation_space.spaces[\\\'user\\\'] must contain \\\'sufficient_\'\n                 \'statistics\\\' key.\')\n    suffstat_observation_space = user_observation_space[\n        \'sufficient_statistics\'].spaces\n    if \'impression_count\' not in suffstat_observation_space:\n      ValueError(\'sufficient_statistics must contain \\\'impression_count\\\' key.\')\n    if \'click_count\' not in suffstat_observation_space:\n      ValueError(\'sufficient_statistics must contain \\\'click_count\\\' key.\')\n    if self._num_arms != suffstat_observation_space[\'impression_count\'].shape[0]:\n      ValueError(\'Dimension of impression_count must be equal to number \'\n                 \'of arms.\')\n    if self._num_arms != suffstat_observation_space[\'click_count\'].shape[0]:\n      ValueError(\'Dimension of click_count must be equal to number \' \'of arms.\')\n    self._base_agents = [\n        base_agent_ctor(**kwargs) for base_agent_ctor in self._base_agent_ctors\n    ]\n\n  def _postprocess_actions(self, actions):\n    slate = []\n    for action in actions:\n      if not bool(action):\n        continue\n      recs_to_use = min(len(action), self._slate_size - len(slate))\n      # Make sure action is not a numpy array.\n      slate += list(action[:recs_to_use])\n      if len(slate) == self._slate_size:\n        break\n    return slate\n\n  def step(self, reward, observation):\n    """"""Records the most recent transition and returns the agent\'s next action.\n\n    We store the observation of the last time step since we want to store it\n    with the reward.\n\n    Args:\n      reward: Unused.\n      observation: A dictionary that includes the most recent observations and\n        should have the following fields:\n        - user: A dictionary representing user\'s observed state. Assumes\n          observation[\'user\'][\'sufficient_statics\'] is a dictionary containing\n          base agent impression counts and base agent click counts.\n\n    Returns:\n      slate: An integer array of size _slate_size, where each element is an\n        index into the list of doc_obs\n    """"""\n    user_obs = observation[\'user\'][\'sufficient_statistics\']\n    pulls = user_obs[\'impression_count\']\n    clicks = user_obs[\'click_count\']\n    mab_alg = self._alg_ctor(len(pulls), self._params, self._random_seed)\n    mab_alg.set_state(pulls, clicks)\n    arm_pctr_ucb = mab_alg.get_score(np.sum(pulls))\n    # Use (topic_pctr_ucb, document_quality) as the criterion.\n    if all(pulls):\n      scores = arm_pctr_ucb\n    else:\n      # Pick the topics that have not beeen pulled.\n      scores = -pulls\n    arm_order = list(np.argsort(scores))\n    docs_so_far = 0\n    actions = []\n    while docs_so_far < self._slate_size:\n      arm = arm_order.pop()\n      action = self._base_agents[arm].step(reward, observation)\n      docs_so_far += len(action)\n      actions.append(action)\n    return self._postprocess_actions(actions)\n'"
recsim/agents/layers/cluster_click_statistics.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Helper class to collect cluster click and impression counts.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom gym import spaces\nimport numpy as np\n\nfrom recsim.agents.layers import sufficient_statistics\n\n\nclass ClusterClickStatsLayer(sufficient_statistics.SufficientStatisticsLayer):\n  """"""Track impressions and clicks on a per-cluster basis and pass down to agent.\n\n  This module assumes each document belongs to single cluster and we know the\n  number of possible clusters. Every time we increase impression count for a\n  cluster if the agent recommends a document from that cluster. We also increase\n  click count for a cluster if user responds a click.\n  """"""\n\n  def __init__(self, base_agent_ctor, observation_space, action_space,\n               **kwargs):\n    """"""Initializes a ClusterClickStatsLayer object.\n\n    Args:\n      base_agent_ctor: a constructor for the base agent.\n      observation_space: a gym.spaces object specifying the format of\n        observations.\n      action_space: A gym.spaces object that specifies the format of actions.\n      **kwargs: arguments to pass to the downstream agent at construction time.\n    """"""\n    single_response_space = observation_space.spaces[\'response\'].spaces[0]\n    if \'cluster_id\' not in single_response_space.spaces:\n      raise ValueError(\'observation_space.spaces[\\\'response\\\']\'\n                       \' must contain \\\'cluster_id\\\' key.\')\n    cluster_id_space = single_response_space.spaces[\'cluster_id\']\n    if isinstance(cluster_id_space, spaces.Box):\n      if len(cluster_id_space.high) > 1:\n        raise ValueError(\'cluster_id response field must be 0 dimensional.\')\n      num_clusters = cluster_id_space.high\n    elif isinstance(cluster_id_space, spaces.Discrete):\n      num_clusters = cluster_id_space.n\n    else:\n      raise ValueError(\'cluster_id response field must be either gym.spaces.Box\'\n                       \' or gym spaces.Discrete\')\n    self._num_clusters = num_clusters\n    if \'click\' not in single_response_space.spaces:\n      raise ValueError(\n          \'observation_space.spaces[\\\'response\\\'] must contain \\\'click\\\' key.\')\n    suf_stat_space = spaces.Dict({\n        \'impression_count\':\n            spaces.Box(\n                shape=(num_clusters,), dtype=np.float32, low=0.0, high=np.inf),\n        \'click_count\':\n            spaces.Box(\n                shape=(num_clusters,), dtype=np.float32, low=0.0, high=np.inf)\n    })\n    super(ClusterClickStatsLayer,\n          self).__init__(base_agent_ctor, observation_space, action_space,\n                         suf_stat_space, **kwargs)\n\n  def _create_observation(self):\n    return {\n        \'impression_count\':\n            np.array(self._sufficient_statistics[\'impression_count\']),\n        \'click_count\':\n            np.array(self._sufficient_statistics[\'click_count\']),\n    }\n\n  def _update(self, observation):\n    """"""Updates user impression/click count given user response on each item.""""""\n    if self._sufficient_statistics is None:\n      self._sufficient_statistics = {\n          \'impression_count\': [\n              0,\n          ] * self._num_clusters,\n          \'click_count\': [\n              0,\n          ] * self._num_clusters\n      }\n    if observation[\'response\'] is not None:\n      for response in observation[\'response\']:\n        cluster_id = int(response[\'cluster_id\'])\n        self._sufficient_statistics[\'impression_count\'][cluster_id] += 1\n        if response[\'click\']:\n          self._sufficient_statistics[\'click_count\'][cluster_id] += 1\n'"
recsim/agents/layers/cluster_click_statistics_test.py,2,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.agents.layers.cluster_click_statistics.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom gym import spaces\nimport mock\nimport numpy as np\nfrom recsim.agents import cluster_bandit_agent\nfrom recsim.agents.layers import cluster_click_statistics\nimport tensorflow.compat.v1 as tf\n\n\nclass ClusterClickStatisticsTest(tf.test.TestCase):\n\n  def setUp(self):\n    self.slate_size = 2\n    self.num_clusters = 2\n    super(ClusterClickStatisticsTest, self).setUp()\n    self.test_action_space = mock.Mock(nvec=mock.Mock(shape=[\n        2,\n    ]))\n    single_response_space = spaces.Dict({\n        \'click\': spaces.Discrete(2),\n        \'cluster_id\': spaces.Discrete(self.num_clusters)\n    })\n\n    self.test_observation_space = spaces.Dict({\n        \'user\':\n            spaces.Discrete(2),\n        \'response\':\n            spaces.Tuple(tuple([\n                single_response_space,\n            ] * self.slate_size)),\n        \'doc\':\n            spaces.Tuple((spaces.Discrete(4),))\n    })\n    self.mock_agent = mock.create_autospec(\n        cluster_bandit_agent.ClusterBanditAgent)\n    self.click_stats = cluster_click_statistics.ClusterClickStatsLayer(\n        self.mock_agent,\n        self.test_observation_space,\n        self.test_action_space,\n        kwarg_for_agent=-1)\n\n  def test_initialization(self):\n    # Base agent got constructed correctly.\n    self.mock_agent.assert_called_once()\n    mock_agent_args, mock_agent_kwargs = self.mock_agent.call_args\n    self.assertEmpty(mock_agent_args)\n    self.assertCountEqual(\n        mock_agent_kwargs.keys(),\n        [\'observation_space\', \'action_space\', \'kwarg_for_agent\'])\n    # Environment features and kwargs are passed down to base agent.\n    self.assertEqual(mock_agent_kwargs[\'action_space\'], self.test_action_space)\n    self.assertEqual(mock_agent_kwargs[\'kwarg_for_agent\'], -1)\n    # Augmented observation is space properly formatted.\n    augmented_observation_space = mock_agent_kwargs[\'observation_space\']\n    for field in [\'doc\', \'response\']:\n      self.assertEqual(augmented_observation_space.spaces[field],\n                       self.test_observation_space[field])\n    self.assertCountEqual(\n        augmented_observation_space.spaces[\'user\'].spaces.keys(),\n        [\'raw_observation\', \'sufficient_statistics\'])\n    self.assertEqual(\n        augmented_observation_space.spaces[\'user\'][\'raw_observation\'],\n        self.test_observation_space[\'user\'])\n    suff_stat_space = spaces.Dict({\n        \'impression_count\':\n            spaces.Box(\n                shape=(self.num_clusters,),\n                dtype=np.float32,\n                low=0.0,\n                high=np.inf),\n        \'click_count\':\n            spaces.Box(\n                shape=(self.num_clusters,),\n                dtype=np.float32,\n                low=0.0,\n                high=np.inf)\n    })\n    self.assertEqual(\n        augmented_observation_space.spaces[\'user\'][\'sufficient_statistics\'],\n        suff_stat_space)\n\n  def test_update_and_observation(self):\n    observation = {\'response\': None}\n    self.click_stats._update(observation)\n    observation = {\n        \'response\': ({\n            \'click\': 1,\n            \'cluster_id\': 0\n        }, {\n            \'click\': 0,\n            \'cluster_id\': 1\n        })\n    }\n    self.click_stats._update(observation)\n    observation = {\n        \'response\': ({\n            \'click\': 0,\n            \'cluster_id\': 0\n        }, {\n            \'click\': 1,\n            \'cluster_id\': 1\n        })\n    }\n    self.click_stats._update(observation)\n    obs = self.click_stats._create_observation()\n    self.assertAllEqual(obs[\'impression_count\'], np.array([2, 2]))\n    self.assertAllEqual(obs[\'click_count\'], np.array([1, 1]))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
recsim/agents/layers/fixed_length_history.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Helper classe to record fixed length history of observations.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom gym import spaces\n\nfrom recsim.agents.layers import sufficient_statistics\n\n\nclass FixedLengthHistoryLayer(sufficient_statistics.SufficientStatisticsLayer):\n  r""""""Creates a buffer of the last k rewards and observations.\n\n  This module introduces sufficient statistics in the form of a buffer holding\n  the last k (specified by history_length) observations. This buffer is injected\n  into observation_space[\\\'user\\\'][\\\'sufficient_statistics\\\'] in the form of a\n  gym.spaces.Tuple of length up to k . In the first k-1 steps of the episode\n  there are not enough observations to fill the buffer, so they will be filled\n  with None. Each non-vacuous element of the tuple is an instance of\n  (a subset of) observation_space.\n  """"""\n\n  def __init__(self,\n               base_agent_ctor,\n               observation_space,\n               action_space,\n               history_length,\n               remember_user=True,\n               remember_response=True,\n               remember_doc=False,\n               **kwargs):\n    r""""""Initializes a FixedLengthHistoryLayer object.\n\n    Args:\n      base_agent_ctor: a constructor for the base agent.\n      observation_space: a gym.spaces object specifying the format of\n        observations.\n      action_space: A gym.spaces object that specifies the format of actions.\n      history_length: positive integer number of observations to remember.\n      remember_user: boolean, indicates whether to track\n        observation_space[\\\'user\\\'].\n      remember_response: boolean, indicates whether to track\n        observation_space[\\\'response\\\'].\n      remember_doc: boolean, indicates whether to track\n        observation_space[\\\'doc\\\'].\n      **kwargs: arguments to pass to the downstream agent at construction time.\n    """"""\n\n    self._history_length = history_length\n    self._features = []\n    if remember_user:\n      self._features.append(\'user\')\n    if remember_response:\n      self._features.append(\'response\')\n    if remember_doc:\n      self._features.append(\'doc\')\n    observation_space_to_remember = spaces.Dict({\n        feature: observation_space[feature] for feature in self._features\n    })\n    suf_stat_space = spaces.Tuple([\n        observation_space_to_remember,\n    ] * history_length)\n    super(FixedLengthHistoryLayer,\n          self).__init__(base_agent_ctor, observation_space, action_space,\n                         suf_stat_space, **kwargs)\n\n  def _create_observation(self):\n    return tuple(self._sufficient_statistics)\n\n  def _update(self, observation):\n    """"""Updates user impression/click count given user response on each item.""""""\n    if self._sufficient_statistics is None:\n      self._sufficient_statistics = self._history_length * [\n          None,\n      ]\n\n    observation_to_remember = {\n        feature: observation[feature] for feature in self._features\n    }\n    self._sufficient_statistics = [\n        observation_to_remember,\n    ] + self._sufficient_statistics[:-1]\n'"
recsim/agents/layers/fixed_length_history_test.py,2,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for recsim.agents.layers.fixed_length_history.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom gym import spaces\nimport mock\nfrom recsim.agents import cluster_bandit_agent\nfrom recsim.agents.layers import fixed_length_history\nimport tensorflow.compat.v1 as tf\n\n\nclass FixedLengthHistoryTest(tf.test.TestCase):\n\n  def setUp(self):\n    self.history_length = 3\n    self.slate_size = 1\n    super(FixedLengthHistoryTest, self).setUp()\n    self.test_action_space = mock.Mock(nvec=mock.Mock(shape=[2,]))\n    self.test_observation_space = spaces.Dict({\n        \'user\': spaces.Discrete(2),\n        \'response\': spaces.Discrete(3),\n        \'doc\': spaces.Tuple((spaces.Discrete(4),))\n    })\n    self.mock_agent = mock.create_autospec(\n        cluster_bandit_agent.ClusterBanditAgent)\n    self.history = fixed_length_history.FixedLengthHistoryLayer(\n        self.mock_agent,\n        self.test_observation_space,\n        self.test_action_space,\n        self.history_length,\n        remember_user=True,\n        remember_response=True,\n        remember_doc=True,\n        kwarg_for_agent=-1)\n\n  def test_initialization(self):\n    # Basic object properties initialized correctly.\n    self.assertEqual(self.history._history_length, self.history_length)\n    self.assertCountEqual(self.history._features, [\'user\', \'doc\', \'response\'])\n    # Base agent got constructed correctly.\n    self.mock_agent.assert_called_once()\n    mock_agent_args, mock_agent_kwargs = self.mock_agent.call_args\n    self.assertEmpty(mock_agent_args)\n    self.assertCountEqual(\n        mock_agent_kwargs.keys(),\n        [\'observation_space\', \'action_space\', \'kwarg_for_agent\'])\n    # Environment features and kwargs are passed down to base agent.\n    self.assertEqual(mock_agent_kwargs[\'action_space\'], self.test_action_space)\n    self.assertEqual(mock_agent_kwargs[\'kwarg_for_agent\'], -1)\n    # Augmented observation is space properly formatted.\n    augmented_observation_space = mock_agent_kwargs[\'observation_space\']\n    for field in [\'doc\', \'response\']:\n      self.assertEqual(augmented_observation_space.spaces[field],\n                       self.test_observation_space[field])\n    self.assertCountEqual(\n        augmented_observation_space.spaces[\'user\'].spaces.keys(),\n        [\'raw_observation\', \'sufficient_statistics\'])\n    self.assertEqual(\n        augmented_observation_space.spaces[\'user\'][\'raw_observation\'],\n        self.test_observation_space[\'user\'])\n    self.assertLen(\n        augmented_observation_space.spaces[\'user\']\n        .spaces[\'sufficient_statistics\'].spaces, self.history_length)\n    for space in (augmented_observation_space.spaces[\'user\']\n                  .spaces[\'sufficient_statistics\'].spaces):\n      self.assertEqual(space, self.test_observation_space)\n\n  def test_update_and_observation(self):\n    observation = self.test_observation_space.sample()\n    self.assertIsNone(self.history._sufficient_statistics)\n    self.history._update(observation)\n    self.assertEqual(self.history._sufficient_statistics,\n                     [observation, None, None])\n    self.assertEqual(self.history._create_observation(),\n                     (observation, None, None))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
recsim/agents/layers/sufficient_statistics.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Helper classes to record user response history on recommendations.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nfrom gym import spaces\n\nfrom recsim import agent\n\n\nclass SufficientStatisticsLayer(agent.AbstractHierarchicalAgentLayer):\n  """"""A module to log user responses on different clusters.\n\n  This module assumes each document belongs to single cluster and we know the\n  number of possible clusters. Every time we increase impression count for a\n  cluster if the agent recommends a document from that cluster. We also increase\n  click count for a cluster if user responds a click.\n  """"""\n\n  def __init__(self, base_agent_ctor, observation_space, action_space,\n               sufficient_statistics_space, **kwargs):\n    """"""Initializes a UserClusterHistory object.\n\n    Args:\n      base_agent_ctor: a constructor for the base agent.\n      observation_space: a gym.spaces object specifying the format of\n        observations.\n      action_space: A gym.spaces object that specifies the format of actions.\n      sufficient_statistics_space: a gym.spaces object specifying the format of\n        the created sufficient statistics.\n      **kwargs: arguments to pass to the downstream agent at construction time.\n    """"""\n    super(SufficientStatisticsLayer, self).__init__(action_space,\n                                                    base_agent_ctor)\n    self._sufficient_statistcs_space = sufficient_statistics_space\n    self._sufficient_statistics = None\n    augmented_observation_space = {\n        \'user\':\n            spaces.Dict({\n                \'raw_observation\': observation_space.spaces[\'user\'],\n                \'sufficient_statistics\': sufficient_statistics_space\n            }),\n        \'response\':\n            observation_space.spaces[\'response\'],\n        \'doc\':\n            observation_space.spaces[\'doc\']\n    }\n    self._observation_space = observation_space\n    self._base_observation_space = spaces.Dict(augmented_observation_space)\n    kwargs[\'observation_space\'] = self._base_observation_space\n    kwargs[\'action_space\'] = action_space\n    self._base_agents = [\n        self._base_agent_ctors[0](**kwargs),\n    ]\n    self._reset()\n\n  @property\n  def observation_space(self):\n    return self._observation_space\n\n  @abc.abstractmethod\n  def _update(self, observation):\n    """"""Updates self._sufficient_statistics given a new observation.\n\n    If self._sufficient_statistics is None, this function must also initialize\n    it.\n\n    Args:\n     observation: an observation conforming to self._observation_space.\n    """"""\n\n  @abc.abstractmethod\n  def _create_observation(self):\n    """"""Formats self._sufficient_statistics into an observation.""""""\n\n  def _preprocess_reward_observation(self, reward, observation):\n    self._update(observation)\n    augmented_observation = {key: value for key, value in observation.items()}\n    augmented_observation[\'user\'] = {\n        \'raw_observation\': augmented_observation[\'user\'],\n        \'sufficient_statistics\': self._create_observation()\n    }\n    return reward, augmented_observation\n\n  def _postprocess_actions(self, action_list):\n    # Does not modify the action of the base agent.\n    return action_list[0]\n\n  def step(self, reward, observation):\n    reward, augmented_observation = self._preprocess_reward_observation(\n        reward, observation)\n    action_list = [\n        base_agent.step(reward, augmented_observation)\n        for base_agent in self._base_agents\n    ]\n    return self._postprocess_actions(action_list)\n\n  def end_episode(self, reward, observation):\n    super(SufficientStatisticsLayer, self).end_episode(reward, observation)\n    self._reset()\n\n  def _reset(self):\n    """"""Resets the memory for a new user.""""""\n    self._sufficient_statistics = None\n'"
recsim/agents/layers/temporal_aggregation.py,0,"b'# coding=utf-8\n# coding=utf-8\n# Copyright 2019 The RecSim Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Temporally aggregated reinforcement learning agent.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nfrom gym import spaces\nimport numpy as np\n\nfrom recsim import agent\nfrom recsim.agents import agent_utils\n\n\nclass TemporalAggregationLayer(agent.AbstractHierarchicalAgentLayer):\n  """"""Temporally aggregated reinforcement learning agent.\n\n  A reinforcement learning agent that implements learns a temporally aggregated\n  policy. This is achieved in two ways:\n    * making a decision only every k-steps;\n    * introducing a switching cost penalty whenever the policy executes two\n      different consequitve actions.\n  See ""Advantage Amplification in Slowly Evolving Latent-State Environments""\n  Martin Mladenov, Ofer Meshi, Jayden Ooi, Dale Schuurmans, Craig Boutilier\n  https://arxiv.org/abs/1905.13559\n  for details.\n  Implementation-wise, this agent relies on an event-level (base) agent suited\n  for the domain.\n  Aggregation is implemented as a preprocessing step for the base agent:\n  in the first case only calling the agent every k steps (and adjusting the\n  discount (gamma) accordingly to keep the objective consistent), in the second\n  case subtracting a penalty from the environment reward whenever the base agent\n  switches actions. For switching cost, it is also necessary to append the last\n  executed action to the\n  state representation, otherwise the learning problem becomes non-Markovian.\n\n  The two methods are not mutually exclusive and may be used in conjunction by\n  specifying a non-unit aggregation_period and a non-zero switching_cost.\n  """"""\n\n  def __init__(self,\n               base_agent_ctor,\n               observation_space,\n               action_space,\n               gamma=0.0,\n               aggregation_period=1,\n               switching_cost=1.0,\n               document_comparison_fcn=None,\n               **kwargs):\n    """"""TemporallyAggregatedAgent init.\n\n    Args:\n      base_agent_ctor: a constructor for the base agent.\n      observation_space: a gym.spaces object specifying the format of\n        observations.\n      action_space: A gym.spaces object that specifies the format of actions.\n      gamma: geometric discounting factor between [0, 1) for the event-level\n        objective.\n      aggregation_period: number of time steps to hold an action fixed.\n      switching_cost: a non-negative penalty for switching an action.\n      document_comparison_fcn: a function taking two document observations and\n        returning a Boolean value that indicates if they are considered\n        equivalent. This is useful for making decisions at a higher abstraction\n        level (e.g. comparing only document topics). If not provided, this will\n        default to direct observation equality.\n      **kwargs: base_agent initialization args.\n    """"""\n    super(TemporalAggregationLayer, self).__init__(action_space,\n                                                   base_agent_ctor)\n    self._step_count = 0\n    self._observation_space = observation_space\n    self._aggregation_period = aggregation_period\n    self._gamma_accumulator = 1.0\n    self._reward_accumulator = 0.0\n    self._switching_cost = switching_cost\n    self._gamma = gamma\n    single_doc_space = list(observation_space.spaces[\'doc\'].spaces.values())[0]\n    if document_comparison_fcn is None:\n      self._doc_equality_walker = agent_utils.GymSpaceWalker(\n          single_doc_space, self._spaces_equal).apply_and_flatten\n      self._doc_comparator = self._default_doc_comparator\n    else:\n      self._doc_comparator = document_comparison_fcn\n    self._slate_comparator = self._default_slate_comparator\n    if aggregation_period > 1:\n      base_agent_gamma = gamma**aggregation_period\n    else:\n      base_agent_gamma = gamma\n    if switching_cost > 0.0:\n      # Attach documents in the last slate as a user observation.\n      slate_tuple = tuple([single_doc_space] * self._slate_size)\n      last_slate_space = spaces.Tuple(slate_tuple)\n      augmented_observation_space = {\n          \'user\':\n              spaces.Dict({\n                  \'original_observation\': observation_space.spaces[\'user\'],\n                  \'last_slate\': last_slate_space\n              }),\n          \'response\':\n              observation_space.spaces[\'response\'],\n          \'doc\':\n              observation_space.spaces[\'doc\']\n      }\n      base_observation_space = spaces.Dict(augmented_observation_space)\n    else:\n      base_observation_space = observation_space\n    self._base_observation_space = base_observation_space\n    kwargs[\'observation_space\'] = base_observation_space\n    kwargs[\'gamma\'] = base_agent_gamma\n    kwargs[\'action_space\'] = action_space\n    self._base_agents = [\n        base_agent_ctor(**kwargs),\n    ]\n    self._last_slate = None\n    self._previous_last_slate = None\n\n  def _default_doc_comparator(self, doc1, doc2):\n    return all(self._doc_equality_walker([doc1, doc2]))\n\n  def _default_slate_comparator(self, slate1, slate2):\n    return all([\n        self._doc_comparator(doc1_obs, doc2_obs)\n        for doc1_obs, doc2_obs in zip(slate1, slate2)\n    ])\n\n  def _spaces_equal(self, gym_space, gym_observations, abs_tolerance=10E-5):\n    if isinstance(gym_space, spaces.box.Box):\n      all_equal = [\n          True,\n      ]\n      gym_observation0 = np.array(gym_observations[0])\n      for gym_observation in gym_observations[1:]:\n        gym_observation = np.array(gym_observations)\n        if not np.allclose(\n            gym_observation0, gym_observation, atol=abs_tolerance):\n          all_equal = [\n              False,\n          ]\n    elif isinstance(gym_space, spaces.discrete.Discrete):\n      all_equal = [\n          not gym_observations or\n          gym_observations.count(gym_observations[0]) == len(gym_observations),\n      ]\n    else:\n      raise NotImplementedError(\'Gym space type \' + str(type(gym_space)) +\n                                \' not implemented yet.\')\n    return list(all_equal)\n\n  def _preprocess_reward_observation(self, reward, observation):\n    # Aggregate reward and adjust discount.\n    if self._switching_cost > 0.0:\n      if self._last_slate is None:\n        # Invent some fake first slate for consistency.\n        self._last_slate = tuple(observation[\'doc\'].values())[:self._slate_size]\n        self._previous_last_slate = self._last_slate\n      # Augment state space if switching cost is nonzero.\n      observation[\'user\'] = {\n          \'original_observation\': observation[\'user\'],\n          \'last_slate\': self._last_slate\n      }\n      # Penalize action switch.\n      if not self._slate_comparator(self._last_slate,\n                                    self._previous_last_slate):\n        reward -= self._switching_cost\n    self._reward_accumulator += self._gamma_accumulator * reward\n    self._gamma_accumulator *= self._gamma\n    return self._reward_accumulator, observation\n\n  def _postprocess_actions(self, action_list):\n    # Does not modify the action of the base agent.\n    return action_list[0]\n\n  def step(self, reward, observation):\n    """"""Preprocesses the reward and observation and calls base agent.\n\n    Args:\n      reward: The reward received from the agent\'s most recent action as a\n        float.\n      observation: A dictionary that includes the most recent observations and\n        should have the following fields:\n        - user: A NumPy array representing user\'s observed state. Assumes it is\n          a concatenation of topic pull counts and topic click counts.\n        - doc: A NumPy array representing observations of document features.\n          Assumes it is a concatenation of one-hot encoding of topic_id and\n          document quality.\n\n    Returns:\n      slate: An integer array of size _slate_size, where each element is an\n        index into the list of doc_obs.\n    Raises:\n      RuntimeError: if the agent has to hold a slate with given features fixed\n        for k steps but the documents needed to reconstruct that slate\n        become unavailable.\n    """"""\n    reward, observation = self._preprocess_reward_observation(\n        reward, observation)\n    # Is this a decision period?\n    if not self._step_count % self._aggregation_period:\n      new_slate_index = self._base_agents[0].step(reward,\n                                                  observation)\n      new_slate_features = [\n          tuple(observation[\'doc\'].values())[i] for i in new_slate_index\n      ]\n      self._previous_last_slate = self._last_slate\n      self._last_slate = new_slate_features\n      slate = new_slate_index\n      self._gamma_accumulator = 1.0\n      self._reward_accumulator = 0.0\n    else:\n      # Not a decision period, we need to recreate the fixed slate by finding\n      # docs with the same features.\n      documents_to_find = list(range(self._slate_size))\n      slate = [None] * self._slate_size\n      for i, doc_features in enumerate(observation[\'doc\'].values()):\n        for missing_doc_position, missing_doc in enumerate(documents_to_find):\n          if not self._doc_comparator(doc_features,\n                                      self._last_slate[missing_doc]):\n            slate[missing_doc] = i\n            documents_to_find.pop(missing_doc_position)\n            break\n        if not documents_to_find:\n          break\n      if documents_to_find:\n        raise RuntimeError((\'Temporal aggregation could not recreate previous \'\n                            \'slate because items became unavailable.\'))\n\n    return slate\n'"
