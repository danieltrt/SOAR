file_path,api_count,code
back_translate/sent_to_paragraph.py,3,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Compose paraphrased sentences back to paragraphs.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nfrom absl import app\nfrom absl import flags\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    ""input_file"", """", ""back translated file of sentences."")\nflags.DEFINE_string(\n    ""output_file"", """", ""paraphrased sentences."")\nflags.DEFINE_string(\n    ""doc_len_file"", """", ""The file that records the length information."")\n\n\ndef main(argv):\n  with tf.gfile.Open(FLAGS.input_file) as inf:\n    sentences = inf.readlines()\n  with tf.gfile.Open(FLAGS.doc_len_file) as inf:\n    doc_len_list = json.load(inf)\n  cnt = 0\n  print(""\\n"" * 2)\n  print(""*** printing paraphrases ***"")\n  with tf.gfile.Open(FLAGS.output_file, ""w"") as ouf:\n    for i, sent_num in enumerate(doc_len_list):\n      para = """"\n      for _ in range(sent_num):\n        para += sentences[cnt].strip() + "" ""\n        cnt += 1\n      print(""paraphrase {}: {}"".format(i, para))\n      ouf.write(para.strip() + ""\\n"")\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
back_translate/split_paragraphs.py,9,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Split the paragraph into sentences for back translation since .\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\nimport tempfile\nfrom absl import app\nfrom absl import flags\nimport nltk\n\nimport tensorflow as tf\n\n\nFLAGS = flags.FLAGS\nflags.DEFINE_string(\n    ""input_file"", """",\n    ""The file to be back translated."")\nflags.DEFINE_string(\n    ""doc_len_file"", """",\n    ""The directory that stores the information of the splitted paragraph."")\nflags.DEFINE_string(\n    ""output_file"", """",\n    ""The directory that stores the splitted sentences."")\nflags.DEFINE_integer(\n    ""replicas"", 1,\n    ""An argument for parallel preprocessing. For example, when replicas=3,""\n    ""we divide the data into three parts, and only process one part""\n    ""according to the worker_id."")\nflags.DEFINE_integer(\n    ""worker_id"", 0,\n    ""An argument for parallel preprocessing. See \'replicas\' for more details"")\n\n\ndef split_sent_by_punc(sent, punc, offset):\n  """"""Further split sentences when nltk\'s sent_tokenizer fail.""""""\n  sent_list = []\n  start = 0\n  while start < len(sent):\n    if punc:\n      pos = sent.find(punc, start + offset)\n    else:\n      pos = start + offset\n    if pos != -1:\n      sent_list += [sent[start: pos + 1]]\n      start = pos + 1\n    else:\n      sent_list += [sent[start:]]\n      break\n  return sent_list\n\n\ndef divide_data_for_worker(contents):\n  data_per_worker = len(contents) // FLAGS.replicas\n  remainder = len(contents) - FLAGS.replicas * data_per_worker\n  worker_id = FLAGS.worker_id\n  if worker_id < remainder:\n    start = (data_per_worker + 1) * worker_id\n    end = (data_per_worker + 1) * (worker_id + 1)\n  else:\n    start = data_per_worker * worker_id + remainder\n    end = data_per_worker * (worker_id + 1) + remainder\n  if worker_id == FLAGS.replicas - 1:\n    assert end == len(contents)\n  tf.logging.info(""processing data from {:d} to {:d}"".format(start, end))\n  contents = contents[start: end]\n  return contents\n\n\ndef main(_):\n  sent_tokenizer = nltk.tokenize.sent_tokenize\n\n\n  tf.logging.info(""loading input data"")\n  with tf.gfile.Open(FLAGS.input_file) as inf:\n    contents = inf.readlines()\n  tf.logging.info(""finished loading input data"")\n  assert len(contents) >= FLAGS.replicas\n\n  contents = divide_data_for_worker(contents)\n\n  new_contents = []\n  doc_len = []\n  # Split paragraphs into sentences since the model is trained on sentence-level\n  # translations.\n  tf.logging.info(""splitting sentence"")\n  for i in range(len(contents)):\n    contents[i] = contents[i].strip()\n    if isinstance(contents[i], bytes):\n      contents[i] = contents[i].decode(""utf-8"")\n    sent_list = sent_tokenizer(contents[i])\n    has_long = False\n    if i % 100 == 0:\n      tf.logging.info(""splitting sentence {:d}"".format(i))\n    for split_punc in [""."", "";"", "","", "" "", """"]:\n      if split_punc == "" "" or not split_punc:\n        offset = 100\n      else:\n        offset = 5\n      has_long = False\n      new_sent_list = []\n      for sent in sent_list:\n        if len(sent) < 300:\n          new_sent_list += [sent]\n        else:\n          has_long = True\n          sent_split = split_sent_by_punc(sent, split_punc, offset)\n          new_sent_list += sent_split\n      sent_list = new_sent_list\n      if not has_long:\n        break\n\n    # free up memory\n    contents[i] = None\n    doc_len += [len(sent_list)]\n    #  nltk.sent_tokenize in python2 will omit some unicode characters\n    for st in sent_list:\n      new_contents += [st]\n\n  tf.logging.info(""finished spliting paragraphs"")\n\n  with tf.gfile.Open(FLAGS.output_file, ""w"") as ouf:\n    for st in new_contents:\n      ouf.write(st + ""\\n"")\n  with tf.gfile.Open(FLAGS.doc_len_file, ""w"") as ouf:\n    json.dump(doc_len, ouf)\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
image/data.py,25,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Loading module of CIFAR && SVHN.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport json\nimport os\n\nfrom absl import flags\nimport absl.logging as _logging  # pylint: disable=unused-import\n\nimport tensorflow as tf\n\n\nFLAGS = flags.FLAGS\n\n\ndef format_sup_filename(split, sup_size=-1):\n  if split == ""test"":\n    return ""test.tfrecord""\n  elif split == ""train"" or split == ""dev"":\n    if sup_size == -1:\n      return ""{}-full.tfrecord"".format(split)\n    else:\n      return ""{}-size_{:d}.tfrecord"".format(split, sup_size)\n  else:\n    assert False\n\n\ndef format_unsup_filename(aug_copy_num):\n  return ""unsup-{:d}.tfrecord"".format(aug_copy_num)\n\n\ndef _postprocess_example(example):\n  """"""Convert tensor type for TPU, cast int64 into int32 and cast sparse to dense.""""""\n  for key in list(example.keys()):\n    val = example[key]\n    if tf.keras.backend.is_sparse(val):\n      val = tf.sparse.to_dense(val)\n    if val.dtype == tf.int64:\n      val = tf.to_int32(val)\n    example[key] = val\n\n\ndef get_dataset(file_prefix_list, record_spec, task_name,\n                split, per_core_bsz):\n\n  is_training = (split == ""train"")\n  is_training_tensor = tf.constant(is_training, dtype=tf.bool)\n\n  def apply_normal_aug(image):\n    if task_name == ""cifar10"":\n      image = flip(image, is_training_tensor)\n    image = crop(image, is_training_tensor)\n    return image\n\n  def parser(record):\n    # retrieve serialized example\n    example = tf.parse_single_example(\n        serialized=record,\n        features=record_spec)\n    # reshape image back to 3D shape\n    for key in example.keys():\n      if ""image"" in key:\n        example[key] = tf.reshape(example[key], [32, 32, 3])\n        example[key] = apply_normal_aug(example[key])\n\n    _postprocess_example(example)\n\n    return example\n\n  all_file_list = []\n  for file_prefix in file_prefix_list:\n    cur_file_list = tf.contrib.slim.parallel_reader.get_data_files(\n        file_prefix + ""*"")\n    all_file_list += cur_file_list\n  dataset = tf.data.Dataset.from_tensor_slices(all_file_list)\n\n  if is_training:\n    dataset = dataset.shuffle(len(all_file_list)).repeat()\n  # read from 4 tfrecord files in parallel\n  dataset = dataset.apply(\n      tf.contrib.data.parallel_interleave(\n          tf.data.TFRecordDataset,\n          sloppy=is_training,\n          cycle_length=4))\n\n  if is_training:\n    # Shuffle and then repeat to maintain the epoch boundary\n    dataset = dataset.shuffle(100000)\n    dataset = dataset.repeat()\n\n  dataset = dataset.map(parser, num_parallel_calls=32)\n  dataset = dataset.batch(per_core_bsz, drop_remainder=True)\n\n  # Safe guard the case that the shuffle buffer size for record is smaller\n  # than the batch size.\n  if is_training:\n    dataset = dataset.shuffle(512)\n  dataset = dataset.prefetch(1)\n\n  return dataset\n\n\ndef flip(image, is_training):\n  def func(inp):\n    flips = tf.to_float(tf.random_uniform([1, 1, 1], 0, 2, tf.int32))\n    flipped_inp = tf.image.flip_left_right(inp)\n    return flips * flipped_inp + (1 - flips) * inp\n\n  return tf.cond(is_training, lambda: func(image), lambda: image)\n\n\ndef crop(image, is_training):\n  def func(inp):\n    amount = 4\n    pad_inp = tf.pad(inp,\n                     tf.constant([[amount, amount],\n                                  [amount, amount],\n                                  [0, 0]]),\n                     ""REFLECT"")\n    cropped_data = tf.random_crop(pad_inp, tf.shape(image))\n    return cropped_data\n\n  return tf.cond(is_training, lambda: func(image), lambda: image)\n\n\ndef get_input_fn(\n    data_dir, split, task_name, sup_size=-1,\n    unsup_ratio=0, aug_copy=0):\n\n  def input_fn(params):\n    per_core_bsz = params[""batch_size""]\n\n    datasets = []\n    # Supervised data\n    filename = format_sup_filename(split, sup_size=sup_size)\n    sup_record_spec = {\n        ""image"": tf.FixedLenFeature([32 * 32 * 3], tf.float32),\n        ""label"": tf.FixedLenFeature([1], tf.int64)\n    }\n    sup_file_list = [os.path.join(data_dir, filename)]\n    tf.logging.info(""getting supervised dataset from {} file prefixes"".format(\n        len(sup_file_list)))\n    sup_dataset = get_dataset(\n        file_prefix_list=sup_file_list,\n        record_spec=sup_record_spec,\n        task_name=task_name,\n        split=split,\n        per_core_bsz=per_core_bsz,\n    )\n\n    datasets.append(sup_dataset)\n\n    if unsup_ratio > 0:\n      aug_record_spec = {\n          ""ori_image"": tf.FixedLenFeature([32 * 32 * 3], tf.float32),\n          ""aug_image"": tf.FixedLenFeature([32 * 32 * 3], tf.float32),\n      }\n      aug_file_list = [\n          os.path.join(data_dir, format_unsup_filename(i)) for i in range(aug_copy)]\n      tf.logging.info(\n          ""getting unsupervised dataset from {} file prefixes"".format(\n              len(aug_file_list))\n      )\n      aug_dataset = get_dataset(\n          file_prefix_list=aug_file_list,\n          record_spec=aug_record_spec,\n          task_name=task_name,\n          split=split,\n          per_core_bsz=per_core_bsz * unsup_ratio\n      )\n      datasets.append(aug_dataset)\n\n    def flatten_input(*features):\n      result = {}\n      for feature in features:\n        for key in feature:\n          assert key not in result\n          result[key] = feature[key]\n      return result\n\n    if len(datasets) > 1:\n      dataset = tf.data.Dataset.zip(tuple(datasets))\n      dataset = dataset.map(flatten_input)\n    else:\n      dataset = datasets[0]\n\n    return dataset\n\n  return input_fn\n'"
image/main.py,99,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""UDA on CIFAR-10 and SVHN.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport os\nimport time\nimport json\nimport functools\n\nimport numpy as np\n\nfrom absl import flags\nimport absl.logging as _logging  # pylint: disable=unused-import\n\nimport tensorflow as tf\n\nfrom randaugment import custom_ops as ops\nimport data\nimport utils\n\nfrom randaugment.wrn import build_wrn_model\nfrom randaugment.shake_drop import build_shake_drop_model\nfrom randaugment.shake_shake import build_shake_shake_model\n\n\n# TPU related\nflags.DEFINE_string(\n    ""master"", default=None,\n    help=""the TPU address. This should be set when using Cloud TPU"")\nflags.DEFINE_string(\n    ""tpu"", default=None,\n    help=""The Cloud TPU to use for training. This should be either the name ""\n    ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url."")\nflags.DEFINE_string(\n    ""gcp_project"", default=None,\n    help=""Project name for the Cloud TPU-enabled project. If not specified, ""\n    ""we will attempt to automatically detect the GCE project from metadata."")\nflags.DEFINE_string(\n    ""tpu_zone"", default=None,\n    help=""GCE zone where the Cloud TPU is located in. If not specified, we ""\n    ""will attempt to automatically detect the GCE project from metadata."")\nflags.DEFINE_bool(\n    ""use_tpu"", default=False,\n    help=""Use TPUs rather than GPU/CPU."")\nflags.DEFINE_enum(\n    ""task_name"", ""cifar10"",\n    enum_values=[""cifar10"", ""svhn""],\n    help=""The task to use"")\n\n# UDA config:\nflags.DEFINE_integer(\n    ""sup_size"", default=4000,\n    help=""Number of supervised pairs to use. ""\n    ""-1: all training samples. 4000: 4000 supervised examples."")\nflags.DEFINE_integer(\n    ""aug_copy"", default=0,\n    help=""Number of different augmented data generated."")\nflags.DEFINE_integer(\n    ""unsup_ratio"", default=0,\n    help=""The ratio between batch size of unlabeled data and labeled data, ""\n    ""i.e., unsup_ratio * train_batch_size is the batch_size for unlabeled data.""\n    ""Do not use the unsupervised objective if set to 0."")\nflags.DEFINE_enum(\n    ""tsa"", """",\n    enum_values=["""", ""linear_schedule"", ""log_schedule"", ""exp_schedule""],\n    help=""anneal schedule of training signal annealing. ""\n    ""tsa=\'\' means not using TSA. See the paper for other schedules."")\nflags.DEFINE_float(\n    ""uda_confidence_thresh"", default=-1,\n    help=""The threshold on predicted probability on unsupervised data. If set,""\n    ""UDA loss will only be calculated on unlabeled examples whose largest""\n    ""probability is larger than the threshold"")\nflags.DEFINE_float(\n    ""uda_softmax_temp"", -1,\n    help=""The temperature of the Softmax when making prediction on unlabeled""\n    ""examples. -1 means to use normal Softmax"")\nflags.DEFINE_float(\n    ""ent_min_coeff"", default=0,\n    help="""")\nflags.DEFINE_integer(\n    ""unsup_coeff"", default=1,\n    help=""The coefficient on the UDA loss. ""\n    ""setting unsup_coeff to 1 works for most settings. ""\n    ""When you have extermely few samples, consider increasing unsup_coeff"")\nflags.DEFINE_float(\n    \'moving_average_decay\', default=0.9999,\n    help=(\'Moving average decay rate.\'))\n\n# Experiment (data/checkpoint/directory) config\nflags.DEFINE_string(\n    ""data_dir"", default=None,\n    help=""Path to data directory containing `*.tfrecords`."")\nflags.DEFINE_string(\n    ""model_dir"", default=None,\n    help=""model dir of the saved checkpoints."")\nflags.DEFINE_bool(\n    ""do_train"", default=True,\n    help=""Whether to run training."")\nflags.DEFINE_bool(\n    ""do_eval"", default=False,\n    help=""Whether to run eval on the test set."")\nflags.DEFINE_integer(\n    ""dev_size"", default=-1,\n    help=""dev set size."")\nflags.DEFINE_bool(\n    ""verbose"", default=False,\n    help=""Whether to print additional information."")\n\n# Training config\nflags.DEFINE_integer(\n    ""train_batch_size"", default=32,\n    help=""Size of train batch."")\nflags.DEFINE_integer(\n    ""eval_batch_size"", default=8,\n    help=""Size of evalation batch."")\nflags.DEFINE_integer(\n    ""train_steps"", default=100000,\n    help=""Total number of training steps."")\nflags.DEFINE_integer(\n    ""iterations"", default=10000,\n    help=""Number of iterations per repeat loop."")\nflags.DEFINE_integer(\n    ""save_steps"", default=10000,\n    help=""number of steps for model checkpointing."")\nflags.DEFINE_integer(\n    ""max_save"", default=10,\n    help=""Maximum number of checkpoints to save."")\n\n# Model config\nflags.DEFINE_enum(\n    ""model_name"", default=""wrn"",\n    enum_values=[""wrn"", ""shake_shake_32"", ""shake_shake_96"", ""shake_shake_112"", ""pyramid_net""],\n    help=""Name of the model"")\nflags.DEFINE_integer(\n    ""num_classes"", default=10,\n    help=""Number of categories for classification."")\nflags.DEFINE_integer(\n    ""wrn_size"", default=32,\n    help=""The size of WideResNet. It should be set to 32 for WRN-28-2""\n    ""and should be set to 160 for WRN-28-10"")\n\n# Optimization config\nflags.DEFINE_float(\n    ""learning_rate"", default=0.03,\n    help=""Maximum learning rate."")\nflags.DEFINE_float(\n    ""weight_decay_rate"", default=5e-4,\n    help=""Weight decay rate."")\nflags.DEFINE_integer(\n    ""warmup_steps"", default=0,\n    help=""Number of steps for linear lr warmup."")\n\n\n\nFLAGS = tf.flags.FLAGS\n\narg_scope = tf.contrib.framework.arg_scope\n\n\ndef get_tsa_threshold(schedule, global_step, num_train_steps, start, end):\n  step_ratio = tf.to_float(global_step) / tf.to_float(num_train_steps)\n  if schedule == ""linear_schedule"":\n    coeff = step_ratio\n  elif schedule == ""exp_schedule"":\n    scale = 5\n    # [exp(-5), exp(0)] = [1e-2, 1]\n    coeff = tf.exp((step_ratio - 1) * scale)\n  elif schedule == ""log_schedule"":\n    scale = 5\n    # [1 - exp(0), 1 - exp(-5)] = [0, 0.99]\n    coeff = 1 - tf.exp((-step_ratio) * scale)\n  return coeff * (end - start) + start\n\n\ndef setup_arg_scopes(is_training):\n  """"""Sets up the argscopes that will be used when building an image model.\n\n  Args:\n    is_training: Is the model training or not.\n\n  Returns:\n    Arg scopes to be put around the model being constructed.\n  """"""\n\n  batch_norm_decay = 0.9\n  batch_norm_epsilon = 1e-5\n  batch_norm_params = {\n      # Decay for the moving averages.\n      ""decay"": batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      ""epsilon"": batch_norm_epsilon,\n      ""scale"": True,\n      # collection containing the moving mean and moving variance.\n      ""is_training"": is_training,\n  }\n\n  scopes = []\n\n  scopes.append(arg_scope([ops.batch_norm], **batch_norm_params))\n  return scopes\n\n\ndef build_model(inputs, num_classes, is_training, update_bn, hparams):\n  """"""Constructs the vision model being trained/evaled.\n\n  Args:\n    inputs: input features/images being fed to the image model build built.\n    num_classes: number of output classes being predicted.\n    is_training: is the model training or not.\n    hparams: additional hyperparameters associated with the image model.\n\n  Returns:\n    The logits of the image model.\n  """"""\n  scopes = setup_arg_scopes(is_training)\n\n  try:\n      from contextlib import nested\n  except ImportError:\n      from contextlib import ExitStack, contextmanager\n\n      @contextmanager\n      def nested(*contexts):\n          with ExitStack() as stack:\n              for ctx in contexts:\n                  stack.enter_context(ctx)\n              yield contexts\n\n  with nested(*scopes):\n    if hparams.model_name == ""pyramid_net"":\n      logits = build_shake_drop_model(\n          inputs, num_classes, is_training)\n    elif hparams.model_name == ""wrn"":\n      logits = build_wrn_model(\n          inputs, num_classes, hparams.wrn_size, update_bn)\n    elif hparams.model_name == ""shake_shake"":\n      logits = build_shake_shake_model(\n          inputs, num_classes, hparams, is_training)\n\n  return logits\n\n\ndef _kl_divergence_with_logits(p_logits, q_logits):\n  p = tf.nn.softmax(p_logits)\n  log_p = tf.nn.log_softmax(p_logits)\n  log_q = tf.nn.log_softmax(q_logits)\n\n  kl = tf.reduce_sum(p * (log_p - log_q), -1)\n  return kl\n\n\ndef anneal_sup_loss(sup_logits, sup_labels, sup_loss, global_step, metric_dict):\n  tsa_start = 1. / FLAGS.num_classes\n  eff_train_prob_threshold = get_tsa_threshold(\n      FLAGS.tsa, global_step, FLAGS.train_steps,\n      tsa_start, end=1)\n\n  one_hot_labels = tf.one_hot(\n      sup_labels, depth=FLAGS.num_classes, dtype=tf.float32)\n  sup_probs = tf.nn.softmax(sup_logits, axis=-1)\n  correct_label_probs = tf.reduce_sum(\n      one_hot_labels * sup_probs, axis=-1)\n  larger_than_threshold = tf.greater(\n      correct_label_probs, eff_train_prob_threshold)\n  loss_mask = 1 - tf.cast(larger_than_threshold, tf.float32)\n  loss_mask = tf.stop_gradient(loss_mask)\n  sup_loss = sup_loss * loss_mask\n  avg_sup_loss = (tf.reduce_sum(sup_loss) /\n                  tf.maximum(tf.reduce_sum(loss_mask), 1))\n  metric_dict[""sup/sup_trained_ratio""] = tf.reduce_mean(loss_mask)\n  metric_dict[""sup/eff_train_prob_threshold""] = eff_train_prob_threshold\n  return sup_loss, avg_sup_loss\n\n\ndef _scaffold_fn(restore_vars_dict):\n  saver = tf.train.Saver(restore_vars_dict, max_to_keep=10000)\n  return tf.train.Scaffold(saver=saver)\n\n\ndef get_ent(logits, return_mean=True):\n  log_prob = tf.nn.log_softmax(logits, axis=-1)\n  prob = tf.exp(log_prob)\n  ent = tf.reduce_sum(-prob * log_prob, axis=-1)\n  if return_mean:\n    ent = tf.reduce_mean(ent)\n  return ent\n\n\ndef get_model_fn(hparams):\n  def model_fn(features, labels, mode, params):\n    sup_labels = tf.reshape(features[""label""], [-1])\n\n    #### Configuring the optimizer\n    global_step = tf.train.get_global_step()\n    metric_dict = {}\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n    if FLAGS.unsup_ratio > 0 and is_training:\n      all_images = tf.concat([features[""image""],\n                              features[""ori_image""],\n                              features[""aug_image""]], 0)\n    else:\n      all_images = features[""image""]\n\n    with tf.variable_scope(""model"", reuse=tf.AUTO_REUSE):\n      all_logits = build_model(\n          inputs=all_images,\n          num_classes=FLAGS.num_classes,\n          is_training=is_training,\n          update_bn=True and is_training,\n          hparams=hparams,\n      )\n\n      sup_bsz = tf.shape(features[""image""])[0]\n      sup_logits = all_logits[:sup_bsz]\n\n      sup_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n          labels=sup_labels,\n          logits=sup_logits)\n      sup_prob = tf.nn.softmax(sup_logits, axis=-1)\n      metric_dict[""sup/pred_prob""] = tf.reduce_mean(\n          tf.reduce_max(sup_prob, axis=-1))\n    if FLAGS.tsa:\n      sup_loss, avg_sup_loss = anneal_sup_loss(sup_logits, sup_labels, sup_loss,\n                                               global_step, metric_dict)\n    else:\n      avg_sup_loss = tf.reduce_mean(sup_loss)\n    total_loss = avg_sup_loss\n\n    if FLAGS.unsup_ratio > 0 and is_training:\n      aug_bsz = tf.shape(features[""ori_image""])[0]\n\n      ori_logits = all_logits[sup_bsz : sup_bsz + aug_bsz]\n      aug_logits = all_logits[sup_bsz + aug_bsz:]\n      if FLAGS.uda_softmax_temp != -1:\n        ori_logits_tgt = ori_logits / FLAGS.uda_softmax_temp\n      else:\n        ori_logits_tgt = ori_logits\n      ori_prob = tf.nn.softmax(ori_logits, axis=-1)\n      aug_prob = tf.nn.softmax(aug_logits, axis=-1)\n      metric_dict[""unsup/ori_prob""] = tf.reduce_mean(\n          tf.reduce_max(ori_prob, axis=-1))\n      metric_dict[""unsup/aug_prob""] = tf.reduce_mean(\n          tf.reduce_max(aug_prob, axis=-1))\n\n      aug_loss = _kl_divergence_with_logits(\n          p_logits=tf.stop_gradient(ori_logits_tgt),\n          q_logits=aug_logits)\n\n      if FLAGS.uda_confidence_thresh != -1:\n        ori_prob = tf.nn.softmax(ori_logits, axis=-1)\n        largest_prob = tf.reduce_max(ori_prob, axis=-1)\n        loss_mask = tf.cast(tf.greater(\n            largest_prob, FLAGS.uda_confidence_thresh), tf.float32)\n        metric_dict[""unsup/high_prob_ratio""] = tf.reduce_mean(loss_mask)\n        loss_mask = tf.stop_gradient(loss_mask)\n        aug_loss = aug_loss * loss_mask\n        metric_dict[""unsup/high_prob_loss""] = tf.reduce_mean(aug_loss)\n\n      if FLAGS.ent_min_coeff > 0:\n        ent_min_coeff = FLAGS.ent_min_coeff\n        metric_dict[""unsup/ent_min_coeff""] = ent_min_coeff\n        per_example_ent = get_ent(ori_logits)\n        ent_min_loss = tf.reduce_mean(per_example_ent)\n        total_loss = total_loss + ent_min_coeff * ent_min_loss\n\n      avg_unsup_loss = tf.reduce_mean(aug_loss)\n      total_loss += FLAGS.unsup_coeff * avg_unsup_loss\n      metric_dict[""unsup/loss""] = avg_unsup_loss\n\n    total_loss = utils.decay_weights(\n        total_loss,\n        FLAGS.weight_decay_rate)\n\n    #### Check model parameters\n    num_params = sum([np.prod(v.shape) for v in tf.trainable_variables()])\n    tf.logging.info(""#params: {}"".format(num_params))\n\n    if FLAGS.verbose:\n      format_str = ""{{:<{0}s}}\\t{{}}"".format(\n          max([len(v.name) for v in tf.trainable_variables()]))\n      for v in tf.trainable_variables():\n        tf.logging.info(format_str.format(v.name, v.get_shape()))\n    if FLAGS.moving_average_decay > 0.:\n      ema = tf.train.ExponentialMovingAverage(\n          decay=FLAGS.moving_average_decay)\n      ema_vars = utils.get_all_variable()\n\n    #### Evaluation mode\n    if mode == tf.estimator.ModeKeys.EVAL:\n      if FLAGS.moving_average_decay > 0:\n        restore_vars_dict = ema.variables_to_restore(ema_vars)\n        scaffold_fn = functools.partial(\n            _scaffold_fn,\n            restore_vars_dict=restore_vars_dict) if FLAGS.moving_average_decay > 0 else None\n      else:\n        scaffold_fn = None\n\n      #### Metric function for classification\n      def metric_fn(per_example_loss, label_ids, logits):\n        # classification loss & accuracy\n        loss = tf.metrics.mean(per_example_loss)\n\n        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n        accuracy = tf.metrics.accuracy(label_ids, predictions)\n\n        ret_dict = {\n            ""eval/classify_loss"": loss,\n            ""eval/classify_accuracy"": accuracy\n        }\n\n        return ret_dict\n\n      eval_metrics = (metric_fn, [sup_loss, sup_labels, sup_logits])\n\n      #### Constucting evaluation TPUEstimatorSpec.\n      eval_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn,\n      )\n\n      return eval_spec\n\n    # increase the learning rate linearly\n    if FLAGS.warmup_steps > 0:\n      warmup_lr = tf.to_float(global_step) / tf.to_float(FLAGS.warmup_steps) \\\n                  * FLAGS.learning_rate\n    else:\n      warmup_lr = 0.0\n\n    # decay the learning rate using the cosine schedule\n    lrate = tf.clip_by_value(tf.to_float(global_step-FLAGS.warmup_steps) / (FLAGS.train_steps-FLAGS.warmup_steps), 0, 1)\n    decay_lr = FLAGS.learning_rate * tf.cos(lrate * (7. / 8) * np.pi / 2)\n\n    learning_rate = tf.where(global_step < FLAGS.warmup_steps,\n                             warmup_lr, decay_lr)\n\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate=learning_rate,\n        momentum=0.9,\n        use_nesterov=True)\n\n    if FLAGS.use_tpu:\n      optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n    grads_and_vars = optimizer.compute_gradients(total_loss)\n    gradients, variables = zip(*grads_and_vars)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.apply_gradients(\n          zip(gradients, variables), global_step=tf.train.get_global_step())\n    if FLAGS.moving_average_decay > 0:\n      with tf.control_dependencies([train_op]):\n        train_op = ema.apply(ema_vars)\n\n    #### Creating training logging hook\n    # compute accuracy\n    sup_pred = tf.argmax(sup_logits, axis=-1, output_type=sup_labels.dtype)\n    is_correct = tf.to_float(tf.equal(sup_pred, sup_labels))\n    acc = tf.reduce_mean(is_correct)\n    metric_dict[""sup/sup_loss""] = avg_sup_loss\n    metric_dict[""training/loss""] = total_loss\n    metric_dict[""sup/acc""] = acc\n    metric_dict[""training/lr""] = learning_rate\n    metric_dict[""training/step""] = global_step\n\n    if not FLAGS.use_tpu:\n      log_info = (""step [{training/step}] lr {training/lr:.6f} ""\n                  ""loss {training/loss:.4f} ""\n                  ""sup/acc {sup/acc:.4f} sup/loss {sup/sup_loss:.6f} "")\n      if FLAGS.unsup_ratio > 0:\n        log_info += ""unsup/loss {unsup/loss:.6f} ""\n      formatter = lambda kwargs: log_info.format(**kwargs)\n      logging_hook = tf.train.LoggingTensorHook(\n          tensors=metric_dict,\n          every_n_iter=FLAGS.iterations,\n          formatter=formatter)\n      training_hooks = [logging_hook]\n      #### Constucting training TPUEstimatorSpec.\n      train_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode, loss=total_loss, train_op=train_op,\n          training_hooks=training_hooks)\n    else:\n      #### Constucting training TPUEstimatorSpec.\n      host_call = utils.construct_scalar_host_call(\n          metric_dict=metric_dict,\n          model_dir=params[""model_dir""],\n          prefix="""",\n          reduce_fn=tf.reduce_mean)\n      train_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode, loss=total_loss, train_op=train_op,\n          host_call=host_call)\n\n    return train_spec\n\n  return model_fn\n\n\ndef train(hparams):\n  ##### Create input function\n  if FLAGS.unsup_ratio == 0:\n    FLAGS.aug_copy = 0\n  else:\n    assert FLAGS.aug_copy > 0, ""Please specify aug_copy""\n  if FLAGS.dev_size != -1:\n    FLAGS.do_train = True\n    FLAGS.do_eval = True\n  if FLAGS.do_train:\n    train_input_fn = data.get_input_fn(\n        data_dir=FLAGS.data_dir,\n        split=""train"",\n        task_name=FLAGS.task_name,\n        sup_size=FLAGS.sup_size,\n        unsup_ratio=FLAGS.unsup_ratio,\n        aug_copy=FLAGS.aug_copy,\n    )\n\n  if FLAGS.do_eval:\n    if FLAGS.dev_size != -1:\n      eval_input_fn = data.get_input_fn(\n          data_dir=FLAGS.data_dir,\n          split=""dev"",\n          task_name=FLAGS.task_name,\n          sup_size=FLAGS.dev_size,\n          unsup_ratio=0,\n          aug_copy=0)\n      eval_size = FLAGS.dev_size\n    else:\n      eval_input_fn = data.get_input_fn(\n          data_dir=FLAGS.data_dir,\n          split=""test"",\n          task_name=FLAGS.task_name,\n          sup_size=-1,\n          unsup_ratio=0,\n          aug_copy=0)\n      if FLAGS.task_name == ""cifar10"":\n        eval_size = 10000\n      elif FLAGS.task_name == ""svhn"":\n        eval_size = 26032\n      else:\n        assert False, ""You need to specify the size of your test set.""\n    eval_steps = eval_size // FLAGS.eval_batch_size\n\n  ##### Get model function\n  model_fn = get_model_fn(hparams)\n  estimator = utils.get_TPU_estimator(FLAGS, model_fn)\n\n  #### Training\n  if FLAGS.dev_size != -1:\n    tf.logging.info(""***** Running training and validation *****"")\n    tf.logging.info(""  Supervised batch size = %d"", FLAGS.train_batch_size)\n    tf.logging.info(""  Unsupervised batch size = %d"",\n                    FLAGS.train_batch_size * FLAGS.unsup_ratio)\n    tf.logging.info(""  Num train steps = %d"", FLAGS.train_steps)\n    curr_step = 0\n    while True:\n      if curr_step >= FLAGS.train_steps:\n        break\n      tf.logging.info(""Current step {}"".format(curr_step))\n      train_step = min(FLAGS.save_steps, FLAGS.train_steps - curr_step)\n      estimator.train(input_fn=train_input_fn, steps=train_step)\n      estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n      curr_step += FLAGS.save_steps\n  else:\n    if FLAGS.do_train:\n      tf.logging.info(""***** Running training *****"")\n      tf.logging.info(""  Supervised batch size = %d"", FLAGS.train_batch_size)\n      tf.logging.info(""  Unsupervised batch size = %d"",\n                      FLAGS.train_batch_size * FLAGS.unsup_ratio)\n      estimator.train(input_fn=train_input_fn, max_steps=FLAGS.train_steps)\n    if FLAGS.do_eval:\n      tf.logging.info(""***** Running evaluation *****"")\n      results = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n      tf.logging.info("">> Results:"")\n      for key in results.keys():\n        tf.logging.info(""  %s = %s"", key, str(results[key]))\n        results[key] = results[key].item()\n      acc = results[""eval/classify_accuracy""]\n      with tf.gfile.Open(""{}/results.txt"".format(FLAGS.model_dir), ""w"") as ouf:\n        ouf.write(str(acc))\n\n\ndef main(_):\n\n  if FLAGS.do_train:\n    tf.gfile.MakeDirs(FLAGS.model_dir)\n    flags_dict = tf.app.flags.FLAGS.flag_values_dict()\n    with tf.gfile.Open(os.path.join(FLAGS.model_dir, ""FLAGS.json""), ""w"") as ouf:\n      json.dump(flags_dict, ouf)\n  hparams = tf.contrib.training.HParams()\n\n  if FLAGS.model_name == ""wrn"":\n    hparams.add_hparam(""model_name"", ""wrn"")\n    hparams.add_hparam(""wrn_size"", FLAGS.wrn_size)\n  elif FLAGS.model_name == ""shake_shake_32"":\n    hparams.add_hparam(""model_name"", ""shake_shake"")\n    hparams.add_hparam(""shake_shake_widen_factor"", 2)\n  elif FLAGS.model_name == ""shake_shake_96"":\n    hparams.add_hparam(""model_name"", ""shake_shake"")\n    hparams.add_hparam(""shake_shake_widen_factor"", 6)\n  elif FLAGS.model_name == ""shake_shake_112"":\n    hparams.add_hparam(""model_name"", ""shake_shake"")\n    hparams.add_hparam(""shake_shake_widen_factor"", 7)\n  elif FLAGS.model_name == ""pyramid_net"":\n    hparams.add_hparam(""model_name"", ""pyramid_net"")\n  else:\n    raise ValueError(""Not Valid Model Name: %s"" % FLAGS.model_name)\n\n  train(hparams)\n\n\nif __name__ == ""__main__"":\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
image/preprocess.py,29,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Preprocess supervised data and unsupervised data.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import cPickle as pickle\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nimport collections\nimport os\nimport sys\nimport tarfile\n\nimport numpy as np\n\nfrom absl import flags\nimport absl.logging as _logging  # pylint: disable=unused-import\n\nimport scipy.io\nimport tensorflow as tf\n\nfrom randaugment import policies as found_policies\nfrom randaugment import augmentation_transforms\n\nFLAGS = flags.FLAGS\n\nCIFAR_TARNAME = ""cifar-10-python.tar.gz""\nCIFAR_DOWNLOAD_URL = ""https://www.cs.toronto.edu/~kriz/"" + CIFAR_TARNAME\nSVHN_DOWNLOAD_URL = ""http://ufldl.stanford.edu/housenumbers/{}_32x32.mat""\n\nDOWNLOAD_DATA_FOLDER = ""downloaded_data""\nMERGE_DATA_FOLDER = ""merged_raw_data""\n\nrandom_seed = np.random.randint(0, 10000)\n\n\ndef format_sup_filename(split, sup_size=-1):\n  if split == ""test"":\n    return ""test.tfrecord""\n  elif split == ""train"" or split == ""dev"":\n    if sup_size == -1:\n      return ""{}-full.tfrecord"".format(split, sup_size)\n    else:\n      return ""{}-size_{:d}.tfrecord"".format(split, sup_size)\n\n\ndef format_unsup_filename(aug_copy_num):\n  return ""unsup-{:d}.tfrecord"".format(aug_copy_num)\n\n\ndef _int64_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=list(value)))\n\n\ndef _float_feature(value):\n  return tf.train.Feature(float_list=tf.train.FloatList(value=list(value)))\n\n\ndef get_raw_data_filenames(split):\n  """"""Returns the file names expected to exist in the input_dir.""""""\n  if FLAGS.task_name == ""cifar10"":\n    if split == ""train"":\n      return [""data_batch_%d"" % i for i in xrange(1, 6)]\n    elif split == ""test"":\n      return [""test_batch""]\n  else:\n    assert False\n\n\ndef read_pickle_from_file(filename):\n  with tf.gfile.Open(filename, ""rb"") as f:\n    if sys.version_info >= (3, 0):\n      data_dict = pickle.load(f, encoding=""bytes"")\n    else:\n      data_dict = pickle.load(f)\n  return data_dict\n\n\ndef obtain_tfrecord_writer(out_path, shard_cnt):\n  tfrecord_writer = tf.python_io.TFRecordWriter(\n      ""{}.{:d}"".format(out_path, shard_cnt))\n  return tfrecord_writer\n\n\ndef save_tfrecord(example_list, out_path, max_shard_size=4096):\n  shard_cnt = 0\n  shard_size = 0\n  record_writer = obtain_tfrecord_writer(out_path, shard_cnt)\n  for example in example_list:\n    if shard_size >= max_shard_size:\n      record_writer.close()\n      shard_cnt += 1\n      record_writer = obtain_tfrecord_writer(out_path, shard_cnt)\n      shard_size = 0\n    shard_size += 1\n    record_writer.write(example.SerializeToString())\n  record_writer.close()\n  tf.logging.info(""saved {} examples to {}"".format(len(example_list), out_path))\n\n\ndef save_merged_data(images, labels, split, merge_folder):\n  with tf.gfile.Open(\n      os.path.join(merge_folder, ""{}_images.npy"".format(split)), ""wb"") as ouf:\n    np.save(ouf, images)\n  with tf.gfile.Open(\n      os.path.join(merge_folder, ""{}_labels.npy"".format(split)), ""wb"") as ouf:\n    np.save(ouf, labels)\n\n\ndef download_and_extract():\n  all_exist = True\n  download_folder = os.path.join(FLAGS.raw_data_dir, DOWNLOAD_DATA_FOLDER)\n  merge_folder = os.path.join(FLAGS.raw_data_dir, MERGE_DATA_FOLDER)\n  for split in [""train"", ""test""]:\n    for field in [""images"", ""labels""]:\n      if not tf.gfile.Exists(os.path.join(merge_folder, ""{}_{}.npy"".format(\n          split, field))):\n        all_exist = False\n  if all_exist:\n    tf.logging.info(""found all merged files"")\n    return\n  tf.logging.info(""downloading dataset"")\n  tf.gfile.MakeDirs(download_folder)\n  tf.gfile.MakeDirs(merge_folder)\n  if FLAGS.task_name == ""cifar10"":\n    tf.contrib.learn.datasets.base.maybe_download(\n        CIFAR_TARNAME, download_folder, CIFAR_DOWNLOAD_URL)\n    tarfile.open(\n        os.path.join(download_folder, CIFAR_TARNAME), ""r:gz"").extractall(download_folder)\n    for split in [""train"", ""test""]:\n      images_list = []\n      labels_list = []\n      for filename in get_raw_data_filenames(split):\n        cur_data = read_pickle_from_file(\n            os.path.join(download_folder, ""cifar-10-batches-py"", filename))\n        labels_list += [cur_data[b""labels""]]\n        images_list += [cur_data[b""data""]]\n      images = np.concatenate(images_list, 0)\n      labels = np.concatenate(labels_list, 0)\n      images = images.reshape([-1, 3, 32, 32])\n      images = images.transpose(0, 2, 3, 1)\n      save_merged_data(images, labels, split, merge_folder)\n  elif FLAGS.task_name == ""svhn"":\n    for split in [""train"", ""test""]:\n      tf.contrib.learn.datasets.base.maybe_download(\n          ""{}_32x32.mat"".format(split),\n          download_folder,\n          SVHN_DOWNLOAD_URL.format(split))\n      filename = os.path.join(download_folder, ""{}_32x32.mat"".format(split))\n      data_dict = scipy.io.loadmat(tf.gfile.Open(filename))\n      images = np.transpose(data_dict[""X""], [3, 0, 1, 2])\n      labels = data_dict[""y""].reshape(-1)\n      labels[labels == 10] = 0\n      save_merged_data(images, labels, split, merge_folder)\n\n\ndef load_dataset():\n  data = {}\n  download_and_extract()\n  merge_folder = os.path.join(FLAGS.raw_data_dir, MERGE_DATA_FOLDER)\n  for split in [""train"", ""test""]:\n    with tf.gfile.Open(\n        os.path.join(merge_folder, ""{}_images.npy"".format(split))) as inf:\n      images = np.load(inf)\n    with tf.gfile.Open(\n        os.path.join(merge_folder, ""{}_labels.npy"".format(split))) as inf:\n      labels = np.load(inf)\n    data[split] = {""images"": images, ""labels"": labels}\n  return data\n\n\ndef get_data_by_size_lim(images, labels, sup_size, return_rest=False):\n  if FLAGS.use_equal_split:\n    chosen_images = []\n    chosen_labels = []\n    rest_images = []\n    rest_labels = []\n    num_classes = 10\n    assert sup_size % num_classes == 0\n    cur_stats = collections.defaultdict(int)\n    for i in range(len(images)):\n      label = labels[i]\n      if cur_stats[label] < sup_size // num_classes:\n        chosen_images += [images[i]]\n        chosen_labels += [labels[i]]\n        cur_stats[label] += 1\n      else:\n        rest_images += [images[i]]\n        rest_labels += [labels[i]]\n    chosen_images = np.array(chosen_images)\n    chosen_labels = np.array(chosen_labels)\n    rest_images = np.array(rest_images)\n    rest_labels = np.array(rest_labels)\n  else:\n    # use the same labeled data as in AutoAugment\n    if FLAGS.task_name == ""cifar10"":\n      chosen_images = images[:sup_size]\n      chosen_labels = labels[:sup_size]\n      rest_images = images[sup_size:]\n      rest_labels = labels[sup_size:]\n    else:\n      np.random.seed(0)\n      perm = np.arange(images.shape[0])\n      np.random.shuffle(perm)\n      chosen_images = images[perm][:sup_size]\n      chosen_labels = labels[perm][:sup_size]\n      rest_images = images[perm][sup_size:]\n      rest_labels = labels[perm][sup_size:]\n  if return_rest:\n    return chosen_images, chosen_labels, rest_images, rest_labels\n  else:\n    return chosen_images, chosen_labels\n\n\ndef process_and_save_sup_data(chosen_images, chosen_labels, split, sup_size=-1):\n  chosen_images = chosen_images / 255.0\n  mean, std = augmentation_transforms.get_mean_and_std()\n  chosen_images = (chosen_images - mean) / std\n  example_list = []\n  for image, label in zip(chosen_images, chosen_labels):\n    # Write example to the tfrecord file\n    example = tf.train.Example(features=tf.train.Features(\n        feature={\n            ""image"": _float_feature(image.reshape(-1)),\n            ""label"": _int64_feature(label.reshape(-1))\n        }))\n    example_list += [example]\n  out_path = os.path.join(\n      FLAGS.output_base_dir,\n      format_sup_filename(split, sup_size)\n  )\n  tf.logging.info("">> saving {} {} examples to {}"".format(\n      len(example_list), split, out_path))\n  save_tfrecord(example_list, out_path)\n\n\ndef proc_and_dump_sup_data(sub_set_data, split, sup_size=-1):\n  images = sub_set_data[""images""]\n  labels = sub_set_data[""labels""]\n  if sup_size != -1:\n    chosen_images, chosen_labels = get_data_by_size_lim(\n        images, labels, sup_size)\n  else:\n    chosen_images = images\n    chosen_labels = labels\n  if split == ""train"" and FLAGS.dev_size != -1:\n    dev_images, dev_labels, train_images, train_labels = get_data_by_size_lim(\n        chosen_images, chosen_labels, FLAGS.dev_size,\n        return_rest=True)\n    process_and_save_sup_data(train_images, train_labels, ""train"", train_images.shape[0])\n    process_and_save_sup_data(dev_images, dev_labels, ""dev"", dev_images.shape[0])\n  else:\n    process_and_save_sup_data(chosen_images, chosen_labels, split)\n\n\ndef proc_and_dump_unsup_data(sub_set_data, aug_copy_num):\n  ori_images = sub_set_data[""images""].copy()\n\n  image_idx = np.arange(len(ori_images))\n  np.random.shuffle(image_idx)\n  ori_images = ori_images[image_idx]\n\n  # tf.logging.info(""first 5 indexes after shuffling: {}"".format(\n  #     str(image_idx[:5])))\n\n  ori_images = ori_images / 255.0\n  mean, std = augmentation_transforms.get_mean_and_std()\n  ori_images = (ori_images - mean) / std\n\n  if FLAGS.task_name == ""cifar10"":\n    aug_policies = found_policies.randaug_policies()\n  elif FLAGS.task_name == ""svhn"":\n    aug_policies = found_policies.randaug_policies()\n\n  example_list = []\n  for image in ori_images:\n    chosen_policy = aug_policies[np.random.choice(\n        len(aug_policies))]\n    aug_image = augmentation_transforms.apply_policy(\n        chosen_policy, image)\n    aug_image = augmentation_transforms.cutout_numpy(aug_image)\n\n    # Write example to the tfrecord file\n    example = tf.train.Example(features=tf.train.Features(\n        feature={\n            ""ori_image"": _float_feature(image.reshape(-1)),\n            ""aug_image"": _float_feature(aug_image.reshape(-1)),\n        }))\n    example_list += [example]\n\n  out_path = os.path.join(\n      FLAGS.output_base_dir,\n      format_unsup_filename(aug_copy_num),\n  )\n  save_tfrecord(example_list, out_path)\n\n\ndef main(unused_argv):\n\n  output_base_dir = FLAGS.output_base_dir\n  if not tf.gfile.Exists(output_base_dir):\n    tf.gfile.MakeDirs(output_base_dir)\n\n  data = load_dataset()\n  if FLAGS.data_type == ""sup"":\n    tf.logging.info(""***** Processing supervised data *****"")\n    # process training set\n    proc_and_dump_sup_data(data[""train""], ""train"", sup_size=FLAGS.sup_size)\n    # process test set\n    proc_and_dump_sup_data(data[""test""], ""test"")\n  elif FLAGS.data_type == ""unsup"":\n    tf.logging.info(""***** Processing unsupervised data *****"")\n    # Just to make sure that different tfrecord files do not have data stored\n    # in the same order. Since we read several tfrecord files in parallel, if\n    # different tfrecord files have the same order, it is more probable that\n    # multiple augmented examples of the same original example appear in the same\n    # mini-batch.\n    tf.logging.info(\n        ""using random seed {:d} for shuffling data"".format(random_seed))\n    np.random.seed(random_seed)\n    for aug_copy_num in range(\n        FLAGS.aug_copy_start, FLAGS.aug_copy_start + FLAGS.aug_copy):\n      tf.logging.info(\n          "">> processing aug copy # {}"".format(aug_copy_num))\n      proc_and_dump_unsup_data(data[""train""], aug_copy_num)\n\n\nif __name__ == ""__main__"":\n  flags.DEFINE_enum(\n      ""task_name"", ""cifar10"",\n      enum_values=[""cifar10"", ""svhn""], help=""Task to use."")\n  flags.DEFINE_enum(\n      ""data_type"", ""sup"",\n      enum_values=[""sup"", ""unsup""],\n      help=""Whether to process supervised data or unsupervised data."")\n  flags.DEFINE_string(\n      ""raw_data_dir"", None, ""Path of the raw data."")\n  flags.DEFINE_string(\n      ""output_base_dir"", """", ""processed data path."")\n\n  # configs for processing supervised data\n  flags.DEFINE_bool(\n      ""use_equal_split"", False, ""If set to True, use equal number of data for each""\n      ""category. If set to False, use the same data as AutoAugment."")\n  flags.DEFINE_integer(\n      ""sup_size"", -1, ""Number of supervised pairs to use.""\n      ""-1: all training samples. 0: no supervised data."")\n  flags.DEFINE_integer(\n      ""dev_size"", -1, ""Number of dev examples to use.""\n      ""-1: no dev set."")\n\n  # configs for processing unsupervised data\n  flags.DEFINE_integer(\n      ""aug_copy"", 0, ""Number of augmented copies to create."")\n  flags.DEFINE_integer(\n      ""aug_copy_start"", 0, ""The index of the first augmented copy."")\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run(main)\n'"
image/utils.py,18,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Helper functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef decay_weights(cost, weight_decay_rate):\n  """"""Calculates the loss for l2 weight decay and adds it to `cost`.""""""\n  costs = []\n  for var in tf.trainable_variables():\n    costs.append(tf.nn.l2_loss(var))\n  cost += tf.multiply(weight_decay_rate, tf.add_n(costs))\n  return cost\n\n\ndef get_TPU_estimator(FLAGS, model_fn, model_dir=None):\n  ##### Create TPUEstimator\n  # TPU Configuration\n  if FLAGS.use_tpu:\n    if FLAGS.tpu:\n      tpu_cluster = tf.contrib.cluster_resolver.TPUClusterResolver(\n          FLAGS.tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n    else:\n      tpu_cluster = None\n    session_config = tf.ConfigProto(\n        allow_soft_placement=True, log_device_placement=True)\n  else:\n    tpu_cluster = None\n    session_config = None\n  per_host_input = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster,\n      master=FLAGS.master,\n      model_dir=model_dir or FLAGS.model_dir,\n      session_config=session_config,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          # if there is name for the job, then add a name here\n          iterations_per_loop=FLAGS.iterations,\n          # num_shards=FLAGS.num_core_per_host * FLAGS.num_hosts,\n          per_host_input_for_training=per_host_input),\n      keep_checkpoint_max=FLAGS.max_save,\n      save_checkpoints_secs=None,\n      save_checkpoints_steps=FLAGS.save_steps\n  )\n\n  # TPU Estimator\n  estimator = tf.contrib.tpu.TPUEstimator(\n      model_fn=model_fn,\n      use_tpu=FLAGS.use_tpu,\n      config=run_config,\n      params={""model_dir"": model_dir or FLAGS.model_dir},\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=FLAGS.eval_batch_size)\n  return estimator\n\n\ndef construct_scalar_host_call(\n    metric_dict,\n    model_dir,\n    prefix="""",\n    reduce_fn=None):\n\n  metric_names = list(metric_dict.keys())\n\n  def host_call_fn(global_step, *args):\n    step = global_step[0]\n    with tf.contrib.summary.create_file_writer(\n        logdir=model_dir, filename_suffix="".host_call"").as_default():\n      with tf.contrib.summary.always_record_summaries():\n        for i, name in enumerate(metric_names):\n          if reduce_fn is None:\n            scalar = args[i][0]\n          else:\n            scalar = reduce_fn(args[i])\n          with tf.contrib.summary.record_summaries_every_n_global_steps(1000, step):\n            tf.contrib.summary.scalar(prefix + name, scalar, step=step)\n\n        return tf.contrib.summary.all_summary_ops()\n\n  global_step_tensor = tf.reshape(tf.train.get_or_create_global_step(), [1])\n  other_tensors = [tf.reshape(metric_dict[key], [-1]) for key in metric_names]\n\n  return host_call_fn, [global_step_tensor] + other_tensors\n\n\ndef get_all_variable():\n  var_list = tf.trainable_variables() + tf.get_collection(\'moving_vars\')\n  for v in tf.global_variables():\n    # We maintain ema for batch norm moving mean and variance as well.\n    if \'moving_mean\' in v.name or \'moving_variance\' in v.name:\n      var_list.append(v)\n  var_list = list(set(var_list))\n  var_list = sorted(var_list, key=lambda var: var.name)\n  return var_list\n'"
text/extract_raw_text.py,9,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Extract raw text for back translation.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import google_type_annotations\nfrom __future__ import print_function\nimport os\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf\n\nfrom utils import raw_data_utils\n\nfrom absl import app\nfrom absl import flags\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_bool(\n    ""separate_doc_by_newline"", False, """")\n\nflags.DEFINE_string(\n    ""output_data_dir"",\n    None, """")\n\nflags.DEFINE_string(\n    ""sub_set"",\n    ""unsup_in"", """")\n\nflags.DEFINE_string(\n    ""task_name"",\n    ""IMDB"", """")\n\nflags.DEFINE_string(\n    ""raw_data_dir"",\n    ""IMDB"", """")\n\ndef dump_raw_examples(examples, separate_doc_by_newline):\n  """"""dump raw examples.""""""\n  tf.logging.info(""dumpping raw examples"")\n  text_path = os.path.join(FLAGS.output_data_dir, ""text.txt"")\n  label_path = os.path.join(FLAGS.output_data_dir, ""label.txt"")\n  with tf.gfile.Open(text_path, ""w"") as text_ouf:\n    with tf.gfile.Open(label_path, ""w"") as label_ouf:\n      for example in examples:\n        text_a = example.text_a\n        text_b = example.text_b\n        label = example.label\n        text_ouf.write(text_a + ""\\n"")\n        if text_b is not None:\n          text_ouf.write(text_b + ""\\n"")\n        if separate_doc_by_newline:\n          text_ouf.write(""\\n"")\n        label_ouf.write(label + ""\\n"")\n  tf.logging.info(""finished dumpping raw examples"")\n\n\ndef main(argv):\n  processor = raw_data_utils.get_processor(FLAGS.task_name)\n  tf.logging.info(""loading examples"")\n  FLAGS.output_data_dir = os.path.join(\n      FLAGS.output_data_dir, FLAGS.sub_set)\n  if not tf.gfile.Exists(FLAGS.output_data_dir):\n    tf.gfile.MakeDirs(FLAGS.output_data_dir)\n  if FLAGS.sub_set == ""train"":\n    examples = processor.get_train_examples(FLAGS.raw_data_dir)\n  elif FLAGS.sub_set.startswith(""unsup""):\n    examples = processor.get_unsup_examples(FLAGS.raw_data_dir, FLAGS.sub_set)\n  else:\n    assert False\n  tf.logging.info(""finished loading examples"")\n  tf.logging.info(""examples num: {:d}"".format(len(examples)))\n  dump_raw_examples(examples, FLAGS.separate_doc_by_newline)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
text/main.py,46,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Runner for UDA that uses BERT.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\nimport tensorflow as tf\n\nimport uda\nfrom bert import modeling\nfrom utils import proc_data_utils\nfrom utils import raw_data_utils\n\n\n\nflags = tf.flags\nFLAGS = flags.FLAGS\n\n\n##### Task\nflags.DEFINE_bool(\n    ""do_train"", True,\n    help=(""Whether to perform training. If both do_train and do_eval are True, ""\n          ""we interleave training with evaluation. But on TPU Pods, ""\n          ""it is not possible to interleave training and evaluation""))\nflags.DEFINE_bool(\n    ""do_eval"", False,\n    help=""Whether to perform evaluation."")\n\n# unsupervised objective related hyperparameters\nflags.DEFINE_integer(\n    ""unsup_ratio"", 0,\n    help=""The ratio between batch size of unlabeled data and labeled data, ""\n    ""The batch_size for the unsupervised loss is unsup_ratio * train_batch_size.""\n    ""Do not use the unsupervised objective if unsup_ratio is set to 0."")\nflags.DEFINE_string(\n    ""aug_ops"", """",\n    help=""Augmentation operations."")\nflags.DEFINE_integer(\n    ""aug_copy"", -1,\n    help=""Number of different augmented data generated."")\nflags.DEFINE_float(\n    ""uda_coeff"", 1,\n    help=""Coefficient on the uda loss. We set it to 1 for all""\n    ""of our text experiments."")\nflags.DEFINE_enum(\n    ""tsa"", """",\n    enum_values=["""", ""linear_schedule"", ""log_schedule"", ""exp_schedule""],\n    help=""anneal schedule of training signal annealing. ""\n    ""tsa=\'\' means not using TSA. See the paper for other schedules."")\nflags.DEFINE_float(\n    ""uda_softmax_temp"", -1,\n    help=""The temperature of the Softmax when making prediction on unlabeled""\n    ""examples. -1 means to use normal Softmax"")\nflags.DEFINE_float(\n    ""uda_confidence_thresh"", default=-1,\n    help=""The threshold on predicted probability on unsupervised data. If set,""\n    ""UDA loss will only be calculated on unlabeled examples whose largest""\n    ""probability is larger than the threshold"")\n\n\n##### TPU/GPU related\nflags.DEFINE_bool(\n    ""use_tpu"", True,\n    ""Whether to use TPU or GPU/CPU."")\nflags.DEFINE_string(\n    ""tpu_name"", default=None,\n    help=""The Cloud TPU to use for training. This should be either the name ""\n    ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url."")\nflags.DEFINE_string(\n    ""gcp_project"", default=None,\n    help=""Project name for the Cloud TPU-enabled project. If not specified, ""\n    ""we will attempt to automatically detect the GCE project from metadata."")\nflags.DEFINE_string(\n    ""tpu_zone"", default=None,\n    help=""GCE zone where the Cloud TPU is located in. If not specified, we ""\n    ""will attempt to automatically detect the GCE project from metadata."")\nflags.DEFINE_string(\n    ""master"", None,\n    ""If using a TPU, the address of the master."")\n\n##### Configs related to training\nflags.DEFINE_string(\n    ""sup_train_data_dir"", None,\n    help=""The input data dir of the supervised data. Should contain""\n    ""`tf_examples.tfrecord*`"")\nflags.DEFINE_string(\n    ""eval_data_dir"", None,\n    help=""The input data dir of the evaluation data. Should contain ""\n    ""`tf_examples.tfrecord*`"")\nflags.DEFINE_string(\n    ""unsup_data_dir"", None,\n    help=""The input data dir of the unsupervised data. Should contain ""\n    ""`tf_examples.tfrecord*`"")\nflags.DEFINE_string(\n    ""bert_config_file"", None,\n    help=""The config json file corresponding to the pre-trained BERT model. ""\n    ""This specifies the model architecture."")\nflags.DEFINE_string(\n    ""vocab_file"", None,\n    help=""The vocabulary file that the BERT model was trained on."")\nflags.DEFINE_string(\n    ""init_checkpoint"", None,\n    help=""Initial checkpoint (usually from a pre-trained BERT model)."")\nflags.DEFINE_string(\n    ""task_name"", None,\n    help=""The name of the task to train."")\nflags.DEFINE_string(\n    ""model_dir"", None,\n    help=""The output directory where the model checkpoints will be written."")\n\n##### Model configuration\nflags.DEFINE_bool(\n    ""use_one_hot_embeddings"", True,\n    help=""If True, tf.one_hot will be used for embedding lookups, otherwise ""\n    ""tf.nn.embedding_lookup will be used. On TPUs, this should be True ""\n    ""since it is much faster."")\nflags.DEFINE_integer(\n    ""max_seq_length"", 512,\n    help=""The maximum total sequence length after WordPiece tokenization. ""\n    ""Sequences longer than this will be truncated, and sequences shorter ""\n    ""than this will be padded."")\nflags.DEFINE_float(\n    ""model_dropout"", -1,\n    help=""Dropout rate for both the attention and the hidden states."")\n\n##### Training hyper-parameters\nflags.DEFINE_integer(\n    ""train_batch_size"", 32,\n    help=""Batch size for the supervised objective."")\nflags.DEFINE_integer(\n    ""eval_batch_size"", 8,\n    help=""Base batch size for evaluation."")\nflags.DEFINE_integer(\n    ""save_checkpoints_num"", 20,\n    help=""How many checkpoints we save in training."")\nflags.DEFINE_integer(\n    ""iterations_per_loop"", 200,\n    help=""How many steps to make in each estimator call."")\nflags.DEFINE_integer(\n    ""num_train_steps"", None,\n    help=""Total number of training steps to perform."")\n\n\n##### Optimizer hyperparameters\nflags.DEFINE_float(\n    ""learning_rate"", 2e-5,\n    help=""The initial learning rate for Adam."")\nflags.DEFINE_integer(\n    ""num_warmup_steps"", None,\n    help=""Number of warmup steps."")\nflags.DEFINE_float(\n    ""clip_norm"", 1.0,\n    help=""Gradient clip hyperparameter."")\n\n\n\n\ndef main(_):\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  processor = raw_data_utils.get_processor(FLAGS.task_name)\n  label_list = processor.get_labels()\n\n  bert_config = modeling.BertConfig.from_json_file(\n      FLAGS.bert_config_file,\n      FLAGS.model_dropout)\n\n\n  tf.gfile.MakeDirs(FLAGS.model_dir)\n\n  flags_dict = tf.app.flags.FLAGS.flag_values_dict()\n  with tf.gfile.Open(os.path.join(FLAGS.model_dir, ""FLAGS.json""), ""w"") as ouf:\n    json.dump(flags_dict, ouf)\n\n  tf.logging.info(""warmup steps {}/{}"".format(\n      FLAGS.num_warmup_steps, FLAGS.num_train_steps))\n\n  save_checkpoints_steps = FLAGS.num_train_steps // FLAGS.save_checkpoints_num\n  tf.logging.info(""setting save checkpoints steps to {:d}"".format(\n      save_checkpoints_steps))\n\n  FLAGS.iterations_per_loop = min(save_checkpoints_steps,\n                                  FLAGS.iterations_per_loop)\n  if FLAGS.use_tpu and FLAGS.tpu_name:\n    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n  else:\n    tpu_cluster_resolver = None\n  # if not FLAGS.use_tpu and FLAGS.num_gpu > 1:\n  #   train_distribute = tf.contrib.distribute.MirroredStrategy(\n  #       num_gpus=FLAGS.num_gpu)\n  # else:\n  #   train_distribute = None\n\n  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      master=FLAGS.master,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_steps=save_checkpoints_steps,\n      keep_checkpoint_max=1000,\n      # train_distribute=train_distribute,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,\n          per_host_input_for_training=is_per_host))\n\n  model_fn = uda.model_fn_builder(\n      bert_config=bert_config,\n      init_checkpoint=FLAGS.init_checkpoint,\n      learning_rate=FLAGS.learning_rate,\n      clip_norm=FLAGS.clip_norm,\n      num_train_steps=FLAGS.num_train_steps,\n      num_warmup_steps=FLAGS.num_warmup_steps,\n      use_tpu=FLAGS.use_tpu,\n      use_one_hot_embeddings=FLAGS.use_one_hot_embeddings,\n      num_labels=len(label_list),\n      unsup_ratio=FLAGS.unsup_ratio,\n      uda_coeff=FLAGS.uda_coeff,\n      tsa=FLAGS.tsa,\n      print_feature=False,\n      print_structure=False,\n  )\n\n  # If TPU is not available, this will fall back to normal Estimator on CPU\n  # or GPU.\n  estimator = tf.contrib.tpu.TPUEstimator(\n      use_tpu=FLAGS.use_tpu,\n      model_fn=model_fn,\n      config=run_config,\n      params={""model_dir"": FLAGS.model_dir},\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=FLAGS.eval_batch_size)\n\n  if FLAGS.do_train:\n    tf.logging.info(""  >>> sup data dir : {}"".format(FLAGS.sup_train_data_dir))\n    if FLAGS.unsup_ratio > 0:\n      tf.logging.info(""  >>> unsup data dir : {}"".format(\n          FLAGS.unsup_data_dir))\n\n    train_input_fn = proc_data_utils.training_input_fn_builder(\n        FLAGS.sup_train_data_dir,\n        FLAGS.unsup_data_dir,\n        FLAGS.aug_ops,\n        FLAGS.aug_copy,\n        FLAGS.unsup_ratio)\n\n  if FLAGS.do_eval:\n    tf.logging.info(""  >>> dev data dir : {}"".format(FLAGS.eval_data_dir))\n    eval_input_fn = proc_data_utils.evaluation_input_fn_builder(\n        FLAGS.eval_data_dir,\n        ""clas"")\n\n    eval_size = processor.get_dev_size()\n    eval_steps = int(eval_size / FLAGS.eval_batch_size)\n\n  if FLAGS.do_train and FLAGS.do_eval:\n    tf.logging.info(""***** Running training & evaluation *****"")\n    tf.logging.info(""  Supervised batch size = %d"", FLAGS.train_batch_size)\n    tf.logging.info(""  Unsupervised batch size = %d"",\n                    FLAGS.train_batch_size * FLAGS.unsup_ratio)\n    tf.logging.info(""  Num steps = %d"", FLAGS.num_train_steps)\n    tf.logging.info(""  Base evaluation batch size = %d"", FLAGS.eval_batch_size)\n    tf.logging.info(""  Num steps = %d"", eval_steps)\n    best_acc = 0\n    for _ in range(0, FLAGS.num_train_steps, save_checkpoints_steps):\n      tf.logging.info(""*** Running training ***"")\n      estimator.train(\n          input_fn=train_input_fn,\n          steps=save_checkpoints_steps)\n      tf.logging.info(""*** Running evaluation ***"")\n      dev_result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n      tf.logging.info("">> Results:"")\n      for key in dev_result.keys():\n        tf.logging.info(""  %s = %s"", key, str(dev_result[key]))\n        dev_result[key] = dev_result[key].item()\n      best_acc = max(best_acc, dev_result[""eval_classify_accuracy""])\n    tf.logging.info(""***** Final evaluation result *****"")\n    tf.logging.info(""Best acc: {:.3f}\\n\\n"".format(best_acc))\n  elif FLAGS.do_train:\n    tf.logging.info(""***** Running training *****"")\n    tf.logging.info(""  Supervised batch size = %d"", FLAGS.train_batch_size)\n    tf.logging.info(""  Unsupervised batch size = %d"",\n                    FLAGS.train_batch_size * FLAGS.unsup_ratio)\n    tf.logging.info(""  Num steps = %d"", FLAGS.num_train_steps)\n    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)\n  elif FLAGS.do_eval:\n    tf.logging.info(""***** Running evaluation *****"")\n    tf.logging.info(""  Base evaluation batch size = %d"", FLAGS.eval_batch_size)\n    tf.logging.info(""  Num steps = %d"", eval_steps)\n    checkpoint_state = tf.train.get_checkpoint_state(FLAGS.model_dir)\n\n    best_acc = 0\n    for ckpt_path in checkpoint_state.all_model_checkpoint_paths:\n      if not tf.gfile.Exists(ckpt_path + "".data-00000-of-00001""):\n        tf.logging.info(\n            ""Warning: checkpoint {:s} does not exist"".format(ckpt_path))\n        continue\n      tf.logging.info(""Evaluating {:s}"".format(ckpt_path))\n      dev_result = estimator.evaluate(\n          input_fn=eval_input_fn,\n          steps=eval_steps,\n          checkpoint_path=ckpt_path,\n      )\n      tf.logging.info("">> Results:"")\n      for key in dev_result.keys():\n        tf.logging.info(""  %s = %s"", key, str(dev_result[key]))\n        dev_result[key] = dev_result[key].item()\n      best_acc = max(best_acc, dev_result[""eval_classify_accuracy""])\n    tf.logging.info(""***** Final evaluation result *****"")\n    tf.logging.info(""Best acc: {:.3f}\\n\\n"".format(best_acc))\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
text/preprocess.py,37,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Preprocessing for text classifications.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport json\nimport os\nfrom absl import app\nfrom absl import flags\n\nimport numpy as np\nimport tensorflow as tf\n\n# from augmentation import aug_policy\nfrom augmentation import sent_level_augment\nfrom augmentation import word_level_augment\nfrom utils import raw_data_utils\nfrom utils import tokenization\n\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    ""task_name"", ""IMDB"", ""The name of the task to train."")\n\nflags.DEFINE_string(\n    ""raw_data_dir"", None, ""Data directory of the raw data"")\n\nflags.DEFINE_string(\n    ""output_base_dir"", None, ""Data directory of the processed data"")\n\nflags.DEFINE_string(\n    ""aug_ops"", ""bt-0.9"", ""augmentation method"")\n\nflags.DEFINE_integer(\n    ""aug_copy_num"", -1,\n    help=""We generate multiple augmented examples for one""\n    ""unlabeled example, aug_copy_num is the index of the generated augmented""\n    ""example"")\n\nflags.DEFINE_integer(\n    ""max_seq_length"", 512,\n    help=""The maximum total sequence length after WordPiece tokenization. ""\n    ""Sequences longer than this will be truncated, and sequences shorter ""\n    ""than this will be padded."")\n\nflags.DEFINE_integer(\n    ""sup_size"", -1, ""size of the labeled set"")\n\nflags.DEFINE_bool(\n    ""trunc_keep_right"", True,\n    help=""Whether to keep the right part when truncate a sentence."")\n\nflags.DEFINE_enum(\n    ""data_type"", default=""sup"",\n    enum_values=[""sup"", ""unsup""],\n    help=""Which preprocess task to perform."")\n\nflags.DEFINE_string(\n    ""sub_set"", ""train"",\n    ""Which sub_set to preprocess. The sub_set can be train, dev and unsup_in"")\n\nflags.DEFINE_string(\n    ""vocab_file"", """", ""The path of the vocab file of BERT."")\n\nflags.DEFINE_bool(\n    ""do_lower_case"", True, ""Whether to use uncased text for BERT."")\n\nflags.DEFINE_string(\n    ""back_translation_dir"", """", ""Directory for back translated sentence."")\n\nflags.DEFINE_integer(\n    ""replicas"", 1,\n    ""An argument for parallel preprocessing. For example, when replicas=3,""\n    ""we divide the data into three parts, and only process one part""\n    ""according to the worker_id."")\n\nflags.DEFINE_integer(\n    ""worker_id"", 0,\n    ""An argument for parallel preprocessing. See \'replicas\' for more details"")\n\n\ndef get_data_for_worker(examples, replicas, worker_id):\n  data_per_worker = len(examples) // replicas\n  remainder = len(examples) - replicas * data_per_worker\n  if worker_id < remainder:\n    start = (data_per_worker + 1) * worker_id\n    end = (data_per_worker + 1) * (worker_id + 1)\n  else:\n    start = data_per_worker * worker_id + remainder\n    end = data_per_worker * (worker_id + 1) + remainder\n  if worker_id == replicas - 1:\n    assert end == len(examples)\n  tf.logging.info(""processing data from {:d} to {:d}"".format(start, end))\n  examples = examples[start: end]\n  return examples, start, end\n\n\ndef build_vocab(examples):\n  vocab = {}\n  def add_to_vocab(word_list):\n    for word in word_list:\n      if word not in vocab:\n        vocab[word] = len(vocab)\n  for i in range(len(examples)):\n    add_to_vocab(examples[i].word_list_a)\n    if examples[i].text_b:\n      add_to_vocab(examples[i].word_list_b)\n  return vocab\n\n\ndef get_data_stats(data_stats_dir, sub_set, sup_size, replicas, examples):\n  data_stats_dir = ""{}/{}"".format(data_stats_dir, sub_set)\n  keys = [""tf_idf"", ""idf""]\n  all_exist = True\n  for key in keys:\n    data_stats_path = ""{}/{}.json"".format(data_stats_dir, key)\n    if not tf.gfile.Exists(data_stats_path):\n      all_exist = False\n      tf.logging.info(""Not exist: {}"".format(data_stats_path))\n  if all_exist:\n    tf.logging.info(""loading data stats from {:s}"".format(data_stats_dir))\n    data_stats = {}\n    for key in keys:\n      with tf.gfile.Open(\n          ""{}/{}.json"".format(data_stats_dir, key)) as inf:\n        data_stats[key] = json.load(inf)\n  else:\n    assert sup_size == -1, ""should use the complete set to get tf_idf""\n    assert replicas == 1, ""should use the complete set to get tf_idf""\n    data_stats = word_level_augment.get_data_stats(examples)\n    tf.gfile.MakeDirs(data_stats_dir)\n    for key in keys:\n      with tf.gfile.Open(""{}/{}.json"".format(data_stats_dir, key), ""w"") as ouf:\n        json.dump(data_stats[key], ouf)\n    tf.logging.info(""dumped data stats to {:s}"".format(data_stats_dir))\n  return data_stats\n\n\ndef tokenize_examples(examples, tokenizer):\n  tf.logging.info(""tokenizing examples"")\n  for i in range(len(examples)):\n    examples[i].word_list_a = tokenizer.tokenize_to_word(examples[i].text_a)\n    if examples[i].text_b:\n      examples[i].word_list_b = tokenizer.tokenize_to_word(examples[i].text_b)\n    if i % 10000 == 0:\n      tf.logging.info(""finished tokenizing example {:d}"".format(i))\n  return examples\n\n\ndef convert_examples_to_features(\n    examples, label_list, seq_length, tokenizer, trunc_keep_right,\n    data_stats=None, aug_ops=None):\n  """"""convert examples to features.""""""\n\n  label_map = {}\n  for (i, label) in enumerate(label_list):\n    label_map[label] = i\n\n  tf.logging.info(""number of examples to process: {}"".format(len(examples)))\n\n  features = []\n\n  if aug_ops:\n    tf.logging.info(""building vocab"")\n    word_vocab = build_vocab(examples)\n    examples = word_level_augment.word_level_augment(\n        examples, aug_ops, word_vocab, data_stats\n    )\n\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.logging.info(""processing {:d}"".format(ex_index))\n    tokens_a = tokenizer.tokenize_to_wordpiece(example.word_list_a)\n    tokens_b = None\n    if example.text_b:\n      tokens_b = tokenizer.tokenize_to_wordpiece(example.word_list_b)\n\n    if tokens_b:\n      # Modifies `tokens_a` and `tokens_b` in place so that the total\n      # length is less than the specified length.\n      # Account for [CLS], [SEP], [SEP] with ""- 3""\n      if trunc_keep_right:\n        _truncate_seq_pair_keep_right(tokens_a, tokens_b, seq_length - 3)\n      else:\n        _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n    else:\n      # Account for [CLS] and [SEP] with ""- 2""\n      if len(tokens_a) > seq_length - 2:\n        if trunc_keep_right:\n          tokens_a = tokens_a[-(seq_length - 2):]\n        else:\n          tokens_a = tokens_a[0:(seq_length - 2)]\n\n    # The convention in BERT is:\n    # (a) For sequence pairs:\n    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n    # (b) For single sequences:\n    #  tokens:   [CLS] the dog is hairy . [SEP]\n    #  type_ids: 0     0   0   0  0     0 0\n    #\n    # Where ""type_ids"" are used to indicate whether this is the first\n    # sequence or the second sequence. The embedding vectors for `type=0` and\n    # `type=1` were learned during pre-training and are added to the wordpiece\n    # embedding vector (and position vector). This is not *strictly* necessary\n    # since the [SEP] token unambigiously separates the sequences, but it makes\n    # it easier for the model to learn the concept of sequences.\n    #\n    # For classification tasks, the first vector (corresponding to [CLS]) is\n    # used as as the ""sentence vector"". Note that this only makes sense because\n    # the entire model is fine-tuned.\n    tokens = []\n    input_type_ids = []\n    tokens.append(""[CLS]"")\n    input_type_ids.append(0)\n    for token in tokens_a:\n      tokens.append(token)\n      input_type_ids.append(0)\n    tokens.append(""[SEP]"")\n    input_type_ids.append(0)\n\n    if tokens_b:\n      for token in tokens_b:\n        tokens.append(token)\n        input_type_ids.append(1)\n      tokens.append(""[SEP]"")\n      input_type_ids.append(1)\n\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    while len(input_ids) < seq_length:\n      input_ids.append(0)\n      input_mask.append(0)\n      input_type_ids.append(0)\n\n    assert len(input_ids) == seq_length\n    assert len(input_mask) == seq_length\n    assert len(input_type_ids) == seq_length\n\n    label_id = label_map[example.label]\n    if ex_index < 1:\n      tf.logging.info(""*** Example ***"")\n      tf.logging.info(""guid: %s"" % (example.guid))\n      # st = "" "".join([str(x) for x in tokens])\n      st = """"\n      for x in tokens:\n        if isinstance(x, unicode):\n          st += x.encode(""ascii"", ""replace"") + "" ""\n        else:\n          st += str(x) + "" ""\n      tf.logging.info(""tokens: %s"" % st)\n      tf.logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n      tf.logging.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n      tf.logging.info(\n          ""input_type_ids: %s"" % "" "".join([str(x) for x in input_type_ids]))\n      tf.logging.info(""label: %s (id = %d)"" % (example.label, label_id))\n\n    features.append(\n        InputFeatures(\n            input_ids=input_ids,\n            input_mask=input_mask,\n            input_type_ids=input_type_ids,\n            label_id=label_id))\n  return features\n\n\ndef _create_int_feature(values):\n  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n  return feature\n\n\nclass InputFeatures(object):\n  """"""A single set of features of data.""""""\n\n  def __init__(self, input_ids, input_mask, input_type_ids, label_id):\n    self.input_ids = input_ids\n    self.input_mask = input_mask\n    self.input_type_ids = input_type_ids\n    self.label_id = label_id\n\n  def get_dict_features(self):\n    return {\n        ""input_ids"": _create_int_feature(self.input_ids),\n        ""input_mask"": _create_int_feature(self.input_mask),\n        ""input_type_ids"": _create_int_feature(self.input_type_ids),\n        ""label_ids"": _create_int_feature([self.label_id])\n    }\n\n\nclass PairedUnsupInputFeatures(object):\n  """"""Features for paired unsup data.""""""\n\n  def __init__(self, ori_input_ids, ori_input_mask, ori_input_type_ids,\n               aug_input_ids, aug_input_mask, aug_input_type_ids):\n    self.ori_input_ids = ori_input_ids\n    self.ori_input_mask = ori_input_mask\n    self.ori_input_type_ids = ori_input_type_ids\n    self.aug_input_ids = aug_input_ids\n    self.aug_input_mask = aug_input_mask\n    self.aug_input_type_ids = aug_input_type_ids\n\n  def get_dict_features(self):\n    return {\n        ""ori_input_ids"": _create_int_feature(self.ori_input_ids),\n        ""ori_input_mask"": _create_int_feature(self.ori_input_mask),\n        ""ori_input_type_ids"": _create_int_feature(self.ori_input_type_ids),\n        ""aug_input_ids"": _create_int_feature(self.aug_input_ids),\n        ""aug_input_mask"": _create_int_feature(self.aug_input_mask),\n        ""aug_input_type_ids"": _create_int_feature(self.aug_input_type_ids),\n    }\n\n\ndef obtain_tfrecord_writer(data_path, worker_id, shard_cnt):\n  tfrecord_writer = tf.python_io.TFRecordWriter(\n      os.path.join(\n          data_path,\n          ""tf_examples.tfrecord.{:d}.{:d}"".format(worker_id, shard_cnt)))\n  return tfrecord_writer\n\n\ndef dump_tfrecord(features, data_path, worker_id=None, max_shard_size=4096):\n  """"""Dump tf record.""""""\n  if not tf.gfile.Exists(data_path):\n    tf.gfile.MakeDirs(data_path)\n  tf.logging.info(""dumping TFRecords"")\n  np.random.shuffle(features)\n  shard_cnt = 0\n  shard_size = 0\n  tfrecord_writer = obtain_tfrecord_writer(data_path, worker_id, shard_cnt)\n  for feature in features:\n    tf_example = tf.train.Example(\n        features=tf.train.Features(feature=feature.get_dict_features()))\n    if shard_size >= max_shard_size:\n      tfrecord_writer.close()\n      shard_cnt += 1\n      tfrecord_writer = obtain_tfrecord_writer(data_path, worker_id, shard_cnt)\n      shard_size = 0\n    shard_size += 1\n    tfrecord_writer.write(tf_example.SerializeToString())\n  tfrecord_writer.close()\n\n\ndef get_data_by_size_lim(train_examples, processor, sup_size):\n  """"""Deterministicly get a dataset with only sup_size examples.""""""\n  # Assuming sup_size < number of labeled data and\n  # that there are same number of examples for each category\n  assert sup_size % len(processor.get_labels()) == 0\n  per_label_size = sup_size // len(processor.get_labels())\n  per_label_examples = {}\n  for i in range(len(train_examples)):\n    label = train_examples[i].label\n    if label not in per_label_examples:\n      per_label_examples[label] = []\n    per_label_examples[label] += [train_examples[i]]\n\n  for label in processor.get_labels():\n    assert len(per_label_examples[label]) >= per_label_size, (\n        ""label {} only has {} examples while the limit""\n        ""is {}"".format(label, len(per_label_examples[label]), per_label_size))\n\n  new_train_examples = []\n  for i in range(per_label_size):\n    for label in processor.get_labels():\n      new_train_examples += [per_label_examples[label][i]]\n  train_examples = new_train_examples\n  return train_examples\n\n\ndef _truncate_seq_pair_keep_right(tokens_a, tokens_b, max_length):\n  """"""Truncates a sequence pair in place to the maximum length.""""""\n\n  # This is a simple heuristic which will always truncate the longer sequence\n  # one token at a time. This makes more sense than truncating an equal percent\n  # of tokens from each, since if one sequence is very short then each token\n  # that\'s truncated likely contains more information than a longer sequence.\n  while True:\n    total_length = len(tokens_a) + len(tokens_b)\n    if total_length <= max_length:\n      break\n    if len(tokens_a) > len(tokens_b):\n      tokens_a.pop(0)\n    else:\n      tokens_b.pop(0)\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n  """"""Truncates a sequence pair in place to the maximum length.""""""\n\n  # This is a simple heuristic which will always truncate the longer sequence\n  # one token at a time. This makes more sense than truncating an equal percent\n  # of tokens from each, since if one sequence is very short then each token\n  # that\'s truncated likely contains more information than a longer sequence.\n  while True:\n    total_length = len(tokens_a) + len(tokens_b)\n    if total_length <= max_length:\n      break\n    if len(tokens_a) > len(tokens_b):\n      tokens_a.pop()\n    else:\n      tokens_b.pop()\n\n\ndef proc_and_save_sup_data(\n    processor, sub_set, raw_data_dir, sup_out_dir,\n    tokenizer, max_seq_length, trunc_keep_right,\n    worker_id, replicas, sup_size):\n  tf.logging.info(""getting examples"")\n  if sub_set == ""train"":\n    examples = processor.get_train_examples(raw_data_dir)\n  elif sub_set == ""dev"":\n    examples = processor.get_dev_examples(raw_data_dir)\n    assert replicas == 1, ""dev set can be processsed with just one worker""\n    assert sup_size == -1, ""should use the full dev set""\n\n  if sup_size != -1:\n    tf.logging.info(""setting number of examples to {:d}"".format(\n        sup_size))\n    examples = get_data_by_size_lim(\n        examples, processor, sup_size)\n  if replicas != 1:\n    if len(examples) < replicas:\n      replicas = len(examples)\n      if worker_id >= replicas:\n        return\n    examples = get_data_for_worker(\n        examples, replicas, worker_id)\n\n  tf.logging.info(""processing data"")\n  examples = tokenize_examples(examples, tokenizer)\n\n  features = convert_examples_to_features(\n      examples, processor.get_labels(), max_seq_length, tokenizer,\n      trunc_keep_right, None, None)\n  dump_tfrecord(features, sup_out_dir, worker_id)\n\n\ndef proc_and_save_unsup_data(\n    processor, sub_set,\n    raw_data_dir, data_stats_dir, unsup_out_dir,\n    tokenizer,\n    max_seq_length, trunc_keep_right,\n    aug_ops, aug_copy_num,\n    worker_id, replicas):\n  # print random seed just to double check that we use different random seeds\n  # for different runs so that we generate different augmented examples for the same original example.\n  random_seed = np.random.randint(0, 100000)\n  tf.logging.info(""random seed: {:d}"".format(random_seed))\n  np.random.seed(random_seed)\n  tf.logging.info(""getting examples"")\n\n  if sub_set == ""train"":\n    ori_examples = processor.get_train_examples(raw_data_dir)\n  elif sub_set.startswith(""unsup""):\n    ori_examples = processor.get_unsup_examples(raw_data_dir, sub_set)\n  else:\n    assert False\n  # this is the size before spliting data for each worker\n  data_total_size = len(ori_examples)\n  if replicas != -1:\n    ori_examples, start, end = get_data_for_worker(\n        ori_examples, replicas, worker_id)\n  else:\n    start = 0\n    end = len(ori_examples)\n\n  tf.logging.info(""getting augmented examples"")\n  aug_examples = copy.deepcopy(ori_examples)\n  aug_examples = sent_level_augment.run_augment(\n      aug_examples, aug_ops, sub_set,\n      aug_copy_num,\n      start, end, data_total_size)\n\n  labels = processor.get_labels() + [""unsup""]\n  tf.logging.info(""processing ori examples"")\n  ori_examples = tokenize_examples(ori_examples, tokenizer)\n  ori_features = convert_examples_to_features(\n      ori_examples, labels, max_seq_length, tokenizer,\n      trunc_keep_right, None, None)\n\n  if ""idf"" in aug_ops:\n    data_stats = get_data_stats(\n        data_stats_dir, sub_set,\n        -1, replicas, ori_examples)\n  else:\n    data_stats = None\n\n  tf.logging.info(""processing aug examples"")\n  aug_examples = tokenize_examples(aug_examples, tokenizer)\n  aug_features = convert_examples_to_features(\n      aug_examples, labels, max_seq_length, tokenizer,\n      trunc_keep_right, data_stats, aug_ops)\n\n  unsup_features = []\n  for ori_feat, aug_feat in zip(ori_features, aug_features):\n    unsup_features.append(PairedUnsupInputFeatures(\n        ori_feat.input_ids,\n        ori_feat.input_mask,\n        ori_feat.input_type_ids,\n        aug_feat.input_ids,\n        aug_feat.input_mask,\n        aug_feat.input_type_ids,\n        ))\n  dump_tfrecord(unsup_features, unsup_out_dir, worker_id)\n\n\ndef main(_):\n\n\n  if FLAGS.max_seq_length > 512:\n    raise ValueError(\n        ""Cannot use sequence length {:d} because the BERT model ""\n        ""was only trained up to sequence length {:d}"".format(\n            FLAGS.max_seq_length, 512))\n\n  processor = raw_data_utils.get_processor(FLAGS.task_name)\n  # Create tokenizer\n  tokenizer = tokenization.FullTokenizer(\n      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n\n  if FLAGS.data_type == ""sup"":\n    sup_out_dir = FLAGS.output_base_dir\n    tf.logging.info(""Create sup. data: subset {} => {}"".format(\n        FLAGS.sub_set, sup_out_dir))\n\n    proc_and_save_sup_data(\n        processor, FLAGS.sub_set, FLAGS.raw_data_dir, sup_out_dir,\n        tokenizer, FLAGS.max_seq_length, FLAGS.trunc_keep_right,\n        FLAGS.worker_id, FLAGS.replicas, FLAGS.sup_size,\n    )\n  elif FLAGS.data_type == ""unsup"":\n    assert FLAGS.aug_ops is not None, \\\n        ""aug_ops is required to preprocess unsupervised data.""\n    unsup_out_dir = os.path.join(\n        FLAGS.output_base_dir,\n        FLAGS.aug_ops,\n        str(FLAGS.aug_copy_num))\n    data_stats_dir = os.path.join(FLAGS.raw_data_dir, ""data_stats"")\n\n\n    tf.logging.info(""Create unsup. data: subset {} => {}"".format(\n        FLAGS.sub_set, unsup_out_dir))\n    proc_and_save_unsup_data(\n        processor, FLAGS.sub_set,\n        FLAGS.raw_data_dir, data_stats_dir, unsup_out_dir,\n        tokenizer, FLAGS.max_seq_length, FLAGS.trunc_keep_right,\n        FLAGS.aug_ops, FLAGS.aug_copy_num,\n        FLAGS.worker_id, FLAGS.replicas)\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
text/uda.py,65,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Code for using the labeled examples and unlabeled examples in unsupervised data augmentation.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport tensorflow as tf\n\nfrom bert import modeling\nfrom bert import optimization\nfrom utils import tpu_utils\n\nflags = tf.flags\nFLAGS = flags.FLAGS\n\n\ndef kl_for_log_probs(log_p, log_q):\n  p = tf.exp(log_p)\n  neg_ent = tf.reduce_sum(p * log_p, axis=-1)\n  neg_cross_ent = tf.reduce_sum(p * log_q, axis=-1)\n  kl = neg_ent - neg_cross_ent\n  return kl\n\n\ndef hidden_to_logits(hidden, is_training, num_classes, scope):\n  hidden_size = hidden.shape[-1].value\n\n  with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n    output_weights = tf.get_variable(\n        ""output_weights"", [num_classes, hidden_size],\n        initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n    output_bias = tf.get_variable(\n        ""output_bias"", [num_classes], initializer=tf.zeros_initializer())\n\n    if is_training:\n      # I.e., 0.1 dropout\n      hidden = tf.nn.dropout(hidden, keep_prob=0.9)\n\n    if hidden.shape.ndims == 3:\n      logits = tf.einsum(""bid,nd->bin"", hidden, output_weights)\n    else:\n      logits = tf.einsum(""bd,nd->bn"", hidden, output_weights)\n    logits = tf.nn.bias_add(logits, output_bias)\n\n  return logits\n\n\ndef get_tsa_threshold(schedule, global_step, num_train_steps, start, end):\n  training_progress = tf.to_float(global_step) / tf.to_float(num_train_steps)\n  if schedule == ""linear_schedule"":\n    threshold = training_progress\n  elif schedule == ""exp_schedule"":\n    scale = 5\n    threshold = tf.exp((training_progress - 1) * scale)\n    # [exp(-5), exp(0)] = [1e-2, 1]\n  elif schedule == ""log_schedule"":\n    scale = 5\n    # [1 - exp(0), 1 - exp(-5)] = [0, 0.99]\n    threshold = 1 - tf.exp((-training_progress) * scale)\n  return threshold * (end - start) + start\n\n\ndef create_model(\n    bert_config,\n    is_training,\n    input_ids,\n    input_mask,\n    input_type_ids,\n    labels,\n    num_labels,\n    use_one_hot_embeddings,\n    tsa,\n    unsup_ratio,\n    global_step,\n    num_train_steps,\n    ):\n\n  num_sample = input_ids.shape[0].value\n  if is_training:\n    assert num_sample % (1 + 2 * unsup_ratio) == 0\n    sup_batch_size = num_sample // (1 + 2 * unsup_ratio)\n    unsup_batch_size = sup_batch_size * unsup_ratio\n  else:\n    sup_batch_size = num_sample\n    unsup_batch_size = 0\n\n  pooled = modeling.bert_model(\n      config=bert_config,\n      is_training=is_training,\n      input_ids=input_ids,\n      input_mask=input_mask,\n      token_type_ids=input_type_ids,\n      use_one_hot_embeddings=use_one_hot_embeddings)\n\n  clas_logits = hidden_to_logits(\n      hidden=pooled,\n      is_training=is_training,\n      num_classes=num_labels,\n      scope=""classifier"")\n\n  log_probs = tf.nn.log_softmax(clas_logits, axis=-1)\n  correct_label_probs = None\n\n  with tf.variable_scope(""sup_loss""):\n    sup_log_probs = log_probs[:sup_batch_size]\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n    tgt_label_prob = one_hot_labels\n\n    per_example_loss = -tf.reduce_sum(tgt_label_prob * sup_log_probs, axis=-1)\n    loss_mask = tf.ones_like(per_example_loss, dtype=per_example_loss.dtype)\n    correct_label_probs = tf.reduce_sum(\n        one_hot_labels * tf.exp(sup_log_probs), axis=-1)\n\n    if tsa:\n      tsa_start = 1. / num_labels\n      tsa_threshold = get_tsa_threshold(\n          tsa, global_step, num_train_steps,\n          tsa_start, end=1)\n\n      larger_than_threshold = tf.greater(\n          correct_label_probs, tsa_threshold)\n      loss_mask = loss_mask * (1 - tf.cast(larger_than_threshold, tf.float32))\n    else:\n      tsa_threshold = 1\n\n    loss_mask = tf.stop_gradient(loss_mask)\n    per_example_loss = per_example_loss * loss_mask\n    sup_loss = (tf.reduce_sum(per_example_loss) /\n                tf.maximum(tf.reduce_sum(loss_mask), 1))\n\n  unsup_loss_mask = None\n  if is_training and unsup_ratio > 0:\n    with tf.variable_scope(""unsup_loss""):\n      ori_start = sup_batch_size\n      ori_end = ori_start + unsup_batch_size\n      aug_start = sup_batch_size + unsup_batch_size\n      aug_end = aug_start + unsup_batch_size\n\n      ori_log_probs = log_probs[ori_start : ori_end]\n      aug_log_probs = log_probs[aug_start : aug_end]\n      unsup_loss_mask = 1\n      if FLAGS.uda_softmax_temp != -1:\n        tgt_ori_log_probs = tf.nn.log_softmax(\n            clas_logits[ori_start : ori_end] / FLAGS.uda_softmax_temp,\n            axis=-1)\n        tgt_ori_log_probs = tf.stop_gradient(tgt_ori_log_probs)\n      else:\n        tgt_ori_log_probs = tf.stop_gradient(ori_log_probs)\n\n      if FLAGS.uda_confidence_thresh != -1:\n        largest_prob = tf.reduce_max(tf.exp(ori_log_probs), axis=-1)\n        unsup_loss_mask = tf.cast(tf.greater(\n            largest_prob, FLAGS.uda_confidence_thresh), tf.float32)\n        unsup_loss_mask = tf.stop_gradient(unsup_loss_mask)\n\n      per_example_kl_loss = kl_for_log_probs(\n          tgt_ori_log_probs, aug_log_probs) * unsup_loss_mask\n      unsup_loss = tf.reduce_mean(per_example_kl_loss)\n\n  else:\n    unsup_loss = 0.\n\n  return (sup_loss, unsup_loss, clas_logits[:sup_batch_size],\n          per_example_loss, loss_mask,\n          tsa_threshold, unsup_loss_mask, correct_label_probs)\n\n\ndef get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n  """"""Compute the union of the current variables and checkpoint variables.""""""\n  assignment_map = {}\n  initialized_variable_names = {}\n\n  name_to_variable = collections.OrderedDict()\n  for var in tvars:\n    name = var.name\n    m = re.match(""^(.*):\\\\d+$"", name)\n    if m is not None:\n      name = m.group(1)\n    name_to_variable[name] = var\n\n  init_vars = tf.train.list_variables(init_checkpoint)\n\n  assignment_map = collections.OrderedDict()\n  for x in init_vars:\n    (name, var) = (x[0], x[1])\n    if name not in name_to_variable:\n      continue\n    assignment_map[name] = name_to_variable[name]\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + "":0""] = 1\n\n  return (assignment_map, initialized_variable_names)\n\n\ndef model_fn_builder(\n    bert_config,\n    init_checkpoint,\n    learning_rate,\n    clip_norm,\n    num_train_steps,\n    num_warmup_steps,\n    use_tpu,\n    use_one_hot_embeddings,\n    num_labels,\n    unsup_ratio,\n    uda_coeff,\n    tsa,\n    print_feature=True,\n    print_structure=True):\n  """"""Returns `model_fn` closure for TPUEstimator.""""""\n\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n    """"""The `model_fn` for TPUEstimator.""""""\n    if print_feature:\n      tf.logging.info(""*** Features ***"")\n      for name in sorted(features.keys()):\n        tf.logging.info(\n            ""  name = %s, shape = %s"" % (name, features[name].shape))\n\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n\n    global_step = tf.train.get_or_create_global_step()\n    ##### Classification objective\n    label_ids = features[""label_ids""]\n    label_ids = tf.reshape(label_ids, [-1])\n\n    if unsup_ratio > 0 and ""ori_input_ids"" in features:\n      input_ids = tf.concat([\n          features[""input_ids""],\n          features[""ori_input_ids""],\n          features[""aug_input_ids""]], 0)\n      input_mask = tf.concat([\n          features[""input_mask""],\n          features[""ori_input_mask""],\n          features[""aug_input_mask""]], 0)\n      input_type_ids = tf.concat([\n          features[""input_type_ids""],\n          features[""ori_input_type_ids""],\n          features[""aug_input_type_ids""]], 0)\n    else:\n      input_ids = features[""input_ids""]\n      input_mask = features[""input_mask""]\n      input_type_ids = features[""input_type_ids""]\n\n    (sup_loss, unsup_loss, logits,\n     per_example_loss, loss_mask,\n     tsa_threshold,\n     unsup_loss_mask, correct_label_probs) = create_model(\n         bert_config=bert_config,\n         is_training=is_training,\n         input_ids=input_ids,\n         input_mask=input_mask,\n         input_type_ids=input_type_ids,\n         labels=label_ids,\n         num_labels=num_labels,\n         use_one_hot_embeddings=use_one_hot_embeddings,\n         tsa=tsa,\n         unsup_ratio=unsup_ratio,\n         global_step=global_step,\n         num_train_steps=num_train_steps,\n         )\n\n    ##### Aggregate losses into total_loss\n    metric_dict = {}\n\n    # number of correct predictions\n    predictions = tf.argmax(logits, axis=-1, output_type=label_ids.dtype)\n    is_correct = tf.to_float(tf.equal(predictions, label_ids))\n    acc = tf.reduce_mean(is_correct)\n    # add sup. metrics to dict\n    metric_dict[""sup/loss""] = sup_loss\n    metric_dict[""sup/accu""] = acc\n    metric_dict[""sup/correct_cat_probs""] = correct_label_probs\n    metric_dict[""sup/tsa_threshold""] = tsa_threshold\n\n    metric_dict[""sup/sup_trained_ratio""] = tf.reduce_mean(loss_mask)\n    total_loss = sup_loss\n\n    if unsup_ratio > 0 and uda_coeff > 0 and ""input_ids"" in features:\n      total_loss += uda_coeff * unsup_loss\n      metric_dict[""unsup/loss""] = unsup_loss\n\n    if unsup_loss_mask is not None:\n      metric_dict[""unsup/high_prob_ratio""] = tf.reduce_mean(unsup_loss_mask)\n\n    ##### Initialize variables with pre-trained models\n    tvars = tf.trainable_variables()\n\n    scaffold_fn = None\n    if init_checkpoint:\n      (assignment_map,\n       initialized_variable_names) = get_assignment_map_from_checkpoint(\n           tvars, init_checkpoint)\n      if use_tpu:\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n    else:\n      initialized_variable_names = {}\n\n    if print_structure:\n      tf.logging.info(""**** Trainable Variables ****"")\n      for var in tvars:\n        init_string = """"\n        if var.name in initialized_variable_names:\n          init_string = "", *INIT_FROM_CKPT*""\n        tf.logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,\n                        init_string)\n\n    ##### Construct TPU Estimator Spec based on the specific mode\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      ## Create optimizer for training\n      train_op, curr_lr = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps,\n          use_tpu, clip_norm, global_step)\n      metric_dict[""learning_rate""] = curr_lr\n\n      ## Create host_call for training\n      host_call = tpu_utils.construct_scalar_host_call(\n          metric_dict=metric_dict,\n          model_dir=params[""model_dir""],\n          prefix=""training/"",\n          reduce_fn=tf.reduce_mean)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          host_call=host_call,\n          scaffold_fn=scaffold_fn)\n\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def clas_metric_fn(per_example_loss, label_ids, logits):\n        ## classification loss & accuracy\n        loss = tf.metrics.mean(per_example_loss)\n\n        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n        accuracy = tf.metrics.accuracy(label_ids, predictions)\n\n        ret_dict = {\n            ""eval_classify_loss"": loss,\n            ""eval_classify_accuracy"": accuracy\n        }\n\n        return ret_dict\n\n      eval_metrics = (clas_metric_fn, [per_example_loss, label_ids, logits])\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      raise ValueError(""Only TRAIN and EVAL modes are supported: %s"" % (mode))\n\n    return output_spec\n\n  return model_fn\n\n'"
image/randaugment/__init__.py,0,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
image/randaugment/augmentation_transforms.py,1,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Transforms used in the Augmentation Policies.\n\nCopied from AutoAugment: https://github.com/tensorflow/models/blob/master/research/autoaugment/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\nimport numpy as np\n# pylint:disable=g-multiple-import\nfrom PIL import ImageOps, ImageEnhance, ImageFilter, Image\n# pylint:enable=g-multiple-import\n\nimport tensorflow as tf\n\nFLAGS = tf.flags.FLAGS\n\n\nIMAGE_SIZE = 32\n# What is the dataset mean and std of the images on the training set\nPARAMETER_MAX = 10  # What is the max \'level\' a transform could be predicted\n\n\ndef get_mean_and_std():\n  if FLAGS.task_name == ""cifar10"":\n    means = [0.49139968, 0.48215841, 0.44653091]\n    stds = [0.24703223, 0.24348513, 0.26158784]\n  elif FLAGS.task_name == ""svhn"":\n    means = [0.4376821, 0.4437697, 0.47280442]\n    stds = [0.19803012, 0.20101562, 0.19703614]\n  else:\n    assert False\n  return means, stds\n\n\ndef _width_height_from_img_shape(img_shape):\n  """"""`img_shape` in autoaugment is (height, width).""""""\n  return (img_shape[1], img_shape[0])\n\n\ndef random_flip(x):\n  """"""Flip the input x horizontally with 50% probability.""""""\n  if np.random.rand(1)[0] > 0.5:\n    return np.fliplr(x)\n  return x\n\n\ndef zero_pad_and_crop(img, amount=4):\n  """"""Zero pad by `amount` zero pixels on each side then take a random crop.\n\n  Args:\n    img: numpy image that will be zero padded and cropped.\n    amount: amount of zeros to pad `img` with horizontally and verically.\n\n  Returns:\n    The cropped zero padded img. The returned numpy array will be of the same\n    shape as `img`.\n  """"""\n  padded_img = np.zeros((img.shape[0] + amount * 2, img.shape[1] + amount * 2,\n                         img.shape[2]))\n  padded_img[amount:img.shape[0] + amount, amount:\n             img.shape[1] + amount, :] = img\n  top = np.random.randint(low=0, high=2 * amount)\n  left = np.random.randint(low=0, high=2 * amount)\n  new_img = padded_img[top:top + img.shape[0], left:left + img.shape[1], :]\n  return new_img\n\n\ndef create_cutout_mask(img_height, img_width, num_channels, size):\n  """"""Creates a zero mask used for cutout of shape `img_height` x `img_width`.\n\n  Args:\n    img_height: Height of image cutout mask will be applied to.\n    img_width: Width of image cutout mask will be applied to.\n    num_channels: Number of channels in the image.\n    size: Size of the zeros mask.\n\n  Returns:\n    A mask of shape `img_height` x `img_width` with all ones except for a\n    square of zeros of shape `size` x `size`. This mask is meant to be\n    elementwise multiplied with the original image. Additionally returns\n    the `upper_coord` and `lower_coord` which specify where the cutout mask\n    will be applied.\n  """"""\n  assert img_height == img_width\n\n  # Sample center where cutout mask will be applied\n  height_loc = np.random.randint(low=0, high=img_height)\n  width_loc = np.random.randint(low=0, high=img_width)\n\n  # Determine upper right and lower left corners of patch\n  upper_coord = (max(0, height_loc - size // 2), max(0, width_loc - size // 2))\n  lower_coord = (min(img_height, height_loc + size // 2),\n                 min(img_width, width_loc + size // 2))\n  mask_height = lower_coord[0] - upper_coord[0]\n  mask_width = lower_coord[1] - upper_coord[1]\n  assert mask_height > 0\n  assert mask_width > 0\n\n  mask = np.ones((img_height, img_width, num_channels))\n  zeros = np.zeros((mask_height, mask_width, num_channels))\n  mask[upper_coord[0]:lower_coord[0], upper_coord[1]:lower_coord[1], :] = (\n      zeros)\n  return mask, upper_coord, lower_coord\n\n\ndef cutout_numpy(img, size=16):\n  """"""Apply cutout with mask of shape `size` x `size` to `img`.\n\n  The cutout operation is from the paper https://arxiv.org/abs/1708.04552.\n  This operation applies a `size`x`size` mask of zeros to a random location\n  within `img`.\n\n  Args:\n    img: Numpy image that cutout will be applied to.\n    size: Height/width of the cutout mask that will be\n\n  Returns:\n    A numpy tensor that is the result of applying the cutout mask to `img`.\n  """"""\n  img_height, img_width, num_channels = (img.shape[0], img.shape[1],\n                                         img.shape[2])\n  assert len(img.shape) == 3\n  mask, _, _ = create_cutout_mask(img_height, img_width, num_channels, size)\n  return img * mask\n\n\ndef float_parameter(level, maxval):\n  """"""Helper function to scale `val` between 0 and maxval .\n\n  Args:\n    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n    maxval: Maximum value that the operation can have. This will be scaled\n      to level/PARAMETER_MAX.\n\n  Returns:\n    A float that results from scaling `maxval` according to `level`.\n  """"""\n  return float(level) * maxval / PARAMETER_MAX\n\n\ndef int_parameter(level, maxval):\n  """"""Helper function to scale `val` between 0 and maxval .\n\n  Args:\n    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n    maxval: Maximum value that the operation can have. This will be scaled\n      to level/PARAMETER_MAX.\n\n  Returns:\n    An int that results from scaling `maxval` according to `level`.\n  """"""\n  return int(level * maxval / PARAMETER_MAX)\n\n\ndef pil_wrap(img, use_mean_std):\n  """"""Convert the `img` numpy tensor to a PIL Image.""""""\n\n  if use_mean_std:\n    MEANS, STDS = get_mean_and_std()\n  else:\n    MEANS = [0, 0, 0]\n    STDS = [1, 1, 1]\n  img_ori = (img * STDS + MEANS) * 255\n\n  return Image.fromarray(\n      np.uint8((img * STDS + MEANS) * 255.0)).convert(\'RGBA\')\n\n\ndef pil_unwrap(pil_img, use_mean_std, img_shape):\n  """"""Converts the PIL img to a numpy array.""""""\n  if use_mean_std:\n    MEANS, STDS = get_mean_and_std()\n  else:\n    MEANS = [0, 0, 0]\n    STDS = [1, 1, 1]\n  pic_array = np.array(pil_img.getdata()).reshape((img_shape[0], img_shape[1], 4)) / 255.0\n  i1, i2 = np.where(pic_array[:, :, 3] == 0)\n  pic_array = (pic_array[:, :, :3] - MEANS) / STDS\n  pic_array[i1, i2] = [0, 0, 0]\n  return pic_array\n\n\ndef apply_policy(policy, img, use_mean_std=True):\n  """"""Apply the `policy` to the numpy `img`.\n\n  Args:\n    policy: A list of tuples with the form (name, probability, level) where\n      `name` is the name of the augmentation operation to apply, `probability`\n      is the probability of applying the operation and `level` is what strength\n      the operation to apply.\n    img: Numpy image that will have `policy` applied to it.\n\n  Returns:\n    The result of applying `policy` to `img`.\n  """"""\n  img_shape = img.shape\n  pil_img = pil_wrap(img, use_mean_std)\n  for xform in policy:\n    assert len(xform) == 3\n    name, probability, level = xform\n    xform_fn = NAME_TO_TRANSFORM[name].pil_transformer(\n        probability, level, img_shape)\n    pil_img = xform_fn(pil_img)\n  return pil_unwrap(pil_img, use_mean_std, img_shape)\n\n\nclass TransformFunction(object):\n  """"""Wraps the Transform function for pretty printing options.""""""\n\n  def __init__(self, func, name):\n    self.f = func\n    self.name = name\n\n  def __repr__(self):\n    return \'<\' + self.name + \'>\'\n\n  def __call__(self, pil_img):\n    return self.f(pil_img)\n\n\nclass TransformT(object):\n  """"""Each instance of this class represents a specific transform.""""""\n\n  def __init__(self, name, xform_fn):\n    self.name = name\n    self.xform = xform_fn\n\n  def pil_transformer(self, probability, level, img_shape):\n\n    def return_function(im):\n      if random.random() < probability:\n        im = self.xform(im, level, img_shape)\n      return im\n\n    name = self.name + \'({:.1f},{})\'.format(probability, level)\n    return TransformFunction(return_function, name)\n\n\n################## Transform Functions ##################\nidentity = TransformT(\'identity\', lambda pil_img, level, _: pil_img)\nflip_lr = TransformT(\n    \'FlipLR\',\n    lambda pil_img, level, _: pil_img.transpose(Image.FLIP_LEFT_RIGHT))\nflip_ud = TransformT(\n    \'FlipUD\',\n    lambda pil_img, level, _: pil_img.transpose(Image.FLIP_TOP_BOTTOM))\n# pylint:disable=g-long-lambda\nauto_contrast = TransformT(\n    \'AutoContrast\',\n    lambda pil_img, level, _: ImageOps.autocontrast(\n        pil_img.convert(\'RGB\')).convert(\'RGBA\'))\nequalize = TransformT(\n    \'Equalize\',\n    lambda pil_img, level, _: ImageOps.equalize(\n        pil_img.convert(\'RGB\')).convert(\'RGBA\'))\ninvert = TransformT(\n    \'Invert\',\n    lambda pil_img, level, _: ImageOps.invert(\n        pil_img.convert(\'RGB\')).convert(\'RGBA\'))\n# pylint:enable=g-long-lambda\nblur = TransformT(\n    \'Blur\', lambda pil_img, level, _: pil_img.filter(ImageFilter.BLUR))\nsmooth = TransformT(\n    \'Smooth\',\n    lambda pil_img, level, _: pil_img.filter(ImageFilter.SMOOTH))\n\n\ndef _rotate_impl(pil_img, level, _):\n  """"""Rotates `pil_img` from -30 to 30 degrees depending on `level`.""""""\n  degrees = int_parameter(level, 30)\n  if random.random() > 0.5:\n    degrees = -degrees\n  return pil_img.rotate(degrees)\n\n\nrotate = TransformT(\'Rotate\', _rotate_impl)\n\n\ndef _posterize_impl(pil_img, level, _):\n  """"""Applies PIL Posterize to `pil_img`.""""""\n  level = int_parameter(level, 4)\n  return ImageOps.posterize(pil_img.convert(\'RGB\'), 4 - level).convert(\'RGBA\')\n\n\nposterize = TransformT(\'Posterize\', _posterize_impl)\n\n\ndef _shear_x_impl(pil_img, level, img_shape):\n  """"""Applies PIL ShearX to `pil_img`.\n\n  The ShearX operation shears the image along the horizontal axis with `level`\n  magnitude.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had ShearX applied to it.\n  """"""\n  level = float_parameter(level, 0.3)\n  if random.random() > 0.5:\n    level = -level\n  return pil_img.transform(\n      _width_height_from_img_shape(img_shape),\n      Image.AFFINE,\n      (1, level, 0, 0, 1, 0))\n\n\nshear_x = TransformT(\'ShearX\', _shear_x_impl)\n\n\ndef _shear_y_impl(pil_img, level, img_shape):\n  """"""Applies PIL ShearY to `pil_img`.\n\n  The ShearY operation shears the image along the vertical axis with `level`\n  magnitude.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had ShearX applied to it.\n  """"""\n  level = float_parameter(level, 0.3)\n  if random.random() > 0.5:\n    level = -level\n  return pil_img.transform(\n      _width_height_from_img_shape(img_shape),\n      Image.AFFINE,\n      (1, 0, 0, level, 1, 0))\n\n\nshear_y = TransformT(\'ShearY\', _shear_y_impl)\n\n\ndef _translate_x_impl(pil_img, level, img_shape):\n  """"""Applies PIL TranslateX to `pil_img`.\n\n  Translate the image in the horizontal direction by `level`\n  number of pixels.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had TranslateX applied to it.\n  """"""\n  level = int_parameter(level, 10)\n  if random.random() > 0.5:\n    level = -level\n  return pil_img.transform(\n      _width_height_from_img_shape(img_shape),\n      Image.AFFINE,\n      (1, 0, level, 0, 1, 0))\n\n\ntranslate_x = TransformT(\'TranslateX\', _translate_x_impl)\n\n\ndef _translate_y_impl(pil_img, level, img_shape):\n  """"""Applies PIL TranslateY to `pil_img`.\n\n  Translate the image in the vertical direction by `level`\n  number of pixels.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had TranslateY applied to it.\n  """"""\n  level = int_parameter(level, 10)\n  if random.random() > 0.5:\n    level = -level\n  return pil_img.transform(\n      _width_height_from_img_shape(img_shape),\n      Image.AFFINE,\n      (1, 0, 0, 0, 1, level))\n\n\ntranslate_y = TransformT(\'TranslateY\', _translate_y_impl)\n\n\ndef _crop_impl(pil_img, level, img_shape, interpolation=Image.BILINEAR):\n  """"""Applies a crop to `pil_img` with the size depending on the `level`.""""""\n  cropped = pil_img.crop((level, level, img_shape[0] - level, img_shape[1] - level))\n  resized = cropped.resize((img_shape[0], img_shape[1]), interpolation)\n  return resized\n\n\ncrop_bilinear = TransformT(\'CropBilinear\', _crop_impl)\n\n\ndef _solarize_impl(pil_img, level, _):\n  """"""Applies PIL Solarize to `pil_img`.\n\n  Translate the image in the vertical direction by `level`\n  number of pixels.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had Solarize applied to it.\n  """"""\n  level = int_parameter(level, 256)\n  return ImageOps.solarize(pil_img.convert(\'RGB\'), 256 - level).convert(\'RGBA\')\n\n\nsolarize = TransformT(\'Solarize\', _solarize_impl)\n\n\ndef _cutout_pil_impl(pil_img, level, img_shape):\n  """"""Apply cutout to pil_img at the specified level.""""""\n  size = int_parameter(level, 20)\n  if size <= 0:\n    return pil_img\n  img_height, img_width, num_channels = (img_shape[0], img_shape[1], 3)\n  _, upper_coord, lower_coord = (\n      create_cutout_mask(img_height, img_width, num_channels, size))\n  pixels = pil_img.load()  # create the pixel map\n  for i in range(upper_coord[0], lower_coord[0]):  # for every col:\n    for j in range(upper_coord[1], lower_coord[1]):  # For every row\n      pixels[i, j] = (125, 122, 113, 0)  # set the colour accordingly\n  return pil_img\n\ncutout = TransformT(\'Cutout\', _cutout_pil_impl)\n\n\ndef _enhancer_impl(enhancer):\n  """"""Sets level to be between 0.1 and 1.8 for ImageEnhance transforms of PIL.""""""\n  def impl(pil_img, level, _):\n    v = float_parameter(level, 1.8) + .1  # going to 0 just destroys it\n    return enhancer(pil_img).enhance(v)\n  return impl\n\n\ncolor = TransformT(\'Color\', _enhancer_impl(ImageEnhance.Color))\ncontrast = TransformT(\'Contrast\', _enhancer_impl(ImageEnhance.Contrast))\nbrightness = TransformT(\'Brightness\', _enhancer_impl(\n    ImageEnhance.Brightness))\nsharpness = TransformT(\'Sharpness\', _enhancer_impl(ImageEnhance.Sharpness))\n\nALL_TRANSFORMS = [\n    flip_lr,\n    flip_ud,\n    auto_contrast,\n    equalize,\n    invert,\n    rotate,\n    posterize,\n    crop_bilinear,\n    solarize,\n    color,\n    contrast,\n    brightness,\n    sharpness,\n    shear_x,\n    shear_y,\n    translate_x,\n    translate_y,\n    cutout,\n    blur,\n    smooth\n]\n\nNAME_TO_TRANSFORM = {t.name: t for t in ALL_TRANSFORMS}\nTRANSFORM_NAMES = NAME_TO_TRANSFORM.keys()\n'"
image/randaugment/custom_ops.py,27,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Contains convenience wrappers for typical Neural Network TensorFlow layers.\n\nOps that have different behavior during training or eval have an is_training\nparameter.\n\nCopied from AutoAugment: https://github.com/tensorflow/models/blob/master/research/autoaugment/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport numpy as np\nimport tensorflow as tf\nfrom absl import flags\n\n\narg_scope = tf.contrib.framework.arg_scope\nFLAGS = tf.flags.FLAGS\n\n\ndef variable(name, shape, dtype, initializer, trainable):\n  """"""Returns a TF variable with the passed in specifications.""""""\n  var = tf.get_variable(\n      name,\n      shape=shape,\n      dtype=dtype,\n      initializer=initializer,\n      trainable=trainable)\n  return var\n\n\ndef global_avg_pool(x, scope=None):\n  """"""Average pools away spatial height and width dimension of 4D tensor.""""""\n  assert x.get_shape().ndims == 4\n  with tf.name_scope(scope, \'global_avg_pool\', [x]):\n    kernel_size = (1, int(x.shape[1]), int(x.shape[2]), 1)\n    squeeze_dims = (1, 2)\n    result = tf.nn.avg_pool(\n        x,\n        ksize=kernel_size,\n        strides=(1, 1, 1, 1),\n        padding=\'VALID\',\n        data_format=\'NHWC\')\n    return tf.squeeze(result, squeeze_dims)\n\n\ndef zero_pad(inputs, in_filter, out_filter):\n  """"""Zero pads `input` tensor to have `out_filter` number of filters.""""""\n  outputs = tf.pad(inputs, [[0, 0], [0, 0], [0, 0],\n                            [(out_filter - in_filter) // 2,\n                             (out_filter - in_filter) // 2]])\n  return outputs\n\n\n@tf.contrib.framework.add_arg_scope\ndef batch_norm(inputs,\n               update_stats=True,\n               decay=0.999,\n               center=True,\n               scale=False,\n               epsilon=0.001,\n               is_training=True,\n               reuse=None,\n               scope=None,\n              ):\n  """"""Small wrapper around tf.contrib.layers.batch_norm.""""""\n  batch_norm_op = tf.layers.batch_normalization(\n      inputs,\n      axis=-1,\n      momentum=decay,\n      epsilon=epsilon,\n      center=center,\n      scale=scale,\n      training=is_training,\n      fused=True,\n      trainable=True,\n  )\n  return batch_norm_op\n\n\ndef stride_arr(stride_h, stride_w):\n  return [1, stride_h, stride_w, 1]\n\n\n@tf.contrib.framework.add_arg_scope\ndef conv2d(inputs,\n           num_filters_out,\n           kernel_size,\n           stride=1,\n           scope=None,\n           reuse=None):\n  """"""Adds a 2D convolution.\n\n  conv2d creates a variable called \'weights\', representing the convolutional\n  kernel, that is convolved with the input.\n\n  Args:\n    inputs: a 4D tensor in NHWC format.\n    num_filters_out: the number of output filters.\n    kernel_size: an int specifying the kernel height and width size.\n    stride: an int specifying the height and width stride.\n    scope: Optional scope for variable_scope.\n    reuse: whether or not the layer and its variables should be reused.\n  Returns:\n    a tensor that is the result of a convolution being applied to `inputs`.\n  """"""\n  with tf.variable_scope(scope, \'Conv\', [inputs], reuse=reuse):\n    num_filters_in = int(inputs.shape[3])\n    weights_shape = [kernel_size, kernel_size, num_filters_in, num_filters_out]\n\n    # Initialization\n    n = int(weights_shape[0] * weights_shape[1] * weights_shape[3])\n    weights_initializer = tf.random_normal_initializer(\n        stddev=np.sqrt(2.0 / n))\n\n    weights = variable(\n        name=\'weights\',\n        shape=weights_shape,\n        dtype=tf.float32,\n        initializer=weights_initializer,\n        trainable=True)\n    strides = stride_arr(stride, stride)\n    outputs = tf.nn.conv2d(\n        inputs, weights, strides, padding=\'SAME\', data_format=\'NHWC\')\n    return outputs\n\n\n@tf.contrib.framework.add_arg_scope\ndef fc(inputs,\n       num_units_out,\n       scope=None,\n       reuse=None):\n  """"""Creates a fully connected layer applied to `inputs`.\n\n  Args:\n    inputs: a tensor that the fully connected layer will be applied to. It\n      will be reshaped if it is not 2D.\n    num_units_out: the number of output units in the layer.\n    scope: Optional scope for variable_scope.\n    reuse: whether or not the layer and its variables should be reused.\n\n  Returns:\n     a tensor that is the result of applying a linear matrix to `inputs`.\n  """"""\n  if len(inputs.shape) > 2:\n    inputs = tf.reshape(inputs, [int(inputs.shape[0]), -1])\n\n  with tf.variable_scope(scope, \'FC\', [inputs], reuse=reuse):\n    num_units_in = inputs.shape[1]\n    weights_shape = [num_units_in, num_units_out]\n    unif_init_range = 1.0 / (num_units_out)**(0.5)\n    weights_initializer = tf.random_uniform_initializer(\n        -unif_init_range, unif_init_range)\n    weights = variable(\n        name=\'weights\',\n        shape=weights_shape,\n        dtype=tf.float32,\n        initializer=weights_initializer,\n        trainable=True)\n    bias_initializer = tf.constant_initializer(0.0)\n    biases = variable(\n        name=\'biases\',\n        shape=[num_units_out,],\n        dtype=tf.float32,\n        initializer=bias_initializer,\n        trainable=True)\n    outputs = tf.nn.xw_plus_b(inputs, weights, biases)\n    return outputs\n\n\n@tf.contrib.framework.add_arg_scope\ndef avg_pool(inputs, kernel_size, stride=2, padding=\'VALID\', scope=None):\n  """"""Wrapper around tf.nn.avg_pool.""""""\n  with tf.name_scope(scope, \'AvgPool\', [inputs]):\n    kernel = stride_arr(kernel_size, kernel_size)\n    strides = stride_arr(stride, stride)\n    return tf.nn.avg_pool(\n        inputs,\n        ksize=kernel,\n        strides=strides,\n        padding=padding,\n        data_format=\'NHWC\')\n\n'"
image/randaugment/policies.py,1,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Augmentation policies found by AutoAugment.""""""\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef imagenet_policies():\n  """"""AutoAugment policies found on ImageNet.\n\n  This policy also transfers to five FGVC datasets with image size similar to\n  ImageNet including Oxford 102 Flowers, Caltech-101, Oxford-IIIT Pets,\n  FGVC Aircraft and Stanford Cars.\n  """"""\n  policies = [\n      [(""Posterize"", 0.4, 8), (""Rotate"", 0.6, 9)],\n      [(""Solarize"", 0.6, 5), (""AutoContrast"", 0.6, 5)],\n      [(""Equalize"", 0.8, 8), (""Equalize"", 0.6, 3)],\n      [(""Posterize"", 0.6, 7), (""Posterize"", 0.6, 6)],\n      [(""Equalize"", 0.4, 7), (""Solarize"", 0.2, 4)],\n      [(""Equalize"", 0.4, 4), (""Rotate"", 0.8, 8)],\n      [(""Solarize"", 0.6, 3), (""Equalize"", 0.6, 7)],\n      [(""Posterize"", 0.8, 5), (""Equalize"", 1.0, 2)],\n      [(""Rotate"", 0.2, 3), (""Solarize"", 0.6, 8)],\n      [(""Equalize"", 0.6, 8), (""Posterize"", 0.4, 6)],\n      [(""Rotate"", 0.8, 8), (""Color"", 0.4, 0)],\n      [(""Rotate"", 0.4, 9), (""Equalize"", 0.6, 2)],\n      [(""Equalize"", 0.0, 7), (""Equalize"", 0.8, 8)],\n      [(""Invert"", 0.6, 4), (""Equalize"", 1.0, 8)],\n      [(""Color"", 0.6, 4), (""Contrast"", 1.0, 8)],\n      [(""Rotate"", 0.8, 8), (""Color"", 1.0, 2)],\n      [(""Color"", 0.8, 8), (""Solarize"", 0.8, 7)],\n      [(""Sharpness"", 0.4, 7), (""Invert"", 0.6, 8)],\n      [(""ShearX"", 0.6, 5), (""Equalize"", 1.0, 9)],\n      [(""Color"", 0.4, 0), (""Equalize"", 0.6, 3)]\n  ]\n  return policies\n\n\ndef get_trans_list():\n  trans_list = [\n      \'Invert\', \'Cutout\', \'Sharpness\', \'AutoContrast\', \'Posterize\',\n      \'ShearX\', \'TranslateX\', \'TranslateY\', \'ShearY\', \'Rotate\',\n      \'Equalize\', \'Contrast\', \'Color\', \'Solarize\', \'Brightness\']\n  return trans_list\n\n\ndef randaug_policies():\n  trans_list = get_trans_list()\n  tf.logging.info(""trans_list: %s"", str(trans_list))\n  op_list = []\n  for trans in trans_list:\n    for magnitude in range(1, 10):\n      op_list += [(trans, 0.5, magnitude)]\n  policies = []\n  for op_1 in op_list:\n    for op_2 in op_list:\n      policies += [[op_1, op_2]]\n  return policies\n\n'"
image/randaugment/shake_drop.py,13,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Builds the Shake-Drop Model.\nCopied from AutoAugment: https://github.com/tensorflow/models/blob/master/research/autoaugment/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nfrom randaugment import custom_ops as ops\n\nimport tensorflow as tf\n\n\ndef round_int(x):\n  """"""Rounds `x` and then converts to an int.""""""\n  return int(math.floor(x + 0.5))\n\n\ndef shortcut(x, output_filters, stride):\n  """"""Applies strided avg pool or zero padding to make output_filters match x.""""""\n  num_filters = int(x.shape[3])\n  if stride == 2:\n    x = ops.avg_pool(x, 2, stride=stride, padding=\'SAME\')\n  if num_filters != output_filters:\n    diff = output_filters - num_filters\n    assert diff > 0\n    # Zero padd diff zeros\n    padding = [[0, 0], [0, 0], [0, 0], [0, diff]]\n    x = tf.pad(x, padding)\n  return x\n\n\ndef calc_prob(curr_layer, total_layers, p_l):\n  """"""Calculates drop prob depending on the current layer.""""""\n  return 1 - (float(curr_layer) / total_layers) * p_l\n\n\ndef bottleneck_layer(x, n, stride, prob, is_training, alpha, beta):\n  """"""Bottleneck layer for shake drop model.""""""\n  assert alpha[1] > alpha[0]\n  assert beta[1] > beta[0]\n  with tf.variable_scope(\'bottleneck_{}\'.format(prob)):\n    input_layer = x\n    x = ops.batch_norm(x, scope=\'bn_1_pre\')\n    x = ops.conv2d(x, n, 1, scope=\'1x1_conv_contract\')\n    x = ops.batch_norm(x, scope=\'bn_1_post\')\n    x = tf.nn.relu(x)\n    x = ops.conv2d(x, n, 3, stride=stride, scope=\'3x3\')\n    x = ops.batch_norm(x, scope=\'bn_2\')\n    x = tf.nn.relu(x)\n    x = ops.conv2d(x, n * 4, 1, scope=\'1x1_conv_expand\')\n    x = ops.batch_norm(x, scope=\'bn_3\')\n\n    # Apply regularization here\n    # Sample bernoulli with prob\n    if is_training:\n      batch_size = tf.shape(x)[0]\n      bern_shape = [batch_size, 1, 1, 1]\n      random_tensor = prob\n      random_tensor += tf.random_uniform(bern_shape, dtype=tf.float32)\n      binary_tensor = tf.floor(random_tensor)\n\n      alpha_values = tf.random_uniform(\n          [batch_size, 1, 1, 1], minval=alpha[0], maxval=alpha[1],\n          dtype=tf.float32)\n      beta_values = tf.random_uniform(\n          [batch_size, 1, 1, 1], minval=beta[0], maxval=beta[1],\n          dtype=tf.float32)\n      rand_forward = (\n          binary_tensor + alpha_values - binary_tensor * alpha_values)\n      rand_backward = (\n          binary_tensor + beta_values - binary_tensor * beta_values)\n      x = x * rand_backward + tf.stop_gradient(x * rand_forward -\n                                               x * rand_backward)\n    else:\n      expected_alpha = (alpha[1] + alpha[0])/2\n      # prob is the expectation of the bernoulli variable\n      x = (prob + expected_alpha - prob * expected_alpha) * x\n\n    res = shortcut(input_layer, n * 4, stride)\n    return x + res\n\n\ndef build_shake_drop_model(images, num_classes, is_training):\n  """"""Builds the PyramidNet Shake-Drop model.\n\n  Build the PyramidNet Shake-Drop model from https://arxiv.org/abs/1802.02375.\n\n  Args:\n    images: Tensor of images that will be fed into the Wide ResNet Model.\n    num_classes: Number of classed that the model needs to predict.\n    is_training: Is the model training or not.\n\n  Returns:\n    The logits of the PyramidNet Shake-Drop model.\n  """"""\n\n  is_training = is_training\n  # ShakeDrop Hparams\n  p_l = 0.5\n  alpha_shake = [-1, 1]\n  beta_shake = [0, 1]\n\n  # PyramidNet Hparams\n  alpha = 200\n  depth = 272\n  # This is for the bottleneck architecture specifically\n  n = int((depth - 2) / 9)\n  start_channel = 16\n  add_channel = alpha / (3 * n)\n\n  # Building the models\n  x = images\n  x = ops.conv2d(x, 16, 3, scope=\'init_conv\')\n  x = ops.batch_norm(x, scope=\'init_bn\')\n\n  layer_num = 1\n  total_layers = n * 3\n  start_channel += add_channel\n  prob = calc_prob(layer_num, total_layers, p_l)\n  x = bottleneck_layer(\n      x, round_int(start_channel), 1, prob, is_training, alpha_shake,\n      beta_shake)\n  layer_num += 1\n  for _ in range(1, n):\n    start_channel += add_channel\n    prob = calc_prob(layer_num, total_layers, p_l)\n    x = bottleneck_layer(\n        x, round_int(start_channel), 1, prob, is_training, alpha_shake,\n        beta_shake)\n    layer_num += 1\n\n  start_channel += add_channel\n  prob = calc_prob(layer_num, total_layers, p_l)\n  x = bottleneck_layer(\n      x, round_int(start_channel), 2, prob, is_training, alpha_shake,\n      beta_shake)\n  layer_num += 1\n  for _ in range(1, n):\n    start_channel += add_channel\n    prob = calc_prob(layer_num, total_layers, p_l)\n    x = bottleneck_layer(\n        x, round_int(start_channel), 1, prob, is_training, alpha_shake,\n        beta_shake)\n    layer_num += 1\n\n  start_channel += add_channel\n  prob = calc_prob(layer_num, total_layers, p_l)\n  x = bottleneck_layer(\n      x, round_int(start_channel), 2, prob, is_training, alpha_shake,\n      beta_shake)\n  layer_num += 1\n  for _ in range(1, n):\n    start_channel += add_channel\n    prob = calc_prob(layer_num, total_layers, p_l)\n    x = bottleneck_layer(\n        x, round_int(start_channel), 1, prob, is_training, alpha_shake,\n        beta_shake)\n    layer_num += 1\n\n  assert layer_num - 1 == total_layers\n  x = ops.batch_norm(x, scope=\'final_bn\')\n  x = tf.nn.relu(x)\n  x = ops.global_avg_pool(x)\n  # Fully connected\n  logits = ops.fc(x, num_classes)\n  return logits\n'"
image/randaugment/shake_shake.py,22,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Builds the Shake-Shake Model.\nCopied from AutoAugment: https://github.com/tensorflow/models/blob/master/research/autoaugment/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom randaugment import custom_ops as ops\n\n\ndef _shake_shake_skip_connection(x, output_filters, stride):\n  """"""Adds a residual connection to the filter x for the shake-shake model.""""""\n  curr_filters = int(x.shape[3])\n  if curr_filters == output_filters:\n    return x\n  stride_spec = ops.stride_arr(stride, stride)\n  # Skip path 1\n  path1 = tf.nn.avg_pool(\n      x, [1, 1, 1, 1], stride_spec, \'VALID\', data_format=\'NHWC\')\n  path1 = ops.conv2d(path1, int(output_filters / 2), 1, scope=\'path1_conv\')\n\n  # Skip path 2\n  # First pad with 0\'s then crop\n  pad_arr = [[0, 0], [0, 1], [0, 1], [0, 0]]\n  path2 = tf.pad(x, pad_arr)[:, 1:, 1:, :]\n  concat_axis = 3\n\n  path2 = tf.nn.avg_pool(\n      path2, [1, 1, 1, 1], stride_spec, \'VALID\', data_format=\'NHWC\')\n  path2 = ops.conv2d(path2, int(output_filters / 2), 1, scope=\'path2_conv\')\n\n  # Concat and apply BN\n  final_path = tf.concat(values=[path1, path2], axis=concat_axis)\n  final_path = ops.batch_norm(final_path, scope=\'final_path_bn\')\n  return final_path\n\n\ndef _shake_shake_branch(x, output_filters, stride, rand_forward, rand_backward,\n                        is_training):\n  """"""Building a 2 branching convnet.""""""\n  x = tf.nn.relu(x)\n  x = ops.conv2d(x, output_filters, 3, stride=stride, scope=\'conv1\')\n  x = ops.batch_norm(x, scope=\'bn1\')\n  x = tf.nn.relu(x)\n  x = ops.conv2d(x, output_filters, 3, scope=\'conv2\')\n  x = ops.batch_norm(x, scope=\'bn2\')\n  if is_training:\n    x = x * rand_backward + tf.stop_gradient(x * rand_forward -\n                                             x * rand_backward)\n  else:\n    x *= 1.0 / 2\n  return x\n\n\ndef _shake_shake_block(x, output_filters, stride, is_training):\n  """"""Builds a full shake-shake sub layer.""""""\n  batch_size = tf.shape(x)[0]\n\n  # Generate random numbers for scaling the branches\n  rand_forward = [\n      tf.random_uniform(\n          [batch_size, 1, 1, 1], minval=0, maxval=1, dtype=tf.float32)\n      for _ in range(2)\n  ]\n  rand_backward = [\n      tf.random_uniform(\n          [batch_size, 1, 1, 1], minval=0, maxval=1, dtype=tf.float32)\n      for _ in range(2)\n  ]\n  # Normalize so that all sum to 1\n  total_forward = tf.add_n(rand_forward)\n  total_backward = tf.add_n(rand_backward)\n  rand_forward = [samp / total_forward for samp in rand_forward]\n  rand_backward = [samp / total_backward for samp in rand_backward]\n  zipped_rand = zip(rand_forward, rand_backward)\n\n  branches = []\n  for branch, (r_forward, r_backward) in enumerate(zipped_rand):\n    with tf.variable_scope(\'branch_{}\'.format(branch)):\n      b = _shake_shake_branch(x, output_filters, stride, r_forward, r_backward,\n                              is_training)\n      branches.append(b)\n  res = _shake_shake_skip_connection(x, output_filters, stride)\n  return res + tf.add_n(branches)\n\n\ndef _shake_shake_layer(x, output_filters, num_blocks, stride,\n                       is_training):\n  """"""Builds many sub layers into one full layer.""""""\n  for block_num in range(num_blocks):\n    curr_stride = stride if (block_num == 0) else 1\n    with tf.variable_scope(\'layer_{}\'.format(block_num)):\n      x = _shake_shake_block(x, output_filters, curr_stride,\n                             is_training)\n  return x\n\n\ndef build_shake_shake_model(images, num_classes, hparams, is_training):\n  """"""Builds the Shake-Shake model.\n\n  Build the Shake-Shake model from https://arxiv.org/abs/1705.07485.\n\n  Args:\n    images: Tensor of images that will be fed into the Wide ResNet Model.\n    num_classes: Number of classed that the model needs to predict.\n    hparams: tf.HParams object that contains additional hparams needed to\n      construct the model. In this case it is the `shake_shake_widen_factor`\n      that is used to determine how many filters the model has.\n    is_training: Is the model training or not.\n\n  Returns:\n    The logits of the Shake-Shake model.\n  """"""\n  depth = 26\n  k = hparams.shake_shake_widen_factor  # The widen factor\n  n = int((depth - 2) / 6)\n  x = images\n\n  x = ops.conv2d(x, 16, 3, scope=\'init_conv\')\n  x = ops.batch_norm(x, scope=\'init_bn\')\n  with tf.variable_scope(\'L1\'):\n    x = _shake_shake_layer(x, 16 * k, n, 1, is_training)\n  with tf.variable_scope(\'L2\'):\n    x = _shake_shake_layer(x, 32 * k, n, 2, is_training)\n  with tf.variable_scope(\'L3\'):\n    x = _shake_shake_layer(x, 64 * k, n, 2, is_training)\n  x = tf.nn.relu(x)\n  x = ops.global_avg_pool(x)\n\n  # Fully connected\n  logits = ops.fc(x, num_classes)\n  return logits\n'"
image/randaugment/wrn.py,10,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Builds the WideResNet Model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom randaugment import custom_ops as ops\n\nimport numpy as np\nimport tensorflow as tf\n\n\n\ndef residual_block(\n    x, in_filter, out_filter, stride, update_bn=True):\n  """"""Adds residual connection to `x` in addition to applying BN->ReLU->3x3 Conv.\n\n  Args:\n    x: Tensor that is the output of the previous layer in the model.\n    in_filter: Number of filters `x` has.\n    out_filter: Number of filters that the output of this layer will have.\n    stride: Integer that specified what stride should be applied to `x`.\n\n  Returns:\n    A Tensor that is the result of applying two sequences of BN->ReLU->3x3 Conv\n    and then adding that Tensor to `x`.\n  """"""\n\n  orig_x = x\n  block_x = x\n  with tf.variable_scope(\'residual_only_activation\'):\n    block_x = ops.batch_norm(block_x, update_stats=update_bn,\n                              scope=\'init_bn\')\n    block_x = tf.nn.relu(block_x)\n\n  with tf.variable_scope(\'sub1\'):\n    block_x = ops.conv2d(\n        block_x, out_filter, 3, stride=stride, scope=\'conv1\')\n\n  with tf.variable_scope(\'sub2\'):\n    block_x = ops.batch_norm(block_x, update_stats=update_bn, scope=\'bn2\')\n    block_x = tf.nn.relu(block_x)\n    block_x = ops.conv2d(\n        block_x, out_filter, 3, stride=1, scope=\'conv2\')\n\n  if stride != 1 or out_filter != in_filter:\n    orig_x = ops.conv2d(\n        orig_x, out_filter, 1, stride=stride, scope=\'conv3\')\n  x = orig_x + block_x\n  return x\n\n\ndef build_wrn_model(images, num_classes, wrn_size, update_bn=True):\n  """"""Builds the WRN model.\n\n  Build the Wide ResNet model from https://arxiv.org/abs/1605.07146.\n\n  Args:\n    images: Tensor of images that will be fed into the Wide ResNet Model.\n    num_classes: Number of classed that the model needs to predict.\n    wrn_size: Parameter that scales the number of filters in the Wide ResNet\n      model.\n\n  Returns:\n    The logits of the Wide ResNet model.\n  """"""\n  # wrn_size = 16 * widening factor k\n  kernel_size = wrn_size\n  filter_size = 3\n  # depth = num_blocks_per_resnet * 6 + 4 = 28\n  num_blocks_per_resnet = 4\n  filters = [\n      min(kernel_size, 16), kernel_size, kernel_size * 2, kernel_size * 4\n  ]\n  strides = [1, 2, 2]  # stride for each resblock\n\n  # Run the first conv\n  with tf.variable_scope(\'init\'):\n    x = images\n    output_filters = filters[0]\n    x = ops.conv2d(x, output_filters, filter_size, scope=\'init_conv\')\n\n  first_x = x  # Res from the beginning\n  orig_x = x  # Res from previous block\n\n  for block_num in range(1, 4):\n    with tf.variable_scope(\'unit_{}_0\'.format(block_num)):\n      x = residual_block(\n          x,\n          filters[block_num - 1],\n          filters[block_num],\n          strides[block_num - 1],\n          update_bn=update_bn)\n    for i in range(1, num_blocks_per_resnet):\n      with tf.variable_scope(\'unit_{}_{}\'.format(block_num, i)):\n        x = residual_block(\n            x,\n            filters[block_num],\n            filters[block_num],\n            1,\n            update_bn=update_bn)\n  with tf.variable_scope(\'unit_last\'):\n    x = ops.batch_norm(x, scope=\'final_bn\')\n    x = tf.nn.relu(x)\n    x = ops.global_avg_pool(x)\n    logits = ops.fc(x, num_classes)\n  return logits\n'"
text/augmentation/__init__.py,0,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
text/augmentation/sent_level_augment.py,9,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Sentence level augmentations: back translation.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nimport random\nfrom absl import flags\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom augmentation import word_level_augment\nfrom utils import raw_data_utils\n\n\nFLAGS = flags.FLAGS\n\n\ndef replace_with_length_check(\n    ori_text, new_text,\n    use_min_length,\n    use_max_length_diff_ratio):\n  """"""Use new_text if the text length satisfies several constraints.""""""\n  if len(ori_text) < use_min_length or len(new_text) < use_min_length:\n    if random.random() < 0.001:\n      tf.logging.info(\n          ""not replacing due to short text: \\n\\tori: {:s}\\n\\tnew: {:s}\\n"".format(\n              word_level_augment.filter_unicode(ori_text),\n              word_level_augment.filter_unicode(new_text)))\n    return ori_text\n  length_diff_ratio = 1.0 * (len(new_text) - len(ori_text)) / len(ori_text)\n  if math.fabs(length_diff_ratio) > use_max_length_diff_ratio:\n    if random.random() < 0.001:\n      tf.logging.info(\n          (""not replacing due to too different text length:\\n""\n           ""\\tori: {:s}\\n\\tnew: {:s}\\n"".format(\n               word_level_augment.filter_unicode(ori_text),\n               word_level_augment.filter_unicode(new_text))))\n    return ori_text\n  return new_text\n\n\ndef back_translation(examples, aug_ops, sub_set, aug_copy_num,\n                     start, end, data_total_size):\n  """"""Run back translation.""""""\n  use_min_length = 10\n  use_max_length_diff_ratio = 0.5\n  tf.logging.info(""running bt augmentation"")\n  bt_args = aug_ops.split(""-"")\n  temp = float(bt_args[1])\n\n  if len(bt_args) > 2:\n    assert len(bt_args) == 3\n    assert float(bt_args[2]) == 1.\n\n  if examples[0].text_b is not None:\n    text_per_example = 2\n  else:\n    text_per_example = 1\n\n  back_translation_file = ""{:s}/{:s}/sample_{:.1f}/para/para_{:d}.txt"".format(\n      FLAGS.back_translation_dir, sub_set,\n      temp, aug_copy_num)\n  tf.logging.info(""Using back translation file: {:s}"".format(\n      back_translation_file))\n\n  with tf.gfile.Open(back_translation_file) as inf:\n    paraphrases = inf.readlines()\n  for i in range(len(paraphrases)):\n    paraphrases[i] = paraphrases[i].strip()\n  assert len(paraphrases) == data_total_size\n\n  paraphrases = paraphrases[start * text_per_example : end * text_per_example]\n  aug_examples = []\n  aug_cnt = 0\n  for i in range(len(examples)):\n    ori_example = examples[i]\n    text_a = replace_with_length_check(\n        ori_example.text_a,\n        paraphrases[i * text_per_example],\n        use_min_length,\n        use_max_length_diff_ratio,\n        )\n    if text_a == paraphrases[i * text_per_example]:\n      aug_cnt += 1\n    if ori_example.text_b is not None:\n      text_b = replace_with_length_check(\n          ori_example.text_b,\n          paraphrases[i * text_per_example + 1],\n          use_min_length,\n          use_max_length_diff_ratio,\n          )\n    else:\n      text_b = None\n\n    example = raw_data_utils.InputExample(\n        guid=ori_example.guid,\n        text_a=text_a,\n        text_b=text_b,\n        label=ori_example.label)\n    aug_examples += [example]\n    if np.random.random() < 0.0001:\n      tf.logging.info(""\\tori:\\n\\t\\t{:s}\\n\\t\\t{:s}\\n\\t\\t{:s}\\n"".format(\n          ori_example.text_a, ori_example.text_b, ori_example.label))\n      tf.logging.info(""\\tnew:\\n\\t\\t{:s}\\n\\t\\t{:s}\\n\\t\\t{:s}\\n"".format(\n          example.text_a, example.text_b, example.label))\n    if i % 10000 == 0:\n      print(""processing example # {:d}"".format(i))\n  tf.logging.info(""applied back translation for {:.1f} percent of data"".format(\n      aug_cnt * 1. / len(examples) * 100))\n  tf.logging.info(""finishing running back translation augmentation"")\n  return aug_examples\n\n\ndef run_augment(\n    examples, aug_ops, sub_set, aug_copy_num,\n    start, end, dst_tot_size):\n  """"""Sentence level augmentations. Used before augmentation.""""""\n  if aug_ops:\n    if aug_ops.startswith(""bt""):\n      examples = back_translation(\n          examples, aug_ops, sub_set, aug_copy_num, start, end, dst_tot_size)\n    else:\n      pass\n  return examples\n'"
text/augmentation/word_level_augment.py,7,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Word level augmentations including Replace words with uniform random words or TF-IDF based word replacement.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport json\nimport math\nimport string\nfrom absl import flags\nimport numpy as np\nimport tensorflow as tf\n\n\nFLAGS = flags.FLAGS\n\n\nprintable = set(string.printable)\n\n\ndef filter_unicode(st):\n  return """".join([c for c in st if c in printable])\n\n\nclass EfficientRandomGen(object):\n  """"""A base class that generate multiple random numbers at the same time.""""""\n\n  def reset_random_prob(self):\n    """"""Generate many random numbers at the same time and cache them.""""""\n    cache_len = 100000\n    self.random_prob_cache = np.random.random(size=(cache_len,))\n    self.random_prob_ptr = cache_len - 1\n\n  def get_random_prob(self):\n    """"""Get a random number.""""""\n    value = self.random_prob_cache[self.random_prob_ptr]\n    self.random_prob_ptr -= 1\n    if self.random_prob_ptr == -1:\n      self.reset_random_prob()\n    return value\n\n  def get_random_token(self):\n    """"""Get a random token.""""""\n    token = self.token_list[self.token_ptr]\n    self.token_ptr -= 1\n    if self.token_ptr == -1:\n      self.reset_token_list()\n    return token\n\n\nclass UnifRep(EfficientRandomGen):\n  """"""Uniformly replace word with random words in the vocab.""""""\n\n  def __init__(self, token_prob, vocab):\n    self.token_prob = token_prob\n    self.vocab_size = len(vocab)\n    self.vocab = vocab\n    self.reset_token_list()\n    self.reset_random_prob()\n\n  def __call__(self, example):\n    example.word_list_a = self.replace_tokens(example.word_list_a)\n    if example.text_b:\n      example.word_list_b = self.replace_tokens(example.word_list_b)\n    return example\n\n  def replace_tokens(self, tokens):\n    """"""Replace tokens randomly.""""""\n    if len(tokens) >= 3:\n      if np.random.random() < 0.001:\n        show_example = True\n      else:\n        show_example = False\n      if show_example:\n        tf.logging.info(""before augment: {:s}"".format(\n            filter_unicode("" "".join(tokens))))\n      for i in range(len(tokens)):\n        if self.get_random_prob() < self.token_prob:\n          tokens[i] = self.get_random_token()\n      if show_example:\n        tf.logging.info(""after augment: {:s}"".format(\n            filter_unicode("" "".join(tokens))))\n    return tokens\n\n  def reset_token_list(self):\n    """"""Generate many random tokens at the same time and cache them.""""""\n    self.token_list = self.vocab.keys()\n    self.token_ptr = len(self.token_list) - 1\n    np.random.shuffle(self.token_list)\n\n\ndef get_data_stats(examples):\n  """"""Compute the IDF score for each word. Then compute the TF-IDF score.""""""\n  word_doc_freq = collections.defaultdict(int)\n  # Compute IDF\n  for i in range(len(examples)):\n    cur_word_dict = {}\n    cur_sent = copy.deepcopy(examples[i].word_list_a)\n    if examples[i].text_b:\n      cur_sent += examples[i].word_list_b\n    for word in cur_sent:\n      cur_word_dict[word] = 1\n    for word in cur_word_dict:\n      word_doc_freq[word] += 1\n  idf = {}\n  for word in word_doc_freq:\n    idf[word] = math.log(len(examples) * 1. / word_doc_freq[word])\n  # Compute TF-IDF\n  tf_idf = {}\n  for i in range(len(examples)):\n    cur_word_dict = {}\n    cur_sent = copy.deepcopy(examples[i].word_list_a)\n    if examples[i].text_b:\n      cur_sent += examples[i].word_list_b\n    for word in cur_sent:\n      if word not in tf_idf:\n        tf_idf[word] = 0\n      tf_idf[word] += 1. / len(cur_sent) * idf[word]\n  return {\n      ""idf"": idf,\n      ""tf_idf"": tf_idf,\n  }\n\n\nclass TfIdfWordRep(EfficientRandomGen):\n  """"""TF-IDF Based Word Replacement.""""""\n\n  def __init__(self, token_prob, data_stats):\n    super(TfIdfWordRep, self).__init__()\n    self.token_prob = token_prob\n    self.data_stats = data_stats\n    self.idf = data_stats[""idf""]\n    self.tf_idf = data_stats[""tf_idf""]\n    data_stats = copy.deepcopy(data_stats)\n    tf_idf_items = data_stats[""tf_idf""].items()\n    tf_idf_items = sorted(tf_idf_items, key=lambda item: -item[1])\n    self.tf_idf_keys = []\n    self.tf_idf_values = []\n    for key, value in tf_idf_items:\n      self.tf_idf_keys += [key]\n      self.tf_idf_values += [value]\n    self.normalized_tf_idf = np.array(self.tf_idf_values)\n    self.normalized_tf_idf = (self.normalized_tf_idf.max()\n                              - self.normalized_tf_idf)\n    self.normalized_tf_idf = (self.normalized_tf_idf\n                              / self.normalized_tf_idf.sum())\n    self.reset_token_list()\n    self.reset_random_prob()\n\n  def get_replace_prob(self, all_words):\n    """"""Compute the probability of replacing tokens in a sentence.""""""\n    cur_tf_idf = collections.defaultdict(int)\n    for word in all_words:\n      cur_tf_idf[word] += 1. / len(all_words) * self.idf[word]\n    replace_prob = []\n    for word in all_words:\n      replace_prob += [cur_tf_idf[word]]\n    replace_prob = np.array(replace_prob)\n    replace_prob = np.max(replace_prob) - replace_prob\n    replace_prob = (replace_prob / replace_prob.sum() *\n                    self.token_prob * len(all_words))\n    return replace_prob\n\n  def __call__(self, example):\n    if self.get_random_prob() < 0.001:\n      show_example = True\n    else:\n      show_example = False\n    all_words = copy.deepcopy(example.word_list_a)\n    if example.text_b:\n      all_words += example.word_list_b\n\n    if show_example:\n      tf.logging.info(""before tf_idf_unif aug: {:s}"".format(\n          filter_unicode("" "".join(all_words))))\n\n    replace_prob = self.get_replace_prob(all_words)\n    example.word_list_a = self.replace_tokens(\n        example.word_list_a,\n        replace_prob[:len(example.word_list_a)]\n        )\n    if example.text_b:\n      example.word_list_b = self.replace_tokens(\n          example.word_list_b,\n          replace_prob[len(example.word_list_a):]\n          )\n\n    if show_example:\n      all_words = copy.deepcopy(example.word_list_a)\n      if example.text_b:\n        all_words += example.word_list_b\n      tf.logging.info(""after tf_idf_unif aug: {:s}"".format(\n          filter_unicode("" "".join(all_words))))\n    return example\n\n  def replace_tokens(self, word_list, replace_prob):\n    """"""Replace tokens in a sentence.""""""\n    for i in range(len(word_list)):\n      if self.get_random_prob() < replace_prob[i]:\n        word_list[i] = self.get_random_token()\n    return word_list\n\n  def reset_token_list(self):\n    cache_len = len(self.tf_idf_keys)\n    token_list_idx = np.random.choice(\n        cache_len, (cache_len,), p=self.normalized_tf_idf)\n    self.token_list = []\n    for idx in token_list_idx:\n      self.token_list += [self.tf_idf_keys[idx]]\n    self.token_ptr = len(self.token_list) - 1\n    tf.logging.info(""sampled token list: {:s}"".format(\n        filter_unicode("" "".join(self.token_list))))\n\n\ndef word_level_augment(\n    examples, aug_ops, vocab, data_stats):\n  """"""Word level augmentations. Used before augmentation.""""""\n  if aug_ops:\n    if aug_ops.startswith(""unif""):\n      tf.logging.info(""\\n>>Using augmentation {}"".format(aug_ops))\n      token_prob = float(aug_ops.split(""-"")[1])\n      op = UnifRep(token_prob, vocab)\n      for i in range(len(examples)):\n        examples[i] = op(examples[i])\n    elif aug_ops.startswith(""tf_idf""):\n      tf.logging.info(""\\n>>Using augmentation {}"".format(aug_ops))\n      token_prob = float(aug_ops.split(""-"")[1])\n      op = TfIdfWordRep(token_prob, data_stats)\n      for i in range(len(examples)):\n        examples[i] = op(examples[i])\n  return examples\n'"
text/bert/__init__.py,0,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
text/bert/modeling.py,73,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""The BERT model.\n\nPart of the code is from https://github.com/google-research/bert\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport json\nimport math\nimport six\nimport tensorflow as tf\n\n\nclass BertConfig(object):\n  """"""Configuration for `BertModel`.""""""\n\n  def __init__(self,\n               vocab_size,\n               hidden_size=768,\n               num_hidden_layers=12,\n               num_attention_heads=12,\n               intermediate_size=3072,\n               hidden_act=""gelu"",\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings=512,\n               type_vocab_size=16,\n               initializer_range=0.02):\n    """"""Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probabilitiy for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The sttdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    """"""\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings = max_position_embeddings\n    self.type_vocab_size = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file, model_dropout):\n    """"""Constructs a `BertConfig` from a json file of parameters.""""""\n    with tf.gfile.GFile(json_file, ""r"") as reader:\n      text = reader.read()\n    config = cls.from_dict(json.loads(text))\n    if model_dropout != -1:\n      config.hidden_dropout_prob = model_dropout\n      config.attention_probs_dropout_prob = model_dropout\n    return config\n\n  def to_dict(self):\n    """"""Serializes this instance to a Python dictionary.""""""\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    """"""Serializes this instance to a JSON string.""""""\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\ndef bert_embedding(config,\n                   is_training,\n                   input_ids,\n                   input_mask,\n                   token_type_ids=None,\n                   use_one_hot_embeddings=True,\n                   scope=None):\n\n  config = copy.deepcopy(config)\n  if not is_training:\n    config.hidden_dropout_prob = 0.0\n    config.attention_probs_dropout_prob = 0.0\n\n  input_shape = get_shape_list(input_ids, expected_rank=2)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n\n  if input_mask is None:\n    input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n  if token_type_ids is None:\n    token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n  with tf.variable_scope(""bert"", scope, reuse=tf.AUTO_REUSE):\n    with tf.variable_scope(""embeddings""):\n      # Perform embedding lookup on the word ids.\n      (embedding_output, embedding_table) = embedding_lookup(\n          input_ids=input_ids,\n          vocab_size=config.vocab_size,\n          embedding_size=config.hidden_size,\n          initializer_range=config.initializer_range,\n          word_embedding_name=""word_embeddings"",\n          use_one_hot_embeddings=use_one_hot_embeddings)\n\n      # Add positional embeddings and token type embeddings, then layer\n      # normalize and perform dropout.\n      embedding_output = embedding_postprocessor(\n          input_tensor=embedding_output,\n          use_token_type=True,\n          token_type_ids=token_type_ids,\n          token_type_vocab_size=config.type_vocab_size,\n          token_type_embedding_name=""token_type_embeddings"",\n          use_position_embeddings=True,\n          position_embedding_name=""position_embeddings"",\n          initializer_range=config.initializer_range,\n          max_position_embeddings=config.max_position_embeddings,\n          dropout_prob=config.hidden_dropout_prob)\n\n    return embedding_output, embedding_table\n\n\ndef bert_attention(config,\n                   is_training,\n                   input_ids,\n                   input_mask,\n                   embedding_output,\n                   scope=None):\n\n  config = copy.deepcopy(config)\n  if not is_training:\n    config.hidden_dropout_prob = 0.0\n    config.attention_probs_dropout_prob = 0.0\n\n  with tf.variable_scope(""bert"", scope, reuse=tf.AUTO_REUSE):\n    with tf.variable_scope(""encoder""):\n      # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n      # mask of shape [batch_size, seq_length, seq_length] which is used\n      # for the attention scores.\n      attention_mask = create_attention_mask_from_input_mask(\n          input_ids, input_mask)\n\n      # Run the stacked transformer.\n      # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n      all_encoder_layers = transformer_model(\n          input_tensor=embedding_output,\n          attention_mask=attention_mask,\n          hidden_size=config.hidden_size,\n          num_hidden_layers=config.num_hidden_layers,\n          num_attention_heads=config.num_attention_heads,\n          intermediate_size=config.intermediate_size,\n          intermediate_act_fn=get_activation(config.hidden_act),\n          hidden_dropout_prob=config.hidden_dropout_prob,\n          attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n          initializer_range=config.initializer_range,\n          do_return_all_layers=True)\n\n    sequence_output = all_encoder_layers[-1]\n\n    return sequence_output\n\n\ndef bert_pooler(config,\n                is_training,\n                sequence_output,\n                scope=None):\n\n  config = copy.deepcopy(config)\n  if not is_training:\n    config.hidden_dropout_prob = 0.0\n    config.attention_probs_dropout_prob = 0.0\n\n  with tf.variable_scope(""bert"", scope, reuse=tf.AUTO_REUSE):\n    # The ""pooler"" converts the encoded sequence tensor of shape\n    # [batch_size, seq_length, hidden_size] to a tensor of shape\n    # [batch_size, hidden_size]. This is necessary for segment-level\n    # (or segment-pair-level) classification tasks where we need a fixed\n    # dimensional representation of the segment.\n    with tf.variable_scope(""pooler""):\n      # We ""pool"" the model by simply taking the hidden state corresponding\n      # to the first token. We assume that this has been pre-trained\n      clas_rep = tf.squeeze(sequence_output[:, 0:1, :], axis=1)\n\n      pooled_output = tf.layers.dense(\n          clas_rep,\n          config.hidden_size,\n          activation=tf.tanh,\n          kernel_initializer=create_initializer(config.initializer_range))\n\n    return pooled_output\n\n\ndef bert_model(config,\n               is_training,\n               input_ids,\n               input_mask,\n               token_type_ids=None,\n               input_embedding=None,\n               output_type=""pooled"",\n               use_one_hot_embeddings=True,\n               scope=None):\n  """"""doc.""""""\n\n  assert output_type in [""embedding"", ""pooled"", ""sequence""], (\n      ""Unsupported output type {}"".format(output_type))\n\n  if input_embedding is None:\n    embedding_output, embedding_table = bert_embedding(\n        config,\n        is_training,\n        input_ids,\n        input_mask,\n        token_type_ids,\n        use_one_hot_embeddings,\n        scope)\n\n    if output_type == ""embedding"":\n      return embedding_output, embedding_table\n\n  sequence_output = bert_attention(\n      config,\n      is_training,\n      input_ids,\n      input_mask,\n      embedding_output,\n      scope)\n\n  if output_type == ""sequence"":\n    return sequence_output\n\n  pooled_output = bert_pooler(\n      config,\n      is_training,\n      sequence_output,\n      scope)\n\n  if output_type == ""pooled"":\n    return pooled_output\n\n\ndef gelu(input_tensor):\n  """"""Gaussian Error Linear Unit.\n\n  This is a smoother version of the RELU.\n  Original paper: https://arxiv.org/abs/1606.08415\n\n  Args:\n    input_tensor: float Tensor to perform activation.\n\n  Returns:\n    `input_tensor` with the GELU activation applied.\n  """"""\n  cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))\n  return input_tensor * cdf\n\n\ndef get_activation(activation_string):\n  """"""Maps a string to a Python function, e.g., ""relu"" => `tf.nn.relu`.\n\n  Args:\n    activation_string: String name of the activation function.\n\n  Returns:\n    A Python function corresponding to the activation function. If\n    `activation_string` is None, empty, or ""linear"",\n    this will return None.\n    If `activation_string` is not a string, it will return `activation_string`.\n\n  Raises:\n    ValueError: The `activation_string` does not correspond to a known\n      activation.\n  """"""\n\n  # We assume that anything that\'s not a string is already an activation\n  # function, so we just return it.\n  if not isinstance(activation_string, (str, unicode)):\n    return activation_string\n\n  if not activation_string:\n    return None\n\n  act = activation_string.lower()\n  if act == ""linear"":\n    return None\n  elif act == ""relu"":\n    return tf.nn.relu\n  elif act == ""gelu"":\n    return gelu\n  elif act == ""tanh"":\n    return tf.tanh\n  else:\n    raise ValueError(""Unsupported activation: %s"" % act)\n\n\ndef dropout(input_tensor, dropout_prob):\n  """"""Perform dropout.\n\n  Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probabiltiy of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n\n  Returns:\n    A version of `input_tensor` with dropout applied.\n  """"""\n  if dropout_prob is None or dropout_prob == 0.0:\n    return input_tensor\n\n  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n  return output\n\n\ndef layer_norm(input_tensor, name=None):\n  """"""Run layer normalization on the last dimension of the tensor.""""""\n  return tf.contrib.layers.layer_norm(\n      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n\n\ndef layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n  """"""Runs layer normalization followed by dropout.""""""\n  output_tensor = layer_norm(input_tensor, name)\n  output_tensor = dropout(output_tensor, dropout_prob)\n  return output_tensor\n\n\ndef create_initializer(initializer_range=0.02):\n  """"""Creates a `truncated_normal_initializer` with the given range.""""""\n  return tf.truncated_normal_initializer(stddev=initializer_range)\n\n\ndef embedding_lookup(input_ids,\n                     vocab_size,\n                     embedding_size=128,\n                     initializer_range=0.02,\n                     word_embedding_name=""word_embeddings"",\n                     use_one_hot_embeddings=False):\n  """"""Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.nn.embedding_lookup()`. One hot is better\n      for TPUs.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  """"""\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  if use_one_hot_embeddings:\n    flat_input_ids = tf.reshape(input_ids, [-1])\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.nn.embedding_lookup(embedding_table, input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n\ndef embedding_postprocessor(input_tensor,\n                            use_token_type=False,\n                            token_type_ids=None,\n                            token_type_vocab_size=16,\n                            token_type_embedding_name=""token_type_embeddings"",\n                            use_position_embeddings=True,\n                            position_embedding_name=""position_embeddings"",\n                            initializer_range=0.02,\n                            max_position_embeddings=512,\n                            dropout_prob=0.1):\n  """"""Performs various post-processing on a word embedding tensor.\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length,\n      embedding_size].\n    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      Must be specified if `use_token_type` is True.\n    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n    token_type_embedding_name: string. The name of the embedding table variable\n      for token type ids.\n    use_position_embeddings: bool. Whether to add position embeddings for the\n      position of each token in the sequence.\n    position_embedding_name: string. The name of the embedding table variable\n      for positional embeddings.\n    initializer_range: float. Range of the weight initialization.\n    max_position_embeddings: int. Maximum sequence length that might ever be\n      used with this model. This can be longer than the sequence length of\n      input_tensor, but cannot be shorter.\n    dropout_prob: float. Dropout probability applied to the final output tensor.\n\n  Returns:\n    float tensor with same shape as `input_tensor`.\n\n  Raises:\n    ValueError: One of the tensor shapes or input values is invalid.\n  """"""\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  width = input_shape[2]\n\n  if seq_length > max_position_embeddings:\n    raise ValueError(""The seq length (%d) cannot be greater than ""\n                     ""`max_position_embeddings` (%d)"" %\n                     (seq_length, max_position_embeddings))\n\n  output = input_tensor\n\n  if use_token_type:\n    if token_type_ids is None:\n      raise ValueError(""`token_type_ids` must be specified if""\n                       ""`use_token_type` is True."")\n    token_type_table = tf.get_variable(\n        name=token_type_embedding_name,\n        shape=[token_type_vocab_size, width],\n        initializer=create_initializer(initializer_range))\n    # This vocab will be small so we always do one-hot here, since it is always\n    # faster for a small vocabulary.\n    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n    token_type_embeddings = tf.reshape(token_type_embeddings,\n                                       [batch_size, seq_length, width])\n    output += token_type_embeddings\n\n  if use_position_embeddings:\n    full_position_embeddings = tf.get_variable(\n        name=position_embedding_name,\n        shape=[max_position_embeddings, width],\n        initializer=create_initializer(initializer_range))\n    # Since the position embedding table is a learned variable, we create it\n    # using a (long) sequence length `max_position_embeddings`. The actual\n    # sequence length might be shorter than this, for faster training of\n    # tasks that do not have long sequences.\n    #\n    # So `full_position_embeddings` is effectively an embedding table\n    # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n    # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n    # perform a slice.\n    position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                   [seq_length, -1])\n    num_dims = len(output.shape.as_list())\n\n    # Only the last two dimensions are relevant (`seq_length` and `width`), so\n    # we broadcast among the first dimensions, which is typically just\n    # the batch size.\n    position_broadcast_shape = []\n    for _ in range(num_dims - 2):\n      position_broadcast_shape.append(1)\n    position_broadcast_shape.extend([seq_length, width])\n    position_embeddings = tf.reshape(position_embeddings,\n                                     position_broadcast_shape)\n    output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n\ndef create_attention_mask_from_input_mask(from_tensor, to_mask):\n  """"""Create 3D attention mask from a 2D tensor mask.\n\n  Args:\n    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n  """"""\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  batch_size = from_shape[0]\n  from_seq_length = from_shape[1]\n\n  to_shape = get_shape_list(to_mask, expected_rank=2)\n  to_seq_length = to_shape[1]\n\n  to_mask = tf.cast(\n      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n  # We don\'t assume that `from_tensor` is a mask (although it could be). We\n  # don\'t actually care if we attend *from* padding tokens (only *to* padding)\n  # tokens so we create a tensor of all ones.\n  #\n  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n  broadcast_ones = tf.ones(\n      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n  # Here we broadcast along two dimensions to create the mask.\n  mask = broadcast_ones * to_mask\n\n  return mask\n\n\ndef attention_layer(from_tensor,\n                    to_tensor,\n                    attention_mask=None,\n                    num_attention_heads=1,\n                    size_per_head=512,\n                    query_act=None,\n                    key_act=None,\n                    value_act=None,\n                    attention_probs_dropout_prob=0.0,\n                    initializer_range=0.02,\n                    do_return_2d_tensor=False,\n                    batch_size=None,\n                    from_seq_length=None,\n                    to_seq_length=None):\n  """"""Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n  This is an implementation of multi-headed attention based on ""Attention\n  is all you Need"". If `from_tensor` and `to_tensor` are the same, then\n  this is self-attention. Each timestep in `from_tensor` attends to the\n  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n  This function first projects `from_tensor` into a ""query"" tensor and\n  `to_tensor` into ""key"" and ""value"" tensors. These are (effectively) a list\n  of tensors of length `num_attention_heads`, where each tensor is of shape\n  [batch_size, seq_length, size_per_head].\n\n  Then, the query and key tensors are dot-producted and scaled. These are\n  softmaxed to obtain attention probabilities. The value tensors are then\n  interpolated by these probabilities, then concatenated back to a single\n  tensor and returned.\n\n  In practice, the multi-headed attention are done with transposes and\n  reshapes rather than actual separate tensors.\n\n  Args:\n    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n      from_width].\n    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n    attention_mask: (optional) int32 Tensor of shape [batch_size,\n      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n      attention scores will effectively be set to -infinity for any positions in\n      the mask that are 0, and will be unchaged for positions that are 1.\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    query_act: (optional) Activation function for the query transform.\n    key_act: (optional) Activation function for the key transform.\n    value_act: (optional) Activation function for the value transform.\n    attention_probs_dropout_prob:\n    initializer_range: float. Range of the weight initializer.\n    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n      output will be of shape [batch_size, from_seq_length, num_attention_heads\n      * size_per_head].\n    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n      of the 3D version of the `from_tensor` and `to_tensor`.\n    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `from_tensor`.\n    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `to_tensor`.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length,\n      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n      true, this will be of shape [batch_size * from_seq_length,\n      num_attention_heads * size_per_head]).\n\n  Raises:\n    ValueError: Any of the arguments or tensor shapes are invalid.\n  """"""\n\n  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width):\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n\n  if len(from_shape) != len(to_shape):\n    raise ValueError(\n        ""The rank of `from_tensor` must match the rank of `to_tensor`."")\n\n  if len(from_shape) == 3:\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_seq_length = to_shape[1]\n  elif len(from_shape) == 2:\n    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n      raise ValueError(\n          ""When passing in rank 2 tensors to attention_layer, the values ""\n          ""for `batch_size`, `from_seq_length`, and `to_seq_length` ""\n          ""must all be specified."")\n\n  # Scalar dimensions referenced here:\n  #   B = batch size (number of sequences)\n  #   F = `from_tensor` sequence length\n  #   T = `to_tensor` sequence length\n  #   N = `num_attention_heads`\n  #   H = `size_per_head`\n\n  from_tensor_2d = reshape_to_matrix(from_tensor)\n  to_tensor_2d = reshape_to_matrix(to_tensor)\n\n  # `query_layer` = [B*F, N*H]\n  query_layer = tf.layers.dense(\n      from_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=query_act,\n      name=""query"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `key_layer` = [B*T, N*H]\n  key_layer = tf.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=key_act,\n      name=""key"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `value_layer` = [B*T, N*H]\n  value_layer = tf.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=value_act,\n      name=""value"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `query_layer` = [B, N, F, H]\n  query_layer = transpose_for_scores(query_layer, batch_size,\n                                     num_attention_heads, from_seq_length,\n                                     size_per_head)\n\n  # `key_layer` = [B, N, T, H]\n  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n                                   to_seq_length, size_per_head)\n\n  # Take the dot product between ""query"" and ""key"" to get the raw\n  # attention scores.\n  # `attention_scores` = [B, N, F, T]\n  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n  attention_scores = tf.multiply(attention_scores,\n                                 1.0 / math.sqrt(float(size_per_head)))\n\n  if attention_mask is not None:\n    # `attention_mask` = [B, 1, F, T]\n    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n\n    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n    # masked positions, this operation will create a tensor which is 0.0 for\n    # positions we want to attend and -10000.0 for masked positions.\n    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n\n    # Since we are adding it to the raw scores before the softmax, this is\n    # effectively the same as removing these entirely.\n    attention_scores += adder\n\n  # Normalize the attention scores to probabilities.\n  # `attention_probs` = [B, N, F, T]\n  attention_probs = tf.nn.softmax(attention_scores)\n\n  # This is actually dropping out entire tokens to attend to, which might\n  # seem a bit unusual, but is taken from the original Transformer paper.\n  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n\n  # `value_layer` = [B, T, N, H]\n  value_layer = tf.reshape(\n      value_layer,\n      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n\n  # `value_layer` = [B, N, T, H]\n  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n\n  # `context_layer` = [B, N, F, H]\n  context_layer = tf.matmul(attention_probs, value_layer)\n\n  # `context_layer` = [B, F, N, H]\n  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n\n  if do_return_2d_tensor:\n    # `context_layer` = [B*F, N*V]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n  else:\n    # `context_layer` = [B, F, N*V]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n\n  return context_layer\n\n\ndef transformer_model(input_tensor,\n                      attention_mask=None,\n                      hidden_size=768,\n                      num_hidden_layers=12,\n                      num_attention_heads=12,\n                      intermediate_size=3072,\n                      intermediate_act_fn=gelu,\n                      hidden_dropout_prob=0.1,\n                      attention_probs_dropout_prob=0.1,\n                      initializer_range=0.02,\n                      do_return_all_layers=False):\n  """"""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".\n\n  This is almost an exact implementation of the original Transformer encoder.\n\n  See the original paper:\n  https://arxiv.org/abs/1706.03762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the ""intermediate"" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  """"""\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        ""The hidden size (%d) is not a multiple of the number of attention ""\n        ""heads (%d)"" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(""The width of the input tensor (%d) != hidden size (%d)"" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(""layer_%d"" % layer_idx):\n      layer_input = prev_output\n\n      with tf.variable_scope(""attention""):\n        attention_heads = []\n        with tf.variable_scope(""self""):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(""output""):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the ""intermediate"" hidden layer.\n      with tf.variable_scope(""intermediate""):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(""output""):\n        layer_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n\ndef get_shape_list(tensor, expected_rank=None, name=None):\n  """"""Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  if expected_rank is not None:\n    assert_rank(tensor, expected_rank, name)\n\n  shape = tensor.shape.as_list()\n\n  non_static_indexes = []\n  for (index, dim) in enumerate(shape):\n    if dim is None:\n      non_static_indexes.append(index)\n\n  if not non_static_indexes:\n    return shape\n\n  dyn_shape = tf.shape(tensor)\n  for index in non_static_indexes:\n    shape[index] = dyn_shape[index]\n  return shape\n\n\ndef reshape_to_matrix(input_tensor):\n  """"""Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).""""""\n  ndims = input_tensor.shape.ndims\n  if ndims < 2:\n    raise ValueError(""Input tensor must have at least rank 2. Shape = %s"" %\n                     (input_tensor.shape))\n  if ndims == 2:\n    return input_tensor\n\n  width = input_tensor.shape[-1]\n  output_tensor = tf.reshape(input_tensor, [-1, width])\n  return output_tensor\n\n\ndef reshape_from_matrix(output_tensor, orig_shape_list):\n  """"""Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""""""\n  if len(orig_shape_list) == 2:\n    return output_tensor\n\n  output_shape = get_shape_list(output_tensor)\n\n  orig_dims = orig_shape_list[0:-1]\n  width = output_shape[-1]\n\n  return tf.reshape(output_tensor, orig_dims + [width])\n\n\ndef assert_rank(tensor, expected_rank, name=None):\n  """"""Raises an exception if the tensor rank is not of the expected rank.\n\n  Args:\n    tensor: A tf.Tensor to check the rank of.\n    expected_rank: Python integer or list of integers, expected rank.\n    name: Optional name of the tensor for the error message.\n\n  Raises:\n    ValueError: If the expected shape doesn\'t match the actual shape.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  expected_rank_dict = {}\n  if isinstance(expected_rank, (int, long)):\n    expected_rank_dict[expected_rank] = True\n  else:\n    for x in expected_rank:\n      expected_rank_dict[x] = True\n\n  actual_rank = tensor.shape.ndims\n  if actual_rank not in expected_rank_dict:\n    scope_name = tf.get_variable_scope().name\n    raise ValueError(\n        ""For the tensor `%s` in scope `%s`, the actual rank ""\n        ""`%d` (shape = %s) is not equal to the expected rank `%s`"" %\n        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n'"
text/bert/multi_gpu_optimizer.py,10,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Functions and classes related to optimization (weight updates).\n\nCopied from https://github.com/JayYip/bert-multitask-learning/blob/master/bert_multitask_learning/optimizer.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.training import optimizer\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import resource_variable_ops\n\n\nclass AdamWeightDecayOptimizer(optimizer.Optimizer):\n  """"""A basic Adam optimizer that includes ""correct"" L2 weight decay.""""""\n\n  def __init__(\n      self,\n      learning_rate,\n      weight_decay_rate=0.0,\n      beta_1=0.9,\n      beta_2=0.999,\n      epsilon=1e-6,\n      exclude_from_weight_decay=None,\n      name=""AdamWeightDecayOptimizer""):\n    """"""Constructs a AdamWeightDecayOptimizer.""""""\n    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n\n    self.learning_rate = learning_rate\n    self.weight_decay_rate = weight_decay_rate\n    self.beta_1 = beta_1\n    self.beta_2 = beta_2\n    self.epsilon = epsilon\n    self.exclude_from_weight_decay = exclude_from_weight_decay\n\n  def _prepare(self):\n    self.learning_rate_t = ops.convert_to_tensor(\n        self.learning_rate, name=\'learning_rate\')\n    self.weight_decay_rate_t = ops.convert_to_tensor(\n        self.weight_decay_rate, name=\'weight_decay_rate\')\n    self.beta_1_t = ops.convert_to_tensor(self.beta_1, name=\'beta_1\')\n    self.beta_2_t = ops.convert_to_tensor(self.beta_2, name=\'beta_2\')\n    self.epsilon_t = ops.convert_to_tensor(self.epsilon, name=\'epsilon\')\n\n  def _create_slots(self, var_list):\n    for v in var_list:\n      self._zeros_slot(v, \'m\', self._name)\n      self._zeros_slot(v, \'v\', self._name)\n\n  def _apply_dense(self, grad, var):\n    learning_rate_t = math_ops.cast(\n        self.learning_rate_t, var.dtype.base_dtype)\n    beta_1_t = math_ops.cast(self.beta_1_t, var.dtype.base_dtype)\n    beta_2_t = math_ops.cast(self.beta_2_t, var.dtype.base_dtype)\n    epsilon_t = math_ops.cast(self.epsilon_t, var.dtype.base_dtype)\n    weight_decay_rate_t = math_ops.cast(\n        self.weight_decay_rate_t, var.dtype.base_dtype)\n\n    m = self.get_slot(var, \'m\')\n    v = self.get_slot(var, \'v\')\n\n    # Standard Adam update.\n    next_m = (\n        tf.multiply(beta_1_t, m) +\n        tf.multiply(1.0 - beta_1_t, grad))\n    next_v = (\n        tf.multiply(beta_2_t, v) + tf.multiply(1.0 - beta_2_t,\n                                               tf.square(grad)))\n\n    update = next_m / (tf.sqrt(next_v) + epsilon_t)\n\n    if self._do_use_weight_decay(var.name):\n      update += weight_decay_rate_t * var\n\n    update_with_lr = learning_rate_t * update\n\n    next_param = var - update_with_lr\n\n    return control_flow_ops.group(*[var.assign(next_param),\n                                    m.assign(next_m),\n                                    v.assign(next_v)])\n\n  def _resource_apply_dense(self, grad, var):\n    learning_rate_t = math_ops.cast(\n        self.learning_rate_t, var.dtype.base_dtype)\n    beta_1_t = math_ops.cast(self.beta_1_t, var.dtype.base_dtype)\n    beta_2_t = math_ops.cast(self.beta_2_t, var.dtype.base_dtype)\n    epsilon_t = math_ops.cast(self.epsilon_t, var.dtype.base_dtype)\n    weight_decay_rate_t = math_ops.cast(\n        self.weight_decay_rate_t, var.dtype.base_dtype)\n\n    m = self.get_slot(var, \'m\')\n    v = self.get_slot(var, \'v\')\n\n    # Standard Adam update.\n    next_m = (\n        tf.multiply(beta_1_t, m) +\n        tf.multiply(1.0 - beta_1_t, grad))\n    next_v = (\n        tf.multiply(beta_2_t, v) + tf.multiply(1.0 - beta_2_t,\n                                               tf.square(grad)))\n\n    update = next_m / (tf.sqrt(next_v) + epsilon_t)\n\n    if self._do_use_weight_decay(var.name):\n      update += weight_decay_rate_t * var\n\n    update_with_lr = learning_rate_t * update\n\n    next_param = var - update_with_lr\n\n    return control_flow_ops.group(*[var.assign(next_param),\n                                    m.assign(next_m),\n                                    v.assign(next_v)])\n\n  def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n    learning_rate_t = math_ops.cast(\n        self.learning_rate_t, var.dtype.base_dtype)\n    beta_1_t = math_ops.cast(self.beta_1_t, var.dtype.base_dtype)\n    beta_2_t = math_ops.cast(self.beta_2_t, var.dtype.base_dtype)\n    epsilon_t = math_ops.cast(self.epsilon_t, var.dtype.base_dtype)\n    weight_decay_rate_t = math_ops.cast(\n        self.weight_decay_rate_t, var.dtype.base_dtype)\n\n    m = self.get_slot(var, \'m\')\n    v = self.get_slot(var, \'v\')\n\n    m_t = state_ops.assign(m, m * beta_1_t,\n                           use_locking=self._use_locking)\n\n    m_scaled_g_values = grad * (1 - beta_1_t)\n    with ops.control_dependencies([m_t]):\n      m_t = scatter_add(m, indices, m_scaled_g_values)\n\n    v_scaled_g_values = (grad * grad) * (1 - beta_2_t)\n    v_t = state_ops.assign(v, v * beta_2_t, use_locking=self._use_locking)\n    with ops.control_dependencies([v_t]):\n      v_t = scatter_add(v, indices, v_scaled_g_values)\n\n    update = m_t / (math_ops.sqrt(v_t) + epsilon_t)\n\n    if self._do_use_weight_decay(var.name):\n      update += weight_decay_rate_t * var\n\n    update_with_lr = learning_rate_t * update\n\n    var_update = state_ops.assign_sub(var,\n                                      update_with_lr,\n                                      use_locking=self._use_locking)\n    return control_flow_ops.group(*[var_update, m_t, v_t])\n\n  def _apply_sparse(self, grad, var):\n    return self._apply_sparse_shared(\n        grad.values, var, grad.indices,\n        lambda x, i, v: state_ops.scatter_add(  # pylint: disable=g-long-lambda\n            x, i, v, use_locking=self._use_locking))\n\n  def _resource_scatter_add(self, x, i, v):\n    with ops.control_dependencies(\n        [resource_variable_ops.resource_scatter_add(\n            x.handle, i, v)]):\n      return x.value()\n\n  def _resource_apply_sparse(self, grad, var, indices):\n    return self._apply_sparse_shared(\n        grad, var, indices, self._resource_scatter_add)\n\n  def _do_use_weight_decay(self, param_name):\n    """"""Whether to use L2 weight decay for `param_name`.""""""\n    if not self.weight_decay_rate:\n      return False\n    if self.exclude_from_weight_decay:\n      for r in self.exclude_from_weight_decay:\n        if re.search(r, param_name) is not None:\n          return False\n    return True\n'"
text/bert/optimization.py,24,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Functions and classes related to optimization (weight updates).\n\nPart of the code is from https://github.com/google-research/bert\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport tensorflow as tf\n\n\nclass AdamWeightDecayOptimizer(tf.train.Optimizer):\n  """"""A basic Adam optimizer that includes ""correct"" L2 weight decay.""""""\n\n  def __init__(self,\n               learning_rate,\n               weight_decay_rate=0.0,\n               beta_1=0.9,\n               beta_2=0.999,\n               epsilon=1e-6,\n               exclude_from_weight_decay=None,\n               name=""AdamWeightDecayOptimizer""):\n    """"""Constructs a AdamWeightDecayOptimizer.""""""\n    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n\n    self.learning_rate = learning_rate\n    self.weight_decay_rate = weight_decay_rate\n    self.beta_1 = beta_1\n    self.beta_2 = beta_2\n    self.epsilon = epsilon\n    self.exclude_from_weight_decay = exclude_from_weight_decay\n\n  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n    """"""See base class.""""""\n    assignments = []\n    for (grad, param) in grads_and_vars:\n      if grad is None or param is None:\n        continue\n\n      param_name = self._get_variable_name(param.name)\n\n      m = tf.get_variable(\n          name=param_name + ""/adam_m"",\n          shape=param.shape.as_list(),\n          dtype=tf.float32,\n          trainable=False,\n          initializer=tf.zeros_initializer())\n      v = tf.get_variable(\n          name=param_name + ""/adam_v"",\n          shape=param.shape.as_list(),\n          dtype=tf.float32,\n          trainable=False,\n          initializer=tf.zeros_initializer())\n\n      # Standard Adam update.\n      next_m = (\n          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n      next_v = (\n          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n                                                    tf.square(grad)))\n\n      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n\n      # Just adding the square of the weights to the loss function is *not*\n      # the correct way of using L2 regularization/weight decay with Adam,\n      # since that will interact with the m and v parameters in strange ways.\n      #\n      # Instead we want ot decay the weights in a manner that doesn\'t interact\n      # with the m/v parameters. This is equivalent to adding the square\n      # of the weights to the loss with plain (non-momentum) SGD.\n      if self._do_use_weight_decay(param_name):\n        update += self.weight_decay_rate * param\n\n      update_with_lr = self.learning_rate * update\n\n      next_param = param - update_with_lr\n\n      assignments.extend(\n          [param.assign(next_param),\n           m.assign(next_m),\n           v.assign(next_v)])\n    return tf.group(*assignments, name=name)\n\n  def _do_use_weight_decay(self, param_name):\n    """"""Whether to use L2 weight decay for `param_name`.""""""\n    if not self.weight_decay_rate:\n      return False\n    if self.exclude_from_weight_decay:\n      for r in self.exclude_from_weight_decay:\n        if re.search(r, param_name) is not None:\n          return False\n    return True\n\n  def _get_variable_name(self, param_name):\n    """"""Get the variable name from the tensor name.""""""\n    m = re.match(""^(.*):\\\\d+$"", param_name)\n    if m is not None:\n      param_name = m.group(1)\n    return param_name\n\n\ndef get_adam_optimizer(learning_rate, use_tpu):\n  """"""get adam optimizer.""""""\n  # It is recommended that you use this optimizer for fine tuning, since this\n  # is how the model was trained (note that the Adam m/v variables are NOT\n  # loaded from init_checkpoint.)\n  optimizer = AdamWeightDecayOptimizer(\n      learning_rate=learning_rate,\n      weight_decay_rate=0.01,\n      beta_1=0.9,\n      beta_2=0.999,\n      epsilon=1e-6,\n      exclude_from_weight_decay=[""LayerNorm"", ""layer_norm"", ""bias""])\n\n  if use_tpu:\n    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n  return optimizer\n\n\ndef create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps,\n                     use_tpu, clip_norm, global_step):\n  """"""Creates an optimizer training op.""""""\n\n  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n\n  # Implements linear decay of the learning rate.\n  learning_rate = tf.train.polynomial_decay(\n      learning_rate,\n      global_step,\n      num_train_steps,\n      end_learning_rate=0.0,\n      power=1.0,\n      cycle=False)\n\n  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n  if num_warmup_steps:\n    global_steps_int = tf.cast(global_step, tf.int32)\n    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n\n    global_steps_float = tf.cast(global_steps_int, tf.float32)\n    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n\n    warmup_percent_done = global_steps_float / warmup_steps_float\n    warmup_learning_rate = init_lr * warmup_percent_done\n\n    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n    learning_rate = (\n        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n\n  tvars = tf.trainable_variables()\n  grads = tf.gradients(loss, tvars)\n  # the model was pre-trained with grad clip 1.0.\n  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=clip_norm)\n\n  optimizer = get_adam_optimizer(learning_rate, use_tpu)\n  train_op = optimizer.apply_gradients(\n      zip(grads, tvars), global_step=global_step)\n\n  new_global_step = global_step + 1\n  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n  return train_op, learning_rate\n\n'"
text/utils/__init__.py,0,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
text/utils/imdb_format.py,0,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Read all data in IMDB and merge them to a csv file.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport csv\nimport os\nfrom absl import app\nfrom absl import flags\n\nFLAGS = flags.FLAGS\nflags.DEFINE_string(""raw_data_dir"", """", ""raw data dir"")\nflags.DEFINE_string(""output_dir"", """", ""output_dir"")\nflags.DEFINE_string(""train_id_path"", """", ""path of id list"")\n\n\ndef dump_raw_data(contents, file_path):\n  with open(file_path, ""w"") as ouf:\n    writer = csv.writer(ouf, delimiter=""\\t"", quotechar=""\\"""")\n    for line in contents:\n      writer.writerow(line)\n\ndef clean_web_text(st):\n  """"""clean text.""""""\n  st = st.replace(""<br />"", "" "")\n  st = st.replace(""&quot;"", ""\\"""")\n  st = st.replace(""<p>"", "" "")\n  if ""<a href="" in st:\n    while ""<a href="" in st:\n      start_pos = st.find(""<a href="")\n      end_pos = st.find("">"", start_pos)\n      if end_pos != -1:\n        st = st[:start_pos] + st[end_pos + 1:]\n      else:\n        print(""incomplete href"")\n        print(""before"", st)\n        st = st[:start_pos] + st[start_pos + len(""<a href="")]\n        print(""after"", st)\n\n    st = st.replace(""</a>"", """")\n  st = st.replace(""\\\\n"", "" "")\n  # st = st.replace(""\\\\"", "" "")\n  # while ""  "" in st:\n  #   st = st.replace(""  "", "" "")\n  return st\n\n\ndef load_data_by_id(sub_set, id_path):\n  with open(id_path) as inf:\n    id_list = inf.readlines()\n  contents = []\n  for example_id in id_list:\n    example_id = example_id.strip()\n    label = example_id.split(""_"")[0]\n    file_path = os.path.join(FLAGS.raw_data_dir, sub_set, label, example_id[len(label) + 1:])\n    with open(file_path) as inf:\n      st_list = inf.readlines()\n      assert len(st_list) == 1\n      st = clean_web_text(st_list[0].strip())\n      contents += [(st, label, example_id)]\n  return contents\n\n\ndef load_all_data(sub_set):\n  contents = []\n  for label in [""pos"", ""neg"", ""unsup""]:\n    data_path = os.path.join(FLAGS.raw_data_dir, sub_set, label)\n    if not os.path.exists(data_path):\n      continue\n    for filename in os.listdir(data_path):\n      file_path = os.path.join(data_path, filename)\n      with open(file_path) as inf:\n        st_list = inf.readlines()\n        assert len(st_list) == 1\n        st = clean_web_text(st_list[0].strip())\n        example_id = ""{}_{}"".format(label, filename)\n        contents += [(st, label, example_id)]\n  return contents\n\n\ndef main(_):\n  # load train\n  header = [""content"", ""label"", ""id""]\n  contents = load_data_by_id(""train"", FLAGS.train_id_path)\n  os.mkdir(FLAGS.output_dir)\n  dump_raw_data(\n      [header] + contents,\n      os.path.join(FLAGS.output_dir, ""train.csv""),\n  )\n  # load test\n  contents = load_all_data(""test"")\n  dump_raw_data(\n      [header] + contents,\n      os.path.join(FLAGS.output_dir, ""test.csv""),\n  )\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
text/utils/proc_data_utils.py,44,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""build datasets.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport json\nimport os\nimport string\n\nfrom absl import flags\n\nimport numpy as np\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\n\n\ndef _decode_record(record, name_to_features):\n  """"""Decodes a record to a TensorFlow example.""""""\n  example = tf.parse_single_example(record, name_to_features)\n\n  # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n  # So cast all int64 to int32.\n  for name in list(example.keys()):\n    t = example[name]\n    if t.dtype == tf.int64:\n      t = tf.to_int32(t)\n    example[name] = t\n\n  return example\n\n\ndef get_sup_feature_specs():\n  """"""Get supervised feature.""""""\n  feature_specs = collections.OrderedDict()\n  feature_specs[""input_ids""] = tf.FixedLenFeature(\n      [FLAGS.max_seq_length], tf.int64)\n  feature_specs[""input_mask""] = tf.FixedLenFeature(\n      [FLAGS.max_seq_length], tf.int64)\n  feature_specs[""input_type_ids""] = tf.FixedLenFeature(\n      [FLAGS.max_seq_length], tf.int64)\n  feature_specs[""label_ids""] = tf.FixedLenFeature(\n      [1], tf.int64)\n  return feature_specs\n\n\ndef get_unsup_feature_specs():\n  """"""Get unsupervised feature.""""""\n  feature_specs = collections.OrderedDict()\n  feature_specs[""ori_input_ids""] = tf.FixedLenFeature(\n      [FLAGS.max_seq_length], tf.int64)\n  feature_specs[""ori_input_mask""] = tf.FixedLenFeature(\n      [FLAGS.max_seq_length], tf.int64)\n  feature_specs[""ori_input_type_ids""] = tf.FixedLenFeature(\n      [FLAGS.max_seq_length], tf.int64)\n  feature_specs[""aug_input_ids""] = tf.FixedLenFeature(\n      [FLAGS.max_seq_length], tf.int64)\n  feature_specs[""aug_input_mask""] = tf.FixedLenFeature(\n      [FLAGS.max_seq_length], tf.int64)\n  feature_specs[""aug_input_type_ids""] = tf.FixedLenFeature(\n      [FLAGS.max_seq_length], tf.int64)\n  return feature_specs\n\n\ndef get_aug_files(data_base_path, aug_ops, aug_copy):\n  """"""get aug files.""""""\n\n  sub_policy_list = aug_ops.split(""+"")\n  total_data_files = []\n  for sub_policy in sub_policy_list:\n    sub_policy_data_files = []\n    exist_copy_num = {}\n    for copy_dir in tf.gfile.ListDirectory(os.path.join(\n        data_base_path, sub_policy)):\n      copy_num = int(copy_dir.strip(""/""))\n      if copy_num >= aug_copy:\n        continue\n      exist_copy_num[copy_num] = 1\n      data_record_path = os.path.join(\n          data_base_path, sub_policy, copy_dir, ""tf_examples.tfrecord*"")\n      data_files = tf.contrib.slim.parallel_reader.get_data_files(\n          data_record_path)\n      sub_policy_data_files += data_files\n    if len(exist_copy_num) < aug_copy * 0.9:\n      tf.logging.info(""not enough copies for aug op: {:s}"".format(aug_ops))\n      tf.logging.info(""found files: {:s}"".format(\n          "" "".join(sub_policy_data_files)))\n      tf.logging.info(""found copy: {:d} / desired copy: {:d}"".format(\n          len(exist_copy_num), aug_copy))\n    assert len(exist_copy_num) > aug_copy * 0.9\n    total_data_files += sub_policy_data_files\n  np.random.shuffle(total_data_files)\n  return total_data_files\n\n\ndef get_training_dataset(total_data_files, batch_size, num_threads, is_training,\n                         shuffle_buffer_size, feature_specs):\n  """"""build dataset from files.""""""\n  d = tf.data.Dataset.from_tensor_slices(tf.constant(total_data_files))\n  d = d.apply(\n      tf.contrib.data.shuffle_and_repeat(\n          buffer_size=len(total_data_files)))\n\n  # `cycle_length` is the number of parallel files that get read.\n  cycle_length = min(num_threads, len(total_data_files))\n\n  # `sloppy` mode means that the interleaving is not exact. This adds\n  # even more randomness to the training pipeline.\n  d = d.apply(\n      tf.contrib.data.parallel_interleave(\n          tf.data.TFRecordDataset,\n          sloppy=is_training,\n          cycle_length=cycle_length))\n  d = d.shuffle(buffer_size=shuffle_buffer_size)\n  d = d.apply(\n      tf.contrib.data.map_and_batch(\n          lambda record: _decode_record(record, feature_specs),\n          batch_size=batch_size,\n          num_parallel_batches=num_threads,\n          drop_remainder=is_training))\n  return d\n\n\ndef get_evaluation_dataset(total_data_files, batch_size, feature_specs):\n  """"""build non-repeat dataset from files.""""""\n  d = tf.data.TFRecordDataset(total_data_files)\n  d = d.apply(\n      tf.contrib.data.map_and_batch(\n          lambda record: _decode_record(record, feature_specs),\n          batch_size=batch_size,\n          num_parallel_batches=None,\n          drop_remainder=True))\n\n  return d\n\n\ndef evaluation_input_fn_builder(data_base_path, task, prefetch_size=1000):\n\n  total_data_files = tf.contrib.slim.parallel_reader.get_data_files(\n      os.path.join(data_base_path, ""tf_examples.tfrecord*""))\n  tf.logging.info(""loading eval {} data from these files: {:s}"".format(\n      task, "" "".join(total_data_files)))\n\n  def input_fn(params):\n    batch_size = params[""batch_size""]\n\n    if task == ""clas"":\n      dataset = get_evaluation_dataset(\n          total_data_files,\n          batch_size,\n          get_sup_feature_specs())\n    else:\n      assert False\n\n    dataset = dataset.prefetch(prefetch_size)\n\n    return dataset\n\n  return input_fn\n\n\ndef training_input_fn_builder(\n    sup_data_base_path=None,\n    unsup_data_base_path=None,\n    aug_ops=None,\n    aug_copy=None,\n    unsup_ratio=None,\n    num_threads=8,\n    shuffle_buffer_size=100000,\n    prefetch_size=1000):\n\n  sup_total_data_files = tf.contrib.slim.parallel_reader.get_data_files(\n      os.path.join(sup_data_base_path, ""tf_examples.tfrecord*""))\n\n  if unsup_ratio is not None and unsup_ratio > 0:\n    assert aug_ops is not None and aug_copy is not None, \\\n        ""Require aug_ops, aug_copy to load augmented unsup data.""\n    assert unsup_data_base_path is not None and unsup_data_base_path != """", \\\n        ""Require unsup_data_base_path to load unsup data. Get {}."".format(\n            unsup_data_base_path)\n\n    unsup_total_data_files = get_aug_files(\n        unsup_data_base_path, aug_ops, aug_copy)\n\n  is_training = True\n\n  def input_fn(params):\n    """"""The `input_fn` for TPUEstimator which generates the feature dataset.""""""\n    sup_batch_size = params[""batch_size""]\n    total_batch_size = 0\n    tf.logging.info(""sup batch size: %d"", (sup_batch_size))\n\n    dataset_list = []\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn\'t matter.\n    if sup_data_base_path is not None:\n      sup_dst = get_training_dataset(\n          sup_total_data_files,\n          sup_batch_size,\n          num_threads,\n          is_training,\n          shuffle_buffer_size,\n          get_sup_feature_specs())\n      total_batch_size += sup_batch_size\n      tf.logging.info(""sup batch size: %d"", (sup_batch_size))\n      dataset_list.append(sup_dst)\n\n      ## only consider unsupervised data when supervised data is considered\n      if unsup_data_base_path is not None and FLAGS.unsup_ratio > 0:\n        unsup_dst = get_training_dataset(\n            unsup_total_data_files,\n            sup_batch_size * unsup_ratio,\n            num_threads,\n            is_training,\n            shuffle_buffer_size,\n            get_unsup_feature_specs())\n        total_batch_size += sup_batch_size * unsup_ratio * 2\n        dataset_list.append(unsup_dst)\n        tf.logging.info(""unsup batch size: %d"", (sup_batch_size * unsup_ratio))\n\n    tf.logging.info(""total sample in a batch: %d"", (total_batch_size))\n\n    def flatten_input(*features):\n      """"""Merging multiple feature dicts resulted from zipped datasets.""""""\n      result = {}\n      for feature in features:\n        for key in feature:\n          assert key not in result\n          result[key] = feature[key]\n\n      return result\n\n    if len(dataset_list) > 1:\n      d = tf.data.Dataset.zip(tuple(dataset_list))\n      d = d.map(flatten_input)\n    else:\n      d = dataset_list[0]\n\n    # Prefetching creates a buffer to make sure there is always data to\n    # read in the event of network latency variance.\n    d = d.prefetch(prefetch_size)\n\n    # TPUEstimator supports returning a dataset instead of just features.\n    # It will call `make_one_shot_iterator()` and such.\n    return d\n  return input_fn\n\n\n'"
text/utils/raw_data_utils.py,2,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Load raw data.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport csv\nimport os\n\nfrom absl import flags\n\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\n\n\nclass InputExample(object):\n  """"""A single training/test example for simple sequence classification.""""""\n\n  def __init__(self, guid, text_a, text_b=None, label=None):\n    """"""Constructs a InputExample.\n\n    Args:\n      guid: Unique id for the example.\n      text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n      text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n      label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    """"""\n    self.guid = guid\n    self.text_a = text_a\n    self.text_b = text_b\n    self.label = label\n\n\nclass DataProcessor(object):\n  """"""Base class for data converters for sequence classification data sets.""""""\n\n  def get_train_examples(self, raw_data_dir):\n    """"""Gets a collection of `InputExample`s for the train set.""""""\n    raise NotImplementedError()\n\n  def get_dev_examples(self, raw_data_dir):\n    """"""Gets a collection of `InputExample`s for the dev set.""""""\n    raise NotImplementedError()\n\n  def get_labels(self):\n    """"""Gets the list of labels for this data set.""""""\n    raise NotImplementedError()\n\n  def get_train_size(self):\n    raise NotImplementedError()\n\n  @classmethod\n  def _read_tsv(cls, input_file, quotechar=None, delimiter=""\\t""):\n    """"""Reads a tab separated value file.""""""\n    with tf.gfile.Open(input_file, ""r"") as f:\n      reader = csv.reader(f, delimiter=delimiter, quotechar=quotechar)\n      lines = []\n      for line in reader:\n        lines.append(line)\n      return lines\n\n\ndef clean_web_text(st):\n  """"""clean text.""""""\n  st = st.replace(""<br />"", "" "")\n  st = st.replace(""&quot;"", ""\\"""")\n  st = st.replace(""<p>"", "" "")\n  if ""<a href="" in st:\n    # print(""before:\\n"", st)\n    while ""<a href="" in st:\n      start_pos = st.find(""<a href="")\n      end_pos = st.find("">"", start_pos)\n      if end_pos != -1:\n        st = st[:start_pos] + st[end_pos + 1:]\n      else:\n        print(""incomplete href"")\n        print(""before"", st)\n        st = st[:start_pos] + st[start_pos + len(""<a href="")]\n        print(""after"", st)\n\n    st = st.replace(""</a>"", """")\n    # print(""after\\n"", st)\n    # print("""")\n  st = st.replace(""\\\\n"", "" "")\n  st = st.replace(""\\\\"", "" "")\n  # while ""  "" in st:\n  #   st = st.replace(""  "", "" "")\n  return st\n\n\nclass IMDbProcessor(DataProcessor):\n  """"""Processor for the CoLA data set (GLUE version).""""""\n\n  def get_train_examples(self, raw_data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(raw_data_dir, ""train.csv""),\n                       quotechar=\'""\'), ""train"")\n\n  def get_dev_examples(self, raw_data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(raw_data_dir, ""test.csv""),\n                       quotechar=\'""\'), ""test"")\n\n  def get_unsup_examples(self, raw_data_dir, unsup_set):\n    """"""See base class.""""""\n    if unsup_set == ""unsup_ext"":\n      return self._create_examples(\n          self._read_tsv(os.path.join(raw_data_dir, ""unsup_ext.csv""),\n                         quotechar=\'""\'), ""unsup_ext"", skip_unsup=False)\n    elif unsup_set == ""unsup_in"":\n      return self._create_examples(\n          self._read_tsv(os.path.join(raw_data_dir, ""train.csv""),\n                         quotechar=\'""\'), ""unsup_in"", skip_unsup=False)\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [""pos"", ""neg""]\n\n  def _create_examples(self, lines, set_type, skip_unsup=True):\n    """"""Creates examples for the training and dev sets.""""""\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      if skip_unsup and line[1] == ""unsup"":\n        continue\n      if line[1] == ""unsup"" and len(line[0]) < 500:\n        # tf.logging.info(""skipping short samples:{:s}"".format(line[0]))\n        continue\n      guid = ""%s-%s"" % (set_type, line[2])\n      text_a = line[0]\n      label = line[1]\n      text_a = clean_web_text(text_a)\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n  def get_train_size(self):\n    return 25000\n\n  def get_dev_size(self):\n    return 25000\n\n\nclass TextClassProcessor(DataProcessor):\n\n  def get_train_examples(self, raw_data_dir):\n    """"""See base class.""""""\n    examples = self._create_examples(\n        self._read_tsv(os.path.join(raw_data_dir, ""train.csv""),\n                       quotechar=""\\"""",\n                       delimiter="",""), ""train"")\n    assert len(examples) == self.get_train_size()\n    return examples\n\n  def get_dev_examples(self, raw_data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(raw_data_dir, ""test.csv""),\n                       quotechar=""\\"""",\n                       delimiter="",""), ""test"")\n\n  def get_unsup_examples(self, raw_data_dir, unsup_set):\n    """"""See base class.""""""\n    if unsup_set == ""unsup_in"":\n      return self._create_examples(\n          self._read_tsv(\n              os.path.join(raw_data_dir, ""train.csv""),\n              quotechar=""\\"""",\n              delimiter="",""),\n          ""unsup_in"", skip_unsup=False)\n    else:\n      return self._create_examples(\n          self._read_tsv(\n              os.path.join(raw_data_dir, ""{:s}.csv"".format(unsup_set)),\n              quotechar=""\\"""",\n              delimiter="",""),\n          unsup_set, skip_unsup=False)\n\n  def _create_examples(self, lines, set_type, skip_unsup=True,\n                       only_unsup=False):\n    """"""Creates examples for the training and dev sets.""""""\n    examples = []\n    for (i, line) in enumerate(lines):\n      if skip_unsup and line[0] == ""unsup"":\n        continue\n      if only_unsup and line[0] != ""unsup"":\n        continue\n      guid = ""%s-%d"" % (set_type, i)\n      if self.has_title:\n        text_a = line[2]\n        text_b = line[1]\n      else:\n        text_a = line[1]\n        text_b = None\n      label = line[0]\n      text_a = clean_web_text(text_a)\n      if text_b is not None:\n        text_b = clean_web_text(text_b)\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n\nclass YELP2Processor(TextClassProcessor):\n\n  def __init__(self):\n    self.has_title = False\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [str(i) for i in range(1, 3)]\n\n  def get_train_size(self):\n    return 560000\n\n  def get_dev_size(self):\n    return 38000\n\n\nclass YELP5Processor(TextClassProcessor):\n\n  def __init__(self):\n    self.has_title = False\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [str(i) for i in range(1, 6)]\n\n  def get_train_size(self):\n    return 650000\n\n  def get_dev_size(self):\n    return 50000\n\n\nclass AMAZON2Processor(TextClassProcessor):\n\n  def __init__(self):\n    self.has_title = True\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [str(i) for i in range(1, 3)]\n\n  def get_train_size(self):\n    return 3600000\n\n  def get_dev_size(self):\n    return 400000\n\n  def get_unsup_examples(self, raw_data_dir, unsup_set):\n    """"""See base class.""""""\n    if unsup_set == ""unsup_in"":\n      return self._create_examples(\n          self._read_tsv(\n              os.path.join(raw_data_dir, ""train.csv""),\n              quotechar=""\\"""",\n              delimiter="",""),\n          ""unsup_in"", skip_unsup=False)\n    else:\n      dir_cell = raw_data_dir[5:7]\n      unsup_dir = None  # update this path if you use unsupervised data\n      return self._create_examples(\n          self._read_tsv(\n              os.path.join(unsup_dir, ""{:s}.csv"".format(unsup_set)),\n              quotechar=""\\"""",\n              delimiter="",""),\n          unsup_set, skip_unsup=False)\n\n\nclass AMAZON5Processor(TextClassProcessor):\n  def __init__(self):\n    self.has_title = True\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [str(i) for i in range(1, 6)]\n\n  def get_unsup_examples(self, raw_data_dir, unsup_set):\n    """"""See base class.""""""\n    if unsup_set == ""unsup_in"":\n      return self._create_examples(\n          self._read_tsv(\n              os.path.join(raw_data_dir, ""train.csv""),\n              quotechar=""\\"""",\n              delimiter="",""),\n          ""unsup_in"", skip_unsup=False)\n    else:\n      dir_cell = raw_data_dir[5:7]\n      unsup_dir = None  # update this path if you use unsupervised data\n      return self._create_examples(\n          self._read_tsv(\n              os.path.join(unsup_dir, ""{:s}.csv"".format(unsup_set)),\n              quotechar=""\\"""",\n              delimiter="",""),\n          unsup_set, skip_unsup=False)\n\n  def get_train_size(self):\n    return 3000000\n\n  def get_dev_size(self):\n    return 650000\n\n\nclass DBPediaProcessor(TextClassProcessor):\n\n  def __init__(self):\n    self.has_title = True\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [str(i) for i in range(1, 15)]\n\n  def get_train_size(self):\n    return 560000\n\n  def get_dev_size(self):\n    return 70000\n\n\ndef get_processor(task_name):\n  """"""get processor.""""""\n  task_name = task_name.lower()\n  processors = {\n      ""imdb"": IMDbProcessor,\n      ""dbpedia"": DBPediaProcessor,\n      ""yelp-2"": YELP2Processor,\n      ""yelp-5"": YELP5Processor,\n      ""amazon-2"": AMAZON2Processor,\n      ""amazon-5"": AMAZON5Processor,\n  }\n  processor = processors[task_name]()\n  return processor\n\n'"
text/utils/tokenization.py,2,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport codecs\nimport collections\nimport unicodedata\nimport six\nimport tensorflow as tf\n\n\ndef open_reader(input_file, encoding=""utf-8""):\n  """"""Opens a text file for reading.""""""\n  return codecs.getreader(encoding)(tf.gfile.GFile(input_file, ""r""))\n\n\ndef load_vocab(vocab_file):\n  """"""Loads a vocabulary file into a dictionary.""""""\n  vocab = collections.OrderedDict()\n  index = 0\n  with open_reader(vocab_file) as reader:\n    while True:\n      token = reader.readline()\n      if not token:\n        break\n      token = token.strip()\n      vocab[token] = index\n      index += 1\n  return vocab\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n  """"""Converts a sequence of tokens into ids using the vocab.""""""\n  ids = []\n  for token in tokens:\n    ids.append(vocab[token])\n  return ids\n\n\ndef whitespace_tokenize(text):\n  """"""Runs basic whitespace cleaning and splitting on a peice of text.""""""\n  text = text.strip()\n  if not text:\n    return []\n  tokens = text.split()\n  return tokens\n\n\nclass FullTokenizer(object):\n  """"""Runs end-to-end tokenziation.""""""\n\n  def __init__(self, vocab_file, do_lower_case=True):\n    self.vocab = load_vocab(vocab_file)\n    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n  def tokenize(self, text):\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(sub_token)\n    return split_tokens\n\n  def tokenize_to_word(self, text):\n    return self.basic_tokenizer.tokenize(text)\n\n  def tokenize_to_wordpiece(self, tokens):\n    split_tokens = []\n    for token in tokens:\n      split_tokens += self.wordpiece_tokenizer.tokenize(token)\n    return split_tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    return convert_tokens_to_ids(self.vocab, tokens)\n\n\nclass BasicTokenizer(object):\n  """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n  def __init__(self, do_lower_case=True):\n    """"""Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    """"""\n    self.do_lower_case = do_lower_case\n\n  def tokenize(self, text):\n    """"""Tokenizes a piece of text.""""""\n    text = _convert_to_unicode_or_throw(text)\n    text = self._clean_text(text)\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n      if self.do_lower_case:\n        token = token.lower()\n        token = self._run_strip_accents(token)\n      split_tokens.extend(self._run_split_on_punc(token))\n\n    output_tokens = whitespace_tokenize("" "".join(split_tokens))\n    return output_tokens\n\n  def _run_strip_accents(self, text):\n    """"""Strips accents from a piece of text.""""""\n    text = unicodedata.normalize(""NFD"", text)\n    output = []\n    for char in text:\n      cat = unicodedata.category(char)\n      if cat == ""Mn"":\n        continue\n      output.append(char)\n    return """".join(output)\n\n  def _run_split_on_punc(self, text):\n    """"""Splits punctuation on a piece of text.""""""\n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i < len(chars):\n      char = chars[i]\n      if _is_punctuation(char):\n        output.append([char])\n        start_new_word = True\n      else:\n        if start_new_word:\n          output.append([])\n        start_new_word = False\n        output[-1].append(char)\n      i += 1\n\n    return ["""".join(x) for x in output]\n\n  def _clean_text(self, text):\n    """"""Performs invalid character removal and whitespace cleanup on text.""""""\n    output = []\n    for char in text:\n      cp = ord(char)\n      if cp == 0 or cp == 0xfffd or _is_control(char):\n        continue\n      if _is_whitespace(char):\n        output.append("" "")\n      else:\n        output.append(char)\n    return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n  """"""Runs WordPiece tokenziation.""""""\n\n  def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=100):\n    self.vocab = vocab\n    self.unk_token = unk_token\n    self.max_input_chars_per_word = max_input_chars_per_word\n\n  def tokenize(self, text):\n    """"""Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = ""unaffable""\n      output = [""un"", ""##aff"", ""##able""]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    """"""\n\n    text = _convert_to_unicode_or_throw(text)\n\n    output_tokens = []\n    for token in whitespace_tokenize(text):\n      chars = list(token)\n      if len(chars) > self.max_input_chars_per_word:\n        output_tokens.append(self.unk_token)\n        continue\n\n      is_bad = False\n      start = 0\n      sub_tokens = []\n      while start < len(chars):\n        end = len(chars)\n        cur_substr = None\n        while start < end:\n          substr = """".join(chars[start:end])\n          if start > 0:\n            substr = ""##"" + substr\n          if substr in self.vocab:\n            cur_substr = substr\n            break\n          end -= 1\n        if cur_substr is None:\n          is_bad = True\n          break\n        sub_tokens.append(cur_substr)\n        start = end\n\n      if is_bad:\n        output_tokens.append(self.unk_token)\n      else:\n        output_tokens.extend(sub_tokens)\n    return output_tokens\n\n\ndef _is_whitespace(char):\n  """"""Checks whether `chars` is a whitespace character.""""""\n  # \\t, \\n, and \\r are technically contorl characters but we treat them\n  # as whitespace since they are generally considered as such.\n  if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n    return True\n  cat = unicodedata.category(char)\n  if cat == ""Zs"":\n    return True\n  return False\n\n\ndef _is_control(char):\n  """"""Checks whether `chars` is a control character.""""""\n  # These are technically control characters but we count them as whitespace\n  # characters.\n  if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n    return False\n  cat = unicodedata.category(char)\n  if cat.startswith(""C""):\n    return True\n  return False\n\n\ndef _is_punctuation(char):\n  """"""Checks whether `chars` is a punctuation character.""""""\n  cp = ord(char)\n  # We treat all non-letter/number ASCII as punctuation.\n  # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n  # Punctuation class but we treat them as punctuation anyways, for\n  # consistency.\n  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n    return True\n  cat = unicodedata.category(char)\n  if cat.startswith(""P""):\n    return True\n  return False\n\n\ndef _convert_to_unicode_or_throw(text):\n  """"""Converts `text` to Unicode (if it\'s not already), assuming utf-8 input.""""""\n  if isinstance(text, str):\n    text = text.decode(""utf-8"", ""ignore"")\n  if not isinstance(text, unicode):\n    raise ValueError(""`text` must be of type `unicode` or `str`, but is ""\n                     ""actually of type: %s"" % (type(text).__name__))\n  return text\n\n\ndef printable_text(text):\n  """"""Returns text encoded in a way suitable for print or `tf.logging`.""""""\n\n  # These functions want `str` for both Python2 and Python3, but in one case\n  # it\'s a Unicode string and in the other it\'s a byte string.\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(""utf-8"", ""ignore"")\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, unicode):\n      return text.encode(""utf-8"")\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  else:\n    raise ValueError(""Not running on Python2 or Python 3?"")\n'"
text/utils/tpu_utils.py,7,"b'# coding=utf-8\n# Copyright 2019 The Google UDA Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef construct_scalar_host_call(\n    metric_dict,\n    model_dir,\n    prefix="""",\n    reduce_fn=None):\n\n  metric_names = list(metric_dict.keys())\n\n  def host_call_fn(global_step, *args):\n    step = global_step[0]\n    with tf.contrib.summary.create_file_writer(\n        logdir=model_dir, filename_suffix="".host_call"").as_default():\n      with tf.contrib.summary.always_record_summaries():\n        for i, name in enumerate(metric_names):\n          if reduce_fn is None:\n            scalar = args[i][0]\n          else:\n            scalar = reduce_fn(args[i])\n          with tf.contrib.summary.record_summaries_every_n_global_steps(\n              1, step):\n            tf.contrib.summary.scalar(prefix + name, scalar, step=step)\n\n        return tf.contrib.summary.all_summary_ops()\n\n  global_step_tensor = tf.reshape(tf.train.get_or_create_global_step(), [1])\n  other_tensors = [tf.reshape(metric_dict[key], [-1]) for key in metric_names]\n\n  return host_call_fn, [global_step_tensor] + other_tensors\n\n'"
