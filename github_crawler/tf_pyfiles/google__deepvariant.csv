file_path,api_count,code
deepvariant/call_variants.py,14,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Code for calling variants with a trained DeepVariant model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport os\nimport time\n\n\n\nfrom absl import flags\nfrom absl import logging\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import errors\nfrom third_party.nucleus.util import proto_utils\nfrom third_party.nucleus.util import variant_utils\nfrom deepvariant import data_providers\nfrom deepvariant import logging_level\nfrom deepvariant import modeling\nfrom deepvariant import tf_utils\nfrom deepvariant.protos import deepvariant_pb2\nfrom google.protobuf import text_format\n\ntf.compat.v1.disable_eager_execution()\n\n_ALLOW_EXECUTION_HARDWARE = [\n    \'auto\',  # Default, no validation.\n    \'cpu\',  # Don\'t use accelerators, even if available.\n    \'accelerator\',  # Must be hardware acceleration or an error will be raised.\n]\n\n# The number of digits past the decimal point that genotype likelihoods are\n# rounded to, for numerical stability.\n_GL_PRECISION = 10\n\n# This number is estimated by the following logic:\n# CPU run is roughly 0.2 sec per 100.\n# 15000 examples will take about 30secs to print each line.\n_LOG_EVERY_N = 15000\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    \'examples\', None,\n    \'Required. tf.Example protos containing DeepVariant candidate variants in \'\n    \'TFRecord format, as emitted by make_examples. Can be a comma-separated \'\n    \'list of files, and the file names can contain wildcard characters.\')\nflags.DEFINE_string(\n    \'outfile\', None,\n    \'Required. Destination path where we will write output candidate variants \'\n    \'with additional likelihood information in TFRecord format of \'\n    \'CallVariantsOutput protos.\')\nflags.DEFINE_string(\n    \'checkpoint\', None,\n    \'Required. Path to the TensorFlow model checkpoint to use to evaluate \'\n    \'candidate variant calls.\')\nflags.DEFINE_integer(\n    \'batch_size\', 512,\n    \'Number of candidate variant tensors to batch together during inference. \'\n    \'Larger batches use more memory but are more computational efficient.\')\nflags.DEFINE_integer(\'max_batches\', None,\n                     \'Max. batches to evaluate. Defaults to all.\')\nflags.DEFINE_integer(\'num_readers\', 8,\n                     \'Number of parallel readers to create for examples.\')\nflags.DEFINE_integer(\'num_mappers\', 48,\n                     \'Number of parallel mappers to create for examples.\')\nflags.DEFINE_string(\'model_name\', \'inception_v3\',\n                    \'The name of the model architecture of --checkpoint.\')\nflags.DEFINE_boolean(\'include_debug_info\', False,\n                     \'If true, include extra debug info in the output.\')\nflags.DEFINE_boolean(\n    \'debugging_true_label_mode\', False,\n    \'If true, read the true labels from examples and add to \'\n    \'output. Note that the program will crash if the input \'\n    \'examples do not have the label field. \'\n    \'When true, this will also fill everything when \'\n    \'--include_debug_info is set to true.\')\nflags.DEFINE_string(\n    \'execution_hardware\', \'auto\',\n    \'When in cpu mode, call_variants will not place any ops on the GPU, even \'\n    \'if one is available. In accelerator mode call_variants validates that at \'\n    \'least some hardware accelerator (GPU/TPU) was available for us. This \'\n    \'option is primarily for QA purposes to allow users to validate their \'\n    \'accelerator environment is correctly configured. In auto mode, the \'\n    \'default, op placement is entirely left up to TensorFlow.  In tpu mode, \'\n    \'use and require TPU.\')\nflags.DEFINE_string(\n    \'config_string\', None,\n    \'String representation of a tf.ConfigProto message, with comma-separated \'\n    \'key: value pairs, such as ""allow_soft_placement: True"". The value can \'\n    \'itself be another message, such as \'\n    \'""gpu_options: {per_process_gpu_memory_fraction: 0.5}"".\')\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'gcp_project\', None,\n    \'Project name for the Cloud TPU-enabled project. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\nflags.DEFINE_string(\n    \'tpu_zone\', None,\n    \'GCE zone where the Cloud TPU is located in. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\nflags.DEFINE_string(\n    \'tpu_name\',\n    None,\n    \'Name of the Cloud TPU for Cluster Resolvers. You must specify either \'\n    \'this flag or --master. An empty value corresponds to no Cloud TPU. See \'\n    \'https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver\'  # pylint: disable=line-too-long\n)\n\nflags.DEFINE_string(\n    \'master\', None,\n    \'GRPC URL of the master (e.g. grpc://ip.address.of.tpu:8470). You \'\n    \'must specify either this flag or --tpu_name.\')\n\nflags.DEFINE_boolean(\'use_tpu\', False, \'Use tpu if available.\')\n\nflags.DEFINE_string(\n    \'kmp_blocktime\', \'0\',\n    \'Value to set the KMP_BLOCKTIME environment variable to for efficient MKL \'\n    \'inference. See https://www.tensorflow.org/performance/performance_guide \'\n    \'for more information. The default value is 0, which provides the best \'\n    \'performance in our tests. Set this flag to """" to not set the variable.\')\n\n\nclass ExecutionHardwareError(Exception):\n  pass\n\n\ndef prepare_inputs(source_path,\n                   use_tpu=False,\n                   num_readers=None,\n                   num_mappers=None):\n  """"""Return a tf.data input_fn from the source_path.\n\n  Args:\n    source_path: Path to a TFRecord file containing deepvariant tf.Example\n      protos.\n    use_tpu: boolean.  Use the tpu code path.\n    num_readers: int > 0 or None. Number of parallel readers to use to read\n      examples from source_path. If None, uses FLAGS.num_readers instead.\n    num_mappers: int > 0 or None. Number of parallel mappers to use to transform\n      examples from source_path. If None, uses FLAGS.num_mappers instead.\n\n  Returns:\n    A tf input_fn yielding batches of image, encoded_variant,\n    encoded_alt_allele_indices.\n\n    The image is a [batch_size, height, width, channel] tensor. The\n    encoded_variants is a tf.string or tpu-encoded tensor containing a\n    serialized Variant proto describing the variant call associated with\n    image. The encoded_alt_allele_indices is a tf.string or tpu-encoded\n    tensor containing a serialized CallVariantsOutput.AltAlleleIndices proto\n    containing the alternate alleles indices used as ""alt"" when constructing\n    the image.\n  """"""\n  if not num_readers:\n    num_readers = FLAGS.num_readers\n  if not num_mappers:\n    num_mappers = FLAGS.num_mappers\n\n  return data_providers.get_input_fn_from_filespec(\n      input_file_spec=source_path,\n      mode=tf.estimator.ModeKeys.PREDICT,\n      use_tpu=use_tpu,\n      input_read_threads=num_readers,\n      input_map_threads=num_mappers,\n      debugging_true_label_mode=FLAGS.debugging_true_label_mode,\n  )\n\n\ndef round_gls(gls, precision=None):\n  """"""Returns genotype likelihoods rounded to the desired precision level.\n\n  Args:\n    gls: A list of floats. The input genotype likelihoods at any precision.\n    precision: Positive int. The number of places past the decimal point to\n      round to. If None, no rounding is performed.\n\n  Returns:\n    A list of floats rounded to the desired precision.\n\n  Raises:\n    ValueError: The input gls do not sum to nearly 1.\n  """"""\n  if abs(sum(gls) - 1) > 1e-6:\n    raise ValueError(\n        \'Invalid genotype likelihoods do not sum to one: sum({}) = {}\'.format(\n            gls, sum(gls)))\n  if precision is None:\n    return gls\n\n  min_ix = 0\n  min_gl = gls[0]\n  for ix, gl in enumerate(gls):\n    if gl < min_gl:\n      min_gl = gl\n      min_ix = ix\n\n  rounded_gls = [round(gl, precision) for gl in gls]\n  rounded_gls[min_ix] = max(\n      0.0,\n      round(1 - sum(rounded_gls[:min_ix] + rounded_gls[min_ix + 1:]),\n            precision))\n  return rounded_gls\n\n\ndef write_variant_call(writer, prediction, use_tpu):\n  """"""Write the variant call based on prediction.\n\n  Args:\n    writer: A object with a write() function that will be called for each\n      encoded_variant and genotype likelihoods.\n    prediction: A [3] tensor of floats. These are the predicted genotype\n      likelihoods (p00, p0x, pxx) for some alt allele x, in the same order as\n      encoded_variants.\n      use_tpu: bool.  Decode the tpu specific encoding of prediction.\n\n  Returns:\n    The return status from writer.\n  """"""\n  encoded_variant = prediction[\'variant\']\n  if use_tpu:\n    encoded_variant = tf_utils.int_tensor_to_string(encoded_variant)\n\n  encoded_alt_allele_indices = prediction[\'alt_allele_indices\']\n  if use_tpu:\n    encoded_alt_allele_indices = tf_utils.int_tensor_to_string(\n        encoded_alt_allele_indices)\n\n  rounded_gls = round_gls(prediction[\'probabilities\'], precision=_GL_PRECISION)\n\n  # Write it out.\n  true_labels = prediction[\'label\'] if FLAGS.debugging_true_label_mode else None\n  cvo = _create_cvo_proto(encoded_variant, rounded_gls,\n                          encoded_alt_allele_indices, true_labels)\n  return writer.write(cvo)\n\n\ndef _create_cvo_proto(encoded_variant,\n                      gls,\n                      encoded_alt_allele_indices,\n                      true_labels=None):\n  """"""Returns a CallVariantsOutput proto from the relevant input information.""""""\n  variant = variants_pb2.Variant.FromString(encoded_variant)\n  alt_allele_indices = (\n      deepvariant_pb2.CallVariantsOutput.AltAlleleIndices.FromString(\n          encoded_alt_allele_indices))\n  debug_info = None\n  if FLAGS.include_debug_info or FLAGS.debugging_true_label_mode:\n    debug_info = deepvariant_pb2.CallVariantsOutput.DebugInfo(\n        has_insertion=variant_utils.has_insertion(variant),\n        has_deletion=variant_utils.has_deletion(variant),\n        is_snp=variant_utils.is_snp(variant),\n        predicted_label=np.argmax(gls),\n        true_label=true_labels,\n    )\n  call_variants_output = deepvariant_pb2.CallVariantsOutput(\n      variant=variant,\n      alt_allele_indices=alt_allele_indices,\n      genotype_probabilities=gls,\n      debug_info=debug_info)\n  return call_variants_output\n\n\ndef call_variants(examples_filename,\n                  checkpoint_path,\n                  model,\n                  output_file,\n                  execution_hardware=\'auto\',\n                  batch_size=16,\n                  max_batches=None,\n                  use_tpu=False,\n                  master=\'\'):\n  """"""Main driver of call_variants.""""""\n  if FLAGS.kmp_blocktime:\n    os.environ[\'KMP_BLOCKTIME\'] = FLAGS.kmp_blocktime\n    logging.info(\'Set KMP_BLOCKTIME to %s\', os.environ[\'KMP_BLOCKTIME\'])\n\n  # Read a single TFExample to make sure we\'re not loading an older version.\n  example_format = tf_utils.get_format_from_examples_path(examples_filename)\n  if example_format is None:\n    logging.warning(\n        \'Unable to read any records from %s. Output will contain \'\n        \'zero records.\', examples_filename)\n    tfrecord.write_tfrecords([], output_file)\n    return\n  elif example_format != six.b(\'raw\'):\n    raise ValueError(\'The TF examples in {} has image/format \\\'{}\\\' \'\n                     \'(expected \\\'raw\\\') which means you might need to rerun \'\n                     \'make_examples to generate the examples again.\'.format(\n                         examples_filename, example_format))\n\n  # Check accelerator status.\n  if execution_hardware not in _ALLOW_EXECUTION_HARDWARE:\n    raise ValueError(\n        \'Unexpected execution_hardware={} value. Allowed values are {}\'.format(\n            execution_hardware, \',\'.join(_ALLOW_EXECUTION_HARDWARE)))\n  init_op = tf.group(tf.compat.v1.global_variables_initializer(),\n                     tf.compat.v1.local_variables_initializer())\n\n  config = tf.compat.v1.ConfigProto()\n  if FLAGS.config_string is not None:\n    text_format.Parse(FLAGS.config_string, config)\n  if execution_hardware == \'cpu\':\n    # Don\'t overwrite entire dictionary.\n    config.device_count[\'GPU\'] = 0\n    config.device_count[\'TPU\'] = 0\n\n  # Perform sanity check.\n  with tf.compat.v1.Session(config=config) as sess:\n    sess.run(init_op)\n    if execution_hardware == \'accelerator\':\n      if not any(dev.device_type != \'CPU\' for dev in sess.list_devices()):\n        raise ExecutionHardwareError(\n            \'execution_hardware is set to accelerator, but no accelerator \'\n            \'was found\')\n    # redacted\n    # sess.list_devices here doesn\'t return the correct answer. That can only\n    # work later, after the device (on the other VM) has been initialized,\n    # which is generally not yet.\n\n  # Prepare input stream and estimator.\n  tf_dataset = prepare_inputs(source_path=examples_filename, use_tpu=use_tpu)\n  estimator = model.make_estimator(\n      batch_size=batch_size,\n      master=master,\n      use_tpu=use_tpu,\n      session_config=config,\n  )\n\n  # Instantiate the prediction ""stream"", and select the EMA values from\n  # the model.\n  if checkpoint_path is None:\n    # Unit tests use this branch.\n    predict_hooks = []\n  else:\n    predict_hooks = [h(checkpoint_path) for h in model.session_predict_hooks()]\n  predictions = iter(\n      estimator.predict(\n          input_fn=tf_dataset,\n          checkpoint_path=checkpoint_path,\n          hooks=predict_hooks))\n\n  # Consume predictions one at a time and write them to output_file.\n  logging.info(\'Writing calls to %s\', output_file)\n  writer = tfrecord.Writer(output_file)\n  with writer:\n    start_time = time.time()\n    n_examples, n_batches = 0, 0\n    while max_batches is None or n_batches <= max_batches:\n      try:\n        prediction = next(predictions)\n      except (StopIteration, tf.errors.OutOfRangeError):\n        break\n      write_variant_call(writer, prediction, use_tpu)\n      n_examples += 1\n      n_batches = n_examples // batch_size + 1\n      duration = time.time() - start_time\n\n      logging.log_every_n(\n          logging.INFO,\n          (\'Processed %s examples in %s batches [%.3f sec per 100]\'),\n          _LOG_EVERY_N, n_examples, n_batches, (100 * duration) / n_examples)\n\n    logging.info(\'Done evaluating variants\')\n\n\ndef main(argv=()):\n  with errors.clean_commandline_error_exit():\n    if len(argv) > 1:\n      errors.log_and_raise(\n          \'Command line parsing failure: call_variants does not accept \'\n          \'positional arguments but some are present on the command line: \'\n          \'""{}"".\'.format(str(argv)), errors.CommandLineError)\n    del argv  # Unused.\n    proto_utils.uses_fast_cpp_protos_or_die()\n\n    logging_level.set_from_flag()\n\n    if FLAGS.use_tpu:\n      master = tf_utils.resolve_master(FLAGS.master, FLAGS.tpu_name,\n                                       FLAGS.tpu_zone, FLAGS.gcp_project)\n    else:\n      master = \'\'\n\n    model = modeling.get_model(FLAGS.model_name)\n    call_variants(\n        examples_filename=FLAGS.examples,\n        checkpoint_path=FLAGS.checkpoint,\n        model=model,\n        execution_hardware=FLAGS.execution_hardware,\n        output_file=FLAGS.outfile,\n        max_batches=FLAGS.max_batches,\n        batch_size=FLAGS.batch_size,\n        master=master,\n        use_tpu=FLAGS.use_tpu,\n    )\n\n\nif __name__ == \'__main__\':\n  flags.mark_flags_as_required([\n      \'examples\',\n      \'outfile\',\n      \'checkpoint\',\n  ])\n  tf.compat.v1.app.run()\n'"
deepvariant/call_variants_accelerator_test.py,1,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests that call_variants can run in a environment with an accelerator.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport six\nimport tensorflow as tf\n\nfrom third_party.nucleus.testing import test_utils\nfrom deepvariant import call_variants\nfrom deepvariant import modeling\nfrom deepvariant import testdata\n\n\ndef setUpModule():\n  testdata.init()\n\n\nclass CallVariantsAcceleratorTests(\n    six.with_metaclass(parameterized.TestGeneratorMetaclass, tf.test.TestCase)):\n\n  @parameterized.parameters(modeling.production_models())\n  def test_call_variants_runs_on_gpus(self, model):\n    call_variants.call_variants(\n        examples_filename=testdata.GOLDEN_CALLING_EXAMPLES,\n        checkpoint_path=None,\n        model=model,\n        execution_hardware=\'accelerator\',\n        output_file=test_utils.test_tmpfile(\'zzz.tfrecord\'))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/call_variants_test.py,12,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .call_variants.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport collections\nimport errno\nimport sys\n\n\n\nfrom absl import flags\nfrom absl import logging\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import variant_utils\nfrom deepvariant import call_variants\nfrom deepvariant import modeling\nfrom deepvariant import testdata\nfrom deepvariant import tf_utils\nfrom deepvariant.protos import deepvariant_pb2\nfrom deepvariant.testing import flagsaver\n\nFLAGS = flags.FLAGS\n\n# NB. This entire collection of tests will be invoked with \'--use_tpu=\' \'true\'\n# and \'false\' by the BUILD file, and a tpu device will be allocated when\n# necessary.\n\n\ndef setUpModule():\n  testdata.init()\n\n\n# For tests that don\'t actually want to read a real checkpoint,\n# return a fake one.  The estimator understands None to mean\n# that all the variables should be left uninitialized.\n_LEAVE_MODEL_UNINITIALIZED = None\n\n\n# Return the stream of batched images from a dataset.\ndef _get_infer_batches(tf_dataset, model, batch_size):\n  """"""Provides batches of pileup images from this dataset.\n\n  This instantiates an iterator on the dataset, and returns the\n  image, variant, alt_allele_indices, features in batches. It calls\n  model.preprocess_images on the images (but note that we will be moving\n  that step into model_fn for the Estimator api).\n\n  Args:\n    tf_dataset: DeepVariantInput.\n    model: DeepVariantModel.\n    batch_size: int.  The batch size.\n\n  Returns:\n    (image, variant, alt_allele_indices)\n\n  Raises:\n    ValueError: if the dataset has the wrong mode.\n  """"""\n  if tf_dataset.mode != tf.estimator.ModeKeys.PREDICT:\n    raise ValueError(\'tf_dataset.mode is {} but must be PREDICT.\'.format(\n        tf_dataset.mode))\n\n  params = dict(batch_size=batch_size)\n  features = tf.compat.v1.data.make_one_shot_iterator(\n      tf_dataset(params)).get_next()\n\n  images = features[\'image\']\n  if tf_dataset.tensor_shape:\n    # tensor_shape will be None if the input was an empty file.\n    images = model.preprocess_images(images)\n  variant = features[\'variant\']\n  alt_allele_indices = features[\'alt_allele_indices\']\n\n  return images, variant, alt_allele_indices\n\n\nclass CallVariantsEndToEndTests(\n    six.with_metaclass(parameterized.TestGeneratorMetaclass,\n                       tf.compat.v1.test.TestCase)):\n\n  def setUp(self):\n    self.checkpoint_dir = tf.compat.v1.test.get_temp_dir()\n\n  def assertCallVariantsEmitsNRecordsForInceptionV3(self, filename,\n                                                    num_examples):\n    outfile = test_utils.test_tmpfile(\'inception_v3.call_variants.tfrecord\')\n    model = modeling.get_model(\'inception_v3\')\n    checkpoint_path = _LEAVE_MODEL_UNINITIALIZED\n\n    call_variants.call_variants(\n        examples_filename=filename,\n        checkpoint_path=checkpoint_path,\n        model=model,\n        output_file=outfile,\n        batch_size=4,\n        max_batches=None)\n    call_variants_outputs = list(\n        tfrecord.read_tfrecords(outfile, deepvariant_pb2.CallVariantsOutput))\n    # Check that we have the right number of output protos.\n    self.assertEqual(len(call_variants_outputs), num_examples)\n\n  def assertCallVariantsEmitsNRecordsForRandomGuess(self, filename,\n                                                    num_examples):\n    checkpoint_path = _LEAVE_MODEL_UNINITIALIZED\n    outfile = test_utils.test_tmpfile(\'call_variants.tfrecord\')\n    model = modeling.get_model(\'random_guess\')\n    call_variants.call_variants(\n        examples_filename=filename,\n        checkpoint_path=checkpoint_path,\n        model=model,\n        output_file=outfile,\n        batch_size=4,\n        max_batches=None,\n        master=\'\',\n        use_tpu=FLAGS.use_tpu)\n    call_variants_outputs = list(\n        tfrecord.read_tfrecords(outfile, deepvariant_pb2.CallVariantsOutput))\n    # Check that we have the right number of output protos.\n    self.assertEqual(len(call_variants_outputs), num_examples)\n\n  def test_call_end2end_with_empty_shards(self):\n    # Get only up to 10 examples.\n    examples = list(\n        tfrecord.read_tfrecords(\n            testdata.GOLDEN_CALLING_EXAMPLES, max_records=10))\n    # Write to 15 shards, which means there will be multiple empty shards.\n    source_path = test_utils.test_tmpfile(\'sharded@{}\'.format(15))\n    tfrecord.write_tfrecords(examples, source_path)\n    self.assertCallVariantsEmitsNRecordsForRandomGuess(source_path,\n                                                       len(examples))\n\n  def test_call_end2end_empty_first_shard(self):\n    # Get only up to 10 examples.\n    examples = list(\n        tfrecord.read_tfrecords(\n            testdata.GOLDEN_CALLING_EXAMPLES, max_records=10))\n    empty_first_file = test_utils.test_tmpfile(\'empty_1st_shard-00000-of-00002\')\n    tfrecord.write_tfrecords([], empty_first_file)\n    second_file = test_utils.test_tmpfile(\'empty_1st_shard-00001-of-00002\')\n    tfrecord.write_tfrecords(examples, second_file)\n    self.assertCallVariantsEmitsNRecordsForRandomGuess(\n        test_utils.test_tmpfile(\'empty_1st_shard@2\'), len(examples))\n\n  def test_call_end2end_zero_record_file_for_inception_v3(self):\n    zero_record_file = test_utils.test_tmpfile(\'zero_record_file\')\n    tfrecord.write_tfrecords([], zero_record_file)\n    self.assertCallVariantsEmitsNRecordsForInceptionV3(\n        test_utils.test_tmpfile(\'zero_record_file\'), 0)\n\n  def _call_end2end_helper(self, examples_path, model, shard_inputs):\n    examples = list(tfrecord.read_tfrecords(examples_path))\n\n    if shard_inputs:\n      # Create a sharded version of our golden examples.\n      source_path = test_utils.test_tmpfile(\'sharded@{}\'.format(3))\n      tfrecord.write_tfrecords(examples, source_path)\n    else:\n      source_path = examples_path\n\n    # If we point the test at a headless server, it will often be 2x2,\n    # which has 8 replicas.  Otherwise a smaller batch size is fine.\n    if FLAGS.use_tpu:\n      batch_size = 8\n    else:\n      batch_size = 4\n\n    if model.name == \'random_guess\':\n      # For the random guess model we can run everything.\n      max_batches = None\n    else:\n      # For all other models we only run a single batch for inference.\n      max_batches = 1\n\n    outfile = test_utils.test_tmpfile(\'call_variants.tfrecord\')\n    call_variants.call_variants(\n        examples_filename=source_path,\n        checkpoint_path=_LEAVE_MODEL_UNINITIALIZED,\n        model=model,\n        output_file=outfile,\n        batch_size=batch_size,\n        max_batches=max_batches,\n        master=\'\',\n        use_tpu=FLAGS.use_tpu,\n    )\n\n    call_variants_outputs = list(\n        tfrecord.read_tfrecords(outfile, deepvariant_pb2.CallVariantsOutput))\n\n    return call_variants_outputs, examples, batch_size, max_batches\n\n  @parameterized.parameters(model for model in modeling.production_models())\n  @flagsaver.FlagSaver\n  def test_call_end2end_with_labels(self, model):\n    FLAGS.debugging_true_label_mode = True\n    (call_variants_outputs, examples, batch_size,\n     max_batches) = self._call_end2end_helper(testdata.GOLDEN_TRAINING_EXAMPLES,\n                                              model, False)\n    # Check that we have the right number of output protos.\n    self.assertEqual(\n        len(call_variants_outputs),\n        batch_size * max_batches if max_batches else len(examples))\n\n    # Checks that at least some of the `true_label`s are filled.\n    self.assertTrue(\n        any(cvo.debug_info.true_label > 0 for cvo in call_variants_outputs))\n\n  @parameterized.parameters((model, shard_inputs, include_debug_info)\n                            for shard_inputs in [False, True]\n                            for model in modeling.production_models()\n                            for include_debug_info in [False, True])\n  @flagsaver.FlagSaver\n  def test_call_end2end(self, model, shard_inputs, include_debug_info):\n    FLAGS.include_debug_info = include_debug_info\n    (call_variants_outputs, examples, batch_size,\n     max_batches) = self._call_end2end_helper(testdata.GOLDEN_CALLING_EXAMPLES,\n                                              model, shard_inputs)\n    # Check that we have the right number of output protos.\n    self.assertEqual(\n        len(call_variants_outputs),\n        batch_size * max_batches if max_batches else len(examples))\n\n    # Check that our CallVariantsOutput (CVO) have the following critical\n    # properties:\n    # - we have one CVO for each example we processed.\n    # - the variant in the CVO is exactly what was in the example.\n    # - the alt_allele_indices of the CVO match those of its corresponding\n    #   example.\n    # - there are 3 genotype probabilities and these are between 0.0 and 1.0.\n    # We can only do this test when processing all of the variants (max_batches\n    # is None), since we processed all of the examples with that model.\n    if max_batches is None:\n      six.assertCountEqual(self, [cvo.variant for cvo in call_variants_outputs],\n                           [tf_utils.example_variant(ex) for ex in examples])\n\n    # Check the CVO debug_info: not filled if include_debug_info is False;\n    # else, filled by logic based on CVO.\n    if not include_debug_info:\n      for cvo in call_variants_outputs:\n        self.assertEqual(cvo.debug_info,\n                         deepvariant_pb2.CallVariantsOutput.DebugInfo())\n    else:\n      for cvo in call_variants_outputs:\n        self.assertEqual(cvo.debug_info.has_insertion,\n                         variant_utils.has_insertion(cvo.variant))\n        self.assertEqual(cvo.debug_info.has_deletion,\n                         variant_utils.has_deletion(cvo.variant))\n        self.assertEqual(cvo.debug_info.is_snp,\n                         variant_utils.is_snp(cvo.variant))\n        self.assertEqual(cvo.debug_info.predicted_label,\n                         np.argmax(cvo.genotype_probabilities))\n\n    def example_matches_call_variants_output(example, call_variants_output):\n      return (tf_utils.example_variant(example) == call_variants_output.variant\n              and tf_utils.example_alt_alleles_indices(\n                  example) == call_variants_output.alt_allele_indices.indices)\n\n    for call_variants_output in call_variants_outputs:\n      # Find all matching examples.\n      matches = [\n          ex for ex in examples\n          if example_matches_call_variants_output(ex, call_variants_output)\n      ]\n      # We should have exactly one match.\n      self.assertEqual(len(matches), 1)\n      example = matches[0]\n      # Check that we\'ve faithfully copied in the alt alleles (though currently\n      # as implemented we find our example using this information so it cannot\n      # fail). Included here in case that changes in the future.\n      self.assertEqual(\n          list(tf_utils.example_alt_alleles_indices(example)),\n          list(call_variants_output.alt_allele_indices.indices))\n      # We should have exactly three genotype probabilities (assuming our\n      # ploidy == 2).\n      self.assertEqual(len(call_variants_output.genotype_probabilities), 3)\n      # These are probabilities so they should be between 0 and 1.\n      self.assertTrue(\n          0 <= gp <= 1 for gp in call_variants_output.genotype_probabilities)\n\n  # pylint: disable=g-complex-comprehension\n  @parameterized.parameters(\n      (model, bad_format)\n      for model in modeling.production_models()\n      for bad_format in [six.b(\'\'), six.b(\'png\')])\n  # pylint: enable=g-complex-comprehension\n  def test_call_variants_with_invalid_format(self, model, bad_format):\n    # Read one good record from a valid file.\n    example = next(tfrecord.read_tfrecords(testdata.GOLDEN_CALLING_EXAMPLES))\n    # Overwrite the image/format field to be an invalid value\n    # (anything but \'raw\').\n    example.features.feature[\'image/format\'].bytes_list.value[0] = bad_format\n    source_path = test_utils.test_tmpfile(\'make_examples_output.tfrecord\')\n    tfrecord.write_tfrecords([example], source_path)\n    outfile = test_utils.test_tmpfile(\'call_variants_invalid_format.tfrecord\')\n\n    with self.assertRaises(ValueError):\n      call_variants.call_variants(\n          examples_filename=source_path,\n          checkpoint_path=_LEAVE_MODEL_UNINITIALIZED,\n          model=model,\n          output_file=outfile,\n          batch_size=1,\n          max_batches=1,\n          use_tpu=FLAGS.use_tpu)\n\n  @parameterized.parameters(model for model in modeling.production_models())\n  def test_call_variants_with_no_shape(self, model):\n    # Read one good record from a valid file.\n    example = next(tfrecord.read_tfrecords(testdata.GOLDEN_CALLING_EXAMPLES))\n    # Remove image/shape.\n    del example.features.feature[\'image/shape\']\n    source_path = test_utils.test_tmpfile(\'make_examples_out_noshape.tfrecord\')\n    tfrecord.write_tfrecords([example], source_path)\n    with six.assertRaisesRegex(\n        self, ValueError,\n        \'Invalid image/shape: we expect to find an image/shape \'\n        \'field with length 3.\'):\n      ds = call_variants.prepare_inputs(source_path)\n      _ = list(_get_infer_batches(ds, model=model, batch_size=1))\n\n  def test_call_variants_with_empty_input(self):\n    source_path = test_utils.test_tmpfile(\'empty.tfrecord\')\n    tfrecord.write_tfrecords([], source_path)\n    # Make sure that prepare_inputs don\'t crash on empty input.\n    ds = call_variants.prepare_inputs(source_path)\n    m = modeling.get_model(\'random_guess\')\n\n    # The API specifies that OutOfRangeError is thrown in this case.\n    batches = list(_get_infer_batches(ds, model=m, batch_size=1))\n    with self.test_session() as sess:\n      sess.run(tf.compat.v1.local_variables_initializer())\n      sess.run(tf.compat.v1.global_variables_initializer())\n      try:\n        _ = sess.run(batches)\n      except tf.errors.OutOfRangeError:\n        pass\n\n\nclass CallVariantsUnitTests(\n    six.with_metaclass(parameterized.TestGeneratorMetaclass, tf.test.TestCase)):\n\n  @classmethod\n  def setUpClass(cls):\n    cls.examples = list(\n        tfrecord.read_tfrecords(testdata.GOLDEN_CALLING_EXAMPLES))\n    cls.variants = [tf_utils.example_variant(ex) for ex in cls.examples]\n    cls.model = modeling.get_model(\'random_guess\')\n\n  @parameterized.parameters(\n      (\'not_sharded\', \'not_sharded\'),\n      (\'sharded@3\', \'sharded@3\'),\n      (\'sharded@3\', \'sharded-?????-of-00003\'),\n      (\'asterisks@2\', \'asterisks-*-of-00002\'),\n  )\n  def test_prepare_inputs(self, filename_to_write, file_string_input):\n    source_path = test_utils.test_tmpfile(filename_to_write)\n    tfrecord.write_tfrecords(self.examples, source_path)\n    # file_string_input could be a comma-separated list. Add the prefix to all\n    # of them, and join it back to a string.\n    file_string_input = \',\'.join(\n        [test_utils.test_tmpfile(f) for f in file_string_input.split(\',\')])\n\n    with self.test_session() as sess:\n      sess.run(tf.compat.v1.local_variables_initializer())\n      sess.run(tf.compat.v1.global_variables_initializer())\n\n      ds = call_variants.prepare_inputs(file_string_input)\n      _, variants, _ = _get_infer_batches(ds, model=self.model, batch_size=1)\n\n      seen_variants = []\n      try:\n        while True:\n          seen_variants.extend(sess.run(variants))\n      except tf.errors.OutOfRangeError:\n        pass\n\n      six.assertCountEqual(self, self.variants,\n                           variant_utils.decode_variants(seen_variants))\n\n  @parameterized.parameters(\n      (None, [3.592555731302127e-5, 0.99992620944976807, 3.78809563699178e-5]),\n      (2, [0.0, 1.0, 0.0]),\n      (10, [3.59096e-5, 0.9999262094, 3.7881e-5]),\n  )\n  def test_round_gls(self, precision, expected):\n    test_data = [3.592555731302127e-5, 0.99992620944976807, 3.78809563699178e-5]\n    actual = call_variants.round_gls(test_data, precision)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\'auto\', \'cpu\')\n  def test_call_variants_non_accelerated_execution_runs(self,\n                                                        execution_hardware):\n    if FLAGS.use_tpu:\n      # predict batch size must be divisible by number of replicas.\n      batch_size = 2\n    else:\n      batch_size = 1\n    outfile = test_utils.test_tmpfile(\'call_variants_cpu_only.tfrecord\')\n    call_variants.call_variants(\n        examples_filename=testdata.GOLDEN_CALLING_EXAMPLES,\n        checkpoint_path=_LEAVE_MODEL_UNINITIALIZED,\n        model=self.model,\n        execution_hardware=execution_hardware,\n        max_batches=1,\n        batch_size=batch_size,\n        output_file=outfile,\n        use_tpu=FLAGS.use_tpu)\n\n  @parameterized.parameters(\n      dict(hardware_env=\'auto\', devices=[\'cpu\'], expect_exception=False),\n      dict(hardware_env=\'auto\', devices=[\'gpu\'], expect_exception=False),\n      dict(hardware_env=\'auto\', devices=[\'tpu\'], expect_exception=False),\n      dict(hardware_env=\'cpu\', devices=[\'cpu\'], expect_exception=False),\n      dict(hardware_env=\'cpu\', devices=[\'gpu\'], expect_exception=False),\n      dict(hardware_env=\'cpu\', devices=[\'tpu\'], expect_exception=False),\n      dict(hardware_env=\'accelerator\', devices=[\'cpu\'], expect_exception=True),\n      dict(hardware_env=\'accelerator\', devices=[\'gpu\'], expect_exception=False),\n      dict(hardware_env=\'accelerator\', devices=[\'tpu\'], expect_exception=False),\n      dict(\n          hardware_env=\'accelerator\',\n          devices=[\'cpu\', \'gpu\'],\n          expect_exception=False),\n      dict(\n          hardware_env=\'accelerator\',\n          devices=[\'cpu\', \'tpu\'],\n          expect_exception=False),\n      dict(\n          hardware_env=\'accelerator\',\n          devices=[\'cpu\', \'gpu\', \'tpu\'],\n          expect_exception=False),\n  )\n  def test_call_variants_execution_hardware(self, hardware_env, devices,\n                                            expect_exception):\n    # We cannot access the full _DeviceAttribute as it\'s not exported. So use a\n    # namedtuple with the same field names instead.\n    device = collections.namedtuple(\'_DeviceAttribute\', [\'name\', \'device_type\'])\n\n    # Mocking the list_devices call means the framework attempts to use a bogus\n    # TPU device, which fails, so don\'t do that.  Handle the TPU case elsewhere.\n    if \'tpu\' in devices or FLAGS.use_tpu:\n      return\n\n    with mock.patch.object(call_variants.tf.compat.v1.Session,\n                           \'list_devices\') as mock_ld:\n      mock_ld.return_value = [\n          device(name=dt + \'/\' + str(i), device_type=dt.upper())\n          for i, dt in enumerate(devices)\n      ]\n\n      # Only run the tpu cases when we have an actual tpu device, supplied\n      # by the flags from the BUILD rule.\n      def _run():\n        call_variants.call_variants(\n            use_tpu=FLAGS.use_tpu,\n            examples_filename=testdata.GOLDEN_CALLING_EXAMPLES,\n            checkpoint_path=_LEAVE_MODEL_UNINITIALIZED,\n            model=self.model,\n            execution_hardware=hardware_env,\n            max_batches=1,\n            batch_size=1,\n            output_file=test_utils.test_tmpfile(\'zzz.tfrecord\'))\n\n      if expect_exception:\n        with self.assertRaises(call_variants.ExecutionHardwareError):\n          _run()\n      else:\n        _run()\n\n  def test_catches_bad_argv(self):\n    with mock.patch.object(logging, \'error\') as mock_logging, mock.patch.object(\n        sys, \'exit\') as mock_exit:\n      call_variants.main([\'call_variants.py\', \'extra_arg\'])\n    mock_logging.assert_called_once_with(\n        \'Command line parsing failure: call_variants does not accept \'\n        \'positional arguments but some are present on the command line: \'\n        \'""[\\\'call_variants.py\\\', \\\'extra_arg\\\']"".\')\n    mock_exit.assert_called_once_with(errno.ENOENT)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/data_providers.py,50,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Data providers for deepvariant images.\n\ntf.data.Dataset and data providers for standard DeepVariant datasets for\ntraining and evaluating germline calling accuracy.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom third_party.nucleus.io import sharded_file_utils\nfrom deepvariant import dv_constants\nfrom deepvariant import tf_utils\nfrom deepvariant.protos import deepvariant_pb2\n\n# These are emperically determined to work well on TPU with our data sets,\n# where lots of buffering and concurrency is necessary to keep the device\n# busy.\n# These are settable in the constructor.\n_DEFAULT_INPUT_READ_THREADS = 32\n_DEFAULT_INPUT_MAP_THREADS = 48\n_DEFAULT_SHUFFLE_BUFFER_ELEMENTS = 100\n_DEFAULT_INITIAL_SHUFFLE_BUFFER_ELEMENTS = 1024\n_DEFAULT_PREFETCH_BUFFER_BYTES = 16 * 1000 * 1000\n# This one doesn\'t seem useful to adjust at runtime.\n_PREFETCH_BATCHES = 4\n\n\nclass DeepVariantInput(object):\n  """"""This class serves as an `input_fn` for the `tf.estimator` framework.""""""\n\n  # Calling this object like a function returns a stream of variadic tuples.\n  # Essentially it is a buffered io library, that handles concurrently\n  # reading and possibly shuffling input records from a set of files. It\n  # knows how to parse features we care about from tf.examples. It records\n  # some extra information about the source of the input, such as the name\n  # and number of classes.\n\n  def __init__(\n      self,\n      mode,\n      input_file_spec,\n      num_examples=None,\n      num_classes=dv_constants.NUM_CLASSES,\n      max_examples=None,\n      tensor_shape=None,\n      name=None,\n      use_tpu=False,\n      input_read_threads=_DEFAULT_INPUT_READ_THREADS,\n      input_map_threads=_DEFAULT_INPUT_MAP_THREADS,\n      shuffle_buffer_size=_DEFAULT_SHUFFLE_BUFFER_ELEMENTS,\n      initial_shuffle_buffer_size=_DEFAULT_INITIAL_SHUFFLE_BUFFER_ELEMENTS,\n      prefetch_dataset_buffer_size=_DEFAULT_PREFETCH_BUFFER_BYTES,\n      sloppy=True,\n      list_files_shuffle=True,\n      debugging_true_label_mode=False):\n    """"""Create an DeepVariantInput object, usable as an `input_fn`.\n\n    Args:\n      mode: the mode string (from `tf.estimator.ModeKeys`).\n      input_file_spec: the input filename for a tfrecord[.gz] file containing\n        examples.  Can contain sharding designators.\n      num_examples: the number of examples contained in the input file. Required\n        for setting learning rate schedule in train/eval only.\n      num_classes: The number of classes in the labels of this dataset.\n        Currently defaults to DEFAULT_NUM_CLASSES.\n      max_examples: The maximum number of examples to use. If None, all examples\n        will be used. If not None, the first n = min(max_examples, num_examples)\n        will be used. This works with training, and the n examples will repeat\n        over and over.\n      tensor_shape: None (which means we get the shape from the first example in\n        source), or list of int [height, width, channel] for testing.\n      name: string, name of the dataset.\n      use_tpu: use code paths tuned for TPU, in particular protobuf encoding.\n        Default False.\n      input_read_threads: number of threads for reading data.  Default 32.\n      input_map_threads: number of threads for mapping data.  Default 48.\n      shuffle_buffer_size: size of the final shuffle buffer, in elements.\n        Default 100.\n      initial_shuffle_buffer_size: int; the size of the dataset.shuffle buffer\n        in elements.  Default is 1024.\n      prefetch_dataset_buffer_size: int; the size of the TFRecordDataset buffer\n        in bytes.  Default is 16 * 1000 * 1000.\n      sloppy: boolean, allow parallel_interleave to be sloppy.  Default True.\n      list_files_shuffle: boolean, allow list_files to shuffle.  Default True.\n      debugging_true_label_mode: boolean. If true, the input examples are\n        created with ""training"" mode. We\'ll parse the \'label\' field even if the\n        `mode` is PREDICT.\n\n    Raises:\n      ValueError: if `num_examples` not provided, in a context requiring it.\n    """"""\n    self.mode = mode\n    self.input_file_spec = input_file_spec\n    self.name = name\n    self.num_examples = num_examples\n    self.num_classes = num_classes\n    self.max_examples = max_examples\n\n    self.use_tpu = use_tpu\n    self.sloppy = sloppy\n    self.list_files_shuffle = list_files_shuffle\n    self.input_read_threads = input_read_threads\n    self.input_map_threads = input_map_threads\n    self.shuffle_buffer_size = shuffle_buffer_size\n    self.initial_shuffle_buffer_size = initial_shuffle_buffer_size\n    self.prefetch_dataset_buffer_size = prefetch_dataset_buffer_size\n    self.debugging_true_label_mode = debugging_true_label_mode\n    self.feature_extraction_spec = self.features_extraction_spec_for_mode(\n        mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL) or\n        debugging_true_label_mode)\n\n    if num_examples is None and mode != tf.estimator.ModeKeys.PREDICT:\n      raise ValueError(\'num_examples argument required for DeepVariantInput\'\n                       \'in TRAIN/EVAL modes.\')\n\n    if max_examples is not None:\n      if max_examples <= 0:\n        raise ValueError(\n            \'max_examples must be > 0 if not None. Got {}\'.format(max_examples))\n      # We update our num_examples in the situation where num_examples is set\n      # (i.e., is not None) to the smaller of max_examples and num_examples.\n      if self.num_examples is not None:\n        self.num_examples = min(max_examples, self.num_examples)\n\n    if tensor_shape:\n      self.tensor_shape = tensor_shape\n    else:\n      self.tensor_shape = tf_utils.get_shape_from_examples_path(input_file_spec)\n    self.input_files = sharded_file_utils.glob_list_sharded_file_patterns(\n        self.input_file_spec)\n\n  def features_extraction_spec_for_mode(self, include_label_and_locus):\n    """"""Returns a dict describing features from a TF.example.""""""\n    spec = {\n        \'image/encoded\': tf.io.FixedLenFeature((), tf.string),\n        \'variant/encoded\': tf.io.FixedLenFeature((), tf.string),\n        \'alt_allele_indices/encoded\': tf.io.FixedLenFeature((), tf.string),\n        \'variant_type\': tf.io.FixedLenFeature((), tf.int64),\n        \'sequencing_type\': tf.io.FixedLenFeature([], tf.int64),\n    }\n    if include_label_and_locus:\n      # N.B. int32 fails here on TPU.\n      spec[\'label\'] = tf.io.FixedLenFeature((), tf.int64)\n      spec[\'locus\'] = tf.io.FixedLenFeature((), tf.string)\n    return spec\n\n  def parse_tfexample(self, tf_example):\n    """"""Parse a DeepVariant pileup tf.Example to features and labels.\n\n    This potentially stores parsed strings as fixed length tensors of integers,\n    as required by TPU.  They have to be handled properly by consumers.\n\n    Args:\n      tf_example: a serialized tf.Example for a DeepVariant ""pileup"".\n\n    Returns:\n      If (mode is EVAL or TRAIN) or debugging_true_label_mode:\n        (features, label) ...\n      If mode is PREDICT,\n        features ...\n    """"""\n    # redacted\n    with tf.compat.v1.name_scope(\'input\'):\n      parsed = tf.io.parse_single_example(\n          serialized=tf_example, features=self.feature_extraction_spec)\n      image = parsed[\'image/encoded\']\n      if self.tensor_shape:\n        # If the input is empty there won\'t be a tensor_shape.\n        image = tf.reshape(tf.io.decode_raw(image, tf.uint8), self.tensor_shape)\n        if self.use_tpu:\n          # Cast to int32 for loading onto the TPU\n          image = tf.cast(image, tf.int32)\n\n      variant = parsed[\'variant/encoded\']\n      alt_allele_indices = parsed[\'alt_allele_indices/encoded\']\n      if self.use_tpu:\n        # Passing a string to a TPU draws this error: TypeError: <dtype:\n        # \'string\'> is not a supported TPU infeed type. Supported types are:\n        # [tf.float32, tf.int32, tf.complex64, tf.int64, tf.bool, tf.bfloat16]\n        # Thus, we must encode the string as a tensor of int.\n        variant = tf_utils.string_to_int_tensor(variant)\n        alt_allele_indices = tf_utils.string_to_int_tensor(alt_allele_indices)\n\n      features = {\n          \'image\': image,\n          \'variant\': variant,\n          \'alt_allele_indices\': alt_allele_indices,\n          \'sequencing_type\': parsed[\'sequencing_type\'],\n      }\n\n      if (self.mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL)\n          or self.debugging_true_label_mode):\n        if self.use_tpu:\n          features[\'locus\'] = tf_utils.string_to_int_tensor(parsed[\'locus\'])\n        else:\n          features[\'locus\'] = parsed[\'locus\']\n\n        # Add variant_type to our features if are in TRAIN or EVAL mode.\n        features[\'variant_type\'] = parsed[\'variant_type\']\n\n        if self.mode in (tf.estimator.ModeKeys.TRAIN,\n                         tf.estimator.ModeKeys.EVAL):\n          label = parsed[\'label\']\n          return features, label\n        features[\'label\'] = parsed[\'label\']\n\n      # For predict model, label is not present. So, returns features only.\n      return features\n\n  def __call__(self, params):\n    """"""Interface to get a data batch, fulfilling `input_fn` contract.\n\n    Args:\n      params: a dict containing an integer value for key \'batch_size\'.\n\n    Returns:\n      the tuple (features, labels), where:\n        - features is a dict of Tensor-valued input features; keys populated\n          are:\n            \'image\'\n            \'variant\'\n            \'alt_allele_indices\'\n          and, if not PREDICT mode, also:\n            \'locus\'\n\n          Aside from \'image\', these may be encoded specially for TPU.\n\n        - label is the Tensor-valued prediction label; in train/eval\n          mode the label value is is populated from the data source; in\n          inference mode, the value is a constant empty Tensor value ""()"".\n    """"""\n    # See https://cloud.google.com/tpu/docs/tutorials/inception-v3-advanced\n    # for some background on tuning this on TPU.\n\n    # TPU optimized implementation for prediction mode\n    if self.mode == tf.estimator.ModeKeys.PREDICT:\n      return self.prediction_input_fn(params)\n\n    # Optimized following:\n    #   https://www.tensorflow.org/guide/performance/datasets\n    # using the information available from xprof.\n    def load_dataset(filename):\n      dataset = tf.data.TFRecordDataset(\n          filename,\n          buffer_size=self.prefetch_dataset_buffer_size,\n          compression_type=compression_type)\n      return dataset\n\n    batch_size = params[\'batch_size\']\n    compression_type = tf_utils.compression_type_of_files(self.input_files)\n\n    # NOTE: The order of the file names returned can be non-deterministic,\n    # even if shuffle is false.  See internal and the note in internal.\n    # We need the shuffle flag to be able to disable reordering in EVAL mode.\n    dataset = None\n    for pattern in self.input_file_spec.split(\',\'):\n      one_dataset = tf.data.Dataset.list_files(\n          sharded_file_utils.normalize_to_sharded_file_pattern(pattern),\n          shuffle=self.mode == tf.estimator.ModeKeys.TRAIN)\n      dataset = dataset.concatenate(one_dataset) if dataset else one_dataset\n\n    # This shuffle applies to the set of files.\n    # redacted\n    if (self.mode == tf.estimator.ModeKeys.TRAIN and\n        self.initial_shuffle_buffer_size > 0):\n      dataset = dataset.shuffle(self.initial_shuffle_buffer_size)\n\n    if self.mode == tf.estimator.ModeKeys.EVAL:\n      # When EVAL, avoid parallel reads for the sake of reproducibility.\n      dataset = dataset.interleave(\n          load_dataset, cycle_length=self.input_read_threads, block_length=1)\n    else:\n      dataset = dataset.apply(\n          # parallel_interleave requires tf 1.5 or later; this is\n          # necessary for good performance.\n          tf.data.experimental.parallel_interleave(\n              load_dataset,\n              cycle_length=self.input_read_threads,\n              sloppy=self.sloppy))\n\n    if self.max_examples is not None:\n      dataset = dataset.take(self.max_examples)\n\n    if self.mode == tf.estimator.ModeKeys.TRAIN:\n      dataset = dataset.repeat()\n\n    # This shuffle applies to the set of records.\n    if self.mode == tf.estimator.ModeKeys.TRAIN:\n      if self.shuffle_buffer_size > 0:\n        dataset = dataset.shuffle(self.shuffle_buffer_size)\n\n    dataset = dataset.apply(\n        tf.data.experimental.map_and_batch(\n            map_func=self.parse_tfexample,\n            batch_size=batch_size,\n            num_parallel_batches=_PREFETCH_BATCHES,\n            drop_remainder=True))\n\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n    return dataset\n\n  def prediction_input_fn(self, params):\n    """"""Implementation of `input_fn` contract for prediction mode.\n\n    Args:\n      params: a dict containing an integer value for key \'batch_size\'.\n\n    Returns:\n      the tuple (features, labels), where:\n        - features is a dict of Tensor-valued input features; keys populated\n          are:\n            \'image\'\n            \'variant\'\n            \'alt_allele_indices\'\n\n          Aside from \'image\', these may be encoded specially for TPU.\n    """"""\n\n    def load_dataset(filename):\n      dataset = tf.data.TFRecordDataset(\n          filename,\n          buffer_size=self.prefetch_dataset_buffer_size,\n          compression_type=compression_type)\n      return dataset\n\n    batch_size = params[\'batch_size\']\n    compression_type = tf_utils.compression_type_of_files(self.input_files)\n    files = tf.data.Dataset.list_files(\n        sharded_file_utils.normalize_to_sharded_file_pattern(\n            self.input_file_spec),\n        shuffle=False,\n    )\n    tf.compat.v1.logging.info(\'self.input_read_threads=%d\',\n                              self.input_read_threads)\n    dataset = files.apply(\n        tf.data.experimental.parallel_interleave(\n            load_dataset,\n            cycle_length=self.input_read_threads,\n            sloppy=self.sloppy))\n    tf.compat.v1.logging.info(\'self.input_map_threads=%d\',\n                              self.input_map_threads)\n    dataset = dataset.apply(\n        tf.data.experimental.map_and_batch(\n            self.parse_tfexample,\n            batch_size=batch_size,\n            num_parallel_batches=self.input_map_threads))\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    return dataset\n\n  def __str__(self):\n    return (\'DeepVariantInput(name={}, input_file_spec={}, num_examples={}, \'\n            \'mode={})\').format(self.name, self.input_file_spec,\n                               self.num_examples, self.mode)\n\n\n# This is the entry point to get a DeepVariantInput when you start with\n# a dataset configuration file name.\ndef get_input_fn_from_dataset(dataset_config_filename, mode, **kwargs):\n  """"""Creates an input_fn from the dataset config file.\n\n  Args:\n    dataset_config_filename: str. Path to the dataset config pbtxt file.\n    mode: one of tf.estimator.ModeKeys.{TRAIN,EVAL,PREDICT}\n    **kwargs: Additional keyword arguments for DeepVariantInput.\n\n  Returns:\n    An input_fn from the specified split in the dataset_config file.\n\n  Raises:\n    ValueError: if the dataset config doesn\'t have the necessary information.\n  """"""\n  # Get the metadata.\n  dataset_config = read_dataset_config(dataset_config_filename)\n  # Return a reader for the data.\n  return get_input_fn_from_filespec(\n      input_file_spec=dataset_config.tfrecord_path,\n      mode=mode,\n      num_examples=dataset_config.num_examples,\n      name=dataset_config.name,\n      **kwargs)\n\n\n# This is the entry point to get a DeepVariantInput when you start with\n# a tf.example file specification, and associated metadata.\ndef get_input_fn_from_filespec(input_file_spec, mode, **kwargs):\n  """"""Create a DeepVariantInput function object from a file spec.\n\n  Args:\n    input_file_spec: the tf.example input file specification, possibly sharded.\n    mode: tf.estimator.ModeKeys.\n    **kwargs: Additional keyword arguments for DeepVariantInput.\n\n  Returns:\n    A DeepVariantInput object usable as an input_fn.\n  """"""\n  return DeepVariantInput(mode=mode, input_file_spec=input_file_spec, **kwargs)\n\n\n# Return the stream of batched images from a dataset.\ndef get_batches(tf_dataset, model, batch_size):\n  """"""Provides batches of pileup images from this dataset.\n\n  Creates a DeepVariantInput for tf_dataset. It instantiates an iterator\n  on the dataset, and returns the images, labels, encoded_variant\n  features in batches. This calls model.preprocess_images on the images\n  (but note that we will be moving that step into model_fn for the\n  Estimator api).\n\n  Args:\n    tf_dataset: a DeepVariantInput object\n    model: a model object\n    batch_size: int batch size\n\n  Returns:\n    (images, labels, encoded_variant)\n\n  Raises:\n    ValueError: if the dataset has the wrong mode.\n  """"""\n  if tf_dataset.mode not in (tf.estimator.ModeKeys.TRAIN,\n                             tf.estimator.ModeKeys.EVAL):\n    raise ValueError(\n        \'tf_dataset.mode is {} but must be one of TRAIN or EVAL.\'.format(\n            tf_dataset.mode))\n\n  params = dict(batch_size=batch_size)\n  features, labels = tf.compat.v1.data.make_one_shot_iterator(\n      tf_dataset(params)).get_next()\n\n  images = features[\'image\']\n  encoded_variant = features[\'variant\']\n\n  images = model.preprocess_images(images)\n  return images, labels, encoded_variant\n\n\n# This reads a pbtxt file and returns the config proto.\ndef read_dataset_config(dataset_config_filename):\n  """"""Returns a DeepVariantDatasetConfig proto read from the dataset config file.\n\n  Args:\n    dataset_config_filename: String. Path to the dataset config pbtxt file.\n\n  Returns:\n    A DeepVariantDatasetConfig proto from the dataset_config file.\n\n  Raises:\n    ValueError: if the dataset config doesn\'t have the necessary information.\n  """"""\n  with tf.io.gfile.GFile(dataset_config_filename) as f:\n    dataset_config = text_format.Parse(\n        f.read(), deepvariant_pb2.DeepVariantDatasetConfig())\n\n  if not dataset_config.name:\n    raise ValueError(\'dataset_config needs to have a name\')\n\n  if not dataset_config.tfrecord_path:\n    raise ValueError(\'The dataset in the config {} does not have a \'\n                     \'tfrecord_path.\'.format(dataset_config_filename))\n\n  # redacted\n  # of num_examples.\n  if not dataset_config.num_examples:\n    raise ValueError(\'The dataset in the config {} does not have a \'\n                     \'num_examples.\'.format(dataset_config_filename))\n\n  return dataset_config\n\n\ndef write_dataset_config_to_pbtxt(dataset_config, dataset_config_filename):\n  """"""Writes the dataset_config to a human-readable text format.\n\n  Args:\n    dataset_config: DeepVariantDatasetConfig. The config to be written out.\n    dataset_config_filename: String. Path to the output pbtxt file.\n  """"""\n  with tf.io.gfile.GFile(dataset_config_filename, mode=\'w\') as writer:\n    writer.write(text_format.MessageToString(dataset_config))\n'"
deepvariant/data_providers_test.py,38,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for learning.genomics.deepvariant.data_provider.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport math\n\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import variant_utils\nfrom tensorflow.core.example import example_pb2\nfrom deepvariant import data_providers\nfrom deepvariant import dv_constants\nfrom deepvariant import testdata\nfrom deepvariant import tf_utils\nfrom deepvariant.protos import deepvariant_pb2\n\n\ndef setUpModule():\n  testdata.init()\n\n\n# Return a DeepVariantInput attached to the golden training data.\n# Run with shuffling off, and in eval mode.\ndef make_golden_dataset(compressed_inputs=False,\n                        mode=tf.estimator.ModeKeys.EVAL,\n                        use_tpu=False):\n  if compressed_inputs:\n    source_path = test_utils.test_tmpfile(\'make_golden_dataset.tfrecord.gz\')\n    tfrecord.write_tfrecords(\n        tfrecord.read_tfrecords(testdata.GOLDEN_TRAINING_EXAMPLES), source_path)\n  else:\n    source_path = testdata.GOLDEN_TRAINING_EXAMPLES\n  return data_providers.get_input_fn_from_filespec(\n      input_file_spec=source_path,\n      num_examples=testdata.N_GOLDEN_TRAINING_EXAMPLES,\n      name=\'labeled_golden\',\n      mode=mode,\n      tensor_shape=None,\n      use_tpu=use_tpu)\n\n\ndef _test_dataset_config(filename, **kwargs):\n  """"""Creates a DeepVariantDatasetConfig(**kwargs) and writes it to filename.""""""\n  dataset_config_pbtext_filename = test_utils.test_tmpfile(filename)\n  dataset_config = deepvariant_pb2.DeepVariantDatasetConfig(**kwargs)\n  data_providers.write_dataset_config_to_pbtxt(dataset_config,\n                                               dataset_config_pbtext_filename)\n  return dataset_config_pbtext_filename\n\n\nclass DataProviderTest(parameterized.TestCase):\n\n  def test_get_dataset(self):\n    dataset_config_pbtext_filename = _test_dataset_config(\n        \'golden.dataset_config.pbtxt\',\n        name=\'some_dataset_name\',\n        tfrecord_path=\'/dev/null\',\n        num_examples=1000)\n    ds = data_providers.get_input_fn_from_dataset(\n        dataset_config_pbtext_filename,\n        mode=tf.estimator.ModeKeys.EVAL,\n        tensor_shape=[3, 4, dv_constants.PILEUP_NUM_CHANNELS])\n\n    self.assertEqual(\'some_dataset_name\', ds.name)\n    self.assertEqual(\'/dev/null\', ds.input_file_spec)\n    self.assertEqual(1000, ds.num_examples)\n    self.assertEqual([3, 4, dv_constants.PILEUP_NUM_CHANNELS], ds.tensor_shape)\n\n  def test_get_dataset_raises_error_for_empty_name(self):\n    dataset_config_pbtext_filename = _test_dataset_config(\n        \'test_get_dataset_raises_error_for_empty_name.pbtxt\')\n    with six.assertRaisesRegex(self, ValueError,\n                               \'dataset_config needs to have a name\'):\n      data_providers.get_input_fn_from_dataset(\n          dataset_config_pbtext_filename, mode=tf.estimator.ModeKeys.EVAL)\n\n  def test_get_dataset_raises_error_for_empty_data_split(self):\n    dataset_config_pbtext_filename = _test_dataset_config(\n        \'test_get_dataset_raises_error_for_empty_data_split.pbtxt\',\n        name=\'some_dataset_name\')\n    expected_exception_message = (\n        \'The dataset in the config {} does not \'\n        \'have a tfrecord_path.\'.format(dataset_config_pbtext_filename))\n    with six.assertRaisesRegex(self, ValueError, expected_exception_message):\n      data_providers.get_input_fn_from_dataset(\n          dataset_config_pbtext_filename, mode=tf.estimator.ModeKeys.EVAL)\n\n  def test_get_dataset_raises_error_for_empty_num_examples(self):\n    dataset_config_pbtext_filename = _test_dataset_config(\n        \'test_get_dataset_raises_error_for_empty_num_examples.pbtxt\',\n        name=\'some_dataset_name\',\n        tfrecord_path=\'/path/to/dataset\')\n    expected_exception_message = (\n        \'The dataset in the config {} does not have \'\n        \'a num_examples.\'.format(dataset_config_pbtext_filename))\n    with six.assertRaisesRegex(self, ValueError, expected_exception_message):\n      data_providers.get_input_fn_from_dataset(\n          dataset_config_pbtext_filename, mode=tf.estimator.ModeKeys.EVAL)\n\n  def test_dataset_definition(self):\n    ds = data_providers.DeepVariantInput(\n        mode=tf.estimator.ModeKeys.PREDICT,\n        name=\'name\',\n        input_file_spec=\'test.tfrecord\',\n        num_examples=10,\n        num_classes=dv_constants.NUM_CLASSES,\n        tensor_shape=[11, 13, dv_constants.PILEUP_NUM_CHANNELS])\n    self.assertEqual(\'name\', ds.name)\n    self.assertEqual(\'test.tfrecord\', ds.input_file_spec)\n    self.assertEqual(10, ds.num_examples)\n    self.assertEqual(dv_constants.NUM_CLASSES, ds.num_classes)\n    self.assertEqual([11, 13, dv_constants.PILEUP_NUM_CHANNELS],\n                     ds.tensor_shape)\n\n  def assertTfDataSetExamplesMatchExpected(self,\n                                           input_fn,\n                                           expected_dataset,\n                                           use_tpu=False,\n                                           workaround_list_files=False):\n    # Note that we use input_fn to get an iterator, while we use\n    # expected_dataset to get a filename, even though they are the same\n    # type (DeepVariantInput), and may even be the same object.\n    with tf.compat.v1.Session() as sess:\n      params = {\'batch_size\': 1}\n      batch_feed = tf.compat.v1.data.make_one_shot_iterator(\n          input_fn(params)).get_next()\n\n      sess.run(tf.compat.v1.global_variables_initializer())\n      sess.run(tf.compat.v1.local_variables_initializer())\n      seen = []\n      while True:\n        try:\n          features, _ = sess.run(batch_feed)\n        except tf.errors.OutOfRangeError:\n          break\n        locus = features[\'locus\'][0]\n        if use_tpu:\n          locus = tf_utils.int_tensor_to_string(locus)\n        # NB, this looks like: array([\'chr20:10001019-10001019\'], dtype=object)\n        seen.append(locus)\n\n    if workaround_list_files:\n      # This really only works for loci, because those are string valued and\n      # are expected to show up in sorted order.  For arbitrary data that\'s\n      # not true.  In prod we have the version of tf that lets us turn off\n      # shuffling so this path is skipped, but kokoro hits this.\n      seen = sorted(seen)\n\n    expected_loci = [\n        example.features.feature[\'locus\'].bytes_list.value[0]\n        for example in tfrecord.read_tfrecords(expected_dataset.input_file_spec)\n    ]\n    self.assertLen(expected_loci, expected_dataset.num_examples)\n    if seen != expected_loci:\n      print(\'\\n\\nlen expected seen\', len(expected_loci), len(seen))\n      print(\'\\n\\nexpected=\', expected_loci)\n      print(\'\\n\\nseen=\', seen)\n    self.assertEqual(expected_loci, seen)\n    # Note that this expected shape comes from the golden dataset. If the data\n    # is remade in the future, the values might need to be modified accordingly.\n    self.assertEqual(dv_constants.PILEUP_DEFAULT_DIMS,\n                     expected_dataset.tensor_shape)\n\n  # pylint: disable=g-complex-comprehension\n  @parameterized.parameters(\n      dict(compressed_inputs=compressed_inputs, use_tpu=use_tpu)\n      for compressed_inputs in [True, False]\n      for use_tpu in [True, False])\n  # pylint: enable=g-complex-comprehension\n  def test_reading_dataset(self, compressed_inputs, use_tpu):\n    golden_dataset = make_golden_dataset(compressed_inputs, use_tpu=use_tpu)\n    self.assertTfDataSetExamplesMatchExpected(\n        input_fn=golden_dataset,\n        expected_dataset=golden_dataset,\n        use_tpu=use_tpu)\n\n  # It looks like tf.data.Dataset.list_files is potentially nondeterministic.\n  # There\'s no guaranteed way to get around that (yet, internal).\n  # A list_files() flag I want is only available in tf 1.7,\n  # so for the short term, work around the problem by asking\n  # self.assertTfDataSetExamplesMatchExpected to sort the\n  # loci it sees.  That doesn\'t generalize well, but we should\n  # be able to fix this soon.\n  # pylint: disable=g-complex-comprehension\n  @parameterized.parameters(\n      dict(compressed_inputs=compressed_inputs, use_tpu=use_tpu)\n      for compressed_inputs in [True, False]\n      for use_tpu in [True, False])\n  # pylint: enable=g-complex-comprehension\n  def test_reading_sharded_dataset(self, compressed_inputs, use_tpu):\n    golden_dataset = make_golden_dataset(compressed_inputs, use_tpu=use_tpu)\n    n_shards = 3\n    sharded_path = test_utils.test_tmpfile(\'sharded@{}\'.format(n_shards))\n    tfrecord.write_tfrecords(\n        tfrecord.read_tfrecords(golden_dataset.input_file_spec), sharded_path)\n\n    config_file = _test_dataset_config(\n        \'test_sharded.pbtxt\',\n        name=\'sharded_test\',\n        tfrecord_path=sharded_path,\n        num_examples=golden_dataset.num_examples)\n\n    self.assertTfDataSetExamplesMatchExpected(\n        data_providers.get_input_fn_from_dataset(\n            config_file, mode=tf.estimator.ModeKeys.EVAL),\n        golden_dataset,\n        # workaround_list_files is needed because wildcards, and so sharded\n        # files, are nondeterministicly ordered (for now).\n        workaround_list_files=True,\n    )\n\n  @parameterized.parameters(\n      dict(compressed_inputs=compressed_inputs, mode=mode, use_tpu=use_tpu)\n      for compressed_inputs in [True, False] for use_tpu in [True, False]\n      for mode in [\'TRAIN\', \'EVAL\'])\n  def test_get_batches(self, compressed_inputs, mode, use_tpu):\n    mode = (\n        tf.estimator.ModeKeys.EVAL\n        if mode == \'EVAL\' else tf.estimator.ModeKeys.TRAIN)\n    input_fn = make_golden_dataset(\n        compressed_inputs, mode=mode, use_tpu=use_tpu)\n    batch_size = 16\n    with tf.compat.v1.Session() as sess:\n      batch = tf.compat.v1.data.make_one_shot_iterator(\n          input_fn(dict(batch_size=batch_size))).get_next()\n\n      # Get our images, labels, and variants for further testing.\n      sess.run(tf.compat.v1.global_variables_initializer())\n      features, labels = sess.run(batch)\n      variants = features[\'variant\']\n      images = features[\'image\']\n\n      # Checks that our labels are the right shape and are one-hot encoded.\n      # Note that the shape is 100, not 107, because we only adjust the image\n      # in the model_fn now, where previously it was done in the input_fn.\n      self.assertEqual([batch_size] + dv_constants.PILEUP_DEFAULT_DIMS,\n                       list(images.shape))\n      self.assertEqual((batch_size,), labels.shape)\n      for label in labels:\n        # pylint: disable=g-generic-assert\n        self.assertTrue(0 <= label < dv_constants.NUM_CLASSES)\n\n      # Check that our variants has the shape we expect and actually contain\n      # variants by decoding them and checking the reference_name.\n      self.assertEqual(batch_size, variants.shape[0])\n      for variant in variants:\n        if use_tpu:\n          variant = tf_utils.int_tensor_to_string(variant)\n        for v in variant_utils.decode_variants([variant]):\n          self.assertEqual(v.reference_name, \'chr20\')\n\n  @parameterized.parameters(\n      (\'test_shape.gz\', \'test_shape.gz\'),\n      (\'test_shape-00000-of-00001.gz\', \'test_shape@1.gz\'),\n      (\'test_shape-00000-of-00001.gz\', \'test_shape-?????-of-00001.gz\'),\n      (\'test_shape-00000-of-00001.gz\', \'test_shape-*.gz\'), (\'output\', \'output\'),\n      (\'test_shape-00000-of-00001\', \'test_shape@1\'),\n      (\'test_shape-00000-of-00001\', \'test_shape-?????-of-00001\'),\n      (\'test_shape-00000-of-00001\', \'test_shape-*\'))\n  def test_get_shape_from_examples_path(self, file_name_to_write,\n                                        tfrecord_path_to_match):\n    example = example_pb2.Example()\n    valid_shape = [1, 2, 3]\n    example.features.feature[\'image/shape\'].int64_list.value.extend(valid_shape)\n    output_file = test_utils.test_tmpfile(file_name_to_write)\n    tfrecord.write_tfrecords([example], output_file)\n    ds = data_providers.DeepVariantInput(\n        mode=tf.estimator.ModeKeys.PREDICT,\n        name=\'test_shape\',\n        input_file_spec=test_utils.test_tmpfile(tfrecord_path_to_match),\n        num_examples=1)\n    self.assertEqual(valid_shape, ds.tensor_shape)\n\n  def test_get_shape_from_examples_path_invalid_path(self):\n    with six.assertRaisesRegex(self, Exception, \'/this/path/does/not\'):\n      data_providers.DeepVariantInput(\n          mode=tf.estimator.ModeKeys.PREDICT,\n          name=\'test_invalid_path\',\n          input_file_spec=\'/this/path/does/not/exist\',\n          num_examples=1)\n\n  # pylint: disable=g-complex-comprehension\n  @parameterized.parameters(\n      dict(max_examples=max_examples, batch_size=batch_size)\n      for max_examples in [2, 4, 8]\n      for batch_size in [4, 8, 16])\n  # pylint: enable=g-complex-comprehension\n  def test_max_examples(self, max_examples, batch_size):\n    input_fn = data_providers.get_input_fn_from_filespec(\n        input_file_spec=testdata.GOLDEN_TRAINING_EXAMPLES,\n        num_examples=testdata.N_GOLDEN_TRAINING_EXAMPLES,\n        name=\'labeled_golden\',\n        max_examples=max_examples,\n        mode=tf.estimator.ModeKeys.TRAIN)\n\n    n_batches_to_read = 100\n    with tf.compat.v1.Session() as sess:\n      sess.run(tf.compat.v1.global_variables_initializer())\n\n      iterator = tf.compat.v1.data.make_one_shot_iterator(\n          input_fn(dict(batch_size=batch_size)))\n      next_element = iterator.get_next()\n\n      def read_loci_in_batches():\n        features, _ = sess.run(next_element)\n        return features[\'locus\']\n\n      batches = [read_loci_in_batches() for _ in range(n_batches_to_read)]\n      # pylint: disable=g-complex-comprehension\n      unique_loci = {locus for batch in batches for locus in batch}\n      # pylint: enable=g-complex-comprehension\n      # assertLen not available OSS.\n      # pylint: disable=g-generic-assert\n      self.assertEqual(len(unique_loci), max_examples)\n      # pylint: enable=g-generic-assert\n\n  @parameterized.parameters(\n      # When max_examples is None, dataset.num_examples will equal num_examples\n      # arg.\n      dict(num_examples=10, max_examples=None, expected=10),\n      # When max_examples is larger than num_examples, dataset.num_examples will\n      # equal the smaller value.\n      dict(num_examples=10, max_examples=100, expected=10),\n      # When max_examples is smaller than num_examples, dataset.num_examples\n      # will equal the smaller max_examples value.\n      dict(num_examples=10, max_examples=5, expected=5),\n      # When num_examples isn\'t provided (None), but max_examples is, we don\'t\n      # update num_examples so it remains None.\n      dict(num_examples=None, max_examples=5, expected=None),\n  )\n  def test_max_examples_overrides_num_examples(self, num_examples, max_examples,\n                                               expected):\n    dataset = data_providers.DeepVariantInput(\n        # Use predict mode so we can have num_examples == None.\n        mode=tf.estimator.ModeKeys.PREDICT,\n        input_file_spec=testdata.GOLDEN_TRAINING_EXAMPLES,\n        num_examples=num_examples,\n        max_examples=max_examples)\n    self.assertEqual(expected, dataset.num_examples)\n\n  def test_features_extraction_spec_for_mode(self):\n    dataset = make_golden_dataset()\n\n    shared_feature_names = {\n        \'image/encoded\', \'variant/encoded\', \'alt_allele_indices/encoded\',\n        \'variant_type\', \'sequencing_type\'\n    }\n    self.assertEqual(\n        shared_feature_names,\n        set(\n            dataset.features_extraction_spec_for_mode(\n                include_label_and_locus=False).keys()))\n    self.assertEqual(\n        shared_feature_names.union({\'label\', \'locus\'}),\n        set(\n            dataset.features_extraction_spec_for_mode(\n                include_label_and_locus=True).keys()))\n\n\nclass InputTest(\n    six.with_metaclass(parameterized.TestGeneratorMetaclass, tf.test.TestCase)):\n  """"""Tests of input_fn, doing end-to-end I/O.\n\n  These tests instantiate an input stream and then check it in various ways,\n  in increasing complexity.\n  """"""\n\n  def get_batch_feed(self, batch_size=1, use_tpu=False):\n    # This is an input_fn reading test_utils.N_GOLDEN_CALLING_EXAMPLES records.\n    # Use PREDICT mode so we get finite input.\n    dvi = data_providers.DeepVariantInput(\n        mode=tf.estimator.ModeKeys.PREDICT,\n        input_file_spec=testdata.GOLDEN_CALLING_EXAMPLES,\n        num_examples=testdata.N_GOLDEN_CALLING_EXAMPLES,\n        tensor_shape=None,\n        use_tpu=use_tpu)\n    params = {\'batch_size\': batch_size}\n    batch_feed = tf.compat.v1.data.make_one_shot_iterator(\n        dvi(params)).get_next()\n    return batch_feed\n\n  def check_batch_feed(self, batch_feed, use_tpu, expected_batch_size,\n                       expected_n_batches):\n    # Consume batch_feed, check that the right number of things is seen.\n    with tf.compat.v1.Session() as sess:\n      sess.run(tf.compat.v1.local_variables_initializer())\n      sess.run(tf.compat.v1.global_variables_initializer())\n\n      n = 0\n      n_valid_entries = 0\n      while True:\n        try:\n          features = sess.run(batch_feed)\n        except tf.errors.OutOfRangeError:\n          break\n        n += 1\n        a = features[\'image\']  # np.ndarray\n        self.assertIsNot(a, None)\n        if use_tpu:\n          self.assertEqual(a.dtype, np.dtype(\'int32\'))\n        else:\n          self.assertEqual(a.dtype, np.dtype(\'uint8\'))\n        current_batch_size = a.shape[0]\n        self.assertLessEqual(current_batch_size, expected_batch_size)\n        self.assertEqual(\n            list(a.shape),\n            [current_batch_size] + dv_constants.PILEUP_DEFAULT_DIMS)\n        n_valid_entries += current_batch_size\n\n      self.assertEqual(expected_n_batches, n)\n      self.assertEqual(testdata.N_GOLDEN_CALLING_EXAMPLES, n_valid_entries)\n\n  @parameterized.parameters(False, True)\n  def testInputStream(self, use_tpu):\n    # Read batch_feed one at a time, check the shape of each, and the\n    # total count.\n    batch_size = 1\n    batch_feed = self.get_batch_feed(batch_size=batch_size, use_tpu=use_tpu)\n    expected_n_batches = math.ceil(\n        float(testdata.N_GOLDEN_CALLING_EXAMPLES) / batch_size)\n    self.check_batch_feed(batch_feed, use_tpu, batch_size, expected_n_batches)\n\n  @parameterized.parameters(False, True)\n  def testBatching(self, use_tpu):\n    # Test reading with a larger batch size.  Similar to testInputStream,\n    # but note that the last batch may be truncated when not in predict mode,\n    # so current_batch_size has to be recovered from the actual output.\n    batch_size = 1024\n    batch_feed = self.get_batch_feed(batch_size=batch_size, use_tpu=use_tpu)\n    expected_n_batches = math.ceil(\n        float(testdata.N_GOLDEN_CALLING_EXAMPLES) / batch_size)\n    self.check_batch_feed(batch_feed, use_tpu, batch_size, expected_n_batches)\n\n  @parameterized.parameters(False, True)\n  def testGoldenCallingExamples(self, use_tpu):\n    # Read the golden calling examples, and read the batch_feed instantiated\n    # from the golden calling examples, and ensure that we get the same\n    # parsed records in both cases.\n\n    # Read and parse the canonical data.\n    expected_decoded_records = list(\n        tfrecord.read_tfrecords(\n            testdata.GOLDEN_CALLING_EXAMPLES, proto=example_pb2.Example))\n\n    # Read and parse the data using tf.  This is the function under test,\n    # although we indirectly check parse_tfexample as well.\n    batch_feed = self.get_batch_feed(batch_size=1, use_tpu=use_tpu)\n\n    with tf.compat.v1.Session() as sess:\n      sess.run(tf.compat.v1.local_variables_initializer())\n      sess.run(tf.compat.v1.global_variables_initializer())\n\n      n = 0\n      while True:\n        # Read from batch.\n        try:\n          features = sess.run(batch_feed)\n        except tf.errors.OutOfRangeError:\n          break\n\n        # Get the corresponding parsed golden example.\n        example = expected_decoded_records[n]\n        expected_alt_allele_indices_encoded = example.features.feature[\n            \'alt_allele_indices/encoded\'].bytes_list.value[0]\n        expected_variant_encoded = example.features.feature[\n            \'variant/encoded\'].bytes_list.value[0]\n        expected_sequencing_type = example.features.feature[\n            \'sequencing_type\'].int64_list.value[0]\n\n        # Compare against the parsed batch feed.\n\n        a = features[\'image\'][0]  # np.ndarray\n        self.assertEqual(list(a.shape), dv_constants.PILEUP_DEFAULT_DIMS)\n        self.assertIsNotNone(a)\n        if use_tpu:\n          self.assertEqual(a.dtype, np.dtype(\'int32\'))\n        else:\n          self.assertEqual(a.dtype, np.dtype(\'uint8\'))\n\n        a = features[\'alt_allele_indices\'][0]\n        if use_tpu:\n          self.assertEqual(a.dtype, np.dtype(\'int32\'))\n          self.assertEqual(a.shape, (tf_utils.STRING_TO_INT_BUFFER_LENGTH,))\n          actual_alt_allele_indices_encoded = tf_utils.int_tensor_to_string(a)\n        else:\n          self.assertIsInstance(a, six.binary_type)\n          actual_alt_allele_indices_encoded = a\n        self.assertEqual(expected_alt_allele_indices_encoded,\n                         actual_alt_allele_indices_encoded)\n\n        a = features[\'variant\'][0]\n        if use_tpu:\n          self.assertEqual(a.dtype, np.dtype(\'int32\'))\n          self.assertEqual(a.shape, (tf_utils.STRING_TO_INT_BUFFER_LENGTH,))\n          actual_variant_encoded = tf_utils.int_tensor_to_string(a)\n        else:\n          self.assertIsInstance(a, six.binary_type)\n          actual_variant_encoded = a\n        self.assertEqual(expected_variant_encoded, actual_variant_encoded)\n\n        self.assertEqual(features[\'sequencing_type\'], expected_sequencing_type)\n\n        n += 1\n\n      self.assertEqual(n, testdata.N_GOLDEN_CALLING_EXAMPLES)\n\n\nif __name__ == \'__main__\':\n  tf.compat.v1.disable_eager_execution()\n  absltest.main()\n'"
deepvariant/dv_constants.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Common constants shared across DeepVariant\'s codebase.\n\nThis file is for very general constants in the code that end up needing to be\naccessed in a variety of places, often in live code as well as throughout the\ncode in tests.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# Default width [in basepairs] for our DeepVariant data tensor.\nPILEUP_DEFAULT_WIDTH = 221\n\n# Default height [in rows] for our DeepVariant data tensor.\nPILEUP_DEFAULT_HEIGHT = 100\n\n# Not a default because it\'s hard-coded into the code.\nPILEUP_NUM_CHANNELS = 6\n\n# The dimensions of a pileup image tensor as height x width x rank.\nPILEUP_DEFAULT_DIMS = [\n    PILEUP_DEFAULT_HEIGHT, PILEUP_DEFAULT_WIDTH, PILEUP_NUM_CHANNELS\n]\n\n# Number of classes represented in the data set. The three classes are\n# homozygous reference (0), heterozygous (1) and homozygous alternative (2).\nNUM_CLASSES = 3\n\n# Default sample name if no sample name is found from the BAM header.\nDEFAULT_SAMPLE_NAME = \'default\'\n'"
deepvariant/dv_vcf_constants.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Library for generating VCF information created by DeepVariant.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import vcf_constants\n\n# FILTER field IDs.\nDEEP_VARIANT_PASS = \'PASS\'\nDEEP_VARIANT_REF_FILTER = \'RefCall\'\nDEEP_VARIANT_QUAL_FILTER = \'LowQual\'\n\n# FORMAT field IDs.\nDEEP_VARIANT_MIN_DP_FORMAT = \'MIN_DP\'\nDEEP_VARIANT_VAF_FORMAT = \'VAF\'\n\n\ndef deepvariant_header(contigs, sample_names):\n  """"""Returns a VcfHeader used for writing VCF output.\n\n  This function fills out the FILTER, INFO, FORMAT, and extra header information\n  created by the DeepVariant pipeline using consistent fields that DeepVariant\n  creates. The `contigs` and `sample_names` fields are unique depending on the\n  input data used, so are required inputs.\n\n  Args:\n    contigs: list(ContigInfo). The list of contigs on which variants were\n      called.\n    sample_names: list(str). The list of samples present in the run.\n\n  Returns:\n    A nucleus.genomics.v1.VcfHeader proto with known fixed headers and the given\n    samples and contigs populated.\n  """"""\n  return variants_pb2.VcfHeader(\n      fileformat=\'VCFv4.2\',\n      filters=[\n          vcf_constants.reserved_filter_field(DEEP_VARIANT_PASS),\n          variants_pb2.VcfFilterInfo(\n              id=DEEP_VARIANT_REF_FILTER,\n              description=\'Genotyping model thinks this site is reference.\'),\n          variants_pb2.VcfFilterInfo(\n              id=DEEP_VARIANT_QUAL_FILTER,\n              description=\'Confidence in this variant being real is below \'\n              \'calling threshold.\'),\n      ],\n      infos=[\n          vcf_constants.reserved_info_field(\'END\'),\n      ],\n      formats=[\n          vcf_constants.reserved_format_field(\'GT\'),\n          vcf_constants.reserved_format_field(\'GQ\'),\n          vcf_constants.reserved_format_field(\'DP\'),\n          variants_pb2.VcfFormatInfo(\n              id=DEEP_VARIANT_MIN_DP_FORMAT,\n              number=\'1\',\n              type=\'Integer\',\n              description=\'Minimum DP observed within the GVCF block.\'),\n          vcf_constants.reserved_format_field(\'AD\'),\n          variants_pb2.VcfFormatInfo(\n              id=DEEP_VARIANT_VAF_FORMAT,\n              number=\'A\',\n              type=\'Float\',\n              description=\'Variant allele fractions.\'),\n          vcf_constants.reserved_format_field(\'PL\'),\n      ],\n      contigs=contigs,\n      sample_names=sample_names,\n  )\n'"
deepvariant/dv_vcf_constants_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .dv_vcf_constants.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport six\nfrom third_party.nucleus.protos import reference_pb2\nfrom deepvariant import dv_vcf_constants\n\n\nclass DvVcfConstantsTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      dict(contigs=[], sample_names=[]),\n      dict(\n          contigs=[reference_pb2.ContigInfo(name=\'chr1\')],\n          sample_names=[\'single_sample\']),\n      dict(\n          contigs=[\n              reference_pb2.ContigInfo(name=\'1\'),\n              reference_pb2.ContigInfo(name=\'2\')\n          ],\n          sample_names=[\'multiple\', \'samples\']),\n  )\n  def test_deepvariant_header(self, contigs, sample_names):\n    header = dv_vcf_constants.deepvariant_header(\n        contigs=contigs, sample_names=sample_names)\n    six.assertCountEqual(self, header.contigs, contigs)\n    six.assertCountEqual(self, header.sample_names, sample_names)\n    self.assertGreater(len(header.filters), 0)\n    self.assertGreater(len(header.infos), 0)\n    self.assertGreater(len(header.formats), 0)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/exclude_contigs.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""List of common problematic contigs in b37 and b38 of human genome reference.\n\nReads which map to these contigs are ignored and so DeepVariant skips over them\nto not waste processing time. Important to note that we only include the\ncommon canonical references for human sequencing here.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pyformat: disable\nEXCLUDED_HUMAN_CONTIGS = [\n    # The two canonical names for the contig representing the human\n    # mitochondrial sequence.\n    \'chrM\',\n    \'MT\',\n    # From hs37d5.\n    # (ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/README_human_reference_20110707)  # pylint:disable=line-too-long\n    \'GL000207.1\',\n    \'GL000226.1\',\n    \'GL000229.1\',\n    \'GL000231.1\',\n    \'GL000210.1\',\n    \'GL000239.1\',\n    \'GL000235.1\',\n    \'GL000201.1\',\n    \'GL000247.1\',\n    \'GL000245.1\',\n    \'GL000197.1\',\n    \'GL000203.1\',\n    \'GL000246.1\',\n    \'GL000249.1\',\n    \'GL000196.1\',\n    \'GL000248.1\',\n    \'GL000244.1\',\n    \'GL000238.1\',\n    \'GL000202.1\',\n    \'GL000234.1\',\n    \'GL000232.1\',\n    \'GL000206.1\',\n    \'GL000240.1\',\n    \'GL000236.1\',\n    \'GL000241.1\',\n    \'GL000243.1\',\n    \'GL000242.1\',\n    \'GL000230.1\',\n    \'GL000237.1\',\n    \'GL000233.1\',\n    \'GL000204.1\',\n    \'GL000198.1\',\n    \'GL000208.1\',\n    \'GL000191.1\',\n    \'GL000227.1\',\n    \'GL000228.1\',\n    \'GL000214.1\',\n    \'GL000221.1\',\n    \'GL000209.1\',\n    \'GL000218.1\',\n    \'GL000220.1\',\n    \'GL000213.1\',\n    \'GL000211.1\',\n    \'GL000199.1\',\n    \'GL000217.1\',\n    \'GL000216.1\',\n    \'GL000215.1\',\n    \'GL000205.1\',\n    \'GL000219.1\',\n    \'GL000224.1\',\n    \'GL000223.1\',\n    \'GL000195.1\',\n    \'GL000212.1\',\n    \'GL000222.1\',\n    \'GL000200.1\',\n    \'GL000193.1\',\n    \'GL000194.1\',\n    \'GL000225.1\',\n    \'GL000192.1\',\n    \'NC_007605\',\n    \'hs37d5\',\n    # From GRCh38.\n    # (ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/README.20150309.GRCh38_full_analysis_set_plus_decoy_hla)  # pylint:disable=line-too-long\n    \'chr1_KI270706v1_random\',\n    \'chr1_KI270707v1_random\',\n    \'chr1_KI270708v1_random\',\n    \'chr1_KI270709v1_random\',\n    \'chr1_KI270710v1_random\',\n    \'chr1_KI270711v1_random\',\n    \'chr1_KI270712v1_random\',\n    \'chr1_KI270713v1_random\',\n    \'chr1_KI270714v1_random\',\n    \'chr2_KI270715v1_random\',\n    \'chr2_KI270716v1_random\',\n    \'chr3_GL000221v1_random\',\n    \'chr4_GL000008v2_random\',\n    \'chr5_GL000208v1_random\',\n    \'chr9_KI270717v1_random\',\n    \'chr9_KI270718v1_random\',\n    \'chr9_KI270719v1_random\',\n    \'chr9_KI270720v1_random\',\n    \'chr11_KI270721v1_random\',\n    \'chr14_GL000009v2_random\',\n    \'chr14_GL000225v1_random\',\n    \'chr14_KI270722v1_random\',\n    \'chr14_GL000194v1_random\',\n    \'chr14_KI270723v1_random\',\n    \'chr14_KI270724v1_random\',\n    \'chr14_KI270725v1_random\',\n    \'chr14_KI270726v1_random\',\n    \'chr15_KI270727v1_random\',\n    \'chr16_KI270728v1_random\',\n    \'chr17_GL000205v2_random\',\n    \'chr17_KI270729v1_random\',\n    \'chr17_KI270730v1_random\',\n    \'chr22_KI270731v1_random\',\n    \'chr22_KI270732v1_random\',\n    \'chr22_KI270733v1_random\',\n    \'chr22_KI270734v1_random\',\n    \'chr22_KI270735v1_random\',\n    \'chr22_KI270736v1_random\',\n    \'chr22_KI270737v1_random\',\n    \'chr22_KI270738v1_random\',\n    \'chr22_KI270739v1_random\',\n    \'chrY_KI270740v1_random\',\n    \'chrUn_KI270302v1\',\n    \'chrUn_KI270304v1\',\n    \'chrUn_KI270303v1\',\n    \'chrUn_KI270305v1\',\n    \'chrUn_KI270322v1\',\n    \'chrUn_KI270320v1\',\n    \'chrUn_KI270310v1\',\n    \'chrUn_KI270316v1\',\n    \'chrUn_KI270315v1\',\n    \'chrUn_KI270312v1\',\n    \'chrUn_KI270311v1\',\n    \'chrUn_KI270317v1\',\n    \'chrUn_KI270412v1\',\n    \'chrUn_KI270411v1\',\n    \'chrUn_KI270414v1\',\n    \'chrUn_KI270419v1\',\n    \'chrUn_KI270418v1\',\n    \'chrUn_KI270420v1\',\n    \'chrUn_KI270424v1\',\n    \'chrUn_KI270417v1\',\n    \'chrUn_KI270422v1\',\n    \'chrUn_KI270423v1\',\n    \'chrUn_KI270425v1\',\n    \'chrUn_KI270429v1\',\n    \'chrUn_KI270442v1\',\n    \'chrUn_KI270466v1\',\n    \'chrUn_KI270465v1\',\n    \'chrUn_KI270467v1\',\n    \'chrUn_KI270435v1\',\n    \'chrUn_KI270438v1\',\n    \'chrUn_KI270468v1\',\n    \'chrUn_KI270510v1\',\n    \'chrUn_KI270509v1\',\n    \'chrUn_KI270518v1\',\n    \'chrUn_KI270508v1\',\n    \'chrUn_KI270516v1\',\n    \'chrUn_KI270512v1\',\n    \'chrUn_KI270519v1\',\n    \'chrUn_KI270522v1\',\n    \'chrUn_KI270511v1\',\n    \'chrUn_KI270515v1\',\n    \'chrUn_KI270507v1\',\n    \'chrUn_KI270517v1\',\n    \'chrUn_KI270529v1\',\n    \'chrUn_KI270528v1\',\n    \'chrUn_KI270530v1\',\n    \'chrUn_KI270539v1\',\n    \'chrUn_KI270538v1\',\n    \'chrUn_KI270544v1\',\n    \'chrUn_KI270548v1\',\n    \'chrUn_KI270583v1\',\n    \'chrUn_KI270587v1\',\n    \'chrUn_KI270580v1\',\n    \'chrUn_KI270581v1\',\n    \'chrUn_KI270579v1\',\n    \'chrUn_KI270589v1\',\n    \'chrUn_KI270590v1\',\n    \'chrUn_KI270584v1\',\n    \'chrUn_KI270582v1\',\n    \'chrUn_KI270588v1\',\n    \'chrUn_KI270593v1\',\n    \'chrUn_KI270591v1\',\n    \'chrUn_KI270330v1\',\n    \'chrUn_KI270329v1\',\n    \'chrUn_KI270334v1\',\n    \'chrUn_KI270333v1\',\n    \'chrUn_KI270335v1\',\n    \'chrUn_KI270338v1\',\n    \'chrUn_KI270340v1\',\n    \'chrUn_KI270336v1\',\n    \'chrUn_KI270337v1\',\n    \'chrUn_KI270363v1\',\n    \'chrUn_KI270364v1\',\n    \'chrUn_KI270362v1\',\n    \'chrUn_KI270366v1\',\n    \'chrUn_KI270378v1\',\n    \'chrUn_KI270379v1\',\n    \'chrUn_KI270389v1\',\n    \'chrUn_KI270390v1\',\n    \'chrUn_KI270387v1\',\n    \'chrUn_KI270395v1\',\n    \'chrUn_KI270396v1\',\n    \'chrUn_KI270388v1\',\n    \'chrUn_KI270394v1\',\n    \'chrUn_KI270386v1\',\n    \'chrUn_KI270391v1\',\n    \'chrUn_KI270383v1\',\n    \'chrUn_KI270393v1\',\n    \'chrUn_KI270384v1\',\n    \'chrUn_KI270392v1\',\n    \'chrUn_KI270381v1\',\n    \'chrUn_KI270385v1\',\n    \'chrUn_KI270382v1\',\n    \'chrUn_KI270376v1\',\n    \'chrUn_KI270374v1\',\n    \'chrUn_KI270372v1\',\n    \'chrUn_KI270373v1\',\n    \'chrUn_KI270375v1\',\n    \'chrUn_KI270371v1\',\n    \'chrUn_KI270448v1\',\n    \'chrUn_KI270521v1\',\n    \'chrUn_GL000195v1\',\n    \'chrUn_GL000219v1\',\n    \'chrUn_GL000220v1\',\n    \'chrUn_GL000224v1\',\n    \'chrUn_KI270741v1\',\n    \'chrUn_GL000226v1\',\n    \'chrUn_GL000213v1\',\n    \'chrUn_KI270743v1\',\n    \'chrUn_KI270744v1\',\n    \'chrUn_KI270745v1\',\n    \'chrUn_KI270746v1\',\n    \'chrUn_KI270747v1\',\n    \'chrUn_KI270748v1\',\n    \'chrUn_KI270749v1\',\n    \'chrUn_KI270750v1\',\n    \'chrUn_KI270751v1\',\n    \'chrUn_KI270752v1\',\n    \'chrUn_KI270753v1\',\n    \'chrUn_KI270754v1\',\n    \'chrUn_KI270755v1\',\n    \'chrUn_KI270756v1\',\n    \'chrUn_KI270757v1\',\n    \'chrUn_GL000214v1\',\n    \'chrUn_KI270742v1\',\n    \'chrUn_GL000216v2\',\n    \'chrUn_GL000218v1\',\n    \'chr1_KI270762v1_alt\',\n    \'chr1_KI270766v1_alt\',\n    \'chr1_KI270760v1_alt\',\n    \'chr1_KI270765v1_alt\',\n    \'chr1_GL383518v1_alt\',\n    \'chr1_GL383519v1_alt\',\n    \'chr1_GL383520v2_alt\',\n    \'chr1_KI270764v1_alt\',\n    \'chr1_KI270763v1_alt\',\n    \'chr1_KI270759v1_alt\',\n    \'chr1_KI270761v1_alt\',\n    \'chr2_KI270770v1_alt\',\n    \'chr2_KI270773v1_alt\',\n    \'chr2_KI270774v1_alt\',\n    \'chr2_KI270769v1_alt\',\n    \'chr2_GL383521v1_alt\',\n    \'chr2_KI270772v1_alt\',\n    \'chr2_KI270775v1_alt\',\n    \'chr2_KI270771v1_alt\',\n    \'chr2_KI270768v1_alt\',\n    \'chr2_GL582966v2_alt\',\n    \'chr2_GL383522v1_alt\',\n    \'chr2_KI270776v1_alt\',\n    \'chr2_KI270767v1_alt\',\n    \'chr3_JH636055v2_alt\',\n    \'chr3_KI270783v1_alt\',\n    \'chr3_KI270780v1_alt\',\n    \'chr3_GL383526v1_alt\',\n    \'chr3_KI270777v1_alt\',\n    \'chr3_KI270778v1_alt\',\n    \'chr3_KI270781v1_alt\',\n    \'chr3_KI270779v1_alt\',\n    \'chr3_KI270782v1_alt\',\n    \'chr3_KI270784v1_alt\',\n    \'chr4_KI270790v1_alt\',\n    \'chr4_GL383528v1_alt\',\n    \'chr4_KI270787v1_alt\',\n    \'chr4_GL000257v2_alt\',\n    \'chr4_KI270788v1_alt\',\n    \'chr4_GL383527v1_alt\',\n    \'chr4_KI270785v1_alt\',\n    \'chr4_KI270789v1_alt\',\n    \'chr4_KI270786v1_alt\',\n    \'chr5_KI270793v1_alt\',\n    \'chr5_KI270792v1_alt\',\n    \'chr5_KI270791v1_alt\',\n    \'chr5_GL383532v1_alt\',\n    \'chr5_GL949742v1_alt\',\n    \'chr5_KI270794v1_alt\',\n    \'chr5_GL339449v2_alt\',\n    \'chr5_GL383530v1_alt\',\n    \'chr5_KI270796v1_alt\',\n    \'chr5_GL383531v1_alt\',\n    \'chr5_KI270795v1_alt\',\n    \'chr6_GL000250v2_alt\',\n    \'chr6_KI270800v1_alt\',\n    \'chr6_KI270799v1_alt\',\n    \'chr6_GL383533v1_alt\',\n    \'chr6_KI270801v1_alt\',\n    \'chr6_KI270802v1_alt\',\n    \'chr6_KB021644v2_alt\',\n    \'chr6_KI270797v1_alt\',\n    \'chr6_KI270798v1_alt\',\n    \'chr7_KI270804v1_alt\',\n    \'chr7_KI270809v1_alt\',\n    \'chr7_KI270806v1_alt\',\n    \'chr7_GL383534v2_alt\',\n    \'chr7_KI270803v1_alt\',\n    \'chr7_KI270808v1_alt\',\n    \'chr7_KI270807v1_alt\',\n    \'chr7_KI270805v1_alt\',\n    \'chr8_KI270818v1_alt\',\n    \'chr8_KI270812v1_alt\',\n    \'chr8_KI270811v1_alt\',\n    \'chr8_KI270821v1_alt\',\n    \'chr8_KI270813v1_alt\',\n    \'chr8_KI270822v1_alt\',\n    \'chr8_KI270814v1_alt\',\n    \'chr8_KI270810v1_alt\',\n    \'chr8_KI270819v1_alt\',\n    \'chr8_KI270820v1_alt\',\n    \'chr8_KI270817v1_alt\',\n    \'chr8_KI270816v1_alt\',\n    \'chr8_KI270815v1_alt\',\n    \'chr9_GL383539v1_alt\',\n    \'chr9_GL383540v1_alt\',\n    \'chr9_GL383541v1_alt\',\n    \'chr9_GL383542v1_alt\',\n    \'chr9_KI270823v1_alt\',\n    \'chr10_GL383545v1_alt\',\n    \'chr10_KI270824v1_alt\',\n    \'chr10_GL383546v1_alt\',\n    \'chr10_KI270825v1_alt\',\n    \'chr11_KI270832v1_alt\',\n    \'chr11_KI270830v1_alt\',\n    \'chr11_KI270831v1_alt\',\n    \'chr11_KI270829v1_alt\',\n    \'chr11_GL383547v1_alt\',\n    \'chr11_JH159136v1_alt\',\n    \'chr11_JH159137v1_alt\',\n    \'chr11_KI270827v1_alt\',\n    \'chr11_KI270826v1_alt\',\n    \'chr12_GL877875v1_alt\',\n    \'chr12_GL877876v1_alt\',\n    \'chr12_KI270837v1_alt\',\n    \'chr12_GL383549v1_alt\',\n    \'chr12_KI270835v1_alt\',\n    \'chr12_GL383550v2_alt\',\n    \'chr12_GL383552v1_alt\',\n    \'chr12_GL383553v2_alt\',\n    \'chr12_KI270834v1_alt\',\n    \'chr12_GL383551v1_alt\',\n    \'chr12_KI270833v1_alt\',\n    \'chr12_KI270836v1_alt\',\n    \'chr13_KI270840v1_alt\',\n    \'chr13_KI270839v1_alt\',\n    \'chr13_KI270843v1_alt\',\n    \'chr13_KI270841v1_alt\',\n    \'chr13_KI270838v1_alt\',\n    \'chr13_KI270842v1_alt\',\n    \'chr14_KI270844v1_alt\',\n    \'chr14_KI270847v1_alt\',\n    \'chr14_KI270845v1_alt\',\n    \'chr14_KI270846v1_alt\',\n    \'chr15_KI270852v1_alt\',\n    \'chr15_KI270851v1_alt\',\n    \'chr15_KI270848v1_alt\',\n    \'chr15_GL383554v1_alt\',\n    \'chr15_KI270849v1_alt\',\n    \'chr15_GL383555v2_alt\',\n    \'chr15_KI270850v1_alt\',\n    \'chr16_KI270854v1_alt\',\n    \'chr16_KI270856v1_alt\',\n    \'chr16_KI270855v1_alt\',\n    \'chr16_KI270853v1_alt\',\n    \'chr16_GL383556v1_alt\',\n    \'chr16_GL383557v1_alt\',\n    \'chr17_GL383563v3_alt\',\n    \'chr17_KI270862v1_alt\',\n    \'chr17_KI270861v1_alt\',\n    \'chr17_KI270857v1_alt\',\n    \'chr17_JH159146v1_alt\',\n    \'chr17_JH159147v1_alt\',\n    \'chr17_GL383564v2_alt\',\n    \'chr17_GL000258v2_alt\',\n    \'chr17_GL383565v1_alt\',\n    \'chr17_KI270858v1_alt\',\n    \'chr17_KI270859v1_alt\',\n    \'chr17_GL383566v1_alt\',\n    \'chr17_KI270860v1_alt\',\n    \'chr18_KI270864v1_alt\',\n    \'chr18_GL383567v1_alt\',\n    \'chr18_GL383570v1_alt\',\n    \'chr18_GL383571v1_alt\',\n    \'chr18_GL383568v1_alt\',\n    \'chr18_GL383569v1_alt\',\n    \'chr18_GL383572v1_alt\',\n    \'chr18_KI270863v1_alt\',\n    \'chr19_KI270868v1_alt\',\n    \'chr19_KI270865v1_alt\',\n    \'chr19_GL383573v1_alt\',\n    \'chr19_GL383575v2_alt\',\n    \'chr19_GL383576v1_alt\',\n    \'chr19_GL383574v1_alt\',\n    \'chr19_KI270866v1_alt\',\n    \'chr19_KI270867v1_alt\',\n    \'chr19_GL949746v1_alt\',\n    \'chr20_GL383577v2_alt\',\n    \'chr20_KI270869v1_alt\',\n    \'chr20_KI270871v1_alt\',\n    \'chr20_KI270870v1_alt\',\n    \'chr21_GL383578v2_alt\',\n    \'chr21_KI270874v1_alt\',\n    \'chr21_KI270873v1_alt\',\n    \'chr21_GL383579v2_alt\',\n    \'chr21_GL383580v2_alt\',\n    \'chr21_GL383581v2_alt\',\n    \'chr21_KI270872v1_alt\',\n    \'chr22_KI270875v1_alt\',\n    \'chr22_KI270878v1_alt\',\n    \'chr22_KI270879v1_alt\',\n    \'chr22_KI270876v1_alt\',\n    \'chr22_KI270877v1_alt\',\n    \'chr22_GL383583v2_alt\',\n    \'chr22_GL383582v2_alt\',\n    \'chrX_KI270880v1_alt\',\n    \'chrX_KI270881v1_alt\',\n    \'chr19_KI270882v1_alt\',\n    \'chr19_KI270883v1_alt\',\n    \'chr19_KI270884v1_alt\',\n    \'chr19_KI270885v1_alt\',\n    \'chr19_KI270886v1_alt\',\n    \'chr19_KI270887v1_alt\',\n    \'chr19_KI270888v1_alt\',\n    \'chr19_KI270889v1_alt\',\n    \'chr19_KI270890v1_alt\',\n    \'chr19_KI270891v1_alt\',\n    \'chr1_KI270892v1_alt\',\n    \'chr2_KI270894v1_alt\',\n    \'chr2_KI270893v1_alt\',\n    \'chr3_KI270895v1_alt\',\n    \'chr4_KI270896v1_alt\',\n    \'chr5_KI270897v1_alt\',\n    \'chr5_KI270898v1_alt\',\n    \'chr6_GL000251v2_alt\',\n    \'chr7_KI270899v1_alt\',\n    \'chr8_KI270901v1_alt\',\n    \'chr8_KI270900v1_alt\',\n    \'chr11_KI270902v1_alt\',\n    \'chr11_KI270903v1_alt\',\n    \'chr12_KI270904v1_alt\',\n    \'chr15_KI270906v1_alt\',\n    \'chr15_KI270905v1_alt\',\n    \'chr17_KI270907v1_alt\',\n    \'chr17_KI270910v1_alt\',\n    \'chr17_KI270909v1_alt\',\n    \'chr17_JH159148v1_alt\',\n    \'chr17_KI270908v1_alt\',\n    \'chr18_KI270912v1_alt\',\n    \'chr18_KI270911v1_alt\',\n    \'chr19_GL949747v2_alt\',\n    \'chr22_KB663609v1_alt\',\n    \'chrX_KI270913v1_alt\',\n    \'chr19_KI270914v1_alt\',\n    \'chr19_KI270915v1_alt\',\n    \'chr19_KI270916v1_alt\',\n    \'chr19_KI270917v1_alt\',\n    \'chr19_KI270918v1_alt\',\n    \'chr19_KI270919v1_alt\',\n    \'chr19_KI270920v1_alt\',\n    \'chr19_KI270921v1_alt\',\n    \'chr19_KI270922v1_alt\',\n    \'chr19_KI270923v1_alt\',\n    \'chr3_KI270924v1_alt\',\n    \'chr4_KI270925v1_alt\',\n    \'chr6_GL000252v2_alt\',\n    \'chr8_KI270926v1_alt\',\n    \'chr11_KI270927v1_alt\',\n    \'chr19_GL949748v2_alt\',\n    \'chr22_KI270928v1_alt\',\n    \'chr19_KI270929v1_alt\',\n    \'chr19_KI270930v1_alt\',\n    \'chr19_KI270931v1_alt\',\n    \'chr19_KI270932v1_alt\',\n    \'chr19_KI270933v1_alt\',\n    \'chr19_GL000209v2_alt\',\n    \'chr3_KI270934v1_alt\',\n    \'chr6_GL000253v2_alt\',\n    \'chr19_GL949749v2_alt\',\n    \'chr3_KI270935v1_alt\',\n    \'chr6_GL000254v2_alt\',\n    \'chr19_GL949750v2_alt\',\n    \'chr3_KI270936v1_alt\',\n    \'chr6_GL000255v2_alt\',\n    \'chr19_GL949751v2_alt\',\n    \'chr3_KI270937v1_alt\',\n    \'chr6_GL000256v2_alt\',\n    \'chr19_GL949752v1_alt\',\n    \'chr6_KI270758v1_alt\',\n    \'chr19_GL949753v2_alt\',\n    \'chr19_KI270938v1_alt\',\n    \'chrEBV\',\n    \'chrUn_KN707606v1_decoy\',\n    \'chrUn_KN707607v1_decoy\',\n    \'chrUn_KN707608v1_decoy\',\n    \'chrUn_KN707609v1_decoy\',\n    \'chrUn_KN707610v1_decoy\',\n    \'chrUn_KN707611v1_decoy\',\n    \'chrUn_KN707612v1_decoy\',\n    \'chrUn_KN707613v1_decoy\',\n    \'chrUn_KN707614v1_decoy\',\n    \'chrUn_KN707615v1_decoy\',\n    \'chrUn_KN707616v1_decoy\',\n    \'chrUn_KN707617v1_decoy\',\n    \'chrUn_KN707618v1_decoy\',\n    \'chrUn_KN707619v1_decoy\',\n    \'chrUn_KN707620v1_decoy\',\n    \'chrUn_KN707621v1_decoy\',\n    \'chrUn_KN707622v1_decoy\',\n    \'chrUn_KN707623v1_decoy\',\n    \'chrUn_KN707624v1_decoy\',\n    \'chrUn_KN707625v1_decoy\',\n    \'chrUn_KN707626v1_decoy\',\n    \'chrUn_KN707627v1_decoy\',\n    \'chrUn_KN707628v1_decoy\',\n    \'chrUn_KN707629v1_decoy\',\n    \'chrUn_KN707630v1_decoy\',\n    \'chrUn_KN707631v1_decoy\',\n    \'chrUn_KN707632v1_decoy\',\n    \'chrUn_KN707633v1_decoy\',\n    \'chrUn_KN707634v1_decoy\',\n    \'chrUn_KN707635v1_decoy\',\n    \'chrUn_KN707636v1_decoy\',\n    \'chrUn_KN707637v1_decoy\',\n    \'chrUn_KN707638v1_decoy\',\n    \'chrUn_KN707639v1_decoy\',\n    \'chrUn_KN707640v1_decoy\',\n    \'chrUn_KN707641v1_decoy\',\n    \'chrUn_KN707642v1_decoy\',\n    \'chrUn_KN707643v1_decoy\',\n    \'chrUn_KN707644v1_decoy\',\n    \'chrUn_KN707645v1_decoy\',\n    \'chrUn_KN707646v1_decoy\',\n    \'chrUn_KN707647v1_decoy\',\n    \'chrUn_KN707648v1_decoy\',\n    \'chrUn_KN707649v1_decoy\',\n    \'chrUn_KN707650v1_decoy\',\n    \'chrUn_KN707651v1_decoy\',\n    \'chrUn_KN707652v1_decoy\',\n    \'chrUn_KN707653v1_decoy\',\n    \'chrUn_KN707654v1_decoy\',\n    \'chrUn_KN707655v1_decoy\',\n    \'chrUn_KN707656v1_decoy\',\n    \'chrUn_KN707657v1_decoy\',\n    \'chrUn_KN707658v1_decoy\',\n    \'chrUn_KN707659v1_decoy\',\n    \'chrUn_KN707660v1_decoy\',\n    \'chrUn_KN707661v1_decoy\',\n    \'chrUn_KN707662v1_decoy\',\n    \'chrUn_KN707663v1_decoy\',\n    \'chrUn_KN707664v1_decoy\',\n    \'chrUn_KN707665v1_decoy\',\n    \'chrUn_KN707666v1_decoy\',\n    \'chrUn_KN707667v1_decoy\',\n    \'chrUn_KN707668v1_decoy\',\n    \'chrUn_KN707669v1_decoy\',\n    \'chrUn_KN707670v1_decoy\',\n    \'chrUn_KN707671v1_decoy\',\n    \'chrUn_KN707672v1_decoy\',\n    \'chrUn_KN707673v1_decoy\',\n    \'chrUn_KN707674v1_decoy\',\n    \'chrUn_KN707675v1_decoy\',\n    \'chrUn_KN707676v1_decoy\',\n    \'chrUn_KN707677v1_decoy\',\n    \'chrUn_KN707678v1_decoy\',\n    \'chrUn_KN707679v1_decoy\',\n    \'chrUn_KN707680v1_decoy\',\n    \'chrUn_KN707681v1_decoy\',\n    \'chrUn_KN707682v1_decoy\',\n    \'chrUn_KN707683v1_decoy\',\n    \'chrUn_KN707684v1_decoy\',\n    \'chrUn_KN707685v1_decoy\',\n    \'chrUn_KN707686v1_decoy\',\n    \'chrUn_KN707687v1_decoy\',\n    \'chrUn_KN707688v1_decoy\',\n    \'chrUn_KN707689v1_decoy\',\n    \'chrUn_KN707690v1_decoy\',\n    \'chrUn_KN707691v1_decoy\',\n    \'chrUn_KN707692v1_decoy\',\n    \'chrUn_KN707693v1_decoy\',\n    \'chrUn_KN707694v1_decoy\',\n    \'chrUn_KN707695v1_decoy\',\n    \'chrUn_KN707696v1_decoy\',\n    \'chrUn_KN707697v1_decoy\',\n    \'chrUn_KN707698v1_decoy\',\n    \'chrUn_KN707699v1_decoy\',\n    \'chrUn_KN707700v1_decoy\',\n    \'chrUn_KN707701v1_decoy\',\n    \'chrUn_KN707702v1_decoy\',\n    \'chrUn_KN707703v1_decoy\',\n    \'chrUn_KN707704v1_decoy\',\n    \'chrUn_KN707705v1_decoy\',\n    \'chrUn_KN707706v1_decoy\',\n    \'chrUn_KN707707v1_decoy\',\n    \'chrUn_KN707708v1_decoy\',\n    \'chrUn_KN707709v1_decoy\',\n    \'chrUn_KN707710v1_decoy\',\n    \'chrUn_KN707711v1_decoy\',\n    \'chrUn_KN707712v1_decoy\',\n    \'chrUn_KN707713v1_decoy\',\n    \'chrUn_KN707714v1_decoy\',\n    \'chrUn_KN707715v1_decoy\',\n    \'chrUn_KN707716v1_decoy\',\n    \'chrUn_KN707717v1_decoy\',\n    \'chrUn_KN707718v1_decoy\',\n    \'chrUn_KN707719v1_decoy\',\n    \'chrUn_KN707720v1_decoy\',\n    \'chrUn_KN707721v1_decoy\',\n    \'chrUn_KN707722v1_decoy\',\n    \'chrUn_KN707723v1_decoy\',\n    \'chrUn_KN707724v1_decoy\',\n    \'chrUn_KN707725v1_decoy\',\n    \'chrUn_KN707726v1_decoy\',\n    \'chrUn_KN707727v1_decoy\',\n    \'chrUn_KN707728v1_decoy\',\n    \'chrUn_KN707729v1_decoy\',\n    \'chrUn_KN707730v1_decoy\',\n    \'chrUn_KN707731v1_decoy\',\n    \'chrUn_KN707732v1_decoy\',\n    \'chrUn_KN707733v1_decoy\',\n    \'chrUn_KN707734v1_decoy\',\n    \'chrUn_KN707735v1_decoy\',\n    \'chrUn_KN707736v1_decoy\',\n    \'chrUn_KN707737v1_decoy\',\n    \'chrUn_KN707738v1_decoy\',\n    \'chrUn_KN707739v1_decoy\',\n    \'chrUn_KN707740v1_decoy\',\n    \'chrUn_KN707741v1_decoy\',\n    \'chrUn_KN707742v1_decoy\',\n    \'chrUn_KN707743v1_decoy\',\n    \'chrUn_KN707744v1_decoy\',\n    \'chrUn_KN707745v1_decoy\',\n    \'chrUn_KN707746v1_decoy\',\n    \'chrUn_KN707747v1_decoy\',\n    \'chrUn_KN707748v1_decoy\',\n    \'chrUn_KN707749v1_decoy\',\n    \'chrUn_KN707750v1_decoy\',\n    \'chrUn_KN707751v1_decoy\',\n    \'chrUn_KN707752v1_decoy\',\n    \'chrUn_KN707753v1_decoy\',\n    \'chrUn_KN707754v1_decoy\',\n    \'chrUn_KN707755v1_decoy\',\n    \'chrUn_KN707756v1_decoy\',\n    \'chrUn_KN707757v1_decoy\',\n    \'chrUn_KN707758v1_decoy\',\n    \'chrUn_KN707759v1_decoy\',\n    \'chrUn_KN707760v1_decoy\',\n    \'chrUn_KN707761v1_decoy\',\n    \'chrUn_KN707762v1_decoy\',\n    \'chrUn_KN707763v1_decoy\',\n    \'chrUn_KN707764v1_decoy\',\n    \'chrUn_KN707765v1_decoy\',\n    \'chrUn_KN707766v1_decoy\',\n    \'chrUn_KN707767v1_decoy\',\n    \'chrUn_KN707768v1_decoy\',\n    \'chrUn_KN707769v1_decoy\',\n    \'chrUn_KN707770v1_decoy\',\n    \'chrUn_KN707771v1_decoy\',\n    \'chrUn_KN707772v1_decoy\',\n    \'chrUn_KN707773v1_decoy\',\n    \'chrUn_KN707774v1_decoy\',\n    \'chrUn_KN707775v1_decoy\',\n    \'chrUn_KN707776v1_decoy\',\n    \'chrUn_KN707777v1_decoy\',\n    \'chrUn_KN707778v1_decoy\',\n    \'chrUn_KN707779v1_decoy\',\n    \'chrUn_KN707780v1_decoy\',\n    \'chrUn_KN707781v1_decoy\',\n    \'chrUn_KN707782v1_decoy\',\n    \'chrUn_KN707783v1_decoy\',\n    \'chrUn_KN707784v1_decoy\',\n    \'chrUn_KN707785v1_decoy\',\n    \'chrUn_KN707786v1_decoy\',\n    \'chrUn_KN707787v1_decoy\',\n    \'chrUn_KN707788v1_decoy\',\n    \'chrUn_KN707789v1_decoy\',\n    \'chrUn_KN707790v1_decoy\',\n    \'chrUn_KN707791v1_decoy\',\n    \'chrUn_KN707792v1_decoy\',\n    \'chrUn_KN707793v1_decoy\',\n    \'chrUn_KN707794v1_decoy\',\n    \'chrUn_KN707795v1_decoy\',\n    \'chrUn_KN707796v1_decoy\',\n    \'chrUn_KN707797v1_decoy\',\n    \'chrUn_KN707798v1_decoy\',\n    \'chrUn_KN707799v1_decoy\',\n    \'chrUn_KN707800v1_decoy\',\n    \'chrUn_KN707801v1_decoy\',\n    \'chrUn_KN707802v1_decoy\',\n    \'chrUn_KN707803v1_decoy\',\n    \'chrUn_KN707804v1_decoy\',\n    \'chrUn_KN707805v1_decoy\',\n    \'chrUn_KN707806v1_decoy\',\n    \'chrUn_KN707807v1_decoy\',\n    \'chrUn_KN707808v1_decoy\',\n    \'chrUn_KN707809v1_decoy\',\n    \'chrUn_KN707810v1_decoy\',\n    \'chrUn_KN707811v1_decoy\',\n    \'chrUn_KN707812v1_decoy\',\n    \'chrUn_KN707813v1_decoy\',\n    \'chrUn_KN707814v1_decoy\',\n    \'chrUn_KN707815v1_decoy\',\n    \'chrUn_KN707816v1_decoy\',\n    \'chrUn_KN707817v1_decoy\',\n    \'chrUn_KN707818v1_decoy\',\n    \'chrUn_KN707819v1_decoy\',\n    \'chrUn_KN707820v1_decoy\',\n    \'chrUn_KN707821v1_decoy\',\n    \'chrUn_KN707822v1_decoy\',\n    \'chrUn_KN707823v1_decoy\',\n    \'chrUn_KN707824v1_decoy\',\n    \'chrUn_KN707825v1_decoy\',\n    \'chrUn_KN707826v1_decoy\',\n    \'chrUn_KN707827v1_decoy\',\n    \'chrUn_KN707828v1_decoy\',\n    \'chrUn_KN707829v1_decoy\',\n    \'chrUn_KN707830v1_decoy\',\n    \'chrUn_KN707831v1_decoy\',\n    \'chrUn_KN707832v1_decoy\',\n    \'chrUn_KN707833v1_decoy\',\n    \'chrUn_KN707834v1_decoy\',\n    \'chrUn_KN707835v1_decoy\',\n    \'chrUn_KN707836v1_decoy\',\n    \'chrUn_KN707837v1_decoy\',\n    \'chrUn_KN707838v1_decoy\',\n    \'chrUn_KN707839v1_decoy\',\n    \'chrUn_KN707840v1_decoy\',\n    \'chrUn_KN707841v1_decoy\',\n    \'chrUn_KN707842v1_decoy\',\n    \'chrUn_KN707843v1_decoy\',\n    \'chrUn_KN707844v1_decoy\',\n    \'chrUn_KN707845v1_decoy\',\n    \'chrUn_KN707846v1_decoy\',\n    \'chrUn_KN707847v1_decoy\',\n    \'chrUn_KN707848v1_decoy\',\n    \'chrUn_KN707849v1_decoy\',\n    \'chrUn_KN707850v1_decoy\',\n    \'chrUn_KN707851v1_decoy\',\n    \'chrUn_KN707852v1_decoy\',\n    \'chrUn_KN707853v1_decoy\',\n    \'chrUn_KN707854v1_decoy\',\n    \'chrUn_KN707855v1_decoy\',\n    \'chrUn_KN707856v1_decoy\',\n    \'chrUn_KN707857v1_decoy\',\n    \'chrUn_KN707858v1_decoy\',\n    \'chrUn_KN707859v1_decoy\',\n    \'chrUn_KN707860v1_decoy\',\n    \'chrUn_KN707861v1_decoy\',\n    \'chrUn_KN707862v1_decoy\',\n    \'chrUn_KN707863v1_decoy\',\n    \'chrUn_KN707864v1_decoy\',\n    \'chrUn_KN707865v1_decoy\',\n    \'chrUn_KN707866v1_decoy\',\n    \'chrUn_KN707867v1_decoy\',\n    \'chrUn_KN707868v1_decoy\',\n    \'chrUn_KN707869v1_decoy\',\n    \'chrUn_KN707870v1_decoy\',\n    \'chrUn_KN707871v1_decoy\',\n    \'chrUn_KN707872v1_decoy\',\n    \'chrUn_KN707873v1_decoy\',\n    \'chrUn_KN707874v1_decoy\',\n    \'chrUn_KN707875v1_decoy\',\n    \'chrUn_KN707876v1_decoy\',\n    \'chrUn_KN707877v1_decoy\',\n    \'chrUn_KN707878v1_decoy\',\n    \'chrUn_KN707879v1_decoy\',\n    \'chrUn_KN707880v1_decoy\',\n    \'chrUn_KN707881v1_decoy\',\n    \'chrUn_KN707882v1_decoy\',\n    \'chrUn_KN707883v1_decoy\',\n    \'chrUn_KN707884v1_decoy\',\n    \'chrUn_KN707885v1_decoy\',\n    \'chrUn_KN707886v1_decoy\',\n    \'chrUn_KN707887v1_decoy\',\n    \'chrUn_KN707888v1_decoy\',\n    \'chrUn_KN707889v1_decoy\',\n    \'chrUn_KN707890v1_decoy\',\n    \'chrUn_KN707891v1_decoy\',\n    \'chrUn_KN707892v1_decoy\',\n    \'chrUn_KN707893v1_decoy\',\n    \'chrUn_KN707894v1_decoy\',\n    \'chrUn_KN707895v1_decoy\',\n    \'chrUn_KN707896v1_decoy\',\n    \'chrUn_KN707897v1_decoy\',\n    \'chrUn_KN707898v1_decoy\',\n    \'chrUn_KN707899v1_decoy\',\n    \'chrUn_KN707900v1_decoy\',\n    \'chrUn_KN707901v1_decoy\',\n    \'chrUn_KN707902v1_decoy\',\n    \'chrUn_KN707903v1_decoy\',\n    \'chrUn_KN707904v1_decoy\',\n    \'chrUn_KN707905v1_decoy\',\n    \'chrUn_KN707906v1_decoy\',\n    \'chrUn_KN707907v1_decoy\',\n    \'chrUn_KN707908v1_decoy\',\n    \'chrUn_KN707909v1_decoy\',\n    \'chrUn_KN707910v1_decoy\',\n    \'chrUn_KN707911v1_decoy\',\n    \'chrUn_KN707912v1_decoy\',\n    \'chrUn_KN707913v1_decoy\',\n    \'chrUn_KN707914v1_decoy\',\n    \'chrUn_KN707915v1_decoy\',\n    \'chrUn_KN707916v1_decoy\',\n    \'chrUn_KN707917v1_decoy\',\n    \'chrUn_KN707918v1_decoy\',\n    \'chrUn_KN707919v1_decoy\',\n    \'chrUn_KN707920v1_decoy\',\n    \'chrUn_KN707921v1_decoy\',\n    \'chrUn_KN707922v1_decoy\',\n    \'chrUn_KN707923v1_decoy\',\n    \'chrUn_KN707924v1_decoy\',\n    \'chrUn_KN707925v1_decoy\',\n    \'chrUn_KN707926v1_decoy\',\n    \'chrUn_KN707927v1_decoy\',\n    \'chrUn_KN707928v1_decoy\',\n    \'chrUn_KN707929v1_decoy\',\n    \'chrUn_KN707930v1_decoy\',\n    \'chrUn_KN707931v1_decoy\',\n    \'chrUn_KN707932v1_decoy\',\n    \'chrUn_KN707933v1_decoy\',\n    \'chrUn_KN707934v1_decoy\',\n    \'chrUn_KN707935v1_decoy\',\n    \'chrUn_KN707936v1_decoy\',\n    \'chrUn_KN707937v1_decoy\',\n    \'chrUn_KN707938v1_decoy\',\n    \'chrUn_KN707939v1_decoy\',\n    \'chrUn_KN707940v1_decoy\',\n    \'chrUn_KN707941v1_decoy\',\n    \'chrUn_KN707942v1_decoy\',\n    \'chrUn_KN707943v1_decoy\',\n    \'chrUn_KN707944v1_decoy\',\n    \'chrUn_KN707945v1_decoy\',\n    \'chrUn_KN707946v1_decoy\',\n    \'chrUn_KN707947v1_decoy\',\n    \'chrUn_KN707948v1_decoy\',\n    \'chrUn_KN707949v1_decoy\',\n    \'chrUn_KN707950v1_decoy\',\n    \'chrUn_KN707951v1_decoy\',\n    \'chrUn_KN707952v1_decoy\',\n    \'chrUn_KN707953v1_decoy\',\n    \'chrUn_KN707954v1_decoy\',\n    \'chrUn_KN707955v1_decoy\',\n    \'chrUn_KN707956v1_decoy\',\n    \'chrUn_KN707957v1_decoy\',\n    \'chrUn_KN707958v1_decoy\',\n    \'chrUn_KN707959v1_decoy\',\n    \'chrUn_KN707960v1_decoy\',\n    \'chrUn_KN707961v1_decoy\',\n    \'chrUn_KN707962v1_decoy\',\n    \'chrUn_KN707963v1_decoy\',\n    \'chrUn_KN707964v1_decoy\',\n    \'chrUn_KN707965v1_decoy\',\n    \'chrUn_KN707966v1_decoy\',\n    \'chrUn_KN707967v1_decoy\',\n    \'chrUn_KN707968v1_decoy\',\n    \'chrUn_KN707969v1_decoy\',\n    \'chrUn_KN707970v1_decoy\',\n    \'chrUn_KN707971v1_decoy\',\n    \'chrUn_KN707972v1_decoy\',\n    \'chrUn_KN707973v1_decoy\',\n    \'chrUn_KN707974v1_decoy\',\n    \'chrUn_KN707975v1_decoy\',\n    \'chrUn_KN707976v1_decoy\',\n    \'chrUn_KN707977v1_decoy\',\n    \'chrUn_KN707978v1_decoy\',\n    \'chrUn_KN707979v1_decoy\',\n    \'chrUn_KN707980v1_decoy\',\n    \'chrUn_KN707981v1_decoy\',\n    \'chrUn_KN707982v1_decoy\',\n    \'chrUn_KN707983v1_decoy\',\n    \'chrUn_KN707984v1_decoy\',\n    \'chrUn_KN707985v1_decoy\',\n    \'chrUn_KN707986v1_decoy\',\n    \'chrUn_KN707987v1_decoy\',\n    \'chrUn_KN707988v1_decoy\',\n    \'chrUn_KN707989v1_decoy\',\n    \'chrUn_KN707990v1_decoy\',\n    \'chrUn_KN707991v1_decoy\',\n    \'chrUn_KN707992v1_decoy\',\n    \'chrUn_JTFH01000001v1_decoy\',\n    \'chrUn_JTFH01000002v1_decoy\',\n    \'chrUn_JTFH01000003v1_decoy\',\n    \'chrUn_JTFH01000004v1_decoy\',\n    \'chrUn_JTFH01000005v1_decoy\',\n    \'chrUn_JTFH01000006v1_decoy\',\n    \'chrUn_JTFH01000007v1_decoy\',\n    \'chrUn_JTFH01000008v1_decoy\',\n    \'chrUn_JTFH01000009v1_decoy\',\n    \'chrUn_JTFH01000010v1_decoy\',\n    \'chrUn_JTFH01000011v1_decoy\',\n    \'chrUn_JTFH01000012v1_decoy\',\n    \'chrUn_JTFH01000013v1_decoy\',\n    \'chrUn_JTFH01000014v1_decoy\',\n    \'chrUn_JTFH01000015v1_decoy\',\n    \'chrUn_JTFH01000016v1_decoy\',\n    \'chrUn_JTFH01000017v1_decoy\',\n    \'chrUn_JTFH01000018v1_decoy\',\n    \'chrUn_JTFH01000019v1_decoy\',\n    \'chrUn_JTFH01000020v1_decoy\',\n    \'chrUn_JTFH01000021v1_decoy\',\n    \'chrUn_JTFH01000022v1_decoy\',\n    \'chrUn_JTFH01000023v1_decoy\',\n    \'chrUn_JTFH01000024v1_decoy\',\n    \'chrUn_JTFH01000025v1_decoy\',\n    \'chrUn_JTFH01000026v1_decoy\',\n    \'chrUn_JTFH01000027v1_decoy\',\n    \'chrUn_JTFH01000028v1_decoy\',\n    \'chrUn_JTFH01000029v1_decoy\',\n    \'chrUn_JTFH01000030v1_decoy\',\n    \'chrUn_JTFH01000031v1_decoy\',\n    \'chrUn_JTFH01000032v1_decoy\',\n    \'chrUn_JTFH01000033v1_decoy\',\n    \'chrUn_JTFH01000034v1_decoy\',\n    \'chrUn_JTFH01000035v1_decoy\',\n    \'chrUn_JTFH01000036v1_decoy\',\n    \'chrUn_JTFH01000037v1_decoy\',\n    \'chrUn_JTFH01000038v1_decoy\',\n    \'chrUn_JTFH01000039v1_decoy\',\n    \'chrUn_JTFH01000040v1_decoy\',\n    \'chrUn_JTFH01000041v1_decoy\',\n    \'chrUn_JTFH01000042v1_decoy\',\n    \'chrUn_JTFH01000043v1_decoy\',\n    \'chrUn_JTFH01000044v1_decoy\',\n    \'chrUn_JTFH01000045v1_decoy\',\n    \'chrUn_JTFH01000046v1_decoy\',\n    \'chrUn_JTFH01000047v1_decoy\',\n    \'chrUn_JTFH01000048v1_decoy\',\n    \'chrUn_JTFH01000049v1_decoy\',\n    \'chrUn_JTFH01000050v1_decoy\',\n    \'chrUn_JTFH01000051v1_decoy\',\n    \'chrUn_JTFH01000052v1_decoy\',\n    \'chrUn_JTFH01000053v1_decoy\',\n    \'chrUn_JTFH01000054v1_decoy\',\n    \'chrUn_JTFH01000055v1_decoy\',\n    \'chrUn_JTFH01000056v1_decoy\',\n    \'chrUn_JTFH01000057v1_decoy\',\n    \'chrUn_JTFH01000058v1_decoy\',\n    \'chrUn_JTFH01000059v1_decoy\',\n    \'chrUn_JTFH01000060v1_decoy\',\n    \'chrUn_JTFH01000061v1_decoy\',\n    \'chrUn_JTFH01000062v1_decoy\',\n    \'chrUn_JTFH01000063v1_decoy\',\n    \'chrUn_JTFH01000064v1_decoy\',\n    \'chrUn_JTFH01000065v1_decoy\',\n    \'chrUn_JTFH01000066v1_decoy\',\n    \'chrUn_JTFH01000067v1_decoy\',\n    \'chrUn_JTFH01000068v1_decoy\',\n    \'chrUn_JTFH01000069v1_decoy\',\n    \'chrUn_JTFH01000070v1_decoy\',\n    \'chrUn_JTFH01000071v1_decoy\',\n    \'chrUn_JTFH01000072v1_decoy\',\n    \'chrUn_JTFH01000073v1_decoy\',\n    \'chrUn_JTFH01000074v1_decoy\',\n    \'chrUn_JTFH01000075v1_decoy\',\n    \'chrUn_JTFH01000076v1_decoy\',\n    \'chrUn_JTFH01000077v1_decoy\',\n    \'chrUn_JTFH01000078v1_decoy\',\n    \'chrUn_JTFH01000079v1_decoy\',\n    \'chrUn_JTFH01000080v1_decoy\',\n    \'chrUn_JTFH01000081v1_decoy\',\n    \'chrUn_JTFH01000082v1_decoy\',\n    \'chrUn_JTFH01000083v1_decoy\',\n    \'chrUn_JTFH01000084v1_decoy\',\n    \'chrUn_JTFH01000085v1_decoy\',\n    \'chrUn_JTFH01000086v1_decoy\',\n    \'chrUn_JTFH01000087v1_decoy\',\n    \'chrUn_JTFH01000088v1_decoy\',\n    \'chrUn_JTFH01000089v1_decoy\',\n    \'chrUn_JTFH01000090v1_decoy\',\n    \'chrUn_JTFH01000091v1_decoy\',\n    \'chrUn_JTFH01000092v1_decoy\',\n    \'chrUn_JTFH01000093v1_decoy\',\n    \'chrUn_JTFH01000094v1_decoy\',\n    \'chrUn_JTFH01000095v1_decoy\',\n    \'chrUn_JTFH01000096v1_decoy\',\n    \'chrUn_JTFH01000097v1_decoy\',\n    \'chrUn_JTFH01000098v1_decoy\',\n    \'chrUn_JTFH01000099v1_decoy\',\n    \'chrUn_JTFH01000100v1_decoy\',\n    \'chrUn_JTFH01000101v1_decoy\',\n    \'chrUn_JTFH01000102v1_decoy\',\n    \'chrUn_JTFH01000103v1_decoy\',\n    \'chrUn_JTFH01000104v1_decoy\',\n    \'chrUn_JTFH01000105v1_decoy\',\n    \'chrUn_JTFH01000106v1_decoy\',\n    \'chrUn_JTFH01000107v1_decoy\',\n    \'chrUn_JTFH01000108v1_decoy\',\n    \'chrUn_JTFH01000109v1_decoy\',\n    \'chrUn_JTFH01000110v1_decoy\',\n    \'chrUn_JTFH01000111v1_decoy\',\n    \'chrUn_JTFH01000112v1_decoy\',\n    \'chrUn_JTFH01000113v1_decoy\',\n    \'chrUn_JTFH01000114v1_decoy\',\n    \'chrUn_JTFH01000115v1_decoy\',\n    \'chrUn_JTFH01000116v1_decoy\',\n    \'chrUn_JTFH01000117v1_decoy\',\n    \'chrUn_JTFH01000118v1_decoy\',\n    \'chrUn_JTFH01000119v1_decoy\',\n    \'chrUn_JTFH01000120v1_decoy\',\n    \'chrUn_JTFH01000121v1_decoy\',\n    \'chrUn_JTFH01000122v1_decoy\',\n    \'chrUn_JTFH01000123v1_decoy\',\n    \'chrUn_JTFH01000124v1_decoy\',\n    \'chrUn_JTFH01000125v1_decoy\',\n    \'chrUn_JTFH01000126v1_decoy\',\n    \'chrUn_JTFH01000127v1_decoy\',\n    \'chrUn_JTFH01000128v1_decoy\',\n    \'chrUn_JTFH01000129v1_decoy\',\n    \'chrUn_JTFH01000130v1_decoy\',\n    \'chrUn_JTFH01000131v1_decoy\',\n    \'chrUn_JTFH01000132v1_decoy\',\n    \'chrUn_JTFH01000133v1_decoy\',\n    \'chrUn_JTFH01000134v1_decoy\',\n    \'chrUn_JTFH01000135v1_decoy\',\n    \'chrUn_JTFH01000136v1_decoy\',\n    \'chrUn_JTFH01000137v1_decoy\',\n    \'chrUn_JTFH01000138v1_decoy\',\n    \'chrUn_JTFH01000139v1_decoy\',\n    \'chrUn_JTFH01000140v1_decoy\',\n    \'chrUn_JTFH01000141v1_decoy\',\n    \'chrUn_JTFH01000142v1_decoy\',\n    \'chrUn_JTFH01000143v1_decoy\',\n    \'chrUn_JTFH01000144v1_decoy\',\n    \'chrUn_JTFH01000145v1_decoy\',\n    \'chrUn_JTFH01000146v1_decoy\',\n    \'chrUn_JTFH01000147v1_decoy\',\n    \'chrUn_JTFH01000148v1_decoy\',\n    \'chrUn_JTFH01000149v1_decoy\',\n    \'chrUn_JTFH01000150v1_decoy\',\n    \'chrUn_JTFH01000151v1_decoy\',\n    \'chrUn_JTFH01000152v1_decoy\',\n    \'chrUn_JTFH01000153v1_decoy\',\n    \'chrUn_JTFH01000154v1_decoy\',\n    \'chrUn_JTFH01000155v1_decoy\',\n    \'chrUn_JTFH01000156v1_decoy\',\n    \'chrUn_JTFH01000157v1_decoy\',\n    \'chrUn_JTFH01000158v1_decoy\',\n    \'chrUn_JTFH01000159v1_decoy\',\n    \'chrUn_JTFH01000160v1_decoy\',\n    \'chrUn_JTFH01000161v1_decoy\',\n    \'chrUn_JTFH01000162v1_decoy\',\n    \'chrUn_JTFH01000163v1_decoy\',\n    \'chrUn_JTFH01000164v1_decoy\',\n    \'chrUn_JTFH01000165v1_decoy\',\n    \'chrUn_JTFH01000166v1_decoy\',\n    \'chrUn_JTFH01000167v1_decoy\',\n    \'chrUn_JTFH01000168v1_decoy\',\n    \'chrUn_JTFH01000169v1_decoy\',\n    \'chrUn_JTFH01000170v1_decoy\',\n    \'chrUn_JTFH01000171v1_decoy\',\n    \'chrUn_JTFH01000172v1_decoy\',\n    \'chrUn_JTFH01000173v1_decoy\',\n    \'chrUn_JTFH01000174v1_decoy\',\n    \'chrUn_JTFH01000175v1_decoy\',\n    \'chrUn_JTFH01000176v1_decoy\',\n    \'chrUn_JTFH01000177v1_decoy\',\n    \'chrUn_JTFH01000178v1_decoy\',\n    \'chrUn_JTFH01000179v1_decoy\',\n    \'chrUn_JTFH01000180v1_decoy\',\n    \'chrUn_JTFH01000181v1_decoy\',\n    \'chrUn_JTFH01000182v1_decoy\',\n    \'chrUn_JTFH01000183v1_decoy\',\n    \'chrUn_JTFH01000184v1_decoy\',\n    \'chrUn_JTFH01000185v1_decoy\',\n    \'chrUn_JTFH01000186v1_decoy\',\n    \'chrUn_JTFH01000187v1_decoy\',\n    \'chrUn_JTFH01000188v1_decoy\',\n    \'chrUn_JTFH01000189v1_decoy\',\n    \'chrUn_JTFH01000190v1_decoy\',\n    \'chrUn_JTFH01000191v1_decoy\',\n    \'chrUn_JTFH01000192v1_decoy\',\n    \'chrUn_JTFH01000193v1_decoy\',\n    \'chrUn_JTFH01000194v1_decoy\',\n    \'chrUn_JTFH01000195v1_decoy\',\n    \'chrUn_JTFH01000196v1_decoy\',\n    \'chrUn_JTFH01000197v1_decoy\',\n    \'chrUn_JTFH01000198v1_decoy\',\n    \'chrUn_JTFH01000199v1_decoy\',\n    \'chrUn_JTFH01000200v1_decoy\',\n    \'chrUn_JTFH01000201v1_decoy\',\n    \'chrUn_JTFH01000202v1_decoy\',\n    \'chrUn_JTFH01000203v1_decoy\',\n    \'chrUn_JTFH01000204v1_decoy\',\n    \'chrUn_JTFH01000205v1_decoy\',\n    \'chrUn_JTFH01000206v1_decoy\',\n    \'chrUn_JTFH01000207v1_decoy\',\n    \'chrUn_JTFH01000208v1_decoy\',\n    \'chrUn_JTFH01000209v1_decoy\',\n    \'chrUn_JTFH01000210v1_decoy\',\n    \'chrUn_JTFH01000211v1_decoy\',\n    \'chrUn_JTFH01000212v1_decoy\',\n    \'chrUn_JTFH01000213v1_decoy\',\n    \'chrUn_JTFH01000214v1_decoy\',\n    \'chrUn_JTFH01000215v1_decoy\',\n    \'chrUn_JTFH01000216v1_decoy\',\n    \'chrUn_JTFH01000217v1_decoy\',\n    \'chrUn_JTFH01000218v1_decoy\',\n    \'chrUn_JTFH01000219v1_decoy\',\n    \'chrUn_JTFH01000220v1_decoy\',\n    \'chrUn_JTFH01000221v1_decoy\',\n    \'chrUn_JTFH01000222v1_decoy\',\n    \'chrUn_JTFH01000223v1_decoy\',\n    \'chrUn_JTFH01000224v1_decoy\',\n    \'chrUn_JTFH01000225v1_decoy\',\n    \'chrUn_JTFH01000226v1_decoy\',\n    \'chrUn_JTFH01000227v1_decoy\',\n    \'chrUn_JTFH01000228v1_decoy\',\n    \'chrUn_JTFH01000229v1_decoy\',\n    \'chrUn_JTFH01000230v1_decoy\',\n    \'chrUn_JTFH01000231v1_decoy\',\n    \'chrUn_JTFH01000232v1_decoy\',\n    \'chrUn_JTFH01000233v1_decoy\',\n    \'chrUn_JTFH01000234v1_decoy\',\n    \'chrUn_JTFH01000235v1_decoy\',\n    \'chrUn_JTFH01000236v1_decoy\',\n    \'chrUn_JTFH01000237v1_decoy\',\n    \'chrUn_JTFH01000238v1_decoy\',\n    \'chrUn_JTFH01000239v1_decoy\',\n    \'chrUn_JTFH01000240v1_decoy\',\n    \'chrUn_JTFH01000241v1_decoy\',\n    \'chrUn_JTFH01000242v1_decoy\',\n    \'chrUn_JTFH01000243v1_decoy\',\n    \'chrUn_JTFH01000244v1_decoy\',\n    \'chrUn_JTFH01000245v1_decoy\',\n    \'chrUn_JTFH01000246v1_decoy\',\n    \'chrUn_JTFH01000247v1_decoy\',\n    \'chrUn_JTFH01000248v1_decoy\',\n    \'chrUn_JTFH01000249v1_decoy\',\n    \'chrUn_JTFH01000250v1_decoy\',\n    \'chrUn_JTFH01000251v1_decoy\',\n    \'chrUn_JTFH01000252v1_decoy\',\n    \'chrUn_JTFH01000253v1_decoy\',\n    \'chrUn_JTFH01000254v1_decoy\',\n    \'chrUn_JTFH01000255v1_decoy\',\n    \'chrUn_JTFH01000256v1_decoy\',\n    \'chrUn_JTFH01000257v1_decoy\',\n    \'chrUn_JTFH01000258v1_decoy\',\n    \'chrUn_JTFH01000259v1_decoy\',\n    \'chrUn_JTFH01000260v1_decoy\',\n    \'chrUn_JTFH01000261v1_decoy\',\n    \'chrUn_JTFH01000262v1_decoy\',\n    \'chrUn_JTFH01000263v1_decoy\',\n    \'chrUn_JTFH01000264v1_decoy\',\n    \'chrUn_JTFH01000265v1_decoy\',\n    \'chrUn_JTFH01000266v1_decoy\',\n    \'chrUn_JTFH01000267v1_decoy\',\n    \'chrUn_JTFH01000268v1_decoy\',\n    \'chrUn_JTFH01000269v1_decoy\',\n    \'chrUn_JTFH01000270v1_decoy\',\n    \'chrUn_JTFH01000271v1_decoy\',\n    \'chrUn_JTFH01000272v1_decoy\',\n    \'chrUn_JTFH01000273v1_decoy\',\n    \'chrUn_JTFH01000274v1_decoy\',\n    \'chrUn_JTFH01000275v1_decoy\',\n    \'chrUn_JTFH01000276v1_decoy\',\n    \'chrUn_JTFH01000277v1_decoy\',\n    \'chrUn_JTFH01000278v1_decoy\',\n    \'chrUn_JTFH01000279v1_decoy\',\n    \'chrUn_JTFH01000280v1_decoy\',\n    \'chrUn_JTFH01000281v1_decoy\',\n    \'chrUn_JTFH01000282v1_decoy\',\n    \'chrUn_JTFH01000283v1_decoy\',\n    \'chrUn_JTFH01000284v1_decoy\',\n    \'chrUn_JTFH01000285v1_decoy\',\n    \'chrUn_JTFH01000286v1_decoy\',\n    \'chrUn_JTFH01000287v1_decoy\',\n    \'chrUn_JTFH01000288v1_decoy\',\n    \'chrUn_JTFH01000289v1_decoy\',\n    \'chrUn_JTFH01000290v1_decoy\',\n    \'chrUn_JTFH01000291v1_decoy\',\n    \'chrUn_JTFH01000292v1_decoy\',\n    \'chrUn_JTFH01000293v1_decoy\',\n    \'chrUn_JTFH01000294v1_decoy\',\n    \'chrUn_JTFH01000295v1_decoy\',\n    \'chrUn_JTFH01000296v1_decoy\',\n    \'chrUn_JTFH01000297v1_decoy\',\n    \'chrUn_JTFH01000298v1_decoy\',\n    \'chrUn_JTFH01000299v1_decoy\',\n    \'chrUn_JTFH01000300v1_decoy\',\n    \'chrUn_JTFH01000301v1_decoy\',\n    \'chrUn_JTFH01000302v1_decoy\',\n    \'chrUn_JTFH01000303v1_decoy\',\n    \'chrUn_JTFH01000304v1_decoy\',\n    \'chrUn_JTFH01000305v1_decoy\',\n    \'chrUn_JTFH01000306v1_decoy\',\n    \'chrUn_JTFH01000307v1_decoy\',\n    \'chrUn_JTFH01000308v1_decoy\',\n    \'chrUn_JTFH01000309v1_decoy\',\n    \'chrUn_JTFH01000310v1_decoy\',\n    \'chrUn_JTFH01000311v1_decoy\',\n    \'chrUn_JTFH01000312v1_decoy\',\n    \'chrUn_JTFH01000313v1_decoy\',\n    \'chrUn_JTFH01000314v1_decoy\',\n    \'chrUn_JTFH01000315v1_decoy\',\n    \'chrUn_JTFH01000316v1_decoy\',\n    \'chrUn_JTFH01000317v1_decoy\',\n    \'chrUn_JTFH01000318v1_decoy\',\n    \'chrUn_JTFH01000319v1_decoy\',\n    \'chrUn_JTFH01000320v1_decoy\',\n    \'chrUn_JTFH01000321v1_decoy\',\n    \'chrUn_JTFH01000322v1_decoy\',\n    \'chrUn_JTFH01000323v1_decoy\',\n    \'chrUn_JTFH01000324v1_decoy\',\n    \'chrUn_JTFH01000325v1_decoy\',\n    \'chrUn_JTFH01000326v1_decoy\',\n    \'chrUn_JTFH01000327v1_decoy\',\n    \'chrUn_JTFH01000328v1_decoy\',\n    \'chrUn_JTFH01000329v1_decoy\',\n    \'chrUn_JTFH01000330v1_decoy\',\n    \'chrUn_JTFH01000331v1_decoy\',\n    \'chrUn_JTFH01000332v1_decoy\',\n    \'chrUn_JTFH01000333v1_decoy\',\n    \'chrUn_JTFH01000334v1_decoy\',\n    \'chrUn_JTFH01000335v1_decoy\',\n    \'chrUn_JTFH01000336v1_decoy\',\n    \'chrUn_JTFH01000337v1_decoy\',\n    \'chrUn_JTFH01000338v1_decoy\',\n    \'chrUn_JTFH01000339v1_decoy\',\n    \'chrUn_JTFH01000340v1_decoy\',\n    \'chrUn_JTFH01000341v1_decoy\',\n    \'chrUn_JTFH01000342v1_decoy\',\n    \'chrUn_JTFH01000343v1_decoy\',\n    \'chrUn_JTFH01000344v1_decoy\',\n    \'chrUn_JTFH01000345v1_decoy\',\n    \'chrUn_JTFH01000346v1_decoy\',\n    \'chrUn_JTFH01000347v1_decoy\',\n    \'chrUn_JTFH01000348v1_decoy\',\n    \'chrUn_JTFH01000349v1_decoy\',\n    \'chrUn_JTFH01000350v1_decoy\',\n    \'chrUn_JTFH01000351v1_decoy\',\n    \'chrUn_JTFH01000352v1_decoy\',\n    \'chrUn_JTFH01000353v1_decoy\',\n    \'chrUn_JTFH01000354v1_decoy\',\n    \'chrUn_JTFH01000355v1_decoy\',\n    \'chrUn_JTFH01000356v1_decoy\',\n    \'chrUn_JTFH01000357v1_decoy\',\n    \'chrUn_JTFH01000358v1_decoy\',\n    \'chrUn_JTFH01000359v1_decoy\',\n    \'chrUn_JTFH01000360v1_decoy\',\n    \'chrUn_JTFH01000361v1_decoy\',\n    \'chrUn_JTFH01000362v1_decoy\',\n    \'chrUn_JTFH01000363v1_decoy\',\n    \'chrUn_JTFH01000364v1_decoy\',\n    \'chrUn_JTFH01000365v1_decoy\',\n    \'chrUn_JTFH01000366v1_decoy\',\n    \'chrUn_JTFH01000367v1_decoy\',\n    \'chrUn_JTFH01000368v1_decoy\',\n    \'chrUn_JTFH01000369v1_decoy\',\n    \'chrUn_JTFH01000370v1_decoy\',\n    \'chrUn_JTFH01000371v1_decoy\',\n    \'chrUn_JTFH01000372v1_decoy\',\n    \'chrUn_JTFH01000373v1_decoy\',\n    \'chrUn_JTFH01000374v1_decoy\',\n    \'chrUn_JTFH01000375v1_decoy\',\n    \'chrUn_JTFH01000376v1_decoy\',\n    \'chrUn_JTFH01000377v1_decoy\',\n    \'chrUn_JTFH01000378v1_decoy\',\n    \'chrUn_JTFH01000379v1_decoy\',\n    \'chrUn_JTFH01000380v1_decoy\',\n    \'chrUn_JTFH01000381v1_decoy\',\n    \'chrUn_JTFH01000382v1_decoy\',\n    \'chrUn_JTFH01000383v1_decoy\',\n    \'chrUn_JTFH01000384v1_decoy\',\n    \'chrUn_JTFH01000385v1_decoy\',\n    \'chrUn_JTFH01000386v1_decoy\',\n    \'chrUn_JTFH01000387v1_decoy\',\n    \'chrUn_JTFH01000388v1_decoy\',\n    \'chrUn_JTFH01000389v1_decoy\',\n    \'chrUn_JTFH01000390v1_decoy\',\n    \'chrUn_JTFH01000391v1_decoy\',\n    \'chrUn_JTFH01000392v1_decoy\',\n    \'chrUn_JTFH01000393v1_decoy\',\n    \'chrUn_JTFH01000394v1_decoy\',\n    \'chrUn_JTFH01000395v1_decoy\',\n    \'chrUn_JTFH01000396v1_decoy\',\n    \'chrUn_JTFH01000397v1_decoy\',\n    \'chrUn_JTFH01000398v1_decoy\',\n    \'chrUn_JTFH01000399v1_decoy\',\n    \'chrUn_JTFH01000400v1_decoy\',\n    \'chrUn_JTFH01000401v1_decoy\',\n    \'chrUn_JTFH01000402v1_decoy\',\n    \'chrUn_JTFH01000403v1_decoy\',\n    \'chrUn_JTFH01000404v1_decoy\',\n    \'chrUn_JTFH01000405v1_decoy\',\n    \'chrUn_JTFH01000406v1_decoy\',\n    \'chrUn_JTFH01000407v1_decoy\',\n    \'chrUn_JTFH01000408v1_decoy\',\n    \'chrUn_JTFH01000409v1_decoy\',\n    \'chrUn_JTFH01000410v1_decoy\',\n    \'chrUn_JTFH01000411v1_decoy\',\n    \'chrUn_JTFH01000412v1_decoy\',\n    \'chrUn_JTFH01000413v1_decoy\',\n    \'chrUn_JTFH01000414v1_decoy\',\n    \'chrUn_JTFH01000415v1_decoy\',\n    \'chrUn_JTFH01000416v1_decoy\',\n    \'chrUn_JTFH01000417v1_decoy\',\n    \'chrUn_JTFH01000418v1_decoy\',\n    \'chrUn_JTFH01000419v1_decoy\',\n    \'chrUn_JTFH01000420v1_decoy\',\n    \'chrUn_JTFH01000421v1_decoy\',\n    \'chrUn_JTFH01000422v1_decoy\',\n    \'chrUn_JTFH01000423v1_decoy\',\n    \'chrUn_JTFH01000424v1_decoy\',\n    \'chrUn_JTFH01000425v1_decoy\',\n    \'chrUn_JTFH01000426v1_decoy\',\n    \'chrUn_JTFH01000427v1_decoy\',\n    \'chrUn_JTFH01000428v1_decoy\',\n    \'chrUn_JTFH01000429v1_decoy\',\n    \'chrUn_JTFH01000430v1_decoy\',\n    \'chrUn_JTFH01000431v1_decoy\',\n    \'chrUn_JTFH01000432v1_decoy\',\n    \'chrUn_JTFH01000433v1_decoy\',\n    \'chrUn_JTFH01000434v1_decoy\',\n    \'chrUn_JTFH01000435v1_decoy\',\n    \'chrUn_JTFH01000436v1_decoy\',\n    \'chrUn_JTFH01000437v1_decoy\',\n    \'chrUn_JTFH01000438v1_decoy\',\n    \'chrUn_JTFH01000439v1_decoy\',\n    \'chrUn_JTFH01000440v1_decoy\',\n    \'chrUn_JTFH01000441v1_decoy\',\n    \'chrUn_JTFH01000442v1_decoy\',\n    \'chrUn_JTFH01000443v1_decoy\',\n    \'chrUn_JTFH01000444v1_decoy\',\n    \'chrUn_JTFH01000445v1_decoy\',\n    \'chrUn_JTFH01000446v1_decoy\',\n    \'chrUn_JTFH01000447v1_decoy\',\n    \'chrUn_JTFH01000448v1_decoy\',\n    \'chrUn_JTFH01000449v1_decoy\',\n    \'chrUn_JTFH01000450v1_decoy\',\n    \'chrUn_JTFH01000451v1_decoy\',\n    \'chrUn_JTFH01000452v1_decoy\',\n    \'chrUn_JTFH01000453v1_decoy\',\n    \'chrUn_JTFH01000454v1_decoy\',\n    \'chrUn_JTFH01000455v1_decoy\',\n    \'chrUn_JTFH01000456v1_decoy\',\n    \'chrUn_JTFH01000457v1_decoy\',\n    \'chrUn_JTFH01000458v1_decoy\',\n    \'chrUn_JTFH01000459v1_decoy\',\n    \'chrUn_JTFH01000460v1_decoy\',\n    \'chrUn_JTFH01000461v1_decoy\',\n    \'chrUn_JTFH01000462v1_decoy\',\n    \'chrUn_JTFH01000463v1_decoy\',\n    \'chrUn_JTFH01000464v1_decoy\',\n    \'chrUn_JTFH01000465v1_decoy\',\n    \'chrUn_JTFH01000466v1_decoy\',\n    \'chrUn_JTFH01000467v1_decoy\',\n    \'chrUn_JTFH01000468v1_decoy\',\n    \'chrUn_JTFH01000469v1_decoy\',\n    \'chrUn_JTFH01000470v1_decoy\',\n    \'chrUn_JTFH01000471v1_decoy\',\n    \'chrUn_JTFH01000472v1_decoy\',\n    \'chrUn_JTFH01000473v1_decoy\',\n    \'chrUn_JTFH01000474v1_decoy\',\n    \'chrUn_JTFH01000475v1_decoy\',\n    \'chrUn_JTFH01000476v1_decoy\',\n    \'chrUn_JTFH01000477v1_decoy\',\n    \'chrUn_JTFH01000478v1_decoy\',\n    \'chrUn_JTFH01000479v1_decoy\',\n    \'chrUn_JTFH01000480v1_decoy\',\n    \'chrUn_JTFH01000481v1_decoy\',\n    \'chrUn_JTFH01000482v1_decoy\',\n    \'chrUn_JTFH01000483v1_decoy\',\n    \'chrUn_JTFH01000484v1_decoy\',\n    \'chrUn_JTFH01000485v1_decoy\',\n    \'chrUn_JTFH01000486v1_decoy\',\n    \'chrUn_JTFH01000487v1_decoy\',\n    \'chrUn_JTFH01000488v1_decoy\',\n    \'chrUn_JTFH01000489v1_decoy\',\n    \'chrUn_JTFH01000490v1_decoy\',\n    \'chrUn_JTFH01000491v1_decoy\',\n    \'chrUn_JTFH01000492v1_decoy\',\n    \'chrUn_JTFH01000493v1_decoy\',\n    \'chrUn_JTFH01000494v1_decoy\',\n    \'chrUn_JTFH01000495v1_decoy\',\n    \'chrUn_JTFH01000496v1_decoy\',\n    \'chrUn_JTFH01000497v1_decoy\',\n    \'chrUn_JTFH01000498v1_decoy\',\n    \'chrUn_JTFH01000499v1_decoy\',\n    \'chrUn_JTFH01000500v1_decoy\',\n    \'chrUn_JTFH01000501v1_decoy\',\n    \'chrUn_JTFH01000502v1_decoy\',\n    \'chrUn_JTFH01000503v1_decoy\',\n    \'chrUn_JTFH01000504v1_decoy\',\n    \'chrUn_JTFH01000505v1_decoy\',\n    \'chrUn_JTFH01000506v1_decoy\',\n    \'chrUn_JTFH01000507v1_decoy\',\n    \'chrUn_JTFH01000508v1_decoy\',\n    \'chrUn_JTFH01000509v1_decoy\',\n    \'chrUn_JTFH01000510v1_decoy\',\n    \'chrUn_JTFH01000511v1_decoy\',\n    \'chrUn_JTFH01000512v1_decoy\',\n    \'chrUn_JTFH01000513v1_decoy\',\n    \'chrUn_JTFH01000514v1_decoy\',\n    \'chrUn_JTFH01000515v1_decoy\',\n    \'chrUn_JTFH01000516v1_decoy\',\n    \'chrUn_JTFH01000517v1_decoy\',\n    \'chrUn_JTFH01000518v1_decoy\',\n    \'chrUn_JTFH01000519v1_decoy\',\n    \'chrUn_JTFH01000520v1_decoy\',\n    \'chrUn_JTFH01000521v1_decoy\',\n    \'chrUn_JTFH01000522v1_decoy\',\n    \'chrUn_JTFH01000523v1_decoy\',\n    \'chrUn_JTFH01000524v1_decoy\',\n    \'chrUn_JTFH01000525v1_decoy\',\n    \'chrUn_JTFH01000526v1_decoy\',\n    \'chrUn_JTFH01000527v1_decoy\',\n    \'chrUn_JTFH01000528v1_decoy\',\n    \'chrUn_JTFH01000529v1_decoy\',\n    \'chrUn_JTFH01000530v1_decoy\',\n    \'chrUn_JTFH01000531v1_decoy\',\n    \'chrUn_JTFH01000532v1_decoy\',\n    \'chrUn_JTFH01000533v1_decoy\',\n    \'chrUn_JTFH01000534v1_decoy\',\n    \'chrUn_JTFH01000535v1_decoy\',\n    \'chrUn_JTFH01000536v1_decoy\',\n    \'chrUn_JTFH01000537v1_decoy\',\n    \'chrUn_JTFH01000538v1_decoy\',\n    \'chrUn_JTFH01000539v1_decoy\',\n    \'chrUn_JTFH01000540v1_decoy\',\n    \'chrUn_JTFH01000541v1_decoy\',\n    \'chrUn_JTFH01000542v1_decoy\',\n    \'chrUn_JTFH01000543v1_decoy\',\n    \'chrUn_JTFH01000544v1_decoy\',\n    \'chrUn_JTFH01000545v1_decoy\',\n    \'chrUn_JTFH01000546v1_decoy\',\n    \'chrUn_JTFH01000547v1_decoy\',\n    \'chrUn_JTFH01000548v1_decoy\',\n    \'chrUn_JTFH01000549v1_decoy\',\n    \'chrUn_JTFH01000550v1_decoy\',\n    \'chrUn_JTFH01000551v1_decoy\',\n    \'chrUn_JTFH01000552v1_decoy\',\n    \'chrUn_JTFH01000553v1_decoy\',\n    \'chrUn_JTFH01000554v1_decoy\',\n    \'chrUn_JTFH01000555v1_decoy\',\n    \'chrUn_JTFH01000556v1_decoy\',\n    \'chrUn_JTFH01000557v1_decoy\',\n    \'chrUn_JTFH01000558v1_decoy\',\n    \'chrUn_JTFH01000559v1_decoy\',\n    \'chrUn_JTFH01000560v1_decoy\',\n    \'chrUn_JTFH01000561v1_decoy\',\n    \'chrUn_JTFH01000562v1_decoy\',\n    \'chrUn_JTFH01000563v1_decoy\',\n    \'chrUn_JTFH01000564v1_decoy\',\n    \'chrUn_JTFH01000565v1_decoy\',\n    \'chrUn_JTFH01000566v1_decoy\',\n    \'chrUn_JTFH01000567v1_decoy\',\n    \'chrUn_JTFH01000568v1_decoy\',\n    \'chrUn_JTFH01000569v1_decoy\',\n    \'chrUn_JTFH01000570v1_decoy\',\n    \'chrUn_JTFH01000571v1_decoy\',\n    \'chrUn_JTFH01000572v1_decoy\',\n    \'chrUn_JTFH01000573v1_decoy\',\n    \'chrUn_JTFH01000574v1_decoy\',\n    \'chrUn_JTFH01000575v1_decoy\',\n    \'chrUn_JTFH01000576v1_decoy\',\n    \'chrUn_JTFH01000577v1_decoy\',\n    \'chrUn_JTFH01000578v1_decoy\',\n    \'chrUn_JTFH01000579v1_decoy\',\n    \'chrUn_JTFH01000580v1_decoy\',\n    \'chrUn_JTFH01000581v1_decoy\',\n    \'chrUn_JTFH01000582v1_decoy\',\n    \'chrUn_JTFH01000583v1_decoy\',\n    \'chrUn_JTFH01000584v1_decoy\',\n    \'chrUn_JTFH01000585v1_decoy\',\n    \'chrUn_JTFH01000586v1_decoy\',\n    \'chrUn_JTFH01000587v1_decoy\',\n    \'chrUn_JTFH01000588v1_decoy\',\n    \'chrUn_JTFH01000589v1_decoy\',\n    \'chrUn_JTFH01000590v1_decoy\',\n    \'chrUn_JTFH01000591v1_decoy\',\n    \'chrUn_JTFH01000592v1_decoy\',\n    \'chrUn_JTFH01000593v1_decoy\',\n    \'chrUn_JTFH01000594v1_decoy\',\n    \'chrUn_JTFH01000595v1_decoy\',\n    \'chrUn_JTFH01000596v1_decoy\',\n    \'chrUn_JTFH01000597v1_decoy\',\n    \'chrUn_JTFH01000598v1_decoy\',\n    \'chrUn_JTFH01000599v1_decoy\',\n    \'chrUn_JTFH01000600v1_decoy\',\n    \'chrUn_JTFH01000601v1_decoy\',\n    \'chrUn_JTFH01000602v1_decoy\',\n    \'chrUn_JTFH01000603v1_decoy\',\n    \'chrUn_JTFH01000604v1_decoy\',\n    \'chrUn_JTFH01000605v1_decoy\',\n    \'chrUn_JTFH01000606v1_decoy\',\n    \'chrUn_JTFH01000607v1_decoy\',\n    \'chrUn_JTFH01000608v1_decoy\',\n    \'chrUn_JTFH01000609v1_decoy\',\n    \'chrUn_JTFH01000610v1_decoy\',\n    \'chrUn_JTFH01000611v1_decoy\',\n    \'chrUn_JTFH01000612v1_decoy\',\n    \'chrUn_JTFH01000613v1_decoy\',\n    \'chrUn_JTFH01000614v1_decoy\',\n    \'chrUn_JTFH01000615v1_decoy\',\n    \'chrUn_JTFH01000616v1_decoy\',\n    \'chrUn_JTFH01000617v1_decoy\',\n    \'chrUn_JTFH01000618v1_decoy\',\n    \'chrUn_JTFH01000619v1_decoy\',\n    \'chrUn_JTFH01000620v1_decoy\',\n    \'chrUn_JTFH01000621v1_decoy\',\n    \'chrUn_JTFH01000622v1_decoy\',\n    \'chrUn_JTFH01000623v1_decoy\',\n    \'chrUn_JTFH01000624v1_decoy\',\n    \'chrUn_JTFH01000625v1_decoy\',\n    \'chrUn_JTFH01000626v1_decoy\',\n    \'chrUn_JTFH01000627v1_decoy\',\n    \'chrUn_JTFH01000628v1_decoy\',\n    \'chrUn_JTFH01000629v1_decoy\',\n    \'chrUn_JTFH01000630v1_decoy\',\n    \'chrUn_JTFH01000631v1_decoy\',\n    \'chrUn_JTFH01000632v1_decoy\',\n    \'chrUn_JTFH01000633v1_decoy\',\n    \'chrUn_JTFH01000634v1_decoy\',\n    \'chrUn_JTFH01000635v1_decoy\',\n    \'chrUn_JTFH01000636v1_decoy\',\n    \'chrUn_JTFH01000637v1_decoy\',\n    \'chrUn_JTFH01000638v1_decoy\',\n    \'chrUn_JTFH01000639v1_decoy\',\n    \'chrUn_JTFH01000640v1_decoy\',\n    \'chrUn_JTFH01000641v1_decoy\',\n    \'chrUn_JTFH01000642v1_decoy\',\n    \'chrUn_JTFH01000643v1_decoy\',\n    \'chrUn_JTFH01000644v1_decoy\',\n    \'chrUn_JTFH01000645v1_decoy\',\n    \'chrUn_JTFH01000646v1_decoy\',\n    \'chrUn_JTFH01000647v1_decoy\',\n    \'chrUn_JTFH01000648v1_decoy\',\n    \'chrUn_JTFH01000649v1_decoy\',\n    \'chrUn_JTFH01000650v1_decoy\',\n    \'chrUn_JTFH01000651v1_decoy\',\n    \'chrUn_JTFH01000652v1_decoy\',\n    \'chrUn_JTFH01000653v1_decoy\',\n    \'chrUn_JTFH01000654v1_decoy\',\n    \'chrUn_JTFH01000655v1_decoy\',\n    \'chrUn_JTFH01000656v1_decoy\',\n    \'chrUn_JTFH01000657v1_decoy\',\n    \'chrUn_JTFH01000658v1_decoy\',\n    \'chrUn_JTFH01000659v1_decoy\',\n    \'chrUn_JTFH01000660v1_decoy\',\n    \'chrUn_JTFH01000661v1_decoy\',\n    \'chrUn_JTFH01000662v1_decoy\',\n    \'chrUn_JTFH01000663v1_decoy\',\n    \'chrUn_JTFH01000664v1_decoy\',\n    \'chrUn_JTFH01000665v1_decoy\',\n    \'chrUn_JTFH01000666v1_decoy\',\n    \'chrUn_JTFH01000667v1_decoy\',\n    \'chrUn_JTFH01000668v1_decoy\',\n    \'chrUn_JTFH01000669v1_decoy\',\n    \'chrUn_JTFH01000670v1_decoy\',\n    \'chrUn_JTFH01000671v1_decoy\',\n    \'chrUn_JTFH01000672v1_decoy\',\n    \'chrUn_JTFH01000673v1_decoy\',\n    \'chrUn_JTFH01000674v1_decoy\',\n    \'chrUn_JTFH01000675v1_decoy\',\n    \'chrUn_JTFH01000676v1_decoy\',\n    \'chrUn_JTFH01000677v1_decoy\',\n    \'chrUn_JTFH01000678v1_decoy\',\n    \'chrUn_JTFH01000679v1_decoy\',\n    \'chrUn_JTFH01000680v1_decoy\',\n    \'chrUn_JTFH01000681v1_decoy\',\n    \'chrUn_JTFH01000682v1_decoy\',\n    \'chrUn_JTFH01000683v1_decoy\',\n    \'chrUn_JTFH01000684v1_decoy\',\n    \'chrUn_JTFH01000685v1_decoy\',\n    \'chrUn_JTFH01000686v1_decoy\',\n    \'chrUn_JTFH01000687v1_decoy\',\n    \'chrUn_JTFH01000688v1_decoy\',\n    \'chrUn_JTFH01000689v1_decoy\',\n    \'chrUn_JTFH01000690v1_decoy\',\n    \'chrUn_JTFH01000691v1_decoy\',\n    \'chrUn_JTFH01000692v1_decoy\',\n    \'chrUn_JTFH01000693v1_decoy\',\n    \'chrUn_JTFH01000694v1_decoy\',\n    \'chrUn_JTFH01000695v1_decoy\',\n    \'chrUn_JTFH01000696v1_decoy\',\n    \'chrUn_JTFH01000697v1_decoy\',\n    \'chrUn_JTFH01000698v1_decoy\',\n    \'chrUn_JTFH01000699v1_decoy\',\n    \'chrUn_JTFH01000700v1_decoy\',\n    \'chrUn_JTFH01000701v1_decoy\',\n    \'chrUn_JTFH01000702v1_decoy\',\n    \'chrUn_JTFH01000703v1_decoy\',\n    \'chrUn_JTFH01000704v1_decoy\',\n    \'chrUn_JTFH01000705v1_decoy\',\n    \'chrUn_JTFH01000706v1_decoy\',\n    \'chrUn_JTFH01000707v1_decoy\',\n    \'chrUn_JTFH01000708v1_decoy\',\n    \'chrUn_JTFH01000709v1_decoy\',\n    \'chrUn_JTFH01000710v1_decoy\',\n    \'chrUn_JTFH01000711v1_decoy\',\n    \'chrUn_JTFH01000712v1_decoy\',\n    \'chrUn_JTFH01000713v1_decoy\',\n    \'chrUn_JTFH01000714v1_decoy\',\n    \'chrUn_JTFH01000715v1_decoy\',\n    \'chrUn_JTFH01000716v1_decoy\',\n    \'chrUn_JTFH01000717v1_decoy\',\n    \'chrUn_JTFH01000718v1_decoy\',\n    \'chrUn_JTFH01000719v1_decoy\',\n    \'chrUn_JTFH01000720v1_decoy\',\n    \'chrUn_JTFH01000721v1_decoy\',\n    \'chrUn_JTFH01000722v1_decoy\',\n    \'chrUn_JTFH01000723v1_decoy\',\n    \'chrUn_JTFH01000724v1_decoy\',\n    \'chrUn_JTFH01000725v1_decoy\',\n    \'chrUn_JTFH01000726v1_decoy\',\n    \'chrUn_JTFH01000727v1_decoy\',\n    \'chrUn_JTFH01000728v1_decoy\',\n    \'chrUn_JTFH01000729v1_decoy\',\n    \'chrUn_JTFH01000730v1_decoy\',\n    \'chrUn_JTFH01000731v1_decoy\',\n    \'chrUn_JTFH01000732v1_decoy\',\n    \'chrUn_JTFH01000733v1_decoy\',\n    \'chrUn_JTFH01000734v1_decoy\',\n    \'chrUn_JTFH01000735v1_decoy\',\n    \'chrUn_JTFH01000736v1_decoy\',\n    \'chrUn_JTFH01000737v1_decoy\',\n    \'chrUn_JTFH01000738v1_decoy\',\n    \'chrUn_JTFH01000739v1_decoy\',\n    \'chrUn_JTFH01000740v1_decoy\',\n    \'chrUn_JTFH01000741v1_decoy\',\n    \'chrUn_JTFH01000742v1_decoy\',\n    \'chrUn_JTFH01000743v1_decoy\',\n    \'chrUn_JTFH01000744v1_decoy\',\n    \'chrUn_JTFH01000745v1_decoy\',\n    \'chrUn_JTFH01000746v1_decoy\',\n    \'chrUn_JTFH01000747v1_decoy\',\n    \'chrUn_JTFH01000748v1_decoy\',\n    \'chrUn_JTFH01000749v1_decoy\',\n    \'chrUn_JTFH01000750v1_decoy\',\n    \'chrUn_JTFH01000751v1_decoy\',\n    \'chrUn_JTFH01000752v1_decoy\',\n    \'chrUn_JTFH01000753v1_decoy\',\n    \'chrUn_JTFH01000754v1_decoy\',\n    \'chrUn_JTFH01000755v1_decoy\',\n    \'chrUn_JTFH01000756v1_decoy\',\n    \'chrUn_JTFH01000757v1_decoy\',\n    \'chrUn_JTFH01000758v1_decoy\',\n    \'chrUn_JTFH01000759v1_decoy\',\n    \'chrUn_JTFH01000760v1_decoy\',\n    \'chrUn_JTFH01000761v1_decoy\',\n    \'chrUn_JTFH01000762v1_decoy\',\n    \'chrUn_JTFH01000763v1_decoy\',\n    \'chrUn_JTFH01000764v1_decoy\',\n    \'chrUn_JTFH01000765v1_decoy\',\n    \'chrUn_JTFH01000766v1_decoy\',\n    \'chrUn_JTFH01000767v1_decoy\',\n    \'chrUn_JTFH01000768v1_decoy\',\n    \'chrUn_JTFH01000769v1_decoy\',\n    \'chrUn_JTFH01000770v1_decoy\',\n    \'chrUn_JTFH01000771v1_decoy\',\n    \'chrUn_JTFH01000772v1_decoy\',\n    \'chrUn_JTFH01000773v1_decoy\',\n    \'chrUn_JTFH01000774v1_decoy\',\n    \'chrUn_JTFH01000775v1_decoy\',\n    \'chrUn_JTFH01000776v1_decoy\',\n    \'chrUn_JTFH01000777v1_decoy\',\n    \'chrUn_JTFH01000778v1_decoy\',\n    \'chrUn_JTFH01000779v1_decoy\',\n    \'chrUn_JTFH01000780v1_decoy\',\n    \'chrUn_JTFH01000781v1_decoy\',\n    \'chrUn_JTFH01000782v1_decoy\',\n    \'chrUn_JTFH01000783v1_decoy\',\n    \'chrUn_JTFH01000784v1_decoy\',\n    \'chrUn_JTFH01000785v1_decoy\',\n    \'chrUn_JTFH01000786v1_decoy\',\n    \'chrUn_JTFH01000787v1_decoy\',\n    \'chrUn_JTFH01000788v1_decoy\',\n    \'chrUn_JTFH01000789v1_decoy\',\n    \'chrUn_JTFH01000790v1_decoy\',\n    \'chrUn_JTFH01000791v1_decoy\',\n    \'chrUn_JTFH01000792v1_decoy\',\n    \'chrUn_JTFH01000793v1_decoy\',\n    \'chrUn_JTFH01000794v1_decoy\',\n    \'chrUn_JTFH01000795v1_decoy\',\n    \'chrUn_JTFH01000796v1_decoy\',\n    \'chrUn_JTFH01000797v1_decoy\',\n    \'chrUn_JTFH01000798v1_decoy\',\n    \'chrUn_JTFH01000799v1_decoy\',\n    \'chrUn_JTFH01000800v1_decoy\',\n    \'chrUn_JTFH01000801v1_decoy\',\n    \'chrUn_JTFH01000802v1_decoy\',\n    \'chrUn_JTFH01000803v1_decoy\',\n    \'chrUn_JTFH01000804v1_decoy\',\n    \'chrUn_JTFH01000805v1_decoy\',\n    \'chrUn_JTFH01000806v1_decoy\',\n    \'chrUn_JTFH01000807v1_decoy\',\n    \'chrUn_JTFH01000808v1_decoy\',\n    \'chrUn_JTFH01000809v1_decoy\',\n    \'chrUn_JTFH01000810v1_decoy\',\n    \'chrUn_JTFH01000811v1_decoy\',\n    \'chrUn_JTFH01000812v1_decoy\',\n    \'chrUn_JTFH01000813v1_decoy\',\n    \'chrUn_JTFH01000814v1_decoy\',\n    \'chrUn_JTFH01000815v1_decoy\',\n    \'chrUn_JTFH01000816v1_decoy\',\n    \'chrUn_JTFH01000817v1_decoy\',\n    \'chrUn_JTFH01000818v1_decoy\',\n    \'chrUn_JTFH01000819v1_decoy\',\n    \'chrUn_JTFH01000820v1_decoy\',\n    \'chrUn_JTFH01000821v1_decoy\',\n    \'chrUn_JTFH01000822v1_decoy\',\n    \'chrUn_JTFH01000823v1_decoy\',\n    \'chrUn_JTFH01000824v1_decoy\',\n    \'chrUn_JTFH01000825v1_decoy\',\n    \'chrUn_JTFH01000826v1_decoy\',\n    \'chrUn_JTFH01000827v1_decoy\',\n    \'chrUn_JTFH01000828v1_decoy\',\n    \'chrUn_JTFH01000829v1_decoy\',\n    \'chrUn_JTFH01000830v1_decoy\',\n    \'chrUn_JTFH01000831v1_decoy\',\n    \'chrUn_JTFH01000832v1_decoy\',\n    \'chrUn_JTFH01000833v1_decoy\',\n    \'chrUn_JTFH01000834v1_decoy\',\n    \'chrUn_JTFH01000835v1_decoy\',\n    \'chrUn_JTFH01000836v1_decoy\',\n    \'chrUn_JTFH01000837v1_decoy\',\n    \'chrUn_JTFH01000838v1_decoy\',\n    \'chrUn_JTFH01000839v1_decoy\',\n    \'chrUn_JTFH01000840v1_decoy\',\n    \'chrUn_JTFH01000841v1_decoy\',\n    \'chrUn_JTFH01000842v1_decoy\',\n    \'chrUn_JTFH01000843v1_decoy\',\n    \'chrUn_JTFH01000844v1_decoy\',\n    \'chrUn_JTFH01000845v1_decoy\',\n    \'chrUn_JTFH01000846v1_decoy\',\n    \'chrUn_JTFH01000847v1_decoy\',\n    \'chrUn_JTFH01000848v1_decoy\',\n    \'chrUn_JTFH01000849v1_decoy\',\n    \'chrUn_JTFH01000850v1_decoy\',\n    \'chrUn_JTFH01000851v1_decoy\',\n    \'chrUn_JTFH01000852v1_decoy\',\n    \'chrUn_JTFH01000853v1_decoy\',\n    \'chrUn_JTFH01000854v1_decoy\',\n    \'chrUn_JTFH01000855v1_decoy\',\n    \'chrUn_JTFH01000856v1_decoy\',\n    \'chrUn_JTFH01000857v1_decoy\',\n    \'chrUn_JTFH01000858v1_decoy\',\n    \'chrUn_JTFH01000859v1_decoy\',\n    \'chrUn_JTFH01000860v1_decoy\',\n    \'chrUn_JTFH01000861v1_decoy\',\n    \'chrUn_JTFH01000862v1_decoy\',\n    \'chrUn_JTFH01000863v1_decoy\',\n    \'chrUn_JTFH01000864v1_decoy\',\n    \'chrUn_JTFH01000865v1_decoy\',\n    \'chrUn_JTFH01000866v1_decoy\',\n    \'chrUn_JTFH01000867v1_decoy\',\n    \'chrUn_JTFH01000868v1_decoy\',\n    \'chrUn_JTFH01000869v1_decoy\',\n    \'chrUn_JTFH01000870v1_decoy\',\n    \'chrUn_JTFH01000871v1_decoy\',\n    \'chrUn_JTFH01000872v1_decoy\',\n    \'chrUn_JTFH01000873v1_decoy\',\n    \'chrUn_JTFH01000874v1_decoy\',\n    \'chrUn_JTFH01000875v1_decoy\',\n    \'chrUn_JTFH01000876v1_decoy\',\n    \'chrUn_JTFH01000877v1_decoy\',\n    \'chrUn_JTFH01000878v1_decoy\',\n    \'chrUn_JTFH01000879v1_decoy\',\n    \'chrUn_JTFH01000880v1_decoy\',\n    \'chrUn_JTFH01000881v1_decoy\',\n    \'chrUn_JTFH01000882v1_decoy\',\n    \'chrUn_JTFH01000883v1_decoy\',\n    \'chrUn_JTFH01000884v1_decoy\',\n    \'chrUn_JTFH01000885v1_decoy\',\n    \'chrUn_JTFH01000886v1_decoy\',\n    \'chrUn_JTFH01000887v1_decoy\',\n    \'chrUn_JTFH01000888v1_decoy\',\n    \'chrUn_JTFH01000889v1_decoy\',\n    \'chrUn_JTFH01000890v1_decoy\',\n    \'chrUn_JTFH01000891v1_decoy\',\n    \'chrUn_JTFH01000892v1_decoy\',\n    \'chrUn_JTFH01000893v1_decoy\',\n    \'chrUn_JTFH01000894v1_decoy\',\n    \'chrUn_JTFH01000895v1_decoy\',\n    \'chrUn_JTFH01000896v1_decoy\',\n    \'chrUn_JTFH01000897v1_decoy\',\n    \'chrUn_JTFH01000898v1_decoy\',\n    \'chrUn_JTFH01000899v1_decoy\',\n    \'chrUn_JTFH01000900v1_decoy\',\n    \'chrUn_JTFH01000901v1_decoy\',\n    \'chrUn_JTFH01000902v1_decoy\',\n    \'chrUn_JTFH01000903v1_decoy\',\n    \'chrUn_JTFH01000904v1_decoy\',\n    \'chrUn_JTFH01000905v1_decoy\',\n    \'chrUn_JTFH01000906v1_decoy\',\n    \'chrUn_JTFH01000907v1_decoy\',\n    \'chrUn_JTFH01000908v1_decoy\',\n    \'chrUn_JTFH01000909v1_decoy\',\n    \'chrUn_JTFH01000910v1_decoy\',\n    \'chrUn_JTFH01000911v1_decoy\',\n    \'chrUn_JTFH01000912v1_decoy\',\n    \'chrUn_JTFH01000913v1_decoy\',\n    \'chrUn_JTFH01000914v1_decoy\',\n    \'chrUn_JTFH01000915v1_decoy\',\n    \'chrUn_JTFH01000916v1_decoy\',\n    \'chrUn_JTFH01000917v1_decoy\',\n    \'chrUn_JTFH01000918v1_decoy\',\n    \'chrUn_JTFH01000919v1_decoy\',\n    \'chrUn_JTFH01000920v1_decoy\',\n    \'chrUn_JTFH01000921v1_decoy\',\n    \'chrUn_JTFH01000922v1_decoy\',\n    \'chrUn_JTFH01000923v1_decoy\',\n    \'chrUn_JTFH01000924v1_decoy\',\n    \'chrUn_JTFH01000925v1_decoy\',\n    \'chrUn_JTFH01000926v1_decoy\',\n    \'chrUn_JTFH01000927v1_decoy\',\n    \'chrUn_JTFH01000928v1_decoy\',\n    \'chrUn_JTFH01000929v1_decoy\',\n    \'chrUn_JTFH01000930v1_decoy\',\n    \'chrUn_JTFH01000931v1_decoy\',\n    \'chrUn_JTFH01000932v1_decoy\',\n    \'chrUn_JTFH01000933v1_decoy\',\n    \'chrUn_JTFH01000934v1_decoy\',\n    \'chrUn_JTFH01000935v1_decoy\',\n    \'chrUn_JTFH01000936v1_decoy\',\n    \'chrUn_JTFH01000937v1_decoy\',\n    \'chrUn_JTFH01000938v1_decoy\',\n    \'chrUn_JTFH01000939v1_decoy\',\n    \'chrUn_JTFH01000940v1_decoy\',\n    \'chrUn_JTFH01000941v1_decoy\',\n    \'chrUn_JTFH01000942v1_decoy\',\n    \'chrUn_JTFH01000943v1_decoy\',\n    \'chrUn_JTFH01000944v1_decoy\',\n    \'chrUn_JTFH01000945v1_decoy\',\n    \'chrUn_JTFH01000946v1_decoy\',\n    \'chrUn_JTFH01000947v1_decoy\',\n    \'chrUn_JTFH01000948v1_decoy\',\n    \'chrUn_JTFH01000949v1_decoy\',\n    \'chrUn_JTFH01000950v1_decoy\',\n    \'chrUn_JTFH01000951v1_decoy\',\n    \'chrUn_JTFH01000952v1_decoy\',\n    \'chrUn_JTFH01000953v1_decoy\',\n    \'chrUn_JTFH01000954v1_decoy\',\n    \'chrUn_JTFH01000955v1_decoy\',\n    \'chrUn_JTFH01000956v1_decoy\',\n    \'chrUn_JTFH01000957v1_decoy\',\n    \'chrUn_JTFH01000958v1_decoy\',\n    \'chrUn_JTFH01000959v1_decoy\',\n    \'chrUn_JTFH01000960v1_decoy\',\n    \'chrUn_JTFH01000961v1_decoy\',\n    \'chrUn_JTFH01000962v1_decoy\',\n    \'chrUn_JTFH01000963v1_decoy\',\n    \'chrUn_JTFH01000964v1_decoy\',\n    \'chrUn_JTFH01000965v1_decoy\',\n    \'chrUn_JTFH01000966v1_decoy\',\n    \'chrUn_JTFH01000967v1_decoy\',\n    \'chrUn_JTFH01000968v1_decoy\',\n    \'chrUn_JTFH01000969v1_decoy\',\n    \'chrUn_JTFH01000970v1_decoy\',\n    \'chrUn_JTFH01000971v1_decoy\',\n    \'chrUn_JTFH01000972v1_decoy\',\n    \'chrUn_JTFH01000973v1_decoy\',\n    \'chrUn_JTFH01000974v1_decoy\',\n    \'chrUn_JTFH01000975v1_decoy\',\n    \'chrUn_JTFH01000976v1_decoy\',\n    \'chrUn_JTFH01000977v1_decoy\',\n    \'chrUn_JTFH01000978v1_decoy\',\n    \'chrUn_JTFH01000979v1_decoy\',\n    \'chrUn_JTFH01000980v1_decoy\',\n    \'chrUn_JTFH01000981v1_decoy\',\n    \'chrUn_JTFH01000982v1_decoy\',\n    \'chrUn_JTFH01000983v1_decoy\',\n    \'chrUn_JTFH01000984v1_decoy\',\n    \'chrUn_JTFH01000985v1_decoy\',\n    \'chrUn_JTFH01000986v1_decoy\',\n    \'chrUn_JTFH01000987v1_decoy\',\n    \'chrUn_JTFH01000988v1_decoy\',\n    \'chrUn_JTFH01000989v1_decoy\',\n    \'chrUn_JTFH01000990v1_decoy\',\n    \'chrUn_JTFH01000991v1_decoy\',\n    \'chrUn_JTFH01000992v1_decoy\',\n    \'chrUn_JTFH01000993v1_decoy\',\n    \'chrUn_JTFH01000994v1_decoy\',\n    \'chrUn_JTFH01000995v1_decoy\',\n    \'chrUn_JTFH01000996v1_decoy\',\n    \'chrUn_JTFH01000997v1_decoy\',\n    \'chrUn_JTFH01000998v1_decoy\',\n    \'chrUn_JTFH01000999v1_decoy\',\n    \'chrUn_JTFH01001000v1_decoy\',\n    \'chrUn_JTFH01001001v1_decoy\',\n    \'chrUn_JTFH01001002v1_decoy\',\n    \'chrUn_JTFH01001003v1_decoy\',\n    \'chrUn_JTFH01001004v1_decoy\',\n    \'chrUn_JTFH01001005v1_decoy\',\n    \'chrUn_JTFH01001006v1_decoy\',\n    \'chrUn_JTFH01001007v1_decoy\',\n    \'chrUn_JTFH01001008v1_decoy\',\n    \'chrUn_JTFH01001009v1_decoy\',\n    \'chrUn_JTFH01001010v1_decoy\',\n    \'chrUn_JTFH01001011v1_decoy\',\n    \'chrUn_JTFH01001012v1_decoy\',\n    \'chrUn_JTFH01001013v1_decoy\',\n    \'chrUn_JTFH01001014v1_decoy\',\n    \'chrUn_JTFH01001015v1_decoy\',\n    \'chrUn_JTFH01001016v1_decoy\',\n    \'chrUn_JTFH01001017v1_decoy\',\n    \'chrUn_JTFH01001018v1_decoy\',\n    \'chrUn_JTFH01001019v1_decoy\',\n    \'chrUn_JTFH01001020v1_decoy\',\n    \'chrUn_JTFH01001021v1_decoy\',\n    \'chrUn_JTFH01001022v1_decoy\',\n    \'chrUn_JTFH01001023v1_decoy\',\n    \'chrUn_JTFH01001024v1_decoy\',\n    \'chrUn_JTFH01001025v1_decoy\',\n    \'chrUn_JTFH01001026v1_decoy\',\n    \'chrUn_JTFH01001027v1_decoy\',\n    \'chrUn_JTFH01001028v1_decoy\',\n    \'chrUn_JTFH01001029v1_decoy\',\n    \'chrUn_JTFH01001030v1_decoy\',\n    \'chrUn_JTFH01001031v1_decoy\',\n    \'chrUn_JTFH01001032v1_decoy\',\n    \'chrUn_JTFH01001033v1_decoy\',\n    \'chrUn_JTFH01001034v1_decoy\',\n    \'chrUn_JTFH01001035v1_decoy\',\n    \'chrUn_JTFH01001036v1_decoy\',\n    \'chrUn_JTFH01001037v1_decoy\',\n    \'chrUn_JTFH01001038v1_decoy\',\n    \'chrUn_JTFH01001039v1_decoy\',\n    \'chrUn_JTFH01001040v1_decoy\',\n    \'chrUn_JTFH01001041v1_decoy\',\n    \'chrUn_JTFH01001042v1_decoy\',\n    \'chrUn_JTFH01001043v1_decoy\',\n    \'chrUn_JTFH01001044v1_decoy\',\n    \'chrUn_JTFH01001045v1_decoy\',\n    \'chrUn_JTFH01001046v1_decoy\',\n    \'chrUn_JTFH01001047v1_decoy\',\n    \'chrUn_JTFH01001048v1_decoy\',\n    \'chrUn_JTFH01001049v1_decoy\',\n    \'chrUn_JTFH01001050v1_decoy\',\n    \'chrUn_JTFH01001051v1_decoy\',\n    \'chrUn_JTFH01001052v1_decoy\',\n    \'chrUn_JTFH01001053v1_decoy\',\n    \'chrUn_JTFH01001054v1_decoy\',\n    \'chrUn_JTFH01001055v1_decoy\',\n    \'chrUn_JTFH01001056v1_decoy\',\n    \'chrUn_JTFH01001057v1_decoy\',\n    \'chrUn_JTFH01001058v1_decoy\',\n    \'chrUn_JTFH01001059v1_decoy\',\n    \'chrUn_JTFH01001060v1_decoy\',\n    \'chrUn_JTFH01001061v1_decoy\',\n    \'chrUn_JTFH01001062v1_decoy\',\n    \'chrUn_JTFH01001063v1_decoy\',\n    \'chrUn_JTFH01001064v1_decoy\',\n    \'chrUn_JTFH01001065v1_decoy\',\n    \'chrUn_JTFH01001066v1_decoy\',\n    \'chrUn_JTFH01001067v1_decoy\',\n    \'chrUn_JTFH01001068v1_decoy\',\n    \'chrUn_JTFH01001069v1_decoy\',\n    \'chrUn_JTFH01001070v1_decoy\',\n    \'chrUn_JTFH01001071v1_decoy\',\n    \'chrUn_JTFH01001072v1_decoy\',\n    \'chrUn_JTFH01001073v1_decoy\',\n    \'chrUn_JTFH01001074v1_decoy\',\n    \'chrUn_JTFH01001075v1_decoy\',\n    \'chrUn_JTFH01001076v1_decoy\',\n    \'chrUn_JTFH01001077v1_decoy\',\n    \'chrUn_JTFH01001078v1_decoy\',\n    \'chrUn_JTFH01001079v1_decoy\',\n    \'chrUn_JTFH01001080v1_decoy\',\n    \'chrUn_JTFH01001081v1_decoy\',\n    \'chrUn_JTFH01001082v1_decoy\',\n    \'chrUn_JTFH01001083v1_decoy\',\n    \'chrUn_JTFH01001084v1_decoy\',\n    \'chrUn_JTFH01001085v1_decoy\',\n    \'chrUn_JTFH01001086v1_decoy\',\n    \'chrUn_JTFH01001087v1_decoy\',\n    \'chrUn_JTFH01001088v1_decoy\',\n    \'chrUn_JTFH01001089v1_decoy\',\n    \'chrUn_JTFH01001090v1_decoy\',\n    \'chrUn_JTFH01001091v1_decoy\',\n    \'chrUn_JTFH01001092v1_decoy\',\n    \'chrUn_JTFH01001093v1_decoy\',\n    \'chrUn_JTFH01001094v1_decoy\',\n    \'chrUn_JTFH01001095v1_decoy\',\n    \'chrUn_JTFH01001096v1_decoy\',\n    \'chrUn_JTFH01001097v1_decoy\',\n    \'chrUn_JTFH01001098v1_decoy\',\n    \'chrUn_JTFH01001099v1_decoy\',\n    \'chrUn_JTFH01001100v1_decoy\',\n    \'chrUn_JTFH01001101v1_decoy\',\n    \'chrUn_JTFH01001102v1_decoy\',\n    \'chrUn_JTFH01001103v1_decoy\',\n    \'chrUn_JTFH01001104v1_decoy\',\n    \'chrUn_JTFH01001105v1_decoy\',\n    \'chrUn_JTFH01001106v1_decoy\',\n    \'chrUn_JTFH01001107v1_decoy\',\n    \'chrUn_JTFH01001108v1_decoy\',\n    \'chrUn_JTFH01001109v1_decoy\',\n    \'chrUn_JTFH01001110v1_decoy\',\n    \'chrUn_JTFH01001111v1_decoy\',\n    \'chrUn_JTFH01001112v1_decoy\',\n    \'chrUn_JTFH01001113v1_decoy\',\n    \'chrUn_JTFH01001114v1_decoy\',\n    \'chrUn_JTFH01001115v1_decoy\',\n    \'chrUn_JTFH01001116v1_decoy\',\n    \'chrUn_JTFH01001117v1_decoy\',\n    \'chrUn_JTFH01001118v1_decoy\',\n    \'chrUn_JTFH01001119v1_decoy\',\n    \'chrUn_JTFH01001120v1_decoy\',\n    \'chrUn_JTFH01001121v1_decoy\',\n    \'chrUn_JTFH01001122v1_decoy\',\n    \'chrUn_JTFH01001123v1_decoy\',\n    \'chrUn_JTFH01001124v1_decoy\',\n    \'chrUn_JTFH01001125v1_decoy\',\n    \'chrUn_JTFH01001126v1_decoy\',\n    \'chrUn_JTFH01001127v1_decoy\',\n    \'chrUn_JTFH01001128v1_decoy\',\n    \'chrUn_JTFH01001129v1_decoy\',\n    \'chrUn_JTFH01001130v1_decoy\',\n    \'chrUn_JTFH01001131v1_decoy\',\n    \'chrUn_JTFH01001132v1_decoy\',\n    \'chrUn_JTFH01001133v1_decoy\',\n    \'chrUn_JTFH01001134v1_decoy\',\n    \'chrUn_JTFH01001135v1_decoy\',\n    \'chrUn_JTFH01001136v1_decoy\',\n    \'chrUn_JTFH01001137v1_decoy\',\n    \'chrUn_JTFH01001138v1_decoy\',\n    \'chrUn_JTFH01001139v1_decoy\',\n    \'chrUn_JTFH01001140v1_decoy\',\n    \'chrUn_JTFH01001141v1_decoy\',\n    \'chrUn_JTFH01001142v1_decoy\',\n    \'chrUn_JTFH01001143v1_decoy\',\n    \'chrUn_JTFH01001144v1_decoy\',\n    \'chrUn_JTFH01001145v1_decoy\',\n    \'chrUn_JTFH01001146v1_decoy\',\n    \'chrUn_JTFH01001147v1_decoy\',\n    \'chrUn_JTFH01001148v1_decoy\',\n    \'chrUn_JTFH01001149v1_decoy\',\n    \'chrUn_JTFH01001150v1_decoy\',\n    \'chrUn_JTFH01001151v1_decoy\',\n    \'chrUn_JTFH01001152v1_decoy\',\n    \'chrUn_JTFH01001153v1_decoy\',\n    \'chrUn_JTFH01001154v1_decoy\',\n    \'chrUn_JTFH01001155v1_decoy\',\n    \'chrUn_JTFH01001156v1_decoy\',\n    \'chrUn_JTFH01001157v1_decoy\',\n    \'chrUn_JTFH01001158v1_decoy\',\n    \'chrUn_JTFH01001159v1_decoy\',\n    \'chrUn_JTFH01001160v1_decoy\',\n    \'chrUn_JTFH01001161v1_decoy\',\n    \'chrUn_JTFH01001162v1_decoy\',\n    \'chrUn_JTFH01001163v1_decoy\',\n    \'chrUn_JTFH01001164v1_decoy\',\n    \'chrUn_JTFH01001165v1_decoy\',\n    \'chrUn_JTFH01001166v1_decoy\',\n    \'chrUn_JTFH01001167v1_decoy\',\n    \'chrUn_JTFH01001168v1_decoy\',\n    \'chrUn_JTFH01001169v1_decoy\',\n    \'chrUn_JTFH01001170v1_decoy\',\n    \'chrUn_JTFH01001171v1_decoy\',\n    \'chrUn_JTFH01001172v1_decoy\',\n    \'chrUn_JTFH01001173v1_decoy\',\n    \'chrUn_JTFH01001174v1_decoy\',\n    \'chrUn_JTFH01001175v1_decoy\',\n    \'chrUn_JTFH01001176v1_decoy\',\n    \'chrUn_JTFH01001177v1_decoy\',\n    \'chrUn_JTFH01001178v1_decoy\',\n    \'chrUn_JTFH01001179v1_decoy\',\n    \'chrUn_JTFH01001180v1_decoy\',\n    \'chrUn_JTFH01001181v1_decoy\',\n    \'chrUn_JTFH01001182v1_decoy\',\n    \'chrUn_JTFH01001183v1_decoy\',\n    \'chrUn_JTFH01001184v1_decoy\',\n    \'chrUn_JTFH01001185v1_decoy\',\n    \'chrUn_JTFH01001186v1_decoy\',\n    \'chrUn_JTFH01001187v1_decoy\',\n    \'chrUn_JTFH01001188v1_decoy\',\n    \'chrUn_JTFH01001189v1_decoy\',\n    \'chrUn_JTFH01001190v1_decoy\',\n    \'chrUn_JTFH01001191v1_decoy\',\n    \'chrUn_JTFH01001192v1_decoy\',\n    \'chrUn_JTFH01001193v1_decoy\',\n    \'chrUn_JTFH01001194v1_decoy\',\n    \'chrUn_JTFH01001195v1_decoy\',\n    \'chrUn_JTFH01001196v1_decoy\',\n    \'chrUn_JTFH01001197v1_decoy\',\n    \'chrUn_JTFH01001198v1_decoy\',\n    \'chrUn_JTFH01001199v1_decoy\',\n    \'chrUn_JTFH01001200v1_decoy\',\n    \'chrUn_JTFH01001201v1_decoy\',\n    \'chrUn_JTFH01001202v1_decoy\',\n    \'chrUn_JTFH01001203v1_decoy\',\n    \'chrUn_JTFH01001204v1_decoy\',\n    \'chrUn_JTFH01001205v1_decoy\',\n    \'chrUn_JTFH01001206v1_decoy\',\n    \'chrUn_JTFH01001207v1_decoy\',\n    \'chrUn_JTFH01001208v1_decoy\',\n    \'chrUn_JTFH01001209v1_decoy\',\n    \'chrUn_JTFH01001210v1_decoy\',\n    \'chrUn_JTFH01001211v1_decoy\',\n    \'chrUn_JTFH01001212v1_decoy\',\n    \'chrUn_JTFH01001213v1_decoy\',\n    \'chrUn_JTFH01001214v1_decoy\',\n    \'chrUn_JTFH01001215v1_decoy\',\n    \'chrUn_JTFH01001216v1_decoy\',\n    \'chrUn_JTFH01001217v1_decoy\',\n    \'chrUn_JTFH01001218v1_decoy\',\n    \'chrUn_JTFH01001219v1_decoy\',\n    \'chrUn_JTFH01001220v1_decoy\',\n    \'chrUn_JTFH01001221v1_decoy\',\n    \'chrUn_JTFH01001222v1_decoy\',\n    \'chrUn_JTFH01001223v1_decoy\',\n    \'chrUn_JTFH01001224v1_decoy\',\n    \'chrUn_JTFH01001225v1_decoy\',\n    \'chrUn_JTFH01001226v1_decoy\',\n    \'chrUn_JTFH01001227v1_decoy\',\n    \'chrUn_JTFH01001228v1_decoy\',\n    \'chrUn_JTFH01001229v1_decoy\',\n    \'chrUn_JTFH01001230v1_decoy\',\n    \'chrUn_JTFH01001231v1_decoy\',\n    \'chrUn_JTFH01001232v1_decoy\',\n    \'chrUn_JTFH01001233v1_decoy\',\n    \'chrUn_JTFH01001234v1_decoy\',\n    \'chrUn_JTFH01001235v1_decoy\',\n    \'chrUn_JTFH01001236v1_decoy\',\n    \'chrUn_JTFH01001237v1_decoy\',\n    \'chrUn_JTFH01001238v1_decoy\',\n    \'chrUn_JTFH01001239v1_decoy\',\n    \'chrUn_JTFH01001240v1_decoy\',\n    \'chrUn_JTFH01001241v1_decoy\',\n    \'chrUn_JTFH01001242v1_decoy\',\n    \'chrUn_JTFH01001243v1_decoy\',\n    \'chrUn_JTFH01001244v1_decoy\',\n    \'chrUn_JTFH01001245v1_decoy\',\n    \'chrUn_JTFH01001246v1_decoy\',\n    \'chrUn_JTFH01001247v1_decoy\',\n    \'chrUn_JTFH01001248v1_decoy\',\n    \'chrUn_JTFH01001249v1_decoy\',\n    \'chrUn_JTFH01001250v1_decoy\',\n    \'chrUn_JTFH01001251v1_decoy\',\n    \'chrUn_JTFH01001252v1_decoy\',\n    \'chrUn_JTFH01001253v1_decoy\',\n    \'chrUn_JTFH01001254v1_decoy\',\n    \'chrUn_JTFH01001255v1_decoy\',\n    \'chrUn_JTFH01001256v1_decoy\',\n    \'chrUn_JTFH01001257v1_decoy\',\n    \'chrUn_JTFH01001258v1_decoy\',\n    \'chrUn_JTFH01001259v1_decoy\',\n    \'chrUn_JTFH01001260v1_decoy\',\n    \'chrUn_JTFH01001261v1_decoy\',\n    \'chrUn_JTFH01001262v1_decoy\',\n    \'chrUn_JTFH01001263v1_decoy\',\n    \'chrUn_JTFH01001264v1_decoy\',\n    \'chrUn_JTFH01001265v1_decoy\',\n    \'chrUn_JTFH01001266v1_decoy\',\n    \'chrUn_JTFH01001267v1_decoy\',\n    \'chrUn_JTFH01001268v1_decoy\',\n    \'chrUn_JTFH01001269v1_decoy\',\n    \'chrUn_JTFH01001270v1_decoy\',\n    \'chrUn_JTFH01001271v1_decoy\',\n    \'chrUn_JTFH01001272v1_decoy\',\n    \'chrUn_JTFH01001273v1_decoy\',\n    \'chrUn_JTFH01001274v1_decoy\',\n    \'chrUn_JTFH01001275v1_decoy\',\n    \'chrUn_JTFH01001276v1_decoy\',\n    \'chrUn_JTFH01001277v1_decoy\',\n    \'chrUn_JTFH01001278v1_decoy\',\n    \'chrUn_JTFH01001279v1_decoy\',\n    \'chrUn_JTFH01001280v1_decoy\',\n    \'chrUn_JTFH01001281v1_decoy\',\n    \'chrUn_JTFH01001282v1_decoy\',\n    \'chrUn_JTFH01001283v1_decoy\',\n    \'chrUn_JTFH01001284v1_decoy\',\n    \'chrUn_JTFH01001285v1_decoy\',\n    \'chrUn_JTFH01001286v1_decoy\',\n    \'chrUn_JTFH01001287v1_decoy\',\n    \'chrUn_JTFH01001288v1_decoy\',\n    \'chrUn_JTFH01001289v1_decoy\',\n    \'chrUn_JTFH01001290v1_decoy\',\n    \'chrUn_JTFH01001291v1_decoy\',\n    \'chrUn_JTFH01001292v1_decoy\',\n    \'chrUn_JTFH01001293v1_decoy\',\n    \'chrUn_JTFH01001294v1_decoy\',\n    \'chrUn_JTFH01001295v1_decoy\',\n    \'chrUn_JTFH01001296v1_decoy\',\n    \'chrUn_JTFH01001297v1_decoy\',\n    \'chrUn_JTFH01001298v1_decoy\',\n    \'chrUn_JTFH01001299v1_decoy\',\n    \'chrUn_JTFH01001300v1_decoy\',\n    \'chrUn_JTFH01001301v1_decoy\',\n    \'chrUn_JTFH01001302v1_decoy\',\n    \'chrUn_JTFH01001303v1_decoy\',\n    \'chrUn_JTFH01001304v1_decoy\',\n    \'chrUn_JTFH01001305v1_decoy\',\n    \'chrUn_JTFH01001306v1_decoy\',\n    \'chrUn_JTFH01001307v1_decoy\',\n    \'chrUn_JTFH01001308v1_decoy\',\n    \'chrUn_JTFH01001309v1_decoy\',\n    \'chrUn_JTFH01001310v1_decoy\',\n    \'chrUn_JTFH01001311v1_decoy\',\n    \'chrUn_JTFH01001312v1_decoy\',\n    \'chrUn_JTFH01001313v1_decoy\',\n    \'chrUn_JTFH01001314v1_decoy\',\n    \'chrUn_JTFH01001315v1_decoy\',\n    \'chrUn_JTFH01001316v1_decoy\',\n    \'chrUn_JTFH01001317v1_decoy\',\n    \'chrUn_JTFH01001318v1_decoy\',\n    \'chrUn_JTFH01001319v1_decoy\',\n    \'chrUn_JTFH01001320v1_decoy\',\n    \'chrUn_JTFH01001321v1_decoy\',\n    \'chrUn_JTFH01001322v1_decoy\',\n    \'chrUn_JTFH01001323v1_decoy\',\n    \'chrUn_JTFH01001324v1_decoy\',\n    \'chrUn_JTFH01001325v1_decoy\',\n    \'chrUn_JTFH01001326v1_decoy\',\n    \'chrUn_JTFH01001327v1_decoy\',\n    \'chrUn_JTFH01001328v1_decoy\',\n    \'chrUn_JTFH01001329v1_decoy\',\n    \'chrUn_JTFH01001330v1_decoy\',\n    \'chrUn_JTFH01001331v1_decoy\',\n    \'chrUn_JTFH01001332v1_decoy\',\n    \'chrUn_JTFH01001333v1_decoy\',\n    \'chrUn_JTFH01001334v1_decoy\',\n    \'chrUn_JTFH01001335v1_decoy\',\n    \'chrUn_JTFH01001336v1_decoy\',\n    \'chrUn_JTFH01001337v1_decoy\',\n    \'chrUn_JTFH01001338v1_decoy\',\n    \'chrUn_JTFH01001339v1_decoy\',\n    \'chrUn_JTFH01001340v1_decoy\',\n    \'chrUn_JTFH01001341v1_decoy\',\n    \'chrUn_JTFH01001342v1_decoy\',\n    \'chrUn_JTFH01001343v1_decoy\',\n    \'chrUn_JTFH01001344v1_decoy\',\n    \'chrUn_JTFH01001345v1_decoy\',\n    \'chrUn_JTFH01001346v1_decoy\',\n    \'chrUn_JTFH01001347v1_decoy\',\n    \'chrUn_JTFH01001348v1_decoy\',\n    \'chrUn_JTFH01001349v1_decoy\',\n    \'chrUn_JTFH01001350v1_decoy\',\n    \'chrUn_JTFH01001351v1_decoy\',\n    \'chrUn_JTFH01001352v1_decoy\',\n    \'chrUn_JTFH01001353v1_decoy\',\n    \'chrUn_JTFH01001354v1_decoy\',\n    \'chrUn_JTFH01001355v1_decoy\',\n    \'chrUn_JTFH01001356v1_decoy\',\n    \'chrUn_JTFH01001357v1_decoy\',\n    \'chrUn_JTFH01001358v1_decoy\',\n    \'chrUn_JTFH01001359v1_decoy\',\n    \'chrUn_JTFH01001360v1_decoy\',\n    \'chrUn_JTFH01001361v1_decoy\',\n    \'chrUn_JTFH01001362v1_decoy\',\n    \'chrUn_JTFH01001363v1_decoy\',\n    \'chrUn_JTFH01001364v1_decoy\',\n    \'chrUn_JTFH01001365v1_decoy\',\n    \'chrUn_JTFH01001366v1_decoy\',\n    \'chrUn_JTFH01001367v1_decoy\',\n    \'chrUn_JTFH01001368v1_decoy\',\n    \'chrUn_JTFH01001369v1_decoy\',\n    \'chrUn_JTFH01001370v1_decoy\',\n    \'chrUn_JTFH01001371v1_decoy\',\n    \'chrUn_JTFH01001372v1_decoy\',\n    \'chrUn_JTFH01001373v1_decoy\',\n    \'chrUn_JTFH01001374v1_decoy\',\n    \'chrUn_JTFH01001375v1_decoy\',\n    \'chrUn_JTFH01001376v1_decoy\',\n    \'chrUn_JTFH01001377v1_decoy\',\n    \'chrUn_JTFH01001378v1_decoy\',\n    \'chrUn_JTFH01001379v1_decoy\',\n    \'chrUn_JTFH01001380v1_decoy\',\n    \'chrUn_JTFH01001381v1_decoy\',\n    \'chrUn_JTFH01001382v1_decoy\',\n    \'chrUn_JTFH01001383v1_decoy\',\n    \'chrUn_JTFH01001384v1_decoy\',\n    \'chrUn_JTFH01001385v1_decoy\',\n    \'chrUn_JTFH01001386v1_decoy\',\n    \'chrUn_JTFH01001387v1_decoy\',\n    \'chrUn_JTFH01001388v1_decoy\',\n    \'chrUn_JTFH01001389v1_decoy\',\n    \'chrUn_JTFH01001390v1_decoy\',\n    \'chrUn_JTFH01001391v1_decoy\',\n    \'chrUn_JTFH01001392v1_decoy\',\n    \'chrUn_JTFH01001393v1_decoy\',\n    \'chrUn_JTFH01001394v1_decoy\',\n    \'chrUn_JTFH01001395v1_decoy\',\n    \'chrUn_JTFH01001396v1_decoy\',\n    \'chrUn_JTFH01001397v1_decoy\',\n    \'chrUn_JTFH01001398v1_decoy\',\n    \'chrUn_JTFH01001399v1_decoy\',\n    \'chrUn_JTFH01001400v1_decoy\',\n    \'chrUn_JTFH01001401v1_decoy\',\n    \'chrUn_JTFH01001402v1_decoy\',\n    \'chrUn_JTFH01001403v1_decoy\',\n    \'chrUn_JTFH01001404v1_decoy\',\n    \'chrUn_JTFH01001405v1_decoy\',\n    \'chrUn_JTFH01001406v1_decoy\',\n    \'chrUn_JTFH01001407v1_decoy\',\n    \'chrUn_JTFH01001408v1_decoy\',\n    \'chrUn_JTFH01001409v1_decoy\',\n    \'chrUn_JTFH01001410v1_decoy\',\n    \'chrUn_JTFH01001411v1_decoy\',\n    \'chrUn_JTFH01001412v1_decoy\',\n    \'chrUn_JTFH01001413v1_decoy\',\n    \'chrUn_JTFH01001414v1_decoy\',\n    \'chrUn_JTFH01001415v1_decoy\',\n    \'chrUn_JTFH01001416v1_decoy\',\n    \'chrUn_JTFH01001417v1_decoy\',\n    \'chrUn_JTFH01001418v1_decoy\',\n    \'chrUn_JTFH01001419v1_decoy\',\n    \'chrUn_JTFH01001420v1_decoy\',\n    \'chrUn_JTFH01001421v1_decoy\',\n    \'chrUn_JTFH01001422v1_decoy\',\n    \'chrUn_JTFH01001423v1_decoy\',\n    \'chrUn_JTFH01001424v1_decoy\',\n    \'chrUn_JTFH01001425v1_decoy\',\n    \'chrUn_JTFH01001426v1_decoy\',\n    \'chrUn_JTFH01001427v1_decoy\',\n    \'chrUn_JTFH01001428v1_decoy\',\n    \'chrUn_JTFH01001429v1_decoy\',\n    \'chrUn_JTFH01001430v1_decoy\',\n    \'chrUn_JTFH01001431v1_decoy\',\n    \'chrUn_JTFH01001432v1_decoy\',\n    \'chrUn_JTFH01001433v1_decoy\',\n    \'chrUn_JTFH01001434v1_decoy\',\n    \'chrUn_JTFH01001435v1_decoy\',\n    \'chrUn_JTFH01001436v1_decoy\',\n    \'chrUn_JTFH01001437v1_decoy\',\n    \'chrUn_JTFH01001438v1_decoy\',\n    \'chrUn_JTFH01001439v1_decoy\',\n    \'chrUn_JTFH01001440v1_decoy\',\n    \'chrUn_JTFH01001441v1_decoy\',\n    \'chrUn_JTFH01001442v1_decoy\',\n    \'chrUn_JTFH01001443v1_decoy\',\n    \'chrUn_JTFH01001444v1_decoy\',\n    \'chrUn_JTFH01001445v1_decoy\',\n    \'chrUn_JTFH01001446v1_decoy\',\n    \'chrUn_JTFH01001447v1_decoy\',\n    \'chrUn_JTFH01001448v1_decoy\',\n    \'chrUn_JTFH01001449v1_decoy\',\n    \'chrUn_JTFH01001450v1_decoy\',\n    \'chrUn_JTFH01001451v1_decoy\',\n    \'chrUn_JTFH01001452v1_decoy\',\n    \'chrUn_JTFH01001453v1_decoy\',\n    \'chrUn_JTFH01001454v1_decoy\',\n    \'chrUn_JTFH01001455v1_decoy\',\n    \'chrUn_JTFH01001456v1_decoy\',\n    \'chrUn_JTFH01001457v1_decoy\',\n    \'chrUn_JTFH01001458v1_decoy\',\n    \'chrUn_JTFH01001459v1_decoy\',\n    \'chrUn_JTFH01001460v1_decoy\',\n    \'chrUn_JTFH01001461v1_decoy\',\n    \'chrUn_JTFH01001462v1_decoy\',\n    \'chrUn_JTFH01001463v1_decoy\',\n    \'chrUn_JTFH01001464v1_decoy\',\n    \'chrUn_JTFH01001465v1_decoy\',\n    \'chrUn_JTFH01001466v1_decoy\',\n    \'chrUn_JTFH01001467v1_decoy\',\n    \'chrUn_JTFH01001468v1_decoy\',\n    \'chrUn_JTFH01001469v1_decoy\',\n    \'chrUn_JTFH01001470v1_decoy\',\n    \'chrUn_JTFH01001471v1_decoy\',\n    \'chrUn_JTFH01001472v1_decoy\',\n    \'chrUn_JTFH01001473v1_decoy\',\n    \'chrUn_JTFH01001474v1_decoy\',\n    \'chrUn_JTFH01001475v1_decoy\',\n    \'chrUn_JTFH01001476v1_decoy\',\n    \'chrUn_JTFH01001477v1_decoy\',\n    \'chrUn_JTFH01001478v1_decoy\',\n    \'chrUn_JTFH01001479v1_decoy\',\n    \'chrUn_JTFH01001480v1_decoy\',\n    \'chrUn_JTFH01001481v1_decoy\',\n    \'chrUn_JTFH01001482v1_decoy\',\n    \'chrUn_JTFH01001483v1_decoy\',\n    \'chrUn_JTFH01001484v1_decoy\',\n    \'chrUn_JTFH01001485v1_decoy\',\n    \'chrUn_JTFH01001486v1_decoy\',\n    \'chrUn_JTFH01001487v1_decoy\',\n    \'chrUn_JTFH01001488v1_decoy\',\n    \'chrUn_JTFH01001489v1_decoy\',\n    \'chrUn_JTFH01001490v1_decoy\',\n    \'chrUn_JTFH01001491v1_decoy\',\n    \'chrUn_JTFH01001492v1_decoy\',\n    \'chrUn_JTFH01001493v1_decoy\',\n    \'chrUn_JTFH01001494v1_decoy\',\n    \'chrUn_JTFH01001495v1_decoy\',\n    \'chrUn_JTFH01001496v1_decoy\',\n    \'chrUn_JTFH01001497v1_decoy\',\n    \'chrUn_JTFH01001498v1_decoy\',\n    \'chrUn_JTFH01001499v1_decoy\',\n    \'chrUn_JTFH01001500v1_decoy\',\n    \'chrUn_JTFH01001501v1_decoy\',\n    \'chrUn_JTFH01001502v1_decoy\',\n    \'chrUn_JTFH01001503v1_decoy\',\n    \'chrUn_JTFH01001504v1_decoy\',\n    \'chrUn_JTFH01001505v1_decoy\',\n    \'chrUn_JTFH01001506v1_decoy\',\n    \'chrUn_JTFH01001507v1_decoy\',\n    \'chrUn_JTFH01001508v1_decoy\',\n    \'chrUn_JTFH01001509v1_decoy\',\n    \'chrUn_JTFH01001510v1_decoy\',\n    \'chrUn_JTFH01001511v1_decoy\',\n    \'chrUn_JTFH01001512v1_decoy\',\n    \'chrUn_JTFH01001513v1_decoy\',\n    \'chrUn_JTFH01001514v1_decoy\',\n    \'chrUn_JTFH01001515v1_decoy\',\n    \'chrUn_JTFH01001516v1_decoy\',\n    \'chrUn_JTFH01001517v1_decoy\',\n    \'chrUn_JTFH01001518v1_decoy\',\n    \'chrUn_JTFH01001519v1_decoy\',\n    \'chrUn_JTFH01001520v1_decoy\',\n    \'chrUn_JTFH01001521v1_decoy\',\n    \'chrUn_JTFH01001522v1_decoy\',\n    \'chrUn_JTFH01001523v1_decoy\',\n    \'chrUn_JTFH01001524v1_decoy\',\n    \'chrUn_JTFH01001525v1_decoy\',\n    \'chrUn_JTFH01001526v1_decoy\',\n    \'chrUn_JTFH01001527v1_decoy\',\n    \'chrUn_JTFH01001528v1_decoy\',\n    \'chrUn_JTFH01001529v1_decoy\',\n    \'chrUn_JTFH01001530v1_decoy\',\n    \'chrUn_JTFH01001531v1_decoy\',\n    \'chrUn_JTFH01001532v1_decoy\',\n    \'chrUn_JTFH01001533v1_decoy\',\n    \'chrUn_JTFH01001534v1_decoy\',\n    \'chrUn_JTFH01001535v1_decoy\',\n    \'chrUn_JTFH01001536v1_decoy\',\n    \'chrUn_JTFH01001537v1_decoy\',\n    \'chrUn_JTFH01001538v1_decoy\',\n    \'chrUn_JTFH01001539v1_decoy\',\n    \'chrUn_JTFH01001540v1_decoy\',\n    \'chrUn_JTFH01001541v1_decoy\',\n    \'chrUn_JTFH01001542v1_decoy\',\n    \'chrUn_JTFH01001543v1_decoy\',\n    \'chrUn_JTFH01001544v1_decoy\',\n    \'chrUn_JTFH01001545v1_decoy\',\n    \'chrUn_JTFH01001546v1_decoy\',\n    \'chrUn_JTFH01001547v1_decoy\',\n    \'chrUn_JTFH01001548v1_decoy\',\n    \'chrUn_JTFH01001549v1_decoy\',\n    \'chrUn_JTFH01001550v1_decoy\',\n    \'chrUn_JTFH01001551v1_decoy\',\n    \'chrUn_JTFH01001552v1_decoy\',\n    \'chrUn_JTFH01001553v1_decoy\',\n    \'chrUn_JTFH01001554v1_decoy\',\n    \'chrUn_JTFH01001555v1_decoy\',\n    \'chrUn_JTFH01001556v1_decoy\',\n    \'chrUn_JTFH01001557v1_decoy\',\n    \'chrUn_JTFH01001558v1_decoy\',\n    \'chrUn_JTFH01001559v1_decoy\',\n    \'chrUn_JTFH01001560v1_decoy\',\n    \'chrUn_JTFH01001561v1_decoy\',\n    \'chrUn_JTFH01001562v1_decoy\',\n    \'chrUn_JTFH01001563v1_decoy\',\n    \'chrUn_JTFH01001564v1_decoy\',\n    \'chrUn_JTFH01001565v1_decoy\',\n    \'chrUn_JTFH01001566v1_decoy\',\n    \'chrUn_JTFH01001567v1_decoy\',\n    \'chrUn_JTFH01001568v1_decoy\',\n    \'chrUn_JTFH01001569v1_decoy\',\n    \'chrUn_JTFH01001570v1_decoy\',\n    \'chrUn_JTFH01001571v1_decoy\',\n    \'chrUn_JTFH01001572v1_decoy\',\n    \'chrUn_JTFH01001573v1_decoy\',\n    \'chrUn_JTFH01001574v1_decoy\',\n    \'chrUn_JTFH01001575v1_decoy\',\n    \'chrUn_JTFH01001576v1_decoy\',\n    \'chrUn_JTFH01001577v1_decoy\',\n    \'chrUn_JTFH01001578v1_decoy\',\n    \'chrUn_JTFH01001579v1_decoy\',\n    \'chrUn_JTFH01001580v1_decoy\',\n    \'chrUn_JTFH01001581v1_decoy\',\n    \'chrUn_JTFH01001582v1_decoy\',\n    \'chrUn_JTFH01001583v1_decoy\',\n    \'chrUn_JTFH01001584v1_decoy\',\n    \'chrUn_JTFH01001585v1_decoy\',\n    \'chrUn_JTFH01001586v1_decoy\',\n    \'chrUn_JTFH01001587v1_decoy\',\n    \'chrUn_JTFH01001588v1_decoy\',\n    \'chrUn_JTFH01001589v1_decoy\',\n    \'chrUn_JTFH01001590v1_decoy\',\n    \'chrUn_JTFH01001591v1_decoy\',\n    \'chrUn_JTFH01001592v1_decoy\',\n    \'chrUn_JTFH01001593v1_decoy\',\n    \'chrUn_JTFH01001594v1_decoy\',\n    \'chrUn_JTFH01001595v1_decoy\',\n    \'chrUn_JTFH01001596v1_decoy\',\n    \'chrUn_JTFH01001597v1_decoy\',\n    \'chrUn_JTFH01001598v1_decoy\',\n    \'chrUn_JTFH01001599v1_decoy\',\n    \'chrUn_JTFH01001600v1_decoy\',\n    \'chrUn_JTFH01001601v1_decoy\',\n    \'chrUn_JTFH01001602v1_decoy\',\n    \'chrUn_JTFH01001603v1_decoy\',\n    \'chrUn_JTFH01001604v1_decoy\',\n    \'chrUn_JTFH01001605v1_decoy\',\n    \'chrUn_JTFH01001606v1_decoy\',\n    \'chrUn_JTFH01001607v1_decoy\',\n    \'chrUn_JTFH01001608v1_decoy\',\n    \'chrUn_JTFH01001609v1_decoy\',\n    \'chrUn_JTFH01001610v1_decoy\',\n    \'chrUn_JTFH01001611v1_decoy\',\n    \'chrUn_JTFH01001612v1_decoy\',\n    \'chrUn_JTFH01001613v1_decoy\',\n    \'chrUn_JTFH01001614v1_decoy\',\n    \'chrUn_JTFH01001615v1_decoy\',\n    \'chrUn_JTFH01001616v1_decoy\',\n    \'chrUn_JTFH01001617v1_decoy\',\n    \'chrUn_JTFH01001618v1_decoy\',\n    \'chrUn_JTFH01001619v1_decoy\',\n    \'chrUn_JTFH01001620v1_decoy\',\n    \'chrUn_JTFH01001621v1_decoy\',\n    \'chrUn_JTFH01001622v1_decoy\',\n    \'chrUn_JTFH01001623v1_decoy\',\n    \'chrUn_JTFH01001624v1_decoy\',\n    \'chrUn_JTFH01001625v1_decoy\',\n    \'chrUn_JTFH01001626v1_decoy\',\n    \'chrUn_JTFH01001627v1_decoy\',\n    \'chrUn_JTFH01001628v1_decoy\',\n    \'chrUn_JTFH01001629v1_decoy\',\n    \'chrUn_JTFH01001630v1_decoy\',\n    \'chrUn_JTFH01001631v1_decoy\',\n    \'chrUn_JTFH01001632v1_decoy\',\n    \'chrUn_JTFH01001633v1_decoy\',\n    \'chrUn_JTFH01001634v1_decoy\',\n    \'chrUn_JTFH01001635v1_decoy\',\n    \'chrUn_JTFH01001636v1_decoy\',\n    \'chrUn_JTFH01001637v1_decoy\',\n    \'chrUn_JTFH01001638v1_decoy\',\n    \'chrUn_JTFH01001639v1_decoy\',\n    \'chrUn_JTFH01001640v1_decoy\',\n    \'chrUn_JTFH01001641v1_decoy\',\n    \'chrUn_JTFH01001642v1_decoy\',\n    \'chrUn_JTFH01001643v1_decoy\',\n    \'chrUn_JTFH01001644v1_decoy\',\n    \'chrUn_JTFH01001645v1_decoy\',\n    \'chrUn_JTFH01001646v1_decoy\',\n    \'chrUn_JTFH01001647v1_decoy\',\n    \'chrUn_JTFH01001648v1_decoy\',\n    \'chrUn_JTFH01001649v1_decoy\',\n    \'chrUn_JTFH01001650v1_decoy\',\n    \'chrUn_JTFH01001651v1_decoy\',\n    \'chrUn_JTFH01001652v1_decoy\',\n    \'chrUn_JTFH01001653v1_decoy\',\n    \'chrUn_JTFH01001654v1_decoy\',\n    \'chrUn_JTFH01001655v1_decoy\',\n    \'chrUn_JTFH01001656v1_decoy\',\n    \'chrUn_JTFH01001657v1_decoy\',\n    \'chrUn_JTFH01001658v1_decoy\',\n    \'chrUn_JTFH01001659v1_decoy\',\n    \'chrUn_JTFH01001660v1_decoy\',\n    \'chrUn_JTFH01001661v1_decoy\',\n    \'chrUn_JTFH01001662v1_decoy\',\n    \'chrUn_JTFH01001663v1_decoy\',\n    \'chrUn_JTFH01001664v1_decoy\',\n    \'chrUn_JTFH01001665v1_decoy\',\n    \'chrUn_JTFH01001666v1_decoy\',\n    \'chrUn_JTFH01001667v1_decoy\',\n    \'chrUn_JTFH01001668v1_decoy\',\n    \'chrUn_JTFH01001669v1_decoy\',\n    \'chrUn_JTFH01001670v1_decoy\',\n    \'chrUn_JTFH01001671v1_decoy\',\n    \'chrUn_JTFH01001672v1_decoy\',\n    \'chrUn_JTFH01001673v1_decoy\',\n    \'chrUn_JTFH01001674v1_decoy\',\n    \'chrUn_JTFH01001675v1_decoy\',\n    \'chrUn_JTFH01001676v1_decoy\',\n    \'chrUn_JTFH01001677v1_decoy\',\n    \'chrUn_JTFH01001678v1_decoy\',\n    \'chrUn_JTFH01001679v1_decoy\',\n    \'chrUn_JTFH01001680v1_decoy\',\n    \'chrUn_JTFH01001681v1_decoy\',\n    \'chrUn_JTFH01001682v1_decoy\',\n    \'chrUn_JTFH01001683v1_decoy\',\n    \'chrUn_JTFH01001684v1_decoy\',\n    \'chrUn_JTFH01001685v1_decoy\',\n    \'chrUn_JTFH01001686v1_decoy\',\n    \'chrUn_JTFH01001687v1_decoy\',\n    \'chrUn_JTFH01001688v1_decoy\',\n    \'chrUn_JTFH01001689v1_decoy\',\n    \'chrUn_JTFH01001690v1_decoy\',\n    \'chrUn_JTFH01001691v1_decoy\',\n    \'chrUn_JTFH01001692v1_decoy\',\n    \'chrUn_JTFH01001693v1_decoy\',\n    \'chrUn_JTFH01001694v1_decoy\',\n    \'chrUn_JTFH01001695v1_decoy\',\n    \'chrUn_JTFH01001696v1_decoy\',\n    \'chrUn_JTFH01001697v1_decoy\',\n    \'chrUn_JTFH01001698v1_decoy\',\n    \'chrUn_JTFH01001699v1_decoy\',\n    \'chrUn_JTFH01001700v1_decoy\',\n    \'chrUn_JTFH01001701v1_decoy\',\n    \'chrUn_JTFH01001702v1_decoy\',\n    \'chrUn_JTFH01001703v1_decoy\',\n    \'chrUn_JTFH01001704v1_decoy\',\n    \'chrUn_JTFH01001705v1_decoy\',\n    \'chrUn_JTFH01001706v1_decoy\',\n    \'chrUn_JTFH01001707v1_decoy\',\n    \'chrUn_JTFH01001708v1_decoy\',\n    \'chrUn_JTFH01001709v1_decoy\',\n    \'chrUn_JTFH01001710v1_decoy\',\n    \'chrUn_JTFH01001711v1_decoy\',\n    \'chrUn_JTFH01001712v1_decoy\',\n    \'chrUn_JTFH01001713v1_decoy\',\n    \'chrUn_JTFH01001714v1_decoy\',\n    \'chrUn_JTFH01001715v1_decoy\',\n    \'chrUn_JTFH01001716v1_decoy\',\n    \'chrUn_JTFH01001717v1_decoy\',\n    \'chrUn_JTFH01001718v1_decoy\',\n    \'chrUn_JTFH01001719v1_decoy\',\n    \'chrUn_JTFH01001720v1_decoy\',\n    \'chrUn_JTFH01001721v1_decoy\',\n    \'chrUn_JTFH01001722v1_decoy\',\n    \'chrUn_JTFH01001723v1_decoy\',\n    \'chrUn_JTFH01001724v1_decoy\',\n    \'chrUn_JTFH01001725v1_decoy\',\n    \'chrUn_JTFH01001726v1_decoy\',\n    \'chrUn_JTFH01001727v1_decoy\',\n    \'chrUn_JTFH01001728v1_decoy\',\n    \'chrUn_JTFH01001729v1_decoy\',\n    \'chrUn_JTFH01001730v1_decoy\',\n    \'chrUn_JTFH01001731v1_decoy\',\n    \'chrUn_JTFH01001732v1_decoy\',\n    \'chrUn_JTFH01001733v1_decoy\',\n    \'chrUn_JTFH01001734v1_decoy\',\n    \'chrUn_JTFH01001735v1_decoy\',\n    \'chrUn_JTFH01001736v1_decoy\',\n    \'chrUn_JTFH01001737v1_decoy\',\n    \'chrUn_JTFH01001738v1_decoy\',\n    \'chrUn_JTFH01001739v1_decoy\',\n    \'chrUn_JTFH01001740v1_decoy\',\n    \'chrUn_JTFH01001741v1_decoy\',\n    \'chrUn_JTFH01001742v1_decoy\',\n    \'chrUn_JTFH01001743v1_decoy\',\n    \'chrUn_JTFH01001744v1_decoy\',\n    \'chrUn_JTFH01001745v1_decoy\',\n    \'chrUn_JTFH01001746v1_decoy\',\n    \'chrUn_JTFH01001747v1_decoy\',\n    \'chrUn_JTFH01001748v1_decoy\',\n    \'chrUn_JTFH01001749v1_decoy\',\n    \'chrUn_JTFH01001750v1_decoy\',\n    \'chrUn_JTFH01001751v1_decoy\',\n    \'chrUn_JTFH01001752v1_decoy\',\n    \'chrUn_JTFH01001753v1_decoy\',\n    \'chrUn_JTFH01001754v1_decoy\',\n    \'chrUn_JTFH01001755v1_decoy\',\n    \'chrUn_JTFH01001756v1_decoy\',\n    \'chrUn_JTFH01001757v1_decoy\',\n    \'chrUn_JTFH01001758v1_decoy\',\n    \'chrUn_JTFH01001759v1_decoy\',\n    \'chrUn_JTFH01001760v1_decoy\',\n    \'chrUn_JTFH01001761v1_decoy\',\n    \'chrUn_JTFH01001762v1_decoy\',\n    \'chrUn_JTFH01001763v1_decoy\',\n    \'chrUn_JTFH01001764v1_decoy\',\n    \'chrUn_JTFH01001765v1_decoy\',\n    \'chrUn_JTFH01001766v1_decoy\',\n    \'chrUn_JTFH01001767v1_decoy\',\n    \'chrUn_JTFH01001768v1_decoy\',\n    \'chrUn_JTFH01001769v1_decoy\',\n    \'chrUn_JTFH01001770v1_decoy\',\n    \'chrUn_JTFH01001771v1_decoy\',\n    \'chrUn_JTFH01001772v1_decoy\',\n    \'chrUn_JTFH01001773v1_decoy\',\n    \'chrUn_JTFH01001774v1_decoy\',\n    \'chrUn_JTFH01001775v1_decoy\',\n    \'chrUn_JTFH01001776v1_decoy\',\n    \'chrUn_JTFH01001777v1_decoy\',\n    \'chrUn_JTFH01001778v1_decoy\',\n    \'chrUn_JTFH01001779v1_decoy\',\n    \'chrUn_JTFH01001780v1_decoy\',\n    \'chrUn_JTFH01001781v1_decoy\',\n    \'chrUn_JTFH01001782v1_decoy\',\n    \'chrUn_JTFH01001783v1_decoy\',\n    \'chrUn_JTFH01001784v1_decoy\',\n    \'chrUn_JTFH01001785v1_decoy\',\n    \'chrUn_JTFH01001786v1_decoy\',\n    \'chrUn_JTFH01001787v1_decoy\',\n    \'chrUn_JTFH01001788v1_decoy\',\n    \'chrUn_JTFH01001789v1_decoy\',\n    \'chrUn_JTFH01001790v1_decoy\',\n    \'chrUn_JTFH01001791v1_decoy\',\n    \'chrUn_JTFH01001792v1_decoy\',\n    \'chrUn_JTFH01001793v1_decoy\',\n    \'chrUn_JTFH01001794v1_decoy\',\n    \'chrUn_JTFH01001795v1_decoy\',\n    \'chrUn_JTFH01001796v1_decoy\',\n    \'chrUn_JTFH01001797v1_decoy\',\n    \'chrUn_JTFH01001798v1_decoy\',\n    \'chrUn_JTFH01001799v1_decoy\',\n    \'chrUn_JTFH01001800v1_decoy\',\n    \'chrUn_JTFH01001801v1_decoy\',\n    \'chrUn_JTFH01001802v1_decoy\',\n    \'chrUn_JTFH01001803v1_decoy\',\n    \'chrUn_JTFH01001804v1_decoy\',\n    \'chrUn_JTFH01001805v1_decoy\',\n    \'chrUn_JTFH01001806v1_decoy\',\n    \'chrUn_JTFH01001807v1_decoy\',\n    \'chrUn_JTFH01001808v1_decoy\',\n    \'chrUn_JTFH01001809v1_decoy\',\n    \'chrUn_JTFH01001810v1_decoy\',\n    \'chrUn_JTFH01001811v1_decoy\',\n    \'chrUn_JTFH01001812v1_decoy\',\n    \'chrUn_JTFH01001813v1_decoy\',\n    \'chrUn_JTFH01001814v1_decoy\',\n    \'chrUn_JTFH01001815v1_decoy\',\n    \'chrUn_JTFH01001816v1_decoy\',\n    \'chrUn_JTFH01001817v1_decoy\',\n    \'chrUn_JTFH01001818v1_decoy\',\n    \'chrUn_JTFH01001819v1_decoy\',\n    \'chrUn_JTFH01001820v1_decoy\',\n    \'chrUn_JTFH01001821v1_decoy\',\n    \'chrUn_JTFH01001822v1_decoy\',\n    \'chrUn_JTFH01001823v1_decoy\',\n    \'chrUn_JTFH01001824v1_decoy\',\n    \'chrUn_JTFH01001825v1_decoy\',\n    \'chrUn_JTFH01001826v1_decoy\',\n    \'chrUn_JTFH01001827v1_decoy\',\n    \'chrUn_JTFH01001828v1_decoy\',\n    \'chrUn_JTFH01001829v1_decoy\',\n    \'chrUn_JTFH01001830v1_decoy\',\n    \'chrUn_JTFH01001831v1_decoy\',\n    \'chrUn_JTFH01001832v1_decoy\',\n    \'chrUn_JTFH01001833v1_decoy\',\n    \'chrUn_JTFH01001834v1_decoy\',\n    \'chrUn_JTFH01001835v1_decoy\',\n    \'chrUn_JTFH01001836v1_decoy\',\n    \'chrUn_JTFH01001837v1_decoy\',\n    \'chrUn_JTFH01001838v1_decoy\',\n    \'chrUn_JTFH01001839v1_decoy\',\n    \'chrUn_JTFH01001840v1_decoy\',\n    \'chrUn_JTFH01001841v1_decoy\',\n    \'chrUn_JTFH01001842v1_decoy\',\n    \'chrUn_JTFH01001843v1_decoy\',\n    \'chrUn_JTFH01001844v1_decoy\',\n    \'chrUn_JTFH01001845v1_decoy\',\n    \'chrUn_JTFH01001846v1_decoy\',\n    \'chrUn_JTFH01001847v1_decoy\',\n    \'chrUn_JTFH01001848v1_decoy\',\n    \'chrUn_JTFH01001849v1_decoy\',\n    \'chrUn_JTFH01001850v1_decoy\',\n    \'chrUn_JTFH01001851v1_decoy\',\n    \'chrUn_JTFH01001852v1_decoy\',\n    \'chrUn_JTFH01001853v1_decoy\',\n    \'chrUn_JTFH01001854v1_decoy\',\n    \'chrUn_JTFH01001855v1_decoy\',\n    \'chrUn_JTFH01001856v1_decoy\',\n    \'chrUn_JTFH01001857v1_decoy\',\n    \'chrUn_JTFH01001858v1_decoy\',\n    \'chrUn_JTFH01001859v1_decoy\',\n    \'chrUn_JTFH01001860v1_decoy\',\n    \'chrUn_JTFH01001861v1_decoy\',\n    \'chrUn_JTFH01001862v1_decoy\',\n    \'chrUn_JTFH01001863v1_decoy\',\n    \'chrUn_JTFH01001864v1_decoy\',\n    \'chrUn_JTFH01001865v1_decoy\',\n    \'chrUn_JTFH01001866v1_decoy\',\n    \'chrUn_JTFH01001867v1_decoy\',\n    \'chrUn_JTFH01001868v1_decoy\',\n    \'chrUn_JTFH01001869v1_decoy\',\n    \'chrUn_JTFH01001870v1_decoy\',\n    \'chrUn_JTFH01001871v1_decoy\',\n    \'chrUn_JTFH01001872v1_decoy\',\n    \'chrUn_JTFH01001873v1_decoy\',\n    \'chrUn_JTFH01001874v1_decoy\',\n    \'chrUn_JTFH01001875v1_decoy\',\n    \'chrUn_JTFH01001876v1_decoy\',\n    \'chrUn_JTFH01001877v1_decoy\',\n    \'chrUn_JTFH01001878v1_decoy\',\n    \'chrUn_JTFH01001879v1_decoy\',\n    \'chrUn_JTFH01001880v1_decoy\',\n    \'chrUn_JTFH01001881v1_decoy\',\n    \'chrUn_JTFH01001882v1_decoy\',\n    \'chrUn_JTFH01001883v1_decoy\',\n    \'chrUn_JTFH01001884v1_decoy\',\n    \'chrUn_JTFH01001885v1_decoy\',\n    \'chrUn_JTFH01001886v1_decoy\',\n    \'chrUn_JTFH01001887v1_decoy\',\n    \'chrUn_JTFH01001888v1_decoy\',\n    \'chrUn_JTFH01001889v1_decoy\',\n    \'chrUn_JTFH01001890v1_decoy\',\n    \'chrUn_JTFH01001891v1_decoy\',\n    \'chrUn_JTFH01001892v1_decoy\',\n    \'chrUn_JTFH01001893v1_decoy\',\n    \'chrUn_JTFH01001894v1_decoy\',\n    \'chrUn_JTFH01001895v1_decoy\',\n    \'chrUn_JTFH01001896v1_decoy\',\n    \'chrUn_JTFH01001897v1_decoy\',\n    \'chrUn_JTFH01001898v1_decoy\',\n    \'chrUn_JTFH01001899v1_decoy\',\n    \'chrUn_JTFH01001900v1_decoy\',\n    \'chrUn_JTFH01001901v1_decoy\',\n    \'chrUn_JTFH01001902v1_decoy\',\n    \'chrUn_JTFH01001903v1_decoy\',\n    \'chrUn_JTFH01001904v1_decoy\',\n    \'chrUn_JTFH01001905v1_decoy\',\n    \'chrUn_JTFH01001906v1_decoy\',\n    \'chrUn_JTFH01001907v1_decoy\',\n    \'chrUn_JTFH01001908v1_decoy\',\n    \'chrUn_JTFH01001909v1_decoy\',\n    \'chrUn_JTFH01001910v1_decoy\',\n    \'chrUn_JTFH01001911v1_decoy\',\n    \'chrUn_JTFH01001912v1_decoy\',\n    \'chrUn_JTFH01001913v1_decoy\',\n    \'chrUn_JTFH01001914v1_decoy\',\n    \'chrUn_JTFH01001915v1_decoy\',\n    \'chrUn_JTFH01001916v1_decoy\',\n    \'chrUn_JTFH01001917v1_decoy\',\n    \'chrUn_JTFH01001918v1_decoy\',\n    \'chrUn_JTFH01001919v1_decoy\',\n    \'chrUn_JTFH01001920v1_decoy\',\n    \'chrUn_JTFH01001921v1_decoy\',\n    \'chrUn_JTFH01001922v1_decoy\',\n    \'chrUn_JTFH01001923v1_decoy\',\n    \'chrUn_JTFH01001924v1_decoy\',\n    \'chrUn_JTFH01001925v1_decoy\',\n    \'chrUn_JTFH01001926v1_decoy\',\n    \'chrUn_JTFH01001927v1_decoy\',\n    \'chrUn_JTFH01001928v1_decoy\',\n    \'chrUn_JTFH01001929v1_decoy\',\n    \'chrUn_JTFH01001930v1_decoy\',\n    \'chrUn_JTFH01001931v1_decoy\',\n    \'chrUn_JTFH01001932v1_decoy\',\n    \'chrUn_JTFH01001933v1_decoy\',\n    \'chrUn_JTFH01001934v1_decoy\',\n    \'chrUn_JTFH01001935v1_decoy\',\n    \'chrUn_JTFH01001936v1_decoy\',\n    \'chrUn_JTFH01001937v1_decoy\',\n    \'chrUn_JTFH01001938v1_decoy\',\n    \'chrUn_JTFH01001939v1_decoy\',\n    \'chrUn_JTFH01001940v1_decoy\',\n    \'chrUn_JTFH01001941v1_decoy\',\n    \'chrUn_JTFH01001942v1_decoy\',\n    \'chrUn_JTFH01001943v1_decoy\',\n    \'chrUn_JTFH01001944v1_decoy\',\n    \'chrUn_JTFH01001945v1_decoy\',\n    \'chrUn_JTFH01001946v1_decoy\',\n    \'chrUn_JTFH01001947v1_decoy\',\n    \'chrUn_JTFH01001948v1_decoy\',\n    \'chrUn_JTFH01001949v1_decoy\',\n    \'chrUn_JTFH01001950v1_decoy\',\n    \'chrUn_JTFH01001951v1_decoy\',\n    \'chrUn_JTFH01001952v1_decoy\',\n    \'chrUn_JTFH01001953v1_decoy\',\n    \'chrUn_JTFH01001954v1_decoy\',\n    \'chrUn_JTFH01001955v1_decoy\',\n    \'chrUn_JTFH01001956v1_decoy\',\n    \'chrUn_JTFH01001957v1_decoy\',\n    \'chrUn_JTFH01001958v1_decoy\',\n    \'chrUn_JTFH01001959v1_decoy\',\n    \'chrUn_JTFH01001960v1_decoy\',\n    \'chrUn_JTFH01001961v1_decoy\',\n    \'chrUn_JTFH01001962v1_decoy\',\n    \'chrUn_JTFH01001963v1_decoy\',\n    \'chrUn_JTFH01001964v1_decoy\',\n    \'chrUn_JTFH01001965v1_decoy\',\n    \'chrUn_JTFH01001966v1_decoy\',\n    \'chrUn_JTFH01001967v1_decoy\',\n    \'chrUn_JTFH01001968v1_decoy\',\n    \'chrUn_JTFH01001969v1_decoy\',\n    \'chrUn_JTFH01001970v1_decoy\',\n    \'chrUn_JTFH01001971v1_decoy\',\n    \'chrUn_JTFH01001972v1_decoy\',\n    \'chrUn_JTFH01001973v1_decoy\',\n    \'chrUn_JTFH01001974v1_decoy\',\n    \'chrUn_JTFH01001975v1_decoy\',\n    \'chrUn_JTFH01001976v1_decoy\',\n    \'chrUn_JTFH01001977v1_decoy\',\n    \'chrUn_JTFH01001978v1_decoy\',\n    \'chrUn_JTFH01001979v1_decoy\',\n    \'chrUn_JTFH01001980v1_decoy\',\n    \'chrUn_JTFH01001981v1_decoy\',\n    \'chrUn_JTFH01001982v1_decoy\',\n    \'chrUn_JTFH01001983v1_decoy\',\n    \'chrUn_JTFH01001984v1_decoy\',\n    \'chrUn_JTFH01001985v1_decoy\',\n    \'chrUn_JTFH01001986v1_decoy\',\n    \'chrUn_JTFH01001987v1_decoy\',\n    \'chrUn_JTFH01001988v1_decoy\',\n    \'chrUn_JTFH01001989v1_decoy\',\n    \'chrUn_JTFH01001990v1_decoy\',\n    \'chrUn_JTFH01001991v1_decoy\',\n    \'chrUn_JTFH01001992v1_decoy\',\n    \'chrUn_JTFH01001993v1_decoy\',\n    \'chrUn_JTFH01001994v1_decoy\',\n    \'chrUn_JTFH01001995v1_decoy\',\n    \'chrUn_JTFH01001996v1_decoy\',\n    \'chrUn_JTFH01001997v1_decoy\',\n    \'chrUn_JTFH01001998v1_decoy\',\n    \'HLA-A*01:01:01:01\',\n    \'HLA-A*01:01:01:02N\',\n    \'HLA-A*01:01:38L\',\n    \'HLA-A*01:02\',\n    \'HLA-A*01:03\',\n    \'HLA-A*01:04N\',\n    \'HLA-A*01:09\',\n    \'HLA-A*01:11N\',\n    \'HLA-A*01:14\',\n    \'HLA-A*01:16N\',\n    \'HLA-A*01:20\',\n    \'HLA-A*02:01:01:01\',\n    \'HLA-A*02:01:01:02L\',\n    \'HLA-A*02:01:01:03\',\n    \'HLA-A*02:01:01:04\',\n    \'HLA-A*02:02:01\',\n    \'HLA-A*02:03:01\',\n    \'HLA-A*02:03:03\',\n    \'HLA-A*02:05:01\',\n    \'HLA-A*02:06:01\',\n    \'HLA-A*02:07:01\',\n    \'HLA-A*02:10\',\n    \'HLA-A*02:251\',\n    \'HLA-A*02:259\',\n    \'HLA-A*02:264\',\n    \'HLA-A*02:265\',\n    \'HLA-A*02:266\',\n    \'HLA-A*02:269\',\n    \'HLA-A*02:279\',\n    \'HLA-A*02:32N\',\n    \'HLA-A*02:376\',\n    \'HLA-A*02:43N\',\n    \'HLA-A*02:455\',\n    \'HLA-A*02:48\',\n    \'HLA-A*02:51\',\n    \'HLA-A*02:533\',\n    \'HLA-A*02:53N\',\n    \'HLA-A*02:57\',\n    \'HLA-A*02:60:01\',\n    \'HLA-A*02:65\',\n    \'HLA-A*02:68\',\n    \'HLA-A*02:77\',\n    \'HLA-A*02:81\',\n    \'HLA-A*02:89\',\n    \'HLA-A*02:95\',\n    \'HLA-A*03:01:01:01\',\n    \'HLA-A*03:01:01:02N\',\n    \'HLA-A*03:01:01:03\',\n    \'HLA-A*03:02:01\',\n    \'HLA-A*03:11N\',\n    \'HLA-A*03:21N\',\n    \'HLA-A*03:36N\',\n    \'HLA-A*11:01:01\',\n    \'HLA-A*11:01:18\',\n    \'HLA-A*11:02:01\',\n    \'HLA-A*11:05\',\n    \'HLA-A*11:110\',\n    \'HLA-A*11:25\',\n    \'HLA-A*11:50Q\',\n    \'HLA-A*11:60\',\n    \'HLA-A*11:69N\',\n    \'HLA-A*11:74\',\n    \'HLA-A*11:75\',\n    \'HLA-A*11:77\',\n    \'HLA-A*23:01:01\',\n    \'HLA-A*23:09\',\n    \'HLA-A*23:38N\',\n    \'HLA-A*24:02:01:01\',\n    \'HLA-A*24:02:01:02L\',\n    \'HLA-A*24:02:01:03\',\n    \'HLA-A*24:02:03Q\',\n    \'HLA-A*24:02:10\',\n    \'HLA-A*24:03:01\',\n    \'HLA-A*24:07:01\',\n    \'HLA-A*24:08\',\n    \'HLA-A*24:09N\',\n    \'HLA-A*24:10:01\',\n    \'HLA-A*24:11N\',\n    \'HLA-A*24:152\',\n    \'HLA-A*24:20\',\n    \'HLA-A*24:215\',\n    \'HLA-A*24:61\',\n    \'HLA-A*24:86N\',\n    \'HLA-A*25:01:01\',\n    \'HLA-A*26:01:01\',\n    \'HLA-A*26:11N\',\n    \'HLA-A*26:15\',\n    \'HLA-A*26:50\',\n    \'HLA-A*29:01:01:01\',\n    \'HLA-A*29:01:01:02N\',\n    \'HLA-A*29:02:01:01\',\n    \'HLA-A*29:02:01:02\',\n    \'HLA-A*29:46\',\n    \'HLA-A*30:01:01\',\n    \'HLA-A*30:02:01:01\',\n    \'HLA-A*30:02:01:02\',\n    \'HLA-A*30:04:01\',\n    \'HLA-A*30:89\',\n    \'HLA-A*31:01:02\',\n    \'HLA-A*31:01:23\',\n    \'HLA-A*31:04\',\n    \'HLA-A*31:14N\',\n    \'HLA-A*31:46\',\n    \'HLA-A*32:01:01\',\n    \'HLA-A*32:06\',\n    \'HLA-A*33:01:01\',\n    \'HLA-A*33:03:01\',\n    \'HLA-A*33:07\',\n    \'HLA-A*34:01:01\',\n    \'HLA-A*34:02:01\',\n    \'HLA-A*36:01\',\n    \'HLA-A*43:01\',\n    \'HLA-A*66:01:01\',\n    \'HLA-A*66:17\',\n    \'HLA-A*68:01:01:01\',\n    \'HLA-A*68:01:01:02\',\n    \'HLA-A*68:01:02:01\',\n    \'HLA-A*68:01:02:02\',\n    \'HLA-A*68:02:01:01\',\n    \'HLA-A*68:02:01:02\',\n    \'HLA-A*68:02:01:03\',\n    \'HLA-A*68:02:02\',\n    \'HLA-A*68:03:01\',\n    \'HLA-A*68:08:01\',\n    \'HLA-A*68:113\',\n    \'HLA-A*68:17\',\n    \'HLA-A*68:18N\',\n    \'HLA-A*68:22\',\n    \'HLA-A*68:71\',\n    \'HLA-A*69:01\',\n    \'HLA-A*74:01\',\n    \'HLA-A*74:02:01:01\',\n    \'HLA-A*74:02:01:02\',\n    \'HLA-A*80:01:01:01\',\n    \'HLA-A*80:01:01:02\',\n    \'HLA-B*07:02:01\',\n    \'HLA-B*07:05:01\',\n    \'HLA-B*07:06\',\n    \'HLA-B*07:156\',\n    \'HLA-B*07:33:01\',\n    \'HLA-B*07:41\',\n    \'HLA-B*07:44\',\n    \'HLA-B*07:50\',\n    \'HLA-B*08:01:01\',\n    \'HLA-B*08:08N\',\n    \'HLA-B*08:132\',\n    \'HLA-B*08:134\',\n    \'HLA-B*08:19N\',\n    \'HLA-B*08:20\',\n    \'HLA-B*08:33\',\n    \'HLA-B*08:79\',\n    \'HLA-B*13:01:01\',\n    \'HLA-B*13:02:01\',\n    \'HLA-B*13:02:03\',\n    \'HLA-B*13:02:09\',\n    \'HLA-B*13:08\',\n    \'HLA-B*13:15\',\n    \'HLA-B*13:25\',\n    \'HLA-B*14:01:01\',\n    \'HLA-B*14:02:01\',\n    \'HLA-B*14:07N\',\n    \'HLA-B*15:01:01:01\',\n    \'HLA-B*15:01:01:02N\',\n    \'HLA-B*15:01:01:03\',\n    \'HLA-B*15:02:01\',\n    \'HLA-B*15:03:01\',\n    \'HLA-B*15:04:01\',\n    \'HLA-B*15:07:01\',\n    \'HLA-B*15:108\',\n    \'HLA-B*15:10:01\',\n    \'HLA-B*15:11:01\',\n    \'HLA-B*15:13:01\',\n    \'HLA-B*15:16:01\',\n    \'HLA-B*15:17:01:01\',\n    \'HLA-B*15:17:01:02\',\n    \'HLA-B*15:18:01\',\n    \'HLA-B*15:220\',\n    \'HLA-B*15:25:01\',\n    \'HLA-B*15:27:01\',\n    \'HLA-B*15:32:01\',\n    \'HLA-B*15:42\',\n    \'HLA-B*15:58\',\n    \'HLA-B*15:66\',\n    \'HLA-B*15:77\',\n    \'HLA-B*15:83\',\n    \'HLA-B*18:01:01:01\',\n    \'HLA-B*18:01:01:02\',\n    \'HLA-B*18:02\',\n    \'HLA-B*18:03\',\n    \'HLA-B*18:17N\',\n    \'HLA-B*18:26\',\n    \'HLA-B*18:94N\',\n    \'HLA-B*27:04:01\',\n    \'HLA-B*27:05:02\',\n    \'HLA-B*27:05:18\',\n    \'HLA-B*27:06\',\n    \'HLA-B*27:07:01\',\n    \'HLA-B*27:131\',\n    \'HLA-B*27:24\',\n    \'HLA-B*27:25\',\n    \'HLA-B*27:32\',\n    \'HLA-B*35:01:01:01\',\n    \'HLA-B*35:01:01:02\',\n    \'HLA-B*35:01:22\',\n    \'HLA-B*35:02:01\',\n    \'HLA-B*35:03:01\',\n    \'HLA-B*35:05:01\',\n    \'HLA-B*35:08:01\',\n    \'HLA-B*35:14:02\',\n    \'HLA-B*35:241\',\n    \'HLA-B*35:41\',\n    \'HLA-B*37:01:01\',\n    \'HLA-B*37:01:05\',\n    \'HLA-B*38:01:01\',\n    \'HLA-B*38:02:01\',\n    \'HLA-B*38:14\',\n    \'HLA-B*39:01:01:01\',\n    \'HLA-B*39:01:01:02L\',\n    \'HLA-B*39:01:01:03\',\n    \'HLA-B*39:01:03\',\n    \'HLA-B*39:01:16\',\n    \'HLA-B*39:01:21\',\n    \'HLA-B*39:05:01\',\n    \'HLA-B*39:06:02\',\n    \'HLA-B*39:10:01\',\n    \'HLA-B*39:13:02\',\n    \'HLA-B*39:14\',\n    \'HLA-B*39:34\',\n    \'HLA-B*39:38Q\',\n    \'HLA-B*40:01:01\',\n    \'HLA-B*40:01:02\',\n    \'HLA-B*40:02:01\',\n    \'HLA-B*40:03\',\n    \'HLA-B*40:06:01:01\',\n    \'HLA-B*40:06:01:02\',\n    \'HLA-B*40:10:01\',\n    \'HLA-B*40:150\',\n    \'HLA-B*40:40\',\n    \'HLA-B*40:72:01\',\n    \'HLA-B*40:79\',\n    \'HLA-B*41:01:01\',\n    \'HLA-B*41:02:01\',\n    \'HLA-B*42:01:01\',\n    \'HLA-B*42:02\',\n    \'HLA-B*42:08\',\n    \'HLA-B*44:02:01:01\',\n    \'HLA-B*44:02:01:02S\',\n    \'HLA-B*44:02:01:03\',\n    \'HLA-B*44:02:17\',\n    \'HLA-B*44:02:27\',\n    \'HLA-B*44:03:01\',\n    \'HLA-B*44:03:02\',\n    \'HLA-B*44:04\',\n    \'HLA-B*44:09\',\n    \'HLA-B*44:138Q\',\n    \'HLA-B*44:150\',\n    \'HLA-B*44:23N\',\n    \'HLA-B*44:26\',\n    \'HLA-B*44:46\',\n    \'HLA-B*44:49\',\n    \'HLA-B*44:56N\',\n    \'HLA-B*45:01:01\',\n    \'HLA-B*45:04\',\n    \'HLA-B*46:01:01\',\n    \'HLA-B*46:01:05\',\n    \'HLA-B*47:01:01:01\',\n    \'HLA-B*47:01:01:02\',\n    \'HLA-B*48:01:01\',\n    \'HLA-B*48:03:01\',\n    \'HLA-B*48:04\',\n    \'HLA-B*48:08\',\n    \'HLA-B*49:01:01\',\n    \'HLA-B*49:32\',\n    \'HLA-B*50:01:01\',\n    \'HLA-B*51:01:01\',\n    \'HLA-B*51:01:02\',\n    \'HLA-B*51:02:01\',\n    \'HLA-B*51:07:01\',\n    \'HLA-B*51:42\',\n    \'HLA-B*52:01:01:01\',\n    \'HLA-B*52:01:01:02\',\n    \'HLA-B*52:01:01:03\',\n    \'HLA-B*52:01:02\',\n    \'HLA-B*53:01:01\',\n    \'HLA-B*53:11\',\n    \'HLA-B*54:01:01\',\n    \'HLA-B*54:18\',\n    \'HLA-B*55:01:01\',\n    \'HLA-B*55:01:03\',\n    \'HLA-B*55:02:01\',\n    \'HLA-B*55:12\',\n    \'HLA-B*55:24\',\n    \'HLA-B*55:48\',\n    \'HLA-B*56:01:01\',\n    \'HLA-B*56:03\',\n    \'HLA-B*56:04\',\n    \'HLA-B*57:01:01\',\n    \'HLA-B*57:03:01\',\n    \'HLA-B*57:06\',\n    \'HLA-B*57:11\',\n    \'HLA-B*57:29\',\n    \'HLA-B*58:01:01\',\n    \'HLA-B*58:31N\',\n    \'HLA-B*59:01:01:01\',\n    \'HLA-B*59:01:01:02\',\n    \'HLA-B*67:01:01\',\n    \'HLA-B*67:01:02\',\n    \'HLA-B*67:02\',\n    \'HLA-B*73:01\',\n    \'HLA-B*78:01:01\',\n    \'HLA-B*81:01\',\n    \'HLA-B*82:02:01\',\n    \'HLA-C*01:02:01\',\n    \'HLA-C*01:02:11\',\n    \'HLA-C*01:02:29\',\n    \'HLA-C*01:02:30\',\n    \'HLA-C*01:03\',\n    \'HLA-C*01:06\',\n    \'HLA-C*01:08\',\n    \'HLA-C*01:14\',\n    \'HLA-C*01:21\',\n    \'HLA-C*01:30\',\n    \'HLA-C*01:40\',\n    \'HLA-C*02:02:02:01\',\n    \'HLA-C*02:02:02:02\',\n    \'HLA-C*02:10\',\n    \'HLA-C*02:11\',\n    \'HLA-C*02:16:02\',\n    \'HLA-C*02:69\',\n    \'HLA-C*02:85\',\n    \'HLA-C*02:86\',\n    \'HLA-C*02:87\',\n    \'HLA-C*03:02:01\',\n    \'HLA-C*03:02:02:01\',\n    \'HLA-C*03:02:02:02\',\n    \'HLA-C*03:02:02:03\',\n    \'HLA-C*03:03:01\',\n    \'HLA-C*03:04:01:01\',\n    \'HLA-C*03:04:01:02\',\n    \'HLA-C*03:04:02\',\n    \'HLA-C*03:04:04\',\n    \'HLA-C*03:05\',\n    \'HLA-C*03:06\',\n    \'HLA-C*03:100\',\n    \'HLA-C*03:13:01\',\n    \'HLA-C*03:20N\',\n    \'HLA-C*03:219\',\n    \'HLA-C*03:261\',\n    \'HLA-C*03:40:01\',\n    \'HLA-C*03:41:02\',\n    \'HLA-C*03:46\',\n    \'HLA-C*03:61\',\n    \'HLA-C*04:01:01:01\',\n    \'HLA-C*04:01:01:02\',\n    \'HLA-C*04:01:01:03\',\n    \'HLA-C*04:01:01:04\',\n    \'HLA-C*04:01:01:05\',\n    \'HLA-C*04:01:62\',\n    \'HLA-C*04:03:01\',\n    \'HLA-C*04:06\',\n    \'HLA-C*04:09N\',\n    \'HLA-C*04:128\',\n    \'HLA-C*04:161\',\n    \'HLA-C*04:177\',\n    \'HLA-C*04:70\',\n    \'HLA-C*04:71\',\n    \'HLA-C*05:01:01:01\',\n    \'HLA-C*05:01:01:02\',\n    \'HLA-C*05:08\',\n    \'HLA-C*05:09:01\',\n    \'HLA-C*05:93\',\n    \'HLA-C*06:02:01:01\',\n    \'HLA-C*06:02:01:02\',\n    \'HLA-C*06:02:01:03\',\n    \'HLA-C*06:23\',\n    \'HLA-C*06:24\',\n    \'HLA-C*06:46N\',\n    \'HLA-C*07:01:01:01\',\n    \'HLA-C*07:01:01:02\',\n    \'HLA-C*07:01:02\',\n    \'HLA-C*07:01:19\',\n    \'HLA-C*07:01:27\',\n    \'HLA-C*07:01:45\',\n    \'HLA-C*07:02:01:01\',\n    \'HLA-C*07:02:01:02\',\n    \'HLA-C*07:02:01:03\',\n    \'HLA-C*07:02:01:04\',\n    \'HLA-C*07:02:01:05\',\n    \'HLA-C*07:02:05\',\n    \'HLA-C*07:02:06\',\n    \'HLA-C*07:02:64\',\n    \'HLA-C*07:04:01\',\n    \'HLA-C*07:04:02\',\n    \'HLA-C*07:06\',\n    \'HLA-C*07:149\',\n    \'HLA-C*07:18\',\n    \'HLA-C*07:19\',\n    \'HLA-C*07:26\',\n    \'HLA-C*07:30\',\n    \'HLA-C*07:32N\',\n    \'HLA-C*07:384\',\n    \'HLA-C*07:385\',\n    \'HLA-C*07:386\',\n    \'HLA-C*07:391\',\n    \'HLA-C*07:392\',\n    \'HLA-C*07:49\',\n    \'HLA-C*07:56:02\',\n    \'HLA-C*07:66\',\n    \'HLA-C*07:67\',\n    \'HLA-C*08:01:01\',\n    \'HLA-C*08:01:03\',\n    \'HLA-C*08:02:01:01\',\n    \'HLA-C*08:02:01:02\',\n    \'HLA-C*08:03:01\',\n    \'HLA-C*08:04:01\',\n    \'HLA-C*08:112\',\n    \'HLA-C*08:20\',\n    \'HLA-C*08:21\',\n    \'HLA-C*08:22\',\n    \'HLA-C*08:24\',\n    \'HLA-C*08:27\',\n    \'HLA-C*08:36N\',\n    \'HLA-C*08:40\',\n    \'HLA-C*08:41\',\n    \'HLA-C*08:62\',\n    \'HLA-C*12:02:02\',\n    \'HLA-C*12:03:01:01\',\n    \'HLA-C*12:03:01:02\',\n    \'HLA-C*12:08\',\n    \'HLA-C*12:13\',\n    \'HLA-C*12:19\',\n    \'HLA-C*12:22\',\n    \'HLA-C*12:99\',\n    \'HLA-C*14:02:01\',\n    \'HLA-C*14:03\',\n    \'HLA-C*14:21N\',\n    \'HLA-C*14:23\',\n    \'HLA-C*15:02:01\',\n    \'HLA-C*15:05:01\',\n    \'HLA-C*15:05:02\',\n    \'HLA-C*15:13\',\n    \'HLA-C*15:16\',\n    \'HLA-C*15:17\',\n    \'HLA-C*15:96Q\',\n    \'HLA-C*16:01:01\',\n    \'HLA-C*16:02:01\',\n    \'HLA-C*16:04:01\',\n    \'HLA-C*17:01:01:01\',\n    \'HLA-C*17:01:01:02\',\n    \'HLA-C*17:01:01:03\',\n    \'HLA-C*17:03\',\n    \'HLA-C*18:01\',\n    \'HLA-DQA1*01:01:02\',\n    \'HLA-DQA1*01:02:01:01\',\n    \'HLA-DQA1*01:02:01:02\',\n    \'HLA-DQA1*01:02:01:03\',\n    \'HLA-DQA1*01:02:01:04\',\n    \'HLA-DQA1*01:03:01:01\',\n    \'HLA-DQA1*01:03:01:02\',\n    \'HLA-DQA1*01:04:01:01\',\n    \'HLA-DQA1*01:04:01:02\',\n    \'HLA-DQA1*01:05:01\',\n    \'HLA-DQA1*01:07\',\n    \'HLA-DQA1*01:10\',\n    \'HLA-DQA1*01:11\',\n    \'HLA-DQA1*02:01\',\n    \'HLA-DQA1*03:01:01\',\n    \'HLA-DQA1*03:02\',\n    \'HLA-DQA1*03:03:01\',\n    \'HLA-DQA1*04:01:02:01\',\n    \'HLA-DQA1*04:01:02:02\',\n    \'HLA-DQA1*04:02\',\n    \'HLA-DQA1*05:01:01:01\',\n    \'HLA-DQA1*05:01:01:02\',\n    \'HLA-DQA1*05:03\',\n    \'HLA-DQA1*05:05:01:01\',\n    \'HLA-DQA1*05:05:01:02\',\n    \'HLA-DQA1*05:05:01:03\',\n    \'HLA-DQA1*05:11\',\n    \'HLA-DQA1*06:01:01\',\n    \'HLA-DQB1*02:01:01\',\n    \'HLA-DQB1*02:02:01\',\n    \'HLA-DQB1*03:01:01:01\',\n    \'HLA-DQB1*03:01:01:02\',\n    \'HLA-DQB1*03:01:01:03\',\n    \'HLA-DQB1*03:02:01\',\n    \'HLA-DQB1*03:03:02:01\',\n    \'HLA-DQB1*03:03:02:02\',\n    \'HLA-DQB1*03:03:02:03\',\n    \'HLA-DQB1*03:05:01\',\n    \'HLA-DQB1*05:01:01:01\',\n    \'HLA-DQB1*05:01:01:02\',\n    \'HLA-DQB1*05:03:01:01\',\n    \'HLA-DQB1*05:03:01:02\',\n    \'HLA-DQB1*06:01:01\',\n    \'HLA-DQB1*06:02:01\',\n    \'HLA-DQB1*06:03:01\',\n    \'HLA-DQB1*06:09:01\',\n    \'HLA-DRB1*01:01:01\',\n    \'HLA-DRB1*01:02:01\',\n    \'HLA-DRB1*03:01:01:01\',\n    \'HLA-DRB1*03:01:01:02\',\n    \'HLA-DRB1*04:03:01\',\n    \'HLA-DRB1*07:01:01:01\',\n    \'HLA-DRB1*07:01:01:02\',\n    \'HLA-DRB1*08:03:02\',\n    \'HLA-DRB1*09:21\',\n    \'HLA-DRB1*10:01:01\',\n    \'HLA-DRB1*11:01:01\',\n    \'HLA-DRB1*11:01:02\',\n    \'HLA-DRB1*11:04:01\',\n    \'HLA-DRB1*12:01:01\',\n    \'HLA-DRB1*12:17\',\n    \'HLA-DRB1*13:01:01\',\n    \'HLA-DRB1*13:02:01\',\n    \'HLA-DRB1*14:05:01\',\n    \'HLA-DRB1*14:54:01\',\n    \'HLA-DRB1*15:01:01:01\',\n    \'HLA-DRB1*15:01:01:02\',\n    \'HLA-DRB1*15:01:01:03\',\n    \'HLA-DRB1*15:01:01:04\',\n    \'HLA-DRB1*15:02:01\',\n    \'HLA-DRB1*15:03:01:01\',\n    \'HLA-DRB1*15:03:01:02\',\n    \'HLA-DRB1*16:02:01\',\n]\n'"
deepvariant/exclude_contigs_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .exclude_contigs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom absl.testing import absltest\n\nfrom deepvariant import exclude_contigs\n\n\nclass ExcludeContigsTests(absltest.TestCase):\n\n  def test_excluded_contigs_doesnt_drop_standard_contigs(self):\n    """"""Make sure we are keeping the common human contigs.""""""\n    for chrom in range(1, 22):\n      for prefix in [\'\', \'chr\', \'Chr\']:\n        self.assertNotIn(\'{}{}\'.format(prefix, chrom),\n                         exclude_contigs.EXCLUDED_HUMAN_CONTIGS)\n    self.assertNotIn(\'chrX\', exclude_contigs.EXCLUDED_HUMAN_CONTIGS)\n    self.assertNotIn(\'chrY\', exclude_contigs.EXCLUDED_HUMAN_CONTIGS)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/haplotypes.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Library for resolving variants into consistent haplotypes.\n\nThe convolutional neural network that evaluates the probability of a candidate\nvariant being non-reference evaluates each candidate variant independently.\nThis can lead to overlapping variant calls that cannot actually exist in an\norganism: for example, a diploid human cannot have overlapping variants for\nwhich one is homozygous alternate and the other is heterozygous alternate, since\nthat implies three total alternate alleles.\n\nThis library tries to resolve overlapping variant calls into consistent\nhaplotypes by using the most likely configuration based on individual call\nprobabilities that is a valid set of two haplotypes. In rare cases where this\nis not possible, the haplotypes are left unmodified.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport itertools\nfrom absl import flags\nfrom absl import logging\nimport numpy as np\n\nfrom third_party.nucleus.util import genomics_math\nfrom third_party.nucleus.util import variant_utils\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_bool(\n    \'disable_haplotype_resolution\', False,\n    \'If True, makes `maybe_resolve_conflicting_variants` a no-op.\')\n\n# The maximum number of overlapping variants to try to resolve into compatible\n# haplotypes. This corresponds to generating 3^12 (= 531,441) possible variant\n# configurations for diploid individuals.\n_MAX_OVERLAPPING_VARIANTS_TO_RESOLVE = 12\n\n\ndef maybe_resolve_conflicting_variants(sorted_variants):\n  """"""Yields Variant protos in sorted order after fixing conflicting haplotypes.\n\n  The input is an iterable of Variants in chromosome and position sorted order,\n  with potential incompatibilies as described in this module\'s docstring. This\n  function tries to resolve variants into valid haplotypes, though is not\n  guaranteed to do so if the variant composition is not amenable to this or it\n  would be computationally intractable.\n\n  Args:\n    sorted_variants: Iterable of Variant protos. Sorted in coordinate order, but\n      with potentially incompatible haplotypes.\n\n  Yields:\n    Variant protos in coordinate-sorted order with no incompatible haplotypes.\n  """"""\n  if FLAGS.disable_haplotype_resolution:\n    logging.info(\'disable_haplotype_resolution is True. \'\n                 \'`maybe_resolve_conflicting_variants` has no effect.\')\n    for v in sorted_variants:\n      yield v\n  else:\n    for overlapping_candidates in _group_overlapping_variants(sorted_variants):\n      for resolved_candidate in _maybe_resolve_mixed_calls(\n          overlapping_candidates):\n        yield resolved_candidate\n\n\ndef _group_overlapping_variants(sorted_variants):\n  """"""Yields lists of Variant protos that overlap on the reference sequence.\n\n  Args:\n    sorted_variants: Iterable of Variant protos, sorted in coordinate order.\n\n  Yields:\n    Lists of variants within `sorted_variants` that overlap with each other on\n    the reference sequence.\n  """"""\n  curr_variants = []\n  prev_chrom = None\n  prev_max_end = -1\n  for variant in sorted_variants:\n    if variant.reference_name != prev_chrom or variant.start >= prev_max_end:\n      if curr_variants:\n        yield curr_variants\n      curr_variants = [variant]\n      prev_chrom = variant.reference_name\n      prev_max_end = variant.end\n    else:\n      curr_variants.append(variant)\n      prev_max_end = max(prev_max_end, variant.end)\n  # Fencepost.\n  if curr_variants:\n    yield curr_variants\n\n\ndef _maybe_resolve_mixed_calls(overlapping_candidates):\n  """"""Yields variants with compatible genotype calls in order.\n\n  This function differs from `_resolve_overlapping_variants` below in that the\n  input here is a block of all candidate calls that overlap in a region, which\n  may contain candidates that are deemed to be most likely reference calls.\n  We often tune DeepVariant to be highly sensitive. Consequently, there can be\n  many candidate calls that are predicted as reference. Since those do not\n  contribute to potential incompatibilities, we split them out from variants\n  predicted to contain non-reference genotypes since the computation of\n  compatible haplotypes is exponential in the number of inputs.\n\n  Args:\n    overlapping_candidates: list(Variant). A non-empty list of Variant protos in\n      coordinate-sorted order that overlap on the reference genome.\n\n  Yields:\n    Variant protos in coordinate-sorted order that try to resolve incompatible\n    haplotypes.\n  """"""\n  # Short circuit the simplest case: A single variant in a region is compatible\n  # with itself by definition.\n  if len(overlapping_candidates) == 1:\n    yield overlapping_candidates[0]\n    return\n\n  def has_variation(candidate):\n    return _nonref_genotype_count(candidate) > 0\n\n  reference_calls = [c for c in overlapping_candidates if not has_variation(c)]\n  variant_calls = [v for v in overlapping_candidates if has_variation(v)]\n\n  resolved_variant_calls = []\n  for variant_group in _group_overlapping_variants(variant_calls):\n    resolved_variant_calls.extend(_resolve_overlapping_variants(variant_group))\n\n  # Merge the reference and resolved variants back together in sorted order.\n  # Note: This could be done in an interleaving fashion, but since the total\n  # number of variants in the input is nearly always < 20 this is not an issue.\n  for variant in sorted(\n      reference_calls + resolved_variant_calls,\n      key=variant_utils.variant_range_tuple):\n    yield variant\n\n\nclass _VariantCompatibilityCalculator(object):\n  """"""Represents the reference genome spanned by overlapping Variants.\n\n  Each Variant affects a portion of the reference genome that is determined by\n  its start and end coordinates. For a given set of Variants, they are deemed\n  compatible if the total area along the reference genome that is called as\n  non-reference genotypes never exceeds the ploidy of the organism.\n  """"""\n\n  def __init__(self, overlapping_variants):\n    """"""Constructor.\n\n    Args:\n      overlapping_variants: list(Variant). The Variant protos of interest.\n    """"""\n    min_start = min(v.start for v in overlapping_variants)\n    self.variant_indices = [\n        (v.start - min_start, v.end - min_start) for v in overlapping_variants\n    ]\n    self.size = max(v.end - min_start for v in overlapping_variants)\n\n  def all_variants_compatible(self, nonref_genotype_counts, ploidy=2):\n    """"""Returns True if and only if all variants are compatible.\n\n    Args:\n      nonref_genotype_counts: list of ints in [0, ploidy]. Element i in this\n        list represents the number of non-reference genotypes for the i\'th\n        variant.\n      ploidy: int. The ploidy of the individual.\n\n    Returns:\n      True if and only if the variants are compatible.\n\n    Raises:\n      ValueError: nonref_genotype_counts is not the same length as\n        self.variant_indices.\n      ValueError: nonref_genotype_counts does not contain elements in [0,\n      ploidy].\n    """"""\n    if len(nonref_genotype_counts) != len(self.variant_indices):\n      raise ValueError(\n          \'Variant counts must have same length as variant indices.\')\n    if not all(0 <= cnt <= ploidy for cnt in nonref_genotype_counts):\n      raise ValueError(\'Invalid variant allele count for ploidy {}: {}\'.format(\n          ploidy, nonref_genotype_counts))\n\n    alts_in_span = np.zeros(self.size, dtype=int)\n    for cnt, (start, end) in zip(nonref_genotype_counts, self.variant_indices):\n      alts_in_span[start:end] += cnt\n    return np.all(alts_in_span <= ploidy)\n\n\nclass _LikelihoodAggregator(object):\n  """"""Container class for genotype likelihoods of allele configurations.\n\n  When evaluating valid genotype configurations across multiple variants, we\n  calculate the likelihood of each configuration. To then calculate the marginal\n  likelihoods for each variant\'s genotypes, for each genotype we need to sum the\n  probabilities of all configurations that include that genotype.\n\n  For numerical stability we do this by storing the genotype likelihoods\n  = log10(p) and then aggregate using the log-sum-exp trick.\n  """"""\n\n  def __init__(self, num_alts):\n    """"""Constructor.\n\n    Args:\n      num_alts: int. The number of alternate alleles in the variant.\n    """"""\n    self._num_likelihoods = variant_utils.genotype_likelihood_index(\n        (num_alts, num_alts)) + 1\n\n    # At each GL index, we keep a list that will include the joint GL across all\n    # variants that include that particular set of allele indices for this\n    # variant.\n    self._genotype_likelihood_containers = []\n    for _ in range(self._num_likelihoods):\n      self._genotype_likelihood_containers.append([])\n\n  def add(self, allele_indices, likelihood):\n    """"""Add some likelihood to a particular allele configuration.\n\n    Args:\n      allele_indices: Pair of (g1, g2) ints representing the genotype.\n      likelihood: float. log10(probability of this genotype configuration).\n    """"""\n    ix = variant_utils.genotype_likelihood_index(allele_indices)\n    self._genotype_likelihood_containers[ix].append(likelihood)\n\n  def scaled_likelihoods(self):\n    """"""Returns the scaled likelihood of each genotype.""""""\n    if not all(bool(x) for x in self._genotype_likelihood_containers):\n      raise ValueError(\n          \'All genotypes must have some probability mass: {}\'.format(\n              self._genotype_likelihood_containers))\n\n    return genomics_math.normalize_log10_probs([\n        genomics_math.log10sumexp(unscaled)\n        for unscaled in self._genotype_likelihood_containers\n    ])\n\n  def most_likely_allele_indices(self):\n    """"""Returns allele indices for the genotype with the largest likelihood.""""""\n    ix = np.argmax(self.scaled_likelihoods())\n    return variant_utils.allele_indices_for_genotype_likelihood_index(\n        ix, ploidy=2)\n\n\ndef _resolve_overlapping_variants(overlapping_variants):\n  """"""Yields variants with compatible haplotypes, if possible.\n\n  Args:\n    overlapping_variants: list(Variant). A non-empty list of Variant protos in\n      coordinate-sorted order that overlap on the reference genome and are\n      predicted to contain alternate allele genotypes.\n\n  Yields:\n    Variant protos in coordinate-sorted order that try to resolve incompatible\n    haplotypes.\n  """"""\n  # Short circuit the simplest case: A single variant in a region is compatible\n  # with itself by definition.\n  if len(overlapping_variants) == 1:\n    yield overlapping_variants[0]\n    return\n\n  # If the actual genotype calls are compatible, we can safely return those\n  # since they would be the most likely configuration also when restricting to\n  # only valid configurations of genotype calls.\n  calculator = _VariantCompatibilityCalculator(overlapping_variants)\n  nonref_counts = [_nonref_genotype_count(v) for v in overlapping_variants]\n  if calculator.all_variants_compatible(nonref_counts):\n    logging.vlog(2, \'Overlapping variants are naturally compatible: %s\',\n                 overlapping_variants)\n    for variant in overlapping_variants:\n      yield variant\n    return\n\n  # The actual genotype calls produce an inconsistent haplotype. If the number\n  # of affected variants is ""too large"", avoid processing since this is an\n  # exponential process.\n  if len(overlapping_variants) > _MAX_OVERLAPPING_VARIANTS_TO_RESOLVE:\n    logging.vlog(\n        2,\n        \'Overlapping variants are not naturally compatible, and there are too \'\n        \'many to exhaustively search (%s). Returning variants without \'\n        \'modification, beginning with %s.\', len(overlapping_variants),\n        overlapping_variants[0])\n    for variant in overlapping_variants:\n      yield variant\n    return\n\n  # Otherwise, the actual genotype calls are incompatible. Since the genotype\n  # likelihoods are generally well-calibrated, we examine all configurations of\n  # genotypes that create compatible haplotypes and retain the single\n  # configuration with the highest joint likelihood across all variants as the\n  # proposed genotype assignment. Separately, we rescale the likelihood of each\n  # individual variant using only the valid genotype configurations. If the\n  # results are concordant (i.e., the genotype predicted by the marginal\n  # likelihood for each variant is the same as the genotype predicted when\n  # maximizing the joint likelihood across all variants), we return variants\n  # with those calls and the rescaled likelihoods. Otherwise, we log a warning\n  # and emit the original (incompatible) variants.\n  #\n  # For example, a biallelic deletion with probabilities of homref, het, homalt\n  # = 0.01, 0.9, 0.09 and inside it a biallelic SNP with probs 0.02, 0.48, 0.5.\n  # Naively this would be called as a heterozygous indel and a homozygous SNP,\n  # which is impossible as there are three total alternate genotypes. The\n  # algorithm does the following:\n  #\n  #   Indel    SNP    Joint prob\n  #   0/0      0/0    0.01 * 0.02 = 0.0002\n  #   0/0      0/1    0.01 * 0.48 = 0.0048\n  #   0/0      1/1    0.01 * 0.50 = 0.0050\n  #   0/1      0/0    0.90 * 0.02 = 0.0180\n  #   0/1      0/1    0.90 * 0.48 = 0.4320*\n  #   0/1      1/1    <invalid>   = 0\n  #   1/1      0/0    0.09 * 0.02 = 0.0018\n  #   1/1      0/1    <invalid>   = 0\n  #   1/1      1/1    <invalid>   = 0\n  #\n  #   So using the highest joint likelihood, we predict het indel and het SNP.\n  #\n  #   The marginal probability of each genotype for the indel is:\n  #   0/0:  0.0002 + 0.0048 + 0.0050 = 0.01\n  #   0/1:  0.0180 + 0.4320          = 0.45\n  #   1/1:  0.0018                   = 0.0018\n  #\n  #   which after normalizing to sum to 1 is roughly 0.022, 0.974, 0.004.\n  #   The marginal probability for the SNP, after performing similar\n  #   calculations, is 0.043, 0.946, 0.011. So the marginals also predict a het\n  #   indel and a het SNP. Since the two calculations agree, we use this\n  #   genotype call and modified likelihoods.\n  #\n  # First, we find all non-reference count configurations that are compatible.\n  # This represents each variant solely based on its number of non-reference\n  # genotypes, and assumes that variants are compatible if the total number of\n  # non-reference genotypes at a single position is at most two. By using\n  # non-reference counts, we avoid testing multiple allele configurations that\n  # will return the same result (e.g. a variant with two possible alternate\n  # alleles has three allele configurations that are homozygous alternate\n  # [1/1, 1/2, 2/2] and either all or none of them will be valid depending on\n  # the variants it interacts with).\n  valid_nonref_count_configurations = [\n      conf\n      for conf in itertools.product([0, 1, 2], repeat=len(overlapping_variants))\n      if calculator.all_variants_compatible(conf)\n  ]\n\n  # Next, we find the single compatible variant assignment with the individually\n  # highest likelihood and track the total likelihood distributed to all variant\n  # genotypes.\n  likelihood_aggregators = [\n      _LikelihoodAggregator(len(v.alternate_bases))\n      for v in overlapping_variants\n  ]\n  most_likely_allele_indices_config = None\n  most_likely_likelihood = None\n  for nonref_count_config in valid_nonref_count_configurations:\n    for allele_indices_config in _get_all_allele_indices_configurations(\n        overlapping_variants, nonref_count_config):\n      config_likelihood = _allele_indices_configuration_likelihood(\n          overlapping_variants, allele_indices_config)\n      if (most_likely_likelihood is None or\n          config_likelihood > most_likely_likelihood):\n        most_likely_likelihood = config_likelihood\n        most_likely_allele_indices_config = allele_indices_config\n      for aggregator, allele_indices in zip(likelihood_aggregators,\n                                            allele_indices_config):\n        aggregator.add(allele_indices, config_likelihood)\n\n  marginal_allele_indices_config = tuple(\n      agg.most_likely_allele_indices() for agg in likelihood_aggregators)\n  if marginal_allele_indices_config == most_likely_allele_indices_config:\n    logging.vlog(\n        2,\n        \'Overlapping variants are not naturally compatible, but the genotype \'\n        \'configuration with the most likely joint likelihood is the same as \'\n        \'that from the scaled marginal likelihoods: %s\',\n        overlapping_variants[0])\n    # Collapse the probabilities of all configurations to a single GL for each\n    # allele, independently for each variant.\n    scaled_gls = [agg.scaled_likelihoods() for agg in likelihood_aggregators]\n\n    for variant, allele_indices, gls in zip(overlapping_variants,\n                                            most_likely_allele_indices_config,\n                                            scaled_gls):\n      newvariant = copy.deepcopy(variant)\n      call = variant_utils.only_call(newvariant)\n      call.genotype[:] = allele_indices\n      call.genotype_likelihood[:] = gls\n      yield newvariant\n  else:\n    logging.vlog(\n        2,\n        \'Overlapping variants are not naturally compatible, and the genotype \'\n        \'configuration with the most likely joint likelihood is different from \'\n        \'that using the scaled marginal likelihoods: %s\',\n        overlapping_variants[0])\n    # redacted\n    for variant in overlapping_variants:\n      yield variant\n\n\ndef _get_all_allele_indices_configurations(variants,\n                                           nonref_count_configuration):\n  """"""Returns an iterable of allele configurations that satisfy the genotype.\n\n  Args:\n    variants: list(Variant). The list of variants for which to generate\n      configurations of valid allele_indices.\n    nonref_count_configuration: list(int). The list of numbers of non-reference\n      genotypes that should be generated for each variant.\n\n  Returns:\n    Iterable of lists of allele indices to assign to each Variant to satisfy the\n    desired configuration of number of non-reference genotypes for each variant.\n\n  Raises:\n    ValueError: variants and nonref_count_configuration do not have the same\n    length.\n  """"""\n  if len(variants) != len(nonref_count_configuration):\n    raise ValueError(\n        \'len(variants) must equal len(nonref_count_configuration): {} vs {}\'\n        .format(len(variants), len(nonref_count_configuration)))\n\n  allele_indices_configs = [\n      variant_utils.allele_indices_with_num_alts(variant, num_alts, ploidy=2)\n      for variant, num_alts in zip(variants, nonref_count_configuration)\n  ]\n  return itertools.product(*allele_indices_configs)\n\n\ndef _allele_indices_configuration_likelihood(variants, allele_indices_config):\n  """"""Returns the joint likelihood of the alleles given to the variants.\n\n  Args:\n    variants: list(Variant). The variants with associated likelihoods.\n    allele_indices_config: list((int, int)). The allele indices to assign to\n      each variant.\n\n  Returns:\n    The joint likelihood of the particular allele configuration.\n\n  Raises:\n    ValueError: variants and allele_indices_config do not have the same length.\n  """"""\n  if len(variants) != len(allele_indices_config):\n    raise ValueError(\n        \'len(variants) must equal len(allele_indices_config): {} vs {}\'.format(\n            len(variants), len(allele_indices_config)))\n\n  retval = 0\n  for variant, alleles in zip(variants, allele_indices_config):\n    retval += variant_utils.genotype_likelihood(\n        variant_utils.only_call(variant), alleles)\n  return retval\n\n\ndef _nonref_genotype_count(variant):\n  """"""Returns the number of non-reference alleles in the called genotype.""""""\n  return sum(g > 0 for g in variant_utils.only_call(variant).genotype)\n'"
deepvariant/haplotypes_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .haplotypes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport types\n\n\nfrom absl import flags\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport numpy as np\nimport six\n\nfrom third_party.nucleus.testing import test_utils\n\nfrom deepvariant import haplotypes\nfrom deepvariant.testing import flagsaver\n\nFLAGS = flags.FLAGS\n\n\ndef _var(chrom=\'1\',\n         start=5,\n         end=None,\n         ref=None,\n         alt=None,\n         qual=50,\n         genotype=None,\n         likelihoods=None,\n         sample_name=\'NA12878\'):\n  """"""Creates a Variant record for testing.\n\n  Args:\n    chrom: reference name for this variant\n    start: start position on the contig\n    end: end position on the contig\n    ref: reference base(s)\n    alt: list(str). alternate base(s)\n    qual: PHRED scaled detection probability\n    genotype: list of integers corresponding to the called genotype\n    likelihoods: genotype likelihoods for this variant\n    sample_name: sample name for the single call in the variant\n\n  Returns:\n    A Variant record created with the specified arguments.\n\n  Raises:\n    ValueError: Both ref and end are specified, and are inconsistent.\n  """"""\n  if ref is None and end is None:\n    ref = \'A\'\n  elif ref is None:\n    ref = \'A\' * (end - start)\n  elif ref is not None and end is not None and end != start + len(ref):\n    raise ValueError(\'Inconsistent end and reference allele.\')\n\n  if alt is None:\n    alt = [\'C\']\n  if genotype is None:\n    genotype = [0, 1]\n  if likelihoods is None:\n    likelihoods = [-1.0, -0.0506099933550872, -2.0]\n  return test_utils.make_variant(\n      chrom=chrom,\n      start=start,\n      alleles=[ref] + alt,\n      qual=qual,\n      filters=None,\n      gt=genotype,\n      gls=likelihoods,\n      sample_name=sample_name)\n\n\ndef _resolvable_incompatible_inputs():\n  """"""Returns a list of variants that are incompatible but can be resolved.""""""\n  return [\n      _var(\n          start=20,\n          ref=\'ACCCCC\',\n          alt=[\'A\'],\n          genotype=[0, 1],\n          likelihoods=[-2., -0.0506099933550872, -1.]),\n      _var(\n          start=23,\n          ref=\'C\',\n          alt=[\'T\'],\n          genotype=[1, 1],\n          likelihoods=[-2., -0.3098039199714863, -0.3010299956639812])\n  ]\n\n\ndef _resolved_compatible_outputs():\n  """"""Returns a list of het variants that are correctly resolved.""""""\n  return [\n      _var(\n          start=20,\n          ref=\'ACCCCC\',\n          alt=[\'A\'],\n          genotype=[0, 1],\n          likelihoods=[\n              -1.658964842664435, -0.010604831683503404, -2.6589648426644352\n          ]),\n      _var(\n          start=23,\n          ref=\'C\',\n          alt=[\'T\'],\n          genotype=[0, 1],\n          likelihoods=[\n              -1.658964842664435, -0.014526253196596468, -1.9599948383284163\n          ])\n  ]\n\n\nclass ResolveOverlappingVariantsTest(parameterized.TestCase):\n\n  @flagsaver.FlagSaver\n  def test_maybe_resolve_conflicting_variants(self):\n    FLAGS.disable_haplotype_resolution = False\n    # Note: Most of the resolution code is tested below in the\n    # test_resolve_overlapping_variants function. This test mostly just ensures\n    # that the interaction with RefCall variants is properly handled.\n    ref_call_deletion = _var(\n        start=1,\n        end=30,\n        genotype=[0, 0],\n        # Not a real likelihood -- if we weren\'t just punting\n        # this would get rescaled to sum to 1.\n        likelihoods=[-1, -2, -3])\n    independent_hom_alts = [\n        _var(start=i, genotype=[1, 1], likelihoods=[-3, -2, -1])\n        for i in range(3, 20)\n    ]\n\n    variants = ([ref_call_deletion] + independent_hom_alts +\n                _resolvable_incompatible_inputs())\n    expected = ([ref_call_deletion] + independent_hom_alts +\n                _resolved_compatible_outputs())\n    actual = haplotypes.maybe_resolve_conflicting_variants(variants)\n    self._assert_generator_of_variants_equals_expected(actual, expected)\n\n  @parameterized.parameters(\n      dict(\n          disable_haplotype_resolution=False,\n          variants=_resolvable_incompatible_inputs(),\n          expected=_resolved_compatible_outputs()),\n      dict(\n          disable_haplotype_resolution=True,\n          variants=_resolvable_incompatible_inputs(),\n          expected=_resolvable_incompatible_inputs()),\n  )\n  @flagsaver.FlagSaver\n  def test_can_disable_haplotype_resolution(self, disable_haplotype_resolution,\n                                            variants, expected):\n    FLAGS.disable_haplotype_resolution = disable_haplotype_resolution\n    actual = haplotypes.maybe_resolve_conflicting_variants(variants)\n    self._assert_generator_of_variants_equals_expected(actual, expected)\n\n  @parameterized.parameters(\n      # The simple case where there is a single variant.\n      dict(\n          variants=[\n              _var(\n                  start=10,\n                  ref=\'A\',\n                  alt=[\'C\'],\n                  genotype=[0, 1],\n                  likelihoods=[-2., -0.0506099933550872, -1.])\n          ],\n          expected=[\n              _var(\n                  start=10,\n                  ref=\'A\',\n                  alt=[\'C\'],\n                  genotype=[0, 1],\n                  likelihoods=[-2., -0.0506099933550872, -1.])\n          ]),\n      # Cases where the actual genotype calls are compatible.\n      dict(\n          variants=[\n              _var(\n                  start=20,\n                  ref=\'ACCCCC\',\n                  alt=[\'A\'],\n                  genotype=[0, 1],\n                  likelihoods=[-2., -0.0506099933550872, -1.]),\n              _var(\n                  start=23,\n                  ref=\'C\',\n                  alt=[\'T\'],\n                  genotype=[0, 1],\n                  likelihoods=[-3., -0.004803708402820599, -2.])\n          ],\n          expected=[\n              _var(\n                  start=20,\n                  ref=\'ACCCCC\',\n                  alt=[\'A\'],\n                  genotype=[0, 1],\n                  likelihoods=[-2., -0.0506099933550872, -1.]),\n              _var(\n                  start=23,\n                  ref=\'C\',\n                  alt=[\'T\'],\n                  genotype=[0, 1],\n                  likelihoods=[-3., -0.004803708402820599, -2.])\n          ]),\n      dict(\n          variants=[\n              _var(\n                  start=20,\n                  ref=\'ACCCCC\',\n                  alt=[\'A\'],\n                  genotype=[0, 1],\n                  likelihoods=[-2., -0.0506099933550872, -1.]),\n              _var(\n                  start=21,\n                  ref=\'C\',\n                  alt=[\'G\'],\n                  genotype=[0, 1],\n                  likelihoods=[-3., -0.004803708402820599, -2.]),\n              _var(\n                  start=23,\n                  ref=\'C\',\n                  alt=[\'T\'],\n                  genotype=[0, 1],\n                  likelihoods=[-3., -0.004803708402820599, -2.])\n          ],\n          expected=[\n              _var(\n                  start=20,\n                  ref=\'ACCCCC\',\n                  alt=[\'A\'],\n                  genotype=[0, 1],\n                  likelihoods=[-2., -0.0506099933550872, -1.]),\n              _var(\n                  start=21,\n                  ref=\'C\',\n                  alt=[\'G\'],\n                  genotype=[0, 1],\n                  likelihoods=[-3., -0.004803708402820599, -2.]),\n              _var(\n                  start=23,\n                  ref=\'C\',\n                  alt=[\'T\'],\n                  genotype=[0, 1],\n                  likelihoods=[-3., -0.004803708402820599, -2.])\n          ]),\n      dict(\n          variants=[\n              _var(\n                  start=20,\n                  ref=\'ACCC\',\n                  alt=[\'A\'],\n                  genotype=[0, 1],\n                  likelihoods=[-2., -0.0506099933550872, -1.]),\n              _var(\n                  start=22,\n                  ref=\'CCCGAGAGAG\',\n                  alt=[\'C\'],\n                  genotype=[0, 1],\n                  likelihoods=[-3., -0.004803708402820599, -2.]),\n              _var(\n                  start=25,\n                  ref=\'G\',\n                  alt=[\'T\'],\n                  genotype=[0, 1],\n                  likelihoods=[-3., -0.004803708402820599, -2.])\n          ],\n          expected=[\n              _var(\n                  start=20,\n                  ref=\'ACCC\',\n                  alt=[\'A\'],\n                  genotype=[0, 1],\n                  likelihoods=[-2., -0.0506099933550872, -1.]),\n              _var(\n                  start=22,\n                  ref=\'CCCGAGAGAG\',\n                  alt=[\'C\'],\n                  genotype=[0, 1],\n                  likelihoods=[-3., -0.004803708402820599, -2.]),\n              _var(\n                  start=25,\n                  ref=\'G\',\n                  alt=[\'T\'],\n                  genotype=[0, 1],\n                  likelihoods=[-3., -0.004803708402820599, -2.])\n          ]),\n      # Cases where the actual genotype calls are incompatible.\n      dict(\n          variants=[\n              _var(\n                  start=20,\n                  ref=\'ACCCCC\',\n                  alt=[\'A\'],\n                  genotype=[0, 1],\n                  likelihoods=[-2., -0.0506099933550872, -1.]),\n              _var(\n                  start=23,\n                  ref=\'C\',\n                  alt=[\'T\'],\n                  genotype=[1, 1],\n                  likelihoods=[-2., -0.3098039199714863, -0.3010299956639812])\n          ],\n          expected=[\n              _var(\n                  start=20,\n                  ref=\'ACCCCC\',\n                  alt=[\'A\'],\n                  genotype=[0, 1],\n                  likelihoods=[\n                      -1.658964842664435, -0.010604831683503404,\n                      -2.6589648426644352\n                  ]),\n              _var(\n                  start=23,\n                  ref=\'C\',\n                  alt=[\'T\'],\n                  genotype=[0, 1],\n                  likelihoods=[\n                      -1.658964842664435, -0.014526253196596468,\n                      -1.9599948383284163\n                  ])\n          ]),\n      dict(\n          variants=[\n              _var(\n                  start=20,\n                  ref=\'ACCCCC\',\n                  alt=[\'A\'],\n                  genotype=[0, 1],\n                  likelihoods=[-2., -0.0506099933550872, -1.]),\n              _var(\n                  start=23,\n                  ref=\'C\',\n                  alt=[\'T\', \'G\'],\n                  genotype=[1, 2],\n                  likelihoods=[\n                      -2.0, -1.0, -0.6989700043360187, -0.958607314841775,\n                      -0.4814860601221125, -0.6020599913279624\n                  ])\n          ],\n          expected=[\n              _var(\n                  start=20,\n                  ref=\'ACCCCC\',\n                  alt=[\'A\'],\n                  genotype=[0, 1],\n                  likelihoods=[\n                      -1.315550534421905, -0.02373784695478589,\n                      -2.315550534421905\n                  ]),\n              _var(\n                  start=23,\n                  ref=\'C\',\n                  alt=[\'T\', \'G\'],\n                  genotype=[0, 2],\n                  likelihoods=[\n                      -1.315550534421905, -0.36130802498257997,\n                      -2.0145205387579237, -0.319915339824355,\n                      -1.7970365945440174, -1.9176105257498672\n                  ])\n          ]),\n      # Issues we can\'t currently resolve.\n      dict(\n          variants=[\n              _var(\n                  start=20,\n                  ref=\'ACCCCC\',\n                  alt=[\'A\'],\n                  genotype=[1, 1],\n                  likelihoods=[\n                      -1.5228787452803376, -0.09691001300805639,\n                      -0.7695510786217261\n                  ]),\n              _var(\n                  start=23,\n                  ref=\'CCCGATGAT\',\n                  alt=[\'C\'],\n                  genotype=[1, 1],\n                  likelihoods=[\n                      -1.3979400086720375, -0.1366771398795441,\n                      -0.638272163982407\n                  ]),\n              _var(\n                  start=24,\n                  ref=\'C\',\n                  alt=[\'G\'],\n                  genotype=[1, 1],\n                  likelihoods=[\n                      -1.5228787452803376, -0.13076828026902382,\n                      -0.638272163982407\n                  ])\n          ],\n          expected=[\n              _var(\n                  start=20,\n                  ref=\'ACCCCC\',\n                  alt=[\'A\'],\n                  genotype=[1, 1],\n                  likelihoods=[\n                      -1.5228787452803376, -0.09691001300805639,\n                      -0.7695510786217261\n                  ]),\n              _var(\n                  start=23,\n                  ref=\'CCCGATGAT\',\n                  alt=[\'C\'],\n                  genotype=[1, 1],\n                  likelihoods=[\n                      -1.3979400086720375, -0.1366771398795441,\n                      -0.638272163982407\n                  ]),\n              _var(\n                  start=24,\n                  ref=\'C\',\n                  alt=[\'G\'],\n                  genotype=[1, 1],\n                  likelihoods=[\n                      -1.5228787452803376, -0.13076828026902382,\n                      -0.638272163982407\n                  ])\n          ]),\n      # Too many variants to resolve.\n      dict(\n          variants=[\n              _var(\n                  start=1,\n                  end=30,\n                  genotype=[0, 1],\n                  # Not a real likelihood -- if we weren\'t just punting\n                  # this would get rescaled to sum to 1.\n                  likelihoods=[-2, -1, -3])\n          ] + [\n              _var(start=i, genotype=[1, 1], likelihoods=[-3, -2, -1])\n              for i in range(3, 25)\n          ],\n          expected=[\n              _var(\n                  start=1,\n                  end=30,\n                  genotype=[0, 1],\n                  # Not a real likelihood -- if we weren\'t just punting\n                  # this would get rescaled to sum to 1.\n                  likelihoods=[-2, -1, -3])\n          ] + [\n              _var(start=i, genotype=[1, 1], likelihoods=[-3, -2, -1])\n              for i in range(3, 25)\n          ]),\n  )\n  def test_resolve_overlapping_variants(self, variants, expected):\n    actual = haplotypes._resolve_overlapping_variants(variants)\n    self._assert_generator_of_variants_equals_expected(actual, expected)\n\n  @parameterized.parameters(\n      dict(variants=[], expected=[]),\n      dict(\n          variants=[_var(chrom=\'1\', start=5, end=6)],\n          expected=[[_var(chrom=\'1\', start=5, end=6)]]),\n      # Test finding overlaps and different chromosomes not overlapping.\n      dict(\n          variants=[\n              _var(chrom=\'1\', start=1, end=5),\n              _var(chrom=\'1\', start=5, end=7),\n              _var(chrom=\'1\', start=6, end=8),\n              _var(chrom=\'2\', start=6, end=10)\n          ],\n          expected=[[_var(chrom=\'1\', start=1, end=5)],\n                    [\n                        _var(chrom=\'1\', start=5, end=7),\n                        _var(chrom=\'1\', start=6, end=8)\n                    ], [_var(chrom=\'2\', start=6, end=10)]]),\n      # Test one large variant spanning multiple others.\n      dict(\n          variants=[\n              _var(chrom=\'1\', start=1, end=25),\n              _var(chrom=\'1\', start=3, end=5),\n              _var(chrom=\'1\', start=7, end=8),\n              _var(chrom=\'1\', start=14, end=20),\n              _var(chrom=\'1\', start=24, end=27)\n          ],\n          expected=[[\n              _var(chrom=\'1\', start=1, end=25),\n              _var(chrom=\'1\', start=3, end=5),\n              _var(chrom=\'1\', start=7, end=8),\n              _var(chrom=\'1\', start=14, end=20),\n              _var(chrom=\'1\', start=24, end=27)\n          ]]),\n      # Test mix of non-overlapping and ending with an overlap.\n      dict(\n          variants=[\n              _var(chrom=\'1\', start=1, end=5),\n              _var(chrom=\'2\', start=3, end=5),\n              _var(chrom=\'2\', start=7, end=10),\n              _var(chrom=\'2\', start=9, end=10)\n          ],\n          expected=[[_var(chrom=\'1\', start=1, end=5)],\n                    [_var(chrom=\'2\', start=3, end=5)],\n                    [\n                        _var(chrom=\'2\', start=7, end=10),\n                        _var(chrom=\'2\', start=9, end=10)\n                    ]]),\n      # Same as prior test but using a generator as input.\n      dict(\n          variants=(_var(chrom=\'1\', start=1,\n                         end=5), _var(chrom=\'2\', start=3,\n                                      end=5), _var(chrom=\'2\', start=7, end=10),\n                    _var(chrom=\'2\', start=9, end=10)),\n          expected=[[_var(chrom=\'1\', start=1, end=5)],\n                    [_var(chrom=\'2\', start=3, end=5)],\n                    [\n                        _var(chrom=\'2\', start=7, end=10),\n                        _var(chrom=\'2\', start=9, end=10)\n                    ]]),\n  )\n  def test_group_overlapping_variants(self, variants, expected):\n    actual = haplotypes._group_overlapping_variants(variants)\n    self.assertIsInstance(actual, types.GeneratorType)\n    actual_list = list(actual)\n    self.assertEqual(actual_list, expected)\n\n  @parameterized.parameters(\n      dict(variants=[_var(start=10, end=20)], nonref_counts=[2], expected=True),\n      dict(variants=[_var(start=10, end=20)], nonref_counts=[1], expected=True),\n      dict(\n          variants=[_var(start=10, end=20),\n                    _var(start=15, end=20)],\n          nonref_counts=[1, 1],\n          expected=True),\n      dict(\n          variants=[_var(start=10, end=20),\n                    _var(start=15, end=20)],\n          nonref_counts=[1, 2],\n          expected=False),\n      dict(\n          variants=[\n              _var(start=10, end=20),\n              _var(start=15, end=25),\n              _var(start=20, end=25)\n          ],\n          nonref_counts=[1, 1, 1],\n          expected=True),\n      dict(\n          variants=[\n              _var(start=10, end=20),\n              _var(start=15, end=25),\n              _var(start=20, end=25)\n          ],\n          nonref_counts=[1, 2, 1],\n          expected=False),\n      dict(\n          variants=[\n              _var(start=10, end=20),\n              _var(start=15, end=25),\n              _var(start=20, end=25)\n          ],\n          nonref_counts=[1, 1, 2],\n          expected=False),\n      dict(\n          variants=[\n              _var(start=10, end=20),\n              _var(start=15, end=25),\n              _var(start=19, end=25)\n          ],\n          nonref_counts=[2, 0, 2],\n          expected=False),\n      dict(\n          variants=[\n              _var(start=10, end=20),\n              _var(start=15, end=25),\n              _var(start=20, end=25)\n          ],\n          nonref_counts=[2, 0, 2],\n          expected=True),\n  )\n  def test_all_variants_compatible(self, variants, nonref_counts, expected):\n    calculator = haplotypes._VariantCompatibilityCalculator(variants)\n    actual = calculator.all_variants_compatible(nonref_counts)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(variants=[_var(start=10, end=20)], nonref_counts=[3]),\n      dict(variants=[_var(start=10, end=20)], nonref_counts=[1, 2]),\n      dict(variants=[_var(start=10, end=20)], nonref_counts=[]),\n      dict(\n          variants=[_var(start=10, end=20),\n                    _var(start=15, end=20)],\n          nonref_counts=[1]),\n      dict(\n          variants=[_var(start=10, end=20),\n                    _var(start=15, end=20)],\n          nonref_counts=[1, 2, 1]),\n  )\n  def test_invalid_all_variants_compatible(self, variants, nonref_counts):\n    calculator = haplotypes._VariantCompatibilityCalculator(variants)\n    with six.assertRaisesRegex(self, ValueError, \'variant\'):\n      calculator.all_variants_compatible(nonref_counts)\n\n  @parameterized.parameters(\n      dict(num_alts_list=[1, 1], config=[0, 1], expected=[\n          ((0, 0), (0, 1)),\n      ]),\n      dict(\n          num_alts_list=[1, 2],\n          config=[0, 1],\n          expected=[((0, 0), (0, 1)), ((0, 0), (0, 2))]),\n      dict(\n          num_alts_list=[2, 2],\n          config=[2, 1],\n          expected=[((1, 1), (0, 1)), ((1, 1), (0, 2)), ((1, 2), (0, 1)),\n                    ((1, 2), (0, 2)), ((2, 2), (0, 1)), ((2, 2), (0, 2))]),\n  )\n  def test_get_all_allele_indices_configurations(self, num_alts_list, config,\n                                                 expected):\n\n    def get_alt_bases(num_alts):\n      return [\'C\' + \'A\' * i for i in range(1, num_alts + 1)]\n\n    variants = [\n        _var(ref=\'C\', alt=get_alt_bases(num_alts)) for num_alts in num_alts_list\n    ]\n    actual = haplotypes._get_all_allele_indices_configurations(variants, config)\n    self.assertEqual(list(actual), expected)\n\n  def test_invalid_get_all_allele_indices_configurations(self):\n    with six.assertRaisesRegex(self, ValueError, r\'len\\(variants\\) must equal\'):\n      haplotypes._get_all_allele_indices_configurations(\n          _resolved_compatible_outputs(), [1])\n\n  def test_invalid_allele_indices_configuration_likelihood(self):\n    with six.assertRaisesRegex(self, ValueError,\n                               r\'equal len\\(allele_indices_conf\'):\n      haplotypes._allele_indices_configuration_likelihood(\n          _resolved_compatible_outputs(), [(1, 1)])\n\n  @parameterized.parameters(\n      dict(genotype=[-1, -1], expected=0),\n      dict(genotype=[0, 0], expected=0),\n      dict(genotype=[0, 1], expected=1),\n      dict(genotype=[1, 0], expected=1),\n      dict(genotype=[1, 1], expected=2),\n      dict(genotype=[1, 2], expected=2),\n      dict(genotype=[2, 2], expected=2),\n  )\n  def test_nonref_genotype_count(self, genotype, expected):\n    variant = _var(ref=\'AC\', alt=[\'A\', \'ACC\'], genotype=genotype)\n    actual = haplotypes._nonref_genotype_count(variant)\n    self.assertEqual(actual, expected)\n\n  def test_invalid_nonref_genotype_count(self):\n    zero_calls_variant = test_utils.make_variant()\n    with six.assertRaisesRegex(self, ValueError,\n                               \'Expected exactly one VariantCal\'):\n      haplotypes._nonref_genotype_count(zero_calls_variant)\n\n  def _assert_generator_of_variants_equals_expected(self, actual, expected):\n    """"""Helper method to compare a generator of Variant protos to a list.""""""\n    self.assertIsInstance(actual, types.GeneratorType)\n    actual_list = list(actual)\n    self.assertEqual(len(actual_list), len(expected))\n    for actual_variant, expected_variant in zip(actual_list, expected):\n      self._assert_variants_equal_with_likelihood_tolerance(\n          actual_variant, expected_variant)\n\n  def _assert_variants_equal_with_likelihood_tolerance(self,\n                                                       v1,\n                                                       v2,\n                                                       tolerance=1e-10):\n    """"""Asserts variant equality allowing numerical differences in GLs.""""""\n    gl1 = list(v1.calls[0].genotype_likelihood)\n    gl2 = list(v2.calls[0].genotype_likelihood)\n    np.testing.assert_allclose(gl1, gl2, rtol=tolerance)\n    v1.calls[0].genotype_likelihood[:] = []\n    v2.calls[0].genotype_likelihood[:] = []\n    self.assertEqual(v1, v2)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/logging_level.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Control the verbosity of the program.\n\nNormally logging at info priority is silent; this\nprovides flags to adjust that.\n\nNote that TF tries to send log messages to stdout,\ninstead of stderr, if it thinks it is interactive.\nThere\'s no flag to override that.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\n\nfrom absl import flags\n\nflags.DEFINE_string(\'logging_level\', \'INFO\',\n                    \'select general logging threshold.\')\n\n\ndef set_level(level_name):\n  for x in (logging.DEBUG, logging.ERROR, logging.FATAL, logging.INFO,\n            logging.WARN):\n    if logging.getLevelName(x) == level_name.upper():\n      logging.getLogger().setLevel(x)\n      return\n\n\ndef set_info():\n  return set_level(\'INFO\')\n\n\ndef set_from_flag():\n  return set_level(flags.FLAGS.logging_level)\n'"
deepvariant/make_examples.py,10,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Step one of DeepVariant: creates tf.Example protos for training/calling.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nimport numpy as np\nimport tensorflow as tf\nfrom deepvariant import dv_constants\nfrom deepvariant import exclude_contigs\nfrom deepvariant import logging_level\nfrom deepvariant import pileup_image\nfrom deepvariant import resources\nfrom deepvariant import tf_utils\nfrom deepvariant import vcf_candidate_importer\nfrom deepvariant import very_sensitive_caller\nfrom deepvariant.labeler import customized_classes_labeler\nfrom deepvariant.labeler import haplotype_labeler\nfrom deepvariant.labeler import positional_labeler\nfrom deepvariant.protos import deepvariant_pb2\nfrom deepvariant.python import allelecounter\nfrom deepvariant.realigner import realigner\nfrom deepvariant.vendor import timer\nfrom google.protobuf import text_format\nfrom third_party.nucleus.io import fasta\nfrom third_party.nucleus.io import sam\nfrom third_party.nucleus.io import sharded_file_utils\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.io.python import hts_verbose\nfrom third_party.nucleus.protos import reads_pb2\nfrom third_party.nucleus.util import errors\nfrom third_party.nucleus.util import proto_utils\nfrom third_party.nucleus.util import ranges\nfrom third_party.nucleus.util import utils\nfrom third_party.nucleus.util import variant_utils\n\nFLAGS = flags.FLAGS\n\n# Sentinel command line flag value indicating no downsampling should occur.\nNO_DOWNSAMPLING = 0.0\n\n# Sentinel command line flag value indicating no random ref sites should be\n# emitted.\nNO_RANDOM_REF = 0.0\n\n# The name used for a sample if one is not specified or present in the reads.\n_UNKNOWN_SAMPLE = \'UNKNOWN\'\n\n# The extension we add to our examples path to write our MakeExamplesRunInfo\n# protobuf.\n_RUN_INFO_FILE_EXTENSION = \'.run_info.pbtxt\'\n\n# Use a default hts_block_size value of 128 MB (see internal for details) to\n# improve SAM/BAM reading throughput, particularly on remote filesystems. Do not\n# modify this default parameter without a systematic evaluation of the impact\n# across a variety of distributed filesystems!\n_DEFAULT_HTS_BLOCK_SIZE = 128 * (1024 * 1024)\n\nflags.DEFINE_string(\n    \'ref\', None,\n    \'Required. Genome reference to use. Must have an associated FAI index as \'\n    \'well. Supports text or gzipped references. Should match the reference \'\n    \'used to align the BAM file provided to --reads.\')\nflags.DEFINE_string(\n    \'reads\', None,\n    \'Required. Aligned, sorted, indexed BAM file containing the reads we want \'\n    \'to call. Should be aligned to a reference genome compatible with --ref. \'\n    \'Can provide multiple BAMs (comma-separated).\')\nflags.DEFINE_bool(\n    \'use_ref_for_cram\', True,\n    \'If true, use the --ref argument as the reference file for the CRAM \'\n    \'file passed to --reads.  In this case, it is required that the reference \'\n    \'file be located on a local POSIX filesystem. To disable, specify \'\n    \'--nouse_ref_for_cram.\')\nflags.DEFINE_string(\n    \'examples\', None,\n    \'Required. Path to write tf.Example protos in TFRecord format.\')\nflags.DEFINE_string(\n    \'candidates\', \'\',\n    \'Candidate DeepVariantCalls in tfrecord format. For DEBUGGING.\')\nflags.DEFINE_string(\'mode\', None,\n                    \'Mode to run. Must be one of calling or training\')\nflags.DEFINE_string(\n    \'regions\', \'\',\n    \'Optional. Space-separated list of regions we want to process. Elements \'\n    \'can be region literals (e.g., chr20:10-20) or paths to BED/BEDPE files.\')\nflags.DEFINE_string(\n    \'exclude_regions\', \'\',\n    \'Optional. Space-separated list of regions we want to exclude from \'\n    \'processing. Elements can be region literals (e.g., chr20:10-20) or paths \'\n    \'to BED/BEDPE files. Region exclusion happens after processing the \'\n    \'--regions argument, so --region 20 --exclude_regions 20:100 does \'\n    \'everything on chromosome 20 excluding base 100\')\nflags.DEFINE_string(\n    \'variant_caller\', \'very_sensitive_caller\',\n    \'The caller to use to make examples. Must be one of the VariantCaller enum \'\n    \'values in the DeepVariantOptions proto.\')\nflags.DEFINE_string(\n    \'gvcf\', \'\',\n    \'Optional. Path where we should write gVCF records in TFRecord of Variant \'\n    \'proto format.\')\nflags.DEFINE_integer(\n    \'gvcf_gq_binsize\', 5,\n    \'Bin size in which to quantize gVCF genotype qualities. Larger bin size \'\n    \'reduces the number of gVCF records at a loss of quality granularity.\')\nflags.DEFINE_string(\n    \'confident_regions\', \'\',\n    \'Regions that we are confident are hom-ref or a variant in BED format. In \'\n    \'BED or other equivalent format, sorted or unsorted. Contig names must \'\n    \'match those of the reference genome.\')\nflags.DEFINE_string(\n    \'truth_variants\', \'\',\n    \'Tabix-indexed VCF file containing the truth variant calls for this labels \'\n    \'which we use to label our examples.\')\nflags.DEFINE_string(\n    \'proposed_variants\', \'\',\n    \'(Only used when --variant_caller=vcf_candidate_importer.) \'\n    \'Tabix-indexed VCF file containing the proposed positions and alts for \'\n    \'`vcf_candidate_importer`. The GTs will be ignored.\')\nflags.DEFINE_integer(\'task\', 0, \'Task ID of this task\')\nflags.DEFINE_integer(\n    \'partition_size\', 1000,\n    \'The maximum number of basepairs we will allow in a region before splitting\'\n    \'it into multiple smaller subregions.\')\nflags.DEFINE_integer(\n    \'max_reads_per_partition\', 1500,\n    \'The maximum number of reads per partition that we consider before \'\n    \'following processing such as sampling and realigner.\')\nflags.DEFINE_string(\n    \'multi_allelic_mode\', \'\',\n    \'How to handle multi-allelic candidate variants. For DEBUGGING\')\nflags.DEFINE_bool(\'realign_reads\', True,\n                  \'If True, locally realign reads before calling variants.\')\nflags.DEFINE_bool(\n    \'write_run_info\', False,\n    \'If True, write out a MakeExamplesRunInfo proto besides our examples in \'\n    \'text_format.\')\nflags.DEFINE_float(\n    \'downsample_fraction\', NO_DOWNSAMPLING,\n    \'If not \' + str(NO_DOWNSAMPLING) + \' must be a value between 0.0 and 1.0. \'\n    \'Reads will be kept (randomly) with a probability of downsample_fraction \'\n    \'from the input BAM. This argument makes it easy to create examples as \'\n    \'though the input BAM had less coverage.\')\nflags.DEFINE_string(\n    \'sample_name\', \'\', \'Sample name to use for our sample_name in the output \'\n    \'Variant/DeepVariantCall protos. If not specified, will be inferred from \'\n    \'the header information from --reads.\')\nflags.DEFINE_string(\'hts_logging_level\',\n                    hts_verbose.htsLogLevel.HTS_LOG_WARNING.name,\n                    \'Sets the htslib logging threshold.\')\nflags.DEFINE_integer(\n    \'hts_block_size\', _DEFAULT_HTS_BLOCK_SIZE,\n    \'Sets the htslib block size. Zero or negative uses default htslib setting; \'\n    \'larger values (e.g. 1M) may be beneficial for using remote files. \'\n    \'Currently only applies to SAM/BAM reading.\')\nflags.DEFINE_integer(\n    \'min_base_quality\', 10,\n    \'Minimum base quality. This field indicates that we are enforcing a \'\n    \'minimum base quality score for alternate alleles. Alternate alleles will \'\n    \'only be considered if all bases in the allele have a quality greater than \'\n    \'min_base_quality.\')\nflags.DEFINE_integer(\n    \'min_mapping_quality\', 10,\n    \'By default, reads with any mapping quality are kept. Setting this field \'\n    \'to a positive integer i will only keep reads that have a MAPQ >= i. Note \'\n    \'this only applies to aligned reads.\')\nflags.DEFINE_integer(\n    \'vsc_min_count_snps\', 2,\n    \'SNP alleles occurring at least this many times in our \'\n    \'AlleleCount will be advanced as candidates.\')\nflags.DEFINE_integer(\n    \'vsc_min_count_indels\', 2,\n    \'Indel alleles occurring at least this many times in \'\n    \'our AlleleCount will be advanced as candidates.\')\nflags.DEFINE_float(\n    \'vsc_min_fraction_snps\', 0.12,\n    \'SNP alleles occurring at least this fraction of all \'\n    \'counts in our AlleleCount will be advanced as \'\n    \'candidates.\')\nflags.DEFINE_float(\n    \'vsc_min_fraction_indels\', 0.06,\n    \'Indel alleles occurring at least this fraction of all counts in our \'\n    \'AlleleCount will be advanced as candidates.\')\nflags.DEFINE_float(\n    \'training_random_emit_ref_sites\', NO_RANDOM_REF,\n    \'If > 0, emit extra random reference examples with this probability.\')\nflags.DEFINE_integer(\n    \'pileup_image_height\', 0,\n    \'Height for the pileup image. If 0, uses the default height\')\nflags.DEFINE_integer(\n    \'pileup_image_width\', 0,\n    \'Width for the pileup image. If 0, uses the default width\')\nflags.DEFINE_string(\n    \'labeler_algorithm\', \'haplotype_labeler\',\n    \'Algorithm to use to label examples in training mode. Must be one of the \'\n    \'LabelerAlgorithm enum values in the DeepVariantOptions proto.\')\nflags.DEFINE_string(\n    \'customized_classes_labeler_classes_list\', \'\',\n    \'A comma-separated list of strings that defines customized class labels \'\n    \'for variants. This is only set when labeler_algorithm is \'\n    \'customized_classes_labeler.\')\nflags.DEFINE_string(\n    \'customized_classes_labeler_info_field_name\', \'\',\n    \'The name from the INFO field of VCF where we should get the customized \'\n    \'class labels from. This is only set when labeler_algorithm is \'\n    \'customized_classes_labeler.\')\nflags.DEFINE_integer(\n    \'logging_every_n_candidates\', 100,\n    \'Print out the log every n candidates. The smaller the number, the more \'\n    \'frequent the logging information emits.\')\nflags.DEFINE_bool(\'keep_duplicates\', False, \'If True, keep duplicate reads.\')\nflags.DEFINE_bool(\'keep_supplementary_alignments\', False,\n                  \'If True, keep reads marked as supplementary alignments.\')\nflags.DEFINE_bool(\'keep_secondary_alignments\', False,\n                  \'If True, keep reads marked as secondary alignments.\')\nflags.DEFINE_bool(\n    \'parse_sam_aux_fields\', False,\n    \'If True, auxiliary fields of the SAM/BAM/CRAM records are parsed.\')\nflags.DEFINE_bool(\'use_original_quality_scores\', False,\n                  \'If True, base quality scores are read from OQ tag.\')\nflags.DEFINE_string(\n    \'select_variant_types\', None,\n    \'If provided, should be a whitespace-separated string of variant types to \'\n    \'keep when generating examples. Permitted values are ""snps"", ""indels"", \'\n    \'""multi-allelics"", and ""all"", which select bi-allelic snps, bi-allelic \'\n    \'indels, multi-allelic variants of any type, and all variants, \'\n    \'respectively. Multiple selectors can be specified, so that \'\n    \'--select_variant_types=""snps indels"" would keep all bi-allelic SNPs and \'\n    \'indels\')\nflags.DEFINE_bool(\n    \'custom_pileup_image\', False,\n    \'Experimental - please do not set this flag. If True, an \'\n    \'additional channel will be added to encode CIGAR op length \'\n    \'for indels.\')\nflags.DEFINE_bool(\n    \'sequencing_type_image\', False,\n    \'If True, add an additional channel representing the sequencing type of \'\n    \'the input example. This flag is experimental and is not currently being \'\n    \'used.\')\nflags.DEFINE_string(\n    \'sequencing_type\', None,\n    \'A string representing input bam file sequencing_type. Permitted values are \'\n    \'""WGS"" and ""WES"", which represent whole genome sequencing and whole exome \'\n    \'sequencing, respectively. This flag is experimental and is not currently \'\n    \'being used.\')\n\n# ---------------------------------------------------------------------------\n# Selecting variants of specific types (e.g., SNPs)\n# ---------------------------------------------------------------------------\n\n\ndef _select_biallelic_snps(v):\n  return variant_utils.is_snp(v) and variant_utils.is_biallelic(v)\n\n\ndef _select_biallelic_indels(v):\n  return variant_utils.is_indel(v) and variant_utils.is_biallelic(v)\n\n\ndef _select_biallelic_insertions(v):\n  return variant_utils.has_insertion(v) and variant_utils.is_biallelic(v)\n\n\ndef _select_biallelic_deletions(v):\n  return variant_utils.has_deletion(v) and variant_utils.is_biallelic(v)\n\n\n_VARIANT_TYPE_SELECTORS = {\n    \'snps\': _select_biallelic_snps,\n    \'indels\': _select_biallelic_indels,\n    \'insertions\': _select_biallelic_insertions,\n    \'deletions\': _select_biallelic_deletions,\n    \'multi-allelics\': variant_utils.is_multiallelic,\n    \'all\': lambda v: True,\n}\n\n# ---------------------------------------------------------------------------\n# Option handling\n# ---------------------------------------------------------------------------\n\n\ndef parse_proto_enum_flag(proto_enum_pb2,\n                          flag_value,\n                          skip_unspecified_option=True):\n  """"""Parses a command line flag string value into a protobuf Enum value.\n\n  Args:\n    proto_enum_pb2: a enum_type_wrapper.EnumTypeWrapper type containing a proto\n      enum definition. For example, this would be\n      deepvariant_pb2.DeepVariantOptions.Mode to get the DeepVariantOptions Mode\n      enum. See:\n      https://developers.google.com/protocol-buffers/docs/reference/python-generated#enum\n        for more information.\n    flag_value: str. The name of the proto enum option from the command line we\n      want to convert into the enum value.\n    skip_unspecified_option: bool. If True, any enum options that include the\n      string \'unspecified\' (in any case) will be excluded from the list of\n      allowed options in the ValueError raised if flag_value isn\'t valid.\n\n  Returns:\n    The enum value for flag_value in proto_enum_pb2\n\n  Raises:\n    ValueError: if flag_value isn\'t a valid enum name in proto_enum_pb2.\n  """"""\n  try:\n    return proto_enum_pb2.Value(flag_value)\n  except ValueError:\n    options = proto_enum_pb2.keys()\n    if skip_unspecified_option:\n      options = [o for o in options if \'unspecified\' not in o.lower()]\n    raise ValueError(\'Unknown enum option ""{}"". Allowed options are {}\'.format(\n        flag_value, \',\'.join(sorted(options))))\n\n\ndef parse_regions_flag(regions_flag_value):\n  if isinstance(regions_flag_value, str):\n    regions_flag_value = regions_flag_value.split()\n  return regions_flag_value\n\n\ndef default_options(add_flags=True, flags_obj=None):\n  """"""Creates a DeepVariantOptions proto populated with reasonable defaults.\n\n  Args:\n    add_flags: bool. defaults to True. If True, we will push the value of\n      certain FLAGS into our options. If False, those option fields are left\n      uninitialized.\n    flags_obj: object.  If not None, use as the source of flags, else use global\n      FLAGS.\n\n  Returns:\n    deepvariant_pb2.DeepVariantOptions protobuf.\n\n  Raises:\n    ValueError: If we observe invalid flag values.\n  """"""\n  if not flags_obj:\n    flags_obj = FLAGS\n\n  read_reqs = reads_pb2.ReadRequirements(\n      keep_duplicates=flags_obj.keep_duplicates,\n      keep_supplementary_alignments=flags_obj.keep_supplementary_alignments,\n      keep_secondary_alignments=flags_obj.keep_secondary_alignments,\n      min_base_quality=flags_obj.min_base_quality,\n      min_mapping_quality=flags_obj.min_mapping_quality,\n      min_base_quality_mode=reads_pb2.ReadRequirements.ENFORCED_BY_CLIENT)\n\n  logging.info(\'ReadRequirements are: %s\', read_reqs)\n\n  pic_options = pileup_image.default_options(read_requirements=read_reqs)\n\n  allele_counter_options = deepvariant_pb2.AlleleCounterOptions(\n      partition_size=flags_obj.partition_size, read_requirements=read_reqs)\n\n  if flags_obj.sample_name:\n    sample_name = flags_obj.sample_name\n  elif flags_obj.reads:\n    # If there are multiple BAM files, use the sample name from the first one.\n    with sam.SamReader(flags_obj.reads.split(\',\')[0]) as sam_reader:\n      sample_name = extract_sample_name_from_sam_reader(sam_reader)\n  else:\n    sample_name = _UNKNOWN_SAMPLE\n\n  variant_caller_options = deepvariant_pb2.VariantCallerOptions(\n      min_count_snps=flags_obj.vsc_min_count_snps,\n      min_count_indels=flags_obj.vsc_min_count_indels,\n      min_fraction_snps=flags_obj.vsc_min_fraction_snps,\n      min_fraction_indels=flags_obj.vsc_min_fraction_indels,\n      # Not specified by default: fraction_reference_sites_to_emit,\n      # Fixed random seed produced with \'od -vAn -N4 -tu4 < /dev/urandom\'.\n      random_seed=1400605801,\n      sample_name=sample_name,\n      p_error=0.001,\n      max_gq=50,\n      gq_resolution=flags_obj.gvcf_gq_binsize,\n      ploidy=2)\n\n  options = deepvariant_pb2.DeepVariantOptions(\n      exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS,\n      # Fixed random seed produced with \'od -vAn -N4 -tu4 < /dev/urandom\'.\n      random_seed=609314161,\n      # # Not specified by default: calling_regions = 3;\n      read_requirements=read_reqs,\n      allele_counter_options=allele_counter_options,\n      variant_caller_options=variant_caller_options,\n      pic_options=pic_options,\n      n_cores=1,\n      task_id=0,\n      num_shards=0,\n      min_shared_contigs_basepairs=0.9,\n  )\n\n  if add_flags:\n    options.mode = parse_proto_enum_flag(\n        deepvariant_pb2.DeepVariantOptions.Mode, flags_obj.mode.upper())\n\n    options.labeler_algorithm = parse_proto_enum_flag(\n        deepvariant_pb2.DeepVariantOptions.LabelerAlgorithm,\n        flags_obj.labeler_algorithm.upper())\n\n    options.variant_caller = parse_proto_enum_flag(\n        deepvariant_pb2.DeepVariantOptions.VariantCaller,\n        flags_obj.variant_caller.upper())\n\n    if flags_obj.ref:\n      options.reference_filename = flags_obj.ref\n    if flags_obj.reads:\n      options.reads_filenames.extend(flags_obj.reads.split(\',\'))\n    if flags_obj.confident_regions:\n      options.confident_regions_filename = flags_obj.confident_regions\n    if flags_obj.truth_variants:\n      options.truth_variants_filename = flags_obj.truth_variants\n    if flags_obj.proposed_variants:\n      options.proposed_variants_filename = flags_obj.proposed_variants\n    if flags_obj.sequencing_type_image:\n      options.pic_options.num_channels += 1\n      options.pic_options.sequencing_type_image = flags_obj.sequencing_type_image\n      if flags_obj.sequencing_type:\n        options.pic_options.sequencing_type = parse_proto_enum_flag(\n            deepvariant_pb2.PileupImageOptions.SequencingType,\n            flags_obj.sequencing_type)\n    if flags_obj.downsample_fraction != NO_DOWNSAMPLING:\n      options.downsample_fraction = flags_obj.downsample_fraction\n\n    if flags_obj.custom_pileup_image:\n      options.pic_options.custom_pileup_image = flags_obj.custom_pileup_image\n      options.pic_options.num_channels += 1\n      options.pic_options.insert_base_char = \'I\'\n      options.pic_options.delete_base_char = \'D\'\n\n    if flags_obj.multi_allelic_mode:\n      multi_allelic_enum = {\n          \'include_het_alt_images\':\n              deepvariant_pb2.PileupImageOptions.ADD_HET_ALT_IMAGES,\n          \'exclude_het_alt_images\':\n              deepvariant_pb2.PileupImageOptions.NO_HET_ALT_IMAGES,\n      }[flags_obj.multi_allelic_mode]\n      options.pic_options.multi_allelic_mode = multi_allelic_enum\n\n    if flags_obj.pileup_image_height:\n      options.pic_options.height = flags_obj.pileup_image_height\n    if flags_obj.pileup_image_width:\n      options.pic_options.width = flags_obj.pileup_image_width\n\n    if flags_obj.select_variant_types:\n      options.select_variant_types[:] = flags_obj.select_variant_types.split()\n      for svt in options.select_variant_types:\n        if svt not in _VARIANT_TYPE_SELECTORS:\n          errors.log_and_raise(\n              \'Select variant type {} not recognized. Allowed values are {}\'\n              .format(svt, \', \'.join(_VARIANT_TYPE_SELECTORS)),\n              errors.CommandLineError)\n\n    num_shards, examples, candidates, gvcf = (\n        sharded_file_utils.resolve_filespecs(flags_obj.task,\n                                             flags_obj.examples or \'\',\n                                             flags_obj.candidates or \'\',\n                                             flags_obj.gvcf or \'\'))\n    options.examples_filename = examples\n    options.candidates_filename = candidates\n    options.gvcf_filename = gvcf\n    options.task_id = flags_obj.task\n    options.num_shards = num_shards\n    if flags_obj.use_original_quality_scores and not flags_obj.parse_sam_aux_fields:\n      errors.log_and_raise(\n          \'If use_original_quality_scores is set then parse_sam_aux_fields \'\n          \'must be set too.\', errors.CommandLineError)\n    options.use_original_quality_scores = flags_obj.use_original_quality_scores\n\n    if flags_obj.write_run_info:\n      options.run_info_filename = examples + _RUN_INFO_FILE_EXTENSION\n\n    options.calling_regions.extend(parse_regions_flag(flags_obj.regions))\n    options.exclude_calling_regions.extend(\n        parse_regions_flag(flags_obj.exclude_regions))\n\n    options.realigner_enabled = flags_obj.realign_reads\n    if options.realigner_enabled:\n      options.realigner_options.CopyFrom(realigner.realigner_config(flags_obj))\n\n    options.max_reads_per_partition = flags_obj.max_reads_per_partition\n\n    if (options.mode == deepvariant_pb2.DeepVariantOptions.TRAINING and\n        flags_obj.training_random_emit_ref_sites != NO_RANDOM_REF):\n      options.variant_caller_options.fraction_reference_sites_to_emit = (\n          flags_obj.training_random_emit_ref_sites)\n\n  return options\n\n\ndef logging_with_options(options, message):\n  """"""If options contain multiple shards, log with task/shard prefix.""""""\n  if options.num_shards > 1:\n    prefix = \'Task {}/{}: \'.format(options.task_id, options.num_shards)\n  else:\n    prefix = \'\'\n  logging.info(prefix + message)\n\n\n# ---------------------------------------------------------------------------\n# Simple utilities\n# ---------------------------------------------------------------------------\n\n\ndef in_training_mode(options):\n  return options.mode == deepvariant_pb2.DeepVariantOptions.TRAINING\n\n\ndef gvcf_output_enabled(options):\n  """"""Returns True if we should be generating gVCF output.""""""\n  return bool(options.gvcf_filename)\n\n\ndef only_true(*elts):\n  """"""Returns the sublist of elements that evaluate to True.""""""\n  return [elt for elt in elts if elt]\n\n\ndef extract_sample_name_from_sam_reader(sam_reader):\n  """"""Returns the sample name as derived from the BAM file of reads.\n\n  Args:\n    sam_reader: Already opened sam_reader to use to extract the sample names\n      from. This sam_reader will not be closed after this function returns.\n\n  Returns:\n    The sample ID annotated in the read group.\n\n  Raises:\n    ValueError: There is not exactly one unique sample name in the SAM/BAM.\n  """"""\n  samples_list = [\n      rg.sample_id for rg in sam_reader.header.read_groups if rg.sample_id\n  ]\n  samples = set(samples_list)\n  if not samples:\n    logging.warning(\n        \'No non-empty sample name found in the input reads. \'\n        \'DeepVariant will use %s as the sample name. You can also \'\n        \'provide a sample name with the --sample_name argument.\',\n        dv_constants.DEFAULT_SAMPLE_NAME)\n    return dv_constants.DEFAULT_SAMPLE_NAME\n  elif len(samples) > 1:\n    logging.warning(\n        \'Multiple samples (%s) were found in the input reads. \'\n        \'Please confirm this is intended. For now, DeepVariant \'\n        \'will use the first sample name %s.\', \', \'.join(sorted(samples)),\n        samples_list[0])\n    return samples_list[0]\n  return next(iter(samples))\n\n\n# ---------------------------------------------------------------------------\n# Utilities for working with labeling metrics\n#\n# ---------------------------------------------------------------------------\n\n\ndef read_make_examples_run_info(path):\n  """"""Reads a MakeExamplesRunInfo proto in text_format from path.""""""\n  with tf.io.gfile.GFile(path) as f:\n    return text_format.Parse(f.read(), deepvariant_pb2.MakeExamplesRunInfo())\n\n\ndef write_make_examples_run_info(run_info_proto, path):\n  """"""Writes a MakeExamplesRunInfo proto in text_format to path.""""""\n  with tf.io.gfile.GFile(path, mode=\'w\') as writer:\n    writer.write(text_format.MessageToString(run_info_proto, float_format=\'\'))\n\n\n# ---------------------------------------------------------------------------\n# Region processing\n# ---------------------------------------------------------------------------\n\n\ndef _ensure_consistent_contigs(ref_contigs,\n                               sam_contigs,\n                               vcf_contigs,\n                               exclude_contig_names=None,\n                               min_coverage_fraction=1.0):\n  """"""Returns the common contigs after ensuring \'enough\' overlap.\n\n  Args:\n    ref_contigs: list of reference_pb2.ContigInfo protos in the reference\n      genome.\n    sam_contigs: list of reference_pb2.ContigInfo protos in the SAM/BAM file.\n    vcf_contigs: list of reference_pb2.ContigInfo protos in the VCF if in\n      training mode, or None otherwise.\n    exclude_contig_names: list of strings of contig names to exclude from\n      overlap consideration.\n    min_coverage_fraction: The fraction of the reference contigs that must be\n      shared with all inputs.\n\n  Returns:\n    The list of contigs common between all input sources.\n\n  Raises:\n    ValueError: The contigs are not sufficiently similar across input sources.\n  """"""\n  # Remove any excluded contigs from the ref_contigs, as we want to use the\n  # selected contigs for our overlap comparison.\n  if exclude_contig_names:\n    ref_contigs = [c for c in ref_contigs if c.name not in exclude_contig_names]\n\n  # Compute the common contigs among our inputs, and check that the contigs are\n  # sufficiently consistent among each other.\n  contigs = common_contigs(only_true(ref_contigs, sam_contigs, vcf_contigs))\n  validate_reference_contig_coverage(ref_contigs, contigs,\n                                     min_coverage_fraction)\n  return contigs\n\n\ndef common_contigs(contigs_list):\n  """"""Gets a list of contigs found in all contigs in contigs_list.\n\n  A common contig is considered one where the name and length in basepairs are\n  the same.\n\n  Args:\n    contigs_list: A sequence of lists of ContigInfo protos.\n\n  Returns:\n    A list of ContigInfo protos. Note that the individual protos found in this\n    returned list are shared with the ContigInfo protos found in contigs_list,\n    so should not be modified.\n  """"""\n\n  def common2(contigs1, contigs2):\n    """"""Computes the common contigs between contigs1 and contigs2.""""""\n    map2 = ranges.contigs_dict(contigs2)\n\n    def is_common(contig1):\n      contig2 = map2.get(contig1.name, None)\n      return contig2 and contig1.n_bases == contig2.n_bases\n\n    return [c for c in contigs1 if is_common(c)]\n\n  # Compute the common contigs by recursively getting common contigs of our\n  # master set of contigs (contigs) and each contig in other_contigs.\n  common = contigs_list[0]\n  for other_contigs in contigs_list[1:]:\n    common = common2(common, other_contigs)\n\n  return common\n\n\ndef validate_reference_contig_coverage(ref_contigs, shared_contigs,\n                                       min_coverage_fraction):\n  """"""Validates that shared_contigs spans a sufficient amount of ref_contigs.\n\n  Args:\n    ref_contigs: List of ContigInfo protos. All of the contigs from our\n      reference genome.\n    shared_contigs: The subset of ref_contigs that we found in common with\n      ref_contigs and all other genomics data sources.\n    min_coverage_fraction: The minimum fraction of basepairs of ref_contigs that\n      should be found among the shared_contigs.\n\n  Raises:\n    ValueError: If the fraction of covered bases is less than\n      min_coverage_fraction.\n  """"""\n\n  def format_contig_matches():\n    pieces = []\n    common_map = ranges.contigs_dict(shared_contigs)\n    for ref_contig in ref_contigs:\n      status = \'matched\' if ref_contig.name in common_map else \'IS MISSING\'\n      pieces.append(\'\\n""{}"" is {} bp and {}\'.format(ref_contig.name,\n                                                    ref_contig.n_bases, status))\n    return \', \'.join(pieces)\n\n  ref_bp = ranges.contigs_n_bases(ref_contigs)\n  common_bp = ranges.contigs_n_bases(shared_contigs)\n  coverage = common_bp / (1. * ref_bp)\n  if not shared_contigs or coverage < min_coverage_fraction:\n    raise ValueError(\'Reference contigs span {} bases but only {} bases \'\n                     \'({:.2%}) were found in common among our input files. \'\n                     \'Check that the sources were created on a common genome \'\n                     \'reference build. Contig matches were: {}. Here is a \'\n                     \'useful article about different human genome reference \'\n                     \'builds:\\n\'\n                     \'https://gatkforums.broadinstitute.org/gatk/discussion/\'\n                     \'11010/human-genome-reference-builds-grch38-hg38-b37-hg19\'\n                     \'\\nPlease make sure the --ref input matches the build \'\n                     \'used for the input in --reads.\'.format(\n                         ref_bp, common_bp, coverage, format_contig_matches()))\n\n\ndef build_calling_regions(contigs, regions_to_include, regions_to_exclude):\n  """"""Builds a RangeSet containing the regions we should call variants in.\n\n  This function intersects the Ranges spanning all of the contigs with those\n  from regions_to_include, if not empty, and removes all of the regions in\n  regions_to_exclude.\n\n  Args:\n    contigs: Sequence of ContigInfo protos. Used to determine the initial ranges\n      to process (i.e., all bases of these contigs).\n    regions_to_include: RangeSet or iterable that can be converted to a\n      RangeSet.\n    regions_to_exclude: RangeSet or iterable that can be converted to a\n      RangeSet.\n\n  Returns:\n    A RangeSet.\n  """"""\n  # Initially we are going to call everything in the reference.\n  regions = ranges.RangeSet.from_contigs(contigs)\n\n  # If we provided a regions to include, intersect it with all of the regions,\n  # producing a common set of regions between the reference and the provided\n  # calling regions.\n  contig_dict = ranges.contigs_dict(contigs)\n  if regions_to_include:\n    regions = regions.intersection(\n        ranges.RangeSet.from_regions(regions_to_include, contig_dict))\n\n  # If we provided regions to exclude, intersect those with the existing calling\n  # regions to further refine our set of contigs to process.\n  if regions_to_exclude:\n    # exclude_regions mutates regions.\n    regions.exclude_regions(\n        ranges.RangeSet.from_regions(regions_to_exclude, contig_dict))\n\n  return regions\n\n\ndef regions_to_process(contigs,\n                       partition_size,\n                       calling_regions=None,\n                       task_id=None,\n                       num_shards=None):\n  """"""Determines the regions to process and partitions them into pieces.\n\n  This function divides the genomes into regions we should process by\n  intersecting the Ranges spanning all of the contigs with those from\n  calling_regions, if provided. These intersected regions are then partitioned\n  into pieces no bigger than partition_size bp in length.\n\n  By construction we ensure that the regions are in genomic order, first w.r.t.\n  the contigs and then within each contig by start and end of each region.\n\n  This function can further subdivide these regions into a subset appropriate\n  for a single task (task_id) among N tasks (num_shards) to process. The\n  function ensures that:\n\n    set(all_regions) = union(regions(task_0), ..., regions(task_n))\n\n  when called with task_ids 0 ... N for num_shards = N.\n\n  Args:\n    contigs: Sequence of ContigInfo protos. Used to determine the initial ranges\n      to process (i.e., all bases of these contigs) and the order of returned\n      ranges.\n    partition_size: The maximum size to make any region when partitioning.\n    calling_regions: None or RangeSet. If provided, we will intersect the\n      regions to process so that only those that overlap a region in this set\n      are included.\n    task_id: int >= 0 or None. The task_id of this job, which will be used to\n      subdivide the total set of regions to process into just those that should\n      be processed by this job. Must be < num_shards.\n    num_shards: int >= 0 or None. The number of shards (i.e., the total number\n      of tasks) we are running in parallel. Together with task_id determines the\n      subset of regions we want to process.\n\n  Returns:\n    An iterable of nucleus.genomics.v1.Range objects.\n\n  Raises:\n    ValueError: if task_id and num_shards are bad or inconsistent.\n  """"""\n  if (task_id is None) != (num_shards is None):\n    raise ValueError(\'Both task_id and num_shards must be present if either is\',\n                     task_id, num_shards)\n  if num_shards:\n    if num_shards < 0:\n      raise ValueError(\'num_shards={} must be >= 0\'.format(num_shards))\n    if task_id < 0 or task_id >= num_shards:\n      raise ValueError(\'task_id={} should be >= 0 and < num_shards={}\'.format(\n          task_id, num_shards))\n\n  regions = ranges.RangeSet.from_contigs(contigs)\n  if calling_regions:\n    regions = regions.intersection(calling_regions)\n  partitioned = regions.partition(partition_size)\n\n  if num_shards:\n    return (r for i, r in enumerate(partitioned) if i % num_shards == task_id)\n  else:\n    return partitioned\n\n\n# ---------------------------------------------------------------------------\n# Region processor\n# ---------------------------------------------------------------------------\n\n\ndef read_confident_regions(options):\n  if options.confident_regions_filename:\n    return ranges.RangeSet.from_bed(options.confident_regions_filename)\n  else:\n    return None\n\n\ndef filter_candidates(candidates, select_variant_types):\n  """"""Yields the candidate variants whose type is one of select_variant_types.\n\n  This function iterates through candidates and yield each candidate in order\n  if it satisfies any of the type constraints implied by select_variant_types.\n  For example, if select_variant_types = [\'snps\'] this function will yield\n  candidates that are bi-allelic SNPs only. Multiple select types are treated\n  as OR\'d together, so [\'snps\', \'indels\'] yields candidates that are bi-allelic\n  SNPs or indels.\n\n  Args:\n    candidates: Iterable of Variant protos. The candidates we want to select\n      from.\n    select_variant_types: List of str. The names of the variant type selectors\n      we want to use to keep/remove variants. Each string must be part of\n      _VARIANT_TYPE_SELECTORS or an error will be raised.\n\n  Raises:\n    ValueError: if any str in select_variant_types isn\'t present in\n      _VARIANT_TYPE_SELECTORS.\n\n  Yields:\n    Candidates in order.\n  """"""\n  if not all(s in _VARIANT_TYPE_SELECTORS for s in select_variant_types):\n    raise ValueError(\'Unexpected select variant type\', select_variant_types)\n\n  for candidate in candidates:\n    v = candidate.variant\n    for select_type in select_variant_types:\n      selector = _VARIANT_TYPE_SELECTORS[select_type]\n      if selector(v):\n        yield candidate\n        break\n\n\nclass RegionProcessor(object):\n  """"""Creates DeepVariant example protos for a single region on the genome.\n\n  This class helps us to run the very sensitive caller, pileup image creator,\n  and variant labeler operations on a single region in parallel across many\n  regions using the PoolExecutor API. In order to do this we need separate three\n  key operations:\n\n  (1) Collect all of the info needed to create our resources (e.g., ref reader)\n      at construction. We cannot actually initialize those resources in the\n      constructor, though, since we actually want different resources in each\n      worker process/thread. I.e., we need lazy resource initialization.\n\n  (2) Actually initialize these resources *after* the worker has been forked\n      in our process pool. This gives us a fresh resource to use in each\n      separate process.\n\n  (3) Process the region to find candidate variants and process those into our\n      tf.Example protos.\n  """"""\n\n  def __init__(self, options):\n    """"""Creates a new RegionProcess.\n\n    Args:\n      options: deepvariant.DeepVariantOptions proto used to specify our\n        resources for calling (e.g., reference_filename).\n    """"""\n    self.options = options\n    self.initialized = False\n    self.ref_reader = None\n    self.sam_readers = None\n    self.in_memory_sam_reader = None\n    self.realigner = None\n    self.pic = None\n    self.labeler = None\n    self.variant_caller = None\n\n  def _make_allele_counter_for_region(self, region):\n    return allelecounter.AlleleCounter(self.ref_reader.c_reader, region,\n                                       self.options.allele_counter_options)\n\n  def _encode_tensor(self, image_tensor):\n    return image_tensor.tostring(), image_tensor.shape, \'raw\'\n\n  def _make_sam_readers(self):\n    """"""Creates a list of SamReaders from self.options.reads_filenames.""""""\n    logging_with_options(\n        self.options,\n        \'Starting from v0.9.0, --use_ref_for_cram is default to true. \'\n        \'If you are using CRAM input, note that we will decode CRAM \'\n        \'using the reference you passed in with --ref\')\n    readers = []\n    for reads_filename in self.options.reads_filenames:\n      readers.append(\n          sam.SamReader(\n              reads_filename,\n              ref_path=FLAGS.ref if FLAGS.use_ref_for_cram else None,\n              read_requirements=self.options.read_requirements,\n              parse_aux_fields=FLAGS.parse_sam_aux_fields,\n              hts_block_size=FLAGS.hts_block_size,\n              downsample_fraction=self.options.downsample_fraction,\n              random_seed=self.options.random_seed,\n              use_original_base_quality_scores=self.options\n              .use_original_quality_scores))\n    return readers\n\n  def _initialize(self):\n    """"""Initialize the resources needed for this work in the current env.""""""\n    if self.initialized:\n      raise ValueError(\'Cannot initialize this object twice\')\n\n    self.ref_reader = fasta.IndexedFastaReader(self.options.reference_filename)\n    self.sam_readers = self._make_sam_readers()\n    self.in_memory_sam_reader = sam.InMemorySamReader([])\n\n    if self.options.realigner_enabled:\n      input_bam_header = sam.SamReader(self.options.reads_filenames[0]).header\n      self.realigner = realigner.Realigner(\n          self.options.realigner_options,\n          self.ref_reader,\n          shared_header=input_bam_header)\n    self.pic = pileup_image.PileupImageCreator(\n        ref_reader=self.ref_reader,\n        sam_reader=self.in_memory_sam_reader,\n        options=self.options.pic_options)\n\n    if in_training_mode(self.options):\n      self.labeler = self._make_labeler_from_options()\n\n    self.variant_caller = self._make_variant_caller_from_options()\n    self.initialized = True\n\n  def _make_labeler_from_options(self):\n    """"""Creates the labeler from options.""""""\n    truth_vcf_reader = vcf.VcfReader(\n        self.options.truth_variants_filename,\n        excluded_format_fields=[\'GL\', \'GQ\', \'PL\'])\n    confident_regions = read_confident_regions(self.options)\n\n    if (self.options.variant_caller ==\n        deepvariant_pb2.DeepVariantOptions.VCF_CANDIDATE_IMPORTER):\n      logging.info(\'For --variant_caller=vcf_candidate_importer, we \'\n                   \'default the labeler_algorithm to positional_labler.\')\n      return positional_labeler.PositionalVariantLabeler(\n          truth_vcf_reader=truth_vcf_reader,\n          confident_regions=confident_regions)\n\n    if (self.options.labeler_algorithm ==\n        deepvariant_pb2.DeepVariantOptions.POSITIONAL_LABELER):\n      return positional_labeler.PositionalVariantLabeler(\n          truth_vcf_reader=truth_vcf_reader,\n          confident_regions=confident_regions)\n    elif (self.options.labeler_algorithm ==\n          deepvariant_pb2.DeepVariantOptions.HAPLOTYPE_LABELER):\n      return haplotype_labeler.HaplotypeLabeler(\n          truth_vcf_reader=truth_vcf_reader,\n          ref_reader=self.ref_reader,\n          confident_regions=confident_regions)\n    elif (self.options.labeler_algorithm ==\n          deepvariant_pb2.DeepVariantOptions.CUSTOMIZED_CLASSES_LABELER):\n      if (not FLAGS.customized_classes_labeler_classes_list or\n          not FLAGS.customized_classes_labeler_info_field_name):\n        raise ValueError(\'For -labeler_algorithm=customized_classes_labeler, \'\n                         \'you need to set \'\n                         \'-customized_classes_labeler_classes_list and \'\n                         \'-customized_classes_labeler_info_field_name.\')\n      return customized_classes_labeler.CustomizedClassesVariantLabeler(\n          truth_vcf_reader=truth_vcf_reader,\n          confident_regions=confident_regions,\n          classes_list=FLAGS.customized_classes_labeler_classes_list,\n          info_field_name=FLAGS.customized_classes_labeler_info_field_name)\n    else:\n      raise ValueError(\'Unexpected labeler_algorithm\',\n                       self.options.labeler_algorithm)\n\n  def _make_variant_caller_from_options(self):\n    """"""Creates the variant_caller from options.""""""\n    if (self.options.variant_caller ==\n        deepvariant_pb2.DeepVariantOptions.VCF_CANDIDATE_IMPORTER):\n      if in_training_mode(self.options):\n        candidates_vcf = self.options.truth_variants_filename\n      else:\n        candidates_vcf = self.options.proposed_variants_filename\n      return vcf_candidate_importer.VcfCandidateImporter(\n          self.options.variant_caller_options, candidates_vcf)\n    elif (self.options.variant_caller ==\n          deepvariant_pb2.DeepVariantOptions.VERY_SENSITIVE_CALLER):\n      return very_sensitive_caller.VerySensitiveCaller(\n          self.options.variant_caller_options)\n    else:\n      raise ValueError(\'Unexpected variant_caller\', self.options.variant_caller)\n\n  def process(self, region):\n    """"""Finds candidates and creates corresponding examples in a region.\n\n    Args:\n      region: A nucleus.genomics.v1.Range proto. Specifies the region on the\n        genome we should process.\n\n    Returns:\n      Three values. First is a list of the found candidates, which are\n      deepvariant.DeepVariantCall objects. The second value is a list of filled\n      in tf.Example protos. For example, these will include the candidate\n      variant, the pileup image, and, if in training mode, the truth variants\n      and labels needed for training. The third value is a list of\n      nucleus.genomics.v1.Variant protos containing gVCF information for all\n      reference sites, if gvcf generation is enabled, otherwise returns [].\n    """"""\n    region_timer = timer.TimerStart()\n\n    # Print some basic information about what we are doing.\n    if not self.initialized:\n      self._initialize()\n\n    self.in_memory_sam_reader.replace_reads(self.region_reads(region))\n    candidates, gvcfs = self.candidates_in_region(region)\n\n    if self.options.select_variant_types:\n      candidates = list(\n          filter_candidates(candidates, self.options.select_variant_types))\n\n    # pylint: disable=g-complex-comprehension\n    if in_training_mode(self.options):\n      examples = [\n          self.add_label_to_example(example, label)\n          for candidate, label in self.label_candidates(candidates, region)\n          for example in self.create_pileup_examples(candidate)\n      ]\n    else:\n      examples = [\n          example for candidate in candidates\n          for example in self.create_pileup_examples(candidate)\n      ]\n    # pylint: enable=g-complex-comprehension\n    logging.vlog(2, \'Found %s candidates in %s [%d bp] [%0.2fs elapsed]\',\n                 len(examples), ranges.to_literal(region),\n                 ranges.length(region), region_timer.Stop())\n    return candidates, examples, gvcfs\n\n  def region_reads(self, region):\n    """"""Update in_memory_sam_reader with read alignments overlapping the region.\n\n    If self.realigner is set, uses realigned reads, otherwise original reads\n    are returned.\n\n    Args:\n      region: A nucleus.genomics.v1.Range object specifying the region we want\n        to realign reads.\n\n    Returns:\n      [genomics.deepvariant.core.genomics.Read], reads overlapping the region.\n    """"""\n    reads = []\n    if self.sam_readers is not None:\n      for sam_reader_index, sam_reader in enumerate(self.sam_readers):\n        try:\n          reads.extend(sam_reader.query(region))\n        except ValueError as err:\n          error_message = str(err)\n          if error_message.startswith(\'Data loss:\'):\n            raise ValueError(\n                error_message + \'\\nFailed to parse BAM/CRAM file. \'\n                \'This is often caused by:\\n\'\n                \'(1) When using a CRAM file, and setting \'\n                \'--use_ref_for_cram to false (which means you want \'\n                \'to use the embedded ref instead of a ref file), \'\n                \'this error could be because of inability to find \'\n                \'the embedded ref file.\\n\'\n                \'(2) Your BAM/CRAM file could be corrupted. Please \'\n                \'check its md5.\\n\'\n                \'If you cannot find out the reason why this error \'\n                \'is occurring, please report to \'\n                \'https://github.com/google/deepvariant/issues\')\n          elif error_message.startswith(\'Not found: Unknown reference_name \'):\n            raise ValueError(\'{}\\nThe region {} does not exist in {}.\'.format(\n                error_message, ranges.to_literal(region),\n                self.options.reads_filenames[sam_reader_index]))\n          else:\n            # By default, raise the ValueError as is for now.\n            raise err\n\n    if self.options.max_reads_per_partition > 0:\n      random_for_region = np.random.RandomState(self.options.random_seed)\n      reads = utils.reservoir_sample(reads,\n                                     self.options.max_reads_per_partition,\n                                     random_for_region)\n    reads = list(reads)\n    if self.realigner:\n      _, reads = self.realigner.realign_reads(reads, region)\n    return reads\n\n  def candidates_in_region(self, region):\n    """"""Finds candidate DeepVariantCall protos in region.\n\n    Args:\n      region: A nucleus.genomics.v1.Range object specifying the region we want\n        to get candidates for.\n\n    Returns:\n      A 2-tuple. The first value is a list of deepvariant_pb2.DeepVariantCalls\n      objects, in coordidate order. The second value is a list of\n      nucleus.genomics.v1.Variant protos containing gVCF information for all\n      reference sites, if gvcf generation is enabled, otherwise returns [].\n    """"""\n    reads = self.in_memory_sam_reader.query(region)\n    if not reads and not gvcf_output_enabled(self.options):\n      # If we are generating gVCF output we cannot safely abort early here as\n      # we need to return the gVCF records calculated by the caller below.\n      return [], []\n\n    allele_counter = self._make_allele_counter_for_region(region)\n    for read in reads:\n      allele_counter.add(read, self.options.variant_caller_options.sample_name)\n\n    candidates, gvcfs = self.variant_caller.calls_and_gvcfs(\n        allele_counter, gvcf_output_enabled(self.options))\n    return candidates, gvcfs\n\n  def create_pileup_examples(self, dv_call):\n    """"""Creates a tf.Example for DeepVariantCall.\n\n    This function calls PileupImageCreator.create_pileup_images on dv_call to\n    get raw image tensors for each alt_allele option (see docs for details).\n    These tensors are encoded as pngs, and all of the key information is encoded\n    as a tf.Example via a call to tf_utils.make_example.\n\n    Args:\n      dv_call: A DeepVariantCall.\n\n    Returns:\n      A list of tf.Example protos.\n    """"""\n    pileup_images = self.pic.create_pileup_images(dv_call)\n    if pileup_images is None:\n      # We cannot build a PileupImage for dv_call, issue a warning.\n      logging.warning(\'Could not create PileupImage for candidate at %s:%s\',\n                      dv_call.variant.reference_name, dv_call.variant.start)\n      return []\n\n    examples = []\n    for alt_alleles, image_tensor in pileup_images:\n      encoded_tensor, shape, tensor_format = self._encode_tensor(image_tensor)\n      examples.append(\n          tf_utils.make_example(\n              dv_call.variant,\n              alt_alleles,\n              encoded_tensor,\n              shape=shape,\n              image_format=tensor_format,\n              sequencing_type=self.options.pic_options.sequencing_type))\n    return examples\n\n  def label_candidates(self, candidates, region):\n    """"""Gets label information for each candidate.\n\n    Args:\n      candidates: list[DeepVariantCalls]: The list of candidate variant calls we\n        want to label.\n      region: A nucleus.genomics.v1.Range object specifying the region we want\n        to get candidates for.\n\n    Yields:\n      Tuples of (candidate, label_variants.Label objects) for each candidate in\n      candidates that could be assigned a label. Candidates that couldn\'t be\n      labeled will not be returned.\n    """"""\n    # Get our list of labels for each candidate variant.\n    labels = self.labeler.label_variants(\n        [candidate.variant for candidate in candidates], region)\n\n    # Remove any candidates we couldn\'t label, yielding candidate, label pairs.\n    for candidate, label in zip(candidates, labels):\n      if label.is_confident:\n        yield candidate, label\n\n  def add_label_to_example(self, example, label):\n    """"""Adds label information about the assigned label to our example.\n\n    Args:\n      example: A tf.Example proto. We will write truth_variant and label into\n        this proto.\n      label: A variant_labeler.Label object containing the labeling information\n        to add to our example.\n\n    Returns:\n      The example proto with label fields added.\n\n    Raises:\n      ValueError: if label isn\'t confident.\n    """"""\n    if not label.is_confident:\n      raise ValueError(\'Cannot add a non-confident label to an example\',\n                       example, label)\n    alt_alleles_indices = tf_utils.example_alt_alleles_indices(example)\n\n    tf_utils.example_set_variant(example, label.variant)\n\n    # Set the label of the example to the # alts given our alt_alleles_indices.\n    tf_utils.example_set_label(example,\n                               label.label_for_alt_alleles(alt_alleles_indices))\n    return example\n\n\ndef processing_regions_from_options(options):\n  """"""Computes the calling regions from our options.\n\n  This function does all of the work needed to read our input files and region\n  specifications to determine the list of regions we should generate examples\n  over. It also computes the confident regions needed to label variants.\n\n  Args:\n    options: deepvariant.DeepVariantOptions proto containing information about\n      our input data sources.\n\n  Raises:\n    ValueError: if the regions to call is empty.\n\n  Returns:\n    Two values. The first is a list of nucleus.genomics.v1.Range protos of the\n    regions we should process. The second is a RangeSet containing the confident\n    regions for labeling, or None if we are running in training mode.\n  """"""\n  ref_contigs = fasta.IndexedFastaReader(\n      options.reference_filename).header.contigs\n\n  # Add in confident regions and vcf_contigs if in training mode.\n  vcf_contigs = None\n  if in_training_mode(options):\n    vcf_contigs = vcf.VcfReader(options.truth_variants_filename).header.contigs\n\n  all_sam_contigs = [\n      sam.SamReader(reads_file).header.contigs\n      for reads_file in options.reads_filenames\n  ]\n  sam_contigs = common_contigs(only_true(*all_sam_contigs))\n\n  contigs = _ensure_consistent_contigs(ref_contigs, sam_contigs, vcf_contigs,\n                                       options.exclude_contigs,\n                                       options.min_shared_contigs_basepairs)\n  logging_with_options(options,\n                       \'Common contigs are %s\' % [c.name for c in contigs])\n  calling_regions = build_calling_regions(ref_contigs, options.calling_regions,\n                                          options.exclude_calling_regions)\n  if not calling_regions:\n    raise ValueError(\'The regions to call is empty. Check your --regions and \'\n                     \'--exclude_regions flags to make sure they are not \'\n                     \'resulting in set of empty region to process. This also \'\n                     \'happens if you use ""chr20"" for a BAM where contig names \'\n                     \'don\\\'t have ""chr""s (or vice versa).\')\n  regions = regions_to_process(\n      contigs=contigs,\n      partition_size=options.allele_counter_options.partition_size,\n      calling_regions=calling_regions,\n      task_id=options.task_id,\n      num_shards=options.num_shards)\n\n  return regions\n\n\n# redacted\nclass OutputsWriter(object):\n  """"""Manages all of the outputs of make_examples in a single place.""""""\n\n  def __init__(self, options):\n    self._writers = {k: None for k in [\'candidates\', \'examples\', \'gvcfs\']}\n\n    if options.candidates_filename:\n      self._add_writer(\'candidates\',\n                       tfrecord.Writer(options.candidates_filename))\n\n    if options.examples_filename:\n      self._add_writer(\'examples\', tfrecord.Writer(options.examples_filename))\n\n    if options.gvcf_filename:\n      self._add_writer(\'gvcfs\', tfrecord.Writer(options.gvcf_filename))\n\n  def write_examples(self, *examples):\n    self._write(\'examples\', *examples)\n\n  def write_gvcfs(self, *gvcfs):\n    self._write(\'gvcfs\', *gvcfs)\n\n  def write_candidates(self, *candidates):\n    self._write(\'candidates\', *candidates)\n\n  def _add_writer(self, name, writer):\n    if name not in self._writers:\n      raise ValueError(\n          \'Expected writer {} to have a None binding in writers.\'.format(name))\n    if self._writers[name] is not None:\n      raise ValueError(\'Expected writer {} to be bound to None in writers but \'\n                       \'saw {} instead\'.format(name, self._writers[name]))\n    self._writers[name] = writer\n\n  def __enter__(self):\n    """"""API function to support with syntax.""""""\n    for writer in self._writers.values():\n      if writer is not None:\n        writer.__enter__()\n    return self\n\n  def __exit__(self, exception_type, exception_value, traceback):\n    for writer in self._writers.values():\n      if writer is not None:\n        writer.__exit__(exception_type, exception_value, traceback)\n\n  def _write(self, writer_name, *protos):\n    writer = self._writers[writer_name]\n    if writer:\n      for proto in protos:\n        writer.write(proto)\n\n\ndef make_examples_runner(options):\n  """"""Runs examples creation stage of deepvariant.""""""\n  resource_monitor = resources.ResourceMonitor().start()\n  logging_with_options(options, \'Preparing inputs\')\n  regions = processing_regions_from_options(options)\n\n  # Create a processor to create candidates and examples for each region.\n  region_processor = RegionProcessor(options)\n\n  logging_with_options(options,\n                       \'Writing examples to %s\' % options.examples_filename)\n  if options.candidates_filename:\n    logging_with_options(\n        options, \'Writing candidates to %s\' % options.candidates_filename)\n  if options.gvcf_filename:\n    logging_with_options(options,\n                         \'Writing gvcf records to %s\' % options.gvcf_filename)\n\n  n_regions, n_candidates, n_examples = 0, 0, 0\n  last_reported = 0\n  with OutputsWriter(options) as writer:\n    running_timer = timer.TimerStart()\n    for region in regions:\n      candidates, examples, gvcfs = region_processor.process(region)\n      n_candidates += len(candidates)\n      n_examples += len(examples)\n      n_regions += 1\n\n      writer.write_candidates(*candidates)\n\n      # If we have any gvcf records, write them out. This if also serves to\n      # protect us from trying to write to the gvcfs output of writer when gvcf\n      # generation is turned off. In that case, gvcfs will always be empty and\n      # we\'ll never execute the write.\n      if gvcfs:\n        writer.write_gvcfs(*gvcfs)\n      writer.write_examples(*examples)\n\n      # Output timing for every N candidates.\n      # redacted\n      if (int(n_candidates / FLAGS.logging_every_n_candidates) > last_reported\n          or n_regions == 1):\n        last_reported = int(n_candidates / FLAGS.logging_every_n_candidates)\n        logging_with_options(\n            options, \'%s candidates (%s examples) [%0.2fs elapsed]\' %\n            (n_candidates, n_examples, running_timer.Stop()))\n        running_timer = timer.TimerStart()\n  # Construct and then write out our MakeExamplesRunInfo proto.\n  if options.run_info_filename:\n    run_info = deepvariant_pb2.MakeExamplesRunInfo(\n        options=options, resource_metrics=resource_monitor.metrics())\n    if in_training_mode(options):\n      if region_processor.labeler.metrics is not None:\n        run_info.labeling_metrics.CopyFrom(region_processor.labeler.metrics)\n      else:\n        logging.warning(\n            \'Labeling metrics requested but the selected labeling \'\n            \'algorithm %s does not collect metrics; skipping.\',\n            options.labeler_algorithm)\n    logging_with_options(\n        options,\n        \'Writing MakeExamplesRunInfo to %s\' % options.run_info_filename)\n    write_make_examples_run_info(run_info, path=options.run_info_filename)\n\n  logging_with_options(options, \'Found %s candidate variants\' % n_candidates)\n  logging_with_options(options, \'Created %s examples\' % n_examples)\n\n\ndef main(argv=()):\n  with errors.clean_commandline_error_exit():\n    if len(argv) > 1:\n      errors.log_and_raise(\n          \'Command line parsing failure: make_examples does not accept \'\n          \'positional arguments but some are present on the command line: \'\n          \'""{}"".\'.format(str(argv)), errors.CommandLineError)\n    del argv  # Unused.\n\n    proto_utils.uses_fast_cpp_protos_or_die()\n\n    logging_level.set_from_flag()\n    hts_verbose.set(hts_verbose.htsLogLevel[FLAGS.hts_logging_level])\n\n    # Set up options; may do I/O.\n    options = default_options(add_flags=True, flags_obj=FLAGS)\n\n    # Check arguments that apply to any mode.\n    if not options.reference_filename:\n      errors.log_and_raise(\'ref argument is required.\', errors.CommandLineError)\n    if not options.reads_filenames:\n      errors.log_and_raise(\'reads argument is required.\',\n                           errors.CommandLineError)\n    if not options.examples_filename:\n      errors.log_and_raise(\'examples argument is required.\',\n                           errors.CommandLineError)\n    if options.n_cores != 1:\n      errors.log_and_raise(\n          \'Currently only supports n_cores == 1 but got {}.\'.format(\n              options.n_cores), errors.CommandLineError)\n\n    # Check for argument issues specific to different modes.\n    if in_training_mode(options):\n      if not options.truth_variants_filename:\n        errors.log_and_raise(\n            \'truth_variants is required when in training mode.\',\n            errors.CommandLineError)\n      if not options.confident_regions_filename:\n        if options.variant_caller == \\\n            deepvariant_pb2.DeepVariantOptions.VCF_CANDIDATE_IMPORTER:\n          logging.info(\'Note: --confident_regions is optional with \'\n                       \'vcf_candidate_importer. \'\n                       \'You did not specify --confident_regions, which means \'\n                       \'examples will be generated for the whole region.\')\n        else:\n          errors.log_and_raise(\n              \'confident_regions is required when in training mode.\',\n              errors.CommandLineError)\n      if options.gvcf_filename:\n        errors.log_and_raise(\'gvcf is not allowed in training mode.\',\n                             errors.CommandLineError)\n      if (options.variant_caller == \\\n          deepvariant_pb2.DeepVariantOptions.VCF_CANDIDATE_IMPORTER and\n          options.proposed_variants_filename):\n        errors.log_and_raise(\n            \'--proposed_variants should not be used with \'\n            \'vcf_candidate_importer in training mode. \'\n            \'Use --truth_variants to pass in the candidates \'\n            \'with correct labels for training.\', errors.CommandLineError)\n    else:\n      # Check for argument issues specific to calling mode.\n      if options.truth_variants_filename:\n        errors.log_and_raise(\'Do not specify --truth_variants in calling mode.\',\n                             errors.CommandLineError)\n      if options.variant_caller_options.sample_name == _UNKNOWN_SAMPLE:\n        errors.log_and_raise(\'sample_name must be specified in calling mode.\',\n                             errors.CommandLineError)\n      if options.variant_caller_options.gq_resolution < 1:\n        errors.log_and_raise(\'gq_resolution must be a non-negative integer.\',\n                             errors.CommandLineError)\n      if options.variant_caller == \\\n          deepvariant_pb2.DeepVariantOptions.VCF_CANDIDATE_IMPORTER:\n        if not options.proposed_variants_filename:\n          errors.log_and_raise(\n              \'--proposed_variants is required with vcf_candidate_importer in \'\n              \'calling mode.\', errors.CommandLineError)\n\n    # Run!\n    make_examples_runner(options)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flags_as_required([\n      \'examples\',\n      \'mode\',\n      \'reads\',\n      \'ref\',\n  ])\n  app.run(main)\n'"
deepvariant/make_examples_test.py,7,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant.make_examples.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport copy\nimport errno\nimport platform\nimport sys\n\n\n\nfrom absl import flags\nfrom absl import logging\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport enum\nimport mock\nimport six\n\nfrom tensorflow.python.platform import gfile\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.protos import reads_pb2\nfrom third_party.nucleus.protos import reference_pb2\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\nfrom third_party.nucleus.util import variant_utils\nfrom third_party.nucleus.util import variantcall_utils\nfrom third_party.nucleus.util import vcf_constants\nfrom deepvariant import dv_constants\nfrom deepvariant import make_examples\nfrom deepvariant import testdata\nfrom deepvariant import tf_utils\nfrom deepvariant.labeler import variant_labeler\nfrom deepvariant.protos import deepvariant_pb2\nfrom deepvariant.protos import realigner_pb2\nfrom deepvariant.testing import flagsaver\n\nFLAGS = flags.FLAGS\n\n# Dictionary mapping keys to decoders for decode_example function.\n_EXAMPLE_DECODERS = {\n    \'locus\': tf_utils.example_locus,\n    \'alt_allele_indices/encoded\': tf_utils.example_alt_alleles_indices,\n    \'image/encoded\': tf_utils.example_encoded_image,\n    \'variant/encoded\': tf_utils.example_variant,\n    \'variant_type\': tf_utils.example_variant_type,\n    \'label\': tf_utils.example_label,\n    \'image/format\': tf_utils.example_image_format,\n    \'image/shape\': tf_utils.example_image_shape,\n    \'sequencing_type\': tf_utils.example_sequencing_type,\n}\n\n\ndef decode_example(example):\n  """"""Decodes a tf.Example from DeepVariant into a dict of Pythonic structures.\n\n  Args:\n    example: tf.Example proto. The example to make into a dictionary.\n\n  Returns:\n    A python dictionary with key/value pairs for each of the fields of example,\n    with each value decoded as needed into Python structures like protos, list,\n    etc.\n\n  Raises:\n    KeyError: If example contains a feature without a known decoder.\n  """"""\n  as_dict = {}\n  for key in example.features.feature:\n    if key not in _EXAMPLE_DECODERS:\n      raise KeyError(\'Unexpected example key\', key)\n    as_dict[key] = _EXAMPLE_DECODERS[key](example)\n  return as_dict\n\n\ndef setUpModule():\n  testdata.init()\n\n\ndef _make_contigs(specs):\n  """"""Makes ContigInfo protos from specs.\n\n  Args:\n    specs: A list of 2- or 3-tuples. All tuples should be of the same length. If\n      2-element, these should be the name and length in basepairs of each\n      contig, and their pos_in_fasta will be set to their index in the list. If\n      the 3-element, the tuple should contain name, length, and pos_in_fasta.\n\n  Returns:\n    A list of ContigInfo protos, one for each spec in specs.\n  """"""\n  if specs and len(specs[0]) == 3:\n    return [\n        reference_pb2.ContigInfo(name=name, n_bases=length, pos_in_fasta=i)\n        for name, length, i in specs\n    ]\n  else:\n    return [\n        reference_pb2.ContigInfo(name=name, n_bases=length, pos_in_fasta=i)\n        for i, (name, length) in enumerate(specs)\n    ]\n\n\ndef _from_literals_list(literals, contig_map=None):\n  """"""Makes a list of Range objects from literals.""""""\n  return ranges.parse_literals(literals, contig_map)\n\n\ndef _from_literals(literals, contig_map=None):\n  """"""Makes a RangeSet of intervals from literals.""""""\n  return ranges.RangeSet.from_regions(literals, contig_map)\n\n\ndef _sharded(basename, num_shards=None):\n  if num_shards:\n    return basename + \'@\' + str(num_shards)\n  else:\n    return basename\n\n\nclass TestConditions(enum.Enum):\n  """"""Enum capturing what the test condition we\'re using.""""""\n  USE_BAM = 1\n  USE_CRAM = 2\n  USE_MULTI_BAMS = 3\n\n\nclass MakeExamplesEnd2EndTest(parameterized.TestCase):\n\n  # Golden sets are created with learning/genomics/internal/create_golden.sh\n  @parameterized.parameters(\n      # All tests are run with fast_pass_aligner enabled. There are no\n      # golden sets version for ssw realigner.\n      dict(mode=\'calling\', num_shards=0),\n      dict(mode=\'calling\', num_shards=3),\n      dict(\n          mode=\'training\', num_shards=0, labeler_algorithm=\'haplotype_labeler\'),\n      dict(\n          mode=\'training\', num_shards=3, labeler_algorithm=\'haplotype_labeler\'),\n      dict(\n          mode=\'training\', num_shards=0,\n          labeler_algorithm=\'positional_labeler\'),\n      dict(\n          mode=\'training\', num_shards=3,\n          labeler_algorithm=\'positional_labeler\'),\n      # The following tests are for CRAM input:\n      dict(\n          mode=\'calling\', num_shards=0, test_condition=TestConditions.USE_CRAM),\n      dict(\n          mode=\'training\',\n          num_shards=0,\n          test_condition=TestConditions.USE_CRAM,\n          labeler_algorithm=\'haplotype_labeler\'),\n      # The following tests are for multiple BAM inputs:\n      dict(\n          mode=\'calling\',\n          num_shards=0,\n          test_condition=TestConditions.USE_MULTI_BAMS),\n      dict(\n          mode=\'training\',\n          num_shards=0,\n          test_condition=TestConditions.USE_MULTI_BAMS,\n          labeler_algorithm=\'haplotype_labeler\'),\n  )\n  @flagsaver.FlagSaver\n  def test_make_examples_end2end(self,\n                                 mode,\n                                 num_shards,\n                                 test_condition=TestConditions.USE_BAM,\n                                 labeler_algorithm=None,\n                                 use_fast_pass_aligner=True):\n    self.assertIn(mode, {\'calling\', \'training\'})\n    region = ranges.parse_literal(\'chr20:10,000,000-10,010,000\')\n    FLAGS.write_run_info = True\n    FLAGS.ref = testdata.CHR20_FASTA\n    if test_condition == TestConditions.USE_BAM:\n      FLAGS.reads = testdata.CHR20_BAM\n    elif test_condition == TestConditions.USE_CRAM:\n      FLAGS.reads = testdata.CHR20_CRAM\n    elif test_condition == TestConditions.USE_MULTI_BAMS:\n      FLAGS.reads = \',\'.join(\n          [testdata.CHR20_BAM_FIRST_HALF, testdata.CHR20_BAM_SECOND_HALF])\n\n    FLAGS.candidates = test_utils.test_tmpfile(\n        _sharded(\'vsc.tfrecord\', num_shards))\n    FLAGS.examples = test_utils.test_tmpfile(\n        _sharded(\'examples.tfrecord\', num_shards))\n    FLAGS.regions = [ranges.to_literal(region)]\n    FLAGS.partition_size = 1000\n    FLAGS.mode = mode\n    FLAGS.gvcf_gq_binsize = 5\n    FLAGS.use_fast_pass_aligner = use_fast_pass_aligner\n    if labeler_algorithm is not None:\n      FLAGS.labeler_algorithm = labeler_algorithm\n\n    if mode == \'calling\':\n      FLAGS.gvcf = test_utils.test_tmpfile(\n          _sharded(\'gvcf.tfrecord\', num_shards))\n    else:\n      FLAGS.truth_variants = testdata.TRUTH_VARIANTS_VCF\n      FLAGS.confident_regions = testdata.CONFIDENT_REGIONS_BED\n\n    for task_id in range(max(num_shards, 1)):\n      FLAGS.task = task_id\n      options = make_examples.default_options(add_flags=True)\n      make_examples.make_examples_runner(options)\n\n      # Check that our run_info proto contains the basic fields we\'d expect:\n      # (a) our options are written to the run_info.options field.\n      run_info = make_examples.read_make_examples_run_info(\n          options.run_info_filename)\n      self.assertEqual(run_info.options, options)\n      # (b) run_info.resource_metrics is present and contains our hostname.\n      self.assertTrue(run_info.HasField(\'resource_metrics\'))\n      self.assertEqual(run_info.resource_metrics.host_name, platform.node())\n\n    # Test that our candidates are reasonable, calling specific helper functions\n    # to check lots of properties of the output.\n    candidates = sorted(\n        tfrecord.read_tfrecords(\n            FLAGS.candidates, proto=deepvariant_pb2.DeepVariantCall),\n        key=lambda c: variant_utils.variant_range_tuple(c.variant))\n    self.verify_deepvariant_calls(candidates, options)\n    self.verify_variants([call.variant for call in candidates],\n                         region,\n                         options,\n                         is_gvcf=False)\n\n    # Verify that the variants in the examples are all good.\n    examples = self.verify_examples(\n        FLAGS.examples, region, options, verify_labels=mode == \'training\')\n    example_variants = [tf_utils.example_variant(ex) for ex in examples]\n    self.verify_variants(example_variants, region, options, is_gvcf=False)\n\n    # Verify the integrity of the examples and then check that they match our\n    # golden labeled examples. Note we expect the order for both training and\n    # calling modes to produce deterministic order because we fix the random\n    # seed.\n    if mode == \'calling\':\n      golden_file = _sharded(testdata.GOLDEN_CALLING_EXAMPLES, num_shards)\n    else:\n      golden_file = _sharded(testdata.GOLDEN_TRAINING_EXAMPLES, num_shards)\n    self.assertDeepVariantExamplesEqual(\n        examples, list(tfrecord.read_tfrecords(golden_file)))\n\n    if mode == \'calling\':\n      nist_reader = vcf.VcfReader(testdata.TRUTH_VARIANTS_VCF)\n      nist_variants = list(nist_reader.query(region))\n      self.verify_nist_concordance(example_variants, nist_variants)\n\n      # Check the quality of our generated gvcf file.\n      gvcfs = variant_utils.sorted_variants(\n          tfrecord.read_tfrecords(FLAGS.gvcf, proto=variants_pb2.Variant))\n      self.verify_variants(gvcfs, region, options, is_gvcf=True)\n      self.verify_contiguity(gvcfs, region)\n      gvcf_golden_file = _sharded(testdata.GOLDEN_POSTPROCESS_GVCF_INPUT,\n                                  num_shards)\n      expected_gvcfs = list(\n          tfrecord.read_tfrecords(gvcf_golden_file, proto=variants_pb2.Variant))\n      self.assertItemsEqual(gvcfs, expected_gvcfs)\n\n    if (mode == \'training\' and num_shards == 0 and\n        labeler_algorithm != \'positional_labeler\'):\n      # The positional labeler doesn\'t track metrics, so don\'t try to read them\n      # in when that\'s the mode.\n      self.assertEqual(\n          make_examples.read_make_examples_run_info(\n              testdata.GOLDEN_MAKE_EXAMPLES_RUN_INFO).labeling_metrics,\n          run_info.labeling_metrics)\n\n  @flagsaver.FlagSaver\n  def test_make_examples_end2end_failed_on_mismatched_multi_bam(self):\n    region = ranges.parse_literal(\'chr20:10,000,000-10,010,000\')\n\n    FLAGS.write_run_info = True\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = \',\'.join([testdata.CHR20_BAM, testdata.NOCHR20_BAM])\n    FLAGS.candidates = test_utils.test_tmpfile(\n        _sharded(\'mismatched_multi_bam.vsc.tfrecord\'))\n    FLAGS.examples = test_utils.test_tmpfile(\n        _sharded(\'mismatched_multi_bam.examples.tfrecord\'))\n    FLAGS.regions = [ranges.to_literal(region)]\n    FLAGS.partition_size = 1000\n    FLAGS.mode = \'calling\'\n    FLAGS.gvcf_gq_binsize = 5\n    options = make_examples.default_options(add_flags=True)\n    # This shows an example of what the error message looks like:\n    with six.assertRaisesRegex(\n        self, ValueError, \'Not found: Unknown reference_name \'\n        \'reference_name: ""chr20"" start: 9999999 end: 10000999\\n\'\n        \'The region chr20:10000000-10000999 does not exist in \'\n        \'.*HG002_NIST_150bp_downsampled_30x.chr20.10_10p1mb.bam.\'):\n      make_examples.make_examples_runner(options)\n\n  @flagsaver.FlagSaver\n  def test_make_examples_end2end_failed_on_cram(self):\n    region = ranges.parse_literal(\'chr20:10,000,000-10,010,000\')\n\n    FLAGS.use_ref_for_cram = False\n    FLAGS.write_run_info = True\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_CRAM\n    FLAGS.candidates = test_utils.test_tmpfile(_sharded(\'failed.vsc.tfrecord\'))\n    FLAGS.examples = test_utils.test_tmpfile(\n        _sharded(\'failed.examples.tfrecord\'))\n    FLAGS.regions = [ranges.to_literal(region)]\n    FLAGS.partition_size = 1000\n    FLAGS.mode = \'calling\'\n    FLAGS.gvcf_gq_binsize = 5\n    options = make_examples.default_options(add_flags=True)\n    with six.assertRaisesRegex(self, ValueError,\n                               \'Failed to parse BAM/CRAM file.\'):\n      make_examples.make_examples_runner(options)\n\n  # Golden sets are created with learning/genomics/internal/create_golden.sh\n  @flagsaver.FlagSaver\n  def test_make_examples_training_end2end_with_custom_pileup_image(self):\n    region = ranges.parse_literal(\'chr20:10,000,000-10,010,000\')\n    FLAGS.regions = [ranges.to_literal(region)]\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.candidates = test_utils.test_tmpfile(_sharded(\'vsc.tfrecord\'))\n    FLAGS.examples = test_utils.test_tmpfile(_sharded(\'examples.tfrecord\'))\n    FLAGS.partition_size = 1000\n    FLAGS.mode = \'training\'\n    FLAGS.gvcf_gq_binsize = 5\n    FLAGS.custom_pileup_image = True\n    FLAGS.truth_variants = testdata.TRUTH_VARIANTS_VCF\n    FLAGS.confident_regions = testdata.CONFIDENT_REGIONS_BED\n    options = make_examples.default_options(add_flags=True)\n    make_examples.make_examples_runner(options)\n    golden_file = _sharded(\n        testdata.CUSTOM_PILEUP_IMAGE_GOLDEN_TRAINING_EXAMPLES)\n    # Verify that the variants in the examples are all good.\n    examples = self.verify_examples(\n        FLAGS.examples, region, options, verify_labels=True)\n    self.assertDeepVariantExamplesEqual(\n        examples, list(tfrecord.read_tfrecords(golden_file)))\n    self.assertEqual(decode_example(examples[0])[\'image/shape\'], [100, 221, 7])\n\n  # Golden sets are created with learning/genomics/internal/create_golden.sh\n  @flagsaver.FlagSaver\n  def test_make_examples_training_end2end_with_customized_classes_labeler(self):\n    FLAGS.labeler_algorithm = \'customized_classes_labeler\'\n    FLAGS.customized_classes_labeler_classes_list = \'ref,class1,class2\'\n    FLAGS.customized_classes_labeler_info_field_name = \'type\'\n    region = ranges.parse_literal(\'chr20:10,000,000-10,004,000\')\n    FLAGS.regions = [ranges.to_literal(region)]\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.candidates = test_utils.test_tmpfile(_sharded(\'vsc.tfrecord\'))\n    FLAGS.examples = test_utils.test_tmpfile(_sharded(\'examples.tfrecord\'))\n    FLAGS.partition_size = 1000\n    FLAGS.mode = \'training\'\n    FLAGS.gvcf_gq_binsize = 5\n    FLAGS.truth_variants = testdata.TRUTH_VARIANTS_VCF_WITH_TYPES\n    FLAGS.confident_regions = testdata.CONFIDENT_REGIONS_BED\n    options = make_examples.default_options(add_flags=True)\n    make_examples.make_examples_runner(options)\n    golden_file = _sharded(testdata.CUSTOMIZED_CLASSES_GOLDEN_TRAINING_EXAMPLES)\n    # Verify that the variants in the examples are all good.\n    examples = self.verify_examples(\n        FLAGS.examples, region, options, verify_labels=True)\n    self.assertDeepVariantExamplesEqual(\n        examples, list(tfrecord.read_tfrecords(golden_file)))\n\n  # Golden sets are created with learning/genomics/internal/create_golden.sh\n  @parameterized.parameters(\n      dict(mode=\'calling\'),\n      dict(mode=\'training\'),\n  )\n  @flagsaver.FlagSaver\n  def test_make_examples_end2end_vcf_candidate_importer(self, mode):\n    FLAGS.variant_caller = \'vcf_candidate_importer\'\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.candidates = test_utils.test_tmpfile(\n        _sharded(\'vcf_candidate_importer.{}.tfrecord\'.format(mode)))\n    FLAGS.examples = test_utils.test_tmpfile(\n        _sharded(\'vcf_candidate_importer.examples.{}.tfrecord\'.format(mode)))\n    FLAGS.mode = mode\n\n    if mode == \'calling\':\n      golden_file = _sharded(\n          testdata.GOLDEN_VCF_CANDIDATE_IMPORTER_CALLING_EXAMPLES)\n      FLAGS.proposed_variants = testdata.VCF_CANDIDATE_IMPORTER_VARIANTS\n      # Adding the following flags to match how the testdata was created.\n      FLAGS.regions = \'chr20:59,777,000-60,000,000\'\n      FLAGS.realign_reads = False\n    else:\n      golden_file = _sharded(\n          testdata.GOLDEN_VCF_CANDIDATE_IMPORTER_TRAINING_EXAMPLES)\n      FLAGS.truth_variants = testdata.TRUTH_VARIANTS_VCF\n    options = make_examples.default_options(add_flags=True)\n    make_examples.make_examples_runner(options)\n    # Verify that the variants in the examples are all good.\n    examples = self.verify_examples(\n        FLAGS.examples, None, options, verify_labels=mode == \'training\')\n    self.assertDeepVariantExamplesEqual(\n        examples, list(tfrecord.read_tfrecords(golden_file)))\n    self.assertEqual(decode_example(examples[0])[\'image/shape\'], [100, 221, 6])\n\n  @flagsaver.FlagSaver\n  def test_make_examples_training_vcf_candidate_importer_regions(self):\n    """"""Confirms confident_regions is used in vcf_candidate_importer training.""""""\n\n    def _get_examples(use_confident_regions=False):\n      # `flag_name` can be either \'confident_regions\' or \'regions\'. Both should\n      # be used to constrain the set of candidates generated, and as a result\n      # generating the same examples.\n      bed_path = test_utils.test_tmpfile(\'vcf_candidate_importer.bed\')\n      with gfile.Open(bed_path, \'w\') as fout:\n        fout.write(\'\\t\'.join([\'chr20\', \'10000000\', \'10001000\']) + \'\\n\')\n      if use_confident_regions:\n        FLAGS.confident_regions = bed_path\n        FLAGS.regions = \'\'\n      else:\n        FLAGS.confident_regions = \'\'\n        FLAGS.regions = bed_path\n\n      FLAGS.examples = test_utils.test_tmpfile(\n          _sharded(\'vcf_candidate_importer.tfrecord\'))\n      FLAGS.mode = \'training\'\n      FLAGS.reads = testdata.CHR20_BAM\n      FLAGS.ref = testdata.CHR20_FASTA\n      FLAGS.truth_variants = testdata.TRUTH_VARIANTS_VCF\n      FLAGS.variant_caller = \'vcf_candidate_importer\'\n\n      options = make_examples.default_options(add_flags=True)\n      make_examples.make_examples_runner(options)\n      # Verify that the variants in the examples are all good.\n      examples = self.verify_examples(\n          FLAGS.examples, None, options, verify_labels=False)\n      return examples\n\n    examples_with_regions = _get_examples(use_confident_regions=False)\n    examples_with_confident_regions = _get_examples(use_confident_regions=True)\n    self.assertNotEmpty(examples_with_regions)\n    self.assertDeepVariantExamplesEqual(examples_with_regions,\n                                        examples_with_confident_regions)\n\n  @parameterized.parameters(\n      dict(select_types=None, expected_count=78),\n      dict(select_types=\'all\', expected_count=78),\n      dict(select_types=\'snps\', expected_count=62),\n      dict(select_types=\'indels\', expected_count=12),\n      dict(select_types=\'snps indels\', expected_count=74),\n      dict(select_types=\'multi-allelics\', expected_count=4),\n  )\n  @flagsaver.FlagSaver\n  def test_make_examples_with_variant_selection(self, select_types,\n                                                expected_count):\n    if select_types is not None:\n      FLAGS.select_variant_types = select_types\n    region = ranges.parse_literal(\'chr20:10,000,000-10,010,000\')\n    FLAGS.regions = [ranges.to_literal(region)]\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.candidates = test_utils.test_tmpfile(_sharded(\'vsc.tfrecord\'))\n    FLAGS.examples = test_utils.test_tmpfile(_sharded(\'examples.tfrecord\'))\n    FLAGS.partition_size = 1000\n    FLAGS.mode = \'calling\'\n\n    options = make_examples.default_options(add_flags=True)\n    make_examples.make_examples_runner(options)\n\n    candidates = list(tfrecord.read_tfrecords(FLAGS.candidates))\n    self.assertLen(candidates, expected_count)\n\n  def verify_nist_concordance(self, candidates, nist_variants):\n    # Tests that we call almost all of the real variants (according to NIST\'s\n    # Genome in a Bottle callset for NA12878) in our candidate callset.\n    # Tests that we don\'t have an enormous number of FP calls. We should have\n    # no more than 5x (arbitrary) more candidate calls than real calls. If we\n    # have more it\'s likely due to some major pipeline problem.\n    self.assertLess(len(candidates), 5 * len(nist_variants))\n    tp_count = 0\n    for nist_variant in nist_variants:\n      if self.assertVariantIsPresent(nist_variant, candidates):\n        tp_count = tp_count + 1\n\n    self.assertGreater(\n        tp_count / len(nist_variants), 0.983,\n        \'Recall must be greater than 0.983. TP={}, Truth variants={}\'.format(\n            tp_count, len(nist_variants)))\n\n  def assertDeepVariantExamplesEqual(self, actual, expected):\n    """"""Asserts that actual and expected tf.Examples from DeepVariant are equal.\n\n    Args:\n      actual: iterable of tf.Examples from DeepVariant. DeepVariant examples\n        that we want to check.\n      expected: iterable of tf.Examples. Expected results for actual.\n    """"""\n    self.assertEqual(len(actual), len(expected))\n    for i in range(len(actual)):\n      actual_example = decode_example(actual[i])\n      expected_example = decode_example(expected[i])\n      self.assertEqual(actual_example.keys(), expected_example.keys())\n      for key in actual_example:\n        self.assertEqual(actual_example[key], expected_example[key],\n                         \'Failed on %s\' % key)\n\n  def assertVariantIsPresent(self, to_find, variants):\n\n    def variant_key(v):\n      return (v.reference_bases, v.start, v.end)\n\n    # Finds a call in our actual call set for each NIST variant, asserting\n    # that we found exactly one.\n    matches = [\n        variant for variant in variants\n        if variant_key(to_find) == variant_key(variant)\n    ]\n    if not matches:\n      return False\n\n    # Verify that every alt allele appears in the call (but the call might)\n    # have more than just those calls.\n    for alt in to_find.alternate_bases:\n      if alt not in matches[0].alternate_bases:\n        return False\n\n    return True\n\n  def verify_variants(self, variants, region, options, is_gvcf):\n    # Verifies simple properties of the Variant protos in variants. For example,\n    # checks that the reference_name() is our expected chromosome. The flag\n    # is_gvcf determines how we check the VariantCall field of each variant,\n    # enforcing expectations for gVCF records if true or variant calls if false.\n    for variant in variants:\n      if region:\n        self.assertEqual(variant.reference_name, region.reference_name)\n        self.assertGreaterEqual(variant.start, region.start)\n        self.assertLessEqual(variant.start, region.end)\n      self.assertNotEqual(variant.reference_bases, \'\')\n      self.assertNotEmpty(variant.alternate_bases)\n      self.assertLen(variant.calls, 1)\n\n      call = variant_utils.only_call(variant)\n      self.assertEqual(call.call_set_name,\n                       options.variant_caller_options.sample_name)\n      if is_gvcf:\n        # GVCF records should have 0/0 or ./. (un-called) genotypes as they are\n        # reference sites, have genotype likelihoods and a GQ value.\n        self.assertIn(list(call.genotype), [[0, 0], [-1, -1]])\n        self.assertLen(call.genotype_likelihood, 3)\n        self.assertGreaterEqual(variantcall_utils.get_gq(call), 0)\n\n  def verify_contiguity(self, contiguous_variants, region):\n    """"""Verifies region is fully covered by gvcf records.""""""\n    # We expect that the intervals cover every base, so the first variant should\n    # be at our interval start and the last one should end at our interval end.\n    self.assertNotEmpty(contiguous_variants)\n    self.assertEqual(region.start, contiguous_variants[0].start)\n    self.assertEqual(region.end, contiguous_variants[-1].end)\n\n    # After this loop completes successfully we know that together the GVCF and\n    # Variants form a fully contiguous cover of our calling interval, as\n    # expected.\n    for v1, v2 in zip(contiguous_variants, contiguous_variants[1:]):\n      # Sequential variants should be contiguous, meaning that v2.start should\n      # be v1\'s end, as the end is exclusive and the start is inclusive.\n      if v1.start == v2.start and v1.end == v2.end:\n        # Skip duplicates here as we may have multi-allelic variants turning\n        # into multiple bi-allelic variants at the same site.\n        continue\n      # We expect to immediately follow the end of a gvcf record but to occur\n      # at the base immediately after a variant, since the variant\'s end can\n      # span over a larger interval when it\'s a deletion and we still produce\n      # gvcf records under the deletion.\n      expected_start = v1.end if v1.alternate_bases == [\'<*>\'] else v1.start + 1\n      self.assertEqual(v2.start, expected_start)\n\n  def verify_deepvariant_calls(self, dv_calls, options):\n    # Verifies simple structural properties of the DeepVariantCall objects\n    # emitted by the VerySensitiveCaller, such as that the AlleleCount and\n    # Variant both have the same position.\n    for call in dv_calls:\n      for alt_allele in call.variant.alternate_bases:\n        # Skip ref calls.\n        if alt_allele == vcf_constants.NO_ALT_ALLELE:\n          continue\n        # Make sure allele appears in our allele_support field and that at\n        # least our min number of reads to call an alt allele are present in\n        # the supporting reads list for that allele.\n        self.assertIn(alt_allele, list(call.allele_support))\n        self.assertGreaterEqual(\n            len(call.allele_support[alt_allele].read_names),\n            options.variant_caller_options.min_count_snps)\n\n  def verify_examples(self, examples_filename, region, options, verify_labels):\n    # Do some simple structural checks on the tf.Examples in the file.\n    expected_features = [\n        \'variant/encoded\', \'locus\', \'image/format\', \'image/encoded\',\n        \'alt_allele_indices/encoded\'\n    ]\n    if verify_labels:\n      expected_features += [\'label\']\n\n    examples = list(tfrecord.read_tfrecords(examples_filename))\n    for example in examples:\n      for label_feature in expected_features:\n        self.assertIn(label_feature, example.features.feature)\n      # pylint: disable=g-explicit-length-test\n      self.assertNotEmpty(tf_utils.example_alt_alleles_indices(example))\n\n    # Check that the variants in the examples are good.\n    variants = [tf_utils.example_variant(x) for x in examples]\n    self.verify_variants(variants, region, options, is_gvcf=False)\n\n    return examples\n\n\nclass MakeExamplesUnitTest(parameterized.TestCase):\n\n  def test_read_write_run_info(self):\n\n    def _read_lines(path):\n      with open(path) as fin:\n        return list(fin.readlines())\n\n    golden_actual = make_examples.read_make_examples_run_info(\n        testdata.GOLDEN_MAKE_EXAMPLES_RUN_INFO)\n    # We don\'t really want to inject too much knowledge about the golden right\n    # here, so we only use a minimal test that (a) the run_info_filename is\n    # a non-empty string and (b) the number of candidates sites in the labeling\n    # metrics field is greater than 0. Any reasonable golden output will have at\n    # least one candidate variant, and the reader should have filled in the\n    # value.\n    self.assertNotEmpty(golden_actual.options.run_info_filename)\n    self.assertEqual(golden_actual.labeling_metrics.n_candidate_variant_sites,\n                     testdata.N_GOLDEN_TRAINING_EXAMPLES)\n\n    # Check that reading + writing the data produces the same lines:\n    tmp_output = test_utils.test_tmpfile(\'written_run_info.pbtxt\')\n    make_examples.write_make_examples_run_info(golden_actual, tmp_output)\n    self.assertEqual(\n        _read_lines(testdata.GOLDEN_MAKE_EXAMPLES_RUN_INFO),\n        _read_lines(tmp_output))\n\n  @parameterized.parameters(\n      dict(\n          flag_value=\'CALLING\',\n          expected=deepvariant_pb2.DeepVariantOptions.CALLING,\n      ),\n      dict(\n          flag_value=\'TRAINING\',\n          expected=deepvariant_pb2.DeepVariantOptions.TRAINING,\n      ),\n  )\n  def test_parse_proto_enum_flag(self, flag_value, expected):\n    enum_pb2 = deepvariant_pb2.DeepVariantOptions.Mode\n    self.assertEqual(\n        make_examples.parse_proto_enum_flag(enum_pb2, flag_value), expected)\n\n  def test_parse_proto_enum_flag_error_handling(self):\n    with six.assertRaisesRegex(\n        self, ValueError,\n        \'Unknown enum option ""foo"". Allowed options are CALLING,TRAINING\'):\n      make_examples.parse_proto_enum_flag(\n          deepvariant_pb2.DeepVariantOptions.Mode, \'foo\')\n\n  @flagsaver.FlagSaver\n  def test_keep_duplicates(self):\n    FLAGS.keep_duplicates = True\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.truth_variants = testdata.TRUTH_VARIANTS_VCF\n    FLAGS.confident_regions = testdata.CONFIDENT_REGIONS_BED\n    FLAGS.mode = \'training\'\n    FLAGS.examples = \'\'\n    options = make_examples.default_options(add_flags=True)\n    self.assertEqual(options.pic_options.read_requirements.keep_duplicates,\n                     True)\n\n  @flagsaver.FlagSaver\n  def test_keep_supplementary_alignments(self):\n    FLAGS.keep_supplementary_alignments = True\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.truth_variants = testdata.TRUTH_VARIANTS_VCF\n    FLAGS.confident_regions = testdata.CONFIDENT_REGIONS_BED\n    FLAGS.mode = \'training\'\n    FLAGS.examples = \'\'\n    options = make_examples.default_options(add_flags=True)\n    self.assertEqual(\n        options.pic_options.read_requirements.keep_supplementary_alignments,\n        True)\n\n  @flagsaver.FlagSaver\n  def test_keep_secondary_alignments(self):\n    FLAGS.keep_secondary_alignments = True\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.truth_variants = testdata.TRUTH_VARIANTS_VCF\n    FLAGS.confident_regions = testdata.CONFIDENT_REGIONS_BED\n    FLAGS.mode = \'training\'\n    FLAGS.examples = \'\'\n    options = make_examples.default_options(add_flags=True)\n    self.assertEqual(\n        options.pic_options.read_requirements.keep_secondary_alignments, True)\n\n  @flagsaver.FlagSaver\n  def test_min_base_quality(self):\n    FLAGS.min_base_quality = 5\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.truth_variants = testdata.TRUTH_VARIANTS_VCF\n    FLAGS.confident_regions = testdata.CONFIDENT_REGIONS_BED\n    FLAGS.mode = \'training\'\n    FLAGS.examples = \'\'\n    options = make_examples.default_options(add_flags=True)\n    self.assertEqual(options.pic_options.read_requirements.min_base_quality, 5)\n\n  @flagsaver.FlagSaver\n  def test_min_mapping_quality(self):\n    FLAGS.min_mapping_quality = 15\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.truth_variants = testdata.TRUTH_VARIANTS_VCF\n    FLAGS.confident_regions = testdata.CONFIDENT_REGIONS_BED\n    FLAGS.mode = \'training\'\n    FLAGS.examples = \'\'\n    options = make_examples.default_options(add_flags=True)\n    self.assertEqual(options.pic_options.read_requirements.min_mapping_quality,\n                     15)\n\n  @flagsaver.FlagSaver\n  def test_default_options_with_training_random_emit_ref_sites(self):\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.truth_variants = testdata.TRUTH_VARIANTS_VCF\n    FLAGS.confident_regions = testdata.CONFIDENT_REGIONS_BED\n    FLAGS.mode = \'training\'\n    FLAGS.examples = \'\'\n\n    FLAGS.training_random_emit_ref_sites = 0.3\n    options = make_examples.default_options(add_flags=True)\n    self.assertAlmostEqual(\n        options.variant_caller_options.fraction_reference_sites_to_emit, 0.3)\n\n  @flagsaver.FlagSaver\n  def test_default_options_without_training_random_emit_ref_sites(self):\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.truth_variants = testdata.TRUTH_VARIANTS_VCF\n    FLAGS.confident_regions = testdata.CONFIDENT_REGIONS_BED\n    FLAGS.mode = \'training\'\n    FLAGS.examples = \'\'\n\n    options = make_examples.default_options(add_flags=True)\n    # In proto3, there is no way to check presence of scalar field:\n    # redacted\n    # As an approximation, we directly check that the value should be exactly 0.\n    self.assertEqual(\n        options.variant_caller_options.fraction_reference_sites_to_emit, 0.0)\n\n  @parameterized.parameters(\n      (False, False, None,\n       deepvariant_pb2.PileupImageOptions.UNSPECIFIED_SEQ_TYPE, 6),\n      (True, True, None,\n       deepvariant_pb2.PileupImageOptions.UNSPECIFIED_SEQ_TYPE, 7),\n      (True, True, \'WES\', deepvariant_pb2.PileupImageOptions.WES, 7),\n      (True, True, \'WGS\', deepvariant_pb2.PileupImageOptions.WGS, 7),\n  )\n  @flagsaver.FlagSaver\n  def test_valid_sequencing_type(self, flag_image, expected_image,\n                                 flag_seq_type, expected_seq_type,\n                                 num_channels):\n    FLAGS.mode = \'training\'\n\n    FLAGS.sequencing_type_image = flag_image\n    FLAGS.sequencing_type = flag_seq_type\n    options = make_examples.default_options(add_flags=True)\n    self.assertEqual(options.pic_options.sequencing_type, expected_seq_type)\n    self.assertEqual(options.pic_options.sequencing_type_image, expected_image)\n    self.assertEqual(options.pic_options.num_channels, num_channels)\n\n  @flagsaver.FlagSaver\n  def test_invalid_sequencing_type(self):\n    FLAGS.mode = \'training\'\n    FLAGS.sequencing_type_image = True\n    FLAGS.sequencing_type = \'wGs\'\n    with self.assertRaises(ValueError):\n      make_examples.default_options(add_flags=True)\n\n  def test_extract_sample_name_from_reads_single_sample(self):\n    mock_sample_reader = mock.Mock()\n    mock_sample_reader.header = reads_pb2.SamHeader(\n        read_groups=[reads_pb2.ReadGroup(sample_id=\'sample_name\')])\n    self.assertEqual(\n        make_examples.extract_sample_name_from_sam_reader(mock_sample_reader),\n        \'sample_name\')\n\n  @parameterized.parameters(\n      # No samples could be found in the reads.\n      dict(samples=[], expected_sample_name=dv_constants.DEFAULT_SAMPLE_NAME),\n      # Check that we detect an empty sample name and use default instead.\n      dict(samples=[\'\'], expected_sample_name=dv_constants.DEFAULT_SAMPLE_NAME),\n      # We have more than one sample in the reads.\n      dict(samples=[\'sample1\', \'sample2\'], expected_sample_name=\'sample1\'),\n  )\n  def test_extract_sample_name_from_reads_uses_default_when_necessary(\n      self, samples, expected_sample_name):\n    mock_sample_reader = mock.Mock()\n    mock_sample_reader.header = reads_pb2.SamHeader(read_groups=[\n        reads_pb2.ReadGroup(sample_id=sample) for sample in samples\n    ])\n    self.assertEqual(\n        expected_sample_name,\n        make_examples.extract_sample_name_from_sam_reader(mock_sample_reader))\n\n  @flagsaver.FlagSaver\n  def test_confident_regions(self):\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.truth_variants = testdata.TRUTH_VARIANTS_VCF\n    FLAGS.confident_regions = testdata.CONFIDENT_REGIONS_BED\n    FLAGS.mode = \'training\'\n    FLAGS.examples = \'\'\n\n    options = make_examples.default_options(add_flags=True)\n    confident_regions = make_examples.read_confident_regions(options)\n\n    # Our expected intervals, inlined from CONFIDENT_REGIONS_BED.\n    expected = _from_literals_list([\n        \'chr20:10000847-10002407\', \'chr20:10002521-10004171\',\n        \'chr20:10004274-10004964\', \'chr20:10004995-10006386\',\n        \'chr20:10006410-10007800\', \'chr20:10007825-10008018\',\n        \'chr20:10008044-10008079\', \'chr20:10008101-10008707\',\n        \'chr20:10008809-10008897\', \'chr20:10009003-10009791\',\n        \'chr20:10009934-10010531\'\n    ])\n    # Our confident regions should be exactly those found in the BED file.\n    six.assertCountEqual(self, expected, list(confident_regions))\n\n  @parameterized.parameters(\n      ({\n          \'examples\': (\'foo\', \'foo\')\n      },),\n      ({\n          \'examples\': (\'foo\', \'foo\'),\n          \'gvcf\': (\'bar\', \'bar\')\n      },),\n      ({\n          \'examples\': (\'foo@10\', \'foo-00000-of-00010\')\n      },),\n      ({\n          \'task\': (0, 0),\n          \'examples\': (\'foo@10\', \'foo-00000-of-00010\')\n      },),\n      ({\n          \'task\': (1, 1),\n          \'examples\': (\'foo@10\', \'foo-00001-of-00010\')\n      },),\n      ({\n          \'task\': (1, 1),\n          \'examples\': (\'foo@10\', \'foo-00001-of-00010\'),\n          \'gvcf\': (\'bar@10\', \'bar-00001-of-00010\')\n      },),\n      ({\n          \'task\': (1, 1),\n          \'examples\': (\'foo@10\', \'foo-00001-of-00010\'),\n          \'gvcf\': (\'bar@10\', \'bar-00001-of-00010\'),\n          \'candidates\': (\'baz@10\', \'baz-00001-of-00010\')\n      },),\n  )\n  @flagsaver.FlagSaver\n  def test_sharded_outputs1(self, settings):\n    # Set all of the requested flag values.\n    for name, (flag_val, _) in settings.items():\n      setattr(FLAGS, name, flag_val)\n\n    FLAGS.mode = \'training\'\n    FLAGS.reads = \'\'\n    FLAGS.ref = \'\'\n    options = make_examples.default_options(add_flags=True)\n\n    # Check all of the flags.\n    for name, option_val in [(\'examples\', options.examples_filename),\n                             (\'candidates\', options.candidates_filename),\n                             (\'gvcf\', options.gvcf_filename)]:\n      expected = settings[name][1] if name in settings else \'\'\n      self.assertEqual(expected, option_val)\n\n  @flagsaver.FlagSaver\n  def test_gvcf_output_enabled_is_false_without_gvcf_flag(self):\n    FLAGS.mode = \'training\'\n    FLAGS.gvcf = \'\'\n    FLAGS.reads = \'\'\n    FLAGS.ref = \'\'\n    FLAGS.examples = \'\'\n    options = make_examples.default_options(add_flags=True)\n    self.assertFalse(make_examples.gvcf_output_enabled(options))\n\n  @flagsaver.FlagSaver\n  def test_gvcf_output_enabled_is_true_with_gvcf_flag(self):\n    FLAGS.mode = \'training\'\n    FLAGS.gvcf = \'/tmp/foo.vcf\'\n    FLAGS.reads = \'\'\n    FLAGS.ref = \'\'\n    FLAGS.examples = \'\'\n    options = make_examples.default_options(add_flags=True)\n    self.assertTrue(make_examples.gvcf_output_enabled(options))\n\n  def test_validate_ref_contig_coverage(self):\n    ref_contigs = _make_contigs([(\'1\', 100), (\'2\', 100)])\n\n    # Fully covered reference contigs don\'t trigger an error.\n    for threshold in [0.5, 0.9, 1.0]:\n      self.assertIsNone(\n          make_examples.validate_reference_contig_coverage(\n              ref_contigs, ref_contigs, threshold))\n\n    # No common contigs always blows up.\n    for threshold in [0.0, 0.1, 0.5, 0.9, 1.0]:\n      with six.assertRaisesRegex(self, ValueError, \'span 200\'):\n        make_examples.validate_reference_contig_coverage(\n            ref_contigs, [], threshold)\n\n    # Dropping either contig brings up below our 0.9 threshold.\n    with six.assertRaisesRegex(self, ValueError, \'span 200\'):\n      make_examples.validate_reference_contig_coverage(\n          ref_contigs, _make_contigs([(\'1\', 100)]), 0.9)\n\n    with six.assertRaisesRegex(self, ValueError, \'span 200\'):\n      make_examples.validate_reference_contig_coverage(\n          ref_contigs, _make_contigs([(\'2\', 100)]), 0.9)\n\n    # Our actual overlap is 50%, so check that we raise when appropriate.\n    with six.assertRaisesRegex(self, ValueError, \'span 200\'):\n      make_examples.validate_reference_contig_coverage(\n          ref_contigs, _make_contigs([(\'2\', 100)]), 0.6)\n    self.assertIsNone(\n        make_examples.validate_reference_contig_coverage(\n            ref_contigs, _make_contigs([(\'2\', 100)]), 0.4))\n\n  @parameterized.parameters(\n      # all intervals are shared.\n      ([[(\'chrM\', 10)], [(\'chrM\', 10)]], [(\'chrM\', 10)]),\n      # No common intervals.\n      ([[(\'chrM\', 10)], [(\'chr1\', 10)]], []),\n      # The names are the same but sizes are different, so not common.\n      ([[(\'chrM\', 10)], [(\'chrM\', 20)]], []),\n      # One common interval and one not.\n      ([[(\'chrM\', 10), (\'chr1\', 20)], [(\'chrM\', 10),\n                                       (\'chr2\', 30)]], [(\'chrM\', 10)]),\n      # Check that the order doesn\'t matter.\n      ([[(\'chr1\', 20), (\'chrM\', 10)], [(\'chrM\', 10),\n                                       (\'chr2\', 30)]], [(\'chrM\', 10, 1)]),\n      # Three-way merges.\n      ([\n          [(\'chr1\', 20), (\'chrM\', 10)],\n          [(\'chrM\', 10), (\'chr2\', 30)],\n          [(\'chr2\', 30), (\'chr3\', 30)],\n      ], []),\n  )\n  def test_common_contigs(self, contigs_list, expected):\n    self.assertEqual(\n        _make_contigs(expected),\n        make_examples.common_contigs(\n            [_make_contigs(contigs) for contigs in contigs_list]))\n\n  @parameterized.parameters(\n      # Note that these tests aren\'t so comprehensive as we are trusting that\n      # the intersection code logic itself is good and well-tested elsewhere.\n      # Here we are focusing on some basic tests and handling of missing\n      # calling_region and confident_region data.\n      ([\'1:1-10\'], [\'1:1-10\']),\n      ([\'1:1-100\'], [\'1:1-100\']),\n      ([\'1:50-150\'], [\'1:50-100\']),\n      (None, [\'1:1-100\', \'2:1-200\']),\n      ([\'1:20-50\'], [\'1:20-50\']),\n      # Chr3 isn\'t part of our contigs; make sure we tolerate it.\n      ([\'1:20-30\', \'1:40-60\', \'3:10-50\'], [\'1:20-30\', \'1:40-60\']),\n      # Check that we handle overlapping calling or confident regions.\n      ([\'1:25-30\', \'1:20-40\'], [\'1:20-40\']),\n  )\n  def test_regions_to_process(self, calling_regions, expected):\n    contigs = _make_contigs([(\'1\', 100), (\'2\', 200)])\n    six.assertCountEqual(\n        self, _from_literals_list(expected),\n        make_examples.regions_to_process(\n            contigs, 1000, calling_regions=_from_literals(calling_regions)))\n\n  @parameterized.parameters(\n      (50, None, [\n          \'1:1-50\', \'1:51-100\', \'2:1-50\', \'2:51-76\', \'3:1-50\', \'3:51-100\',\n          \'3:101-121\'\n      ]),\n      (120, None, [\'1:1-100\', \'2:1-76\', \'3:1-120\', \'3:121\']),\n      (500, None, [\'1:1-100\', \'2:1-76\', \'3:1-121\']),\n      (10, [\'1:1-20\', \'1:30-35\'], [\'1:1-10\', \'1:11-20\', \'1:30-35\']),\n      (8, [\'1:1-20\', \'1:30-35\'], [\'1:1-8\', \'1:9-16\', \'1:17-20\', \'1:30-35\']),\n  )\n  def test_regions_to_process_partition(self, max_size, calling_regions,\n                                        expected):\n    contigs = _make_contigs([(\'1\', 100), (\'2\', 76), (\'3\', 121)])\n    six.assertCountEqual(\n        self, _from_literals_list(expected),\n        make_examples.regions_to_process(\n            contigs, max_size, calling_regions=_from_literals(calling_regions)))\n\n  @parameterized.parameters(\n      dict(includes=[], excludes=[], expected=[\'1:1-100\', \'2:1-200\']),\n      dict(includes=[\'1\'], excludes=[], expected=[\'1:1-100\']),\n      # Check that excludes work as expected.\n      dict(includes=[], excludes=[\'1\'], expected=[\'2:1-200\']),\n      dict(includes=[], excludes=[\'2\'], expected=[\'1:1-100\']),\n      dict(includes=[], excludes=[\'1\', \'2\'], expected=[]),\n      # Check that excluding pieces works. The main checks on taking the\n      # difference between two RangeSets live in ranges.py so here we are just\n      # making sure some basic logic works.\n      dict(includes=[\'1\'], excludes=[\'1:1-10\'], expected=[\'1:11-100\']),\n      # Check that includes and excludes work together.\n      dict(\n          includes=[\'1\', \'2\'],\n          excludes=[\'1:5-10\', \'1:20-50\', \'2:10-20\'],\n          expected=[\'1:1-4\', \'1:11-19\', \'1:51-100\', \'2:1-9\', \'2:21-200\']),\n      dict(\n          includes=[\'1\'],\n          excludes=[\'1:5-10\', \'1:20-50\', \'2:10-20\'],\n          expected=[\'1:1-4\', \'1:11-19\', \'1:51-100\']),\n      dict(\n          includes=[\'2\'],\n          excludes=[\'1:5-10\', \'1:20-50\', \'2:10-20\'],\n          expected=[\'2:1-9\', \'2:21-200\']),\n      # A complex example of including and excluding.\n      dict(\n          includes=[\'1:10-20\', \'2:50-60\', \'2:70-80\'],\n          excludes=[\'1:1-13\', \'1:19-50\', \'2:10-65\'],\n          expected=[\'1:14-18\', \'2:70-80\']),\n  )\n  def test_build_calling_regions(self, includes, excludes, expected):\n    contigs = _make_contigs([(\'1\', 100), (\'2\', 200)])\n    actual = make_examples.build_calling_regions(contigs, includes, excludes)\n    six.assertCountEqual(self, actual, _from_literals_list(expected))\n\n  def test_regions_to_process_sorted_within_contig(self):\n    # These regions are out of order but within a single contig.\n    contigs = _make_contigs([(\'z\', 100)])\n    in_regions = _from_literals([\'z:15\', \'z:20\', \'z:6\', \'z:25-30\', \'z:3-4\'])\n    sorted_regions = _from_literals_list(\n        [\'z:3-4\', \'z:6\', \'z:15\', \'z:20\', \'z:25-30\'])\n    actual_regions = list(\n        make_examples.regions_to_process(\n            contigs, 100, calling_regions=in_regions))\n    # The assertEqual here is checking the order is exactly what we expect.\n    self.assertEqual(sorted_regions, actual_regions)\n\n  def test_regions_to_process_sorted_contigs(self):\n    # These contig names are out of order lexicographically.\n    contigs = _make_contigs([(\'z\', 100), (\'a\', 100), (\'n\', 100)])\n    in_regions = _from_literals([\'a:10\', \'n:1\', \'z:20\', \'z:5\'])\n    sorted_regions = _from_literals_list([\'z:5\', \'z:20\', \'a:10\', \'n:1\'])\n    actual_regions = list(\n        make_examples.regions_to_process(\n            contigs, 100, calling_regions=in_regions))\n    # The assertEqual here is checking the order is exactly what we expect.\n    self.assertEqual(sorted_regions, actual_regions)\n\n  @parameterized.parameters([2, 3, 4, 5, 50])\n  def test_regions_to_process_sharding(self, num_shards):\n    """"""Makes sure we deterministically split up regions.""""""\n\n    def get_regions(task_id, num_shards):\n      return make_examples.regions_to_process(\n          contigs=_make_contigs([(\'z\', 100), (\'a\', 100), (\'n\', 100)]),\n          partition_size=5,\n          task_id=task_id,\n          num_shards=num_shards)\n\n    # Check that the regions are the same unsharded vs. sharded.\n    unsharded_regions = get_regions(0, 0)\n    sharded_regions = []\n    for task_id in range(num_shards):\n      task_regions = get_regions(task_id, num_shards)\n      sharded_regions.extend(task_regions)\n    six.assertCountEqual(self, unsharded_regions, sharded_regions)\n\n  @parameterized.parameters(\n      # Providing one of task id and num_shards but not the other is bad.\n      (None, 0),\n      (None, 2),\n      (2, None),\n      (0, None),\n      # Negative values are illegal.\n      (-1, 2),\n      (0, -2),\n      # task_id >= num_shards is bad.\n      (2, 2),\n      (3, 2),\n  )\n  def test_regions_to_process_fails_with_bad_shard_args(self, task, num_shards):\n    with self.assertRaises(ValueError):\n      make_examples.regions_to_process(\n          contigs=_make_contigs([(\'z\', 100), (\'a\', 100), (\'n\', 100)]),\n          partition_size=10,\n          task_id=task,\n          num_shards=num_shards)\n\n  def test_catches_bad_argv(self):\n    with mock.patch.object(logging, \'error\') as mock_logging,\\\n        mock.patch.object(sys, \'exit\') as mock_exit:\n      make_examples.main([\'make_examples.py\', \'extra_arg\'])\n    mock_logging.assert_called_once_with(\n        \'Command line parsing failure: make_examples does not accept \'\n        \'positional arguments but some are present on the command line: \'\n        \'""[\\\'make_examples.py\\\', \\\'extra_arg\\\']"".\')\n    mock_exit.assert_called_once_with(errno.ENOENT)\n\n  @flagsaver.FlagSaver\n  def test_catches_bad_flags(self):\n    # Set all of the requested flag values.\n    region = ranges.parse_literal(\'chr20:10,000,000-10,010,000\')\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.candidates = test_utils.test_tmpfile(\'vsc.tfrecord\')\n    FLAGS.examples = test_utils.test_tmpfile(\'examples.tfrecord\')\n    FLAGS.regions = [ranges.to_literal(region)]\n    FLAGS.partition_size = 1000\n    FLAGS.mode = \'training\'\n    FLAGS.truth_variants = testdata.TRUTH_VARIANTS_VCF\n    # This is the bad flag.\n    FLAGS.confident_regions = \'\'\n\n    with mock.patch.object(logging, \'error\') as mock_logging,\\\n        mock.patch.object(sys, \'exit\') as mock_exit:\n      make_examples.main([\'make_examples.py\'])\n    mock_logging.assert_called_once_with(\n        \'confident_regions is required when in training mode.\')\n    mock_exit.assert_called_once_with(errno.ENOENT)\n\n  @parameterized.parameters(\n      dict(\n          ref_names=[\'1\', \'2\', \'3\'],\n          sam_names=[\'1\', \'2\', \'3\'],\n          vcf_names=None,\n          names_to_exclude=[],\n          min_coverage_fraction=1.0,\n          expected_names=[\'1\', \'2\', \'3\']),\n      dict(\n          ref_names=[\'1\', \'2\', \'3\'],\n          sam_names=[\'1\', \'2\'],\n          vcf_names=None,\n          names_to_exclude=[],\n          min_coverage_fraction=0.66,\n          expected_names=[\'1\', \'2\']),\n      dict(\n          ref_names=[\'1\', \'2\', \'3\'],\n          sam_names=[\'1\', \'2\'],\n          vcf_names=[\'1\', \'3\'],\n          names_to_exclude=[],\n          min_coverage_fraction=0.33,\n          expected_names=[\'1\']),\n      dict(\n          ref_names=[\'1\', \'2\', \'3\', \'4\', \'5\'],\n          sam_names=[\'1\', \'2\', \'3\'],\n          vcf_names=None,\n          names_to_exclude=[\'4\', \'5\'],\n          min_coverage_fraction=1.0,\n          expected_names=[\'1\', \'2\', \'3\']),\n  )\n  def test_ensure_consistent_contigs(self, ref_names, sam_names, vcf_names,\n                                     names_to_exclude, min_coverage_fraction,\n                                     expected_names):\n    ref_contigs = _make_contigs([(name, 100) for name in ref_names])\n    sam_contigs = _make_contigs([(name, 100) for name in sam_names])\n    if vcf_names is not None:\n      vcf_contigs = _make_contigs([(name, 100) for name in vcf_names])\n    else:\n      vcf_contigs = None\n    actual = make_examples._ensure_consistent_contigs(ref_contigs, sam_contigs,\n                                                      vcf_contigs,\n                                                      names_to_exclude,\n                                                      min_coverage_fraction)\n    self.assertEqual([a.name for a in actual], expected_names)\n\n  @parameterized.parameters(\n      dict(\n          ref_names=[\'1\', \'2\', \'3\'],\n          sam_names=[\'1\', \'2\'],\n          vcf_names=None,\n          names_to_exclude=[],\n          min_coverage_fraction=0.67),\n      dict(\n          ref_names=[\'1\', \'2\', \'3\'],\n          sam_names=[\'1\', \'2\'],\n          vcf_names=[\'1\', \'3\'],\n          names_to_exclude=[],\n          min_coverage_fraction=0.34),\n  )\n  def test_ensure_inconsistent_contigs(self, ref_names, sam_names, vcf_names,\n                                       names_to_exclude, min_coverage_fraction):\n    ref_contigs = _make_contigs([(name, 100) for name in ref_names])\n    sam_contigs = _make_contigs([(name, 100) for name in sam_names])\n    if vcf_names is not None:\n      vcf_contigs = _make_contigs([(name, 100) for name in vcf_names])\n    else:\n      vcf_contigs = None\n    with six.assertRaisesRegex(self, ValueError, \'Reference contigs span\'):\n      make_examples._ensure_consistent_contigs(ref_contigs, sam_contigs,\n                                               vcf_contigs, names_to_exclude,\n                                               min_coverage_fraction)\n\n  @flagsaver.FlagSaver\n  def test_regions_and_exclude_regions_flags(self):\n    FLAGS.mode = \'calling\'\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.regions = \'chr20:10,000,000-11,000,000\'\n    FLAGS.examples = \'examples.tfrecord\'\n    FLAGS.exclude_regions = \'chr20:10,010,000-10,100,000\'\n\n    options = make_examples.default_options(add_flags=True)\n    six.assertCountEqual(\n        self,\n        list(\n            ranges.RangeSet(\n                make_examples.processing_regions_from_options(options))),\n        _from_literals_list(\n            [\'chr20:10,000,000-10,009,999\', \'chr20:10,100,001-11,000,000\']))\n\n  @flagsaver.FlagSaver\n  def test_incorrect_empty_regions(self):\n    FLAGS.mode = \'calling\'\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    # Deliberately incorrect contig name.\n    FLAGS.regions = \'20:10,000,000-11,000,000\'\n    FLAGS.examples = \'examples.tfrecord\'\n\n    options = make_examples.default_options(add_flags=True)\n    with six.assertRaisesRegex(self, ValueError,\n                               \'The regions to call is empty.\'):\n      make_examples.processing_regions_from_options(options)\n\n\nclass RegionProcessorTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.region = ranges.parse_literal(\'chr20:10,000,000-10,000,100\')\n\n    FLAGS.reads = \'\'\n    self.options = make_examples.default_options(add_flags=False)\n    self.options.reference_filename = testdata.CHR20_FASTA\n    if not self.options.reads_filenames:\n      self.options.reads_filenames.extend(testdata.CHR20_BAM)\n    self.options.truth_variants_filename = testdata.TRUTH_VARIANTS_VCF\n    self.options.mode = deepvariant_pb2.DeepVariantOptions.TRAINING\n    self.options.variant_caller_options.sample_name = \'sample_id\'\n\n    self.processor = make_examples.RegionProcessor(self.options)\n    self.mock_init = self.add_mock(\'_initialize\')\n    self.default_shape = [5, 5, 7]\n    self.default_format = \'raw\'\n\n  def add_mock(self, name, retval=\'dontadd\', side_effect=\'dontadd\'):\n    patcher = mock.patch.object(self.processor, name, autospec=True)\n    self.addCleanup(patcher.stop)\n    mocked = patcher.start()\n    if retval != \'dontadd\':\n      mocked.return_value = retval\n    if side_effect != \'dontadd\':\n      mocked.side_effect = side_effect\n    return mocked\n\n  def test_on_demand_initialization_called_if_not_initialized(self):\n    candidates = [\'Candidates\']\n    self.assertFalse(self.processor.initialized)\n    self.processor.in_memory_sam_reader = mock.Mock()\n    mock_rr = self.add_mock(\'region_reads\', retval=[])\n    mock_cir = self.add_mock(\'candidates_in_region\', retval=(candidates, []))\n    mock_lc = self.add_mock(\'label_candidates\', retval=[])\n    self.processor.process(self.region)\n    test_utils.assert_called_once_workaround(self.mock_init)\n    mock_rr.assert_called_once_with(self.region)\n    self.processor.in_memory_sam_reader.replace_reads.assert_called_once_with(\n        [])\n    mock_cir.assert_called_once_with(self.region)\n    mock_lc.assert_called_once_with(candidates, self.region)\n\n  def test_on_demand_initialization_not_called_if_initialized(self):\n    self.processor.initialized = True\n    self.assertTrue(self.processor.initialized)\n    self.processor.in_memory_sam_reader = mock.Mock()\n    mock_rr = self.add_mock(\'region_reads\', retval=[])\n    mock_cir = self.add_mock(\'candidates_in_region\', retval=([], []))\n    mock_lc = self.add_mock(\'label_candidates\', retval=[])\n    self.processor.process(self.region)\n    test_utils.assert_not_called_workaround(self.mock_init)\n    mock_rr.assert_called_once_with(self.region)\n    self.processor.in_memory_sam_reader.replace_reads.assert_called_once_with(\n        [])\n    mock_cir.assert_called_once_with(self.region)\n    test_utils.assert_called_once_workaround(mock_lc)\n\n  def test_process_calls_no_candidates(self):\n    self.processor.in_memory_sam_reader = mock.Mock()\n    mock_rr = self.add_mock(\'region_reads\', retval=[])\n    mock_cir = self.add_mock(\'candidates_in_region\', retval=([], []))\n    mock_cpe = self.add_mock(\'create_pileup_examples\', retval=[])\n    mock_lc = self.add_mock(\'label_candidates\')\n    self.assertEqual(([], [], []), self.processor.process(self.region))\n    mock_rr.assert_called_once_with(self.region)\n    self.processor.in_memory_sam_reader.replace_reads.assert_called_once_with(\n        [])\n    mock_cir.assert_called_once_with(self.region)\n    test_utils.assert_not_called_workaround(mock_cpe)\n    mock_lc.assert_called_once_with([], self.region)\n\n  @parameterized.parameters([\n      deepvariant_pb2.DeepVariantOptions.TRAINING,\n      deepvariant_pb2.DeepVariantOptions.CALLING\n  ])\n  def test_process_calls_with_candidates(self, mode):\n    self.processor.options.mode = mode\n\n    self.processor.in_memory_sam_reader = mock.Mock()\n    mock_read = mock.MagicMock()\n    mock_candidate = mock.MagicMock()\n    mock_example = mock.MagicMock()\n    mock_label = mock.MagicMock()\n    mock_rr = self.add_mock(\'region_reads\', retval=[mock_read])\n    mock_cir = self.add_mock(\n        \'candidates_in_region\', retval=([mock_candidate], []))\n    mock_cpe = self.add_mock(\'create_pileup_examples\', retval=[mock_example])\n    mock_lc = self.add_mock(\n        \'label_candidates\', retval=[(mock_candidate, mock_label)])\n    mock_alte = self.add_mock(\'add_label_to_example\', retval=mock_example)\n    self.assertEqual(([mock_candidate], [mock_example], []),\n                     self.processor.process(self.region))\n    mock_rr.assert_called_once_with(self.region)\n    self.processor.in_memory_sam_reader.replace_reads.assert_called_once_with(\n        [mock_read])\n    mock_cir.assert_called_once_with(self.region)\n    mock_cpe.assert_called_once_with(mock_candidate)\n\n    if mode == deepvariant_pb2.DeepVariantOptions.TRAINING:\n      mock_lc.assert_called_once_with([mock_candidate], self.region)\n      mock_alte.assert_called_once_with(mock_example, mock_label)\n    else:\n      # In training mode we don\'t label our candidates.\n      test_utils.assert_not_called_workaround(mock_lc)\n      test_utils.assert_not_called_workaround(mock_alte)\n\n  @parameterized.parameters([\n      deepvariant_pb2.DeepVariantOptions.TRAINING,\n      deepvariant_pb2.DeepVariantOptions.CALLING\n  ])\n  def test_process_keeps_ordering_of_candidates_and_examples(self, mode):\n    self.processor.options.mode = mode\n\n    r1, r2 = mock.Mock(), mock.Mock()\n    c1, c2 = mock.Mock(), mock.Mock()\n    l1, l2 = mock.Mock(), mock.Mock()\n    e1, e2, e3 = mock.Mock(), mock.Mock(), mock.Mock()\n    self.processor.in_memory_sam_reader = mock.Mock()\n    self.add_mock(\'region_reads\', retval=[r1, r2])\n    self.add_mock(\'candidates_in_region\', retval=([c1, c2], []))\n    mock_cpe = self.add_mock(\n        \'create_pileup_examples\', side_effect=[[e1], [e2, e3]])\n    mock_lc = self.add_mock(\'label_candidates\', retval=[(c1, l1), (c2, l2)])\n    mock_alte = self.add_mock(\'add_label_to_example\', side_effect=[e1, e2, e3])\n    self.assertEqual(([c1, c2], [e1, e2, e3], []),\n                     self.processor.process(self.region))\n    self.processor.in_memory_sam_reader.replace_reads.assert_called_once_with(\n        [r1, r2])\n    # We don\'t try to label variants when in calling mode.\n    self.assertEqual([mock.call(c1), mock.call(c2)], mock_cpe.call_args_list)\n\n    if mode == deepvariant_pb2.DeepVariantOptions.CALLING:\n      # In calling mode, we never try to label.\n      test_utils.assert_not_called_workaround(mock_lc)\n      test_utils.assert_not_called_workaround(mock_alte)\n    else:\n      mock_lc.assert_called_once_with([c1, c2], self.region)\n      self.assertEqual([\n          mock.call(e1, l1),\n          mock.call(e2, l2),\n          mock.call(e3, l2),\n      ], mock_alte.call_args_list)\n\n  def test_process_with_realigner(self):\n    self.processor.options.mode = deepvariant_pb2.DeepVariantOptions.CALLING\n    self.processor.options.realigner_options.CopyFrom(\n        realigner_pb2.RealignerOptions())\n    self.processor.realigner = mock.Mock()\n    self.processor.realigner.realign_reads.return_value = [], []\n\n    self.processor.sam_readers = [mock.Mock()]\n    self.processor.sam_readers[0].query.return_value = []\n    self.processor.in_memory_sam_reader = mock.Mock()\n\n    c1, c2 = mock.Mock(), mock.Mock()\n    e1, e2, e3 = mock.Mock(), mock.Mock(), mock.Mock()\n    self.add_mock(\'candidates_in_region\', retval=([c1, c2], []))\n    mock_cpe = self.add_mock(\n        \'create_pileup_examples\', side_effect=[[e1], [e2, e3]])\n    mock_lc = self.add_mock(\'label_candidates\')\n\n    self.assertEqual(([c1, c2], [e1, e2, e3], []),\n                     self.processor.process(self.region))\n    self.processor.sam_readers[0].query.assert_called_once_with(self.region)\n    self.processor.realigner.realign_reads.assert_called_once_with([],\n                                                                   self.region)\n    self.processor.in_memory_sam_reader.replace_reads.assert_called_once_with(\n        [])\n    self.assertEqual([mock.call(c1), mock.call(c2)], mock_cpe.call_args_list)\n    test_utils.assert_not_called_workaround(mock_lc)\n\n  def test_candidates_in_region_no_reads(self):\n    self.processor.in_memory_sam_reader = mock.Mock()\n    self.processor.in_memory_sam_reader.query.return_value = []\n    mock_ac = self.add_mock(\'_make_allele_counter_for_region\')\n\n    self.assertEqual(([], []), self.processor.candidates_in_region(self.region))\n\n    self.processor.in_memory_sam_reader.query.assert_called_once_with(\n        self.region)\n    # A region with no reads should return out without making an AlleleCounter.\n    test_utils.assert_not_called_workaround(mock_ac)\n\n  @parameterized.parameters(True, False)\n  def test_candidates_in_region(self, include_gvcfs):\n    self.options.gvcf_filename = \'foo.vcf\' if include_gvcfs else \'\'\n    self.processor.in_memory_sam_reader = mock.Mock()\n    reads = [\'read1\', \'read2\']\n    self.processor.in_memory_sam_reader.query.return_value = reads\n    # Setup our make_allele_counter and other mocks.\n    mock_ac = mock.Mock()\n    mock_make_ac = self.add_mock(\n        \'_make_allele_counter_for_region\', retval=mock_ac)\n    # Setup our make_variant_caller and downstream mocks.\n    mock_vc = mock.Mock()\n    expected_calls = ([\'variant\'], [\'gvcf\'] if include_gvcfs else [])\n    mock_vc.calls_and_gvcfs.return_value = expected_calls\n    self.processor.variant_caller = mock_vc\n\n    actual = self.processor.candidates_in_region(self.region)\n\n    # Make sure we\'re getting our reads for the region.\n    self.processor.in_memory_sam_reader.query.assert_called_once_with(\n        self.region)\n\n    # Make sure we\'re creating an AlleleCounter once and adding each of our\n    # reads to it.\n    mock_make_ac.assert_called_once_with(self.region)\n    self.assertEqual([mock.call(r, \'sample_id\') for r in reads],\n                     mock_ac.add.call_args_list)\n\n    # Make sure we call CallVariant for each of the counts returned by the\n    # allele counter.\n    mock_vc.calls_and_gvcfs.assert_called_once_with(mock_ac, include_gvcfs)\n\n    # Finally, our actual result should be the single \'variant\' and potentially\n    # the gvcf records.\n    self.assertEqual(expected_calls, actual)\n\n  def test_create_pileup_examples_handles_none(self):\n    self.processor.pic = mock.Mock()\n    dv_call = mock.Mock()\n    self.processor.pic.create_pileup_images.return_value = None\n    self.assertEqual([], self.processor.create_pileup_examples(dv_call))\n    self.processor.pic.create_pileup_images.assert_called_once_with(dv_call)\n\n  def test_create_pileup_examples(self):\n    self.processor.pic = mock.Mock()\n    self.add_mock(\n        \'_encode_tensor\',\n        side_effect=[\n            (six.b(\'tensor1\'), self.default_shape, self.default_format),\n            (six.b(\'tensor2\'), self.default_shape, self.default_format)\n        ])\n    dv_call = mock.Mock()\n    dv_call.variant = test_utils.make_variant(start=10, alleles=[\'A\', \'C\', \'G\'])\n    ex = mock.Mock()\n    alt1, alt2 = [\'C\'], [\'G\']\n    self.processor.pic.create_pileup_images.return_value = [\n        (alt1, six.b(\'tensor1\')), (alt2, six.b(\'tensor2\'))\n    ]\n\n    actual = self.processor.create_pileup_examples(dv_call)\n\n    self.processor.pic.create_pileup_images.assert_called_once_with(dv_call)\n\n    self.assertLen(actual, 2)\n    for ex, (alt, img) in zip(actual, [(alt1, six.b(\'tensor1\')),\n                                       (alt2, six.b(\'tensor2\'))]):\n      self.assertEqual(tf_utils.example_alt_alleles(ex), alt)\n      self.assertEqual(tf_utils.example_variant(ex), dv_call.variant)\n      self.assertEqual(tf_utils.example_encoded_image(ex), img)\n      self.assertEqual(tf_utils.example_image_shape(ex), self.default_shape)\n      self.assertEqual(\n          tf_utils.example_image_format(ex), six.b(self.default_format))\n\n  @parameterized.parameters(\n      # Test that a het variant gets a label value of 1 assigned to the example.\n      dict(\n          label=variant_labeler.VariantLabel(\n              is_confident=True,\n              variant=test_utils.make_variant(start=10, alleles=[\'A\', \'C\']),\n              genotype=(0, 1)),\n          expected_label_value=1,\n      ),\n      # Test that a reference variant gets a label value of 0 in the example.\n      dict(\n          label=variant_labeler.VariantLabel(\n              is_confident=True,\n              variant=test_utils.make_variant(start=10, alleles=[\'A\', \'.\']),\n              genotype=(0, 0)),\n          expected_label_value=0,\n      ),\n  )\n  def test_add_label_to_example(self, label, expected_label_value):\n    example = self._example_for_variant(label.variant)\n    labeled = copy.deepcopy(example)\n    actual = self.processor.add_label_to_example(labeled, label)\n\n    # The add_label_to_example command modifies labeled and returns it.\n    self.assertIs(actual, labeled)\n\n    # Check that all keys from example are present in labeled.\n    for key, value in example.features.feature.items():\n      if key != \'variant/encoded\':  # Special case tested below.\n        self.assertEqual(value, labeled.features.feature[key])\n\n    # The genotype of our example_variant should be set to the true genotype\n    # according to our label.\n    self.assertEqual(expected_label_value, tf_utils.example_label(labeled))\n    labeled_variant = tf_utils.example_variant(labeled)\n    call = variant_utils.only_call(labeled_variant)\n    self.assertEqual(tuple(call.genotype), label.genotype)\n\n    # The original variant and labeled_variant from out tf.Example should be\n    # equal except for the genotype field, since this is set by\n    # add_label_to_example.\n    label.variant.calls[0].genotype[:] = []\n    call.genotype[:] = []\n    self.assertEqual(label.variant, labeled_variant)\n\n  def test_label_variant_raises_for_non_confident_variant(self):\n    label = variant_labeler.VariantLabel(\n        is_confident=False,\n        variant=test_utils.make_variant(start=10, alleles=[\'A\', \'C\']),\n        genotype=(0, 1))\n    example = self._example_for_variant(label.variant)\n    with six.assertRaisesRegex(\n        self, ValueError, \'Cannot add a non-confident label to an example\'):\n      self.processor.add_label_to_example(example, label)\n\n  def _example_for_variant(self, variant):\n    return tf_utils.make_example(variant, list(variant.alternate_bases),\n                                 six.b(\'foo\'), self.default_shape,\n                                 self.default_format)\n\n  def test_use_original_quality_scores_without_parse_sam_aux_fields(self):\n    FLAGS.mode = \'calling\'\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.reads = testdata.CHR20_BAM\n    FLAGS.examples = \'examples.tfrecord\'\n    FLAGS.use_original_quality_scores = True\n\n    with six.assertRaisesRegex(\n        self, Exception,\n        \'If use_original_quality_scores is set then parse_sam_aux_fields must be set too.\'\n    ):\n      make_examples.default_options(add_flags=True)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/model_eval.py,8,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Evaluates a DeepVariant model during training.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport json\nimport os\n\n\n\nfrom absl import flags\nfrom absl import logging\nimport tensorflow as tf\n\nfrom third_party.nucleus.util import proto_utils\nfrom tensorflow.python.framework.errors_impl import NotFoundError\nfrom deepvariant import data_providers\nfrom deepvariant import logging_level\nfrom deepvariant import modeling\nfrom deepvariant import tf_utils\n\nFLAGS = flags.FLAGS\n\n\ndef _get_metric_names(required_ordering):\n  metrics = modeling.eval_function_metrics(has_variant_types=True)\n  return [\n      name for name, ordering in metrics.items()\n      if ordering == required_ordering\n  ]\n\n\nincreasing_metrics = _get_metric_names(\n    required_ordering=modeling.EvalMetricOrdering.BIGGER_IS_BETTER)\ndecreasing_metrics = [\'loss\'] + _get_metric_names(\n    required_ordering=modeling.EvalMetricOrdering.SMALLER_IS_BETTER)\n\nflags.DEFINE_integer(\'batch_size\', 1024, \'The number of samples in each batch.\')\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'gcp_project\', None,\n    \'Project name for the Cloud TPU-enabled project. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\nflags.DEFINE_string(\n    \'tpu_zone\', None,\n    \'GCE zone where the Cloud TPU is located in. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\nflags.DEFINE_string(\n    \'tpu_name\', None,\n    \'Name of the Cloud TPU for Cluster Resolvers. You must specify either \'\n    \'this flag or --master.\')\n\nflags.DEFINE_string(\n    \'master\', None,\n    \'GRPC URL of the master (e.g. grpc://ip.address.of.tpu:8470). You \'\n    \'must specify either this flag or --tpu_name.\')\n\nflags.DEFINE_string(\'checkpoint_dir\', \'/tmp/deepvariant/\',\n                    \'Directory where the model was written to.\')\n\nflags.DEFINE_string(\n    \'eval_name\', None,\n    \'Name of the evaluation if user needs to run multiple evaluations on \'\n    \'different data sets, such as on training data vs test data. Metrics for \'\n    \'different evaluations are saved in separate directories, and appear \'\n    \'separately in tensorboard.  The directory will be named ""eval_""+eval_name\')\n\nflags.DEFINE_string(\n    \'eval_dir\', None,\n    \'This is used only to generate eval_name, if that is not provided.\')\n\nflags.DEFINE_integer(\'min_eval_interval_s\', 180,\n                     \'Minimum seconds between evaluations.\')\n\nflags.DEFINE_integer(\n    \'eval_timeout\', 10000,\n    \'Maximum seconds between checkpoints before evaluation \'\n    \'terminates.\')\n\nflags.DEFINE_integer(\'max_evaluations\', None,\n                     \'Max number of batches to evaluate\')\n\nflags.DEFINE_integer(\n    \'max_examples\', None,\n    \'Maximum number of examples to evaluate. Set to None (default) to evaluate \'\n    \'all examples. If not None, must be a positive integer and at most \'\n    \'`n_examples // max_example batches` will be evaluated.\')\n\nflags.DEFINE_string(\'model_name\', \'inception_v3\',\n                    \'The name of the model to use for predictions.\')\n\nflags.DEFINE_string(\'dataset_config_pbtxt\', None,\n                    \'The path to the dataset config file.\')\n\nflags.DEFINE_boolean(\'use_tpu\', False, \'use tpu if available\')\n\nflags.DEFINE_string(\n    \'kmp_blocktime\', \'0\',\n    \'Value to set the KMP_BLOCKTIME environment variable to for efficient MKL \'\n    \'evaluation. See https://www.tensorflow.org/performance/performance_guide \'\n    \'for more information. The default value is 0, which provides the best \'\n    \'performance in our tests. Set this flag to """" to not set the variable.\')\n\nflags.DEFINE_enum(\'best_checkpoint_metric\', \'F1/All\',\n                  increasing_metrics + decreasing_metrics,\n                  \'The metric for measuring the best checkpoint.\')\n\n\ndef main(_):\n  proto_utils.uses_fast_cpp_protos_or_die()\n\n  if not FLAGS.dataset_config_pbtxt:\n    logging.error(\'Need to specify --dataset_config_pbtxt\')\n  logging_level.set_from_flag()\n\n  if FLAGS.kmp_blocktime:\n    os.environ[\'KMP_BLOCKTIME\'] = FLAGS.kmp_blocktime\n    logging.info(\'Set KMP_BLOCKTIME to %s\', os.environ[\'KMP_BLOCKTIME\'])\n\n  master = tf_utils.resolve_master(FLAGS.master, FLAGS.tpu_name, FLAGS.tpu_zone,\n                                   FLAGS.gcp_project) if FLAGS.use_tpu else \'\'\n  eval_loop(\n      master=master,\n      dataset_config_pbtxt=FLAGS.dataset_config_pbtxt,\n      checkpoint_dir=FLAGS.checkpoint_dir,\n      model_name=FLAGS.model_name,\n      batch_size=FLAGS.batch_size,\n      max_examples=FLAGS.max_examples,\n      eval_name=FLAGS.eval_name,\n      max_evaluations=FLAGS.max_evaluations,\n      use_tpu=FLAGS.use_tpu,\n  )\n\n\ndef checkpoints_iterator(checkpoint_dir,\n                         min_interval_secs=0,\n                         timeout=None,\n                         timeout_fn=None):\n  # This is here to make it easy to mock out the iterator for tests.\n  return tf.train.checkpoints_iterator(checkpoint_dir, min_interval_secs,\n                                       timeout, timeout_fn)\n\n\ndef eval_loop(master,\n              dataset_config_pbtxt,\n              checkpoint_dir,\n              model_name,\n              batch_size,\n              max_examples,\n              eval_name,\n              max_evaluations,\n              use_tpu=False):\n  """"""Evaluate incoming checkpoints, until the specified end.""""""\n  logging.info(\'Running fixed eval for: %s\', dataset_config_pbtxt)\n\n  tf_dataset = data_providers.get_input_fn_from_dataset(\n      dataset_config_filename=dataset_config_pbtxt,\n      mode=tf.estimator.ModeKeys.EVAL,\n      use_tpu=use_tpu,\n  )\n\n  best_ckpt = None\n  ckpt_metric = FLAGS.best_checkpoint_metric\n  ckpt_metric_increasing = ckpt_metric in increasing_metrics\n\n  model = modeling.get_model(model_name)\n  logging.info(\'Running evaluations on %s with model %s\', tf_dataset, model)\n\n  # Compute when to stop reading, in terms of batches.\n  num_examples = tf_dataset.num_examples\n  if max_examples is not None:\n    num_examples = min(max_examples, num_examples)\n  num_batches = num_examples // batch_size\n  num_samples = batch_size * num_batches\n  logging.info(\n      \'Dataset has %s samples, doing eval over %s; \'\n      \'max_examples is %s, num examples to be used %s; num_batches is %s\',\n      tf_dataset.num_examples, num_samples, max_examples, num_examples,\n      num_batches)\n\n  # This loads EMA variables.\n  eval_hooks = [h(checkpoint_dir) for h in model.session_eval_hooks()]\n\n  classifier = model.make_estimator(\n      batch_size=batch_size,\n      model_dir=checkpoint_dir,\n      use_tpu=use_tpu,\n      master=master)\n\n  def terminate_eval():\n    logging.info(\'Terminating eval after %d seconds of no checkpoints\',\n                 FLAGS.eval_timeout)\n    return True\n\n  # Run evaluation when there\'s a new checkpoint\n  num_evaluations = 0\n  for ckpt in checkpoints_iterator(\n      checkpoint_dir=checkpoint_dir,\n      min_interval_secs=FLAGS.min_eval_interval_s,\n      timeout=FLAGS.eval_timeout,\n      timeout_fn=terminate_eval):\n\n    logging.info(\'Starting to evaluate.\')\n\n    # For each step, calls input_fn, which returns one batch of data.\n    # Evaluates until either steps batches are processed, or input_fn raises an\n    # end-of-input exception (OutOfRangeError or StopIteration).\n    eval_results = classifier.evaluate(\n        input_fn=tf_dataset,\n        steps=num_batches,\n        hooks=eval_hooks,\n        checkpoint_path=ckpt,\n        name=eval_name)\n    logging.info(\'Eval results: %s\', eval_results)\n\n    # Track best checkpoint seen so far, measured by ckpt_metric.\n    if not best_ckpt:\n      # If the training jobs died, pick up where we left off.\n      try:\n        best_metrics = read_metrics(ckpt, eval_name, \'best_checkpoint.metrics\')\n        logging.info(\'Found existing best_checkpoint: %s\', best_metrics)\n        best_ckpt = (best_metrics, ckpt)\n      except NotFoundError:\n        logging.info(\'best_checkpoint file does not exist.\')\n        best_ckpt = (eval_results, ckpt)\n        _write_best_checkpoint(ckpt, eval_results, eval_name)\n    if ((ckpt_metric_increasing and\n         eval_results[ckpt_metric] > best_ckpt[0][ckpt_metric]) or\n        (not ckpt_metric_increasing and\n         eval_results[ckpt_metric] < best_ckpt[0][ckpt_metric])):\n      best_ckpt = (eval_results, ckpt)\n      _write_best_checkpoint(ckpt, eval_results, eval_name)\n\n    _write_checkpoint_metrics(ckpt, eval_results, eval_name)\n\n    # An alternative strategy might check step-number-of-ckpt >= train_steps.\n    num_evaluations += 1\n    if max_evaluations is not None and num_evaluations >= max_evaluations:\n      logging.info(\'Evaluation finished after %d evaluations\', num_evaluations)\n      break\n\n  return\n\n\ndef checkpoint_metrics_path(checkpoint_path, eval_name, file_name=None):\n  """"""Gets a path to the JSON of eval metrics for checkpoint in eval_name.""""""\n  checkpoint_dir = os.path.dirname(checkpoint_path)\n  checkpoint_name = os.path.basename(checkpoint_path)\n  if eval_name:\n    # This bit of magic is defined by the estimator framework, and isn\'t easy\n    # to change.  We only get to specify the suffix.\n    checkpoint_dir = os.path.join(checkpoint_dir, \'eval_\' + eval_name)\n  if not file_name:\n    return os.path.join(checkpoint_dir, checkpoint_name + \'.metrics\')\n  return os.path.join(checkpoint_dir, file_name)\n\n\ndef read_metrics(checkpoint_path, eval_name, file_name=None):\n  """"""Reads the JSON of metrics for checkpoint_path in eval_dir.""""""\n  metrics_path = checkpoint_metrics_path(checkpoint_path, eval_name, file_name)\n  with tf.io.gfile.GFile(metrics_path) as fin:\n    return {k: float(v) for k, v in json.load(fin).items()}\n\n\ndef _write_best_checkpoint(checkpoint_path, metrics_and_values, eval_name):\n  """"""Writes files containing best checkpoint path and best checkpoint metrics.\n\n  Args:\n    checkpoint_path: str; a path to the best checkpoint.\n    metrics_and_values: dict[string,object]; a dictionary of key/value pairs\n      containing metrics for the best checkpoint we have seen. These will be\n      converted to a JSON of key/string pairs and written out to disk.\n    eval_name: str; the name of the eval run, which is used to derive the\n      subdirectory of checkpoint_path where the eval metrics will be written.\n  """"""\n  best_checkpoint_path = checkpoint_metrics_path(\n      checkpoint_path, eval_name, file_name=\'best_checkpoint.txt\')\n  best_checkpoint_metrics_path = checkpoint_metrics_path(\n      checkpoint_path, eval_name, file_name=\'best_checkpoint.metrics\')\n  serializable = {k: str(v) for k, v in metrics_and_values.items()}\n  logging.info(\'Writing new best checkpoint %s with values %s\',\n               best_checkpoint_path, metrics_and_values)\n  try:\n    with tf.io.gfile.GFile(best_checkpoint_path, \'w\') as fout:\n      fout.write(checkpoint_path + \'\\n\')\n    with tf.io.gfile.GFile(best_checkpoint_metrics_path, \'w\') as fout:\n      json.dump(serializable, fout, sort_keys=True, indent=4)\n  except:  # pylint: disable=bare-except\n    # Note we have a bare exception here as as there\'s no clear TF base\n    # exception to catch will cover all of the potential issues that might arise\n    # trying to write our metrics to our metrics file.\n    logging.warning(\'Failed to write best checkpoint to path %s\',\n                    best_checkpoint_path)\n\n\ndef _write_checkpoint_metrics(checkpoint_path,\n                              metrics_and_values,\n                              eval_name,\n                              current_metrics=\'current.metrics\'):\n  """"""Writes a JSON of metrics for checkpoint_path in eval_name.\n\n  This function writes out metrics to a JSON for a checkpoint into\n  .../eval_name.\n  The exact path of this file will be computed with:\n\n    `checkpoint_metrics_path(checkpoint_path, eval_name)`\n\n  and the values for metrics_and_values (a dict of strings => objects) written\n  out as key: str(object) into a JSON file.\n\n  Args:\n    checkpoint_path: str; a path to the checkpoint we computed metrics on.\n    metrics_and_values: dict[string,object]; a dictionary of key/value pairs\n      containing our metrics. These will be converted to a JSON of key/string\n      pairs and written out to disk.\n    eval_name: str; the name of the eval run, which is used to derive the the\n      subdirectory of checkpoint_path where the eval metrics will be written.\n    current_metrics: str; a single file that contains most recent metrics\n      values, and gets updated at each checkpoint.\n  """"""\n  path = checkpoint_metrics_path(checkpoint_path, eval_name)\n  experiment_metrics_path = checkpoint_metrics_path(\n      checkpoint_path, eval_name, file_name=current_metrics)\n  serializable = {k: str(v) for k, v in metrics_and_values.items()}\n  logging.info(\'Writing checkpoint metrics %s\', path)\n  try:\n    with tf.io.gfile.GFile(path, \'w\') as fout:\n      json.dump(serializable, fout, sort_keys=True, indent=4)\n    with tf.io.gfile.GFile(experiment_metrics_path, \'w\') as eout:\n      json.dump(serializable, eout, sort_keys=True, indent=4)\n  except:  # pylint: disable=bare-except\n    # Note we have a bare exception here as as there\'s no clear TF base\n    # exception to catch will cover all of the potential issues that might arise\n    # trying to write our metrics to our metrics file.\n    logging.warning(\'Failed to write checkpoint metrics to path %s\', path)\n\n\nif __name__ == \'__main__\':\n  tf.compat.v1.app.run()\n'"
deepvariant/model_eval_test.py,5,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for genomics.deepvariant.model_eval.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport os\n\n\nfrom absl import flags\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\nimport six\nimport tensorflow as tf\n\nfrom deepvariant import data_providers_test\nfrom deepvariant import model_eval\nfrom deepvariant import testdata\nfrom deepvariant.testing import flagsaver\nfrom deepvariant.testing import tf_test_utils\n\nFLAGS = flags.FLAGS\n\n# Note that this test suite is invoked twice, with --use_tpu set both ways.\n\n\ndef setUpModule():\n  testdata.init()\n\n\nclass ModelEvalTest(\n    six.with_metaclass(parameterized.TestGeneratorMetaclass, tf.test.TestCase)):\n\n  def setUp(self):\n    self.checkpoint_dir = tf.compat.v1.test.get_temp_dir()\n    # Use this to generate a random name.  The framework\n    # will create the directory under self.checkpoint_dir.\n    self.eval_name = os.path.basename(tf.compat.v1.test.get_temp_dir())\n\n  @parameterized.parameters([\'inception_v3\'])\n  @flagsaver.FlagSaver\n  @mock.patch(\'deepvariant.data_providers.\'\n              \'get_input_fn_from_dataset\')\n  def test_end2end(self, model_name, mock_get_input_fn_from_dataset):\n    """"""End-to-end test of model_eval.""""""\n    tf_test_utils.write_fake_checkpoint(\'inception_v3\', self.test_session(),\n                                        self.checkpoint_dir,\n                                        FLAGS.moving_average_decay)\n\n    # Start up eval, loading that checkpoint.\n    FLAGS.batch_size = 2\n    FLAGS.checkpoint_dir = self.checkpoint_dir\n    FLAGS.eval_name = self.eval_name\n    FLAGS.max_evaluations = 1\n    FLAGS.max_examples = 2\n    FLAGS.best_checkpoint_metric = \'F1/All\'\n    FLAGS.model_name = model_name\n    FLAGS.dataset_config_pbtxt = \'/path/to/mock.pbtxt\'\n    FLAGS.master = \'\'\n    # Always try to read in compressed inputs to stress that case. Uncompressed\n    # inputs are certain to work. This test is expensive to run, so we want to\n    # minimize the number of times we need to run this.\n    mock_get_input_fn_from_dataset.return_value = (\n        data_providers_test.make_golden_dataset(\n            compressed_inputs=True, use_tpu=FLAGS.use_tpu))\n    model_eval.main(0)\n    mock_get_input_fn_from_dataset.assert_called_once_with(\n        dataset_config_filename=FLAGS.dataset_config_pbtxt,\n        mode=tf.estimator.ModeKeys.EVAL,\n        use_tpu=FLAGS.use_tpu)\n    self.assertTrue(\n        tf_test_utils.check_file_exists(\n            \'best_checkpoint.txt\', eval_name=self.eval_name))\n    self.assertTrue(\n        tf_test_utils.check_file_exists(\n            \'best_checkpoint.metrics\', eval_name=self.eval_name))\n\n  # Using a constant model, check that running an eval returns the expected\n  # metrics.\n  @flagsaver.FlagSaver\n  @mock.patch(\n      \'deepvariant.model_eval.checkpoints_iterator\')\n  @mock.patch(\'deepvariant.data_providers.\'\n              \'get_input_fn_from_dataset\')\n  def test_fixed_eval_sees_the_same_evals(self, mock_get_input_fn_from_dataset,\n                                          mock_checkpoints_iterator):\n    dataset = data_providers_test.make_golden_dataset(use_tpu=FLAGS.use_tpu)\n    n_checkpoints = 3\n    checkpoints = [\n        tf_test_utils.write_fake_checkpoint(\n            \'constant\',\n            self.test_session(),\n            self.checkpoint_dir,\n            FLAGS.moving_average_decay,\n            name=\'model\' + str(i)) for i in range(n_checkpoints)\n    ]\n\n    # Setup our mocks.\n    mock_checkpoints_iterator.return_value = checkpoints\n    mock_get_input_fn_from_dataset.return_value = dataset\n\n    # Start up eval, loading that checkpoint.\n    FLAGS.batch_size = 2\n    FLAGS.checkpoint_dir = self.checkpoint_dir\n    FLAGS.eval_name = self.eval_name\n    FLAGS.max_evaluations = n_checkpoints\n    FLAGS.model_name = \'constant\'\n    FLAGS.dataset_config_pbtxt = \'/path/to/mock.pbtxt\'\n    FLAGS.master = \'\'\n    model_eval.main(0)\n\n    self.assertEqual(mock_get_input_fn_from_dataset.call_args_list, [\n        mock.call(\n            use_tpu=FLAGS.use_tpu,\n            dataset_config_filename=FLAGS.dataset_config_pbtxt,\n            mode=tf.estimator.ModeKeys.EVAL)\n    ])\n\n    metrics = [\n        model_eval.read_metrics(checkpoint, eval_name=FLAGS.eval_name)\n        for checkpoint in checkpoints\n    ]\n\n    # Check that our metrics are what we expect them to be.\n    # See internal for details on how to compute these counts:\n    # Counts of labels in our golden dataset:\n    #  1 0\n    # 12 1\n    # 35 2\n    expected_values_for_all_exact = {\n        # We have 12 correct calls [there are 12 variants with a label of 1] and\n        # 1 label 0 + 35 with a label of 2, so we have an accuracy of 12 / 48,\n        # which is 0.25.\n        \'Accuracy/All\': 0.25,\n        # We don\'t have any FNs because we call everything het.\n        \'FNs/All\': 0,\n        # Two of our labels are 0, which we call het, giving us 2 FP.\n        \'FPs/All\': 1.0,\n        # We call everything as het, so the recall has to be 1.\n        \'Recall/All\': 1.0,\n        # redacted\n        # # We don\'t call anything but hets, so TNs has to be 0.\n        # \'TNs/All\': 0,\n        # We find 47 positives, so this has to be 47.\n        \'TPs/All\': 47,\n    }\n    for key, expected_value in expected_values_for_all_exact.items():\n      print(str(key) + \'=\' + str(metrics[0][key]))\n\n    for key, expected_value in expected_values_for_all_exact.items():\n      self.assertEqual(metrics[0][key], expected_value)\n\n    expected_values_for_all_close = {\n        # We called 47 / 48 correctly ~ 0.979167\n        \'Precision/All\': 0.979167,\n        # We called (2 * 47 / 48) / (1 + 47 / 48) correctly ~ 0.989474\n        \'F1/All\': 0.989474,\n    }\n    for key, expected_value in expected_values_for_all_close.items():\n      self.assertAlmostEqual(metrics[0][key], expected_value, places=6)\n\n    for m1, m2 in zip(metrics, metrics[1:]):\n      self.assertEqual(m1, m2)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/model_train.py,17,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Trains the DeepVariant model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport json\nimport os\n\n\n\nfrom absl import flags\nfrom absl import logging\nimport tensorflow as tf\n\nfrom third_party.nucleus.util import proto_utils\nfrom deepvariant import data_providers\nfrom deepvariant import logging_level\nfrom deepvariant import modeling\nfrom deepvariant import tf_utils\n\n\nFLAGS = flags.FLAGS\n\n# Data set selection parameters\nflags.DEFINE_string(\'dataset_config_pbtxt\', None,\n                    \'The path to the dataset config file.\')\n\nflags.DEFINE_string(\'model_name\', \'inception_v3\',\n                    \'The name of the model to use for predictions.\')\n\nflags.DEFINE_integer(\'batch_size\', 4096, \'The number of samples in each batch.\')\n\n# Cloud TPU Cluster Resolvers\nflags.DEFINE_string(\n    \'gcp_project\', None,\n    \'Project name for the Cloud TPU-enabled project. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\nflags.DEFINE_string(\n    \'tpu_zone\', None,\n    \'GCE zone where the Cloud TPU is located in. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\nflags.DEFINE_string(\n    \'tpu_name\', None,\n    \'Name of the Cloud TPU for Cluster Resolvers. You must specify either \'\n    \'this flag or --master.\')\n\nflags.DEFINE_string(\n    \'master\', None,\n    \'GRPC URL of the master (e.g. grpc://ip.address.of.tpu:8470). You \'\n    \'must specify either this flag or --tpu_name.\')\n\nflags.DEFINE_string(\'train_dir\', \'/tmp/deepvariant/\',\n                    \'Directory where to write event logs.\')\n\nflags.DEFINE_boolean(\'use_tpu\', False, \'use tpu if available\')\n\nflags.DEFINE_integer(\'worker_replicas\', 1, \'Number of worker replicas.\')\n\nflags.DEFINE_integer(\n    \'ps_tasks\', 0,\n    \'The number of parameter servers. If the value is 0, then the parameters \'\n    \'are handled locally by the worker.\')\n\nflags.DEFINE_integer(\'task\', 0, \'Task id of the replica running the training.\')\n\nflags.DEFINE_integer(\'number_of_steps\', 8000000,\n                     \'Maximum number of global steps to take when training.\')\n\nflags.DEFINE_boolean(\'use_early_stopping\', False, \'Use early stopping hook.\')\n\nflags.DEFINE_string(\n    \'early_stopping_directory\', \'eval_on_tune\',\n    \'Directory containing event files for early stopping hook to monitor.\')\n\nflags.DEFINE_string(\n    \'early_stopping_tag\', \'F1/All\',\n    \'The metric to monitor for early stopping (eg. loss, accuracy, etc.)\')\n\nflags.DEFINE_float(\n    \'early_stopping_plateau_delta\', 1e-7,\n    \'The amount of change of a metric over num_plateau_steps that indicates \'\n    \'the metric has stopped improving.\')\n\nflags.DEFINE_integer(\'early_stopping_num_plateau_steps\', 1000000,\n                     \'Number of steps the metric needs to be plateaued for.\')\n\nflags.DEFINE_enum(\n    \'early_stopping_metric_direction\', \'increase\', [\'increase\', \'decrease\'],\n    \'Whether to check if the metric has increased by delta or decreased by \'\n    \'delta.\')\n\nflags.DEFINE_integer(\'early_stopping_every_n_steps\', 10,\n                     \'Run early stopping hook every n steps.\')\n\nflags.DEFINE_integer(\n    \'num_retries\', 0,\n    \'The number of times to retry on InternalError or UnavailableError.\')\n\nflags.DEFINE_integer(\n    \'max_examples\', None,\n    \'The maximum number of examples to use in training. If None, all examples \'\n    \'will be used. If not None, the first max_examples examples from the \'\n    \'dataset will be used, with those same examples repeating over and over.\')\n\n# Pre-trained model parameters\nflags.DEFINE_string(\n    \'start_from_checkpoint\', \'model_default\',\n    \'A path to a checkpoint of model weights to initialize our model at the \'\n    \'start of training. If None or """", the model will start from random weights\'\n    \'. The special value ""model_default"" will use the default pretrained \'\n    \'path for the selected model.\')\n\nflags.DEFINE_integer(\n    \'max_checkpoints_to_keep\', 10,\n    \'Number of last checkpoints to keep during training. \'\n    \'Passing ""0"" preserves all checkpoints.\')\n\nflags.DEFINE_string(\n    \'kmp_blocktime\', \'0\',\n    \'Value to set the KMP_BLOCKTIME environment variable to for efficient MKL \'\n    \'training. See https://www.tensorflow.org/performance/performance_guide \'\n    \'for more information. The default value is 0, which provides the best \'\n    \'performance in our tests. Set this flag to """" to not set the variable.\')\n\nflags.DEFINE_integer(\n    \'random_seed\', 400620758,\n    \'Random seed value to use for TensorFlow. Providing a value != 0 will \'\n    \'result in a call to tf.set_random_seed(FLAGS.random_seed), making \'\n    \'training more deterministic. If set to 0, the TensorFlow random seed \'\n    \'will not be set at all, and TensorFlow will assign it a pseudo-random \'\n    \'value each time model_train is run.\')\n\n\ndef loss(logits, one_hot_labels, label_smoothing):\n  """"""Creates a loss function for training logits against one_hot_labels.\n\n  Args:\n      logits: tensor. logits of the model we want to train.\n    one_hot_labels: One-hot encoded truth labels that we want to train this\n      model to predict.\n    label_smoothing: float. label_smoothing value for softmax_cross_entropy.\n\n  Returns:\n    A `Tensor` whose value represents the total loss.\n  """"""\n  tf.compat.v1.losses.softmax_cross_entropy(\n      logits, one_hot_labels, label_smoothing=label_smoothing, weights=1.0)\n  return tf.compat.v1.losses.get_total_loss()\n\n\ndef run(target, unused_is_chief, device_fn, use_tpu):\n  """"""Run training.\n\n  Args:\n     target: The target of the TensorFlow standard server to use. Can be the\n       empty string to run locally using an inprocess server.\n     device_fn: Device function used to assign ops to devices.\n     use_tpu: turn on tpu code path.\n  """"""\n  if not FLAGS.dataset_config_pbtxt:\n    logging.error(\'Need to specify --dataset_config_pbtxt\')\n    return\n\n  g = tf.Graph()\n  with g.as_default():\n    with tf.device(device_fn):\n      # If ps_tasks is zero, the local device is used. When using multiple\n      # (non-local) replicas, the ReplicaDeviceSetter distributes the variables\n      # across the different devices.\n\n      tf_dataset = data_providers.get_input_fn_from_dataset(\n          dataset_config_filename=FLAGS.dataset_config_pbtxt,\n          mode=tf.estimator.ModeKeys.TRAIN,\n          max_examples=FLAGS.max_examples,\n          use_tpu=use_tpu)\n      model = modeling.get_model(FLAGS.model_name)\n      logging.info(\'Running training on %s with model %s and tpu %s\',\n                   tf_dataset, FLAGS.model_name, use_tpu)\n\n      batches_per_epoch = tf_dataset.num_examples // FLAGS.batch_size\n      logging.info(\'Batches per epoch %s\', batches_per_epoch)\n      params = dict(batches_per_epoch=batches_per_epoch,)\n      estimator = model.make_estimator(\n          batch_size=FLAGS.batch_size,\n          model_dir=FLAGS.train_dir,\n          params=params,\n          use_tpu=use_tpu,\n          master=target,\n          start_from_checkpoint=FLAGS.start_from_checkpoint,\n      )\n\n      training_hooks = None\n      if FLAGS.use_early_stopping:\n        # redacted\n        raise ValueError(\'Currently not implemented.\')\n\n      estimator.train(\n          input_fn=tf_dataset,\n          max_steps=FLAGS.number_of_steps,\n          hooks=training_hooks)\n\n\ndef parse_and_run():\n  """"""Parse TF_CONFIG to cluster_spec and call run().\n\n  TF_CONFIG environment variable is available when running using\n  gcloud either locally or on cloud. It has all the information required\n  to create a ClusterSpec which is important for running distributed code.\n\n  Raises:\n    ValueError: If flags are invalid.\n  """"""\n  tf_config = os.environ.get(\'TF_CONFIG\')\n  logging.info(\'TF_CONFIG %s\', tf_config)\n\n  for name in [\'master\', \'task\', \'ps_tasks\']:\n    if getattr(FLAGS, name) and tf_config:\n      raise ValueError(\n          \'Either the flag --%s or the environment variable TF_CONFIG can be\'\n          \' set but not both.\' % name)\n\n  # redacted\n  #\n  # If TF_CONFIG is not available we are either running locally in Cloud\n  # or distributed inside Google. On Cloud the default values of\n  # FLAGS.master and FLAGS.task correspond to running training locally.\n  # Inside Google they will be set as needed to configure local or distributed\n  # training. Inside Google we don\'t need to explicitly set worker_device\n  # in replica_device_setter becaue this will be set automatically based\n  # on various flags.\n  if not tf_config:\n    device_fn = tf.compat.v1.train.replica_device_setter(FLAGS.ps_tasks)\n    # pylint: disable=g-long-ternary\n    master = tf_utils.resolve_master(FLAGS.master, FLAGS.tpu_name,\n                                     FLAGS.tpu_zone,\n                                     FLAGS.gcp_project) if FLAGS.use_tpu else \'\'\n    return run(\n        master, FLAGS.task == 0, device_fn=device_fn, use_tpu=FLAGS.use_tpu)\n\n  tf_config_json = json.loads(tf_config)\n\n  cluster = tf_config_json.get(\'cluster\')\n  job_name = tf_config_json.get(\'task\', {}).get(\'type\')\n  task_index = tf_config_json.get(\'task\', {}).get(\'index\')\n\n  # If cluster information is empty run local\n  if job_name is None or task_index is None:\n    device_fn = tf.compat.v1.train.replica_device_setter(0)\n    return run(\'\', True, device_fn=device_fn, use_tpu=FLAGS.use_tpu)\n\n  ps = cluster.get(\'ps\', [])\n  num_ps = len(ps)\n\n  cluster_spec = tf.train.ClusterSpec(cluster)\n  server = tf.distribute.Server(\n      cluster_spec, job_name=job_name, task_index=task_index)\n\n  if job_name == \'ps\':\n    server.join()\n    return\n  elif job_name in [\'master\', \'worker\']:\n    device_fn = tf.compat.v1.train.replica_device_setter(\n        num_ps,\n        worker_device=\'/job:%s/task:%d\' % (job_name, task_index),\n        cluster=cluster_spec)\n    return run(\n        server.target,\n        job_name == \'master\',\n        device_fn=device_fn,\n        use_tpu=FLAGS.use_tpu)\n\n\ndef main(_):\n  """"""Run and handle retryable errors.""""""\n  proto_utils.uses_fast_cpp_protos_or_die()\n\n  logging_level.set_from_flag()\n\n  if FLAGS.random_seed:\n    logging.info(\'Setting tf.random_seed to %d\', FLAGS.random_seed)\n    tf.compat.v1.set_random_seed(FLAGS.random_seed)\n  else:\n    logging.info(\'Not setting tf.random_seed, will be assigned a random value\')\n\n  if FLAGS.kmp_blocktime:\n    os.environ[\'KMP_BLOCKTIME\'] = FLAGS.kmp_blocktime\n    logging.info(\'Set KMP_BLOCKTIME to %s\', os.environ[\'KMP_BLOCKTIME\'])\n\n  for _ in range(FLAGS.num_retries + 1):\n    try:\n      parse_and_run()\n      return\n    except tf.errors.UnavailableError as e:\n      # An UnavailableError indicates a gRPC error, typically this is\n      # retryable.\n      logging.error(\'Caught UnavailableError %s; will retry.\', e)\n    except tf.errors.InternalError as e:\n      # Retry on an InternalError.\n      logging.error(\'Caught InternalError %s; will retry.\', e)\n\n\nif __name__ == \'__main__\':\n  tf.compat.v1.app.run()\n'"
deepvariant/model_train_test.py,10,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .model_train.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport json\nimport uuid\n\n\n\nfrom absl import flags\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\nimport six\nimport tensorflow as tf\n\nfrom deepvariant import data_providers_test\nfrom deepvariant import model_train\nfrom deepvariant import modeling\nfrom deepvariant import testdata\nfrom deepvariant.testing import flagsaver\nfrom deepvariant.testing import tf_test_utils\n\n\nFLAGS = flags.FLAGS\nMOCK_SENTINEL_RETURN_VALUE = \'mocked_return_value\'\n\n# Note that this test suite is invoked twice, with --use_tpu set both ways.\n\n\ndef setUpModule():\n  testdata.init()\n\n\nclass ModelTrainTest(parameterized.TestCase, tf.test.TestCase):\n\n  def _run_tiny_training(self, model_name, dataset, warm_start_from=\'\'):\n    """"""Runs one training step. This function always starts a new train_dir.""""""\n    with mock.patch(\n        \'deepvariant.data_providers.\'\n        \'get_input_fn_from_dataset\') as mock_get_input_fn_from_dataset:\n      mock_get_input_fn_from_dataset.return_value = dataset\n      FLAGS.train_dir = tf_test_utils.test_tmpdir(uuid.uuid4().hex)\n      FLAGS.batch_size = 2\n      FLAGS.model_name = model_name\n      FLAGS.save_interval_secs = -1\n      FLAGS.save_interval_steps = 1\n      FLAGS.number_of_steps = 1\n      FLAGS.dataset_config_pbtxt = \'/path/to/mock.pbtxt\'\n      FLAGS.start_from_checkpoint = warm_start_from\n      FLAGS.master = \'\'\n      model_train.parse_and_run()\n      # We have a checkpoint after training.\n      mock_get_input_fn_from_dataset.assert_called_once_with(\n          dataset_config_filename=FLAGS.dataset_config_pbtxt,\n          mode=tf.estimator.ModeKeys.TRAIN,\n          use_tpu=mock.ANY,\n          max_examples=None,\n      )\n      self.assertIsNotNone(tf.train.latest_checkpoint(FLAGS.train_dir))\n\n  @mock.patch(\'deepvariant\'\n              \'.modeling.tf.compat.v1.losses.softmax_cross_entropy\')\n  @mock.patch(\'deepvariant\'\n              \'.modeling.tf.compat.v1.losses.get_total_loss\')\n  def test_loss(self, mock_total_loss, mock_cross):\n    labels = [[0, 1, 0], [1, 0, 0]]\n    logits = \'Logits\'\n    smoothing = 0.01\n    actual = model_train.loss(logits, labels, smoothing)\n    mock_total_loss.assert_called_once_with()\n    self.assertEqual(actual, mock_total_loss.return_value)\n    mock_cross.assert_called_once_with(\n        logits, labels, label_smoothing=smoothing, weights=1.0)\n\n  # pylint: disable=g-complex-comprehension\n  @parameterized.parameters((model.name, compressed_inputs)\n                            for model in modeling.production_models()\n                            if model.is_trainable\n                            for compressed_inputs in [True, False])\n  # pylint: enable=g-complex-comprehension\n  @flagsaver.FlagSaver\n  def test_end2end(self, model_name, compressed_inputs):\n    """"""End-to-end test of model_train script.""""""\n    self._run_tiny_training(\n        model_name=model_name,\n        dataset=data_providers_test.make_golden_dataset(\n            compressed_inputs=compressed_inputs, use_tpu=FLAGS.use_tpu))\n\n  @flagsaver.FlagSaver\n  def test_end2end_inception_v3_warm_up_from(self):\n    """"""End-to-end test of model_train script.""""""\n    checkpoint_dir = tf_test_utils.test_tmpdir(\'inception_v3_warm_up_from\')\n    tf_test_utils.write_fake_checkpoint(\'inception_v3\', self.test_session(),\n                                        checkpoint_dir)\n    self._run_tiny_training(\n        model_name=\'inception_v3\',\n        dataset=data_providers_test.make_golden_dataset(use_tpu=FLAGS.use_tpu),\n        warm_start_from=checkpoint_dir + \'/model\')\n\n  @flagsaver.FlagSaver\n  def test_end2end_inception_v3_failed_warm_up_from(self):\n    """"""End-to-end test of model_train script with a non-existent path.""""""\n    with self.assertRaises(tf.errors.OpError):\n      self._run_tiny_training(\n          model_name=\'inception_v3\',\n          dataset=data_providers_test.make_golden_dataset(\n              use_tpu=FLAGS.use_tpu),\n          warm_start_from=\'this/path/does/not/exist\')\n\n  @flagsaver.FlagSaver\n  def test_end2end_inception_v3_embedding_invalid_embedding_size(self):\n    """"""End-to-end test of model_train script with an invalid embedding size.""""""\n    with six.assertRaisesRegex(\n        self, ValueError, \'Expected seq_type_embedding_size \'\n        \'to be a positive number but saw -100 \'\n        \'instead.\'):\n      FLAGS.seq_type_embedding_size = -100\n      self._run_tiny_training(\n          model_name=\'inception_v3_embedding\',\n          dataset=data_providers_test.make_golden_dataset(\n              use_tpu=FLAGS.use_tpu))\n\n  @parameterized.parameters((False), (True))\n  @flagsaver.FlagSaver\n  @mock.patch(\'deepvariant.model_train.\'\n              \'tf.compat.v1.train.replica_device_setter\')\n  @mock.patch(\'deepvariant.model_train.run\')\n  def test_main_internal(self, use_tpu, mock_run, mock_device_setter):\n    FLAGS.master = \'some_master\'\n    FLAGS.use_tpu = use_tpu\n    FLAGS.ps_tasks = 10\n    FLAGS.task = 5\n\n    model_train.parse_and_run()\n\n    mock_device_setter.assert_called_once_with(10)\n    mock_run.assert_called_once_with(\n        \'some_master\' if use_tpu else \'\',\n        False,\n        device_fn=mock.ANY,\n        use_tpu=mock.ANY)\n\n  @mock.patch(\'deepvariant.model_train.os.environ\')\n  @mock.patch(\'deepvariant.model_train.\'\n              \'tf.compat.v1.train.replica_device_setter\')\n  @mock.patch(\'deepvariant.model_train.run\')\n  def test_main_tfconfig_local(self, mock_run, mock_device_setter,\n                               mock_environ):\n    mock_environ.get.return_value = \'{}\'\n    model_train.parse_and_run()\n\n    mock_device_setter.assert_called_once_with(0)\n    mock_run.assert_called_once_with(\n        \'\', True, device_fn=mock.ANY, use_tpu=mock.ANY)\n\n  @parameterized.named_parameters(\n      (\'master\', \'master\', 0, True, \'/job:master/task:0\'),\n      (\'worker\', \'worker\', 10, False, \'/job:worker/task:10\'),\n  )\n  @mock.patch(\n      \'deepvariant.model_train.tf.distribute.Server\')\n  @mock.patch(\'deepvariant.model_train.os.environ\')\n  @mock.patch(\'deepvariant.model_train.\'\n              \'tf.compat.v1.train.replica_device_setter\')\n  @mock.patch(\'deepvariant.model_train.run\')\n  def test_main_tfconfig_dist(self, job_name, task_index, expected_is_chief,\n                              expected_worker, mock_run, mock_device_setter,\n                              mock_environ, mock_server):\n    tf_config = {\n        \'cluster\': {\n            \'ps\': [\'ps1:800\', \'ps2:800\']\n        },\n        \'task\': {\n            \'type\': job_name,\n            \'index\': task_index,\n        },\n    }\n\n    class FakeServer(object):\n      target = \'some-target\'\n\n    mock_environ.get.return_value = json.dumps(tf_config)\n    mock_server.return_value = FakeServer()\n\n    model_train.parse_and_run()\n\n    mock_device_setter.assert_called_once_with(\n        2, worker_device=expected_worker, cluster=mock.ANY)\n    mock_run.assert_called_once_with(\n        \'some-target\', expected_is_chief, device_fn=mock.ANY, use_tpu=mock.ANY)\n\n  @parameterized.parameters(\n      (\'master\', \'some-master\'),\n      (\'task\', 10),\n      (\'ps_tasks\', 5),\n  )\n  @flagsaver.FlagSaver\n  @mock.patch(\'deepvariant.model_train.os.environ\')\n  def test_main_invalid_args(self, flag_name, flag_value, mock_environ):\n    # Ensure an exception is raised if flags and TF_CONFIG are set.\n    tf_config = {\n        \'cluster\': {\n            \'ps\': [\'ps1:800\', \'ps2:800\']\n        },\n        \'task\': {\n            \'type\': \'master\',\n            \'index\': 0,\n        },\n    }\n\n    mock_environ.get.return_value = json.dumps(tf_config)\n    setattr(FLAGS, flag_name, flag_value)\n    self.assertRaises(ValueError, model_train.parse_and_run)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/modeling.py,148,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Provides an abstraction around deep learning models in DeepVariant.\n\nThis class allows us to encapsulate all of the model management, loading,\nsaving, and data processing in a single place so those details don\'t spill over\ninto the more general deepvariant codebase. The key thing we are aiming for here\nis to make sure we can easily play with other model architectures without\nmodifying the surrounding training and evaluation code.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport math\n\n\n\nfrom absl import flags\nfrom absl import logging\nimport enum\n\nimport tensorflow as tf\nimport tf_slim\nfrom deepvariant import dv_constants\nfrom deepvariant import tf_utils\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.tpu import tpu_config\nfrom tensorflow.python.tpu import tpu_estimator\nfrom tensorflow.python.tpu import tpu_optimizer\n# pylint: enable=g-direct-tensorflow-import\nfrom tf_slim.nets import inception\n\ntf.compat.v1.disable_eager_execution()\n\nflags.DEFINE_float(\n    \'label_smoothing\', 1e-6,\n    \'Amount of label smoothing to use. By default this is 0.0001% \'\n    \'meaning that we expect a label error at a rate of 1 / 1,000,000\')\n\n# Training parameters.\nflags.DEFINE_float(\'learning_rate\', 0.064, \'Initial learning rate.\')\n\nflags.DEFINE_float(\'rmsprop_momentum\', 0.9, \'Momentum.\')\n\nflags.DEFINE_float(\'rmsprop_decay\', 0.9, \'Decay term for RMSProp.\')\n\nflags.DEFINE_float(\'rmsprop_epsilon\', 1.0, \'Epsilon term for RMSProp.\')\n\nflags.DEFINE_float(\'learning_rate_decay_factor\', 0.94,\n                   \'Learning rate decay factor.\')\n\nflags.DEFINE_float(\'num_epochs_per_decay\', 2.0,\n                   \'Number of epochs after which learning rate decays.\')\n\nflags.DEFINE_float(\'moving_average_decay\', 0.9999,\n                   \'The decay to use for the moving average.\')\n\nflags.DEFINE_integer(\n    \'save_summary_steps\', 100, \'Number of steps which must have run before \'\n    \'showing summaries.\')\n\nflags.DEFINE_integer(\n    \'save_interval_secs\', 60 * 10,\n    \'Interval (in seconds) at which the model data \'\n    \'should be checkpointed. Set to 0 to disable, -1 to ignore. \'\n    \'Exclusive with save_interval_steps.\')\n\nflags.DEFINE_integer(\n    \'save_interval_steps\', -1, \'Interval (in steps) at which the model data \'\n    \'should be checkpointed. Set to 0 to disable, -1 to ignore. \'\n    \'Exclusive with save_interval_secs.\')\n\nflags.DEFINE_integer(\n    \'seq_type_embedding_size\', 200,\n    \'Set the embedding size for the sequencing type embeddings. Default is 200. \'\n    \'This flag is only useful when model_name is `inception_v3_embedding`.\')\n\nFLAGS = flags.FLAGS\n\nslim = tf_slim\n\n\nclass UnsupportedImageDimensionsError(Exception):\n  """"""Exception indicating the image dimensions aren\'t supported by our model.""""""\n\n\ndef binarize(labels, target_class):\n  """"""Binarize labels and predictions.\n\n  The labels that are not equal to target_class parameter are set to zero.\n\n  Args:\n    labels: the ground-truth labels for the examples.\n    target_class: index of the class that is left as non-zero.\n\n  Returns:\n    Tensor of the same shape as labels.\n  """"""\n  labels_binary = tf.compat.v1.where(\n      tf.equal(labels, tf.constant(target_class, dtype=tf.int64)),\n      tf.zeros_like(labels), labels)\n  return labels_binary\n\n\ndef get_class_recall(labels, predicted_class, target_class):\n  """"""Compute recall from labels and predicted_class for target_class.\n\n  Examples with label target_class are positives. Other classes are negatives.\n\n  Args:\n    labels: the ground-truth labels for the examples.\n    predicted_class: the predicted labels for the examples.\n    target_class: index of the class that is left as non-zero.\n\n  Returns:\n    Tensor containing the recall value.\n  """"""\n  labels_binary = binarize(labels, target_class)\n  predicted_class_binary = binarize(predicted_class, target_class)\n  return tf.compat.v1.metrics.recall(labels_binary, predicted_class_binary)\n\n\ndef get_class_precision(labels, predicted_class, target_class):\n  """"""Compute precision from labels and predicted_class for target_class.\n\n  Examples with label target_class are positives. Other classes are negatives.\n\n  Args:\n    labels: the ground-truth labels for the examples.\n    predicted_class: the predicted labels for the examples.\n    target_class: index of the class that is left as non-zero.\n\n  Returns:\n    Tensor containing the precision value.\n  """"""\n  labels_binary = binarize(labels, target_class)\n  predicted_class_binary = binarize(predicted_class, target_class)\n  return tf.compat.v1.metrics.precision(labels_binary, predicted_class_binary)\n\n\n# redacted\ndef get_f1_score(labels, predictions, target_class=None):\n  """"""Compute F1 score of predictions with respect to the labels.\n\n  Args:\n    labels: tensor whose dimensions must match predictions. The ground-truth\n      labels for the examples.\n    predictions: tensor of arbitrary dimension. The predicted labels for the\n      examples.\n    target_class: int. Index of the class that is left as non-zero.\n\n  Returns:\n    f1_score: scalar float tensor whose dimensions match predictions. The\n    calculated f1 score.\n    update_op: operation that updates the f1 score streaming metric.\n  """"""\n  if target_class:\n    labels = binarize(labels, target_class)\n    predictions = binarize(predictions, target_class)\n\n  precision, precision_op = tf.compat.v1.metrics.precision(labels, predictions)\n  recall, recall_op = tf.compat.v1.metrics.recall(labels, predictions)\n\n  def compute_f1_score(name):\n    pr_product = tf.multiply(precision, recall)\n    return tf.compat.v1.div(\n        tf.multiply(2.0, pr_product), tf.add(tf.add(precision, recall), 1e-12),\n        name)\n\n  f1_score = compute_f1_score(\'value\')\n  with ops.control_dependencies([precision_op, recall_op]):\n    update_op = compute_f1_score(\'update_op\')\n\n  return f1_score, update_op\n\n\ndef is_encoded_variant_type(variant_types_tensor, type_to_select):\n  """"""Returns a bool tensor indicating which variant_types match type_to_select.\n\n  Args:\n    variant_types_tensor: Tensor of shape (batch_size, 1) containing\n      EncodedVariantType.value int64 values. Each element of this tensor should\n      be a EncodedVariantType.value int64 value indicating the type of the\n      variant.\n    type_to_select: EncodedVariantType. The type of variant we want to select.\n\n  Returns:\n    Tensor of shape (batch_size, 1) of type tf.bool. A True value indicates that\n    the variant_type at that position matched type_to_select. Has a False\n    otherwise.\n  """"""\n  return tf.equal(variant_types_tensor,\n                  tf.constant(type_to_select.value, dtype=tf.int64))\n\n\n# This dictionary contains a mapping from the human readable name of a metric\n# function (e.g., Accuracy) and its associated TensorFlow metric function. All\n# of the entries here will be stratified by variant_type in eval_metric_fn.\n_METRICS_FUNCS_BY_VARIANT_TYPE = {\n    \'Accuracy\': tf.compat.v1.metrics.accuracy,\n    \'Precision\': tf.compat.v1.metrics.precision,\n    \'Recall\': tf.compat.v1.metrics.recall,\n    \'FPs\': tf.compat.v1.metrics.false_positives,\n    \'FNs\': tf.compat.v1.metrics.false_negatives,\n    \'TPs\': tf.compat.v1.metrics.true_positives,\n    \'TNs\': tf.compat.v1.metrics.true_negatives,\n}\n\n# A set containing the names of the variant types we split our metrics by type\n# by. This data structure isn\'t a dictionary like it\'s neighbors because\n# eval_metric_fn requires special logic to compute the values here associated\n# with each of these names.\n_METRICS_BY_VARIANT_TYPE = {\'All\', \'SNPs\', \'Indels\'}\n\n# This dictionary contains a mapping from the human readable name of a genotype\n# class (e.g., Het) and its associated class label (e.g., 1). All of the entries\n# here will be stratified by genotype_class in eval_metric_fn.\n_METRICS_GENOTYPE_CLASSES = {\n    \'HomRef\': 0,\n    \'Het\': 1,\n    \'HomVar\': 2,\n}\n\n# This dictionary contains a mapping from the human readable name of a metric\n# function (e.g., Accuracy) and its associated metric function. All\n# of the entries here will be stratified by genotype class (e.g., Het) in\n# eval_metric_fn.\n_METRICS_FUNCS_BY_GENOTYPE_CLASS = {\n    \'Precision\': get_class_precision,\n    \'Recall\': get_class_recall,\n    \'F1\': get_f1_score,\n}\n\n\ndef _eval_name(metric_name, stratification_name):\n  return metric_name + \'/\' + stratification_name\n\n\nclass EvalMetricOrdering(enum.Enum):\n  """"""Enum capturing whether a better metric should be larger or smaller.""""""\n  BIGGER_IS_BETTER = 1\n  SMALLER_IS_BETTER = 2\n\n\ndef eval_function_metrics(has_variant_types=True):\n  """"""Gets the set of eval_metrics names and their directionality.\n\n  Args:\n    has_variant_types: bool. Will we be providing variant_type information\n      during eval so that we\'ll have metrics stratified by variant_type?\n\n  Returns:\n    dict mapping from a metric name string (e.g., ""F1/All"") and a\n    EvalMetricOrdering enum indicating whether larger metric values are better\n    or worse.\n  """"""\n  names = {_eval_name(\'F1\', \'All\'): EvalMetricOrdering.BIGGER_IS_BETTER}\n\n  if has_variant_types:\n    variant_type_names = _METRICS_BY_VARIANT_TYPE\n  else:\n    variant_type_names = {\'All\'}\n  for m, s in itertools.product(_METRICS_FUNCS_BY_VARIANT_TYPE,\n                                variant_type_names):\n    names[_eval_name(m, s)] = EvalMetricOrdering.BIGGER_IS_BETTER\n\n  for m, s in itertools.product(_METRICS_FUNCS_BY_GENOTYPE_CLASS,\n                                _METRICS_GENOTYPE_CLASSES):\n    names[_eval_name(m, s)] = EvalMetricOrdering.BIGGER_IS_BETTER\n\n  return names\n\n\n# NB. This includes only a subset of our usual metrics.\n# We\'ll add the rest back in a subsequent change.\ndef eval_metric_fn(labels, predictions, variant_types):\n  """"""Calculate eval metrics from Tensors, on CPU host.\n\n  Args:\n    labels: the ground-truth labels for the examples.\n    predictions: the predicted labels for the examples.\n    variant_types: variant types (int64 of EncodedVariantType.value) as a tensor\n      of (batch_size,) or None. The types of these variants. If None, no type\n      specific evals will be performed.\n\n  Returns:\n    A dictionary of string name to metric.\n  """"""\n  predicted_classes = tf.argmax(input=predictions, axis=1)\n\n  metrics = {}\n\n  # Add the metrics stratified by variant_type\n  weights_by_type = {\'All\': None}\n  if variant_types is not None:\n    weights_by_type[\'SNPs\'] = is_encoded_variant_type(\n        variant_types, tf_utils.EncodedVariantType.SNP)\n    weights_by_type[\'Indels\'] = is_encoded_variant_type(\n        variant_types, tf_utils.EncodedVariantType.INDEL)\n\n  for metric_name, metric_func in _METRICS_FUNCS_BY_VARIANT_TYPE.items():\n    for weight_name, weights in weights_by_type.items():\n      metrics[_eval_name(metric_name, weight_name)] = metric_func(\n          labels, predicted_classes, weights=weights)\n\n  # Add the metrics stratified by predicted class.\n  for metric_name, metric_func in _METRICS_FUNCS_BY_GENOTYPE_CLASS.items():\n    for class_name, class_value in _METRICS_GENOTYPE_CLASSES.items():\n      metrics[_eval_name(metric_name,\n                         class_name)] = metric_func(labels, predicted_classes,\n                                                    class_value)\n\n  # Special case F1/All to avoid a clash between the two different ways that we\n  # can compute Precision and Recall (e.g., get_class_precision vs.\n  # tf.compat.v1.metrics.precision.\n  metrics[_eval_name(\'F1\', \'All\')] = get_f1_score(labels, predicted_classes)\n\n  logging.info(\'Metrics are %s\', metrics.keys())\n\n  # Make sure our metrics are consistent with the expected names from\n  # eval_function_metrics.\n  expected_metrics = eval_function_metrics(\n      has_variant_types=variant_types is not None)\n  if set(expected_metrics) != set(metrics):\n    raise AssertionError(\n        \'Bug: actual metrics={} not equal to expected={}\'.format(\n            \',\'.join(metrics), \',\'.join(expected_metrics)))\n\n  return metrics\n\n\n# The following two classes support loading exponential moving averages into\n# their corresponding variables when a checkpoint is loaded. They\'re called\n# as hooks by the Estimators. Note for future work: this is the documented\n# way, but someone on the mailing list suggested that using the scaffold_fn\n# mechanism might be better.\n\n\nclass LoadEMAHook(tf.estimator.SessionRunHook):\n  """"""Hook to load EMA into their corresponding variables.\n\n  This looks for the latest checkpoint in the model dir.\n  """"""\n\n  def __init__(self, model_dir, ignore_missing_vars=False):\n    super(LoadEMAHook, self).__init__()\n    self._model_dir = model_dir\n    self._ignore_missing_vars = ignore_missing_vars\n\n  def begin(self):\n    ema = tf.train.ExponentialMovingAverage(FLAGS.moving_average_decay)\n    variables_to_restore = ema.variables_to_restore()\n    self._load_ema = slim.assign_from_checkpoint_fn(\n        tf.train.latest_checkpoint(self._model_dir),\n        variables_to_restore,\n        ignore_missing_vars=self._ignore_missing_vars)\n\n  def after_create_session(self, sess, coord):\n    tf.compat.v1.logging.info(\'Reloading EMA...\')\n    self._load_ema(sess)\n\n\nclass PredictEMAHook(tf.estimator.SessionRunHook):\n  """"""Hook to load EMA into their corresponding variables.\n\n  This reads the specified checkpoint.\n  """"""\n\n  def __init__(self, checkpoint_path, ignore_missing_vars=False):\n    super(PredictEMAHook, self).__init__()\n    self._checkpoint_path = checkpoint_path\n    self._ignore_missing_vars = ignore_missing_vars\n\n  def begin(self):\n    ema = tf.train.ExponentialMovingAverage(FLAGS.moving_average_decay)\n    variables_to_restore = ema.variables_to_restore()\n    self._load_ema = slim.assign_from_checkpoint_fn(\n        self._checkpoint_path,\n        variables_to_restore,\n        ignore_missing_vars=self._ignore_missing_vars)\n\n  def after_create_session(self, sess, coord):\n    tf.compat.v1.logging.info(\'Reloading EMA...\')\n    self._load_ema(sess)\n\n\nclass DeepVariantModel(object):\n  """"""Base class for models that compute genotype likelihoods from an image.\n\n  This class is intended for use anywhere in DeepVariant where we want to train\n  or evaluate a model that computes genotype likelihoods from a pileup image. A\n  bit of encapsulation helps us to try new models (beyond inception_v3) and unit\n  test our code.\n\n  The base class cannot be used directly; concrete subclasses actually implement\n  specific models and all of the associated machinery to create/load/save\n  models.\n\n  Attributes:\n    name: str. The name of this model, such as `inception_v3`.\n    pretrained_model_path: str. Path to a root checkpoint where we can start\n      training the model, if we are not starting from scratch.\n    supported_dimensions_message: str. A human-readable string containing info\n      about what image dimensions are supported by this model. E.g., ""only\n      widths between 42 and 189"".\n    use_tpu: bool or None. If True, we are executing the model on a TPU, False\n      if we are using some other hardware. If None, the execution hardware is\n      not yet known.\n    model_dir: str or None. The path to the location where model checkpoint are\n      being stored. If None, the path hasn\'t been set yet or is unknown.\n  """"""\n\n  def __init__(self, name, pretrained_model_path):\n    """"""Creates a new DeepVariantModel with name and pretrained_model_path.\n\n    Args:\n      name: str. The name of the model. Passed to DeepVariantModel name.\n      pretrained_model_path: str. A path to a pretrained model to initialize our\n        network from when starting from the \'model_default\'.  If None, training\n        will start from randomly-initialized parameters.\n\n    Raises:\n      ValueError: if any of the arguments is invalid.\n    """"""\n    if not name:\n      raise ValueError(\'Got an empty value for name\', name)\n    self.name = name\n    self.pretrained_model_path = pretrained_model_path\n    self.supported_dimensions_message = \'unknown\'\n    self.use_tpu = None\n\n    # Set the model_dir to None by default. We capture its actual value during\n    # a call to make_estimator below.\n    self.model_dir = None\n\n  def construct_scalar_host_call(self,\n                                 metric_dict,\n                                 model_dir,\n                                 prefix=\'\',\n                                 record_frequency_in_steps=100):\n    """"""Construct a host call to log scalars when training on TPU.\n\n    Args:\n      metric_dict: A dict of the tensors to be logged.\n      model_dir: The location to write the summary.\n      prefix: The prefix (if any) to prepend to the metric names.\n      record_frequency_in_steps: int; How often should we log our metrics in\n        step units.\n\n    Returns:\n      A tuple of (function, args_to_be_passed_to_said_function)\n    """"""\n    # type: (dict, str) -> (function, list)\n    metric_names = list(metric_dict.keys())\n\n    def host_call_fn(global_step, *args):\n      """"""Training host call.\n\n      Creates scalar summaries for training metrics.\n\n      This function is executed on the CPU and should not directly reference\n      any Tensors in the rest of the `model_fn`. To pass Tensors from the\n      model to the `metric_fn`, provide as part of the `host_call`. See\n      https://www.tensorflow.org/api_docs/python/tf/compat/v1/estimator/tpu/TPUEstimator\n      for more information.\n      Arguments should match the list of `Tensor` objects passed as the second\n      element in the tuple passed to `host_call`.\n      Args:\n        global_step: Tensor with shape `[batch]` for the global_step\n        *args: Remaining tensors to log.\n\n      Returns:\n        List of summary ops to run on the CPU host.\n      """"""\n      step = global_step[0]\n      with tf.compat.v2.summary.create_file_writer(\n          logdir=model_dir, filename_suffix=\'.host_call\').as_default():\n        with tf.compat.v2.summary.record_if(\n            lambda: tf.math.equal(step % record_frequency_in_steps, 0)):\n          for i, name in enumerate(metric_names):\n            tf.compat.v2.summary.scalar(\n                name=prefix + name, data=args[i][0], step=step)\n          return tf.compat.v1.summary.all_v2_summary_ops()\n\n    # To log the current learning rate, and gradient norm for Tensorboard, the\n    # summary op needs to be run on the host CPU via host_call. host_call\n    # expects [batch_size, ...] Tensors, thus reshape to introduce a batch\n    # dimension. These Tensors are implicitly concatenated to\n    # [params[\'batch_size\']].\n    global_step_tensor = tf.reshape(\n        tf.compat.v1.train.get_or_create_global_step(), [1])\n    other_tensors = [tf.reshape(metric_dict[key], [1]) for key in metric_names]\n\n    return host_call_fn, [global_step_tensor] + other_tensors\n\n  def _create_warm_start_settings(self, start_from_checkpoint):\n    """"""Create a proper WarmStartSettings based on start_from_checkpoint.""""""\n    # If the special value ""model_default"" was passed, ask the model for\n    # its default.\n    if start_from_checkpoint == \'model_default\':\n      start_from_checkpoint = self.pretrained_model_path\n\n    # If the path is non-False, use it.\n    if start_from_checkpoint:\n      logging.info(\'Initializing model from checkpoint at %s\',\n                   start_from_checkpoint)\n      reader = tf.compat.v1.train.NewCheckpointReader(start_from_checkpoint)\n      var_to_shape_map = reader.get_variable_to_shape_map()\n      if tf_utils.model_num_classes(\n          start_from_checkpoint,\n          self.n_classes_model_variable) == dv_constants.NUM_CLASSES:\n        logging.info(\'The model checkpoint to warm start from has the same \'\n                     \'number of classes. If this is in training, we will \'\n                     \'clear excluded_scopes_for_incompatible_shapes so we \'\n                     \'include everything for \'\n                     \'warm starting....\')\n        vars_to_include = var_to_shape_map.keys()\n      else:\n        logging.info(\n            \'The model checkpoint to warm start from has different \'\n            \'number of classes. If this is in training, we will \'\n            \'use excluded_scopes_for_incompatible_shapes=%s\',\n            self.excluded_scopes_for_incompatible_shapes)\n        vars_to_include = [\n            v for v in var_to_shape_map.keys() if not v.startswith(\n                tuple(self.excluded_scopes_for_incompatible_shapes))\n        ]\n      return tf.estimator.WarmStartSettings(\n          ckpt_to_initialize_from=start_from_checkpoint,\n          vars_to_warm_start=\'|\'.join(vars_to_include))\n    else:\n      # If warm_start_from is an empty string, specifically set it to None.\n      logging.info(\'Initializing model with random parameters\')\n      return None\n\n  def make_estimator(self,\n                     batch_size,\n                     model_dir=None,\n                     max_checkpoints_to_keep=100000,\n                     iterations_per_loop=100,\n                     params=None,\n                     unused_device_fn=None,\n                     master=\'\',\n                     use_tpu=False,\n                     start_from_checkpoint=None,\n                     session_config=None):\n    """"""Returns a new tf.estimator.Estimator object for training or prediction.\n\n    The estimator needs to know batch_size. We use the same value for all\n    of eval, train, and predict. The estimator will automatically save\n    checkpoints to model_dir and keep the specified number of them. The value\n    of iterations_per_loop is not critical, and we default to the recommended\n    value. Some optional arguments are only required for use with TPU.\n\n    This function will use self.model_fn and self.use_tpu when constructing the\n    model specific Estimator object.\n\n    Estimators are also sometimes called classifiers.\n\n    Args:\n      batch_size: the batch size to use (for TRAIN, EVAL, and PREDICT modes).\n      model_dir: an (optional) string directory to use as the model directory.\n      max_checkpoints_to_keep: an (optional) integer count of saved checkpoints.\n      iterations_per_loop: an (optional) integer count of log_step_count_steps.\n      params: an (optional) dictionary of parameters to pass to the Estimator\n        constructor.\n      unused_device_fn: a device_fn to pass to RunConfig, if not use_tpu.\n      master: a string necessary for TPU, pass FLAGS.master through.\n      use_tpu: boolean.  set self.use_tpu if not None.\n      start_from_checkpoint: string. If not None, initialize model from this\n        path. According to the current implementation of Estimator, this will\n        only be used in training. The inference checkpoint is loaded in a\n        different place.\n      session_config: a tf.ConfigProto to pass to RunConfig, if not use_tpu.\n\n    Returns:\n      an object implementing the tf.estimator.Estimator interface (will be a\n      TPUEstimator if self.use_tpu is True).\n    """"""\n    if use_tpu is not None:\n      self.use_tpu = use_tpu\n\n    # Set the model dir of this class to the model_dir passed in here. It\'s not\n    # so clean but it appears to be necessary due to the way estimators are\n    # constructed (i.e., model_dir is set late).\n    self.model_dir = model_dir\n\n    # These flags are exclusive if not None, and 0 means disable.\n    save_checkpoints_secs = None\n    save_checkpoints_steps = None\n    if FLAGS.save_interval_secs >= 0:\n      save_checkpoints_secs = FLAGS.save_interval_secs\n    if FLAGS.save_interval_steps >= 0:\n      save_checkpoints_steps = FLAGS.save_interval_steps\n\n    params = params if params is not None else {}\n    warm_start_from = self._create_warm_start_settings(start_from_checkpoint)\n    if self.use_tpu:\n      tpu_cfg=tpu_config.TPUConfig(\n          iterations_per_loop=iterations_per_loop)\n      config = tpu_config.RunConfig(\n          master=master,\n          evaluation_master=master,\n          model_dir=model_dir,\n          log_step_count_steps=iterations_per_loop,\n          keep_checkpoint_max=max_checkpoints_to_keep,\n          save_checkpoints_secs=save_checkpoints_secs,\n          save_checkpoints_steps=save_checkpoints_steps,\n          save_summary_steps=FLAGS.save_summary_steps,\n          tpu_config=tpu_cfg)\n\n      classifier = tpu_estimator.TPUEstimator(\n          use_tpu=self.use_tpu,\n          model_fn=self.model_fn,\n          config=config,\n          # redacted\n          train_batch_size=batch_size,\n          eval_batch_size=batch_size,\n          predict_batch_size=batch_size,\n          params=params,\n          warm_start_from=warm_start_from,\n      )\n    else:\n      config = tf.estimator.RunConfig(\n          model_dir=model_dir,\n          log_step_count_steps=iterations_per_loop,\n          keep_checkpoint_max=max_checkpoints_to_keep,\n          # device_fn=device_fn,  # Not in tf1.8?\n          save_checkpoints_secs=save_checkpoints_secs,\n          save_checkpoints_steps=save_checkpoints_steps,\n          save_summary_steps=FLAGS.save_summary_steps,\n          session_config=session_config,\n      )\n      # The TPUEstimator interface implicitly adds batch_size to the params\n      # dict. Do so explicitly here, so that we can use the same model_fn.\n      params_with_batch_size = {\'batch_size\': batch_size}\n      params_with_batch_size.update(params)\n\n      classifier = tf.estimator.Estimator(\n          model_fn=self.model_fn,\n          config=config,\n          params=params_with_batch_size,\n          warm_start_from=warm_start_from)\n    return classifier\n\n  def model_fn(self, features, labels, mode, params):\n    """"""A model_fn satisfying the Estimator API.\n\n    Args:\n      features: a dictionary supplying features.\n      labels: a tensor of labels.\n      mode: one of tf.estimator.ModeKeys.{EVAL,TRAIN}\n      params: a dictionary of parameters.\n\n    Returns:\n      a tf.estimator.EstimatorSpec or tpu_estimator.TPUEstimatorSpec,\n      depending on self.use_tpu.\n    """"""\n    raise NotImplementedError\n\n  def session_eval_hooks(self):\n    """"""Returns a list of tf.train.SessionRunHook classes.\n\n    A typical use case is to provide a hook to load the EMA variables.\n\n    These will be instantiated and invoked by\n      eval_hooks = [\n          h(model_dir) for h in model.session_eval_hooks()\n      ]\n      estimator.evaluate(hooks=...).\n\n    Note that this is done according to the instructions in\n    cloud_tpu/models/inception/inception_v3.py. A newer idea is in\n    tpuestimator-scaffold, but we haven\'t tried that approach.\n    """"""\n    return []\n\n  def session_predict_hooks(self):\n    """"""Returns a list of tf.train.SessionRunHook classes.\n\n    A typical use case is to provide a hook to load the EMA variables.\n\n    These will be instantiated and invoked by\n      predict_hooks = [\n          h(checkpoint_path) for h in model.session_predict_hooks()\n      ]\n      estimator.predict(hooks=...).\n\n    Note that this is done according to the instructions in\n    cloud_tpu/models/inception/inception_v3.py. A newer idea is in\n    tpuestimator-scaffold, but we haven\'t tried that approach.\n    """"""\n    return []\n\n  def create(self, images, num_classes, is_training):\n    """"""Creates a new model.\n\n    Args:\n      images: A 4-D tensor of (batch_size, height, width, channels) of pileup\n        images.\n      num_classes: integer. How many prediction classes are we expecting in\n        model?\n      is_training: boolean. Should we setup model for training (True) or for\n        inference (False).\n\n    Returns:\n      A dictionary, containing string keys mapped to endpoint tensors of this\n      model. The dictionary must contain a key \'Predictions\' that contains the\n      probability of having each of \'num_classes\' classes.\n    """"""\n    try:\n      return self._create(images, num_classes, is_training)\n    except (ValueError, tf.errors.OpError) as e:\n      if self._is_bad_image_dimension_exception(e):\n        _, height, width, _ = images.get_shape().as_list()\n        message = (\n            \'Unsupported image dimensions detected: model {} was given images \'\n            \'of w={} x h={} but a TensorFlow exception occurred while building \'\n            \'the model, which typically indicates those dimensions are not \'\n            \'supported by the model. The supported dimensions for {} are {}\'\n        ).format(self.name, width, height, self.name,\n                 self.supported_dimensions_message)\n        raise UnsupportedImageDimensionsError(message)\n      else:\n        raise\n\n  def _is_bad_image_dimension_exception(self, exception):\n    return any(\n        x in str(exception) for x in [\'Negative dimension\', \'SpatialSqueeze\'])\n\n  def _create(self, images, num_classes, is_training):\n    """"""To be overloaded by subclasses to actually create the model.""""""\n    raise NotImplementedError\n\n  def preprocess_images(self, images):\n    """"""Preprocessing steps needed for this model to process a batch of images.\n\n    Args:\n      images: A (batch_size, height, width, channels) 4-D Tensor of type uint8.\n\n    Returns:\n      A new batch of images, potentially with different dimensions, based on the\n      input but transformed as necessary to use with this model.\n    """"""\n    raise NotImplementedError\n\n  @property\n  def is_trainable(self):\n    """"""Returns True if this model can be trained.""""""\n    return True\n\n  # redacted\n\n  def __str__(self):\n    return \'DeepVariantModel(name={})\'.format(self.name)\n\n  def variables_to_restore_from_model(self, exclude_scopes=None):\n    """"""Gets the list of model variables that should be restored.\n\n    The primary use of this function is to get a subset of tf.Variables from a\n    slim-defined model that we\'d like to restore from a checkpoint. The\n    checkpoint generally contains all of the variables in the graph during\n    training, including things like the backprop variables, moving averages for\n    visualization, etc. Simply restoring all of those variables is brittle, as\n    we often want to start a new training run, maybe using a different\n    optimizer, different visualization variables, or replacing part of the model\n    with a new classification layer, as unneeded variables from the checkpoint\n    get loaded into the graph and/or new TF variables not present in the graph\n    cannot be found, raising exceptions. This function allows a clean API to get\n    just the *model* variables from a graph, excluding all of those non-model\n    variables, along with optionally removing parts of the model graph via\n    exclude scopes.\n\n    This function calls slim.get_model_variables() to get the raw list of all\n    variables associated with the MODEL_VARIABLES collection. It then filters\n    away all variables that match any of the scopes in exclude_scopes. For\n    example, suppose we have a model with three variables with names:\n\n      w1 = model/l1/weight1\n      w2 = model/l2/weight2\n      w3 = model/l2/weight3\n\n    Without any exclude scopes, we would return these three variables [w1, w2,\n    and w3]. Providing exclude_scopes=[\'model/l2\'] would return only [w1], while\n    exclude_scopes=[\'model/l1\'] would return [w2, w3].\n\n    Args:\n      exclude_scopes: None, or a list of strings. Each string is a scope\n        specification, such as ""model/l1"" to match all variables whose name\n        starts with ""model/l1"".\n\n    Returns:\n      A list of tf.Variable objects.\n    """"""\n    vars_to_include = slim.get_model_variables()\n    # We aren\'t excluding any variables, so just return vars_to_include.\n    if not exclude_scopes:\n      return vars_to_include\n\n    vars_to_exclude = set()\n    for scope in exclude_scopes:\n      vars_to_exclude |= set(slim.get_variables(scope))\n    return [v for v in vars_to_include if v not in vars_to_exclude]\n\n\nclass DeepVariantSlimModel(DeepVariantModel):\n  """"""Baseclass for DeepVariant models based on Slim networks.""""""\n\n  def __init__(self, name, pretrained_model_path, n_classes_model_variable,\n               excluded_scopes_for_incompatible_shapes):\n    """"""Creates an DeepVariant CNN network based on a tf.slim model.\n\n    Args:\n      name: see baseclass.\n      pretrained_model_path: see baseclass.\n      n_classes_model_variable: str. A fully-qualitified TF variable name in the\n        model that we can use to determine the shape of the output\n        classification layer of the model. For example, in inception-v3 from\n        slim this is \'InceptionV3/Logits/Conv2d_1c_1x1/weights\'.\n      excluded_scopes_for_incompatible_shapes: list of str. A list of scopes\n        that will be excluded when restoring from a checkpoint to avoid loading\n        incompatible shapes.\n\n    Raises:\n      ValueError: If any of the arguments are invalid.\n    """"""\n    super(DeepVariantSlimModel, self).__init__(\n        name=name, pretrained_model_path=pretrained_model_path)\n    if not excluded_scopes_for_incompatible_shapes:\n      raise ValueError(\n          \'Got an empty value for \'\n          \'excluded_scopes_for_incompatible_shapes\',\n          excluded_scopes_for_incompatible_shapes)\n    self.n_classes_model_variable = n_classes_model_variable\n    self.excluded_scopes_for_incompatible_shapes = (\n        excluded_scopes_for_incompatible_shapes)\n\n  def preprocess_images(self, images):\n    """"""Applies preprocessing operations for Inception images.\n\n    Because this will run in model_fn, on the accelerator, we use operations\n    that efficiently execute there.\n\n    Args:\n      images: An Tensor of shape [batch_size height, width, channel] with uint8\n        values.\n\n    Returns:\n      A tensor of images of shape [batch_size height, width, channel]\n      containing floating point values, with all points rescaled between\n      -1 and 1 and possibly resized.\n    """"""\n    images = tf.cast(images, dtype=tf.float32)\n    images = tf.subtract(images, 128.0)\n    images = tf.compat.v1.div(images, 128.0)\n    return images\n\n  def model_fn(self, features, labels, mode, params):\n    """"""A model_fn for slim (really inception_v3), satisfying the Estimator API.\n\n    Args:\n      features: a single Tensor or dict of same (from input_fn).\n      labels: a single Tensor or dict of same (from input_fn).\n      mode: tf.estimator.ModeKeys.\n      params: dict.\n\n    Returns:\n      EstimatorSpec or TPUEstimatorSpec depending on self.use_tpu.\n    """"""\n    # NB. The basic structure of this started from\n    # //third_party/cloud_tpu/models/inception/inception_v3.py\n\n    # redacted\n    num_classes = dv_constants.NUM_CLASSES\n\n    images = features[\'image\']\n    images = self.preprocess_images(images)\n\n    endpoints = self.create(\n        images=images,\n        num_classes=num_classes,\n        is_training=mode == tf.estimator.ModeKeys.TRAIN)\n\n    logits = endpoints[\'Logits\']\n\n    predictions = endpoints\n    predictions.update({\n        \'classes\': tf.argmax(input=logits, axis=1, output_type=tf.int32),\n        \'probabilities\': tf.nn.softmax(logits, name=\'softmax_tensor\')\n    })\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      return self._model_fn_predict(mode, features, logits)\n\n    # Compute loss.\n    one_hot_labels = tf.one_hot(labels, num_classes, dtype=tf.int32)\n    tf.compat.v1.losses.softmax_cross_entropy(\n        onehot_labels=one_hot_labels,\n        logits=logits,\n        weights=1.0,\n        label_smoothing=FLAGS.label_smoothing)\n    total_loss = tf.compat.v1.losses.get_total_loss(\n        add_regularization_losses=True)\n    return self.make_ops_and_estimator(features, endpoints, labels, logits,\n                                       predictions, total_loss, mode, params)\n\n  def make_ops_and_estimator(self, features, endpoints, labels, logits,\n                             predictions, total_loss, mode, params):\n    """"""Make EstimatorSpec for the current model.\n\n    Args:\n      features: a single Tensor or dict of same (from input_fn).\n      endpoints:  a dictionary, containing string keys mapped to endpoint\n        tensors of this model. The dictionary must contain a key \'Predictions\'\n        that contains the probability of having each of \'num_classes\' classes.\n      labels: a single Tensor or dict of same (from input_fn).\n      logits: a single Tensor with logits\n      predictions: A dictionaty that must contain the following keys: \'Logits\'\n        and \'Predictions\'.\n      total_loss:  a single Tensor with a loss\n      mode: tf.estimator.ModeKeys.\n      params: dict.\n\n    Returns:\n      EstimatorSpec or TPUEstimatorSpec depending on self.use_tpu.\n    """"""\n    # Note, below, one of train_op or eval_metrics will be None, and the other\n    # will be populated, depending on mode.\n\n    # There are a lot of arguments here; that\'s to avoid referencing flags in\n    # leaf functions.\n    train_op, host_call = self._model_fn_train(\n        mode=mode,\n        total_loss=total_loss,\n        # get() here to be robust when we are in eval mode and batches_per_epoch\n        # hasn\'t been provided. In eval mode, model_fn_train will return without\n        # doing anything.\n        batches_per_epoch=params.get(\'batches_per_epoch\', None),\n        num_epochs_per_decay=FLAGS.num_epochs_per_decay,\n        initial_learning_rate=FLAGS.learning_rate,\n        learning_rate_decay_factor=FLAGS.learning_rate_decay_factor,\n        rmsprop_decay=FLAGS.rmsprop_decay,\n        rmsprop_momentum=FLAGS.rmsprop_momentum,\n        rmsprop_epsilon=FLAGS.rmsprop_epsilon,\n        moving_average_decay=FLAGS.moving_average_decay)\n\n    eval_metrics = self._model_fn_eval(\n        mode=mode,\n        features=features,\n        labels=labels,\n        endpoints=endpoints,\n        logits=logits,\n        use_logits=False)\n\n    spec = tpu_estimator.TPUEstimatorSpec(\n        mode=mode,\n        loss=total_loss,\n        train_op=train_op,\n        host_call=host_call,\n        eval_metrics=eval_metrics,\n        predictions=predictions)\n    if self.use_tpu:\n      return spec\n    else:\n      return spec.as_estimator_spec()\n\n  def _model_fn_predict(self, mode, features, logits):\n    """"""This is the PREDICT part of model_fn.""""""\n    assert mode == tf.estimator.ModeKeys.PREDICT\n    predictions = {\n        # We don\'t actually use classes downstream right now.\n        # \'classes\': tf.argmax(input=logits, axis=1, output_type=tf.int32),\n        \'probabilities\': tf.nn.softmax(logits, name=\'softmax_tensor\'),\n        # DV2 call_variants wants these passed through.\n        \'variant\': features[\'variant\'],\n        \'alt_allele_indices\': features[\'alt_allele_indices\'],\n    }\n    if \'label\' in features:\n      predictions[\'label\'] = features[\'label\']\n    if \'locus\' in features:\n      predictions[\'locus\'] = features[\'locus\']\n    if self.use_tpu:\n      return tpu_estimator.TPUEstimatorSpec(mode=mode, predictions=predictions)\n    else:\n      return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n  def _model_fn_eval(self, mode, features, labels, endpoints, logits,\n                     use_logits):\n    """"""This is the EVAL part of model_fn.""""""\n    if mode != tf.estimator.ModeKeys.EVAL:\n      return None\n    if use_logits:\n      eval_predictions = logits\n    else:\n      eval_predictions = endpoints[\'Predictions\']\n    variant_type = features[\'variant_type\']\n    eval_metrics = (eval_metric_fn, [labels, eval_predictions, variant_type])\n    if not self.use_tpu:\n      for name, value in eval_metrics[0](*eval_metrics[1]).items():\n        tf.compat.v1.summary.scalar(tensor=value, name=name)\n    return eval_metrics\n\n  def _model_fn_train(self, mode, total_loss, batches_per_epoch,\n                      num_epochs_per_decay, initial_learning_rate,\n                      learning_rate_decay_factor, rmsprop_decay,\n                      rmsprop_momentum, rmsprop_epsilon, moving_average_decay):\n    """"""This is the TRAIN part of model_fn.""""""\n    if mode != tf.estimator.ModeKeys.TRAIN:\n      return None, None\n\n    # Configure the learning rate using an exponetial decay.\n    global_step = tf.compat.v1.train.get_or_create_global_step()\n    current_epoch = tf.cast(global_step, tf.float32) / batches_per_epoch\n    decay_steps = int(1.0 * batches_per_epoch * num_epochs_per_decay)\n\n    learning_rate = tf.compat.v1.train.exponential_decay(\n        learning_rate=initial_learning_rate,\n        global_step=global_step,\n        decay_steps=decay_steps,\n        decay_rate=learning_rate_decay_factor,\n        staircase=True)\n\n    # Set a minimum boundary for the learning rate to be a fixed value of 1e-9.\n    # It\'s common to see these tf.max(...) operations when training inception,\n    # with a max of 1e-4 * initial_learning_rate but this makes it hard to\n    # explore learning rate schedules that decay quickly or by a lot of each\n    # step. Here we just use a very small constant 1e-9 as the minimum value.\n    learning_rate = tf.maximum(learning_rate, 1e-9, name=\'learning_rate\')\n\n    optimizer = tf.compat.v1.train.RMSPropOptimizer(\n        learning_rate,\n        rmsprop_decay,\n        momentum=rmsprop_momentum,\n        epsilon=rmsprop_epsilon)\n    if self.use_tpu:\n      optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)\n    update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(total_loss, global_step=global_step)\n\n    # NB. In the inception code this was ""tf.trainable_variables()\n    # + tf.moving_average_variables()"", but we\'ve settled on just\n    # tf.model_variables() in the existing production DV2.\n    variables_to_average = tf.compat.v1.model_variables()\n    variable_averages = tf.train.ExponentialMovingAverage(\n        decay=moving_average_decay, num_updates=global_step)\n    with tf.control_dependencies([train_op\n                                 ]), tf.compat.v1.name_scope(\'moving_average\'):\n      train_op = variable_averages.apply(variables_to_average)\n    tf.compat.v1.add_to_collection(tf.compat.v1.GraphKeys.UPDATE_OPS, train_op)\n\n    # Compute the current epoch and associated learning rate from global_step.\n    metric_dict = {\n        \'current_epoch\': current_epoch,\n        \'total_loss\': total_loss,\n        \'learning_rate\': learning_rate,\n    }\n    host_call = self.construct_scalar_host_call(\n        metric_dict=metric_dict, model_dir=self.model_dir, prefix=\'training/\')\n\n    return train_op, host_call\n\n  def session_eval_hooks(self):\n    return [LoadEMAHook]\n\n  def session_predict_hooks(self):\n    return [PredictEMAHook]\n\n\nclass DeepVariantInceptionV3(DeepVariantSlimModel):\n  """"""DeepVariant inception_v3 network.""""""\n\n  def __init__(self):\n    """"""Creates an inception-v3 network for DeepVariant.""""""\n    super(DeepVariantInceptionV3, self).__init__(\n        name=\'inception_v3\',\n        n_classes_model_variable=\'InceptionV3/Logits/Conv2d_1c_1x1/weights\',\n        excluded_scopes_for_incompatible_shapes=[\n            \'InceptionV3/Logits\', \'InceptionV3/Conv2d_1a_3x3\'\n        ],\n        pretrained_model_path=(\'/namespace/vale-project/models/classification/\'\n                               \'imagenet/inception_v3/model.ckpt-9591376\'))\n    self.supported_dimensions_message = (\n        \'odd widths between 75-361 and any heights between 75-362\')\n\n  def _create(self, images, num_classes, is_training):\n    """"""See baseclass.""""""\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n      _, endpoints = inception.inception_v3(\n          images, num_classes, create_aux_logits=False, is_training=is_training)\n      return endpoints\n\n\nclass DeepVariantInceptionV3Embedding(DeepVariantInceptionV3):\n  """"""DeepVariant inception_v3_embedding network.""""""\n\n  def __init__(self):\n    """"""Creates an inception_v3_embedding network for DeepVariant.""""""\n    super(DeepVariantInceptionV3Embedding, self).__init__()\n    self.name = \'inception_v3_embedding\'\n    # vocab_size should be a number larger than the number of sequencing types\n    self.vocab_size = 5\n    self.embedding_size = 200\n    self.dropout_keep_prob = 0.8\n\n  def _create(self, inputs, num_classes, is_training):\n    """"""Creates a new inception_v3_embedding model.\n\n    Args:\n      inputs: A tuple of two elements (images, sequencing_types). images is a\n        4-D tensor of (batch_size, height, width, channels) of pileup images.\n        sequencing_types is a 1-D tensor of (batch_size) of example sequencing\n        types.\n      num_classes: integer. How many prediction classes are we expecting in\n        model?\n      is_training: boolean. Should we setup model for training (True) or for\n        inference (False).\n\n    Returns:\n      A dictionary, containing string keys mapped to endpoint tensors of this\n      model.\n    """"""\n    images, sequencing_type = inputs\n    endpoints = super(DeepVariantInceptionV3Embedding,\n                      self)._create(images, num_classes, is_training)\n\n    with tf.compat.v1.variable_scope(\'Embeddings\'):\n      # Take the graph all the way till PreLogits\n      net = endpoints[\'PreLogits\']\n      net = slim.flatten(net)\n      embeddings = self._create_embeddings(sequencing_type)\n      net = tf.concat([net, embeddings], 1)\n\n      endpoints[\'Embeddings\'] = net\n\n    with tf.compat.v1.variable_scope(\'Logits\'):\n      if isinstance(net.shape[1], int):\n        hidden_size = net.shape[1] // 2\n      else:\n        hidden_size = net.shape[1].value // 2\n\n      net = slim.fully_connected(net, hidden_size, activation_fn=None)\n      # redacted\n      net = slim.layer_norm(net, scale=False, activation_fn=tf.nn.relu)\n      net = slim.dropout(net, self.dropout_keep_prob, is_training=is_training)\n      net = slim.fully_connected(net, num_classes, activation_fn=None)\n\n      endpoints.update({\'Logits\': net, \'Predictions\': tf.nn.softmax(net)})\n\n    return endpoints\n\n  def _create_embeddings(self, indices):\n    """"""Create word embeddings.""""""\n    embeddings = self._embedding_lookup(indices)\n    embeddings = slim.fully_connected(\n        embeddings, self.embedding_size, activation_fn=None)\n    return embeddings\n\n  def _embedding_lookup(self, input_ids, word_embedding_name=\'seq_type_emb\'):\n    """"""Looks up words embeddings for id tensor.\n\n    Args:\n      input_ids: int64 Tensor of shape [batch_size, ] containing word ids.\n      word_embedding_name: string. Name of the embedding table.\n\n    Returns:\n      float Tensor of shape [batch_size, embedding_size].\n    """"""\n    embedding_table = tf.compat.v1.get_variable(\n        name=word_embedding_name,\n        shape=[self.vocab_size, self.embedding_size],\n        initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n            scale=1.0, mode=\'fan_avg\', distribution=\'uniform\'),\n        collections=[\n            tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES,\n            tf.compat.v1.GraphKeys.MODEL_VARIABLES,\n            tf.compat.v1.GraphKeys.GLOBAL_VARIABLES\n        ])\n\n    return tf.nn.embedding_lookup(params=embedding_table, ids=input_ids)\n\n  def model_fn(self, features, labels, mode, params):\n    """"""A model_fn for slim, satisfying the Estimator API.\n\n    Args:\n      features: a single Tensor or dict of same (from input_fn).\n      labels: a single Tensor or dict of same (from input_fn).\n      mode: tf.estimator.ModeKeys.\n      params: dict.\n\n    Returns:\n      EstimatorSpec or TPUEstimatorSpec depending on self.use_tpu.\n\n    Raises:\n      ValueError: if FLAGS.seq_type_embedding_size is not positive.\n    """"""\n    # NB. The basic structure of this started from\n    # //third_party/cloud_tpu/models/inception/inception_v3.py\n\n    # redacted\n    num_classes = dv_constants.NUM_CLASSES\n\n    if FLAGS.seq_type_embedding_size <= 0:\n      raise ValueError(\n          \'Expected seq_type_embedding_size to be a positive number but saw %i \'\n          \'instead.\' % FLAGS.seq_type_embedding_size)\n    self.embedding_size = FLAGS.seq_type_embedding_size\n\n    images = features[\'image\']\n    images = self.preprocess_images(images)\n    sequencing_type = features[\'sequencing_type\']\n\n    endpoints = self.create(\n        images=(images, sequencing_type),\n        num_classes=num_classes,\n        is_training=mode == tf.estimator.ModeKeys.TRAIN)\n\n    logits = endpoints[\'Logits\']\n\n    predictions = endpoints\n    predictions.update({\n        \'classes\': tf.argmax(input=logits, axis=1, output_type=tf.int32),\n        \'probabilities\': tf.nn.softmax(logits, name=\'softmax_tensor\')\n    })\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      return self._model_fn_predict(mode, features, logits)\n\n    # Compute loss.\n    one_hot_labels = tf.one_hot(labels, num_classes, dtype=tf.int32)\n    tf.compat.v1.losses.softmax_cross_entropy(\n        onehot_labels=one_hot_labels,\n        logits=logits,\n        weights=1.0,\n        label_smoothing=FLAGS.label_smoothing)\n    total_loss = tf.compat.v1.losses.get_total_loss(\n        add_regularization_losses=True)\n\n    return self.make_ops_and_estimator(features, endpoints, labels, logits,\n                                       predictions, total_loss, mode, params)\n\n\nclass DeepVariantDummyModel(DeepVariantModel):\n  """"""BaseClass for dummy models that are useful for testing and benchmarking.""""""\n\n  def __init__(self, name):\n    """"""Creates a Dummy model.""""""\n    # Note the pretrained model path isn\'t used but we must return a valid\n    # string so here we just return ""UNUSED"".\n    super(DeepVariantDummyModel, self).__init__(\n        name=name, pretrained_model_path=\'UNUSED\')\n\n  def preprocess_images(self, images):\n    """"""Preprocess images for dummy model.""""""\n    # Note these calculations aren\'t necessary, but they are included here to\n    # mimic the data processing pipeline used by inception. We may consider\n    # removing them in a future CL, or making them optional, to reduce CPU cost\n    # of this model.\n    images = tf.cast(images, dtype=tf.float32)\n    images = tf.subtract(images, 128.0)\n    images = tf.compat.v1.div(images, 128.0)\n    return images\n\n  @property\n  def is_trainable(self):\n    """"""A dummy model cannot be trained.""""""\n    return False\n\n\nclass DeepVariantRandomGuessModel(DeepVariantDummyModel):\n  """"""Assigns a random probability to each class.\n\n  This model is mostly useful for testing of DeepVariant, as the evaluation of\n  this model is essentially free.\n  """"""\n\n  def __init__(self, seed=1268458594):\n    """"""Creates a RandomGuessing model.\n\n    Args:\n      seed: int. The random number seed to use for our tf.random_uniform op.\n    """"""\n    super(DeepVariantRandomGuessModel, self).__init__(name=\'random_guess\')\n    self.seed = seed\n\n  def _create(self, images, num_classes, is_training):\n    """"""The Random model emits a random uniform probability for each class.""""""\n    batch_size = tf.shape(input=images)[0]\n    rand_probs = tf.random.uniform(\n        shape=(batch_size, num_classes), seed=self.seed)\n    return {\'Predictions\': tf.nn.softmax(rand_probs)}\n\n  def model_fn(self, features, labels, mode, params):\n    """"""A model_fn for the random model.""""""\n    # redacted\n    num_classes = dv_constants.NUM_CLASSES\n\n    # In predict-mode the last batch may be smaller; so use the measured\n    # batch size.\n    encoded_variants = features[\'variant\']\n\n    tf.compat.v1.set_random_seed(self.seed)\n    rand_probs = tf.map_fn(\n        fn=lambda _: tf.random.uniform([num_classes]),\n        elems=features[\'image\'],\n        dtype=tf.float32,\n    )\n\n    # In a normal model we\'d call create to get the endpoints,\n    # but it\'s inconvenient to pass batch size in this case.\n    endpoints = {\'Predictions\': tf.nn.softmax(rand_probs)}\n\n    predictions = endpoints\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      predictions = {\n          \'probabilities\': endpoints[\'Predictions\'],\n          \'variant\': encoded_variants,\n          \'alt_allele_indices\': features[\'alt_allele_indices\'],\n      }\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n      eval_metrics = (eval_metric_fn, [\n          labels, endpoints[\'Predictions\'], endpoints[\'variant_type\']\n      ])\n    else:\n      eval_metrics = None\n\n    loss = tf.constant(0.0)\n    train_op = None\n    spec = tpu_estimator.TPUEstimatorSpec(\n        mode=mode,\n        loss=loss,\n        train_op=train_op,\n        eval_metrics=eval_metrics,\n        predictions=predictions)\n    if self.use_tpu:\n      return spec\n    else:\n      return spec.as_estimator_spec()\n\n\nclass DeepVariantConstantModel(DeepVariantDummyModel):\n  """"""Returns a constant probability distribution for each example.""""""\n\n  def __init__(self, predictions=None):\n    """"""Creates a constant model.\n\n    Args:\n      predictions: list[float]. Values to return for Predictions, which should\n        be a floatting point value between 0 and 1 for each class, normalized so\n        the sum of the values is 1. Predictions should have dimension\n        [num_classes].\n\n    Raises:\n      ValueError: if sum(predictions) is not close to 1.\n    """"""\n    # Note the pretrained model path isn\'t used but we must return a valid\n    # string so here we just return ""UNUSED"".\n    super(DeepVariantConstantModel, self).__init__(name=\'constant\')\n    if predictions is None:\n      self.predictions = [0.0, 1.0, 0.0]\n    elif math.abs(sum(predictions) - 1) > 1e-6:\n      raise ValueError(\'Sum of predictions should be ~1\', predictions)\n    else:\n      self.predictions = predictions\n\n  @staticmethod\n  def _predictions(pred_const, batch_size):\n    return {\n        \'Predictions\':\n            tf.reshape(\n                tf.tile(pred_const, [batch_size]),\n                shape=(batch_size, tf.shape(input=pred_const)[0]))\n    }\n\n  def _create(self, images, num_classes, is_training):\n    assert num_classes == len(self.predictions)\n    batch_size = tf.shape(input=images)[0]\n    pred_const = tf.constant(self.predictions)\n    return self._predictions(pred_const, batch_size)\n\n  def model_fn(self, features, labels, mode, params):\n    """"""A model_fn for the constant model.""""""\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      batch_size = tf.shape(input=features[\'image\'])[0]\n      logging.info(\'actual_batch_size %s\', batch_size)\n    else:\n      batch_size = params[\'batch_size\']\n      logging.info(\'batch_size %s\', batch_size)\n    pred_const = tf.constant(self.predictions)\n    endpoints = self._predictions(pred_const, batch_size)\n    encoded_variants = features[\'variant\']\n    # For the constant model, which is for testing only, we just set the\n    # variant_types to 0s. This is needed because it doesn\'t work to fetch\n    # \'variant_type\' from either features or endpoints here. Annoying.\n    # variant_types = features[\'variant_type\']     # Fails.\n    # variant_types = endpoints[\'variant_type\']    # Fails.\n    variant_types = tf.zeros(shape=(batch_size,), dtype=tf.int64)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      predictions = {\n          \'probabilities\': endpoints[\'Predictions\'],\n          \'variant\': encoded_variants,\n          \'alt_allele_indices\': features[\'alt_allele_indices\'],\n      }\n      endpoints.update(predictions)\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n      eval_metrics = (eval_metric_fn,\n                      [labels, endpoints[\'Predictions\'], variant_types])\n    else:\n      eval_metrics = None\n\n    loss = tf.constant(0.0)\n    train_op = None\n\n    spec = tpu_estimator.TPUEstimatorSpec(\n        mode=mode,\n        loss=loss,\n        train_op=train_op,\n        eval_metrics=eval_metrics,\n        predictions=endpoints)\n    if self.use_tpu:\n      return spec\n    else:\n      return spec.as_estimator_spec()\n\n\nclass DeepVariantSmallModel(DeepVariantSlimModel):\n  """"""A smaller of version of the DeepVariant model.\n\n     Uses only the first layers of Inception net.\n  """"""\n\n  def __init__(self, representation_layer=\'Mixed_5d\'):\n    """"""Creates an DeepVariant CNN network based on a tf.slim model.\n\n    Args:\n      representation_layer: string. The name of the layer from the Inception net\n        which will be used as an endpoint.\n\n    Raises:\n      ValueError: If any of the arguments are invalid.\n    """"""\n    super(DeepVariantSmallModel, self).__init__(\n        name=\'small_inception\',\n        pretrained_model_path=(\'/namespace/vale-project/models/classification/\'\n                               \'imagenet/inception_v3/model.ckpt-9591376\'),\n        n_classes_model_variable=\'InceptionV3/Logits/Conv2d_1c_1x1/weights\',\n        excluded_scopes_for_incompatible_shapes=[\n            \'InceptionV3/Logits\', \'InceptionV3/Conv2d_1a_3x3\'\n        ])\n\n    self.representation_layer = representation_layer\n\n  def model_fn(self, features, labels, mode, params):\n    """"""A model_fn for slim (really inception_v3), satisfying the Estimator API.\n\n    Args:\n      features: a single Tensor or dict of same (from input_fn).\n      labels: a single Tensor or dict of same (from input_fn).\n      mode: tf.estimator.ModeKeys.\n      params: dict.\n\n    Returns:\n      EstimatorSpec or TPUEstimatorSpec depending on self.use_tpu.\n\n    Raises:\n      ValueError: If representation_layer was not found in the Inception\n      architecture\n    """"""\n    # NB. The basic structure of this started from\n    # //third_party/cloud_tpu/models/inception/inception_v3.py\n\n    # redacted\n    num_classes = dv_constants.NUM_CLASSES\n\n    images = features[\'image\']\n    images = self.preprocess_images(images)\n\n    endpoints = self.create(\n        images=images,\n        num_classes=num_classes,\n        is_training=mode == tf.estimator.ModeKeys.TRAIN)\n\n    if self.representation_layer not in endpoints.keys():\n      raise ValueError(\'Layer {} is not found Inception endpoints.\'\n                       \'Available Inception net endpoints: {}\'.format(\n                           self.representation_layer, endpoints.keys()))\n\n    mid_layer = endpoints[self.representation_layer]\n    # Perform 1x1 convolution similarly to the Inception architecture\n    # (see \'Predictions\' end points in inception_v3 architecture)\n\n    tower = tf.nn.conv2d(\n        mid_layer, 1, [1, 1], stride=1, activation_fn=tf.nn.relu)\n\n    batch_size = tower.get_shape()[0].value\n    tower = tf.reshape(tower, [batch_size, -1])\n\n    with tf.compat.v1.variable_scope(\'denselayers\'):\n      with slim.arg_scope([slim.fully_connected], activation_fn=tf.nn.relu):\n        logits = slim.fully_connected(tower, num_classes, scope=\'Dense\')\n\n    predictions = endpoints\n    predictions.update({\n        \'classes\': tf.argmax(input=logits, axis=1, output_type=tf.int32),\n        \'probabilities\': tf.nn.softmax(logits, name=\'softmax_tensor\'),\n        \'Logits\': logits,\n        \'Predictions\': slim.softmax(logits)\n    })\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      return self._model_fn_predict(mode, features, logits)\n\n    # Compute loss.\n    one_hot_labels = tf.one_hot(labels, num_classes, dtype=tf.int32)\n    tf.compat.v1.losses.softmax_cross_entropy(\n        onehot_labels=one_hot_labels,\n        logits=logits,\n        weights=1.0,\n        label_smoothing=FLAGS.label_smoothing)\n    total_loss = tf.compat.v1.losses.get_total_loss(\n        add_regularization_losses=True)\n\n    return self.make_ops_and_estimator(features, endpoints, labels, logits,\n                                       predictions, total_loss, mode, params)\n\n  def _create(self, images, num_classes, is_training):\n    """"""See baseclass.""""""\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n      _, endpoints = inception.inception_v3(\n          images, num_classes, create_aux_logits=False, is_training=is_training)\n      return endpoints\n\n\n# Our list of pre-defined models.\n_MODELS = [\n    DeepVariantSmallModel(),\n    DeepVariantInceptionV3(),\n    DeepVariantRandomGuessModel(),\n    DeepVariantConstantModel(),\n    DeepVariantInceptionV3Embedding(),\n]\n\n\ndef all_models():\n  """"""Gets a list of the all of the known models.""""""\n  return list(_MODELS)\n\n\ndef production_models():\n  """"""Gets a list of the models that we test extensively.""""""\n  return [get_model(\'inception_v3\'), get_model(\'inception_v3_embedding\')]\n\n\ndef get_model(model_name):\n  """"""Looks up a DeepVariantModel by name.\n\n  Args:\n    model_name: String. Looks for a pre-defined DeepVariantModel with a name\n      equal to this model_name string.\n\n  Returns:\n    A DeepVariantModel instance.\n\n  Raises:\n    ValueError: If no model exists with model_name.\n  """"""\n  for model in _MODELS:\n    if model_name == model.name:\n      return model\n  raise ValueError(\'Unknown model_name {}, options are {}\'.format(\n      model_name, [model.name for model in _MODELS]))\n'"
deepvariant/modeling_test.py,29,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for learning.genomics.deepvariant.modeling.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport numpy as np\nimport six\nimport tensorflow as tf\nimport tf_slim\n\nfrom deepvariant import dv_constants\nfrom deepvariant import modeling\nfrom deepvariant import tf_utils\n\ntf.compat.v1.disable_eager_execution()\nslim = tf_slim\n\n\nclass ModelingTest(\n    six.with_metaclass(parameterized.TestGeneratorMetaclass, tf.test.TestCase)):\n\n  @parameterized.parameters(\n      (model.name, type(model)) for model in modeling.all_models())\n  def test_get_model_existing_models(self, model_name, expected_class):\n    self.assertIsInstance(modeling.get_model(model_name), expected_class)\n\n  def test_get_model_unknown_model_signals_error(self):\n    with six.assertRaisesRegex(self, ValueError, \'Unknown model\'):\n      modeling.get_model(\'unknown_model_1234\')\n\n  def test_make_deepvariant_slim_model(self):\n    model = modeling.DeepVariantSlimModel(\n        name=\'foo\',\n        n_classes_model_variable=[\'n_classes\'],\n        excluded_scopes_for_incompatible_shapes=[\'logits\'],\n        pretrained_model_path=\'path\')\n\n    self.assertEqual(\'foo\', model.name)\n    self.assertEqual([\'n_classes\'], model.n_classes_model_variable)\n    self.assertEqual([\'logits\'], model.excluded_scopes_for_incompatible_shapes)\n    self.assertEqual(\'path\', model.pretrained_model_path)\n\n  def test_is_encoded_variant_type(self):\n    types = [\n        tf_utils.EncodedVariantType.SNP.value,\n        tf_utils.EncodedVariantType.INDEL.value\n    ]\n    tensor = tf.constant(types * 4, dtype=tf.int64)\n\n    def _run(tensor_to_run):\n      with self.test_session() as sess:\n        return list(sess.run(tensor_to_run))\n\n    self.assertEqual(\n        _run(\n            modeling.is_encoded_variant_type(tensor,\n                                             tf_utils.EncodedVariantType.SNP)),\n        [True, False] * 4)\n    self.assertEqual(\n        _run(\n            modeling.is_encoded_variant_type(\n                tensor, tf_utils.EncodedVariantType.INDEL)), [False, True] * 4)\n\n  @parameterized.parameters([True, False])\n  def test_eval_metric_fn(self, include_variant_types):\n    labels = tf.constant([1, 0], dtype=tf.int64)\n    predictions = tf.constant([[1, 0], [0, 1]], dtype=tf.int64)\n    if include_variant_types:\n      variant_types = tf.constant([0, 1], dtype=tf.int64)\n    else:\n      variant_types = None\n\n    expected = modeling.eval_function_metrics(\n        has_variant_types=include_variant_types)\n    actual = modeling.eval_metric_fn(labels, predictions, variant_types)\n    self.assertEqual(set(expected.keys()), set(actual.keys()))\n\n  def test_variables_to_restore_from_model(self):\n    model = modeling.DeepVariantModel(\'test\', \'path\')\n    # We haven\'t created a slim model, so the variables_to_restore_from_model\n    # should be returning an empty list.\n    self.assertEqual([], model.variables_to_restore_from_model())\n\n    # Create two model variable and one regular variables.\n    with tf.compat.v1.variable_scope(\'model\'):\n      with tf.compat.v1.variable_scope(\'l1\'):\n        w1 = slim.model_variable(\'w1\', shape=[10, 3, 3])\n      with tf.compat.v1.variable_scope(\'l2\'):\n        w2 = slim.model_variable(\'w2\', shape=[10, 3, 3])\n        w3 = slim.model_variable(\'w3\', shape=[10, 3, 3])\n    v1 = slim.variable(\'my_var\', shape=[20, 1])\n\n    # The only variables in the system are the three we\'ve created.\n    six.assertCountEqual(self, [w1, w2, w3, v1], slim.get_variables())\n\n    # We get just the three model variables without any excludes.\n    six.assertCountEqual(self, [w1, w2, w3],\n                         model.variables_to_restore_from_model())\n    # As well as when exclude_scopes is an empty list.\n    six.assertCountEqual(\n        self, [w1, w2, w3],\n        model.variables_to_restore_from_model(exclude_scopes=[]))\n\n    # Excluding model/l1 variables gives us w2 and w3.\n    six.assertCountEqual(\n        self, [w2, w3],\n        model.variables_to_restore_from_model(exclude_scopes=[\'model/l1\']))\n    # Excluding model/l2 gives us just w1 back.\n    six.assertCountEqual(\n        self, [w1],\n        model.variables_to_restore_from_model(exclude_scopes=[\'model/l2\']))\n    # Excluding multiple scopes works as expected.\n    six.assertCountEqual(\n        self, [],\n        model.variables_to_restore_from_model(\n            exclude_scopes=[\'model/l1\', \'model/l2\']))\n    # Excluding the root model scope also produces no variables..\n    six.assertCountEqual(\n        self, [],\n        model.variables_to_restore_from_model(exclude_scopes=[\'model\']))\n\n\n# Hide the baseclass inside an enclosing scope so that unittest doesn\'t try to\n# run our baseclass tests directly. http://stackoverflow.com/a/1323554.\nclass HiddenFromUnitTest(object):\n\n  class SlimModelBaseTest(\n      six.with_metaclass(parameterized.TestGeneratorMetaclass,\n                         tf.test.TestCase)):\n\n    @parameterized.parameters(\n        dict(is_training=True),\n        dict(is_training=False),\n    )\n    def test_create(self, is_training):\n      # Creates a training=False model.\n      self.assertEqual(\n          len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)),\n          0)\n      images = tf.compat.v1.placeholder(\n          tf.float32,\n          (4, dv_constants.PILEUP_DEFAULT_HEIGHT,\n           dv_constants.PILEUP_DEFAULT_WIDTH, dv_constants.PILEUP_NUM_CHANNELS))\n      endpoints = self.model.create(\n          images, dv_constants.NUM_CLASSES, is_training=is_training)\n      if is_training:\n        self.assertNotEqual(\n            len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)),\n            0)\n      else:\n        self.assertEqual(\n            len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)),\n            0)\n      self.assertIn(\'Predictions\', endpoints)\n      self.assertIn(\'Logits\', endpoints)\n      self.assertEqual(endpoints[\'Predictions\'].shape,\n                       (4, dv_constants.NUM_CLASSES))\n\n    def test_preprocess_images(self):\n      with self.test_session() as sess:\n        batch_size = 3\n        values = range(91, 91 + 2 * 1 * dv_constants.PILEUP_NUM_CHANNELS)\n        all_values = list(np.tile(values, batch_size))\n        raw = np.array(\n            all_values, dtype=\'uint8\').reshape(\n                (batch_size, 2, 1, dv_constants.PILEUP_NUM_CHANNELS))\n        images = sess.run(self.model.preprocess_images(raw))\n        for i in range(batch_size):\n          image = images[i]\n\n          # Check that our image has the right shape and that all values are\n          # floats between between -1 and 1.\n          self.assertEqual(tf.float32, image.dtype)\n          self.assertTrue((image >= -1).all() and (image <= 1).all())\n          self.assertEqual((2, 1, dv_constants.PILEUP_NUM_CHANNELS),\n                           image.shape)\n\n          # The preprocess step resizes the image to h x w as needed by\n          # inception. We don\'t really care where it goes in the image (and the\n          # calculation is complex. So we are simply checking here that all\n          # values are zero except for the transformed values we see in values.\n          # We are relying here on the tf operations to be correct and to not\n          # change their behavior over time. Because we are doing assertEqual\n          # we are also testing the order of the values, which means that we\n          # are sure that the pixels have been translated in the right order in\n          # the image, wherever the actual translation might be.\n          self.assertEqual([(x - 128.0) / 128.0 for x in values],\n                           [x for x in np.nditer(image) if x != 0.0])\n\n\nclass InceptionV3ModelTest(HiddenFromUnitTest.SlimModelBaseTest):\n\n  @classmethod\n  def setUpClass(cls):\n    super(InceptionV3ModelTest, cls).setUpClass()\n    cls.model = modeling.get_model(\'inception_v3\')\n\n  # Note this test is only applied to inception_v3.\n  @parameterized.parameters(\n      dict(width=221, height=100),\n      dict(width=221, height=200),\n      dict(width=75, height=362),\n  )\n  def test_image_dimensions(self, width, height):\n    with self.test_session():\n      images = tf.compat.v1.placeholder(tf.float32, (4, height, width, 3))\n      # We shouldn\'t get an exception creating images with these sizes.\n      _ = self.model.create(images, 3, is_training=True)\n\n  @parameterized.parameters(\n      dict(width=73, height=100),\n      dict(width=221, height=2000),\n      dict(width=73, height=2000),\n  )\n  def test_bad_inception_v3_image_dimensions_get_custom_exception(\n      self, width, height):\n    with self.test_session():\n      images = tf.compat.v1.placeholder(tf.float32, (4, height, width, 3))\n      expected_message = (\'Unsupported image dimensions.* model \'\n                          \'inception_v3.*w={} x h={}.*\').format(width, height)\n      with six.assertRaisesRegex(self, modeling.UnsupportedImageDimensionsError,\n                                 expected_message):\n        self.model.create(images, 3, is_training=True)\n\n\nclass InceptionV3EmbeddingModelTest(\n    six.with_metaclass(parameterized.TestGeneratorMetaclass, tf.test.TestCase)):\n\n  @classmethod\n  def setUpClass(cls):\n    super(InceptionV3EmbeddingModelTest, cls).setUpClass()\n    cls.model = modeling.get_model(\'inception_v3_embedding\')\n\n  @parameterized.parameters(\n      dict(is_training=True),\n      dict(is_training=False),\n  )\n  def test_create(self, is_training):\n    self.assertEqual(\n        len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)), 0)\n    images = tf.compat.v1.placeholder(\n        tf.float32,\n        (4, dv_constants.PILEUP_DEFAULT_HEIGHT,\n         dv_constants.PILEUP_DEFAULT_WIDTH, dv_constants.PILEUP_NUM_CHANNELS))\n    seq_type = tf.compat.v1.placeholder(tf.int64, (4,))\n    endpoints = self.model._create((images, seq_type),\n                                   dv_constants.NUM_CLASSES,\n                                   is_training=is_training)\n    if is_training:\n      self.assertNotEqual(\n          len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)),\n          0)\n    else:\n      self.assertEqual(\n          len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)),\n          0)\n    self.assertIn(\'Predictions\', endpoints)\n    self.assertIn(\'Logits\', endpoints)\n    self.assertEqual(endpoints[\'Predictions\'].shape,\n                     (4, dv_constants.NUM_CLASSES))\n    self.assertIn(\'Embeddings\', endpoints)\n    self.assertEqual(endpoints[\'Embeddings\'].shape,\n                     (4, 2048 + self.model.embedding_size))\n\n  def test_create_embeddings(self):\n    indices = tf.compat.v1.placeholder(tf.int64, (4,))\n    embeddings = self.model._create_embeddings(indices)\n    self.assertEqual(embeddings.shape, (4, self.model.embedding_size))\n\n  def test_embedding_lookup(self):\n    indices = tf.compat.v1.placeholder(tf.int64, (4,))\n    embeddings = self.model._embedding_lookup(indices)\n    self.assertEqual(embeddings.shape, (4, self.model.embedding_size))\n\n\nclass RandomGuessModelTest(tf.test.TestCase):\n\n  def test_deterministic_predictions_for_fixed_seed(self):\n\n    def predictions(seed):\n      with self.test_session() as sess:\n        model = modeling.DeepVariantRandomGuessModel(seed=seed)\n        images = tf.compat.v1.placeholder(tf.float32, (4, 10, 10, 3))\n        predictions = sess.run(model.create(images, 3, False)[\'Predictions\'])\n        return predictions\n\n    # Note we do not use assertAllClose here as there\'s no assertNotAllClose().\n    self.assertTrue((predictions(seed=123) == predictions(seed=123)).all())\n    self.assertFalse((predictions(seed=123) == predictions(seed=456)).all())\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/pileup_image.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Encodes reference and read data into a PileupImage for DeepVariant.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\n\nimport numpy as np\n\nfrom third_party.nucleus.protos import reads_pb2\nfrom third_party.nucleus.util import ranges\nfrom third_party.nucleus.util import utils\nfrom deepvariant import dv_constants\nfrom deepvariant.protos import deepvariant_pb2\nfrom deepvariant.python import pileup_image_native\n\n\ndef default_options(read_requirements=None):\n  """"""Creates a PileupImageOptions populated with good default values.""""""\n  if not read_requirements:\n    read_requirements = reads_pb2.ReadRequirements(\n        min_base_quality=10,\n        min_mapping_quality=10,\n        min_base_quality_mode=reads_pb2.ReadRequirements.ENFORCED_BY_CLIENT)\n\n  return deepvariant_pb2.PileupImageOptions(\n      reference_band_height=5,\n      base_color_offset_a_and_g=40,\n      base_color_offset_t_and_c=30,\n      base_color_stride=70,\n      allele_supporting_read_alpha=1.0,\n      allele_unsupporting_read_alpha=0.6,\n      reference_matching_read_alpha=0.2,\n      reference_mismatching_read_alpha=1.0,\n      indel_anchoring_base_char=\'*\',\n      reference_alpha=0.4,\n      reference_base_quality=60,\n      positive_strand_color=70,\n      negative_strand_color=240,\n      base_quality_cap=40,\n      mapping_quality_cap=60,\n      height=dv_constants.PILEUP_DEFAULT_HEIGHT,\n      width=dv_constants.PILEUP_DEFAULT_WIDTH,\n      num_channels=dv_constants.PILEUP_NUM_CHANNELS,\n      read_overlap_buffer_bp=5,\n      read_requirements=read_requirements,\n      multi_allelic_mode=deepvariant_pb2.PileupImageOptions.ADD_HET_ALT_IMAGES,\n      # Fixed random seed produced with \'od -vAn -N4 -tu4 < /dev/urandom\'.\n      random_seed=2101079370,\n      custom_pileup_image=False,\n      sequencing_type_image=False,\n      sequencing_type=deepvariant_pb2.PileupImageOptions.UNSPECIFIED_SEQ_TYPE)\n\n\ndef _compute_half_width(width):\n  return int((width - 1) / 2)\n\n\nclass PileupImageCreator(object):\n  """"""High-level API for creating images of pileups of reads and reference bases.\n\n  This class provides a higher-level and more natural API for constructing\n  images at a candidate variant call site. Given a DeepVariantCall, which\n  contains the candidate variant call along with key supplementary information,\n  this class provides create_pileup_images() that will do all of the necessary\n  fetching of reads and reference bases from readers and pass those off to the\n  lower-level PileupImageEncoder to construct the image Tensor.\n\n  for dv_call in candidates:\n    allele_and_images = pic.create_pileup_images(dv_call)\n    ...\n\n  A quick note on how we deal with multiple alt alleles:\n\n  Suppose variant has ref and two alt alleles. Assuming the sample is diploid,\n  we have the following six possible genotypes:\n\n    ref/ref   => 0/0\n    ref/alt1  => 0/1\n    alt1/alt1 => 1/1\n    ref/alt2  => 0/2\n    alt1/alt2 => 1/2\n    alt2/alt2 => 2/2\n\n  In DeepVariant we predict the genotype count (0, 1, 2) for a specific set of\n  alternate alleles. If we only had a single alt, we\'d construct an image for\n  ref vs. alt1:\n\n    image1 => ref vs. alt1 => determine if we are 0/0, 0/1, 1/1\n\n  If we add a second image for alt2, we get:\n\n    image2 => ref vs. alt2 => determine if we are 0/0, 0/2, 2/2\n\n  but the problem here is that we don\'t have a good estimate for the het-alt\n  state 1/2. So we construct a third image contrasting ref vs. either alt1 or\n  alt2:\n\n    image3 => ref vs. alt1 or alt2 => determines 0/0, 0/{1,2}, {1,2}/{1,2}\n\n  Given the predictions for each image:\n\n    image1 => p00, p01, p11\n    image2 => p00, p02, p22\n    image3 => p00, p0x, pxx where x is {1,2}\n\n  we calculate our six genotype likelihoods as:\n\n    0/0 => p00 [from any image]\n    0/1 => p01 [image1]\n    1/1 => p11 [image1]\n    0/2 => p02 [image2]\n    2/2 => p22 [image2]\n    1/2 => pxx [image3]\n\n  The function create_pileup_images() returns all of the necessary images, along\n  with the alt alleles used for each image.\n  """"""\n\n  def __init__(self, options, ref_reader, sam_reader):\n    self._options = options\n    self._encoder = pileup_image_native.PileupImageEncoderNative(self._options)\n    self._ref_reader = ref_reader\n    self._sam_reader = sam_reader\n\n  def __getattr__(self, attr):\n    """"""Gets attributes from self._options as though they are our attributes.""""""\n    return self._options.__getattribute__(attr)\n\n  @property\n  def half_width(self):\n    return _compute_half_width(self._options.width)\n\n  @property\n  def max_reads(self):\n    return self.height - self.reference_band_height\n\n  def get_reads(self, variant):\n    """"""Gets the reads used to construct the pileup image around variant.\n\n    Args:\n      variant: A third_party.nucleus.protos.Variant proto describing the variant\n        we are creating the pileup image of.\n\n    Returns:\n      A list of third_party.nucleus.protos.Read protos.\n    """"""\n    query_start = variant.start - self._options.read_overlap_buffer_bp\n    query_end = variant.end + self._options.read_overlap_buffer_bp\n    region = ranges.make_range(variant.reference_name, query_start, query_end)\n    return list(self._sam_reader.query(region))\n\n  def get_reference_bases(self, variant):\n    """"""Gets the reference bases used to make the pileup image around variant.\n\n    Args:\n      variant: A third_party.nucleus.protos.Variant proto describing the variant\n        we are creating the pileup image of.\n\n    Returns:\n      A string of reference bases or None. Returns None if the reference\n      interval for variant isn\'t valid for some reason.\n    """"""\n    start = variant.start - self.half_width\n    end = start + self._options.width\n    region = ranges.make_range(variant.reference_name, start, end)\n    if self._ref_reader.is_valid(region):\n      return self._ref_reader.query(region)\n    else:\n      return None\n\n  def _alt_allele_combinations(self, variant):\n    """"""Yields the set of all alt_alleles for variant.\n\n    This function computes the sets of alt_alleles we want to use to cover all\n    genotype likelihood calculations we need for n alt alleles (see class docs\n    for background). The easiest way to do this is to calculate all combinations\n    of 2 alleles from ref + alts and then strip away the reference alleles,\n    leaving us with the set of alts for the pileup image encoder.\n\n    Args:\n      variant: third_party.nucleus.protos.Variant to generate the alt allele\n        combinations for.\n\n    Yields:\n      A series of sets containing the alt alleles we want to use for a single\n      pileup image. The entire series covers all combinations of alt alleles\n      needed for variant.\n\n    Raises:\n      ValueError: if options.multi_allelic_mode is UNSPECIFIED.\n    """"""\n    ref = variant.reference_bases\n    alts = list(variant.alternate_bases)\n\n    if (self.multi_allelic_mode ==\n        deepvariant_pb2.PileupImageOptions.UNSPECIFIED):\n      raise ValueError(\'multi_allelic_mode cannot be UNSPECIFIED\')\n    elif (self.multi_allelic_mode ==\n          deepvariant_pb2.PileupImageOptions.NO_HET_ALT_IMAGES):\n      for alt in alts:\n        yield set([alt])\n    else:\n      for combination in itertools.combinations([ref] + alts, 2):\n        yield set(combination) - {ref}\n\n  def build_pileup(self, dv_call, refbases, reads, alt_alleles):\n    """"""Creates a pileup tensor for dv_call.\n\n    Args:\n      dv_call: learning.genomics.deepvariant.DeepVariantCall object with\n        information on our candidate call and allele support information.\n      refbases: A string options.width in length containing the reference base\n        sequence to encode. The middle base of this string should be at the\n        start of the variant in dv_call.\n      reads: Iterable of third_party.nucleus.protos.Read objects that we\'ll use\n        to encode the read information supporting our call. Assumes each read is\n        aligned and is well-formed (e.g., has bases and quality scores, cigar).\n        Rows of the image are encoded in the same order as reads.\n      alt_alleles: A collection of alternative_bases from dv_call.variant that\n        we are treating as ""alt"" when constructing this pileup image. A read\n        will be considered supporting the ""alt"" allele if it occurs in the\n        support list for any alt_allele in this collection.\n\n    Returns:\n      A [self.width, self.height, DEFAULT_NUM_CHANNEL] uint8 Tensor image.\n\n    Raises:\n      ValueError: if any arguments are invalid.\n    """"""\n    if len(refbases) != self.width:\n      raise ValueError(\'refbases is {} long but width is {}\'.format(\n          len(refbases), self.width))\n\n    if not alt_alleles:\n      raise ValueError(\'alt_alleles cannot be empty\')\n    if any(alt not in dv_call.variant.alternate_bases for alt in alt_alleles):\n      raise ValueError(\n          \'all elements of alt_alleles must be the alternate bases\'\n          \' of dv_call.variant\', alt_alleles, dv_call.variant)\n\n    image_start_pos = dv_call.variant.start - self.half_width\n    if (len(dv_call.variant.reference_bases) == 1 and\n        refbases[self.half_width] != dv_call.variant.reference_bases):\n      raise ValueError(\'center of refbases doesnt match variant.refbases\',\n                       self.half_width, refbases[self.half_width],\n                       dv_call.variant)\n\n    # We start with n copies of our encoded reference bases.\n    rows = ([self._encoder.encode_reference(refbases)] *\n            self.reference_band_height)\n\n    # A generator that yields tuples of the form (position, row), iff the read\n    # can be encoded as a valid row to be used in the pileup image.\n    def _row_generator():\n      for read in reads:\n        read_row = self._encoder.encode_read(dv_call, refbases, read,\n                                             image_start_pos, alt_alleles)\n        if read_row is not None:\n          yield read.alignment.position.position, read_row\n\n    # We add a row for each read in order, down-sampling if the number of reads\n    # is greater than self.max_reads. Sort the reads by their alignment\n    # position.\n    random_for_image = np.random.RandomState(self._options.random_seed)\n    sample = sorted(\n        utils.reservoir_sample(\n            _row_generator(), self.max_reads, random=random_for_image),\n        key=lambda x: x[0])\n\n    rows += [read_row for _, read_row in sample]\n\n    # Finally, fill in any missing rows to bring our image to self.height rows\n    # with empty (all black) pixels.\n    n_missing_rows = self.height - len(rows)\n    if n_missing_rows > 0:\n      # Add values to rows to fill it out with zeros.\n      rows += [self._empty_image_row()] * n_missing_rows\n\n    # Vertically stack the image rows to create a single\n    # h x w x DEFAULT_NUM_CHANNEL image.\n    return np.vstack(rows)\n\n  def _empty_image_row(self):\n    """"""Creates an empty image row as an uint8 np.array.""""""\n    return np.zeros((1, self.width, self.num_channels), dtype=np.uint8)\n\n  def create_pileup_images(self, dv_call):\n    """"""Creates a DeepVariant TF.Example for the DeepVariant call dv_call.\n\n    See class documents for more details.\n\n    Args:\n      dv_call: A learning.genomics.deepvariant.DeepVariantCall proto that we\n        want to create a TF.Example pileup image of.\n\n    Returns:\n      A list of tuples. The first element of the tuple is a set of alternate\n      alleles used as \'alt\' when encoding this image. The second element is a\n      [w, h, DEFAULT_NUM_CHANNEL] uint8 Tensor of the pileup image for those\n      alt alleles.\n    """"""\n    variant = dv_call.variant\n    ref = self.get_reference_bases(variant)\n    if not ref:\n      # This interval isn\'t valid => we off the edge of the chromosome so return\n      # None to indicate we couldn\'t process this variant.\n      return None\n    reads = self.get_reads(variant)\n\n    def _make_one(alt_alleles):\n      image = self.build_pileup(dv_call, ref, reads, alt_alleles)\n      return alt_alleles, image\n\n    return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]\n'"
deepvariant/pileup_image_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant.pileup_image.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport itertools\n\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport enum\nimport mock\nimport numpy as np\nimport numpy.testing as npt\n\nfrom third_party.nucleus.io import fasta\nfrom third_party.nucleus.protos import reads_pb2\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\n\nfrom deepvariant import pileup_image\nfrom deepvariant.protos import deepvariant_pb2\nfrom deepvariant.python import pileup_image_native\n\n\nclass SequencingTypeColor(enum.Enum):\n  """"""Enum capturing the int64 values we encode sequencing type colors.""""""\n  UNSPECIFIED_SEQ_TYPE = 0\n  WGS = 85\n  WES = 170\n  TRIO = 255\n\n\ndef _supporting_reads(*names):\n  return deepvariant_pb2.DeepVariantCall.SupportingReads(read_names=names)\n\n\ndef _make_dv_call(ref_bases=\'A\', alt_bases=\'C\'):\n  return deepvariant_pb2.DeepVariantCall(\n      variant=variants_pb2.Variant(\n          reference_name=\'chr1\',\n          start=10,\n          end=11,\n          reference_bases=ref_bases,\n          alternate_bases=[alt_bases]),\n      allele_support={\'C\': _supporting_reads(\'read1/1\', \'read2/1\')})\n\n\ndef _make_encoder(read_requirements=None, **kwargs):\n  """"""Make a PileupImageEncoderNative with overrideable default options.""""""\n  options = pileup_image.default_options(read_requirements)\n  options.MergeFrom(deepvariant_pb2.PileupImageOptions(**kwargs))\n  return pileup_image_native.PileupImageEncoderNative(options)\n\n\ndef _make_image_creator(ref_reader, sam_reader_obj, **kwargs):\n  options = pileup_image.default_options()\n  options.MergeFrom(deepvariant_pb2.PileupImageOptions(**kwargs))\n  return pileup_image.PileupImageCreator(options, ref_reader, sam_reader_obj)\n\n\nclass PileupImageEncoderTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.options = pileup_image.default_options()\n\n  @parameterized.parameters((\'A\', 250), (\'G\', 180), (\'T\', 100), (\'C\', 30),\n                            (\'N\', 0), (\'X\', 0))\n  def test_base_color(self, base, expected_color):\n    pie = _make_encoder(\n        base_color_offset_a_and_g=40,\n        base_color_offset_t_and_c=30,\n        base_color_stride=70)\n    self.assertAlmostEqual(pie.base_color(base), expected_color)\n\n  @parameterized.parameters(\n      (0, 0),\n      (5, int(254 * 0.25)),\n      (10, int(254 * 0.5)),\n      (15, int(254 * 0.75)),\n      (19, int(254 * 0.95)),\n      (20, 254),\n      (21, 254),\n      (25, 254),\n      (40, 254),\n  )\n  def test_base_quality_color(self, base_qual, expected_color):\n    pie = _make_encoder(base_quality_cap=20)\n    self.assertAlmostEqual(pie.base_quality_color(base_qual), expected_color)\n\n  @parameterized.parameters(\n      (0, 0),\n      (5, int(254 * 0.25)),\n      (10, int(254 * 0.5)),\n      (15, int(254 * 0.75)),\n      (19, int(254 * 0.95)),\n      (20, 254),\n      (21, 254),\n      (25, 254),\n      (40, 254),\n  )\n  def test_mapping_quality_color(self, mapping_qual, expected_color):\n    pie = _make_encoder(mapping_quality_cap=20)\n    self.assertAlmostEqual(\n        pie.mapping_quality_color(mapping_qual), expected_color)\n\n  @parameterized.parameters((True, 1), (False, 2))\n  def test_strand_color(self, on_positive_strand, expected_color):\n    pie = _make_encoder(positive_strand_color=1, negative_strand_color=2)\n    self.assertAlmostEqual(pie.strand_color(on_positive_strand), expected_color)\n\n  @parameterized.parameters(\n      (False, int(254.0 * 0.2)),\n      (True, int(254.0 * 0.1)),\n  )\n  def test_supports_alt_color(self, supports_alt, expected_color):\n    pie = _make_encoder(\n        allele_supporting_read_alpha=0.1, allele_unsupporting_read_alpha=0.2)\n    self.assertAlmostEqual(pie.supports_alt_color(supports_alt), expected_color)\n\n  @parameterized.parameters(\n      (False, int(254.0 * 0.4)),\n      (True, int(254.0 * 0.3)),\n  )\n  def test_matches_ref_color(self, matches_ref, expected_color):\n    pie = _make_encoder(\n        reference_matching_read_alpha=0.3, reference_mismatching_read_alpha=0.4)\n    self.assertAlmostEqual(pie.matches_ref_color(matches_ref), expected_color)\n\n  @parameterized.parameters(\n      (deepvariant_pb2.PileupImageOptions.UNSPECIFIED_SEQ_TYPE,\n       SequencingTypeColor.UNSPECIFIED_SEQ_TYPE.value),\n      (deepvariant_pb2.PileupImageOptions.WGS, SequencingTypeColor.WGS.value),\n      (deepvariant_pb2.PileupImageOptions.WES, SequencingTypeColor.WES.value),\n      (deepvariant_pb2.PileupImageOptions.TRIO, SequencingTypeColor.TRIO.value))\n  def test_sequencing_type_color(self, sequencing_type, expected_color):\n    # Default\n    pileup_image_encoder = _make_encoder(\n        sequencing_type_image=True,\n        sequencing_type=sequencing_type,\n        num_channels=7)\n    self.assertAlmostEqual(pileup_image_encoder.sequencing_type_color(),\n                           expected_color)\n\n  def test_reference_encoding(self):\n    self.assertImageRowEquals(\n        _make_encoder().encode_reference(\'ACGTN\'),\n        np.dstack([\n            # Base.\n            (250, 30, 180, 100, 0),\n            # Base quality.\n            (254, 254, 254, 254, 254),\n            # Mapping quality.\n            (254, 254, 254, 254, 254),\n            # Strand channel (forward or reverse)\n            (70, 70, 70, 70, 70),\n            # Supports alt or not.\n            (152, 152, 152, 152, 152),\n            # Matches ref or not.\n            (50, 50, 50, 50, 50)\n        ]).astype(np.uint8))\n\n  def assertImageRowEquals(self, image_row, expected):\n    npt.assert_equal(image_row, expected.astype(np.uint8))\n\n  def test_encode_read_matches(self):\n    start = 10\n    dv_call = _make_dv_call()\n    alt_allele = dv_call.variant.alternate_bases[0]\n    read = test_utils.make_read(\n        \'ACCGT\', start=start, cigar=\'5M\', quals=range(10, 15), name=\'read1\')\n    full_expected = np.dstack([\n        # Base.\n        (250, 30, 30, 180, 100),\n        # Base quality.\n        (63, 69, 76, 82, 88),\n        # Mapping quality.\n        (211, 211, 211, 211, 211),\n        # Strand channel (forward or reverse)\n        (70, 70, 70, 70, 70),\n        # Supports alt or not.\n        (254, 254, 254, 254, 254),\n        # Matches ref or not.\n        (50, 50, 254, 50, 50)\n    ]).astype(np.uint8)\n\n    self.assertImageRowEquals(\n        _make_encoder().encode_read(dv_call, \'ACAGT\', read, start, alt_allele),\n        full_expected)\n\n  @parameterized.parameters((bases_start, bases_end)\n                            for bases_start in range(0, 5)\n                            for bases_end in range(6, 12))\n  def test_encode_read_spans2(self, bases_start, bases_end):\n    bases = \'AAAACCGTCCC\'\n    quals = [9, 9, 9, 10, 11, 12, 13, 14, 8, 8, 8]\n    bases_start_offset = 7\n    ref_start = 10\n    ref_size = 5\n    read_bases = bases[bases_start:bases_end]\n    read_quals = quals[bases_start:bases_end]\n    read_start = bases_start_offset + bases_start\n\n    # Create our expected image row encoding.\n    full_expected = np.dstack([\n        # Base.\n        (250, 30, 30, 180, 100),\n        # Base quality.\n        (63, 69, 76, 82, 88),\n        # Mapping quality.\n        (211, 211, 211, 211, 211),\n        # Strand channel (forward or reverse)\n        (70, 70, 70, 70, 70),\n        # Supports alt or not.\n        (254, 254, 254, 254, 254),\n        # Matches ref or not.\n        (50, 50, 254, 50, 50)\n    ]).astype(np.uint8)\n    expected = np.zeros((1, ref_size, self.options.num_channels),\n                        dtype=np.uint8)\n    for i in range(read_start, read_start + len(read_bases)):\n      if ref_start <= i < ref_start + ref_size:\n        expected[0, i - ref_start] = full_expected[0, i - ref_start]\n\n    read = test_utils.make_read(\n        read_bases,\n        start=read_start,\n        cigar=str(len(read_bases)) + \'M\',\n        quals=read_quals,\n        name=\'read1\')\n    dv_call = _make_dv_call()\n    alt_allele = dv_call.variant.alternate_bases[0]\n    self.assertImageRowEquals(\n        _make_encoder().encode_read(dv_call, \'ACAGT\', read, ref_start,\n                                    alt_allele), expected)\n\n  def test_encode_read_deletion(self):\n    # ref:  AACAG\n    # read: AA--G\n    start = 2\n    read = test_utils.make_read(\n        \'AAG\', start=start, cigar=\'2M2D1M\', quals=range(10, 13), name=\'read1\')\n    dv_call = _make_dv_call()\n    alt_allele = dv_call.variant.alternate_bases[0]\n    full_expected = np.dstack([\n        # Base. The second A is 0 because it\'s the anchor of the deletion.\n        (250, 0, 0, 0, 180),\n        # Base quality.\n        (63, 69, 0, 0, 76),\n        # Mapping quality.\n        (211, 211, 0, 0, 211),\n        # Strand channel (forward or reverse)\n        (70, 70, 0, 0, 70),\n        # Supports alt or not.\n        (254, 254, 0, 0, 254),\n        # Matches ref or not.\n        (50, 254, 0, 0, 50)\n    ]).astype(np.uint8)\n    self.assertImageRowEquals(\n        _make_encoder().encode_read(dv_call, \'AACAG\', read, start, alt_allele),\n        full_expected)\n\n  def test_encode_read_insertion(self):\n    # ref:  AA-CAG\n    # read: AAACAG\n    start = 2\n    read = test_utils.make_read(\n        \'AAACAG\',\n        start=start,\n        cigar=\'2M1I3M\',\n        quals=range(10, 16),\n        name=\'read1\')\n    dv_call = _make_dv_call()\n    alt_allele = dv_call.variant.alternate_bases[0]\n    full_expected = np.dstack([\n        # Base.\n        (250, 0, 30, 250, 180),\n        # Base quality.\n        (63, 76, 82, 88, 95),\n        # Mapping quality.\n        (211, 211, 211, 211, 211),\n        # Strand channel (forward or reverse)\n        (70, 70, 70, 70, 70),\n        # Supports alt or not.\n        (254, 254, 254, 254, 254),\n        # Matches ref or not.\n        (50, 254, 50, 50, 50)\n    ]).astype(np.uint8)\n    self.assertImageRowEquals(\n        _make_encoder().encode_read(dv_call, \'AACAG\', read, start, alt_allele),\n        full_expected)\n\n  def test_encode_read_custom_pileup_read_deletion(self):\n    pie = _make_encoder(\n        custom_pileup_image=True,\n        num_channels=7,\n        insert_base_char=\'I\',\n        delete_base_char=\'D\')\n    # ref:  AACAG\n    # read: AA--G\n    start = 2\n    read = test_utils.make_read(\n        \'AAG\', start=start, cigar=\'2M2D1M\', quals=range(10, 13), name=\'read1\')\n    dv_call = _make_dv_call()\n    alt_allele = dv_call.variant.alternate_bases[0]\n    full_expected = np.dstack([\n        # Base. Fills in the whole deletion with 130, starting at the anchor.\n        (250, 130, 130, 130, 180),\n        # Base quality.\n        (63, 69, 0, 0, 76),\n        # Mapping quality.\n        (211, 211, 0, 0, 211),\n        # Strand channel (forward or reverse)\n        (70, 70, 0, 0, 70),\n        # Supports alt or not.\n        (254, 254, 0, 0, 254),\n        # Matches ref or not.\n        (50, 254, 0, 0, 50),\n        # Operation length.\n        (0, 2, 2, 2, 0)\n    ]).astype(np.uint8)\n    self.assertImageRowEquals(\n        pie.encode_read(dv_call, \'AACAG\', read, start, alt_allele),\n        full_expected)\n\n  def test_encode_read_custom_pileup_read_insertion(self):\n    pie = _make_encoder(\n        custom_pileup_image=True,\n        num_channels=7,\n        insert_base_char=\'I\',\n        delete_base_char=\'D\')\n    # ref:  AA-CAG\n    # read: AAACAG\n    start = 2\n    read = test_utils.make_read(\n        \'AAACAG\',\n        start=start,\n        cigar=\'2M1I3M\',\n        quals=range(10, 16),\n        name=\'read1\')\n    dv_call = _make_dv_call()\n    alt_allele = dv_call.variant.alternate_bases[0]\n    full_expected = np.dstack([\n        # Base.\n        (250, 150, 30, 250, 180),\n        # Base quality.\n        (63, 76, 82, 88, 95),\n        # Mapping quality.\n        (211, 211, 211, 211, 211),\n        # Strand channel (forward or reverse)\n        (70, 70, 70, 70, 70),\n        # Supports alt or not.\n        (254, 254, 254, 254, 254),\n        # Matches ref or not.\n        (50, 254, 50, 50, 50),\n        # Operation length.\n        (0, 1, 0, 0, 0),\n    ]).astype(np.uint8)\n    self.assertImageRowEquals(\n        pie.encode_read(dv_call, \'AACAG\', read, start, alt_allele),\n        full_expected)\n\n  @parameterized.parameters(\n      (deepvariant_pb2.PileupImageOptions.UNSPECIFIED_SEQ_TYPE,\n       SequencingTypeColor.UNSPECIFIED_SEQ_TYPE.value),\n      (deepvariant_pb2.PileupImageOptions.WGS, SequencingTypeColor.WGS.value),\n      (deepvariant_pb2.PileupImageOptions.WES, SequencingTypeColor.WES.value),\n  )\n  def test_sequencing_type_image(self, sequencing_type, color):\n    pileup_image_encoder = _make_encoder(\n        sequencing_type_image=True,\n        sequencing_type=sequencing_type,\n        num_channels=7)\n    start = 10\n    dv_call = _make_dv_call()\n    alt_allele = dv_call.variant.alternate_bases[0]\n    read = test_utils.make_read(\n        \'ACCGT\', start=start, cigar=\'5M\', quals=range(10, 15), name=\'read1\')\n    full_expected = np.dstack([\n        # Base.\n        (250, 30, 30, 180, 100),\n        # Base quality.\n        (63, 69, 76, 82, 88),\n        # Mapping quality.\n        (211, 211, 211, 211, 211),\n        # Strand channel (forward or reverse)\n        (70, 70, 70, 70, 70),\n        # Supports alt or not.\n        (254, 254, 254, 254, 254),\n        # Matches ref or not.\n        (50, 50, 254, 50, 50),\n        # sequencing type\n        (color, color, color, color, color)\n    ]).astype(np.uint8)\n    self.assertImageRowEquals(\n        pileup_image_encoder.encode_read(dv_call, \'ACAGT\', read, start,\n                                         alt_allele), full_expected)\n\n  @parameterized.parameters(\n      (min_base_qual, min_mapping_qual)\n      for min_base_qual, min_mapping_qual in itertools.product(\n          range(0, 5), range(0, 5)))\n  def test_ignores_reads_with_low_quality_bases(self, min_base_qual,\n                                                min_mapping_qual):\n    """"""Check that we discard reads with low quality bases at variant start site.\n\n    We have the following scenario:\n\n    position    0    1    2    3    4    5\n    reference        A    A    C    A    G\n    read             A    A    A\n    variant               C\n\n    We set the base quality of the middle base in the read to different values\n    of `base_qual`. Since the middle position of the read is where the variant\n    starts, the read should only be kept if `base_qual` >= `min_base_qual`.\n\n    Args:\n      min_base_qual: Reads are discarded if the base at a variant start position\n        does not meet this base quality requirement.\n      min_mapping_qual: Reads are discarded if they do not meet this mapping\n        quality requirement.\n    """"""\n    dv_call = deepvariant_pb2.DeepVariantCall(\n        variant=variants_pb2.Variant(\n            reference_name=\'chr1\',\n            start=2,\n            end=3,\n            reference_bases=\'A\',\n            alternate_bases=[\'C\']))\n\n    read_requirements = reads_pb2.ReadRequirements(\n        min_base_quality=min_base_qual,\n        min_mapping_quality=min_mapping_qual,\n        min_base_quality_mode=reads_pb2.ReadRequirements.ENFORCED_BY_CLIENT)\n    pie = _make_encoder(read_requirements=read_requirements)\n\n    for base_qual in range(min_base_qual + 5):\n      quals = [min_base_qual, base_qual, min_base_qual]\n      read = test_utils.make_read(\n          \'AAA\', start=1, cigar=\'3M\', quals=quals, mapq=min_mapping_qual)\n      actual = pie.encode_read(dv_call, \'AACAG\', read, 1, \'C\')\n      if base_qual < min_base_qual:\n        self.assertIsNone(actual)\n      else:\n        self.assertIsNotNone(actual)\n\n  @parameterized.parameters(\n      (min_base_qual, min_mapping_qual)\n      for min_base_qual, min_mapping_qual in itertools.product(\n          range(0, 5), range(0, 5)))\n  def test_keeps_reads_with_low_quality_bases(self, min_base_qual,\n                                              min_mapping_qual):\n    """"""Check that we keep reads with adequate quality at variant start position.\n\n    We have the following scenario:\n\n    position    0    1    2    3    4    5\n    reference        A    A    C    A    G\n    read             A    A    A\n    variant               C\n\n    We set the base quality of the first and third bases in the read to\n    different functions of `base_qual`. The middle position of the read is\n    where the variant starts, and this position always has base quality greater\n    than `min_base_qual`. Thus, the read should always be kept.\n\n    Args:\n      min_base_qual: Reads are discarded if the base at a variant start position\n        does not meet this base quality requirement.\n      min_mapping_qual: Reads are discarded if they do not meet this mapping\n        quality requirement.\n    """"""\n    dv_call = deepvariant_pb2.DeepVariantCall(\n        variant=variants_pb2.Variant(\n            reference_name=\'chr1\',\n            start=2,\n            end=3,\n            reference_bases=\'A\',\n            alternate_bases=[\'C\']))\n\n    read_requirements = reads_pb2.ReadRequirements(\n        min_base_quality=min_base_qual,\n        min_mapping_quality=min_mapping_qual,\n        min_base_quality_mode=reads_pb2.ReadRequirements.ENFORCED_BY_CLIENT)\n    pie = _make_encoder(read_requirements=read_requirements)\n\n    for base_qual in range(min_base_qual + 5):\n      quals = [base_qual - 1, min_base_qual, base_qual + 1]\n      read = test_utils.make_read(\n          \'AAA\', start=1, cigar=\'3M\', quals=quals, mapq=min_mapping_qual)\n      actual = pie.encode_read(dv_call, \'AACAG\', read, 1, \'C\')\n      self.assertIsNotNone(actual)\n\n  @parameterized.parameters(\n      (min_base_qual, min_mapping_qual)\n      for min_base_qual, min_mapping_qual in itertools.product(\n          range(0, 5), range(0, 5)))\n  def test_ignores_reads_with_low_mapping_quality(self, min_base_qual,\n                                                  min_mapping_qual):\n    """"""Check that we discard reads with low mapping quality.\n\n    We have the following scenario:\n\n    position    0    1    2    3    4    5\n    reference        A    A    C    A    G\n    read             A    A    A\n    variant               C\n\n    We set the mapping quality of the read to different values of\n    `mapping_qual`. All bases in the read have base quality greater than\n    `min_base_qual`. The read should only be kept if\n    `mapping_qual` > `min_mapping_qual`.\n\n    Args:\n      min_base_qual: Reads are discarded if the base at a variant start position\n        does not meet this base quality requirement.\n      min_mapping_qual: Reads are discarded if they do not meet this mapping\n        quality requirement.\n    """"""\n    dv_call = deepvariant_pb2.DeepVariantCall(\n        variant=variants_pb2.Variant(\n            reference_name=\'chr1\',\n            start=2,\n            end=3,\n            reference_bases=\'A\',\n            alternate_bases=[\'C\']))\n\n    read_requirements = reads_pb2.ReadRequirements(\n        min_base_quality=min_base_qual,\n        min_mapping_quality=min_mapping_qual,\n        min_base_quality_mode=reads_pb2.ReadRequirements.ENFORCED_BY_CLIENT)\n    pie = _make_encoder(read_requirements=read_requirements)\n\n    for mapping_qual in range(min_mapping_qual + 5):\n      quals = [min_base_qual, min_base_qual, min_base_qual]\n      read = test_utils.make_read(\n          \'AAA\', start=1, cigar=\'3M\', quals=quals, mapq=mapping_qual)\n      actual = pie.encode_read(dv_call, \'AACAG\', read, 1, \'C\')\n      if mapping_qual < min_mapping_qual:\n        self.assertIsNone(actual)\n      else:\n        self.assertIsNotNone(actual)\n\n  @parameterized.parameters(\n      # alt_allele is C, supported by only frag 1 of read1 and frag 2 of read 3.\n      (\'read1\', 1, \'C\', \'C\', True),\n      (\'read1\', 2, \'C\', \'C\', False),\n      (\'read2\', 1, \'C\', \'G\', False),\n      (\'read2\', 2, \'C\', \'G\', False),\n      (\'read3\', 1, \'C\', \'C\', False),\n      (\'read3\', 2, \'C\', \'C\', True),\n      # alt_allele is now G, so only read2 (both frags) support alt.\n      (\'read1\', 1, \'G\', \'C\', False),\n      (\'read1\', 2, \'G\', \'C\', False),\n      (\'read2\', 1, \'G\', \'G\', True),\n      (\'read2\', 2, \'G\', \'G\', True),\n      (\'read3\', 1, \'G\', \'C\', False),\n      (\'read3\', 2, \'G\', \'C\', False),\n  )\n  def test_read_support_is_respected(self, read_name, read_number, alt_allele,\n                                     read_base, supports_alt):\n    """"""supports_alt is encoded as the 5th channel out of the 7 channels.""""""\n    dv_call = deepvariant_pb2.DeepVariantCall(\n        variant=variants_pb2.Variant(\n            reference_name=\'chr1\',\n            start=10,\n            end=11,\n            reference_bases=\'A\',\n            alternate_bases=[alt_allele]),\n        allele_support={\n            \'C\': _supporting_reads(\'read1/1\', \'read3/2\'),\n            \'G\': _supporting_reads(\'read2/1\', \'read2/2\'),\n        })\n    read = test_utils.make_read(\n        read_base,\n        start=dv_call.variant.start,\n        cigar=\'1M\',\n        quals=[50],\n        name=read_name)\n    read.read_number = read_number\n    actual = _make_encoder().encode_read(dv_call, \'TAT\', read,\n                                         dv_call.variant.start - 1, alt_allele)\n    expected_base_values = {\'C\': 30, \'G\': 180}\n    expected_supports_alt_channel = [152, 254]\n    expected = [\n        expected_base_values[read_base], 254, 211, 70,\n        expected_supports_alt_channel[supports_alt], 254\n    ]\n\n    self.assertEqual(list(actual[0, 1]), expected)\n\n\nclass PileupImageCreatorEncodePileupTest(parameterized.TestCase):\n  """"""Tests of PileupImageCreator build_pileup routine.""""""\n\n  def setUp(self):\n    self.alt_allele = \'C\'\n    self.dv_call = _make_dv_call(ref_bases=\'G\', alt_bases=self.alt_allele)\n    self.pic = _make_image_creator(\n        None, None, width=3, height=4, reference_band_height=2)\n    self.ref = \'AGC\'\n    self.read1 = test_utils.make_read(\'AGC\', start=0, cigar=\'3M\', name=\'read1\')\n    self.read2 = test_utils.make_read(\'AGC\', start=1, cigar=\'3M\', name=\'read2\')\n    self.read3 = test_utils.make_read(\'AGC\', start=2, cigar=\'3M\', name=\'read3\')\n    self.read4 = test_utils.make_read(\'AGC\', start=3, cigar=\'3M\', name=\'read4\')\n\n    self.expected_rows = {\n        \'ref\':\n            np.asarray(range(0, 3 * self.pic.num_channels),\n                       np.uint8).reshape(1, 3, self.pic.num_channels),\n        \'empty\':\n            np.zeros((1, 3, self.pic.num_channels), dtype=np.uint8),\n        \'read1\':\n            np.full((1, 3, self.pic.num_channels), 1, dtype=np.uint8),\n        \'read2\':\n            np.full((1, 3, self.pic.num_channels), 2, dtype=np.uint8),\n        \'read3\':\n            None,\n        \'read4\':\n            np.full((1, 3, self.pic.num_channels), 3, dtype=np.uint8),\n    }\n\n    # Setup our shared mocks.\n    mock_encoder = mock.Mock(spec=[\'encode_read\', \'encode_reference\'])\n    mock_encoder.encode_reference.return_value = self.expected_rows[\'ref\']\n\n    # pylint: disable=unused-argument\n    def get_read_row(dv_call, refbases, read, pos, alt_allele):\n      return self.expected_rows[read.fragment_name]\n\n    mock_encoder.encode_read.side_effect = get_read_row\n\n    self.mock_enc_ref = mock_encoder.encode_reference\n    self.mock_enc_read = mock_encoder.encode_read\n\n    self.pic._encoder = mock_encoder\n\n  def assertImageMatches(self, actual_image, *row_names):\n    """"""Checks that actual_image matches an image from constructed row_names.""""""\n    self.assertEqual(actual_image.shape,\n                     (self.pic.height, self.pic.width, self.pic.num_channels))\n    expected_image = np.vstack([self.expected_rows[name] for name in row_names])\n    npt.assert_equal(actual_image, expected_image)\n\n  def test_image_no_reads(self):\n    # This image is created just from reference and no reads. Checks that the\n    # function is listening to all of our image creation parameters (e.g.,\n    # reference_band_height, width, height, etc) and is filling the image with\n    # empty rows when it runs out of reads.\n    image = self.pic.build_pileup(self.dv_call, self.ref, [], {self.alt_allele})\n    self.mock_enc_ref.assert_called_once_with(self.ref)\n    test_utils.assert_not_called_workaround(self.mock_enc_read)\n    self.assertImageMatches(image, \'ref\', \'ref\', \'empty\', \'empty\')\n\n  def test_image_one_read(self):\n    # We add a single read to our image.\n    image = self.pic.build_pileup(self.dv_call, self.ref, [self.read1],\n                                  {self.alt_allele})\n    self.mock_enc_ref.assert_called_once_with(self.ref)\n    self.mock_enc_read.assert_called_once_with(self.dv_call, self.ref,\n                                               self.read1, 9, {self.alt_allele})\n    self.assertImageMatches(image, \'ref\', \'ref\', \'read1\', \'empty\')\n\n  def test_image_creation_with_more_reads_than_rows(self):\n    # Read1 should be dropped because there\'s only space for Read2 and Read4.\n    # If there are more reads than rows, a deterministic random subset is used.\n    image = self.pic.build_pileup(self.dv_call, self.ref,\n                                  [self.read1, self.read2, self.read4],\n                                  {self.alt_allele})\n    self.mock_enc_ref.assert_called_once_with(self.ref)\n    self.assertEqual(self.mock_enc_read.call_args_list, [\n        mock.call(self.dv_call, self.ref, self.read1, 9, {self.alt_allele}),\n        mock.call(self.dv_call, self.ref, self.read2, 9, {self.alt_allele}),\n        mock.call(self.dv_call, self.ref, self.read4, 9, {self.alt_allele}),\n    ])\n    self.assertImageMatches(image, \'ref\', \'ref\', \'read2\', \'read4\')\n\n  def test_image_creation_with_bad_read(self):\n    # Read 3 is bad (return value is None) so it should be skipped.\n    image = self.pic.build_pileup(self.dv_call, self.ref,\n                                  [self.read1, self.read3, self.read2],\n                                  {self.alt_allele})\n    self.mock_enc_ref.assert_called_once_with(self.ref)\n    self.assertEqual(self.mock_enc_read.call_args_list, [\n        mock.call(self.dv_call, self.ref, self.read1, 9, {self.alt_allele}),\n        mock.call(self.dv_call, self.ref, self.read3, 9, {self.alt_allele}),\n        mock.call(self.dv_call, self.ref, self.read2, 9, {self.alt_allele}),\n    ])\n    self.assertImageMatches(image, \'ref\', \'ref\', \'read1\', \'read2\')\n\n  def test_image_creation_with_all_reads_in_new_order(self):\n    # Read 3 is bad (return value is None) so it should be skipped. Read2 should\n    # also be dropped because there\'s only space for Read1 and Read4. If there\n    # are more reads than rows, a deterministic random subset is used.\n    image = self.pic.build_pileup(\n        self.dv_call, self.ref,\n        [self.read2, self.read3, self.read4, self.read1], {self.alt_allele})\n    self.mock_enc_ref.assert_called_once_with(self.ref)\n    self.assertEqual(self.mock_enc_read.call_args_list, [\n        mock.call(self.dv_call, self.ref, self.read2, 9, {self.alt_allele}),\n        mock.call(self.dv_call, self.ref, self.read3, 9, {self.alt_allele}),\n        mock.call(self.dv_call, self.ref, self.read4, 9, {self.alt_allele}),\n        mock.call(self.dv_call, self.ref, self.read1, 9, {self.alt_allele}),\n    ])\n    self.assertImageMatches(image, \'ref\', \'ref\', \'read1\', \'read4\')\n\n\nclass PileupImageCreatorTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.options = pileup_image.default_options()\n    self.options.width = 5\n    self.mock_ref_reader = mock.MagicMock(spec=fasta.IndexedFastaReader)\n    self.mock_ref_reader.query.return_value = \'ACGT\'\n    self.mock_ref_reader.is_valid.return_value = True\n    self.mock_sam_reader = mock.MagicMock()\n    self.mock_sam_reader.query.return_value = [\'read1\', \'read2\']\n    self.dv_call = _make_dv_call()\n    self.variant = self.dv_call.variant\n    self.pic = self._make_pic()\n\n  def _make_pic(self, **kwargs):\n    return pileup_image.PileupImageCreator(self.options, self.mock_ref_reader,\n                                           self.mock_sam_reader, **kwargs)\n\n  @parameterized.parameters(\n      (\'A\', [\'C\'], [{\'C\'}]),\n      (\'A\', [\'C\', \'G\'], [{\'C\'}, {\'G\'}, {\'C\', \'G\'}]),\n  )\n  def test_alt_combinations(self, ref, alts, expected):\n    variant = variants_pb2.Variant(reference_bases=ref, alternate_bases=alts)\n    self.assertEqual(expected, list(self.pic._alt_allele_combinations(variant)))\n\n  @parameterized.parameters(\n      (\'A\', [\'C\'], [{\'C\'}]),\n      (\'A\', [\'C\', \'G\'], [{\'C\'}, {\'G\'}]),\n  )\n  def test_alt_combinations_no_het_alt(self, ref, alts, expected):\n    options = pileup_image.default_options()\n    options.multi_allelic_mode = (\n        deepvariant_pb2.PileupImageOptions.NO_HET_ALT_IMAGES)\n    pic = pileup_image.PileupImageCreator(options, self.mock_ref_reader,\n                                          self.mock_sam_reader)\n    variant = variants_pb2.Variant(reference_bases=ref, alternate_bases=alts)\n    self.assertEqual(expected, list(pic._alt_allele_combinations(variant)))\n\n  def test_get_reference_bases_good_region(self):\n    self.dv_call.variant.start = 10\n    region = ranges.make_range(self.variant.reference_name, 8, 13)\n\n    actual = self.pic.get_reference_bases(self.variant)\n    self.assertEqual(\'ACGT\', actual)\n    self.mock_ref_reader.is_valid.assert_called_once_with(region)\n    self.mock_ref_reader.query.assert_called_once_with(region)\n\n  def test_get_reference_bases_bad_region_returns_none(self):\n    self.mock_ref_reader.is_valid.return_value = False\n    self.dv_call.variant.start = 3\n\n    self.assertIsNone(self.pic.get_reference_bases(self.variant))\n    test_utils.assert_called_once_workaround(self.mock_ref_reader.is_valid)\n    self.mock_ref_reader.query.assert_not_called()\n\n  def test_create_pileup_image_returns_none_for_bad_region(self):\n    self.mock_ref_reader.is_valid.return_value = False\n    self.dv_call.variant.start = 3\n    self.assertIsNone(self.pic.create_pileup_images(self.dv_call))\n    test_utils.assert_called_once_workaround(self.mock_ref_reader.is_valid)\n    self.mock_ref_reader.query.assert_not_called()\n\n  def test_create_pileup_image(self):\n    self.dv_call.variant.alternate_bases[:] = [\'C\', \'T\']\n\n    with mock.patch.object(\n        self.pic, \'build_pileup\', autospec=True) as mock_encoder:\n      mock_encoder.side_effect = [\'mi1\', \'mi2\', \'mi3\']\n\n      self.assertEqual([\n          ({\'C\'}, \'mi1\'),\n          ({\'T\'}, \'mi2\'),\n          ({\'C\', \'T\'}, \'mi3\'),\n      ], self.pic.create_pileup_images(self.dv_call))\n\n      def _expected_call(alts):\n        return mock.call(self.dv_call, self.mock_ref_reader.query.return_value,\n                         self.mock_sam_reader.query.return_value, alts)\n\n      self.assertEqual(mock_encoder.call_count, 3)\n      mock_encoder.assert_has_calls([\n          _expected_call({\'C\'}),\n          _expected_call({\'T\'}),\n          _expected_call({\'C\', \'T\'}),\n      ])\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/postprocess_variants.py,1,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Postprocess output from call_variants to produce a VCF file.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport collections\nimport copy\nimport itertools\nimport tempfile\nimport time\n\n\nfrom absl import flags\nfrom absl import logging\nimport numpy as np\nimport tensorflow as tf\n\nfrom third_party.nucleus.io import fasta\nfrom third_party.nucleus.io import sharded_file_utils\nfrom third_party.nucleus.io import tabix\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.protos import struct_pb2\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import errors\nfrom third_party.nucleus.util import genomics_math\nfrom third_party.nucleus.util import proto_utils\nfrom third_party.nucleus.util import ranges\nfrom third_party.nucleus.util import variant_utils\nfrom third_party.nucleus.util import variantcall_utils\nfrom third_party.nucleus.util import vcf_constants\nfrom deepvariant import dv_constants\nfrom deepvariant import dv_vcf_constants\nfrom deepvariant import haplotypes\nfrom deepvariant import logging_level\nfrom deepvariant import tf_utils\nfrom deepvariant import vcf_stats\nfrom deepvariant.protos import deepvariant_pb2\nfrom deepvariant.python import postprocess_variants as postprocess_variants_lib\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    \'infile\', None,\n    \'Required. Path(s) to CallVariantOutput protos in TFRecord format to \'\n    \'postprocess. These should be the complete set of outputs for \'\n    \'call_variants.py.\')\nflags.DEFINE_string(\n    \'outfile\', None,\n    \'Required. Destination path where we will write output variant calls in \'\n    \'VCF format.\')\nflags.DEFINE_string(\n    \'ref\', None,\n    \'Required. Genome reference in FAI-indexed FASTA format. Used to determine \'\n    \'the sort order for the emitted variants and the VCF header.\')\nflags.DEFINE_float(\n    \'qual_filter\', 1.0,\n    \'Any variant with QUAL < qual_filter will be filtered in the VCF file.\')\nflags.DEFINE_float(\n    \'cnn_homref_call_min_gq\', 20.0,\n    \'All CNN RefCalls whose GQ is less than this value will have ./. genotype \'\n    \'instead of 0/0.\')\nflags.DEFINE_float(\n    \'multi_allelic_qual_filter\', 1.0,\n    \'The qual value below which to filter multi-allelic variants.\')\nflags.DEFINE_string(\n    \'nonvariant_site_tfrecord_path\', None,\n    \'Optional. Path(s) to the non-variant sites protos in TFRecord format to \'\n    \'convert to gVCF file. This should be the complete set of outputs from the \'\n    \'--gvcf flag of make_examples.py.\')\nflags.DEFINE_string(\n    \'gvcf_outfile\', None,\n    \'Optional. Destination path where we will write the Genomic VCF output.\')\nflags.DEFINE_boolean(\n    \'group_variants\', True, \'If using vcf_candidate_importer and multi-allelic \'\n    \'sites are split across multiple lines in VCF, set to False so that \'\n    \'variants are not grouped when transforming CallVariantsOutput to \'\n    \'Variants.\')\nflags.DEFINE_boolean(\n    \'vcf_stats_report\', True, \'Optional. Output a visual report (HTML) of \'\n    \'statistics about the output VCF at the same base path given by --outfile.\')\n\n# Some format fields are indexed by alt allele, such as AD (depth by allele).\n# These need to be cleaned up if we remove any alt alleles. Any info field\n# listed here will be have its values cleaned up if we\'ve removed any alt\n# alleles.\n# Each tuple contains: field name, ref_is_zero.\n_ALT_ALLELE_INDEXED_FORMAT_FIELDS = frozenset([(\'AD\', True), (\'VAF\', False)])\n\n# The number of places past the decimal point to round QUAL estimates to.\n_QUAL_PRECISION = 7\n# The genotype likelihood of the gVCF alternate allele for variant calls.\n_GVCF_ALT_ALLELE_GL = -99\n\n# FASTA cache size. Span 300 Mb so that we query each chromosome at most once.\n_FASTA_CACHE_SIZE = 300000000\n\n# When this was set, it\'s about 20 seconds per log.\n_LOG_EVERY_N = 100000\n\n\ndef _extract_single_sample_name(record):\n  """"""Returns the name of the single sample within the CallVariantsOutput file.\n\n  Args:\n    record: A deepvariant_pb2.CallVariantsOutput record.\n\n  Returns:\n    The name of the single individual in the first proto in the file.\n\n  Raises:\n    ValueError: There is not exactly one VariantCall in the proto or the\n        call_set_name of the VariantCall is not populated.\n  """"""\n  variant = record.variant\n  call = variant_utils.only_call(variant)\n  name = call.call_set_name\n  if not name:\n    raise ValueError(\n        \'Error extracting name: no call_set_name set: {}\'.format(record))\n\n  return name\n\n\ndef compute_filter_fields(variant, min_quality):\n  """"""Computes the filter fields for this variant.\n\n  Variant filters are generated based on its quality score value and particular\n  genotype call.\n\n  Args:\n    variant: Variant to filter.\n    min_quality: Minimum acceptable phred scaled variant detection probability.\n\n  Returns:\n    Filter field strings to be added to the variant.\n  """"""\n  if variant_utils.genotype_type(variant) == variant_utils.GenotypeType.hom_ref:\n    return [dv_vcf_constants.DEEP_VARIANT_REF_FILTER]\n  elif variant.quality < min_quality:\n    return [dv_vcf_constants.DEEP_VARIANT_QUAL_FILTER]\n  else:\n    return [dv_vcf_constants.DEEP_VARIANT_PASS]\n\n\ndef most_likely_genotype(predictions, ploidy=2, n_alleles=2):\n  """"""Gets the most likely genotype from predictions.\n\n  From https://samtools.github.io/hts-specs/VCFv4.3.pdf:\n\n  Genotype Ordering. In general case of ploidy P and N alternate alleles (0 is\n  the REF and 1..N the alternate alleles), the ordering of genotypes for the\n  likelihoods can be expressed by the following pseudocode with as many nested\n  loops as ploidy:\n\n  * Note that we use inclusive for loop boundaries.\n  for a_P = 0 . . . N\n    for a_P-1 = 0 . . . aP\n      . . .\n      for a_1 = 0 . . . a2\n        println a1 a2 . . . aP\n\n  Alternatively, the same can be achieved recursively with the following\n  pseudocode:\n\n    Ordering (P , N , suffix =""""):\n      for a in 0 . . . N\n        if (P == 1) println str (a) + suffix\n        if (P > 1) Ordering (P -1 , a, str (a) + suffix)\n\n  Examples:\n  * for P=2 and N=1, the ordering is 00,01,11\n  * for P=2 and N=2, the ordering is 00,01,11,02,12,22\n  * for P=3 and N=2, the ordering is 000,001,011,111,002,012,112,022,122,222\n  * for P=1, the index of the genotype a is a\n  * for P=2, the index of the genotype ""a/b"", where a <= b, is b(b + 1)/2 + a\n  * for P=2 and arbitrary N, the ordering can be easily derived from a\n    triangular matrix:\n      b / a 0 1 2 3\n      0     0\n      1     1 2\n      2     3 4 5\n      3     6 7 8 9\n\n  Args:\n    predictions: N element array-like. The real-space probabilities of each\n      genotype state for this variant. The number of elements in predictions is\n      related to ploidy and n_alleles is given by N = choose(ploidy + n_alleles\n      - 1, n_alleles -1)\n      for more information see:\n      http://genome.sph.umich.edu/wiki/Relationship_between_Ploidy,_Alleles_and_Genotypes\n    ploidy: int >= 1. The ploidy (e.g., number of chromosomes) of this sample.\n    n_alleles: int >= 2. The number of alleles (ref + n_alts).\n\n  Returns:\n    Two values. The first is the index of the most likely prediction in\n    predictions. The second is a list of P elements with the VCF-style genotype\n    indices corresponding to this index. For example, with P = 2 and an index of\n    1, this returns the value (1, [0, 1]).\n\n  Raises:\n    NotImplementedError: if ploidy != 2 as this not yet implemented.\n    ValueError: If n_alleles < 2.\n    ValueError: If we cannot determine the genotype given prediction, n_alts,\n      and ploidy.\n  """"""\n  # redacted\n  if ploidy != 2:\n    raise NotImplementedError(\'Ploidy != 2 not yet implemented.\')\n  if n_alleles < 2:\n    raise ValueError(\'n_alleles must be >= 2 but got\', n_alleles)\n  # redacted\n  # number of elements. But that would involve calculating the binomial\n  # coefficient of n_alleles and ploidy, which would be expensive. Probably\n  # need to memoize the whole function if we are going to add this.\n  index_of_max = np.argmax(predictions)\n  # This is the general case solution for fixed ploidy of 2 and arbitrary\n  # n_alleles. We should generalize this code to the arbitrary ploidy case when\n  # needed and memoize the mapping here.\n  index = 0\n  for h1 in range(0, n_alleles + 1):\n    for h2 in range(0, h1 + 1):\n      if index == index_of_max:\n        return index, [h2, h1]\n      index += 1\n  raise ValueError(\'No corresponding GenotypeType for predictions\', predictions)\n\n\ndef uncall_homref_gt_if_lowqual(variant, min_homref_gq):\n  """"""Converts genotype to ""./."" if variant is CNN RefCall and has low GQ.\n\n  If the variant has ""RefCall"" filter (which means an example was created for\n  this site but CNN didn\'t call this as variant) and if the GQ is less than\n  the given min_homref_gq threshold, set the genotype of the variant proto\n  to ""./."". See http://internal for more info.\n\n  Args:\n    variant: third_party.nucleus.protos.Variant proto.\n    min_homref_gq: float.\n  """"""\n  vcall = variant_utils.only_call(variant)\n  if (variant.filter == [dv_vcf_constants.DEEP_VARIANT_REF_FILTER] and\n      variantcall_utils.get_gq(vcall) < min_homref_gq):\n    vcall.genotype[:] = [-1, -1]\n\n\ndef add_call_to_variant(variant, predictions, qual_filter=0, sample_name=None):\n  """"""Fills in Variant record using the prediction probabilities.\n\n  This functions sets the call[0].genotype, call[0].info[\'GQ\'],\n  call[0].genotype_probabilities, variant.filter, and variant.quality fields of\n  variant based on the genotype likelihoods in predictions.\n\n  Args:\n    variant: third_party.nucleus.protos.Variant protobuf to be filled in with\n      info derived from predictions.\n    predictions: N element array-like. The real-space probabilities of each\n      genotype state for this variant.\n    qual_filter: float. If predictions implies that this isn\'t a reference call\n      and the QUAL of the prediction isn\'t larger than qual_filter variant will\n      be marked as FILTERed.\n    sample_name: str. The name of the sample to assign to the Variant proto\n      call_set_name field.\n\n  Returns:\n    A Variant record.\n\n  Raises:\n    ValueError: If variant doesn\'t have exactly one variant.call record.\n  """"""\n  call = variant_utils.only_call(variant)\n  n_alleles = len(variant.alternate_bases) + 1\n  index, genotype = most_likely_genotype(predictions, n_alleles=n_alleles)\n  gq, variant.quality = compute_quals(predictions, index)\n  call.call_set_name = sample_name\n  variantcall_utils.set_gt(call, genotype)\n  variantcall_utils.set_gq(call, gq)\n  gls = [genomics_math.perror_to_bounded_log10_perror(gp) for gp in predictions]\n  variantcall_utils.set_gl(call, gls)\n  variant.filter[:] = compute_filter_fields(variant, qual_filter)\n  uncall_homref_gt_if_lowqual(variant, FLAGS.cnn_homref_call_min_gq)\n  return variant\n\n\ndef compute_quals(predictions, prediction_index):\n  """"""Computes GQ and QUAL values from a set of prediction probabilities.\n\n  Prediction probabilities are represented as a probability distribution over\n  the N genotype states (e.g., for 3 genotype states {HOM_REF, HET, HOM_VAR}).\n  Genotype Quality (or GQ) represents the PHRED scaled confidence in the\n  particular genotype assignment. Likewise the QUAL representes the PHRED scaled\n  confidence in variant as compared to reference, that is, P(NON_REF) / P(ALL)\n  which in the diploid genotype case is P(HET) + P(HOM_VAR) / P(ALL). These\n  quality scores are capped by _MAX_CONFIDENCE.\n\n  Args:\n    predictions: N element array-like. The real-space probabilities of each\n      genotype state for this variant.\n    prediction_index: int. The actual called genotype from the distribution.\n\n  Returns:\n    GQ and QUAL values for output in a Variant record.\n  """"""\n  # GQ is prob(genotype) / prob(all genotypes)\n  # GQ is rounded to the nearest integer to comply with the VCF spec.\n  gq = int(\n      np.around(\n          genomics_math.ptrue_to_bounded_phred(predictions[prediction_index])))\n  # QUAL is prob(variant genotype) / prob(all genotypes)\n  # Taking the min to avoid minor numerical issues than can push sum > 1.0.\n  # redacted\n  #   genomics_math.perror_to_phred(max(predictions[0], min_ref_confidence))\n  # where min_ref_confidence is roughly 1.25e-10 (producing a qual of 99).\n  qual = genomics_math.ptrue_to_bounded_phred(min(sum(predictions[1:]), 1.0))\n  rounded_qual = round(qual, _QUAL_PRECISION)\n  return gq, rounded_qual\n\n\ndef expected_alt_allele_indices(num_alternate_bases):\n  """"""Returns (sorted) expected list of alt_allele_indices, given #alt bases.""""""\n  num_alleles = num_alternate_bases + 1\n  alt_allele_indices_list = [\n      sorted(list(set(x) - {0}))\n      for x in itertools.combinations(range(num_alleles), 2)\n  ]\n  # alt_allele_indices starts from 0, where 0 refers to the first alt allele.\n  # pylint: disable=g-complex-comprehension\n  return sorted([[i - 1\n                  for i in alt_allele_indices]\n                 for alt_allele_indices in alt_allele_indices_list])\n  # pylint: enable=g-complex-comprehension\n\n\ndef _check_alt_allele_indices(call_variants_outputs):\n  """"""Returns True if and only if the alt allele indices are valid.""""""\n  all_alt_allele_indices = sorted([\n      list(call_variants_output.alt_allele_indices.indices)\n      for call_variants_output in call_variants_outputs\n  ])\n  if all_alt_allele_indices != expected_alt_allele_indices(\n      len(call_variants_outputs[0].variant.alternate_bases)):\n    logging.warning(\n        \'Alt allele indices found from call_variants_outputs for \'\n        \'variant %s is %s, which is invalid.\', call_variants_outputs[0].variant,\n        all_alt_allele_indices)\n    return False\n  return True\n\n\ndef is_valid_call_variants_outputs(call_variants_outputs):\n  """"""Returns True if the call_variants_outputs follows our assumptions.\n\n  Args:\n    call_variants_outputs: list of CallVariantsOutput to check.\n\n  Returns:\n    True if the sanity check passes.\n  """"""\n\n  if not call_variants_outputs:\n    return True  # An empty list is a degenerate case.\n\n  if not _check_alt_allele_indices(call_variants_outputs):\n    return False\n\n  first_call, other_calls = call_variants_outputs[0], call_variants_outputs[1:]\n  # Sanity check that all call_variants_outputs have the same `variant`.\n  for call_to_check in other_calls:\n    if first_call.variant != call_to_check.variant:\n      logging.warning(\n          \'Expected all inputs to merge_predictions to have the \'\n          \'same `variant`, but getting %s and %s.\', first_call.variant,\n          call_to_check.variant)\n      return False\n  return True\n\n\ndef convert_call_variants_outputs_to_probs_dict(canonical_variant,\n                                                call_variants_outputs,\n                                                alt_alleles_to_remove):\n  """"""Converts a list of CallVariantsOutput to an internal allele probs dict.\n\n  Args:\n    canonical_variant: variants_pb2.Variant.\n    call_variants_outputs: list of CallVariantsOutput.\n    alt_alleles_to_remove: set of strings. Alleles to remove.\n\n  Returns:\n    Dictionary of {(allele1, allele2): list of probabilities},\n    where allele1 and allele2 are strings.\n  """"""\n  flattened_dict = collections.defaultdict(lambda: [])\n  if not call_variants_outputs:\n    return flattened_dict\n\n  for call_variants_output in call_variants_outputs:\n    allele_set1 = frozenset([canonical_variant.reference_bases])\n    allele_set2 = frozenset(\n        canonical_variant.alternate_bases[index]\n        for index in call_variants_output.alt_allele_indices.indices)\n    if alt_alleles_to_remove.intersection(allele_set2):\n      continue\n    p11, p12, p22 = call_variants_output.genotype_probabilities\n    for (set1, set2, p) in [(allele_set1, allele_set1, p11),\n                            (allele_set1, allele_set2, p12),\n                            (allele_set2, allele_set2, p22)]:\n      for indices in itertools.product(set1, set2):\n        flattened_dict[indices].append(p)\n  return flattened_dict\n\n\ndef get_alt_alleles_to_remove(call_variants_outputs, qual_filter):\n  """"""Returns all the alt alleles with quality below qual_filter.\n\n  Quality is defined as (1-p(ref/ref)). This removes all alt alleles whose\n  quality is below the filter value, with the exception that if the set of\n  removed alt alleles covers everything in the alternate_bases, the single alt\n  allele where the 1-p(ref/ref) is the highest is retained.\n\n  Args:\n    call_variants_outputs: list of CallVariantsOutput.\n    qual_filter: double. The qual value below which to filter variants.\n\n  Returns:\n    Set of strings: alt alleles to remove.\n  """"""\n  alt_alleles_to_remove = set()  # first alt is represented as 0.\n  if not qual_filter or not call_variants_outputs:\n    return alt_alleles_to_remove\n\n  max_qual, max_qual_allele = None, None\n  canonical_variant = call_variants_outputs[0].variant\n  for call_variants_output in call_variants_outputs:\n    # Go through the ones where alt_allele_indices has\n    # exactly one element. There are the pileup images that contains information\n    # like:\n    #    p00, p01, p11\n    # or p00, p02, p22\n    # ...p00, p0N, pNN\n    if len(call_variants_output.alt_allele_indices.indices) == 1:\n      # From here, we want to see which ones of these alt alleles (1-N) that we\n      # can skip. We can use the concept of QUAL in VCF, and filter out ones\n      # where QUAL < FLAGS.qual_filter. This is because if QUAL is too low,\n      # it means it is unlikely this has a variant genotype.\n      _, qual = compute_quals(\n          call_variants_output.genotype_probabilities, prediction_index=0)\n      alt_allele_index = call_variants_output.alt_allele_indices.indices[0]\n      # Keep track of one alt allele with the highest qual score.\n      if max_qual is None or max_qual < qual:\n        max_qual, max_qual_allele = (\n            qual, canonical_variant.alternate_bases[alt_allele_index])\n      if qual < qual_filter:\n        alt_alleles_to_remove.add(\n            canonical_variant.alternate_bases[alt_allele_index])\n\n  # If all alt alleles are below `qual_filter`, keep at least one.\n  if len(alt_alleles_to_remove) == len(canonical_variant.alternate_bases):\n    alt_alleles_to_remove -= set([max_qual_allele])\n  # redacted\n  return alt_alleles_to_remove\n\n\n# redacted\nclass AlleleRemapper(object):\n  """"""Facilitates removing alt alleles from a Variant.\n\n  This class provides a one-to-shop for managing the information needed to\n  remove alternative alleles from Variant. It provides functions and properties\n  to get the original alts, the new alts, and asking if alleles (strings) or\n  indices (integers) should be retained or eliminated.\n  """"""\n\n  # redacted\n\n  def __init__(self, original_alt_alleles, alleles_to_remove):\n    self.original_alts = list(original_alt_alleles)\n    self.alleles_to_remove = set(alleles_to_remove)\n\n  def keep_index(self, allele_index, ref_is_zero=False):\n    if ref_is_zero:\n      return True if allele_index == 0 else self.keep_index(allele_index - 1)\n    else:\n      return self.original_alts[allele_index] not in self.alleles_to_remove\n\n  def retained_alt_alleles(self):\n    return [\n        alt for alt in self.original_alts if alt not in self.alleles_to_remove\n    ]\n\n  def reindex_allele_indexed_fields(self, variant, fields):\n    """"""Updates variant.call fields indexed by ref + alt_alleles.\n\n    Args:\n      variant: Variant proto. We will update the info fields of the Variant.call\n        protos.\n      fields: Iterable of string. Each string should provide a key to an\n        alternative allele indexed field in VariantCall.info fields. Each field\n        specified here will be updated to remove values associated with alleles\n        no longer wanted according to this remapper object.\n    """"""\n    for field_info in fields:\n      field = field_info[0]\n      ref_is_zero = field_info[1]\n      for call in variant.calls:\n        if field in call.info:\n          entry = call.info[field]\n          updated = [\n              v for i, v in enumerate(entry.values)\n              if self.keep_index(i, ref_is_zero=ref_is_zero)\n          ]\n          # We cannot do entry.values[:] = updated as the ListValue type ""does\n          # not support assignment"" so we have to do this grossness.\n          del entry.values[:]\n          entry.values.extend(updated)\n\n\ndef prune_alleles(variant, alt_alleles_to_remove):\n  """"""Remove the alt alleles in alt_alleles_to_remove from canonical_variant.\n\n  Args:\n    variant: variants_pb2.Variant.\n    alt_alleles_to_remove: iterable of str. Alt alleles to remove from variant.\n\n  Returns:\n    variants_pb2.Variant with the alt alleles removed from alternate_bases.\n  """"""\n  # If we aren\'t removing any alt alleles, just return the unmodified variant.\n  if not alt_alleles_to_remove:\n    return variant\n\n  new_variant = variants_pb2.Variant()\n  new_variant.CopyFrom(variant)\n\n  # Cleanup any VariantCall.info fields indexed by alt allele.\n  remapper = AlleleRemapper(variant.alternate_bases, alt_alleles_to_remove)\n  remapper.reindex_allele_indexed_fields(new_variant,\n                                         _ALT_ALLELE_INDEXED_FORMAT_FIELDS)\n  new_variant.alternate_bases[:] = remapper.retained_alt_alleles()\n\n  return new_variant\n\n\ndef simplify_alleles(variant):\n  """"""Replaces the alleles in variants with their simplified versions.\n\n  This function takes a variant and replaces its ref and alt alleles with those\n  produced by a call to variant_utils.simplify_alleles() to remove common\n  postfix bases in the alleles that may be present due to pruning away alleles.\n\n  Args:\n    variant: learning.genomics.genomics.Variant proto we want to simplify.\n\n  Returns:\n    variant with its ref and alt alleles replaced with their simplified\n      equivalents.\n  """"""\n  simplified_alleles = variant_utils.simplify_alleles(variant.reference_bases,\n                                                      *variant.alternate_bases)\n  variant.reference_bases = simplified_alleles[0]\n  variant.alternate_bases[:] = simplified_alleles[1:]\n  variant.end = variant.start + len(variant.reference_bases)\n  return variant\n\n\ndef merge_predictions(call_variants_outputs, qual_filter=None):\n  """"""Merges the predictions from the multi-allelic calls.""""""\n  # See the logic described in the class PileupImageCreator pileup_image.py\n  #\n  # Because of the logic above, this function expects all cases above to have\n  # genotype_predictions that we can combine from.\n  if not call_variants_outputs:\n    raise ValueError(\'Expected 1 or more call_variants_outputs.\')\n\n  if not is_valid_call_variants_outputs(call_variants_outputs):\n    raise ValueError(\'`call_variants_outputs` did not pass sanity check.\')\n\n  first_call, other_calls = call_variants_outputs[0], call_variants_outputs[1:]\n  canonical_variant = first_call.variant\n  if not other_calls:\n    return canonical_variant, first_call.genotype_probabilities\n\n  alt_alleles_to_remove = get_alt_alleles_to_remove(call_variants_outputs,\n                                                    qual_filter)\n  flattened_probs_dict = convert_call_variants_outputs_to_probs_dict(\n      canonical_variant, call_variants_outputs, alt_alleles_to_remove)\n\n  canonical_variant = prune_alleles(canonical_variant, alt_alleles_to_remove)\n  predictions = [\n      min(flattened_probs_dict[(m, n)]) for _, _, m, n in\n      variant_utils.genotype_ordering_in_likelihoods(canonical_variant)\n  ]\n  if sum(predictions) == 0:\n    predictions = [1.0] * len(predictions)\n  denominator = sum(predictions)\n  # Note the simplify_alleles call *must* happen after the predictions\n  # calculation above. flattened_probs_dict is indexed by alt allele, and\n  # simplify can change those alleles so we cannot simplify until afterwards.\n  canonical_variant = simplify_alleles(canonical_variant)\n  return canonical_variant, [i / denominator for i in predictions]\n\n\ndef write_variants_to_vcf(variant_iterable, output_vcf_path, header):\n  """"""Writes Variant protos to a VCF file.\n\n  Args:\n    variant_iterable: iterable. An iterable of sorted Variant protos.\n    output_vcf_path: str. Output file in VCF format.\n    header: VcfHeader proto. The VCF header to use for writing the variants.\n  """"""\n  logging.info(\'Writing output to VCF file: %s\', output_vcf_path)\n  with vcf.VcfWriter(\n      output_vcf_path, header=header, round_qualities=True) as writer:\n    for idx, variant in enumerate(variant_iterable):\n      logging.log_every_n(logging.INFO, \'%s variants written.\', _LOG_EVERY_N,\n                          idx + 1)\n      writer.write(variant)\n\n\ndef _zero_scale_gl(variant):\n  """"""Zero-scales GL to mimic write-then-read.\n\n  When writing variants using VcfWriter, GLs are converted to PLs, which is an\n  integer format scaled so the most likely genotype has value 0. This function\n  modifies the input variant to mimic this transformation of GL -> PL -> GL.\n\n  Args:\n    variant: Variant proto. The variant to scale.\n\n  Returns:\n    variant: Variant proto. The input variant with its GLs modified.\n  """"""\n  call = variant_utils.only_call(variant)\n  max_gl = max(call.genotype_likelihood)\n  call.genotype_likelihood[:] = [\n      (gl - max_gl) for gl in call.genotype_likelihood\n  ]\n  return variant\n\n\ndef _sort_grouped_variants(group):\n  return sorted(group, key=lambda x: sorted(x.alt_allele_indices.indices))\n\n\ndef _transform_call_variants_output_to_variants(input_sorted_tfrecord_path,\n                                                qual_filter,\n                                                multi_allelic_qual_filter,\n                                                sample_name, group_variants):\n  """"""Yields Variant protos in sorted order from CallVariantsOutput protos.\n\n  Variants present in the input TFRecord are converted to Variant protos, with\n  the following filters applied: 1) variants are omitted if their quality is\n  lower than the `qual_filter` threshold. 2) multi-allelic variants omit\n  individual alleles whose qualities are lower than the\n  `multi_allelic_qual_filter` threshold.\n\n  Args:\n    input_sorted_tfrecord_path: str. TFRecord format file containing sorted\n      CallVariantsOutput protos.\n    qual_filter: double. The qual value below which to filter variants.\n    multi_allelic_qual_filter: double. The qual value below which to filter\n      multi-allelic variants.\n    sample_name: str. Sample name to write to VCF file.\n    group_variants: bool. If true, group variants that have same start and end\n      position.\n\n  Yields:\n    Variant protos in sorted order representing the CallVariantsOutput calls.\n  """"""\n  group_fn = None\n  if group_variants:\n    group_fn = lambda x: variant_utils.variant_range(x.variant)\n  for _, group in itertools.groupby(\n      tfrecord.read_tfrecords(\n          input_sorted_tfrecord_path, proto=deepvariant_pb2.CallVariantsOutput),\n      group_fn):\n    outputs = _sort_grouped_variants(group)\n    canonical_variant, predictions = merge_predictions(\n        outputs, multi_allelic_qual_filter)\n    variant = add_call_to_variant(\n        canonical_variant,\n        predictions,\n        qual_filter=qual_filter,\n        sample_name=sample_name)\n    yield variant\n\n\ndef _get_contig_based_variant_sort_keyfn(contigs):\n  """"""Returns a callable used to sort variants based on genomic position.\n\n  Args:\n    contigs: list(ContigInfo). The list of contigs in the desired sort order.\n\n  Returns:\n    A callable that takes a single Variant proto as input and returns a value\n    that sorts based on contig and then start position. Note that if the variant\n    has a contig not represented in the list of contigs this will raise\n    IndexError.\n  """"""\n  contig_index = {contig.name: ix for ix, contig in enumerate(contigs)}\n\n  def keyfn(variant):\n    return contig_index[variant.reference_name], variant.start\n\n  return keyfn\n\n\ndef _get_contig_based_lessthan(contigs):\n  """"""Returns a callable that compares variants on genomic position.\n\n  The returned function takes two arguments, both of which should be Variant\n  protos or None. The function returns True if and only if the first Variant is\n  strictly less than the second, which occurs if the first variant is on a\n  previous chromosome or is on the same chromosome and its entire span lies\n  before the start position of the second variant. `None` is treated as a\n  sentinel value that does not compare less than any valid Variant.\n\n  Args:\n    contigs: list(ContigInfo). The list of contigs in the desired sort order.\n\n  Returns:\n    A callable that takes two Variant protos as input and returns True iff the\n    first is strictly less than the second. Note that if the variant has a\n    contig not represented in the list of contigs this will raise IndexError.\n  """"""\n  contig_index = {contig.name: i for i, contig in enumerate(contigs)}\n\n  def lessthanfn(variant1, variant2):\n    if variant1 is None:\n      return False\n    if variant2 is None:\n      return True\n    contig1 = contig_index[variant1.reference_name]\n    contig2 = contig_index[variant2.reference_name]\n    return (contig1 < contig2 or\n            (contig1 == contig2 and variant1.end <= variant2.start))\n\n  return lessthanfn\n\n\ndef _create_record_from_template(template, start, end, fasta_reader):\n  """"""Returns a copy of the template variant with the new start and end.\n\n  Updates to the start position cause a different reference base to be set.\n\n  Args:\n    template: third_party.nucleus.protos.Variant. The template variant whose\n      non-location and reference base information to use.\n    start: int. The desired new start location.\n    end: int. The desired new end location.\n    fasta_reader: GenomeReferenceFai object. The reader used to determine the\n      correct start base to use for the updated variant.\n\n  Returns:\n    An updated third_party.nucleus.protos.Variant with the proper start, end,\n    and reference base set and all other fields inherited from the template.\n  """"""\n  retval = copy.deepcopy(template)\n  retval.start = start\n  retval.end = end\n  if start != template.start:\n    retval.reference_bases = fasta_reader.query(\n        ranges.make_range(retval.reference_name, start, start + 1))\n  return retval\n\n\ndef _transform_to_gvcf_record(variant):\n  """"""Modifies a variant to include gVCF allele and associated likelihoods.\n\n  Args:\n    variant: third_party.nucleus.protos.Variant. The Variant to modify.\n\n  Returns:\n    The variant after applying the modification to its alleles and\n    allele-related FORMAT fields.\n  """"""\n  if vcf_constants.GVCF_ALT_ALLELE not in variant.alternate_bases:\n    variant.alternate_bases.append(vcf_constants.GVCF_ALT_ALLELE)\n    # Add one new GL for het allele/gVCF for each of the other alleles, plus one\n    # for the homozygous gVCF allele.\n    num_new_gls = len(variant.alternate_bases) + 1\n    call = variant_utils.only_call(variant)\n    call.genotype_likelihood.extend([_GVCF_ALT_ALLELE_GL] * num_new_gls)\n    if call.info and \'AD\' in call.info:\n      call.info[\'AD\'].values.extend([struct_pb2.Value(int_value=0)])\n    if call.info and \'VAF\' in call.info:\n      call.info[\'VAF\'].values.extend([struct_pb2.Value(number_value=0)])\n\n  return variant\n\n\ndef merge_and_write_variants_and_nonvariants(variant_iterable,\n                                             nonvariant_iterable, lessthan,\n                                             fasta_reader, vcf_writer,\n                                             gvcf_writer):\n  """"""Writes records consisting of the merging of variant and non-variant sites.\n\n  The merging strategy used for single-sample records is to emit variants\n  without modification. Any non-variant sites that overlap a variant are\n  truncated to only report on regions not affected by the variant. Note that\n  Variants are represented using zero-based half-open coordinates, so a VCF\n  record of `chr1  10  A  T` would have `start=9` and `end=10`.\n\n  Args:\n    variant_iterable: Iterable of Variant protos. A sorted iterable of the\n      variants to merge.\n    nonvariant_iterable: Iterable of Variant protos. A sorted iterable of the\n      non-variant sites to merge.\n    lessthan: Callable. A function that takes two Variant protos as input and\n      returns True iff the first argument is located ""before"" the second and the\n      variants do not overlap.\n    fasta_reader: GenomeReferenceFai object. The reference genome reader used to\n      ensure gVCF records have the correct reference base.\n    vcf_writer: VcfWriter. Writes variants to VCF.\n    gvcf_writer: VcfWriter. Writes merged variants and nonvariants to gVCF.\n  """"""\n\n  def next_or_none(iterable):\n    try:\n      return next(iterable)\n    except StopIteration:\n      return None\n\n  variant = next_or_none(variant_iterable)\n  nonvariant = next_or_none(nonvariant_iterable)\n\n  while variant is not None or nonvariant is not None:\n    if lessthan(variant, nonvariant):\n      vcf_writer.write(variant)\n      gvcf_variant = _transform_to_gvcf_record(_zero_scale_gl(variant))\n      gvcf_writer.write(gvcf_variant)\n      variant = next_or_none(variant_iterable)\n      continue\n    elif lessthan(nonvariant, variant):\n      gvcf_writer.write(nonvariant)\n      nonvariant = next_or_none(nonvariant_iterable)\n      continue\n    else:\n      # The variant and non-variant are on the same contig and overlap.\n      assert max(variant.start, nonvariant.start) < min(\n          variant.end, nonvariant.end), \'{} and {}\'.format(variant, nonvariant)\n      if nonvariant.start < variant.start:\n        # Write a non-variant region up to the start of the variant.\n        v = _create_record_from_template(nonvariant, nonvariant.start,\n                                         variant.start, fasta_reader)\n        gvcf_writer.write(v)\n      if nonvariant.end > variant.end:\n        # There is an overhang of the non-variant site after the variant is\n        # finished, so update the non-variant to point to that.\n        nonvariant = _create_record_from_template(nonvariant, variant.end,\n                                                  nonvariant.end, fasta_reader)\n      else:\n        # This non-variant site is subsumed by a Variant. Ignore it.\n        nonvariant = next_or_none(nonvariant_iterable)\n\n\ndef _get_base_path(input_vcf):\n  """"""Returns the base path for the output files.\n\n  Args:\n    input_vcf: string. Path to VCF for which to compute stats.\n\n  Returns:\n    A string with the base path.\n  """"""\n  if input_vcf.endswith(\'.vcf\'):\n    return input_vcf[:-4]\n  elif input_vcf.endswith(\'.vcf.gz\'):\n    return input_vcf[:-7]\n  else:\n    return input_vcf\n\n\ndef _decide_to_use_csi(contigs):\n  """"""Return True if CSI index is to be used over tabix index format.\n\n  If the length of any reference chromosomes exceeds 512M\n  (here we use 5e8 to keep a safety margin), we will choose csi\n  as the index format. Otherwise we use tbi as default.\n\n  Args:\n    contigs: list of contigs.\n\n  Returns:\n    A boolean variable indicating if the csi format is to be used or not.\n  """"""\n  max_chrom_length = max([c.n_bases for c in contigs])\n  return max_chrom_length > 5e8\n\n\ndef build_index(vcf_file, csi=False):\n  """"""A helper function for indexing VCF files.\n\n  Args:\n    vcf_file: string. Path to the VCF file to be indexed.\n    csi: bool. If true, index using the CSI format.\n  """"""\n\n  if csi:\n    tabix.build_csi_index(vcf_file, min_shift=14)\n  else:\n    tabix.build_index(vcf_file)\n\n\ndef main(argv=()):\n  with errors.clean_commandline_error_exit():\n    if len(argv) > 1:\n      errors.log_and_raise(\n          \'Command line parsing failure: postprocess_variants does not accept \'\n          \'positional arguments but some are present on the command line: \'\n          \'""{}"".\'.format(str(argv)), errors.CommandLineError)\n    del argv  # Unused.\n\n    if (not FLAGS.nonvariant_site_tfrecord_path) != (not FLAGS.gvcf_outfile):\n      errors.log_and_raise(\n          \'gVCF creation requires both nonvariant_site_tfrecord_path and \'\n          \'gvcf_outfile flags to be set.\', errors.CommandLineError)\n\n    proto_utils.uses_fast_cpp_protos_or_die()\n\n    logging_level.set_from_flag()\n\n    fasta_reader = fasta.IndexedFastaReader(\n        FLAGS.ref, cache_size=_FASTA_CACHE_SIZE)\n    contigs = fasta_reader.header.contigs\n    paths = sharded_file_utils.maybe_generate_sharded_filenames(FLAGS.infile)\n    # Read one CallVariantsOutput record and extract the sample name from it.\n    # Note that this assumes that all CallVariantsOutput protos in the infile\n    # contain a single VariantCall within their constituent Variant proto, and\n    # that the call_set_name is identical in each of the records.\n    record = tf_utils.get_one_example_from_examples_path(\n        \',\'.join(paths), proto=deepvariant_pb2.CallVariantsOutput)\n    if record is None:\n      logging.info(\'call_variants_output is empty. Writing out empty VCF.\')\n      sample_name = dv_constants.DEFAULT_SAMPLE_NAME\n      variant_generator = iter([])\n    else:\n      sample_name = _extract_single_sample_name(record)\n      temp = tempfile.NamedTemporaryFile()\n      start_time = time.time()\n      postprocess_variants_lib.process_single_sites_tfrecords(\n          contigs, paths, temp.name)\n      logging.info(\'CVO sorting took %s minutes\',\n                   (time.time() - start_time) / 60)\n\n      logging.info(\'Transforming call_variants_output to variants.\')\n      independent_variants = _transform_call_variants_output_to_variants(\n          input_sorted_tfrecord_path=temp.name,\n          qual_filter=FLAGS.qual_filter,\n          multi_allelic_qual_filter=FLAGS.multi_allelic_qual_filter,\n          sample_name=sample_name,\n          group_variants=FLAGS.group_variants)\n      variant_generator = haplotypes.maybe_resolve_conflicting_variants(\n          independent_variants)\n\n    header = dv_vcf_constants.deepvariant_header(\n        contigs=contigs, sample_names=[sample_name])\n    use_csi = _decide_to_use_csi(contigs)\n\n    start_time = time.time()\n    if not FLAGS.nonvariant_site_tfrecord_path:\n      logging.info(\'Writing variants to VCF.\')\n      write_variants_to_vcf(\n          variant_iterable=variant_generator,\n          output_vcf_path=FLAGS.outfile,\n          header=header)\n      if FLAGS.outfile.endswith(\'.gz\'):\n        build_index(FLAGS.outfile, use_csi)\n      logging.info(\'VCF creation took %s minutes\',\n                   (time.time() - start_time) / 60)\n    else:\n      logging.info(\'Merging and writing variants to VCF and gVCF.\')\n      lessthanfn = _get_contig_based_lessthan(contigs)\n      with vcf.VcfWriter(\n          FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \\\n          vcf.VcfWriter(\n              FLAGS.gvcf_outfile, header=header, round_qualities=True) \\\n          as gvcf_writer:\n        nonvariant_generator = tfrecord.read_shard_sorted_tfrecords(\n            FLAGS.nonvariant_site_tfrecord_path,\n            key=_get_contig_based_variant_sort_keyfn(contigs),\n            proto=variants_pb2.Variant)\n        merge_and_write_variants_and_nonvariants(variant_generator,\n                                                 nonvariant_generator,\n                                                 lessthanfn, fasta_reader,\n                                                 vcf_writer, gvcf_writer)\n      if FLAGS.outfile.endswith(\'.gz\'):\n        build_index(FLAGS.outfile, use_csi)\n      if FLAGS.gvcf_outfile.endswith(\'.gz\'):\n        build_index(FLAGS.gvcf_outfile, use_csi)\n      logging.info(\'Finished writing VCF and gVCF in %s minutes.\',\n                   (time.time() - start_time) / 60)\n    if FLAGS.vcf_stats_report:\n      outfile_base = _get_base_path(FLAGS.outfile)\n      with vcf.VcfReader(FLAGS.outfile) as reader:\n        vcf_stats.create_vcf_report(\n            variants=reader.iterate(),\n            output_basename=outfile_base,\n            sample_name=sample_name,\n            vcf_reader=reader)\n    if record:\n      temp.close()\n\n\nif __name__ == \'__main__\':\n  flags.mark_flags_as_required([\'infile\', \'outfile\', \'ref\'])\n  tf.compat.v1.app.run()\n'"
deepvariant/postprocess_variants_test.py,7,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .postprocess_variants.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport copy\nimport errno\nimport gzip\nimport io\nimport itertools\nimport os\nimport shutil\nimport sys\n\n\nfrom absl import flags\nfrom absl import logging\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom third_party.nucleus.io import fasta\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.protos import reference_pb2\nfrom third_party.nucleus.protos import struct_pb2\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import genomics_math\nfrom third_party.nucleus.util import vcf_constants\nfrom deepvariant import dv_constants\nfrom deepvariant import dv_vcf_constants\nfrom deepvariant import postprocess_variants\nfrom deepvariant import testdata\nfrom deepvariant.protos import deepvariant_pb2\nfrom deepvariant.testing import flagsaver\n\nFLAGS = flags.FLAGS\n\n_DEFAULT_SAMPLE_NAME = \'NA12878\'\n\n# Test contigs for gVCF merging code.\n_CONTIGS = [\n    reference_pb2.ContigInfo(name=\'1\', n_bases=100),\n    reference_pb2.ContigInfo(name=\'2\', n_bases=200),\n    reference_pb2.ContigInfo(name=\'10\', n_bases=300),\n]\n\n\ndef dummy_reference_reader():\n  return fasta.InMemoryFastaReader(chromosomes=[\n      (\'1\', 0, \'AACCGGTTACGTTCGATTTTAAAACCCCGGGG\'),\n      (\'2\', 0, \'GCAGTGACGTAGCGATGACGTAGACGCTTACG\'),\n  ])\n\n\ndef setUpModule():\n  testdata.init()\n\n\nclass MockVcfWriter(object):\n  """"""A mock VcfWriter that records the variants written in a list.""""""\n\n  def __init__(self):\n    self.variants_written = []\n\n  def write(self, proto):\n    self.variants_written.append(copy.deepcopy(proto))\n\n\ndef _create_variant(ref_name, start, ref_base, alt_bases, qual, filter_field,\n                    genotype, gq, likelihoods):\n  """"""Creates a Variant record for testing.\n\n  Args:\n    ref_name: reference name for this variant\n    start: start position on the contig\n    ref_base: reference base(s)\n    alt_bases: list(str). alternate base(s)\n    qual: PHRED scaled detection probability\n    filter_field: filter string for this variant\n    genotype: list of integers corresponding to the called genotype\n    gq: PHRED scaled genotype quality\n    likelihoods: genotype likelihoods for this variant\n\n  Returns:\n    A Variant record created with the specified arguments.\n  """"""\n  return test_utils.make_variant(\n      chrom=ref_name,\n      start=start,\n      alleles=[ref_base] + alt_bases,\n      qual=qual,\n      filters=filter_field,\n      gt=genotype,\n      gq=gq,\n      gls=likelihoods,\n      sample_name=_DEFAULT_SAMPLE_NAME)\n\n\ndef _create_variant_with_alleles(ref=None, alts=None, start=0):\n  """"""Creates a Variant record with specified alternate_bases.""""""\n  return variants_pb2.Variant(\n      reference_bases=ref,\n      alternate_bases=alts,\n      start=start,\n      calls=[variants_pb2.VariantCall(call_set_name=_DEFAULT_SAMPLE_NAME)])\n\n\ndef _create_call_variants_output(indices,\n                                 probabilities,\n                                 ref=None,\n                                 alts=None,\n                                 variant=None):\n  if alts is None != variant is None:\n    raise ValueError(\'Exactly one of either `alts` or `variant` should be set.\')\n  if not variant:\n    variant = _create_variant_with_alleles(ref=ref, alts=alts)\n  return deepvariant_pb2.CallVariantsOutput(\n      genotype_probabilities=probabilities,\n      alt_allele_indices=deepvariant_pb2.CallVariantsOutput.AltAlleleIndices(\n          indices=indices),\n      variant=variant)\n\n\ndef _simple_variant(ref_name, start, ref_base):\n  """"""Creates a Variant record for testing variant and non-variant merge.\n\n  Args:\n    ref_name: str. Reference name for this variant.\n    start: int. start position on the contig [0-based, half open).\n    ref_base: str. reference base(s).\n\n  Returns:\n    A Variant record created with the specified arguments.\n  """"""\n  return test_utils.make_variant(\n      chrom=ref_name,\n      start=start,\n      end=start + len(ref_base),\n      alleles=[ref_base, \'A\' if ref_base != \'A\' else \'C\'],\n      gt=[0, 1],\n      gls=[-3, -.01, -1])\n\n\ndef _simple_gv(ref_name, start, ref_base):\n  """"""Creates a gVCF variant record for testing variant and non-variant merge.\n\n  The genotypes of a _simple_gv variant are identical to those of a\n  _simple_variant variant. The alleles include the <*> allele, since it is a\n  gVCF record, and the genotype likelihoods are zero-scaled and have the <*>\n  allele likelihood set to postprocess_variants._GVCF_ALT_ALLELE_GL.\n\n  Args:\n    ref_name: str. Reference name for this variant.\n    start: int. start position on the contig [0-based, half open).\n    ref_base: str. reference base(s).\n\n  Returns:\n    A gVCF Variant record created with the specified arguments.\n  """"""\n  return test_utils.make_variant(\n      chrom=ref_name,\n      start=start,\n      end=start + len(ref_base),\n      alleles=[\n          ref_base, \'A\' if ref_base != \'A\' else \'C\',\n          vcf_constants.GVCF_ALT_ALLELE\n      ],\n      gt=[0, 1],\n      gls=[-2.99, 0, -0.99] + [postprocess_variants._GVCF_ALT_ALLELE_GL] * 3)\n\n\ndef _read_contents(path, decompress=False):\n  with tf.io.gfile.GFile(path, \'rb\') as fin:\n    contents = fin.read()\n    if decompress:\n      contents = gzip.GzipFile(path, fileobj=io.BytesIO(contents)).read()\n    return contents\n\n\ndef _create_nonvariant(ref_name, start, end, ref_base):\n  """"""Creates a non-variant Variant record for testing.\n\n  Args:\n    ref_name: str. Reference name for this variant.\n    start: int. start position on the contig [0-based, half open).\n    end: int. end position on the contig [0-based, half open).\n    ref_base: str. reference base at the start position.\n\n  Returns:\n    A non-variant Variant record created with the specified arguments.\n  """"""\n  return test_utils.make_variant(\n      chrom=ref_name,\n      start=start,\n      end=end,\n      alleles=[ref_base, vcf_constants.GVCF_ALT_ALLELE],\n      gt=[0, 0],\n      gls=[-.001, -5, -10])\n\n\ndef make_golden_dataset(compressed_inputs=False):\n  if compressed_inputs:\n    source_path = test_utils.test_tmpfile(\n        \'golden.postprocess_single_site_input.tfrecord.gz\')\n    tfrecord.write_tfrecords(\n        tfrecord.read_tfrecords(\n            testdata.GOLDEN_POSTPROCESS_INPUT,\n            proto=deepvariant_pb2.CallVariantsOutput), source_path)\n  else:\n    source_path = testdata.GOLDEN_POSTPROCESS_INPUT\n  return source_path\n\n\ndef create_outfile(file_name, compressed_outputs=False):\n  if compressed_outputs:\n    file_name += \'.gz\'\n  return test_utils.test_tmpfile(file_name)\n\n\nclass AlleleRemapperTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      (list(\'C\'), [], [True]),\n      (list(\'C\'), [\'C\'], [False]),\n      (list(\'CT\'), [], [True, True]),\n      (list(\'CT\'), [\'C\'], [False, True]),\n      (list(\'CT\'), [\'T\'], [True, False]),\n      (list(\'CT\'), [\'C\', \'T\'], [False, False]),\n      (list(\'CTG\'), [\'C\'], [False, True, True]),\n      (list(\'CTG\'), [\'T\'], [True, False, True]),\n      (list(\'CTG\'), [\'G\'], [True, True, False]),\n      (list(\'CTG\'), [\'C\', \'G\'], [False, True, False]),\n  )\n  def test_basic(self, alt_alleles, remove, keep_index_expected):\n    remapper = postprocess_variants.AlleleRemapper(alt_alleles, remove)\n    self.assertEqual(remapper.original_alts, alt_alleles)\n    self.assertEqual(remapper.alleles_to_remove, set(remove))\n    self.assertEqual(keep_index_expected,\n                     [remapper.keep_index(i) for i in range(len(alt_alleles))])\n    # When our i is 1 for the first alt allele, we expect that we will get back\n    # our keep_index_expected but also that keep_index(i==0) is True for the\n    # reference allele.\n    self.assertEqual([True] + keep_index_expected, [\n        remapper.keep_index(i, ref_is_zero=True)\n        for i in range(len(alt_alleles) + 1)\n    ])\n\n  def test_makes_copy_of_inputs(self):\n    alt_alleles = [\'A\', \'B\']\n    removes = {\'B\'}\n    remapper = postprocess_variants.AlleleRemapper(alt_alleles, removes)\n    del alt_alleles[0]\n    removes -= {\'B\'}\n    self.assertEqual(remapper.original_alts, [\'A\', \'B\'])\n    self.assertEqual(remapper.alleles_to_remove, {\'B\'})\n\n\nclass PostprocessVariantsTest(parameterized.TestCase):\n\n  @parameterized.parameters(False, True)\n  @flagsaver.FlagSaver\n  def test_call_end2end(self, compressed_inputs_and_outputs):\n    FLAGS.infile = make_golden_dataset(compressed_inputs_and_outputs)\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.outfile = create_outfile(\'calls.vcf\', compressed_inputs_and_outputs)\n    FLAGS.nonvariant_site_tfrecord_path = (\n        testdata.GOLDEN_POSTPROCESS_GVCF_INPUT)\n    FLAGS.gvcf_outfile = create_outfile(\'gvcf_calls.vcf\',\n                                        compressed_inputs_and_outputs)\n\n    postprocess_variants.main([\'postprocess_variants.py\'])\n\n    self.assertEqual(\n        _read_contents(FLAGS.outfile, compressed_inputs_and_outputs),\n        _read_contents(testdata.GOLDEN_POSTPROCESS_OUTPUT))\n    self.assertEqual(\n        _read_contents(FLAGS.gvcf_outfile, compressed_inputs_and_outputs),\n        _read_contents(testdata.GOLDEN_POSTPROCESS_GVCF_OUTPUT))\n\n    if compressed_inputs_and_outputs:\n      self.assertTrue(tf.io.gfile.exists(FLAGS.outfile + \'.tbi\'))\n      self.assertTrue(tf.io.gfile.exists(FLAGS.gvcf_outfile + \'.tbi\'))\n\n  @flagsaver.FlagSaver\n  def test_group_variants(self):\n    FLAGS.infile = testdata.GOLDEN_VCF_CANDIDATE_IMPORTER_POSTPROCESS_INPUT\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.outfile = create_outfile(\'calls.vcf\')\n\n    FLAGS.group_variants = True\n    with six.assertRaisesRegex(\n        self, ValueError, \'`call_variants_outputs` did not pass sanity check.\'):\n      postprocess_variants.main([\'postprocess_variants.py\'])\n\n    FLAGS.group_variants = False\n    postprocess_variants.main([\'postprocess_variants.py\'])\n    self.assertEqual(\n        _read_contents(FLAGS.outfile),\n        _read_contents(\n            testdata.GOLDEN_VCF_CANDIDATE_IMPORTER_POSTPROCESS_OUTPUT))\n\n  @parameterized.parameters(False, True)\n  def test_build_index(self, use_csi):\n    vcf_file_gz = os.path.join(absltest.get_default_test_tmpdir(),\n                               \'call_test_id_%s.vcf.gz\' % (use_csi))\n    shutil.copy(testdata.GOLDEN_POSTPROCESS_OUTPUT_COMPRESSED, vcf_file_gz)\n    postprocess_variants.build_index(vcf_file_gz, use_csi)\n\n    if use_csi:\n      self.assertFalse(tf.io.gfile.exists(vcf_file_gz + \'.tbi\'))\n      self.assertTrue(tf.io.gfile.exists(vcf_file_gz + \'.csi\'))\n    else:\n      self.assertFalse(tf.io.gfile.exists(vcf_file_gz + \'.csi\'))\n      self.assertTrue(tf.io.gfile.exists(vcf_file_gz + \'.tbi\'))\n\n  @flagsaver.FlagSaver\n  def test_reading_sharded_input_with_empty_shards_does_not_crash(self):\n    valid_variants = tfrecord.read_tfrecords(\n        testdata.GOLDEN_POSTPROCESS_INPUT,\n        proto=deepvariant_pb2.CallVariantsOutput)\n    empty_shard_one = test_utils.test_tmpfile(\n        \'reading_empty_shard.tfrecord-00000-of-00002\')\n    none_empty_shard_two = test_utils.test_tmpfile(\n        \'reading_empty_shard.tfrecord-00001-of-00002\')\n    tfrecord.write_tfrecords([], empty_shard_one)\n    tfrecord.write_tfrecords(valid_variants, none_empty_shard_two)\n    FLAGS.infile = test_utils.test_tmpfile(\'reading_empty_shard.tfrecord@2\')\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.outfile = test_utils.test_tmpfile(\'calls_reading_empty_shard.vcf\')\n\n    postprocess_variants.main([\'postprocess_variants.py\'])\n\n  @flagsaver.FlagSaver\n  def test_reading_empty_input_outputs_vcf_and_gvcf(self):\n    empty_shard_one = test_utils.test_tmpfile(\n        \'no_records.tfrecord-00000-of-00002\')\n    empty_shard_two = test_utils.test_tmpfile(\n        \'no_records.tfrecord-00001-of-00002\')\n    tfrecord.write_tfrecords([], empty_shard_one)\n    tfrecord.write_tfrecords([], empty_shard_two)\n    FLAGS.infile = test_utils.test_tmpfile(\'no_records.tfrecord@2\')\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.outfile = test_utils.test_tmpfile(\'no_records.vcf\')\n\n    FLAGS.nonvariant_site_tfrecord_path = test_utils.test_tmpfile(\n        \'empty.postprocess_gvcf_input.tfrecord.gz\')\n    tfrecord.write_tfrecords([], FLAGS.nonvariant_site_tfrecord_path)\n    FLAGS.gvcf_outfile = test_utils.test_tmpfile(\'no_records.g.vcf\')\n\n    postprocess_variants.main([\'postprocess_variants.py\'])\n\n    fasta_reader = fasta.IndexedFastaReader(FLAGS.ref)\n    contigs = fasta_reader.header.contigs\n    expected_sample_name = dv_constants.DEFAULT_SAMPLE_NAME\n    expected_vcf = dv_vcf_constants.deepvariant_header(\n        contigs=contigs, sample_names=[expected_sample_name])\n    actual_vcf = vcf.VcfReader(FLAGS.outfile).header\n    self.assertEqual(actual_vcf, expected_vcf)\n\n    expected_gvcf = dv_vcf_constants.deepvariant_header(\n        contigs=contigs, sample_names=[expected_sample_name])\n    actual_gvcf = vcf.VcfReader(FLAGS.outfile).header\n    self.assertEqual(actual_gvcf, expected_gvcf)\n\n  def test_extract_single_variant_name(self):\n    record = _create_call_variants_output(\n        indices=[0], probabilities=[0.19, 0.75, 0.06], ref=\'A\', alts=[\'C\', \'T\'])\n    expected = _DEFAULT_SAMPLE_NAME\n    actual = postprocess_variants._extract_single_sample_name(record)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      ([],),\n      ([\'multiple\', \'names\'],),\n  )\n  def test_exception_extract_single_variant_name(self, names):\n    variant_calls = [\n        variants_pb2.VariantCall(call_set_name=name) for name in names\n    ]\n    variant = variants_pb2.Variant(calls=variant_calls)\n    record = deepvariant_pb2.CallVariantsOutput(variant=variant)\n    with six.assertRaisesRegex(self, ValueError,\n                               \'Expected exactly one VariantCal\'):\n      postprocess_variants._extract_single_sample_name(record)\n\n  @parameterized.parameters(\n      ([\n          _create_call_variants_output(\n              indices=[0],\n              probabilities=[0.19, 0.75, 0.06],\n              ref=\'A\',\n              alts=[\'C\', \'T\']),\n          _create_call_variants_output(\n              indices=[1],\n              probabilities=[0.03, 0.93, 0.04],\n              ref=\'A\',\n              alts=[\'C\', \'T\']),\n          _create_call_variants_output(\n              indices=[0, 1],\n              probabilities=[0.03, 0.92, 0.05],\n              ref=\'A\',\n              alts=[\'C\', \'T\'])\n      ], set(), {\n          (\'A\', \'A\'): [0.19, 0.03, 0.03],\n          (\'A\', \'C\'): [0.75, 0.92],\n          (\'C\', \'C\'): [0.06, 0.05],\n          (\'A\', \'T\'): [0.93, 0.92],\n          (\'T\', \'T\'): [0.04, 0.05],\n          (\'C\', \'T\'): [0.05],\n          (\'T\', \'C\'): [0.05],\n      }),\n      # Example where all alt alleles are below qual_filter, but we keep one\n      # where the qual is highest among the ones filtered out (\'T\')\n      ([\n          _create_call_variants_output(\n              indices=[0],\n              probabilities=[0.19, 0.75, 0.06],\n              ref=\'A\',\n              alts=[\'C\', \'T\']),\n          _create_call_variants_output(\n              indices=[1],\n              probabilities=[0.03, 0.93, 0.04],\n              ref=\'A\',\n              alts=[\'C\', \'T\']),\n          _create_call_variants_output(\n              indices=[0, 1],\n              probabilities=[0.03, 0.92, 0.05],\n              ref=\'A\',\n              alts=[\'C\', \'T\'])\n      ], set([\'C\']), {\n          (\'A\', \'A\'): [0.03],\n          (\'A\', \'T\'): [0.93],\n          (\'T\', \'T\'): [0.04],\n      }),\n  )\n  def test_convert_call_variants_outputs_to_probs_dict(self,\n                                                       call_variants_outputs,\n                                                       alt_alleles_to_remove,\n                                                       expected_probs_dict):\n    # In the current code, all call_variants_outputs have the same variant\n    # field.\n    canonical_variant = call_variants_outputs[0].variant\n    self.assertEqual(\n        postprocess_variants.convert_call_variants_outputs_to_probs_dict(\n            canonical_variant,\n            call_variants_outputs,\n            alt_alleles_to_remove=alt_alleles_to_remove), expected_probs_dict)\n\n  @parameterized.parameters(\n      # Example with 2 alternate_bases:\n      # expected_unnormalized_probs is min of 0/0, 0/1, 1/1, 0/2, 1/2, 2/2\n      ([\n          _create_call_variants_output(\n              indices=[0], probabilities=[0.19, 0.75, 0.06], alts=[\'C\', \'T\']),\n          _create_call_variants_output(\n              indices=[1], probabilities=[0.03, 0.93, 0.04], alts=[\'C\', \'T\']),\n          _create_call_variants_output(\n              indices=[0, 1], probabilities=[0.03, 0.92, 0.05], alts=[\'C\', \'T\'])\n      ], [0.03, 0.75, 0.05, 0.92, 0.05, 0.04]),\n      # One more example with 2 alternate_bases:\n      # expected_unnormalized_probs is min of 0/0, 0/1, 1/1, 0/2, 1/2, 2/2\n      ([\n          _create_call_variants_output(\n              indices=[1], probabilities=[0.978, 0.03, 0.002], alts=[\'C\', \'T\']),\n          _create_call_variants_output(\n              indices=[0, 1],\n              probabilities=[0.992, 0.007, 0.001],\n              alts=[\'C\', \'T\']),\n          _create_call_variants_output(\n              indices=[0],\n              probabilities=[0.99997, 0.00002, 0.00001],\n              alts=[\'C\', \'T\']),\n      ], [0.978, 0.00002, 0.00001, 0.007, 0.001, 0.001]),\n      # An extreme case where our logic could result in ZeroDivisionError if\n      # we don\'t handle this special case.\n      ([\n          _create_call_variants_output(\n              indices=[0], probabilities=[0.0, 1.0, 0.0], alts=[\'C\', \'T\']),\n          _create_call_variants_output(\n              indices=[1], probabilities=[0.00, 1.0, 0.0], alts=[\'C\', \'T\']),\n          _create_call_variants_output(\n              indices=[0, 1], probabilities=[1.0, 0.0, 0.0], alts=[\'C\', \'T\'])\n      ], [1.0 / 6] * 6),\n      # expected_unnormalized_probs is min of 0/0, 0/1, 1/1.\n      ([\n          _create_call_variants_output(\n              indices=[0], probabilities=[0.19, 0.75, 0.06], alts=[\'A\']),\n      ], [0.19, 0.75, 0.06]),\n      # expected_unnormalized_probs is min of\n      # 0/0, 0/1, 1/1, 0/2, 1/2, 2/2, 0/3, 1/3, 2/3, 3/3.\n      ([\n          _create_call_variants_output(\n              indices=[0],\n              probabilities=[0.999, 0.001, 0],\n              alts=[\'C\', \'G\', \'T\']),\n          _create_call_variants_output(\n              indices=[0, 1], probabilities=[0, 1, 0], alts=[\'C\', \'G\', \'T\']),\n          _create_call_variants_output(\n              indices=[0, 2],\n              probabilities=[0.0001, 0.9996, 0.0003],\n              alts=[\'C\', \'G\', \'T\']),\n          _create_call_variants_output(\n              indices=[1], probabilities=[0, 1, 0], alts=[\'C\', \'G\', \'T\']),\n          _create_call_variants_output(\n              indices=[1, 2],\n              probabilities=[0.0001, 0.0002, 0.9997],\n              alts=[\'C\', \'G\', \'T\']),\n          _create_call_variants_output(\n              indices=[2],\n              probabilities=[0.00004, 0.9999, 0.00006],\n              alts=[\'C\', \'G\', \'T\']),\n      ], [0, 0.001, 0, 0.0002, 0, 0, 0.0002, 0.0003, 0.9997, 0.00006]),\n  )\n  def test_merge_predictions(self, inputs, expected_unnormalized_probs):\n    denominator = sum(expected_unnormalized_probs)\n    for permuted_inputs in itertools.permutations(inputs):\n      _, predictions = postprocess_variants.merge_predictions(permuted_inputs)\n      np.testing.assert_almost_equal(\n          predictions, [x / denominator for x in expected_unnormalized_probs])\n\n  @parameterized.parameters(\n      # With 1 alt allele, we expect to see 1 alt_allele_indices: [0].\n      (\n          [\n              _create_call_variants_output(\n                  indices=[1], probabilities=[0.19, 0.75, 0.06], alts=[\'A\']),\n          ],\n          \'`call_variants_outputs` did not pass sanity check.\',\n      ),\n      # With 2 alt alleles, we expect to see 3 alt_allele_indices.\n      (\n          [\n              _create_call_variants_output(\n                  indices=[0],\n                  probabilities=[0.19, 0.75, 0.06],\n                  alts=[\'G\', \'T\']),\n              _create_call_variants_output(\n                  indices=[1],\n                  probabilities=[0.03, 0.93, 0.04],\n                  alts=[\'G\', \'T\']),\n          ],\n          \'`call_variants_outputs` did not pass sanity check.\',\n      ),\n      # With 2 alt alleles, we expect to see 3 alt_allele_indices:\n      # [0], [1], [0, 1].\n      (\n          [\n              _create_call_variants_output(\n                  indices=[0],\n                  probabilities=[0.19, 0.75, 0.06],\n                  alts=[\'G\', \'T\']),\n              _create_call_variants_output(\n                  indices=[0],\n                  probabilities=[0.03, 0.93, 0.04],\n                  alts=[\'G\', \'T\']),\n              _create_call_variants_output(\n                  indices=[0, 1],\n                  probabilities=[0.03, 0.93, 0.04],\n                  alts=[\'G\', \'T\']),\n          ],\n          \'`call_variants_outputs` did not pass sanity check.\',\n      ),\n      (\n          [\n              _create_call_variants_output(\n                  indices=[0],\n                  probabilities=[0.19, 0.75, 0.06],\n                  alts=[\'G\', \'T\']),\n          ],\n          \'`call_variants_outputs` did not pass sanity check.\',\n      ),\n      ([], \'Expected 1 or more call_variants_outputs.\'),\n      # With 3 alt alleles, we expect to see 6 alt_allele_indices.\n      (\n          [\n              _create_call_variants_output(\n                  indices=[0],\n                  probabilities=[0.999, 0.001, 0],\n                  alts=[\'AA\', \'T\', \'AAA\']),\n              _create_call_variants_output(\n                  indices=[0, 1],\n                  probabilities=[0, 1, 0],\n                  alts=[\'AA\', \'T\', \'AAA\']),\n              _create_call_variants_output(\n                  indices=[0, 2],\n                  probabilities=[0.0001, 0.9996, 0.0003],\n                  alts=[\'AA\', \'T\', \'AAA\'])\n          ],\n          \'`call_variants_outputs` did not pass sanity check.\',\n      ),\n      # reference_bases have to be exactly the same.\n      ([\n          _create_call_variants_output(\n              indices=[0],\n              probabilities=[0.999, 0.001, 0],\n              variant=_create_variant_with_alleles(ref=\'A\', alts=[\'T\', \'C\'])),\n          _create_call_variants_output(\n              indices=[1],\n              probabilities=[0.2, 0.8, 0],\n              variant=_create_variant_with_alleles(ref=\'A\', alts=[\'T\', \'C\'])),\n          _create_call_variants_output(\n              indices=[0, 1],\n              probabilities=[0.2, 0.8, 0],\n              variant=_create_variant_with_alleles(ref=\'G\', alts=[\'T\', \'C\'])),\n      ], \'`call_variants_outputs` did not pass sanity check.\'),\n      # alternate_bases have to be exactly the same. Different orders are\n      # not acceptable either.\n      (\n          [\n              _create_call_variants_output(\n                  indices=[0],\n                  probabilities=[0.999, 0.001, 0],\n                  variant=_create_variant_with_alleles(alts=[\'T\', \'C\'])),\n              _create_call_variants_output(\n                  indices=[1],\n                  probabilities=[0.2, 0.8, 0],\n                  variant=_create_variant_with_alleles(alts=[\'T\', \'C\'])),\n              _create_call_variants_output(\n                  indices=[0, 1],\n                  probabilities=[0.2, 0.8, 0],\n                  variant=_create_variant_with_alleles(alts=[\'C\', \'T\'])),\n          ],\n          \'`call_variants_outputs` did not pass sanity check.\',\n      ),\n  )\n  def test_exception_merge_predictions(self, inputs, text):\n    with six.assertRaisesRegex(self, ValueError, text):\n      postprocess_variants.merge_predictions(inputs)\n\n  @parameterized.parameters(\n      (_create_variant(\'1\', 1, \'A\', [\'C\'], 20.0,\n                       dv_vcf_constants.DEEP_VARIANT_PASS, [1, 1], 15,\n                       [-2.0, -9.90308995105826, -0.0043648054]), [1, 1]),\n      (_create_variant(\'GL000220.1\', 10000210, \'C\', [\'T\'], 20.0,\n                       dv_vcf_constants.DEEP_VARIANT_PASS, [1, 1], 20,\n                       [-2.0, -9.90308995105826, -0.0043648054]), [1, 1]),\n      (_create_variant(\'20\', 10000210, \'C\', [\'CT\'], 30.0,\n                       dv_vcf_constants.DEEP_VARIANT_PASS, [0, 1], 30,\n                       [-3.0, -0.00043451177, -9.90308995105826]), [0, 1]),\n      (_create_variant(\n          \'X\', 10000210, \'CACA\', [\'C\'], 0.04364805402,\n          dv_vcf_constants.DEEP_VARIANT_REF_FILTER, [0, 0], 19,\n          [-0.0043648054, -2.30102999566, -2.30102999566]), [-1, -1]),\n      (_create_variant(\'chrY\', 10000210, \'C\', [\'T\'], 0.00043431619,\n                       dv_vcf_constants.DEEP_VARIANT_REF_FILTER, [0, 0], 20,\n                       [-0.00004343161, -4.0, -9.90308995105826]), [0, 0]),\n      (_create_variant(\'X\', 10000210, \'CACA\', [\'C\', \'A\'], 0.0217691925,\n                       dv_vcf_constants.DEEP_VARIANT_REF_FILTER, [0, 0], 13,\n                       [-0.00217691925, -3, -3, -3, -3, -3]), [-1, -1]),\n  )\n  def test_uncall_homref_gt_if_lowqual(self, variant, expected_gt):\n    postprocess_variants.uncall_homref_gt_if_lowqual(variant, 20)\n    self.assertEqual(variant.calls[0].genotype, expected_gt)\n\n  @parameterized.parameters(\n      ([0.01, 0.0, 0.99],\n       _create_variant(\'GL000220.1\', 1, \'A\', [\'.\'], 20.0,\n                       dv_vcf_constants.DEEP_VARIANT_PASS, [1, 1], 20,\n                       [-2.0, -9.90308995105826, -0.0043648054])),\n      ([0.01, 0.0, 0.99],\n       _create_variant(\'GL000220.1\', 10000210, \'C\', [\'T\'], 20.0,\n                       dv_vcf_constants.DEEP_VARIANT_PASS, [1, 1], 20,\n                       [-2.0, -9.90308995105826, -0.0043648054])),\n      ([0.001, 0.999, 0.0],\n       _create_variant(\'20\', 10000210, \'C\', [\'CT\'], 30.0,\n                       dv_vcf_constants.DEEP_VARIANT_PASS, [0, 1], 30,\n                       [-3.0, -0.00043451177, -9.90308995105826])),\n      ([0.0001, 0.0, 0.9999],\n       _create_variant(\'1\', 1, \'C\', [\'T\'], 40.0,\n                       dv_vcf_constants.DEEP_VARIANT_PASS, [1, 1], 40,\n                       [-4.0, -9.90308995105826, -0.00004343161])),\n      ([0.1, 0.90, 0.0],\n       _create_variant(\'20\', 10000210, \'A\', [\'T\'], 10.0,\n                       dv_vcf_constants.DEEP_VARIANT_PASS, [0, 1], 10,\n                       [-1.0, -0.04575749056, -9.90308995105826])),\n      ([0.99, 0.005, 0.005],\n       _create_variant(\'X\', 10000210, \'CACA\', [\'C\'], 0.04364805402,\n                       dv_vcf_constants.DEEP_VARIANT_REF_FILTER, [0, 0], 20,\n                       [-0.0043648054, -2.30102999566, -2.30102999566])),\n      ([0.9999, 0.0001, 0.0],\n       _create_variant(\'chrY\', 10000210, \'C\', [\'T\'], 0.00043431619,\n                       dv_vcf_constants.DEEP_VARIANT_REF_FILTER, [0, 0], 40,\n                       [-0.00004343161, -4.0, -9.90308995105826])),\n      # Multi-allelic test examples.\n      ([0.995, 0.001, 0.001, 0.001, 0.001, 0.001],\n       _create_variant(\'X\', 10000210, \'CACA\', [\'C\', \'A\'], 0.0217691925,\n                       dv_vcf_constants.DEEP_VARIANT_REF_FILTER, [0, 0], 23,\n                       [-0.00217691925, -3, -3, -3, -3, -3])),\n      ([0.001, 0.001, 0.001, 0.995, 0.001, 0.001],\n       _create_variant(\'X\', 10000210, \'CACA\', [\'C\', \'A\'], 30,\n                       dv_vcf_constants.DEEP_VARIANT_PASS, [0, 2], 23,\n                       [-3, -3, -3, -0.00217691925, -3, -3])),\n  )\n  def test_add_call_to_variant(self, probs, expected):\n    raw_variant = variants_pb2.Variant(\n        reference_name=expected.reference_name,\n        reference_bases=expected.reference_bases,\n        alternate_bases=expected.alternate_bases,\n        start=expected.start,\n        end=expected.end,\n        calls=[variants_pb2.VariantCall(call_set_name=_DEFAULT_SAMPLE_NAME)])\n    variant = postprocess_variants.add_call_to_variant(\n        variant=raw_variant,\n        predictions=probs,\n        sample_name=_DEFAULT_SAMPLE_NAME)\n    self.assertEqual(variant.reference_bases, expected.reference_bases)\n    self.assertEqual(variant.alternate_bases, expected.alternate_bases)\n    self.assertEqual(variant.reference_name, expected.reference_name)\n    self.assertEqual(variant.start, expected.start)\n    self.assertEqual(variant.end, expected.end)\n    self.assertAlmostEqual(variant.quality, expected.quality, places=6)\n    self.assertEqual(variant.filter, expected.filter)\n    self.assertLen(variant.calls, 1)\n    self.assertLen(expected.calls, 1)\n    self.assertEqual(variant.calls[0].genotype, expected.calls[0].genotype)\n    self.assertEqual(variant.calls[0].info[\'GQ\'], expected.calls[0].info[\'GQ\'])\n    for gl, expected_gl in zip(variant.calls[0].genotype_likelihood,\n                               expected.calls[0].genotype_likelihood):\n      self.assertAlmostEqual(gl, expected_gl, places=6)\n\n  @parameterized.parameters(\n      (\n          0,\n          [0, 0],\n      ),\n      (\n          1,\n          [0, 1],\n      ),\n      (\n          2,\n          [1, 1],\n      ),\n      (\n          3,\n          [0, 2],\n      ),\n      (\n          4,\n          [1, 2],\n      ),\n      (\n          5,\n          [2, 2],\n      ),\n  )\n  def test_triallelic_genotype_in_add_call_to_variant(self,\n                                                      highest_prob_position,\n                                                      expected_best_genotype):\n    """"""Ensures the order of GLs are interpreted correctly for triallelics.""""""\n    raw_variant = _create_variant_with_alleles(ref=\'CACA\', alts=[\'C\', \'A\'])\n    # Create a probability with 6 elements, one of them 0.995 (best genotype),\n    # and the rest 0.001.\n    probs = [0.001] * 6\n    assert 0 <= highest_prob_position <= len(probs)\n    probs[highest_prob_position] = 0.995\n    variant = postprocess_variants.add_call_to_variant(\n        variant=raw_variant,\n        predictions=probs,\n        sample_name=_DEFAULT_SAMPLE_NAME)\n    self.assertEqual(variant.calls[0].genotype, expected_best_genotype)\n\n  @parameterized.parameters(\n      # Q20 tests\n      ([0.01, 0.0, 0.99], 0, 0, 20.0),\n      ([0.01, 0.0, 0.99], 1, 0, 20.0),\n      ([0.01, 0.0, 0.99], 2, 20, 20.0),\n      # Q30 tests\n      ([0.001, 0.0, 0.999], 0, 0, 30.0),\n      ([0.001, 0.0, 0.999], 1, 0, 30.0),\n      ([0.001, 0.0, 0.999], 2, 30, 30.0),\n      # Q40 tests\n      ([0.0001, 0.0, 0.9999], 0, 0, 40.0),\n      ([0.0001, 0.0, 0.9999], 1, 0, 40.0),\n      ([0.0001, 0.0, 0.9999], 2, 40, 40.0),\n      # Test that this works with any sized genotype vector.\n      ([0.0001, 0.0, 0.0, 0.9999], 0, 0, 40.0),\n      ([0.0001, 0.0, 0.0, 0.0, 0.9999], 0, 0, 40.0),\n      ([0.0001, 0.0, 0.0, 0.0, 0.0, 0.9999], 0, 0, 40.0),\n      ([0.0001, 0.0, 0.0, 0.0, 0.0, 0.9999], 5, 40, 40.0),\n      ([0.0001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9999], 0, 0, 40.0),\n      # Test that probabilities more extreme than genomics_math._MAX_CONFIDENCE\n      # are appropriately rounded.\n      ([1e-11, 1 - 1e-11, 0.0], 0, 0, 99.03089987),\n      ([1e-11, 1 - 1e-11, 0.0], 1, 99, 99.03089987),\n      ([1e-11, 1 - 1e-11, 0.0], 2, 0, 99.03089987),\n      ([1e-15, 1 - 1e-15, 0.0], 0, 0, 99.03089987),\n      ([1e-15, 1 - 1e-15, 0.0], 1, 99, 99.03089987),\n      ([1e-15, 1 - 1e-15, 0.0], 2, 0, 99.03089987),\n  )\n  def test_compute_quals(self, probs, call, expected_gq, expected_qual):\n    gq, qual = postprocess_variants.compute_quals(probs, call)\n    self.assertEqual(gq, expected_gq)\n    self.assertAlmostEqual(qual, expected_qual, places=6)\n\n  @parameterized.parameters(\n      # Make sure code is robust to minor numerical issues where the sum of\n      # the vector isn\'t exactly 1.0.\n      # This vector sums to ~1.0, minus any parsing uncertainty, and will\n      # return a GQ of 40 but a qual of MAX_QUAL.\n      ([0.0, 0.0001, 0.9999], 2, 40),\n      # This vector sums to >1.0, but we should get back a max qual value\n      # instead of throwing an exception.\n      ([0.0, 0.00011, 0.9999], 2, 40),\n  )\n  def test_compute_quals_numerical_stability(self, probs, call, expected_gq):\n    max_qual = round(\n        genomics_math.ptrue_to_bounded_phred(1.0),\n        postprocess_variants._QUAL_PRECISION)\n    gq, qual = postprocess_variants.compute_quals(probs, call)\n    self.assertEqual(expected_gq, gq)\n    self.assertEqual(max_qual, qual)\n\n  @parameterized.parameters(\n      # Standard diploid case with 1 alt allele.\n      ([1, 0, 0], 1, (0, [0, 0])),\n      ([0, 1, 0], 1, (1, [0, 1])),\n      ([0, 0, 1], 1, (2, [1, 1])),\n      # Diploid case with 2 alt alleles.\n      ([1, 0, 0, 0, 0, 0], 2, (0, [0, 0])),\n      ([0, 1, 0, 0, 0, 0], 2, (1, [0, 1])),\n      ([0, 0, 1, 0, 0, 0], 2, (2, [1, 1])),\n      ([0, 0, 0, 1, 0, 0], 2, (3, [0, 2])),\n      ([0, 0, 0, 0, 1, 0], 2, (4, [1, 2])),\n      ([0, 0, 0, 0, 0, 1], 2, (5, [2, 2])),\n  )\n  def test_most_likely_genotype(self, probs, n_alts, expected):\n    del n_alts\n    self.assertEqual(expected, postprocess_variants.most_likely_genotype(probs))\n\n  @parameterized.parameters(1, 3, 4)\n  def test_most_likely_genotype_raises_with_non2_ploidy(self, bad_ploidy):\n    with self.assertRaises(NotImplementedError):\n      postprocess_variants.most_likely_genotype([1, 0, 0], ploidy=bad_ploidy)\n\n  def test_compute_filter_fields(self):\n    # This generates too many tests as a parameterized test.\n    for qual, min_qual in itertools.product(range(100), range(100)):\n      # First test with no call and filter threshold\n      variant = variants_pb2.Variant()\n      variant.quality = qual\n      expected = []\n      expected.append(dv_vcf_constants.DEEP_VARIANT_PASS if qual >= min_qual\n                      else dv_vcf_constants.DEEP_VARIANT_QUAL_FILTER)\n      self.assertEqual(\n          postprocess_variants.compute_filter_fields(variant, min_qual),\n          expected)\n      # Now add hom ref genotype --> qual shouldn\'t affect filter field\n      del variant.filter[:]\n      variant.calls.add(genotype=[0, 0])\n      expected = []\n      expected.append(dv_vcf_constants.DEEP_VARIANT_REF_FILTER)\n      self.assertEqual(\n          postprocess_variants.compute_filter_fields(variant, min_qual),\n          expected)\n      # Now add variant genotype --> qual filter should matter again\n      del variant.filter[:]\n      del variant.calls[:]\n      variant.calls.add(genotype=[0, 1])\n      expected = []\n      expected.append(dv_vcf_constants.DEEP_VARIANT_PASS if qual >= min_qual\n                      else dv_vcf_constants.DEEP_VARIANT_QUAL_FILTER)\n      self.assertEqual(\n          postprocess_variants.compute_filter_fields(variant, min_qual),\n          expected)\n\n  @parameterized.parameters(\n      (\n          [\n              _create_call_variants_output(\n                  indices=[0],\n                  probabilities=[0.01, 0.98, 0.01],\n                  variant=_create_variant_with_alleles(alts=[\'C\', \'T\', \'TT\'])),\n              _create_call_variants_output(\n                  indices=[1],\n                  probabilities=[1, 0, 0],\n                  variant=_create_variant_with_alleles(alts=[\'C\', \'T\', \'TT\'])),\n              _create_call_variants_output(\n                  indices=[2],\n                  probabilities=[0.01, 0.97, 0.02],\n                  variant=_create_variant_with_alleles(alts=[\'C\', \'T\', \'TT\'])),\n              _create_call_variants_output(\n                  indices=[0, 1],\n                  probabilities=[0.01, 0.98, 0.01],\n                  variant=_create_variant_with_alleles(alts=[\'C\', \'T\', \'TT\'])),\n              _create_call_variants_output(\n                  indices=[0, 2],\n                  probabilities=[0.04, 0.95, 0.01],\n                  variant=_create_variant_with_alleles(alts=[\'C\', \'T\', \'TT\'])),\n              _create_call_variants_output(\n                  indices=[1, 2],\n                  probabilities=[0.01, 0.98, 0.01],\n                  variant=_create_variant_with_alleles(alts=[\'C\', \'T\', \'TT\'])),\n          ],\n          6,\n          set([\'T\']),\n      ),\n      # Example where all alt alleles are below qual_filter, but we keep one\n      # where the qual is highest among the ones filtered out.\n      (\n          [\n              _create_call_variants_output(\n                  indices=[0, 1],\n                  probabilities=[1, 0, 0],\n                  variant=_create_variant_with_alleles(alts=[\'C\', \'T\'])),\n              _create_call_variants_output(\n                  indices=[0],\n                  probabilities=[0.99, 0.01, 0],\n                  variant=_create_variant_with_alleles(alts=[\'C\', \'T\'])),\n              _create_call_variants_output(\n                  indices=[1],\n                  probabilities=[1, 0, 0],\n                  variant=_create_variant_with_alleles(alts=[\'C\', \'T\'])),\n          ],\n          6,\n          set([\'T\']),\n      ),\n  )\n  def test_get_alt_alleles_to_remove(self, call_variants_outputs, qual_filter,\n                                     expected_output):\n    self.assertEqual(\n        postprocess_variants.get_alt_alleles_to_remove(call_variants_outputs,\n                                                       qual_filter),\n        expected_output)\n\n  @parameterized.parameters(\n      (\n          _create_variant_with_alleles(alts=[\'C\', \'T\', \'TT\']),\n          set([]),\n          _create_variant_with_alleles(alts=[\'C\', \'T\', \'TT\']),\n      ),\n      (\n          _create_variant_with_alleles(alts=[\'C\', \'T\', \'TT\']),\n          set([\'C\']),\n          _create_variant_with_alleles(alts=[\'T\', \'TT\']),\n      ),\n      (\n          _create_variant_with_alleles(alts=[\'C\', \'T\', \'TT\']),\n          set([\'T\']),\n          _create_variant_with_alleles(alts=[\'C\', \'TT\']),\n      ),\n      (\n          _create_variant_with_alleles(alts=[\'C\', \'T\', \'TT\']),\n          set([\'TT\']),\n          _create_variant_with_alleles(alts=[\'C\', \'T\']),\n      ),\n      (\n          _create_variant_with_alleles(alts=[\'C\', \'T\', \'TT\']),\n          set([\'C\', \'TT\']),\n          _create_variant_with_alleles(alts=[\'T\']),\n      ),\n      (\n          _create_variant_with_alleles(alts=[\'C\', \'T\', \'TT\']),\n          set([\'C\', \'T\', \'TT\']),\n          _create_variant_with_alleles(alts=[]),\n      ),\n  )\n  def test_prune_alleles(self, canonical_variant, alt_alleles_to_remove,\n                         expected_variant):\n    self.assertEqual(\n        postprocess_variants.prune_alleles(canonical_variant,\n                                           alt_alleles_to_remove),\n        expected_variant)\n\n  @parameterized.parameters(\n      # Check that we are simplifying alleles and that the simplification deps\n      # on the alleles we\'ve removed.\n      dict(\n          alleles=[\'CAA\', \'CA\', \'C\'],\n          start=5,\n          alt_alleles_to_remove=[],\n          expected_alleles=[\'CAA\', \'CA\', \'C\'],\n          expected_end=8),\n      # Removing the C allele allows us to simplify CAA + CA => CA + C.\n      dict(\n          alleles=[\'CAA\', \'CA\', \'C\'],\n          start=4,\n          alt_alleles_to_remove=[\'C\'],\n          expected_alleles=[\'CA\', \'C\'],\n          expected_end=6),\n      # Removing the CA allele doesn\'t allow any simplification.\n      dict(\n          alleles=[\'CAA\', \'CA\', \'C\'],\n          start=3,\n          alt_alleles_to_remove=[\'CA\'],\n          expected_alleles=[\'CAA\', \'C\'],\n          expected_end=6),\n      # Make sure we keep at least one anchor base when pruning.\n      dict(\n          alleles=[\'CCA\', \'CA\', \'T\'],\n          start=2,\n          alt_alleles_to_remove=[\'T\'],\n          expected_alleles=[\'CC\', \'C\'],\n          expected_end=4),\n  )\n  def test_simplify_alleles(self, alleles, start, alt_alleles_to_remove,\n                            expected_alleles, expected_end):\n    """"""Test that prune_alleles + simplify_alleles works as expected.""""""\n    variant = _create_variant_with_alleles(\n        ref=alleles[0], alts=alleles[1:], start=start)\n    pruned = postprocess_variants.prune_alleles(variant, alt_alleles_to_remove)\n    simplified = postprocess_variants.simplify_alleles(pruned)\n    self.assertEqual(simplified.reference_bases, expected_alleles[0])\n    self.assertEqual(simplified.alternate_bases, expected_alleles[1:])\n    self.assertEqual(simplified.start, start)\n    self.assertEqual(simplified.end, expected_end)\n\n  def test_merge_predictions_simplifies_alleles(self):\n    """"""Checks that merge_predictions simplifies alleles.""""""\n    ref, alts = \'CCA\', [\'CA\', \'C\']\n    inputs = [\n        _create_call_variants_output(\n            ref=ref, indices=[0], probabilities=[0.0, 1.0, 0.0], alts=alts),\n        _create_call_variants_output(\n            ref=ref, indices=[1], probabilities=[1.0, 0.0, 0.0], alts=alts),\n        _create_call_variants_output(\n            ref=ref, indices=[0, 1], probabilities=[0.0, 1.0, 0.0], alts=alts),\n    ]\n\n    for permuted_inputs in itertools.permutations(inputs):\n      # qual_filter=2 is needed so we remove our middle \'C\' allele.\n      variant, probs = postprocess_variants.merge_predictions(\n          permuted_inputs, qual_filter=2)\n      np.testing.assert_almost_equal(probs, [0.0, 1.0, 0.0])\n      self.assertEqual(variant.reference_bases, \'CC\')\n      self.assertEqual(variant.alternate_bases, [\'C\'])\n\n  @parameterized.parameters(\n      ([\'A\'], {}, [1, 2], [1, 2]),\n      ([\'A\'], {\'A\'}, [1, 2], [1]),\n      ([\'A\', \'C\'], {}, [1, 2, 3], [1, 2, 3]),\n      ([\'A\', \'C\'], {\'A\'}, [1, 2, 3], [1, 3]),\n      ([\'A\', \'C\'], {\'C\'}, [1, 2, 3], [1, 2]),\n      ([\'A\', \'C\'], {\'A\', \'C\'}, [1, 2, 3], [1]),\n  )\n  def test_prune_alleles_handles_format_fields(self, alts, to_remove, orig_ad,\n                                               expected_ad):\n    variant = _create_variant_with_alleles(alts=alts)\n    test_utils.set_list_values(variant.calls[0].info[\'AD\'], orig_ad)\n    actual = postprocess_variants.prune_alleles(variant, to_remove)\n    self.assertEqual([v.int_value for v in actual.calls[0].info[\'AD\'].values],\n                     expected_ad)\n\n  @parameterized.parameters(\n      (1, [[0]]),\n      (2, [[0], [0, 1], [1]]),\n      (3, [[0], [0, 1], [0, 2], [1], [1, 2], [2]]),\n      (4, [[0], [0, 1], [0, 2], [0, 3], [1], [1, 2], [1, 3], [2], [2, 3], [3]]),\n      (8, [[0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [1],\n           [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [2], [2, 3], [2, 4],\n           [2, 5], [2, 6], [2, 7], [3], [3, 4], [3, 5], [3, 6], [3, 7], [4],\n           [4, 5], [4, 6], [4, 7], [5], [5, 6], [5, 7], [6], [6, 7], [7]]),\n  )\n  def test_expected_alt_allele_indices(self, num_alternate_bases,\n                                       expected_indices):\n    self.assertEqual(\n        postprocess_variants.expected_alt_allele_indices(num_alternate_bases),\n        expected_indices)\n\n  @flagsaver.FlagSaver\n  def test_catches_bad_argv(self):\n    # Define valid flags to ensure raise occurs due to argv issues.\n    FLAGS.infile = make_golden_dataset(False)\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.outfile = test_utils.test_tmpfile(\'nonempty_outfile.vcf\')\n    with mock.patch.object(logging, \'error\') as mock_logging,\\\n        mock.patch.object(sys, \'exit\') as mock_exit:\n      postprocess_variants.main([\'postprocess_variants.py\', \'extra_arg\'])\n    mock_logging.assert_called_once_with(\n        \'Command line parsing failure: postprocess_variants does not accept \'\n        \'positional arguments but some are present on the command line: \'\n        \'""[\\\'postprocess_variants.py\\\', \\\'extra_arg\\\']"".\')\n    mock_exit.assert_called_once_with(errno.ENOENT)\n\n  @flagsaver.FlagSaver\n  def test_catches_bad_flags(self):\n    FLAGS.infile = make_golden_dataset(False)\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.outfile = \'nonempty_outfile.vcf\'\n    FLAGS.nonvariant_site_tfrecord_path = (\n        testdata.GOLDEN_POSTPROCESS_GVCF_INPUT)\n    # This is the bad flag.\n    FLAGS.gvcf_outfile = \'\'\n    with mock.patch.object(logging, \'error\') as mock_logging, \\\n        mock.patch.object(sys, \'exit\') as mock_exit:\n      postprocess_variants.main([\'postprocess_variants.py\'])\n    mock_logging.assert_called_once_with(\n        \'gVCF creation requires both nonvariant_site_tfrecord_path and \'\n        \'gvcf_outfile flags to be set.\')\n    mock_exit.assert_called_once_with(errno.ENOENT)\n\n\nclass MergeVcfAndGvcfTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      # Smaller chromosome should return less than.\n      ((\'1\', 10, 20), (\'2\', 1, 2), True),\n      ((\'2\', 10, 20), (\'10\', 1, 2), True),\n      # Larger chromosome should not return less than.\n      ((\'2\', 1, 2), (\'1\', 10, 20), False),\n      ((\'10\', 1, 2), (\'2\', 10, 20), False),\n      # Same chromosome, smaller.\n      ((\'1\', 1, 2), (\'1\', 3, 4), True),\n      ((\'1\', 1, 3), (\'1\', 3, 4), True),\n      # Same chromosome, overlapping.\n      ((\'1\', 1, 4), (\'1\', 3, 6), False),\n      ((\'1\', 1, 9), (\'1\', 3, 6), False),\n      ((\'1\', 4, 9), (\'1\', 3, 6), False),\n      # Same chromosome, larger.\n      ((\'1\', 6, 9), (\'1\', 3, 4), False),\n  )\n  def test_lessthan_comparison(self, v1, v2, expected):\n    variant1 = _create_nonvariant(*v1, ref_base=\'A\')\n    variant2 = _create_nonvariant(*v2, ref_base=\'C\')\n    lessthan = postprocess_variants._get_contig_based_lessthan(_CONTIGS)\n    self.assertEqual(lessthan(variant1, variant2), expected)\n    self.assertEqual(lessthan(variant1, None), True)\n    self.assertEqual(lessthan(None, variant1), False)\n\n  @parameterized.parameters(\n      (_create_nonvariant(\'1\', 10, 15,\n                          \'G\'), 10, 12, _create_nonvariant(\'1\', 10, 12, \'G\')),\n      (_create_nonvariant(\'2\', 1, 9,\n                          \'C\'), 3, 4, _create_nonvariant(\'2\', 3, 4, \'G\')),\n  )\n  def test_create_record_from_template(self, template, start, end, expected):\n    reader = dummy_reference_reader()\n    actual = postprocess_variants._create_record_from_template(\n        template, start, end, reader)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(input_vcf=\'/tmp/test.vcf\', expected_base_path=\'/tmp/test\'),\n      dict(input_vcf=\'/tmp/test.vcf.gz\', expected_base_path=\'/tmp/test\'),\n      dict(input_vcf=\'test\', expected_base_path=\'test\'),\n  )\n  def test_get_base_path(self, input_vcf, expected_base_path):\n    path = postprocess_variants._get_base_path(input_vcf)\n    self.assertEqual(path, expected_base_path)\n\n  @parameterized.parameters(\n      # One alt, with het GLs.\n      ([\'C\'], [-2.0457574905606752, -0.004364805402450088, -3.0], [0.75]),\n      # Multi alts.\n      ([\'G\', \'C\'], [\n          -1.1368906918484387, -0.5279124552610386, -0.5923808731731073,\n          -0.8155431286425007, -0.8415961054266092, -1.108308924501657\n      ], [0.5, 0.1]),\n      ([\'G\', \'C\', \'T\'], [\n          -0.7956722868920258, -0.663917423732382, -1.493986734511771,\n          -0.8202531343562444, -0.9377869397242453, -1.0415699718993066,\n          -1.4176189291054515, -1.5795151893394743, -1.8101482990393198,\n          -0.8139951558313916\n      ], [0.5, 0.1, 0.05]),\n  )\n  def test_transform_to_gvcf_add_allele(self, prior_alts, prior_gls, prior_vaf):\n    variant = _create_variant(\n        ref_name=\'chr1\',\n        start=10,\n        ref_base=\'A\',\n        alt_bases=prior_alts,\n        qual=40,\n        filter_field=\'PASS\',\n        genotype=[0, 1],\n        gq=None,\n        likelihoods=prior_gls)\n    prior_vaf_values = [struct_pb2.Value(number_value=v) for v in prior_vaf]\n    variant.calls[0].info[\'VAF\'].values.extend(prior_vaf_values)\n    expected = _create_variant(\n        ref_name=\'chr1\',\n        start=10,\n        ref_base=\'A\',\n        alt_bases=prior_alts + [vcf_constants.GVCF_ALT_ALLELE],\n        qual=40,\n        filter_field=\'PASS\',\n        genotype=[0, 1],\n        gq=None,\n        likelihoods=prior_gls + ([postprocess_variants._GVCF_ALT_ALLELE_GL] *\n                                 (len(prior_alts) + 2)))\n    expected.calls[0].info[\'VAF\'].values.extend(\n        prior_vaf_values + [struct_pb2.Value(number_value=0)])\n    actual = postprocess_variants._transform_to_gvcf_record(variant)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      # One alt, with het GLs.\n      ([vcf_constants.GVCF_ALT_ALLELE\n       ], [-2.0457574905606752, -0.004364805402450088, -3.0], [0.5]),\n      # Multi alts.\n      ([\'G\', vcf_constants.GVCF_ALT_ALLELE], [\n          -1.1368906918484387, -0.5279124552610386, -0.5923808731731073,\n          -0.8155431286425007, -0.8415961054266092, -1.108308924501657\n      ], [0.5, 0.1]),\n      ([\'G\', \'C\', vcf_constants.GVCF_ALT_ALLELE], [\n          -0.7956722868920258, -0.663917423732382, -1.493986734511771,\n          -0.8202531343562444, -0.9377869397242453, -1.0415699718993066,\n          -1.4176189291054515, -1.5795151893394743, -1.8101482990393198,\n          -0.8139951558313916\n      ], [0, 0.5, 0.1]),\n  )\n  def test_transform_to_gvcf_no_allele_addition(self, alts, gls, vaf):\n    variant = _create_variant(\n        ref_name=\'chr1\',\n        start=10,\n        ref_base=\'A\',\n        alt_bases=alts,\n        qual=40,\n        filter_field=\'PASS\',\n        genotype=[0, 1],\n        gq=None,\n        likelihoods=gls)\n    vaf_values = [struct_pb2.Value(number_value=v) for v in vaf]\n    variant.calls[0].info[\'VAF\'].values.extend(vaf_values)\n    expected = variants_pb2.Variant()\n    expected.CopyFrom(variant)\n    actual = postprocess_variants._transform_to_gvcf_record(variant)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      # Simple inputs.\n      # Empty.\n      ([], [], []),\n      # Non-overlapping records.\n      ([(\'1\', 1, \'A\')], [], [_simple_gv(\'1\', 1, \'A\')]),\n      ([(\'1\', 3, \'C\'), (\'1\', 7, \'T\'),\n        (\'2\', 6, \'A\')], [(\'2\', 3, 6, \'G\'), (\'2\', 7, 9, \'C\')], [\n            _simple_gv(\'1\', 3, \'C\'),\n            _simple_gv(\'1\', 7, \'T\'),\n            _create_nonvariant(\'2\', 3, 6, \'G\'),\n            _simple_gv(\'2\', 6, \'A\'),\n            _create_nonvariant(\'2\', 7, 9, \'C\')\n        ]),\n      # Non-variant record overlaps a variant from the left.\n      ([(\'1\', 5, \'GTTACG\')], [(\'1\', 2, 8, \'C\')],\n       [_create_nonvariant(\'1\', 2, 5, \'C\'),\n        _simple_gv(\'1\', 5, \'GTTACG\')]),\n      # Non-variant record overlaps a variant from the right.\n      ([(\'1\', 5, \'GTTACG\')], [(\'1\', 8, 15, \'A\')],\n       [_simple_gv(\'1\', 5, \'GTTACG\'),\n        _create_nonvariant(\'1\', 11, 15, \'T\')]),\n      # Non-variant record is subsumed by a variant.\n      ([(\'1\', 5, \'GTTACG\')], [(\'1\', 5, 11, \'G\')\n                             ], [_simple_gv(\'1\', 5, \'GTTACG\')]),\n      # Non-variant record subsumes a variant.\n      ([(\'1\', 5, \'GTTACG\')], [(\'1\', 4, 12, \'G\')], [\n          _create_nonvariant(\'1\', 4, 5, \'G\'),\n          _simple_gv(\'1\', 5, \'GTTACG\'),\n          _create_nonvariant(\'1\', 11, 12, \'T\')\n      ]),\n      # Non-variant record subsumes multiple overlapping variants.\n      ([(\'1\', 3, \'CGGTTAC\'), (\'1\', 5, \'G\')], [(\'1\', 1, 15, \'A\')], [\n          _create_nonvariant(\'1\', 1, 3, \'A\'),\n          _simple_gv(\'1\', 3, \'CGGTTAC\'),\n          _simple_gv(\'1\', 5, \'G\'),\n          _create_nonvariant(\'1\', 10, 15, \'G\')\n      ]),\n      ([(\'1\', 3, \'CGGTTAC\'), (\'1\', 5, \'GTTACGT\')], [(\'1\', 1, 15, \'A\')], [\n          _create_nonvariant(\'1\', 1, 3, \'A\'),\n          _simple_gv(\'1\', 3, \'CGGTTAC\'),\n          _simple_gv(\'1\', 5, \'GTTACGT\'),\n          _create_nonvariant(\'1\', 12, 15, \'T\')\n      ]),\n  )\n  def test_merge_variants_and_nonvariants(self, variants, nonvariants,\n                                          expected):\n    viter = (_simple_variant(*v) for v in variants)\n    nonviter = (_create_nonvariant(*nv) for nv in nonvariants)\n    lessthan = postprocess_variants._get_contig_based_lessthan(_CONTIGS)\n    reader = dummy_reference_reader()\n\n    mock_vcf_writer = MockVcfWriter()\n    mock_gvcf_writer = MockVcfWriter()\n\n    postprocess_variants.merge_and_write_variants_and_nonvariants(\n        viter, nonviter, lessthan, reader, mock_vcf_writer, mock_gvcf_writer)\n\n    vcf_expected = [_simple_variant(*v) for v in variants]\n\n    self.assertEqual(mock_vcf_writer.variants_written, vcf_expected)\n    self.assertEqual(mock_gvcf_writer.variants_written, expected)\n\n  # redacted\n  def test_sort_grouped_variants(self):\n    group = [\n        _create_call_variants_output(\n            indices=[0, 1],\n            probabilities=[0.98, 0.02, 0],\n            variant=_create_variant_with_alleles(ref=\'CA\', alts=[\'C\', \'CAA\'])),\n        _create_call_variants_output(\n            indices=[1],\n            probabilities=[0.995, 0.005, 0],\n            variant=_create_variant_with_alleles(ref=\'CA\', alts=[\'C\', \'CAA\'])),\n        _create_call_variants_output(\n            indices=[0],\n            probabilities=[0.95, 0.05, 0],\n            variant=_create_variant_with_alleles(ref=\'CA\', alts=[\'C\', \'CAA\'])),\n    ]\n    output = postprocess_variants._sort_grouped_variants(group)\n    # In sorted output, 1st has indices=[0].\n    self.assertEqual(output[0], group[2])\n    self.assertEqual(output[0].alt_allele_indices.indices, [0])\n    # In sorted output, 2nd has indices=[0, 1].\n    self.assertEqual(output[1], group[0])\n    self.assertEqual(output[1].alt_allele_indices.indices, [0, 1])\n    # In sorted output, 3rd has indices=[1].\n    self.assertEqual(output[2], group[1])\n    self.assertEqual(output[2].alt_allele_indices.indices, [1])\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/resources.py,0,"b'# Copyright 2017 Google LLC.  All Rights Reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Library to gather runtime performance metrics.\n\nThis module exposes the ResourceMonitor class, which client code can use to\ngather resource usage metrics about their program. An example usage would look\nsomething like:\n\n  with ResourceMonitor() as monitor:\n    ... do work ...\n    metrics = monitor.metrics()\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport platform\nimport resource\nimport time\n\nimport psutil\n\nfrom deepvariant.protos import resources_pb2\n\n\nclass ResourceMonitor(object):\n  """"""Class for collecting resource usage info from this or child process.""""""\n\n  def __init__(self):\n    """"""Constructs a ResourceMonitor object.""""""\n    self.wall_start = None\n    self.metrics_pb = self._initial_metrics_protobuf()\n\n  def _initial_metrics_protobuf(self):\n    """"""Returns an initialized ResourceMetrics proto.\n\n    This function also fills in the ""constant"" fields of the ResourceMetrics\n    proto that don\'t depend on the actual running commands, such as host_name.\n\n    Returns:\n      learning.genomics.deepvariant.ResourceMetrics proto.\n    """"""\n    return resources_pb2.ResourceMetrics(\n        host_name=_get_host_name(),\n        cpu_frequency_mhz=_get_cpu_frequency(),\n        physical_core_count=_get_cpu_count(),\n        total_memory_mb=_get_total_memory())\n\n  def __enter__(self):\n    return self.start()\n\n  def __exit__(self, unused_type, unused_value, unused_traceback):\n    pass\n\n  def start(self):\n    """"""Starts timers associated with resource collection.\n\n    This method must be called before metrics().\n\n    Returns:\n      self to enable the idiom `monitor = ResourceMonitor().start()`.\n    """"""\n    self.wall_start = time.time()\n    return self\n\n  def metrics(self):\n    """"""Collects and return runtime metrics as a ResourceMetrics proto.\n\n    This method can be called multiple times, but wall clock time is always\n    reckoned from the time of the last start() call.\n\n    Returns:\n      A learning.genomics.deepvariant.ResourceMetrics proto message.\n\n    Raises:\n      RuntimeError: if start() was not called previously.\n    """"""\n    if self.wall_start is None:\n      raise RuntimeError(\'start() must be called prior to metrics()\')\n\n    self.metrics_pb.wall_time_seconds = time.time() - self.wall_start\n\n    # Consider using psutil.cpu_times() instead to get more detailed information\n    # about the usage in self and all children.\n    try:\n      rusage = resource.getrusage(resource.RUSAGE_SELF)\n      self.metrics_pb.cpu_user_time_seconds = rusage.ru_utime\n      self.metrics_pb.cpu_system_time_seconds = rusage.ru_stime\n      self.metrics_pb.memory_peak_rss_mb = int(rusage.ru_maxrss / 1024)\n    except resource.error:\n      # The OS call to get rusage failed, so just don\'t set the field values,\n      # leaving them as the defalt values of 0.\n      pass\n\n    # Create a psutil.Process pointed at the current process.\n    process = psutil.Process()\n    io_counters = process.io_counters()\n    self.metrics_pb.read_bytes = io_counters.read_bytes\n    self.metrics_pb.write_bytes = io_counters.write_bytes\n\n    return self.metrics_pb\n\n\n# ------------------------------------------------------------------------------\n# Simple functions for getting host_name, cpu count, etc. Isolated here to make\n# them mockable.\n# ------------------------------------------------------------------------------\n\n\ndef _get_host_name():\n  """"""Gets the host name of this machine.""""""\n  return platform.node()\n\n\ndef _get_cpu_count():\n  """"""Gets the number of physical cores in this machine.\n\n  Returns:\n    int >= 1 if the call to get the cpu_count succeeded, or 0 if not.\n  """"""\n  return psutil.cpu_count(logical=False) or 0\n\n\ndef _get_cpu_frequency():\n  """"""Gets the frequency in MHz of the cpus in this machine.\n\n  Returns:\n    float > 0 if the call to get the cpu_frequency succeeded. This information\n    may not be available on all systems, in which case we return 0.0.\n  """"""\n  try:\n    freq = psutil.cpu_freq()\n    return freq.current if freq is not None else 0.0\n  except NotImplementedError:\n    return 0.0\n\n\ndef _get_total_memory():\n  """"""Gets the total memory in megabytes in this machine.""""""\n  return int(psutil.virtual_memory().total / (1024 * 1024))\n'"
deepvariant/resources_test.py,0,"b'# Copyright 2017 Google LLC.  All Rights Reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for learning.genomics.deepvariant.resources.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport resource\nfrom absl.testing import absltest\n# https://stackoverflow.com/questions/34630393/python2-7-contextlib-exitstack-equivalent\nimport contextlib2\nimport mock\n\nfrom deepvariant import resources\n\n\nclass ResourcesTest(absltest.TestCase):\n\n  def test_monitor_real_function_calls(self):\n    # We want to actually make all of the real function calls under test, but\n    # we of course don\'t know their values and can only do sanity checks.\n    with resources.ResourceMonitor() as monitor:\n      metrics = monitor.metrics()\n      self.assertNotEmpty(metrics.host_name)\n      self.assertGreater(metrics.physical_core_count, 0)\n      self.assertGreater(metrics.total_memory_mb, 0)\n      self.assertGreater(metrics.cpu_user_time_seconds, 0)\n      self.assertGreater(metrics.cpu_system_time_seconds, 0)\n      self.assertGreater(metrics.memory_peak_rss_mb, 0)\n\n      # We unfortunately cannot make sure that read_bytes and write_bytes is\n      # greater than zero, so these tests are commented out.\n      # self.assertGreater(metrics.read_bytes, 0)\n      # self.assertGreater(metrics.write_bytes, 0)\n\n      # CPU frequency may not be available on all systems, so the value is\n      # either a real frequency (> 0) or the magic value of 0.0 indicating that\n      # the value could not be determined.\n      self.assertGreaterEqual(metrics.cpu_frequency_mhz, 0.0)\n\n  def test_monitor_gets_expected_metric_values(self):\n    patchers = [\n        mock.patch.object(resources, \'_get_host_name\', return_value=\'hostname\'),\n        mock.patch.object(resources, \'_get_cpu_count\', return_value=3),\n        mock.patch.object(resources, \'_get_cpu_frequency\', return_value=2312.4),\n        mock.patch.object(resources, \'_get_total_memory\', return_value=1234),\n        mock.patch.object(\n            resources.resource,\n            \'getrusage\',\n            return_value=mock.Mock(\n                ru_utime=1.0,\n                ru_stime=2.0,\n                ru_maxrss=3 * 1024  # 3 Megabytes\n            )),\n    ]\n\n    process_mock = mock.Mock()\n    process_mock.io_counters.return_value = mock.Mock(\n        read_bytes=12, write_bytes=34)\n    patchers.append(\n        mock.patch.object(\n            resources.psutil, \'Process\', return_value=process_mock))\n\n    with contextlib2.ExitStack() as stack:\n      for ctx in patchers:\n        stack.enter_context(ctx)\n      with resources.ResourceMonitor() as monitor:\n        metrics = monitor.metrics()\n\n        # Environment metrics; all mocked out.\n        self.assertEqual(metrics.host_name, \'hostname\')\n        self.assertEqual(metrics.physical_core_count, 3)\n        self.assertEqual(metrics.cpu_frequency_mhz, 2312.4)\n        self.assertEqual(metrics.total_memory_mb, 1234)\n\n        # Runtime metrics; they are all mocked out.\n        self.assertEqual(metrics.cpu_user_time_seconds, 1.0)\n        self.assertEqual(metrics.cpu_system_time_seconds, 2.0)\n        self.assertEqual(metrics.memory_peak_rss_mb, 3)\n        self.assertEqual(metrics.read_bytes, 12)\n        self.assertEqual(metrics.write_bytes, 34)\n\n  def test_start_returns_self(self):\n    monitor = resources.ResourceMonitor()\n    self.assertIs(monitor.start(), monitor)\n\n  def test_metrics_without_start_raises_exception(self):\n    monitor = resources.ResourceMonitor()\n    with self.assertRaises(RuntimeError):\n      monitor.metrics()\n\n  def test_metrics_is_ok_if_rusage_fails(self):\n    # Some psutil functions, such as cpu_freq(), can return None depending on\n    # the environment; make sure we don\'t crash when that occurs.\n    with mock.patch.object(\n        resources.resource, \'getrusage\', side_effect=resource.error):\n      with resources.ResourceMonitor() as monitor:\n        self.assertEqual(monitor.metrics().cpu_user_time_seconds, 0)\n        self.assertEqual(monitor.metrics().cpu_system_time_seconds, 0)\n        self.assertEqual(monitor.metrics().memory_peak_rss_mb, 0)\n\n  def test_metrics_is_ok_when_cpu_freq_returns_none(self):\n    # Some psutil functions, such as cpu_freq(), can return None depending on\n    # the environment; make sure we don\'t crash when that occurs.\n    with mock.patch.object(resources.psutil, \'cpu_freq\', return_value=None):\n      with resources.ResourceMonitor() as monitor:\n        self.assertEqual(monitor.metrics().cpu_frequency_mhz, 0)\n\n  def test_metrics_is_ok_when_cpu_freq_throws(self):\n    # psutil.cpu_freq() can throw NotImplementedError in certain environments;\n    # make sure we don\'t crash when that occurs.\n    with mock.patch.object(\n        resources.psutil, \'cpu_freq\', side_effect=NotImplementedError):\n      with resources.ResourceMonitor() as monitor:\n        self.assertEqual(monitor.metrics().cpu_frequency_mhz, 0)\n\n  def test_metrics_is_ok_when_cpu_count_returns_none(self):\n    # Some psutil functions, such as cpu_freq(), can return None depending on\n    # the environment; make sure we don\'t crash when that occurs.\n    with mock.patch.object(resources.psutil, \'cpu_count\', return_value=None):\n      with resources.ResourceMonitor() as monitor:\n        self.assertEqual(monitor.metrics().physical_core_count, 0)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/testdata.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Utilities to help with testing DeepVariant code.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\n\n\nfrom third_party.nucleus.testing import test_utils as nucleus_test_utils\n\nDEEPVARIANT_DATADIR = \'\'\n\n\ndef deepvariant_testdata(filename):\n  """"""Gets the path to filename in genomics/deepvariant/testdata.\n\n  These paths are only known at runtime, after flag parsing\n  has occurred.\n\n  Args:\n    filename: The name of a testdata file in the core genomics testdata\n      directory. For example, if you have a test file in\n      ""learning/genomics/deepvariant/testdata/foo.txt"", filename should be\n      ""foo.txt"" to get a path to it.\n\n  Returns:\n    The absolute path to a testdata file.\n  """"""\n  return nucleus_test_utils.genomics_testdata(\n      os.path.join(\'deepvariant/testdata\', filename), DEEPVARIANT_DATADIR)\n\n\nCHR20_FASTA = None\nCHR20_BAM = None\nCHR20_BAM_FIRST_HALF = None\nCHR20_BAM_SECOND_HALF = None\nNOCHR20_BAM = None\nCHR20_CRAM = None\nGOLDEN_TRAINING_EXAMPLES = None\nGOLDEN_CALLING_CANDIDATES = None\nGOLDEN_CALLING_EXAMPLES = None\nCONFIDENT_REGIONS_BED = None\nTRUTH_VARIANTS_VCF = None\nTRUTH_VARIANTS_VCF_WITH_TYPES = None\nGOLDEN_POSTPROCESS_INPUT = None\nGOLDEN_POSTPROCESS_OUTPUT = None\nGOLDEN_POSTPROCESS_OUTPUT_COMPRESSED = None\nGOLDEN_POSTPROCESS_GVCF_INPUT = None\nGOLDEN_POSTPROCESS_GVCF_OUTPUT = None\nGOLDEN_POSTPROCESS_GVCF_OUTPUT_COMPRESSED = None\nGOLDEN_MAKE_EXAMPLES_RUN_INFO = None\nWS_ALLELE_COUNT_LINEAR_MODEL = None\nWS_ALLELE_COUNT_LINEAR_MODEL_PCKL = None\nWS_VARIANT_READS_THRESHOLD_MODEL = None\nGOLDEN_VCF_CANDIDATE_IMPORTER_POSTPROCESS_INPUT = None\nGOLDEN_VCF_CANDIDATE_IMPORTER_POSTPROCESS_OUTPUT = None\n\nN_GOLDEN_TRAINING_EXAMPLES = 49\nN_GOLDEN_CALLING_EXAMPLES = 86\n\n# For CustomizedClassesVariantLabeler\nCUSTOMIZED_CLASSES_GOLDEN_TRAINING_EXAMPLES = None\nCUSTOM_PILEUP_IMAGE_GOLDEN_TRAINING_EXAMPLES = None\n\n# For VcfCandidateImporter\nGOLDEN_VCF_CANDIDATE_IMPORTER_TRAINING_EXAMPLES = None\nGOLDEN_VCF_CANDIDATE_IMPORTER_CALLING_EXAMPLES = None\nVCF_CANDIDATE_IMPORTER_VARIANTS = None\n\n\ndef init():\n  """"""Initialize global variables from flag values.""""""\n  global CHR20_FASTA\n  global CHR20_BAM\n  global CHR20_BAM_FIRST_HALF\n  global CHR20_BAM_SECOND_HALF\n  global NOCHR20_BAM\n  global CHR20_CRAM\n  global GOLDEN_TRAINING_EXAMPLES\n  global GOLDEN_CALLING_CANDIDATES\n  global GOLDEN_CALLING_EXAMPLES\n  global CONFIDENT_REGIONS_BED\n  global TRUTH_VARIANTS_VCF\n  global TRUTH_VARIANTS_VCF_WITH_TYPES\n  global GOLDEN_POSTPROCESS_INPUT\n  global GOLDEN_POSTPROCESS_OUTPUT\n  global GOLDEN_POSTPROCESS_OUTPUT_COMPRESSED\n  global GOLDEN_POSTPROCESS_GVCF_INPUT\n  global GOLDEN_POSTPROCESS_GVCF_OUTPUT\n  global GOLDEN_POSTPROCESS_GVCF_OUTPUT_COMPRESSED\n  global GOLDEN_MAKE_EXAMPLES_RUN_INFO\n  global WS_ALLELE_COUNT_LINEAR_MODEL\n  global WS_ALLELE_COUNT_LINEAR_MODEL_PCKL\n  global WS_VARIANT_READS_THRESHOLD_MODEL\n  global GOLDEN_VCF_CANDIDATE_IMPORTER_POSTPROCESS_INPUT\n  global GOLDEN_VCF_CANDIDATE_IMPORTER_POSTPROCESS_OUTPUT\n\n  CHR20_FASTA = deepvariant_testdata(\'ucsc.hg19.chr20.unittest.fasta.gz\')\n  CHR20_BAM = deepvariant_testdata(\'NA12878_S1.chr20.10_10p1mb.bam\')\n  # # Here is how ""NA12878_S1.chr20.10_10p1mb.first_half.bam""\n  # # and ""NA12878_S1.chr20.10_10p1mb.second_half.bam"" are split\n  # # from NA12878_S1.chr20.10_10p1mb.bam.\n  # READS_FIRST_HALF=${TESTDATA_DIR}/NA12878_S1.chr20.10_10p1mb.first_half.bam\n  # READS_SECOND_HALF=${TESTDATA_DIR}/NA12878_S1.chr20.10_10p1mb.second_half.bam\n  # READS=${TESTDATA_DIR}/NA12878_S1.chr20.10_10p1mb.bam\n  # samtools view -H ${READS} > /tmp/f1.sam\n  # cp /tmp/f1.sam /tmp/f2.sam\n  # # Because ${READS} has total of 52035 lines, we split in roughly half.\n  # samtools view ${READS} | head -26000 >> /tmp/f1.sam\n  # samtools view ${READS} | tail -26035 >> /tmp/f2.sam\n  # samtools view -S -b /tmp/f1.sam > ${READS_FIRST_HALF}\n  # samtools view -S -b /tmp/f2.sam > ${READS_SECOND_HALF}\n  # samtools index ${READS_FIRST_HALF}\n  # samtools index ${READS_SECOND_HALF}\n  CHR20_BAM_FIRST_HALF = deepvariant_testdata(\n      \'NA12878_S1.chr20.10_10p1mb.first_half.bam\')\n  CHR20_BAM_SECOND_HALF = deepvariant_testdata(\n      \'NA12878_S1.chr20.10_10p1mb.second_half.bam\')\n  # # Here is how the ""HG002_NIST_150bp_downsampled_30x.chr20.10_10p1mb.bam""\n  # # file was created.\n  # samtools view -hb HG002_NIST_150bp_downsampled_30x.bam \\\n  #     20:10,000,000-10,100,000 \\\n  #     > HG002_NIST_150bp_downsampled_30x.chr20.10_10p1mb.bam\n  # samtools index HG002_NIST_150bp_downsampled_30x.chr20.10_10p1mb.bam\n  NOCHR20_BAM = deepvariant_testdata(\n      \'HG002_NIST_150bp_downsampled_30x.chr20.10_10p1mb.bam\')\n  CHR20_CRAM = deepvariant_testdata(\'NA12878_S1.chr20.10_10p1mb.cram\')\n  GOLDEN_TRAINING_EXAMPLES = deepvariant_testdata(\n      \'golden.training_examples.tfrecord.gz\')\n  GOLDEN_CALLING_CANDIDATES = deepvariant_testdata(\n      \'golden.calling_examples.tfrecord.gz\')\n  GOLDEN_CALLING_EXAMPLES = deepvariant_testdata(\n      \'golden.calling_examples.tfrecord.gz\')\n  CONFIDENT_REGIONS_BED = deepvariant_testdata(\n      \'test_nist.b37_chr20_100kbp_at_10mb.bed\')\n  TRUTH_VARIANTS_VCF = deepvariant_testdata(\n      \'test_nist.b37_chr20_100kbp_at_10mb.vcf.gz\')\n  TRUTH_VARIANTS_VCF_WITH_TYPES = deepvariant_testdata(\n      \'with_types.test_nist.b37_chr20_4kbp_at_10mb.vcf.gz\')\n  GOLDEN_POSTPROCESS_INPUT = deepvariant_testdata(\n      \'golden.postprocess_single_site_input.tfrecord.gz\')\n  GOLDEN_POSTPROCESS_OUTPUT = deepvariant_testdata(\n      \'golden.postprocess_single_site_output.vcf\')\n  GOLDEN_POSTPROCESS_OUTPUT_COMPRESSED = deepvariant_testdata(\n      \'golden.postprocess_single_site_output.vcf.gz\')\n  GOLDEN_POSTPROCESS_GVCF_INPUT = deepvariant_testdata(\n      \'golden.postprocess_gvcf_input.tfrecord.gz\')\n  GOLDEN_POSTPROCESS_GVCF_OUTPUT = deepvariant_testdata(\n      \'golden.postprocess_gvcf_output.g.vcf\')\n  GOLDEN_MAKE_EXAMPLES_RUN_INFO = deepvariant_testdata(\n      \'golden.training_examples.tfrecord.gz.run_info.pbtxt\')\n  WS_ALLELE_COUNT_LINEAR_MODEL = deepvariant_testdata(\n      \'window_selector_allele_count_linear.pbtxt\')\n  WS_ALLELE_COUNT_LINEAR_MODEL_PCKL = deepvariant_testdata(\n      \'window_selector_allele_count_linear.pckl\')\n  WS_VARIANT_READS_THRESHOLD_MODEL = deepvariant_testdata(\n      \'window_selector_variant_read_threshold.pbtxt\')\n  GOLDEN_VCF_CANDIDATE_IMPORTER_POSTPROCESS_INPUT = deepvariant_testdata(\n      \'golden.vcf_candidate_importer_postprocess_single_site_input.tfrecord.gz\')\n  GOLDEN_VCF_CANDIDATE_IMPORTER_POSTPROCESS_OUTPUT = deepvariant_testdata(\n      \'golden.vcf_candidate_importer_postprocess_single_site_output.vcf\')\n\n  # For CustomizedClassesVariantLabeler.\n  global CUSTOMIZED_CLASSES_GOLDEN_TRAINING_EXAMPLES\n  CUSTOMIZED_CLASSES_GOLDEN_TRAINING_EXAMPLES = deepvariant_testdata(\n      \'customized_classes.golden.training_examples.tfrecord.gz\')\n\n  # For adding an extra channel.\n  global CUSTOM_PILEUP_IMAGE_GOLDEN_TRAINING_EXAMPLES\n  CUSTOM_PILEUP_IMAGE_GOLDEN_TRAINING_EXAMPLES = deepvariant_testdata(\n      \'custom_pileup_image.golden.training_examples.tfrecord.gz\')\n\n  # For VcfCandidateImporter\n  global GOLDEN_VCF_CANDIDATE_IMPORTER_TRAINING_EXAMPLES\n  global GOLDEN_VCF_CANDIDATE_IMPORTER_CALLING_EXAMPLES\n  global VCF_CANDIDATE_IMPORTER_VARIANTS\n  GOLDEN_VCF_CANDIDATE_IMPORTER_TRAINING_EXAMPLES = deepvariant_testdata(\n      \'golden.vcf_candidate_importer.training_examples.tfrecord.gz\')\n  GOLDEN_VCF_CANDIDATE_IMPORTER_CALLING_EXAMPLES = deepvariant_testdata(\n      \'golden.vcf_candidate_importer_calling_examples.tfrecord\')\n  VCF_CANDIDATE_IMPORTER_VARIANTS = deepvariant_testdata(\n      \'vcf_candidate_importer.indels.chr20.vcf.gz\')\n'"
deepvariant/tf_utils.py,17,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Utility functions for working with TensorFlow in DeepVariant.\n\nA collection of utilities for working with the TF models and evaluations we use\nin DeepVariant.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport enum\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom deepvariant.protos import deepvariant_pb2\nfrom third_party.nucleus.io import sharded_file_utils\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import ranges\nfrom third_party.nucleus.util import variant_utils\nfrom tensorflow.core.example import example_pb2\n\n# Convert strings up to this length, then clip.  We picked a number that\n# was less than 1K, with a bit of extra space for the length element,\n# to give enough space without overflowing to a larger multiple of 128.\nSTRING_TO_INT_MAX_CONTENTS_LEN = 1020\n# This is the length of the resulting buffer (including the length entry).\nSTRING_TO_INT_BUFFER_LENGTH = STRING_TO_INT_MAX_CONTENTS_LEN + 1\n\n\nclass EncodedVariantType(enum.Enum):\n  """"""Enum capturing the int64 values we encode for different variant types.\n\n  TPUs really like fixed length features, which makes it very difficult to use\n  extract the type of a variant for an example using an encoded Variant\n  protobufs or even a string value like ""snp"". The current best option appears\n  to be to encode the type of a variant directly in an example as an int64. This\n  enum provides a mapping between those raw int64 values in the example and a\n  human-meaningful name for that type.\n  """"""\n  UNKNOWN = 0  # A variant of unknown type.\n  SNP = 1  # The variant is a SNP.\n  INDEL = 2  # The variant is an indel.\n\n\ndef encoded_variant_type(variant):\n  """"""Gets the EncodedVariantType for variant.\n\n  This function examines variant and returns the EncodedVariantType that best\n  describes the variation type of variant. For example, if variant has\n  `reference_bases = ""A""` and `alternative_bases = [""C""]` this function would\n  return EncodedVariantType.SNP.\n\n  Args:\n    variant: nucleus.Variant proto. The variant whose EncodedVariantType we want\n      to get.\n\n  Returns:\n    EncodedVariantType enum value.\n  """"""\n  if variant_utils.is_snp(variant):\n    return EncodedVariantType.SNP\n  elif variant_utils.is_indel(variant):\n    return EncodedVariantType.INDEL\n  else:\n    return EncodedVariantType.UNKNOWN\n\n\ndef example_variant_type(example):\n  """"""Gets the locus field from example as a string.""""""\n  return example.features.feature[\'variant_type\'].int64_list.value[0]\n\n\ndef example_locus(example):\n  """"""Gets the locus field from example as a string.""""""\n  return example.features.feature[\'locus\'].bytes_list.value[0]\n\n\ndef example_alt_alleles_indices(example):\n  """"""Gets an iterable of the alt allele indices in example.""""""\n  return deepvariant_pb2.CallVariantsOutput.AltAlleleIndices.FromString(\n      example.features.feature[\'alt_allele_indices/encoded\'].bytes_list.value[0]\n  ).indices\n\n\ndef example_alt_alleles(example, variant=None):\n  """"""Gets a list of the alt alleles in example.""""""\n  variant = variant if variant else example_variant(example)\n  return [\n      variant.alternate_bases[i] for i in example_alt_alleles_indices(example)\n  ]\n\n\ndef example_encoded_image(example):\n  """"""Gets image field from example as a string.""""""\n  return example.features.feature[\'image/encoded\'].bytes_list.value[0]\n\n\ndef example_variant(example):\n  """"""Gets and decodes the variant field from example as a Variant.""""""\n  encoded = example.features.feature[\'variant/encoded\'].bytes_list.value[0]\n  return variants_pb2.Variant.FromString(encoded)\n\n\ndef example_label(example):\n  """"""Gets the label field from example as a string.""""""\n  return int(example.features.feature[\'label\'].int64_list.value[0])\n\n\ndef example_image_format(example):\n  """"""Gets the image format field from example as a string.""""""\n  return example.features.feature[\'image/format\'].bytes_list.value[0]\n\n\ndef example_image_shape(example):\n  """"""Gets the image shape field from example as a list of int64.""""""\n  if len(example.features.feature[\'image/shape\'].int64_list.value) != 3:\n    raise ValueError(\'Invalid image/shape: we expect to find an image/shape \'\n                     \'field with length 3.\')\n  return example.features.feature[\'image/shape\'].int64_list.value[0:3]\n\n\ndef example_key(example):\n  """"""Constructs a key for example based on its position and alleles.""""""\n  variant = example_variant(example)\n  alts = example_alt_alleles(example)\n  return \'{}:{}:{}->{}\'.format(variant.reference_name, variant.start + 1,\n                               variant.reference_bases, \'/\'.join(alts))\n\n\ndef example_set_label(example, numeric_label):\n  """"""Sets the label features of example.\n\n  Sets the label feature of example to numeric_label.\n\n  Args:\n    example: a tf.Example proto.\n    numeric_label: A numeric (int64 compatible) label for example.\n  """"""\n  example.features.feature[\'label\'].int64_list.value[:] = [numeric_label]\n\n\ndef example_set_variant(example, variant):\n  """"""Sets the variant/encoded feature of example to variant.SerializeToString().\n\n  Args:\n    example: a tf.Example proto.\n    variant: third_party.nucleus.protos.Variant protobuf containing information\n      about a candidate variant call.\n  """"""\n  example.features.feature[\'variant/encoded\'].bytes_list.value[:] = [\n      variant.SerializeToString()\n  ]\n\n\ndef example_sequencing_type(example):\n  return example.features.feature[\'sequencing_type\'].int64_list.value[0]\n\n\ndef get_one_example_from_examples_path(source, proto=None):\n  """"""Get the first record from `source`.\n\n  Args:\n    source: str. A pattern or a comma-separated list of patterns that represent\n      file names.\n    proto: A proto class. proto.FromString() will be called on each serialized\n      record in path to parse it.\n\n  Returns:\n    The first record, or None.\n  """"""\n  files = sharded_file_utils.glob_list_sharded_file_patterns(source)\n  if not files:\n    raise ValueError(\n        \'Cannot find matching files with the pattern ""{}""\'.format(source))\n  for f in files:\n    try:\n      return next(tfrecord.read_tfrecords(f, proto=proto))\n    except StopIteration:\n      # Getting a StopIteration from one next() means source_path is empty.\n      # Move on to the next one to try to get one example.\n      pass\n  return None\n\n\ndef get_shape_from_examples_path(source):\n  """"""Reads one record from source to determine the tensor shape for all.""""""\n  one_example = get_one_example_from_examples_path(source)\n  if one_example:\n    return example_image_shape(one_example)\n  return None\n\n\ndef get_format_from_examples_path(source):\n  """"""Reads one record from source to determine the format for all.""""""\n  one_example = get_one_example_from_examples_path(source)\n  if one_example:\n    return example_image_format(one_example)\n  return None\n\n\ndef _simplify_variant(variant):\n  """"""Returns a new Variant with only the basic fields of variant.""""""\n\n  def _simplify_variant_call(call):\n    """"""Returns a new VariantCall with the basic fields of call.""""""\n    return variants_pb2.VariantCall(\n        call_set_name=call.call_set_name,\n        genotype=call.genotype,\n        info=dict(call.info))  # dict() is necessary to actually set info.\n\n  return variants_pb2.Variant(\n      reference_name=variant.reference_name,\n      start=variant.start,\n      end=variant.end,\n      reference_bases=variant.reference_bases,\n      alternate_bases=variant.alternate_bases,\n      filter=variant.filter,\n      quality=variant.quality,\n      calls=[_simplify_variant_call(call) for call in variant.calls])\n\n\ndef make_example(variant,\n                 alt_alleles,\n                 encoded_image,\n                 shape,\n                 image_format,\n                 second_image=None,\n                 sequencing_type=0):\n  """"""Creates a new tf.Example suitable for use with DeepVariant.\n\n  Args:\n    variant: third_party.nucleus.protos.Variant protobuf containing information\n      about a candidate variant call.\n    alt_alleles: A set of strings. Indicates the alternate alleles used as ""alt""\n      when constructing the image.\n    encoded_image: a Tensor of type tf.string. Should contain an image encoding\n      the reference and read data supporting variant. The encoding should be\n      consistent with the image_format argument.\n    shape: a list of (width, height, channel).\n    image_format: string. The scheme used to encode our image.\n    second_image: a Tensor of type tf.string or None. Contains second image that\n      encodes read data from another DNA sample. Must satisfy the same\n      requirements as encoded_image.\n    sequencing_type: int. The sequencing type of the input image.\n\n  Returns:\n    A tf.Example proto containing the standard DeepVariant features.\n  """"""\n  example = example_pb2.Example()\n  features = example.features\n  features.feature[\'locus\'].bytes_list.value.append(\n      six.b(\n          ranges.to_literal(\n              ranges.make_range(variant.reference_name, variant.start,\n                                variant.end))))\n  example_set_variant(example, variant)\n  variant_type = encoded_variant_type(variant).value\n  features.feature[\'variant_type\'].int64_list.value.append(variant_type)\n  all_alts = list(variant.alternate_bases)\n  alt_indices = sorted(all_alts.index(alt) for alt in alt_alleles)\n\n  features.feature[\'alt_allele_indices/encoded\'].bytes_list.value.append(\n      deepvariant_pb2.CallVariantsOutput.AltAlleleIndices(\n          indices=alt_indices).SerializeToString())\n\n  features.feature[\'image/encoded\'].bytes_list.value.append(encoded_image)\n  features.feature[\'image/format\'].bytes_list.value.append(six.b(image_format))\n  features.feature[\'image/shape\'].int64_list.value.extend(shape)\n  if second_image is not None:\n    features.feature[\'second_image/encoded\'].bytes_list.value.append(\n        six.b(second_image))\n    features.feature[\'second_image/format\'].bytes_list.value.append(\n        six.b(image_format))\n    features.feature[\'second_image/shape\'].int64_list.value.extend(shape)\n  features.feature[\'sequencing_type\'].int64_list.value.append(sequencing_type)\n  return example\n\n\ndef model_shapes(checkpoint_path, variables_to_get=None):\n  """"""Returns the shape of each tensor in the model at checkpoint_path.\n\n  Args:\n    checkpoint_path: string. The path to a tensorflow checkpoint containing a\n      model whose tensor shapes we want to get.\n    variables_to_get: options, list of strings. If provided, only returns the\n      shapes of tensors in variables whose name is present in this list. If\n      None, the default, gets all of the tensors. A KeyError will be raised if\n      any variable name in variables_to_get isn\'t present in the checkpointed\n      model.\n\n  Returns:\n    A dictionary mapping variable names [string] to tensor shapes [tuple].\n  """"""\n  reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path)\n  var_to_shape_map = reader.get_variable_to_shape_map()\n  keys = variables_to_get if variables_to_get else var_to_shape_map.keys()\n  return {key: tuple(var_to_shape_map[key]) for key in keys}\n\n\ndef model_num_classes(checkpoint_path, n_classes_model_variable):\n  """"""Returns the number of classes in the checkpoint.""""""\n  if not checkpoint_path:\n    return None\n\n  # Figure out how many classes this inception model was trained to predict.\n  try:\n    shapes = model_shapes(checkpoint_path, [n_classes_model_variable])\n  except KeyError:\n    return None\n  if n_classes_model_variable not in shapes:\n    return None\n  return shapes[n_classes_model_variable][-1]\n\n\ndef string_to_int_tensor(x):\n  """"""Graph operations decode a string into a fixed-size tensor of ints.""""""\n  decoded = tf.compat.v1.decode_raw(x, tf.uint8)\n  clipped = decoded[:STRING_TO_INT_MAX_CONTENTS_LEN]  # clip to allowed max_len\n  shape = tf.shape(input=clipped)\n  slen = shape[0]\n  # pad to desired max_len\n  padded = tf.pad(\n      tensor=clipped, paddings=[[0, STRING_TO_INT_MAX_CONTENTS_LEN - slen]])\n  casted = tf.cast(padded, tf.int32)\n  casted.set_shape([STRING_TO_INT_MAX_CONTENTS_LEN])\n  return tf.concat([[slen], casted], 0)\n\n\ndef int_tensor_to_string(x):\n  """"""Python operations to encode a tensor of ints into string of bytes.""""""\n  slen = x[0]\n  v = x[1:slen + 1]\n  return np.array(v, dtype=np.uint8).tostring()\n\n\ndef compression_type_of_files(files):\n  """"""Return GZIP or None for the compression type of the files.""""""\n  return \'GZIP\' if all(f.endswith(\'.gz\') for f in files) else None\n\n\ndef tpu_available(sess=None):\n  """"""Return true if a TPU device is available to the default session.""""""\n  if sess is None:\n    init_op = tf.group(tf.compat.v1.global_variables_initializer(),\n                       tf.compat.v1.local_variables_initializer())\n    with tf.compat.v1.Session() as sess:\n      sess.run(init_op)\n      devices = sess.list_devices()\n  else:\n    devices = sess.list_devices()\n  return any(dev.device_type == \'TPU\' for dev in devices)\n\n\ndef resolve_master(master, tpu_name, tpu_zone, gcp_project):\n  """"""Resolve the master\'s URL given standard flags.""""""\n  if master is not None:\n    return master\n  elif tpu_name is not None:\n    return tf.distribute.cluster_resolver.TPUClusterResolver(\n        tpu=[tpu_name], zone=tpu_zone, project=gcp_project).get_master()\n  else:\n    # For k8s TPU we do not have/need tpu_name. See\n    # https://cloud.google.com/tpu/docs/kubernetes-engine-setup#tensorflow-code\n    return tf.distribute.cluster_resolver.TPUClusterResolver().get_master()\n'"
deepvariant/tf_utils_test.py,16,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant.tf_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\nimport six\nimport tensorflow as tf\n\nfrom deepvariant import tf_utils\nfrom tensorflow.python.platform import gfile\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom tensorflow.core.example import example_pb2\n\n\nclass TFUtilsTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.alts = [\'A\']\n    self.variant = variants_pb2.Variant(\n        reference_name=\'1\',\n        start=10,\n        end=11,\n        reference_bases=\'C\',\n        alternate_bases=self.alts)\n    self.encoded_image = six.b(\'encoded_image_data\')\n    self.default_shape = [5, 5, 7]\n    self.default_format = \'raw\'\n\n  def testModelShapes(self):\n    # Builds a graph.\n    v0 = tf.Variable([[1, 2, 3], [4, 5, 6]], dtype=tf.float32, name=\'v0\')\n    v1 = tf.Variable([[[1], [2]], [[3], [4]], [[5], [6]]],\n                     dtype=tf.float32,\n                     name=\'v1\')\n    init_all_op = tf.compat.v1.initialize_all_variables()\n    save = tf.compat.v1.train.Saver({\'v0\': v0, \'v1\': v1})\n    save_path = test_utils.test_tmpfile(\'ckpt_for_debug_string\')\n    with tf.compat.v1.Session() as sess:\n      sess.run(init_all_op)\n      # Saves a checkpoint.\n      save.save(sess, save_path)\n\n      # Model shapes without any variable requests gives you all variables.\n      self.assertEqual({\n          \'v0\': (2, 3),\n          \'v1\': (3, 2, 1)\n      }, tf_utils.model_shapes(save_path))\n      # Asking for v0 gives you only v0\'s shape.\n      self.assertEqual({\'v0\': (2, 3)}, tf_utils.model_shapes(save_path, [\'v0\']))\n      # Asking for v1 gives you only v1\'s shape.\n      self.assertEqual({\'v1\': (3, 2, 1)},\n                       tf_utils.model_shapes(save_path, [\'v1\']))\n\n      # Verifies model_shapes() fails for non-existent tensors.\n      with six.assertRaisesRegex(self, KeyError, \'v3\'):\n        tf_utils.model_shapes(save_path, [\'v3\'])\n\n  def testModelNumClasses(self):\n    # Builds a graph.\n    class_variable_name = \'class_variable_name\'\n    v0 = tf.Variable([[1, 2, 3]], dtype=tf.int32, name=\'class_variable_name\')\n    v1 = tf.Variable([[[1], [2]], [[3], [4]], [[5], [6]]],\n                     dtype=tf.float32,\n                     name=\'v1\')\n    init_all_op = tf.compat.v1.initialize_all_variables()\n    save = tf.compat.v1.train.Saver({class_variable_name: v0, \'v1\': v1})\n    save_path = test_utils.test_tmpfile(\'ckpt_for_debug_classes\')\n    with tf.compat.v1.Session() as sess:\n      sess.run(init_all_op)\n      # Saves a checkpoint.\n      save.save(sess, save_path)\n\n      # If you pass in the correct class_variable_name, you\'ll find the number\n      # of classes.\n      self.assertEqual(\n          3, tf_utils.model_num_classes(save_path, class_variable_name))\n      # If the class variable name doesn\'t existin the checkpoint, return None.\n      self.assertEqual(\n          None, tf_utils.model_num_classes(save_path, \'non-existent-var\'))\n      # If the checkpoint doesn\'t exist, return none.\n      self.assertEqual(None,\n                       tf_utils.model_num_classes(None, class_variable_name))\n\n  def testMakeExample(self):\n    example = tf_utils.make_example(self.variant, self.alts, self.encoded_image,\n                                    self.default_shape, self.default_format)\n\n    self.assertEqual(self.encoded_image,\n                     tf_utils.example_encoded_image(example))\n    self.assertEqual(\n        six.b(\'raw\'),\n        example.features.feature[\'image/format\'].bytes_list.value[0])\n    self.assertEqual(self.variant, tf_utils.example_variant(example))\n    self.assertEqual(six.b(\'1:11-11\'), tf_utils.example_locus(example))\n    self.assertEqual([0], tf_utils.example_alt_alleles_indices(example))\n    self.assertEqual(\'1:11:C->A\', tf_utils.example_key(example))\n    self.assertEqual(tf_utils.EncodedVariantType.SNP.value,\n                     tf_utils.example_variant_type(example))\n\n  def testMakeExampleMultiAllelic(self):\n    alts = [\'AA\', \'CC\', \'GG\']\n    self.variant.alternate_bases[:] = alts\n    # Providing GG, AA checks that we\'re sorting the indices.\n    example = tf_utils.make_example(self.variant, [\'GG\', \'AA\'], six.b(\'foo\'),\n                                    self.default_shape, self.default_format)\n    self.assertEqual([0, 2], tf_utils.example_alt_alleles_indices(example))\n    self.assertEqual([\'AA\', \'GG\'], tf_utils.example_alt_alleles(example))\n    self.assertEqual(\'1:11:C->AA/GG\', tf_utils.example_key(example))\n    self.assertEqual(tf_utils.EncodedVariantType.INDEL.value,\n                     tf_utils.example_variant_type(example))\n\n  def testAltAllelesWithVariant(self):\n    alts = list(self.variant.alternate_bases)\n    example = tf_utils.make_example(self.variant, alts, six.b(\'foo\'),\n                                    self.default_shape, self.default_format)\n    self.assertEqual([0], tf_utils.example_alt_alleles_indices(example))\n    with mock.patch(\n        \'deepvariant.tf_utils.example_variant\'\n    ) as mock_ex_variant:\n      # Providing variant directly avoids the call to example_variant().\n      self.assertEqual(\n          alts, tf_utils.example_alt_alleles(example, variant=self.variant))\n      mock_ex_variant.assert_not_called()\n\n      # Checks that we load the variant if needed and that our mock is working.\n      mock_ex_variant.return_value = self.variant\n      self.assertEqual(alts, tf_utils.example_alt_alleles(example))\n      mock_ex_variant.assert_called_once_with(example)\n\n  def assertIsNotAFeature(self, label, example):\n    self.assertNotIn(label, example.features.feature)\n\n  def testExampleSetLabel(self):\n    example = tf_utils.make_example(self.variant, self.alts, self.encoded_image,\n                                    self.default_shape, self.default_format)\n\n    self.assertIsNotAFeature(\'label\', example)\n    for label in [0, 1, 2]:\n      tf_utils.example_set_label(example, label)\n      self.assertEqual(label, tf_utils.example_label(example))\n\n  def testExampleImageShape(self):\n    example = tf_utils.make_example(self.variant, self.alts, self.encoded_image,\n                                    self.default_shape, self.default_format)\n    self.assertEqual(self.default_shape, tf_utils.example_image_shape(example))\n\n  def testFailedExampleImageShape(self):\n    # Create an empty example that doesn\'t have the required image/shape field.\n    example = example_pb2.Example()\n    with six.assertRaisesRegex(\n        self, ValueError, \'Invalid image/shape: we expect to find an \'\n        \'image/shape field with length 3.\'):\n      tf_utils.example_image_shape(example)\n\n  @parameterized.parameters(\n      (\'test_shape.gz\', \'test_shape.gz\'),\n      (\'test_shape-00000-of-00001.gz\', \'test_shape@1.gz\'),\n      (\'test_shape-00000-of-00001.gz\', \'test_shape-?????-of-00001.gz\'),\n      (\'test_shape-00000-of-00001.gz\', \'test_shape-*.gz\'), (\'output\', \'output\'),\n      (\'test_shape-00000-of-00001\', \'test_shape@1\'),\n      (\'test_shape-00000-of-00001\', \'test_shape-?????-of-00001\'),\n      (\'test_shape-00000-of-00001\', \'test_shape-*\'))\n  def testGetShapeFromExamplesPath(self, file_name_to_write,\n                                   tfrecord_path_to_match):\n    example = example_pb2.Example()\n    valid_shape = [1, 2, 3]\n    example.features.feature[\'image/shape\'].int64_list.value.extend(valid_shape)\n    output_file = test_utils.test_tmpfile(file_name_to_write)\n    tfrecord.write_tfrecords([example], output_file)\n    self.assertEqual(\n        valid_shape,\n        tf_utils.get_shape_from_examples_path(\n            test_utils.test_tmpfile(tfrecord_path_to_match)))\n    # clean up\n    gfile.Remove(output_file)\n\n  @parameterized.parameters(\n      (\'test_shape.gz\', \'test_shape.gz\'),\n      (\'test_shape-00000-of-00001.gz\', \'test_shape@1.gz\'),\n      (\'test_shape-00000-of-00001.gz\', \'test_shape-?????-of-00001.gz\'),\n      (\'test_shape-00000-of-00001.gz\', \'test_shape-*.gz\'), (\'output\', \'output\'),\n      (\'test_shape-00000-of-00001\', \'test_shape@1\'),\n      (\'test_shape-00000-of-00001\', \'test_shape-?????-of-00001\'),\n      (\'test_shape-00000-of-00001\', \'test_shape-*\'))\n  def testGetNoneShapeFromEmptyExamplesPath(self, file_name_to_write,\n                                            tfrecord_path_to_match):\n    output_file = test_utils.test_tmpfile(file_name_to_write)\n    tfrecord.write_tfrecords([], output_file)\n    self.assertIsNone(\n        tf_utils.get_shape_from_examples_path(\n            test_utils.test_tmpfile(tfrecord_path_to_match)))\n    # Clean up\n    gfile.Remove(output_file)\n\n  @parameterized.parameters(\n      (\'/this/path/does/not/exist\', \'/this/path/does/not\'),\n      (\'/bad/pathA/a,/bad/pathB/b\', \'/bad/pathA\'))\n  def testGetShapeFromExamplesPathInvalidPath(self, source_paths,\n                                              expected_partial_message):\n    # This calls tf.io.gfile.Glob, which will raise errors.OpError,\n    # at least on a Posix filesystem.  Other filesystems might\n    # not fail like that, and will return an empty list, which\n    # is turned into a different exception.\n    with six.assertRaisesRegex(self, Exception, expected_partial_message):\n      tf_utils.get_shape_from_examples_path(source_paths)\n\n  def testStringToIntTensor(self):\n    with tf.compat.v1.Session() as sess:\n      s = \'\\001\\002\\003\\004\\005\\006\\007\'\n      it = tf_utils.string_to_int_tensor(s)\n      x = sess.run(it)\n      a = x[0]\n      self.assertLen(s, a)\n      b = list(x[1:a + 1])\n      self.assertEqual(b, [1, 2, 3, 4, 5, 6, 7])\n\n  def testIntTensorToString(self):\n    with tf.compat.v1.Session() as sess:\n      s = six.b(\'\\001\\002\\003\\004\\005\\006\\007\')\n      it = tf_utils.string_to_int_tensor(s)\n      x = sess.run(it)\n      t = tf_utils.int_tensor_to_string(x)\n      self.assertEqual(t, s)\n\n  def testCompressionTypeOfFiles(self):\n    self.assertEqual(\n        \'GZIP\', tf_utils.compression_type_of_files([\'/tmp/foo.tfrecord.gz\']))\n    self.assertEqual(None,\n                     tf_utils.compression_type_of_files([\'/tmp/foo.tfrecord\']))\n\n\nif __name__ == \'__main__\':\n  tf.compat.v1.disable_eager_execution()\n  absltest.main()\n'"
deepvariant/variant_caller.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""A VariantCaller producing DeepVariantCall and gVCF records.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport collections\nimport itertools\nimport math\nimport operator\n\n\nimport numpy as np\n\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import genomics_math\nfrom third_party.nucleus.util import variantcall_utils\nfrom third_party.nucleus.util import vcf_constants\nfrom deepvariant.python import variant_calling\n\n# Reference bases with genotype calls must be one of these four values.\nCANONICAL_DNA_BASES = frozenset(\'ACGT\')\n\n# Possible DNA base codes seen in a reference genome.\nEXTENDED_IUPAC_CODES = frozenset(\'ACGTRYSWKMBDHVN\')\n\n# Data collection class used in creation of gVCF records.\n_GVCF = collections.namedtuple(\'_GVCF\', [\n    \'summary_counts\', \'quantized_gq\', \'raw_gq\', \'likelihoods\', \'read_depth\',\n    \'has_valid_gl\'\n])\n\nLOG_10 = math.log(10.0)\n\n\ndef _rescale_read_counts_if_necessary(n_ref_reads, n_total_reads,\n                                      max_allowed_reads):\n  """"""Ensures that n_total_reads <= max_allowed_reads, rescaling if necessary.\n\n  This function ensures that n_total_reads <= max_allowed_reads. If\n  n_total_reads is <= max_allowed_reads, n_ref_reads and n_total_reads are just\n  returned. However, if n_total_reads > max_allowed_reads, then n_ref_reads and\n  n_total_reads are rescaled to new values n_ref_reads\' and n_total_reads\' so\n  that n_total_reads\' == max_allowed_reads and n_ref_reads\' / n_total_reads\' ~\n  n_ref_reads / n_total_reads.\n\n  Args:\n    n_ref_reads: int. Number of reference supporting reads.\n    n_total_reads: int. Total number of reads.\n    max_allowed_reads: int. The maximum value allowed for n_total after\n      rescaling, if necessary.\n\n  Returns:\n    New values for n_ref_reads and n_total_reads.\n  """"""\n  if n_total_reads > max_allowed_reads:\n    ratio = n_ref_reads / (1.0 * n_total_reads)\n    n_ref_reads = int(math.ceil(ratio * max_allowed_reads))\n    n_total_reads = max_allowed_reads\n  return n_ref_reads, n_total_reads\n\n\ndef _quantize_gq(raw_gq, binsize):\n  """"""Returns a quantized value of GQ in units of binsize.\n\n  Args:\n    raw_gq: int. The raw GQ value to quantize.\n    binsize: positive int. The size of bins to quantize within.\n\n  Returns:\n    A quantized GQ integer.\n  """"""\n  if raw_gq < 1:\n    return 0\n  else:\n    bin_number = (raw_gq - 1) // binsize\n    return bin_number * binsize + 1\n\n\nclass VariantCaller(object):\n  """"""BaseClass for variant callers.""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, options, use_cache_table, max_cache_coverage):\n    self.options = options\n    self.cpp_variant_caller = variant_calling.VariantCaller(self.options)\n\n    self.max_cache_coverage = max_cache_coverage\n    # pylint: disable=g-complex-comprehension\n    if use_cache_table:\n      self.table = [[\n          self._calc_reference_confidence(n_ref, n_total)\n          for n_ref in range(n_total + 1)\n      ]\n                    for n_total in range(self.max_cache_coverage + 1)]\n    else:\n      self.table = None\n    # pylint: enable=g-complex-comprehension\n\n  def reference_confidence(self, n_ref, n_total):\n    """"""Computes the confidence that a site in the genome has no variation.\n\n    Computes this confidence using only the counts of the number of reads\n    supporting the reference allele and the total number of reads at this site.\n\n    See: https:www.broadinstitute.org/gatk/guide/article?id=4017 for\n    background.\n\n    Computes the reference confidence for site allele_count.\n\n    Examines the number of reference supporting and alternate supporting reads\n    in allele_count and estimates the genotype likelihoods and confidence that\n    this site is homozygous reference. These values are written into the first\n    VariantCall record of variant, into the repeated field genotype_likelihood\n    and the map field GQ.\n\n    The genotype likelihoods are computed against any possible alternative\n    allele, the so-called <*> allele, which boils down to a model that looks\n    like:\n\n     log10_p_ref = (1 - p_error)^(ref_n) (p_error)^(non_ref_n)\n     log10_p_het = (0.5)^(total_n)\n     log10_p_hom_alt = (p_e)^(ref_n) (1 - p_error)^(non_ref_n)\n\n    ref_n is the number of reference supporting reads and non_ref_n is the sum\n    of any reads supporting any alternate alleles. Non-informative reads are\n    excluded from the calculation.\n\n    and written in as the normalized log10 values so that:\n\n     sum(10^genotype_likelihoods) = 1\n\n    The GQ, according to the VCF specification, is the conditional genotype\n    quality, encoded as a phred quality -10 * log10 p(genotype call is wrong,\n    conditioned on the site\'s being variant, as an integer. See:\n    https:samtools.github.io/hts-specs/VCFv4.3.pdf\n    We are calculating the GQ not for the best genotype, but the GQ of the 0/0\n    genotype, regardless of the likelihoods.\n    1 = pRR + pRA + pAA; [R is reference, A=<*> is any alternative alternative]\n    GQ of 0/0 = -10 * log10(pRA + pAA) [prob that any other differen genotype]\n              = -10 * log10(1 - pRR) [substitution from the previous equation]\n    Here we don\'t have pRR directly, but rather log10(pRR).\n\n    Args:\n      n_ref: int >= 0 and <= n_total: The number of reads supporting the\n        reference allele.\n      n_total: int >= 0 and >= n_ref: The number of reads supporting any allele\n        at this site.\n\n    Returns:\n      A tuple of two values. The first is an integer value for the GQ (genotype\n      quality) and the second is an array-like of the log10 probabilities for\n      each of the three genotype configurations.\n    """"""\n    if self.table is None:\n      return self._calc_reference_confidence(n_ref, n_total)\n    else:\n      ref_index, total_index = _rescale_read_counts_if_necessary(\n          n_ref, n_total, self.max_cache_coverage)\n      return self.table[total_index][ref_index]\n\n  def _calc_reference_confidence(self, n_ref, n_total):\n    """"""Performs the calculation described in reference_confidence().""""""\n    if n_ref < 0:\n      raise ValueError(\'n_ref={} must be >= 0\'.format(n_ref))\n    if n_total < n_ref:\n      raise ValueError(\'n_total={} must be >= n_ref={}\'.format(n_total, n_ref))\n    if self.options.ploidy != 2:\n      raise ValueError(\'ploidy={} but we only support ploidy=2\'.format(\n          self.options.ploidy))\n\n    if n_total == 0:\n      # No coverage case - all likelihoods are log10 of 1/3, 1/3, 1/3.\n      log10_probs = genomics_math.normalize_log10_probs([-1.0, -1.0, -1.0])\n    else:\n      n_alts = n_total - n_ref\n      logp = math.log(self.options.p_error) / LOG_10\n      log1p = math.log1p(-self.options.p_error) / LOG_10\n      log10_p_ref = n_ref * log1p + n_alts * logp\n      log10_p_het = -n_total * math.log(self.options.ploidy) / LOG_10\n      log10_p_hom_alt = n_ref * logp + n_alts * log1p\n      log10_probs = genomics_math.normalize_log10_probs(\n          [log10_p_ref, log10_p_het, log10_p_hom_alt])\n\n    gq = genomics_math.log10_ptrue_to_phred(log10_probs[0], self.options.max_gq)\n    gq = int(min(np.floor(gq), self.options.max_gq))\n    return gq, log10_probs\n\n  def make_gvcfs(self, allele_count_summaries):\n    """"""Primary interface function for computing gVCF confidence at a site.\n\n    Looks at the counts in the provided list of AlleleCountSummary protos and\n    returns properly-formatted Variant protos containing gVCF reference\n    blocks for all sites in allele_count_summaries. The returned Variant has\n    reference_name, start, end are set and contains a single VariantCall in the\n    calls field with call_set_name of options.sample_name, genotypes set to 0/0\n    (diploid reference), a GQ value bound in the info field appropriate to the\n    data in allele_count, and a MIN_DP value which is the minimum read coverage\n    seen in the block.\n\n    The provided allele count must have either a canonical DNA sequence base (\n    A, C, G, T) or be ""N"".\n\n    Args:\n      allele_count_summaries: iterable of AlleleCountSummary protos in\n        coordinate-sorted order. Each proto is used to get the read counts for\n        reference and alternate alleles, the reference position, and reference\n        base.\n\n    Yields:\n      third_party.nucleus.protos.Variant proto in\n      coordinate-sorted order containing gVCF records.\n    """"""\n\n    def with_gq_and_likelihoods(summary_counts):\n      """"""Returns summary_counts along with GQ and genotype likelihoods.\n\n      If the reference base is not in CANONICAL_DNA_BASES, both GQ and genotype\n      likelihoods are set to None.\n\n      Args:\n        summary_counts: A single AlleleCountSummary.\n\n      Returns:\n        A tuple of summary_counts, quantized GQ, raw GQ, and genotype\n        likelihoods for summary_counts where raw GQ and genotype_likelihood are\n        calculated by self.reference_confidence.\n\n      Raises:\n        ValueError: The reference base is not a valid DNA or IUPAC base.\n      """"""\n      if summary_counts.ref_base not in CANONICAL_DNA_BASES:\n        if summary_counts.ref_base in EXTENDED_IUPAC_CODES:\n          # Skip calculating gq and likelihoods, since this is an ambiguous\n          # reference base.\n          quantized_gq, raw_gq, likelihoods = None, None, None\n          has_valid_gl = True\n          n_total = summary_counts.total_read_count\n        else:\n          raise ValueError(\'Invalid reference base={} found during gvcf \'\n                           \'calculation\'.format(summary_counts.ref_base))\n      else:\n        n_ref = summary_counts.ref_supporting_read_count\n        n_total = summary_counts.total_read_count\n        raw_gq, likelihoods = self.reference_confidence(n_ref, n_total)\n        quantized_gq = _quantize_gq(raw_gq, self.options.gq_resolution)\n        has_valid_gl = (np.amax(likelihoods) == likelihoods[0])\n      return _GVCF(\n          summary_counts=summary_counts,\n          quantized_gq=quantized_gq,\n          raw_gq=raw_gq,\n          likelihoods=likelihoods,\n          read_depth=n_total,\n          has_valid_gl=has_valid_gl)\n\n    # Combines contiguous, compatible single-bp blocks into larger gVCF blocks,\n    # respecting non-reference variants interspersed among them. Yields each\n    # combined gVCF Variant proto, in order. Compatible right now means that the\n    # blocks to be merged have the same non-None GQ value.\n    for key, combinable in itertools.groupby(\n        (with_gq_and_likelihoods(sc) for sc in allele_count_summaries),\n        key=operator.attrgetter(\'quantized_gq\', \'has_valid_gl\')):\n      quantized_gq_val, gl_is_valid = key\n      if quantized_gq_val is None:\n        # A None key indicates that a non-DNA reference base was encountered, so\n        # skip this group.\n        continue\n\n      if gl_is_valid:\n        combinable = list(combinable)\n        min_gq = min(elt.raw_gq for elt in combinable)\n        min_dp = min(elt.read_depth for elt in combinable)\n        first_record, last_record = combinable[0], combinable[-1]\n        call = variants_pb2.VariantCall(\n            call_set_name=self.options.sample_name,\n            genotype=[0, 0],\n            genotype_likelihood=first_record.likelihoods)\n        variantcall_utils.set_gq(call, min_gq)\n        variantcall_utils.set_min_dp(call, min_dp)\n        yield variants_pb2.Variant(\n            reference_name=first_record.summary_counts.reference_name,\n            reference_bases=first_record.summary_counts.ref_base,\n            alternate_bases=[vcf_constants.GVCF_ALT_ALLELE],\n            start=first_record.summary_counts.position,\n            end=last_record.summary_counts.position + 1,\n            calls=[call])\n      else:\n        # After evaluating the effect of including sites with contradictory GL\n        # (where the value for hom_ref is not maximal), we concluded that\n        # un-calling these sites (by setting its genotype ""./."") is better\n        # for cohort merging.\n        # See internal for detail.\n        for elt in combinable:\n          uncalled_gt = [-1, -1]\n          call = variants_pb2.VariantCall(\n              call_set_name=self.options.sample_name,\n              genotype=uncalled_gt,\n              genotype_likelihood=elt.likelihoods)\n          variantcall_utils.set_gq(call, elt.raw_gq)\n          variantcall_utils.set_min_dp(call, elt.read_depth)\n          yield variants_pb2.Variant(\n              reference_name=elt.summary_counts.reference_name,\n              reference_bases=elt.summary_counts.ref_base,\n              alternate_bases=[vcf_constants.GVCF_ALT_ALLELE],\n              start=elt.summary_counts.position,\n              end=elt.summary_counts.position + 1,\n              calls=[call])\n\n  def calls_and_gvcfs(self, allele_counter, include_gvcfs):\n    """"""Gets variant calls and gvcf records for all sites in allele_counter.\n\n    Args:\n      allele_counter: AlleleCounter object holding the allele counts we will use\n        to find candidate variants and create gvcf records.\n      include_gvcfs: boolean. If True, we will compute gVCF records for all of\n        the AlleleCounts in AlleleCounter.\n\n    Returns:\n      Two values. The first is a list of DeepVariantCall protos containing our\n      candidate variants. The second is a list of gVCF blocks in Variant proto\n      format, if include_gvcfs is True. If False, an empty list is returned.\n    """"""\n    candidates = self.get_candidates(allele_counter)\n    gvcfs = []\n    if include_gvcfs:\n      gvcfs = list(self.make_gvcfs(allele_counter.summary_counts()))\n    return candidates, gvcfs\n\n  @abc.abstractmethod\n  def get_candidates(self, allele_counter):\n    raise NotImplementedError\n'"
deepvariant/variant_caller_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .variant_caller.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\nimport numpy as np\nimport numpy.testing as npt\nimport six\n\nfrom third_party.nucleus.util import variant_utils\nfrom third_party.nucleus.util import variantcall_utils\nfrom deepvariant import testdata\nfrom deepvariant import variant_caller\nfrom deepvariant.protos import deepvariant_pb2\n\n\ndef setUpModule():\n  testdata.init()\n\n\ndef _reference_model_options(p_error, max_gq, gq_resolution=1):\n  return deepvariant_pb2.VariantCallerOptions(\n      sample_name=\'UNKNOWN\',\n      p_error=p_error,\n      max_gq=max_gq,\n      gq_resolution=gq_resolution,\n      ploidy=2)\n\n\nclass DummyVariantCaller(variant_caller.VariantCaller):\n  """"""A dummy VariantCaller.\n\n  This class provides a get_candidates implementation and so allows\n  the base class to be instantiated and its methods tested.\n  """"""\n\n  def __init__(self,\n               p_error,\n               max_gq,\n               gq_resolution=1,\n               use_cache_table=False,\n               max_cache_coverage=100):\n    super(DummyVariantCaller, self).__init__(\n        options=_reference_model_options(p_error, max_gq, gq_resolution),\n        use_cache_table=use_cache_table,\n        max_cache_coverage=max_cache_coverage)\n\n  def get_candidates(self, allele_counter):\n    return None\n\n\nclass VariantCallerTests(parameterized.TestCase):\n\n  def fake_allele_counter(self, start_pos, counts):\n    allele_counter = mock.Mock()\n    # pylint: disable=g-complex-comprehension\n    allele_counter.summary_counts.return_value = [\n        deepvariant_pb2.AlleleCountSummary(\n            ref_supporting_read_count=n_ref,\n            total_read_count=n_ref + n_alt,\n            ref_base=ref,\n            reference_name=\'chr1\',\n            position=start_pos + i)\n        for i, (n_alt, n_ref, ref) in enumerate(counts)\n    ]\n    # pylint: enable=g-complex-comprehension\n    return allele_counter\n\n  # R code to produce the testdata expectation table.\n  # expected <- function(n_ref, n_alt, perr, max_gq = 100) {\n  #   p_ref <- dbinom(n_alt, n_ref, perr)\n  #   p_het <- dbinom(n_alt, n_ref, 0.5)\n  #   p_alt <- dbinom(n_ref - n_alt, n_ref, perr)\n  #   raw <- c(p_ref, p_het, p_alt)\n  #   norm <- raw / sum(raw)\n  #   gq = min(floor(-10 * log10(1 - norm[1])), max_gq)\n  #   likelihoods = paste(sprintf(""%.6f"", log10(norm)), collapse="", "")\n  #   likelihoods = paste(""["", likelihoods, ""]"", sep="""")\n  #   result = paste(n_ref, n_alt, perr, 100, 1, likelihoods, gq, sep="", "")\n  #   cat(paste(""["", result, ""],\\n"", sep=""""))\n  # }\n  #\n  # for (n in c(10, 20)) {\n  #  for (k in seq(0, n)) {\n  #     expected(n, k, 0.01)\n  #   }\n  # }\n  #\n  # for (perr in c(0.1, 0.01, 0.001, 0.0001)) {\n  #   expected(10, 0, perr)\n  #   expected(10, 1, perr)\n  # }\n  #\n  # for (n_ref in c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 1000, 10000)) {\n  #   expected(n_ref, 0, 0.01)\n  # }\n  @parameterized.parameters(\n      # No coverage case.\n      [0, 0, 0.01, 100, [-0.477121, -0.477121, -0.477121], 1],\n      # Test systematically values of n and k.\n      [10, 0, 0.01, 100, [-0.000469, -2.967121, -19.956821], 29],\n      [10, 1, 0.01, 100, [-0.044109, -1.015126, -16.009190], 10],\n      [10, 2, 0.01, 100, [-1.063830, -0.039211, -13.037641], 0],\n      [10, 3, 0.01, 100, [-3.020668, -0.000414, -11.003209], 0],\n      [10, 4, 0.01, 100, [-5.015893, -0.000004, -9.007163], 0],\n      [10, 5, 0.01, 100, [-7.011524, -0.000000, -7.011524], 0],\n      [10, 6, 0.01, 100, [-9.007163, -0.000004, -5.015893], 0],\n      [10, 7, 0.01, 100, [-11.003209, -0.000414, -3.020668], 0],\n      [10, 8, 0.01, 100, [-13.037641, -0.039211, -1.063830], 0],\n      [10, 9, 0.01, 100, [-16.009190, -1.015126, -0.044109], 0],\n      [10, 10, 0.01, 100, [-19.956821, -2.967121, -0.000469], 0],\n      [20, 0, 0.01, 100, [-0.000001, -5.933304, -39.912704], 59],\n      [20, 1, 0.01, 100, [-0.000050, -3.937719, -35.921484], 39],\n      [20, 2, 0.01, 100, [-0.004935, -1.946968, -31.935098], 19],\n      [20, 3, 0.01, 100, [-0.328657, -0.275056, -28.267550], 2],\n      [20, 4, 0.01, 100, [-2.053097, -0.003860, -26.000720], 0],\n      [20, 5, 0.01, 100, [-4.044911, -0.000039, -24.001263], 0],\n      [20, 6, 0.01, 100, [-6.040508, -0.000000, -22.005589], 0],\n      [20, 7, 0.01, 100, [-8.036143, -0.000000, -20.009954], 0],\n      [20, 8, 0.01, 100, [-10.031778, -0.000000, -18.014319], 0],\n      [20, 9, 0.01, 100, [-12.027413, -0.000000, -16.018683], 0],\n      [20, 10, 0.01, 100, [-14.023048, -0.000000, -14.023048], 0],\n      [20, 11, 0.01, 100, [-16.018683, -0.000000, -12.027413], 0],\n      [20, 12, 0.01, 100, [-18.014319, -0.000000, -10.031778], 0],\n      [20, 13, 0.01, 100, [-20.009954, -0.000000, -8.036143], 0],\n      [20, 14, 0.01, 100, [-22.005589, -0.000000, -6.040508], 0],\n      [20, 15, 0.01, 100, [-24.001263, -0.000039, -4.044911], 0],\n      [20, 16, 0.01, 100, [-26.000720, -0.003860, -2.053097], 0],\n      [20, 17, 0.01, 100, [-28.267550, -0.275056, -0.328657], 0],\n      [20, 18, 0.01, 100, [-31.935098, -1.946968, -0.004935], 0],\n      [20, 19, 0.01, 100, [-35.921484, -3.937719, -0.000050], 0],\n      [20, 20, 0.01, 100, [-39.912704, -5.933304, -0.000001], 0],\n      # Testing different values of p_error.\n      [10, 0, 0.1, 100, [-0.001215, -2.553940, -9.543640], 25],\n      [10, 1, 0.1, 100, [-0.010811, -1.609294, -7.644752], 16],\n      [10, 0, 0.01, 100, [-0.000469, -2.967121, -19.956821], 29],\n      [10, 1, 0.01, 100, [-0.044109, -1.015126, -16.009190], 10],\n      [10, 0, 0.001, 100, [-0.000428, -3.006383, -29.996083], 30],\n      [10, 1, 0.001, 100, [-0.297847, -0.304236, -24.294371], 3],\n      [10, 0, 1e-04, 100, [-0.000424, -3.010290, -39.999990], 30],\n      [10, 1, 1e-04, 100, [-1.032394, -0.042303, -33.032046], 0],\n      # Test scaling of calculation with more coverage, hitting max_gq.\n      [10, 0, 0.01, 100, [-0.000469, -2.967121, -19.956821], 29],\n      [20, 0, 0.01, 100, [-0.000001, -5.933304, -39.912704], 59],\n      [30, 0, 0.01, 100, [-0.000000, -8.899956, -59.869056], 88],\n      [40, 0, 0.01, 100, [-0.000000, -11.866608, -79.825408], 100],\n      [50, 0, 0.01, 100, [-0.000000, -14.833260, -99.781760], 100],\n      [60, 0, 0.01, 100, [0.000000, -17.799911, -119.738112], 100],\n      [70, 0, 0.01, 100, [0.000000, -20.766563, -139.694464], 100],\n      [80, 0, 0.01, 100, [0.000000, -23.733215, -159.650816], 100],\n      [90, 0, 0.01, 100, [0.000000, -26.699867, -179.607168], 100],\n      [100, 0, 0.01, 100, [0.000000, -29.666519, -199.563519], 100],\n  )\n  def test_ref_calc(self, total_n, alt_n, p_error, max_gq, expected_likelihoods,\n                    expected_gq):\n    caller = DummyVariantCaller(p_error, max_gq)\n    gq, likelihoods = caller.reference_confidence(total_n - alt_n, total_n)\n    npt.assert_allclose(expected_likelihoods, likelihoods, atol=1e-6)\n    self.assertEqual(expected_gq, gq)\n\n  @parameterized.parameters(\n      # Values below max_allowed_reads are returned without modification.\n      [0, 10, 100, (0, 10)],\n      [5, 10, 100, (5, 10)],\n      [10, 10, 100, (10, 10)],\n      [10, 100, 100, (10, 100)],\n      [100, 100, 100, (100, 100)],\n\n      # Checks that the rescaling works when n_total_reads > max_allowed.\n      [0, 200, 100, (0, 100)],\n      [0, 200, 100, (0, 100)],\n      [0, 1000, 100, (0, 100)],\n      [0, 10000, 100, (0, 100)],\n      [1, 200, 100, (1, 100)],\n      [1, 1000, 100, (1, 100)],\n      [1, 10000, 100, (1, 100)],\n      [1, 100000, 100, (1, 100)],\n      [2, 200, 100, (1, 100)],\n      [3, 200, 100, (2, 100)],\n      [4, 200, 100, (2, 100)],\n      [10, 200, 100, (5, 100)],\n      [50, 200, 100, (25, 100)],\n      [100, 200, 100, (50, 100)],\n      [200, 200, 100, (100, 100)],\n      # I saw a bug at runtime, and the testcase makes sure we scale values of\n      # n_ref_reads close to n_total_reads appropriately.\n      [99, 100, 100, (99, 100)],\n  )\n  def test_rescale_read_counts(self, n_ref, n_total, max_allowed_reads,\n                               expected):\n    actual = variant_caller._rescale_read_counts_if_necessary(\n        n_ref, n_total, max_allowed_reads)\n    self.assertEqual(actual, expected)\n\n  # pylint: disable=g-complex-comprehension\n  @parameterized.parameters((n_ref, n_alt_fraction)\n                            for n_ref in [1000, 10000, 100000, 1000000]\n                            for n_alt_fraction in [0.0, 0.01, 0.02])\n  # pylint: enable=g-complex-comprehension\n  def test_handles_large_reference_counts(self, n_ref, n_alt_fraction):\n    """"""Tests that we don\'t blow up when the coverage gets really high.""""""\n    caller = DummyVariantCaller(0.01, 100)\n    n_alt = int(n_alt_fraction * n_ref)\n    gq, likelihoods = caller._calc_reference_confidence(n_ref, n_ref + n_alt)\n    self.assertTrue(\n        np.isfinite(likelihoods).all(),\n        \'Non-finite likelihoods {}\'.format(likelihoods))\n    self.assertEqual(100, gq)\n\n  @parameterized.parameters(*variant_caller.CANONICAL_DNA_BASES)\n  def test_gvcf_basic(self, ref):\n    options = _reference_model_options(0.01, 100)\n    caller = DummyVariantCaller(0.01, 100)\n    allele_counter = self.fake_allele_counter(100, [(0, 0, ref)])\n    gvcfs = list(caller.make_gvcfs(allele_counter.summary_counts()))\n    self.assertLen(gvcfs, 1)\n    self.assertGVCF(\n        gvcfs[0],\n        ref=ref,\n        gq=1.0,\n        start=100,\n        end=101,\n        min_dp=0,\n        chrom=\'chr1\',\n        gls=[-0.47712125472] * 3,\n        sample_name=options.sample_name)\n\n  @parameterized.parameters(\'N\', \'R\', \'W\', \'B\')\n  def test_gvcf_basic_skips_iupac_ref_base(self, ref):\n    caller = DummyVariantCaller(0.01, 100)\n    allele_counter = self.fake_allele_counter(100, [(0, 0, ref)])\n    self.assertEmpty(list(caller.make_gvcfs(allele_counter.summary_counts())))\n\n  @parameterized.parameters(\'X\', \'>\', \'!\')\n  def test_gvcf_basic_raises_with_bad_ref_base(self, ref):\n    caller = DummyVariantCaller(0.01, 100)\n    allele_counter = self.fake_allele_counter(100, [(0, 0, ref)])\n    with six.assertRaisesRegex(self, ValueError,\n                               \'Invalid reference base={}\'.format(ref)):\n      list(caller.make_gvcfs(allele_counter.summary_counts()))\n\n  def assertGVCF(self,\n                 gvcf,\n                 ref,\n                 gq,\n                 start,\n                 end,\n                 min_dp,\n                 chrom=\'chr1\',\n                 gls=None,\n                 sample_name=None,\n                 gts=None):\n    if chrom:\n      self.assertEqual(gvcf.reference_name, chrom)\n    call = variant_utils.only_call(gvcf)\n    self.assertNotEmpty(gvcf.reference_name)\n    self.assertEqual(gvcf.reference_bases, ref)\n    self.assertEqual(gvcf.alternate_bases, [\'<*>\'])\n    self.assertEqual(gvcf.start, start)\n    self.assertEqual(gvcf.end, end if end else start + 1)\n    self.assertEqual(variantcall_utils.get_gq(call), gq)\n    self.assertNotEmpty(call.genotype_likelihood)\n    self.assertIn(\'MIN_DP\', call.info)\n    self.assertLen(call.info[\'MIN_DP\'].values, 1)\n    self.assertEqual(variantcall_utils.get_min_dp(call), min_dp)\n    if gls is not None:\n      npt.assert_allclose(list(gvcf.calls[0].genotype_likelihood), gls)\n    if sample_name:\n      self.assertEqual(gvcf.calls[0].call_set_name, sample_name)\n    if gts is not None:\n      self.assertEqual(list(gvcf.calls[0].genotype), gts)\n\n  @parameterized.parameters(\n      # Check some basics.\n      ([(0, 0, \'A\')], [dict(start=1, end=2, ref=\'A\', gq=1, min_dp=0)]),\n      # Two equal records are merged, and the reference base is the first one.\n      ([(0, 0, \'A\'),\n        (0, 0, \'C\')], [dict(start=1, end=3, ref=\'A\', gq=1, min_dp=0)]),\n      ([(0, 0, \'C\'),\n        (0, 0, \'A\')], [dict(start=1, end=3, ref=\'C\', gq=1, min_dp=0)]),\n      # Three equal records are merged into a single block.\n      ([(0, 0, \'A\'), (0, 0, \'C\'),\n        (0, 0, \'T\')], [dict(start=1, end=4, ref=\'A\', gq=1, min_dp=0)]),\n      # We don\'t merge together different GQ value blocks:\n      ([(0, 0, \'A\'), (0, 100, \'C\')], [\n          dict(start=1, end=2, ref=\'A\', gq=1, min_dp=0),\n          dict(start=2, end=3, ref=\'C\', gq=100, min_dp=100),\n      ]),\n      ([(0, 100, \'A\'), (0, 0, \'C\')], [\n          dict(start=1, end=2, ref=\'A\', gq=100, min_dp=100),\n          dict(start=2, end=3, ref=\'C\', gq=1, min_dp=0),\n      ]),\n      ([(0, 0, \'A\'), (0, 20, \'C\'), (0, 100, \'T\')], [\n          dict(start=1, end=2, ref=\'A\', gq=1, min_dp=0),\n          dict(start=2, end=3, ref=\'C\', gq=59, min_dp=20),\n          dict(start=3, end=4, ref=\'T\', gq=100, min_dp=100),\n      ]),\n  )\n  def test_make_gvcfs(self, counts, expecteds):\n    allele_counts = self.fake_allele_counter(1, counts).summary_counts()\n    caller = DummyVariantCaller(0.01, 100)\n    gvcfs = list(caller.make_gvcfs(allele_counts))\n\n    self.assertLen(gvcfs, len(expecteds))\n    for actual, expected in zip(gvcfs, expecteds):\n      self.assertGVCF(actual, **expected)\n\n  @parameterized.parameters(\n      dict(\n          gq_resolution=1,\n          expecteds=[\n              dict(start=1, end=2, ref=\'A\', gq=53, min_dp=18),\n              dict(start=2, end=3, ref=\'C\', gq=56, min_dp=19),\n              dict(start=3, end=4, ref=\'A\', gq=0, min_dp=35),\n              dict(start=4, end=5, ref=\'T\', gq=0, min_dp=20),\n              dict(start=5, end=6, ref=\'A\', gq=0, min_dp=16),\n              dict(start=6, end=7, ref=\'A\', gq=72, min_dp=31),\n              dict(start=7, end=8, ref=\'C\', gq=83, min_dp=35),\n              dict(start=8, end=9, ref=\'T\', gq=59, min_dp=20),\n              dict(start=9, end=10, ref=\'G\', gq=56, min_dp=19),\n          ]),\n      # Binning by 3 does not cause any records to be merged.\n      dict(\n          gq_resolution=3,\n          expecteds=[\n              dict(start=1, end=2, ref=\'A\', gq=53, min_dp=18),\n              dict(start=2, end=3, ref=\'C\', gq=56, min_dp=19),\n              dict(start=3, end=4, ref=\'A\', gq=0, min_dp=35),\n              dict(start=4, end=5, ref=\'T\', gq=0, min_dp=20),\n              dict(start=5, end=6, ref=\'A\', gq=0, min_dp=16),\n              dict(start=6, end=7, ref=\'A\', gq=72, min_dp=31),\n              dict(start=7, end=8, ref=\'C\', gq=83, min_dp=35),\n              dict(start=8, end=9, ref=\'T\', gq=59, min_dp=20),\n              dict(start=9, end=10, ref=\'G\', gq=56, min_dp=19),\n          ]),\n      # Binning by 4 causes the first merge, of the first two records.\n      dict(\n          gq_resolution=4,\n          expecteds=[\n              dict(start=1, end=3, ref=\'A\', gq=53, min_dp=18),\n              dict(start=3, end=4, ref=\'A\', gq=0, min_dp=35),\n              dict(start=4, end=5, ref=\'T\', gq=0, min_dp=20),\n              dict(start=5, end=6, ref=\'A\', gq=0, min_dp=16),\n              dict(start=6, end=7, ref=\'A\', gq=72, min_dp=31),\n              dict(start=7, end=8, ref=\'C\', gq=83, min_dp=35),\n              dict(start=8, end=9, ref=\'T\', gq=59, min_dp=20),\n              dict(start=9, end=10, ref=\'G\', gq=56, min_dp=19),\n          ]),\n      dict(\n          gq_resolution=10,\n          expecteds=[\n              dict(start=1, end=3, ref=\'A\', gq=53, min_dp=18),\n              dict(start=3, end=4, ref=\'A\', gq=0, min_dp=35),\n              dict(start=4, end=5, ref=\'T\', gq=0, min_dp=20),\n              dict(start=5, end=6, ref=\'A\', gq=0, min_dp=16),\n              dict(start=6, end=7, ref=\'A\', gq=72, min_dp=31),\n              dict(start=7, end=8, ref=\'C\', gq=83, min_dp=35),\n              dict(start=8, end=10, ref=\'T\', gq=56, min_dp=19),\n          ]),\n      dict(\n          gq_resolution=45,\n          expecteds=[\n              dict(start=1, end=3, ref=\'A\', gq=53, min_dp=18),\n              dict(start=3, end=4, ref=\'A\', gq=0, min_dp=35),\n              dict(start=4, end=5, ref=\'T\', gq=0, min_dp=20),\n              dict(start=5, end=6, ref=\'A\', gq=0, min_dp=16),\n              dict(start=6, end=10, ref=\'A\', gq=56, min_dp=19),\n          ]),\n  )\n  def test_quantize_gvcfs(self, gq_resolution, expecteds):\n    # Each count tuple is n_alt, n_ref, ref_base.\n    # The third, fourth, and the fifth ones should never be merged, since\n    # either het or hom_alt has bigger GL than hom_ref.\n    counts = [(0, 18, \'A\'), (0, 19, \'C\'), (35, 0, \'A\'), (10, 10, \'T\'),\n              (4, 12, \'A\'), (1, 30, \'A\'), (1, 34, \'C\'), (0, 20, \'T\'),\n              (0, 19, \'G\')]\n    allele_counts = self.fake_allele_counter(1, counts).summary_counts()\n    caller = DummyVariantCaller(0.01, 100, gq_resolution)\n    gvcfs = list(caller.make_gvcfs(allele_counts))\n    self.assertLen(gvcfs, len(expecteds))\n    for actual, expected in zip(gvcfs, expecteds):\n      self.assertGVCF(actual, **expected)\n\n  @parameterized.parameters(True, False)\n  def test_gvcfs_counts(self, include_gvcfs):\n    # Only tests the \'gvcfs\' creation part of calls_and_gvcfs. The `calls`\n    # portion of this method needs to be tested in subclasses, which have\n    # implemented the get_candidates method.\n    counts = [(0, 0, \'A\'), (10, 10, \'G\'), (0, 0, \'G\'), (0, 0, \'G\'),\n              (10, 10, \'T\')]\n    caller = DummyVariantCaller(0.01, 100)\n    allele_counter = self.fake_allele_counter(10, counts)\n    _, gvcfs = caller.calls_and_gvcfs(allele_counter, include_gvcfs)\n    # We expect our gvcfs to occur at the 10 position and that 12 and 13 have\n    # been merged into a 2 bp block, if enabled. Otherwise should be empty.\n    if include_gvcfs:\n      self.assertLen(gvcfs, 4)\n      # Expected diploid genotype likelihoods when there\'s no coverage. The\n      # chance of having each genotype is 1/3, in log10 space.\n      flat_gls = np.log10([1.0 / 3] * 3)\n      self.assertGVCF(\n          gvcfs[0], ref=\'A\', start=10, end=11, gq=1, min_dp=0, gls=flat_gls)\n      self.assertGVCF(\n          gvcfs[1],\n          ref=\'G\',\n          start=11,\n          end=12,\n          gq=0,\n          min_dp=20,\n          gls=np.array([-14.0230482368, -7.993606e-15, -14.0230482368]),\n          # The genotype should NOT be called here (""./."") as the likelihood\n          # for het is greater than hom_ref.\n          gts=[-1, -1])\n      self.assertGVCF(\n          gvcfs[2], ref=\'G\', start=12, end=14, gq=1, min_dp=0, gls=flat_gls)\n    else:\n      self.assertEmpty(gvcfs)\n\n\n_CACHE_COVERAGE = 20  # Outside class so we can refer to it in @Parameters.\n\n\nclass VariantCallerCacheTests(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(VariantCallerCacheTests, cls).setUpClass()\n    cls.raw_caller = DummyVariantCaller(0.1, 50, use_cache_table=False)\n    cls.cache_caller = DummyVariantCaller(\n        0.1, 50, use_cache_table=True, max_cache_coverage=_CACHE_COVERAGE)\n\n  # pylint: disable=g-complex-comprehension\n  @parameterized.parameters((n_alt, n_total)\n                            for n_total in range(_CACHE_COVERAGE + 1)\n                            for n_alt in range(n_total + 1))\n  # pylint: enable=g-complex-comprehension\n  def test_caching(self, n_alt, n_total):\n    # Note that we only expect the gq and gls to be close if we are not\n    # rescaling the counts, so we are only looping over values that should be\n    # cached. In practice the cache is set to values sufficiently large that\n    # these differences don\'t matter, but for this test we are limiting the\n    # cache size to a small value in _CACHE_COVERAGE so we can test that the\n    # cache lookups are correct.\n    raw_gq, raw_gls = self.raw_caller.reference_confidence(n_alt, n_total)\n    cache_gq, cache_gls = self.cache_caller.reference_confidence(n_alt, n_total)\n    self.assertEqual(raw_gq, cache_gq)\n    npt.assert_allclose(raw_gls, cache_gls)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/vcf_candidate_importer.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""A VcfCandidateImporter producing DeepVariantCall and gVCF records.\n\nThis module provides a way to call variants with a proposed VCF that contains\ncandidates to consider.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\n\nfrom third_party.nucleus.io import vcf\nfrom deepvariant import variant_caller\n\nFLAGS = flags.FLAGS\n\n\nclass VcfCandidateImporter(variant_caller.VariantCaller):\n  """"""Call variants and gvcf records from a VCF.""""""\n\n  def __init__(self,\n               options,\n               candidates_vcf,\n               use_cache_table=True,\n               max_cache_coverage=100):\n    super(VcfCandidateImporter, self).__init__(\n        options=options,\n        use_cache_table=use_cache_table,\n        max_cache_coverage=max_cache_coverage)\n    self.vcf_reader = vcf.NativeVcfReader(candidates_vcf).c_reader\n\n  def get_candidates(self, allele_counter):\n    return self.cpp_variant_caller.calls_from_vcf(allele_counter,\n                                                  self.vcf_reader)\n'"
deepvariant/vcf_candidate_importer_test.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .vcf_candidate_importer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\n\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import variant_utils\nfrom deepvariant import testdata\nfrom deepvariant import vcf_candidate_importer\nfrom deepvariant.labeler import labeled_examples_to_vcf\nfrom deepvariant.protos import deepvariant_pb2\n\n\ndef setUpModule():\n  testdata.init()\n\n\ndef _reference_model_options(p_error, max_gq, gq_resolution=1):\n  return deepvariant_pb2.VariantCallerOptions(\n      sample_name=\'UNKNOWN\',\n      p_error=p_error,\n      max_gq=max_gq,\n      gq_resolution=gq_resolution,\n      ploidy=2)\n\n\nclass VcfCandidateImporterTests(parameterized.TestCase):\n\n  def make_test_caller(self, p_error, max_gq, gq_resolution=1):\n    options = _reference_model_options(p_error, max_gq, gq_resolution)\n    return vcf_candidate_importer.VcfCandidateImporter(\n        options, testdata.TRUTH_VARIANTS_VCF, use_cache_table=False)\n\n  def fake_allele_counter(self, start_pos, counts):\n    allele_counter = mock.Mock()\n    # pylint: disable=g-complex-comprehension\n    allele_counter.summary_counts.return_value = [\n        deepvariant_pb2.AlleleCountSummary(\n            ref_supporting_read_count=n_ref,\n            total_read_count=n_ref + n_alt,\n            ref_base=ref,\n            reference_name=\'chr1\',\n            position=start_pos + i)\n        for i, (n_alt, n_ref, ref) in enumerate(counts)\n    ]\n    # pylint: enable=g-complex-comprehension\n    return allele_counter\n\n  def test_calls_from_vcf(self):\n    # Our test AlleleCounts are 5 positions:\n    #\n    # 10: A ref [no reads]\n    # 11: G/C variant\n    # 12: G ref [no reads]\n    # 13: G ref [no reads]\n    # 14: T/C variant\n    #\n    # The ref sites have no reads for ref or any alt simply because it\n    # simplifies comparing them with the expected variant genotype likelihoods.\n    # We aren\'t testing the correctness of the gvcf calculation here (that\'s\n    # elsewhere) but rather focusing here on the separation of variants from\n    # gvcf records, and the automatic merging of the gvcf blocks.\n    allele_counter = self.fake_allele_counter(10, [\n        (0, 0, \'A\'),\n        (10, 10, \'G\'),\n        (0, 0, \'G\'),\n        (0, 0, \'G\'),\n        (10, 10, \'T\'),\n    ])\n    fake_candidates = [\n        deepvariant_pb2.DeepVariantCall(\n            variant=test_utils.make_variant(alleles=[\'G\', \'C\'], start=11)),\n        deepvariant_pb2.DeepVariantCall(\n            variant=test_utils.make_variant(alleles=[\'T\', \'C\'], start=14)),\n    ]\n\n    caller = self.make_test_caller(0.01, 100)\n    with mock.patch.object(caller, \'cpp_variant_caller\') as mock_cpp:\n      mock_cpp.calls_from_vcf.return_value = fake_candidates\n      candidates, _ = caller.calls_and_gvcfs(allele_counter, False)\n\n    mock_cpp.calls_from_vcf.assert_called_once_with(allele_counter,\n                                                    caller.vcf_reader)\n    self.assertEqual(candidates, fake_candidates)\n\n  # Golden sets are created with learning/genomics/internal/create_golden.sh.\n  def test_vcf_caller_end2end_outputs(self):\n    # Confirming that the proposed VCF (input) has the same variants\n    # as the VCF output converted from the output of make_examples.\n    variants = list(\n        labeled_examples_to_vcf.examples_to_variants(\n            testdata.GOLDEN_VCF_CANDIDATE_IMPORTER_TRAINING_EXAMPLES))\n    with vcf.VcfReader(testdata.TRUTH_VARIANTS_VCF) as proposed_vcf_reader:\n      # This checks the keys (like chr20:10099832:A->G) are the same.\n      self.assertEqual([variant_utils.variant_key(v1) for v1 in variants], [\n          variant_utils.variant_key(v2) for v2 in proposed_vcf_reader.iterate()\n      ])\n\n    with vcf.VcfReader(testdata.TRUTH_VARIANTS_VCF) as proposed_vcf_reader:\n      self.assertEqual(\n          [variant_utils.genotype_as_alleles(v1) for v1 in variants], [\n              variant_utils.genotype_as_alleles(\n                  variant_utils.unphase_all_genotypes(v2))\n              for v2 in proposed_vcf_reader.iterate()\n          ])\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/vcf_stats.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\nr""""""Library to produce variant statistics from a VCF file.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport itertools\nimport math\nimport numpy as np\n\nfrom third_party.nucleus.util import variant_utils\nfrom third_party.nucleus.util import variantcall_utils\nfrom deepvariant import vcf_stats_vis\n\n_VARIANT_STATS_COLUMNS = [\n    \'reference_name\', \'position\', \'reference_bases\', \'alternate_bases\',\n    \'variant_type\', \'is_variant\', \'is_transition\', \'is_transversion\', \'depth\',\n    \'genotype_quality\', \'genotype\', \'vaf\', \'qual\'\n]\n\nVariantStats = collections.namedtuple(\'VariantStats\', _VARIANT_STATS_COLUMNS)\n\nBIALLELIC_SNP = \'Biallelic_SNP\'\nBIALLELIC_INSERTION = \'Biallelic_Insertion\'\nBIALLELIC_DELETION = \'Biallelic_Deletion\'\nBIALLELIC_MNP = \'Biallelic_MNP\'\nMULTIALLELIC_SNP = \'Multiallelic_SNP\'\nMULTIALLELIC_INSERTION = \'Multiallelic_Insertion\'\nMULTIALLELIC_DELETION = \'Multiallelic_Deletion\'\nMULTIALLELIC_COMPLEX = \'Multiallelic_Complex\'\nREFCALL = \'RefCall\'\n\n\ndef _get_variant_type(variant):\n  """"""Returns the type of variant as a string.""""""\n  if variant_utils.is_variant_call(variant):\n    biallelic = variant_utils.is_biallelic(variant)\n    snp = variant_utils.is_snp(variant)\n    insertion = variant_utils.variant_is_insertion(variant)\n    deletion = variant_utils.variant_is_deletion(variant)\n\n    if biallelic:\n      if snp:\n        return BIALLELIC_SNP\n      elif insertion:\n        return BIALLELIC_INSERTION\n      elif deletion:\n        return BIALLELIC_DELETION\n      else:\n        return BIALLELIC_MNP\n    else:\n      if snp:\n        return MULTIALLELIC_SNP\n      elif insertion:\n        return MULTIALLELIC_INSERTION\n      elif deletion:\n        return MULTIALLELIC_DELETION\n      else:\n        return MULTIALLELIC_COMPLEX\n  else:\n    return REFCALL\n\n\ndef _tstv(variant, vtype):\n  """"""Returns a pair of bools indicating Transition, Transversion status.""""""\n  if vtype == BIALLELIC_SNP:\n    is_transition = variant_utils.is_transition(variant.reference_bases,\n                                                variant.alternate_bases[0])\n    is_transversion = not is_transition\n  else:\n    is_transition = is_transversion = False\n\n  return is_transition, is_transversion\n\n\ndef _get_vaf(variant, vcf_reader):\n  """"""Gets the VAF (variant allele frequency).""""""\n  vafs = variantcall_utils.get_format(\n      variant_utils.only_call(variant), \'VAF\', vcf_reader)\n  return sum(vafs)\n\n\ndef _get_variant_stats(variant, vaf_available=False, vcf_reader=None):\n  """"""Returns a VariantStats object corresponding to the input variant.""""""\n  vtype = _get_variant_type(variant)\n  is_transition, is_transversion = _tstv(variant, vtype)\n  vaf = None\n  if vaf_available:\n    vaf = _get_vaf(variant, vcf_reader)\n\n  return VariantStats(\n      reference_name=variant.reference_name,\n      position=(variant.start + 1),\n      reference_bases=variant.reference_bases,\n      alternate_bases=list(variant.alternate_bases),\n      variant_type=vtype,\n      is_transition=is_transition,\n      is_transversion=is_transversion,\n      is_variant=variant_utils.is_variant_call(variant),\n      depth=variantcall_utils.get_format(\n          variant_utils.only_call(variant), \'DP\'),\n      genotype_quality=variantcall_utils.get_gq(\n          variant_utils.only_call(variant)),\n      genotype=str(\n          sorted(variantcall_utils.get_gt(variant_utils.only_call(variant)))),\n      vaf=vaf,\n      qual=variant.quality)\n\n\ndef _single_variant_stats(variants, vaf_available=False, vcf_reader=None):\n  return [\n      _get_variant_stats(v, vaf_available=vaf_available, vcf_reader=vcf_reader)\n      for v in variants\n  ]\n\n\ndef _format_histogram_for_vega(counts, bins):\n  """"""Format histogram counts and bins for vega.\n\n  Args:\n    counts: list of bin counts from np.histogram\n    bins: list of bins from np.histogram\n\n  Returns:\n    A list of objects with s (bin start), e (bin end), and c (bin count) for\n    each bin in the histogram.\n  """"""\n  # Avoid floats becoming 0.6000000000000001 to save space in output json\n  rounded_bins = [round(x, 10) for x in bins]\n  # pylint: disable=g-complex-comprehension\n  vega_formatted_hist = [{\n      \'s\': rounded_bins[idx],\n      \'e\': rounded_bins[idx + 1],\n      \'c\': count\n  } for idx, count in enumerate(counts)]\n  # pylint: enable=g-complex-comprehension\n  return vega_formatted_hist\n\n\ndef _fraction_histogram(values, number_of_bins=10):\n  counts, bins = np.histogram(values, bins=number_of_bins, range=(0, 1))\n  return _format_histogram_for_vega(counts, bins)\n\n\ndef _vaf_histograms_by_genotype(single_stats, number_of_bins=10):\n  """"""Computes histograms of allele frequency for each genotype.\n\n  Args:\n    single_stats: list of VariantStats objects.\n    number_of_bins: integer, number of bins in allele frequency histogram.\n\n  Returns:\n    A dictionary keyed by genotype where each value is a list of bins.\n  """"""\n\n  # Group by genotype\n  sorted_by_genotype = sorted(single_stats, key=lambda x: x.genotype)\n  grouped_by_genotype = itertools.groupby(sorted_by_genotype,\n                                          lambda x: x.genotype)\n  # Fill in empty placeholders for genotypes to populate all five charts\n  stats_by_genotype = {}\n  required_genotypes = [\'[0, 0]\', \'[0, 1]\', \'[1, 1]\', \'[-1, -1]\', \'[1, 2]\']\n  for genotype in required_genotypes:\n    # Create a few placeholder bins\n    stats_by_genotype[genotype] = _fraction_histogram([], 2)\n  # Count vafs from variants (replacing placeholders)\n  for genotype, group in grouped_by_genotype:\n    # Get VAF for each variant where it is defined\n    vafs = [x.vaf for x in group if x.vaf is not None]\n    stats_by_genotype[genotype] = _fraction_histogram(vafs, number_of_bins)\n\n  return stats_by_genotype\n\n\ndef _count_base_changes_and_indel_sizes(single_stats):\n  """"""Count each base change, such as A->G or C->T, and count the number of indels of each size.\n\n  Args:\n    single_stats: list of VariantStats objects.\n\n  Returns:\n    base_changes: {(ref, alt): count, ...}\n    indel_sizes: {size: count, ...}\n  """"""\n  base_changes = collections.defaultdict(int)\n  indel_sizes = collections.defaultdict(int)\n  for v in single_stats:\n    ref = v.reference_bases\n    alts = v.alternate_bases\n    # RefCalls are ignored\n    if v.is_variant:\n      # Multiallelic variants ignored here because they have different indel\n      # sizes and/or base changes\n      if v.variant_type == BIALLELIC_SNP:\n        # SNV: get base change\n        base_changes[(ref, alts[0])] += 1\n      elif v.variant_type in [BIALLELIC_INSERTION, BIALLELIC_DELETION]:\n        # indel: get size\n        # + = insertion\n        # - = deletion\n        size = len(alts[0]) - len(ref)\n        indel_sizes[size] += 1\n\n  base_changes_for_json = []\n  for key in base_changes:\n    ref, alt = key\n    base_changes_for_json.append([ref, alt, base_changes[key]])\n\n  indel_sizes_for_json = []\n  for key in indel_sizes:\n    indel_sizes_for_json.append([int(key), indel_sizes[key]])\n\n  return base_changes_for_json, indel_sizes_for_json\n\n\ndef _round_down(num):\n  return int(math.floor(num))\n\n\ndef _round_up(num):\n  return int(math.ceil(num))\n\n\ndef _compute_qual_histogram(single_var_stats):\n  """"""Compute a histogram over variant quality (QUAL column in VCF).\n\n  Args:\n    single_var_stats: list of VariantStats objects.\n\n  Returns:\n    histogram of variant quality scores.\n  """"""\n  quals = [round(v.qual, 4) for v in single_var_stats]\n\n  if quals:\n    bin_range = (_round_down(min(quals)), _round_up(max(quals) + 1))\n    counts, bins = np.histogram(\n        quals, range=bin_range, bins=bin_range[1] - bin_range[0])\n    hist = _format_histogram_for_vega(counts, bins)\n    return [x for x in hist if x[\'c\'] > 0]\n  else:\n    return []\n\n\ndef _get_integer_counts(nums):\n  """"""Turn a list of integers into a list of counts of those integers.\n\n  Args:\n    nums: a list of numbers (e.g. [1,2,2,4])\n\n  Returns:\n    a list of [num, count] (e.g. [[1,1],[2,2],[4,1]]) for all integers with\n    non-zero counts\n  """"""\n  bin_counts = np.bincount(nums)\n  non_zero_counts = [[i, x] for i, x in enumerate(bin_counts) if x > 0]\n  return non_zero_counts\n\n\ndef _compute_gq_histogram(single_var_stats):\n  """"""Compute a histogram over genotype quality (GQ sub-column under FORMAT in VCF).\n\n  Args:\n    single_var_stats: list of VariantStats objects.\n\n  Returns:\n    histogram of genotype quality scores.\n  """"""\n  quals = [\n      v.genotype_quality\n      for v in single_var_stats\n      if not isinstance(v.genotype_quality, list)\n  ]\n  return _get_integer_counts(quals)\n\n\ndef _compute_depth_histogram(single_var_stats):\n  """"""Compute a histogram on the depth, with larger bins as depth increases.""""""\n  depths = [v.depth for v in single_var_stats if not isinstance(v.depth, list)]\n  return _get_integer_counts(depths)\n\n\ndef _count_variant_types(single_stats):\n  count_all_variant_types = collections.defaultdict(int)\n  for v in single_stats:\n    count_all_variant_types[v.variant_type] += 1\n\n  return count_all_variant_types\n\n\ndef _count_titv(single_stats):\n  titv_counts = {\'Transition\': 0, \'Transversion\': 0}\n  titv_counts[\'Transition\'] = sum([v.is_transition for v in single_stats])\n  titv_counts[\'Transversion\'] = sum([v.is_transversion for v in single_stats])\n  return titv_counts\n\n\ndef _compute_variant_stats_for_charts(variants, vcf_reader=None):\n  """"""Computes variant statistics of each variant.\n\n  Args:\n    variants: iterable(Variant).\n    vcf_reader: VcfReader.\n\n  Returns:\n    A dict with summarized data prepared for charts.\n  """"""\n  vaf_available = False\n  if vcf_reader:\n    vcf_columns = [col.id for col in vcf_reader.header.formats]\n    vaf_available = \'VAF\' in vcf_columns\n\n  single_var_stats = _single_variant_stats(\n      variants, vaf_available=vaf_available, vcf_reader=vcf_reader)\n\n  titv_counts = _count_titv(single_var_stats)\n  variant_type_counts = _count_variant_types(single_var_stats)\n\n  base_changes, indel_sizes = _count_base_changes_and_indel_sizes(\n      single_var_stats)\n\n  histograms = _vaf_histograms_by_genotype(single_var_stats, number_of_bins=50)\n\n  qual_histogram = _compute_qual_histogram(single_var_stats)\n  gq_hist = _compute_gq_histogram(single_var_stats)\n  depth_histogram = _compute_depth_histogram(single_var_stats)\n\n  vis_data = {\n      \'vaf_histograms_by_genotype\': histograms,\n      \'indel_sizes\': indel_sizes,\n      \'base_changes\': base_changes,\n      \'qual_histogram\': qual_histogram,\n      \'gq_histogram\': gq_hist,\n      \'variant_type_counts\': variant_type_counts,\n      \'depth_histogram\': depth_histogram,\n      \'titv_counts\': titv_counts\n  }\n\n  return vis_data\n\n\ndef create_vcf_report(variants, output_basename, sample_name, vcf_reader=None):\n  """"""Calculate VCF stats and create a visual report.""""""\n  vis_data = _compute_variant_stats_for_charts(variants, vcf_reader=vcf_reader)\n\n  vcf_stats_vis.create_visual_report(output_basename, vis_data, sample_name)\n'"
deepvariant/vcf_stats_report.py,1,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\nr""""""Creates a visual HTML report about the variants from a VCF file.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport itertools\nfrom absl import flags\nimport tensorflow as tf\n\nfrom deepvariant import vcf_stats\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.util import errors\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'input_vcf\', None, \'Path to the input VCF.\')\nflags.DEFINE_string(\n    \'outfile_base\', None,\n    \'Base path to the output JSON stats and visualization dir.\')\nflags.DEFINE_integer(\n    \'num_records\', -1, \'Maximum number of records to emit. If \'\n    \'negative, emit all records.\')\n\n\ndef main(argv):\n  with errors.clean_commandline_error_exit():\n    if len(argv) > 1:\n      errors.log_and_raise(\n          \'Command line parsing failure: vcf_stats_report does not accept \'\n          \'positional arguments but some are present on the command line: \'\n          \'""{}"".\'.format(str(argv[1:])), errors.CommandLineError)\n\n  with vcf.VcfReader(FLAGS.input_vcf) as reader:\n    sample_names = reader.header.sample_names\n    if len(sample_names) != 1:\n      raise ValueError(\'There must be exactly one sample in VCF: {}\'.format(\n          FLAGS.input_vcf))\n    sample_name = sample_names[0]\n\n    # Missing GT causes error later while reading, so throw a clearer error here\n    vcf_columns = [col.id for col in reader.header.formats]\n    if \'GT\' not in vcf_columns:\n      errors.log_and_raise(\'ERROR: No GT sub-column in VCF.\')\n\n    if FLAGS.num_records == -1:\n      variants = reader.iterate()\n    else:\n      variants = itertools.islice(reader.iterate(), FLAGS.num_records)\n\n    vcf_stats.create_vcf_report(\n        variants,\n        output_basename=FLAGS.outfile_base,\n        sample_name=sample_name,\n        vcf_reader=reader)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flags_as_required([\'input_vcf\', \'outfile_base\'])\n  tf.compat.v1.app.run()\n'"
deepvariant/vcf_stats_test.py,1,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\nr""""""Tests for deepvariant .vcf_stats.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport collections\nimport json\nimport os\nimport tempfile\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport six\nimport tensorflow as tf\n\nfrom deepvariant import testdata\nfrom deepvariant import vcf_stats\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import variant_utils\nfrom third_party.nucleus.util import variantcall_utils\n\n\ndef setUpModule():\n  testdata.init()\n\n\nclass VcfStatsTest(parameterized.TestCase):\n\n  def setUp(self):\n    super(VcfStatsTest, self).setUp()\n    self.variant = test_utils.make_variant(\n        chrom=\'chr1\', start=10, alleles=[\'A\', \'G\'], gt=[0, 1], gq=59)\n    variantcall_utils.set_format(\n        variant_utils.only_call(self.variant), \'DP\', 20)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), vcf_stats.BIALLELIC_SNP),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<*>\']),\n       vcf_stats.BIALLELIC_SNP),\n      (test_utils.make_variant(alleles=[\'A\', \'AG\']),\n       vcf_stats.BIALLELIC_INSERTION),\n      (test_utils.make_variant(alleles=[\'A\', \'AG\', \'<*>\']),\n       vcf_stats.BIALLELIC_INSERTION),\n      (test_utils.make_variant(alleles=[\'AG\', \'A\']),\n       vcf_stats.BIALLELIC_DELETION),\n      (test_utils.make_variant(alleles=[\'AG\', \'A\', \'<*>\']),\n       vcf_stats.BIALLELIC_DELETION),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'G\']),\n       vcf_stats.MULTIALLELIC_SNP),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'G\', \'<*>\']),\n       vcf_stats.MULTIALLELIC_SNP),\n      (test_utils.make_variant(alleles=[\'A\', \'AC\', \'AG\']),\n       vcf_stats.MULTIALLELIC_INSERTION),\n      (test_utils.make_variant(alleles=[\'A\', \'AC\', \'AG\', \'<*>\']),\n       vcf_stats.MULTIALLELIC_INSERTION),\n      (test_utils.make_variant(alleles=[\'AGC\', \'AC\', \'A\', \'AG\']),\n       vcf_stats.MULTIALLELIC_DELETION),\n      (test_utils.make_variant(alleles=[\'AGC\', \'AC\', \'A\', \'AG\', \'<*>\']),\n       vcf_stats.MULTIALLELIC_DELETION),\n      (test_utils.make_variant(alleles=[\'AG\', \'AC\', \'A\']),\n       vcf_stats.MULTIALLELIC_COMPLEX),\n      (test_utils.make_variant(alleles=[\'AG\', \'AC\', \'A\', \'<*>\']),\n       vcf_stats.MULTIALLELIC_COMPLEX),\n      (test_utils.make_variant(alleles=[\'A\', \'G\', \'AT\']),\n       vcf_stats.MULTIALLELIC_COMPLEX),\n      (test_utils.make_variant(alleles=[\'A\', \'G\', \'AT\', \'<*>\']),\n       vcf_stats.MULTIALLELIC_COMPLEX),\n      (test_utils.make_variant(alleles=[\'AG\', \'TC\']), vcf_stats.BIALLELIC_MNP),\n      (test_utils.make_variant(alleles=[\'AG\', \'TC\', \'<*>\']),\n       vcf_stats.BIALLELIC_MNP),\n      (test_utils.make_variant(alleles=[\'A\']), vcf_stats.REFCALL),\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\']), vcf_stats.REFCALL),\n      (test_utils.make_variant(alleles=[\'A\', \'G\'],\n                               filters=\'FAIL\'), vcf_stats.REFCALL),\n      (test_utils.make_variant(alleles=[\'A\', \'G\', \'<*>\'],\n                               filters=\'FAIL\'), vcf_stats.REFCALL),\n      (test_utils.make_variant(alleles=[\'A\', \'G\'],\n                               filters=[\'FAIL\']), vcf_stats.REFCALL),\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\'],\n                               filters=\'.\'), vcf_stats.REFCALL),\n  )\n  def test_get_variant_type(self, variant, expected_type):\n    self.assertEqual(vcf_stats._get_variant_type(variant), expected_type)\n\n  @parameterized.parameters(\n      dict(\n          alleles=[\'A\', \'G\'],\n          gt=[0, 1],\n          expected_variant_type=vcf_stats.BIALLELIC_SNP,\n          expected_transition=True,\n          expection_transversion=False,\n          expected_is_variant=True),\n      dict(\n          alleles=[\'A\', \'AG\'],\n          gt=[0, 1],\n          expected_variant_type=vcf_stats.BIALLELIC_INSERTION,\n          expected_transition=False,\n          expection_transversion=False,\n          expected_is_variant=True),\n      dict(\n          alleles=[\'AT\', \'A\'],\n          gt=[0, 1],\n          expected_variant_type=vcf_stats.BIALLELIC_DELETION,\n          expected_transition=False,\n          expection_transversion=False,\n          expected_is_variant=True),\n      dict(\n          alleles=[\'AT\', \'GC\'],\n          gt=[0, 1],\n          expected_variant_type=vcf_stats.BIALLELIC_MNP,\n          expected_transition=False,\n          expection_transversion=False,\n          expected_is_variant=True),\n      dict(\n          alleles=[\'A\', \'G\', \'T\'],\n          gt=[0, 1],\n          expected_variant_type=vcf_stats.MULTIALLELIC_SNP,\n          expected_transition=False,\n          expection_transversion=False,\n          expected_is_variant=True),\n      dict(\n          alleles=[\'A\', \'AG\', \'AT\'],\n          gt=[0, 1],\n          expected_variant_type=vcf_stats.MULTIALLELIC_INSERTION,\n          expected_transition=False,\n          expection_transversion=False,\n          expected_is_variant=True),\n      dict(\n          alleles=[\'AT\', \'A\', \'T\'],\n          gt=[0, 1],\n          expected_variant_type=vcf_stats.MULTIALLELIC_DELETION,\n          expected_transition=False,\n          expection_transversion=False,\n          expected_is_variant=True),\n      dict(\n          alleles=[\'A\', \'GT\', \'G\'],\n          gt=[0, 1],\n          expected_variant_type=vcf_stats.MULTIALLELIC_COMPLEX,\n          expected_transition=False,\n          expection_transversion=False,\n          expected_is_variant=True),\n      dict(\n          alleles=[\'A\', \'G\'],\n          gt=[0, 0],\n          expected_variant_type=vcf_stats.REFCALL,\n          expected_transition=False,\n          expection_transversion=False,\n          expected_is_variant=False),\n  )\n  def test_get_variant_stats(self, alleles, gt, expected_variant_type,\n                             expected_transition, expection_transversion,\n                             expected_is_variant):\n    variant = test_utils.make_variant(\n        chrom=\'chr1\', start=10, alleles=alleles, gt=gt, gq=59)\n    variant_stats = vcf_stats._get_variant_stats(variant)\n    self.assertEqual(\n        variant_stats,\n        vcf_stats.VariantStats(\n            reference_name=\'chr1\',\n            position=11,\n            reference_bases=alleles[0],\n            alternate_bases=alleles[1:],\n            variant_type=expected_variant_type,\n            is_transition=expected_transition,\n            is_transversion=expection_transversion,\n            is_variant=expected_is_variant,\n            depth=[],\n            genotype_quality=59,\n            genotype=str(gt),\n            vaf=None,\n            qual=0.0))\n\n  def test_compute_variant_stats_for_charts(self):\n    expected_keys = [\n        \'vaf_histograms_by_genotype\', \'indel_sizes\', \'base_changes\',\n        \'qual_histogram\', \'gq_histogram\', \'variant_type_counts\',\n        \'depth_histogram\', \'titv_counts\'\n    ]\n    vis_data = vcf_stats._compute_variant_stats_for_charts([self.variant])\n    six.assertCountEqual(\n        self,\n        vis_data.keys(),\n        expected_keys,\n        msg=\'vis_data does not have the right keys\')\n\n  def test_vaf_histograms_by_genotype(self):\n    variant_stats_lite = collections.namedtuple(\'variant_stats_lite\',\n                                                [\'genotype\', \'vaf\'])\n    variant_stats = [\n        variant_stats_lite(genotype=\'[0, 0]\', vaf=0),\n        variant_stats_lite(genotype=\'[1, 1]\', vaf=1),\n        variant_stats_lite(genotype=\'[0, 1]\', vaf=0.5),\n        variant_stats_lite(genotype=\'[0, 1]\', vaf=0.5),\n        variant_stats_lite(genotype=\'[0, 0]\', vaf=0.08),\n        variant_stats_lite(genotype=\'[0, 0]\', vaf=0.19),\n        variant_stats_lite(genotype=\'[0, 1]\', vaf=0.45),\n        variant_stats_lite(genotype=\'[0, 1]\', vaf=0.65)\n    ]\n    # s = bin_start, e = bin_end, c = count\n    truth_histograms = """"""\n    {\n      ""[0, 1]"": [{""c"": 0, ""e"": 0.1, ""s"": 0.0}, {""c"": 0, ""e"": 0.2, ""s"": 0.1}, {""c"": 0, ""e"": 0.3, ""s"": 0.2}, {""c"": 0, ""e"": 0.4, ""s"": 0.3}, {""c"": 1, ""e"": 0.5, ""s"": 0.4}, {""c"": 2, ""e"": 0.6, ""s"": 0.5}, {""c"": 1, ""e"": 0.7, ""s"": 0.6}, {""c"": 0, ""e"": 0.8, ""s"": 0.7}, {""c"": 0, ""e"": 0.9, ""s"": 0.8}, {""c"": 0, ""e"": 1.0, ""s"": 0.9}],\n      ""[1, 1]"": [{""c"": 0, ""e"": 0.1, ""s"": 0.0}, {""c"": 0, ""e"": 0.2, ""s"": 0.1}, {""c"": 0, ""e"": 0.3, ""s"": 0.2}, {""c"": 0, ""e"": 0.4, ""s"": 0.3}, {""c"": 0, ""e"": 0.5, ""s"": 0.4}, {""c"": 0, ""e"": 0.6, ""s"": 0.5}, {""c"": 0, ""e"": 0.7, ""s"": 0.6}, {""c"": 0, ""e"": 0.8, ""s"": 0.7}, {""c"": 0, ""e"": 0.9, ""s"": 0.8}, {""c"": 1, ""e"": 1.0, ""s"": 0.9}],\n      ""[0, 0]"": [{""c"": 2, ""e"": 0.1, ""s"": 0.0}, {""c"": 1, ""e"": 0.2, ""s"": 0.1}, {""c"": 0, ""e"": 0.3, ""s"": 0.2}, {""c"": 0, ""e"": 0.4, ""s"": 0.3}, {""c"": 0, ""e"": 0.5, ""s"": 0.4}, {""c"": 0, ""e"": 0.6, ""s"": 0.5}, {""c"": 0, ""e"": 0.7, ""s"": 0.6}, {""c"": 0, ""e"": 0.8, ""s"": 0.7}, {""c"": 0, ""e"": 0.9, ""s"": 0.8}, {""c"": 0, ""e"": 1.0, ""s"": 0.9}],\n      ""[-1, -1]"": [{""c"": 0, ""e"": 0.5, ""s"": 0.0}, {""c"": 0, ""e"": 1.0, ""s"": 0.5}],\n      ""[1, 2]"": [{""c"": 0, ""e"": 0.5, ""s"": 0.0}, {""c"": 0, ""e"": 1.0, ""s"": 0.5}]\n      }\n    """"""\n    self.assertEqual(\n        vcf_stats._vaf_histograms_by_genotype(variant_stats),\n        json.loads(truth_histograms))\n\n  def test_format_histogram_for_vega(self):\n    # s = bin_start, e = bin_end, c = count\n    self.assertEqual(\n        vcf_stats._format_histogram_for_vega(counts=[2, 2], bins=[1, 2.5, 4]),\n        [{\n            \'s\': 1,\n            \'e\': 2.5,\n            \'c\': 2\n        }, {\n            \'s\': 2.5,\n            \'e\': 4,\n            \'c\': 2\n        }])\n\n  def test_count_titv(self):\n    variant_stats_lite = collections.namedtuple(\n        \'variant_stats_lite\', [\'is_transition\', \'is_transversion\'])\n    variant_stats = [\n        variant_stats_lite(is_transition=True, is_transversion=False),\n        variant_stats_lite(is_transition=True, is_transversion=False),\n        variant_stats_lite(is_transition=True, is_transversion=False),\n        variant_stats_lite(is_transition=False, is_transversion=True),\n        variant_stats_lite(is_transition=False, is_transversion=True),\n        variant_stats_lite(is_transition=False, is_transversion=False)\n    ]\n    truth_counts = {\'Transition\': 3, \'Transversion\': 2}\n    self.assertEqual(vcf_stats._count_titv(variant_stats), truth_counts)\n\n  def test_count_variant_types(self):\n    variant_stats_lite = collections.namedtuple(\'variant_stats_lite\',\n                                                [\'variant_type\'])\n    variant_stats = [\n        variant_stats_lite(variant_type=\'A\'),\n        variant_stats_lite(variant_type=\'B\'),\n        variant_stats_lite(variant_type=\'C\'),\n        variant_stats_lite(variant_type=\'A\'),\n        variant_stats_lite(variant_type=\'B\')\n    ]\n    truth_counts = {\'A\': 2, \'B\': 2, \'C\': 1}\n    self.assertEqual(\n        vcf_stats._count_variant_types(variant_stats), truth_counts)\n\n  def test_count_base_changes_and_indel_sizes(self):\n    variant_stats_lite = collections.namedtuple(\n        \'variant_stats_lite\',\n        [\'reference_bases\', \'alternate_bases\', \'is_variant\', \'variant_type\'])\n    variant_stats = [\n        variant_stats_lite(\n            reference_bases=\'A\',\n            alternate_bases=[\'G\'],\n            is_variant=True,\n            variant_type=vcf_stats.BIALLELIC_SNP),\n        variant_stats_lite(\n            reference_bases=\'A\',\n            alternate_bases=[\'AGGG\'],\n            is_variant=True,\n            variant_type=vcf_stats.BIALLELIC_INSERTION),\n        variant_stats_lite(\n            reference_bases=\'A\',\n            alternate_bases=[\'G\'],\n            is_variant=False,\n            variant_type=vcf_stats.REFCALL),\n        variant_stats_lite(\n            reference_bases=\'A\',\n            alternate_bases=[\'G\', \'T\'],\n            is_variant=True,\n            variant_type=vcf_stats.MULTIALLELIC_COMPLEX)\n    ]\n    truth_base_changes = [[\'A\', \'G\', 1]]\n    truth_indel_sizes = [[3, 1]]\n    base_changes, indel_sizes = vcf_stats._count_base_changes_and_indel_sizes(\n        variant_stats)\n    self.assertEqual(base_changes, truth_base_changes)\n    self.assertEqual(indel_sizes, truth_indel_sizes)\n\n  def test_compute_qual_histogram(self):\n    variant_stats_lite = collections.namedtuple(\'variant_stats_lite\', [\'qual\'])\n    variant_stats = [variant_stats_lite(qual=100), variant_stats_lite(qual=49)]\n    hist = vcf_stats._compute_qual_histogram(variant_stats)\n    # s = bin_start, e = bin_end, c = count\n    self.assertEqual(hist, [{\n        \'c\': 1,\n        \'s\': 49.0,\n        \'e\': 50.0\n    }, {\n        \'c\': 1,\n        \'s\': 100.0,\n        \'e\': 101.0\n    }])\n\n  def test_get_integer_counts(self):\n    self.assertEqual(\n        vcf_stats._get_integer_counts([1, 2, 2, 4]), [[1, 1], [2, 2], [4, 1]])\n\n  def test_compute_gq_histogram(self):\n    variant_stats_lite = collections.namedtuple(\'variant_stats_lite\',\n                                                [\'genotype_quality\'])\n    variant_stats = [\n        variant_stats_lite(genotype_quality=100),\n        variant_stats_lite(genotype_quality=100),\n        variant_stats_lite(genotype_quality=49)\n    ]\n    hist = vcf_stats._compute_gq_histogram(variant_stats)\n    self.assertEqual(hist, [[49, 1], [100, 2]])\n\n  def test_compute_depth_histogram(self):\n    variant_stats_lite = collections.namedtuple(\'variant_stats_lite\', [\'depth\'])\n    variant_stats = [\n        variant_stats_lite(depth=100),\n        variant_stats_lite(depth=30),\n        variant_stats_lite(depth=30)\n    ]\n    hist = vcf_stats._compute_depth_histogram(variant_stats)\n    self.assertEqual(hist, [[30, 2], [100, 1]])\n\n  def test_create_vcf_report(self):\n    base_dir = tempfile.mkdtemp()\n    outfile_base = os.path.join(base_dir, \'stats_test\')\n    sample_name = \'test_sample_name\'\n    with vcf.VcfReader(testdata.GOLDEN_POSTPROCESS_OUTPUT) as reader:\n      vcf_stats.create_vcf_report(\n          variants=reader.iterate(),\n          output_basename=outfile_base,\n          sample_name=sample_name,\n          vcf_reader=reader)\n    self.assertTrue(tf.io.gfile.exists(outfile_base + \'.visual_report.html\'))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/vcf_stats_vis.py,1,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Create a visual report from a VCF file.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport io\nimport json\nimport os\n\nimport altair as alt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\n# Altair uses a lot of method chaining, such as\n# chart.mark_bar().encode(...).properties(...), so allowing backslash\n# continuation to break this into separate lines makes the code more readable.\n# pylint: disable=g-backslash-continuation\n\nOLD_LIB_BASE_URL = \'https://cdn.jsdelivr.net/npm//\'\nNEW_LIB_BASE_URL = \'https://storage.googleapis.com/deepvariant/lib/vega/\'\n\nVEGA_VERSION = \'5\'\nVEGA_LITE_VERSION = \'3.4.0\'\nVEGA_EMBED_VERSION = \'4\'\n\n# ""pretty"" genotype strings:\nREF = \'Ref (0/0)\'\nHET = \'Het (0/x)\'\nHOM = \'Hom (x/x)\'\nUNCALLED = \'Uncalled (./.)\'\nHET_BOTH = \'Het - two variants (x/y)\'\n\n# Establish ordering of bases to keep it consistent\nBASES = [\'A\', \'G\', \'T\', \'C\']\n\nBAR_COLOR_DEPTH = \'#4a1486\'\nBAR_COLOR_QUAL = \'#0c2c84\'\nBAR_COLOR_GQ = \'#0c2c84\'\n\nBIALLELIC_SNP = \'Biallelic_SNP\'\nBIALLELIC_INSERTION = \'Biallelic_Insertion\'\nBIALLELIC_DELETION = \'Biallelic_Deletion\'\nBIALLELIC_MNP = \'Biallelic_MNP\'\nMULTIALLELIC_SNP = \'Multiallelic_SNP\'\nMULTIALLELIC_INSERTION = \'Multiallelic_Insertion\'\nMULTIALLELIC_DELETION = \'Multiallelic_Deletion\'\nMULTIALLELIC_COMPLEX = \'Multiallelic_Complex\'\nREFCALL = \'RefCall\'\n\nordered_variant_type_labels = [\n    BIALLELIC_INSERTION, BIALLELIC_DELETION, BIALLELIC_SNP, BIALLELIC_MNP,\n    MULTIALLELIC_INSERTION, MULTIALLELIC_DELETION, MULTIALLELIC_SNP,\n    MULTIALLELIC_COMPLEX, REFCALL\n]\n\n\ndef _dict_to_dataframe(dictionary):\n  """"""Turn a dict object into a dataframe of with label and value columns.""""""\n  df = pd.DataFrame({\n      \'label\': list(dictionary.keys()),\n      \'value\': list(dictionary.values())\n  })\n  return df\n\n\ndef _prettify_genotype(genotype):\n  """"""Get more human-readable display name and grouping for a given genotype.""""""\n  pretty = genotype\n  group = \'others\'\n  alleles = json.loads(genotype)\n  if len(alleles) == 2:\n    g1, g2 = sorted(alleles)\n    if g1 == 0 and g2 == 0:\n      pretty = REF\n      group = \'main\'\n    elif g1 == -1 and g2 == -1:\n      pretty = UNCALLED\n    elif g1 == 0 and g2 > 0:\n      pretty = HET\n      group = \'main\'\n    elif g1 == g2:\n      pretty = HOM\n      group = \'main\'\n    else:\n      pretty = HET_BOTH\n  return pretty, group\n\n\ndef _build_type_chart(variant_type_counts):\n  """"""Create a chart of the counts of each variant type.""""""\n  width = 400\n  height = 200\n  title = \'Variant types\'\n  variant_type_data = _dict_to_dataframe(variant_type_counts)\n  type_chart = _placeholder_for_empty_chart(\n      \'No entries in VCF\', width=width, height=height, title=title)\n  if not variant_type_data.empty:\n    bars = alt.Chart(variant_type_data).mark_bar().encode(\n        x=alt.X(\n            \'label\',\n            title=None,\n            sort=ordered_variant_type_labels,\n            axis=alt.Axis(labelAngle=-45)),\n        y=alt.Y(\'value\', axis=alt.Axis(title=\'Count\', format=\'s\')),\n        tooltip=alt.Tooltip(\'value\', format=\'.4s\'),\n        color=alt.Color(\n            \'label\',\n            legend=None,\n            scale=alt.Scale(scheme=\'set1\', domain=ordered_variant_type_labels)))\n    labels = bars.mark_text(dy=-5).encode(text=alt.Text(\'value\', format=\'.4s\'))\n    type_chart = (bars + labels).properties(\n        width=width, height=height, title=title)\n  return type_chart\n\n\ndef _build_qual_histogram(data):\n  """"""Create the Quality(QUAL) histogram.""""""\n  width = 200\n  height = 200\n  title = \'Quality score\'\n  qual_data = pd.DataFrame(data)\n  qual_histogram = _placeholder_for_empty_chart(\n      \'No entries in VCF\', width=width, height=height, title=title)\n  if not qual_data.empty:\n    # s = bin_start, e = bin_end, c = count\n    domain = [min(0, data[0][\'s\']), max(150, data[-1][\'e\'])]\n    qual_histogram = alt.Chart(qual_data).mark_bar(color=BAR_COLOR_QUAL) \\\n        .encode(\n            x=alt.X(\'s\', title=\'QUAL\', scale=alt.Scale(domain=domain)),\n            x2=\'e\',\n            y=alt.Y(\'c\', title=\'Count\', stack=True, axis=alt.Axis(format=\'s\'))) \\\n        .properties(\n            width=width, height=height,\n            title=title) \\\n        .interactive(bind_y=False)\n  return qual_histogram\n\n\ndef _build_gq_histogram(data):\n  """"""Create the Genotype quality (GQ) histogram.""""""\n  # gq = genotype quality, found at :GQ: in FORMAT column of VCF\n  width = 200\n  height = 200\n  title = \'Genotype quality\'\n  gq_data = _integer_counts_to_histogram(data)\n  gq_histogram = _placeholder_for_empty_chart(\n      \'No entries in VCF with GQ\', width=width, height=height, title=title)\n  if not gq_data.empty:\n    # standardize x-axis limits across reports\n    domain = [min(0, data[0][0]), max(150, data[-1][0])]\n    # s = bin_start, e = bin_end, c = count\n    gq_histogram = alt.Chart(gq_data).mark_bar(color=BAR_COLOR_GQ) \\\n        .encode(\n            x=alt.X(\'s\', title=\'GQ\', scale=alt.Scale(domain=domain)),\n            x2=\'e\',\n            y=alt.Y(\'c\', title=\'Count\', stack=True, axis=alt.Axis(format=\'s\'))) \\\n        .properties(width=width, height=height, title=title) \\\n        .interactive(bind_y=False)\n  return gq_histogram\n\n\ndef _build_vaf_histograms(histogram_json):\n  """"""Create VAF histograms split by genotype.""""""\n  guides = {REF: 0, HET: 0.5, HOM: 1}\n  hist_data = pd.DataFrame()\n  for key in histogram_json:\n    g = pd.DataFrame(histogram_json[key])\n    pretty, group = _prettify_genotype(key)\n    g[\'GT\'] = pretty  # pretty genotype name\n    g[\'g\'] = group  # main/other genotypes\n    g[\'l\'] = guides.get(pretty, None)  # vertical line as guide\n    hist_data = hist_data.append(g)\n\n  main_hist_data = hist_data[hist_data[\'g\'] == \'main\']\n  other_hist_data = hist_data[hist_data[\'g\'] == \'others\']\n\n  # Main genotypes (ref, het, hom-alt)\n  # Histogram bars themselves\n  # s = bin_start, e = bin_end, c = count\n  bars = alt.Chart(main_hist_data).mark_bar().encode(\n      x=alt.X(\'s\', title=\'VAF\'),\n      x2=\'e\',\n      y=alt.Y(\'c\', title=\'Count\', stack=True, axis=alt.Axis(format=\'s\')))\n  # Vertical lines\n  guides = alt.Chart(main_hist_data).mark_rule().encode(x=\'l\')\n  # Facet into 3 plots by genotype\n  vaf_histograms = (bars + guides) \\\n    .properties(width=200, height=200) \\\n    .facet(column=alt.Column(\'GT\',\n                             title=\'Main genotypes\',\n                             sort=[REF, HET, HOM])) \\\n    .resolve_scale(y=\'independent\')\n\n  # Other genotypes (uncalled, het with two alt alleles)\n  # s = bin_start, e = bin_end, c = count\n  other_vaf_histograms = alt.Chart(other_hist_data) \\\n    .mark_bar().encode(\n        x=alt.X(\'s\', title=\'VAF\'),\n        x2=\'e\',\n        y=alt.Y(\'c\', title=\'Count\', stack=True, axis=alt.Axis(format=\'s\')),\n        column=alt.Column(\'GT\', title=\'Other genotypes\')) \\\n    .properties(width=150, height=150) \\\n    .resolve_scale(y=\'independent\')\n  return vaf_histograms, other_vaf_histograms\n\n\ndef _placeholder_for_empty_chart(text_to_display,\n                                 width=100,\n                                 height=100,\n                                 title=\'\'):\n  chart = alt.Chart({\'values\': [{\'placeholder\': text_to_display}]}) \\\n      .mark_text(size=14).encode(text=\'placeholder:N\') \\\n      .properties(width=width, height=height, title=title)\n  return chart\n\n\ndef _build_base_change_chart(data):\n  """"""Create the base change chart.""""""\n  width = 100\n  height = 200\n  placeholder_width = (4 * width) + 80  # 4 charts, plus constant spacing\n  title = \'Biallelic base changes from reference\'\n  base_change_data = pd.DataFrame(data, columns=[\'ref\', \'alt\', \'count\'])\n\n  base_change_chart = _placeholder_for_empty_chart(\n      \'No biallelic SNPs\', width=placeholder_width, height=height, title=title)\n  if not base_change_data.empty:\n    bars = alt.Chart(base_change_data).mark_bar().encode(\n        x=alt.X(\'alt\', title=\'to alt\'),\n        y=alt.Y(\'count\', title=\'Count\', axis=alt.Axis(format=\'s\')),\n        color=alt.Color(\n            \'alt\',\n            legend=None,\n            sort=BASES,\n            scale=alt.Scale(scheme=\'category20\', domain=BASES)),\n        tooltip=alt.Tooltip(\'count\', format=\'.4s\'))\n    labels = bars.mark_text(dy=-5, fontWeight=\'bold\').encode(text=\'alt\')\n\n    base_change_chart = (bars + labels) \\\n        .properties(width=100, height=200) \\\n        .facet(column=alt.Column(\'ref\',\n                                 title=title,\n                                 sort=BASES))\n\n  return base_change_chart\n\n\ndef _integer_counts_to_histogram(num_count_pairs):\n  """"""Turn paired numbers and their counts into data for a histogram.\n\n  This centers the bars on the exact integer for clarity. For example, the bar\n  for 3 is centered on 3 instead of being between 3 and 4 as in numpy\'s default\n  histogram.\n\n  Args:\n    num_count_pairs: list of [num, count] pairs\n\n  Returns:\n    a pandas dataframe with num, count (bin count), s (bin start), e (bin end)\n  """"""\n  histogram_data = pd.DataFrame(num_count_pairs, columns=[\'num\', \'c\'])\n  # For a proper histogram, use s and e to force each bar to cover\n  # exactly one integer position:\n  histogram_data[\'s\'] = histogram_data[\'num\'] - 0.5\n  histogram_data[\'e\'] = histogram_data[\'num\'] + 0.5\n  histogram_data = histogram_data.drop(columns=[\'num\'])\n  return histogram_data\n\n\ndef _build_indel_size_chart(data):\n  """"""Create the indel size chart.""""""\n  width = 400\n  height = 100\n  placeholder_height = (2 * height) + 20  # 2 charts, plus spacing\n  title = \'Biallelic indel size distribution\'\n  ordered_labels = [\'Insertion\', \'Deletion\']\n  indel_size_data = _integer_counts_to_histogram(data)\n  indel_size_data[\'type\'] = np.where(indel_size_data[\'s\'] > 0, \'Insertion\',\n                                     \'Deletion\')\n\n  indel_size_chart = _placeholder_for_empty_chart(\n      \'No biallelic indels\',\n      width=width,\n      height=placeholder_height,\n      title=title)\n\n  if not indel_size_data.empty:\n    indels_linear = alt.Chart(indel_size_data).mark_bar().encode(\n        x=alt.X(\'s\', title=\'size\'),\n        x2=\'e\',\n        y=alt.Y(\'c\', title=\'Count\', axis=alt.Axis(format=\'s\')),\n        color=alt.Color(\'type\', sort=ordered_labels,\n                        scale=alt.Scale(scheme=\'set1\'))) \\\n      .properties(width=400, height=100,\n                  title=title) \\\n      .interactive(bind_y=False)\n\n    indel_log = alt.Chart(indel_size_data).mark_bar().encode(\n        x=alt.X(\'s\', title=\'size\'),\n        x2=\'e\',\n        y=alt.Y(\n            \'c\',\n            title=\'Count\',\n            axis=alt.Axis(format=\'s\'),\n            scale=alt.Scale(type=\'log\', base=10)),\n        color=alt.Color(\'type\', sort=ordered_labels,\n                        scale=alt.Scale(scheme=\'set1\'))) \\\n      .properties(width=400, height=100) \\\n      .interactive(bind_y=False)\n\n    indel_size_chart = alt.vconcat(indels_linear, indel_log) \\\n        .resolve_scale(color=\'shared\')\n  return indel_size_chart\n\n\ndef _build_depth_histogram(data):\n  """"""Build histogram with depth (DP).""""""\n  width = 200\n  height = 200\n  title = \'Depth\'\n  depth_data = _integer_counts_to_histogram(data)\n  depth_histogram = _placeholder_for_empty_chart(\n      \'No entries in VCF with DP\', width=width, height=height, title=title)\n  if not depth_data.empty:\n    # s = bin_start, e = bin_end, c = count\n    depth_histogram = alt.Chart(depth_data).mark_bar(color=BAR_COLOR_DEPTH) \\\n        .encode(x=alt.X(\'s\', title=\'Depth\'),\n                x2=\'e\',\n                y=alt.Y(\'c\', title=\'Count\', stack=True, axis=alt.Axis(format=\'s\'))) \\\n        .properties(width=width, height=height, title=title) \\\n        .interactive(bind_y=False)\n  return depth_histogram\n\n\ndef _build_tt_chart(titv_counts):\n  """"""Built chart showing counts of transitions and transversions.""""""\n  width = 150\n  height = 200\n\n  ti = titv_counts[\'Transition\']\n  tv = titv_counts[\'Transversion\']\n  # Show TiTv ratio with fallback to avoid division by 0\n  titv_ratio = \'%.2f\' % (float(ti) / tv) if tv > 0 else \'%d / 0\' % (ti)\n  title = \'Biallelic Ti/Tv ratio: %s\' % (titv_ratio)\n\n  tt_chart = _placeholder_for_empty_chart(\n      \'No biallelic SNPs\', width=width, height=height, title=title)\n  tt_labels = [\'Transition\', \'Transversion\']\n  if sum([titv_counts[k] for k in titv_counts]) > 0:\n    tt_data = _dict_to_dataframe(titv_counts)\n    bars = alt.Chart(tt_data).mark_bar().encode(\n        x=alt.X(\n            \'label\', sort=tt_labels, axis=alt.Axis(title=None, labelAngle=0)),\n        y=alt.Y(\'value\', axis=alt.Axis(title=\'Count\', format=\'s\')),\n        tooltip=alt.Tooltip(\'value\', format=\'.4s\'),\n        color=alt.Color(\n            \'label\',\n            legend=None,\n            sort=tt_labels,\n            scale=alt.Scale(scheme=\'teals\', domain=tt_labels)))\n    labels = bars.mark_text(dy=-5).encode(text=alt.Text(\'value\', format=\'.4s\'))\n    tt_chart = (bars + labels).properties(\n        title=title, width=width, height=height)\n  return tt_chart\n\n\ndef _build_all_charts(vis_data, sample_name=\'\'):\n  """"""Build all charts and combine into a single interface.""""""\n\n  # Row 1\n  type_chart = _build_type_chart(vis_data[\'variant_type_counts\'])\n  depth_chart = _build_depth_histogram(vis_data[\'depth_histogram\'])\n  qual_histogram = _build_qual_histogram(vis_data[\'qual_histogram\'])\n  gq_histogram = _build_gq_histogram(vis_data[\'gq_histogram\'])\n  row1 = alt.hconcat(type_chart, depth_chart, qual_histogram, gq_histogram) \\\n      .resolve_scale(color=\'independent\')\n\n  # Row 2\n  vaf_histograms, other_vaf_histograms = _build_vaf_histograms(\n      vis_data[\'vaf_histograms_by_genotype\'])\n  row2 = alt.hconcat(vaf_histograms, other_vaf_histograms)\n\n  # Row 3\n  base_change_chart = _build_base_change_chart(vis_data[\'base_changes\'])\n  indel_size_chart = _build_indel_size_chart(vis_data[\'indel_sizes\'])\n  tt_chart = _build_tt_chart(vis_data[\'titv_counts\'])\n  row3 = alt.hconcat(base_change_chart, tt_chart, indel_size_chart) \\\n      .resolve_scale(color=\'independent\')\n\n  # Putting it all together\n  all_charts = alt.vconcat(row1, row2, row3)\n\n  all_charts = all_charts.properties(title=sample_name, spacing=70) \\\n      .configure_header(labelFontSize=16, titleFontSize=20) \\\n      .configure_title(fontSize=20)\n  return all_charts\n\n\ndef _altair_chart_to_html(altair_chart, download_filename):\n  """"""Write to a temporary string stand-in for the file to replace import URLs.\n\n  Args:\n    altair_chart: a chart object made by Altair.\n    download_filename: string filename base for when users export images.\n\n  Returns:\n    HTML in string format.\n  """"""\n  temp_writer = io.StringIO()\n  altair_chart.save(\n      temp_writer,\n      format=\'html\',\n      embed_options={\'downloadFileName\': download_filename},\n      vegalite_version=VEGA_LITE_VERSION,\n      vega_version=VEGA_VERSION,\n      vegaembed_version=VEGA_EMBED_VERSION)\n  temp_html_string = temp_writer.getvalue()\n  html_with_new_cdn = temp_html_string.replace(OLD_LIB_BASE_URL,\n                                               NEW_LIB_BASE_URL)\n  return html_with_new_cdn\n\n\ndef _save_html(basename, all_charts):\n  """"""Save Altair chart as an HTML file.""""""\n  output_path = basename + \'.visual_report.html\'\n  image_download_filename = os.path.basename(basename) + \'.visual_report\'\n  html_string = _altair_chart_to_html(\n      altair_chart=all_charts, download_filename=image_download_filename)\n\n  with tf.io.gfile.GFile(output_path, \'w\') as writer:\n    writer.write(html_string)\n\n\ndef create_visual_report(basename, vis_data, sample_name=\'\'):\n  """"""Build visual report with several charts.""""""\n  all_charts = _build_all_charts(vis_data, sample_name)\n  _save_html(basename, all_charts)\n'"
deepvariant/vcf_stats_vis_test.py,1,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .vcf_stats_vis.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport os\nimport tempfile\nfrom absl.testing import absltest\nimport altair as alt\nimport pandas as pd\nimport six\nimport tensorflow as tf\n\nfrom deepvariant import vcf_stats_vis\n\n# Note: histograms all have keys s, e, and c, shortened versions of\n# bin_start, bin_end, and count to save space in output HTML\nVIS_DATA = {\n    \'base_changes\': [[\'G\', \'A\', 56], [\'T\', \'A\', 17], [\'C\', \'T\', 47],\n                     [\'G\', \'C\', 19], [\'T\', \'C\', 48], [\'C\', \'A\', 14],\n                     [\'A\', \'T\', 9], [\'A\', \'C\', 15], [\'T\', \'G\', 9],\n                     [\'G\', \'T\', 15], [\'A\', \'G\', 60], [\'C\', \'G\', 11]],\n    \'gq_histogram\': [[1, 3], [2, 24]],\n    \'indel_sizes\': [[1, 6], [2, 4], [4, 2], [5, 2], [7, 2], [8, 1], [12, 1],\n                    [-2, 6], [-5, 1], [-4, 7], [-3, 4], [-1, 11]],\n    \'qual_histogram\': [{\n        \'s\': 0,\n        \'e\': 50,\n        \'c\': 10\n    }, {\n        \'s\': 50,\n        \'e\': 99,\n        \'c\': 10\n    }],\n    \'depth_histogram\': [[0, 10], [1, 20]],\n    \'vaf_histograms_by_genotype\': {\n        \'[-1, -1]\': [{\n            \'e\': 0.5,\n            \'s\': 0,\n            \'c\': 10\n        }, {\n            \'e\': 1,\n            \'s\': 0.5,\n            \'c\': 10\n        }],\n        \'[0, 0]\': [{\n            \'e\': 0.5,\n            \'s\': 0,\n            \'c\': 10\n        }, {\n            \'e\': 1,\n            \'s\': 0.5,\n            \'c\': 10\n        }],\n        \'[0, 1]\': [{\n            \'e\': 0.5,\n            \'s\': 0,\n            \'c\': 10\n        }, {\n            \'e\': 1,\n            \'s\': 0.5,\n            \'c\': 10\n        }],\n        \'[0, 2]\': [{\n            \'e\': 0.5,\n            \'s\': 0,\n            \'c\': 10\n        }, {\n            \'e\': 1,\n            \'s\': 0.5,\n            \'c\': 10\n        }],\n        \'[1, 1]\': [{\n            \'e\': 0.5,\n            \'s\': 0,\n            \'c\': 10\n        }, {\n            \'e\': 1,\n            \'s\': 0.5,\n            \'c\': 10\n        }],\n        \'[1, 2]\': [{\n            \'e\': 0.5,\n            \'s\': 0,\n            \'c\': 10\n        }, {\n            \'e\': 1,\n            \'s\': 0.5,\n            \'c\': 10\n        }],\n        \'[1, 3]\': [{\n            \'e\': 0.5,\n            \'s\': 0,\n            \'c\': 10\n        }, {\n            \'e\': 1,\n            \'s\': 0.5,\n            \'c\': 10\n        }]\n    },\n    \'variant_type_counts\': {\n        \'Biallelic_SNP\': 10,\n        \'RefCall\': 3,\n        \'Multiallelic_Insertion\': 1\n    },\n    \'titv_counts\': {\n        \'Transition\': 20,\n        \'Transversion\': 10\n    }\n}\n\nALTAIR_CHART = ""<class \'altair.vegalite.v3.api.Chart\'>""\nFACET_CHART = ""<class \'altair.vegalite.v3.api.FacetChart\'>""\nLAYER_CHART = ""<class \'altair.vegalite.v3.api.LayerChart\'>""\nV_CONCAT_CHART = ""<class \'altair.vegalite.v3.api.VConcatChart\'>""\n\n\nclass VcfStatsVisTest(absltest.TestCase):\n\n  def test_dict_to_dataframe(self):\n    self.assertEqual(\'K\', \'K\')\n    self.assertEqual(\n        vcf_stats_vis._dict_to_dataframe({\n            \'A\': \'a\'\n        }).to_dict(\'records\'), [{\n            \'label\': \'A\',\n            \'value\': \'a\'\n        }])\n\n  def test_prettify_genotype(self):\n    self.assertEqual(\n        vcf_stats_vis._prettify_genotype(\'[0, 0]\'), (vcf_stats_vis.REF, \'main\'))\n    self.assertEqual(\n        vcf_stats_vis._prettify_genotype(\'[-1, -1]\'),\n        (vcf_stats_vis.UNCALLED, \'others\'))\n    self.assertEqual(\n        vcf_stats_vis._prettify_genotype(\'[3, 3]\'), (vcf_stats_vis.HOM, \'main\'))\n    self.assertEqual(\n        vcf_stats_vis._prettify_genotype(\'[0, 3]\'), (vcf_stats_vis.HET, \'main\'))\n    self.assertEqual(\n        vcf_stats_vis._prettify_genotype(\'[6, 3]\'),\n        (vcf_stats_vis.HET_BOTH, \'others\'))\n\n  def test_integer_counts_to_histogram(self):\n    test_input = [[1, 1], [2, 2], [4, 1]]\n    expected_output = pd.DataFrame(\n        data={\n            \'c\': [1, 2, 1],\n            \'s\': [0.5, 1.5, 3.5],\n            \'e\': [1.5, 2.5, 4.5]\n        },\n        columns=[\'c\', \'s\', \'e\'])\n    observed_output = vcf_stats_vis._integer_counts_to_histogram(test_input)\n    six.assertCountEqual(\n        self,\n        list(observed_output.columns),\n        list(expected_output.columns),\n        msg=\'Wrong column names\')\n    self.assertEqual(\n        list(observed_output[\'c\']),\n        list(expected_output[\'c\']),\n        msg=\'column c differs\')\n    self.assertEqual(\n        list(observed_output[\'s\']),\n        list(expected_output[\'s\']),\n        msg=\'column s differs\')\n    self.assertEqual(\n        list(observed_output[\'e\']),\n        list(expected_output[\'e\']),\n        msg=\'column e differs\')\n    self.assertTrue((observed_output == expected_output).all().all())\n\n  def test_build_type_chart(self):\n    chart = vcf_stats_vis._build_type_chart(VIS_DATA[\'variant_type_counts\'])\n    self.assertEqual(str(type(chart)), LAYER_CHART)\n\n  def test_build_tt_chart(self):\n    chart = vcf_stats_vis._build_tt_chart(VIS_DATA[\'titv_counts\'])\n    self.assertEqual(str(type(chart)), LAYER_CHART)\n\n  def test_build_qual_histogram(self):\n    chart = vcf_stats_vis._build_qual_histogram(VIS_DATA[\'qual_histogram\'])\n    self.assertEqual(str(type(chart)), ALTAIR_CHART)\n\n  def test_build_depth_histogram(self):\n    chart = vcf_stats_vis._build_depth_histogram(VIS_DATA[\'depth_histogram\'])\n    self.assertEqual(str(type(chart)), ALTAIR_CHART)\n\n  def test_build_gq_histogram(self):\n    chart = vcf_stats_vis._build_gq_histogram(VIS_DATA[\'gq_histogram\'])\n    self.assertEqual(str(type(chart)), ALTAIR_CHART)\n\n  def test_build_vaf_histograms(self):\n    chart = vcf_stats_vis._build_vaf_histograms(\n        VIS_DATA[\'vaf_histograms_by_genotype\'])\n    self.assertEqual(str(type(chart[0])), FACET_CHART)\n    self.assertEqual(str(type(chart[1])), ALTAIR_CHART)\n\n  def test_build_base_change_chart(self):\n    chart = vcf_stats_vis._build_base_change_chart(VIS_DATA[\'base_changes\'])\n    self.assertEqual(str(type(chart)), FACET_CHART)\n\n  def test_build_indel_size_chart(self):\n    chart = vcf_stats_vis._build_indel_size_chart(VIS_DATA[\'indel_sizes\'])\n    self.assertEqual(str(type(chart)), V_CONCAT_CHART)\n\n  def test_build_all_charts(self):\n    chart = vcf_stats_vis._build_all_charts(VIS_DATA)\n    self.assertEqual(str(type(chart)), V_CONCAT_CHART)\n\n  def test_altair_chart_to_html(self):\n    df = pd.DataFrame({\'x\': [\'A\', \'B\'], \'y\': [28, 55]})\n    c = alt.Chart(df).mark_bar().encode(x=\'x\', y=\'y\')\n    html_string = vcf_stats_vis._altair_chart_to_html(\n        altair_chart=c, download_filename=\'TEST_DOWNLOAD_FILENAME\')\n    import_base = \'src=""https://storage.googleapis.com/deepvariant/lib/vega/\'\n    self.assertNotEqual(\n        html_string.find(import_base + \'vega@%s""\' %\n                         (vcf_stats_vis.VEGA_VERSION)), -1)\n    self.assertNotEqual(\n        html_string.find(import_base + \'vega-lite@%s""\' %\n                         (vcf_stats_vis.VEGA_LITE_VERSION)), -1)\n    self.assertNotEqual(\n        html_string.find(import_base + \'vega-embed@%s""\' %\n                         (vcf_stats_vis.VEGA_EMBED_VERSION)), -1)\n    self.assertEqual(html_string.find(\'jsdelivr.net\'), -1)\n    self.assertNotEqual(html_string.find(\'TEST_DOWNLOAD_FILENAME\'), -1)\n\n  def test_create_visual_report(self):\n    base_dir = tempfile.mkdtemp()\n    outfile_base = os.path.join(base_dir, \'stats_test\')\n    sample_name = \'test_sample_name\'\n    vcf_stats_vis.create_visual_report(\n        outfile_base, VIS_DATA, sample_name=sample_name)\n    self.assertTrue(tf.io.gfile.exists(outfile_base + \'.visual_report.html\'))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/very_sensitive_caller.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""A VerySensitiveCaller producing DeepVariantCall and gVCF records.\n\nThis module provides the primary interface for calling candidate variants using\nfor the AlleleCounts in an AlleleCounter by wrapping the low-level C++ code and\nadding a nicer API and functions to compute gVCF records as well.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom deepvariant import variant_caller\n\n\nclass VerySensitiveCaller(variant_caller.VariantCaller):\n  """"""Call variants and gvcf records from an AlleleCounter.""""""\n\n  def __init__(self, options, use_cache_table=True, max_cache_coverage=100):\n    super(VerySensitiveCaller, self).__init__(\n        options=options,\n        use_cache_table=use_cache_table,\n        max_cache_coverage=max_cache_coverage)\n\n  def get_candidates(self, allele_counter):\n    return self.cpp_variant_caller.calls_from_allele_counter(allele_counter)\n'"
deepvariant/very_sensitive_caller_test.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .very_sensitive_caller.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\nfrom third_party.nucleus.testing import test_utils\nfrom deepvariant import testdata\nfrom deepvariant import very_sensitive_caller\nfrom deepvariant.protos import deepvariant_pb2\n\n\ndef setUpModule():\n  testdata.init()\n\n\ndef _reference_model_options(p_error, max_gq, gq_resolution=1):\n  return deepvariant_pb2.VariantCallerOptions(\n      sample_name=\'UNKNOWN\',\n      p_error=p_error,\n      max_gq=max_gq,\n      gq_resolution=gq_resolution,\n      ploidy=2)\n\n\nclass VerySensitiveCallerTests(parameterized.TestCase):\n\n  def make_test_caller(self, p_error, max_gq, gq_resolution=1):\n    options = _reference_model_options(p_error, max_gq, gq_resolution)\n    return very_sensitive_caller.VerySensitiveCaller(\n        options, use_cache_table=False)\n\n  def fake_allele_counter(self, start_pos, counts):\n    allele_counter = mock.Mock()\n    # pylint: disable=g-complex-comprehension\n    allele_counter.summary_counts.return_value = [\n        deepvariant_pb2.AlleleCountSummary(\n            ref_supporting_read_count=n_ref,\n            total_read_count=n_ref + n_alt,\n            ref_base=ref,\n            reference_name=\'chr1\',\n            position=start_pos + i)\n        for i, (n_alt, n_ref, ref) in enumerate(counts)\n    ]\n    # pylint: enable=g-complex-comprehension\n    return allele_counter\n\n  def test_calls_from_allele_counts(self):\n    # Our test AlleleCounts are 5 positions:\n    #\n    # 10: A ref [no reads]\n    # 11: G/C variant\n    # 12: G ref [no reads]\n    # 13: G ref [no reads]\n    # 14: T/C variant\n    #\n    # The ref sites have no reads for ref or any alt simply because it\n    # simplifies comparing them with the expected variant genotype likelihoods.\n    # We aren\'t testing the correctness of the gvcf calculation here (that\'s\n    # elsewhere) but rather focusing here on the separation of variants from\n    # gvcf records, and the automatic merging of the gvcf blocks.\n    allele_counter = self.fake_allele_counter(10, [\n        (0, 0, \'A\'),\n        (10, 10, \'G\'),\n        (0, 0, \'G\'),\n        (0, 0, \'G\'),\n        (10, 10, \'T\'),\n    ])\n    fake_candidates = [\n        deepvariant_pb2.DeepVariantCall(\n            variant=test_utils.make_variant(alleles=[\'G\', \'C\'], start=11)),\n        deepvariant_pb2.DeepVariantCall(\n            variant=test_utils.make_variant(alleles=[\'T\', \'C\'], start=14)),\n    ]\n\n    caller = self.make_test_caller(0.01, 100)\n    with mock.patch.object(caller, \'cpp_variant_caller\') as mock_cpp:\n      mock_cpp.calls_from_allele_counter.return_value = fake_candidates\n      candidates, _ = caller.calls_and_gvcfs(allele_counter, False)\n\n    mock_cpp.calls_from_allele_counter.assert_called_once_with(allele_counter)\n    self.assertEqual(candidates, fake_candidates)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
scripts/run_deepvariant.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Runs all 3 steps to go from input DNA reads to output VCF/gVCF files.\n\nThis script currently provides the most common use cases and standard models.\nIf you want to access more flags that are available in `make_examples`,\n`call_variants`, and `postprocess_variants`, you can also call them separately\nusing the binaries in the Docker image.\n\nFor more details, see:\nhttps://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport subprocess\nimport tempfile\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nFLAGS = flags.FLAGS\n\n# Required flags.\nflags.DEFINE_enum(\n    \'model_type\', None, [\'WGS\', \'WES\', \'PACBIO\'],\n    \'Required. Type of model to use for variant calling. Each \'\n    \'model_type has an associated default model, which can be \'\n    \'overridden by the --customized_model flag.\')\nflags.DEFINE_string(\n    \'ref\', None,\n    \'Required. Genome reference to use. Must have an associated FAI index as \'\n    \'well. Supports text or gzipped references. Should match the reference \'\n    \'used to align the BAM file provided to --reads.\')\nflags.DEFINE_string(\n    \'reads\', None,\n    \'Required. Aligned, sorted, indexed BAM file containing the reads we want \'\n    \'to call. Should be aligned to a reference genome compatible with --ref.\')\nflags.DEFINE_string(\'output_vcf\', None,\n                    \'Required. Path where we should write VCF file.\')\n# Optional flags.\nflags.DEFINE_string(\n    \'intermediate_results_dir\', None,\n    \'Optional. If specified, this should be an existing \'\n    \'directory that is visible insider docker, and will be \'\n    \'used to to store intermediate outputs.\')\n# Optional flags for call_variants.\nflags.DEFINE_string(\n    \'customized_model\', None,\n    \'Optional. A path to a model checkpoint to load for the `call_variants` \'\n    \'step. If not set, the default for each --model_type will be used\')\n# Optional flags for make_examples.\nflags.DEFINE_integer(\'num_shards\', 1,\n                     \'Optional. Number of shards for make_examples step.\')\nflags.DEFINE_string(\n    \'regions\', None,\n    \'Optional. Space-separated list of regions we want to process. Elements \'\n    \'can be region literals (e.g., chr20:10-20) or paths to BED/BEDPE files.\')\nflags.DEFINE_string(\n    \'sample_name\', None,\n    \'Sample name to use instead of the sample name from the input reads BAM \'\n    \'(SM tag in the header).\')\nflags.DEFINE_string(\n    \'make_examples_extra_args\', None,\n    \'A comma-separated list of flag_name=flag_value. ""flag_name"" has to be \'\n    \'valid flags for make_examples.py. If the flag_value is boolean, it has to \'\n    \'be flag_name=true or flag_name=false.\')\nflags.DEFINE_string(\n    \'call_variants_extra_args\', None,\n    \'A comma-separated list of flag_name=flag_value. ""flag_name"" has to be \'\n    \'valid flags for call_variants.py. If the flag_value is boolean, it has to \'\n    \'be flag_name=true or flag_name=false.\')\nflags.DEFINE_string(\n    \'postprocess_variants_extra_args\', None,\n    \'A comma-separated list of flag_name=flag_value. ""flag_name"" has to be \'\n    \'valid flags for calpostprocess_variants.py. If the flag_value is boolean, \'\n    \'it has to be flag_name=true or flag_name=false.\')\n\n# Optional flags for postprocess_variants.\nflags.DEFINE_string(\'output_gvcf\', None,\n                    \'Optional. Path where we should write gVCF file.\')\nflags.DEFINE_boolean(\n    \'vcf_stats_report\', True, \'Optional. Output a visual report (HTML) of \'\n    \'statistics about the output VCF.\')\n\nMODEL_TYPE_MAP = {\n    \'WGS\': \'/opt/models/wgs/model.ckpt\',\n    \'WES\': \'/opt/models/wes/model.ckpt\',\n    \'PACBIO\': \'/opt/models/pacbio/model.ckpt\',\n}\n\n\ndef _add_quotes(value):\n  if isinstance(value, str) and value.startswith(\'""\') and value.endswith(\'""\'):\n    return value\n  return \'""{}""\'.format(value)\n\n\ndef _extra_args_to_dict(extra_args):\n  """"""Parses comma-separated list of flag_name=flag_value to dict.""""""\n  args_dict = {}\n  if extra_args is None:\n    return args_dict\n  for extra_arg in extra_args.split(\',\'):\n    (flag_name, flag_value) = extra_arg.split(\'=\')\n    # Check for boolean values.\n    if flag_value.lower() == \'true\':\n      flag_value = True\n    elif flag_value.lower() == \'false\':\n      flag_value = False\n    args_dict[flag_name] = flag_value\n  return args_dict\n\n\ndef _extend_command_by_args_dict(command, extra_args):\n  """"""Adds `extra_args` to the command string.""""""\n  for key in sorted(extra_args):\n    value = extra_args[key]\n    if value is None:\n      continue\n    if isinstance(value, bool):\n      added_arg = \'\' if value else \'no\'\n      added_arg += key\n      command.extend([\'--\' + added_arg])\n    else:\n      command.extend([\'--\' + key, _add_quotes(value)])\n  return command\n\n\ndef make_examples_command(ref, reads, examples, extra_args, **kwargs):\n  """"""Returns a make_examples command for subprocess.check_call.\n\n  Args:\n    ref: Input FASTA file.\n    reads: Input BAM file.\n    examples: Output tfrecord file containing tensorflow.Example files.\n    extra_args: Comma-separated list of flag_name=flag_value.\n    **kwargs: Additional arguments to pass in for make_examples.\n\n  Returns:\n    (string) A command to run.\n  """"""\n  command = [\n      \'time\', \'seq 0 {} |\'.format(FLAGS.num_shards - 1),\n      \'parallel --halt 2 --line-buffer\', \'/opt/deepvariant/bin/make_examples\'\n  ]\n  command.extend([\'--mode\', \'calling\'])\n  command.extend([\'--ref\', \'""{}""\'.format(ref)])\n  command.extend([\'--reads\', \'""{}""\'.format(reads)])\n  command.extend([\'--examples\', \'""{}""\'.format(examples)])\n  # Extend the command with all items in kwargs and extra_args.\n  kwargs.update(_extra_args_to_dict(extra_args))\n  if FLAGS.model_type == \'PACBIO\':\n    kwargs[\'realign_reads\'] = False\n    kwargs[\'vsc_min_fraction_indels\'] = 0.12\n  command = _extend_command_by_args_dict(command, kwargs)\n\n  command.extend([\'--task {}\'])\n  return \' \'.join(command)\n\n\ndef call_variants_command(outfile, examples, model_ckpt, extra_args):\n  """"""Returns a call_variants command for subprocess.check_call.""""""\n  command = [\'time\', \'/opt/deepvariant/bin/call_variants\']\n  command.extend([\'--outfile\', \'""{}""\'.format(outfile)])\n  command.extend([\'--examples\', \'""{}""\'.format(examples)])\n  command.extend([\'--checkpoint\', \'""{}""\'.format(model_ckpt)])\n  # Extend the command with all items in extra_args.\n  command = _extend_command_by_args_dict(command,\n                                         _extra_args_to_dict(extra_args))\n  return \' \'.join(command)\n\n\ndef postprocess_variants_command(ref,\n                                 infile,\n                                 outfile,\n                                 extra_args,\n                                 nonvariant_site_tfrecord_path=None,\n                                 gvcf_outfile=None,\n                                 vcf_stats_report=True):\n  """"""Returns a postprocess_variants command for subprocess.check_call.""""""\n  command = [\'time\', \'/opt/deepvariant/bin/postprocess_variants\']\n  command.extend([\'--ref\', \'""{}""\'.format(ref)])\n  command.extend([\'--infile\', \'""{}""\'.format(infile)])\n  command.extend([\'--outfile\', \'""{}""\'.format(outfile)])\n  if nonvariant_site_tfrecord_path is not None:\n    command.extend([\n        \'--nonvariant_site_tfrecord_path\',\n        \'""{}""\'.format(nonvariant_site_tfrecord_path)\n    ])\n  if gvcf_outfile is not None:\n    command.extend([\'--gvcf_outfile\', \'""{}""\'.format(gvcf_outfile)])\n  if not vcf_stats_report:\n    command.extend([\'--novcf_stats_report\'])\n  # Extend the command with all items in extra_args.\n  command = _extend_command_by_args_dict(command,\n                                         _extra_args_to_dict(extra_args))\n  return \' \'.join(command)\n\n\ndef check_or_create_intermediate_results_dir(intermediate_results_dir):\n  """"""Checks or creates the path to the directory for intermediate results.""""""\n  if intermediate_results_dir is None:\n    intermediate_results_dir = tempfile.mkdtemp()\n  if not os.path.isdir(intermediate_results_dir):\n    logging.info(\'Creating a directory for intermediate results in %s\',\n                 intermediate_results_dir)\n    os.makedirs(intermediate_results_dir)\n  else:\n    logging.info(\'Re-using the directory for intermediate results in %s\',\n                 intermediate_results_dir)\n  return intermediate_results_dir\n\n\ndef check_flags():\n  """"""Additional logic to make sure flags are set appropriately.""""""\n  if FLAGS.customized_model is not None:\n    logging.info(\n        \'You set --customized_model. Instead of using the default \'\n        \'model for %s, `call_variants` step will load %s \'\n        \'instead.\', FLAGS.model_type, FLAGS.customized_model)\n\n\ndef get_model_ckpt(model_type, customized_model):\n  """"""Return the path to the model checkpoint based on the input args.""""""\n  if customized_model is not None:\n    return customized_model\n  else:\n    return MODEL_TYPE_MAP[model_type]\n\n\ndef create_all_commands(intermediate_results_dir):\n  """"""Creates 3 commands to be executed later.""""""\n  commands = []\n  # make_examples\n  nonvariant_site_tfrecord_path = None\n  if FLAGS.output_gvcf is not None:\n    nonvariant_site_tfrecord_path = os.path.join(\n        intermediate_results_dir,\n        \'gvcf.tfrecord@{}.gz\'.format(FLAGS.num_shards))\n\n  examples = os.path.join(\n      intermediate_results_dir,\n      \'make_examples.tfrecord@{}.gz\'.format(FLAGS.num_shards))\n\n  commands.append(\n      make_examples_command(\n          FLAGS.ref,\n          FLAGS.reads,\n          examples,\n          FLAGS.make_examples_extra_args,\n          gvcf=nonvariant_site_tfrecord_path,\n          regions=FLAGS.regions,\n          sample_name=FLAGS.sample_name))\n\n  # call_variants\n  call_variants_output = os.path.join(intermediate_results_dir,\n                                      \'call_variants_output.tfrecord.gz\')\n  model_ckpt = get_model_ckpt(FLAGS.model_type, FLAGS.customized_model)\n  commands.append(\n      call_variants_command(call_variants_output, examples, model_ckpt,\n                            FLAGS.call_variants_extra_args))\n\n  # postprocess_variants\n  commands.append(\n      postprocess_variants_command(\n          FLAGS.ref,\n          call_variants_output,\n          FLAGS.output_vcf,\n          FLAGS.postprocess_variants_extra_args,\n          nonvariant_site_tfrecord_path=nonvariant_site_tfrecord_path,\n          gvcf_outfile=FLAGS.output_gvcf,\n          vcf_stats_report=FLAGS.vcf_stats_report))\n\n  return commands\n\n\ndef main(_):\n  intermediate_results_dir = check_or_create_intermediate_results_dir(\n      FLAGS.intermediate_results_dir)\n  check_flags()\n\n  commands = create_all_commands(intermediate_results_dir)\n  print(\'\\n***** Intermediate results will be written to {} \'\n        \'in docker. ****\\n\'.format(intermediate_results_dir))\n  for command in commands:\n    print(\'\\n***** Running the command:*****\\n{}\\n\'.format(command))\n    try:\n      subprocess.check_call(command, shell=True, executable=\'/bin/bash\')\n    except subprocess.CalledProcessError as e:\n      logging.info(e.output)\n      raise\n\n\nif __name__ == \'__main__\':\n  flags.mark_flags_as_required([\n      \'model_type\',\n      \'ref\',\n      \'reads\',\n      \'output_vcf\',\n  ])\n  app.run(main)\n'"
tools/print_f1.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\nr""""""Parse and extract metrics from *.metrics files.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport json\nimport logging\nfrom os import listdir\nfrom os.path import isfile\nfrom os.path import join\nimport re\n\n\ndef parse_cmdline(argv):\n  """"""Parse the commandline.""""""\n  parser = argparse.ArgumentParser()\n\n  parser.add_argument(\n      \'--metrics_dir\', help=\'Path to the directory with metrics files.\')\n\n  known_args, _ = parser.parse_known_args(argv)\n\n  return known_args\n\n\ndef extract_checkpoint_number_from_metrics_filename(filename):\n  match = re.search(r\'ckpt-([\\d]*)\\.metrics\', filename)\n  if match:\n    return int(match.group(1))\n\n\ndef read_metrics_file(path):\n  """"""Reads metrics f and outputs metrics in a dict.""""""\n  with open(path) as f:\n    metrics = {\n        key.replace(\'/\', \'_\'): float(value)\n        for key, value in json.loads(f.read()).items()\n    }\n  metrics[\'checkpoint\'] = extract_checkpoint_number_from_metrics_filename(path)\n  metrics[\'F1_All\'] = 2 * metrics[\'TPs_All\'] / (\n      2 * metrics[\'TPs_All\'] + metrics[\'FNs_All\'] + metrics[\'FPs_All\'])\n  metrics[\'TPs+FNs_All\'] = metrics[\'TPs_All\'] + metrics[\'FNs_All\']\n  return metrics\n\n\ndef main(argv=None):\n  """"""Main entry point.""""""\n  known_args = parse_cmdline(argv)\n  metrics_dir = known_args.metrics_dir\n  metrics_files = [\n      join(metrics_dir, f)\n      for f in listdir(metrics_dir)\n      if isfile(join(metrics_dir, f))\n  ]\n  metrics = [read_metrics_file(f) for f in metrics_files]\n  for m in metrics:\n    print(\'%s\\t%s\\t%s\' % (m[\'checkpoint\'], m[\'TPs+FNs_All\'], m[\'F1_All\']))\n\n\nif __name__ == \'__main__\':\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
tools/shuffle_tfrecords_beam.py,2,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n# pylint: disable=line-too-long\nr""""""Shuffle tf.Example files using beam.\n\nTo run locally:\n1) Install beam on your machine following the instructions at\n   https://beam.apache.org/get-started/quickstart-py/\n\n2) Copy any inputs to be on local disk.\n\n3) Run\n  python path/to/shuffle_tfrecords_beam.py \\\n    --input_pattern_list=""/tmp/some.examples-?????-of-00200.tfrecord.gz"" \\\n    --output_pattern_prefix=""/tmp/training.examples"" \\\n    --output_dataset_name=""HG001"" \\\n    --runner=DirectRunner\n\nTo run on Google Cloud Dataflow Service:\n1) Follow the Google Cloud Dataflow setup instructions at\nhttps://beam.apache.org/documentation/runners/dataflow/\n\n2) Upload this file to your GCE instance.\n\n3) Run\n  python shuffle_tfrecords_beam.py \\\n  --job_name=shuffle-tfrecords \\\n  --input_pattern_list=""gs://YOUR_INPUT_BUCKET/A.tfrecord.gz"" \\\n  --output_pattern_prefix=""gs://YOUR_OUTPUT_BUCKET/training.examples"" \\\n  --output_dataset_name=""HG001"" \\\n  --runner=DataflowRunner \\\n  --project=SET_YOUR_PROJECT_ID_HERE \\\n  --staging_location=gs://YOUR_BUCKET_NAME/AND_STAGING_DIRECTORY \\\n  --temp_location=gs://YOUR_BUCKET_NAME/AND_TEMP_DIRECTORY\n\n4) (Optional) To monitor or cancel the job while it is running, you can\nuse either the Dataflow Monitoring Interface\nhttps://cloud.google.com/dataflow/pipelines/dataflow-monitoring-intf\nor the Dataflow Command-line Interface\nhttps://cloud.google.com/dataflow/pipelines/dataflow-command-line-intf\n""""""\n# pylint: enable=line-too-long\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport hashlib\nimport logging\nimport textwrap\n\nimport apache_beam as beam\nfrom apache_beam import coders\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nCOMMENT_HEADER = """"""# Generated by shuffle_tfrecords_beam.py\n#\n# --input_pattern_list={}\n# --output_pattern_prefix={}\n#\n""""""\n\n\ndef parse_cmdline(argv):\n  """"""Parse the commandline into known and pipeline arguments.\n\n  The known arguments are required for this specific program to function,\n  and the other pipeline arguments can be used to configure beam and the\n  specific beam backend being used.  See\n  https://github.com/apache/beam/blob/master/sdks/python/apache_beam/options/pipeline_options.py\n  for a list and description of the pipeline arguments accepted.\n\n  Args:\n    argv: List containing command-line arguments.\n\n  Returns:\n    A pair, the first of which are the known (non-pipeline) arguments\n    and the second of which are the pipeline arguments.\n  """"""\n  parser = argparse.ArgumentParser()\n\n  parser.add_argument(\n      \'--input_pattern_list\',\n      help=\'Comma-separated list of TFRecord filename patterns.\')\n  parser.add_argument(\n      \'--output_pattern_prefix\',\n      help=\'Filename pattern for the output TFRecords.\')\n  parser.add_argument(\n      \'--output_dataset_config_pbtxt\',\n      help=\'Optional.  If set, print out a human-readable version of \'\n      \'DeepVariantDatasetConfig.\')\n  parser.add_argument(\n      \'--output_dataset_name\',\n      help=\'Optional unless --output_dataset_config_pbtxt is set.\')\n\n  known_args, pipeline_args = parser.parse_known_args(argv)\n\n  return known_args, pipeline_args\n\n\ndef read_from_tfrecords_files(pipeline, input_filename_pattern_list):\n  """"""Reads records from TFRecord files.\n\n  Args:\n    pipeline: Beam pipeline object.\n    input_filename_pattern_list: List of filename patterns.\n\n  Returns:\n    A PCollection of read tf.Examples.\n  """"""\n  readers = []\n  for i, filepattern in enumerate(input_filename_pattern_list):\n    readers.append(pipeline\n                   | \'ReadTFRecordFiles_{}[{}]\'.format(i, filepattern) >> beam\n                   .io.ReadFromTFRecord(filepattern, coder=coders.BytesCoder()))\n  return readers | \'Flatten\' >> beam.Flatten()\n\n\ndef shuffle_records(input_examples):\n  """"""Shuffles the input_examples in a effectively random order.""""""\n\n  def sha1(input_bytes):\n    """"""Returns the sha1 hash of input_bytes.""""""\n    m = hashlib.sha1()\n    m.update(input_bytes)\n    return m.digest()\n\n  return (input_examples\n          | \'Randomize\' >> beam.Map(lambda x: (sha1(x), x))\n          | \'Groupby\' >> beam.GroupByKey()\n          | \'DropKey\' >> beam.FlatMap(lambda x: x[1]))\n\n\ndef make_config_string(name, tfrecord_path, num_examples):\n  return textwrap.dedent(""""""\n  name: ""{}""\n  tfrecord_path: ""{}-?????-of-?????.tfrecord.gz""\n  num_examples: {}\n  """""".format(name, tfrecord_path, num_examples))\n\n\ndef write_summary_string_to_file(pipeline, output_examples, input_pattern_list,\n                                 dataset_name, output_pattern_prefix,\n                                 output_filename):\n  """"""Writes a file summarizing the PCollection of Examples.\n\n  Args:\n    pipeline: Beam pipeline object.\n    output_examples: PCollection of examples.\n    input_pattern_list: str. A comma-separated string of input files.\n    dataset_name: str. The name of the dataset to be written in the output.\n    output_pattern_prefix: str. The prefix of the sharded output files.\n    output_filename: the output text file that contains the summary that can be\n      parsed into DeepVariantDatasetConfig.\n  """"""\n\n  # Beam currently has no way to materialize pipeline values, so we have\n  # to construct the file entirely in Beam pipeline operations.\n  comment_str = pipeline | \'CreateFileHeader\' >> beam.Create(\n      [COMMENT_HEADER.format(input_pattern_list, output_pattern_prefix)])\n  num_examples = (\n      output_examples\n      | \'CountOutputExamples\' >> beam.combiners.Count.Globally())\n  config_str = num_examples | \'MakeConfigStr\' >> beam.Map(\n      lambda n: make_config_string(dataset_name, output_pattern_prefix, n))\n\n  merged_strings = (comment_str, config_str) | \'FlattenStrs\' >> beam.Flatten()\n  _ = (\n      merged_strings\n      | \'Concat2\' >> beam.CombineGlobally(\'\'.join)\n      | \'WriteToFile\' >> beam.io.WriteToText(\n          output_filename, shard_name_template=\'\'))\n\n\ndef main(argv=None):\n  """"""Main entry point; defines and runs the pipeline.""""""\n  known_args, pipeline_args = parse_cmdline(argv)\n  pipeline_options = PipelineOptions(pipeline_args)\n  with beam.Pipeline(options=pipeline_options) as p:\n    input_examples = read_from_tfrecords_files(\n        p, known_args.input_pattern_list.split(\',\'))\n\n    output_examples = shuffle_records(input_examples)\n\n    _ = output_examples | beam.io.WriteToTFRecord(\n        file_path_prefix=known_args.output_pattern_prefix,\n        file_name_suffix=\'.tfrecord.gz\',\n        coder=coders.BytesCoder())\n    if known_args.output_dataset_config_pbtxt:\n      if not known_args.output_dataset_name:\n        raise ValueError(\'Need to set output_dataset_name.\')\n      write_summary_string_to_file(p, output_examples,\n                                   known_args.input_pattern_list,\n                                   known_args.output_dataset_name,\n                                   known_args.output_pattern_prefix,\n                                   known_args.output_dataset_config_pbtxt)\n\n\nif __name__ == \'__main__\':\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n'"
deepvariant/environment_tests/env_smoke_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Smoke tests for the genomics environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# We use unittest and not TF Test because the point of this test is to be\n# able to test our environment without having to build TensorFlow.\nimport unittest\n\n\nclass EnvironmentSmokeTest(unittest.TestCase):\n\n  def test_imports(self):\n    """"""End-to-end test of model_train script.""""""\n    # Test various imports work\n    # pylint: disable=unused-variable\n    # pylint: disable=g-import-not-at-top\n    import enum\n    import mock\n    import intervaltree\n    import contextlib2\n    # pylint: enable=unused-variable\n    # pylint: enable=g-import-not-at-top\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
deepvariant/environment_tests/protobuf_implementation_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Test that our protobuf implementation behaves as we\'d expect.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport unittest\n\nfrom google.protobuf.internal import api_implementation\n\n\nclass ProtobufImplementationTest(unittest.TestCase):\n  """"""Checks that our protobufs have the properties we expect.""""""\n\n  def test_protobuf_uses_fast_cpp(self):\n    """"""Checks that we are using the fast cpp version of python protobufs.""""""\n    self.assertEqual(api_implementation.Type(), \'cpp\')\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
deepvariant/labeler/customized_classes_labeler.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""variant_labeler for DeepVariant.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom deepvariant.labeler import positional_labeler\nfrom deepvariant.labeler import variant_labeler\nfrom third_party.nucleus.util import struct_utils\n\n\n# ---------------------------------------------------------------------------\n# CustomizedClassesVariantLabel\n#\nclass CustomizedClassesVariantLabel(variant_labeler.VariantLabel):\n  """"""Dataclass containing information about a label assigned to a variant.\n\n  Attributes:\n    is_confident: bool. True if we could confidently assign a label to this\n      variant, False otherwise.\n    variant: nucleus.protos.Variant proto that we assigned a label for.\n    class_status: string. One of the keys in classes_dict\n  """"""\n\n  classes_dict = None\n  info_field_name = None\n\n  def __init__(self, is_confident, variant, truth_variant, classes_list,\n               info_field_name):\n    self.info_field_name = info_field_name\n    self.classes_dict = {k: v for v, k in enumerate(classes_list.split(\',\'))}\n    self.is_confident = is_confident\n    self.variant = variant\n    self.truth_variant = truth_variant\n\n  def label_for_alt_alleles(self, alt_alleles_indices):\n    """"""Computes the label value for an example.\n\n    This function computes the TensorFlow label value (0, 1, 2, .. N-1) we train\n    DeepVariant to predict.\n    The `alt_alleles_indices` being passed in is from the candidates (not\n    truth), so they could still have multiple alts. If any of the alt alleles\n    matches the truth, we\'ll return the label of the truth.\n    redacted\n    Note that this function currently doesn\'t handle multi-allelic cases\n    correctly. For example it assumes `truth_alt` is the first one.\n\n    Args:\n      alt_alleles_indices: list[int]. A list of the alt_allele_indices.\n\n    Returns:\n      int >= 0. Label for the classes in `classes_dict`.\n    """"""\n    if not self.truth_variant:\n      return 0\n\n    if self.truth_variant.calls[0].genotype == [0, 0]:\n      return 0\n\n    # If the ref of the candidate and the truth doesn\'t match, return 0 (ref).\n    if self.truth_variant.reference_bases != self.variant.reference_bases:\n      return 0\n    true_class_status = self.get_class_status(self.truth_variant.info)\n    truth_alt = self.truth_variant.alternate_bases[0]\n    # Default is label 0. Usually reference.\n    label = 0\n    # Note that this logic below might not be the best when\n    # `alt_alleles_indices` is a composite one, like [0, 1]. For now we\'ll\n    # return the corresponding label if any of them matches truth_alt.\n    for ind in alt_alleles_indices:\n      if self.variant.alternate_bases[ind] == truth_alt:\n        # allele in called variant is the same as truth_alt\n        label = self.classes_dict[true_class_status]\n\n    return label\n\n  def get_class_status(self, info_field):\n    """"""Extract class status from nucleus.protos.Variant.info.\n\n    Args:\n      info_field: INFO field of nucleus.protos.Variant proto to extract the\n        classes status from. Must contain `info_field_name` field which is set\n        to one of self.classes_dict.keys().\n\n    Returns:\n      string. Class status. Has to be one of the keys of `classes_dict`.\n\n    Raises:\n      ValueError: if type is missing in info_field\n      ValueError: if type is not in self.classes_dict.keys()\n    """"""\n\n    if self.info_field_name not in info_field.keys():\n      raise ValueError(\'Cannot create class labels: \' +\n                       \'VCF file does not contain INFO/{} field\'.format(\n                           self.info_field_name))\n\n    class_status = struct_utils.get_string_field(info_field,\n                                                 self.info_field_name, True)\n\n    if class_status not in self.classes_dict.keys():\n      raise ValueError(\'class_status status unknown: {}. \'\n                       \'Known status: {}\'.format(class_status,\n                                                 self.classes_dict.keys()))\n    return class_status\n\n\n# ---------------------------------------------------------------------------\n# CustomizedClassesVariantLabeler\n#\nclass CustomizedClassesVariantLabeler(\n    positional_labeler.PositionalVariantLabeler):\n  """"""Extracts the class of the variant (possible values are keys in\n\n     `classes_dict`) from INFO/`info_field_name` field in VCF file.\n  """"""\n\n  def __init__(self, truth_vcf_reader, confident_regions, classes_list,\n               info_field_name):\n    """"""Creates a new CustomizedClassesVariantLabeler.\n\n    Args:\n      truth_vcf_reader: a VcfReader object that points to our truth variant set.\n      confident_regions: A RangeSet containing all of the confidently called\n        regions. A variant that falls outside of one of these regions will be\n        receive a special not-confident marker.\n      classes_list: A common-separated string of classes.\n      info_field_name: the name in INFO field where we should get the customized\n        field from.\n\n    Raises:\n      ValueError: if vcf_reader is None.\n    """"""\n    super(CustomizedClassesVariantLabeler, self).__init__(\n        truth_vcf_reader=truth_vcf_reader, confident_regions=confident_regions)\n    self.classes_list = classes_list\n    self.info_field_name = info_field_name\n\n  def label_variants(self, variants, region=None):\n    """"""Gets label information for each variant in variants.\n\n    This is the primary API for assigning labels to variants. This function\n    takes and iterable of variants and yield a VariantLabel object for each\n    variant. The VariantLabel can be used to determine the variant type label\n    for each variant suitable for training a DeepVariant model. The API accepts\n    an iterable of Variants because, in the general case, the labeling of\n    variants aren\'t independent, in that the label assigned to one variant may\n    impact the label we assign to a nearby variant.\n\n    Args:\n      variants: iterable[nucleus.protos.Variant]: An iterable of variants to\n        label. The variants should be in coordinate-sorted order and all on the\n        same chromosome.\n      region: A nucleus.genomics.v1.Range object specifying the region over\n        which we are labeling variants. This should span at least the span of\n        variants, but may be larger. Statistics about the labeling will be\n        computed over region.\n\n    Yields:\n      A VariantLabel object for each variant in variants, in order.\n    """"""\n    for variant in variants:\n      is_confident, truth_variant = self._match(variant)\n\n      yield CustomizedClassesVariantLabel(\n          is_confident=is_confident,\n          variant=variant,\n          truth_variant=truth_variant,\n          classes_list=self.classes_list,\n          info_field_name=self.info_field_name)\n'"
deepvariant/labeler/customized_classes_labeler_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .variant_labeler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport collections\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\nfrom third_party.nucleus.util import variant_utils\nfrom deepvariant import testdata\nfrom deepvariant.labeler import customized_classes_labeler\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import vcf_constants\n\n\ndef setUpModule():\n  testdata.init()\n\n\nFakeVCFObject = collections.namedtuple(\'FakeVCFObject\', [\'field_access_cache\'])\nCUSTOMIZED_INFO_FIELD_NAME = \'type\'\nCUSTOMIZED_CLASSES_LIST = \'ref,class1,class2\'\n\n\ndef _add_class_to_variant(variant, class_status):\n  if class_status is None:\n    return variant\n\n  header = variants_pb2.VcfHeader(infos=[\n      variants_pb2.VcfInfo(\n          id=CUSTOMIZED_INFO_FIELD_NAME,\n          number=\'A\',\n          type=vcf_constants.STRING_TYPE,\n          description=\'Customized class label for the variant.\')\n  ])\n  my_cache = vcf.VcfHeaderCache(header)\n  vcf_object = FakeVCFObject(field_access_cache=my_cache)\n  variant_utils.set_info(\n      variant, CUSTOMIZED_INFO_FIELD_NAME, class_status, vcf_object=vcf_object)\n  return variant\n\n\nclass CustomizedClassesVariantLabelerTest(parameterized.TestCase):\n\n  # Confident variants: SNP, deletion, and multi-allelic.\n  snp_class1 = _add_class_to_variant(\n      test_utils.make_variant(start=10, alleles=[\'A\', \'C\'], gt=[0, 1]),\n      class_status=\'class1\')\n\n  snp_class2 = _add_class_to_variant(\n      test_utils.make_variant(start=20, alleles=[\'ACG\', \'A\'], gt=[1, 1]),\n      class_status=\'class2\')\n\n  multiallelic = _add_class_to_variant(\n      test_utils.make_variant(\n          start=30, alleles=[\'ACT\', \'ACTGT\', \'A\'], gt=[1, 2]),\n      class_status=\'class2\')\n\n  # Outside our confident regions.\n  non_confident = _add_class_to_variant(\n      test_utils.make_variant(start=200, alleles=[\'A\', \'C\'], gt=[0, 1]),\n      class_status=\'class1\')\n\n  filtered = _add_class_to_variant(\n      test_utils.make_variant(start=40, filters=\'FAILED\', gt=[0, 1]),\n      class_status=\'class1\')\n\n  # redacted\n  # no_class_status\n  # invalid_class_status\n  # (Value error should be produced in both cases)\n\n  variants = [snp_class1, snp_class2, multiallelic, non_confident, filtered]\n\n  def _make_labeler(self, variants, confident_regions):\n    return customized_classes_labeler.CustomizedClassesVariantLabeler(\n        truth_vcf_reader=vcf.InMemoryVcfReader(variants),\n        confident_regions=confident_regions,\n        classes_list=CUSTOMIZED_CLASSES_LIST,\n        info_field_name=CUSTOMIZED_INFO_FIELD_NAME)\n\n  @parameterized.parameters(\n      # Simple tests: we get back our matching variants in the confident regions\n      dict(\n          candidate=snp_class1,\n          expected_confident=True,\n          expected_truth=snp_class1,\n          expected_label=1),\n      dict(\n          candidate=snp_class2,\n          expected_confident=True,\n          expected_truth=snp_class2,\n          expected_label=2),\n      # For multiallelic variants, we default to class 0.\n      dict(\n          candidate=multiallelic,\n          expected_confident=True,\n          expected_truth=multiallelic,\n          expected_label=2),\n\n      # Test the behavior outside of our confident regions.\n      # If we provide a variant outside the confident regions (non_confident) we\n      # don\'t get back any expected_truth variants.\n      dict(\n          candidate=non_confident,\n          expected_confident=False,\n          expected_truth=None,\n          expected_label=0),\n      # No matching variant, so we get a None as well as False.\n      dict(\n          candidate=test_utils.make_variant(start=300, alleles=[\'A\', \'C\']),\n          expected_confident=False,\n          expected_truth=None,\n          expected_label=0),\n\n      # This variant doesn\'t have any match but we\'re confident in it.\n      dict(\n          candidate=test_utils.make_variant(start=15, alleles=[\'C\', \'A\']),\n          expected_confident=True,\n          expected_label=0,\n          expected_truth=test_utils.make_variant(\n              start=15, alleles=[\'C\', \'A\'], gt=[0, 0])),\n\n      # These variant start at our SNP but has a different allele. We are\n      # confident and we get back the true snp variant.\n      # However, we are on a different allele, so its status is unknown.\n      # redacted\n      dict(\n          candidate=test_utils.make_variant(\n              start=snp_class1.start, alleles=[\'A\', \'G\']),\n          expected_confident=True,\n          expected_label=0,\n          expected_truth=snp_class1),\n      # redacted\n      # If the alleles don\'t match, return class 0?\n      dict(\n          candidate=test_utils.make_variant(\n              start=snp_class1.start, alleles=[\'AC\', \'C\']),\n          expected_confident=True,\n          expected_label=0,\n          expected_truth=snp_class1),\n      dict(\n          candidate=test_utils.make_variant(\n              start=snp_class1.start, alleles=[\'A\', \'CA\']),\n          expected_confident=True,\n          expected_label=0,\n          expected_truth=snp_class1),\n      # Checks that we don\'t match against the filtered truth variant in our\n      # database. This means that we return not the filtered variant but one\n      # with a (0, 0) genotype.\n      dict(\n          candidate=test_utils.make_variant(start=filtered.start),\n          expected_confident=True,\n          expected_label=0,\n          expected_truth=test_utils.make_variant(\n              start=filtered.start, gt=(0, 0))),\n      # These variant start at our SNP but has a different first alt allele \'G\'.\n      # The second alt (\'C\') matches snp_class1, so we still got back the\n      # expected_label.\n      dict(\n          candidate=test_utils.make_variant(\n              start=snp_class1.start, alleles=[\'A\', \'G\', \'C\']),\n          expected_confident=True,\n          expected_label=1,\n          expected_truth=snp_class1,\n          variant_alt_alleles_indices=[1]),\n      # And, even if the variant_alt_alleles_indices is a composite one ([0,1]),\n      # We still label it as long as one of them matches the truth_alt.\n      dict(\n          candidate=test_utils.make_variant(\n              start=snp_class1.start, alleles=[\'A\', \'G\', \'C\']),\n          expected_confident=True,\n          expected_label=1,\n          expected_truth=snp_class1,\n          variant_alt_alleles_indices=[0, 1]),\n      # ... But we won\'t label it if the alt_allele_indices does not cover the\n      # truth alt.\n      dict(\n          candidate=test_utils.make_variant(\n              start=snp_class1.start, alleles=[\'A\', \'G\', \'C\']),\n          expected_confident=True,\n          expected_label=0,\n          expected_truth=snp_class1,\n          variant_alt_alleles_indices=[0]),\n  )\n  def test_label_variants(self,\n                          candidate,\n                          expected_confident,\n                          expected_truth,\n                          expected_label=None,\n                          variant_alt_alleles_indices=None):\n    if variant_alt_alleles_indices is None:\n      variant_alt_alleles_indices = [0]\n    labeler = self._make_labeler(\n        self.variants,\n        ranges.RangeSet(\n            [ranges.make_range(self.snp_class1.reference_name, 10, 100)]))\n\n    # Call _match so we can compare our expected truth with the actual one.\n    is_confident, truth_variant = labeler._match(candidate)\n    self.assertEqual(expected_truth, truth_variant)\n    self.assertEqual(is_confident, expected_confident)\n\n    # Now call label_variants to exercise the higher-level API.\n    classes_dict = (\n        customized_classes_labeler.CustomizedClassesVariantLabel.classes_dict)\n    if expected_label is None and expected_truth is not None:\n      expected_class_str = expected_truth.info[\n          customized_classes_labeler.CustomizedClassesVariantLabel\n          .info_field_name].values[0].string_value\n      expected_label = classes_dict[expected_class_str]\n\n    labels = list(labeler.label_variants([candidate]))\n    self.assertLen(labels, 1)\n    self.assertEqual(candidate, labels[0].variant)\n    self.assertEqual(expected_confident, labels[0].is_confident)\n    self.assertEqual(\n        expected_label,\n        labels[0].label_for_alt_alleles(variant_alt_alleles_indices))\n\n  def test_match_selects_variant_by_start(self):\n    # Tests that match() selects the variant at the same start even if that\n    # variant doesn\'t have the same alleles at candidate and there\'s an\n    # overlapping with the same alleles.\n    overlapping = [\n        test_utils.make_variant(start=20, alleles=[\'CC\', \'A\'], gt=[1, 1]),\n        test_utils.make_variant(start=21, alleles=[\'AAA\', \'A\'], gt=[0, 1]),\n        test_utils.make_variant(start=22, alleles=[\'AA\', \'A\'], gt=[1, 1]),\n    ]\n    candidate = test_utils.make_variant(start=21, alleles=[\'CC\', \'A\'])\n\n    labeler = self._make_labeler(\n        overlapping,\n        ranges.RangeSet(\n            [ranges.make_range(overlapping[0].reference_name, 0, 100)]))\n    is_confident, truth_variant = labeler._match(candidate)\n    self.assertEqual(is_confident, True)\n    self.assertEqual(truth_variant, overlapping[1])\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/labeler/haplotype_labeler.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Haplotype-based labeling algorithm for DeepVariant.\n\nThis module provides a haplotype-aware labeling algorithm. This is a more\nsophisticated approach to labeling that allows for slight representational\ndifferences between candidate and truth variant sets. See:\n\nhttps://github.com/ga4gh/benchmarking-tools\nhttps://www.biorxiv.org/content/early/2018/03/15/270157\n\nfor an introduction to the concepts and why this is important.\n\n\nThe module is implemented in two big pieces of functionality:\n\nfind_best_matching_haplotypes(candidates, truths) provides an function that\naccepts a list of candidate variants and a list of truth variants with known\ngenotypes and finds an assignment of genotypes for candidates and truth that\nresults in the same two haplotype sequences in the region. Since the truth\nvariants have known genotypes, the search there is constrained to those\ngenotypes and their potential set of false negatives (e.g., if truth is (0, 1)\nwe may have missed the variant so we consider both (0, 1) and (0, 0)). The\nreturned value is a HaplotypeMatch object describing the genotype assignments\nfor candidates and truth.\n\nHaplotypeLabeler implements the variant_labeler.VariantLabeler API by calling\nour find_best_matching_haplotypes function to get teh HaplotypeMatch objects and\nreturning variant_labeler.VariantLabel objects for each candidate variant.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport heapq\nimport itertools\n\nfrom absl import logging\nimport enum\n\nfrom third_party.nucleus.io import fasta\nfrom third_party.nucleus.util import ranges\nfrom third_party.nucleus.util import variant_utils\nfrom third_party.nucleus.util import variantcall_utils\nfrom deepvariant.labeler import variant_labeler\nfrom deepvariant.protos import deepvariant_pb2\n\nVariantAndGenotypes = collections.namedtuple(\'VariantAndGenotype\',\n                                             [\'variant\', \'genotypes\'])\n\n# The default maximum size of a variant group we\'ll try to label. See\n# the HaplotypeLabeler class for more information.\n_MAX_GROUP_SIZE = 8\n\n# The default maximum distance between subsequent variants within a group. See\n# the HaplotypeLabeler class for more information.\n_MAX_SEPARATION_WITHIN_VARIANT_GROUP = 30\n\n# True we will generate enough information into our logs to help debug bad\n# regions.\n_DEBUG_PRINTING_IS_ENABLED = False\n\n\nclass HaplotypeLabeler(variant_labeler.VariantLabeler):\n  """"""Haplotype-based variant labeler.""""""\n\n  def __init__(self,\n               truth_vcf_reader,\n               ref_reader,\n               confident_regions,\n               max_group_size=_MAX_GROUP_SIZE,\n               max_separation=_MAX_SEPARATION_WITHIN_VARIANT_GROUP):\n    """"""Creates a new HaplotypeVariantLabeler.\n\n    Args:\n      truth_vcf_reader: a VcfReader object that points to our truth variant set.\n      ref_reader: A FastaReader object we can use to get reference bases.\n      confident_regions: A RangeSet containing all of the confidently called\n        regions. A variant that falls outside of one of these regions will be\n        receive a special not-confident marker.\n      max_group_size: int >= 1. The maximum number of variants we\'ll attempt to\n        label together. Larger values increase the runtime of the algorithm.\n      max_separation: int >= 0. The maximum distance between variants within a\n        group. Sequential variants separated by more than this value will be\n        placed in separate groups for labeling.\n\n    Raises:\n      ValueError: if vcf_reader is None.\n    """"""\n    super(HaplotypeLabeler, self).__init__(\n        truth_vcf_reader=truth_vcf_reader, confident_regions=confident_regions)\n    if confident_regions is None:\n      raise ValueError(\'confident_regions cannot be None for HaplotypeLabeler.\')\n    self._ref_reader = ref_reader\n    self.max_group_size = max_group_size\n    self.max_separation = max_separation\n    self._metrics = deepvariant_pb2.LabelingMetrics()\n\n  def label_variants(self, variants, region):\n    # Grab our truth variants and group up variants + truth into small enough\n    # chunks that we can safely send them into our find_best_matching_haplotypes\n    # function.\n    grouped = group_variants(\n        candidates=list(variants),\n        truths=list(self._get_truth_variants(region)),\n        max_group_size=self.max_group_size,\n        max_separation=self.max_separation)\n\n    # Now loop over our grouped variants, labeling them, and yielding\n    # VariantLabel objects.\n    for candidates_group, truth_group in grouped:\n      assert len(candidates_group) <= self.max_group_size\n      assert len(truth_group) <= self.max_group_size\n\n      ref = self.make_labeler_ref(candidates_group, truth_group)\n      labeling = find_best_matching_haplotypes(candidates_group, truth_group,\n                                               ref)\n      if labeling is None:\n        # Note this test must be \'is None\' since label_variants can return an\n        # empty list.\n        raise ValueError(\'Failed to assign labels for variants\',\n                         candidates_group, truth_group, ref)\n\n      self._update_metrics(labeling)\n      for labeled in labeling.candidates_with_assigned_genotypes():\n        # This logic doesn\'t make a huge amount of sense when you are doing\n        # haplotype-based labeling. Currently we only say a variant is confident\n        # if it overlaps the confident regions, which is the baseline behavior.\n        # However, it may be useful to rethink how we establish a variant is\n        # confident, as the ""event"" may be within the confident regions but\n        # shifted outside due to differences in representational choices. Seems\n        # like another approach would be to assign confidence if it has a\n        # non-ref genotype (as we only consider confident truth variants) or if\n        # it overlaps the confident regions.\n        yield variant_labeler.VariantLabel(\n            is_confident=self._confident_regions.variant_overlaps(labeled),\n            genotype=tuple(labeled.calls[0].genotype),\n            variant=labeled)\n\n  @property\n  def metrics(self):\n    """"""Gets the LabelingMetrics proto tracking metrics for this labeler.""""""\n    return self._metrics\n\n  def _update_metrics(self, labeling):\n    """"""Update self._metrics with the HaplotypeMatch labeling results.\n\n    This function updates the LabelingMetrics information in self._metrics using\n    the labeling results in labeling.\n\n    Args:\n      labeling: HaplotypeMatch. The labeling information to use to update our\n        LabelingMetrics.\n    """"""\n\n    def _n_alts_by_genotype(gt):\n      """"""Returns the number of distinct alt alleles with non-zero genotype.""""""\n      return len({g for g in gt if g > 0})\n\n    def _is_hom_ref(gt):\n      """"""Are all genotypes in gt the reference alleles (i.e., == 0)?""""""\n      return all(g == 0 for g in gt)\n\n    def _has_alt_genotypes(gt):\n      """"""Is any genotype in gt a non-ref (> 0) genotype?""""""\n      return any(g > 0 for g in gt)\n\n    # Iterate over the truth variant and its associated original genotypes\n    # (those provided by the input VCF) and the assigned genotypes (i.e., the\n    # genotypes assigned to truth to make candidates and truth match haplotypes)\n    # and compute a few metric values.\n    for truth, original_gt, assigned_gt in zip(\n        labeling.truths, labeling.original_truth_genotypes,\n        labeling.truth_genotypes):\n      n_alts_original = _n_alts_by_genotype(original_gt)\n\n      self._metrics.n_truth_variant_sites += 1\n      self._metrics.n_truth_variant_alleles += n_alts_original\n      self._metrics.n_true_positive_sites += _has_alt_genotypes(assigned_gt)\n      self._metrics.n_false_negative_sites += _is_hom_ref(assigned_gt)\n\n      # If we have more than one alt allele in the original genotypes and the\n      # assigned genotypes imply more or more are missing then we\'ve got a\n      # multi-allelic truth variant with some missing alleles.\n      if (n_alts_original > 1 and\n          _n_alts_by_genotype(assigned_gt) < n_alts_original):\n        self._metrics.n_truth_multiallelics_sites_with_missed_alleles += 1\n\n      # Iterate over the original and assigned genotypes for the truth variants\n      # and count up the number of true positive alleles (i.e. original and\n      # assigned genotypes are non-ref) and false negative alleles (i.e.,\n      # original is non-ref but assigned is ref).\n      for og, ag in zip(original_gt, assigned_gt):\n        if og > 0:\n          if ag > 0:\n            self._metrics.n_true_positive_alleles += 1\n          elif ag == 0:\n            self._metrics.n_false_negative_alleles += 1\n\n    # Create a dict from the start of truth to the truth variant itself and its\n    # assigned genotypes. This is needed to compute site match counts below.\n    truth_by_pos = {\n        truth.start: (truth, gt)\n        for truth, gt in zip(labeling.truths, labeling.truth_genotypes)\n    }\n\n    # Iterate over the candidates and their assigned genotypes to compute\n    # the remaining metrics.\n    #\n    # Note that this counts all candidates, not just the ones in the confident\n    # regions of the genome. This seems like a reasonable first approach, but it\n    # may be necessary to restrict ourselves to only those overlapping the\n    # confident regions.\n    for candidate, genotype in zip(labeling.candidates,\n                                   labeling.candidate_genotypes):\n      # If candidate isn\'t confident, add it to the non_confident count and\n      # continue as the other metrics are only computed over confident\n      # candidates.\n      if not self._confident_regions.variant_overlaps(candidate):\n        self._metrics.n_non_confident_candidate_variant_sites += 1\n        continue\n\n      n_alt_alleles = len(candidate.alternate_bases)\n      self._metrics.n_candidate_variant_sites += 1\n      self._metrics.n_candidate_variant_alleles += n_alt_alleles\n      self._metrics.n_false_positive_sites += _is_hom_ref(genotype)\n      self._metrics.n_false_positive_alleles += (\n          n_alt_alleles - _n_alts_by_genotype(genotype))\n\n      # Use the truth_by_pos dict to determine which candidates occur at the\n      # same position as a truth variant. If there is one, grab it and its\n      # genotypes so we can compute metrics on exact position, allele, genotype\n      # matches. If not, update the number of inexact matches if our candidate\n      # is non-reference itself.\n      truth, assigned_gt = truth_by_pos.get(candidate.start, (None, None))\n      if truth:\n        self._metrics.n_exact_position_matches += 1\n        if sorted(candidate.alternate_bases) == sorted(truth.alternate_bases):\n          self._metrics.n_exact_position_and_allele_matches += 1\n          if sorted(genotype) == sorted(assigned_gt):\n            self._metrics.n_exact_position_and_allele_and_genotype_matches += 1\n      elif _has_alt_genotypes(genotype):\n        self._metrics.n_inexact_position_matches += 1\n\n  def make_labeler_ref(self, candidates, true_variants, bufsize=20):\n    all_variants = candidates + true_variants\n    contig = all_variants[0].reference_name\n    start = min(x.start for x in all_variants)\n    end = max(x.end for x in all_variants)\n    contig_nbp = self._ref_reader.contig(contig).n_bases\n    region = ranges.make_range(contig, max(start - 1, 0),\n                               min(end + bufsize, contig_nbp))\n    ref_bases = self._ref_reader.query(region)\n    return ReferenceRegion(ref_bases, start=region.start)\n\n\nclass ReferenceRegion(fasta.InMemoryFastaReader):\n  """"""Allows us to get bases from a cached reference interval.""""""\n\n  # We don\'t want to worry about the chromosome we are working on for code\n  # clarity, so we create an InMemoryFastaReader that has a single chromosome\n  # named _DUMMY_CHROM_NAME which allows us to provide a bases(start, end)\n  # function for convenient reading of bases.\n  _DUMMY_CHROM_NAME = \'*\'\n\n  def __init__(self, bases, start):\n    super(ReferenceRegion,\n          self).__init__([(self._DUMMY_CHROM_NAME, start, bases)])\n    self.start = start\n    self.end = start + len(bases)\n\n  def bases(self, start, end):\n    return self.query(ranges.make_range(self._DUMMY_CHROM_NAME, start, end))\n\n\n_CANDIDATE_MARKER = \'candidate\'\n_TRUTH_MARKER = \'truth\'\n_VariantToGroup = collections.namedtuple(\'_VariantToGroup\',\n                                         [\'start\', \'type\', \'variant\'])\n\n\ndef _raise_if_not_sorted_or_not_on_same_chromosome(variants):\n  """"""Raises a ValueError if variants isn\'t sorted on the same chromosome.""""""\n  if not variant_utils.variants_are_sorted(variants):\n    raise ValueError(\'Variants must be sorted\', variants)\n  for v in variants[1:]:\n    if variants[0].reference_name != v.reference_name:\n      raise ValueError(\n          \'Variants (v1={}, v2={}) not on the same chromosome\'.format(\n              v.reference_name, variants[0].reference_name))\n\n\ndef group_variants(candidates,\n                   truths,\n                   max_group_size=_MAX_GROUP_SIZE,\n                   max_separation=_MAX_SEPARATION_WITHIN_VARIANT_GROUP):\n  """"""Splits candidate and truth variants into smaller groups if necessary.\n\n  This function takes in a list of candidate and truth variants and splits up\n  those lists into groups that respect the requirements of the max_group_size\n  and max_separation arguments. This is necessary because the labeling algorithm\n  is very expensive as a function of the number of input variants, so to avoid\n  excessive runtime we break up our potentially large list of candidate and\n  truth variants into smaller groups (max number controlled by max_group_size)\n  based on a maximum distance allowed between the closest variants within the\n  group.\n\n  The current algorithm is a simple greedy one; we effectively merge the two\n  variant lists together, make groups greedily on that list until either the\n  maximum number of elements of a specific type (i.e., max_group_size of 2\n  implies we can have up to two candidate variants or truth variants within a\n  group) or we encounter a variant further away from the closest variant within\n  the current group than allowed by max_separation.\n\n  Args:\n    candidates: list[nucleus.proto.Variant]. A sorted list of candidate variants\n      on the same chromosome.\n    truths: list[nucleus.proto.Variant]. A sorted list of truth variants on the\n      same chromosome.\n    max_group_size: int >= 0. The maximum number of variants of a specific type\n      allowed within a group.\n    max_separation: int >= 0. The maximum distance, in basepairs, allowed\n      between the closest variants within a group.\n\n  Returns:\n    A list of grouped variants in 2-tuples, such as:\n\n      [(candidates1, truth_variants1), ...]\n\n    where each tuple contains the candidate and truth variants for that group.\n\n  Raises:\n    ValueError: if any of the inputs are malformed.\n  """"""\n  if max_group_size < 0:\n    raise ValueError(\'max_group_size={} must be >= 0\'.format(max_group_size))\n  if max_separation < 0:\n    raise ValueError(\'max_separation={} must be >= 0\'.format(max_separation))\n  _raise_if_not_sorted_or_not_on_same_chromosome(candidates)\n  _raise_if_not_sorted_or_not_on_same_chromosome(truths)\n\n  def to_grouped_variants(variants, candidate_type):\n    """"""Converts a Variant proto to a _VariantToGroup tuple.""""""\n    return [_VariantToGroup(v.start, candidate_type, v) for v in variants]\n\n  def _of_type(group, required_type):\n    """"""Selects a list of Variant protos from list[_VariantToGroup] of type.""""""\n    return [gv.variant for gv in group if gv.type == required_type]\n\n  def _split_grouped_variants(group):\n    """"""Splits a list of _VariantToGroup into candidate and truth variants.""""""\n    return _of_type(group, _CANDIDATE_MARKER), _of_type(group, _TRUTH_MARKER)\n\n  def _include_in_variant_group(group, group_variant):\n    if not group:\n      return True\n    n_of_type = sum(1 for g in group if g.type == group_variant.type)\n    if n_of_type >= max_group_size:\n      return False\n    else:\n      return any(\n          group_variant.variant.start - g.variant.end + 1 <= max_separation\n          for g in group)\n\n  # Convert our lists of variant protos into _VariantToGroup tuples compatible\n  # with the heapq API (sorts on tuples), so we get a single iterable of\n  # variants with marked types and sorted by start position (first element of\n  # each tuple).\n  groupable_variants = heapq.merge(\n      to_grouped_variants(candidates, _CANDIDATE_MARKER),\n      to_grouped_variants(truths, _TRUTH_MARKER))\n\n  # Go through out groupable_variants and split them up into groups according to\n  # the predicate _include_in_variant_group.\n  groups = []\n  current_group = []\n  for group_variant in groupable_variants:\n    if _include_in_variant_group(current_group, group_variant):\n      current_group.append(group_variant)\n    else:\n      groups.append(current_group)\n      current_group = [group_variant]\n  if current_group:\n    groups.append(current_group)\n\n  # Finally split up each group into candidates and truths.\n  return [_split_grouped_variants(g) for g in groups]\n\n\ndef with_false_negative_genotypes(gt):\n  """"""Returns a set of genotypes that includes false negatives.\n\n  This function takes a concrete genotype for a Variant, such as (0, 1), and\n  returns a set of genotypes that includes gt as well as all possible genotypes\n  consistent with some of the alleles in gt being missed. For example, here are\n  a few outputs for help understand what this means:\n\n    input genotype (gt) => returned set of genotypes\n    ------------------------------------------------\n\n    # A hom-ref genotype doesn\'t have any alleles to miss.\n    (0, 0)  => {(0, 0)}\n\n    # Might miss the 1 allele, or not.\n    (0, 1)  => {(0, 0), (0, 1)}\n\n    # We could miss one, or both of the 1 alleles.\n    (1, 1)  => {(0, 0), (0, 1), (1, 1)}\n\n    # Multi-allelics are more complex, in that we could miss either the 1 or the\n    # 2 allele, or both.\n    (1, 2)  => {(0, 0), (0, 1), (0, 2), (1, 2)}\n\n  Args:\n    gt: iterable[int]: A genotype for a Variant, such as [0, 1] or [1, 1].\n\n  Returns:\n    A set of tuples containing diploid genotypes.\n  """"""\n  alts = set(gt) - {0}\n  return {(0, 0), tuple(gt)} | {(0, alt) for alt in alts}\n\n\nclass ImpossibleHaplotype(Exception):\n  """"""Indicates that an impossible haplotype configuration has been observed.""""""\n  pass\n\n\ndef enumerate_all_possible_haplotypes(variants, ref, enumeration_type):\n  """"""Yields all possible haplotype/genotype combinations for variants.\n\n  Args:\n    variants: list[nucleus.protos.Variant]. A list of candidate variants, in\n      coordinate-sorted order, all on the same chromosome.\n    ref: ReferenceRegion. Used to get reference bases for variants. Must cover\n      at least the span of the variants.\n    enumeration_type: EnumerationType enum value. What kind of enumeration do we\n      want to do? Can be either CANDIDATES or TRUTH.\n\n  Yields:\n    2-tuple of haplotypes and genotypes. Haplotypes is a set of either one or\n    two strings, where each string is a haplotype (i.e., a series of bases)\n    generated by the genotypes assigned to each variant. genotypes is a list of\n    genotype tuples, in the same order as variants, indicating the genotype\n    assignment for each variant. These genotypes are phased, so [(0, 1), (0, 1)]\n    is not the same as [(0, 1), (1, 0)].\n  """"""\n\n  def create_haplotypes_recursive(variants_and_genotypes, last_pos):\n    """"""Recursive driver to enumerate all haplotypes.""""""\n    if not variants_and_genotypes:\n      yield {ref.bases(last_pos, ref.end)} if last_pos != ref.end else {\'\'}\n    else:\n      group, remaining = split_independent_variants(variants_and_genotypes)\n      group_haplotypes, next_pos = phased_genotypes_to_haplotypes(\n          group, last_pos, ref)\n      prefix_haplotypes = list(all_diploid_haplotypes(group, group_haplotypes))\n\n      if not prefix_haplotypes:\n        # prefix_haplotypes can be empty when group contains incompatible\n        # variants making it impossible to construct any haplotypes for group.\n        # For example, if group is:\n        #   variant(start=6, alleles=(""AAT"", ""A""), genotype=(0, 1))\n        #   variant(start=7, alleles=(""AT"", ""T""), genotype=(1, 1))\n        # prefix_haplotypes will be empty because there\'s no way to construct\n        # the haplotype where the variant@6 has a 1 genotype since it is\n        # deleting away bases that overlap the variant@7 which has a genotype of\n        # (1, 1), meaning it *has* to be present in some haplotype. In this\n        # situation we raise a ImpossibleHaplotype exception, which is caught\n        # in the outer loop, allowing us to bail out of the search ASAP.\n        raise ImpossibleHaplotype\n\n      for haplotypes in create_haplotypes_recursive(remaining, next_pos):\n        for result in extend_haplotypes(prefix_haplotypes, haplotypes):\n          yield result\n\n  def create_haplotypes(variants_and_genotypes, last_pos):\n    try:\n      for r in create_haplotypes_recursive(variants_and_genotypes, last_pos):\n        yield r\n    except ImpossibleHaplotype:\n      # See comment in create_haplotypes_recursive for more information, but in\n      # this case we simply `pass`, as we cannot construct any valid haplotypes.\n      pass\n\n  genotype_options = genotype_options_for_variants(variants, enumeration_type)\n  for genotypes in itertools.product(*genotype_options):\n    paired = [VariantAndGenotypes(v, g) for v, g in zip(variants, genotypes)]\n    for haplotypes in create_haplotypes(paired, ref.start):\n      yield haplotypes, genotypes\n\n\ndef all_diploid_haplotypes(variants_and_genotypes, genotypes2haplotype):\n  """"""Returns all diploid haplotypes for variants given their genotypes.""""""\n\n  def complement_haploid_genotype(haploid_genotype, genotypes):\n    assert len(haploid_genotype) == len(genotypes)\n    return tuple(g1[1] if hg1 == g1[0] and len(g1) == 2 else g1[0]\n                 for hg1, g1 in zip(haploid_genotype, genotypes))\n\n  genotypes = [vg.genotypes for vg in variants_and_genotypes]\n  generated_already = set()\n  for haploid_genotype, haplotype in genotypes2haplotype.items():\n    complement = complement_haploid_genotype(haploid_genotype, genotypes)\n    complement_haplotype = genotypes2haplotype.get(complement, None)\n    if complement_haplotype is not None and complement not in generated_already:\n      generated_already.add(haploid_genotype)\n      yield {haplotype, complement_haplotype}\n\n\nclass EnumerationType(enum.Enum):\n  """"""Enumeration type indicating how we should explore genotype configurations.\n\n  See genotype_options_for_variants for more information.\n  """"""\n  # This enumeration produces all possible genotype combinations for a variant.\n  CANDIDATES = 1\n  # This enumeration type will produce all combinations of the provided genotype\n  # for the variant with genotypes that allow for one or more of the\n  # non-reference genotypes to be missed.\n  TRUTH = 2\n  # This enumeration type will only produce a single (0, 0) genotype option for\n  # each variant.\n  ONLY_HOM_REF = 3\n\n\ndef genotype_options_for_variants(variants, enumeration_type):\n  """"""Returns a list of sets of possible genotypes for each variant in variants.\n\n  This function takes a list of variants and enumeration_type and produces a\n  list of possible genotypes for each variant in order.\n\n  If enumeration_type is ONLY_HOM_REF, then we return a singleton set for each\n  variant containing only the hom-ref genotype (0, 0). If enumeration_type is\n  TRUTH, then each variant must have an associated genotype field values, say\n  (A, B), and we return the set genotype as well as all possible false negative\n  genotypes. In our example, this means we\'d return {(A, B), (0, A), (0, B),\n  (0, 0)} as we could miss either the A, the B, or both alleles. If the\n  enumeration_type is CANDIDATES, we don\'t require the Variant protos to have\n  existing genotype field values and instead enumerate all possible unphased\n  genotypes for each variant given its alternative alleles of each variant. For\n  example, if we have a Variant with alleles = \'A\' and \'C\', we would return the\n  three possible diploid genotypes {(0, 0), (0, 1), (1, 1)}.\n\n  Args:\n    variants: List[nucleus.protos.Variant]. A list of Variant protos to provide\n      genotype options for. Some enumeration types may require the protos to\n      have existing genotypes in their calls[] subfield.\n    enumeration_type: EnumerationType. The kind of genotypes we want to explore\n      for each variant.\n\n  Returns:\n    A list of sets with the same length and ""order"" as variants. Each set\n    contains one or more diploid genotype tuples [e.g., (0, 1)] that\n    collectively represent the possible genotypes we need to explore.\n\n  Raises:\n    ValueError: if enumeration_type isn\'t one of the valid options.\n  """"""\n  if enumeration_type == EnumerationType.TRUTH:\n    return [\n        with_false_negative_genotypes(x) for x in _variant_genotypes(variants)\n    ]\n  elif enumeration_type == EnumerationType.CANDIDATES:\n    return [{(i, j)\n             for i, j, _, _ in variant_utils.genotype_ordering_in_likelihoods(v)\n            }\n            for v in variants]\n  elif enumeration_type == EnumerationType.ONLY_HOM_REF:\n    return [{(0, 0)}] * len(variants)\n  else:\n    raise ValueError(\'Unexpected EnumerationType\', enumeration_type)\n\n\ndef split_independent_variants(variants_and_genotypes):\n  """"""Splits variants_and_genotypes into an overlapping group and remaining.""""""\n  if not variants_and_genotypes:\n    raise ValueError(\'Expected at least one value in variants_and_genotypes\')\n\n  overlaps = [variants_and_genotypes[0]]\n  for i in range(1, len(variants_and_genotypes)):\n    vgi = variants_and_genotypes[i].variant\n    if any(variant_utils.variants_overlap(vg.variant, vgi) for vg in overlaps):\n      overlaps.append(variants_and_genotypes[i])\n    else:\n      return overlaps, variants_and_genotypes[i:]\n  return overlaps, []\n\n\ndef extend_haplotypes(prefix_haplotypes_list, haplotypes):\n  """"""Yields all diploid combinations of prefix_haplotypes_list x haplotypes.\n\n  Args:\n    prefix_haplotypes_list: list[set[string]]: prefix_haplotypes_list contains a\n      list of set[string], which are just like haplotypes (i.e., contains 1 or 2\n      strings), that collectively represent all possible prefixes of haplotypes.\n    haplotypes: set[string]. A set containing 1 or 2 haplotype strings. So it\n      looks like {h} or {h1, h2}.\n\n  Yields:\n    A series of set[string], each containing 1 or 2 haplotype strings.\n\n  Raises:\n    ValueError: if any of the arguments are invalid.\n  """"""\n  if not prefix_haplotypes_list:\n    raise ValueError(\'prefix_haplotypes_list cannot be empty\')\n  if len(haplotypes) not in {1, 2}:\n    raise ValueError(\'haplotypes must have exactly 1 or 2 elements\', haplotypes)\n\n  for prefix_haplotypes in prefix_haplotypes_list:\n    if len(prefix_haplotypes) == 1:\n      f, = prefix_haplotypes\n      yield {f + h for h in haplotypes}\n    else:\n      f1, f2 = prefix_haplotypes\n      if len(haplotypes) == 1:\n        h, = haplotypes\n        yield {f1 + h, f2 + h}\n      else:\n        h1, h2 = haplotypes\n        yield {f1 + h1, f2 + h2}\n        yield {f1 + h2, f2 + h1}\n\n\ndef phased_genotypes_to_haplotypes(variants_and_genotypes, start, ref):\n  """"""Returns a map from phased genotypes => haplotype sequences.\n\n  This function creates a map from all possible haploid genotypes of the\n  genotypes in variants_and_genotypes to their corresponding haplotype sequences\n  implied by the variants, ref, start, and their genotypes. This map can be used\n  to efficiently look up the haplotype sequence for any haploid genotype.\n\n  Args:\n    variants_and_genotypes: list[VariantAndGenotypes]. The variants and\n      associated genotypes to use to build the dictionary.\n    start: int >= 0. The position on the genome to start constructing our\n      haplotypes at.\n    ref: ReferenceRegion. Object containing the reference genome bases we use to\n      construct our haplotypes.\n\n  Returns:\n    A 2-tuple. The first element is a dictionary[tuple, string], where each key\n    is a phased haploid genotype and its value is the haplotype sequence implied\n    by that genotype given the variants and the reference genome. The second\n    position is the ending position of the haplotype on the reference genome.\n  """"""\n  genotypes_to_haplotypes = {}\n  genotypes = [vg.genotypes for vg in variants_and_genotypes]\n  variants = [vg.variant for vg in variants_and_genotypes]\n  all_haploid_genotypes = sorted(set(itertools.product(*genotypes)))\n  end = max(v.end for v in variants)\n  for phased in all_haploid_genotypes:\n    haplotype = build_haplotype(variants, phased, ref, start, end)\n    if haplotype:\n      genotypes_to_haplotypes[phased] = haplotype\n  return genotypes_to_haplotypes, end\n\n\ndef build_haplotype(variants, allele_indices, ref, ref_start, ref_end):\n  """"""Builds the haplotype string from variants and its phased gneotypes.\n\n  This function takes a list of variants and associated phased genotypes and\n  constructs the haplotype sequence implied by variants and its genotypes. For\n  example, suppose we have two variants:\n\n    ref: CAGC where the first base (C) is at position 10.\n    chr20:10 A=>C\n    chr20:11 G=>T\n    allele_indices: [0, 1]\n\n  We would receive arguments here of a list of two variants and a list of the\n  allele_indices [0, 1]. We now look up which base is implied for the variant\n  (e.g., [0, 1] takes the reference bases from variant 1 at 10 and then the\n  first alternate allele of variant 2 at 11). If ref_start is 9, we would then\n  construct the haplotype as:\n\n  haplotype is \'CATC\' derived as follows:\n    \'C\' [ref_prefix, since ref_start=]\n    +\n    \'A\' (variant1 has a genotype of 0)\n    +\n    \'T\' (variant2 has a genotype of 1)\n    +\n    \'C\' [ref_postfix if ref_end == 13]\n    = \'CATC\'\n\n  Args:\n    variants: list[nucleus.protos.Variant]: The variants to use in constructing\n      the haplotype.\n    allele_indices: list[int]: The list of allele_indexes (where 0 means\n      reference_bases and > 0 implies alternative bases, following the\n      VariantCall genotypes semantics).\n    ref: ReferenceRegion. Used to get the reference bases.\n    ref_start: int >= 0. The first position (zero-indexed, inclusive) in the\n      genome where we want to start constructing our haplotype.\n    ref_end: int >= 0 and > ref_start. The last position (zero-indexed,\n      exclusive) in the genome where we want to end constructing our haplotype.\n\n  Returns:\n    A string containing the haplotype bases, or None, if any variant starts\n    before ref_start and has a non-reference genotype.\n\n  Raises:\n    ValueError: If any of the input arguments are malformed or otherwise violate\n    the assumptions of this algorithm.\n  """"""\n  if len(variants) != len(allele_indices):\n    raise ValueError(\n        \'Expected the same number of variants {} as allele_indices {}\'.format(\n            len(variants), len(allele_indices)))\n  if ref_start < 0 or ref_start >= ref_end:\n    raise ValueError(\'expected ref_start {} < ref_end {}\'.format(\n        ref_start, ref_end))\n\n  parts = []\n  position = ref_start\n  for variant, allele_index in zip(variants, allele_indices):\n    if variant.start < position:\n      if allele_index != 0:\n        return None\n    else:\n      ref_prefix = ref.bases(position, variant.start)\n      allele = _allele_from_index(variant, allele_index)\n      if allele_index == 0:\n        # Update our position variable to be the next reference base we want to\n        # use when further constructing our haplotype string. If we are using\n        # the reference base, we start our position at the base after variant\n        # start, whereas if we are using a non-reference base we use the\n        # variant.end.\n        #\n        # This special-case is needed to handle deletion alleles properly. If we\n        # have a deletion (e.g., AA => A with start = 10 and end = 12) then we\n        # only want to skip to position 12 for the next reference bases if we\n        # have have the deletion, otherwise we\'d miss the second \'A\' base which\n        # is really there (the variant isn\'t present, after all). Another\n        # consequence of this choice we only want to add the first base of the\n        # reference allele, not the whole string, since this would append all of\n        # deletion bases inappropriately to our haplotype.\n        allele = allele[0]\n        position = variant.start + 1\n      else:\n        position = variant.end\n      parts.append(ref_prefix + allele)\n\n  # We have some bases left to add between the position of our last variant\n  # and the ref_end, so append those now.\n  if position < ref_end:\n    parts.append(ref.bases(position, ref_end))\n\n  return \'\'.join(parts)\n\n\nclass HaplotypeMatch(object):\n  """"""DataClass holding information about a matching of variants.\n\n  The haplotype labeling algorithm, at its core, searches for an assignment of\n  genotypes to the candidate variants and the truth variants that result in the\n  same diploid haplotype sequences, which we call a match. All of the\n  information in that previous sentence is captured here as class attributes:\n\n  Attributes:\n    haplotypes: list[str]. The sorted list of haplotypes produced by this match.\n    candidates: list[nucleus.proto.Variant]: The list of candidate variants.\n    truths: list[nucleus.proto.Variant]: The list of true variants.\n    candidate_genotypes: list[tuple]: The genotypes that, when assigned to the\n      candidate variants, give rise to haplotypes.\n    truth_genotypes: list[tuple]: The genotypes that, when assigned to the known\n      variants, give rise to haplotypes.\n  """"""\n\n  def __init__(self, haplotypes, candidates, candidate_genotypes, truths,\n               truth_genotypes):\n    if len(haplotypes) not in {1, 2}:\n      raise ValueError(\'Expected 1 or 2 haplotypes but got\', haplotypes)\n    if len(candidates) != len(candidate_genotypes):\n      raise ValueError(\n          \'candidates and candidate_genotypes should have the same length\')\n    if len(truths) != len(truth_genotypes):\n      raise ValueError((\'truths and truth_genotypes should have the same \'\n                        \'length\'))\n    if any(sum(gt) == 0 for gt in _variant_genotypes(truths)):\n      raise ValueError(\'No truth genotypes should be hom-ref\')\n\n    self.haplotypes = sorted(haplotypes)\n    self.candidates = candidates\n    self.truths = truths\n    self.candidate_genotypes = candidate_genotypes\n    self.truth_genotypes = truth_genotypes\n\n    # Computed on-demand.\n    self._n_false_positives = None\n    self._n_false_negatives = None\n\n  def __str__(self):\n    return (\'HaplotypeMatch(haplotypes={}, false_negatives={}, \'\n            \'false_positives={} true_positives={} match_metrics={}, \'\n            \'variant_gts={}, true_gts={})\').format(\n                self.haplotypes, self.n_false_negatives, self.n_false_positives,\n                self.n_true_positives, self.match_metrics,\n                self.candidate_genotypes, self.truth_genotypes)\n\n  __repr__ = __str__\n\n  @property\n  def original_truth_genotypes(self):\n    return _variant_genotypes(self.truths)\n\n  @property\n  def match_metrics(self):\n    """"""Quality of this match.\n\n    Lower scores are better.\n\n    Returns:\n      tuple[int] where all elements are >= 0: The tuple is suitable for sorting\n      matches, so that sorted(matches, key=lambda x: x.match_metrics) will rank\n      matches so that the best option is first.\n    """"""\n    return (self.n_false_negatives, self.n_false_positives,\n            self.n_true_positives)\n\n  @property\n  def n_true_positives(self):\n    """"""Gets the number of candidates whose matched genotype is not (0, 0).\n\n    Since the candidates don\'t have expected genotypes, we can only count each\n    site instead of each genotype. So this is the number of candidates whose\n    matched genotype is not (0, 0).\n\n    Returns:\n      int >= 0.\n    """"""\n    return len(self.candidate_genotypes) - self.n_false_positives\n\n  @property\n  def n_false_positives(self):\n    """"""Gets the number of candidates whose matched genotype is (0, 0).\n\n    Since the candidates don\'t have expected genotypes, we can only count each\n    site instead of each genotype. So this is the number of candidates whose\n    matched genotype is (0, 0).\n\n    Returns:\n      int >= 0.\n    """"""\n    if self._n_false_positives is None:\n      self._n_false_positives = sum(\n          sum(gt) == 0 for gt in self.candidate_genotypes)\n    return self._n_false_positives\n\n  @property\n  def n_false_negatives(self):\n    """"""Gets the number of missed true genotypes.\n\n    This is the sum of missed non-ref genotypes over all truth variants. So if\n    we have a matched truth genotype of (0, 1) and the true genotype is (1, 1),\n    then we have 1 FN. If the matched genotype were (0, 0), we\'d have 2 FNs.\n\n    Returns:\n      int >= 0.\n    """"""\n    if self._n_false_negatives is None:\n      self._n_false_negatives = sum(\n          n_zeroes(assigned_gt) - n_zeroes(original_gt)\n          for original_gt, assigned_gt in zip(self.original_truth_genotypes,\n                                              self.truth_genotypes))\n    return self._n_false_negatives\n\n  def candidates_with_assigned_genotypes(self):\n    """"""Gets a copy of our candidates with their matched genotypes.\n\n    Returns:\n      list[Variant protobuf]: Returns a copy of self.candidates in order, with\n      genotypes corresponding to their matched genotypes. Any previous genotypes\n      in these Variants will be overwrite. If no VariantCall is present one will\n      be added.\n    """"""\n    with_gts = [copy.deepcopy(v) for v in self.candidates]\n    for variant, gt in zip(with_gts, self.candidate_genotypes):\n      call = variant.calls[0] if variant.calls else variant.calls.add()\n      variantcall_utils.set_gt(call, gt)\n    return with_gts\n\n\ndef deduplicate_haplotypes(haplotypes_and_genotypes):\n  """"""Returns a new list of haplotypes_and_genotypes with duplicates removed.\n\n  This function goes through the haplotypes_and_genotypes iterable and keeps\n  only a single example of (haplotypes, genotypes) if there are multiple\n  elements of the list that have the same haplotypes. Duplicates are expected\n  in the list because different genotype configurations can sometimes produce\n  the same set of haplotypes, and analyzing a list of possible\n  haplotypes/genotypes combinations with duplicates is much harder and less\n  efficient than the deduplicated list.\n\n  Args:\n    haplotypes_and_genotypes: iterable[(haplotype, genotype)]. The\n      haplotype/genotype tuples we want to deduplicate.\n\n  Returns:\n    A subset of haplotype/genotype tuples, as a list, without duplicates.\n  """"""\n  haplotypes_and_genotypes = list(haplotypes_and_genotypes)\n  return [\n      (vh1, vg1)\n      for i, (vh1, vg1) in enumerate(haplotypes_and_genotypes)\n      if not any(vh1 == vh2 for (vh2, _) in haplotypes_and_genotypes[i + 1:])\n  ]\n\n\n# redacted\n# variants and truths, and yields information about each variant and\n# truth variant sequentially. This should be the primary API. Refactor\n# label_examples to use this new API. Then create a new implementation that does\n# the fast version.\ndef find_best_matching_haplotypes(candidates, truths, ref):\n  """"""Assigns genotypes to each variant to best match truths.\n\n  See the module-level documentation for general information on how this\n  algorithm works.\n\n  Args:\n    candidates: list[nucleus.protos.Variant]. A list of candidate variants, in\n      coordinate-sorted order, all on the same chromosome.\n    truths: list[nucleus.protos.Variant]. A list of truth variants, in\n      coordinate-sorted order, for the same interval on the genome as variants.\n    ref: ReferenceRegion. Used to get reference bases for variants. Must cover\n      at least the span of the variants.\n\n  Returns:\n    A HaplotypeMatch object describing the best assignment of genotypes between\n    the candidates and truth_variants, or None, if no consistent assignment can\n    be found.\n\n  Raises:\n    ValueError: If any inputs are malformed.\n  """"""\n  candidates = list(candidates)\n  truths = list(truths)\n\n  if _DEBUG_PRINTING_IS_ENABLED:\n    _log_variants(\'candidates\', candidates)\n    _log_variants(\'truth\', truths)\n\n  if not variant_utils.variants_are_sorted(candidates):\n    raise ValueError(\'candidates are not sorted\', candidates)\n  if not variant_utils.variants_are_sorted(truths):\n    raise ValueError(\'truths are not sorted\', truths)\n\n  def _hom_ref_enum_if_empty(list_of_variants, non_empty_enum):\n    """"""If list_of_variants is empty, use a ONLY_HOM_REF enum for speed.""""""\n    return non_empty_enum if list_of_variants else EnumerationType.ONLY_HOM_REF\n\n  truth_haplotypes = deduplicate_haplotypes(\n      enumerate_all_possible_haplotypes(\n          truths, ref, _hom_ref_enum_if_empty(candidates,\n                                              EnumerationType.TRUTH)))\n\n  # Note, it may be worth deduplicating these haplotypes as well.\n  variant_haplotypes = enumerate_all_possible_haplotypes(\n      candidates, ref, _hom_ref_enum_if_empty(truths,\n                                              EnumerationType.CANDIDATES))\n\n  found = []\n  for vh, vgt in variant_haplotypes:\n    for th, tgt in truth_haplotypes:\n      if th == vh:\n        found.append(\n            HaplotypeMatch(\n                haplotypes=th,\n                candidates=candidates,\n                candidate_genotypes=vgt,\n                truths=truths,\n                truth_genotypes=tgt))\n\n  if not found:\n    return None\n  else:\n    return select_best_haplotype_match(found)\n\n\ndef select_best_haplotype_match(all_matches):\n  """"""Returns the best HaplotypeMatch among all_matches.\n\n  The best matching HaplotypeMatch is the one with the lowest match_metrics\n  score.\n\n  Args:\n    all_matches: iterable[HaplotypeMatch]. An iterable of HaplotypeMatch objects\n      we want to select the best match from.\n\n  Returns:\n    The best matching HaplotypeMatch object.\n  """"""\n  sorted_matches = sorted(all_matches, key=lambda x: x.match_metrics)\n  best = sorted_matches[0]\n  equivalents = [\n      f for f in all_matches if f.match_metrics == best.match_metrics\n  ]\n\n  # redacted\n  if len(equivalents) > 1:\n    for i, f in enumerate(equivalents):\n      extra_info = \'best\' if i == 0 else i\n      logging.warning(\'Equivalent match to best: %s [%s]\', f, extra_info)\n\n  return equivalents[0]\n\n\n# -----------------------------------------------------------------------------\n#\n# Private utility functions\n#\n# -----------------------------------------------------------------------------\n\n\ndef _variant_genotypes(variants, missing_genotypes_default=(-1, -1)):\n  """"""Returns the genotypes of variants as a list of tuples.\n\n  Args:\n    variants: iterable[nucleus.protos.Variant]. The variants whose genotypes we\n      want to get.\n    missing_genotypes_default: tuple. If a variant in variants doesn\'t have\n      genotypes, this value is returned. The default value is (-1, -1), the\n      standard representation for ""missing"" diploid genotypes.\n\n  Returns:\n    list[nucleus.protos.Variant] protos in the same order as variants.\n  """"""\n  return [\n      tuple(v.calls[0].genotype) if v.calls else missing_genotypes_default\n      for v in variants\n  ]\n\n\ndef _log_haplotypes(name, haplotypes):\n  """"""Write basic information about haplotypes to logging.info.""""""\n  logging.info(\'haplotypes: %s\', name)\n  for haplotype, genotypes in haplotypes:\n    logging.info(\'  %s with %s\', haplotype, genotypes)\n\n\ndef _log_variants(name, variants):\n  """"""Write basic information about variants to logging.info.""""""\n  logging.info(\'variants: %s [%d]\', name, len(variants))\n  for v in variants:\n    logging.info(\'  %s gt=%s\', variant_utils.variant_key(v),\n                 _variant_genotypes([v])[0])\n\n\ndef n_zeroes(l):\n  """"""Returns the number of elements of l that are 0.""""""\n  return sum(1 for x in l if x == 0)\n\n\ndef _allele_from_index(variant, allele_index):\n  """"""Gets the reference_bases or alternative_bases for allele_index.""""""\n  if allele_index == 0:\n    return variant.reference_bases\n  else:\n    return variant.alternate_bases[allele_index - 1]\n'"
deepvariant/labeler/haplotype_labeler_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant.haplotype_labeler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport itertools\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\nimport six\nfrom third_party.nucleus.io import fasta\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.protos import reference_pb2\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import ranges\nfrom third_party.nucleus.util import variant_utils\n\nfrom deepvariant.labeler import haplotype_labeler\nfrom deepvariant.protos import deepvariant_pb2\n\n\ndef _test_variant(start=10, alleles=(\'A\', \'C\'), gt=None):\n  variant = variants_pb2.Variant(\n      reference_name=\'20\',\n      start=start,\n      end=start + len(alleles[0]),\n      reference_bases=alleles[0],\n      alternate_bases=alleles[1:],\n  )\n\n  if gt:\n    variant.calls.add(genotype=gt)\n\n  return variant\n\n\ndef _variants_from_grouped_positions(grouped_positions):\n  groups = [\n      [_test_variant(start=s) for s in starts] for starts in grouped_positions\n  ]\n  variants = [v for subgroup in groups for v in subgroup]\n  return variants, [(g, []) for g in groups]\n\n\ndef _make_labeler(truths=None,\n                  confident_regions=None,\n                  ref_reader=None,\n                  **kwargs):\n  if ref_reader is None:\n    ref_reader = mock.MagicMock()\n\n  if confident_regions is None:\n    # Use the reference of the truth variants if possible, otherwise just use\n    # a dummy placeholder value for the contig name and make the confident\n    # region a giant span.\n    contig = truths[0].reference_name if truths else \'dummy\'\n    confident_regions = ranges.RangeSet(\n        [ranges.make_range(contig, 0, 1000000000)])\n\n  return haplotype_labeler.HaplotypeLabeler(\n      truth_vcf_reader=vcf.InMemoryVcfReader(truths or []),\n      ref_reader=ref_reader,\n      confident_regions=confident_regions,\n      **kwargs)\n\n\nclass HaplotypeLabelerClassUnitTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      # If there are no variants, we don\'t get any groups.\n      dict(grouped_positions=[]),\n      # A single variant at 10 gets put in a single group.\n      dict(grouped_positions=[[10]]),\n      # A two variants (at 10 and 11) get grouped because 1 < the max_dist=10.\n      dict(grouped_positions=[[10, 11]]),\n      # A two variants at 10 and 50 get grouped separately because their\n      # distance > max_dist=10.\n      dict(grouped_positions=[[10], [50]]),\n      # Check the behavior right around max_dist.\n      dict(grouped_positions=[[10, 19]]),\n      dict(grouped_positions=[[10, 20]]),\n      dict(grouped_positions=[[10], [21]]),\n      # A few complex examples with multiple variants getting grouped.\n      dict(grouped_positions=[[10, 15], [45, 50], [65]]),\n      dict(grouped_positions=[[10, 12, 15], [45, 49, 50], [65, 70]]),\n      # The distance calculation compares not first variant in the group but the\n      # closest one, so we can have a group with collective distance > max_dist\n      # as long as the distance between successive variants <= max_dist\n      dict(grouped_positions=[[10, 20, 30, 40]]),\n  )\n  def test_group_variants_examples(self, grouped_positions):\n    variants, groups = _variants_from_grouped_positions(grouped_positions)\n    self.assertEqual(\n        groups,\n        haplotype_labeler.group_variants(variants, [], max_separation=10))\n\n  @parameterized.parameters(\n      dict(separation=s, max_separation=d)\n      for d in range(5)\n      for s in range(d + 1))\n  def test_group_variants_respects_max_dist(self, separation, max_separation):\n    self.assertLessEqual(separation, max_separation)\n    variants = [\n        _test_variant(start=10),\n        _test_variant(start=10 + separation),\n    ]\n    # Because separation <= max_separation, all variants should be in a\n    # single group.\n    self.assertEqual([(variants, [])],\n                     haplotype_labeler.group_variants(\n                         variants, [], max_separation=max_separation))\n\n  @parameterized.parameters(range(1, 10))\n  def test_group_variants_works_with_any_number_of_variants(self, n_variants):\n    variants = [_test_variant(start=10 + i) for i in range(n_variants)]\n    self.assertEqual([(variants, [])],\n                     haplotype_labeler.group_variants(\n                         variants, [],\n                         max_group_size=n_variants,\n                         max_separation=n_variants))\n\n  @parameterized.parameters(\n      dict(\n          grouped_positions=[[10, 11, 12, 13]],\n          max_group_size=4,\n      ),\n      dict(\n          grouped_positions=[[10, 11, 12], [13]],\n          max_group_size=3,\n      ),\n      dict(\n          grouped_positions=[[10, 11], [12, 13]],\n          max_group_size=2,\n      ),\n      dict(\n          grouped_positions=[[10], [11], [12], [13]],\n          max_group_size=1,\n      ),\n  )\n  def test_group_variants_group_size_works(self, grouped_positions,\n                                           max_group_size):\n    variants, groups = _variants_from_grouped_positions(grouped_positions)\n    self.assertEqual(\n        groups,\n        haplotype_labeler.group_variants(\n            variants, [], max_group_size=max_group_size))\n\n  @parameterized.parameters(\n      # A few basic tests of functionality to start:\n      # We group a single variant without associated truth variants.\n      dict(\n          variant_positions=[10],\n          truth_positions=[],\n          expected_positions=[([10], [])],\n      ),\n      # We group an isolated truth variant in isolation without any candidates.\n      dict(\n          variant_positions=[],\n          truth_positions=[10],\n          expected_positions=[([], [10])],\n      ),\n      # A single candidate + truth at the same position are grouped together.\n      dict(\n          variant_positions=[10],\n          truth_positions=[10],\n          expected_positions=[([10], [10])],\n      ),\n      # We respect our distance between variants across positional boundaries.\n      dict(\n          variant_positions=[10],\n          truth_positions=[11],\n          expected_positions=[([10], [11])],\n      ),\n      # Another test of grouping across candidates and truth.\n      dict(\n          variant_positions=[9, 11],\n          truth_positions=[10, 12],\n          expected_positions=[([9, 11], [10, 12])],\n      ),\n      # These tests actually result in broken up groups, with isolated truth\n      # and candidates as well as grouped ones.\n      dict(\n          variant_positions=[1, 9, 11, 20, 25, 50],\n          truth_positions=[10, 12, 23, 45, 100],\n          expected_positions=[\n              ([1], []),\n              ([9, 11], [10, 12]),\n              ([20, 25], [23]),\n              ([50], [45]),\n              ([], [100]),\n          ],\n      ),\n      # Now some tests to exercise the max group size with both candidates and\n      # truth variants. We vary the max group size to make sure the grouping\n      # algorithm splits correctly.\n      dict(\n          variant_positions=[1, 2, 3, 4, 5],\n          truth_positions=[1, 2, 3, 4, 5],\n          expected_positions=[\n              ([1, 2], [1, 2]),\n              ([3, 4], [3, 4]),\n              ([5], [5]),\n          ],\n          max_group_size=2,\n      ),\n      dict(\n          variant_positions=[1, 2, 3, 4, 5],\n          truth_positions=[1, 2, 3, 4, 5],\n          expected_positions=[\n              ([1, 2, 3], [1, 2, 3]),\n              ([4, 5], [4, 5]),\n          ],\n          max_group_size=3,\n      ),\n  )\n  def test_group_variants_with_truth(self,\n                                     variant_positions,\n                                     truth_positions,\n                                     expected_positions,\n                                     max_group_size=10):\n    variants = [_test_variant(start=pos) for pos in variant_positions]\n    truths = [_test_variant(start=pos) for pos in truth_positions]\n    actual = haplotype_labeler.group_variants(\n        variants, truths, max_group_size=max_group_size, max_separation=5)\n    self.assertEqual([([v.start\n                        for v in variant_group], [v.start\n                                                  for v in truth_group])\n                      for variant_group, truth_group in actual],\n                     expected_positions)\n\n  @parameterized.parameters(\n      dict(truth_position=7, expected_together=False),\n      dict(truth_position=8, expected_together=False),\n      dict(truth_position=9, expected_together=True),\n      dict(truth_position=10, expected_together=True),\n      dict(truth_position=11, expected_together=True),\n      dict(truth_position=12, expected_together=True),\n      dict(truth_position=13, expected_together=True),\n      dict(truth_position=14, expected_together=True),\n      dict(truth_position=15, expected_together=True),\n      dict(truth_position=16, expected_together=False),\n      dict(truth_position=17, expected_together=False),\n  )\n  def test_group_variants_respects_end(self, truth_position, expected_together):\n    # Deletion spans from 10-15 as it\'s a deletion.\n    deletion = _test_variant(start=10, alleles=(\'AAAAA\', \'A\'))\n    truth = _test_variant(start=truth_position)\n\n    # The actual result returned by group_variants is a list of tuples\n    # containing the grouped candidates and truth. The order they appear depends\n    # on the truth_position, since our deletion starts at 10.\n    if expected_together:\n      expected = [([deletion], [truth])]\n    elif truth_position < 10:\n      expected = [([], [truth]), ([deletion], [])]\n    else:\n      expected = [([deletion], []), ([], [truth])]\n\n    self.assertEqual(\n        haplotype_labeler.group_variants([deletion], [truth], max_separation=1),\n        expected)\n\n  @parameterized.parameters(\n      # Check a simple case of two SNPs.\n      dict(\n          candidates=[\n              _test_variant(start=10, alleles=[\'A\', \'C\']),\n              _test_variant(start=20, alleles=[\'A\', \'C\']),\n          ],\n          truths=[],\n          expected_start=9,\n          expected_end=21,\n          bufsize=0,\n      ),\n      # Check that we respect the deletion\'s span in the last candidate.\n      dict(\n          candidates=[\n              _test_variant(start=10, alleles=[\'A\', \'C\']),\n              _test_variant(start=20, alleles=[\'AAA\', \'C\']),\n          ],\n          truths=[],\n          expected_start=9,\n          expected_end=23,\n          bufsize=0,\n      ),\n      # Check that we respect handle truth variants, as the interval is entirely\n      # determined by truth variants here.\n      dict(\n          candidates=[\n              _test_variant(start=15, alleles=[\'A\', \'C\']),\n          ],\n          truths=[\n              _test_variant(start=10, alleles=[\'A\', \'C\']),\n              _test_variant(start=20, alleles=[\'AAA\', \'C\']),\n          ],\n          expected_start=9,\n          expected_end=23,\n          bufsize=0,\n      ),\n      # Check that bufsize is respected.\n      dict(\n          candidates=[\n              _test_variant(start=10, alleles=[\'A\', \'C\']),\n              _test_variant(start=20, alleles=[\'AAA\', \'C\']),\n          ],\n          truths=[],\n          expected_start=9,\n          expected_end=23 + 10,  # 10 is the bufsize.\n          bufsize=10,\n      ),\n  )\n  def test_make_labeler_ref(self, candidates, truths, expected_start,\n                            expected_end, bufsize):\n    expected_bases = \'A\' * (expected_end - expected_start)\n\n    labeler = _make_labeler()\n    labeler._ref_reader.query.return_value = expected_bases\n    labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(\n        name=\'20\', n_bases=50)\n\n    labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize)\n\n    labeler._ref_reader.query.assert_called_once_with(\n        ranges.make_range(\'20\', expected_start, expected_end))\n    self.assertEqual(labeler_ref.start, expected_start)\n    self.assertEqual(labeler_ref.end, expected_end)\n    self.assertEqual(\n        labeler_ref.bases(expected_start, expected_end), expected_bases)\n\n  # Check that we don\'t issue a query with a bad start if the variant is at\n  # position 0 on the genome. See internal.\n  def test_make_labeler_ref_handles_variant_at_pos_zero(self):\n    labeler = _make_labeler(\n        ref_reader=fasta.InMemoryFastaReader([(\'20\', 0, \'ACGT\')]))\n    labeler_ref = labeler.make_labeler_ref(\n        candidates=[\n            _test_variant(start=0, alleles=[\'A\', \'C\']),\n        ],\n        true_variants=[],\n        bufsize=2)\n    expected_start, expected_end = 0, 3\n    self.assertEqual(labeler_ref.start, expected_start)\n    self.assertEqual(labeler_ref.end, expected_end)\n    self.assertEqual(labeler_ref.bases(expected_start, expected_end), \'ACG\')\n\n  # Check that we don\'t issue a query with a bad end if the variant is at\n  # the last position on the genome. See internal.\n  def test_make_labeler_ref_handles_variant_at_end_of_chrom(self):\n    labeler = _make_labeler(\n        ref_reader=fasta.InMemoryFastaReader([(\'20\', 0, \'ACGT\')]))\n    labeler_ref = labeler.make_labeler_ref(\n        candidates=[\n            _test_variant(start=3, alleles=[\'T\', \'A\']),\n        ],\n        true_variants=[],\n        bufsize=2)\n    expected_start, expected_end = 2, 4\n    self.assertEqual(labeler_ref.start, expected_start)\n    self.assertEqual(labeler_ref.end, expected_end)\n    self.assertEqual(labeler_ref.bases(expected_start, expected_end), \'GT\')\n\n  def test_label_variants(self):\n    variants = [\n        _test_variant(start=10),\n        _test_variant(start=11),\n        _test_variant(start=20),\n        _test_variant(start=30),\n        _test_variant(start=31),\n    ]\n    truths = [\n        _test_variant(start=10, gt=(0, 1)),\n        _test_variant(start=30, gt=(1, 1)),\n    ]\n\n    labeler = _make_labeler(\n        truths=truths,\n        max_separation=5,\n        ref_reader=fasta.InMemoryFastaReader([(\'20\', 0, \'A\' * 100)]))\n    region = ranges.make_range(\'20\', 1, 50)\n    result = list(labeler.label_variants(variants, region))\n\n    expected_genotypes_by_pos = {\n        10: (0, 1),\n        11: (0, 0),\n        20: (0, 0),\n        30: (1, 1),\n        31: (0, 0),\n    }\n    self.assertEqual(len(result), len(variants))\n    for variant, label in zip(variants, result):\n      self.assertEqual(variant.start, label.variant.start)\n      self.assertTrue(label.is_confident)\n      self.assertEqual(\n          tuple(label.variant.calls[0].genotype),\n          expected_genotypes_by_pos[variant.start],\n          \'Bad genotype for \' + str(label.variant))\n\n  def test_label_variants_bug1(self):\n    # Test for a bug encountered in make_examples.\n    #\n    # variants: candidates [0]\n    # variants: truth [1]\n    #    20:6299587:C->T gt=(1, 1)\n    # Top-level exception: (\'Failed to assign labels for variants\', [])\n    labeler = _make_labeler(\n        truths=[_test_variant(6299586, alleles=(\'C\', \'T\'), gt=(1, 1))],\n        ref_reader=fasta.InMemoryFastaReader([(\'20\', 6299585,\n                                               \'TCCTGCTTTCTCTTGTGGGCAT\')]))\n    region = ranges.make_range(\'20\', 6299000, 6299999)\n    self.assertIsNotNone(labeler.label_variants([], region))\n\n  @parameterized.parameters(\n      # A single TP bi-allelic variant.\n      dict(\n          candidates=[\n              _test_variant(start=2, alleles=(\'A\', \'C\')),\n          ],\n          truths=[\n              _test_variant(start=2, alleles=(\'A\', \'C\'), gt=(0, 1)),\n          ],\n          n_truth_variant_sites=1,\n          n_truth_variant_alleles=1,\n          n_candidate_variant_sites=1,\n          n_candidate_variant_alleles=1,\n          n_true_positive_sites=1,\n          n_true_positive_alleles=1,\n          n_false_negative_sites=0,\n          n_false_negative_alleles=0,\n          n_false_positive_sites=0,\n          n_false_positive_alleles=0,\n          n_inexact_position_matches=0,\n          n_exact_position_matches=1,\n          n_exact_position_and_allele_matches=1,\n          n_exact_position_and_allele_and_genotype_matches=1,\n          n_truth_multiallelics_sites_with_missed_alleles=0,\n      ),\n      # A single TP tri-allelic variant.\n      dict(\n          candidates=[\n              _test_variant(start=2, alleles=(\'A\', \'C\', \'G\')),\n          ],\n          truths=[\n              _test_variant(start=2, alleles=(\'A\', \'C\', \'G\'), gt=(1, 2)),\n          ],\n          n_truth_variant_sites=1,\n          n_truth_variant_alleles=2,\n          n_candidate_variant_sites=1,\n          n_candidate_variant_alleles=2,\n          n_true_positive_sites=1,\n          n_true_positive_alleles=2,\n          n_false_negative_sites=0,\n          n_false_negative_alleles=0,\n          n_false_positive_sites=0,\n          n_false_positive_alleles=0,\n          n_inexact_position_matches=0,\n          n_exact_position_matches=1,\n          n_exact_position_and_allele_matches=1,\n          n_exact_position_and_allele_and_genotype_matches=1,\n          n_truth_multiallelics_sites_with_missed_alleles=0,\n      ),\n      # Here we have an extra alt in our candidate and one tri-allelic truth.\n      # Because of this we have a FP allele and our exact matching counts are\n      # different than the above example.\n      dict(\n          candidates=[\n              _test_variant(start=2, alleles=(\'A\', \'C\', \'G\', \'T\')),\n          ],\n          truths=[\n              _test_variant(start=2, alleles=(\'A\', \'C\', \'G\'), gt=(1, 2)),\n          ],\n          n_truth_variant_sites=1,\n          n_truth_variant_alleles=2,\n          n_candidate_variant_sites=1,\n          n_candidate_variant_alleles=3,\n          n_true_positive_sites=1,\n          n_true_positive_alleles=2,\n          n_false_negative_sites=0,\n          n_false_negative_alleles=0,\n          n_false_positive_sites=0,\n          n_false_positive_alleles=1,\n          n_inexact_position_matches=0,\n          n_exact_position_matches=1,\n          n_exact_position_and_allele_matches=0,\n          n_exact_position_and_allele_and_genotype_matches=0,\n          n_truth_multiallelics_sites_with_missed_alleles=0,\n      ),\n      # A single FP variant in candidates with 3 alt alleles.\n      dict(\n          candidates=[\n              _test_variant(start=2, alleles=(\'A\', \'C\', \'G\')),\n          ],\n          truths=[],\n          n_truth_variant_sites=0,\n          n_truth_variant_alleles=0,\n          n_candidate_variant_sites=1,\n          n_candidate_variant_alleles=2,\n          n_true_positive_sites=0,\n          n_true_positive_alleles=0,\n          n_false_negative_sites=0,\n          n_false_negative_alleles=0,\n          n_false_positive_sites=1,\n          n_false_positive_alleles=2,\n          n_inexact_position_matches=0,\n          n_exact_position_matches=0,\n          n_exact_position_and_allele_matches=0,\n          n_exact_position_and_allele_and_genotype_matches=0,\n          n_truth_multiallelics_sites_with_missed_alleles=0,\n      ),\n      # A single FN variant in truth with 4 alt alleles.\n      dict(\n          candidates=[],\n          truths=[\n              _test_variant(start=2, alleles=(\'A\', \'C\', \'G\', \'T\'), gt=(1, 2)),\n          ],\n          n_truth_variant_sites=1,\n          # We only have 2 non-ref alleles in truth, so this is 2 not 3.\n          n_truth_variant_alleles=2,\n          n_candidate_variant_sites=0,\n          n_candidate_variant_alleles=0,\n          n_true_positive_sites=0,\n          n_true_positive_alleles=0,\n          n_false_negative_sites=1,\n          n_false_negative_alleles=2,\n          n_false_positive_sites=0,\n          n_false_positive_alleles=0,\n          n_inexact_position_matches=0,\n          n_exact_position_matches=0,\n          n_exact_position_and_allele_matches=0,\n          n_exact_position_and_allele_and_genotype_matches=0,\n          n_truth_multiallelics_sites_with_missed_alleles=1,\n      ),\n  )\n  def test_metrics(self, candidates, truths, **kwargs):\n    self.assertMetricsEqual(candidates=candidates, truths=truths, **kwargs)\n\n  @parameterized.parameters(range(1, 5))\n  def test_metrics_multiple_variants(self, max_separation):\n    # This is parameterized over the max_separation so we can test that the\n    # metrics are properly calculated no matter the grouping. The candidates and\n    # truth variants below should give the same metrics regardless of grouping.\n    self.assertMetricsEqual(\n        candidates=[\n            # Example one from our narrative in LabelerMetrics proto.\n            _test_variant(start=2, alleles=(\'A\', \'C\')),\n            # Example two from our narrative in LabelerMetrics proto.\n            _test_variant(start=3, alleles=(\'A\', \'C\', \'T\')),\n            # A genuine false positive => no corresponding truth variant.\n            _test_variant(start=4, alleles=(\'A\', \'G\', \'C\')),\n        ],\n        truths=[\n            _test_variant(start=2, alleles=(\'A\', \'C\'), gt=(0, 1)),\n            _test_variant(start=3, alleles=(\'A\', \'C\', \'G\'), gt=(1, 2)),\n            # A genuine false negative => no corresponding variant in truth.\n            _test_variant(start=5, alleles=(\'A\', \'C\'), gt=(0, 1)),\n        ],\n        max_separation=max_separation,\n        n_truth_variant_sites=3,\n        n_truth_variant_alleles=4,\n        n_candidate_variant_sites=3,\n        n_candidate_variant_alleles=5,\n        n_true_positive_sites=2,\n        n_true_positive_alleles=2,\n        n_false_negative_sites=1,\n        n_false_negative_alleles=2,\n        n_false_positive_sites=1,\n        n_false_positive_alleles=3,\n        n_inexact_position_matches=0,\n        n_exact_position_matches=2,\n        n_exact_position_and_allele_matches=1,\n        n_exact_position_and_allele_and_genotype_matches=1,\n        n_truth_multiallelics_sites_with_missed_alleles=1)\n\n  def test_metrics_inexact_matches(self):\n    # ref looks like AACTG. Truth is just a single SNP turning the C into a G.\n    # Candidates do the same but via an insertion + deletion. This test ensures\n    # that the metrics work even in the case where we have different\n    # representations for the same haplotype.\n    self.assertMetricsEqual(\n        candidates=[\n            _test_variant(start=1, alleles=(\'A\', \'AG\')),\n            _test_variant(start=2, alleles=(\'CT\', \'T\')),\n        ],\n        truths=[\n            _test_variant(start=2, alleles=(\'C\', \'G\'), gt=(0, 1)),\n        ],\n        ref_prefix=\'AACTG\',\n        n_truth_variant_sites=1,\n        n_truth_variant_alleles=1,\n        n_candidate_variant_sites=2,\n        n_candidate_variant_alleles=2,\n        n_true_positive_sites=1,\n        n_true_positive_alleles=1,\n        n_false_negative_sites=0,\n        n_false_negative_alleles=0,\n        n_false_positive_sites=0,\n        n_false_positive_alleles=0,\n        n_inexact_position_matches=1,\n        n_exact_position_matches=1,\n        n_exact_position_and_allele_matches=0,\n        n_exact_position_and_allele_and_genotype_matches=0,\n        n_truth_multiallelics_sites_with_missed_alleles=0)\n\n  def assertMetricsEqual(self,\n                         candidates,\n                         truths,\n                         ref_prefix=\'\',\n                         max_separation=10,\n                         **metric_kwargs):\n    labeler = _make_labeler(\n        truths=truths,\n        max_separation=max_separation,\n        confident_regions=ranges.RangeSet([ranges.make_range(\'20\', 0, 1000)]),\n        ref_reader=fasta.InMemoryFastaReader([(\'20\', 0, ref_prefix + \'A\' * 50)\n                                             ]))\n    region = ranges.make_range(\'20\', 1, 10)\n    _ = list(labeler.label_variants(candidates, region))\n    self.assertEqual(labeler.metrics,\n                     deepvariant_pb2.LabelingMetrics(**metric_kwargs))\n\n  def test_metrics_respects_confident_regions(self):\n    # The confident region is 2-4, so we should only count the variant starting\n    # at 3.\n    candidates = [\n        _test_variant(start=1),\n        _test_variant(start=3),\n        _test_variant(start=5),\n    ]\n    labeler = _make_labeler(\n        truths=[],\n        confident_regions=ranges.RangeSet(\n            [ranges.make_range(candidates[0].reference_name, 2, 4)]),\n        ref_reader=fasta.InMemoryFastaReader([(\'20\', 0, \'A\' * 50)]))\n    _ = list(labeler.label_variants(candidates, ranges.make_range(\'20\', 1, 10)))\n    self.assertEqual(labeler.metrics.n_candidate_variant_sites, 1)\n    self.assertEqual(labeler.metrics.n_candidate_variant_alleles, 1)\n    self.assertEqual(labeler.metrics.n_non_confident_candidate_variant_sites, 2)\n    self.assertEqual(labeler.metrics.n_false_positive_sites, 1)\n    self.assertEqual(labeler.metrics.n_false_positive_alleles, 1)\n\n\nclass HaplotypeMatchTests(parameterized.TestCase):\n\n  def setUp(self):\n    self.haplotypes = [\'AC\', \'GT\']\n    self.variants = [\n        _test_variant(42, [\'A\', \'G\']),\n        _test_variant(43, [\'G\', \'A\']),\n        _test_variant(44, [\'C\', \'T\']),\n    ]\n    self.truths = [\n        _test_variant(42, [\'A\', \'G\'], [0, 1]),\n        _test_variant(44, [\'C\', \'T\'], [0, 1]),\n        _test_variant(45, [\'G\', \'A\'], [1, 1]),\n    ]\n    self.candidate_genotypes = [(0, 1), (0, 0), (0, 1)]\n    self.truth_genotypes = [(0, 1), (0, 1), (0, 0)]\n    self.match = haplotype_labeler.HaplotypeMatch(self.haplotypes,\n                                                  self.variants,\n                                                  self.candidate_genotypes,\n                                                  self.truths,\n                                                  self.truth_genotypes)\n\n  def test_fields_are_expected(self):\n    self.assertEqual(self.match.haplotypes, self.haplotypes)\n    self.assertEqual(self.match.candidates, self.variants)\n    self.assertEqual(self.match.truths, self.truths)\n    self.assertEqual(self.match.candidate_genotypes, self.candidate_genotypes)\n    self.assertEqual(self.match.truth_genotypes, self.truth_genotypes)\n\n    # Computed fields.\n    self.assertEqual(self.match.original_truth_genotypes,\n                     haplotype_labeler._variant_genotypes(self.truths))\n    self.assertEqual(self.match.n_false_positives, 1)\n    self.assertEqual(self.match.n_false_negatives, 2)\n    self.assertEqual(self.match.n_true_positives, 2)\n    self.assertEqual(self.match.match_metrics, (2, 1, 2))\n\n  def test_str(self):\n    self.assertIn(\'HaplotypeMatch(\', str(self.match))\n\n  def test_candidates_with_assigned_genotypes(self):\n    self.assertEqual(self.match.candidates_with_assigned_genotypes(), [\n        _test_variant(42, [\'A\', \'G\'], [0, 1]),\n        _test_variant(43, [\'G\', \'A\'], [0, 0]),\n        _test_variant(44, [\'C\', \'T\'], [0, 1]),\n    ])\n    # Assert that we have no genotypes in self.variants to check that\n    # candidates_with_assigned_genotypes isn\'t modifying our variants.\n    for v in self.match.candidates:\n      self.assertFalse(v.calls, \'Variant genotypes modified\')\n\n  @parameterized.parameters(\n      # All genotypes match, so we have no FNs and no FPs.\n      dict(\n          vgenotypes=[(0, 1), (0, 1)],\n          matched_tgenotypes=[(0, 1), (0, 1)],\n          tgenotypes=[(0, 1), (0, 1)],\n          expected_fns=0,\n          expected_fps=0,\n          expected_tps=2,\n      ),\n      #\n      # Here are a few cases where we false negatives.\n      #\n      dict(\n          vgenotypes=[(0, 1), (0, 1)],\n          # True het is 0, 0 in second true variant.\n          matched_tgenotypes=[(0, 1), (0, 0)],\n          tgenotypes=[(0, 1), (0, 1)],\n          expected_fns=1,\n          expected_fps=0,\n          expected_tps=2,\n      ),\n      dict(\n          vgenotypes=[(0, 1), (0, 1)],\n          # True hom-alt is het in second true variant.\n          matched_tgenotypes=[(0, 1), (0, 1)],\n          tgenotypes=[(0, 1), (1, 1)],\n          expected_fns=1,\n          expected_fps=0,\n          expected_tps=2,\n      ),\n      dict(\n          vgenotypes=[(0, 1), (0, 1)],\n          # True hom-alts are het in first and second true variants.\n          matched_tgenotypes=[(0, 1), (0, 1)],\n          tgenotypes=[(1, 1), (1, 1)],\n          expected_fns=2,\n          expected_fps=0,\n          expected_tps=2,\n      ),\n      dict(\n          vgenotypes=[(0, 1), (0, 1)],\n          # True hom-alts are hom-ref in first and second true variants.\n          matched_tgenotypes=[(0, 0), (0, 0)],\n          tgenotypes=[(1, 1), (1, 1)],\n          expected_fns=4,\n          expected_fps=0,\n          expected_tps=2,\n      ),\n      #\n      # Here are a few cases where we false positives.\n      #\n      dict(\n          # The first variant genotype is hom-ref so we have one FP.\n          vgenotypes=[(0, 0), (0, 1)],\n          matched_tgenotypes=[(0, 1), (0, 1)],\n          tgenotypes=[(0, 1), (0, 1)],\n          expected_fns=0,\n          expected_fps=1,\n          expected_tps=1,\n      ),\n      dict(\n          # The second variant genotype is hom-ref so we have one FP.\n          vgenotypes=[(0, 1), (0, 0)],\n          matched_tgenotypes=[(0, 1), (0, 1)],\n          tgenotypes=[(0, 1), (0, 1)],\n          expected_fns=0,\n          expected_fps=1,\n          expected_tps=1,\n      ),\n      dict(\n          # Both variant genotypes are hom-ref, so we have two FPs.\n          vgenotypes=[(0, 0), (0, 0)],\n          matched_tgenotypes=[(0, 1), (0, 1)],\n          tgenotypes=[(0, 1), (0, 1)],\n          expected_fns=0,\n          expected_fps=2,\n          expected_tps=0,\n      ),\n  )\n  def test_fns_fps(self, vgenotypes, matched_tgenotypes, tgenotypes,\n                   expected_fns, expected_fps, expected_tps):\n    match = haplotype_labeler.HaplotypeMatch(\n        haplotypes=self.haplotypes,\n        candidates=[\n            _test_variant(42, [\'A\', \'G\']),\n            _test_variant(43, [\'G\', \'A\']),\n        ],\n        candidate_genotypes=vgenotypes,\n        truths=[\n            _test_variant(42, [\'A\', \'G\'], tgenotypes[0]),\n            _test_variant(43, [\'G\', \'A\'], tgenotypes[1]),\n        ],\n        truth_genotypes=matched_tgenotypes)\n    self.assertEqual(match.n_false_negatives, expected_fns)\n    self.assertEqual(match.n_false_positives, expected_fps)\n    self.assertEqual(match.n_true_positives, expected_tps)\n    self.assertEqual(match.n_true_positives + match.n_false_positives, 2)\n    self.assertEqual(match.match_metrics,\n                     (expected_fns, expected_fps, expected_tps))\n\n\nclass LabelExamplesTest(parameterized.TestCase):\n  # Many of these tests are cases from our labeler analysis doc:\n  # https://docs.google.com/document/d/1V89IIT0YM3P0gH_tQb-ahodf8Jvnz0alXEnjCf6JVNo\n\n  def assertGetsCorrectLabels(self,\n                              candidates,\n                              true_variants,\n                              ref,\n                              expected_genotypes,\n                              start=None,\n                              end=None):\n    start = start or ref.start\n    end = end or ref.end\n    labeling = haplotype_labeler.find_best_matching_haplotypes(\n        candidates, true_variants, ref)\n    self.assertIsNotNone(labeling)\n\n    # Check that the genotypes of our labeled variants are the ones we expect.\n    labeled_variants = labeling.candidates_with_assigned_genotypes()\n    self.assertEqual(\n        haplotype_labeler._variant_genotypes(labeled_variants),\n        [tuple(x) for x in expected_genotypes])\n\n  @parameterized.parameters(\n      dict(genotype=[0, 0], expected={(0, 0)}),\n      dict(genotype=[0, 1], expected={(0, 0), (0, 1)}),\n      dict(genotype=[1, 1], expected={(0, 0), (0, 1), (1, 1)}),\n      dict(genotype=[0, 2], expected={(0, 0), (0, 2)}),\n      dict(genotype=[2, 2], expected={(0, 0), (0, 2), (2, 2)}),\n      dict(genotype=[1, 2], expected={(0, 0), (0, 2), (0, 1), (1, 2)}),\n  )\n  def test_with_false_negative_genotypes(self, genotype, expected):\n    self.assertEqual(\n        haplotype_labeler.with_false_negative_genotypes(genotype), expected)\n\n  @parameterized.parameters(\n      dict(\n          prefix_haplotypes_list=[{\'a\'}],\n          haplotypes={\'A\'},\n          expected=[{\'aA\'}],\n      ),\n      dict(\n          prefix_haplotypes_list=[{\'a\', \'b\'}],\n          haplotypes={\'A\'},\n          expected=[{\'aA\', \'bA\'}],\n      ),\n      dict(\n          prefix_haplotypes_list=[{\'a\'}],\n          haplotypes={\'A\', \'B\'},\n          expected=[{\'aA\', \'aB\'}],\n      ),\n      dict(\n          prefix_haplotypes_list=[{\'a\', \'b\'}],\n          haplotypes={\'A\', \'B\'},\n          expected=[{\'aA\', \'bB\'}, {\'aB\', \'bA\'}],\n      ),\n      dict(\n          prefix_haplotypes_list=[{\'a\', \'b\'}, {\'c\'}, {\'d\', \'e\'}],\n          haplotypes={\'A\'},\n          expected=[{\'aA\', \'bA\'}, {\'cA\'}, {\'dA\', \'eA\'}],\n      ),\n      dict(\n          prefix_haplotypes_list=[{\'a\', \'b\'}, {\'c\'}, {\'d\', \'e\'}],\n          haplotypes={\'A\', \'B\'},\n          expected=[{\'aA\', \'bB\'}, {\'aB\', \'bA\'}, {\'cA\', \'cB\'}, {\'dA\', \'eB\'},\n                    {\'dB\', \'eA\'}],\n      ),\n  )\n  def test_extend_haplotypes(self, prefix_haplotypes_list, haplotypes,\n                             expected):\n    six.assertCountEqual(\n        self, expected,\n        list(\n            haplotype_labeler.extend_haplotypes(prefix_haplotypes_list,\n                                                haplotypes)))\n\n  def test_extend_haplotypes_raises_on_empty_prefix_list(self):\n    with six.assertRaisesRegex(self, ValueError,\n                               \'prefix_haplotypes_list.*empty\'):\n      list(haplotype_labeler.extend_haplotypes([], {\'A\'}))\n\n  def test_extend_haplotypes_raises_on_empty_haplotypes(self):\n    with six.assertRaisesRegex(self, ValueError, \'haplotypes\'):\n      list(haplotype_labeler.extend_haplotypes([{\'A\'}], set()))\n\n  def test_extend_haplotypes_raises_on_too_many_haplotypes(self):\n    with six.assertRaisesRegex(self, ValueError, \'haplotypes\'):\n      list(haplotype_labeler.extend_haplotypes([{\'A\'}], {\'a\', \'b\', \'c\'}))\n\n  # The reference sequence is xAAAAAy.\n  @parameterized.parameters(\n      # Test a SNP at a few positions.\n      dict(\n          variants=[\n              _test_variant(1, alleles=(\'A\', \'C\')),\n          ],\n          allele_indices_and_expected={\n              (0,): \'xAAAAAy\',\n              (1,): \'xCAAAAy\',\n          }),\n      dict(\n          variants=[\n              _test_variant(3, alleles=(\'A\', \'C\')),\n          ],\n          allele_indices_and_expected={\n              (0,): \'xAAAAAy\',\n              (1,): \'xAACAAy\',\n          }),\n      # Test an insertion at a few positions.\n      dict(\n          variants=[\n              _test_variant(1, alleles=(\'A\', \'CC\')),\n          ],\n          allele_indices_and_expected={\n              (0,): \'xAAAAAy\',\n              (1,): \'xCCAAAAy\',\n          }),\n      dict(\n          variants=[\n              _test_variant(3, alleles=(\'A\', \'CC\')),\n          ],\n          allele_indices_and_expected={\n              (0,): \'xAAAAAy\',\n              (1,): \'xAACCAAy\',\n          }),\n      # Test an deletion at a few positions.\n      dict(\n          variants=[\n              _test_variant(1, alleles=(\'AAA\', \'A\')),\n          ],\n          allele_indices_and_expected={\n              (0,): \'xAAAAAy\',\n              (1,): \'xAAAy\',\n          }),\n      dict(\n          variants=[\n              _test_variant(3, alleles=(\'AAA\', \'A\')),\n          ],\n          allele_indices_and_expected={\n              (0,): \'xAAAAAy\',\n              (1,): \'xAAAy\',\n          }),\n      # A complete example with multiple variants.\n      dict(\n          variants=[\n              _test_variant(1, alleles=(\'A\', \'C\')),\n              _test_variant(2, alleles=(\'A\', \'CC\')),\n              _test_variant(4, alleles=(\'AA\', \'A\')),\n          ],\n          allele_indices_and_expected={\n              (0, 0, 0): \'xAAAAAy\',\n              (0, 0, 1): \'xAAAAy\',\n              (0, 1, 0): \'xACCAAAy\',\n              (0, 1, 1): \'xACCAAy\',\n              (1, 0, 0): \'xCAAAAy\',\n              (1, 0, 1): \'xCAAAy\',\n              (1, 1, 0): \'xCCCAAAy\',\n              (1, 1, 1): \'xCCCAAy\',\n          }),\n  )\n  def test_build_haplotype(self, variants, allele_indices_and_expected):\n    refseq = \'xAAAAAy\'\n    for allele_indices, expected in allele_indices_and_expected.items():\n      self.assertEqual(\n          expected,\n          haplotype_labeler.build_haplotype(\n              variants,\n              allele_indices,\n              ref=haplotype_labeler.ReferenceRegion(refseq, 0),\n              ref_start=0,\n              ref_end=len(refseq)))\n\n  @parameterized.parameters(\n      # All possible genotypes for a simple tri-allelic case.\n      (\n          dict(\n              variants=[\n                  _test_variant(11, [\'TG\', \'A\', \'TGC\'], gt),\n              ],\n              ref=haplotype_labeler.ReferenceRegion(\'TG\', 11),\n              expected_frags=expected,\n              expected_next_pos=13) for gt, expected in {\n                  # Simple bi-allelic configurations:\n                  (0, 0): {\n                      (0,): \'TG\'\n                  },\n                  (0, 1): {\n                      (0,): \'TG\',\n                      (1,): \'A\'\n                  },\n                  (1, 0): {\n                      (0,): \'TG\',\n                      (1,): \'A\'\n                  },\n                  (1, 1): {\n                      (1,): \'A\'\n                  },\n                  # Multi-allelic configurations:\n                  (0, 2): {\n                      (0,): \'TG\',\n                      (2,): \'TGC\'\n                  },\n                  (1, 2): {\n                      (1,): \'A\',\n                      (2,): \'TGC\'\n                  },\n                  (2, 2): {\n                      (2,): \'TGC\'\n                  },\n              }.items()),)\n  def test_phased_genotypes_to_haplotypes_single_variant(\n      self, variants, ref, expected_frags, expected_next_pos):\n    variants_and_genotypes = [\n        haplotype_labeler.VariantAndGenotypes(v, tuple(v.calls[0].genotype))\n        for v in variants\n    ]\n    frags, next_pos = haplotype_labeler.phased_genotypes_to_haplotypes(\n        variants_and_genotypes, ref.start, ref)\n    self.assertEqual(frags, expected_frags)\n    self.assertEqual(next_pos, expected_next_pos)\n\n  @parameterized.parameters(\n      (\'G\', \'A\'),\n      (\'GG\', \'A\'),\n      (\'GGG\', \'A\'),\n      (\'GGGG\', \'A\'),\n      (\'A\', \'G\'),\n      (\'A\', \'GG\'),\n      (\'A\', \'GGG\'),\n      (\'A\', \'GGGG\'),\n  )\n  def test_phased_genotypes_to_haplotypes_next_pos_is_correct(self, ref, alt):\n    # Check that the next_pos calculation is working.\n    pos = 10\n    for gt in [(0, 0), (0, 1), (1, 1)]:\n      _, next_pos = haplotype_labeler.phased_genotypes_to_haplotypes(\n          [\n              haplotype_labeler.VariantAndGenotypes(\n                  _test_variant(pos, [ref, alt]), gt)\n          ],\n          start=pos,\n          ref=haplotype_labeler.ReferenceRegion(ref, pos))\n      self.assertEqual(next_pos, pos + len(ref))\n\n  @parameterized.parameters(\n      # A single deletion overlapping a SNP:\n      # ref: xTG\n      # v1:   A-\n      # v2:    C\n      dict(\n          variants=[\n              _test_variant(11, [\'TG\', \'A\'], (0, 1)),\n              _test_variant(12, [\'G\', \'C\'], (0, 1)),\n          ],\n          ref=haplotype_labeler.ReferenceRegion(\'xTG\', 10),\n          expected_frags={\n              (0, 0): \'xTG\',  # haplotype 0|0.\n              (0, 1): \'xTC\',  # haplotype 0|1.\n              (1, 0): \'xA\',  # haplotype 1|0.\n              (1, 1): None,  # haplotype 1|1 => invalid.\n          },\n          expected_next_pos=13),\n      # Deletion overlapping two downstream events (SNP and insertion):\n      # ref: xTGC\n      # v1:   A--\n      # v2:    C\n      # v3:     TTT\n      dict(\n          variants=[\n              _test_variant(11, [\'TGC\', \'A\'], (0, 1)),\n              _test_variant(12, [\'G\', \'C\'], (0, 1)),\n              _test_variant(13, [\'C\', \'TTT\'], (0, 1)),\n          ],\n          ref=haplotype_labeler.ReferenceRegion(\'xTGC\', 10),\n          expected_frags={\n              (0, 0, 0): \'xTGC\',  # haplotype 0|0|0.\n              (0, 0, 1): \'xTGTTT\',  # haplotype 0|0|1.\n              (0, 1, 0): \'xTCC\',  # haplotype 0|1|0.\n              (0, 1, 1): \'xTCTTT\',  # haplotype 0|1|1.\n              (1, 0, 0): \'xA\',  # haplotype 1|0|0.\n              (1, 0, 1): None,  # haplotype 1|0|1 => invalid.\n              (1, 1, 0): None,  # haplotype 1|1|0 => invalid.\n              (1, 1, 1): None,  # haplotype 1|1|1 => invalid.\n          },\n          expected_next_pos=14),\n      # Two incompatible deletions to check that the end extension is working:\n      # pos: 01234\n      # ref: xTGCA\n      # v1:   T-\n      # v2:    G-\n      dict(\n          variants=[\n              _test_variant(11, [\'TG\', \'T\'], (0, 1)),\n              _test_variant(12, [\'GC\', \'G\'], (0, 1)),\n          ],\n          ref=haplotype_labeler.ReferenceRegion(\'xTGCA\', 10),\n          expected_frags={\n              (0, 0): \'xTGC\',  # haplotype 0|0.\n              (0, 1): \'xTG\',  # haplotype 0|1.\n              (1, 0): \'xTC\',  # haplotype 1|0.\n              (1, 1): None,  # haplotype 1|1 => invalid.\n          },\n          expected_next_pos=14),\n      # Multiple overlapping deletions with complex incompatibilities:\n      # ref: xTGCGA\n      # v1:   A--\n      # v2:    G---  [conflicts with v1]\n      # v3:     C-   [conflicts with v1 and v2]\n      # v4:      G-  [conflicts with v2 and v3, ok with v1]\n      # v5:       C  [conflicts with v2 and v4, ok with v1, v3]\n      dict(\n          variants=[\n              _test_variant(11, [\'TGC\', \'A\'], (0, 1)),\n              _test_variant(12, [\'GCGA\', \'G\'], (0, 1)),\n              _test_variant(13, [\'CG\', \'C\'], (0, 1)),\n              _test_variant(14, [\'GA\', \'G\'], (0, 1)),\n              _test_variant(15, [\'A\', \'C\'], (0, 1)),\n          ],\n          ref=haplotype_labeler.ReferenceRegion(\'xTGCGA\', 10),\n          expected_frags={\n              (0, 0, 0, 0, 0): \'xTGCGA\',  # haplotype 0|0|0|0|0.\n              (0, 0, 0, 0, 1): \'xTGCGC\',  # haplotype 0|0|0|0|1.\n              (0, 0, 0, 1, 0): \'xTGCG\',  # haplotype 0|0|0|1|0.\n              (0, 0, 0, 1, 1): None,  # haplotype 0|0|0|1|1.\n              (0, 0, 1, 0, 0): \'xTGCA\',  # haplotype 0|0|1|0|0.\n              (0, 0, 1, 0, 1): \'xTGCC\',  # haplotype 0|0|1|0|1.\n              (0, 0, 1, 1, 0): None,  # haplotype 0|0|1|1|0.\n              (0, 0, 1, 1, 1): None,  # haplotype 0|0|1|1|1.\n              (0, 1, 0, 0, 0): \'xTG\',  # haplotype 0|1|0|0|0.\n              (0, 1, 0, 0, 1): None,  # haplotype 0|1|0|0|1.\n              (0, 1, 0, 1, 0): None,  # haplotype 0|1|0|1|0.\n              (0, 1, 0, 1, 1): None,  # haplotype 0|1|0|1|1.\n              (0, 1, 1, 0, 0): None,  # haplotype 0|1|1|0|0.\n              (0, 1, 1, 0, 1): None,  # haplotype 0|1|1|0|1.\n              (0, 1, 1, 1, 0): None,  # haplotype 0|1|1|1|0.\n              (0, 1, 1, 1, 1): None,  # haplotype 0|1|1|1|1.\n              (1, 0, 0, 0, 0): \'xAGA\',  # haplotype 1|0|0|0|0.\n              (1, 0, 0, 0, 1): \'xAGC\',  # haplotype 1|0|0|0|1.\n              (1, 0, 0, 1, 0): \'xAG\',  # haplotype 1|0|0|1|0.\n              (1, 0, 0, 1, 1): None,  # haplotype 1|0|0|1|1.\n              (1, 0, 1, 0, 0): None,  # haplotype 1|0|1|0|0.\n              (1, 0, 1, 0, 1): None,  # haplotype 1|0|1|0|1.\n              (1, 0, 1, 1, 0): None,  # haplotype 1|0|1|1|0.\n              (1, 0, 1, 1, 1): None,  # haplotype 1|0|1|1|1.\n              (1, 1, 0, 0, 0): None,  # haplotype 1|1|0|0|0.\n              (1, 1, 0, 0, 1): None,  # haplotype 1|1|0|0|1.\n              (1, 1, 0, 1, 0): None,  # haplotype 1|1|0|1|0.\n              (1, 1, 0, 1, 1): None,  # haplotype 1|1|0|1|1.\n              (1, 1, 1, 0, 0): None,  # haplotype 1|1|1|0|0.\n              (1, 1, 1, 0, 1): None,  # haplotype 1|1|1|0|1.\n              (1, 1, 1, 1, 0): None,  # haplotype 1|1|1|1|0.\n              (1, 1, 1, 1, 1): None,  # haplotype 1|1|1|1|1.\n          },\n          expected_next_pos=16),\n  )\n  def test_phased_genotypes_to_haplotypes_overlapping(self, variants, ref,\n                                                      expected_frags,\n                                                      expected_next_pos):\n    variants_and_genotypes = [\n        haplotype_labeler.VariantAndGenotypes(v, tuple(v.calls[0].genotype))\n        for v in variants\n    ]\n    frags, next_pos = haplotype_labeler.phased_genotypes_to_haplotypes(\n        variants_and_genotypes, ref.start, ref)\n    self.assertEqual(frags,\n                     {k: v for k, v in expected_frags.items() if v is not None})\n    self.assertEqual(next_pos, expected_next_pos)\n\n  @parameterized.parameters(\n      # Check that simple bi-allelic matching works for all possible possible\n      # genotypes and a variety of types of alleles.\n      (\n          dict(\n              candidate_alleles=alleles,\n              truth_alleles=alleles,\n              truth_genotype=gt,\n              # Returns [0, 1] even if truth is [1, 0], so sort the genotypes for\n              # the expected value.\n              expected_genotype=sorted(gt),\n          )\n          for gt in [[0, 1], [1, 0], [1, 1]]\n          for alleles in [[\'A\', \'C\'], [\'ACC\', \'A\'], [\'A\', \'ATG\'], [\'AC\', \'GT\']]\n      ),)\n  def test_single_variants(self, candidate_alleles, truth_alleles,\n                           truth_genotype, expected_genotype):\n    candidate = _test_variant(42, candidate_alleles)\n    truth = _test_variant(42, truth_alleles, truth_genotype)\n    ref_allele = sorted([candidate_alleles[0], truth_alleles[0]], key=len)[0]\n    self.assertGetsCorrectLabels(\n        candidates=[candidate],\n        true_variants=[truth],\n        ref=haplotype_labeler.ReferenceRegion(\'x\' + ref_allele + \'y\', 41),\n        expected_genotypes=[expected_genotype])\n\n  @parameterized.parameters(\n      dict(true_genotype=(0, 1)),\n      dict(true_genotype=(1, 1)),\n  )\n  def test_no_candidates_only_truth_variants(self, true_genotype):\n    labeling = haplotype_labeler.find_best_matching_haplotypes(\n        candidates=[],\n        truths=[_test_variant(42, gt=true_genotype)],\n        ref=haplotype_labeler.ReferenceRegion(\'xAy\', 41))\n    self.assertIsNotNone(labeling)\n\n    # Since we don\'t have any candidates, our relabeled variants should be [].\n    self.assertEqual(labeling.candidates_with_assigned_genotypes(), [])\n\n  @parameterized.parameters(\n      dict(empty=\'variants\'),\n      dict(empty=\'truth\'),\n  )\n  def test_no_variants_or_truth_is_fast(self, empty):\n    # This test will time out if we aren\'t able to efficiently handle the case\n    # where we have a lot of candidate or truth variants but none of the other.\n    many_variants = [_test_variant(i, gt=(0, 1)) for i in range(10, 50)]\n    if empty == \'truth\':\n      variants, truth = many_variants, []\n    else:\n      variants, truth = [], many_variants\n\n    labeling = haplotype_labeler.find_best_matching_haplotypes(\n        candidates=variants,\n        truths=truth,\n        ref=haplotype_labeler.ReferenceRegion(\'A\' * 50, 10))\n\n    # Since we don\'t have any truth variants, all of the variants should get a\n    # (0, 0) [i.e., hom-ref] genotype assigned.\n    self.assertEqual(\n        haplotype_labeler._variant_genotypes(\n            labeling.candidates_with_assigned_genotypes()),\n        [(0, 0)] * len(variants))\n\n  def test_genotype_options_for_variants_truth_enum(self):\n    # Check all configurations for the TRUTH enumeration:\n    enum_type = haplotype_labeler.EnumerationType.TRUTH\n\n    # Bi-allelic cases.\n    self.assertEqual(\n        haplotype_labeler.genotype_options_for_variants(\n            [_test_variant(1, gt=(0, 0))], enum_type), [{(0, 0)}])\n    self.assertEqual(\n        haplotype_labeler.genotype_options_for_variants(\n            [_test_variant(1, gt=(0, 1))], enum_type), [{(0, 0), (0, 1)}])\n    self.assertEqual(\n        haplotype_labeler.genotype_options_for_variants(\n            [_test_variant(1, gt=(1, 1))], enum_type), [{(0, 0), (0, 1),\n                                                         (1, 1)}])\n\n    # Multi-allelic cases.\n    self.assertEqual(\n        haplotype_labeler.genotype_options_for_variants(\n            [_test_variant(1, alleles=(\'A\', \'C\', \'G\'), gt=(0, 0))], enum_type),\n        [{(0, 0)}])\n    self.assertEqual(\n        haplotype_labeler.genotype_options_for_variants(\n            [_test_variant(1, alleles=(\'A\', \'C\', \'G\'), gt=(0, 1))], enum_type),\n        [{(0, 0), (0, 1)}])\n    self.assertEqual(\n        haplotype_labeler.genotype_options_for_variants(\n            [_test_variant(1, alleles=(\'A\', \'C\', \'G\'), gt=(1, 1))], enum_type),\n        [{(0, 0), (0, 1), (1, 1)}])\n    self.assertEqual(\n        haplotype_labeler.genotype_options_for_variants(\n            [_test_variant(1, alleles=(\'A\', \'C\', \'G\'), gt=(0, 2))], enum_type),\n        [{(0, 0), (0, 2)}])\n    self.assertEqual(\n        haplotype_labeler.genotype_options_for_variants(\n            [_test_variant(1, alleles=(\'A\', \'C\', \'G\'), gt=(2, 2))], enum_type),\n        [{(0, 0), (0, 2), (2, 2)}])\n    self.assertEqual(\n        haplotype_labeler.genotype_options_for_variants(\n            [_test_variant(1, alleles=(\'A\', \'C\', \'G\'), gt=(1, 2))], enum_type),\n        [{(0, 0), (0, 1), (0, 2), (1, 2)}])\n\n  def test_genotype_options_for_variants_candidates_enum(self):\n    # Check all configurations for the CANDIDATES enumeration:\n    enum_type = haplotype_labeler.EnumerationType.CANDIDATES\n    # Note we don\'t need to provide a genotype for the candidate enumeration.\n    self.assertEqual(\n        haplotype_labeler.genotype_options_for_variants([_test_variant(1)],\n                                                        enum_type), [{(0, 0),\n                                                                      (0, 1),\n                                                                      (1, 1)}])\n    self.assertEqual(\n        haplotype_labeler.genotype_options_for_variants(\n            [_test_variant(1, alleles=(\'A\', \'C\', \'G\'))], enum_type), [{(0, 0),\n                                                                       (0, 1),\n                                                                       (1, 1),\n                                                                       (0, 2),\n                                                                       (1, 2),\n                                                                       (2, 2)}])\n\n  def test_genotype_options_for_variants_only_hom_ref(self):\n    # Check all configurations for the ONLY_HOM_REF enumeration:\n    enum_type = haplotype_labeler.EnumerationType.ONLY_HOM_REF\n    variants = [\n        _test_variant(1),\n        _test_variant(1, gt=(0, 1)),\n        _test_variant(1, gt=(1, 1)),\n        _test_variant(1, alleles=(\'A\', \'C\', \'G\')),\n        _test_variant(1, alleles=(\'A\', \'C\', \'G\'), gt=(1, 2)),\n    ]\n    self.assertEqual(\n        haplotype_labeler.genotype_options_for_variants(variants, enum_type),\n        [{(0, 0)}] * len(variants))\n\n  def test_genotype_options_for_variants_no_variants(self):\n    # All enumeration types return [] if not provided with any variants.\n    for enum_type in haplotype_labeler.EnumerationType:\n      self.assertEqual(\n          haplotype_labeler.genotype_options_for_variants([], enum_type), [])\n\n  @parameterized.parameters(\n      dict(\n          candidate_alleles=[\'A\', \'C\'],\n          truth_alleles=[\'A\', \'G\', \'C\'],\n          truth_genotypes_and_expected={\n              (0, 2): (0, 1),  # A/C => 0/C\n              (1, 2): (0, 1),  # G/C => 0/C\n              (1, 1): (0, 0),  # G/G => 0/0\n              (2, 2): (1, 1),  # C/C => C/C\n          }),\n      dict(\n          candidate_alleles=[\'A\', \'C\'],\n          truth_alleles=[\'A\', \'C\', \'G\'],\n          truth_genotypes_and_expected={\n              (0, 1): (0, 1),  # A/C => 0/C\n              (2, 1): (0, 1),  # G/C => 0/C\n              (2, 2): (0, 0),  # G/G => 0/0\n              (1, 1): (1, 1),  # C/C => C/C\n          },\n      ),\n      dict(\n          candidate_alleles=[\'A\', \'TT\', \'TTT\'],\n          truth_alleles=[\'A\', \'C\', \'G\'],\n          truth_genotypes_and_expected={\n              (i, j): (0, 0) for i, j in itertools.combinations([0, 1, 2], 2)\n          }),\n      # Here the candidate is also multi-allelic\n      dict(\n          candidate_alleles=[\'A\', \'G\', \'C\'],\n          truth_alleles=[\'A\', \'C\', \'G\'],\n          truth_genotypes_and_expected={\n              (0, 1): (0, 2),\n              (0, 2): (0, 1),\n              (1, 1): (2, 2),\n              (1, 2): (1, 2),\n              (2, 2): (1, 1),\n          },\n      ),\n  )\n  def test_multi_allelic(self, candidate_alleles, truth_alleles,\n                         truth_genotypes_and_expected):\n    candidate = _test_variant(42, candidate_alleles)\n    for true_gt, expected_gt in truth_genotypes_and_expected.items():\n      truth = _test_variant(42, truth_alleles, true_gt)\n      ref_allele = sorted([candidate_alleles[0], truth_alleles[0]], key=len)[0]\n      self.assertGetsCorrectLabels(\n          candidates=[candidate],\n          true_variants=[truth],\n          ref=haplotype_labeler.ReferenceRegion(\'x\' + ref_allele + \'y\', 41),\n          expected_genotypes=[expected_gt])\n\n  def test_false_variants_get_homref_genotype(self):\n    ref = haplotype_labeler.ReferenceRegion(\'xACGTAy\', 10)\n    v1 = _test_variant(11, [\'A\', \'T\'], [0, 1])\n    v2 = _test_variant(13, [\'G\', \'GG\'], [1, 1])\n    all_fps = [\n        _test_variant(12, [\'C\', \'G\'], [0, 0]),\n        _test_variant(14, [\'T\', \'A\'], [0, 0]),\n        _test_variant(15, [\'A\', \'AA\'], [0, 0]),\n    ]\n    for n_fps in range(1, len(all_fps) + 1):\n      for fps in itertools.combinations(all_fps, n_fps):\n        candidates = variant_utils.sorted_variants([v1, v2] + list(fps))\n        self.assertGetsCorrectLabels(\n            candidates=candidates,\n            true_variants=[v1, v2],\n            ref=ref,\n            expected_genotypes=haplotype_labeler._variant_genotypes(candidates))\n\n  def test_false_negatives(self):\n    ref = haplotype_labeler.ReferenceRegion(\'xACGTAy\', 10)\n    v1 = _test_variant(11, [\'A\', \'T\'], [0, 1])\n    v2 = _test_variant(13, [\'G\', \'GG\'], [1, 1])\n    all_fns = [\n        _test_variant(12, [\'C\', \'G\'], [0, 1]),\n        _test_variant(14, [\'T\', \'A\', \'G\'], [1, 2]),\n        _test_variant(15, [\'A\', \'AA\'], [1, 1]),\n    ]\n    for n_fns in [1]:\n      for fns in itertools.combinations(all_fns, n_fns):\n        candidates = [v1, v2]\n        self.assertGetsCorrectLabels(\n            candidates=candidates,\n            true_variants=variant_utils.sorted_variants([v1, v2] + list(fns)),\n            ref=ref,\n            expected_genotypes=haplotype_labeler._variant_genotypes(candidates))\n\n  # example 20:3528533 and 20:3528534\n  def test_example1(self):\n    self.assertGetsCorrectLabels(\n        candidates=[\n            _test_variant(3528531, [\'ATAG\', \'A\']),\n            _test_variant(3528537, [\'A\', \'ATT\']),\n        ],\n        true_variants=[\n            _test_variant(3528533, [\'A\', \'T\'], [1, 1]),\n            _test_variant(3528534, [\'G\', \'A\'], [1, 1]),\n            _test_variant(3528536, [\'TA\', \'T\'], [1, 1]),\n        ],\n        ref=haplotype_labeler.ReferenceRegion(\'xATAGTTATC\', 3528530),\n        expected_genotypes=[\n            [1, 1],\n            [1, 1],\n        ])\n\n  # example 20:4030071\n  def test_example2(self):\n    self.assertGetsCorrectLabels(\n        candidates=[\n            _test_variant(4030067, [\'TC\', \'T\']),\n            _test_variant(4030072, [\'C\', \'G\']),\n        ],\n        true_variants=[\n            _test_variant(4030071, [\'CC\', \'G\'], [1, 1]),\n        ],\n        ref=haplotype_labeler.ReferenceRegion(\'xTCCCCCA\', 4030066),\n        expected_genotypes=[\n            [1, 1],\n            [1, 1],\n        ])\n\n  # example 20:4568152\n  def test_example3(self):\n    self.assertGetsCorrectLabels(\n        candidates=[\n            _test_variant(4568151, [\'AC\', \'A\']),\n            _test_variant(4568154, [\'TG\', \'T\']),\n            _test_variant(4568156, [\'G\', \'T\']),\n            _test_variant(4568157, [\'A\', \'ATACCCTTT\']),\n        ],\n        true_variants=[\n            _test_variant(4568152, [\'C\', \'A\'], [1, 1]),\n            _test_variant(4568153, [\'A\', \'T\'], [1, 1]),\n            _test_variant(4568155, [\'G\', \'A\'], [1, 1]),\n            _test_variant(4568156, [\'G\', \'T\'], [1, 1]),\n            _test_variant(4568157, [\'A\', \'ACCCTTT\'], [1, 1]),\n        ],\n        ref=haplotype_labeler.ReferenceRegion(\'xACATGGATGGA\', 4568150),\n        expected_genotypes=[\n            [1, 1],\n            [1, 1],\n            [1, 1],\n            [1, 1],\n        ])\n\n  # example 20:1689636, 20:1689639, 20:1689640, 20:1689641\n  def test_example4(self):\n    # CTGTAAACAGAA [phased alts] + CGTGAATGAAA [phased ref]\n    self.assertGetsCorrectLabels(\n        candidates=[\n            _test_variant(1689633, [\'C\', \'CT\']),\n            _test_variant(1689635, [\'TG\', \'T\']),\n            _test_variant(1689638, [\'ATG\', \'A\']),\n            _test_variant(1689641, [\'A\', \'ACAG\']),\n        ],\n        true_variants=[\n            _test_variant(1689633, [\'C\', \'CT\'], [1, 0]),\n            _test_variant(1689636, [\'G\', \'A\'], [1, 0]),\n            _test_variant(1689639, [\'T\', \'C\'], [1, 0]),\n            _test_variant(1689640, [\'G\', \'A\'], [1, 0]),\n            _test_variant(1689641, [\'A\', \'G\'], [1, 0]),\n        ],\n        ref=haplotype_labeler.ReferenceRegion(\'xCGTGAATGAAA\', 1689632),\n        expected_genotypes=[\n            [0, 1],\n            [0, 1],\n            [0, 1],\n            [0, 1],\n        ])\n\n  # 20:2401511\n  def test_example5(self):\n    self.assertGetsCorrectLabels(\n        candidates=[\n            _test_variant(2401510, [\'ATGT\', \'A\']),\n            _test_variant(2401515, [\'C\', \'T\']),\n        ],\n        true_variants=[\n            _test_variant(2401511, [\'TG\', \'A\'], [1, 1]),\n            _test_variant(2401513, [\'TAC\', \'T\'], [1, 1]),\n        ],\n        ref=haplotype_labeler.ReferenceRegion(\'xATGTACACAG\', 2401509),\n        expected_genotypes=[\n            [1, 1],\n            [1, 1],\n        ])\n\n  # 20:2525695: genotype assign was incorrect in a previous run. This is because\n  # the candidate variants overlap:\n  #\n  # ref: AAATT\n  #  v1:  A--\n  #  v2:   A-\n  #\n  # And this is causing us to construct incorrect haplotypes.\n  def test_example6(self):\n    self.assertGetsCorrectLabels(\n        candidates=[\n            _test_variant(2525696, [\'AAT\', \'A\']),\n            _test_variant(2525697, [\'AT\', \'T\']),\n        ],\n        true_variants=[\n            _test_variant(2525696, [\'AAT\', \'A\'], [0, 1]),\n        ],\n        ref=haplotype_labeler.ReferenceRegion(\'xAATT\', 2525695),\n        expected_genotypes=[\n            [0, 1],\n            [0, 0],\n        ])\n\n  # Variants were getting incorrect genotypes due to complex region.\n  #\n  # variants: candidates\n  #   20:279768:G->C gt=(-1, -1)\n  #   20:279773:ATA->C/CTA gt=(-1, -1)\n  # variants: truth\n  #   20:279773:A->C gt=(1, 0)\n  #\n  # pos    : 789012345678901\n  # ref    : CGCCCCATACCTTTT\n  # truth  :       C          => CGCCCCCTACCTTTT\n  # DV 1   :  C               => CCCCCCATACCTTTT [bad]\n  # DV 2.a :       C--        => CGCCCCCCTTTT    [bad]\n  # DV 2.b :       CTA        => CGCCCCCTACCTTTT [match]\n  #\n  def test_example7(self):\n    self.assertGetsCorrectLabels(\n        candidates=[\n            _test_variant(279768, [\'G\', \'C\']),\n            _test_variant(279773, [\'ATA\', \'C\', \'CTA\']),\n        ],\n        true_variants=[\n            _test_variant(279773, [\'A\', \'C\'], [0, 1]),\n        ],\n        ref=haplotype_labeler.ReferenceRegion(\'CGCCCCATACCTTTT\', 279767),\n        expected_genotypes=[\n            [0, 0],\n            [0, 2],\n        ])\n\n  # redacted\n  # that accepts a whole region of variants so we make sure it divides up the\n  # problem into more fine-grained pieces that run quickly. The current call is\n  # to a lower-level API that doesn\'t do variant chunking.\n  # Commented out because this remains super slow.\n  # def test_super_slow_example(self):\n  #   self.assertGetsCorrectLabels(\n  #       candidates=[\n  #           _test_variant(32274452, [\'C\', \'G\']),\n  #           _test_variant(32274453, [\'T\', \'G\']),\n  #           _test_variant(32274456, [\'A\', \'G\']),\n  #           _test_variant(32274459, [\'C\', \'G\']),\n  #           _test_variant(32274461, [\'T\', \'G\']),\n  #           _test_variant(32274465, [\'GACA\', \'G\']),\n  #           _test_variant(32274467, [\'CA\', \'C\']),\n  #           _test_variant(32274470, [\'C\', \'G\']),\n  #           _test_variant(32274473, [\'A\', \'G\']),\n  #           _test_variant(32274474, [\'AC\', \'A\']),\n  #           _test_variant(32274475, [\'C\', \'A\']),\n  #           _test_variant(32274477, [\'T\', \'A\']),\n  #           _test_variant(32274480, [\'G\', \'C\']),\n  #       ],\n  #       true_variants=[\n  #           _test_variant(32274470, [\'C\', \'G\'], (1, 1)),\n  #       ],\n  #       ref=haplotype_labeler.ReferenceRegion(\n  #           \'GCTGGAGGCGTGGGGACACCGGAACATAGGCCCCGCCCCGCCCCGACGC\', 32274451),\n  #       expected_genotypes=[\n  #           [0, 0],\n  #           [0, 0],\n  #           [0, 0],\n  #           [0, 0],\n  #           [0, 0],\n  #           [0, 0],\n  #           [0, 0],\n  #           [1, 1],\n  #           [0, 0],\n  #           [0, 0],\n  #           [0, 0],\n  #           [0, 0],\n  #           [0, 0],\n  #       ])\n\n  # Variants were getting incorrect genotypes in an exome callset.\n  #\n  # ref: AGACACACACACACAAAAAAAAATCATAAAATGAAG, start=214012389\n  # candidates 2:214012390:G->GAC\n  # candidates 2:214012402:CAA->C\n  # candidates 2:214012404:A->C\n  # true_variants 2:214012404:A->C\n  #\n  # 2:214012390:G->GAC => gt=(1, 1) new_label=2 old_label=0 alts=[0]\n  # 2:214012402:CAA->C => gt=(1, 1) new_label=2 old_label=0 alts=[0]\n  # 2:214012404:A->C => gt=(0, 0) new_label=0 old_label=2 alts=[0]\n  #\n  #           90--------- 0---------10--------20---\n  # pos    : 90  1234567890123456789012345678901234\n  # ref    : AG  ACACACACACACAAAAAAAAATCATAAAATGAAG\n  # truth  :                  C => AGACACACACACACACAAAAAAATCATAAAATGAAG\n  # DV 1   :  GAC               => [doesn\'t match]\n  # DV 2   :                C-- => [doesn\'t match]\n  # DV 1+2 : AGACACACACACACAC  AAAAAAATCATAAAATGAAG\n  # DV 1+2 :                    => AGACACACACACACACAAAAAAATCATAAAATGAAG [match]\n  # DV 3   :                  C => AGACACACACACACACAAAAAAATCATAAAATGAAG [match]\n  #\n  # So this is an interesting case. G->GAC + CAA->C matches the true haplotype,\n  # and the SNP itself gets assigned a FP status since we can have either two\n  # FPs (dv1 and dv2 candidates) or have just one (dv3). What\'s annoying here is\n  # that DV3 exactly matches the variant as described in the truth set. It\'s\n  # also strange that we\'ve generated multiple equivalent potential variants\n  # here.\n  #\n  # This test ensures that we are picking the most parsimonous genotype\n  # assignment (e.g., fewest number of TPs) needed to explain the truth, after\n  # accounting for minimizing the number of FNs and FPs.\n  def test_exome_variants_multiple_equivalent_representations(self):\n    self.assertGetsCorrectLabels(\n        candidates=[\n            _test_variant(214012390, [\'G\', \'GAC\']),\n            _test_variant(214012402, [\'CAA\', \'C\']),\n            _test_variant(214012404, [\'A\', \'C\']),\n        ],\n        true_variants=[\n            _test_variant(214012404, [\'A\', \'C\'], [1, 1]),\n        ],\n        ref=haplotype_labeler.ReferenceRegion(\'AGACACACACACACAAAAAAAAATCAT\',\n                                              214012389),\n        expected_genotypes=[\n            [0, 1],\n            [0, 1],\n            [0, 1],\n            # This configuration makes the most sense but we cannot choose it\n            # if we want to minimize the number of FNs, FPs, and then TPs.\n            # [0, 0],\n            # [0, 0],\n            # [1, 1],\n        ])\n\n  # Variant group: 5 candidates 2 truth variants\n  # ref: ReferenceRegion(bases=TGTTTTTTTTTAAAAAAATTATTTCTTCTTT, start=167012239)\n  #   candidates 4:167012240:GT->G\n  #   candidates 4:167012246:TTTT->A\n  #   candidates 4:167012247:T->A\n  #   candidates 4:167012248:T->A\n  #   candidates 4:167012249:T->A\n  #   true_variants 4:167012240:GTTT->G/GTT [2, 1]\n  #   true_variants 4:167012249:T->A/TAA [2, 1]\n  #   4:167012240:GT->G => gt=(1, 1) new_label=2 old_label=1 alts=[0]\n  #   4:167012246:TTTT->A => gt=(0, 0) new_label=0 old_label=0 alts=[0]\n  #   4:167012247:T->A => gt=(0, 0) new_label=0 old_label=0 alts=[0]\n  #   4:167012248:T->A => gt=(0, 1) new_label=1 old_label=0 alts=[0]\n  #   4:167012249:T->A => gt=(1, 1) new_label=2 old_label=1 alts=[0]\n  def test_exome_complex_example(self):\n    self.assertGetsCorrectLabels(\n        candidates=[\n            _test_variant(167012240, [\'GT\', \'G\']),\n            _test_variant(167012246, [\'TTTT\', \'A\']),\n            _test_variant(167012247, [\'T\', \'A\']),\n            _test_variant(167012248, [\'T\', \'A\']),\n            _test_variant(167012249, [\'T\', \'A\']),\n        ],\n        true_variants=[\n            _test_variant(167012240, [\'GTTT\', \'G\', \'GTT\'], [1, 2]),\n            _test_variant(167012249, [\'T\', \'A\', \'TAA\'], [1, 2]),\n        ],\n        ref=haplotype_labeler.ReferenceRegion(\'TGTTTTTTTTTAAAAAAATTATTTCTTCTTT\',\n                                              167012239),\n        expected_genotypes=[\n            [1, 1],\n            [0, 0],\n            [0, 0],\n            [0, 1],\n            [1, 1],\n        ])\n\n  # ref: GGGTGTGTGTGTGTGTGTGTGTGTGTGCGTGTGTGTGTTTGTGTTG, start=9508942\n  #   candidates 20:9508943:GGT->G\n  #   candidates 20:9508967:T->C/TGC\n  #   candidates 20:9508967:T->C/TGC\n  #   candidates 20:9508967:T->C/TGC\n  #   true_variants 20:9508943:GGT->G [0, 1]\n  #   true_variants 20:9508967:T->C/TGC [1, 2]\n  #   20:9508943:GGT->G => gt=(0, 0) new_label=0 old_label=1 alts=[0\n  #   20:9508967:T->C/TGC => gt=(1, 1) new_label=2 old_label=1 alts=[0]\n  #   20:9508967:T->C/TGC => gt=(1, 1) new_label=0 old_label=1 alts=[1]\n  #   20:9508967:T->C/TGC => gt=(1, 1) new_label=2 old_label=2 alts=[0, 1]\n  #\n  # This test fixes a bug where we weren\'t scoring our matches properly.\n  # Previously we were not accounting for FPs in our score, so we were taking\n  # a match with 0 FN, 1 FP, 1 TP over one with 0 FN, 0 FP, and 2 TP!\n  #\n  #      40------50--------60---------\n  # pos: 2345678901234567890123456789012345678901234567\n  # ref: GGGTGTGTGTGTGTGTGTGTGTGTGTGCGTGTGTGTGTTTGTGTTG\n  # t1:   G--\n  # t2a:                          C\n  # t2b:                          Tgc\n  #\n  def test_bad_scoring_bug(self):\n    self.assertGetsCorrectLabels(\n        candidates=[\n            _test_variant(9508943, [\'GGT\', \'G\']),\n            _test_variant(9508967, [\'T\', \'C\', \'TGC\']),\n        ],\n        true_variants=[\n            _test_variant(9508943, [\'GGT\', \'G\'], [0, 1]),\n            _test_variant(9508967, [\'T\', \'C\', \'TGC\'], [1, 2]),\n        ],\n        ref=haplotype_labeler.ReferenceRegion(\n            \'GGGTGTGTGTGTGTGTGTGTGTGTGTGCGTGTGTGTGTTTGTGTTG\', 9508942),\n        expected_genotypes=[\n            [0, 1],\n            [1, 2],\n        ])\n\n  def test_variants_at_edge_of_contig_work_end_to_end(self):\n    # This test checks that we can label end-to-end variants at occur at the\n    # start and at the end of a chromosome. This is unlikely in humans but can\n    # occur in bacterial genomes. See internal for a motivating example.\n    self.assertGetsCorrectLabels(\n        candidates=[\n            # At chrom start.\n            _test_variant(0, [\'A\', \'G\']),\n            # At chrom end. I\'ve included an insertion here because that\'s a\n            # common representation when there are bases at the start.\n            _test_variant(3, [\'T\', \'C\', \'TCCC\']),\n        ],\n        true_variants=[\n            _test_variant(0, [\'A\', \'G\'], [1, 1]),\n            _test_variant(3, [\'T\', \'C\'], [0, 1]),\n        ],\n        ref=haplotype_labeler.ReferenceRegion(\'ACGT\', 0),\n        expected_genotypes=[\n            [1, 1],\n            [0, 1],\n        ])\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/labeler/labeled_examples_to_vcf.py,3,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n# pylint: disable=line-too-long\nr""""""Converts labeled DeepVariant examples protos into a VCF file.\n\n./blaze-py3/bin/learning/genomics/deepvariant/labeler/labeled_examples_to_vcf \\\n  --ref\n  $(pwd)/learning/genomics/deepvariant/testdata/ucsc.hg19.chr20.unittest.fasta.gz\n  \\\n  --examples\n  $(pwd)/learning/genomics/deepvariant/testdata/golden.training_examples.tfrecord\n  \\\n  --output_vcf /tmp/golden_training.vcf\n""""""\n# pylint: enable=line-too-long\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport itertools\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom third_party.nucleus.io import fasta\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.util import variant_utils\nfrom third_party.nucleus.util import variantcall_utils\n\nfrom deepvariant import dv_vcf_constants\nfrom deepvariant import tf_utils\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    \'ref\', None,\n    \'Required. Genome reference. Used to get the reference contigs for the \'\n    \'VCF file.\')\nflags.DEFINE_string(\n    \'examples\', None,\n    \'Required. Path to labeled, DeepVariant tf.Example protos.\')\nflags.DEFINE_string(\'output_vcf\', None,\n                    \'Required. Path where we will write out output VCF.\')\nflags.DEFINE_string(\n    \'sample_name\', \'\',\n    \'The sample name to write into the VCF. By default this is None, \'\n    \'indicating we will use the call_set_name of the sample encoded in the \'\n    \'example variant.\')\nflags.DEFINE_integer(\n    \'max_records\', -1,\n    \'If provided, we will only read in at most max_record examples for \'\n    \'conversion to VCF.\')\nflags.DEFINE_integer(\n    \'log_every\', 10000,\n    \'How frequently should we provide updates on the conversion process? We \'\n    \'will log our conversion of every `log_every` variants.\')\n\n\ndef _example_sort_key(example):\n  return variant_utils.variant_range_tuple(tf_utils.example_variant(example))\n\n\ndef examples_to_variants(examples_path, max_records=None):\n  """"""Yields Variant protos from the examples in examples_path.\n\n  This function reads in tf.Examples produced by DeepVariant from examples_path,\n  which may contain a sharded spec, sorts them, selects a representive example\n  when there are multiple versions representing different alt_alleles, and\n  yields the example_variant field from those examples.\n\n  Args:\n    examples_path: str. Path, or sharded spec, to labeled tf.Examples produced\n      by DeepVariant in training mode.\n    max_records: int or None. Maximum number of records to read, or None, to\n      read all of the records.\n\n  Yields:\n    nucleus.protos.Variant protos in coordinate-sorted order.\n\n  Raises:\n    ValueError: if we find a Variant in any example that doesn\'t have genotypes.\n  """"""\n  examples = tfrecord.read_tfrecords(examples_path, max_records=max_records)\n  variants = sorted((tf_utils.example_variant(example) for example in examples),\n                    key=variant_utils.variant_range_tuple)\n\n  for _, group in itertools.groupby(variants,\n                                    variant_utils.variant_range_tuple):\n    variant = next(group)\n    if not variantcall_utils.has_genotypes(variant_utils.only_call(variant)):\n      raise ValueError(\n          (\'Variant {} does not have any genotypes. This tool only works with \'\n           \'variants that have been labeled.\').format(\n               variant_utils.variant_key(variant)))\n    yield variant\n\n\ndef peek_sample_name(variants_iter):\n  """"""Gets the call_set_name from the first Variant of variants_iter.\n\n  Args:\n    variants_iter: iterable[nucleus.protos.Variant]. Our source of variants.\n\n  Returns:\n    tuple of (str, iterable[Variant]). The first element is the call_set_name of\n    the first variant of variants_iter, or \'UNKNOWN\' if the iterable is empty.\n    The second is a new iterable that yields the same elements of variant_iter,\n    in the same order, which is necessary to return as we need to peek into\n    the original iterator.\n  """"""\n  try:\n    first = next(variants_iter)\n    return first.calls[0].call_set_name, itertools.chain([first], variants_iter)\n  except StopIteration:\n    # No variants, just return a dummy value.\n    return \'UNKNOWN\', iter([])\n\n\ndef main(argv):\n  del argv\n\n  contigs = fasta.IndexedFastaReader(FLAGS.ref).header.contigs\n  max_records = FLAGS.max_records if FLAGS.max_records >= 0 else None\n  variants_iter = examples_to_variants(FLAGS.examples, max_records=max_records)\n\n  if not FLAGS.sample_name:\n    sample_name, variants_iter = peek_sample_name(variants_iter)\n  else:\n    sample_name = FLAGS.sample_name\n  header = dv_vcf_constants.deepvariant_header(\n      contigs=contigs, sample_names=[sample_name])\n  with vcf.VcfWriter(FLAGS.output_vcf, header=header) as writer:\n    for variant in variants_iter:\n      variant.calls[0].call_set_name = sample_name\n      logging.log_every_n(logging.INFO, \'Converted %s\', FLAGS.log_every,\n                          variant_utils.variant_key(variant))\n      writer.write(variant)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flags_as_required([\n      \'examples\',\n      \'ref\',\n      \'output_vcf\',\n  ])\n  app.run(main)\n'"
deepvariant/labeler/labeled_examples_to_vcf_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant.labeled_examples_to_vcf.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl import flags\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport six\n\nfrom third_party.nucleus.io import vcf\n\nfrom third_party.nucleus.testing import test_utils\nfrom deepvariant import testdata\nfrom deepvariant.labeler import labeled_examples_to_vcf\nfrom deepvariant.testing import flagsaver\n\nFLAGS = flags.FLAGS\n\n\ndef setUpModule():\n  testdata.init()\n\n\nclass ExamplesToVCFUnitTest(parameterized.TestCase):\n\n  @flagsaver.FlagSaver\n  def test_end2end(self):\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.examples = testdata.GOLDEN_TRAINING_EXAMPLES + \'@3\'  # Sharded.\n    FLAGS.output_vcf = test_utils.test_tmpfile(\'examples_to_vcf.vcf\')\n\n    labeled_examples_to_vcf.main(0)\n\n    self.assertEqual(\n        open(FLAGS.output_vcf).readlines(),\n        open(testdata.deepvariant_testdata(\n            \'golden.training_examples.vcf\')).readlines())\n\n  @flagsaver.FlagSaver\n  def test_sample_name_flag(self):\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.examples = testdata.GOLDEN_TRAINING_EXAMPLES\n    FLAGS.sample_name = \'sample_name\'\n    FLAGS.output_vcf = test_utils.test_tmpfile(\'no_sample_name.vcf\')\n\n    labeled_examples_to_vcf.main(0)\n\n    with vcf.VcfReader(FLAGS.output_vcf) as vcf_reader:\n      self.assertEqual(\n          list(vcf_reader.header.sample_names), [FLAGS.sample_name])\n\n  @flagsaver.FlagSaver\n  def test_raises_for_unlabeled_examples(self):\n    FLAGS.ref = testdata.CHR20_FASTA\n    FLAGS.examples = testdata.GOLDEN_CALLING_EXAMPLES\n    FLAGS.output_vcf = test_utils.test_tmpfile(\'unlabeled.vcf\')\n\n    with six.assertRaisesRegex(\n        self, ValueError,\n        (\'Variant .* does not have any genotypes. This tool only works with \'\n         \'variants that have been labeled\')):\n      labeled_examples_to_vcf.main(0)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/labeler/positional_labeler.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""variant_labeler for DeepVariant.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom absl import logging\n\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import variant_utils\nfrom third_party.nucleus.util import variantcall_utils\nfrom deepvariant.labeler import variant_labeler\n\n\nclass PositionalVariantLabeler(variant_labeler.VariantLabeler):\n  """"""Finds matching ""truth"" variants using a position-specific labeler.\n\n  This is the original variant labeler from DeepVariant used up until v0.5,\n  which assigns labels to variant calls by matching the chrom:position of a\n  candidate variant with ones in truth, and then if one exists, assigns the\n  label based on the genotype of the matched truth variant. This method works\n  reasonably well but cannot handle complex representational differences between\n  the candidate variants and the truth variants.\n  """"""\n\n  def __init__(self, truth_vcf_reader, confident_regions=None):\n    """"""Creates a new VariantLabeler.\n\n    Args:\n      truth_vcf_reader: a VcfReader object that points to our truth variant set.\n      confident_regions: A RangeSet containing all of the confidently called\n        regions. A variant that falls outside of one of these regions will be\n        receive a special not-confident marker. If None, the confident regions\n        constraint won\'t be enforced, and all variants will be included.\n\n    Raises:\n      ValueError: if vcf_reader is None.\n    """"""\n    super(PositionalVariantLabeler, self).__init__(\n        truth_vcf_reader=truth_vcf_reader, confident_regions=confident_regions)\n\n  def label_variants(self, variants, region=None):\n    for variant in variants:\n      is_confident, truth_variant = self._match(\n          variant_utils.unphase_all_genotypes(variant))\n\n      genotype = None\n      if truth_variant is not None:\n        genotype = _genotype_from_matched_truth(variant, truth_variant)\n\n      yield variant_labeler.VariantLabel(\n          is_confident=is_confident, variant=variant, genotype=genotype)\n\n  def _match(self, variant):\n    """"""Get a truth variant matching variant.\n\n    A matching variant is defined here as one that starts at the same position\n    on the genome as variant. The best match is then narrowed down by finding\n    the variant with a matching alt allele, if it exists, otherwise the first\n    matching variant is used regardless of alt alleles. This allows the client\n    to make decisions on how to translate a matched between variant and\n    truth_variant into a label (e.g. by comparing the alleles).\n\n    This code will emit a logging.info() if it detects multiple\n    variants with the same chrom/start as variant provided by the\n    vcf_reader and return the best matching variant as described above. Though\n    technically correct - VCF allows this - most files in practice merge\n    variants that occur at the same location into a single multi-allelic variant\n    record. So this assumption is reasonable.\n\n    Args:\n      variant: Our candidate third_party.nucleus.protos.Variant variant.\n\n    Returns:\n      A tuple of (match_status, truth_variant) where match_status is True if\n      we are confident in our truth_variant call or False if not. truth_variant\n      is a third_party.nucleus.protos.Variant object of\n      the truth variant that matched\n      variant, or None if none was found and we aren\'t confident in being\n      hom-ref here, or a synthetic variant with the same position and alleles as\n      variant but with a hom-ref genotype.\n    """"""\n    matched_variant = self._find_matching_variant_in_reader(variant)\n    confident_or_no_constraint = (\n        self._confident_regions is None or\n        self._confident_regions.variant_overlaps(\n            variant, empty_set_return_value=False))\n    if matched_variant is None and confident_or_no_constraint:\n      matched_variant = self._make_synthetic_hom_ref(variant)\n    return confident_or_no_constraint, matched_variant\n\n  def _make_synthetic_hom_ref(self, variant):\n    """"""Creates a version of variant with a hom-ref genotype.\n\n    Args:\n      variant: Our candidate third_party.nucleus.protos.Variant variant.\n\n    Returns:\n      A new Variant with the same position and alleles as variant but with a\n      hom-ref genotype.\n    """"""\n    return variants_pb2.Variant(\n        reference_name=variant.reference_name,\n        start=variant.start,\n        end=variant.end,\n        reference_bases=variant.reference_bases,\n        alternate_bases=variant.alternate_bases,\n        calls=[variants_pb2.VariantCall(genotype=[0, 0])])\n\n  def _find_matching_variant_in_reader(self, variant):\n    """"""Finds a variant in vcf_reader compatible with variant, if one exists.""""""\n    region = variant_utils.variant_position(variant)\n    matches = [\n        truth_variant for truth_variant in self._get_truth_variants(region)\n        if variant.start == truth_variant.start\n    ]\n\n    if not matches:\n      return None\n    elif len(matches) > 1:\n      logging.info(\'Multiple matches detected for variant %s: %s\', variant,\n                   matches)\n\n    best_match = matches[0]\n    for match in matches:\n      if match.alternate_bases == variant.alternate_bases:\n        best_match = match\n    return best_match\n\n\ndef _genotype_from_matched_truth(candidate_variant, truth_variant):\n  """"""Gets the diploid genotype for candidate_variant from matched truth_variant.\n\n  This method figures out the genotype for candidate_variant by matching alleles\n  in candidate_variant with those used by the genotype assigned to\n  truth_variant. For example, if candidate is A/C and truth is A/C with a 0/1\n  genotype, then this function would return (0, 1) indicating that there\'s one\n  copy of the A allele and one of C in truth. If the true genotype is 1/1, then\n  this routine would return (1, 1).\n\n  The routine allows candidate_variant and truth_variant to differ in both\n  the number of alternate alleles, and even in the representation of the same\n  alleles due to those differences. For example, candidate could be:\n\n      AGT/A/AGTGT => 2 bp deletion and 2 bp insertion\n\n  and truth could have:\n\n      A/AGT => just the simplified 2 bp insertion\n\n  And this routine will correctly equate the AGT/AGTGT allele in candidate\n  with the A/AGT in truth and use the number of copies of AGT in truth to\n  compute the number of copies of AGTGT when determining the returned genotype.\n\n  Args:\n    candidate_variant: Our candidate third_party.nucleus.protos.Variant variant.\n    truth_variant: Our third_party.nucleus.protos.Variant truth variant\n      containing true alleles and genotypes.\n\n  Returns:\n    A tuple genotypes with the same semantics at the genotype field of the\n    VariantCall proto.\n\n  Raises:\n    ValueError: If candidate_variant is None, truth_variant is None, or\n      truth_variant doesn\'t have genotypes.\n  """"""\n  if candidate_variant is None:\n    raise ValueError(\'candidate_variant cannot be None\')\n  if truth_variant is None:\n    raise ValueError(\'truth_variant cannot be None\')\n  if not variantcall_utils.has_genotypes(\n      variant_utils.only_call(truth_variant)):\n    raise ValueError(\'truth_variant needs genotypes to be used for labeling\',\n                     truth_variant)\n\n  def _match_one_allele(true_allele):\n    if true_allele == truth_variant.reference_bases:\n      return 0\n    else:\n      simplified_true_allele = variant_utils.simplify_alleles(\n          truth_variant.reference_bases, true_allele)\n      for alt_index, alt_allele in enumerate(candidate_variant.alternate_bases):\n        simplified_alt_allele = variant_utils.simplify_alleles(\n            candidate_variant.reference_bases, alt_allele)\n        if simplified_true_allele == simplified_alt_allele:\n          return alt_index + 1\n      # If nothing matched, we don\'t have this alt, so the alt allele index for\n      # should be 0 (i.e., not any alt).\n      return 0\n\n  # If our candidate_variant is a reference call, return a (0, 0) genotype.\n  if variant_utils.is_ref(candidate_variant):\n    return (0, 0)\n  else:\n    return tuple(\n        _match_one_allele(true_allele)\n        for true_allele in variant_utils.genotype_as_alleles(\n            variant_utils.unphase_all_genotypes(truth_variant)))\n'"
deepvariant/labeler/positional_labeler_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .variant_labeler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\nfrom deepvariant import testdata\nfrom deepvariant.labeler import positional_labeler\n\n\ndef setUpModule():\n  testdata.init()\n\n\nclass PositionalVariantLabelerTest(parameterized.TestCase):\n  # Confident variants: SNP, deletion, and multi-allelic.\n  snp = test_utils.make_variant(start=10, alleles=[\'A\', \'C\'], gt=[0, 1])\n  deletion = test_utils.make_variant(start=20, alleles=[\'ACG\', \'A\'], gt=[1, 1])\n  multiallelic = test_utils.make_variant(\n      start=30, alleles=[\'ACT\', \'ACTGT\', \'A\'], gt=[1, 2])\n  # Outside our confident regions.\n  non_confident = test_utils.make_variant(\n      start=200, alleles=[\'A\', \'C\'], gt=[0, 1])\n  filtered = test_utils.make_variant(start=40, filters=\'FAILED\', gt=[0, 1])\n\n  variants = [snp, deletion, multiallelic, non_confident, filtered]\n\n  def _make_labeler(self, variants, confident_regions):\n    return positional_labeler.PositionalVariantLabeler(\n        truth_vcf_reader=vcf.InMemoryVcfReader(variants),\n        confident_regions=confident_regions)\n\n  @parameterized.parameters(\n      # Simple tests: we get back our matching variants in the confident regions\n      dict(candidate=snp, expected_confident=True, expected_truth=snp),\n      dict(\n          candidate=deletion, expected_confident=True, expected_truth=deletion),\n      dict(\n          candidate=multiallelic,\n          expected_confident=True,\n          expected_truth=multiallelic),\n\n      # Test the behavior outside of our confident regions.\n      # If we provide a variant outside the confident regions (non_confident) we\n      # don\'t get back any expected_truth variants.\n      dict(\n          candidate=non_confident,\n          expected_confident=False,\n          expected_truth=None),\n      # No matching variant, so we get a None as well as False.\n      dict(\n          candidate=test_utils.make_variant(start=300, alleles=[\'A\', \'C\']),\n          expected_confident=False,\n          expected_truth=None),\n\n      # This variant doesn\'t have any match but we\'re confident in it.\n      dict(\n          candidate=test_utils.make_variant(start=15, alleles=[\'C\', \'A\']),\n          expected_confident=True,\n          expected_genotype=(0, 0),\n          expected_truth=test_utils.make_variant(\n              start=15, alleles=[\'C\', \'A\'], gt=[0, 0])),\n\n      # These variant start at our SNP but has a different allele. We are\n      # confident and we get back the true snp variant, despite having the\n      # different alleles. snp has alleles=[\'A\', \'C\'] and gt=[0, 1].\n      dict(\n          candidate=test_utils.make_variant(\n              start=snp.start, alleles=[\'A\', \'G\']),\n          expected_confident=True,\n          expected_genotype=(0, 0),\n          expected_truth=snp),\n      dict(\n          candidate=test_utils.make_variant(\n              start=snp.start, alleles=[\'AC\', \'C\']),\n          expected_confident=True,\n          expected_genotype=(0, 0),\n          expected_truth=snp),\n      dict(\n          candidate=test_utils.make_variant(\n              start=snp.start, alleles=[\'A\', \'CA\']),\n          expected_confident=True,\n          expected_genotype=(0, 0),\n          expected_truth=snp),\n      # Checks that we don\'t match against the filtered truth variant in our\n      # database. This means that we return not the filtered variant but one\n      # with a (0, 0) genotype.\n      dict(\n          candidate=test_utils.make_variant(start=filtered.start),\n          expected_confident=True,\n          expected_genotype=(0, 0),\n          expected_truth=test_utils.make_variant(\n              start=filtered.start, gt=(0, 0))),\n  )\n  def test_label_variants(self,\n                          candidate,\n                          expected_confident,\n                          expected_truth,\n                          expected_genotype=None):\n    labeler = self._make_labeler(\n        self.variants,\n        ranges.RangeSet([ranges.make_range(self.snp.reference_name, 10, 100)]))\n\n    # Call _match so we can compare our expected truth with the actual one.\n    is_confident, truth_variant = labeler._match(candidate)\n    self.assertEqual(expected_truth, truth_variant)\n    self.assertEqual(is_confident, expected_confident)\n\n    # Now call label_variants to exercise the higher-level API.\n    if expected_genotype is None and expected_truth is not None:\n      expected_genotype = tuple(expected_truth.calls[0].genotype)\n    labels = list(labeler.label_variants([candidate]))\n    self.assertLen(labels, 1)\n    self.assertEqual(candidate, labels[0].variant)\n    self.assertEqual(expected_confident, labels[0].is_confident)\n    self.assertEqual(expected_genotype, labels[0].genotype)\n\n  def test_match_selects_variant_by_start(self):\n    # Tests that match() selects the variant at the same start even if that\n    # variant doesn\'t have the same alleles at candidate and there\'s an\n    # overlapping with the same alleles.\n    overlapping = [\n        test_utils.make_variant(start=20, alleles=[\'CC\', \'A\'], gt=[1, 1]),\n        test_utils.make_variant(start=21, alleles=[\'AAA\', \'A\'], gt=[0, 1]),\n        test_utils.make_variant(start=22, alleles=[\'AA\', \'A\'], gt=[1, 1]),\n    ]\n    candidate = test_utils.make_variant(start=21, alleles=[\'CC\', \'A\'])\n\n    labeler = self._make_labeler(\n        overlapping,\n        ranges.RangeSet(\n            [ranges.make_range(overlapping[0].reference_name, 0, 100)]))\n    is_confident, truth_variant = labeler._match(candidate)\n    self.assertEqual(is_confident, True)\n    self.assertEqual(truth_variant, overlapping[1])\n\n  @parameterized.parameters(\n      dict(\n          overlapping_variants=[\n              test_utils.make_variant(start=20, alleles=[\'A\', \'CC\'], gt=[1, 1]),\n              test_utils.make_variant(\n                  start=20, alleles=[\'A\', \'AAA\'], gt=[0, 1]),\n              test_utils.make_variant(start=20, alleles=[\'A\', \'AA\'], gt=[1, 1]),\n          ],\n          candidate=test_utils.make_variant(start=20, alleles=[\'A\', \'AAA\']),\n          expected_confident=True,\n          truth_variant_idx=1),\n\n      # No candidate variant with matching alt, so use first candidate.\n      dict(\n          overlapping_variants=[\n              test_utils.make_variant(start=20, alleles=[\'A\', \'CC\'], gt=[1, 1]),\n              test_utils.make_variant(\n                  start=20, alleles=[\'A\', \'AAA\'], gt=[0, 1]),\n              test_utils.make_variant(start=20, alleles=[\'A\', \'AA\'], gt=[1, 1]),\n          ],\n          candidate=test_utils.make_variant(start=20, alleles=[\'A\', \'TT\']),\n          expected_confident=True,\n          truth_variant_idx=0),\n  )\n  def test_match_multiple_matches(self, overlapping_variants, candidate,\n                                  expected_confident, truth_variant_idx):\n    labeler = self._make_labeler(\n        overlapping_variants,\n        ranges.RangeSet(\n            [ranges.make_range(overlapping_variants[0].reference_name, 0,\n                               100)]))\n    is_confident, variant_match = labeler._match(candidate)\n    expected_variant = overlapping_variants[truth_variant_idx]\n    self.assertEqual(is_confident, expected_confident)\n    self.assertEqual(variant_match, expected_variant)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/labeler/variant_labeler.py,1,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""variant_labeler for DeepVariant.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\n\nfrom absl import logging\nfrom third_party.nucleus.util import variant_utils\nfrom third_party.nucleus.util import variantcall_utils\nfrom deepvariant import tf_utils\n\n# ---------------------------------------------------------------------------\n# VariantLabel\n#\n\n\nclass VariantLabel(object):\n  """"""Dataclass containing information about a label assigned to a variant.\n\n  Attributes:\n    is_confident: bool. True if we could confidently assign a label to this\n      variant, False otherwise.\n    variant: nucleus.protos.Variant proto that we assigned a label for.\n    genotype: tuple of ints. The labeled genotype (e.g., (0, 1) for a het) in\n      the standard nucleus.proto.VariantCall style. Genotype can be None if the\n      labeler doesn\'t have any genotype to assign. If Genotype is not None, the\n      genotype of variant will be set to genotype.\n  """"""\n\n  def __init__(self, is_confident, variant, genotype=None):\n    if genotype is not None:\n      if not variant.calls:\n        variant.calls.add(genotype=genotype)\n      else:\n        variant.calls[0].genotype[:] = genotype\n\n    self.is_confident = is_confident\n    self.variant = variant\n    self.genotype = genotype\n\n  def label_for_alt_alleles(self, alt_alleles_indices):\n    """"""Computes the label value for an example using alt_alleles_indices.\n\n    This function computes the TensorFlow label value (0, 1, 2) we train\n    DeepVariant to predict. The label value is an int >= which is the number of\n    copies of the alt allele present, which is computed from the true genotypes\n    (self.genotypes) and the alt_allele_indices ([0] for the first alt, [1] for\n    the second, [0, 1] to combine the first and second). For example, suppose we\n    have a variant with alts A and C, and a true genotype of (0, 1), indicating\n    that we have 1 copy of the A allele. We\'d expect:\n\n      label_for_alt_alleles([0]) => 1 since there\'s 1 copy of the first alt.\n      label_for_alt_alleles([1]) => 0 since there\'s 0 copies of the second alt.\n      label_for_alt_alleles([0, 1]) => 1 since there\'s 1 copy of the first or\n        second allele.\n\n    Args:\n      alt_alleles_indices: list[int]. A list of the alt_allele_indices used to\n        compute the tf.Example for this candidate.\n\n    Returns:\n      int >= 0. The number of copies of alt_allele_indices we\'d expect to be\n      called for this example.\n    """"""\n    return sum(gt - 1 in alt_alleles_indices for gt in self.genotype if gt != 0)\n\n  def convert_to_class(self, example):\n    """"""Convert label to the class id.""""""\n    alt_alleles_indices = tf_utils.example_alt_alleles_indices(example)\n    # Set the label of the example to the # alts given our alt_alleles_indices.\n    return self.label_for_alt_alleles(alt_alleles_indices)\n\n  def set_variant_genotype(self, variant):\n    if not variant.calls:\n      variant.calls.add(genotype=self.genotype)\n    else:\n      variant.calls[0].genotype[:] = self.genotype\n\n\n# ---------------------------------------------------------------------------\n# _VariantLabeler base class\n#\n\n\nclass VariantLabeler(object):\n  """"""BaseClass for systems that want to provide training labels for examples.\n\n  A VariantLabeler provides methods to assign a genotype label to each of a\n  series of candidate variants using data from a truth set of variants\n  accessible with vcf_reader and an optional RangeSet of confident regions.\n\n  The basic logic of this class is something like:\n\n  candidates = [third_party.nucleus.protos.Variant(...), ...]\n  labeler = ConcreteSubclassOfVariantLabeler(vcf_reader, confident_regions)\n  for label in labeler.label_variants(candidates):\n    if label.is_confident:\n      for i in range(len(label.variant.alternate_bases)\n        genotype_label_value = label.label_for_alt_alleles([i])\n\n  See the docs on each individual function to get a better understanding of what\n  each function does and the meaning of the return values.\n  """"""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, truth_vcf_reader, confident_regions=None):\n    if truth_vcf_reader is None:\n      raise ValueError(\'truth_vcf_reader cannot be None\')\n    if confident_regions is None:\n      logging.warning(\'Note: confident_regions for VariantLabeler is None. \'\n                      \'It is possible that this is not allowed for some \'\n                      \'subtype of VariantLabelers.\')\n    self._truth_vcf_reader = truth_vcf_reader\n    self._confident_regions = confident_regions\n\n  @property\n  def metrics(self):\n    """"""Gets the LabelingMetrics proto tracking metrics for this labeler.\n\n    A variant labeler may provide information information about the labeling of\n    variants via the metrics property. If the labeler provides metrics, a\n    filled in LabelingMetrics protobuf will be returned by this property. If\n    metrics aren\'t supported, a None value will be returned.\n    """"""\n    return None\n\n  @abc.abstractmethod\n  def label_variants(self, variants, region):\n    """"""Gets label information for each variant in variants.\n\n    This is the primary API for assigning labels to variants. This function\n    takes and iterable of variants and yield a VariantLabel object for each\n    variant. The VariantLabel can be used to determine the genotype label for\n    each variant suitable for training a DeepVariant model. The API accepts\n    an iterable of Variants because, in the general case, the labeling of\n    variants aren\'t independent, in that the label assigned to one variant may\n    impact the label we assign to a nearby variant.\n\n    Args:\n      variants: iterable[nucleus.protos.Variant]: An iterable of variants to\n        label. The variants should be in coordinate-sorted order and all on the\n        same chromosome.\n      region: A nucleus.genomics.v1.Range object specifying the region over\n        which we are labeling variants. This should span at least the span of\n        variants, but may be larger. Statistics about the labeling will be\n        computed over region.\n\n    Yields:\n      A VariantLabel object for each variant in variants, in order.\n    """"""\n    raise NotImplementedError\n\n  def _get_truth_variants(self, region):\n    """"""Gets truth variants within region to use in labeling calculations.\n\n    This function queries _truth_vcf_reader in region to get a complete list of\n    truth variants that overlap region, and then filters them down by removing\n    filtered truth variants and ones that aren\'t contained in the truth\n    intervals.\n\n    Args:\n      region: nucleus.Range proto describing the region on the genome where we\n        want to get our truth variants.\n\n    Yields:\n      nucleus.Variant proto.\n    """"""\n    for variant in self._truth_vcf_reader.query(region):\n      if (not variant_utils.is_filtered(variant) and\n          (self._confident_regions is None or\n           self._confident_regions.variant_overlaps(\n               variant, empty_set_return_value=False))):\n        yield variant\n\n\ndef _genotype_from_matched_truth(candidate_variant, truth_variant):\n  """"""Gets the diploid genotype for candidate_variant from matched truth_variant.\n\n  This method figures out the genotype for candidate_variant by matching alleles\n  in candidate_variant with those used by the genotype assigned to\n  truth_variant. For example, if candidate is A/C and truth is A/C with a 0/1\n  genotype, then this function would return (0, 1) indicating that there\'s one\n  copy of the A allele and one of C in truth. If the true genotype is 1/1, then\n  this routine would return (1, 1).\n\n  The routine allows candidate_variant and truth_variant to differ in both\n  the number of alternate alleles, and even in the representation of the same\n  alleles due to those differences. For example, candidate could be:\n\n      AGT/A/AGTGT => 2 bp deletion and 2 bp insertion\n\n  and truth could have:\n\n      A/AGT => just the simplified 2 bp insertion\n\n  And this routine will correctly equate the AGT/AGTGT allele in candidate\n  with the A/AGT in truth and use the number of copies of AGT in truth to\n  compute the number of copies of AGTGT when determining the returned genotype.\n\n  Args:\n    candidate_variant: Our candidate third_party.nucleus.protos.Variant variant.\n    truth_variant: Our third_party.nucleus.protos.Variant truth variant\n      containing true alleles and genotypes.\n\n  Returns:\n    A tuple genotypes with the same semantics at the genotype field of the\n    VariantCall proto.\n\n  Raises:\n    ValueError: If candidate_variant is None, truth_variant is None, or\n      truth_variant doesn\'t have genotypes.\n  """"""\n  if candidate_variant is None:\n    raise ValueError(\'candidate_variant cannot be None\')\n  if truth_variant is None:\n    raise ValueError(\'truth_variant cannot be None\')\n  if not variantcall_utils.has_genotypes(\n      variant_utils.only_call(truth_variant)):\n    raise ValueError(\'truth_variant needs genotypes to be used for labeling\',\n                     truth_variant)\n\n  def _match_one_allele(true_allele):\n    if true_allele == truth_variant.reference_bases:\n      return 0\n    else:\n      simplifed_true_allele = variant_utils.simplify_alleles(\n          truth_variant.reference_bases, true_allele)\n      for alt_index, alt_allele in enumerate(candidate_variant.alternate_bases):\n        simplifed_alt_allele = variant_utils.simplify_alleles(\n            candidate_variant.reference_bases, alt_allele)\n        if simplifed_true_allele == simplifed_alt_allele:\n          return alt_index + 1\n      # If nothing matched, we don\'t have this alt, so the alt allele index for\n      # should be 0 (i.e., not any alt).\n      return 0\n\n  # If our candidate_variant is a reference call, return a (0, 0) genotype.\n  if variant_utils.is_ref(candidate_variant):\n    return (0, 0)\n  else:\n    return tuple(\n        sorted(\n            _match_one_allele(true_allele) for true_allele in\n            variant_utils.genotype_as_alleles(truth_variant)))\n'"
deepvariant/labeler/variant_labeler_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .variant_labeler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport six\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\nfrom deepvariant import testdata\nfrom deepvariant.labeler import variant_labeler\n\n\ndef setUpModule():\n  testdata.init()\n\n\nclass DummyVariantLabeler(variant_labeler.VariantLabeler):\n  """"""A dummy VariantLabeler.\n\n  This class provides a label_variants implementation and so allows the base\n  class to be instantiated and its methods tested.\n  """"""\n\n  def __init__(self, *pos, **kwargs):\n    super(DummyVariantLabeler, self).__init__(*pos, **kwargs)\n\n  def label_variants(self, variants, region):\n    raise NotImplementedError\n\n\nclass VariantLabelerTest(parameterized.TestCase):\n  snp = test_utils.make_variant(start=10, alleles=[\'A\', \'C\'], gt=[0, 1])\n\n  def test_get_truth_variants(self):\n    v1 = test_utils.make_variant(chrom=\'1\', start=10)\n    v2 = test_utils.make_variant(chrom=\'1\', start=20)\n    v3_filtered = test_utils.make_variant(chrom=\'1\', start=30, filters=[\'FAIL\'])\n    v4_del = test_utils.make_variant(chrom=\'1\', start=40, alleles=[\'AAAA\', \'A\'])\n    v5_non_confident = test_utils.make_variant(chrom=\'1\', start=150)\n\n    variants = [v1, v2, v3_filtered, v4_del, v5_non_confident]\n    reader = vcf.InMemoryVcfReader(variants=variants)\n    confident_regions = ranges.RangeSet([ranges.make_range(\'1\', 1, 100)])\n    labeler = DummyVariantLabeler(\n        truth_vcf_reader=reader, confident_regions=confident_regions)\n\n    # Check that we get v1 and v2 specifically when only they are covered by the\n    # query.\n    self.assertEqual(\n        list(labeler._get_truth_variants(ranges.parse_literal(\'1:1-15\'))), [v1])\n    self.assertEqual(\n        list(labeler._get_truth_variants(ranges.parse_literal(\'1:15-25\'))),\n        [v2])\n\n    # We don\'t include filtered variants.\n    self.assertEqual(\n        list(labeler._get_truth_variants(ranges.parse_literal(\'1:25-35\'))), [])\n\n    # Check that we get all overlapping variants of our query.\n    for del_query in [\'1:35-45\', \'1:42-43\', \'1:38-42\', \'1:42-50\']:\n      self.assertEqual(\n          list(labeler._get_truth_variants(ranges.parse_literal(del_query))),\n          [v4_del])\n\n    # Checks that a simple query gets all our non-filtered variants.\n    self.assertEqual(\n        list(labeler._get_truth_variants(ranges.parse_literal(\'1:1-100\'))),\n        [v1, v2, v4_del])\n    # Even through our query covers v5, it\'s not confident, so we don\'t get it.\n    self.assertEqual(\n        list(labeler._get_truth_variants(ranges.parse_literal(\'1:1-1000\'))),\n        [v1, v2, v4_del])\n\n  @parameterized.parameters(\n      # Make sure we get the right alt counts for all diploid genotypes.\n      ([\'A\', \'C\'], [\'C\'], [\'A\', \'C\'], [0, 0], (0, 0), 0),\n      ([\'A\', \'C\'], [\'C\'], [\'A\', \'C\'], [0, 1], (0, 1), 1),\n      ([\'A\', \'C\'], [\'C\'], [\'A\', \'C\'], [1, 0], (0, 1), 1),\n      ([\'A\', \'C\'], [\'C\'], [\'A\', \'C\'], [1, 1], (1, 1), 2),\n\n      # Make sure get back a zero alt count for a reference variant.\n      ([\'A\'], [], [\'A\'], [0, 0], (0, 0), 0),\n\n      # Basic multi-allelic tests, without having to deal with simplifying\n      # alleles as all of the alleles are SNPs. Our candidates have an extra\n      # allele, but the true GT is A/C.\n      ([\'A\', \'C\', \'G\'], [\'C\'], [\'A\', \'C\'], [0, 1], (0, 1), 1),\n      ([\'A\', \'C\', \'G\'], [\'C\'], [\'A\', \'C\'], [1, 1], (1, 1), 2),\n\n      # When considering A/G our answer should be 0 as we have no copies\n      # of the G allele.\n      ([\'A\', \'C\', \'G\'], [\'G\'], [\'A\', \'C\'], [0, 1], (0, 1), 0),\n      ([\'A\', \'C\', \'G\'], [\'G\'], [\'A\', \'C\'], [1, 1], (1, 1), 0),\n\n      # We are considering the het-alt configuration here of A vs. C+G. We\'ve\n      # got one copy of the C allele so our true genotype is het. If truth is\n      # hom-var for the C, though, we again label the composite as hom_var as\n      # we have two copies of the C/G alt.\n      ([\'A\', \'C\', \'G\'], [\'C\', \'G\'], [\'A\', \'C\'], [0, 1], (0, 1), 1),\n      ([\'A\', \'C\', \'G\'], [\'C\', \'G\'], [\'A\', \'C\'], [1, 1], (1, 1), 2),\n\n      # Here we have an extra allele in truth, while candidate is bi-allelic.\n      # This example \'G\' is unused in truth, so we are simply the normal\n      # bi-allelic result.\n      ([\'A\', \'C\'], [\'C\'], [\'A\', \'C\', \'G\'], [0, 0], (0, 0), 0),\n      ([\'A\', \'C\'], [\'C\'], [\'A\', \'C\', \'G\'], [0, 1], (0, 1), 1),\n      ([\'A\', \'C\'], [\'C\'], [\'A\', \'C\', \'G\'], [1, 1], (1, 1), 2),\n\n      # We check here that we get the bi-allelic result even when the extra\n      # allele is in position 1 not 2.\n      ([\'A\', \'G\'], [\'G\'], [\'A\', \'C\', \'G\'], [0, 0], (0, 0), 0),\n      ([\'A\', \'G\'], [\'G\'], [\'A\', \'C\', \'G\'], [0, 2], (0, 1), 1),\n      ([\'A\', \'G\'], [\'G\'], [\'A\', \'C\', \'G\'], [2, 2], (1, 1), 2),\n\n      # Now for a real het-alt. We\'ve got three alleles in both, and the true\n      # genotype is 1/2.\n      ([\'A\', \'C\', \'G\'], [\'C\'], [\'A\', \'C\', \'G\'], [1, 2], (1, 2), 1),\n      ([\'A\', \'C\', \'G\'], [\'G\'], [\'A\', \'C\', \'G\'], [1, 2], (1, 2), 1),\n      ([\'A\', \'C\', \'G\'], [\'C\', \'G\'], [\'A\', \'C\', \'G\'], [1, 2], (1, 2), 2),\n\n      # Test all possible values in candidate against het-alt:\n      ([\'A\', \'C\', \'G\', \'T\'], [\'C\'], [\'A\', \'C\', \'G\'], [1, 2], (1, 2), 1),\n      ([\'A\', \'C\', \'G\', \'T\'], [\'G\'], [\'A\', \'C\', \'G\'], [1, 2], (1, 2), 1),\n      ([\'A\', \'C\', \'G\', \'T\'], [\'T\'], [\'A\', \'C\', \'G\'], [1, 2], (1, 2), 0),\n      ([\'A\', \'C\', \'G\', \'T\'], [\'C\', \'G\'], [\'A\', \'C\', \'G\'], [1, 2], (1, 2), 2),\n      ([\'A\', \'C\', \'G\', \'T\'], [\'C\', \'T\'], [\'A\', \'C\', \'G\'], [1, 2], (1, 2), 1),\n      ([\'A\', \'C\', \'G\', \'T\'], [\'G\', \'T\'], [\'A\', \'C\', \'G\'], [1, 2], (1, 2), 1),\n\n      # Simple start for indel alleles => exact matching works here.\n      ([\'A\', \'AC\'], [\'AC\'], [\'A\', \'AC\'], [0, 0], (0, 0), 0),\n      ([\'A\', \'AC\'], [\'AC\'], [\'A\', \'AC\'], [0, 1], (0, 1), 1),\n      ([\'A\', \'AC\'], [\'AC\'], [\'A\', \'AC\'], [1, 1], (1, 1), 2),\n\n      # We\'ve got a multi-allelic truth, but again exact matching is enough.\n      ([\'A\', \'AC\'], [\'AC\'], [\'A\', \'AC\', \'ACC\'], [0, 0], (0, 0), 0),\n      ([\'A\', \'AC\'], [\'AC\'], [\'A\', \'AC\', \'ACC\'], [0, 1], (0, 1), 1),\n      ([\'A\', \'AC\'], [\'AC\'], [\'A\', \'AC\', \'ACC\'], [1, 1], (1, 1), 2),\n      ([\'A\', \'AC\'], [\'AC\'], [\'A\', \'AC\', \'ACC\'], [0, 2], (0, 0), 0),\n      ([\'A\', \'AC\'], [\'AC\'], [\'A\', \'AC\', \'ACC\'], [1, 2], (0, 1), 1),\n      ([\'A\', \'AC\'], [\'AC\'], [\'A\', \'AC\', \'ACC\'], [2, 2], (0, 0), 0),\n\n      # This case has an extra allele (A) in truth but the true genotype\n      # corresponds to our candidate alleles exactly.\n      ([\'A\', \'AC\'], [\'AC\'], [\'AC\', \'A\', \'ACC\'], [0, 2], (0, 1), 1),\n      ([\'A\', \'AC\'], [\'AC\'], [\'AC\', \'A\', \'ACC\'], [2, 2], (1, 1), 2),\n      # If the true genotype involved just the deletion (A) allele, we don\'t\n      # have that allele in our candidate so we always get 0 copies.\n      ([\'A\', \'AC\'], [\'AC\'], [\'AC\', \'A\', \'ACC\'], [0, 1], (0, 0), 0),\n      ([\'A\', \'AC\'], [\'AC\'], [\'AC\', \'A\', \'ACC\'], [1, 1], (0, 0), 0),\n      # If the truth is het-alt, we can\'t match the deletion A allele but we do\n      # in fact have the A => AC allele as this matches the AC => ACC allele in\n      # truth set.\n      ([\'A\', \'AC\'], [\'AC\'], [\'AC\', \'A\', \'ACC\'], [1, 2], (0, 1), 1),\n\n      # We have a multi-allelic candidate but a simple bi-allelic truth. Make\n      # sure we match correctly. This is an key case, as we should expect that\n      # our candidates frequently have extra alleles changing the represention\n      # relative to our truth candidates.\n      ([\'ACT\', \'A\', \'AACT\'], [\'A\'], [\'A\', \'AA\'], [0, 1], (0, 2), 0),\n      ([\'ACT\', \'A\', \'AACT\'], [\'A\'], [\'A\', \'AA\'], [1, 1], (2, 2), 0),\n      ([\'ACT\', \'A\', \'AACT\'], [\'AACT\'], [\'A\', \'AA\'], [0, 1], (0, 2), 1),\n      ([\'ACT\', \'A\', \'AACT\'], [\'AACT\'], [\'A\', \'AA\'], [1, 1], (2, 2), 2),\n      ([\'ACT\', \'A\', \'AACT\'], [\'A\', \'AACT\'], [\'A\', \'AA\'], [0, 1], (0, 2), 1),\n      ([\'ACT\', \'A\', \'AACT\'], [\'A\', \'AACT\'], [\'A\', \'AA\'], [1, 1], (2, 2), 2),\n\n      # The whole complexity: multi-allelic candidate and truth, all with\n      # different allele representations.\n      # True genotype here is A/AGTGT where ref is AGT [common\n      # dinucleotide expansion]. Both candidate and truth have this but each\n      # as a different ref so none of the alleles exactly match.\n      #\n      # Truth     : AGT   => A [1] + AGTGT [2]\n      # Candidate : AGTGT => AGT [2] + AGTGTGT [3]\n      ([\'AGTGT\', \'A\', \'AGT\', \'AGTGTGT\'\n       ], [\'A\'], [\'AGT\', \'A\', \'AGTGT\', \'AGTGTGT\'], [1, 2], (2, 3), 0),\n      ([\'AGTGT\', \'A\', \'AGT\', \'AGTGTGT\'\n       ], [\'AGT\'], [\'AGT\', \'A\', \'AGTGT\', \'AGTGTGT\'], [1, 2], (2, 3), 1),\n      ([\'AGTGT\', \'A\', \'AGT\', \'AGTGTGT\'\n       ], [\'AGTGTGT\'], [\'AGT\', \'A\', \'AGTGT\', \'AGTGTGT\'], [1, 2], (2, 3), 1),\n      ([\'AGTGT\', \'A\', \'AGT\', \'AGTGTGT\'\n       ], [\'A\', \'AGT\'], [\'AGT\', \'A\', \'AGTGT\', \'AGTGTGT\'], [1, 2], (2, 3), 1),\n      ([\'AGTGT\', \'A\', \'AGT\', \'AGTGTGT\'], [\'A\', \'AGTGTGT\'],\n       [\'AGT\', \'A\', \'AGTGT\', \'AGTGTGT\'], [1, 2], (2, 3), 1),\n      ([\'AGTGT\', \'A\', \'AGT\', \'AGTGTGT\'], [\'AGT\', \'AGTGTGT\'],\n       [\'AGT\', \'A\', \'AGTGT\', \'AGTGTGT\'], [1, 2], (2, 3), 2),\n\n      # Misc. checks with block substititions.\n      ([\'AT\', \'A\', \'GC\'], [\'A\'], [\'ATT\', \'AT\', \'A\'], [0, 1], (0, 1), 1),\n      ([\'AT\', \'A\', \'GT\'], [\'A\'], [\'A\', \'G\'], [0, 1], (0, 2), 0),\n      ([\'AT\', \'A\', \'GT\'], [\'GT\'], [\'A\', \'G\'], [0, 1], (0, 2), 1),\n  )\n  def test_genotype_from_matched_truth(self, variant_alleles, alt_alleles,\n                                       truth_alleles, truth_gt,\n                                       expected_genotype, expected_label):\n    variant = test_utils.make_variant(start=10, alleles=variant_alleles)\n    truth_variant = test_utils.make_variant(\n        start=10, alleles=truth_alleles, gt=truth_gt)\n    self.assertEqual(\n        expected_genotype,\n        variant_labeler._genotype_from_matched_truth(variant, truth_variant))\n    labeled = variant_labeler.VariantLabel(\n        is_confident=True, variant=variant, genotype=expected_genotype)\n    indices = [variant_alleles.index(alt) - 1 for alt in alt_alleles]\n    self.assertEqual(labeled.label_for_alt_alleles(indices), expected_label)\n\n  def test_genotype_from_matched_truth_none_truth_variant_raises(self):\n    with six.assertRaisesRegex(self, ValueError,\n                               \'truth_variant cannot be None\'):\n      variant_labeler._genotype_from_matched_truth(self.snp, None)\n\n  def test_genotype_from_matched_truth_no_call_truth_variant_raises(self):\n    with six.assertRaisesRegex(self, ValueError,\n                               \'Expected exactly one VariantCal\'):\n      variant_labeler._genotype_from_matched_truth(\n          self.snp, test_utils.make_variant(\n              start=10,\n              alleles=[\'A\', \'C\'],\n          ))\n\n  def test_genotype_from_matched_truth_no_gt_truth_variant_raises(self):\n    with six.assertRaisesRegex(self, ValueError,\n                               \'truth_variant needs genotypes\'):\n      variant_labeler._genotype_from_matched_truth(\n          self.snp,\n          test_utils.make_variant(\n              start=10,\n              alleles=[\'A\', \'C\'],\n              gt=[-1, -1],\n          ))\n\n  def test_genotype_from_matched_truth_none_variant_raises(self):\n    with six.assertRaisesRegex(self, ValueError, \'variant cannot be None\'):\n      variant_labeler._genotype_from_matched_truth(None, self.snp)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/python/allelecounter_wrap_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for AlleleCounter CLIF python wrappers.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\n\n\nfrom absl.testing import absltest\n\nfrom third_party.nucleus.io import fasta\nfrom third_party.nucleus.io import sam\nfrom third_party.nucleus.util import ranges\nfrom deepvariant import testdata\n\nfrom deepvariant.protos import deepvariant_pb2\nfrom deepvariant.python import allelecounter as _allelecounter\n\n\ndef setUpModule():\n  testdata.init()\n\n\nclass WrapAlleleCounterTest(absltest.TestCase):\n\n  def test_wrap(self):\n    ref = fasta.IndexedFastaReader(testdata.CHR20_FASTA)\n    sam_reader = sam.SamReader(testdata.CHR20_BAM)\n    size = 100\n    region = ranges.make_range(\'chr20\', 10000000, 10000000 + size)\n    options = deepvariant_pb2.AlleleCounterOptions(partition_size=size)\n    allele_counter = _allelecounter.AlleleCounter(ref.c_reader, region, options)\n    reads = list(sam_reader.query(region))\n    self.assertGreater(len(reads), 0)\n    for read in reads:\n      allele_counter.add(read, \'sample_id\')\n    counts = allele_counter.counts()\n    self.assertLen(counts, size)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/python/variant_calling_wrap_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for VariantCalling CLIF python wrappers.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\n\n\nfrom absl.testing import absltest\n\nfrom third_party.nucleus.io import fasta\nfrom third_party.nucleus.io import sam\nfrom third_party.nucleus.util import ranges\nfrom deepvariant import testdata\n\nfrom deepvariant.protos import deepvariant_pb2\nfrom deepvariant.python import allelecounter as _allelecounter\nfrom deepvariant.python import variant_calling\n\n\ndef setUpModule():\n  testdata.init()\n\n\nclass WrapVariantCallingTest(absltest.TestCase):\n\n  def test_call_from_allele_counter(self):\n    ref = fasta.IndexedFastaReader(testdata.CHR20_FASTA)\n    sam_reader = sam.SamReader(testdata.CHR20_BAM)\n    size = 1000\n    region = ranges.make_range(\'chr20\', 10000000, 10000000 + size)\n    allele_counter = _allelecounter.AlleleCounter(\n        ref.c_reader, region,\n        deepvariant_pb2.AlleleCounterOptions(partition_size=size))\n    caller = variant_calling.VariantCaller(\n        deepvariant_pb2.VariantCallerOptions(\n            min_count_snps=2,\n            min_count_indels=2,\n            min_fraction_snps=0.12,\n            min_fraction_indels=0.12,\n            sample_name=\'sample_name\',\n            p_error=0.001,\n            max_gq=50,\n            gq_resolution=1,\n            ploidy=2))\n\n    # Grab all of the reads in our region and add them to the allele_counter.\n    reads = list(sam_reader.query(region))\n    self.assertNotEmpty(reads)\n    for read in reads:\n      allele_counter.add(read, \'sample_id\')\n\n    # Get the candidates records for this whole region.\n    candidates = caller.calls_from_allele_counter(allele_counter)\n\n    # We should have at least some candidates and some gvcf records.\n    self.assertNotEmpty(candidates)\n\n    # Each candidate should be a DeepVariantCall.\n    for candidate in candidates:\n      self.assertIsInstance(candidate, deepvariant_pb2.DeepVariantCall)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/realigner/realigner.py,3,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Correct read alignment by realigning the read to its most likely haplotype.\n\nThis is achieved by constructing de-Bruijn graphs in candidate regions with\npotential variations, and determining the mostly likely X haplotypes (where X is\nthe ploidy).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport csv\nimport os\nimport os.path\n\nfrom absl import flags\nimport tensorflow as tf\n\nfrom third_party.nucleus.io import sam\nfrom third_party.nucleus.util import ranges\nfrom third_party.nucleus.util import utils\nfrom deepvariant.protos import realigner_pb2\nfrom deepvariant.realigner import window_selector\nfrom deepvariant.realigner.python import debruijn_graph\nfrom deepvariant.realigner.python import fast_pass_aligner\nfrom deepvariant.vendor import timer\nfrom google.protobuf import text_format\n\n_UNSET_WS_INT_FLAG = -1\n\nflags.DEFINE_bool(\'ws_use_window_selector_model\', False,\n                  \'Activate the use of window selector models.\')\nflags.DEFINE_string(\n    \'ws_window_selector_model\', None,\n    \'Path to a text format proto of the window selector model to use.\')\nflags.DEFINE_integer(\n    \'ws_min_num_supporting_reads\', _UNSET_WS_INT_FLAG,\n    \'Minimum number of supporting reads to call a reference position for local \'\n    \'assembly.\')\nflags.DEFINE_integer(\n    \'ws_max_num_supporting_reads\', _UNSET_WS_INT_FLAG,\n    \'Maximum number of supporting reads to call a reference position for local \'\n    \'assembly.\')\nflags.DEFINE_integer(\n    \'ws_min_mapq\', 20,\n    \'Minimum read alignment quality to consider in calling a reference \'\n    \'position for local assembly.\')\nflags.DEFINE_integer(\n    \'ws_min_base_quality\', 20,\n    \'Minimum base quality to consider in calling a reference position for \'\n    \'local assembly.\')\nflags.DEFINE_integer(\n    \'ws_min_windows_distance\', 80,\n    \'Minimum distance between candidate windows for local assembly.\')\nflags.DEFINE_integer(\n    \'ws_max_window_size\', 1000,\n    \'Maximum window size to consider for local assembly. Large noisy regions \'\n    \'are skipped for realignment.\')\nflags.DEFINE_integer(\n    \'ws_region_expansion_in_bp\', 20,\n    \'Number of bases to expand the region when calculating windows; larger \'\n    \'values add overhead but allow larger nearby events to contribute evidence \'\n    \'for assembling an region even if they are not contained by the region.\')\nflags.DEFINE_integer(\'dbg_min_k\', 10, \'Initial k-mer size to build the graph.\')\nflags.DEFINE_integer(\n    \'dbg_max_k\', 101,\n    \'Maximum k-mer size. Larger k-mer size is used to resolve graph cycles.\')\nflags.DEFINE_integer(\'dbg_step_k\', 1,\n                     \'Increment size for k to try in resolving graph cycles.\')\nflags.DEFINE_integer(\n    \'dbg_min_mapq\', 14,\n    \'Minimum read alignment quality to consider in building the graph.\')\nflags.DEFINE_integer(\n    \'dbg_min_base_quality\', 15,\n    \'Minimum base quality in a k-mer sequence to consider in building the \'\n    \'graph.\')\nflags.DEFINE_integer(\'dbg_min_edge_weight\', 2,\n                     \'Minimum number of supporting reads to keep an edge.\')\nflags.DEFINE_integer(\n    \'dbg_max_num_paths\', 256,\n    \'Maximum number of paths within a graph to consider for realignment. \'\n    \'Set max_num_paths to 0 to have unlimited number of paths.\')\nflags.DEFINE_integer(\'aln_match\', 4,\n                     \'Match score (expected to be a non-negative score).\')\nflags.DEFINE_integer(\'aln_mismatch\', 6,\n                     \'Mismatch score (expected to be a non-negative score).\')\nflags.DEFINE_integer(\n    \'aln_gap_open\', 8, \'Gap open score (expected to be a non-negative score). \'\n    \'Score for a gap of length g is -(gap_open + (g - 1) * gap_extend).\')\nflags.DEFINE_integer(\n    \'aln_gap_extend\', 2,\n    \'Gap extend score (expected to be a non-negative score). \'\n    \'Score for a gap of length g is -(gap_open + (g - 1) * gap_extend).\')\nflags.DEFINE_integer(\'aln_k\', 23, \'k-mer size used to index target sequence.\')\nflags.DEFINE_float(\'aln_error_rate\', .01, \'Estimated sequencing error rate.\')\nflags.DEFINE_string(\n    \'realigner_diagnostics\', \'\',\n    \'Root directory where the realigner should place diagnostic output (such as\'\n    \' a dump of the DeBruijn graph, and a log of metrics reflecting the graph \'\n    \'and  realignment to the haplotypes).  If empty, no diagnostics are output.\'\n)\nflags.DEFINE_bool(\n    \'emit_realigned_reads\', False,\n    \'If True, we will emit realigned reads if our realigner_diagnostics are \'\n    \'also enabled.\')\nflags.DEFINE_bool(\n    \'use_fast_pass_aligner\', True,\n    \'If True, fast_pass_aligner (improved performance) implementation is used \')\nflags.DEFINE_integer(\n    \'max_num_mismatches\', 2,\n    \'Num of maximum allowed mismatches for quick read to \'\n    \'haplotype alignment.\')\nflags.DEFINE_float(\n    \'realignment_similarity_threshold\', 0.16934,\n    \'Similarity threshold used in realigner in Smith-Waterman\'\n    \'alignment.\')\nflags.DEFINE_integer(\'kmer_size\', 32,\n                     \'K-mer size for fast pass alinger reads index.\')\n\n# Margin added to the reference sequence for the aligner module.\n_REF_ALIGN_MARGIN = 20\n\n_DEFAULT_MIN_SUPPORTING_READS = 2\n_DEFAULT_MAX_SUPPORTING_READS = 300\n_ALLELE_COUNT_LINEAR_MODEL_DEFAULT = realigner_pb2.WindowSelectorModel(\n    model_type=realigner_pb2.WindowSelectorModel.ALLELE_COUNT_LINEAR,\n    allele_count_linear_model=realigner_pb2.WindowSelectorModel\n    .AlleleCountLinearModel(\n        bias=-0.683379,\n        coeff_soft_clip=2.997000,\n        coeff_substitution=-0.086644,\n        coeff_insertion=2.493585,\n        coeff_deletion=1.795914,\n        coeff_reference=-0.059787,\n        decision_boundary=3))\n\n# ---------------------------------------------------------------------------\n# Set configuration settings.\n# ---------------------------------------------------------------------------\n\n\ndef window_selector_config(flags_obj):\n  """"""Creates a WindowSelectorOptions proto based on input and default settings.\n\n  Args:\n    flags_obj: configuration FLAGS.\n\n  Returns:\n    realigner_pb2.WindowSelector protobuf.\n\n  Raises:\n    ValueError: If either ws_{min,max}_supporting_reads are set and\n      ws_use_window_selector_model is True.\n      Or if ws_window_selector_model > ws_max_num_supporting_reads.\n      Or if ws_use_window_selector_model is False and\n      ws_window_selector_model is not None.\n  """"""\n  if not flags_obj.ws_use_window_selector_model:\n    if flags_obj.ws_window_selector_model is not None:\n      raise ValueError(\'Cannot specify a ws_window_selector_model \'\n                       \'if ws_use_window_selector_model is False.\')\n\n    min_num_supporting_reads = (\n        _DEFAULT_MIN_SUPPORTING_READS\n        if flags_obj.ws_min_num_supporting_reads == _UNSET_WS_INT_FLAG else\n        flags_obj.ws_min_num_supporting_reads)\n    max_num_supporting_reads = (\n        _DEFAULT_MAX_SUPPORTING_READS\n        if flags_obj.ws_max_num_supporting_reads == _UNSET_WS_INT_FLAG else\n        flags_obj.ws_max_num_supporting_reads)\n    window_selector_model = realigner_pb2.WindowSelectorModel(\n        model_type=realigner_pb2.WindowSelectorModel.VARIANT_READS,\n        variant_reads_model=realigner_pb2.WindowSelectorModel\n        .VariantReadsThresholdModel(\n            min_num_supporting_reads=min_num_supporting_reads,\n            max_num_supporting_reads=max_num_supporting_reads))\n  else:\n    if flags_obj.ws_min_num_supporting_reads != _UNSET_WS_INT_FLAG:\n      raise ValueError(\'Cannot use both ws_min_num_supporting_reads and \'\n                       \'ws_use_window_selector_model flags.\')\n    if flags_obj.ws_max_num_supporting_reads != _UNSET_WS_INT_FLAG:\n      raise ValueError(\'Cannot use both ws_max_num_supporting_reads and \'\n                       \'ws_use_window_selector_model flags.\')\n\n    if flags_obj.ws_window_selector_model is None:\n      window_selector_model = _ALLELE_COUNT_LINEAR_MODEL_DEFAULT\n    else:\n      with tf.io.gfile.GFile(flags_obj.ws_window_selector_model) as f:\n        window_selector_model = text_format.Parse(\n            f.read(), realigner_pb2.WindowSelectorModel())\n\n  if (window_selector_model.model_type ==\n      realigner_pb2.WindowSelectorModel.VARIANT_READS):\n    model = window_selector_model.variant_reads_model\n    if model.max_num_supporting_reads < model.min_num_supporting_reads:\n      raise ValueError(\'ws_min_supporting_reads should be smaller than \'\n                       \'ws_max_supporting_reads.\')\n\n  ws_config = realigner_pb2.WindowSelectorOptions(\n      min_mapq=flags_obj.ws_min_mapq,\n      min_base_quality=flags_obj.ws_min_base_quality,\n      min_windows_distance=flags_obj.ws_min_windows_distance,\n      max_window_size=flags_obj.ws_max_window_size,\n      region_expansion_in_bp=flags_obj.ws_region_expansion_in_bp,\n      window_selector_model=window_selector_model)\n\n  return ws_config\n\n\ndef realigner_config(flags_obj):\n  """"""Creates a RealignerOptions proto based on input and default settings.\n\n  Args:\n    flags_obj: configuration FLAGS.\n\n  Returns:\n    realigner_pb2.RealignerOptions protobuf.\n\n  Raises:\n    ValueError: If we observe invalid flag values.\n  """"""\n  ws_config = window_selector_config(flags_obj)\n\n  dbg_config = realigner_pb2.DeBruijnGraphOptions(\n      min_k=flags_obj.dbg_min_k,\n      max_k=flags_obj.dbg_max_k,\n      step_k=flags_obj.dbg_step_k,\n      min_mapq=flags_obj.dbg_min_mapq,\n      min_base_quality=flags_obj.dbg_min_base_quality,\n      min_edge_weight=flags_obj.dbg_min_edge_weight,\n      max_num_paths=flags_obj.dbg_max_num_paths)\n\n  aln_config = realigner_pb2.AlignerOptions(\n      match=flags_obj.aln_match,\n      mismatch=flags_obj.aln_mismatch,\n      gap_open=flags_obj.aln_gap_open,\n      gap_extend=flags_obj.aln_gap_extend,\n      k=flags_obj.aln_k,\n      error_rate=flags_obj.aln_error_rate,\n      max_num_of_mismatches=flags_obj.max_num_mismatches,\n      realignment_similarity_threshold=flags_obj\n      .realignment_similarity_threshold,\n      kmer_size=flags_obj.kmer_size)\n\n  diagnostics = realigner_pb2.Diagnostics(\n      enabled=bool(flags_obj.realigner_diagnostics),\n      output_root=flags_obj.realigner_diagnostics,\n      emit_realigned_reads=flags_obj.emit_realigned_reads)\n\n  return realigner_pb2.RealignerOptions(\n      ws_config=ws_config,\n      dbg_config=dbg_config,\n      aln_config=aln_config,\n      diagnostics=diagnostics)\n\n\nclass DiagnosticLogger(object):\n  """"""Writes diagnostic information about the assembler.""""""\n\n  def __init__(self,\n               config,\n               graph_filename=\'graph.dot\',\n               metrics_filename=\'realigner_metrics.csv\',\n               realigned_reads_filename=\'realigned_reads.bam\'):\n    self.config = config\n    self.graph_filename = graph_filename\n    self.metrics_filename = metrics_filename\n    self.realigned_reads_filename = realigned_reads_filename\n\n    # Setup diagnostics outputs if requested.\n    if self.enabled:\n      self._csv_file = open(self._root_join(self.metrics_filename), \'w\')\n      self._csv_writer = csv.writer(self._csv_file)\n      self._write_csv_line(\'window\', \'k\', \'n_haplotypes\', \'time\')\n    else:\n      self._csv_file = None\n      self._csv_writer = None\n\n  def close(self):\n    if self.enabled:\n      self._csv_file.close()\n\n  @property\n  def enabled(self):\n    return self.config and self.config.enabled\n\n  def _root_join(self, path, makedirs=True):\n    fullpath = os.path.join(self.config.output_root, path)\n    subdir = os.path.dirname(fullpath)\n    if makedirs and subdir:\n      tf.io.gfile.makedirs(subdir)\n    return fullpath\n\n  def _write_csv_line(self, *args):\n    assert self.enabled, \'only callable when diagnostics are on\'\n    self._csv_writer.writerow(args)\n\n  def _file_for_region(self, region, basename):\n    """"""Returns the path to a file in a region-specific subdirectory.""""""\n    assert self.enabled, \'only callable when diagnostics are on\'\n    return self._root_join(os.path.join(ranges.to_literal(region), basename))\n\n  def log_realigned_reads(self, region, reads, shared_header=None):\n    """"""Logs, if enabled, the realigned reads for region.""""""\n    if self.enabled and self.config.emit_realigned_reads and shared_header is not None:\n      path = self._file_for_region(region, self.realigned_reads_filename)\n      with sam.SamWriter(path, header=shared_header) as writer:\n        for read in reads:\n          writer.write(read)\n\n  def log_graph_metrics(self, region, graph, candidate_haplotypes,\n                        graph_building_time):\n    """"""Logs, if enabled, graph construction information for region.""""""\n    if self.enabled:\n      if graph:\n        dest_file = self._file_for_region(region, self.graph_filename)\n        with tf.io.gfile.GFile(dest_file, \'w\') as f:\n          f.write(graph.graphviz())\n      self._write_csv_line(\n          ranges.to_literal(region), graph.kmer_size if graph else \'NA\',\n          len(candidate_haplotypes), graph_building_time)\n\n\nclass AssemblyRegion(object):\n  """"""A region to assemble, holding the region Range and the reads.\n\n  It is not safe to directly modify any of the attributes here. Use the accessor\n  functions to add a read to the reads.\n\n  Attributes:\n    candidate_haplotypes: realigner.CandidateHaplotypes for this region.\n    reads: list[reads_pb2.Read]. Reads for this region.\n    region: range_pb2.Range. This is the span of the assembled region on the\n      genome.\n    read_span: range_pb2.Range. This is the span of reads added to this region.\n      The read_span in general is expected to be wider than the region itself,\n      since we often include all reads that overlap the region at all. It is\n      possible that read_span will be smaller than region, which can happen, for\n      example, when we only have reads starts in the middle of the region.\n  Here\'s a picture of when this can happen:\n  ref      : acgtACGTACgtgt\n  region   :     ------\n  read1    :       GGa\n  read_span:       ---\n  """"""\n\n  def __init__(self, candidate_haplotypes):\n    self.candidate_haplotypes = candidate_haplotypes\n    self.reads = []\n    self._read_span = None\n\n  def __str__(self):\n    return (\'AssemblyRegion(region={}, span={}) with {} haplotypes and {} \'\n            \'reads\').format(\n                ranges.to_literal(self.region),\n                ranges.to_literal(self.read_span), len(self.haplotypes),\n                len(self.reads))\n\n  @property\n  def haplotypes(self):\n    """"""Returns the haplotypes list[str] of our candidate_haplotypes.""""""\n    return self.candidate_haplotypes.haplotypes\n\n  @property\n  def region(self):\n    return self.candidate_haplotypes.span\n\n  @property\n  def read_span(self):\n    if self._read_span is None and self.reads:\n      spans = [utils.read_range(r) for r in self.reads]\n      self._read_span = ranges.make_range(spans[0].reference_name,\n                                          min(s.start for s in spans),\n                                          max(s.end for s in spans))\n    return self._read_span\n\n  def add_read(self, read):\n    self.reads.append(read)\n    self._read_span = None  # Adding a read invalidates our _read_span cache.\n\n\ndef assign_reads_to_assembled_regions(assembled_regions, reads):\n  """"""Assign each read to the maximally overlapped window.\n\n  Args:\n    assembled_regions: list[AssemblyRegion], list of AssemblyRegion to assign\n      reads to. Does not assume AssemblyRegion are sorted.\n    reads: iterable[learning.genomics.genomics.Read], to be processed. Does not\n      assume the reads are sorted.\n\n  Returns:\n    [AssemblyRegion], information on assigned reads for each assembled region.\n    list[learning.genomics.genomics.Read], the list of unassigned reads.\n  """"""\n  regions = [ar.region for ar in assembled_regions]\n  unassigned_reads = []\n  for read in reads:\n    read_range = utils.read_range(read)\n    window_i = ranges.find_max_overlapping(read_range, regions)\n    if window_i is not None:\n      assembled_regions[window_i].add_read(read)\n    else:\n      unassigned_reads.append(read)\n  return unassigned_reads\n\n\nclass Realigner(object):\n  """"""Realign reads in regions to assembled haplotypes.\n\n  This class helps us to realign reads in regions by:\n\n  (1) Create smaller windows in which to operate over the region. These windows\n  are created by finding evidence of genetic variation surrounded by stretches\n  of reference-matching seqence.\n\n  (2) Build a de-Bruijn assembly graph of the window. Edges are pruned if they\n  don\'t meet the required weight threshold. Every remaining haplotype is listed\n  by traversing the graph.\n\n  (3) Realign reads using a Smith-Waterman algorithm to the best candidate\n  haplotype and then realign that haplotype to the reference sequence to modify\n  the read\'s alignment.\n  """"""\n\n  def __init__(self, config, ref_reader, shared_header=None):\n    """"""Creates a new Realigner.\n\n    Args:\n      config: realigner_pb2.RealignerOptions protobuf.\n      ref_reader: GenomeReferenceFai, indexed reference genome to query bases.\n      shared_header: header info from the input bam file\n    """"""\n    self.config = config\n    self.ref_reader = ref_reader\n    self.diagnostic_logger = DiagnosticLogger(self.config.diagnostics)\n    self.shared_header = shared_header\n\n  def call_debruijn_graph(self, windows, reads):\n    """"""Helper function to call debruijn_graph module.""""""\n    windows_haplotypes = []\n    # Build and process de-Bruijn graph for each window.\n    sam_reader = sam.InMemorySamReader(reads)\n\n    for window in windows:\n      if window.end - window.start > self.config.ws_config.max_window_size:\n        continue\n      if not self.ref_reader.is_valid(window):\n        continue\n      ref = self.ref_reader.query(window)\n      window_reads = list(sam_reader.query(window))\n\n      with timer.Timer() as t:\n        graph = debruijn_graph.build(ref, window_reads, self.config.dbg_config)\n      graph_building_time = t.GetDuration()\n\n      if not graph:\n        candidate_haplotypes = [ref]\n      else:\n        candidate_haplotypes = graph.candidate_haplotypes()\n      if candidate_haplotypes and candidate_haplotypes != [ref]:\n        candidate_haplotypes_info = realigner_pb2.CandidateHaplotypes(\n            span=window, haplotypes=candidate_haplotypes)\n        windows_haplotypes.append(candidate_haplotypes_info)\n\n      self.diagnostic_logger.log_graph_metrics(window, graph,\n                                               candidate_haplotypes,\n                                               graph_building_time)\n\n    return windows_haplotypes\n\n  def call_fast_pass_aligner(self, assembled_region):\n    """"""Helper function to call fast pass aligner module.""""""\n    if not assembled_region.reads:\n      return []\n\n    contig = assembled_region.region.reference_name\n    ref_start = max(\n        0,\n        min(assembled_region.read_span.start, assembled_region.region.start) -\n        _REF_ALIGN_MARGIN)\n    ref_end = min(\n        self.ref_reader.contig(contig).n_bases,\n        max(assembled_region.read_span.end, assembled_region.region.end) +\n        _REF_ALIGN_MARGIN)\n\n    ref_prefix = self.ref_reader.query(\n        ranges.make_range(contig, ref_start, assembled_region.region.start))\n    ref = self.ref_reader.query(assembled_region.region)\n\n    # If we can\'t create the ref suffix then return the original alignments.\n    if ref_end <= assembled_region.region.end:\n      return assembled_region.reads\n    else:\n      ref_suffix = self.ref_reader.query(\n          ranges.make_range(contig, assembled_region.region.end, ref_end))\n\n    ref_seq = ref_prefix + ref + ref_suffix\n\n    fast_pass_realigner = fast_pass_aligner.FastPassAligner()\n    # Read sizes may vary. We need this for realigner initialization and sanity\n    # checks.\n    self.config.aln_config.read_size = len(\n        assembled_region.reads[0].aligned_sequence)\n    fast_pass_realigner.set_options(self.config.aln_config)\n    fast_pass_realigner.set_reference(ref_seq)\n    fast_pass_realigner.set_ref_start(contig, ref_start)\n    fast_pass_realigner.set_ref_prefix_len(len(ref_prefix))\n    fast_pass_realigner.set_ref_suffix_len(len(ref_suffix))\n    fast_pass_realigner.set_haplotypes([\n        ref_prefix + target + ref_suffix\n        for target in assembled_region.haplotypes\n    ])\n    return fast_pass_realigner.realign_reads(assembled_region.reads)\n\n  def realign_reads(self, reads, region):\n    """"""Run realigner.\n\n    This is the main function that\n      - parses the input reads and reference sequence.\n      - select candidate windows for local assembly (WindowSelector (ws)\n        module).\n        - Windows larger than max_window_size are skipped.\n      - build pruned De-Bruijn graph for each candidate window (DeBruijnGraph\n        (dbg) module).\n        - Graphs with more than max_num_paths candidate haplotypes or\n          with reference sequence as the only candidate are skipped.\n      - Align reads based on candidate haplotypes (Aligner (aln) module).\n      - Output all input reads (whether they required realignment or not).\n\n    Args:\n      reads: [`third_party.nucleus.protos.Read` protos]. The list of input reads\n        to realign.\n      region: A `third_party.nucleus.protos.Range` proto. Specifies the region\n        on the genome we should process.\n\n    Returns:\n      [realigner_pb2.CandidateHaplotypes]. Information on the list of candidate\n        haplotypes.\n      [`third_party.nucleus.protos.Read` protos]. The realigned\n        reads for the region. NOTE THESE READS MAY NO LONGER BE IN THE SAME\n        ORDER AS BEFORE.\n    """"""\n    # Compute the windows where we need to assemble in the region.\n    candidate_windows = window_selector.select_windows(self.config.ws_config,\n                                                       self.ref_reader, reads,\n                                                       region)\n\n    # Assemble each of those regions.\n    candidate_haplotypes = self.call_debruijn_graph(candidate_windows, reads)\n    # Create our simple container to store candidate / read mappings.\n    assembled_regions = [AssemblyRegion(ch) for ch in candidate_haplotypes]\n\n    # Our realigned_reads start off with all of the unassigned reads.\n    realigned_reads = assign_reads_to_assembled_regions(assembled_regions,\n                                                        reads)\n\n    # Walk over each region and align the reads in that region, adding them to\n    # our realigned_reads.\n    for assembled_region in assembled_regions:\n      if flags.FLAGS.use_fast_pass_aligner:\n        realigned_reads_copy = self.call_fast_pass_aligner(assembled_region)\n      else:\n        raise ValueError(\'--use_fast_pass_aligner is always true. \'\n                         \'The older implementation is deprecated and removed.\')\n\n      realigned_reads.extend(realigned_reads_copy)\n\n    self.diagnostic_logger.log_realigned_reads(region, realigned_reads,\n                                               self.shared_header)\n\n    return candidate_haplotypes, realigned_reads\n'"
deepvariant/realigner/realigner_test.py,7,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .realigner.realigner.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport csv\nimport itertools\nimport os\n\n\nfrom absl import flags\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom third_party.nucleus.io import fasta\nfrom third_party.nucleus.io import sam\nfrom third_party.nucleus.protos import reads_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\nfrom deepvariant import testdata\nfrom deepvariant.protos import realigner_pb2\nfrom deepvariant.realigner import realigner\nfrom deepvariant.realigner import utils\nfrom deepvariant.testing import flagsaver\n\nFLAGS = flags.FLAGS\n\n\ndef setUpModule():\n  testdata.init()\n\n\ndef _get_reads(region):\n  with sam.SamReader(testdata.CHR20_BAM) as in_sam_reader:\n    return list(in_sam_reader.query(region))\n\n\ndef _get_reads_and_header(region):\n  with sam.SamReader(testdata.CHR20_BAM) as in_sam_reader:\n    return list(in_sam_reader.query(region)), in_sam_reader.header\n\n\ndef _test_assembled_region(region_str, haplotypes=None):\n  return realigner.AssemblyRegion(\n      realigner_pb2.CandidateHaplotypes(\n          span=ranges.parse_literal(region_str), haplotypes=haplotypes or []))\n\n\nclass ReadAssignmentTests(parameterized.TestCase):\n\n  def setUp(self):\n    reads = [\n        test_utils.make_read(\'ACG\', start=1, cigar=\'3M\', name=\'read1\'),\n        test_utils.make_read(\'ACG\', start=6, cigar=\'3M\', name=\'read2\'),\n        test_utils.make_read(\'ACG\', start=9, cigar=\'3M\', name=\'read3\'),\n        test_utils.make_read(\'ACG\', start=28, cigar=\'3M\', name=\'read4\'),\n        test_utils.make_read(\'A\' * 10, start=3, cigar=\'10M\', name=\'read5\'),\n    ]\n    self.reads = {read.fragment_name: read for read in reads}\n    self.regions = {\n        \'r1\': _test_assembled_region(\'chr1:1-5\'),\n        \'r2\': _test_assembled_region(\'chr1:10-15\'),\n        \'r3\': _test_assembled_region(\'chr1:20-30\'),\n    }\n    self.assembled_regions = [self.regions[r] for r in sorted(self.regions)]\n\n  def get_reads_by_name(self, names):\n    return [self.reads[name] for name in names]\n\n  def test_construction(self):\n    aregion = _test_assembled_region(\'chr1:1-5\', haplotypes=[\'A\', \'C\'])\n    self.assertEqual(aregion.region, ranges.parse_literal(\'chr1:1-5\'))\n    self.assertEqual(aregion.haplotypes, [\'A\', \'C\'])\n    self.assertEqual(aregion.reads, [])\n\n  def test_adding_reads(self):\n    aregion = _test_assembled_region(\'chr1:3-15\')\n\n    # We haven\'t added any reads, so reads is empty and the span is None.\n    self.assertEqual(aregion.reads, [])\n    self.assertIsNone(aregion.read_span)\n\n    # Add read2, giving us a real read span and a read in our region\'s reads.\n    read_to_add = self.get_reads_by_name([\'read2\'])[0]\n    expected_reads = [read_to_add]\n    aregion.add_read(read_to_add)\n    self.assertEqual(aregion.reads, expected_reads)\n    self.assertEqual(aregion.read_span, ranges.parse_literal(\'chr1:7-9\'))\n\n    # Add read1, increasing the span on the left.\n    read_to_add = self.get_reads_by_name([\'read1\'])[0]\n    expected_reads += [read_to_add]\n    aregion.add_read(read_to_add)\n    self.assertEqual(aregion.reads, expected_reads)\n    self.assertEqual(aregion.read_span, ranges.parse_literal(\'chr1:2-9\'))\n\n    # Finally, add in all of the reads.\n    reads_to_add = self.get_reads_by_name([\'read3\', \'read4\', \'read5\'])\n    expected_reads += reads_to_add\n    for read in reads_to_add:\n      aregion.add_read(read)\n    self.assertEqual(aregion.reads, expected_reads)\n    self.assertEqual(aregion.read_span, ranges.parse_literal(\'chr1:2-31\'))\n\n  @parameterized.parameters(\n      # Single read tests.\n      # read1 overlaps r1.\n      dict(read_name=\'read1\', expected_region=\'r1\'),\n      # read2 falls between r1 and r2, should be unassigned.\n      dict(read_name=\'read2\', expected_region=None),\n      # read3 starts before r2 but overlaps it.\n      dict(read_name=\'read3\', expected_region=\'r2\'),\n      # read4 starts in r3 but extends beyond it.\n      dict(read_name=\'read4\', expected_region=\'r3\'),\n      # read5 overlaps r1 and r2 but is more in r2 then r1.\n      dict(read_name=\'read5\', expected_region=\'r2\'),\n  )\n  def test_assign_reads_to_assembled_regions_single_read(\n      self, read_name, expected_region):\n    assignment = {expected_region: [read_name]} if expected_region else {}\n    self.assertReadsGoToCorrectRegions(\n        reads=self.get_reads_by_name([read_name]),\n        expected_assignments=assignment)\n\n  @parameterized.parameters(\n      # Let\'s make sure adding all of the reads together results in the correct\n      # assignment across all regions.\n      dict(\n          read_names=names,\n          expected_assignments={\n              \'r1\': [\'read1\'],\n              \'r2\': [\'read3\', \'read5\'],\n              \'r3\': [\'read4\'],\n          }) for names in itertools.permutations(\n              [\'read1\', \'read2\', \'read3\', \'read4\', \'read5\']))\n  def test_assign_reads_to_assembled_regions_multiple_reads(\n      self, read_names, expected_assignments):\n    self.assertReadsGoToCorrectRegions(\n        self.get_reads_by_name(read_names), expected_assignments)\n\n  def assertReadsGoToCorrectRegions(self, reads, expected_assignments):\n    unassigned = realigner.assign_reads_to_assembled_regions(\n        self.assembled_regions, reads)\n\n    # Every read should be in the assembled regions or unassigned.\n    six.assertCountEqual(\n        self,\n        [r for ar in self.assembled_regions for r in ar.reads] + unassigned,\n        reads)\n\n    # Go through each region and make sure the reads that are supposed to\n    # appear in each region do in appear there.\n    for region_name, region in self.regions.items():\n      expected_reads = self.get_reads_by_name(\n          expected_assignments.get(region_name, []))\n      six.assertCountEqual(self, region.reads, expected_reads)\n\n\nclass RealignerTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.ref_reader = fasta.IndexedFastaReader(testdata.CHR20_FASTA)\n    # redacted\n    FLAGS.ws_use_window_selector_model = True\n    self.config = realigner.realigner_config(FLAGS)\n    self.reads_realigner = realigner.Realigner(self.config, self.ref_reader)\n\n  @parameterized.parameters(\n      # Arguments passed by ws_{min,max}_supporting_reads.\n      dict(\n          model=None, min_supporting=2, max_supporting=300, use_ws_model=False),\n      # No flags passed for the window_selection.\n      dict(\n          model=None, min_supporting=-1, max_supporting=-1, use_ws_model=False),\n      # VariantReadsThresholdModel.\n      dict(\n          model=\'VARIANT_READS_THRESHOLD\',\n          min_supporting=-1,\n          max_supporting=-1,\n          use_ws_model=True),\n      # AlleleCountLinearModel.\n      dict(\n          model=\'ALLELE_COUNT_LINEAR\',\n          min_supporting=-1,\n          max_supporting=-1,\n          use_ws_model=True),\n      # Use the default AlleleCountLinearModel.\n      dict(model=None, min_supporting=-1, max_supporting=-1, use_ws_model=True))\n  @flagsaver.FlagSaver\n  def test_window_selector_model_flags(self, model, min_supporting,\n                                       max_supporting, use_ws_model):\n    # This indirection is needed because the symbols in testdata are not set\n    # when the @parameterized decorator is called.\n    symbol_to_testdata = {\n        None: None,\n        \'VARIANT_READS_THRESHOLD\': testdata.WS_VARIANT_READS_THRESHOLD_MODEL,\n        \'ALLELE_COUNT_LINEAR\': testdata.WS_ALLELE_COUNT_LINEAR_MODEL\n    }\n    FLAGS.ws_max_num_supporting_reads = max_supporting\n    FLAGS.ws_min_num_supporting_reads = min_supporting\n    FLAGS.ws_window_selector_model = symbol_to_testdata[model]\n    FLAGS.ws_use_window_selector_model = use_ws_model\n    # We only make sure that reading the model does not crash or raise\n    # exceptions.\n    _ = realigner.realigner_config(FLAGS)\n\n  @flagsaver.FlagSaver\n  def test_window_selector_model_flags_failures(self):\n    with six.assertRaisesRegex(\n        self, ValueError, \'ws_min_supporting_reads should be smaller than ws_\'\n        \'max_supporting_reads.\'):\n      FLAGS.ws_max_num_supporting_reads = 1\n      FLAGS.ws_min_num_supporting_reads = 2\n      FLAGS.ws_window_selector_model = None\n      FLAGS.ws_use_window_selector_model = False\n      _ = realigner.realigner_config(FLAGS)\n\n    with six.assertRaisesRegex(\n        self, ValueError, \'Cannot specify a ws_window_selector_model \'\n        \'if ws_use_window_selector_model is False.\'):\n      FLAGS.ws_max_num_supporting_reads = -1\n      FLAGS.ws_min_num_supporting_reads = -1\n      FLAGS.ws_window_selector_model = testdata.WS_ALLELE_COUNT_LINEAR_MODEL\n      FLAGS.ws_use_window_selector_model = False\n      _ = realigner.realigner_config(FLAGS)\n\n    with six.assertRaisesRegex(\n        self, ValueError, \'Cannot use both ws_min_num_supporting_reads and \'\n        \'ws_use_window_selector_model flags.\'):\n      FLAGS.ws_max_num_supporting_reads = -1\n      FLAGS.ws_min_num_supporting_reads = 1\n      FLAGS.ws_window_selector_model = None\n      FLAGS.ws_use_window_selector_model = True\n      _ = realigner.realigner_config(FLAGS)\n\n    with six.assertRaisesRegex(\n        self, ValueError, \'Cannot use both ws_max_num_supporting_reads and \'\n        \'ws_use_window_selector_model flags.\'):\n      FLAGS.ws_max_num_supporting_reads = 1\n      FLAGS.ws_min_num_supporting_reads = -1\n      FLAGS.ws_window_selector_model = None\n      FLAGS.ws_use_window_selector_model = True\n      _ = realigner.realigner_config(FLAGS)\n\n  @parameterized.parameters(\n      dict(\n          region_literal=\'chr20:10,095,379-10,095,500\',\n          expected_window_literal=\'chr20:10,095,353-10,095,553\',\n          expected_haplotypes={\n              \'AGTGATCTAGTCCTTTTTGTTGTGCAAAAGGAAGTGCTAAAATCAGAATGAGAACCATGGTCAC\'\n              \'CTGACATAGACACAAGTGATGATGATGATGATGATGATGATGATGATGATGATATCCATGTTCA\'\n              \'AGTACTAATTCTGGGCAAGACACTGTTCTAAGTGCTATGAATATATTACCTCATTTAATCATC\'\n              \'T\',\n              \'AGTGATCTAGTCCTTTTTGTTGTGCAAAAGGAAGTGCTAAAATCAGAATGAGAACCATGGTCAC\'\n              \'CTGACATAGACACAAGTGATGATGATGATGATGATGATGATGATGATGATGATGATGATGATAT\'\n              \'CCATGTTCAAGTACTAATTCTGGGCAAGACACTGTTCTAAGTGCTATGAATATATTACCTCATT\'\n              \'TAATCATCT\'\n          },\n          comment=\'There is a heterozygous 9 bp deletion of tandem TGA repeat.\'\n      ),\n      dict(\n          region_literal=\'chr20:10,046,080-10,046,307\',\n          expected_window_literal=\'chr20:10,046,096-10,046,267\',\n          expected_haplotypes={\n              \'CCCAAAAAAAGAGTTAGGGATGCTGGAAAGGCAGAAAGAAAAGGGAAGGGAAGAGGAAGGGGAA\'\n              \'AAGGAAAGAAAAAAAAGAAAGAAAGAAAGAGAAAGAAAGAGAAAGAGAAAGAAAGAGGAAAGAG\'\n              \'AGAAAGAGAAAGAGAAGGAAAGAGAAAGAAAGAGAAGGAAAGAG\',\n              \'CCCAAAAAAAGAGTTAGGGATGCTGGAAAGGCAGAAAGAAAAGGGAAGGGAAGAGGAAGGGGAA\'\n              \'AAGGAAAGAAAAAAAAGAAAGAAAGAAAGAGAAAGAGAAAGAAAGAGGAAAGAGAGAAAGAGAA\'\n              \'AGAGAAGGAAAGAGAAAGAAAGAGAAGGAAAGAG\'\n          },\n          comment=\'There is a heterozygous 10 bp deletion.\'),\n  )\n  def test_realigner_example_region(self, region_literal,\n                                    expected_window_literal,\n                                    expected_haplotypes, comment):\n    region = ranges.parse_literal(region_literal)\n    reads = _get_reads(region)\n    windows_haplotypes, realigned_reads = self.reads_realigner.realign_reads(\n        reads, region)\n\n    self.assertEqual(len(reads), len(realigned_reads))\n    self.assertEqual(\n        ranges.parse_literal(expected_window_literal),\n        windows_haplotypes[0].span)\n    self.assertEqual(expected_haplotypes, set(windows_haplotypes[0].haplotypes))\n\n  @parameterized.parameters(\n      [\n          dict(\n              region_literal=\'chr20:10,046,080-10,046,307\',\n              variant_literal=\'chr20:10,046,179-10,046,188\')\n      ],)\n  def test_realigner_example_variant(self, region_literal, variant_literal):\n    """"""All overlapping reads should include 10bp deletion at chr20:10046178.""""""\n    region = ranges.parse_literal(region_literal)\n    variant = ranges.parse_literal(variant_literal)\n\n    reads = _get_reads(region)\n    _, realigned_reads = self.reads_realigner.realign_reads(reads, region)\n\n    for read in realigned_reads:\n      has_variant = False\n      self.assertTrue(read.HasField(\'alignment\'))\n      self.assertEqual(variant.reference_name,\n                       read.alignment.position.reference_name)\n      ref_pos = read.alignment.position.position\n      for cigar in read.alignment.cigar:\n        self.assertIn(cigar.operation, utils.CIGAR_OPS)\n        if cigar.operation in utils.CIGAR_ALIGN_OPS:\n          ref_pos += cigar.operation_length\n        elif cigar.operation in utils.CIGAR_DELETE_OPS:\n          if (ref_pos == variant.start and\n              cigar.operation_length == variant.end - ref_pos):\n            has_variant = True\n          ref_pos += cigar.operation_length\n      if (read.alignment.position.position <= variant.start and\n          ref_pos >= variant.end):\n        self.assertTrue(has_variant)\n\n  def test_realigner_doesnt_create_invalid_intervals(self):\n    """"""Tests that read sets don\'t result in a crash in reference_fai.cc.""""""\n    region = ranges.parse_literal(\'chr20:63,025,320-63,025,520\')\n\n    # pylint: disable=g-complex-comprehension\n    reads = [\n        test_utils.make_read(\n            \'ACCGT\' * 50,\n            start=63025520 - 250,\n            cigar=\'250M\',\n            quals=list(np.tile(range(30, 35), 50))) for _ in range(20)\n    ]\n    # pylint: enable=g-complex-comprehension\n    self.reads_realigner.realign_reads(reads, region)\n\n    # These reads are aligned off the edge of the contig. Note that the\n    # reference bases in this interval are all Ns as well.\n    # pylint: disable=g-complex-comprehension\n    reads = [\n        test_utils.make_read(\n            \'TTATA\' * 50,\n            start=63025520 - 200,\n            cigar=\'200M50S\',\n            quals=list(np.tile(range(30, 35), 50))) for _ in range(20)\n    ]\n    # pylint: enable=g-complex-comprehension\n    self.reads_realigner.realign_reads(reads, region)\n\n  @parameterized.parameters(\n      dict(enabled=False, emit_reads=False),\n      dict(enabled=True, emit_reads=False),\n      dict(enabled=True, emit_reads=True),\n  )\n  def test_realigner_diagnostics(self, enabled, emit_reads):\n    # Make sure that by default we aren\'t emitting any diagnostic outputs.\n    dx_dir = test_utils.test_tmpfile(\'dx_enabled{}_emitreads_{}\'.format(\n        enabled, emit_reads))\n    region_str = \'chr20:10046178-10046188\'\n    region = ranges.parse_literal(region_str)\n    assembled_region_str = \'chr20:10046096-10046267\'\n    reads, header = _get_reads_and_header(region)\n    self.config = realigner.realigner_config(FLAGS)\n    self.config.diagnostics.enabled = enabled\n    self.config.diagnostics.output_root = dx_dir\n    self.config.diagnostics.emit_realigned_reads = emit_reads\n    self.reads_realigner = realigner.Realigner(self.config, self.ref_reader,\n                                               header)\n    _, _ = self.reads_realigner.realign_reads(reads, region)\n    self.reads_realigner.diagnostic_logger.close()  # Force close all resources.\n\n    if not enabled:\n      # Make sure our diagnostic output isn\'t emitted.\n      self.assertFalse(tf.io.gfile.exists(dx_dir))\n    else:\n      # Our root directory exists.\n      self.assertTrue(tf.io.gfile.isdir(dx_dir))\n\n      # We expect a realigner_metrics.csv in our rootdir with 1 entry in it.\n      metrics_file = os.path.join(\n          dx_dir, self.reads_realigner.diagnostic_logger.metrics_filename)\n      self.assertTrue(tf.io.gfile.exists(metrics_file))\n      with tf.io.gfile.GFile(metrics_file) as fin:\n        rows = list(csv.DictReader(fin))\n        self.assertLen(rows, 1)\n        self.assertEqual(\n            set(rows[0].keys()), {\'window\', \'k\', \'n_haplotypes\', \'time\'})\n        self.assertEqual(rows[0][\'window\'], assembled_region_str)\n        self.assertEqual(int(rows[0][\'k\']), 25)\n        self.assertTrue(int(rows[0][\'n_haplotypes\']), 2)\n        # Check that our runtime is reasonable (greater than 0, less than 10 s).\n        self.assertTrue(0.0 < float(rows[0][\'time\']) < 10.0)\n\n      # As does the subdirectory for this region.\n      region_subdir = os.path.join(dx_dir, assembled_region_str)\n      self.assertTrue(tf.io.gfile.isdir(region_subdir))\n\n      # We always have a graph.dot\n      self.assertTrue(\n          tf.io.gfile.exists(\n              os.path.join(\n                  region_subdir,\n                  self.reads_realigner.diagnostic_logger.graph_filename)))\n\n      reads_file = os.path.join(\n          dx_dir, region_str,\n          self.reads_realigner.diagnostic_logger.realigned_reads_filename)\n\n      # if emit_reads=False then file should not exist and vice versa.\n      self.assertEqual(emit_reads, tf.io.gfile.exists(reads_file))\n\n\nclass RealignerIntegrationTest(absltest.TestCase):\n\n  def test_realigner_end2end(self):\n    ref_reader = fasta.IndexedFastaReader(testdata.CHR20_FASTA)\n    config = realigner.realigner_config(FLAGS)\n    reads_realigner = realigner.Realigner(config, ref_reader)\n    region_str = \'chr20:10,000,000-10,009,999\'\n    windows_count = 0\n\n    regions = ranges.RangeSet.from_regions([region_str])\n    for region in regions.partition(1000):\n      with sam.SamReader(\n          testdata.CHR20_BAM,\n          read_requirements=reads_pb2.ReadRequirements()) as sam_reader:\n        in_reads = list(sam_reader.query(region))\n      windows, out_reads = reads_realigner.realign_reads(in_reads, region)\n\n      # We should always get back all of the reads we sent in. Instead of just\n      # checking the lengths are the same, make sure all the read names are the\n      # same.\n      six.assertCountEqual(self, [r.fragment_name for r in in_reads],\n                           [r.fragment_name for r in out_reads])\n\n      # Check each window to make sure it\'s reasonable.\n      for window in windows:\n        # We always expect the reference sequence to be one of our haplotypes.\n        ref_seq = ref_reader.query(window.span)\n        self.assertIn(ref_seq, set(window.haplotypes))\n      windows_count += len(windows)\n\n    self.assertGreater(windows_count, 0)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/realigner/utils.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Utility functions for realigner.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom third_party.nucleus.protos import cigar_pb2\n\nCIGAR_ALIGN_OPS = [\n    cigar_pb2.CigarUnit.ALIGNMENT_MATCH, cigar_pb2.CigarUnit.SEQUENCE_MATCH,\n    cigar_pb2.CigarUnit.SEQUENCE_MISMATCH\n]\nCIGAR_INSERT_OPS = [cigar_pb2.CigarUnit.INSERT, cigar_pb2.CigarUnit.CLIP_SOFT]\nCIGAR_DELETE_OPS = [cigar_pb2.CigarUnit.DELETE, cigar_pb2.CigarUnit.SKIP]\nCIGAR_NO_OPS = [cigar_pb2.CigarUnit.CLIP_HARD]\n\n# Suppported cigar operations in realigner.\nCIGAR_OPS = CIGAR_ALIGN_OPS + CIGAR_INSERT_OPS + CIGAR_DELETE_OPS + CIGAR_NO_OPS\n'"
deepvariant/realigner/window_selector.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Determine genomic ranges to perform local assembly.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom third_party.nucleus.protos import reads_pb2\nfrom third_party.nucleus.util import ranges\nfrom deepvariant.protos import deepvariant_pb2\nfrom deepvariant.protos import realigner_pb2\nfrom deepvariant.python import allelecounter\nfrom deepvariant.realigner.python import window_selector as cpp_window_selector\n\n\ndef _candidates_from_reads(config, ref_reader, reads, region):\n  """"""Returns a list of candidate positions.\n\n  Args:\n    config: learning.genomics.deepvariant.realigner.WindowSelectorOptions\n      options determining the behavior of this window selector.\n    ref_reader: GenomeReference. Indexed reference genome to query bases.\n    reads: list[nucleus.protos.Read]. The reads we are processing into candidate\n      positions.\n    region: nucleus.protos.Range. The region we are processing.\n\n  Returns:\n    A list. The elements are reference positions within region.\n\n  Raises:\n    ValueError: if config.window_selector_model.model_type isn\'t a valid enum\n    name in realigner_pb2.WindowSelectorModel.ModelType.\n  """"""\n  allele_counter_options = deepvariant_pb2.AlleleCounterOptions(\n      read_requirements=reads_pb2.ReadRequirements(\n          min_mapping_quality=config.min_mapq,\n          min_base_quality=config.min_base_quality))\n\n  expanded_region = ranges.expand(\n      region,\n      config.region_expansion_in_bp,\n      contig_map=ranges.contigs_dict(ref_reader.header.contigs))\n\n  allele_counter = allelecounter.AlleleCounter(ref_reader.c_reader,\n                                               expanded_region,\n                                               allele_counter_options)\n\n  for read in reads:\n    allele_counter.add(read, \'dummy_sample_id\')\n\n  model_type = config.window_selector_model.model_type\n  if model_type == realigner_pb2.WindowSelectorModel.VARIANT_READS:\n    return _variant_reads_threshold_selector(\n        allele_counter, config.window_selector_model.variant_reads_model,\n        expanded_region)\n  elif model_type == realigner_pb2.WindowSelectorModel.ALLELE_COUNT_LINEAR:\n    return _allele_count_linear_selector(\n        allele_counter, config.window_selector_model.allele_count_linear_model,\n        expanded_region)\n  else:\n    raise ValueError(\'Unknown enum option ""{}"" for \'\n                     \'WindowSelectorModel.model_type\'.format(\n                         config.window_selector_model.model_type))\n\n\ndef _variant_reads_threshold_selector(allele_counter, model_conf,\n                                      expanded_region):\n  """"""Returns a list of candidate positions.\n\n  Following cigar operations generate candidate position:\n    - ALIGNMENT_MATCH, SEQUENCE_MISMATCH, SEQUENCE_MATCH: at mismatch positions\n      in the read when compared to the reference sequence.\n    - DELETE: at positions within [cigar_start, cigar_start + cigar_len)\n    - INSERT, CLIP_SOFT: at positions within\n        [cigar_start - cigar_len, cigar_start + cigar_len)\n\n   Note. Function implementation has changed to return positions beyond input\n   region in case we have variants there. See the change at internal and\n   internal.\n\n  Args:\n    allele_counter: learning.genomics.deepvariant.realigner.AlleleCounter in the\n      considered region.\n    model_conf: learning.genomics.deepvariant.realigner\n      .WindowSelectorOptions.VariantReadsThresholdModel options determining the\n      behavior of this window selector.\n    expanded_region: nucleus.protos.Range. The region we are processing.\n\n  Returns:\n    A list. The elements are reference positions within region.\n  """"""\n\n  counts_vec = cpp_window_selector.variant_reads_candidates_from_allele_counter(\n      allele_counter)\n\n  return [\n      expanded_region.start + i\n      for i, count in enumerate(counts_vec)\n      if (count >= model_conf.min_num_supporting_reads and\n          count <= model_conf.max_num_supporting_reads)\n  ]\n\n\ndef _allele_count_linear_selector(allele_counter, model_conf, expanded_region):\n  """"""Returns a list of candidate positions.\n\n  Candidate positions for realignment are generated by scoring each location.\n  The score at a location is a weighted sum of the number of reads with each\n  CIGAR operation at the location, where the weights are determined by the model\n  coefficients. Locations whose score exceed the model decision boundary value\n  are used to create realignment windows.\n\n  Note. Function implementation has changed to return positions beyond input\n  region in case we have variants there. See the change at internal and\n  internal.\n\n  Args:\n    allele_counter: learning.genomics.deepvariant.realigner.AlleleCounter in the\n      considered region.\n    model_conf: learning.genomics.deepvariant.realigner\n      .WindowSelectorOptions.AlleleCountLinearModel options determining the\n      behavior of this window selector.\n    expanded_region: nucleus.protos.Range. The region we are processing.\n\n  Returns:\n    A list. The elements are reference positions within region.\n  """"""\n\n  scores_vec = (\n      cpp_window_selector.allele_count_linear_candidates_from_allele_counter(\n          allele_counter, model_conf))\n\n  return [\n      expanded_region.start + i\n      for i, score in enumerate(scores_vec)\n      if score > model_conf.decision_boundary\n  ]\n\n\ndef _candidates_to_windows(config, candidate_pos, ref_name):\n  """"""""Process candidate positions to determine windows for local assembly.\n\n  Windows are within range of\n    [min(pos) - config.min_windows_distance,\n     max(pos) + config.min_windows_distance)\n\n  Args:\n    config: learning.genomics.deepvariant.realigner.WindowSelectorOptions\n      options determining the behavior of this window selector.\n    candidate_pos: A list of ref_pos.\n    ref_name: Reference name, used in setting the output\n      genomics.range.reference_name value.\n\n  Returns:\n    A sorted list of nucleus.protos.Range protos for all windows in this region.\n  """"""\n  windows = []\n\n  def _add_window(start_pos, end_pos):\n    windows.append(\n        ranges.make_range(ref_name, start_pos - config.min_windows_distance,\n                          end_pos + config.min_windows_distance))\n\n  start_pos, end_pos = None, None\n  for pos in sorted(candidate_pos):\n    if start_pos is None:\n      start_pos = pos\n      end_pos = pos\n    # We need to check if the previous end_pos is within 2*window_distance as we\n    # generate a window of radius window_distance around each position.\n    #\n    #   <-------end_pos------->\n    #                          <-------pos------->\n    # where window_distance = ------->\n    #\n    # If this is the case, we need to merge the two windows.\n    elif pos > end_pos + 2 * config.min_windows_distance:\n      _add_window(start_pos, end_pos)\n      start_pos = pos\n      end_pos = pos\n    else:\n      end_pos = pos\n  if start_pos is not None:\n    _add_window(start_pos, end_pos)\n\n  return sorted(windows, key=ranges.as_tuple)\n\n\ndef select_windows(config, ref_reader, reads, region):\n  """"""""Process reads to determine candidate windows for local assembly.\n\n  Windows are within range of\n    [0 - config.min_windows_distance, ref_len + config.min_windows_distance)\n\n  Args:\n    config: learning.genomics.deepvariant.realigner.WindowSelectorOptions\n      options determining the behavior of this window selector.\n    ref_reader: GenomeReference. Indexed reference genome to query bases.\n    reads: A list of genomics.Read records.\n    region: nucleus.protos.Range. The region we are processing.\n\n  Returns:\n    A list of nucleus.protos.Range protos sorted by their genomic position.\n  """"""\n  # This is a fast path for the case where we have no reads, so we have no\n  # windows to assemble.\n  if not reads:\n    return []\n\n  candidates = _candidates_from_reads(config, ref_reader, reads, region)\n  return _candidates_to_windows(config, candidates, region.reference_name)\n'"
deepvariant/realigner/window_selector_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant.realigner.window_selector.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import fasta\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\nfrom deepvariant.protos import realigner_pb2\nfrom deepvariant.realigner import window_selector\n\n\nclass AlleleCountLinearWindowSelectorTest(parameterized.TestCase):\n\n  def setUp(self):\n    window_selector_model = realigner_pb2.WindowSelectorModel(\n        model_type=realigner_pb2.WindowSelectorModel.ALLELE_COUNT_LINEAR,\n        allele_count_linear_model=realigner_pb2.WindowSelectorModel\n        .AlleleCountLinearModel(\n            bias=0,\n            coeff_soft_clip=0,\n            coeff_substitution=-0.5,\n            coeff_insertion=1,\n            coeff_deletion=1,\n            coeff_reference=-0.5,\n            decision_boundary=0))\n    self.config = realigner_pb2.WindowSelectorOptions(\n        min_mapq=20,\n        min_base_quality=20,\n        min_windows_distance=4,\n        region_expansion_in_bp=20,\n        window_selector_model=window_selector_model)\n\n  def assertCandidatesFromReadsEquals(self,\n                                      reads,\n                                      expected,\n                                      start=None,\n                                      end=None,\n                                      ref=None):\n    chrom = reads[0].alignment.position.reference_name\n    start = 0 if start is None else start\n    end = 20 if end is None else end\n    region = ranges.make_range(chrom, start, end)\n\n    if ref is None:\n      ref = \'A\' * (ranges.length(region) + 512)\n\n    ref_reader = fasta.InMemoryFastaReader([(chrom, 0, ref)])\n    if isinstance(expected, type) and issubclass(expected, Exception):\n      with self.assertRaises(expected):\n        window_selector._candidates_from_reads(self.config, ref_reader, reads,\n                                               region)\n    else:\n      actual = window_selector._candidates_from_reads(self.config, ref_reader,\n                                                      reads, region)\n      self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      # ------------------------------------------------------------------------\n      # These reads are all simple and just test the basic position calculation.\n      # ------------------------------------------------------------------------\n      dict(\n          read=test_utils.make_read(\n              \'AAGA\', start=10, cigar=\'4M\', quals=[64] * 4),\n          expected=[]),\n      dict(\n          read=test_utils.make_read(\n              \'AAGTA\', start=10, cigar=\'2M2I1M\', quals=[64] * 5),\n          expected=[10, 11, 12, 13]),\n      dict(\n          read=test_utils.make_read(\n              \'AAA\', start=10, cigar=\'2M2D1M\', quals=[64] * 3),\n          expected=[12, 13]),\n      dict(\n          read=test_utils.make_read(\n              \'TGATAC\', start=10, cigar=\'2S3M1S\', quals=[64] * 6),\n          expected=[]),\n      dict(\n          read=test_utils.make_read(\n              \'AAGA\', start=10, cigar=\'2M1X1M\', quals=[64] * 4),\n          expected=[]),\n  )\n  def test_candidates_from_one_read(self, read, expected):\n    """"""Test WindowSelector.process_read() with reads of low quality.""""""\n    self.assertCandidatesFromReadsEquals(reads=[read], expected=expected)\n\n  @parameterized.parameters(\n      # --------------------------------------------------\n      # Systematic combination of simple CIGAR operations.\n      # --------------------------------------------------\n      dict(\n          reads=[\n              test_utils.make_read(\n                  \'AAGA\', start=10, cigar=\'4M\', quals=[64] * 4),\n              test_utils.make_read(\n                  \'AAAA\', start=10, cigar=\'4M\', quals=[64] * 4),\n          ],\n          expected=[]),\n      dict(\n          reads=[\n              test_utils.make_read(\n                  \'AAAA\', start=10, cigar=\'4M\', quals=[64] * 4),\n              test_utils.make_read(\n                  \'AAA\', start=10, cigar=\'3M1D\', quals=[64] * 3),\n          ],\n          expected=[13]),\n      dict(\n          reads=[\n              test_utils.make_read(\n                  \'AAGA\', start=10, cigar=\'4M\', quals=[64] * 4),\n              test_utils.make_read(\n                  \'AAA\', start=10, cigar=\'3M1D\', quals=[64] * 3),\n          ],\n          expected=[13]),\n      dict(\n          reads=[\n              test_utils.make_read(\n                  \'AAAA\', start=10, cigar=\'4M\', quals=[64] * 4),\n              test_utils.make_read(\n                  \'AAAAT\', start=10, cigar=\'4M1I\', quals=[64] * 5),\n          ],\n          expected=[13, 14]),\n      dict(\n          reads=[\n              test_utils.make_read(\n                  \'AAAT\', start=10, cigar=\'3M1S\', quals=[64] * 4),\n              test_utils.make_read(\n                  \'AAAAT\', start=10, cigar=\'4M1I\', quals=[64] * 5),\n          ],\n          expected=[13, 14]),\n  )\n  def test_candidates_from_reads(self, reads, expected):\n    """"""Test WindowSelector.process_read() with reads of low quality.""""""\n    self.assertCandidatesFromReadsEquals(reads=reads, expected=expected)\n\n\nclass WindowSelectorTest(parameterized.TestCase):\n\n  def setUp(self):\n    window_selector_model = realigner_pb2.WindowSelectorModel(\n        model_type=realigner_pb2.WindowSelectorModel.VARIANT_READS,\n        variant_reads_model=realigner_pb2.WindowSelectorModel\n        .VariantReadsThresholdModel(\n            min_num_supporting_reads=1, max_num_supporting_reads=10))\n    self.config = realigner_pb2.WindowSelectorOptions(\n        min_mapq=20,\n        min_base_quality=20,\n        min_windows_distance=4,\n        region_expansion_in_bp=20,\n        window_selector_model=window_selector_model)\n\n  def assertCandidatesFromReadsEquals(self,\n                                      reads,\n                                      expected,\n                                      start=None,\n                                      end=None,\n                                      ref=None):\n    chrom = reads[0].alignment.position.reference_name\n    start = 0 if start is None else start\n    end = 20 if end is None else end\n    region = ranges.make_range(chrom, start, end)\n\n    if ref is None:\n      ref = \'A\' * (ranges.length(region) + 512)\n\n    ref_reader = fasta.InMemoryFastaReader([(chrom, 0, ref)])\n    if isinstance(expected, type) and issubclass(expected, Exception):\n      with self.assertRaises(expected):\n        window_selector._candidates_from_reads(self.config, ref_reader, reads,\n                                               region)\n    else:\n      actual = window_selector._candidates_from_reads(self.config, ref_reader,\n                                                      reads, region)\n      self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      # ------------------------------------------------------------------------\n      # These reads are all simple and just test the basic position calculation.\n      # ------------------------------------------------------------------------\n      dict(\n          read=test_utils.make_read(\n              \'AAGA\', start=10, cigar=\'4M\', quals=[64] * 4),\n          expected=[12]),\n      dict(\n          read=test_utils.make_read(\n              \'AAGTA\', start=10, cigar=\'2M2I1M\', quals=[64] * 5),\n          expected=[10, 11, 12, 13]),\n      dict(\n          read=test_utils.make_read(\n              \'AAA\', start=10, cigar=\'2M2D1M\', quals=[64] * 3),\n          expected=[12, 13]),\n      dict(\n          read=test_utils.make_read(\n              \'TGATAC\', start=10, cigar=\'2S3M1S\', quals=[64] * 6),\n          expected=[8, 9, 10, 11, 12, 13]),\n      dict(\n          read=test_utils.make_read(\n              \'AAGA\', start=10, cigar=\'2M1X1M\', quals=[64] * 4),\n          expected=[12]),\n      # ------------------------------------------------------------------------\n      # These reads test that we correctly ignore bases with low qualities.\n      # ------------------------------------------------------------------------\n      dict(\n          read=test_utils.make_read(\n              \'AAGA\', start=10, cigar=\'4M\', quals=[64, 64, 10, 30]),\n          expected=[]),\n      # Only insertions/soft clips where all bases have above our minimum base\n      # quality are included.\n      dict(\n          read=test_utils.make_read(\n              \'AAGTA\', start=10, cigar=\'2M2I1M\', quals=[64, 64, 10, 30, 64]),\n          expected=[]),\n      # The left 2S operator is ignored because one base has a 10 quality.\n      dict(\n          read=test_utils.make_read(\n              \'TGATAC\',\n              start=10,\n              cigar=\'2S3M1S\',\n              quals=[64, 10, 64, 64, 64, 64]),\n          expected=[11, 12, 13]),\n      # The right 1S operator is ignored because one base has a 10 quality.\n      dict(\n          read=test_utils.make_read(\n              \'TGATAC\',\n              start=10,\n              cigar=\'2S3M1S\',\n              quals=[64, 64, 64, 64, 64, 10]),\n          expected=[8, 9, 10, 11]),\n      dict(\n          read=test_utils.make_read(\n              \'AAGA\', start=10, cigar=\'2M1X1M\', quals=[64, 64, 30, 10]),\n          expected=[12]),\n  )\n  def test_candidates_from_one_read(self, read, expected):\n    """"""Test WindowSelector.process_read() with reads of low quality.""""""\n    self.assertCandidatesFromReadsEquals(reads=[read], expected=expected)\n\n  # Systematically test all combinations of cigar operations and positions in a\n  # read.\n  @parameterized.parameters(\n      # Check that the M operator works. We have to look at the bases on the\n      # genome to decide if it generates a position at 10.\n      dict(bases=\'A\', cigar=\'1M\', expected=[]),\n      dict(bases=\'C\', cigar=\'1M\', expected=[10]),\n      # The mismatch operator X indicates that a position mismatches the\n      # reference regardless of whether it actually matches the genome or not.\n      # The window selector inspects the actual reference genome bases, though,\n      # and generates candidate positions only if we genuinely mismatch.\n      dict(bases=\'A\', cigar=\'1X\', expected=[]),\n      dict(bases=\'C\', cigar=\'1X\', expected=[10]),\n      # The match operator = indicates that a position matches the reference\n      # even if that base actually mismatches the reference genome. The window\n      # selector inspects the actual reference genome bases, though, and\n      # generates candidate positions only if we genuinely mismatch.\n      dict(bases=\'A\', cigar=\'1=\', expected=[]),\n      dict(bases=\'C\', cigar=\'1=\', expected=[10]),\n      # The deletion operator generates positions at start for operator length\n      # in the 5\' direction starting at the base after the deletion.\n      dict(bases=\'A\', cigar=\'1M1D\', expected=[11]),\n      dict(bases=\'A\', cigar=\'1M2D\', expected=[11, 12]),\n      dict(bases=\'A\', cigar=\'1M3D\', expected=[11, 12, 13]),\n      dict(bases=\'A\', cigar=\'1M4D\', expected=[11, 12, 13, 14]),\n      # The insertion operator generates positions at start for + length\n      # basepairs in the 5\' direction and length - 1 in the 3\' direction.\n      dict(bases=\'AA\', cigar=\'1M1I\', expected=[10, 11]),\n      dict(bases=\'AAA\', cigar=\'1M2I\', expected=[9, 10, 11, 12]),\n      dict(bases=\'AAAA\', cigar=\'1M3I\', expected=[8, 9, 10, 11, 12, 13]),\n      # The soft-clip operator generates positions at the start for operator\n      # length bases.\n      dict(bases=\'AA\', cigar=\'1M1S\', expected=[10, 11]),\n      dict(bases=\'AAA\', cigar=\'1M2S\', expected=[9, 10, 11, 12]),\n      dict(bases=\'AAAA\', cigar=\'1M3S\', expected=[8, 9, 10, 11, 12, 13]),\n      dict(bases=\'AA\', cigar=\'1S1M\', expected=[9, 10]),\n      dict(bases=\'AAA\', cigar=\'2S1M\', expected=[8, 9, 10, 11]),\n      dict(bases=\'AAAA\', cigar=\'3S1M\', expected=[7, 8, 9, 10, 11, 12]),\n      # The skip (N) and hard clip (H) operators are both ignored.\n      dict(bases=\'AA\', cigar=\'1M1N1M\', expected=[]),\n      dict(bases=\'AA\', cigar=\'1M2N1M\', expected=[]),\n      dict(bases=\'A\', cigar=\'1M1H\', expected=[]),\n      dict(bases=\'A\', cigar=\'1M1H\', expected=[]),\n      dict(bases=\'A\', cigar=\'1H1M\', expected=[]),\n      dict(bases=\'A\', cigar=\'1H1M\', expected=[]),\n      # The python version raises an exception when seeing a PAD, which is ok\n      # but isn\'t strictly necessary. The C++ implementation handles PADs when\n      # counting alleles, so we\'ve commented out this test.\n      # C++ version:\n      # dict(bases=\'AA\', cigar=\'1M1P1M\', expected=[]),\n      # dict(bases=\'AA\', cigar=\'1M2P1M\', expected=[]),\n      # Python version:\n      # dict(bases=\'AA\', cigar=\'1M1P1M\', expected=ValueError),\n      # dict(bases=\'AA\', cigar=\'1M2P1M\', expected=ValueError),\n  )\n  def test_candidates_from_reads_all_cigars(self, bases, cigar, expected):\n    """"""Test WindowSelector.process_read() with reads of low quality.""""""\n    read = test_utils.make_read(\n        bases, start=10, cigar=cigar, quals=[64] * len(bases))\n    self.assertCandidatesFromReadsEquals(reads=[read], expected=expected)\n\n  @parameterized.parameters(\n      dict(\n          read=test_utils.make_read(\n              \'AGA\', start=read_start, cigar=\'3M\', quals=[64] * 3),\n          region_start=region_start,\n          region_end=region_start + 100,\n          expected=[read_start + 1],\n      ) for region_start in range(10) for read_start in range(region_start, 10))\n  def test_candidates_from_reads_position_invariance(self, read, region_start,\n                                                     region_end, expected):\n    # Tests that a read with a mismatch at position read_start + 1 produces a\n    # single candidate position at read_start + 1 regardless of where it occurs\n    # within a single region spanning region_start - region_end.\n    self.assertCandidatesFromReadsEquals(\n        reads=[read], expected=expected, start=region_start, end=region_end)\n\n  # Our region is 5-8 and we are testing that the read\'s mismatch is only\n  # included when it\'s within the region and not when it\'s outside.\n  # Expected region boundaries are extended according to region_expansion_in_bp\n  # flag. region_expansion_in_bp is set to 20 by default,\n  # so 5 to 8  becomes 5 - 20 to 8 + 20 <=> 0 to 28\n  @parameterized.parameters(\n      dict(\n          read=test_utils.make_read(\'G\', start=start, cigar=\'1M\', quals=[64]),\n          expected=[start] if 0 <= start < 28 else [],\n      ) for start in range(10))\n  def test_candidates_from_reads_respects_region(self, read, expected):\n    self.assertCandidatesFromReadsEquals(\n        reads=[read], expected=expected, start=5, end=8)\n\n  # Our region is 5-8 and we have a 4 basepair deletion in our read. We expect\n  # a mismatch count of one for each position in the deletion that overlaps the\n  # interval.\n  # Expected region boundaries are extended according to region_expansion_in_bp\n  # flag. region_expansion_in_bp is set to 20 by default,\n  # so 5 to 8  becomes 5 - 20 to 8 + 20 <=> 0 to 28\n  @parameterized.parameters(\n      dict(\n          read=test_utils.make_read(\n              \'AA\', start=start, cigar=\'1M4D1M\', quals=[64, 64]),\n          expected=[\n              pos for pos in range(start + 1, start + 5) if 0 <= pos < 28\n          ],\n      ) for start in range(10))\n  def test_candidates_from_reads_respects_region_deletion(self, read, expected):\n    self.assertCandidatesFromReadsEquals(\n        reads=[read], expected=expected, start=5, end=8, ref=\'A\' * 100)\n\n  def test_candidates_from_reads_counts_overlapping_events(self):\n    # This read has a mismatch at position 2 and a 2 bp insertion at position 4,\n    # so we need to double count the candidate positions from the mismatch and\n    # insertion at position 2.\n    read = test_utils.make_read(\n        \'AAGACCAAA\', start=0, cigar=\'4M2I3M\', quals=[64] * 9)\n    expected = [2, 3, 4, 5]\n    self.assertCandidatesFromReadsEquals(reads=[read], expected=expected)\n\n  @parameterized.parameters(\n      dict(\n          read_mapq=read_mapq,\n          min_mapq=min_mapq,\n          expect_read_to_be_included=read_mapq >= min_mapq)\n      for read_mapq in range(10, 15)\n      for min_mapq in range(8, 17))\n  def test_candidates_from_reads_respects_mapq(self, read_mapq, min_mapq,\n                                               expect_read_to_be_included):\n    read = test_utils.make_read(\n        \'AGA\', start=10, cigar=\'3M\', quals=[64] * 3, mapq=read_mapq)\n    self.config.min_mapq = min_mapq\n    self.assertCandidatesFromReadsEquals(\n        reads=[read], expected=[11] if expect_read_to_be_included else [])\n\n  @parameterized.parameters(\n      dict(\n          candidates=[100, 200],\n          expected_ranges=[\n              ranges.make_range(\'ref\', 96, 104),\n              ranges.make_range(\'ref\', 196, 204),\n          ]),\n      # Check that this works with 3 isolated regions.\n      dict(\n          candidates=[100, 200, 300],\n          expected_ranges=[\n              ranges.make_range(\'ref\', 96, 104),\n              ranges.make_range(\'ref\', 196, 204),\n              ranges.make_range(\'ref\', 296, 304),\n          ]),\n      # Check a simple example where we have two candidates from the same\n      # region:\n      dict(\n          candidates=[2, 8],\n          expected_ranges=[\n              ranges.make_range(\'ref\', -2, 12),\n          ]),\n      # Check a simple example where we have candidates from two regions:\n      dict(\n          candidates=[2, 14],\n          expected_ranges=[\n              ranges.make_range(\'ref\', -2, 6),\n              ranges.make_range(\'ref\', 10, 18),\n          ]),\n      # Check boundary conditions for merging windows: should merge.\n      dict(\n          candidates=[2, 10],\n          expected_ranges=[\n              ranges.make_range(\'ref\', -2, 14),\n          ]),\n      # Check boundary conditions for merging windows: should not merge.\n      dict(\n          candidates=[2, 11],\n          expected_ranges=[\n              ranges.make_range(\'ref\', -2, 6),\n              ranges.make_range(\'ref\', 7, 15),\n          ]),\n  )\n  def test_candidates_to_windows(self, candidates, expected_ranges):\n    self.assertEqual(\n        window_selector._candidates_to_windows(self.config, candidates, \'ref\'),\n        expected_ranges)\n\n  @parameterized.parameters(range(1, 20))\n  def test_candidates_to_windows_window_size(self, size):\n    # We have a single candidate at position 100 with a 5 count.\n    candidates = [100]\n    # We expect the created window to be +/- size from 100.\n    expected = ranges.make_range(\'ref\', 100 - size, 100 + size)\n    self.config.min_windows_distance = size\n    self.assertEqual(\n        window_selector._candidates_to_windows(self.config, candidates, \'ref\'),\n        [expected])\n\n  @parameterized.parameters(range(1, 20))\n  def test_candidates_to_windows_min_window_distance(self, distance):\n    candidates = [\n        # We one candidate at position 100 with a 5 count.\n        100,\n        # We have another candidate at outside of our distance with a 5 count,\n        # so it should produce a candidate but not be joined with our our\n        # candidate at 100.\n        100 - 2 * distance - 1,\n        # Finally, we have another variant that is exactly distance away from\n        # 100. It should be joined with the candidate at 100 to produce a single\n        # larger window.\n        100 + distance\n    ]\n    expected = [\n        # Our first window is for the 100 - 2 * distance one.\n        ranges.make_range(\'ref\', 100 - 3 * distance - 1, 100 - distance - 1),\n        # Our second window starts at 100 (- distance for the window size) and\n        # ends at 100 + distance + distance (again for window size).\n        ranges.make_range(\'ref\', 100 - distance, 100 + 2 * distance),\n    ]\n    self.config.min_windows_distance = distance\n    self.assertEqual(\n        window_selector._candidates_to_windows(self.config, candidates, \'ref\'),\n        expected)\n\n  @parameterized.parameters(range(1, 20))\n  def test_candidates_to_windows_merged_close_candidates(self, distance):\n    # Create five candidates separated by exactly distance from each other:\n    # 100, 101, 102, 103, 104 for distance == 1\n    # 100, 102, 104, 106, 108 for distance == 2\n    candidates = [100 + i * distance for i in range(5)]\n    # Which should all be merged together into one giant window.\n    expected = [\n        ranges.make_range(\'ref\', 100 - distance,\n                          max(candidates) + distance),\n    ]\n    self.config.min_windows_distance = distance\n    self.assertEqual(\n        window_selector._candidates_to_windows(self.config, candidates, \'ref\'),\n        expected)\n\n  def test_select_windows(self):\n    # Simple end-to-end test of the high-level select_windows function. We give\n    # it a few reads with a single candidate at 100 and we expect a window back\n    # centered at 100.\n    reads = [\n        test_utils.make_read(\'AGA\', start=99, cigar=\'3M\', quals=[64] * 3),\n        test_utils.make_read(\'AGA\', start=99, cigar=\'3M\', quals=[63] * 3),\n        test_utils.make_read(\'AGA\', start=99, cigar=\'3M\', quals=[62] * 3),\n    ]\n    chrom = reads[0].alignment.position.reference_name\n    ref_reader = fasta.InMemoryFastaReader([(chrom, 0, \'A\' * 300)])\n    region = ranges.make_range(chrom, 0, 200)\n\n    self.assertEqual(\n        window_selector.select_windows(self.config, ref_reader, reads, region),\n        [ranges.make_range(chrom, 96, 104)])\n\n  def test_select_windows_returns_empty_list_when_no_reads(self):\n    self.assertEqual([],\n                     window_selector.select_windows(\n                         self.config,\n                         ref_reader=fasta.InMemoryFastaReader([(\'chr1\', 0,\n                                                                \'A\' * 500)]),\n                         reads=[],\n                         region=ranges.make_range(\'chr1\', 1, 100)))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/testing/flagsaver.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Save and restore flag values after the decorated function completes.\n\nThere are many ways to save and restore.  Always use the most convenient method\nfor a given use case.\n\nHere are examples of each method.  They all call DoStuff() while FLAGS.someflag\nis temporarily set to \'foo\'.\n\n  # Use a decorator which can override flags via arguments.\n  @flagsaver.FlagOverrider(someflag=\'foo\')\n  def SomeFunc():\n    DoStuff()\n\n  # Use a decorator which does not override flags itself.\n  @flagsaver.FlagSaver\n  def SomeFunc():\n    FLAGS.someflag = \'foo\'\n    DoStuff()\n\n  # Use a context manager which can optionally override flags via arguments.\n  with flagsaver.FlagContext(someflag=\'foo\'):\n    DoStuff()\n\n  # Save and restore the flag values yourself.\n  saved_flag_values = flagsaver.SaveFlagValues()\n  try:\n    FLAGS.someflag = \'foo\'\n    DoStuff()\n  finally:\n    flagsaver.RestoreFlagValues(saved_flag_values)\n\nWe save and restore a shallow copy of each Flag object\'s __dict__ attribute.\nThis preserves all attributes of the flag, such as whether or not it was\noverridden from its default value.\n\nWARNING: Currently a flag that is saved and then deleted cannot be restored.  An\nexception will be raised.  However if you *add* a flag after saving flag values,\nand then restore flag values, the added flag will be deleted with no errors.  If\nyou wish to delete and then restore saved flags, send a CL.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport functools\nimport inspect\n\n\nfrom absl import flags\n\nfrom tensorflow.python.platform import flags\n\nFLAGS = flags.FLAGS\n\n\n@contextlib.contextmanager\ndef FlagContext(**overrides):\n  """"""Context manager that saves flag values on entry and restores them on exit.\n\n  Args:\n    **overrides: Set these flag values while within the context manager.\n\n  Yields:\n    Nothing, it\'s a context manager.\n  """"""\n  saved_flag_values = SaveFlagValues()\n  for name, value in overrides.items():\n    setattr(FLAGS, name, value)\n  try:\n    yield\n  finally:\n    RestoreFlagValues(saved_flag_values)\n\n\ndef _FlagSaver(func, overrides):\n  """"""Creates a wrapper function that saves/restores flag values.\n\n  Args:\n    func: function object - This will be called between saving flags and\n      restoring flags.\n    overrides: {str: object} - Flag names mapped to their values.  These flags\n      will be set after saving the original flag state.\n\n  Returns:\n    return value from func()\n  """"""\n\n  @functools.wraps(func)\n  def _FlagSaverWrapper(*args, **kwargs):\n    """"""Wrapper function that saves and restores flags.""""""\n    with FlagContext(**overrides):\n      return func(*args, **kwargs)\n\n  return _FlagSaverWrapper\n\n\ndef FlagSaver(func):\n  """"""Saves/restores flag values after decorated method completes.""""""\n  if inspect.isclass(func):\n    raise TypeError(\'@flagsaver.FlagSaver cannot be applied to a class.\')\n  return _FlagSaver(func, {})\n\n\nclass FlagOverrider(object):\n  """"""Overrides flags for the duration of the decorated function call.\n\n  It also restores all original values of flags after decorated method\n  completes.\n  """"""\n\n  def __init__(self, **overrides):\n    self._overrides = overrides\n\n  def __call__(self, func):\n    if inspect.isclass(func):\n      raise TypeError(\'@flagsaver.FlagOverrider cannot be applied to a class.\')\n    return _FlagSaver(func, self._overrides)\n\n\ndef _CopyFlagDict(flag):\n  """"""Returns a copy of the flag object\'s __dict__.\n\n  It\'s mostly a shallow copy of the __dict__, except it also does a shallow\n  copy of the validator list.\n\n  Args:\n    flag: A flags.Flag instance.\n\n  Returns:\n    A copy of the flag object\'s __dict__.\n  """"""\n  copy = flag.__dict__.copy()\n  copy[\'validators\'] = list(flag.validators)\n  return copy\n\n\ndef SaveFlagValues():\n  """"""Returns copy of flag values as a dict.\n\n  Returns:\n    Dictionary mapping keys to values. Keys are flag names, values are\n    corresponding __dict__ members. E.g. {\'key\': value_dict, ...}.\n  """"""\n  if hasattr(flags, \'_FlagValues\'):  # pylint:disable=protected-access\n    # In OSS code we use tensorflow/python/platform/flags.py:_FlagValues\n    # which is not iterable.\n    flag_dict = FLAGS.__dict__[\'__flags\']\n    # Make a shallow copy of the flags.\n    return {name: flag_dict[name] for name in flag_dict}\n  else:\n    # FLAGS is iterable and provides __getitem__.\n    return {name: _CopyFlagDict(FLAGS[name]) for name in FLAGS}\n\n\ndef RestoreFlagValues(saved_flag_values):\n  """"""Restores flag values based on the dictionary of flag values.\n\n  Args:\n    saved_flag_values: {\'key\': value_dict, ...}\n  """"""\n  if hasattr(flags, \'_FlagValues\'):  # pylint:disable=protected-access\n    # In OSS code we use tensorflow/python/platform/flags.py:_FlagValues\n    # which is not iterable.\n    new_flag_names = FLAGS.__dict__[\'__flags\'].keys()\n  else:\n    # FLAGS is iterable and provides __getitem__.\n    new_flag_names = list(FLAGS)\n\n  for name in new_flag_names:\n    value = saved_flag_values.get(name)\n    if value is None:\n      # If value was not saved delete it\n      if hasattr(flags, \'_FlagValues\'):  # pylint:disable=protected-access\n        # In OSS code we use tensorflow/python/platform/flags.py.\n        del FLAGS.__dict__[\'__flags\'][name]\n      else:\n        delattr(FLAGS, name)\n\n    else:\n      if hasattr(flags, \'_FlagValues\'):  # pylint:disable=protected-access\n        # In OSS code we use tensorflow/python/platform/flags.py:_FlagValues\n        # which is not iterable.\n        FLAGS.__dict__[\'__flags\'][name] = value\n      else:\n        # We bypass the overridden __setitem__ and __setattr__ methods of FLAGS.\n        FLAGS[name].__dict__ = value\n'"
deepvariant/testing/tf_test_utils.py,14,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Utility functions for creating TensforFlow mock models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import flags\nimport tensorflow as tf\nimport tf_slim\nfrom deepvariant import dv_constants\nfrom deepvariant import modeling\n\nFLAGS = flags.FLAGS\n# Constants dictating moving average.\n_MOVING_AVERAGE_DECAY = 0.995\n\nslim = tf_slim\n\n\ndef write_fake_checkpoint(model_name,\n                          session,\n                          checkpoint_dir,\n                          moving_average_decay=_MOVING_AVERAGE_DECAY,\n                          name=\'model\'):\n  """"""Writes a fake TensorFlow checkpoint to checkpoint_dir.""""""\n  path = os.path.join(checkpoint_dir, name)\n  with session as sess:\n    model = modeling.get_model(model_name)\n    # Needed to protect ourselves for models without an input image shape.\n    h, w = getattr(\n        model, \'input_image_shape\',\n        (dv_constants.PILEUP_DEFAULT_HEIGHT, dv_constants.PILEUP_DEFAULT_WIDTH))\n    images = tf.compat.v1.placeholder(\n        tf.float32, shape=(4, h, w, dv_constants.PILEUP_NUM_CHANNELS))\n    model.create(images, num_classes=3, is_training=True)\n    # This is gross, but necessary as model_eval assumes the model was trained\n    # with model_train which uses exp moving averages. Unfortunately we cannot\n    # just call into model_train as it uses FLAGS which conflict with the\n    # flags in use by model_eval. So we inline the creation of the EMA here.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        moving_average_decay, tf.compat.v1.train.get_or_create_global_step())\n    tf.compat.v1.add_to_collection(\n        tf.compat.v1.GraphKeys.UPDATE_OPS,\n        variable_averages.apply(slim.get_model_variables()))\n    sess.run(tf.compat.v1.global_variables_initializer())\n    save = tf.compat.v1.train.Saver(slim.get_variables())\n    save.save(sess, path)\n  return path\n\n\ndef test_tmpdir(name):\n  """"""Returns a path to a temp directory name in the test tmpdir.\n\n  Args:\n    name: str; the name of the file, should not contain any slashes.\n\n  Returns:\n    str path to a tmpfile with filename name in our test tmpfile directory.\n  """"""\n  dirname = os.path.join(tf.compat.v1.test.get_temp_dir(), name)\n  tf.io.gfile.makedirs(dirname)\n  return dirname\n\n\ndef check_file_exists(name, eval_name=None):\n  """"""Returns true if file exists in directory.""""""\n  if not eval_name:\n    file_name = os.path.join(tf.compat.v1.test.get_temp_dir(), name)\n  else:\n    directory = os.path.join(tf.compat.v1.test.get_temp_dir(),\n                             \'eval_\' + eval_name)\n    file_name = os.path.join(directory, name)\n  return tf.io.gfile.exists(file_name)\n\n\n# redacted\n# taking a required_variables_regexps and have a function that looks like:\n# return all(any(re.match(var, pat) for var in var_to_shape_map.keys()\n#   for pat in required_variable_regexps))\ndef check_equals_checkpoint_top_scopes(checkpoint, list_of_scopes):\n  """"""Returns true if list_of_scopes equals the top scopes in checkpoint.""""""\n  reader = tf.compat.v1.train.NewCheckpointReader(checkpoint)\n  var_to_shape_map = reader.get_variable_to_shape_map()\n  top_scopes_in_ckpt = set([x.split(\'/\')[0] for x in var_to_shape_map.keys()])\n  return top_scopes_in_ckpt == set(list_of_scopes)\n'"
deepvariant/vendor/timer.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""This module implements classes useful for timing how long an action took.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\n\nclass TimerError(Exception):\n  """"""Timer Error Exception.""""""\n\n\nclass Timer(object):\n  """"""Measure time elapsed since some event.\n\n  Example usage:\n\n  from deepvariant.vendor import timer\n  a_timer = timer.Timer()\n  a_timer.Start()\n\n  # Do stuff here ...\n\n  a_timer.Stop()\n  if a_timer.GetDuration() > 100:\n    print \'This took too long\'\n\n  Another way to use this class is the with statement:\n\n  with timer.Timer() as t:\n    # Do time consuming stuff ...\n  print \'Time consuming stuff took %f seconds.\' % t.GetDuration()\n  """"""\n\n  def __init__(self):\n    """"""Initializes a timer.""""""\n    self.duration = 0\n    self.running = False\n    self.start = None\n\n  def Start(self):\n    """"""Resets and starts a timer.""""""\n\n    self._StartInternal()\n\n  def Stop(self):\n    """"""Stops a timer, and records the duration.\n\n    Stop is idempotent.\n\n    Returns:\n      The duration, i.e., the time (in seconds) for which the timer ran.\n    """"""\n    self._StopInternal()\n    return self.duration\n\n  def __enter__(self):\n    """"""Resets and starts a timer.\n\n    This allows Timer to be used as a ContextManager type.\n\n    Returns:\n      The object itself so that it can be bound to a variable in a with\n      statement.\n    """"""\n    self._StartInternal()\n    return self\n\n  def __exit__(self, unused_ex_type, unused_ex, unused_ex_trace):\n    """"""Stops a timer and records the duration.\n\n    This allows Timer to be used as a ContextManager type.\n\n    Returns:\n      False.  This means that any exceptions raised within the body of the\n      caller\'s with statement will propagate up the stack without interference.\n    """"""\n    self._StopInternal()\n    return False\n\n  def _StartInternal(self):\n    """"""Resets and starts a timer.\n\n    Common implementation for Start and __enter__.\n    """"""\n    self.start = time.time()\n    self.running = 1\n\n  def _StopInternal(self):\n    """"""Stops a timer and records the duration.\n\n    Common implementation for Stop and __exit__.\n    """"""\n    if self.running:\n      self.duration = time.time() - self.start\n      self.running = 0\n\n  def IsRunning(self):\n    return self.running\n\n  def __str__(self):\n    return str(self.GetDuration())\n\n  def GetDuration(self):\n    """"""Returns the elapsed time, in seconds.\n\n    Returns:\n       If timer is still running: the time (in seconds) elapsed since the start\n       Otherwise: the time (in seconds) for which the timer ran.\n    """"""\n    if self.running:\n      return time.time() - self.start\n    else:\n      return self.duration\n\n  def GetStartTime(self):\n    """"""Returns start time of the timer.\n\n    Returns:\n       The start time of the timer, floating-point seconds since the epoch.\n    Raises:\n       TimerError: if the timer was not started\n    """"""\n    if self.start:\n      return self.start\n    else:\n      raise TimerError(\'TimerNotStarted\')\n\n  def GetStopTime(self):\n    """"""Returns stop time of the timer.\n\n    Returns:\n       The stop time of the timer, floating-point seconds since the epoch.\n    Raises:\n       TimerError: if the timer was never started or is still running\n    """"""\n    if not self.start:\n      raise TimerError(\'TimerNotStarted\')\n    elif self.running:\n      raise TimerError(\'TimerStillRunning\')\n    else:\n      return self.start + self.duration\n\n\nclass TimerStart(Timer):\n  """"""A timer that automatically starts when you construct it.\n\n  This is otherwise identical in interface to the Timer class in this module.\n  """"""\n\n  def __init__(self):\n    Timer.__init__(self)\n    self.Start()\n\n\nclass MultiIntervalTimer(Timer):\n  """"""A timer that records cumulative time intervals.\n\n  Example usage:\n\n  from deepvariant.vendor import timer\n  a_timer = timer.MultiIntervalTimer()\n\n  # Do stuff the first time\n  a_timer.Start()\n  # ... stuff ...\n  a_timer.Stop()\n\n  # Do stuff the second time (repeat as many times as you like)\n  a_timer.Start()\n  # ... stuff ...\n  a_timer.Stop()\n\n  if a_timer.GetDuration() > 1000:\n    print \'Total time spent doing stuff is too much!\'\n\n  Another way to use this class is the with statement:\n\n  t = timer.MultiIntervalTimer()\n  with t:\n    # Do stuff the first time\n\n  with t:\n    # Do stuff the second time (repeat as many times as you like)\n\n  print \'Total time spent doing stuff was %f seconds.\' % t.GetDuration()\n  """"""\n\n  def __init__(self):\n    super(MultiIntervalTimer, self).__init__()\n    self.stop = None\n\n  def Reset(self):\n    """"""Resets the measured cumulative time to zero.""""""\n    self.duration = 0\n\n  def _StopInternal(self):\n    """"""Stops a timer and adds the current duration to the total.\n\n    Common implementation for Stop and __exit__.\n    """"""\n    if self.running:\n      self.stop = time.time()\n      self.duration += self.stop - self.start\n      self.running = 0\n\n  def GetDuration(self):\n    """"""Returns the total elapsed time, in seconds.\n\n    Returns:\n       The total time (in seconds) accumulated so far, regardless of counter\n       state (i.e. if stopped, total time spent in all running windows; if\n       running, total time spent in all previous running windows plus time\n       elapsed from the last call to Start() until GetDuration() was called).\n    """"""\n    if self.running:\n      return self.duration + (time.time() - self.start)\n    else:\n      return self.duration\n\n  def GetStopTime(self):\n    """"""Returns the last stop time of the timer.\n\n    Returns:\n       The last stop time of the timer, same format as time.time()\n    Raises:\n       TimerError: if the timer was never started or is still running\n    """"""\n    if not self.start:\n      return super(MultiIntervalTimer, self).GetStopTime()\n    elif self.running:\n      return super(MultiIntervalTimer, self).GetStopTime()\n    else:\n      return self.stop\n'"
deepvariant/vendor/timer_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for deepvariant .vendor.timer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom absl.testing import absltest\n\nfrom deepvariant.vendor import timer\n\n\nclass TimerTest(absltest.TestCase):\n\n  def testVacuous(self):\n    with timer.Timer() as t:\n      pass\n    print(\'Time consuming stuff took %f seconds.\' % t.GetDuration())\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Init file for Nucleus package.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n'"
deepvariant/realigner/python/debruijn_graph_wrap_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import fasta\nfrom third_party.nucleus.io import sam\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\nfrom deepvariant import testdata\nfrom deepvariant.protos import realigner_pb2\nfrom deepvariant.realigner.python import debruijn_graph\n\n\ndef setUpModule():\n  testdata.init()\n\n\nclass DeBruijnGraphWrapTest(parameterized.TestCase):\n  """"""Basic tests for the wrapped DeBruijnGraph class.""""""\n\n  def dbg_options(self):\n    return realigner_pb2.DeBruijnGraphOptions(\n        min_k=12,\n        max_k=50,\n        step_k=2,\n        min_mapq=20,\n        min_base_quality=20,\n        min_edge_weight=2,\n        max_num_paths=10)\n\n  def single_k_dbg_options(self, k):\n    """"""Get a DeBruijnGraphOptions allowing us to try a single kmer size.""""""\n    test_options = self.dbg_options()\n    test_options.min_k = k\n    test_options.max_k = k\n    test_options.step_k = 1\n    return test_options\n\n  def assertGraphEqual(self, graphviz_string, dbg):\n    """"""Assert that the DeBruijn has the given graphviz representation.\n\n    Args:\n      graphviz_string: the graphviz representation, potentially including common\n        leading whitespace.\n      dbg: the DeBruijn graph object.\n    """"""\n    # Remove all whitespace before comparison to avoid failing over trivial\n    # indentation / newline differences.\n    self.assertEqual(\'\'.join(graphviz_string.split()),\n                     \'\'.join(dbg.graphviz().split()))\n\n  def test_basics(self):\n    """"""Basic example.""""""\n    ref_str = \'GATTACA\'\n    read_str = \'GATGACA\'\n    read = test_utils.make_read(\n        read_str,\n        chrom=\'chr20\',\n        start=1,\n        cigar=[(len(read_str), \'M\')],\n        quals=[30] * len(read_str),\n        name=\'read\')\n\n    self.assertEqual(self.single_k_dbg_options(3).min_k, 3)\n    # Use two reads so read path doesn\'t get pruned.\n    dbg = debruijn_graph.build(ref_str, [read, read],\n                               self.single_k_dbg_options(3))\n\n    self.assertItemsEqual([ref_str, read_str], dbg.candidate_haplotypes())\n\n    self.assertGraphEqual(\n        """"""\\\n          digraph G {\n          0[label=GAT];\n          1[label=ATT];\n          2[label=TTA];\n          3[label=TAC];\n          4[label=ACA];\n          5[label=ATG];\n          6[label=TGA];\n          7[label=GAC];\n          0->1 [label=1 color=red];\n          1->2 [label=1 color=red];\n          2->3 [label=1 color=red];\n          3->4 [label=1 color=red];\n          0->5 [label=2];\n          5->6 [label=2];\n          6->7 [label=2];\n          7->4 [label=2];\n          }\n          """""", dbg)\n\n  def test_pruning_1(self):\n    """"""Test that pruning removes a path traced by only one read.""""""\n    ref_str = \'GATTACA\'\n    read_str = \'GATGACA\'\n    read = test_utils.make_read(\n        read_str,\n        chrom=\'chr20\',\n        start=1,\n        cigar=[(len(read_str), \'M\')],\n        quals=[30] * len(read_str),\n        name=\'read\')\n    dbg = debruijn_graph.build(ref_str, [read], self.single_k_dbg_options(3))\n    self.assertGraphEqual(\n        """"""\\\n        digraph G {\n        0[label=GAT];\n        1[label=ATT];\n        2[label=TTA];\n        3[label=TAC];\n        4[label=ACA];\n        0->1 [label=1 color=red];\n        1->2 [label=1 color=red];\n        2->3 [label=1 color=red];\n        3->4 [label=1 color=red];\n        }\n        """""", dbg)\n\n  def test_pruning_2(self):\n    """"""Test that pruning removes edges not between source and sink.""""""\n    ref_str = \'GATTACA\'\n    read_str = \'CCGATGACACC\'\n    read = test_utils.make_read(\n        read_str,\n        chrom=\'chr20\',\n        start=1,\n        cigar=[(len(read_str), \'M\')],\n        quals=[30] * len(read_str),\n        name=\'read\')\n    # Use two reads so read path doesn\'t get pruned.\n    dbg = debruijn_graph.build(ref_str, [read, read],\n                               self.single_k_dbg_options(3))\n\n    self.assertGraphEqual(\n        """"""\\\n        digraph G {\n        0[label=GAT];\n        1[label=ATT];\n        2[label=TTA];\n        3[label=TAC];\n        4[label=ACA];\n        5[label=ATG];\n        6[label=TGA];\n        7[label=GAC];\n        0->1 [label=1 color=red];\n        1->2 [label=1 color=red];\n        2->3 [label=1 color=red];\n        3->4 [label=1 color=red];\n        0->5 [label=2];\n        5->6 [label=2];\n        6->7 [label=2];\n        7->4 [label=2];\n        }\n        """""", dbg)\n\n  @parameterized.parameters(\n      # No bad positions => all edges get +1 to counts.\n      dict(bad_position=None, dropped_edges={}),\n\n      # Ref and read are   GATTACA\n      # Bad position: 0 => *\n      # Breaks kmers:      GA->AT\n      dict(bad_position=0, dropped_edges={\'GA->AT\'}),\n\n      # Ref and read are   GATTACA\n      # Bad position: 1 =>  *\n      # Breaks kmers:      GA->AT, AT->TT\n      dict(bad_position=1, dropped_edges={\'GA->AT\', \'AT->TT\'}),\n\n      # Ref and read are   GATTACA\n      # Bad position: 2 =>   *\n      # Breaks kmers:      GA->AT, AT->TT, TT->TA\n      dict(bad_position=2, dropped_edges={\'GA->AT\', \'AT->TT\', \'TT->TA\'}),\n\n      # Ref and read are   GATTACA\n      # Bad position: 3 =>    *\n      # Breaks kmers:      AT->TT, TT->TA, TA->AC\n      dict(bad_position=3, dropped_edges={\'AT->TT\', \'TT->TA\', \'TA->AC\'}),\n\n      # Ref and read are   GATTACA\n      # Bad position: 4 =>     *\n      # Breaks kmers:      TT->TA, TA->AC, AC->CA\n      dict(bad_position=4, dropped_edges={\'TT->TA\', \'TA->AC\', \'AC->CA\'}),\n\n      # Ref and read are   GATTACA\n      # Bad position: 5 =>      *\n      # Breaks kmers:      TA->AC, AC->CA\n      dict(bad_position=5, dropped_edges={\'TA->AC\', \'AC->CA\'}),\n\n      # Ref and read are   GATTACA\n      # Bad position: 6 =>       *\n      # Breaks kmers:      AC->CA\n      dict(bad_position=6, dropped_edges={\'AC->CA\'}),\n  )\n  def test_adding_edges_with_bad_positions(self, bad_position, dropped_edges):\n    """"""Test that we filter out edges containing low-quality basecalls.""""""\n    ref_str = \'GATTACA\'\n    read_str = \'GATTACA\'\n\n    kmer_indices = {\n        \'GA\': 0,\n        \'AT\': 1,\n        \'TT\': 2,\n        \'TA\': 3,\n        \'AC\': 4,\n        \'CA\': 5,\n    }\n\n    def kmer_to_index_edge(kmer_edge):\n      k1, k2 = kmer_edge.split(\'->\')\n      return \'{}->{}\'.format(kmer_indices[k1], kmer_indices[k2])\n\n    dropped_edges = {kmer_to_index_edge(edge) for edge in dropped_edges}\n\n    for bad_type in [\'qual\', \'base\']:\n      bases = list(read_str)\n      quals = [30] * len(bases)\n      cigar = [(len(bases), \'M\')]\n      if bad_position is not None:\n        if bad_type == \'qual\':\n          quals[bad_position] = 1\n        elif bad_type == \'base\':\n          bases[bad_position] = \'N\'\n        else:\n          raise ValueError(\'Unexpected base type\')\n\n      read = test_utils.make_read(\n          \'\'.join(bases), start=0, cigar=cigar, quals=quals)\n\n      # Use two reads so read path doesn\'t get pruned.\n      dbg = debruijn_graph.build(ref_str, [read, read],\n                                 self.single_k_dbg_options(2))\n\n      expected_edges = \'\\n\'.join(\n          \'{} [label={} color=red];\'.format(edge, 1 if edge in\n                                            dropped_edges else 3)\n          for edge in [\'0->1\', \'1->2\', \'2->3\', \'3->4\', \'4->5\'])\n\n      self.assertGraphEqual(\n          """"""\\\n            digraph G {\n            0[label=GA];\n            1[label=AT];\n            2[label=TT];\n            3[label=TA];\n            4[label=AC];\n            5[label=CA];\n            %s\n            }\n            """""" % expected_edges, dbg)\n\n  def test_straightforward_region(self):\n    ref_reader = fasta.IndexedFastaReader(testdata.CHR20_FASTA)\n    bam_reader = sam.SamReader(testdata.CHR20_BAM)\n    region = ranges.parse_literal(\'chr20:10,000,000-10,000,100\')\n    ref_seq = ref_reader.query(region)\n\n    all_reads = list(bam_reader.query(region))\n    dbg30 = debruijn_graph.build(ref_seq, all_reads,\n                                 self.single_k_dbg_options(30))\n    self.assertIsNotNone(dbg30)\n    self.assertEqual([ref_seq], dbg30.candidate_haplotypes())\n\n  def test_complex_region(self):\n    # There is a heterozygous 9 bp deletion of tandem TGA repeat.\n    # ""chr20:10,095,379-10,095,500""\n    ref_reader = fasta.IndexedFastaReader(testdata.CHR20_FASTA)\n    bam_reader = sam.SamReader(testdata.CHR20_BAM)\n    region = ranges.parse_literal(\'chr20:10,095,379-10,095,500\')\n    ref_seq = ref_reader.query(region)\n    reads = list(bam_reader.query(region))\n    dbg = debruijn_graph.build(ref_seq, reads, self.dbg_options())\n    self.assertIsNotNone(dbg)\n    self.assertEqual(44, dbg.kmer_size)\n    self.assertLen(dbg.candidate_haplotypes(), 2)\n    self.assertIn(ref_seq, dbg.candidate_haplotypes())\n\n  def test_k_exceeds_read_length(self):\n    """"""This is a regression test for internal.""""""\n    # If k > read length, no edges will go into the graph from this read.\n    # This crashed prior to the bugfix.\n    ref_str = \'GATTACATG\'\n    read_str = \'GATGACA\'\n    read = test_utils.make_read(\n        read_str,\n        chrom=\'chr20\',\n        start=1,\n        cigar=[(len(read_str), \'M\')],\n        quals=[30] * len(read_str),\n        name=\'read\')\n    dbg = debruijn_graph.build(ref_str, [read, read],\n                               self.single_k_dbg_options(8))\n    self.assertIsNotNone(dbg)\n\n  def test_k_exceeds_ref_length(self):\n    """"""This is a regression test for internal.""""""\n    # We don\'t allow a k >= ref length.  This crashed prior to the bugfix.\n    ref_str = \'GATTACA\'\n    dbg = debruijn_graph.build(ref_str, [], self.single_k_dbg_options(7))\n    self.assertIsNone(dbg)\n    dbg = debruijn_graph.build(ref_str, [], self.single_k_dbg_options(8))\n    self.assertIsNone(dbg)\n\n  @parameterized.parameters(\n      dict(ref=\'ACGTACGT\', smallest_good_k=5),\n      dict(ref=\'ACGTAAACGT\', smallest_good_k=5),\n      dict(ref=\'ACGTAAACGTAAA\', smallest_good_k=8),\n      dict(ref=\'AAACGTAAACGT\', smallest_good_k=7),\n      dict(ref=\'AAACGTAAACGTAAA\', smallest_good_k=10),\n      # Actual example where the cycle detector failed because the cycle only\n      # occurs with the last kmer in the reference.\n      dict(\n          ref=(\n              \'TGGTAAGTTTATAAGGTTATAAGCTGAGAGGTTTTGCTGATCTTGGCTGAGCTCAGCTGGGCAGGTC\'\n              \'TTCCGGTCTTGGCTGGGGTTCACTGACACACAAGCAGCTGACAGTTGGCTGATCTAGGATGGCCTCA\'\n              \'GCTGGG\'),\n          smallest_good_k=11),\n  )\n  def test_ref_cycle_detector(self, ref, smallest_good_k):\n    min_k = max(smallest_good_k - 5, 1)\n    max_k = min(smallest_good_k + 5, len(ref))\n    for k in range(min_k, max_k):\n      # The build fails, returning None, with a k < smallest_good_k. If\n      # k >= smallest_good_k, then we expect a real non-None instance.\n      result = debruijn_graph.build(ref, [], self.single_k_dbg_options(k))\n      if k < smallest_good_k:\n        self.assertIsNone(result, \'Cycle not detected for k={}\'.format(k))\n      else:\n        self.assertIsNotNone(result, \'False cycle detected for k={}\'.format(k))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/realigner/python/ssw_misc_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nimport six\nfrom deepvariant.realigner.python import ssw\n\n\ndef p(obj):\n  for x in dir(obj):\n    if not x.startswith(\'_\'):\n      print(x + \':\' + repr(getattr(obj, x, \'\')))\n\n\nclass SswGccTest(absltest.TestCase):\n  """"""Tests for the wrapped SSW aligner in a way that fails with gcc5.4.""""""\n\n  def test_short(self):\n    """"""Test very short strings.""""""\n    ref = \'tttt\'\n    query = \'ttAtt\'\n    match = 4\n    mismatch = 2\n    gap_extend_penalty = 2\n    gap_open_penalty = 4\n\n    aligner = ssw.Aligner.construct(\n        match_score=match,\n        mismatch_penalty=mismatch,\n        gap_opening_penalty=gap_open_penalty,\n        gap_extending_penalty=gap_extend_penalty)\n    filter_ = ssw.Filter()\n    length = aligner.set_reference_sequence(ref)\n    self.assertLen(ref, length)\n    alignment = aligner.align(query, filter_)\n    p(alignment)\n    self.assertEqual(six.b(\'2=1I2=\'), alignment.cigar_string)\n\n  def test_longer(self):\n    """"""Test longer strings, so the second-best alignment is considered.""""""\n    ref = \'TTTTGGGGGGGGGGGGG\'\n    query = \'TTATTGGGGGGGGGGGGG\'\n    match = 4\n    mismatch = 2\n    gap_extend_penalty = 2\n    gap_open_penalty = 4\n\n    aligner = ssw.Aligner.construct(\n        match_score=match,\n        mismatch_penalty=mismatch,\n        gap_opening_penalty=gap_open_penalty,\n        gap_extending_penalty=gap_extend_penalty)\n    filter_ = ssw.Filter()\n    length = aligner.set_reference_sequence(ref)\n    self.assertLen(ref, length)\n    alignment = aligner.align(query, filter_)\n    p(alignment)\n    self.assertEqual(six.b(\'2=1I15=\'), alignment.cigar_string)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
deepvariant/realigner/python/ssw_wrap_test.py,0,"b'# Copyright 2017 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\n\n\nfrom absl.testing import absltest\nimport six\nfrom deepvariant.realigner.python import ssw\n\nREF = \'CAGCCTTTCTGACCCGGAAATCAAAATAGGCACAACAAA\'\nQUERY = \'CTGAGCCGGTAAATC\'\n\n# Expected alignment:\n#   CAGCCTTTCTGACCCGG-AAATCAAAATAGGCACAACAAA\n#           ||||*|||| |||||\n#           CTGAGCCGGTAAATC\n\n\nclass SswWrapTest(absltest.TestCase):\n  """"""Basic tests for the wrapped SSW aligner.""""""\n\n  def test_Align(self):\n    """"""Tests the Align method.""""""\n    aligner = ssw.Aligner()\n    filter_ = ssw.Filter()\n    length = aligner.set_reference_sequence(REF)\n    self.assertLen(REF, length)\n    alignment = aligner.align(QUERY, filter_)\n    self.assertEqual(21, alignment.sw_score)\n    self.assertEqual(8, alignment.sw_score_next_best)\n    self.assertEqual(8, alignment.ref_begin)\n    self.assertEqual(21, alignment.ref_end)\n    self.assertEqual(0, alignment.query_begin)\n    self.assertEqual(14, alignment.query_end)\n    self.assertEqual(4, alignment.ref_end_next_best)\n    self.assertEqual(2, alignment.mismatches)\n    self.assertEqual(six.b(\'4=1X4=1I5=\'), alignment.cigar_string)\n\n  def test_Align2_reversed(self):\n    """"""Tests the Align method, reversing query and ref from above.""""""\n    aligner = ssw.Aligner()\n    filter_ = ssw.Filter()\n    aligner.set_reference_sequence(QUERY)\n    alignment = aligner.align(REF, filter_)\n    self.assertEqual(21, alignment.sw_score)\n    self.assertEqual(8, alignment.query_begin)\n    self.assertEqual(21, alignment.query_end)\n    self.assertEqual(0, alignment.ref_begin)\n    self.assertEqual(14, alignment.ref_end)\n    self.assertEqual(2, alignment.mismatches)\n    self.assertEqual(six.b(\'8S4=1X4=1D5=17S\'), alignment.cigar_string)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n'"
third_party/nucleus/io/bed.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Classes for reading and writing BED files.\n\nThe BED format is described at\nhttps://genome.ucsc.edu/FAQ/FAQformat.html#format1\n\nAPI for reading:\n\n```python\nfrom third_party.nucleus.io import bed\n\n# Iterate through all records.\nwith bed.BedReader(input_path) as reader:\n  for record in reader:\n    print(record)\n```\n\nwhere `record` is a `nucleus.genomics.v1.BedRecord` protocol buffer.\n\nAPI for writing:\n\n```python\nfrom third_party.nucleus.io import bed\nfrom third_party.nucleus.protos import bed_pb2\n\n# records is an iterable of nucleus.genomics.v1.BedRecord protocol buffers.\nrecords = ...\n\n# header defines how many fields to write out.\nheader = bed_pb2.BedHeader(num_fields=5)\n\n# Write all records to the desired output path.\nwith bed.BedWriter(output_path, header) as writer:\n  for record in records:\n    writer.write(record)\n```\n\nFor both reading and writing, if the path provided to the constructor contains\n\'.tfrecord\' as an extension, a `TFRecord` file is assumed and attempted to be\nread or written. Otherwise, the filename is treated as a true BED file.\n\nFiles that end in a \'.gz\' suffix cause the file to be treated as compressed\n(with BGZF if it is a true BED file, and with gzip if it is a TFRecord file).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom third_party.nucleus.io import genomics_reader\nfrom third_party.nucleus.io import genomics_writer\nfrom third_party.nucleus.io.python import bed_reader\nfrom third_party.nucleus.io.python import bed_writer\nfrom third_party.nucleus.protos import bed_pb2\n\n\nclass NativeBedReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from native BED files.\n\n  Most users will want to use BedReader instead, because it dynamically\n  dispatches between reading native BED files and TFRecord files based on the\n  filename\'s extension.\n  """"""\n\n  def __init__(self, input_path, num_fields=0):\n    """"""Initializes a NativeBedReader.\n\n    Args:\n      input_path: string. A path to a resource containing BED records.\n      num_fields: int. The number of fields to read in the BED. If unset or set\n        to zero, all fields in the input are read.\n    """"""\n    super(NativeBedReader, self).__init__()\n\n    bed_path = input_path.encode(\'utf8\')\n    options = bed_pb2.BedReaderOptions(num_fields=num_fields)\n    self._reader = bed_reader.BedReader.from_file(bed_path, options)\n    self.header = self._reader.header\n\n  def query(self):\n    """"""Returns an iterator for going through the records in the region.\n\n    NOTE: This function is not currently implemented by NativeBedReader though\n    it could be implemented for sorted, tabix-indexed BED files.\n    """"""\n    raise NotImplementedError(\'Can not currently query a BED file\')\n\n  def iterate(self):\n    """"""Returns an iterable of BedRecord protos in the file.""""""\n    return self._reader.iterate()\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass BedReader(genomics_reader.DispatchingGenomicsReader):\n  """"""Class for reading BedRecord protos from BED or TFRecord files.""""""\n\n  def _native_reader(self, input_path, **kwargs):\n    return NativeBedReader(input_path, **kwargs)\n\n  def _record_proto(self):\n    return bed_pb2.BedRecord\n\n\nclass NativeBedWriter(genomics_writer.GenomicsWriter):\n  """"""Class for writing to native BED files.\n\n  Most users will want BedWriter, which will write to either native BED\n  files or TFRecord files, based on the output filename\'s extension.\n  """"""\n\n  def __init__(self, output_path, header=None):\n    """"""Initializer for NativeBedWriter.\n\n    Args:\n      output_path: str. The path to which to write the BED file.\n      header: nucleus.genomics.v1.BedHeader. The header that defines all\n        information germane to the constituent BED records.\n    """"""\n    super(NativeBedWriter, self).__init__()\n    if header is None:\n      header = bed_pb2.BedHeader(num_fields=3)\n    writer_options = bed_pb2.BedWriterOptions()\n    self._writer = bed_writer.BedWriter.to_file(output_path, header,\n                                                writer_options)\n\n  def write(self, proto):\n    self._writer.write(proto)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass BedWriter(genomics_writer.DispatchingGenomicsWriter):\n  """"""Class for writing BedRecord protos to BED or TFRecord files.""""""\n\n  def _native_writer(self, output_path, header):\n    return NativeBedWriter(output_path, header=header)\n'"
third_party/nucleus/io/bed_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for third_party.nucleus.io.bed.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import bed\nfrom third_party.nucleus.protos import bed_pb2\nfrom third_party.nucleus.testing import test_utils\n\n_VALID_NUM_BED_FIELDS = [3, 4, 5, 6, 8, 9, 12]\n\n\nclass BedReaderTests(parameterized.TestCase):\n\n  @parameterized.parameters(\'test_regions.bed\', \'test_regions.bed.gz\',\n                            \'test_regions.bed.tfrecord\',\n                            \'test_regions.bed.tfrecord.gz\')\n  def test_iterate_bed_reader(self, bed_filename):\n    bed_path = test_utils.genomics_core_testdata(bed_filename)\n    expected = [(\'chr1\', 10, 20), (\'chr1\', 100, 200)]\n    with bed.BedReader(bed_path) as reader:\n      records = list(reader.iterate())\n    self.assertLen(records, 2)\n    self.assertEqual([(r.reference_name, r.start, r.end) for r in records],\n                     expected)\n\n  @parameterized.parameters(\'test_regions.bed\', \'test_regions.bed.gz\')\n  def test_native_bed_header(self, bed_filename):\n    bed_path = test_utils.genomics_core_testdata(bed_filename)\n    with bed.BedReader(bed_path) as reader:\n      self.assertEqual(reader.header.num_fields, 12)\n    with bed.NativeBedReader(bed_path) as native_reader:\n      self.assertEqual(native_reader.header.num_fields, 12)\n\n  @parameterized.parameters(1, 2, 7, 10, 11, 13)\n  def test_invalid_num_fields(self, invalid_num_fields):\n    bed_path = test_utils.genomics_core_testdata(\'test_regions.bed\')\n    with self.assertRaisesRegexp(ValueError, \'Invalid requested number of fie\'):\n      _ = bed.BedReader(bed_path, num_fields=invalid_num_fields)\n\n\nclass BedWriterTests(parameterized.TestCase):\n  """"""Tests for BedWriter.""""""\n\n  def setUp(self):\n    self.records = [\n        bed_pb2.BedRecord(\n            reference_name=\'chr1\', start=30, end=40, name=\'first\', score=55.5),\n        bed_pb2.BedRecord(\n            reference_name=\'chr2\', start=32, end=38, name=\'second\', score=0),\n        bed_pb2.BedRecord(\n            reference_name=\'chr3\', start=40, end=50, name=\'third\', score=99),\n    ]\n    self.tokens = [\n        [\n            \'chr1\', \'30\', \'40\', \'first\', \'55.5\', \'+\', \'35\', \'38\', \'128,242,16\',\n            \'2\', \'5,3\', \'30,37\'\n        ],\n        [\n            \'chr2\', \'32\', \'38\', \'second\', \'0\', \'.\', \'32\', \'38\', \'128,128,128\',\n            \'1\', \'6\', \'32\'\n        ],\n        [\n            \'chr3\', \'40\', \'50\', \'third\', \'99\', \'-\', \'40\', \'44\', \'0,0,0\', \'3\',\n            \'40,43,48\', \'3,2,2\'\n        ],\n    ]\n\n  @parameterized.parameters(\'test_raw.bed\', \'test_zipped.bed.gz\',\n                            \'test_raw.tfrecord\', \'test_zipped.tfrecord.gz\')\n  def test_roundtrip_writer(self, filename):\n    output_path = test_utils.test_tmpfile(filename)\n    with bed.BedWriter(\n        output_path, header=bed_pb2.BedHeader(num_fields=5)) as writer:\n      for record in self.records:\n        writer.write(record)\n\n    with bed.BedReader(output_path) as reader:\n      v2_records = list(reader.iterate())\n\n    self.assertEqual(self.records, v2_records)\n\n  @parameterized.parameters(3, 4, 5, 6, 8, 9, 12)\n  def test_roundtrip_num_fields(self, num_fields):\n    all_num_fields_in_file = [\n        n for n in _VALID_NUM_BED_FIELDS if n >= num_fields\n    ]\n    for num_fields_in_file in all_num_fields_in_file:\n      lines = [\'\\t\'.join(line[:num_fields_in_file]) for line in self.tokens]\n      contents = \'{}\\n\'.format(\'\\n\'.join(lines))\n      input_path = test_utils.test_tmpfile(\'test_field.bed\', contents=contents)\n\n      with bed.BedReader(input_path, num_fields=num_fields) as reader:\n        records = list(reader.iterate())\n      output_path = test_utils.test_tmpfile(\'test_field2.bed\')\n      with bed.BedWriter(output_path, header=reader.header) as writer:\n        for record in records:\n          writer.write(record)\n\n      with bed.BedReader(output_path) as reader2:\n        v2_records = list(reader2.iterate())\n\n      self.assertLen(records, 3)\n      self.assertEqual(records, v2_records)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/bedgraph.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Classes for reading and writing BedGraph files.\n\nThe BedGraph format is described at\nhttps://genome.ucsc.edu/goldenpath/help/bedgraph.html\n\nAPI for reading:\n\n```python\nfrom third_party.nucleus.io import bedgraph\n\n# Iterate through all records.\nwith bed.BedGraphReader(input_path) as reader:\n  for record in reader:\n    print(record)\n```\n\nwhere `record` is a `nucleus.genomics.v1.BedGraphRecord` protocol buffer.\n\nAPI for writing:\n\n```python\nfrom third_party.nucleus.io import bedgraph\nfrom third_party.nucleus.protos import bedgraph_pb2\n\n# records is an iterable of nucleus.genomics.v1.BedGraphRecord protocol buffers.\nrecords = ...\n\n# Write all records to the desired output path.\nwith bed.BedGraphWriter(output_path) as writer:\n  for record in records:\n    writer.write(record)\n```\n\nFor both reading and writing, if the path provided to the constructor contains\n\'.tfrecord\' as an extension, a `TFRecord` file is assumed and attempted to be\nread or written. Otherwise, the filename is treated as a true BedGraph file.\n\nFiles that end in a \'.gz\' suffix cause the file to be treated as compressed\n(with BGZF if it is a BedGraph file, and with gzip if it is a TFRecord file).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom third_party.nucleus.io import genomics_reader\nfrom third_party.nucleus.io import genomics_writer\nfrom third_party.nucleus.io.python import bedgraph_reader\nfrom third_party.nucleus.io.python import bedgraph_writer\nfrom third_party.nucleus.protos import bedgraph_pb2\n\n\nclass NativeBedGraphReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from native BedGraph files.\n\n  Most users will want to use BedGraphReader instead, because it dynamically\n  dispatches between reading native BedGraph files and TFRecord files based on\n  the filename\'s extension.\n  """"""\n\n  def __init__(self, input_path, num_fields=0):\n    """"""Initializes a NativeBedGraphReader.\n\n    Args:\n      input_path: string. A path to a resource containing BedGraph records.\n      num_fields: int. The number of fields to read in the BedGraph. If unset or\n        set to zero, all fields in the input are read.\n    """"""\n    super(NativeBedGraphReader, self).__init__()\n\n    bedgraph_path = input_path.encode(\'utf8\')\n    self._reader = bedgraph_reader.BedGraphReader.from_file(bedgraph_path)\n\n  def query(self):\n    """"""Returns an iterator for going through the records in the region.\n\n    NOTE: This function is not currently implemented by NativeBedGraphReader\n    though it could be implemented for sorted, tabix-indexed BedGraph files.\n    """"""\n    raise NotImplementedError(\'Can not currently query a BedGraph file\')\n\n  def iterate(self):\n    """"""Returns an iterable of BedGraphRecord protos in the file.""""""\n    return self._reader.iterate()\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass BedGraphReader(genomics_reader.DispatchingGenomicsReader):\n  """"""Class for reading BedGraphRecord protos from BedGraph or TFRecord files.""""""\n\n  def _native_reader(self, input_path, **kwargs):\n    return NativeBedGraphReader(input_path, **kwargs)\n\n  def _record_proto(self):\n    return bedgraph_pb2.BedGraphRecord\n\n\nclass NativeBedGraphWriter(genomics_writer.GenomicsWriter):\n  """"""Class for writing to native BedGraph files.\n\n  Most users will want BedGraphWriter, which will write to either native\n  BedGraph files or TFRecord files, based on the output filename\'s extension.\n  """"""\n\n  def __init__(self, output_path, header=None):\n    """"""Initializer for NativeBedGraphWriter.\n\n    Args:\n      output_path: str. The path to which to write the BedGraph file.\n    """"""\n    super(NativeBedGraphWriter, self).__init__()\n    self._writer = bedgraph_writer.BedGraphWriter.to_file(output_path)\n\n  def write(self, proto):\n    self._writer.write(proto)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass BedGraphWriter(genomics_writer.DispatchingGenomicsWriter):\n  """"""Class for writing BedGraphRecord protos to BedGraph or TFRecord files.""""""\n\n  def _native_writer(self, output_path):\n    return NativeBedGraphWriter(output_path)\n'"
third_party/nucleus/io/bedgraph_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for third_party.nucleus.io.bedgraph.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import bedgraph\nfrom third_party.nucleus.protos import bedgraph_pb2\nfrom third_party.nucleus.testing import test_utils\n\n\nclass BedGraphTests(parameterized.TestCase):\n\n  @parameterized.parameters(\'test_regions.bedgraph\', \'test_regions.bedgraph.gz\')\n  def test_iterate_bedgraph_reader(self, bedgraph_path):\n    bedgraph_path = test_utils.genomics_core_testdata(bedgraph_path)\n    expected = [(\'chr1\', 10, 20, 100), (\'chr1\', 100, 200, 250),\n                (\'chr1\', 300, 400, 150.1), (\'chr1\', 500, 501, 20.13)]\n    with bedgraph.BedGraphReader(bedgraph_path) as reader:\n      records = list(reader.iterate())\n    self.assertLen(records, 4)\n    self.assertEqual(\n        [(r.reference_name, r.start, r.end, r.data_value) for r in records],\n        expected)\n\n  @parameterized.parameters(\'test_regions.bedgraph\', \'test_regions.bedgraph.gz\')\n  def test_roundtrip_writer(self, bedgraph_path):\n    output_path = test_utils.test_tmpfile(bedgraph_path)\n    input_path = test_utils.genomics_core_testdata(bedgraph_path)\n    records = []\n    with bedgraph.BedGraphReader(input_path) as reader:\n      records = list(reader.iterate())\n\n    with bedgraph.BedGraphWriter(output_path) as writer:\n      for record in records:\n        writer.write(record)\n\n    with bedgraph.BedGraphReader(output_path) as reader:\n      v2_records = list(reader.iterate())\n\n    self.assertLen(records, 4)\n    self.assertEqual(records, v2_records)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/clif_postproc.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""CLIF postprocessors.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport six\n\nfrom third_party.nucleus.protos import bed_pb2\nfrom third_party.nucleus.protos import bedgraph_pb2\nfrom third_party.nucleus.protos import fastq_pb2\nfrom third_party.nucleus.protos import gff_pb2\nfrom third_party.nucleus.protos import reads_pb2\nfrom third_party.nucleus.protos import variants_pb2\n\n\ndef ValueErrorOnFalse(ok, *args):\n  """"""Returns None / arg / (args,...) if ok.""""""\n  if not isinstance(ok, bool):\n    raise TypeError(\'Use ValueErrorOnFalse only on bool return value\')\n  if not ok:\n    raise ValueError(\'CLIF wrapped call returned False\')\n  # Plain return args will turn 1 into (1,)  and None into () which is unwanted.\n  if args:\n    return args if len(args) > 1 else args[0]\n  return None\n\n\nclass WrappedCppIterable(six.Iterator):\n  """"""This class gives Python iteration semantics on top of a C++ \'Iterable\'.""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, cc_iterable):\n    self._cc_iterable = cc_iterable\n\n  def __enter__(self):\n    self._cc_iterable.__enter__()\n    return self\n\n  def __exit__(self, type_, value, traceback):\n    self._cc_iterable.__exit__(type_, value, traceback)\n\n  def __iter__(self):\n    return self\n\n  @abc.abstractmethod\n  def _raw_next(self):\n    """"""Sub-classes should implement __next__ in this method.""""""\n\n  def __next__(self):\n    try:\n      record, not_done = self._raw_next()\n    except AttributeError:\n      if self._cc_iterable is None:\n        raise ValueError(\'No underlying iterable. This may occur if trying to \'\n                         \'create multiple concurrent iterators from the same \'\n                         \'reader. Try wrapping your call to the iterator in a \'\n                         \'`with` block or materializing the entire iterable \'\n                         \'explicitly.\')\n      else:\n        raise\n    if not_done:\n      return record\n    else:\n      raise StopIteration\n\n\nclass WrappedBedIterable(WrappedCppIterable):\n\n  def _raw_next(self):\n    record = bed_pb2.BedRecord()\n    not_done = self._cc_iterable.PythonNext(record)\n    return record, not_done\n\n\nclass WrappedBedGraphIterable(WrappedCppIterable):\n\n  def _raw_next(self):\n    record = bedgraph_pb2.BedGraphRecord()\n    not_done = self._cc_iterable.PythonNext(record)\n    return record, not_done\n\n\nclass WrappedFastqIterable(WrappedCppIterable):\n\n  def _raw_next(self):\n    record = fastq_pb2.FastqRecord()\n    not_done = self._cc_iterable.PythonNext(record)\n    return record, not_done\n\n\nclass WrappedGffIterable(WrappedCppIterable):\n\n  def _raw_next(self):\n    record = gff_pb2.GffRecord()\n    not_done = self._cc_iterable.PythonNext(record)\n    return record, not_done\n\n\nclass WrappedReferenceIterable(WrappedCppIterable):\n\n  def _raw_next(self):\n    not_done, record = self._cc_iterable.Next()\n    return record, not_done\n\n\nclass WrappedSamIterable(WrappedCppIterable):\n\n  def _raw_next(self):\n    record = reads_pb2.Read()\n    not_done = self._cc_iterable.PythonNext(record)\n    return record, not_done\n\n\nclass WrappedVariantIterable(WrappedCppIterable):\n\n  def _raw_next(self):\n    record = variants_pb2.Variant()\n    not_done = self._cc_iterable.PythonNext(record)\n    return record, not_done\n'"
third_party/nucleus/io/converter.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""A universal converter program for nucleus-supported genomics file formats.\n\nInvoked with a single argument, this program will open a genomics data file and\niterate over its contents, doing no writing.  This is a good benchmark for I/O\nand reader processing speed.\n\nInvoked with two arguments, the program will open the first file, read its\nrecords, and write them, one at a time, to the second file.  The filetypes for\nthe first and second filename must be compatible ways of encoding the same\nnucleus genomics record type (for example, `infile.gff` and\n`outfile.gff.tfrecord.gz` are compatible, but `infile.gff` and `outfile.bam` are\nnot.\n\nNote: at present we have no convention for encoding a file *header* in\ntfrecords, so conversion is not possible from tfrecord to any native file format\nfor which a header is compulsory.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport sys\nimport time\n\nfrom absl import app\nfrom absl import logging\n\nfrom third_party.nucleus.io import bed\nfrom third_party.nucleus.io import fastq\nfrom third_party.nucleus.io import gff\nfrom third_party.nucleus.io import sam\nfrom third_party.nucleus.io import vcf\n\n\ndef _is_native_file(filename):\n  """"""Returns true if filename is a native (non-tfrecord) genomics data file.""""""\n  return not re.match(r"".*\\.tfrecord(\\.gz)?"", filename)\n\n\ndef _filename_pattern(ext):\n  """"""Returns an re matching native or tfrecord files of format `ext`.""""""\n  return r"".*\\.{}(\\.tfrecord)?(\\.gz)?"".format(ext)\n\n\n_FileType = collections.namedtuple(\n    ""_FileType"", (""reader_class"", ""writer_class"", ""has_header""))\n\n_FILETYPE_LOOKUP = {\n    _filename_pattern(""bed""):\n        _FileType(bed.BedReader, bed.BedWriter, False),\n    _filename_pattern(""(fastq|fq)""):\n        _FileType(fastq.FastqReader, fastq.FastqWriter, False),\n    _filename_pattern(""gff""):\n        _FileType(gff.GffReader, gff.GffWriter, True),\n    _filename_pattern(""(bam|sam)""):\n        _FileType(sam.SamReader, sam.SamWriter, True),\n    _filename_pattern(""vcf""):\n        _FileType(vcf.VcfReader, vcf.VcfWriter, True),\n}\n\n\ndef _lookup_filetype(filename):\n  for pattern in _FILETYPE_LOOKUP:\n    if re.match(pattern, filename):\n      return _FILETYPE_LOOKUP[pattern]\n  raise ConversionError(""Unrecognized extension!"")\n\nLOG_EVERY = 100000\n\n\nclass ConversionError(Exception):\n  """"""An exception used to signal file conversion error.""""""\n  pass\n\n\nclass NullWriter(object):\n  """"""A writer class whose .write() method is a no-op.\n\n  This allows us to create and use a writer object where one is required by\n  context but we do not wish to write to any file.\n  """"""\n\n  def __init__(self, unused_filename, header=None):\n    pass\n\n  def write(self, unused_record):\n    pass\n\n  def __enter__(self):\n    return self\n\n  def __exit__(self, exc_type, exc_value, traceback):\n    pass\n\n\ndef _reader_writer_classes(in_filename, out_filename):\n  """"""Returns reader, writer classes for filenames, if conversion is possible.\n\n  Args:\n    in_filename: filename of a genomics data file to use as input.\n    out_filename: filename of a genomics data file to use as output, or None,\n      if no output should be written.\n\n  Raises:\n    ConversionError: if in_filename is not convertible to out_filename.\n  """"""\n  in_filetype = _lookup_filetype(in_filename)\n  out_filetype = _lookup_filetype(out_filename) if out_filename else None\n\n  if out_filetype:\n    if in_filetype != out_filetype:\n      raise ConversionError(\n          ""Input and output filetypes specified are incompatible."")\n    input_has_header = in_filetype.has_header and _is_native_file(in_filename)\n    output_requires_header = (\n        out_filetype.has_header and _is_native_file(out_filename))\n    if output_requires_header and not input_has_header:\n      raise ConversionError(\n          ""Input file does not have a header, which is needed to construct ""\n          ""output file"")\n    writer_class = out_filetype.writer_class\n\n  else:\n    writer_class = NullWriter\n\n  return in_filetype.reader_class, writer_class\n\n\ndef convert(in_filename, out_filename):\n  """"""Converts a recognized genomics file `in_filename` to `out_filename`.\n\n  Args:\n    in_filename: str; filename of a genomics data file to use as input.\n    out_filename: str; filename of a genomics data file to use as output, or\n      None, if no output should be written.\n\n  Raises:\n    ConversionError, if the conversion could not be executed.\n  """"""\n  reader_class, writer_class = _reader_writer_classes(in_filename, out_filename)\n  reader = reader_class(in_filename)\n\n  with reader_class(in_filename) as reader:\n    with writer_class(out_filename, header=reader.header) as writer:\n      start = time.time()\n      i = 0\n      for record in reader:\n        i += 1\n        writer.write(record)\n        logging.log_every_n(logging.INFO, ""Progress: %d records"", LOG_EVERY, i)\n      elapsed = time.time() - start\n      logging.info(""Done, processed %d records in %0.2f seconds."", i, elapsed)\n\n\ndef main(argv):\n  if len(argv) not in (2, 3):\n    print(""Usage: %s <input_filename> [<output_filename>]"" % argv[0])\n    sys.exit(1)\n\n  input_filename = argv[1]\n  output_filename = None if len(argv) == 2 else argv[2]\n\n  try:\n    convert(input_filename, output_filename)\n  except ConversionError as e:\n    print(""Could not execute conversion:"", e)\n    sys.exit(1)\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
third_party/nucleus/io/converter_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for third_party.nucleus.examples.convert_genomics_file.\n\nThese tests do NOT establish the correctness of conversions---tests of the\nfidelity of the Reader and Writer classes exist elsewhere in Nucleus.  Rather,\nthese tests simply exercise that the conversion *runs* for each input/output\nfile type.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport os\nimport unittest\n\nfrom absl import logging\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom third_party.nucleus.io import converter\nfrom third_party.nucleus.testing import test_utils\n\nbasename = os.path.basename\n\n# Initial (native) input files we will use to begin conversions.\nORIGINAL_TEST_FILES = [\n    ""test.bed"", ""test_reads.fastq"", ""test_features.gff"", ""test.sam"",\n    ""test_sites.vcf""\n]\n\n# These formats require a header, so conversion from tfrecord to a native file\n# format cannot be done faithfully.\nFORMATS_REQUIRING_HEADER = ["".bam"", "".gff"", "".sam"", "".vcf""]\n\n\nclass ConvertGenomicsFileTest(parameterized.TestCase):\n\n  def _convert(self, src, dest):\n    logging.info(""#### Converting: %s --> %s ... "", basename(src),\n                 basename(dest))\n    converter.convert(src, dest)\n\n  @parameterized.parameters(*ORIGINAL_TEST_FILES)\n  def test_conversion_to_tfrecord_and_back(self, original_input_file):\n    """"""Test conversion from a native file format to tfrecord.gz, then back.""""""\n    input_path = test_utils.genomics_core_testdata(original_input_file)\n    tfrecord_output_path = test_utils.test_tmpfile(original_input_file +\n                                                   "".tfrecord.gz"")\n    native_output_path = test_utils.test_tmpfile(original_input_file)\n\n    # Test conversion from native format to tfrecord.\n    self._convert(input_path, tfrecord_output_path)\n\n    # redacted\n    if native_output_path.endswith("".sam""):\n      raise unittest.SkipTest(""SAM writing not yet supported"")\n\n    # Test conversion from tfrecord format back to native format.  Ensure that\n    # conversions where we would need a header, but don\'t have one from the\n    # input, trigger an error message.\n    if any(\n        native_output_path.endswith(ext) for ext in FORMATS_REQUIRING_HEADER):\n      with self.assertRaisesRegexp(\n          converter.ConversionError,\n          ""Input file does not have a header, which is needed to construct ""\n          ""output file""):\n        self._convert(tfrecord_output_path, native_output_path)\n\n    else:\n      self._convert(tfrecord_output_path, native_output_path)\n\n\nif __name__ == ""__main__"":\n  absltest.main()\n \n'"
third_party/nucleus/io/fasta.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Classes for reading FASTA files.\n\nThe FASTA format is described at\nhttps://en.wikipedia.org/wiki/FASTA_format\n\nAPI for reading:\n\n```python\nfrom third_party.nucleus.io import fasta\nfrom third_party.nucleus.protos import range_pb2\n\nwith fasta.IndexedFastaReader(input_path) as reader:\n  region = range_pb2.Range(reference_name=\'chrM\', start=1, end=6)\n  basepair_string = reader.query(region)\n  print(basepair_string)\n```\n\nIf `input_path` ends with \'.gz\', it is assumed to be compressed.  All FASTA\nfiles are assumed to be indexed with the index file located at\n`input_path + \'.fai\'`.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport six\n\nfrom tensorflow.python.platform import gfile\nfrom third_party.nucleus.io import genomics_reader\nfrom third_party.nucleus.io.python import reference\nfrom third_party.nucleus.protos import fasta_pb2\nfrom third_party.nucleus.protos import reference_pb2\nfrom third_party.nucleus.util import ranges\n\n# redacted\nRefFastaHeader = collections.namedtuple(\n    \'RefFastaHeader\', [\'contigs\'])\n\n\nclass FastaReader(genomics_reader.DispatchingGenomicsReader):\n  """"""Class for reading (name, bases) tuples from FASTA files.""""""\n\n  def _native_reader(self, input_path, **kwargs):\n    fai_path = input_path + \'.fai\'\n    if gfile.Exists(fai_path):\n      return IndexedFastaReader(input_path, **kwargs)\n    return UnindexedFastaReader(input_path, **kwargs)\n\n  def _record_proto(self):\n    return fasta_pb2.FastaRecord\n\n\nclass IndexedFastaReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from FASTA files containing a reference genome.""""""\n\n  def __init__(self, input_path, keep_true_case=False, cache_size=None):\n    """"""Initializes an IndexedFastaReader.\n\n    Args:\n      input_path: string. A path to a resource containing FASTA records.\n      keep_true_case: bool. If False, casts all bases to uppercase before\n        returning them.\n      cache_size: integer. Number of bases to cache from previous queries.\n        Defaults to 64K.  The cache can be disabled using cache_size=0.\n    """"""\n    super(IndexedFastaReader, self).__init__()\n\n    options = fasta_pb2.FastaReaderOptions(keep_true_case=keep_true_case)\n\n    fasta_path = input_path\n    fai_path = fasta_path + \'.fai\'\n    if cache_size is None:\n      # Use the C++-defined default cache size.\n      self._reader = reference.IndexedFastaReader.from_file(\n          fasta_path, fai_path, options)\n    else:\n      self._reader = reference.IndexedFastaReader.from_file(\n          fasta_path, fai_path, options, cache_size)\n\n    # redacted\n    self.header = RefFastaHeader(contigs=self._reader.contigs)\n\n  def iterate(self):\n    """"""Returns an iterable of (name, bases) tuples contained in this file.""""""\n    return self._reader.iterate()\n\n  def query(self, region):\n    """"""Returns the base pairs (as a string) in the given region.""""""\n    return self._reader.bases(region)\n\n  def is_valid(self, region):\n    """"""Returns whether the region is contained in this FASTA file.""""""\n    return self._reader.is_valid_interval(region)\n\n  def contig(self, contig_name):\n    """"""Returns a ContigInfo proto for contig_name.""""""\n    return self._reader.contig(contig_name)\n\n  @property\n  def c_reader(self):\n    """"""Returns the underlying C++ reader.""""""\n    return self._reader\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass UnindexedFastaReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from unindexed FASTA files.""""""\n\n  def __init__(self, input_path):\n    """"""Initializes an UnindexedFastaReader.\n\n    Args:\n      input_path: string. A path to a resource containing FASTA records.\n    """"""\n    super(UnindexedFastaReader, self).__init__()\n\n    self._reader = reference.UnindexedFastaReader.from_file(input_path)\n\n  def iterate(self):\n    """"""Returns an iterable of (name, bases) tuples contained in this file.""""""\n    return self._reader.iterate()\n\n  def query(self, region):\n    """"""Returns the base pairs (as a string) in the given region.""""""\n    raise NotImplementedError(\'Can not query an unindexed FASTA file\')\n\n  def is_valid(self, region):\n    """"""Returns whether the region is contained in this FASTA file.""""""\n    return self._reader.is_valid_interval(region)\n\n  def contig(self, contig_name):\n    """"""Returns a ContigInfo proto for contig_name.""""""\n    raise NotImplementedError(\'Contigs unknown for an unindexed FASTA file\')\n\n  @property\n  def c_reader(self):\n    """"""Returns the underlying C++ reader.""""""\n    return self._reader\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass InMemoryFastaReader(genomics_reader.GenomicsReader):\n  """"""An `IndexedFastaReader` getting its bases from an in-memory data structure.\n\n  An `InMemoryFastaReader` provides the same API as `IndexedFastaReader` but\n  doesn\'t fetch its data from an on-disk FASTA file but rather fetches the bases\n  from an in-memory cache containing (chromosome, start, bases) tuples.\n\n  In particular, the `query(Range(chrom, start, end))` operation fetches bases\n  from the tuple where `chrom` == chromosome, and then from the bases where the\n  first base of bases starts at start. If start > 0, then the bases string is\n  assumed to contain bases starting from that position in the region. For\n  example, the record (\'1\', 10, \'ACGT\') implies that\n  `query(ranges.make_range(\'1\', 11, 12))` will return the base \'C\', as the \'A\'\n  base is at position 10. This makes it straightforward to cache a small region\n  of a full chromosome without having to store the entire chromosome sequence in\n  memory (potentially big!).\n  """"""\n\n  def __init__(self, chromosomes):\n    """"""Initializes an InMemoryFastaReader using data from chromosomes.\n\n    Args:\n      chromosomes: list[tuple]. The chromosomes we are caching in memory as a\n        list of tuples. Each tuple must be exactly three elements in length,\n        containing (chromosome name [str], start [int], bases [str]).\n\n    Raises:\n      ValueError: If any of the chromosomes tuples are invalid.\n    """"""\n    super(InMemoryFastaReader, self).__init__()\n\n    ref_seqs = []\n    contigs = []\n    for i, (contig_name, start, bases) in enumerate(chromosomes):\n      if start < 0:\n        raise ValueError(\'start={} must be >= for chromosome={}\'.format(\n            start, contig_name))\n      if not bases:\n        raise ValueError(\n            \'Bases must contain at least one base, but got ""{}""\'.format(bases))\n\n      end = start + len(bases)\n      ref_seqs.append(reference_pb2.ReferenceSequence(\n          region=ranges.make_range(contig_name, start, end), bases=bases))\n      contigs.append(\n          reference_pb2.ContigInfo(\n              name=contig_name, n_bases=end, pos_in_fasta=i))\n\n    self._reader = reference.InMemoryFastaReader.create(contigs, ref_seqs)\n    self.header = RefFastaHeader(contigs=self._reader.contigs)\n\n  def iterate(self):\n    """"""Returns an iterable of (name, bases) tuples contained in this file.""""""\n    return self._reader.iterate()\n\n  def query(self, region):\n    """"""Returns the base pairs (as a string) in the given region.""""""\n    return self._reader.bases(region)\n\n  def is_valid(self, region):\n    """"""Returns whether the region is contained in this FASTA file.""""""\n    return self._reader.is_valid_interval(region)\n\n  def contig(self, contig_name):\n    """"""Returns a ContigInfo proto for contig_name.""""""\n    return self._reader.contig(contig_name)\n\n  @property\n  def c_reader(self):\n    """"""Returns the underlying C++ reader.""""""\n    return self._reader\n\n  def __str__(self):\n\n    def _format_refseq(refseq):\n      bases = refseq.bases\n      if len(bases) >= 50:\n        bases = bases[0:50] + \'...\'\n      return \'Contig(chrom={} start={}, end={}, bases={})\'.format(\n          refseq.region.reference_name, refseq.region.start, refseq.region.end,\n          bases)\n\n    contigs_strs = [\n        _format_refseq(refseq)\n        for refseq in six.itervalues(self._reader.reference_sequences)\n    ]\n    return \'InMemoryFastaReader(contigs={})\'.format(\'\'.join(contigs_strs))\n\n  __repr__ = __str__\n'"
third_party/nucleus/io/fasta_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for third_party.nucleus.io.fasta.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nimport six\nfrom third_party.nucleus.io import fasta\nfrom third_party.nucleus.io.python import reference\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\n\n\nclass FastaReaderTests(parameterized.TestCase):\n\n  def test_dispatching_reader(self):\n    with fasta.FastaReader(\n        test_utils.genomics_core_testdata(\'test.fasta\')) as reader:\n      # The reader is an instance of IndexedFastaReader which supports query().\n      self.assertEqual(reader.query(ranges.make_range(\'chrM\', 1, 6)), \'ATCAC\')\n    with fasta.FastaReader(\n        test_utils.genomics_core_testdata(\'unindexed.fasta\')) as reader:\n      # The reader is an instance of UnindexedFastaReader which doesn\'t support\n      # query().\n      with self.assertRaises(NotImplementedError):\n        reader.query(ranges.make_range(\'chrM\', 1, 5))\n\n\nclass IndexedFastaReaderTests(parameterized.TestCase):\n\n  @parameterized.parameters(\'test.fasta\', \'test.fasta.gz\')\n  def test_make_ref_reader_default(self, fasta_filename):\n    fasta_path = test_utils.genomics_core_testdata(fasta_filename)\n    with fasta.IndexedFastaReader(fasta_path) as reader:\n      self.assertEqual(reader.query(ranges.make_range(\'chrM\', 1, 6)), \'ATCAC\')\n\n  @parameterized.parameters(\'test.fasta\', \'test.fasta.gz\')\n  def test_make_ref_reader_with_true_case(self, fasta_filename):\n    fasta_path = test_utils.genomics_core_testdata(fasta_filename)\n    with fasta.IndexedFastaReader(fasta_path, keep_true_case=True) as reader:\n      self.assertEqual(reader.query(ranges.make_range(\'chrM\', 22, 27)), \'TaaCC\')\n\n  @parameterized.parameters(\'test.fasta\', \'test.fasta.gz\')\n  def test_make_ref_reader_cache_specified(self, fasta_filename):\n    fasta_path = test_utils.genomics_core_testdata(fasta_filename)\n    with fasta.IndexedFastaReader(fasta_path, cache_size=10) as reader:\n      self.assertEqual(reader.query(ranges.make_range(\'chrM\', 1, 5)), \'ATCA\')\n\n  def test_c_reader(self):\n    with fasta.IndexedFastaReader(\n        test_utils.genomics_core_testdata(\'test.fasta\')) as reader:\n      self.assertIsInstance(reader.c_reader,\n                            reference.IndexedFastaReader)\n\n\nclass UnindexedFastaReaderTests(parameterized.TestCase):\n\n  def test_query(self):\n    unindexed_fasta_reader = fasta.UnindexedFastaReader(\n        test_utils.genomics_core_testdata(\'unindexed.fasta\'))\n    with self.assertRaises(NotImplementedError):\n      unindexed_fasta_reader.query(ranges.make_range(\'chrM\', 1, 5))\n\n  @parameterized.parameters(\'test.fasta\', \'test.fasta.gz\')\n  def test_iterate(self, fasta_filename):\n    # Check the indexed fasta file\'s iterable matches that of the unindexed\n    # fasta file.\n    indexed_fasta_reader = fasta.IndexedFastaReader(\n        test_utils.genomics_core_testdata(fasta_filename))\n    unindexed_fasta_reader = fasta.UnindexedFastaReader(\n        test_utils.genomics_core_testdata(fasta_filename))\n    self.assertEqual(\n        list(indexed_fasta_reader.iterate()),\n        list(unindexed_fasta_reader.iterate()))\n\n\nclass InMemoryFastaReaderTests(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    cls.fasta_reader = fasta.IndexedFastaReader(\n        test_utils.genomics_core_testdata(\'test.fasta\'))\n\n    cls.in_mem = fasta.InMemoryFastaReader(\n        [(contig.name, 0,\n          cls.fasta_reader.query(\n              ranges.make_range(contig.name, 0, contig.n_bases)))\n         for contig in cls.fasta_reader.header.contigs])\n\n  def test_non_zero_start_query(self):\n    bases = \'ACGTAACCGGTT\'\n    for start in range(len(bases)):\n      reader = fasta.InMemoryFastaReader([(\'1\', start, bases[start:])])\n      self.assertEqual(reader.header.contigs[0].name, \'1\')\n      self.assertEqual(reader.header.contigs[0].n_bases, len(bases))\n\n      # Check that our query operation works as expected with a start position.\n      for end in range(start, len(bases)):\n        self.assertEqual(reader.query(ranges.make_range(\'1\', start, end)),\n                         bases[start:end])\n\n  @parameterized.parameters(\n      # Start is 10, so this raises because it\'s before the bases starts.\n      dict(start=0, end=1),\n      # Spans into the start of the bases; make sure it detects it\'s bad.\n      dict(start=8, end=12),\n      # Spans off the end of the bases.\n      dict(start=12, end=15),\n  )\n  def test_bad_query_with_start(self, start, end):\n    reader = fasta.InMemoryFastaReader([(\'1\', 10, \'ACGT\')])\n    with self.assertRaises(ValueError):\n      reader.query(ranges.make_range(\'1\', start, end))\n\n  def test_query_edge_cases(self):\n    reader = fasta.InMemoryFastaReader([(\'1\', 0, \'ACGT\')])\n    # Check that we can query the first base correctly.\n    self.assertEqual(reader.query(ranges.make_range(\'1\', 0, 1)), \'A\')\n    # Check that we can query the last base correctly.\n    self.assertEqual(reader.query(ranges.make_range(\'1\', 3, 4)), \'T\')\n    # Check that we can query the entire sequence correctly.\n    self.assertEqual(reader.query(ranges.make_range(\'1\', 0, 4)), \'ACGT\')\n\n  def test_contigs(self):\n    # Our contigs can have a different order, descriptions are dropped, etc so\n    # we need to check specific fields by hand.\n    fasta_contigs = {\n        contig.name: contig for contig in self.fasta_reader.header.contigs\n    }\n    mem_contigs = {contig.name: contig for contig in self.in_mem.header.contigs}\n\n    self.assertEqual(fasta_contigs.keys(), mem_contigs.keys())\n    for name, fasta_contig in fasta_contigs.items():\n      self.assertContigsAreEqual(mem_contigs[name], fasta_contig)\n\n  def assertContigsAreEqual(self, actual, expected):\n    self.assertEqual(actual.name, expected.name)\n    self.assertEqual(actual.n_bases, expected.n_bases)\n    self.assertEqual(actual.pos_in_fasta, expected.pos_in_fasta)\n\n  def test_iterate(self):\n    # Check the in-memory fasta file\'s iterable matches the info in the header.\n    expected_names = [\n        contig.name for contig in self.fasta_reader.header.contigs]\n    expected_lengths = [\n        contig.n_bases for contig in self.fasta_reader.header.contigs]\n    in_mem_records = list(self.in_mem.iterate())\n    self.assertLen(in_mem_records, len(self.fasta_reader.header.contigs))\n    self.assertEqual([r[0] for r in in_mem_records], expected_names)\n    self.assertEqual([len(r[1]) for r in in_mem_records], expected_lengths)\n\n    # Check the in-memory fasta file\'s iterable matches that of the indexed\n    # fasta file.\n    fasta_records = list(self.fasta_reader.iterate())\n    self.assertEqual(in_mem_records, fasta_records)\n\n  @parameterized.parameters(\n      dict(region=ranges.make_range(\'chr1\', 0, 10), expected=True),\n      dict(region=ranges.make_range(\'chr1\', 10, 50), expected=True),\n      dict(region=ranges.make_range(\'chr1\', 10, 500), expected=False),\n      dict(region=ranges.make_range(\'chr3\', 10, 20), expected=False),\n  )\n  def test_is_valid(self, region, expected):\n    self.assertEqual(self.in_mem.is_valid(region), expected)\n    self.assertEqual(self.fasta_reader.is_valid(region), expected)\n\n  def test_known_contig(self):\n    for contig in self.fasta_reader.header.contigs:\n      self.assertContigsAreEqual(self.in_mem.contig(contig.name), contig)\n\n  def test_str_and_repr(self):\n    self.assertIsInstance(str(self.in_mem), six.string_types)\n    self.assertIsInstance(repr(self.in_mem), six.string_types)\n\n  def test_unknown_contig(self):\n    for reader in [self.fasta_reader, self.in_mem]:\n      with self.assertRaises(ValueError):\n        reader.contig(\'unknown\')\n\n  def test_good_query(self):\n    for contig in self.fasta_reader.header.contigs:\n      for start in range(contig.n_bases):\n        for end in range(start, contig.n_bases):\n          region = ranges.make_range(contig.name, start, end)\n          self.assertEqual(\n              self.in_mem.query(region), self.fasta_reader.query(region))\n\n  @parameterized.parameters(\n      ranges.make_range(\'chr1\', -1, 10),     # bad start.\n      ranges.make_range(\'chr1\', 10, 1),      # end < start.\n      ranges.make_range(\'chr1\', 0, 1000),    # off end of chromosome.\n      ranges.make_range(\'unknown\', 0, 10),   # unknown chromosome.\n  )\n  def test_bad_query(self, region):\n    for reader in [self.fasta_reader, self.in_mem]:\n      with self.assertRaises(ValueError):\n        reader.query(region)\n\n  def test_bad_create_args(self):\n    with self.assertRaisesRegexp(ValueError, \'multiple ones were found on 1\'):\n      fasta.InMemoryFastaReader([\n          (\'1\', 10, \'AC\'),\n          (\'1\', 20, \'AC\'),\n      ])\n\n  def test_c_reader(self):\n    self.assertIsInstance(self.in_mem.c_reader,\n                          reference.InMemoryFastaReader)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/fastq.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Classes for reading and writing FASTQ files.\n\nThe FASTQ format is described at\nhttps://en.wikipedia.org/wiki/FASTQ_format\n\nAPI for reading:\n\n```python\nfrom third_party.nucleus.io import fastq\n\nwith fastq.FastqReader(input_path) as reader:\n  for record in reader:\n    print(record)\n```\n\nwhere `record` is a `nucleus.genomics.v1.FastqRecord` protocol buffer.\n\nAPI for writing:\n\n```python\nfrom third_party.nucleus.io import fastq\n\n# records is an iterable of nucleus.genomics.v1.FastqRecord protocol buffers.\nrecords = ...\n\nwith fastq.FastqWriter(output_path) as writer:\n  for record in records:\n    writer.write(record)\n```\n\nFor both reading and writing, if the path provided to the constructor contains\n\'.tfrecord\' as an extension, a `TFRecord` file is assumed and attempted to be\nread or written. Otherwise, the filename is treated as a true FASTQ file.\n\nFiles that end in a \'.gz\' suffix cause the file to be treated as compressed\n(with BGZF if it is a true FASTQ file, and with gzip if it is a TFRecord file).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom third_party.nucleus.io import genomics_reader\nfrom third_party.nucleus.io import genomics_writer\nfrom third_party.nucleus.io.python import fastq_reader\nfrom third_party.nucleus.io.python import fastq_writer\nfrom third_party.nucleus.protos import fastq_pb2\n\n\nclass NativeFastqReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from native FASTQ files.\n\n  Most users will want to use FastqReader instead, because it dynamically\n  dispatches between reading native FASTQ files and TFRecord files based on the\n  filename\'s extension.\n  """"""\n\n  def __init__(self, input_path):\n    """"""Initializes a NativeFastqReader.\n\n    Args:\n      input_path: str. A path to a resource containing FASTQ records.\n    """"""\n    super(NativeFastqReader, self).__init__()\n\n    fastq_path = input_path.encode(\'utf8\')\n    options = fastq_pb2.FastqReaderOptions()\n    self._reader = fastq_reader.FastqReader.from_file(fastq_path, options)\n    self.header = None\n\n  def query(self, region):\n    """"""Returns an iterator for going through the records in the region.\n\n    NOTE: This function is not implemented by NativeFastqReader as there is no\n    concept of genome ordering in the FASTQ format.\n    """"""\n    raise NotImplementedError(\'Can not query a FASTQ file\')\n\n  def iterate(self):\n    """"""Returns an iterable of FastqRecord protos in the file.""""""\n    return self._reader.iterate()\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass FastqReader(genomics_reader.DispatchingGenomicsReader):\n  """"""Class for reading FastqRecord protos from FASTQ or TFRecord files.""""""\n\n  def _native_reader(self, input_path, **kwargs):\n    return NativeFastqReader(input_path, **kwargs)\n\n  def _record_proto(self):\n    return fastq_pb2.FastqRecord\n\n\nclass NativeFastqWriter(genomics_writer.GenomicsWriter):\n  """"""Class for writing to native FASTQ files.\n\n  Most users will want FastqWriter, which will write to either native FASTQ\n  files or TFRecord files, based on the output filename\'s extension.\n  """"""\n\n  def __init__(self, output_path, **kwargs):\n    """"""Initializer for NativeFastqWriter.\n\n    Args:\n      output_path: str. The path to which to write the FASTQ file.\n      **kwargs: optional arguments; presently ignored.\n    """"""\n    super(NativeFastqWriter, self).__init__()\n\n    writer_options = fastq_pb2.FastqWriterOptions()\n    self._writer = fastq_writer.FastqWriter.to_file(output_path, writer_options)\n\n  def write(self, proto):\n    self._writer.write(proto)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass FastqWriter(genomics_writer.DispatchingGenomicsWriter):\n  """"""Class for writing FastqRecord protos to FASTQ or TFRecord files.""""""\n\n  def _native_writer(self, output_path, **kwargs):\n    return NativeFastqWriter(output_path, **kwargs)\n'"
third_party/nucleus/io/fastq_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for third_party.nucleus.io.fastq.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import fastq\nfrom third_party.nucleus.protos import fastq_pb2\nfrom third_party.nucleus.testing import test_utils\n\n\nclass FastqReaderTests(parameterized.TestCase):\n\n  @parameterized.parameters(\n      \'test_reads.fastq\', \'test_reads.fastq.gz\', \'test_reads.bgzip.fastq.gz\',\n      \'test_reads.fastq.tfrecord\', \'test_reads.fastq.tfrecord.gz\')\n  def test_iterate_fastq_reader(self, fastq_filename):\n    fastq_path = test_utils.genomics_core_testdata(fastq_filename)\n    expected_ids = [\n        \'NODESC:header\', \'M01321:49:000000000-A6HWP:1:1101:17009:2216\', \'FASTQ\',\n        \'FASTQ_with_trailing_space\'\n    ]\n    with fastq.FastqReader(fastq_path) as reader:\n      records = list(reader.iterate())\n    self.assertLen(records, 4)\n    self.assertEqual([r.id for r in records], expected_ids)\n\n\nclass FastqWriterTests(parameterized.TestCase):\n  """"""Tests for FastqWriter.""""""\n\n  def setUp(self):\n    self.records = [\n        fastq_pb2.FastqRecord(id=\'id1\', sequence=\'ACGTG\', quality=\'ABCDE\'),\n        fastq_pb2.FastqRecord(id=\'id2\', sequence=\'ATTT\', quality=\'ABC@\'),\n        fastq_pb2.FastqRecord(\n            id=\'ID3\',\n            description=\'multi desc\',\n            sequence=\'GATAC\',\n            quality=\'ABC@!\'),\n    ]\n\n  @parameterized.parameters(\'test_raw.fastq\', \'test_zipped.fastq.gz\',\n                            \'test_raw.tfrecord\', \'test_zipped.tfrecord.gz\')\n  def test_roundtrip_writer(self, filename):\n    output_path = test_utils.test_tmpfile(filename)\n    with fastq.FastqWriter(output_path) as writer:\n      for record in self.records:\n        writer.write(record)\n\n    with fastq.FastqReader(output_path) as reader:\n      v2_records = list(reader.iterate())\n\n    self.assertEqual(self.records, v2_records)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/genomics_io_noplugin_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for genomics_io\'s plugin system.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\n\nfrom third_party.nucleus.io import sam\n\n\nclass PluginAbsenceTest(absltest.TestCase):\n  """"""Test that we get the right error when the plugin cannot load.""""""\n\n  def test_tfbam_plugin_does_not_load(self):\n    with self.assertRaisesRegexp(\n        ImportError, \'tfbam_lib module not found, cannot read .tfbam files.\'):\n      _ = sam.SamReader(\'mouse@25.tfbam\')\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/genomics_reader.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Classes that provide the interface for reading genomics data.\n\n`GenomicsReader` defines the core API supported by readers, and is subclassed\ndirectly or indirectly (via `DispatchingGenomicsReader`) for all concrete\nimplementations.\n\n`TFRecordReader` is an implementation of the `GenomicsReader` API for reading\n`TFRecord` files. This is usable for all data types when encoding data in\nprotocol buffers.\n\n`DispatchingGenomicsReader` is an abstract class defined for convenience on top\nof `GenomicsReader` that supports reading from either the native file format or\nfrom `TFRecord` files of the corresponding protocol buffer used to encode data\nof that file type. The input format assumed is dependent upon the filename of\nthe input data.\n\nConcrete implementations for individual file types (e.g. BED, SAM, VCF, etc.)\nreside in type-specific modules in this package. The instantiation of readers\nmay have reader-specific requirements documented there. General examples of the\n`iterate()` and `query()` functionality are shown below.\n\n```python\n# Equivalent ways to iterate through all elements in a reader.\n# 1. Using the reader itself as an iterable object.\nkwargs = ...  # Reader-specific keyword arguments.\nwith GenomicsReaderSubClass(output_path, **kwargs) as reader:\n  for proto in reader:\n    do_something(reader.header, proto)\n\n# 2. Calling the iterate() method of the reader explicitly.\nwith GenomicsReaderSubClass(output_path, **kwargs) as reader:\n  for proto in reader.iterate():\n    do_something(reader.header, proto)\n\n# Querying for all elements within a specific region of the genome.\nfrom third_party.nucleus.protos import range_pb2\nregion = range_pb2.Range(reference_name=\'chr1\', start=10, end=20)\n\nwith GenomicsReaderSubClass(output_path, **kwargs) as reader:\n  for proto in reader.query(region):\n    do_something(reader.header, proto)\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport errno\n\nfrom absl import logging\nimport six\n\nfrom third_party.nucleus.io.python import tfrecord_reader\n\n\nclass GenomicsReader(six.Iterator):\n  """"""Abstract base class for reading genomics data.\n\n  In addition to the abstractmethods defined below, subclasses should\n  also set a `header` member variable in their objects.\n  """"""\n\n  __metaclass__ = abc.ABCMeta\n\n  @abc.abstractmethod\n  def iterate(self):\n    """"""Returns an iterator for going through all the file\'s records.""""""\n\n  @abc.abstractmethod\n  def query(self, region):\n    """"""Returns an iterator for going through the records in the region.\n\n    Args:\n      region:  A nucleus.genomics.v1.Range.\n\n    Returns:\n      An iterator containing all and only records within the specified region.\n    """"""\n\n  def __enter__(self):\n    """"""Enter a `with` block.""""""\n    return self\n\n  def __exit__(self, unused_type, unused_value, unused_traceback):\n    """"""Exit a `with` block.  Typically, this will close the file.""""""\n\n  def __init__(self):\n    """"""Initializer.""""""\n    # Some readers can only support one iterator at a time, so don\'t\n    # create one now.  Rather, create it when needed in next().\n    self.iterator = None\n\n  def __iter__(self):\n    """"""Allows users to use the object itself as an iterator.""""""\n    return self.iterate()\n\n  def __next__(self):\n    """"""Allows users to use the object itself as an iterator.""""""\n    if self.iterator is None:\n      self.iterator = self.iterate()\n    return six.next(self.iterator)\n\n\nclass TFRecordReader(GenomicsReader):\n  """"""A GenomicsReader that reads protocol buffers from a TFRecord file.\n\n  Example usage:\n    reader = TFRecordReader(\'/tmp/my_file.tfrecords.gz\',\n                            proto=tensorflow.Example)\n    for example in reader:\n      process(example)\n\n  Note that TFRecord files do not have headers, and do not need\n  to be wrapped in a ""with"" block.\n  """"""\n\n  def __init__(self, input_path, proto, compression_type=None):\n    """"""Initializes the TFRecordReader.\n\n    Args:\n      input_path:  The filename of the file to read.\n      proto:  The protocol buffer type the TFRecord file is expected to\n        contain.  For example, variants_pb2.Variant or reads_pb2.Read.\n      compression_type:  Either \'ZLIB\', \'GZIP\', \'\' (uncompressed), or\n        None.  If None, __init__ will guess the compression type based on\n        the input_path\'s suffix.\n\n    Raises:\n      IOError: if there was any problem opening input_path for reading.\n    """"""\n    super(TFRecordReader, self).__init__()\n\n    self.input_path = input_path\n    self.proto = proto\n    self.header = None\n\n    if compression_type is None:\n      compression_type = \'GZIP\' if input_path.endswith(\'.gz\') else \'\'\n\n    self.reader = tfrecord_reader.TFRecordReader.from_file(\n        input_path, compression_type)\n    if self.reader is None:\n      raise IOError(errno.EIO,\n                    \'Error trying to open %s for reading\' % input_path)\n\n  def iterate(self):\n    """"""Returns an iterator for going through all the file\'s records.""""""\n    while self.reader.get_next():\n      yield self.proto.FromString(self.reader.get_record())\n\n  def query(self, region):\n    """"""Returns an iterator for going through the records in the region.\n\n    NOTE: This function is not currently implemented by TFRecordReader as the\n    TFRecord format does not provide a general mechanism for fast random access\n    to elements in genome order.\n    """"""\n    raise NotImplementedError(\'Can not query TFRecord file\')\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self.reader.close()\n\n  @property\n  def c_reader(self):\n    """"""Returns the underlying C++ reader.""""""\n    return self.reader\n\n\nclass DispatchingGenomicsReader(GenomicsReader):\n  """"""A GenomicsReader that dispatches based on the file extension.\n\n  If \'.tfrecord\' is present in the filename, a TFRecordReader is used.\n  Otherwise, a native reader is.\n\n  Subclasses of DispatchingGenomicsReader must define the following methods:\n    * _native_reader()\n    * _record_proto()\n  """"""\n\n  def __init__(self, input_path, **kwargs):\n    super(DispatchingGenomicsReader, self).__init__()\n\n    if \'.tfrecord\' in input_path:\n      self._reader = TFRecordReader(\n          input_path, proto=self._record_proto(),\n          compression_type=kwargs.get(\'compression_type\', None))\n    else:\n      # Remove compression_type, if present, from the arguments we pass to the\n      # native reader.\n      kwargs.pop(\'compression_type\', None)\n      self._reader = self._native_reader(input_path, **kwargs)\n    logging.info(\'Reading %s with %s\',\n                 input_path, self._reader.__class__.__name__)\n    self.header = getattr(self._reader, \'header\', None)\n    self._post_init_hook()\n\n  @abc.abstractmethod\n  def _native_reader(self, input_path, **kwargs):\n    """"""Returns a GenomicsReader for reading the records `natively`.\n\n    Args:\n      input_path: The path to the native file to read.\n      **kwargs:  Zero or more keyword arguments.\n\n    Returns:\n      A GenomicsReader.\n    """"""\n\n  @abc.abstractmethod\n  def _record_proto(self):\n    """"""Returns the protocol buffer type used by this reader.""""""\n\n  def iterate(self):\n    return self._reader.iterate()\n\n  def query(self, region):\n    return self._reader.query(region)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n  def _post_init_hook(self):\n    """"""Hook for subclasses to run code at the end of __init__.""""""\n'"
third_party/nucleus/io/genomics_reader_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for third_party.nucleus.io.vcf.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nimport six\n\nfrom third_party.nucleus.io import genomics_reader\nfrom third_party.nucleus.protos import gff_pb2\nfrom third_party.nucleus.testing import test_utils\n\n\nclass DummyReader(genomics_reader.GenomicsReader):\n  """"""A GenomicsReader that produces consecutive integers.""""""\n\n  def __init__(self, input_path):\n    self.limit = int(input_path)\n    super(DummyReader, self).__init__()\n\n  def iterate(self):\n    for i in range(self.limit):\n      yield i\n\n  def query(self, region):\n    raise NotImplementedError(\'Can not query DummyReader\')\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    pass\n\n\nclass GenomicsReaderTests(absltest.TestCase):\n  """"""Tests for GenomicsReader.""""""\n\n  def testIteration(self):\n    dreader = DummyReader(\'10\')\n    self.assertEqual(list(range(10)), list(dreader))\n\n  def testTwoIteratorsAtTheSameTime(self):\n    dreader = DummyReader(\'100\')\n    iter2 = iter(dreader)\n    for i in range(100):\n      self.assertEqual(i, six.next(dreader))\n      self.assertEqual(i, six.next(iter2))\n\n\nclass TFRecordReaderTests(absltest.TestCase):\n  """"""Tests for TFRecordReader.""""""\n\n  def testUncompressed(self):\n    reader = genomics_reader.TFRecordReader(\n        test_utils.genomics_core_testdata(\'test_features.gff.tfrecord\'),\n        gff_pb2.GffRecord(),\n    )\n    records = list(reader.iterate())\n    self.assertEqual(\'GenBank\', records[0].source)\n    self.assertEqual(\'ctg123\', records[1].range.reference_name)\n    self.assertNotEqual(reader.c_reader, 0)\n\n  def testUncompressedExplicit(self):\n    reader = genomics_reader.TFRecordReader(\n        test_utils.genomics_core_testdata(\'test_features.gff.tfrecord\'),\n        gff_pb2.GffRecord(),\n        compression_type=\'\'\n    )\n    records = list(reader.iterate())\n    self.assertEqual(\'GenBank\', records[0].source)\n    self.assertEqual(\'ctg123\', records[1].range.reference_name)\n\n  def testCompressed(self):\n    reader = genomics_reader.TFRecordReader(\n        test_utils.genomics_core_testdata(\'test_features.gff.tfrecord.gz\'),\n        gff_pb2.GffRecord(),\n    )\n    records = list(reader.iterate())\n    self.assertEqual(\'GenBank\', records[0].source)\n    self.assertEqual(\'ctg123\', records[1].range.reference_name)\n\n  def testCompressedExplicit(self):\n    reader = genomics_reader.TFRecordReader(\n        test_utils.genomics_core_testdata(\'test_features.gff.tfrecord.gz\'),\n        gff_pb2.GffRecord(),\n        compression_type=\'GZIP\'\n    )\n    records = list(reader.iterate())\n    self.assertEqual(\'GenBank\', records[0].source)\n    self.assertEqual(\'ctg123\', records[1].range.reference_name)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/genomics_writer.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Classes that provide the interface for writing genomics data.\n\n`GenomicsWriter` defines the core API supported by writers, and is subclassed\ndirectly or indirectly (via `DispatchingGenomicsWriter`) for all concrete\nimplementations.\n\n`TFRecordWriter` is an implementation of the `GenomicsWriter` API for reading\n`TFRecord` files. This is usable for all data types when writing data as\nserialized protocol buffers.\n\n`DispatchingGenomicsWriter` is an abstract class defined for convenience on top\nof `GenomicsWriter` that supports writing to either the native file format or to\n`TFRecord` files of the corresponding protocol buffer used to encode data of\nthat file type. The output format chosen is dependent upon the filename to which\nthe data are being written.\n\nConcrete implementations for individual file types (e.g. BED, SAM, VCF, etc.)\nreside in type-specific modules in this package. A general example of the write\nfunctionality is shown below.\n\n```python\n# options is a writer-specific set of options.\noptions = ...\n\n# records is an iterable of protocol buffers of the specific data type.\nrecords = ...\n\nwith GenomicsWriterSubClass(output_path, options) as writer:\n  for proto in records:\n    writer.write(proto)\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport errno\n\nfrom absl import logging\n\nfrom third_party.nucleus.io.python import tfrecord_writer\n\n\nclass GenomicsWriter(object):\n  """"""Abstract base class for writing genomics data.\n\n  A GenomicsWriter only has one method, write, which writes a single\n  protocol buffer to a file.\n  """"""\n\n  __metaclass__ = abc.ABCMeta\n\n  @abc.abstractmethod\n  def write(self, proto):\n    """"""Writes proto to the file.\n\n    Args:\n      proto:  A protocol buffer.\n    """"""\n\n  def __enter__(self):\n    """"""Enter a `with` block.""""""\n    return self\n\n  @abc.abstractmethod\n  def __exit__(self, unused_type, unused_value, unused_traceback):\n    """"""Exit a `with` block.  Typically, this will close the file.""""""\n\n\nclass TFRecordWriter(GenomicsWriter):\n  """"""A GenomicsWriter that writes to a TFRecord file.\n\n  Example usage:\n    writer = TFRecordWriter(\'/tmp/my_output.tfrecord.gz\')\n    for record in records:\n      writer.write(record)\n\n  Note that TFRecord files do not need to be wrapped in a ""with"" block.\n  """"""\n\n  def __init__(self, output_path, header=None, compression_type=None):\n    """"""Initializer.\n\n    Args:\n      output_path: str. The output path to which the records are written.\n      header: An optional header for the particular data type. This can be\n        useful for file types that have logical headers where some operations\n        depend on that header information (e.g. VCF using its headers to\n        determine type information of annotation fields).\n      compression_type:  Either \'ZLIB\', \'GZIP\', \'\' (uncompressed), or\n        None.  If None, __init__ will guess the compression type based on\n        the input_path\'s suffix.\n\n    Raises:\n      IOError:  if there was any problem opening output_path for writing.\n    """"""\n    super(TFRecordWriter, self).__init__()\n    self.header = header\n\n    if compression_type is None:\n      compression_type = \'GZIP\' if output_path.endswith(\'.gz\') else \'\'\n\n    self._writer = tfrecord_writer.TFRecordWriter.from_file(\n        output_path, compression_type)\n    if self._writer is None:\n      raise IOError(errno.EIO, \'Error opening %s for writing\' % output_path)\n\n  def write(self, proto):\n    """"""Writes the proto to the TFRecord file.""""""\n    self._writer.write(proto.SerializeToString())\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.close()\n\n\nclass DispatchingGenomicsWriter(GenomicsWriter):\n  """"""A GenomicsWriter that dispatches based on the file extension.\n\n  If \'.tfrecord\' is present in the filename, a TFRecordWriter is used.\n  Otherwise, a native writer is.\n\n  Sub-classes of DispatchingGenomicsWriter must define a _native_writer()\n  method.\n  """"""\n\n  def __init__(self, output_path, **kwargs):\n    """"""Initializer.\n\n    Args:\n      output_path: str. The output path to which the records are written.\n      **kwargs: k=v named args. Keyword arguments used to instantiate the native\n        writer, if applicable.\n    """"""\n    super(DispatchingGenomicsWriter, self).__init__()\n    self.header = kwargs.get(\'header\', None)\n\n    if \'.tfrecord\' in output_path:\n      self._writer = TFRecordWriter(output_path, header=self.header)\n    else:\n      self._writer = self._native_writer(output_path, **kwargs)\n    logging.info(\'Writing %s with %s\',\n                 output_path, self._writer.__class__.__name__)\n    self._post_init_hook()\n\n  @abc.abstractmethod\n  def _native_writer(self, output_path, **kwargs):\n    """"""Returns a GenomicsWriter for writing the records `natively`.\n\n    Args:\n      output_path: The path to write the records to.\n      **kwargs:  Zero or more keyword arguments.\n\n    Returns:\n      A GenomicsWriter.\n    """"""\n\n  def write(self, proto):\n    self._writer.write(proto)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.__exit__(exit_type, exit_value, exit_traceback)\n\n  def _post_init_hook(self):\n    """"""Hook for subclasses to run code at the end of __init__.""""""\n'"
third_party/nucleus/io/gff.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Classes for reading and writing GFF files.\n\nThe GFF format is described at\nhttps://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md.\n\nAPI for reading:\n\n```python\nfrom third_party.nucleus.io import gff\n\n# Iterate through all records.\nwith gff.GffReader(input_path) as reader:\n  for record in reader:\n    print(record)\n```\n\nwhere `record` is a `nucleus.genomics.v1.GffRecord` protocol buffer.\n\nAPI for writing:\n\n```python\nfrom third_party.nucleus.io import gff\nfrom third_party.nucleus.protos import gff_pb2\n\n# records is an iterable of nucleus.genomics.v1.GffRecord protocol buffers.\nrecords = ...\n\nheader = gff_pb2.GffHeader()\n\n# Write all records to the desired output path.\nwith gff.GffWriter(output_path, header) as writer:\n  for record in records:\n    writer.write(record)\n```\n\nFor both reading and writing, if the path provided to the constructor contains\n\'.tfrecord\' as an extension, a `TFRecord` file is assumed and attempted to be\nread or written. Otherwise, the filename is treated as a true GFF file.\n\nFiles that end in a \'.gz\' suffix cause the file to be treated as compressed\n(with BGZF if it is a true GFF file, and with gzip if it is a TFRecord file).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom third_party.nucleus.io import genomics_reader\nfrom third_party.nucleus.io import genomics_writer\nfrom third_party.nucleus.io.python import gff_reader\nfrom third_party.nucleus.io.python import gff_writer\nfrom third_party.nucleus.protos import gff_pb2\n\n\nclass NativeGffReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from native GFF files.\n\n  Most users will want to use GffReader instead, because it dynamically\n  dispatches between reading native GFF files and TFRecord files based on the\n  filename\'s extension.\n  """"""\n\n  def __init__(self, input_path):\n    """"""Initializes a NativeGffReader.\n\n    Args:\n      input_path: string. A path to a resource containing GFF records.\n    """"""\n    super(NativeGffReader, self).__init__()\n    gff_path = input_path.encode(\'utf8\')\n    reader_options = gff_pb2.GffReaderOptions()\n    self._reader = gff_reader.GffReader.from_file(gff_path, reader_options)\n    self.header = self._reader.header\n\n  def query(self):\n    """"""Returns an iterator for going through the records in the region.\n\n    NOTE: This function is not currently implemented by NativeGffReader though\n    it could be implemented for sorted, tabix-indexed GFF files.\n    """"""\n    raise NotImplementedError(\'Can not currently query a GFF file\')\n\n  def iterate(self):\n    """"""Returns an iterable of GffRecord protos in the file.""""""\n    return self._reader.iterate()\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass GffReader(genomics_reader.DispatchingGenomicsReader):\n  """"""Class for reading GffRecord protos from GFF or TFRecord files.""""""\n\n  def _native_reader(self, input_path, **kwargs):\n    return NativeGffReader(input_path, **kwargs)\n\n  def _record_proto(self):\n    return gff_pb2.GffRecord\n\n\nclass NativeGffWriter(genomics_writer.GenomicsWriter):\n  """"""Class for writing to native GFF files.\n\n  Most users will want GffWriter, which will write to either native GFF\n  files or TFRecord files, based on the output filename\'s extension.\n  """"""\n\n  def __init__(self, output_path, header):\n    """"""Initializer for NativeGffWriter.\n\n    Args:\n      output_path: str. The path to which to write the GFF file.\n      header: nucleus.genomics.v1.GffHeader. The header that defines all\n        information germane to the constituent GFF records.\n    """"""\n    super(NativeGffWriter, self).__init__()\n    writer_options = gff_pb2.GffWriterOptions()\n    self._writer = gff_writer.GffWriter.to_file(output_path, header,\n                                                writer_options)\n\n  def write(self, proto):\n    self._writer.write(proto)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass GffWriter(genomics_writer.DispatchingGenomicsWriter):\n  """"""Class for writing GffRecord protos to GFF or TFRecord files.""""""\n\n  def _native_writer(self, output_path, header):\n    return NativeGffWriter(output_path, header=header)\n'"
third_party/nucleus/io/gff_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for third_party.nucleus.io.gff.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import gff\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.protos import gff_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\n\n# Names of testdata GFF files; we also reuse these basenames for output files\n# in the tmp directory.\nTEXT_GFF_FILES = (\'test_features.gff\', \'test_features.gff.gz\')\nTFRECORD_GFF_FILES = (\'test_features.gff.tfrecord\',\n                      \'test_features.gff.tfrecord.gz\')\nALL_GFF_FILES = TEXT_GFF_FILES + TFRECORD_GFF_FILES\n\nEXPECTED_GFF_VERSION = \'gff-version 3.2.1\'\n\n\nclass GffReaderTests(parameterized.TestCase):\n\n  @parameterized.parameters(*ALL_GFF_FILES)\n  def test_iterate_gff_reader(self, gff_filename):\n    gff_path = test_utils.genomics_core_testdata(gff_filename)\n    expected = [(\'ctg123\', 999, 9000), (\'ctg123\', 999, 1012)]\n\n    with gff.GffReader(gff_path) as reader:\n      records = list(reader.iterate())\n    self.assertLen(records, 2)\n    self.assertEqual(\n        [(r.range.reference_name, r.range.start, r.range.end) for r in records],\n        expected)\n\n  @parameterized.parameters(*TEXT_GFF_FILES)\n  def test_native_gff_header(self, gff_filename):\n    gff_path = test_utils.genomics_core_testdata(gff_filename)\n    with gff.GffReader(gff_path) as reader:\n      self.assertEqual(EXPECTED_GFF_VERSION, reader.header.gff_version)\n    with gff.NativeGffReader(gff_path) as native_reader:\n      self.assertEqual(EXPECTED_GFF_VERSION, native_reader.header.gff_version)\n\n\nclass GffWriterTests(parameterized.TestCase):\n  """"""Tests for GffWriter.""""""\n\n  def setUp(self):\n    tfrecord_file = test_utils.genomics_core_testdata(\n        \'test_features.gff.tfrecord\')\n    self.records = list(\n        tfrecord.read_tfrecords(tfrecord_file, proto=gff_pb2.GffRecord))\n    self.header = gff_pb2.GffHeader(\n        sequence_regions=[ranges.make_range(\'ctg123\', 0, 1497228)])\n\n  @parameterized.parameters(*ALL_GFF_FILES)\n  def test_roundtrip_writer(self, filename):\n    output_path = test_utils.test_tmpfile(filename)\n    with gff.GffWriter(output_path, header=self.header) as writer:\n      for record in self.records:\n        writer.write(record)\n\n    with gff.GffReader(output_path) as reader:\n      v2_records = list(reader.iterate())\n\n    self.assertEqual(self.records, v2_records)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/gfile.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""A Python interface for files.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\n\nfrom third_party.nucleus.io.python import gfile\n\n\ndef Exists(filename):\n  return gfile.Exists(filename)\n\n\ndef Glob(pattern):\n  return gfile.Glob(pattern)\n\n\nclass ReadableFile(six.Iterator):\n  """"""Wraps gfile.ReadableFile to add iteration, enter/exit and readlines.""""""\n\n  def __init__(self, filename):\n    self._file = gfile.ReadableFile.New(filename)\n\n  def __enter__(self):\n    return self\n\n  def __exit__(self, type_, value, traceback):\n    self._file.__exit__()\n\n  def __iter__(self):\n    return self\n\n  def __next__(self):\n    ok, line = self._file.Readline()\n    if ok:\n      return line\n    else:\n      raise StopIteration\n\n  def readlines(self):\n    lines = []\n    while True:\n      ok, line = self._file.Readline()\n      if ok:\n        lines.append(line)\n      else:\n        break\n    return lines\n\n\ndef Open(filename, mode=""r""):\n  if mode.startswith(""r""):\n    return ReadableFile(filename)\n  elif mode.startswith(""w""):\n    return gfile.WritableFile.New(filename)\n  else:\n    raise ValueError(""Unsupported mode \'{}\' for Open"".format(mode))\n'"
third_party/nucleus/io/gfile_test.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for third_party.nucleus.io.gfile.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\n\nfrom third_party.nucleus.io import gfile\nfrom third_party.nucleus.testing import test_utils\n\n\nclass GfileTest(absltest.TestCase):\n\n  def test_exists(self):\n    self.assertTrue(gfile.Exists(\n        test_utils.genomics_core_testdata(\'test_regions.bedgraph\')))\n    self.assertFalse(gfile.Exists(\n        test_utils.genomics_core_testdata(\'does_not_exist\')))\n\n  def test_glob(self):\n    g1 = gfile.Glob(test_utils.genomics_core_testdata(\'test*\'))\n    self.assertGreater(len(g1), 1)\n    self.assertIn(\n        test_utils.genomics_core_testdata(\'test.bam\'), g1)\n    g2 = gfile.Glob(test_utils.genomics_core_testdata(\'does_not_exist*\'))\n    self.assertEqual([], g2)\n\n  def test_reading(self):\n    with gfile.Open(test_utils.genomics_core_testdata(\'headerless.sam\')) as f:\n      for line in f:\n        self.assertTrue(line.startswith(\'SRR3656745\'))\n\n  def test_writing(self):\n    path = test_utils.test_tmpfile(\'test_gfile\')\n    with gfile.Open(path, \'w\') as f:\n      f.write(\'test\\n\')\n      f.write(\'end\\n\')\n\n    with gfile.Open(path, \'r\') as f2:\n      lines = f2.readlines()\n\n    self.assertEqual([\'test\\n\', \'end\\n\'], lines)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/sam.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n# pylint: disable=line-too-long\n""""""Classes for reading and writing SAM and BAM files.\n\nThe SAM/BAM/CRAM formats are described at\nhttps://samtools.github.io/hts-specs/SAMv1.pdf\nhttps://samtools.github.io/hts-specs/CRAMv3.pdf\n\nAPI for reading:\n\n```python\nfrom third_party.nucleus.io import sam\n\nwith sam.SamReader(input_path) as reader:\n  for read in reader:\n    print(read)\n```\n\nwhere `read` is a `nucleus.genomics.v1.Read` protocol buffer. input_path will\ndynamically decode the underlying records depending the file extension, with\n`.sam` for SAM files, `.bam` for BAM files, and `.cram` for CRAM files. It will\nalso search for an appropriate index file to use to enable calls to the\n`query()` method.\n\nAPI for writing SAM/BAM:\n\n```python\nfrom third_party.nucleus.io import sam\n\n# reads is an iterable of nucleus.genomics.v1.Read protocol buffers.\nreads = ...\n\nwith sam.SamWriter(output_path, header=header) as writer:\n  for read in reads:\n    writer.write(read)\n```\n\nAPI for writing CRAM:\n\n```python\n# ref_path is required for writing CRAM files. If embed_ref, the output CRAM\n# file will embed reference sequences.\nwith sam.SamWriter(output_path, header=header, ref_path=ref_path,\n                   embed_ref=embed_ref) as writer:\n  for read in reads:\n    writer.write(read)\n```\n\nFor both reading and writing, if the path provided to the constructor contains\n\'.tfrecord\' as an extension, a `TFRecord` file is assumed and attempted to be\nread or written. Otherwise, the filename is treated as a true SAM/BAM/CRAM file.\n\nFor `TFRecord` files, ending in a \'.gz\' suffix causes the file to be treated as\ncompressed with gzip.\n\nNotes on using CRAM with SamReader\n--------------------------------\n\nNucleus supports reading from CRAM files using the same API as for SAM/BAM:\n\n```python\nfrom third_party.nucleus.io import sam\n\nwith sam.SamReader(""/path/to/sample.cram"") as reader:\n  for read in reader:\n    print(read)\n```\n\nThere is one type of CRAM file, though, that has a slightly more complicated\nAPI. If the CRAM file uses read sequence compression with an external reference\nfile, and this reference file is no longer accessible in the location specified\nby the CRAM file\'s ""UR"" tag and cannot be found in the local genome cache, its\nlocation must be passed to SamReader via the ref_path parameter:\n\n```python\nfrom third_party.nucleus.io import sam\n\ncram_path = ""/path/to/sample.cram""\nref_path = ""/path/to/genome.fasta""\nwith sam.SamReader(cram_path, ref_path=ref_path) as reader:\n  for read in reader:\n    print(read)\n```\n\nUnfortunately, htslib is unable to load the ref_path from anything other than a\nPOSIX filesystem. (htslib plugin filesystems like S3 or GCS buckets won\'t work).\nFor that reason, we don\'t recommend the use of CRAM files with external\nreference files, but instead suggest using read sequence compression with\nembedded reference data. (This has a minor impact on file size, but\nsignificantly improves file access simplicity and safety.)\n\nFor more information about CRAM, see:\n* The `samtools` documentation at http://www.htslib.org/doc/samtools.html\n* The ""Global Options"" section of the samtools docs at http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS\n* How reference sequences are encoded in CRAM at http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES\n* Finally, benchmarking of different CRAM options http://www.htslib.org/benchmarks/CRAM.html\n""""""\n# pylint: enable=line-too-long\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom third_party.nucleus.io import genomics_reader\nfrom third_party.nucleus.io import genomics_writer\nfrom third_party.nucleus.io.python import sam_reader\nfrom third_party.nucleus.io.python import sam_writer\nfrom third_party.nucleus.protos import reads_pb2\nfrom third_party.nucleus.util import ranges\nfrom third_party.nucleus.util import utils\n\n\nclass NativeSamReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from native SAM/BAM/CRAM files.\n\n  Most users will want to use SamReader instead, because it dynamically\n  dispatches between reading native SAM/BAM/CRAM files and TFRecord files based\n  on the filename\'s extensions.\n  """"""\n\n  def __init__(self,\n               input_path,\n               ref_path=None,\n               read_requirements=None,\n               parse_aux_fields=False,\n               hts_block_size=None,\n               downsample_fraction=None,\n               random_seed=None,\n               use_original_base_quality_scores=False):\n    """"""Initializes a NativeSamReader.\n\n    Args:\n      input_path: str. A path to a resource containing SAM/BAM/CRAM records.\n        Currently supports SAM text format, BAM binary format, and CRAM.\n      ref_path: optional str or None. Only used for CRAM decoding, and only\n        necessary if the UR encoded path in the CRAM itself needs to be\n        overridden. If provided, we will tell the CRAM decoder to use this FASTA\n        for the reference sequence.\n      read_requirements: optional ReadRequirement proto. If not None, this proto\n        is used to control which reads are filtered out by the reader before\n        they are passed to the client.\n      parse_aux_fields: optional bool, defaulting to False. If False, we do not\n        parse the auxiliary fields of the SAM/BAM/CRAM records (see SAM spec for\n        details). Parsing the aux fields is unnecessary for many applications,\n        and adds a significant parsing cost to access. If you need these aux\n        fields, set parse_aux_fields to True and these fields will be parsed and\n        populate the appropriate Read proto fields (e.g., read.info).\n      hts_block_size: int or None. If specified, this configures the block size\n        of the underlying htslib file object. Larger values (e.g. 1M) may be\n        beneficial for reading remote files. If None, the reader uses the\n        default htslib block size.\n      downsample_fraction: float in the interval [0.0, 1.0] or None. If\n        specified as a positive float, the reader will only keep each read with\n        probability downsample_fraction, randomly. If None or zero, all reads\n        are kept.\n      random_seed: None or int. The random seed to use with this sam reader, if\n        needed. If None, a fixed random value will be assigned.\n      use_original_base_quality_scores: optional bool, defaulting to False. If\n        True, quality scores are read from OQ tag.\n\n    Raises:\n      ValueError: If downsample_fraction is not None and not in the interval\n        (0.0, 1.0].\n      ImportError: If someone tries to load a tfbam file.\n    """"""\n    if input_path.endswith(\'.tfbam\'):\n      # Delayed loading of tfbam_lib.\n      try:\n        from tfbam_lib import tfbam_reader  # pylint: disable=g-import-not-at-top\n        self._reader = tfbam_reader.make_sam_reader(\n            input_path,\n            read_requirements=read_requirements,\n            unused_block_size=hts_block_size,\n            downsample_fraction=downsample_fraction,\n            random_seed=random_seed)\n      except ImportError:\n        raise ImportError(\n            \'tfbam_lib module not found, cannot read .tfbam files.\')\n    else:\n      aux_field_handling = reads_pb2.SamReaderOptions.SKIP_AUX_FIELDS\n      if parse_aux_fields:\n        aux_field_handling = reads_pb2.SamReaderOptions.PARSE_ALL_AUX_FIELDS\n\n      # We make 0 be a valid value that means ""keep all reads"" so that proto\n      # defaults (=0) do not omit all reads.\n      if downsample_fraction is not None and downsample_fraction != 0:\n        if not 0.0 < downsample_fraction <= 1.0:\n          raise ValueError(\n              \'downsample_fraction must be in the interval (0.0, 1.0]\',\n              downsample_fraction)\n\n      if random_seed is None:\n        # Fixed random seed produced with \'od -vAn -N4 -tu4 < /dev/urandom\'.\n        random_seed = 2928130004\n\n      self._reader = sam_reader.SamReader.from_file(\n          input_path.encode(\'utf8\'),\n          ref_path.encode(\'utf8\') if ref_path is not None else \'\',\n          reads_pb2.SamReaderOptions(\n              read_requirements=read_requirements,\n              aux_field_handling=aux_field_handling,\n              hts_block_size=(hts_block_size or 0),\n              downsample_fraction=downsample_fraction,\n              random_seed=random_seed,\n              use_original_base_quality_scores=use_original_base_quality_scores)\n      )\n\n      self.header = self._reader.header\n\n    super(NativeSamReader, self).__init__()\n\n  def iterate(self):\n    """"""Returns an iterable of Read protos in the file.""""""\n    return self._reader.iterate()\n\n  def query(self, region):\n    """"""Returns an iterator for going through the reads in the region.""""""\n    return self._reader.query(region)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass SamReader(genomics_reader.DispatchingGenomicsReader):\n  """"""Class for reading Read protos from SAM/BAM/CRAM or TFRecord files.""""""\n\n  def _native_reader(self, input_path, **kwargs):\n    return NativeSamReader(input_path, **kwargs)\n\n  def _record_proto(self):\n    return reads_pb2.Read\n\n\nclass NativeSamWriter(genomics_writer.GenomicsWriter):\n  """"""Class for writing to native SAM/BAM/CRAM files.\n\n  Most users will want SamWriter, which will write to either native SAM/BAM/CRAM\n  files or TFRecords files, based on the output filename\'s extensions.\n  """"""\n\n  def __init__(self, output_path, header, ref_path=None, embed_ref=False):\n    """"""Initializer for NativeSamWriter.\n\n    Args:\n      output_path: str. A path where we\'ll write our SAM/BAM/CRAM file.\n      ref_path: str. Path to the reference file. Required for CRAM file.\n      embed_ref: bool. Whether to embed the reference sequences in CRAM file.\n        Default is False.\n      header: A nucleus.SamHeader proto.  The header is used both for writing\n        the header, and to control the sorting applied to the rest of the file.\n    """"""\n    super(NativeSamWriter, self).__init__()\n    self._writer = sam_writer.SamWriter.to_file(\n        output_path,\n        ref_path.encode(\'utf8\') if ref_path is not None else \'\', embed_ref,\n        header)\n\n  def write(self, proto):\n    self._writer.write(proto)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass SamWriter(genomics_writer.DispatchingGenomicsWriter):\n  """"""Class for writing Read protos to SAM or TFRecord files.""""""\n\n  def _native_writer(self, output_path, **kwargs):\n    return NativeSamWriter(output_path, **kwargs)\n\n\nclass InMemorySamReader(object):\n  """"""Python interface class for in-memory SAM/BAM/CRAM reader.\n\n  Attributes:\n    reads: list[nucleus.genomics.v1.Read]. The list of in-memory reads.\n    is_sorted: bool, True if reads are sorted.\n  """"""\n\n  def __init__(self, reads, is_sorted=False):\n    self.replace_reads(reads, is_sorted=is_sorted)\n\n  def replace_reads(self, reads, is_sorted=False):\n    """"""Replace the reads stored by this reader.""""""\n    self.reads = reads\n    self.is_sorted = is_sorted\n\n  def iterate(self):\n    """"""Iterate over all records in the reads.\n\n    Returns:\n      An iterator over nucleus.genomics.v1.Read\'s.\n    """"""\n    return self.reads\n\n  def query(self, region):\n    """"""Returns an iterator for going through the reads in the region.\n\n    Args:\n      region: nucleus.genomics.v1.Range. The query region.\n\n    Returns:\n      An iterator over nucleus.genomics.v1.Read protos.\n    """"""\n    # redacted\n    return (\n        read for read in self.reads if utils.read_overlaps_region(read, region))\n'"
third_party/nucleus/io/sam_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for third_party.nucleus.util.io.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport itertools\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nimport six\n\nfrom tensorflow.python.platform import gfile\nfrom third_party.nucleus.io import sam\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.protos import reads_pb2\nfrom third_party.nucleus.protos import reference_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\n\n\nclass SamReaderTests(parameterized.TestCase):\n  """"""Test the iteration functionality provided by io.SamReader.""""""\n\n  def test_sam_iterate(self):\n    reader = sam.SamReader(test_utils.genomics_core_testdata(\'test.sam\'))\n    with reader:\n      self.assertEqual(test_utils.iterable_len(reader.iterate()), 6)\n\n  def test_bam_iterate(self):\n    reader = sam.SamReader(test_utils.genomics_core_testdata(\'test.bam\'))\n    with reader:\n      self.assertEqual(test_utils.iterable_len(reader.iterate()), 106)\n\n  def test_bam_iterate_partially(self):\n    """"""Verify that iteration provides results incrementally, not all at once.""""""\n    reader = sam.SamReader(test_utils.genomics_core_testdata(\'test.bam\'))\n    with reader:\n      iterable = reader.iterate()\n      # We expect 106 records in total.\n      for _ in range(10):\n        results = list(itertools.islice(iterable, 10))\n        self.assertEqual(len(results), 10)\n      results = list(itertools.islice(iterable, 10))\n      self.assertEqual(len(results), 6)\n\n  def test_sam_query(self):\n    reader = sam.SamReader(test_utils.genomics_core_testdata(\'test.bam\'))\n    expected = [(ranges.parse_literal(\'chr20:10,000,000-10,000,100\'), 106),\n                (ranges.parse_literal(\'chr20:10,000,000-10,000,000\'), 45)]\n    with reader:\n      for interval, n_expected in expected:\n        with reader.query(interval) as iterable:\n          self.assertEqual(test_utils.iterable_len(iterable), n_expected)\n\n  def test_sam_query_alternate_index_name(self):\n    reader = sam.SamReader(\n        test_utils.genomics_core_testdata(\'test_alternate_index.bam\'))\n    expected = [(ranges.parse_literal(\'chr20:10,000,000-10,000,100\'), 106),\n                (ranges.parse_literal(\'chr20:10,000,000-10,000,000\'), 45)]\n    with reader:\n      for interval, n_expected in expected:\n        with reader.query(interval) as iterable:\n          self.assertEqual(test_utils.iterable_len(iterable), n_expected)\n\n  @parameterized.parameters((\'\\t\'.join(x[0] for x in items), {\n      k: v for t in items for k, v in t[1].items()\n  }) for r in [1, 2] for items in itertools.permutations(\n      [\n          (\'X1:i:0\', {\n              \'X1\': 0\n          }),\n          (\'X2:i:127\', {\n              \'X2\': 127\n          }),\n          (\'X3:i:255\', {\n              \'X3\': 255\n          }),\n          (\'X4:A:a\', {\n              \'X4\': \'a\'\n          }),\n          (\'X5:f:1.234\', {\n              \'X5\': 1.234\n          }),\n          (\'X6:Z:string\', {\n              \'X6\': \'string\'\n          }),\n          (\'X7:Z:with spaces\', {\n              \'X7\': \'with spaces\'\n          }),\n          (\'ZJ:Z:\', {\n              \'ZJ\': \'\'\n          }),  # Empty string.\n          # We skip H hex byte-array tags as they appear deprecated.\n          (\'X8:H:1AE301\', {}),\n          (\'X9:B:i,1\', {\n              \'X9\': [1]\n          }),\n          (\'XA:B:i,1,2\', {\n              \'XA\': [1, 2]\n          }),\n          (\'XB:B:i,1,2,3\', {\n              \'XB\': [1, 2, 3]\n          }),\n          (\'XC:B:i,1,2,3,4,5,6,7,8,9,10\', {\n              \'XC\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n          }),\n          (\'XD:B:I,1,2,3\', {\n              \'XD\': [1, 2, 3]\n          }),\n          (\'XE:B:c,1,2,3\', {\n              \'XE\': [1, 2, 3]\n          }),\n          (\'XF:B:C,1,2,3\', {\n              \'XF\': [1, 2, 3]\n          }),\n          (\'XG:B:f,0.12,0.34\', {\n              \'XG\': [0.12, 0.34]\n          }),\n          (\'XH:B:s,1,2,3\', {\n              \'XH\': [1, 2, 3]\n          }),\n          (\'XI:B:S,1,2,3\', {\n              \'XI\': [1, 2, 3]\n          }),\n      ],\n      r=r))\n  def test_parsing_aux_tags(self, tag_string, expected_info):\n    # Minimal header line to create a valid SAM file.\n    reads = self._parse_read_with_aux_tags(tag_string)\n    self.assertLen(reads, 1)\n    self.assertInfoMapEqual(reads[0].info, expected_info)\n\n  @parameterized.parameters(\n      \'\\t\'.join(tags) for r in [1, 2, 3] for tags in itertools.permutations(\n          [\n              \'X2:i:x\',  # Integer with character value.\n              \'X3:f:string\',  # A string instead of the expected float.\n              \'X4:A:\',  # Supposed to be single char, but we none here.\n              \'X5:A:ab\',  # Supposed to be single char, but we have two here.\n              \'X6:B:i\',  # Empty byte array.\n              \'X7:B:i,1.23\',  # Integer byte array with a float.\n              \'X8:B:i,string\',  # Integer byte array with a string.\n              \'X8:B:f,string\',  # Float byte array with a string.\n              \'X8:B:z,1,2,3\',  # z is not a valid subtype.\n          ],\n          r=r))\n  def test_survives_malformed_lines(self, tag_string):\n    try:\n      reads = self._parse_read_with_aux_tags(tag_string)\n      # If we didn\'t detect the error, make sure we actually still parsed the\n      # read itself.\n      self.assertLen(reads, 1)\n      self.assertEqual(reads[0].fragment_name, \'read_name\')\n      self.assertEqual(reads[0].aligned_sequence, \'CCC\')\n      self.assertEqual(reads[0].alignment.position.reference_name, \'chr1\')\n      self.assertEqual(reads[0].alignment.position.position, 0)\n    except ValueError as e:\n      if \'Failed to parse SAM record\' not in str(e):\n        self.fail(\'Parsing failed but unexpected exception was seen: \' + str(e))\n\n  def _parse_read_with_aux_tags(self, tag_string):\n    # Minimal header line to create a valid SAM file.\n    header_lines = \'@HD\\tVN:1.3\\tSO:coordinate\\n@SQ\\tSN:chr1\\tLN:248956422\\n\'\n    # A single stock read we\'ll add our AUX fields to.\n    read = \'read_name\\t0\\tchr1\\t1\\t0\\t3M\\t*\\t0\\t0\\tCCC\\tAAA\\t\' + tag_string\n    path = test_utils.test_tmpfile(\'aux_tags.bam\')\n    with gfile.Open(path, \'w\') as fout:\n      fout.write(header_lines)\n      fout.write(read + \'\\n\')\n    with sam.SamReader(path, parse_aux_fields=True) as reader:\n      return list(reader.iterate())\n\n  def assertInfoMapEqual(self, info_map, expected_info):\n    self.assertCountEqual(\n        info_map.keys(), expected_info.keys(),\n        \'info has {} keys but we expected {}\'.format(info_map.keys(),\n                                                     expected_info.keys()))\n    for key, expected_values in expected_info.items():\n      if not isinstance(expected_values, list):\n        expected_values = [expected_values]\n      for actual_value, expected_value in zip(info_map[key].values,\n                                              expected_values):\n        if isinstance(expected_value, float):\n          self.assertAlmostEqual(actual_value.number_value, expected_value)\n        elif isinstance(expected_value, six.integer_types):\n          self.assertEqual(actual_value.int_value, expected_value)\n        elif isinstance(expected_value, str):\n          self.assertEqual(actual_value.string_value, expected_value)\n        else:\n          self.fail(\'Unsupported expected_value type {}\'.format(expected_value))\n\n  @parameterized.parameters(\n      # These expected counts are deterministic because we always set the random\n      # seed in each test.\n      # There are 106 total reads if we iterate.\n      (\'iterate\', None, 1.0, 106),\n      (\'iterate\', None, 0.5, 59),\n      (\'iterate\', None, 0.25, 31),\n      # There are 45 total reads if we don\'t downsample.\n      (\'query\', \'chr20:10,000,000-10,000,000\', 1.0, 45),\n      (\'query\', \'chr20:10,000,000-10,000,000\', 0.5, 25),\n      (\'query\', \'chr20:10,000,000-10,000,000\', 0.25, 13),\n  )\n  def test_downsampling(self, method, maybe_range, fraction, expected_n_reads):\n    reader = sam.SamReader(\n        test_utils.genomics_core_testdata(\'test.bam\'),\n        downsample_fraction=fraction,\n        random_seed=12345)\n    with reader:\n      if method == \'iterate\':\n        reads_iter = reader.iterate()\n      elif method == \'query\':\n        reads_iter = reader.query(ranges.parse_literal(maybe_range))\n      else:\n        self.fail(\'Unexpected method \' + str(method))\n      self.assertEqual(test_utils.iterable_len(reads_iter), expected_n_reads)\n\n\n# Note that CRAM version 2.1 files work with Nucleus but they cannot be used in\n# our test here because CRAM 2.1 embeds an exact path to the reference file\n# which LEAKR flags as leaking internal google paths.\n@parameterized.parameters(\n    dict(\n        filename=\'test_cram.embed_ref_0_version_3.0.cram\',\n        has_embedded_ref=False),\n    dict(\n        filename=\'test_cram.embed_ref_1_version_3.0.cram\',\n        has_embedded_ref=True),\n)\nclass CramReaderTests(parameterized.TestCase):\n  """"""Test io.SamReader on CRAM formatted files.""""""\n\n  def _make_reader(self, filename, has_embedded_ref):\n    if has_embedded_ref:\n      # If we have an embedded reference, force the reader to use it by not\n      # providing an argument for ref_path.\n      return sam.SamReader(test_utils.genomics_core_testdata(filename))\n    else:\n      # Otherwise we need to explicitly override the reference encoded in the UR\n      # of the CRAM file to use the path provided to our test.fasta.\n      return sam.SamReader(\n          test_utils.genomics_core_testdata(filename),\n          ref_path=test_utils.genomics_core_testdata(\'test.fasta\'))\n\n  def test_header(self, filename, has_embedded_ref):\n    with self._make_reader(filename, has_embedded_ref) as reader:\n      self.assertEqual(reader.header.format_version, \'1.3\')\n      self.assertEqual([contig.name for contig in reader.header.contigs],\n                       [\'chrM\', \'chr1\', \'chr2\'])\n\n  def test_iterate(self, filename, has_embedded_ref):\n    with self._make_reader(filename, has_embedded_ref) as reader:\n      reads = list(reader.iterate())\n      self.assertEqual(len(reads), 3)\n      self.assertEqual([read.fragment_name for read in reads],\n                       [\'cram1\', \'cram2\', \'cram3\'])\n      self.assertEqual([read.aligned_sequence for read in reads], [\n          \'CCCTAACCCTAACCCTAACCCTAACCCTANNNNNN\',\n          (\'TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCAAAACGAATCAAAAAAGAAAAACGAA\'\n           \'AAAAAAA\'),\n          \'CACAGACGCTT\'\n      ])\n\n  def test_query(self, filename, has_embedded_ref):\n    with self._make_reader(filename, has_embedded_ref) as reader:\n      for interval, n_expected in [(\'chr1:1-100\', 3), (\'chr2:1-121\', 0)]:\n        with reader.query(ranges.parse_literal(interval)) as iterable:\n          self.assertEqual(test_utils.iterable_len(iterable), n_expected)\n\n\nclass ReadWriterTests(parameterized.TestCase):\n  """"""Tests for sam.SamWriter.""""""\n\n  def setUp(self):\n    self.read1 = test_utils.make_read(\n        bases=\'ACCGT\',\n        chrom=\'chr1\',\n        start=10,\n        cigar=\'5M\',\n        mapq=50,\n        quals=range(30, 35),\n        name=\'read1\')\n    self.read2 = test_utils.make_read(\n        bases=\'AACCTT\',\n        chrom=\'chr2\',\n        start=15,\n        cigar=\'7M\',\n        mapq=40,\n        quals=range(20, 26),\n        name=\'read2\')\n    self.contigs = [\n        reference_pb2.ContigInfo(name=\'chr1\'),\n        reference_pb2.ContigInfo(name=\'chr2\'),\n    ]\n    self.header = reads_pb2.SamHeader()\n\n  def test_make_read_writer_tfrecords(self):\n    outfile = test_utils.test_tmpfile(\'test.tfrecord\')\n    writer = sam.SamWriter(outfile, header=self.header)\n\n    # Test that the writer is a context manager and that we can write a read to\n    # it.\n    with writer:\n      writer.write(self.read1)\n      writer.write(self.read2)\n\n    # Our output should have exactly one read in it.\n    self.assertEqual([self.read1, self.read2],\n                     list(\n                         tfrecord.read_tfrecords(outfile,\n                                                 proto=reads_pb2.Read)))\n\n  @parameterized.parameters(\'test.bam\', \'test.sam\')\n  def test_roundtrip_writer(self, filename):\n    output_path = test_utils.test_tmpfile(filename)\n    original_reader = sam.SamReader(test_utils.genomics_core_testdata(filename))\n    original_records = list(original_reader.iterate())\n    with sam.SamWriter(output_path, header=original_reader.header) as writer:\n      for record in original_records:\n        writer.write(record)\n    with sam.SamReader(output_path) as new_reader:\n      self.assertEqual(original_records, list(new_reader.iterate()))\n\n  @parameterized.parameters(\n      dict(\n          filename=\'test_cram.embed_ref_0_version_3.0.cram\',\n          has_embedded_ref=False),\n      dict(\n          filename=\'test_cram.embed_ref_1_version_3.0.cram\',\n          has_embedded_ref=True))\n  def test_roundtrip_cram_writer(self, filename, has_embedded_ref):\n    output_path = test_utils.test_tmpfile(filename)\n    writer_ref_path = test_utils.genomics_core_testdata(\'test.fasta\')\n    reader_ref_path = \'\'\n    if not has_embedded_ref:\n      reader_ref_path = writer_ref_path\n    original_reader = sam.SamReader(\n        test_utils.genomics_core_testdata(filename), ref_path=reader_ref_path)\n    original_records = list(original_reader.iterate())\n    with sam.SamWriter(\n        output_path,\n        header=original_reader.header,\n        ref_path=writer_ref_path,\n        embed_ref=has_embedded_ref) as writer:\n      for record in original_records:\n        writer.write(record)\n    with sam.SamReader(output_path, ref_path=reader_ref_path) as new_reader:\n      self.assertEqual(original_records, list(new_reader.iterate()))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/sharded_file_utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Utility functions for working with sharded files.\n\nA sharded file is a single conceptual file that is broken into a collection\nof files to make parallelization easier.  A sharded file spec is like a\nfilename for a sharded file; the file spec ""/some/path/prefix@200.txt""\nsays that the sharded file consists of 200 actual files that have names like\n""/some/path/prefix-00000-of-00200.txt"", ""/some/path/prefix-00001-of-00200.txt"",\netc.  This module contains functions for parsing, generating, detecting and\nresolving sharded file specs.\n""""""\n\n# Important: Please keep this module free of TensorFlow c++ extensions.\n# This makes it easy to build pure python packages for training that work with\n# CMLE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport re\nimport six\n\nfrom tensorflow.python.platform import gfile\n\nSHARD_SPEC_PATTERN = re.compile(R\'((.*)\\@(\\d*[1-9]\\d*)(?:\\.(.+))?)\')\n\n\nclass ShardError(Exception):\n  """"""An I/O error.""""""\n\n\ndef parse_sharded_file_spec(spec):\n  """"""Parse a sharded file specification.\n\n  Args:\n    spec: str. The sharded file specification. A sharded file spec is one like\n      \'gs://some/file@200.txt\'. Here, \'@200\' specifies the number of shards.\n\n  Returns:\n    basename: str. The basename for the files.\n    num_shards: int >= 0. The number of shards.\n    suffix: str. The suffix if there is one, or \'\' if not.\n  Raises:\n    ShardError: If the spec is not a valid sharded specification.\n  """"""\n  m = SHARD_SPEC_PATTERN.match(spec)\n  if not m:\n    raise ShardError((\'The file specification {0} is not a sharded file \'\n                      \'specification because it did not match the regex \'\n                      \'{1}\').format(spec, SHARD_SPEC_PATTERN.pattern))\n\n  # If there\'s a non-empty suffix, we need to prepend \'.\' so we get files like\n  # foo@20.ext instead of foo@ext. The original C++ parser version has:\n  # string ext = StrCat(suff.empty() ? """" : ""."", suff);\n  suffix = \'.\' + m.group(4) if m.group(4) else \'\'\n\n  return m.group(2), int(m.group(3)), suffix\n\n\ndef _shard_width(num_shards):\n  """"""Return the width of the shard matcher based on the number of shards.""""""\n  return max(5, int(math.floor(math.log10(num_shards)) + 1))\n\n\ndef generate_sharded_filenames(spec):\n  """"""Generate the list of filenames corresponding to the sharding path.\n\n  Args:\n    spec: str. Represents a filename with a sharding specification.\n      e.g., \'gs://some/file@200.txt\' represents a file sharded 200 ways.\n\n  Returns:\n    List of filenames.\n\n  Raises:\n    ShardError: If spec is not a valid sharded file specification.\n  """"""\n  basename, num_shards, suffix = parse_sharded_file_spec(spec)\n  files = []\n  width = _shard_width(num_shards)\n  format_str = \'{{0}}-{{1:0{0}}}-of-{{2:0{0}}}{{3}}\'.format(width)\n  for i in range(num_shards):\n    files.append(format_str.format(basename, i, num_shards, suffix))\n\n  return files\n\n\ndef glob_list_sharded_file_patterns(comma_separated_patterns, sep=\',\'):\n  """"""Generate list of filenames corresponding to `comma_separated_patterns`.\n\n  Args:\n    comma_separated_patterns: str. A pattern or a comma-separated list of\n      patterns that represent file names.\n    sep: char. Separator character.\n\n  Returns:\n    List of filenames, sorted and dedupped.\n  """"""\n  return sorted(set([\n      f\n      for pattern in comma_separated_patterns.split(sep)\n      for f in gfile.Glob(normalize_to_sharded_file_pattern(pattern))\n  ]))\n\n\ndef generate_sharded_file_pattern(basename, num_shards, suffix):\n  """"""Generate a sharded file pattern.\n\n  Args:\n    basename: str. The basename for the files.\n    num_shards: int. The number of shards.\n    suffix: str. The suffix if there is one or \'\'.\n  Returns:\n    pattern:\n  """"""\n  width = _shard_width(num_shards)\n  specifier = \'?\' * width\n  format_str = \'{{0}}-{{1}}-of-{{2:0{0}}}{{3}}\'.format(width)\n  return format_str.format(basename, specifier, num_shards, suffix)\n\n\ndef normalize_to_sharded_file_pattern(spec_or_pattern):\n  """"""Take a sharding spec or sharding file pattern and return a sharded pattern.\n\n  The input can be a sharding spec(e.g \'/some/file@10\') or a sharded file\n  pattern (e.g. \'/some/file-?????-of-00010)\n\n  Args:\n    spec_or_pattern: str. A sharded file specification or sharded file pattern.\n\n  Returns:\n    A sharded file pattern.\n  """"""\n  try:\n    basename, num_shards, suffix = parse_sharded_file_spec(spec_or_pattern)\n  except ShardError:\n    return spec_or_pattern\n  return generate_sharded_file_pattern(basename, num_shards, suffix)\n\n\ndef is_sharded_file_spec(spec):\n  """"""Returns True if spec is a sharded file specification.""""""\n  m = SHARD_SPEC_PATTERN.match(spec)\n  return m is not None\n\n\n# redacted\ndef sharded_filename(spec, i):\n  """"""Gets a path appropriate for writing the ith file of a sharded spec.""""""\n  return generate_sharded_filenames(spec)[i]\n\n\n# redacted\n# readability when there are multiple input filespecs.\ndef resolve_filespecs(shard, *filespecs):\n  """"""Transforms potentially sharded filespecs into their paths for single shard.\n\n  This function takes a shard number and a varargs of potentially-sharded\n  filespecs, and returns a list where the filespecs have been resolved into\n  concrete file paths for a single shard.\n\n  This function has a concept of a master filespec, which is used to constrain\n  and check the validity of other filespecs. The first filespec is considered\n  the master, and it cannot be None. For example, if master is not sharded, none\n  of the other specs can be sharded, and vice versa. They must all also have a\n  consistent sharding (e.g., master is @10, then all others must be @10).\n\n  Note that filespecs (except the master) may be None or any other False value,\n  which are returned as-is in the output list.\n\n  Args:\n    shard: int >= 0. Our shard number.\n    *filespecs: list[str]. Contains all of the filespecs we want to resolve into\n      shard-specific file paths.\n\n  Returns:\n    A list. The first element is the number of shards, which is an int >= 1 when\n    filespecs contains sharded paths and 0 if none do. All subsequent\n    returned values follow the shard-specific paths for each filespec, in order.\n\n  Raises:\n    ValueError: if any filespecs are inconsistent.\n  """"""\n  if not filespecs:\n    raise ValueError(\'filespecs must have at least one element.\')\n\n  master = filespecs[0]\n  master_is_sharded = is_sharded_file_spec(master)\n\n  master_num_shards = 0\n  if master_is_sharded:\n    _, master_num_shards, _ = parse_sharded_file_spec(master)\n    if shard >= master_num_shards or shard < 0:\n      raise ValueError(\'Invalid shard={} value with master={} sharding\'.format(\n          shard, master))\n  elif shard > 0:\n    raise ValueError(\'Output is not sharded but shard > 0: {}\'.format(shard))\n\n  def resolve_one(filespec):\n    """"""Resolves a single filespec into a concrete filepath.""""""\n    if not filespec:\n      return filespec\n\n    is_sharded = is_sharded_file_spec(filespec)\n    if master_is_sharded != is_sharded:\n      raise ValueError(\'Master={} and {} have inconsistent sharding\'.format(\n          master, filespec))\n\n    if not is_sharded:  # Not sharded => filespec is the actual filename.\n      return filespec\n\n    _, filespec_num_shards, _ = parse_sharded_file_spec(filespec)\n    if filespec_num_shards != master_num_shards:\n      raise ValueError(\'Master={} and {} have inconsistent sharding\'.format(\n          master, filespec))\n    return sharded_filename(filespec, shard)\n\n  return [master_num_shards] + [resolve_one(spec) for spec in filespecs]\n\n\ndef maybe_generate_sharded_filenames(filespec):\n  """"""Potentially expands sharded filespec into a list of paths.\n\n  This function takes in a potentially sharded filespec and expands it into a\n  list containing the full set of corresponding concrete sharded file paths. If\n  the input filespec is not sharded then a list containing just that file path\n  is returned. This function is useful, for example, when the input to a binary\n  can either be sharded or not.\n\n  Args:\n    filespec: String. A potentially sharded filespec to expand.\n\n  Returns:\n    A list of file paths.\n\n  Raises:\n    TypeError: if filespec is not in valid string_types.\n  """"""\n  if not isinstance(filespec, six.string_types):\n    raise TypeError(\'Invalid filespec: %s\' % filespec)\n  if is_sharded_file_spec(filespec):\n    return generate_sharded_filenames(filespec)\n  else:\n    return [filespec]\n'"
third_party/nucleus/io/sharded_file_utils_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for third_party.nucleus.io.sharded_file_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import sharded_file_utils as io\nfrom third_party.nucleus.testing import test_utils\n\n\nclass IOTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      # Unsharded outputs pass through as expected.\n      dict(task_id=0, filespecs=[\'foo.txt\'], expected=[0, \'foo.txt\']),\n      dict(\n          task_id=0,\n          filespecs=[\'foo.txt\', \'bar.txt\'],\n          expected=[0, \'foo.txt\', \'bar.txt\']),\n      dict(\n          task_id=0,\n          filespecs=[\'bar.txt\', \'foo.txt\'],\n          expected=[0, \'bar.txt\', \'foo.txt\']),\n      # It\'s ok to have False values for other bindings.\n      dict(\n          task_id=0, filespecs=[\'foo.txt\', None], expected=[0, \'foo.txt\',\n                                                            None]),\n      dict(task_id=0, filespecs=[\'foo.txt\', \'\'], expected=[0, \'foo.txt\', \'\']),\n      dict(\n          task_id=0,\n          filespecs=[\'foo@10.txt\', None],\n          expected=[10, \'foo-00000-of-00010.txt\', None]),\n      dict(\n          task_id=0,\n          filespecs=[\'foo@10.txt\', \'\'],\n          expected=[10, \'foo-00000-of-00010.txt\', \'\']),\n      # Simple check that master behaves as expected.\n      dict(\n          task_id=0,\n          filespecs=[\'foo@10.txt\', None],\n          expected=[10, \'foo-00000-of-00010.txt\', None]),\n      dict(\n          task_id=0,\n          filespecs=[\'foo@10\', None],\n          expected=[10, \'foo-00000-of-00010\', None]),\n      dict(\n          task_id=1,\n          filespecs=[\'foo@10\', None],\n          expected=[10, \'foo-00001-of-00010\', None]),\n      dict(\n          task_id=9,\n          filespecs=[\'foo@10\', None],\n          expected=[10, \'foo-00009-of-00010\', None]),\n      # Make sure we handle sharding of multiple filespecs.\n      dict(\n          task_id=0,\n          filespecs=[\'foo@10\', \'bar@10\', \'baz@10\'],\n          expected=[\n              10, \'foo-00000-of-00010\', \'bar-00000-of-00010\',\n              \'baz-00000-of-00010\'\n          ]),\n      dict(\n          task_id=9,\n          filespecs=[\'foo@10\', \'bar@10\', \'baz@10\'],\n          expected=[\n              10, \'foo-00009-of-00010\', \'bar-00009-of-00010\',\n              \'baz-00009-of-00010\'\n          ]),\n  )\n  def test_resolve_filespecs(self, task_id, filespecs, expected):\n    self.assertEqual(io.resolve_filespecs(task_id, *filespecs), expected)\n\n  @parameterized.parameters(\n      # shard >= num_shards.\n      (10, [\'foo@10\']),\n      # shard > 0 but master isn\'t sharded.\n      (1, [\'foo\']),\n      # Inconsistent sharding.\n      (0, [\'foo@10\', \'bad@11\']),\n      # master isn\'t sharded but bad is.\n      (0, [\'foo\', \'bad@11\']),\n  )\n  def test_resolve_filespecs_raises_with_bad_inputs(self, task_id, outputs):\n    with self.assertRaises(ValueError):\n      io.resolve_filespecs(task_id, *outputs)\n\n  @parameterized.parameters(\n      # Unsharded files work.\n      (\'foo.txt\', [\'foo.txt\']),\n      (\'foo-00000-of-00010.txt\', [\'foo-00000-of-00010.txt\']),\n      # Sharded file patterns work.\n      (\'foo@3.txt\', [\n          \'foo-00000-of-00003.txt\', \'foo-00001-of-00003.txt\',\n          \'foo-00002-of-00003.txt\'\n      ]),\n      (\'foo@3\',\n       [\'foo-00000-of-00003\', \'foo-00001-of-00003\', \'foo-00002-of-00003\']),\n  )\n  def test_maybe_generate_sharded_filenames(self, filespec, expected):\n    self.assertEqual(io.maybe_generate_sharded_filenames(filespec), expected)\n\n\nclass ShardsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (\'no_suffix\', \'/dir/foo/bar@3\', \'/dir/foo/bar\', 3, \'\'),\n      (\'suffix-dot\', \'/dir/foo/bar@3.txt\', \'/dir/foo/bar\', 3, \'.txt\'),\n  )\n  def testParseShardedFileSpec(self, spec, expected_basename,\n                               expected_num_shards, expected_suffix):\n\n    basename, num_shards, suffix = io.parse_sharded_file_spec(spec)\n    self.assertEqual(basename, expected_basename)\n    self.assertEqual(num_shards, expected_num_shards)\n    self.assertEqual(suffix, expected_suffix)\n\n  def testParseShardedFileSpecInvalid(self):\n    self.assertRaises(io.ShardError,\n                      io.parse_sharded_file_spec, \'/dir/foo/bar@0\')\n\n  @parameterized.named_parameters(\n      (\'no_suffix\', \'/dir/foo/bar@3\', [\n          \'/dir/foo/bar-00000-of-00003\', \'/dir/foo/bar-00001-of-00003\',\n          \'/dir/foo/bar-00002-of-00003\'\n      ]),\n      (\'suffix\', \'/dir/foo/bar@3.txt\', [\n          \'/dir/foo/bar-00000-of-00003.txt\', \'/dir/foo/bar-00001-of-00003.txt\',\n          \'/dir/foo/bar-00002-of-00003.txt\'\n      ]),\n  )\n  def testGenerateShardedFilenames(self, spec, expected):\n    names = io.generate_sharded_filenames(spec)\n    self.assertEqual(names, expected)\n\n  def testGenerateShardedFilenamesManyShards(self):\n    names = io.generate_sharded_filenames(\'/dir/foo/bar@100000\')\n    self.assertEqual(len(names), 100000)\n    self.assertEqual(names[99999], \'/dir/foo/bar-099999-of-100000\')\n\n  @parameterized.named_parameters(\n      (\'no_spec\', \'/dir/foo/bar\'),\n      (\'zero_shards\', \'/dir/foo/bar@0\'),\n  )\n  def testGenerateShardedFilenamesError(self, spec):\n    self.assertRaises(io.ShardError, io.generate_sharded_filenames, spec)\n\n  @parameterized.named_parameters(\n      (\'basic\', \'/dir/foo/bar@3\', True),\n      (\'suffix\', \'/dir/foo/bar@3,txt\', True),\n      (\'many_shards\', \'/dir/foo/bar@123456\', True),\n      (\'invalid_spec\', \'/dir/foo/bar@0\', False),\n      (\'not_spec\', \'/dir/foo/bar\', False),\n  )\n  def testIsShardedFileSpec(self, spec, expected):\n    actual = io.is_sharded_file_spec(spec)\n    self.assertEqual(actual, expected,\n                      \'io.IshShardedFileSpec({0}) is {1} expected {2}\'.format(\n                          spec, actual, expected))\n\n  @parameterized.named_parameters(\n      (\'no_suffix\', \'/dir/foo/bar\', 3, \'\', \'/dir/foo/bar-?????-of-00003\'),\n      (\'suffix\', \'/dir/foo/bar\', 3, \'.txt\', \'/dir/foo/bar-?????-of-00003.txt\'),\n      (\'many\', \'/dir/foo/bar\', 1234567, \'.txt\',\n       \'/dir/foo/bar-???????-of-1234567.txt\'),\n  )\n  def testGenerateShardedFilePattern(self, basename, num_shards, suffix,\n                                     expected):\n\n    self.assertEqual(io.generate_sharded_file_pattern(\n        basename, num_shards, suffix), expected)\n\n  @parameterized.named_parameters(\n      (\'no_spec\', \'/dir/foo/bar\', \'/dir/foo/bar\'),\n      (\'suffix\', \'/dir/foo/bar@3.txt\', \'/dir/foo/bar-?????-of-00003.txt\'),\n      (\'no_suffix\', \'/dir/foo/bar@3\', \'/dir/foo/bar-?????-of-00003\'),\n      (\'1000\', \'/dir/foo/bar@1000\', \'/dir/foo/bar-?????-of-01000\'),\n      (\'many\', \'/dir/foo/bar@12345678\', \'/dir/foo/bar-????????-of-12345678\'),\n  )\n  def testNormalizeToShardedFilePattern(self, spec, expected):\n    self.assertEqual(expected, io.normalize_to_sharded_file_pattern(spec))\n\n  @parameterized.named_parameters(\n      (\'no_spec\', \'no_spec\', [\'no_spec\']),\n      (\'sharded\', \'sharded@3\', [\'sharded-00000-of-00003\',\n                                \'sharded-00001-of-00003\',\n                                \'sharded-00002-of-00003\']),\n      (\'wildcard1\', \'*.ext\', [\'cat.ext\', \'dog.ext\']),\n      (\'wildcard2\', \'fo?bar\', [\'foobar\']),\n      (\'comma_list\', \'file1,file2,file3\', [\'file1\', \'file2\', \'file3\']),\n      (\'mixed_list\', \'mixed.*txt,mixed@1,mixed_file\',\n       [\'mixed.1txt\', \'mixed.2txt\', \'mixed-00000-of-00001\', \'mixed_file\']),\n      (\'with_dups\', \'with_dups*\',\n       [\'with_dups.1txt\', \'with_dups.2txt\', \'with_dups-00000-of-00001\',\n        \'with_dups\']),\n  )\n  def testGlobListShardedFilePatterns(self, specs, expected_files):\n    # First, create all expected_files so Glob will work later.\n    expected_full_files = [test_utils.test_tmpfile(f, \'\')\n                           for f in expected_files]\n    # Create the full spec names. This one doesn\'t create the files.\n    full_specs = \',\'.join(\n        [test_utils.test_tmpfile(spec) for spec in specs.split(\',\')])\n    self.assertEqual(sorted(set(expected_full_files)),\n                     io.glob_list_sharded_file_patterns(full_specs))\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/tabix.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Creates tabix indices for VCFs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom third_party.nucleus.io.python import tabix_indexer\n\n\ndef build_index(path):\n  """"""Builds a tabix index for VCF at the specified path.""""""\n  tabix_indexer.tbx_index_build(path)\n\n\ndef build_csi_index(path, min_shift):\n  """"""Builds a csi index for VCF at the specified path.""""""\n  tabix_indexer.csi_index_build(path, min_shift)\n'"
third_party/nucleus/io/tabix_test.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for third_party.nucleus.io.tabix.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport os\nimport shutil\n\nfrom absl.testing import absltest\n\nfrom tensorflow.python.platform import gfile\nfrom third_party.nucleus.io import tabix\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\n\n\nclass TabixTest(absltest.TestCase):\n  """"""Test the functionality of tabix.build_index.""""""\n\n  def setUp(self):\n    super(TabixTest, self).setUp()\n    self.input_file = test_utils.genomics_core_testdata(\'test_samples.vcf.gz\')\n    self.output_file = test_utils.test_tmpfile(\'test_samples.vcf.gz\')\n    shutil.copyfile(self.input_file, self.output_file)\n    self.tbx_index_file = self.output_file + \'.tbi\'\n    self.csi_index_file = self.output_file + \'.csi\'\n\n  def tearDown(self):\n    super(TabixTest, self).tearDown()\n    os.remove(self.output_file)\n    try:\n      os.remove(self.tbx_index_file)\n    except OSError:\n      pass\n    try:\n      os.remove(self.csi_index_file)\n    except OSError:\n      pass\n\n  def test_build_index_tbx(self):\n    self.assertFalse(gfile.Exists(self.tbx_index_file))\n    tabix.build_index(self.output_file)\n    self.assertTrue(gfile.Exists(self.tbx_index_file))\n\n  def test_build_index_csi(self):\n    min_shift = 14\n    self.assertFalse(gfile.Exists(self.csi_index_file))\n    tabix.build_csi_index(self.output_file, min_shift)\n    self.assertTrue(gfile.Exists(self.csi_index_file))\n\n  def test_vcf_query_tbx(self):\n    tabix.build_index(self.output_file)\n    self.input_reader = vcf.VcfReader(self.input_file)\n    self.output_reader = vcf.VcfReader(self.output_file)\n\n    range1 = ranges.parse_literal(\'chr3:100,000-500,000\')\n    self.assertEqual(\n        list(self.input_reader.query(range1)),\n        list(self.output_reader.query(range1)))\n\n  def test_vcf_query_csi(self):\n    min_shift = 14\n    tabix.build_csi_index(self.output_file, min_shift)\n    self.input_reader = vcf.VcfReader(self.input_file)\n    self.output_reader = vcf.VcfReader(self.output_file)\n\n    range1 = ranges.parse_literal(\'chr3:100,000-500,000\')\n    self.assertEqual(\n        list(self.input_reader.query(range1)),\n        list(self.output_reader.query(range1)))\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/tfrecord.py,1,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""I/O for TFRecord files.\n\nUtilities for reading and writing TFRecord files, especially those containing\nserialized TensorFlow Example protocol buffers.\n""""""\n\n# Important: Please keep this module free of TensorFlow C++ extensions.\n# This makes it easy to build pure python packages for training that work with\n# CMLE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport heapq\n\nimport contextlib2\n\nfrom third_party.nucleus.io import genomics_reader\nfrom third_party.nucleus.io import genomics_writer\nfrom third_party.nucleus.io import sharded_file_utils\nfrom tensorflow.core.example import example_pb2\n\n\n# pylint: disable=invalid-name\ndef Reader(path, proto=None, compression_type=None):\n  """"""A TFRecordReader that defaults to tf.Example protos.""""""\n  if not proto:\n    proto = example_pb2.Example\n\n  return genomics_reader.TFRecordReader(\n      path, proto, compression_type=compression_type)\n\n\ndef Writer(path, compression_type=None):\n  """"""A convenience wrapper around genomics_writer.TFRecordWriter.""""""\n  return genomics_writer.TFRecordWriter(\n      path, compression_type=compression_type)\n# pylint: enable=invalid-name\n\n\n# redacted\ndef read_tfrecords(path, proto=None, max_records=None, compression_type=None):\n  """"""Yields the parsed records in a TFRecord file path.\n\n  Note that path can be sharded filespec (path@N) in which case this function\n  will read each shard in order; i.e. shard 0 will read each entry in order,\n  then shard 1, ...\n\n  Args:\n    path: String. A path to a TFRecord file containing protos.\n    proto: A proto class. proto.FromString() will be called on each serialized\n      record in path to parse it.\n    max_records: int >= 0 or None. Maximum number of records to read from path.\n      If None, the default, all records will be read.\n    compression_type: \'GZIP\', \'ZLIB\', \'\' (uncompressed), or None to autodetect\n      based on file extension.\n\n  Yields:\n    proto.FromString() values on each record in path in order.\n  """"""\n  if sharded_file_utils.is_sharded_file_spec(path):\n    paths = sharded_file_utils.generate_sharded_filenames(path)\n  else:\n    paths = [path]\n\n  i = 0\n  for path in paths:\n    with Reader(path, proto, compression_type=compression_type) as reader:\n      for record in reader.iterate():\n        i += 1\n        if max_records is not None and i > max_records:\n          return\n        yield record\n\n\ndef read_shard_sorted_tfrecords(path,\n                                key,\n                                proto=None,\n                                max_records=None,\n                                compression_type=None):\n  """"""Yields the parsed records in a TFRecord file path in sorted order.\n\n  The input TFRecord file must have each shard already in sorted order when\n  using the key function for comparison (but elements can be interleaved across\n  shards). Under those constraints, the elements will be yielded in a global\n  sorted order.\n\n  Args:\n    path: String. A path to a TFRecord-formatted file containing protos.\n    key: Callable. A function that takes as input a single instance of the proto\n      class and returns a value on which the comparison for sorted ordering is\n      performed.\n    proto: A proto class. proto.FromString() will be called on each serialized\n      record in path to parse it.\n    max_records: int >= 0 or None. Maximum number of records to read from path.\n      If None, the default, all records will be read.\n    compression_type: \'GZIP\', \'ZLIB\', \'\' (uncompressed), or None to autodetect\n      based on file extension.\n\n  Yields:\n    proto.FromString() values on each record in path in sorted order.\n  """"""\n  if sharded_file_utils.is_sharded_file_spec(path):\n    paths = sharded_file_utils.generate_sharded_filenames(path)\n  else:\n    paths = [path]\n\n  keyed_iterables = []\n  for path in paths:\n    protos = Reader(path, proto, compression_type=compression_type).iterate()\n    keyed_iterables.append(((key(elem), elem) for elem in protos))\n\n  for i, (_, value) in enumerate(heapq.merge(*keyed_iterables)):\n    if max_records is not None and i >= max_records:\n      return\n    yield value\n\n\ndef write_tfrecords(protos, output_path, compression_type=None):\n  """"""Writes protos to output_path.\n\n  This function writes serialized strings of each proto in protos to output_path\n  in their original order. If output_path is a sharded file (e.g., foo@2), this\n  function will write the protos spread out as evenly as possible among the\n  individual components of the sharded spec (e.g., foo-00000-of-00002 and\n  foo-00001-of-00002). Note that the order of records in the sharded files may\n  differ from the order in protos due to the striping.\n\n  Args:\n    protos: An iterable of protobufs. The objects we want to write out.\n    output_path: str. The filepath where we want to write protos.\n    compression_type: \'GZIP\', \'ZLIB\', \'\' (uncompressed), or None to autodetect\n      based on file extension.\n  """"""\n  if sharded_file_utils.is_sharded_file_spec(output_path):\n    with contextlib2.ExitStack() as stack:\n      _, n_shards, _ = sharded_file_utils.parse_sharded_file_spec(output_path)\n      writers = [\n          stack.enter_context(\n              Writer(sharded_file_utils.sharded_filename(\n                  output_path, i), compression_type=compression_type))\n          for i in range(n_shards)\n      ]\n      for i, proto in enumerate(protos):\n        writers[i % n_shards].write(proto)\n  else:\n    with Writer(output_path, compression_type=compression_type) as writer:\n      for proto in protos:\n        writer.write(proto)\n'"
third_party/nucleus/io/tfrecord_test.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for third_party.nucleus.io.tfrecord.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport types\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import tfrecord\n\nfrom third_party.nucleus.protos import reference_pb2\nfrom third_party.nucleus.testing import test_utils\n\n\nclass IOTest(parameterized.TestCase):\n\n  def write_test_protos(self, filename):\n    protos = [reference_pb2.ContigInfo(name=str(i)) for i in range(10)]\n    path = test_utils.test_tmpfile(filename)\n    tfrecord.write_tfrecords(protos, path)\n    return protos, path\n\n  @parameterized.parameters(\'foo.tfrecord\', \'foo@2.tfrecord\', \'foo@3.tfrecord\')\n  def test_read_write_tfrecords(self, filename):\n    protos, path = self.write_test_protos(filename)\n\n    # Create our generator of records from read_tfrecords.\n    reader = tfrecord.read_tfrecords(path, reference_pb2.ContigInfo)\n\n    # Make sure it\'s actually a generator.\n    self.assertEqual(type(reader), types.GeneratorType)\n\n    # Check the round-trip contents.\n    if \'@\' in filename:\n      # Sharded outputs are striped across shards, so order isn\'t preserved.\n      self.assertCountEqual(protos, reader)\n    else:\n      self.assertEqual(protos, list(reader))\n\n  @parameterized.parameters((filename, max_records)\n                            for max_records in [None, 0, 1, 3, 100]\n                            for filename in [\'foo.tfrecord\', \'foo@2.tfrecord\'])\n  def test_read_tfrecords_max_records(self, filename, max_records):\n    protos, path = self.write_test_protos(filename)\n\n    # Create our generator of records from read_tfrecords.\n    if max_records is None:\n      expected_n = len(protos)\n    else:\n      expected_n = min(max_records, len(protos))\n    actual = tfrecord.read_tfrecords(\n        path, reference_pb2.ContigInfo, max_records=max_records)\n    self.assertLen(list(actual), expected_n)\n\n  @parameterized.parameters(\'foo.tfrecord\', \'foo@2.tfrecord\', \'foo@3.tfrecord\')\n  def test_shard_sorted_tfrecords(self, filename):\n    protos, path = self.write_test_protos(filename)\n\n    # Create our generator of records.\n    key = lambda x: int(x.name)\n    reader = tfrecord.read_shard_sorted_tfrecords(\n        path, key=key, proto=reference_pb2.ContigInfo)\n\n    # Make sure it\'s actually a generator.\n    self.assertEqual(type(reader), types.GeneratorType)\n\n    # Check the round-trip contents.\n    contents = list(reader)\n    self.assertEqual(protos, contents)\n    self.assertEqual(contents, sorted(contents, key=key))\n\n  @parameterized.parameters((filename, max_records)\n                            for max_records in [None, 0, 1, 3, 100]\n                            for filename in [\'foo.tfrecord\', \'foo@2.tfrecord\'])\n  def test_shard_sorted_tfrecords_max_records(self, filename, max_records):\n    protos, path = self.write_test_protos(filename)\n\n    if max_records is None:\n      expected_n = len(protos)\n    else:\n      expected_n = min(max_records, len(protos))\n    # Create our generator of records from read_tfrecords.\n    actual = tfrecord.read_shard_sorted_tfrecords(\n        path,\n        key=lambda x: int(x.name),\n        proto=reference_pb2.ContigInfo,\n        max_records=max_records)\n    self.assertLen(list(actual), expected_n)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/vcf.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Classes for reading and writing VCF files.\n\nThe VCF format is described at\nhttps://samtools.github.io/hts-specs/VCFv4.3.pdf\n\nAPI for reading:\n\n```python\nfrom third_party.nucleus.io import vcf\n\nwith vcf.VcfReader(input_path) as reader:\n  for variant in reader:\n    print(variant)\n```\n\nAPI for writing:\n\n```python\nfrom third_party.nucleus.io import vcf\n\n# variants is an iterable of nucleus.genomics.v1.Variant protocol buffers.\nvariants = ...\n\nwith vcf.VcfWriter(output_path, header=header) as writer:\n  for variant in variants:\n    writer.write(variant)\n```\n\nThe class attempts to infer the file format (`TFRecord` vs VCF) from the file\npath provided to the constructor.\n\n1. For files that end with \'.tfrecord\' and \'.tfrecord.gz\' (a gzipped version),\n  a `TFRecord` file is assumed and is attempted to be read or written.\n\n2. For all other files, the VCF format will be used.\n\n  VCF format used in writing is inferred from file paths:\n    - ending in \'.bcf.gz\': BGZF compressed BCF format will be written;\n    - ending in \'.bcf\': uncompressed BCF format will be written;\n    - ending in \'.gz\' and not in \'.bcf.gz\': BGZP compressed VCF format will be\n        written;\n    - all other suffixes: uncompressed VCF format will be written.\n\n  VCF format used in reading is inferred from the contents of the file.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom third_party.nucleus.io import genomics_reader\nfrom third_party.nucleus.io import genomics_writer\nfrom third_party.nucleus.io.python import vcf_reader\nfrom third_party.nucleus.io.python import vcf_writer\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import ranges\nfrom third_party.nucleus.util import variant_utils\nfrom third_party.nucleus.util import vcf_constants\n\n\ndef _create_get_fn_cache(fields):\n  """"""Returns a dictionary from field to a callable that extracts its value.""""""\n  return {\n      field.id: vcf_constants.create_get_fn(field.type, field.number)\n      for field in fields\n  }\n\n\ndef _create_set_fn_cache(fields):\n  """"""Returns a dictionary from field to a callable that sets its value.""""""\n  return {field.id: vcf_constants.SET_FN_LOOKUP[field.type] for field in fields}\n\n\nclass VcfHeaderCache(object):\n  """"""This class creates a cache of accessors to structured fields in Variants.\n\n  The INFO and FORMAT fields within Variant protos are structured and typed,\n  with types defined by the corresponding VCF header. This cache object provides\n  provides {info,format}_field_{get,set}_fn functions that can be used to\n  extract information from the structured Variant protos based on the types\n  defined therein.\n\n  NOTE: Users should not need to interact with this class at all. It is used\n  by the variant_utils.{get,set}_info and variantcall_utils.{get,set}_format\n  functions for interacting with the INFO and FORMAT fields in a Variant proto.\n  """"""\n\n  def __init__(self, header):\n    """"""Initializer.\n\n    Args:\n      header: nucleus.genomics.v1.VcfHeader proto. Used to define the accessor\n        functions needed.\n    """"""\n    if header is None:\n      header = variants_pb2.VcfHeader()\n    self._info_get_cache = _create_get_fn_cache(header.infos)\n    self._info_set_cache = _create_set_fn_cache(header.infos)\n    self._format_get_cache = _create_get_fn_cache(header.formats)\n    self._format_set_cache = _create_set_fn_cache(header.formats)\n\n  def info_field_get_fn(self, field_name):\n    """"""Returns a callable that extracts the given INFO field based on its type.\n\n    Args:\n      field_name: str. The INFO field name of interest, e.g. \'AA\', \'DB\', \'AF\'.\n\n    Returns:\n      A callable used to extract the given INFO field from a Variant proto.\n    """"""\n    return self._info_get_cache[field_name]\n\n  def info_field_set_fn(self, field_name):\n    """"""Returns a callable that sets the given INFO field based on its type.""""""\n    return self._info_set_cache[field_name]\n\n  def format_field_get_fn(self, field_name):\n    """"""Returns a callable that gets the given FORMAT field based on its type.""""""\n    return self._format_get_cache[field_name]\n\n  def format_field_set_fn(self, field_name):\n    """"""Returns a callable that sets the given FORMAT field based on its type.""""""\n    return self._format_set_cache[field_name]\n\n\nclass NativeVcfReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from native VCF files.\n\n  Most users will want to use VcfReader instead, because it dynamically\n  dispatches between reading native VCF files and TFRecord files based\n  on the filename\'s extensions.\n  """"""\n\n  def __init__(self,\n               input_path,\n               excluded_info_fields=None,\n               excluded_format_fields=None,\n               store_gl_and_pl_in_info_map=False,\n               header=None):\n    """"""Initializer for NativeVcfReader.\n\n    Args:\n      input_path: str. The path to the VCF file to read.\n      excluded_info_fields: list(str). A list of INFO field IDs that should not\n        be parsed into the Variants. If None, all INFO fields are included.\n      excluded_format_fields: list(str). A list of FORMAT field IDs that should\n        not be parsed into the Variants. If None, all FORMAT fields are\n        included.\n      store_gl_and_pl_in_info_map: bool. If True, the ""GL"" and ""PL"" FORMAT\n        fields are stored in the VariantCall.info map rather than as top-level\n        values in the VariantCall.genotype_likelihood field.\n      header: If not None, specifies the variants_pb2.VcfHeader. The file at\n        input_path must not contain any header information.\n    """"""\n    super(NativeVcfReader, self).__init__()\n\n    options = variants_pb2.VcfReaderOptions(\n        excluded_info_fields=excluded_info_fields,\n        excluded_format_fields=excluded_format_fields,\n        store_gl_and_pl_in_info_map=store_gl_and_pl_in_info_map)\n    if header is not None:\n      self._reader = vcf_reader.VcfReader.from_file_with_header(\n          input_path.encode(\'utf8\'), options, header)\n    else:\n      self._reader = vcf_reader.VcfReader.from_file(\n          input_path.encode(\'utf8\'), options)\n\n    self.header = self._reader.header\n    self.field_access_cache = VcfHeaderCache(self.header)\n\n  def iterate(self):\n    """"""Returns an iterable of Variant protos in the file.""""""\n    return self._reader.iterate()\n\n  def query(self, region):\n    """"""Returns an iterator for going through variants in the region.""""""\n    return self._reader.query(region)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n  @property\n  def c_reader(self):\n    """"""Returns the underlying C++ reader.""""""\n    return self._reader\n\n\nclass VcfReader(genomics_reader.DispatchingGenomicsReader):\n  """"""Class for reading Variant protos from VCF or TFRecord files.""""""\n\n  def _native_reader(self, input_path, **kwargs):\n    return NativeVcfReader(input_path, **kwargs)\n\n  def _record_proto(self):\n    return variants_pb2.Variant\n\n  def _post_init_hook(self):\n    # Initialize field_access_cache.  If we are dispatching to a\n    # NativeVcfReader, we use its field_access_cache. Otherwise, we need to\n    # create a new one.\n    self.field_access_cache = getattr(\n        self._reader, \'field_access_cache\', VcfHeaderCache(self.header))\n\n  @property\n  def c_reader(self):\n    """"""Returns the underlying C++ reader.\n\n    Note that the C++ reader might be a VcfReader or it might be a\n    TFRecordReader, depending on the input_path\'s extension.\n    """"""\n    return self._reader.c_reader\n\n\nclass NativeVcfWriter(genomics_writer.GenomicsWriter):\n  """"""Class for writing to native VCF files.\n\n  Most users will want VcfWriter, which will write to either native VCF\n  files or TFRecords files, based on the output filename\'s extensions.\n  """"""\n\n  def __init__(self,\n               output_path,\n               header=None,\n               round_qualities=False,\n               excluded_info_fields=None,\n               excluded_format_fields=None,\n               retrieve_gl_and_pl_from_info_map=False,\n               exclude_header=False):\n    """"""Initializer for NativeVcfWriter.\n\n    Args:\n      output_path: str. The path to which to write the VCF file.\n      header: nucleus.genomics.v1.VcfHeader. The header that defines all\n        information germane to the constituent variants. This includes contigs,\n        FILTER fields, INFO fields, FORMAT fields, samples, and all other\n        structured and unstructured header lines.\n      round_qualities: bool. If True, the QUAL field is rounded to one point\n        past the decimal.\n      excluded_info_fields: list(str). A list of INFO field IDs that should not\n        be written to the output. If None, all INFO fields are included.\n      excluded_format_fields: list(str). A list of FORMAT field IDs that should\n        not be written to the output. If None, all FORMAT fields are included.\n      retrieve_gl_and_pl_from_info_map: bool. If True, the ""GL"" and ""PL"" FORMAT\n        fields are retrieved from the VariantCall.info map rather than from the\n        top-level value in the VariantCall.genotype_likelihood field.\n      exclude_header: bool. If True, write a headerless VCF.\n    """"""\n    super(NativeVcfWriter, self).__init__()\n\n    if header is None:\n      header = variants_pb2.VcfHeader()\n    writer_options = variants_pb2.VcfWriterOptions(\n        round_qual_values=round_qualities,\n        excluded_info_fields=excluded_info_fields,\n        excluded_format_fields=excluded_format_fields,\n        retrieve_gl_and_pl_from_info_map=retrieve_gl_and_pl_from_info_map,\n        exclude_header=exclude_header,\n    )\n    self._writer = vcf_writer.VcfWriter.to_file(output_path, header,\n                                                writer_options)\n    self.field_access_cache = VcfHeaderCache(header)\n\n  def write(self, proto):\n    self._writer.write(proto)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass VcfWriter(genomics_writer.DispatchingGenomicsWriter):\n  """"""Class for writing Variant protos to VCF or TFRecord files.""""""\n\n  def _native_writer(self,\n                     output_path,\n                     header,\n                     round_qualities=False,\n                     excluded_info_fields=None,\n                     excluded_format_fields=None,\n                     retrieve_gl_and_pl_from_info_map=False,\n                     exclude_header=False):\n    return NativeVcfWriter(\n        output_path,\n        header=header,\n        round_qualities=round_qualities,\n        excluded_info_fields=excluded_info_fields,\n        excluded_format_fields=excluded_format_fields,\n        retrieve_gl_and_pl_from_info_map=retrieve_gl_and_pl_from_info_map,\n        exclude_header=exclude_header)\n\n  def _post_init_hook(self):\n    # Initialize field_access_cache.  If we are dispatching to a\n    # NativeVcfWriter, we use its field_access_cache.  Otherwise, we\n    # need to create a new one.\n    self.field_access_cache = getattr(\n        self._writer, \'field_access_cache\', VcfHeaderCache(self.header))\n\n\nclass InMemoryVcfReader(genomics_reader.GenomicsReader):\n  """"""Class for ""reading"" Variant protos from an in-memory cache of variants.\n\n  ```python\n  from third_party.nucleus.io import vcf\n  from third_party.nucleus.protos import variants_pb2\n\n  variants = [... Variant protos ...]\n  header = variants_pb2.VcfHeader()\n\n  with vcf.InMemoryVcfReader(variants, header) as reader:\n    for variant in reader:\n      print(variant)\n  ```\n\n  This class accepts a collection of variants and optionally a header and\n  provides all of the standard API functions of VcfReader but instead of\n  fetching variants from a file the variants are queried from an in-memory cache\n  of variant protos.\n\n  Note that the input variants provided to this class aren\'t checked in any way,\n  and their ordering determines the order of variants emitted by this class for\n  the iterate() and query() operations. This is intentional, to make this class\n  easy to use for testing where you often want to use less-than-perfectly formed\n  inputs. In order to fully meet the contract of a standard VcfReader, variants\n  should be sorted by their contig ordering and then by their start and finally\n  by their ends.\n\n  Implementation note:\n    The current implementation will be very slow for query() if the provided\n    cache of variants is large, as we do a O(n) search to collect all of the\n    overlapping variants for each query. There are several straightforward\n    optimizations to do if we need/want to scale this up. (a) sort the variants\n    and use a binary search to find overlapping variants (b) partition the\n    variants by contig, so we have dict[contig] => [variants on contig], which\n    allows us to completely avoid considering any variants on any other contigs.\n    Neither of these optimizations are worth it if len(variants) is small, but\n    it may be worth considering if we want to use this functionality with a\n    large number of variants.\n  """"""\n\n  def __init__(self, variants, header=None):\n    """"""Creates a VCFReader backed by a collection of variants.\n\n    Args:\n      variants: list of nucleus.genomics.v1.Variant protos we will ""read""\n        from.\n      header: a VCFHeader object to provide as a result to calls to self.header,\n        or None, indicating that we don\'t have a header associated with this\n        reader.\n    """"""\n    super(InMemoryVcfReader, self).__init__()\n    self.variants = list(variants)\n    self.header = header\n\n  def iterate(self):\n    return iter(self.variants)\n\n  def query(self, region):\n    return iter(\n        variant for variant in self.variants\n        if ranges.ranges_overlap(variant_utils.variant_range(variant), region)\n    )\n'"
third_party/nucleus/io/vcf_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for third_party.nucleus.io.vcf.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom tensorflow.python.platform import gfile\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.protos import reference_pb2\nfrom third_party.nucleus.protos import struct_pb2\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\n\n\nclass VcfHeaderCacheTests(parameterized.TestCase):\n  """"""Test the functionality of the VcfHeaderCache class.""""""\n\n  def setUp(self):\n    self.vcf_reader = vcf.VcfReader(\n        test_utils.genomics_core_testdata(\'test_sites.vcf\'))\n    self.cache = self.vcf_reader.field_access_cache\n\n  @parameterized.parameters(\n      \'DP\',\n      \'AF\',\n      \'END\',\n      \'ExcessHet\',\n      \'culprit\',\n  )\n  def test_valid_info_get_funcs(self, field_name):\n    fn = self.cache.info_field_get_fn(field_name)\n    self.assertTrue(callable(fn))\n\n  @parameterized.parameters(\n      \'DP\',\n      \'AF\',\n      \'END\',\n      \'ExcessHet\',\n      \'culprit\',\n      \'HaplotypeScore\',\n      \'InbreedingCoeff\',\n  )\n  def test_valid_info_set_funcs(self, field_name):\n    fn = self.cache.info_field_set_fn(field_name)\n    self.assertTrue(callable(fn))\n\n  def test_invalid_info_funcs(self):\n    with self.assertRaises(KeyError):\n      self.cache.info_field_get_fn(\'RGQ\')\n    with self.assertRaises(KeyError):\n      self.cache.info_field_set_fn(\'PID\')\n\n  @parameterized.parameters(\n      \'AD\',\n      \'DP\',\n      \'PID\',\n      \'RGQ\',\n  )\n  def test_valid_format_get_funcs(self, field_name):\n    fn = self.cache.format_field_get_fn(field_name)\n    self.assertTrue(callable(fn))\n\n  @parameterized.parameters(\n      \'AD\',\n      \'DP\',\n      \'PID\',\n      \'RGQ\',\n  )\n  def test_valid_format_set_funcs(self, field_name):\n    fn = self.cache.format_field_set_fn(field_name)\n    self.assertTrue(callable(fn))\n\n  def test_invalid_format_funcs(self):\n    with self.assertRaises(KeyError):\n      self.cache.format_field_get_fn(\'culprit\')\n    with self.assertRaises(KeyError):\n      self.cache.format_field_set_fn(\'ExcessHet\')\n\n\nclass VcfReaderTests(absltest.TestCase):\n  """"""Test the iteration functionality provided by vcf.VcfReader.""""""\n\n  def setUp(self):\n    self.sites_reader = vcf.VcfReader(\n        test_utils.genomics_core_testdata(\'test_sites.vcf\'))\n\n    self.samples_reader = vcf.VcfReader(\n        test_utils.genomics_core_testdata(\'test_samples.vcf.gz\'))\n\n  def test_vcf_iterate(self):\n    self.assertEqual(test_utils.iterable_len(self.sites_reader.iterate()), 5)\n\n  def test_vcf_query(self):\n    range1 = ranges.parse_literal(\'chr3:100,000-500,000\')\n    self.assertEqual(\n        test_utils.iterable_len(self.samples_reader.query(range1)), 4)\n\n  def test_vcf_iter(self):\n    n = 0\n    for _ in self.sites_reader:\n      n += 1\n    self.assertEqual(n, 5)\n\n  def test_fail_multiple_concurrent_iterations(self):\n    range1 = ranges.parse_literal(\'chr3:100,000-500,000\')\n    reads = self.samples_reader.query(range1)\n    for read in reads:\n      pass\n\n    r2 = self.samples_reader.query(range1)\n    with self.assertRaisesRegexp(ValueError, \'No underlying iterable. This \'):\n      next(r2)\n\n  def test_c_reader(self):\n    self.assertNotEqual(self.sites_reader.c_reader, 0)\n    self.assertNotEqual(self.samples_reader.c_reader, 0)\n\n    tfrecord_reader = vcf.VcfReader(\n        test_utils.genomics_core_testdata(\'test_samples.vcf.golden.tfrecord\'))\n    self.assertNotEqual(tfrecord_reader.c_reader, 0)\n\n\ndef _format_expected_variant(ref, alts, format_spec, *samples):\n  base = [\'20\', 1, \'.\', ref, alts, 0, \'.\', \'.\', format_spec]\n  return base + list(samples)\n\n\ndef _format_test_variant(alleles, call_infos):\n  variant = test_utils.make_variant(chrom=\'20\', start=0, alleles=alleles)\n  for i, call_info in enumerate(call_infos):\n    call = variant.calls.add(call_set_name=\'sample\' + str(i))\n    for key, value in call_info.items():\n      if not isinstance(value, (list, tuple)):\n        value = [value]\n      call.info[key].values.extend(\n          [struct_pb2.Value(int_value=v) for v in value])\n  return variant\n\n\nclass VcfWriterTests(parameterized.TestCase):\n  """"""Tests for VcfWriter.""""""\n\n  def assertWrittenVCFRecordsEqual(self, path, expected_lines):\n\n    def cleanup_line(line):\n      if isinstance(line, (list, tuple)):\n        return \'\\t\'.join(str(x) for x in line)\n      else:\n        return line\n\n    expected_lines = [cleanup_line(line) for line in expected_lines]\n    with gfile.Open(path, \'r\') as fin:\n      self.assertEqual([\n          line.strip() for line in fin.readlines() if not line.startswith(\'#\')\n      ], expected_lines)\n\n  def write_variant_to_tempfile(self, variant):\n    output_path = test_utils.test_tmpfile(\'test.vcf\')\n    header = variants_pb2.VcfHeader(\n        contigs=[reference_pb2.ContigInfo(name=\'20\')],\n        sample_names=[call.call_set_name for call in variant.calls],\n        formats=[\n            variants_pb2.VcfFormatInfo(\n                id=\'DP\', number=\'1\', type=\'Integer\', description=\'Read depth\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'AD\',\n                number=\'R\',\n                type=\'Integer\',\n                description=\'Read depth for each allele\')\n        ])\n    writer = vcf.VcfWriter(output_path, header=header)\n    with writer:\n      writer.write(variant)\n    return output_path\n\n  @parameterized.parameters(\n      # Check that our DP field is getting written out properly.\n      (_format_test_variant([\'A\', \'T\'], [{\n          \'DP\': 1\n      }, {\n          \'DP\': 2\n      }]), _format_expected_variant(\'A\', \'T\', \'DP\', \'1\', \'2\')),\n      # Checks that we get the missing value when DP is missing in some samples.\n      (_format_test_variant([\'A\', \'T\'], [{\n          \'DP\': 1\n      }, {}]), _format_expected_variant(\'A\', \'T\', \'DP\', \'1\', \'.\')),\n      (_format_test_variant([\'A\', \'T\'], [{}, {\n          \'DP\': 2\n      }]), _format_expected_variant(\'A\', \'T\', \'DP\', \'.\', \'2\')),\n  )\n  def test_single_value_format_field(self, variant, expected_vcf_line):\n    self.assertWrittenVCFRecordsEqual(\n        self.write_variant_to_tempfile(variant), [expected_vcf_line])\n\n  @parameterized.parameters(\n      # Check that our AD field is getting written correctly.\n      (_format_test_variant([\'A\', \'T\'], [{\n          \'AD\': [0, 1]\n      }, {\n          \'AD\': [2, 3]\n      }]), _format_expected_variant(\'A\', \'T\', \'AD\', \'0,1\', \'2,3\')),\n      (_format_test_variant([\'A\', \'T\'], [{}, {\n          \'AD\': [2, 3]\n      }]), _format_expected_variant(\'A\', \'T\', \'AD\', \'.\', \'2,3\')),\n      (_format_test_variant([\'A\', \'T\'], [{\n          \'AD\': [0, 1]\n      }, {}]), _format_expected_variant(\'A\', \'T\', \'AD\', \'0,1\', \'.\')),\n      # Let\'s try a tri-allelic site where we have 3 AD values / sample.\n      (_format_test_variant([\'A\', \'T\', \'C\'], [{\n          \'AD\': [0, 1, 2]\n      }, {\n          \'AD\': [4, 5, 6]\n      }]), _format_expected_variant(\'A\', \'T,C\', \'AD\', \'0,1,2\', \'4,5,6\')),\n      # Check that we handle missing values properly.\n      (_format_test_variant([\'A\', \'T\', \'C\'], [{\n          \'AD\': [0, 1, 2]\n      }, {}]), _format_expected_variant(\'A\', \'T,C\', \'AD\', \'0,1,2\', \'.\')),\n      (_format_test_variant([\'A\', \'T\', \'C\'], [{}, {\n          \'AD\': [4, 5, 6]\n      }]), _format_expected_variant(\'A\', \'T,C\', \'AD\', \'.\', \'4,5,6\')),\n  )\n  def test_multi_value_format_field(self, variant, expected_vcf_line):\n    self.assertWrittenVCFRecordsEqual(\n        self.write_variant_to_tempfile(variant), [expected_vcf_line])\n\n  @parameterized.parameters(\n      # Now let\'s combine some AD and DP fields.\n      (_format_test_variant([\'A\', \'T\', \'C\'], [{\n          \'DP\': 3,\n          \'AD\': [0, 1, 2]\n      }, {\n          \'DP\': 12,\n          \'AD\': [3, 4, 5]\n      }]), _format_expected_variant(\'A\', \'T,C\', \'DP:AD\', \'3:0,1,2\', \'12:3,4,5\')\n      ),\n      (_format_test_variant([\'A\', \'T\', \'C\'], [{\n          \'DP\': 3\n      }, {\n          \'AD\': [3, 4, 5]\n      }]), _format_expected_variant(\'A\', \'T,C\', \'DP:AD\', \'3:.\', \'.:3,4,5\')),\n  )\n  def test_multiple_format_fields(self, variant, expected_vcf_line):\n    self.assertWrittenVCFRecordsEqual(\n        self.write_variant_to_tempfile(variant), [expected_vcf_line])\n\n\nclass VcfWriterHeaderlessTests(absltest.TestCase):\n  """"""Tests for VcfWriter with exclude_header=True.""""""\n\n  def test_headerless_vcf(self):\n    """"""Writes a headerless vcf and reads it back out.""""""\n    test_vcf = test_utils.genomics_core_testdata(\'test_sites.vcf\')\n    output_vcf = test_utils.test_tmpfile(\'output.vcf\')\n    expected_variants = []\n    with vcf.VcfReader(test_vcf) as reader:\n      with vcf.VcfWriter(\n          output_vcf, header=reader.header, exclude_header=True) as writer:\n        for record in reader:\n          expected_variants.append(record)\n          writer.write(record)\n\n      with vcf.VcfReader(output_vcf, header=reader.header) as actual_reader:\n        self.assertEqual(expected_variants, list(actual_reader))\n\n\nclass VcfRoundtripTests(parameterized.TestCase):\n  """"""Test the ability to round-trip VCF files.""""""\n\n  def setUp(self):\n    self.header = (\n        \'##fileformat=VCFv4.2\\n##FILTER=<ID=PASS,Description=""All filters \'\n        \'passed"">\\n##INFO=<ID=DB,Number=0,Type=Flag,Description=""In \'\n        \'dbSNP"">\\n##INFO=<ID=MIN_DP,Number=1,Type=Integer,Description=""Min \'\n        \'DP"">\\n##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">\\n##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic\'\n        \' depths"">\\n##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read \'\n        \'depth"">\\n##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Genotype \'\n        \'likelihood,Phred-encoded"">\\n##contig=<ID=chr1,length=248956422>\\n#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\tFORMAT\\tS1\\tS2\\n\')\n    self.record_format_strings = [\n        \'chr1\\t13613\\t.\\tT\\tA\\t39.88\\tPASS\\t{info}\\t{fmt}\\t0/1{efmts1}\\t1/1{efmts2}\\n\',\n        \'chr1\\t13813\\trs1\\tT\\tG\\t90.28\\tPASS\\t{info}\\t{fmt}\\t1/1{efmts1}\\t0|1{efmts2}\\n\',\n        \'chr1\\t13838\\t.\\tC\\tT\\t62.74\\tPASS\\t{info}\\t{fmt}\\t0/1{efmts1}\\t0/1{efmts2}\\n\',\n    ]\n\n  @parameterized.parameters(\n      dict(\n          expected_infos=[\'DB;MIN_DP=4\', \'MIN_DP=15\', \'DB;MIN_DP=10\'],\n          expected_fmt=\'GT:AD:DP:PL\',\n          expected_fmt1=[\n              \':1,3:4:10,5,0\', \':11,13:24:55,0,50\', \':5,5:10:20,0,20\'\n          ],\n          expected_fmt2=[\n              \':1,19:20:100,90,0\', \':7,8:15:15,0,12\', \':.:10:0,0,50\'\n          ],\n      ),\n      dict(\n          expected_infos=[\'DB\', \'.\', \'DB\'],\n          expected_fmt=\'GT:AD:DP:PL\',\n          expected_fmt1=[\n              \':1,3:4:10,5,0\', \':11,13:24:55,0,50\', \':5,5:10:20,0,20\'\n          ],\n          expected_fmt2=[\n              \':1,19:20:100,90,0\', \':7,8:15:15,0,12\', \':.:10:0,0,50\'\n          ],\n          reader_excluded_info=[\'MIN_DP\'],\n      ),\n      dict(\n          expected_infos=[\'DB\', \'.\', \'DB\'],\n          expected_fmt=\'GT\',\n          expected_fmt1=[\'\', \'\', \'\'],\n          expected_fmt2=[\'\', \'\', \'\'],\n          reader_excluded_info=[\'MIN_DP\'],\n          reader_excluded_format=[\'AD\', \'DP\', \'PL\'],\n      ),\n      dict(\n          expected_infos=[\'DB\', \'.\', \'DB\'],\n          expected_fmt=\'GT\',\n          expected_fmt1=[\'\', \'\', \'\'],\n          expected_fmt2=[\'\', \'\', \'\'],\n          writer_excluded_info=[\'MIN_DP\'],\n          writer_excluded_format=[\'AD\', \'DP\', \'PL\'],\n      ),\n      dict(\n          expected_infos=[\'DB\', \'.\', \'DB\'],\n          expected_fmt=\'GT\',\n          expected_fmt1=[\'\', \'\', \'\'],\n          expected_fmt2=[\'\', \'\', \'\'],\n          reader_excluded_info=[\'MIN_DP\'],\n          reader_excluded_format=[\'AD\'],\n          writer_excluded_info=[\'MIN_DP\'],\n          writer_excluded_format=[\'DP\', \'PL\'],\n      ),\n  )\n  def test_roundtrip(self,\n                     expected_infos,\n                     expected_fmt,\n                     expected_fmt1,\n                     expected_fmt2,\n                     reader_excluded_info=None,\n                     reader_excluded_format=None,\n                     writer_excluded_info=None,\n                     writer_excluded_format=None):\n    expected_records = [\n        record.format(info=info, fmt=expected_fmt, efmts1=e1,\n                      efmts2=e2) for record, info, e1, e2 in zip(\n                          self.record_format_strings, expected_infos,\n                          expected_fmt1, expected_fmt2)\n    ]\n    expected = self.header + \'\'.join(expected_records)\n    for info_map_pl in [False, True]:\n      with vcf.VcfReader(\n          test_utils.genomics_core_testdata(\'test_py_roundtrip.vcf\'),\n          excluded_info_fields=reader_excluded_info,\n          excluded_format_fields=reader_excluded_format,\n          store_gl_and_pl_in_info_map=info_map_pl) as reader:\n        records = list(reader.iterate())\n        output_path = test_utils.test_tmpfile(\n            \'test_roundtrip_tmpfile_{}.vcf\'.format(info_map_pl))\n        with vcf.VcfWriter(\n            output_path,\n            header=reader.header,\n            excluded_info_fields=writer_excluded_info,\n            excluded_format_fields=writer_excluded_format,\n            retrieve_gl_and_pl_from_info_map=info_map_pl) as writer:\n          for record in records:\n            writer.write(record)\n\n      with open(output_path) as f:\n        actual = f.read()\n      self.assertEqual(actual, expected)\n\n\nclass InMemoryVcfReaderTests(parameterized.TestCase):\n  """"""Test the functionality provided by vcf.InMemoryVcfReader.""""""\n\n  def setUp(self):\n    self.variants = [\n        test_utils.make_variant(chrom=\'1\', start=10),\n        test_utils.make_variant(chrom=\'1\', start=20),\n        test_utils.make_variant(chrom=\'1\', start=30),\n        test_utils.make_variant(chrom=\'2\', start=25),\n        test_utils.make_variant(chrom=\'2\', start=55),\n        test_utils.make_variant(chrom=\'3\', start=10),\n    ]\n    self.header = variants_pb2.VcfHeader(\n        contigs=[\n            reference_pb2.ContigInfo(name=\'1\', n_bases=100),\n            reference_pb2.ContigInfo(name=\'2\', n_bases=100),\n            reference_pb2.ContigInfo(name=\'3\', n_bases=100),\n            reference_pb2.ContigInfo(name=\'4\', n_bases=100),\n        ],\n        filters=[],\n        sample_names=[\'NA12878\'])\n    self.reader = vcf.InMemoryVcfReader(\n        self.variants, self.header)\n\n  def test_iterate(self):\n    """"""Tests that iterate returns an iterable containing our variants.""""""\n    self.assertEqual(list(self.reader.iterate()), self.variants)\n\n  def test_header(self):\n    """"""Tests that the reader provides us back the header we gave it.""""""\n    self.assertEqual(self.reader.header, self.header)\n\n  @parameterized.parameters(\n      dict(query=\'1\', expected_variant_indices=[0, 1, 2]),\n      dict(query=\'2\', expected_variant_indices=[3, 4]),\n      dict(query=\'3\', expected_variant_indices=[5]),\n      dict(query=\'4\', expected_variant_indices=[]),\n      dict(query=\'1:1-15\', expected_variant_indices=[0]),\n      dict(query=\'1:1-25\', expected_variant_indices=[0, 1]),\n      dict(query=\'1:1-35\', expected_variant_indices=[0, 1, 2]),\n      dict(query=\'1:15-35\', expected_variant_indices=[1, 2]),\n      dict(query=\'1:25-35\', expected_variant_indices=[2]),\n  )\n  def test_query(self, query, expected_variant_indices):\n    range1 = ranges.parse_literal(query, ranges.contigs_dict(\n        self.header.contigs))\n    self.assertEqual(\n        list(self.reader.query(range1)),\n        [self.variants[i] for i in expected_variant_indices])\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/pip_package/setup.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Fake setup.py module for installing Nucleus.\n\nUsually, setup.py is invoked twice:  first, to build the pip package\nand second to install it.\n\nThis setup.py is only used for installation; build_pip_package.sh is\nused to create the package.  We do it this way because we need our\npackage to include symbolic links, which normal setup.py doesn\'t\nsupport.\n\nFor the same reason, this setup.py is not implemented using setuptools.\nInstead, we directly implement the four commands run by pip install\n(https://pip.pypa.io/en/stable/reference/pip_install/#id46):\n  * setup.py egg_info [--egg-base XXX]\n  * setup.py install --record XXX [--single-version-externally-managed]\n          [--root XXX] [--compile|--no-compile] [--install-headers XXX]\n  * setup.py bdist_wheel -d XXX\n  * setup.py clean\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom distutils import dist\nimport distutils.command.install as dist_install\nimport glob\nimport os\nimport shutil\nimport sys\n\n\ndef touch(fname):\n  open(fname, \'w+\').close()\n\n\ndef find_destination(is_user):\n  """"""Returns the directory we are supposed to install into.""""""\n  install_cmd = dist_install.install(dist.Distribution())\n  install_cmd.finalize_options()\n  if is_user:\n    return install_cmd.install_usersite\n  else:\n    return install_cmd.install_platlib\n\n\ndef main():\n  if len(sys.argv) < 2:\n    print(\'Missing command\')\n    sys.exit(1)\n\n  cmd = sys.argv[1]\n  args = sys.argv[2:]\n\n  if cmd == \'egg_info\':\n    egg_srcs = glob.glob(\'google_nucleus-*-py*.egg-info\')\n    if not egg_srcs:\n      print(\'Could not find source .egg-info directory\')\n      sys.exit(1)\n    egg_src = egg_srcs[0]\n\n    egg_dir = \'google_nucleus.egg-info\'\n    if len(args) > 1 and args[0] == \'--egg-base\':\n      egg_dir = os.path.join(args[1], egg_dir)\n\n    print(\'Copying egg-info from \', egg_src, \' to \', egg_dir)\n    shutil.copytree(egg_src, egg_dir)\n    sys.exit(0)\n\n  if cmd == \'install\':\n    destination = find_destination(\'--user\' in args)\n\n    record_file = \'install-record.txt\'\n    if \'--record\' in args:\n      i = args.index(\'--record\')\n      record_file = args[i+1]\n\n    print(\'Removing old protobuf files\')\n    os.system(\'rm -fR \' + destination + \'/google/protobuf\')\n\n    print(\'Installing Nucleus to \' + destination\n          + \' with record file at \' + record_file)\n    os.system(\'cp -R -v google nucleus \' + destination\n              + "" | awk \'{print substr($3,2,length($3)-2)}\' > "" + record_file)\n\n    sys.exit(0)\n\n  if cmd == \'bdist_wheel\':\n    print(\'This package does not support wheel creation.\')\n    sys.exit(1)\n\n  if cmd == \'clean\':\n    # Nothing to do\n    sys.exit(0)\n\n  print(\'Unknown command: \', cmd)\n  sys.exit(1)\n\n\nif __name__ == \'__main__\':\n  main()\n\n'"
third_party/nucleus/protos/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n'"
third_party/nucleus/testing/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n'"
third_party/nucleus/testing/protobuf_implementation_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Test that our protobuf implementation behaves as we\'d expect.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\n\nfrom google.protobuf.internal import api_implementation\n# This next import is unused, but we are testing that any program\n# which includes a Nucleus library uses the cpp protobuf\n# implementation.\n# pylint: disable=unused-import\nfrom third_party.nucleus.io import sam\n\n\nclass ProtobufImplementationTest(absltest.TestCase):\n  """"""Checks that our protobufs have the properties we expect.""""""\n\n  def test_protobuf_uses_fast_cpp(self):\n    """"""Checks that we are using the fast cpp version of python protobufs.""""""\n    self.assertEqual(api_implementation.Type(), \'cpp\')\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/testing/test_utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Utilities to help with testing code.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import flags\nfrom absl.testing import absltest\nimport six\n\nfrom tensorflow.python.platform import gfile\nfrom third_party.nucleus.protos import position_pb2\nfrom third_party.nucleus.protos import reads_pb2\nfrom third_party.nucleus.protos import struct_pb2\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import cigar as _cigar\n\nFLAGS = flags.FLAGS\n\n# In the OSS version these will be \'\'.\nDATADIR = \'\'\nDEFAULT_WORKSPACE = \'\'\n\n# In the OSS version this becomes \'nucleus/testdata\'\nRELATIVE_TESTDATA_PATH = \'third_party/nucleus/testdata\'\n\n\ndef genomics_testdata(path, datadir=DATADIR):\n  """"""Gets the path to a testdata file in genomics at relative path.\n\n  Args:\n    path: A path to a testdata file *relative* to the genomics root\n      directory. For example, if you have a test file in\n      ""datadir/nucleus/testdata/foo.txt"", path should be\n      ""nucleus/testdata/foo.txt"" to get a path to it.\n    datadir: The path of the genomics root directory *relative* to\n      the testing source directory.\n\n  Returns:\n    The absolute path to a testdata file.\n  """"""\n  if hasattr(FLAGS, \'test_srcdir\'):\n    # Google code uses FLAG.test_srcdir\n    # TensorFlow uses a routine googletest.test_src_dir_path.\n    test_workspace = os.environ.get(\'TEST_WORKSPACE\', DEFAULT_WORKSPACE)\n    test_srcdir = os.path.join(FLAGS.test_srcdir, test_workspace)\n  else:\n    # In bazel TEST_SRCDIR points at the runfiles directory, and\n    # TEST_WORKSPACE names the workspace.  We need to append to the\n    # path the name of the workspace in order to get to the root of our\n    # source tree.\n    test_workspace = os.environ[\'TEST_WORKSPACE\']\n    test_srcdir = os.path.join(os.environ[\'TEST_SRCDIR\'], test_workspace)\n  return os.path.join(test_srcdir, datadir, path)\n\n\n# redacted\ndef genomics_core_testdata(filename):\n  """"""Gets the path to a testdata named filename in util/testdata.\n\n  Args:\n    filename: The name of a testdata file in the core genomics testdata\n      directory. For example, if you have a test file in\n      ""third_party/nucleus/util/testdata/foo.txt"", filename should be\n      ""foo.txt"" to get a path to it.\n\n  Returns:\n    The absolute path to a testdata file.\n  """"""\n  return genomics_testdata(os.path.join(RELATIVE_TESTDATA_PATH, filename))\n\n\ndef test_tmpfile(name, contents=None):\n  """"""Returns a path to a tempfile named name in the test_tmpdir.\n\n  Args:\n    name: str; the name of the file, should not contain any slashes.\n    contents: bytes, or None. If not None, tmpfile\'s contents will be set to\n      contents before returning the path.\n\n  Returns:\n    str path to a tmpfile with filename name in our test tmpfile directory.\n  """"""\n  path = os.path.join(absltest.get_default_test_tmpdir(), name)\n  if contents is not None:\n    with gfile.Open(path, \'wb\') as fout:\n      fout.write(contents)\n  return path\n\n\ndef set_list_values(list_value, values):\n  """"""Sets a ListValue to have the values in values.""""""\n\n  def format_one(value):\n    if isinstance(value, str):\n      return struct_pb2.Value(string_value=value)\n    elif isinstance(value, float):\n      return struct_pb2.Value(number_value=value)\n    elif isinstance(value, six.integer_types):\n      return struct_pb2.Value(int_value=value)\n    else:\n      raise ValueError(\'Unsupported type \', value)\n\n  del list_value.values[:]\n  list_value.values.extend([format_one(value) for value in values])\n  # list_value.values.extend(vals)\n\n\ndef make_variant(chrom=\'chr1\',\n                 start=10,\n                 alleles=None,\n                 end=None,\n                 filters=None,\n                 qual=None,\n                 gt=None,\n                 gq=None,\n                 sample_name=None,\n                 gls=None,\n                 is_phased=None):\n  """"""Creates a new Variant proto from args.\n\n  Args:\n    chrom: str. The reference_name for this variant.\n    start: int. The starting position of this variant.\n    alleles: list of str with at least one element. alleles[0] is the reference\n      bases and alleles[1:] will be set to alternate_bases of variant. If None,\n      defaults to [\'A\', \'C\'].\n    end: int or None. If not None, the variant\'s end will be set to this value.\n      If None, will be set to the start + len(reference_bases).\n    filters: str, list of str, or None. Sets the filters field of the variant to\n      this value if not None. If filters is a string `value`, this is equivalent\n      to an argument [`value`]. If None, no value will be assigned to the\n      filters field.\n    qual: int or None. The quality score for this variant. If None, no quality\n      score will be written in the Variant.\n    gt: A list of ints, or None. If present, creates a VariantCall in Variant\n      with genotype field set to this value. The special \'DEFAULT\' value, if\n      provided, will set the genotype to [0, 1]. This is the default behavior.\n    gq: int or None. If not None and gt is not None, we will add an this GQ\n      value to our VariantCall.\n    sample_name: str or None. If not None and gt is not None, sets the\n      call_set_name of our VariantCall to this value.\n    gls: array-list of float, or None. If not None and gt is not None, sets the\n      genotype_likelihoods of our VariantCall to this value.\n    is_phased: bool. Indicates whether a VariantCall should be phased.\n\n  Returns:\n    nucleus.genomics.v1.Variant proto.\n  """"""\n  return make_variant_multiple_calls(\n      chrom=chrom,\n      start=start,\n      alleles=alleles,\n      end=end,\n      filters=filters,\n      qual=qual,\n      gts=None if gt is None else [gt],\n      gqs=None if gq is None else [gq],\n      sample_names=None if sample_name is None else [sample_name],\n      glss=None if gls is None else [gls],\n      is_phased=None if is_phased is None else [is_phased])\n\n\ndef make_variant_multiple_calls(chrom=\'chr1\',\n                                start=10,\n                                alleles=None,\n                                end=None,\n                                filters=None,\n                                qual=None,\n                                gts=None,\n                                gqs=None,\n                                sample_names=None,\n                                glss=None,\n                                is_phased=None):\n  """"""Creates a new Variant proto from args that contains multi-sample calls.\n\n  Args:\n    chrom: str. The reference_name for this variant.\n    start: int. The starting position of this variant.\n    alleles: list of str with at least one element. alleles[0] is the reference\n      bases and alleles[1:] will be set to alternate_bases of variant. If None,\n        defaults to [\'A\', \'C\'].\n    end: int or None. If not None, the variant\'s end will be set to this value.\n      If None, will be set to the start + len(reference_bases).\n    filters: str, list of str, or None. Sets the filters field of the variant to\n      this value if not None. If filters is a string `value`, this is equivalent\n      to an argument [`value`]. If None, no value will be assigned to the\n      filters field.\n    qual: int or None. The quality score for this variant. If None, no quality\n      score will be written in the Variant.\n    gts: A list of lists of ints. For each list in this list, creates a\n      VariantCall in Variant with genotype field set to this value.\n    gqs: A list of ints or None. Must match the gts arg if specified. Sets the\n      GQ value of corresponding VariantCall.\n    sample_names: A list of strs or None. Must match the gts arg if specified.\n      Sets the call_set_name of the corresponding VariantCall.\n    glss: A list of array-lists of float, or None. Must match the gts arg if\n      specified. Sets the genotype_likelihoods of the corresponding VariantCall.\n    is_phased: list of bools. Must match the gts arg if specified. Indicates\n      whether the corresponding VariantCall should be phased.\n\n  Returns:\n    nucleus.genomics.v1.Variant proto.\n  """"""\n  if alleles is None:\n    alleles = [\'A\', \'C\']\n\n  if not end:\n    end = start + len(alleles[0])\n\n  variant = variants_pb2.Variant(\n      reference_name=chrom,\n      start=start,\n      end=end,\n      reference_bases=alleles[0],\n      alternate_bases=alleles[1:],\n      quality=qual,\n  )\n\n  if filters is not None:\n    if not isinstance(filters, (list, tuple)):\n      filters = [filters]\n    variant.filter[:] = filters\n\n  if gts:\n    for i in range(len(gts)):\n      call = variant.calls.add(genotype=gts[i])\n\n      if sample_names and sample_names[i] is not None:\n        call.call_set_name = sample_names[i]\n\n      if gqs and gqs[i] is not None:\n        set_list_values(call.info[\'GQ\'], [gqs[i]])\n\n      if glss and glss[i] is not None:\n        call.genotype_likelihood.extend(glss[i])\n\n      if is_phased and is_phased[i] is not None:\n        call.is_phased = is_phased[i]\n\n  return variant\n\n\ndef make_read(bases,\n              start,\n              quals=None,\n              cigar=None,\n              mapq=50,\n              chrom=\'chr1\',\n              name=None):\n  """"""Makes a nucleus.genomics.v1.Read for testing.""""""\n  if quals and len(bases) != len(quals):\n    raise ValueError(\'Incompatable bases and quals\', bases, quals)\n  read = reads_pb2.Read(\n      fragment_name=name if name else \'read_\' + str(make_read.counter),\n      proper_placement=True,\n      read_number=1,\n      number_reads=2,\n      aligned_sequence=bases,\n      aligned_quality=quals,\n      alignment=reads_pb2.LinearAlignment(\n          position=position_pb2.Position(reference_name=chrom, position=start),\n          mapping_quality=mapq,\n          cigar=_cigar.to_cigar_units(cigar) if cigar else []))\n  make_read.counter += 1\n  return read\nmake_read.counter = 0\n\n\ndef cc_iterable_len(cc_iterable):\n  """"""Count the number of elements in an Iterable object.\n\n  Args:\n    cc_iterable: a CLIF-wrap of a subclass of the C++ Iterable class.\n\n  Returns:\n    integer count\n  """"""\n  count = 0\n  while True:\n    not_done, _ = cc_iterable.Next()\n    if not not_done:\n      break\n    count += 1\n  return count\n\n\ndef iterable_len(iterable):\n  """"""Returns the length of a Python iterable, by advancing it.""""""\n  return sum(1 for _ in iterable)\n\n\n# redacted\ndef assert_not_called_workaround(mock):\n  """"""Asserts that a mock has not been called.\n\n  There\'s a bug in mock where some of the assert functions on a mock are being\n  dropped when that mock is created with an autospec:\n\n    https://bugs.python.org/issue28380\n\n  The mock 2.0.0 backport doesn\'t have the fix yet. The required patch is:\n\n    https://bugs.python.org/file44991/fix_autospecced_mock_functions.patch\n\n  but the current mock (checked 07/22/17) backport code is missing the fix:\n\n    https://github.com/testing-cabal/mock/blob/master/mock/mock.py#L315\n\n  This is an open issue on the mock github repo:\n\n    https://github.com/testing-cabal/mock/issues/398\n\n  And they claim that it\'ll be a few months (as of April 2017) before it is\n  incorporated into the backport.\n\n  Args:\n    mock: The mock to assert hasn\'t been called.\n\n  Raises:\n    AssertionError: mock has been called.\n  """"""\n  if mock.call_count != 0:\n    raise AssertionError(""Expected no calls to \'{}\' but was called {} times""\n                         .format(mock.name, mock.call_count))\n\n\n# redacted\ndef assert_called_once_workaround(mock):\n  """"""Asserts that a mock has been called exactly once.\n\n  See assert_not_called_workaround for the backstory on why this function\n  exists.\n\n  Args:\n    mock: The mock that should have been called exactly once.\n\n  Raises:\n    AssertionError: mock wasn\'t called exactly once.\n  """"""\n  if mock.call_count != 1:\n    raise AssertionError(\n        ""Expected exactly one call to \'{}\' but was called {} times"".format(\n            mock.name, mock.call_count))\n'"
third_party/nucleus/testing/test_utils_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for nucleus\'s testing.test_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\n\nfrom third_party.nucleus.protos import cigar_pb2\nfrom third_party.nucleus.testing import test_utils\n\n\nclass TestUtilsTests(absltest.TestCase):\n\n  def test_make_read(self):\n    bases = \'ACG\'\n    quals = [30, 40, 50]\n    cigar = \'3M\'\n    mapq = 42\n    chrom = \'chr10\'\n    start = 123\n    name = \'myname\'\n    read = test_utils.make_read(\n        bases,\n        quals=quals,\n        cigar=cigar,\n        mapq=mapq,\n        chrom=chrom,\n        start=start,\n        name=name)\n\n    self.assertEqual(read.aligned_sequence, bases)\n    self.assertEqual(read.aligned_quality, quals)\n    self.assertEqual(list(read.alignment.cigar), [\n        cigar_pb2.CigarUnit(\n            operation_length=3, operation=cigar_pb2.CigarUnit.ALIGNMENT_MATCH)\n    ])\n    self.assertEqual(read.alignment.mapping_quality, mapq)\n    self.assertEqual(read.alignment.position.reference_name, chrom)\n    self.assertEqual(read.alignment.position.position, start)\n    self.assertEqual(read.fragment_name, name)\n\n  def test_make_read_produces_unique_read_names(self):\n    start = 0\n    read1 = test_utils.make_read(\'A\', start=start)\n    read2 = test_utils.make_read(\'A\', start=start)\n    self.assertGreater(len(read1.fragment_name), 0)\n    self.assertGreater(len(read2.fragment_name), 0)\n    self.assertNotEqual(read1.fragment_name, read2.fragment_name)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/tools/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n'"
third_party/nucleus/util/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n'"
third_party/nucleus/util/cigar.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Utility functions for working with alignment CIGAR operations.\n\nThe CIGAR format is defined within the SAM spec, available at\nhttps://samtools.github.io/hts-specs/SAMv1.pdf\n\nThis module provides utility functions for interacting with the parsed\nrepresentations of CIGAR strings.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport six\nfrom third_party.nucleus.protos import cigar_pb2\n\n# A frozenset of all CigarUnit.Operation enum values that advance the alignment\n# with respect to the reference genome.\nREF_ADVANCING_OPS = frozenset([\n    cigar_pb2.CigarUnit.ALIGNMENT_MATCH, cigar_pb2.CigarUnit.SEQUENCE_MATCH,\n    cigar_pb2.CigarUnit.DELETE, cigar_pb2.CigarUnit.SKIP,\n    cigar_pb2.CigarUnit.SEQUENCE_MISMATCH\n])\n\n# A map from CigarUnit.Operation (e.g., CigarUnit.ALIGNMENT_MATCH) enum values\n# to their corresponding single character cigar codes (e.g., \'M\').\nCIGAR_OPS_TO_CHAR = {\n    cigar_pb2.CigarUnit.ALIGNMENT_MATCH: \'M\',\n    cigar_pb2.CigarUnit.INSERT: \'I\',\n    cigar_pb2.CigarUnit.DELETE: \'D\',\n    cigar_pb2.CigarUnit.SKIP: \'N\',\n    cigar_pb2.CigarUnit.CLIP_SOFT: \'S\',\n    cigar_pb2.CigarUnit.CLIP_HARD: \'H\',\n    cigar_pb2.CigarUnit.PAD: \'P\',\n    cigar_pb2.CigarUnit.SEQUENCE_MATCH: \'=\',\n    cigar_pb2.CigarUnit.SEQUENCE_MISMATCH: \'X\',\n}\n\n# A map from single character cigar codes (e.g., \'M\') to their corresponding\n# CigarUnit.Operation (e.g., CigarUnit.ALIGNMENT_MATCH) enum values.\nCHAR_TO_CIGAR_OPS = {v: k for k, v in CIGAR_OPS_TO_CHAR.items()}\n\n# All of the CigarUnit.Operation values in a frozen set.\nALL_CIGAR_OPS = frozenset(CIGAR_OPS_TO_CHAR.keys())\n\n# Regular expression that matches only valid full cigar strings.\nVALID_CIGAR_RE = re.compile(\n    r\'^(\\d+[\' + \'\'.join(CHAR_TO_CIGAR_OPS.keys()) + \'])+$\')\n\n# Regular expression that matches a single len/op cigar element. The match is\n# grouped, so CIGAR_STR_SPLITTER_RE.finditer(cigar_str) returns grouped units\n# of the cigar string in order.\nCIGAR_STR_SPLITTER_RE = re.compile(\n    r\'(\\d+[\' + \'\'.join(CHAR_TO_CIGAR_OPS.keys()) + \'])\')\n\n\ndef format_cigar_units(cigar_units):\n  """"""Returns the string version of an iterable of CigarUnit protos.\n\n  Args:\n    cigar_units: iterable[CigarUnit] protos.\n\n  Returns:\n    A string representation of the CigarUnit protos that conforms to the\n    CIGAR string specification.\n  """"""\n  return \'\'.join(\n      str(unit.operation_length) + CIGAR_OPS_TO_CHAR[unit.operation]\n      for unit in cigar_units)\n\n\ndef parse_cigar_string(cigar_str):\n  """"""Parse a cigar string into a list of cigar units.\n\n  For example, if cigar_str is 150M2S, this function will return:\n\n  [\n    CigarUnit(operation=ALIGNMENT_MATCH, operation_length=150),\n    CigarUnit(operation=SOFT_CLIP, operation_length=2)\n  ]\n\n  Args:\n    cigar_str: str containing a valid cigar.\n\n  Returns:\n    list[cigar_pb2.CigarUnit].\n\n  Raises:\n    ValueError: If cigar_str isn\'t a well-formed CIGAR.\n  """"""\n  if not cigar_str:\n    raise ValueError(\'cigar_str cannot be empty\')\n  if not VALID_CIGAR_RE.match(cigar_str):\n    raise ValueError(\'Malformed CIGAR string {}\'.format(cigar_str))\n  parts = CIGAR_STR_SPLITTER_RE.finditer(cigar_str)\n  return [to_cigar_unit(part.group(1)) for part in parts]\n\n\ndef alignment_length(cigar_units):\n  """"""Computes the span in basepairs of the cigar units.\n\n  Args:\n    cigar_units: iterable[CigarUnit] whose alignment length we want to compute.\n\n  Returns:\n    The number of basepairs spanned by the cigar_units.\n  """"""\n  return sum(unit.operation_length\n             for unit in cigar_units\n             if unit.operation in REF_ADVANCING_OPS)\n\n\ndef to_cigar_unit(source):\n  """"""Creates a cigar_pb2 CigarUnit from source.\n\n  This function attempts to convert source into a CigarUnit protobuf. If\n  source is a string, it must be a single CIGAR string specification like\n  \'12M\'. If source is a tuple or a list, must have exactly two elements\n  (operation_length, opstr). operation_length can be a string or int, and must\n  be >= 1. opstr should be a single character CIGAR specification (e.g., \'M\').\n  If source is already a CigarUnit, it is just passed through unmodified.\n\n  Args:\n    source: many types allowed. The object we want to convert to a CigarUnit\n      proto.\n\n  Returns:\n    CigarUnit proto with operation_length and operation set to values from\n      source.\n\n  Raises:\n    ValueError: if source cannot be converted or is malformed.\n  """"""\n  try:\n    if isinstance(source, cigar_pb2.CigarUnit):\n      return source\n    elif isinstance(source, six.string_types):\n      l, op = source[:-1], source[-1]\n    elif isinstance(source, (tuple, list)):\n      l, op = source\n    else:\n      raise ValueError(\'Unexpected source\', source)\n\n    if isinstance(op, six.string_types):\n      op = CHAR_TO_CIGAR_OPS[op]\n    l = int(l)\n    if l < 1:\n      raise ValueError(\'Length must be >= 1\', l)\n    return cigar_pb2.CigarUnit(operation=op, operation_length=int(l))\n  except (KeyError, IndexError):\n    raise ValueError(\'Failed to convert {} into a CigarUnit\'.format(source))\n\n\ndef to_cigar_units(source):\n  """"""Converts object to a list of CigarUnit.\n\n  This function attempts to convert source into a list of CigarUnit protos.\n  If source is a string, we assume it is a CIGAR string and call\n  parse_cigar_string on it, returning the result. It not, we assume it\'s an\n  iterable containing element to be converted with to_cigar_unit(). The\n  resulting list of converted elements is returned.\n\n  Args:\n    source: str or iterable to convert to CigarUnit protos.\n\n  Returns:\n    list[CigarUnit].\n  """"""\n  if isinstance(source, six.string_types):\n    return parse_cigar_string(source)\n  else:\n    return [to_cigar_unit(singleton) for singleton in source]\n'"
third_party/nucleus/util/cigar_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for cigar.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport itertools\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.protos import cigar_pb2\nfrom third_party.nucleus.util import cigar\n\n_CIGAR_TUPLES_AND_CIGAR_UNITS = [\n    ((1, \'M\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.ALIGNMENT_MATCH, operation_length=1)),\n    ((2, \'I\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.INSERT, operation_length=2)),\n    ((3, \'D\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.DELETE, operation_length=3)),\n    ((4, \'N\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.SKIP, operation_length=4)),\n    ((5, \'S\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.CLIP_SOFT, operation_length=5)),\n    ((6, \'H\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.CLIP_HARD, operation_length=6)),\n    ((7, \'P\'),\n     cigar_pb2.CigarUnit(operation=cigar_pb2.CigarUnit.PAD,\n                         operation_length=7)),\n    ((8, \'=\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.SEQUENCE_MATCH, operation_length=8)),\n    ((9, \'X\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.SEQUENCE_MISMATCH, operation_length=9)),\n]\n\n\ndef _example_cigar_string_and_units(repeat=3):\n  examples = {}\n  for x in itertools.product(_CIGAR_TUPLES_AND_CIGAR_UNITS, repeat=repeat):\n    lengths_and_opstrs, cigar_units = zip(*x)\n    cigar_str = \'\'.join(str(l) + opstr for l, opstr in lengths_and_opstrs)\n    examples[cigar_str] = list(cigar_units)\n  return examples\n\n\nclass CigarTests(parameterized.TestCase):\n\n  @parameterized.parameters((cigar_units, cigar_str)\n                            for cigar_str, cigar_units in\n                            _example_cigar_string_and_units(3).items())\n  def test_format_cigar_units(self, cigar_units, expected):\n    self.assertEqual(cigar.format_cigar_units(cigar_units), expected)\n\n  @parameterized.parameters(\n      (\'10M\', 10),\n      (\'10=\', 10),\n      (\'10X\', 10),\n      (\'10M2I3M\', 13),\n      (\'10M2D3M\', 15),\n      (\'10M2N3M\', 15),\n      (\'1S10M2D3M\', 15),\n      (\'1S10M2D3M1S\', 15),\n      (\'1S10M2D3M1S5H\', 15),\n      (\'8H1S10M2D3M1S5H\', 15),\n      (\'8H1S10M2N3M1S5H\', 15),\n  )\n  def test_alignment_length(self, cigar_str, expected):\n    cigar_units = cigar.parse_cigar_string(cigar_str)\n    self.assertEqual(cigar.alignment_length(cigar_units), expected)\n\n  @parameterized.parameters(\n      (length, opstr, expected)\n      for (length, opstr), expected in _CIGAR_TUPLES_AND_CIGAR_UNITS)\n  def test_to_cigar_unit(self, length, opstr, expected):\n    # Check we can convert a tuple and list of length and opstr.\n    self.assertEqual(cigar.to_cigar_unit((length, opstr)), expected)\n    self.assertEqual(cigar.to_cigar_unit([length, opstr]), expected)\n\n    # Check that we can convert a string version len+opstr.\n    self.assertEqual(cigar.to_cigar_unit(str(length) + opstr), expected)\n\n    # Check that we can pass a CigarUnit through without modification.\n    self.assertEqual(cigar.to_cigar_unit(expected), expected)\n\n    # Check that we can pass length as a string.\n    self.assertEqual(cigar.to_cigar_unit((str(length), opstr)), expected)\n\n  @parameterized.parameters(\n      \'-1M\',\n      \'0M\',\n      \'\',\n      \'M\',\n      \'M12\',\n      \'4m\',\n      # Have to be wrapped in a list to stop parameterized from treating the\n      # tuple as the positional arguments to the test function.\n      [()],\n      [(4)],\n      [(4, \'M\', \'X\')],\n      [(4, \'M\', 10)],\n      [{4, \'M\'}],\n      # This integer is too large to fit in an int64 cigar, make sure an\n      # exception is thrown. Max int64 is 9,223,372,036,854,775,807, so we try\n      # one more.\n      \'9223372036854775808M\',\n  )\n  def test_to_cigar_unit_detects_bad_args(self, bad):\n    with self.assertRaises(ValueError):\n      cigar.to_cigar_unit(bad)\n\n  @parameterized.parameters(\n      list(zip(*to_convert))\n      for to_convert in itertools.product(\n          _CIGAR_TUPLES_AND_CIGAR_UNITS, repeat=3))\n  def test_to_cigar_units(self, to_convert, expected):\n    # We can convert the raw form.\n    to_convert = list(to_convert)\n    expected = list(expected)\n\n    actual = cigar.to_cigar_units(to_convert)\n    self.assertEqual(actual, expected)\n\n    # We can also convert the string form by formatting actual.\n    self.assertEqual(\n        cigar.to_cigar_units(cigar.format_cigar_units(actual)), expected)\n\n  @parameterized.parameters(\n      (str(length) + opstr, [expected])\n      for (length, opstr), expected in _CIGAR_TUPLES_AND_CIGAR_UNITS)\n  def test_parse_cigar_string_single(self, cigar_str, expected):\n    self.assertEqual(cigar.parse_cigar_string(cigar_str), expected)\n\n  @parameterized.parameters(_example_cigar_string_and_units(3).items())\n  def test_parse_cigar_string_three_pieces(self, cigar_str, expected):\n    self.assertEqual(cigar.parse_cigar_string(cigar_str), expected)\n\n  @parameterized.parameters(\n      \'\',\n      \'12\',\n      \'12m\',\n      \'12?\',\n      \'M12\',\n      \'12M1\',\n      \'12MI\',\n      \'12M-1I\',\n      \'12.0M\',\n  )\n  def test_parse_cigar_string_detects_bad_inputs(self, bad_cigar_str):\n    with self.assertRaises(ValueError):\n      cigar.parse_cigar_string(bad_cigar_str)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/util/errors.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Library of application-specific errors.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport errno\nimport sys\n\nfrom absl import logging\n\n\nclass Error(Exception):\n  """"""Base class for core error types.""""""\n\n\nclass CommandLineError(Error):\n  """"""Exception class related to invalid command-line flags.""""""\n\n\ndef log_and_raise(msg, exception_class=Error):\n  """"""Logs the given message at ERROR level and raises exception.\n\n  Args:\n    msg: [`string`]. The message to log and use in the raised exception.\n    exception_class: [`Exception`]. The class of exception to raise.\n\n  Raises:\n    Error: An exception of the type specified by the input exception_class.\n  """"""\n  logging.error(msg)\n  raise exception_class(msg)\n\n\n@contextlib.contextmanager\ndef clean_commandline_error_exit(\n    allowed_exceptions=(Error, CommandLineError), exit_value=errno.ENOENT):\n  """"""Wraps commands to capture certain exceptions and exit without stacktraces.\n\n  This function is intended to wrap all code within main() of Python binaries\n  to provide a mechanism for user errors to exit abnormally without causing\n  exceptions to be thrown. Any exceptions that are subclasses of those listed\n  in `allowed_exceptions` will be caught and the program will quietly exit with\n  `exit_value`. Other exceptions are propagated normally.\n\n  NOTE: This function should only be used as a context manager and its usage\n  should be limited to main().\n\n  Args:\n    allowed_exceptions: [`tuple of Exception`]. A tuple of Exception classes\n      that should not be raised, but instead quietly caused to exit the program.\n    exit_value: [`int`]. The value to return upon program exit.\n\n  Yields:\n    The yield in this function is used to allow the block nested in the ""with""\n    statement to be executed.\n  """"""\n  try:\n    yield\n  except allowed_exceptions:\n    sys.exit(exit_value)\n'"
third_party/nucleus/util/errors_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for third_party.nucleus.util.errors.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport errno\nimport sys\n\nfrom absl import logging\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\nfrom third_party.nucleus.util import errors\n\n\nclass ErrorsTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      (\'empty flag\', errors.CommandLineError),\n      (\'bad value\', ValueError),\n      (\'base error\', errors.Error),\n  )\n  def test_log_and_raise(self, msg, cls):\n    with mock.patch.object(logging, \'error\') as mock_logging:\n      with self.assertRaisesRegexp(cls, msg):\n        errors.log_and_raise(msg, cls)\n      mock_logging.assert_called_once_with(msg)\n\n  @parameterized.parameters(\n      (ValueError, \'ValueError exception\'),\n      (IOError, \'IOError exception\'),\n  )\n  def test_clean_commandline_error_exit_raise_non_allowed(self, exc_type, msg):\n    with self.assertRaisesRegexp(exc_type, msg):\n      with errors.clean_commandline_error_exit():\n        raise exc_type(msg)\n\n  @parameterized.parameters(\n      (errors.CommandLineError, errno.ENOENT),\n      (errors.Error, errno.EINVAL),\n  )\n  def test_clean_commandline_error_exit_clean_exit(self, exc_type, exit_value):\n    with mock.patch.object(sys, \'exit\') as mock_exit:\n      with errors.clean_commandline_error_exit(exit_value=exit_value):\n        raise exc_type()\n    mock_exit.assert_called_once_with(exit_value)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/util/genomics_math.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Mathematics functions for working with genomics data.\n\nA quick note on terminology here.\n\nThere are a bunch kinds of probabilities used commonly in genomics:\n\n-- pError: the probability of being wrong.\n-- pTrue: the probability of being correct.\n\nNormalized probabilities vs. unnormalized likelihoods:\n\n-- Normalized probabilities: p_1, ..., p_n such that sum(p_i) == 1 are said\n   said to be normalized because they represent a valid probability\n   distribution over the states 1 ... n.\n-- Unnormalized likelihoods: p_1, ..., p_n where sum(p_i) != 1. These are not\n   normalized and so aren\'t a valid probabilities distribution.\n\nTo add even more complexity, probabilities are often represented in three\nsemi-equivalent spaces:\n\n-- Real-space: the classic space, with values ranging from [0.0, 1.0]\n   inclusive.\n-- log10-space: If p is the real-space value, in log10-space this would be\n   represented as log10(p). How the p == 0 case is handled is often function\n   dependent, which may accept/return -Inf or not handle the case entirely.\n-- Phred-scaled: See https://en.wikipedia.org/wiki/Phred_quality_score for\n   more information. Briefly, the Phred-scale maintains resolution in the lower\n   parts of the probability space using integer quality scores (though using\n   ints is optional, really). The phred-scale is defined as\n\n     `phred(p) = -10 * log10(p)`\n\n   where p is a real-space probability.\n\nThe functions in math.h dealing with probabilities are very explicit about what\nkinds of probability and representation they expect and return, as unfortunately\nthese are all commonly represented as doubles in C++. Though it is tempting to\naddress this issue with classic software engineering practices like creating\na Probability class, in practice this is extremely difficult to do as this\ncode is often performance critical and the low-level mathematical operations\nused in this code (e.g., log10) don\'t distiguish themselves among the types\nof probabilities.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport numpy as np\n\nfrom third_party.nucleus.util.python import math as math_\n\n# C++ CLIF functions:\n#\n# We are enumerating the C++ functions exported by python/math.clif here, so\n# it\'s clear to people what functions are available in python without digging\n# into the raw python/C++ CLIF code.\nlog10_ptrue_to_phred = math_.log10_ptrue_to_phred\nphred_to_perror = math_.phred_to_perror\nphred_to_log10_perror = math_.phred_to_log10_perror\nperror_to_log10_perror = math_.perror_to_log10_perror\nperror_to_phred = math_.perror_to_phred\nlog10_perror_to_phred = math_.log10_perror_to_phred\nperror_to_rounded_phred = math_.perror_to_rounded_phred\nlog10_perror_to_rounded_phred = math_.log10_perror_to_rounded_phred\nlog10_perror_to_perror = math_.log10_perror_to_perror\nzero_shift_log10_probs = math_.zero_shift_log10_probs\n\n# Maximum confidence in a variant call. Used to prevent overflow with log10.\n# Note: -10 * log_10(1.25e-10) ~= 99.\n_MAX_CONFIDENCE = 1.0 - 1.25e-10\n\nLOG_10_OF_E = np.log10(np.e)\nLOG_E_OF_10 = np.log(10.0)\n\n\ndef perror_to_bounded_log10_perror(perror, min_prob=1.0 - _MAX_CONFIDENCE):\n  """"""Computes log10(p) for the given probability.\n\n  The log10 probability is capped by -_MAX_CONFIDENCE.\n\n  Args:\n    perror: float. The probability to log10.\n    min_prob: float. The minimum allowed probability.\n\n  Returns:\n    log10(p).\n\n  Raises:\n    ValueError: If probability is outside of [0.0, 1.0].\n  """"""\n  if not 0 <= perror <= 1:\n    raise ValueError(\'perror must be between zero and one: {}\'.format(perror))\n  return perror_to_log10_perror(max(perror, min_prob))\n\n\ndef ptrue_to_bounded_phred(ptrue, max_prob=_MAX_CONFIDENCE):\n  """"""Computes the Phred-scaled confidence from the given ptrue probability.\n\n  See https://en.wikipedia.org/wiki/Phred_quality_score for more information.\n  The quality score is capped by _MAX_CONFIDENCE.\n\n  Args:\n    ptrue: float. The ptrue probability to Phred scale.\n    max_prob: float. The maximum allowed probability.\n\n  Returns:\n    Phred-scaled version of 1 - ptrue.\n\n  Raises:\n    ValueError: If ptrue is outside of [0.0, 1.0].\n  """"""\n  if not 0 <= ptrue <= 1:\n    raise ValueError(\'ptrue must be between zero and one: {}\'.format(ptrue))\n  return perror_to_phred(1.0 - min(ptrue, max_prob))\n\n\ndef log10_binomial(k, n, p):\n  """"""Calculates numerically-stable value of log10(binomial(k, n, p)).\n\n  Returns the log10 of the binomial density for k successes in n trials where\n  each success has a probability of occurring of p.\n\n  In real-space, we would calculate:\n\n     result = (n choose k) * (1-p)^(n-k) * p^k\n\n  This function computes the log10 of result, which is:\n\n     log10(result) = log10(n choose k) + (n-k) * log10(1-p) + k * log10(p)\n\n  This is equivalent to invoking the R function:\n    dbinom(x=k, size=n, prob=p, log=TRUE)\n\n  See https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Binomial.html\n  for more details on the binomial.\n\n  Args:\n    k: int >= 0. Number of successes.\n    n: int >= k. Number of trials.\n    p: 0.0 <= float <= 1.0. Probability of success.\n\n  Returns:\n    log10 probability of seeing k successes in n trials with p.\n  """"""\n  r = math.lgamma(n + 1) - (math.lgamma(k + 1) + math.lgamma(n - k + 1))\n  if k > 0:\n    r += k * math.log(p)\n  if n > k:\n    r += (n-k) * math.log1p(-p)\n  return r / LOG_E_OF_10\n\n\ndef log10sumexp(log10_probs):\n  """"""Returns log10(sum(10^log10_probs)) computed in a numerically-stable way.\n\n  Args:\n    log10_probs: array-like of floats. An array of log10 probabilties.\n\n  Returns:\n    Float.\n  """"""\n  m = max(log10_probs)\n  return m + math.log10(sum(pow(10.0, x - m) for x in log10_probs))\n\n\ndef normalize_log10_probs(log10_probs):\n  """"""Approximately normalizes log10 probabilities.\n\n  This function normalizes log10 probabilities. What this means is that we\n  return an equivalent array of probabilities but whereas sum(10^log10_probs) is\n  not necessarily 1.0, the resulting array is sum(10^result) ~= 1.0. The ~=\n  indicates that the result is not necessarily == 1.0 but very close.\n\n  This function is a fast and robust approximation of the true normalization of\n  a log10 transformed probability vector. To understand the approximation,\n  let\'s start with the exact calculation. Suppose I have three models, each\n  emitting a probability that some data was generated by that model:\n\n    data = {0.1, 0.01, 0.001} => probabilities from models A, B, and C\n\n  These probabilities are unnormalized, in the sense that the total probability\n  over the vector doesn\'t sum to 1 (sum(data) = 0.111). In many applications we\n  want to normalize this vector so that sum(normalized(data)) = 1 and the\n  relative magnitudes of the original probabilities are preserved (i.e,:\n\n    data[i] / data[j] = normalized(data)[i] / normalized(data)[j]\n\n  for all pairs of values indexed by i and j. For much of the work we do in\n  genomics, we have so much data that representing these raw probability\n  vectors in real-space risks numeric underflow/overflow, so we instead\n  represent our probability vectors in log10 space:\n\n    log10_data = log10(data) = {-1, -2, -3}\n\n  Given that we expect numeric problems in real-space, normalizing this log10\n  vector is hard, because the standard way you\'d do the normalization is via:\n\n    data[i] = data[i] / sum(data)\n    log10_data[i] = log10_data[i] - log10(sum(10^data))\n\n  But computing the sum of log10 values this way is dangerous because the naive\n  implementation converts back to real-space to do the sum, the very operation\n  we\'re trying to avoid due to numeric instability.\n\n  This function implements an approximate normalization, which relaxes the need\n  for an exact calculation of the sum. This function ensures that the\n  normalization is numerically safe at the expense of the sum not being exactly\n  equal to 1 but rather just close.\n\n  Args:\n    log10_probs: array-like of floats. An array of log10 probabilties.\n\n  Returns:\n    np.array with the same shape as log10_probs but where sum(10^result) ~= 1.0.\n\n  Raises:\n    ValueError: if any log10_probs > 0.0\n  """"""\n  log10_probs = np.array(log10_probs)\n  if np.max(log10_probs) > 0.0:\n    raise ValueError(\'log10_probs all must be <= 0\', log10_probs)\n  lse = log10sumexp(log10_probs)\n  # np.minimum protects us from producing values slightly > 0.0 (e.g., 1e-16).\n  return np.minimum(log10_probs - lse, 0.0)\n'"
third_party/nucleus/util/genomics_math_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for third_party.nucleus.util.genomics_math.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport numpy as np\nimport numpy.testing as npt\n\nfrom third_party.nucleus.util import genomics_math\n\n\nclass MathTests(parameterized.TestCase):\n\n  @parameterized.parameters(\n      (0.9000000, None, 10.0),\n      (0.9900000, None, 20.0),\n      (0.9990000, None, 30.0),\n      (0.9999000, None, 40.0),\n      (0.9999900, None, 50.0),\n      (0.9999990, None, 60.0),\n      (0.9999999, None, 70.0),\n      # Check that bounding works.\n      (0.9999999, 1 - 1e-1, 10.0),\n      (0.9999999, 1 - 1e-2, 20.0),\n      (0.9999999, 1 - 1e-3, 30.0),\n      (0.9999999, 1 - 1e-9, 70.0),\n  )\n  def test_phred_scale(self, prob, bound, expected):\n    if bound:\n      actual = genomics_math.ptrue_to_bounded_phred(prob, bound)\n    else:\n      actual = genomics_math.ptrue_to_bounded_phred(prob)\n    self.assertAlmostEqual(actual, expected, places=6)\n\n  @parameterized.parameters(\n      (1.000000, None, 0.0),\n      (0.100000, None, -1.0),\n      (0.010000, None, -2.0),\n      (0.001000, None, -3.0),\n      (0.000100, None, -4.0),\n      (0.000010, None, -5.0),\n      (0.000001, None, -6.0),\n      # Check that bounding works.\n      (0.000100, 1e-1, -1.0),\n      (0.000100, 1e-2, -2.0),\n      (0.000100, 1e-3, -3.0),\n      (0.000100, 1e-4, -4.0),\n      (0.000100, 1e-5, -4.0),\n      (0.000100, 1e-6, -4.0),\n  )\n  def test_log10_prob(self, prob, bound, expected):\n    if bound:\n      actual = genomics_math.perror_to_bounded_log10_perror(prob, bound)\n    else:\n      actual = genomics_math.perror_to_bounded_log10_perror(prob)\n    self.assertAlmostEqual(actual, expected, places=6)\n\n  @parameterized.parameters(\n      (np.log10(0.900000), -1.0, 10.0),\n      (np.log10(0.990000), -1.0, 20.0),\n      (np.log10(0.999000), -1.0, 30.0),\n      # A huge negative value is handled correctly.\n      (-10000000.0, -1.0, 0.0),\n      # Check that we can pass in a 0.0 probability and get a good value.\n      (0.0, -1.0, -1.0),\n      # This probability is still calculated correctly, included for safety.\n      (0 - 1e-16, -1.0, 156.53559774527022),\n      # Pass in a prob close to one, making sure we get bound value back.\n      (0 - 1e-32, -1.0, -1.0),\n  )\n  def test_log10_ptrue_to_phred(self, prob, value_if_not_finite, expected):\n    actual = genomics_math.log10_ptrue_to_phred(prob, value_if_not_finite)\n    self.assertAlmostEqual(actual, expected, places=6)\n\n  # R code to produce the expectation table.\n  # expected <- function(k, n, p) {\n  #   pbin <- dbinom(k, n, p, log=T) * log10(exp(1))\n  #   likelihoods = paste(sprintf(""%.6f"", pbin), collapse="", "")\n  #   result = paste(k, n, p, pbin, sep="", "")\n  #   cat(paste(""("", result, ""),\\n"", sep=""""))\n  # }\n  #\n  # for (n in c(0, 5, 10)) {\n  #   for (k in seq(0, n)) {\n  #     for (p in c(0.01, 0.5)) {\n  #       expected(k, n, p)\n  #     }\n  #   }\n  # }\n  # expected(0, 1000, 0.5)\n  # expected(0, 10000, 0.5)\n  # expected(100, 10000, 0.5)\n  @parameterized.parameters(\n      (0, 0, 0.01, 0),\n      (0, 0, 0.5, 0),\n      (0, 5, 0.01, -0.0218240270122504),\n      (0, 5, 0.5, -1.50514997831991),\n      (1, 5, 0.01, -1.31848921727378),\n      (1, 5, 0.5, -0.806179973983887),\n      (2, 5, 0.01, -3.01309441620735),\n      (2, 5, 0.5, -0.505149978319906),\n      (3, 5, 0.01, -5.0087296108049),\n      (3, 5, 0.5, -0.505149978319906),\n      (4, 5, 0.01, -7.30539480106643),\n      (4, 5, 0.5, -0.806179973983887),\n      (5, 5, 0.01, -10),\n      (5, 5, 0.5, -1.50514997831991),\n      (0, 10, 0.01, -0.0436480540245008),\n      (0, 10, 0.5, -3.01029995663981),\n      (1, 10, 0.01, -1.03928324862205),\n      (1, 10, 0.5, -2.01029995663981),\n      (2, 10, 0.01, -2.38170592944426),\n      (2, 10, 0.5, -1.35708744286447),\n      (3, 10, 0.01, -3.95137239176953),\n      (3, 10, 0.5, -0.931118710592187),\n      (4, 10, 0.01, -5.70396953768078),\n      (4, 10, 0.5, -0.688080661905893),\n      (5, 10, 0.01, -7.62042348623071),\n      (5, 10, 0.5, -0.608899415858268),\n      (6, 10, 0.01, -9.69523992687588),\n      (6, 10, 0.5, -0.688080661905893),\n      (7, 10, 0.01, -11.9339131701597),\n      (7, 10, 0.5, -0.931118710592187),\n      (8, 10, 0.01, -14.3555170970296),\n      (8, 10, 0.5, -1.35708744286447),\n      (9, 10, 0.01, -17.0043648054024),\n      (9, 10, 0.5, -2.01029995663981),\n      (10, 10, 0.01, -20),\n      (10, 10, 0.5, -3.01029995663981),\n      (0, 1000, 0.5, -301.029995663981),\n      (0, 10000, 0.5, -3010.29995663981),\n      (100, 10000, 0.5, -2768.48565263445),\n  )\n  def test_log10_binomial(self, k, n, p, expected):\n    self.assertAlmostEqual(genomics_math.log10_binomial(k, n, p), expected)\n\n  @parameterized.parameters(\n      ([0], 0.0),\n      ([0.0], 0.0),\n      ([0.0, -10000.0], 0.0),\n      ([-1000.0, -10000.0], -1000.0),\n      ([-1, -10, -100], -1.0),\n      ([-1, -10, -1], -0.69897),\n      ([-1, -1, -1], -0.5228787),\n      ([-1, -1, -1, -100], -0.5228787),\n      ([-1, -1, -1, -100, -1000], -0.5228787),\n      ([-1, -1, -1, -100, -1000, -10000], -0.5228787),\n      ([-1, -1, -1, -100, -1000, -10000, -100000], -0.5228787),\n  )\n  def test_log10sumexp(self, log10_probs, expected):\n    self.assertAlmostEqual(genomics_math.log10sumexp(log10_probs), expected)\n\n  # R code to compute expected results.\n  # expected <- function(lprobs) {\n  #   result = lprobs - log10(sum(10^lprobs))\n  #   lprob_str = paste(""["", paste(sprintf(""%.6f"", lprobs), collapse="", ""),\n  #                     ""]"", sep="""")\n  #   result_str = paste(""["", paste(sprintf(""%.6f"", result), collapse="", ""),\n  #                     ""]"", sep="""")\n  #   cat(paste(""("", lprob_str, "", "", result_str, ""),\\n"", sep=""""))\n  # }\n  #\n  # expected(c(0))\n  # expected(c(-1, -10))\n  # expected(c(-1, -100))\n  # expected(c(-1, -1000))\n  # expected(c(-1, -2))\n  # expected(c(-1, -2, -3))\n  # expected(c(-1, -2, -3, -100))\n  # expected(c(-1, -2, -100))\n  # expected(c(-1, -2, -100, -100))\n  @parameterized.parameters(\n      dict(\n          log10_probs=[0.000000],\n          expected=[0.000000]),\n      dict(\n          log10_probs=[-1.000000, -10.000000],\n          expected=[-0.000000, -9.000000]),\n      dict(\n          log10_probs=[-1.000000, -100.000000],\n          expected=[0.000000, -99.000000]),\n      dict(\n          log10_probs=[-1.000000, -1000.000000],\n          expected=[0.000000, -999.000000]),\n      dict(\n          log10_probs=[-1.000000, -2.000000],\n          expected=[-0.041393, -1.041393]),\n      dict(\n          log10_probs=[-1.000000, -2.000000, -3.000000],\n          expected=[-0.045323, -1.045323, -2.045323]),\n      dict(\n          log10_probs=[-1.000000, -2.000000, -3.000000, -100.000000],\n          expected=[-0.045323, -1.045323, -2.045323, -99.045323]),\n      dict(\n          log10_probs=[-1.000000, -2.000000, -100.000000],\n          expected=[-0.041393, -1.041393, -99.041393]),\n      dict(\n          log10_probs=[-1.000000, -2.000000, -100.000000, -100.000000],\n          expected=[-0.041393, -1.041393, -99.041393, -99.041393]),\n  )\n  def test_normalize_log10_probs(self, log10_probs, expected):\n    npt.assert_allclose(\n        genomics_math.normalize_log10_probs(log10_probs), expected, atol=1e-6)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/util/proto_utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Utility library for working with protobufs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom google.protobuf.internal import api_implementation\n\n\ndef uses_fast_cpp_protos_or_die():\n  """"""Raises an error if a slow protobuf implementation is being used.""""""\n  if api_implementation.Type() != \'cpp\':\n    raise ValueError(\'Expected to be using C++ protobuf implementation \'\n                     \'(api_implementation.Type() == ""cpp"") but it is {}\'.format(\n                         api_implementation.Type()))\n'"
third_party/nucleus/util/ranges.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Utilities for Range overlap detection.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\n\nfrom absl import logging\nimport intervaltree\nimport six\n\nfrom tensorflow.python.platform import gfile\nfrom third_party.nucleus.io import bed\nfrom third_party.nucleus.protos import position_pb2\nfrom third_party.nucleus.protos import range_pb2\n\n# Regular expressions for matching literal chr:start-stop strings.\n_REGION_LITERAL_REGEXP = re.compile(r\'^(\\S+):([0-9,]+)-([0-9,]+)$\')\n\n# Regular expressions for matching literal chr:start strings.\n_POSITION_LITERAL_REGEXP = re.compile(r\'^(\\S+):([0-9,]+)$\')\n\n# Logging frequency when building our rangeset objects, which can take some time\n# to complete. Rather than just pausing for a few minutes, we provide an update\n# logging message every _LOG_EVERY_N_RANGES_IN_RANGESET_INIT records added. See\n# internal for more information.\n_LOG_EVERY_N_RANGES_IN_RANGESET_INIT = 250000\n\n\nclass RangeSet(object):\n  """"""Fast overlap detection of a genomic position against a database of Ranges.\n\n  Enables O(log n) computation of whether a point chr:pos falls within one of a\n  large number of genomic ranges.\n\n  This class does not supports overlapping or adjacent intervals. Any such\n  intervals will be automatically merged together in the constructor.\n\n  This class is immutable. No methods should be added that directly modify the\n  ranges held by the class.\n  """"""\n\n  def __init__(self, ranges=None, contigs=None, quiet=False):\n    """"""Creates a RangeSet backed by ranges.\n\n    Note that the Range objects in ranges are *not* stored directly here, so\n    they can safely be modified after they are passed to this RangeSet.\n\n    Args:\n      ranges: list(nucleus.genomics.v1.Range) protos (or anything with\n        reference_name, start, and end properties following the Range\n        convention). If None, no ranges will be used, and overlaps() will always\n        return False.\n      contigs: list(nucleus.genomics.v1.ContigInfo) protos. Used to define the\n        iteration order over contigs (i.e., by contig.pos_in_fasta).  If this\n        list is not provided, the iteration order will be determined by the\n        alphabetical order of the contig names.\n      quiet: bool; defaults to False: If False, we will emit a logging message\n        every _LOG_EVERY_N_RANGES_IN_RANGESET_INIT records processed while\n        building this intervaltree. Set to True to stop all of the logging.\n\n    Raises:\n      ValueError: if any range\'s reference_name does not correspond to any\n        contig in `contigs`.\n    """"""\n    if contigs is not None:\n      self._contigs = contigs\n      self._contig_map = contigs_dict(contigs)\n      self._contig_sort_key_fn = (\n          lambda name: self._contig_map[name].pos_in_fasta)\n      self._is_valid_contig = lambda name: name in self._contig_map\n    else:\n      self._contigs = None\n      self._contig_map = None\n      self._contig_sort_key_fn = lambda name: name\n      self._is_valid_contig = lambda name: True\n\n    if ranges is None:\n      ranges = []\n\n    # Add each range to our contig-specific intervaltrees.\n    self._by_chr = collections.defaultdict(intervaltree.IntervalTree)\n    for i, range_ in enumerate(ranges):\n      if not self._is_valid_contig(range_.reference_name):\n        raise ValueError(\n            \'Range {} is on an unrecognized contig.\'.format(range_))\n      self._by_chr[range_.reference_name].addi(range_.start, range_.end, None)\n      if not quiet and i > 0 and i % _LOG_EVERY_N_RANGES_IN_RANGESET_INIT == 0:\n        # We do our test directly here on i > 0 so we only see the log messages\n        # if we add at least _LOG_EVERY_N_RANGES_IN_RANGESET_INIT records.\n        logging.info(\'Adding interval %s to intervaltree\', to_literal(range_))\n\n    # Merge overlapping / adjacent intervals in each tree.\n    for tree in six.itervalues(self._by_chr):\n      tree.merge_overlaps(strict=False)\n\n  def __iter__(self):\n    """"""Iterate over the ranges in this RangeSet.\n\n    Yields:\n      Each range of this RangeSet, in sorted order (by chromosome, then start\n      end positions). Relative ordering of chromosomes is defined by the\n      contig.pos_in_fasta integer key for the associated contig. These objects\n      are new range protos so can be freely modified.\n    """"""\n    for refname in sorted(\n        six.iterkeys(self._by_chr), key=self._contig_sort_key_fn):\n      for start, end, _ in sorted(self._by_chr[refname]):\n        yield make_range(refname, start, end)\n\n  @classmethod\n  def from_regions(cls, regions, contig_map=None):\n    """"""Parses a command-line style literal regions flag into a RangeSet.\n\n    Args:\n      regions: An iterable or None. If not None, regions will be parsed with\n        ranges.from_regions.\n      contig_map: An optional dictionary mapping from contig names to ContigInfo\n        protobufs. If provided, allows literals of the format ""contig_name"",\n        which will be parsed into a Range with reference_name=contig_name,\n        start=0, end=n_bases where n_bases comes from the ContigInfo;\n        additionally the sort order of the RangeSet will be determined by\n        contig.pos_in_fasta.\n\n    Returns:\n      A RangeSet object.\n    """"""\n    if regions is None:\n      return cls(ranges=[])\n    else:\n      return cls(ranges=from_regions(regions, contig_map=contig_map))\n\n  @classmethod\n  def from_contigs(cls, contigs):\n    """"""Creates a RangeSet with an interval covering each base of each contig.""""""\n    return cls(\n        (make_range(contig.name, 0, contig.n_bases) for contig in contigs),\n        contigs)\n\n  @classmethod\n  def from_bed(cls, source, contigs=None):\n    """"""Creates a RangeSet containing the intervals from source.\n\n    Args:\n      source: A path to a BED (or equivalent) file of intervals.\n      contigs: An optional list of ContigInfo proto, used by RangeSet\n        constructor.\n\n    Returns:\n      A RangeSet.\n    """"""\n    return cls(bed_parser(source), contigs)\n\n  def intersection(self, *others):\n    """"""Computes the intersection among this RangeSet and *others RangeSets.\n\n    This function computes the intersection of all of the intervals in self and\n    *others, returning a RangeSet containing only intervals common to all. The\n    intersection here is an ranged intersection, not an identity intersection,\n    so the resulting set of intervals may not contain any of the original\n    intervals in any of the sets.\n\n    To be concrete, suppose we have three sets to intersect, each having two\n    intervals:\n\n      self   : chr1:1-10, chr2:20-30\n      other1 : chr1:5-8, chr3:10-40\n      other2 : chr1:3-7, chr3:10-30\n\n    self.intersection(other1, other2) produces a RangeSet with one interval\n    chr1:5-7, the common bases on chr1 in self, other1, and other2. No intervals\n    on chr2 or chr3 are included since the chr2 only occurs in self and the two\n    intervals on chr3, despite having some shared bases, don\'t have an\n    overlapping interval in self.\n\n    Args:\n      *others: A list of RangeSet objects to intersect with the intervals in\n        this RangeSet.\n\n    Returns:\n      A RangeSet. If *others is empty, this function returns self rather than\n      making an unnecessary copy. In all other cases, the returned value will be\n      a freshly allocated RangeSet.\n    """"""\n\n    def _intersect2(refname, tree1, tree2):\n      """"""Intersects the intervals of two IntervalTrees.""""""\n      # Yields all of the overlapping intervals from each interval of tree1\n      # found in tree2. Since each tree has only non-adjacent, non-overlapping,\n      # intervals this calculation is straightforward and safe and produces only\n      # non-adjacent, non-overlapping intervals.\n      if len(tree1) > len(tree2):\n        (bigtree, smalltree) = (tree1, tree2)\n      else:\n        (bigtree, smalltree) = (tree2, tree1)\n      return (make_range(refname, max(interval1.begin, overlapping.begin),\n                         min(interval1.end, overlapping.end))\n              for interval1 in bigtree\n              for overlapping in smalltree.overlap(interval1))\n\n    # Iteratively intersect each of our *other RangeSets with this RangeSet.\n    # Sort by size so we do the smallest number of element merge first.\n    # redacted\n    # common contigs upfront across all others and only looping over those.\n    intersected = self\n    for other in sorted(others, key=len):\n      intersected_intervals = []\n      # pylint: disable=protected-access\n      # So we can intersect intervals within each contig separately.\n      for refname, intervals in six.iteritems(intersected._by_chr):\n        # If refname is present in other, intersect those two IntervalTrees\n        # directly and add those contigs to our growing list of intersected\n        # intervals. If refname isn\'t present, all of the intervals on refname\n        # should be dropped as there are no intervals to overlap.\n        other_chr = other._by_chr.get(refname, None)\n        if other_chr:\n          intersected_intervals.extend(\n              _intersect2(refname, intervals, other_chr))\n\n      # Update our intersected RangeSet with the new intervals.\n      intersected = RangeSet(intersected_intervals, self._contigs)\n\n    return intersected\n\n  def exclude_regions(self, other):\n    """"""Chops out all of the intervals in other from this this RangeSet.\n\n    NOTE: This is a *MUTATING* operation for performance reasons. Make a copy\n    of self if you want to avoid modifying the RangeSet.\n\n    Args:\n      other: A RangeSet object whose intervals will be removed from this\n        RangeSet.\n    """"""\n    # pylint: disable=protected-access\n    for chrname, chr_intervals in six.iteritems(other._by_chr):\n      # If refname is present in self, difference those two IntervalTrees.\n      self_intervals = self._by_chr.get(chrname, None)\n      if self_intervals:\n        for begin, end, _ in chr_intervals:\n          self_intervals.chop(begin, end)\n        if self_intervals.is_empty():\n          # Cleanup after ourselves by removing empty trees from our map.\n          del self._by_chr[chrname]\n\n  def __len__(self):\n    """"""Gets the number of ranges used by this RangeSet.""""""\n    return sum(len(for_chr) for for_chr in six.itervalues(self._by_chr))\n\n  def __nonzero__(self):\n    """"""Returns True if this RangeSet is not empty.""""""\n    return bool(self._by_chr)\n\n  __bool__ = __nonzero__  # Python 3 compatibility.\n\n  def variant_overlaps(self, variant, empty_set_return_value=True):\n    """"""Returns True if the variant\'s range overlaps with any in this set.""""""\n    if not self:\n      return empty_set_return_value\n    else:\n      return self.overlaps(variant.reference_name, variant.start)\n\n  def overlaps(self, chrom, pos):\n    """"""Returns True if chr:pos overlaps with any range in this RangeSet.\n\n    Uses a fast bisection algorithm to determine the overlap in O(log n) time.\n\n    Args:\n      chrom: str. The chromosome name.\n      pos: int. The position (0-based).\n\n    Returns:\n      True if chr:pos overlaps with a range.\n    """"""\n    chr_ranges = self._by_chr.get(chrom, None)\n    if chr_ranges is None:\n      return False\n    return chr_ranges.overlaps(pos)\n\n  def partition(self, max_size):\n    """"""Splits our intervals so that none are larger than max_size.\n\n    Slices up the intervals in this RangeSet into a equivalent set of intervals\n    (i.e., spanning the same set of bases), each of which is at most max_size in\n    length.\n\n    This function does not modify this RangeSet.\n\n    Because RangeSet merges adjacent intervals, this function cannot use a\n    RangeSet to represent the partitioned intervals and so instead generates\n    these intervals via a yield statement.\n\n    Args:\n      max_size: int > 0. The maximum size of any interval.\n\n    Yields:\n      nucleus.genomics.v1.Range protos, in sorted order (see comment about order\n      in __iter__).\n\n    Raises:\n      ValueError: if max_size <= 0.\n    """"""\n    if max_size <= 0:\n      raise ValueError(\'max_size must be > 0: {}\'.format(max_size))\n\n    for interval in self:\n      refname = interval.reference_name\n      for pos in range(interval.start, interval.end, max_size):\n        yield make_range(refname, pos, min(interval.end, pos + max_size))\n\n  def envelops(self, chrom, start, end):\n    """"""Returns True iff some range in this RangeSet envelops the range.\n\n    Args:\n      chrom: str. The chromosome of interest.\n      start: int. Zero-based inclusive index of the query range.\n      end: int: Zero-based exclusive index of the query range.\n\n    Returns:\n      True if and only if some range in `self` completely spans the query\n      range.\n    """"""\n    chr_ranges = self._by_chr.get(chrom, None)\n    if chr_ranges is None:\n      return False\n    # The intervaltree package does the inverse check, i.e. whether ranges\n    # contained in it overlap with the query region. So it returns nothing\n    # when start == end. We by convention want anything overlapping the start\n    # position to still indicate enveloping in this case.\n    if start == end:\n      return chr_ranges.overlaps(start)\n    else:\n      overlap_set = chr_ranges.overlap(begin=start, end=end)\n      return any(ov.begin <= start and ov.end >= end for ov in overlap_set)\n\n\ndef make_position(chrom, position, reverse_strand=False):\n  """"""Returns a nucleus.genomics.v1.Position.\n\n  Args:\n    chrom: str. The chromosome name.\n    position: int. The start position (0-based, inclusive).\n    reverse_strand: bool. If True, indicates the position is on the negative\n      strand.\n  """"""\n  return position_pb2.Position(\n      reference_name=chrom, position=position, reverse_strand=reverse_strand)\n\n\ndef make_range(chrom, start, end):\n  """"""Returns a nucleus.genomics.v1.Range.\n\n  Args:\n    chrom: str. The chromosome name.\n    start: int. The start position (0-based, inclusive) of this range.\n    end: int. The end position (0-based, exclusive) of this range.\n\n  Returns:\n    A nucleus.genomics.v1.Range.\n  """"""\n  return range_pb2.Range(reference_name=chrom, start=start, end=end)\n\n\ndef position_overlaps(chrom, pos, interval):\n  """"""Returns True iff the position chr:pos overlaps the interval.\n\n  Args:\n    chrom: str. The chromosome name.\n    pos: int. The position (0-based, inclusive).\n    interval: nucleus.genomics.v1.Range object.\n\n  Returns:\n    True if interval overlaps chr:pos.\n  """"""\n  return (chrom == interval.reference_name and\n          interval.start <= pos < interval.end)\n\n\ndef ranges_overlap(i1, i2):\n  """"""Returns True iff ranges i1 and i2 overlap.\n\n  Args:\n    i1: nucleus.genomics.v1.Range object.\n    i2: nucleus.genomics.v1.Range object.\n\n  Returns:\n    True if and only if i1 and i2 overlap.\n  """"""\n  return (i1.reference_name == i2.reference_name and i1.end > i2.start and\n          i1.start < i2.end)\n\n\ndef bedpe_parser(filename):\n  """"""Parses Range objects from a BEDPE-formatted file object.\n\n  See http://bedtools.readthedocs.org/en/latest/content/general-usage.html\n  for more information on the BEDPE format.\n\n  Skips events that span across chromosomes. For example, if the starting\n  location is on chr1 and the ending location is on chr2, that record will\n  not appear in the output.\n\n  Args:\n    filename: file name of a BEDPE-formatted file.\n\n  Yields:\n    nucleus.genomics.v1.Range protobuf objects.\n  """"""\n  for line in gfile.Open(filename):\n    parts = line.split(\'\\t\')\n    if parts[0] == parts[3]:\n      # only keep events on the same chromosome\n      yield make_range(parts[0], int(parts[1]), int(parts[5]))\n\n\ndef bed_parser(filename):\n  """"""Parses Range objects from a BED-formatted file object.\n\n  See http://bedtools.readthedocs.org/en/latest/content/general-usage.html\n  for more information on the BED format.\n\n  Args:\n    filename: file name of a BED-formatted file.\n\n  Yields:\n    nucleus.genomics.v1.Range protobuf objects.\n  """"""\n  with bed.BedReader(filename) as fin:\n    for r in fin.iterate():\n      yield make_range(r.reference_name, r.start, r.end)\n\n\ndef from_regions(regions, contig_map=None):\n  """"""Parses each region of `regions` into a Range proto.\n\n  This function provides a super high-level interface for\n  reading/parsing/converting objects into Range protos. Each `region` of\n  `regions` is processed in turn, yielding one or more Range protos. This\n  function inspects the contents of `region` to determine how to convert it to\n  Range(s) protos. The following types of `region` strings are supported:\n\n    * If region ends with an extension known in _get_parser_for_file, we treat\n      region as a file and read the Range protos from it with the corresponding\n      reader from _get_parser_for_file, yielding each Range from the file in\n      order.\n    * Otherwise we parse region as a region literal (`chr20:1-10`) and return\n      the Range proto.\n\n  Args:\n    regions: iterable[str]. Converts each element of this iterable into\n      region(s).\n    contig_map: An optional dictionary mapping from contig names to ContigInfo\n      protobufs. If provided, allows literals of the format ""contig_name"",\n      which will be parsed into a Range with reference_name=contig_name,\n      start=0, end=n_bases where n_bases comes from the ContigInfo.\n\n  Yields:\n    A Range proto.\n  """"""\n  for region in regions:\n    reader = _get_parser_for_file(region)\n    if reader:\n      for elt in reader(region):\n        yield elt\n    else:\n      yield parse_literal(region, contig_map)\n\n\n# Cannot be at the top of the file because these parser functions need to be\n# defined before adding them to the dictionary.\n_REGION_FILE_READERS = {\n    bed_parser: frozenset([\'.bed\']),\n    bedpe_parser: frozenset([\'.bedpe\']),\n}\n\n\ndef _get_parser_for_file(filename):\n  for reader, exts in six.iteritems(_REGION_FILE_READERS):\n    if any(filename.lower().endswith(ext) for ext in exts):\n      return reader\n  return None\n\n\ndef to_literal(range_pb):\n  """"""Converts Range protobuf into string literal form.\n\n  The string literal form looks like:\n\n    reference_name:start+1-end\n\n  since start and end are zero-based inclusive (start) and exclusive (end),\n  while the literal form is one-based inclusive on both ends.\n\n  Args:\n    range_pb: A nucleus.genomics.v1.Range object.\n\n  Returns:\n    A string representation of the Range.\n  """"""\n  return \'{}:{}-{}\'.format(range_pb.reference_name, range_pb.start + 1,\n                           range_pb.end)\n\n\ndef parse_literal(region_literal, contig_map=None):\n  """"""Parses a Range from a string representation like chr:start-end.\n\n  The region literal must conform to the following pattern:\n\n    chromosome:start-end\n    chromosome:position\n    chromosome  [if contig_map is provided]\n\n  chromosome can be any non-empty string without whitespace. start and end must\n  both be positive integers. They can contain commas for readability. start and\n  end are positions not offsets, so start == 1 means an offset of zero. If only\n  a single position is provided, this creates a 1 bp interval starting at\n  position - 1 and ending at position.\n\n  Inspired by the samtools region specification:\n  http://www.htslib.org/doc/samtools.html\n\n  Args:\n    region_literal: str. The literal to parse.\n    contig_map: An optional dictionary mapping from contig names to ContigInfo\n      protobufs. If provided, allows literals of the format ""contig_name"", which\n      will be parsed into a Range with reference_name=contig_name, start=0,\n      end=n_bases where n_bases comes from the ContigInfo.\n\n  Returns:\n    nucleus.genomics.v1.Range.\n\n  Raises:\n    ValueError: if region_literal cannot be parsed.\n  """"""\n\n  def parse_position(pos_str):\n    return int(pos_str.replace(\',\', \'\'))\n\n  matched = _REGION_LITERAL_REGEXP.match(region_literal)\n  if matched:\n    chrom, start, end = matched.groups()\n    return make_range(chrom, parse_position(start) - 1, parse_position(end))\n\n  matched = _POSITION_LITERAL_REGEXP.match(region_literal)\n  if matched:\n    chrom, pos = matched.groups()\n    pos = parse_position(pos)\n    return make_range(chrom, pos - 1, pos)\n\n  if contig_map and region_literal in contig_map:\n    # If the region_literals is an exact contig name like chr1 or MT return a\n    # range over the entire contig.\n    return make_range(region_literal, 0, contig_map[region_literal].n_bases)\n  raise ValueError(\n      \'Could not parse ""{}"" as a region literal.  Region literals \'\n      \'should have the form ""chr:start-stop"" or ""chr:start"" or \'\n      \'just ""chr"".  A common error is to use the ""chr"" prefix on \'\n      \'inputs that don\\\'t have it, or vice-versa.\'.format(region_literal))\n\n\ndef parse_literals(region_literals, contig_map=None):\n  """"""Parses each literal of region_literals in order.""""""\n  return [parse_literal(literal, contig_map) for literal in region_literals]\n\n\ndef contigs_n_bases(contigs):\n  """"""Returns the sum of all n_bases of contigs.""""""\n  return sum(c.n_bases for c in contigs)\n\n\ndef contigs_dict(contigs):\n  """"""Creates a dictionary for contigs.\n\n  Args:\n    contigs: Iterable of ContigInfo protos.\n\n  Returns:\n    A dictionary mapping contig.name: contig for each contig in contigs.\n  """"""\n  return {contig.name: contig for contig in contigs}\n\n\ndef sorted_ranges(ranges, contigs=None):\n  """"""Sorts ranges by reference_name, start, and end.\n\n  Args:\n    ranges: Iterable of nucleus.genomics.v1.Range protos that we want to sort.\n    contigs: None or an iterable of ContigInfo protos. If not None, we will use\n      the order of the contigs (as defined by their pos_in_fasta field values)\n      to sort the Ranges on different contigs with respect to each other.\n\n  Returns:\n    A newly allocated list of nucleus.genomics.v1.Range protos.\n  """"""\n  if contigs:\n    contig_map = contigs_dict(contigs)\n\n    def to_key(range_):\n      pos = contig_map[range_.reference_name].pos_in_fasta\n      return pos, range_.start, range_.end\n  else:\n    to_key = as_tuple\n\n  return sorted(ranges, key=to_key)\n\n\ndef as_tuple(range_):\n  """"""Returns a Python tuple (reference_name, start, end).""""""\n  return range_.reference_name, range_.start, range_.end\n\n\ndef overlap_len(range1, range2):\n  """"""Computes the number of overlapping bases of range1 and range2.\n\n  Args:\n    range1: nucleus.genomics.v1.Range.\n    range2: nucleus.genomics.v1.Range.\n\n  Returns:\n    int. The number of basepairs in common. 0 if the ranges are not on the same\n    contig.\n  """"""\n  if range1.reference_name != range2.reference_name:\n    return 0\n  return max(0, (min(range1.end, range2.end) - max(range1.start, range2.start)))\n\n\ndef find_max_overlapping(query_range, search_ranges):\n  """"""Gets the index of the element in search_ranges with max overlap with query.\n\n  In case of ties, selects the lowest index range in search_ranges.\n\n  Args:\n    query_range: nucleus.genomics.v1.Range, read genomic range.\n    search_ranges: list[nucleus.genomics.v1.Read]. The list of regions we want\n      to search for the maximal overlap with query_range. NOTE: this must be a\n      list (not a generator) as we loop over the search_ranges multiple times.\n\n  Returns:\n    int, the search_ranges index with the maximum read overlap. Returns None\n    when read has no overlap with any of the search_ranges or search_ranges is\n    empty.\n  """"""\n  if not search_ranges:\n    return None\n  overlaps = [overlap_len(query_range, srange) for srange in search_ranges]\n  argmax = max(range(len(search_ranges)), key=lambda i: overlaps[i])\n  # We return None if the read doesn\'t overlap at all.\n  return None if overlaps[argmax] == 0 else argmax\n\n\ndef expand(region, n_bp, contig_map=None):\n  """"""Expands region by n_bp in both directions.\n\n  Takes a Range(chrom, start, stop) and returns a new\n  Range(chrom, new_start, new_stop), where:\n\n  -- new_start is max(start - n_bp, 0)\n  -- new_stop is stop + n_bp if contig_map is None, or min(stop + n_bp, max_bp)\n     where max_bp is contig_map[chrom].n_bp.\n\n  Args:\n    region: A nucleus.genomics.v1.Range proto.\n    n_bp: int >= 0. how many basepairs to increase region by.\n    contig_map: dict[string, ContigInfo] or None. If not None, used to get the\n      maximum extent to increase stop by. Must have region.reference_name as a\n      key.\n\n  Returns:\n    nucleus.genomics.v1.Range proto.\n\n  Raises:\n    ValueError: if n_bp is invalid.\n    KeyError: contig_map is not None and region.reference_name isn\'t a key.\n  """"""\n  if n_bp < 0:\n    raise ValueError(\'n_bp must be >= 0 but got {}\'.format(n_bp))\n\n  new_start = max(region.start - n_bp, 0)\n  new_end = region.end + n_bp\n  if contig_map is not None:\n    new_end = min(new_end, contig_map[region.reference_name].n_bases)\n  return make_range(region.reference_name, new_start, new_end)\n\n\ndef span(regions):\n  """"""Returns a region that spans all of the bases in regions.\n\n  This function returns a Range(chrom, start, stop), where start is the min\n  of the starts in regions, and stop is the max end in regions. It may not be\n  freshly allocated.\n\n  Args:\n    regions: list[Range]: a list of Range protos.\n\n  Returns:\n    A single Range proto.\n\n  Raises:\n    ValueError: if not all regions have the same reference_name.\n    ValueError: if regions is empty.\n  """"""\n  if not regions:\n    raise ValueError(\'regions is empty but must have at least one region\')\n  elif len(regions) == 1:\n    return regions[0]\n  elif any(r.reference_name != regions[0].reference_name for r in regions):\n    raise ValueError(\'regions must be all on the same contig\')\n  else:\n    start = min(r.start for r in regions)\n    end = max(r.end for r in regions)\n    return make_range(regions[0].reference_name, start, end)\n\n\ndef length(region):\n  """"""Returns the length in basepairs of region.""""""\n  return region.end - region.start\n'"
third_party/nucleus/util/ranges_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for ranges.py.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport itertools\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\n\nfrom third_party.nucleus.protos import position_pb2\nfrom third_party.nucleus.protos import reference_pb2\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\n\n_TEST_BED_REGIONS = [\n    ranges.make_range(\'chr1\', 1, 10),\n    ranges.make_range(\'chr2\', 20, 30),\n    ranges.make_range(\'chr2\', 40, 60),\n    ranges.make_range(\'chr3\', 80, 90),\n]\n\n_TEST_CONTIGS = [\n    reference_pb2.ContigInfo(name=\'chr1\', n_bases=10, pos_in_fasta=0),\n    reference_pb2.ContigInfo(name=\'chr2\', n_bases=100, pos_in_fasta=1),\n    reference_pb2.ContigInfo(name=\'chr3\', n_bases=500, pos_in_fasta=2),\n]\n\n\nclass RangesTests(parameterized.TestCase):\n\n  def test_ranges_overlaps(self):\n\n    def check_overlaps(chr1, start1, end1, chr2, start2, end2, expected):\n      i1 = ranges.make_range(chr1, start1, end1)\n      i2 = ranges.make_range(chr2, start2, end2)\n      self.assertEqual(ranges.ranges_overlap(i1, i2), expected)\n      self.assertEqual(ranges.ranges_overlap(i2, i1), expected)\n\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 4, 10, False)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 3, 10, False)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 2, 10, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 1, 10, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 0, 10, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 0, 1, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 0, 2, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 0, 3, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 1, 2, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 1, 3, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 2, 3, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 3, 3, False)\n    check_overlaps(\'chr1\', 1, 3, \'chr1\', 0, 4, True)\n    check_overlaps(\'chr1\', 1, 3, \'chr1\', 1, 4, True)\n\n  def test_detector_no_ranges(self):\n    range_set = ranges.RangeSet()\n    # don\'t have any ranges by default\n    self.assertEqual(bool(range_set), False)\n    # make sure we can call overlaps without any ranges\n    self.assertFalse(range_set.overlaps(\'chr1\', 10))\n\n  def test_from_regions_not_empty(self):\n    literals = [\'chr1\', \'chr2:10-20\']\n    self.assertItemsEqual(\n        [ranges.make_range(\'chr1\', 0, 10),\n         ranges.make_range(\'chr2\', 9, 20)],\n        ranges.RangeSet.from_regions(\n            literals, ranges.contigs_dict(_TEST_CONTIGS)))\n\n  def test_from_regions_empty_literals(self):\n    range_set = ranges.RangeSet.from_regions([])\n    # The set is empty.\n    self.assertItemsEqual([], range_set)\n    self.assertFalse(range_set)\n\n  def test_unrecognized_contig_triggers_exception(self):\n    with self.assertRaises(ValueError):\n      _ = ranges.RangeSet([ranges.make_range(\'bogus_chromosome\', 1, 10)],\n                          _TEST_CONTIGS)\n\n  @parameterized.parameters(\n      # Overlapping intervals get merged.\n      ([\'1:1-5\', \'1:3-8\'], [\'1:1-8\']),\n      ([\'1:1-5\', \'1:3-8\', \'1:6-9\'], [\'1:1-9\']),\n      # Adjacent intervals are merged.\n      ([\'1:1-5\', \'1:5-8\'], [\'1:1-8\']),\n      ([\'1:1-5\', \'1:5-8\', \'1:8-10\'], [\'1:1-10\']),\n      # Sanity check that non-overlapping aren\'t merged.\n      ([\'1:1-5\', \'1:6-8\'], [\'1:1-5\', \'1:6-8\']),\n  )\n  def test_overlapping_and_adjacent_ranges_are_merged(self, regions, expected):\n    self.assertCountEqual(\n        ranges.RangeSet.from_regions(expected),\n        ranges.RangeSet.from_regions(regions))\n\n  def test_detector_ranges(self):\n    test_ranges = [\n        ranges.make_range(\'chr1\', 0, 5),\n        ranges.make_range(\'chr1\', 8, 10),\n        ranges.make_range(\'chr1\', 12, 13),\n        ranges.make_range(\'chr2\', 2, 5),\n    ]\n    range_set = ranges.RangeSet(test_ranges)\n    self.assertEqual(bool(range_set), True)\n    self.assertEqual(len(range_set), 4)\n\n    self.assertEqual(range_set.overlaps(\'chr1\', 0), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 1), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 2), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 3), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 4), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 5), False)\n    self.assertEqual(range_set.overlaps(\'chr1\', 6), False)\n    self.assertEqual(range_set.overlaps(\'chr1\', 7), False)\n    self.assertEqual(range_set.overlaps(\'chr1\', 8), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 9), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 10), False)\n    self.assertEqual(range_set.overlaps(\'chr1\', 11), False)\n    self.assertEqual(range_set.overlaps(\'chr1\', 12), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 13), False)\n    self.assertEqual(range_set.overlaps(\'chr1\', 100), False)\n    self.assertEqual(range_set.overlaps(\'chr1\', 1000), False)\n    self.assertEqual(range_set.overlaps(\'chr2\', 0), False)\n    self.assertEqual(range_set.overlaps(\'chr2\', 1), False)\n    self.assertEqual(range_set.overlaps(\'chr2\', 2), True)\n    self.assertEqual(range_set.overlaps(\'chr2\', 3), True)\n    self.assertEqual(range_set.overlaps(\'chr2\', 4), True)\n    self.assertEqual(range_set.overlaps(\'chr2\', 5), False)\n    self.assertEqual(range_set.overlaps(\'chr2\', 6), False)\n    self.assertEqual(range_set.overlaps(\'chr3\', 3), False)\n\n  def test_overlaps_variant_with_ranges(self):\n    variant = variants_pb2.Variant(reference_name=\'chr2\', start=10, end=11)\n    range_set = ranges.RangeSet([ranges.make_range(\'chr1\', 0, 5)])\n    with mock.patch.object(range_set, \'overlaps\') as mock_overlaps:\n      mock_overlaps.return_value = True\n      self.assertEqual(range_set.variant_overlaps(variant), True)\n      mock_overlaps.assert_called_once_with(\'chr2\', 10)\n\n  def test_overlaps_variant_empty_range(self):\n    variant = variants_pb2.Variant(reference_name=\'chr2\', start=10, end=11)\n    empty_set = ranges.RangeSet()\n    self.assertEqual(\n        empty_set.variant_overlaps(variant, empty_set_return_value=\'foo\'),\n        \'foo\')\n\n  def test_envelops(self):\n    start_ix = 5\n    end_ix = 10\n    start_ix2 = end_ix + 1\n    end_ix2 = end_ix + 5\n    range_set = ranges.RangeSet([\n        ranges.make_range(\'chr1\', start_ix, end_ix),\n        ranges.make_range(\'chr1\', start_ix2, end_ix2)\n    ])\n\n    # No start position before the first start range is enveloped.\n    for i in range(start_ix):\n      self.assertFalse(range_set.envelops(\'chr1\', i, start_ix + 1))\n\n    # All regions within a single record are enveloped.\n    for six in range(start_ix, end_ix):\n      for eix in range(six, end_ix + 1):\n        self.assertTrue(\n            range_set.envelops(\'chr1\', six, eix),\n            \'chr1 {} {} not enveloped\'.format(six, eix))\n\n    # Bridging across two ranges is not enveloped.\n    for six in range(start_ix, end_ix):\n      for eix in range(start_ix2, end_ix2 + 1):\n        self.assertFalse(range_set.envelops(\'chr1\', six, eix))\n\n    # Other chromosome is not spanned.\n    self.assertFalse(range_set.envelops(\'chr2\', start_ix, start_ix + 1))\n\n  @parameterized.parameters(\n      (ranges.make_range(\'1\', 10, 50), \'1\', 9, False),\n      (ranges.make_range(\'1\', 10, 50), \'1\', 10, True),\n      (ranges.make_range(\'1\', 10, 50), \'2\', 10, False),\n      (ranges.make_range(\'1\', 10, 50), \'1\', 30, True),\n      (ranges.make_range(\'1\', 10, 50), \'2\', 30, False),\n      (ranges.make_range(\'1\', 10, 50), \'1\', 49, True),\n      (ranges.make_range(\'1\', 10, 50), \'1\', 50, False),\n      (ranges.make_range(\'1\', 10, 50), \'1\', 51, False),\n  )\n  def test_position_overlaps(self, interval, chrom, pos, expected):\n    self.assertEqual(ranges.position_overlaps(chrom, pos, interval), expected)\n\n  def test_make_position(self):\n    self.assertEqual(\n        ranges.make_position(\'chr1\', 10),\n        position_pb2.Position(\n            reference_name=\'chr1\', position=10, reverse_strand=False))\n    self.assertEqual(\n        ranges.make_position(\'chr2\', 100, reverse_strand=True),\n        position_pb2.Position(\n            reference_name=\'chr2\', position=100, reverse_strand=True))\n\n  def test_make_range(self):\n    interval = ranges.make_range(\'chr1\', 1, 10)\n    self.assertEqual(interval.reference_name, \'chr1\')\n    self.assertEqual(interval.start, 1)\n    self.assertEqual(interval.end, 10)\n\n  def test_to_literal(self):\n    self.assertEqual(\n        ranges.to_literal(ranges.make_range(\'chr1\', 0, 20)), \'chr1:1-20\')\n\n  @parameterized.parameters([\'chr1\', \'1\', \'MT\', \'chrM\', \'chrX\', \'X\', \'Y\'])\n  def test_parse_literal_chromosomes(self, chrom):\n    self.assertEqual(\n        ranges.parse_literal(chrom + \':1-20\'), ranges.make_range(chrom, 0, 20))\n\n  @parameterized.parameters(\n      (\'chr1:{}-{}\'.format(start_str, end_str), start_val, end_val)\n      for start_str, start_val in [(\'12\', 11), (\'1,234\', 1233)]\n      for end_str, end_val in [(\'56789\', 56789), (\'56,789\', 56789)])\n  def test_parse_literal_numerics(self, literal, start_val, end_val):\n    self.assertEqual(\n        ranges.parse_literal(literal),\n        ranges.make_range(\'chr1\', start_val, end_val))\n\n  def test_parse_literal_one_bp(self):\n    self.assertEqual(\n        ranges.parse_literal(\'1:10\'), ranges.make_range(\'1\', 9, 10))\n    self.assertEqual(\n        ranges.parse_literal(\'1:100\'), ranges.make_range(\'1\', 99, 100))\n    self.assertEqual(\n        ranges.parse_literal(\'1:1,000\'), ranges.make_range(\'1\', 999, 1000))\n\n  @parameterized.parameters([\'x\', \'chr1\', \'chr1:\', \'chr1:10-\', \'chr1:-1-10\'])\n  def test_parse_literal_bad(self, bad_literal):\n    with self.assertRaisesRegexp(ValueError, bad_literal):\n      ranges.parse_literal(bad_literal)\n\n  @parameterized.parameters(\'test.bed\', \'test.bed.gz\')\n  def test_from_bed(self, bed_filename):\n    source = test_utils.genomics_core_testdata(bed_filename)\n    self.assertCountEqual([\n        ranges.make_range(\'chr1\', 1, 10),\n        ranges.make_range(\'chr2\', 20, 30),\n        ranges.make_range(\'chr2\', 40, 60),\n        ranges.make_range(\'chr3\', 80, 90),\n    ], ranges.RangeSet.from_bed(source))\n\n  @parameterized.parameters(\n      dict(regions=[], expected=[]),\n      dict(regions=[\'chr1:10-20\'], expected=[ranges.make_range(\'chr1\', 9, 20)]),\n      dict(regions=[\'test.bed\'], expected=_TEST_BED_REGIONS),\n      dict(\n          regions=[\'test.bed\', \'test.bed\'],\n          expected=_TEST_BED_REGIONS + _TEST_BED_REGIONS),\n      dict(\n          regions=[\'chr1:10-20\', \'test.bed\'],\n          expected=[ranges.make_range(\'chr1\', 9, 20)] + _TEST_BED_REGIONS),\n      dict(\n          regions=[\'test.bed\', \'chr1:10-20\'],\n          expected=_TEST_BED_REGIONS + [ranges.make_range(\'chr1\', 9, 20)]),\n      dict(\n          regions=[\'chr1:9-19\', \'test.bed\', \'chr1:10-20\'],\n          expected=([ranges.make_range(\'chr1\', 8, 19)] + _TEST_BED_REGIONS +\n                    [ranges.make_range(\'chr1\', 9, 20)])),\n  )\n  def test_from_regions(self, regions, expected):\n    # For convenience we allow \'test.bed\' in our regions but the actual file\n    # path is in our testdata directory.\n    for i in range(len(regions)):\n      if regions[i] == \'test.bed\':\n        regions[i] = test_utils.genomics_core_testdata(\'test.bed\')\n\n    self.assertEqual(list(ranges.from_regions(regions)), expected)\n\n  @parameterized.parameters(\n      # Intersection with 1, 2, 3 identical RangeSets produces the original set.\n      ([[\'1:1-10\']], [\'1:1-10\']),\n      ([[\'1:1-10\'], [\'1:1-10\']], [\'1:1-10\']),\n      ([[\'1:1-10\'], [\'1:1-10\'], [\'1:1-10\']], [\'1:1-10\']),\n      # Test some simple overlap configurations.\n      ([[\'1:1-10\'], [\'1:11-15\']], []),\n      ([[\'1:1-10\'], [\'1:10-15\']], [\'1:10\']),\n      ([[\'1:1-10\'], [\'1:9-15\']], [\'1:9-10\']),\n      ([[\'1:5-10\'], [\'1:1-15\']], [\'1:5-10\']),\n      ([[\'1:5-10\'], [\'1:1-4\']], []),\n      ([[\'1:5-10\'], [\'1:1-5\']], [\'1:5\']),\n      # Check cutting a single interval into multiple pieces.\n      ([[\'1:5-15\'], [\'1:6-8\', \'1:10-12\']], [\'1:6-8\', \'1:10-12\']),\n      ([[\'1:5-15\'], [\'1:3-8\', \'1:10-12\']], [\'1:5-8\', \'1:10-12\']),\n      ([[\'1:5-15\'], [\'1:3-8\', \'1:10-20\']], [\'1:5-8\', \'1:10-15\']),\n      # We have multiple overlapping intervals; make sure we merge intervals.\n      ([[\'1:5-15\'], [\'1:3-8\', \'1:6-10\']], [\'1:5-10\']),\n      ([[\'1:5-15\'], [\'1:3-8\', \'1:6-10\', \'1:13\']], [\'1:5-10\', \'1:13\']),\n      # Check that multiple intervals work.\n      ([[\'1:5-15\', \'1:20-25\'], [\'1:3-8\', \'1:16-23\']], [\'1:5-8\', \'1:20-23\']),\n      ([[\'1:5-15\', \'1:20-25\'], [\'1:3-8\', \'1:50-60\']], [\'1:5-8\']),\n      ([[\'1:5-15\', \'1:20-25\'], [\'1:3-4\', \'1:16-23\']], [\'1:20-23\']),\n      # Check that multiple sets can be intersected.\n      ([[\'1:10-20\'], [\'1:5-15\']], [\'1:10-15\']),\n      ([[\'1:10-20\'], [\'1:5-15\'], [\'1:13-30\']], [\'1:13-15\']),\n      ([[\'1:10-20\'], [\'1:5-15\'], [\'1:25-30\']], []),\n      # Check that different chromosomes are kept separate.\n      ([[\'1:10-20\'], [\'2:10-20\']], []),\n      ([[\'1:10-20\', \'2:11-14\'], [\'1:11-14\']], [\'1:11-14\']),\n      ([[\'1:10-20\', \'2:11-14\'], [\'2:10-20\']], [\'2:11-14\']),\n  )\n  def test_intersection(self, regions, expected):\n    regions_list = [ranges.RangeSet.from_regions(r) for r in regions]\n    copies = [ranges.RangeSet(rs) for rs in regions_list]\n\n    # Check that the intersection is as expected.\n    self.assertCountEqual(\n        ranges.RangeSet.from_regions(expected),\n        regions_list[0].intersection(*regions_list[1:]))\n\n    # Check that the intersection is as expected even if we do it in a different\n    # direction.\n    self.assertCountEqual(\n        ranges.RangeSet.from_regions(expected),\n        regions_list[-1].intersection(*regions_list[:-1]))\n\n    # Check that no one was modified.\n    for pre, post in zip(copies, regions_list):\n      self.assertCountEqual(pre, post)\n\n  @parameterized.parameters(\n      dict(lhs=[\'1:1-100\'], rhs=[\'1:10-20\'], expected=[\'1:1-9\', \'1:21-100\']),\n      dict(lhs=[\'1:1-100\'], rhs=[], expected=[\'1:1-100\']),\n      dict(lhs=[\'1:1-100\', \'2:1-10\'], rhs=[\'2:1-100\'], expected=[\'1:1-100\']),\n      dict(\n          lhs=[\'1:1-100\'],\n          rhs=[\'1:10-20\', \'1:15-30\'],\n          expected=[\'1:1-9\', \'1:31-100\']),\n      dict(\n          lhs=[\'1:1-100\'],\n          rhs=[\'1:10-20\', \'1:30-40\'],\n          expected=[\'1:1-9\', \'1:21-29\', \'1:41-100\']),\n      # Excluding regions not in lhs has no impact.\n      dict(lhs=[\'1:1-100\'], rhs=[\'2:1-100\'], expected=[\'1:1-100\']),\n      # Check that excluding the whole region results in an empty RangeSet.\n      dict(lhs=[\'1:1-100\'], rhs=[\'1:1-100\'], expected=[]),\n      # An empty tree remains empty.\n      dict(lhs=[], rhs=[\'1:1-100\'], expected=[]),\n  )\n  def test_exclude_regions(self, lhs, rhs, expected):\n    lhs = ranges.RangeSet.from_regions(lhs)\n    rhs = ranges.RangeSet.from_regions(rhs)\n    # Mutating operation returns None.\n    self.assertIsNone(lhs.exclude_regions(rhs))\n    self.assertCountEqual(ranges.RangeSet.from_regions(expected), lhs)\n\n  @parameterized.parameters((\'chr1\', ranges.make_range(\'chr1\', 0, 10)),\n                            (\'chr2\', ranges.make_range(\'chr2\', 0, 5)))\n  def test_parse_literal_with_contig_map(self, contig_name, expected):\n    contig_map = {\n        \'chr1\': reference_pb2.ContigInfo(name=\'chr1\', n_bases=10),\n        \'chr2\': reference_pb2.ContigInfo(name=\'chr2\', n_bases=5),\n    }\n    self.assertEqual(\n        ranges.parse_literal(contig_name, contig_map=contig_map), expected)\n\n  @parameterized.parameters([\'x\', \'chr1:\', \'chr1:10-\', \'chr1:-1-10\'])\n  def test_parse_literal_with_contig_map_and_bad_input_raises_exception(\n      self, bad_literal):\n    with self.assertRaises(ValueError):\n      ranges.parse_literal(\n          bad_literal,\n          contig_map={\n              \'chr1\': reference_pb2.ContigInfo(name=\'chr1\', n_bases=10)\n          })\n\n  def test_from_contigs(self):\n    contigs = [\n        reference_pb2.ContigInfo(name=\'chr1\', n_bases=10),\n        reference_pb2.ContigInfo(name=\'chr2\', n_bases=5),\n    ]\n    self.assertCountEqual([\n        ranges.make_range(\'chr1\', 0, 10),\n        ranges.make_range(\'chr2\', 0, 5),\n    ], ranges.RangeSet.from_contigs(contigs))\n\n  @parameterized.parameters(\n      # Chop our contigs into 50 bp pieces.\n      (50, [(\'chr1\', 0, 50), (\'chr1\', 50, 76), (\'chr2\', 0, 50),\n            (\'chr2\', 50, 100), (\'chr2\', 100, 121), (\'chrM\', 0, 50),\n            (\'chrM\', 50, 100)]),\n      # Chop our contigs in 120 bp pieces, leaving a 1 bp fragment in chr2.\n      (120, [(\'chr1\', 0, 76), (\'chr2\', 0, 120), (\'chr2\', 120, 121),\n             (\'chrM\', 0, 100)]),\n      # A 500 max size spans each of our contigs fully.\n      (500, [(\'chr1\', 0, 76), (\'chr2\', 0, 121), (\'chrM\', 0, 100)]),\n  )\n  def test_partitions(self, interval_size, expected):\n    rangeset = ranges.RangeSet([\n        ranges.make_range(\'chrM\', 0, 100),\n        ranges.make_range(\'chr1\', 0, 76),\n        ranges.make_range(\'chr2\', 0, 121),\n    ])\n    self.assertEqual([ranges.make_range(*args) for args in expected],\n                     list(rangeset.partition(interval_size)))\n\n  def test_partitions_bad_interval_size_raises(self):\n    # list() is necessary to force the generator to execute.\n    with self.assertRaisesRegexp(ValueError, \'max_size\'):\n      list(ranges.RangeSet([ranges.make_range(\'chrM\', 0, 100)]).partition(-10))\n    with self.assertRaisesRegexp(ValueError, \'max_size\'):\n      list(ranges.RangeSet([ranges.make_range(\'chrM\', 0, 100)]).partition(0))\n\n  @parameterized.parameters(\n      (10, [(\'1\', 0, 10), (\'1\', 20, 30), (\'1\', 30, 40), (\'1\', 45, 50)]),\n      (7, [(\'1\', 0, 7), (\'1\', 7, 10), (\'1\', 20, 27), (\'1\', 27, 34),\n           (\'1\', 34, 40), (\'1\', 45, 50)]),\n      (50, [(\'1\', 0, 10), (\'1\', 20, 40), (\'1\', 45, 50)]),\n  )\n  def test_partition_of_multiple_intervals(self, interval_size, expected):\n    rangeset = ranges.RangeSet([\n        ranges.make_range(\'1\', 0, 10),\n        ranges.make_range(\'1\', 20, 40),\n        ranges.make_range(\'1\', 45, 50),\n    ])\n    self.assertCountEqual([ranges.make_range(*args) for args in expected],\n                          rangeset.partition(interval_size))\n\n  def test_bed_parser(self):\n    test_bed_path = test_utils.test_tmpfile(\n        \'test_bed_parser.bed\', \'\\n\'.join([\n            \'chr20\\t61724611\\t61725646\', \'chr20\\t61304163\\t61305182\',\n            \'chr20\\t61286467\\t61286789\'\n        ]))\n    self.assertEqual(\n        list(ranges.bed_parser(test_bed_path)), [\n            ranges.make_range(\'chr20\', 61724611, 61725646),\n            ranges.make_range(\'chr20\', 61304163, 61305182),\n            ranges.make_range(\'chr20\', 61286467, 61286789),\n        ])\n\n  def test_bedpe_parser(self):\n    # pylint: disable=line-too-long\n    data = \'\\n\'.join([\n        \'chr20\\t25763416\\t25765517\\tchr20\\t25825181\\t25826882\\tP2_PM_20_1549\\t63266\\t+\\tTYPE:DELETION\',\n        \'chr20\\t25972820\\t25972991\\tchr20\\t26045347\\t26045538\\tP2_PM_20_696\\t72548\\t+\\tTYPE:DELETION\',\n        \'chr20\\t23719873\\t23721974\\tchr20\\t23794822\\t23796523\\tP2_PM_20_1548\\t76450\\t+\\tTYPE:DELETION\',\n    ])\n    test_bedpe_path = test_utils.test_tmpfile(\'test_bedpe_parser.bedpe\', data)\n    self.assertEqual(\n        list(ranges.bedpe_parser(test_bedpe_path)), [\n            ranges.make_range(\'chr20\', 25763416, 25826882),\n            ranges.make_range(\'chr20\', 25972820, 26045538),\n            ranges.make_range(\'chr20\', 23719873, 23796523),\n        ])\n\n  def test_bedpe_parser_skips_cross_chr_events(self):\n    # pylint: disable=line-too-long\n    data = \'\\n\'.join([\n        \'chr20\\t25763416\\t25765517\\tchr21\\t25825181\\t25826882\\tP2_PM_20_1549\\t63266\\t+\\tTYPE:DELETION\',\n        \'chr20\\t25972820\\t25972991\\tchr20\\t26045347\\t26045538\\tP2_PM_20_696\\t72548\\t+\\tTYPE:DELETION\',\n        \'chr20\\t23719873\\t23721974\\tchr20\\t23794822\\t23796523\\tP2_PM_20_1548\\t76450\\t+\\tTYPE:DELETION\',\n    ])\n    test_bedpe_path = test_utils.test_tmpfile(\'test_bedpe_parser2.bedpe\', data)\n    self.assertEqual(\n        list(ranges.bedpe_parser(test_bedpe_path)), [\n            ranges.make_range(\'chr20\', 25972820, 26045538),\n            ranges.make_range(\'chr20\', 23719873, 23796523),\n        ])\n\n  def test_contigs_n_bases(self):\n    c1 = reference_pb2.ContigInfo(name=\'c\', n_bases=100, pos_in_fasta=0)\n    c2 = reference_pb2.ContigInfo(name=\'a\', n_bases=50, pos_in_fasta=1)\n    c3 = reference_pb2.ContigInfo(name=\'b\', n_bases=25, pos_in_fasta=2)\n    self.assertEqual(100, ranges.contigs_n_bases([c1]))\n    self.assertEqual(50, ranges.contigs_n_bases([c2]))\n    self.assertEqual(25, ranges.contigs_n_bases([c3]))\n    self.assertEqual(150, ranges.contigs_n_bases([c1, c2]))\n    self.assertEqual(125, ranges.contigs_n_bases([c1, c3]))\n    self.assertEqual(175, ranges.contigs_n_bases([c1, c2, c3]))\n\n  def test_rangeset_iteration_order(self):\n    contigs = [\n        reference_pb2.ContigInfo(name=\'c\', n_bases=100, pos_in_fasta=0),\n        reference_pb2.ContigInfo(name=\'b\', n_bases=121, pos_in_fasta=2),\n        reference_pb2.ContigInfo(name=\'a\', n_bases=76, pos_in_fasta=1),\n    ]\n    unsorted = ranges.parse_literals(\n        [\'a:10\', \'c:20\', \'b:30\', \'b:10-15\', \'a:5\'])\n\n    # Iteration order over a RangeSet instantiated with a contigs list is\n    # determined by pos_in_fasta, start, end.\n    range_set_with_contigs = ranges.RangeSet(unsorted, contigs)\n    self.assertEqual(\n        ranges.parse_literals(\n            [\'c:20\', \'a:5\', \'a:10\', \'b:10-15\', \'b:30\']),\n        [range_ for range_ in range_set_with_contigs])\n\n    # For a RangeSet instantiated *without* a contig map, the iteration order\n    # is determined by reference_name, start, end.\n    range_set_no_contigs = ranges.RangeSet(unsorted)\n    self.assertEqual(\n        ranges.parse_literals(\n            [\'a:5\', \'a:10\', \'b:10-15\', \'b:30\', \'c:20\']),\n        [range_ for range_ in range_set_no_contigs])\n\n  def test_sort_ranges(self):\n    contigs = [\n        reference_pb2.ContigInfo(name=\'c\', n_bases=100, pos_in_fasta=0),\n        reference_pb2.ContigInfo(name=\'a\', n_bases=76, pos_in_fasta=1),\n        reference_pb2.ContigInfo(name=\'b\', n_bases=121, pos_in_fasta=2),\n    ]\n    unsorted = ranges.parse_literals(\n        [\'a:10\', \'c:20\', \'b:30\', \'b:10-15\', \'b:10\', \'a:5\'])\n\n    # Without contigs we sort the contigs by name lexicographically.\n    self.assertEqual(\n        ranges.parse_literals(\n            [\'a:5\', \'a:10\', \'b:10\', \'b:10-15\', \'b:30\', \'c:20\']),\n        ranges.sorted_ranges(unsorted))\n\n    # With contigs we sort by the position of the contigs themselves.\n    self.assertEqual(\n        ranges.parse_literals(\n            [\'c:20\', \'a:5\', \'a:10\', \'b:10\', \'b:10-15\', \'b:30\']),\n        ranges.sorted_ranges(unsorted, contigs))\n\n  @parameterized.parameters(\n      (ranges.make_range(\'1\', 0, 10), ranges.make_range(\'2\', 0, 10), 0),\n      (ranges.make_range(\'1\', 0, 10), ranges.make_range(\'1\', 10, 20), 0),\n      (ranges.make_range(\'1\', 0, 10), ranges.make_range(\'1\', 100, 200), 0),\n      (ranges.make_range(\'1\', 10, 10), ranges.make_range(\'1\', 0, 20), 0),\n      (ranges.make_range(\'1\', 0, 100), ranges.make_range(\'1\', 50, 99), 49),\n      # Check that the overlap handles a few key edge cases.\n      (ranges.make_range(\'1\', 0, 10), ranges.make_range(\'1\', 0, 1), 1),\n      (ranges.make_range(\'1\', 0, 10), ranges.make_range(\'1\', 0, 2), 2),\n      (ranges.make_range(\'1\', 1, 10), ranges.make_range(\'1\', 0, 1), 0),\n  )\n  def test_overlap_len(self, region_1, region_2, expected_overlap):\n    """"""Test ReadAssigner.overlap_len().""""""\n    self.assertEqual(expected_overlap, ranges.overlap_len(region_1, region_2))\n    self.assertEqual(expected_overlap, ranges.overlap_len(region_2, region_1))\n\n  @parameterized.parameters(\n      # No search_regions produces None.\n      dict(\n          query_range=ranges.make_range(\'1\', 20, 30),\n          search_ranges=[],\n          expected=None),\n\n      # Read overlaps with none of the ranges returns None.\n      dict(\n          query_range=ranges.make_range(\'1\', 20, 30),\n          search_ranges=[\n              ranges.make_range(\'1\', 0, 10),\n              ranges.make_range(\'1\', 5, 10)\n          ],\n          expected=None),\n\n      # Read has longer overlap with the first range.\n      dict(\n          query_range=ranges.make_range(\'1\', 4, 10),\n          search_ranges=[\n              ranges.make_range(\'1\', 0, 10),\n              ranges.make_range(\'1\', 5, 10)\n          ],\n          expected=0),\n\n      # Read has longer overlap with the second range.\n      dict(\n          query_range=ranges.make_range(\'1\', 9, 20),\n          search_ranges=[\n              ranges.make_range(\'1\', 0, 10),\n              ranges.make_range(\'1\', 5, 15)\n          ],\n          expected=1),\n\n      # Read has the maximum overlap with the third range.\n      dict(\n          query_range=ranges.make_range(\'1\', 9, 20),\n          search_ranges=[\n              ranges.make_range(\'1\', 0, 10),\n              ranges.make_range(\'1\', 0, 15),\n              ranges.make_range(\'1\', 5, 20)\n          ],\n          expected=2),\n\n      # Read has the maximum overlap with the middle range.\n      dict(\n          query_range=ranges.make_range(\'1\', 5, 13),\n          search_ranges=[\n              ranges.make_range(\'1\', 0, 10),\n              ranges.make_range(\'1\', 0, 15),\n              ranges.make_range(\'1\', 10, 20)\n          ],\n          expected=1),\n\n      # Read has a different reference_name with other ranges.\n      dict(\n          query_range=ranges.make_range(\'2\', 0, 10),\n          search_ranges=[\n              ranges.make_range(\'1\', 0, 10),\n              ranges.make_range(\'2\', 5, 15),\n              ranges.make_range(\'3\', 0, 10)\n          ],\n          expected=1),\n\n      # Read has equal overlap in two ranges.\n      dict(\n          query_range=ranges.make_range(\'1\', 5, 15),\n          search_ranges=[\n              ranges.make_range(\'1\', 0, 10),\n              ranges.make_range(\'1\', 10, 20),\n              ranges.make_range(\'1\', 12, 20)\n          ],\n          expected=0),\n  )\n  def test_find_max_overlapping(self, query_range, search_ranges, expected):\n    actual = ranges.find_max_overlapping(query_range, search_ranges)\n    self.assertEqual(expected, actual)\n\n  def test_find_max_overlapping_allows_unordered_search_ranges(self):\n    query_range = ranges.make_range(\'1\', 4, 12)\n    search_ranges = [\n        ranges.make_range(\'1\', 0, 10),\n        ranges.make_range(\'1\', 10, 20),\n        ranges.make_range(\'1\', 12, 20)\n    ]\n    max_overlapping_range = search_ranges[0]\n\n    for permutated_ranges in itertools.permutations(search_ranges):\n      self.assertEqual(\n          permutated_ranges.index(max_overlapping_range),\n          ranges.find_max_overlapping(query_range, permutated_ranges))\n\n  def test_find_max_overlapping_returns_least_index(self):\n    query_range = ranges.make_range(\'1\', 0, 10)\n    search_ranges = [\n        ranges.make_range(\'1\', 0, 5),\n        ranges.make_range(\'1\', 5, 10)\n    ]\n\n    for to_search in [search_ranges, list(reversed(search_ranges))]:\n      self.assertEqual(0, ranges.find_max_overlapping(query_range, to_search))\n\n  @parameterized.parameters(\n      dict(\n          regions=[\n              ranges.make_range(\'1\', 1, 10),\n          ],\n          expected_span=ranges.make_range(\'1\', 1, 10),\n      ),\n      dict(\n          regions=[\n              ranges.make_range(\'1\', 1, 10),\n              ranges.make_range(\'1\', 10, 100),\n          ],\n          expected_span=ranges.make_range(\'1\', 1, 100),\n      ),\n      dict(\n          regions=[\n              ranges.make_range(\'1\', 1, 10),\n              ranges.make_range(\'1\', 10, 100),\n              ranges.make_range(\'1\', 2, 20),\n          ],\n          expected_span=ranges.make_range(\'1\', 1, 100),\n      ),\n      # potential edge cases:\n      # same start, different ends.\n      dict(\n          regions=[\n              ranges.make_range(\'1\', 1, 10),\n              ranges.make_range(\'1\', 1, 100),\n          ],\n          expected_span=ranges.make_range(\'1\', 1, 100),\n      ),\n      # same end, different starts.\n      dict(\n          regions=[\n              ranges.make_range(\'1\', 1, 10),\n              ranges.make_range(\'1\', 2, 10),\n          ],\n          expected_span=ranges.make_range(\'1\', 1, 10),\n      ),\n  )\n  def test_span_computes_span_correctly(self, regions, expected_span):\n    for permutation in itertools.permutations(regions, len(regions)):\n      self.assertEqual(expected_span, ranges.span(permutation))\n\n  @parameterized.parameters(\n      dict(regions=[], regexp=\'empty\'),\n      dict(\n          regions=[\n              ranges.make_range(\'1\', 0, 2),\n              ranges.make_range(\'2\', 0, 2),\n          ],\n          regexp=\'regions must be all on the same contig\'),\n      dict(\n          regions=[\n              ranges.make_range(\'1\', 0, 2),\n              ranges.make_range(\'1\', 0, 3),\n              ranges.make_range(\'2\', 0, 2),\n          ],\n          regexp=\'regions must be all on the same contig\'),\n  )\n  def test_span_raises_on_bad_input(self, regions, regexp):\n    with self.assertRaisesRegexp(ValueError, regexp):\n      ranges.span(regions)\n\n  @parameterized.parameters(\n      dict(\n          region=ranges.make_range(\'1\', 10, 20),\n          n_bp=n_bp,\n          contig_map=None,\n          expected=ranges.make_range(\'1\', 10 - n_bp, 20 + n_bp),\n      ) for n_bp in range(10))\n  def test_expand_is_correct(self, region, n_bp, contig_map, expected):\n    self.assertEqual(expected, ranges.expand(region, n_bp, contig_map))\n\n  @parameterized.parameters(\n      # Check that we don\'t create Ranges with negative starts.\n      dict(\n          region=ranges.make_range(\'1\', 10, 20),\n          n_bp=20,\n          contig_map=None,\n          expected=ranges.make_range(\'1\', 0, 40),\n      ),\n      # Check that we respect n_bp if contig_map is provided.\n      dict(\n          region=ranges.make_range(\'1\', 10, 20),\n          n_bp=40,\n          contig_map={\n              \'1\': reference_pb2.ContigInfo(name=\'1\', n_bases=50),\n          },\n          expected=ranges.make_range(\'1\', 0, 50),\n      ),\n  )\n  def test_expand_handles_boundaries(self, region, n_bp, contig_map, expected):\n    self.assertEqual(expected, ranges.expand(region, n_bp, contig_map))\n\n  def test_expand_raises_on_negative_n_bp(self):\n    with self.assertRaisesRegexp(ValueError, \'n_bp must be >= 0 but got -10\'):\n      ranges.expand(ranges.make_range(\'1\', 10, 20), -10)\n\n  def test_expand_raises_with_missing_contig_in_map(self):\n    # Empty contig_map should raise.\n    with self.assertRaises(KeyError):\n      ranges.expand(ranges.make_range(\'1\', 10, 20), 1, contig_map={})\n\n    # Missing \'1\' from the contig map should raise.\n    with self.assertRaises(KeyError):\n      ranges.expand(\n          ranges.make_range(\'1\', 10, 20),\n          1,\n          contig_map={\n              \'2\': reference_pb2.ContigInfo(name=\'2\', n_bases=50),\n          })\n\n  @parameterized.parameters(\n      dict(\n          region=ranges.make_range(chrom, start, start + length),\n          expected_length=length,\n      )\n      for length in range(10)\n      for start in [10, 20, 1000]\n      for chrom in [\'1\', \'20\']\n  )\n  def test_length_is_correct(self, region, expected_length):\n    self.assertEqual(expected_length, ranges.length(region))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/util/sequence_utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Utility functions for manipulating DNA sequences.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass Error(Exception):\n  """"""Base error class.""""""\n\n\ndef _add_lowercase(d):\n  """"""Returns a dictionary with the lowercase keys and values entered.""""""\n  retval = d.copy()\n  retval.update({k.lower(): v.lower() for k, v in d.items()})\n  return retval\n\n\nSTRICT_DNA_COMPLEMENT_UPPER = {\'A\': \'T\', \'T\': \'A\', \'C\': \'G\', \'G\': \'C\'}\nDNA_COMPLEMENT_UPPER = {\'A\': \'T\', \'T\': \'A\', \'C\': \'G\', \'G\': \'C\', \'N\': \'N\'}\nIUPAC_DNA_COMPLEMENT_UPPER = {\n    \'A\': \'T\',\n    \'T\': \'A\',\n    \'C\': \'G\',\n    \'G\': \'C\',\n    \'R\': \'Y\',  # R is A/G\n    \'Y\': \'R\',  # Y is C/T\n    \'S\': \'S\',  # S is C/G\n    \'W\': \'W\',  # W is A/T\n    \'K\': \'M\',  # K is G/T\n    \'M\': \'K\',  # M is A/C\n    \'B\': \'V\',  # B is C/G/T\n    \'V\': \'B\',  # V is A/C/G\n    \'D\': \'H\',  # D is A/G/T\n    \'H\': \'D\',  # H is A/C/T\n    \'N\': \'N\',  # N is any base\n}\n\nIUPAC_TO_CANONICAL_BASES_UPPER = {\n    \'A\': [\'A\'],\n    \'T\': [\'T\'],\n    \'C\': [\'C\'],\n    \'G\': [\'G\'],\n    \'R\': [\'A\', \'G\'],\n    \'Y\': [\'C\', \'T\'],\n    \'S\': [\'C\', \'G\'],\n    \'W\': [\'A\', \'T\'],\n    \'K\': [\'G\', \'T\'],\n    \'M\': [\'A\', \'C\'],\n    \'B\': [\'C\', \'G\', \'T\'],\n    \'V\': [\'A\', \'C\', \'G\'],\n    \'D\': [\'A\', \'G\', \'T\'],\n    \'H\': [\'A\', \'C\', \'T\'],\n    \'N\': [\'A\', \'C\', \'G\', \'T\'],\n}\n\nSTRICT_DNA_COMPLEMENT = _add_lowercase(STRICT_DNA_COMPLEMENT_UPPER)\nDNA_COMPLEMENT = _add_lowercase(DNA_COMPLEMENT_UPPER)\nIUPAC_DNA_COMPLEMENT = _add_lowercase(IUPAC_DNA_COMPLEMENT_UPPER)\n\n\nSTRICT_DNA_BASES_UPPER = frozenset([\'A\', \'C\', \'G\', \'T\'])\nSTRICT_DNA_BASES = frozenset([\'a\', \'c\', \'g\', \'t\', \'A\', \'C\', \'G\', \'T\'])\nDNA_BASES_UPPER = frozenset([\'A\', \'C\', \'G\', \'T\', \'N\'])\nDNA_BASES = frozenset([\'a\', \'c\', \'g\', \'t\', \'n\', \'A\', \'C\', \'G\', \'T\', \'N\'])\n\n\ndef reverse_complement(sequence, complement_dict=None):\n  """"""Returns the reverse complement of a DNA sequence.\n\n  By default this will successfully reverse complement sequences comprised\n  solely of A, C, G, and T letters. Other complement dictionaries can be\n  passed in for more permissive matching.\n\n  Args:\n    sequence: str. The input sequence to reverse complement.\n    complement_dict: dict[str, str]. The lookup dictionary holding the\n      complement base pairs.\n\n  Returns:\n    The reverse complement DNA sequence.\n\n  Raises:\n    Error: The sequence contains letters not present in complement_dict.\n  """"""\n  if complement_dict is None:\n    complement_dict = STRICT_DNA_COMPLEMENT_UPPER\n\n  try:\n    return \'\'.join(complement_dict[nt] for nt in reversed(sequence))\n  except KeyError:\n    raise Error(\'Unknown base in {}, cannot reverse complement using {}\'.format(\n        sequence, str(complement_dict)))\n'"
third_party/nucleus/util/sequence_utils_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for third_party.nucleus.util.sequence_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\n\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.util import sequence_utils\n\n\nclass SequenceUtilsTests(parameterized.TestCase):\n\n  @parameterized.parameters(\n      dict(seq=\'\', expected=\'\'),\n      dict(seq=\'A\', expected=\'T\'),\n      dict(seq=\'T\', expected=\'A\'),\n      dict(seq=\'C\', expected=\'G\'),\n      dict(seq=\'G\', expected=\'C\'),\n      dict(seq=\'GGGCAGATT\', expected=\'AATCTGCCC\'),\n      dict(\n          seq=\'GGGCAGANN\',\n          expected=\'NNTCTGCCC\',\n          complement_dict=sequence_utils.DNA_COMPLEMENT_UPPER),\n      dict(\n          seq=\'accgt\',\n          expected=\'acggt\',\n          complement_dict=sequence_utils.DNA_COMPLEMENT),\n      dict(\n          seq=\'ATCGRYSWKMBVDHN\',\n          expected=\'NDHBVKMWSRYCGAT\',\n          complement_dict=sequence_utils.IUPAC_DNA_COMPLEMENT_UPPER),\n      dict(\n          seq=\'ATCGRYSWKMBVDHNatcgryswkmbvdhn\',\n          expected=\'ndhbvkmwsrycgatNDHBVKMWSRYCGAT\',\n          complement_dict=sequence_utils.IUPAC_DNA_COMPLEMENT),\n  )\n  def test_reverse_complement(self, seq, expected, complement_dict=None):\n    """"""Tests canonical DNA sequences are reverse complemented correctly.""""""\n    self.assertEqual(\n        sequence_utils.reverse_complement(seq, complement_dict), expected)\n\n  @parameterized.parameters(\n      dict(seq=\'GGGCAGANN\'),\n      dict(seq=\'accgt\'),\n      dict(\n          seq=\'ATCGRYSWKMBVDHNatcgryswkmbvdhn\',\n          complement_dict=sequence_utils.IUPAC_DNA_COMPLEMENT_UPPER),\n      dict(seq=\'X\', complement_dict=sequence_utils.IUPAC_DNA_COMPLEMENT),\n  )\n  def test_bad_reverse_complement(self, seq, complement_dict=None):\n    """"""Tests error is raised when complement_dict does not cover given seq.""""""\n    with self.assertRaisesRegexp(sequence_utils.Error, \'Unknown base in\'):\n      sequence_utils.reverse_complement(seq, complement_dict)\n\n  @parameterized.parameters(\n      dict(\n          bases_set=sequence_utils.STRICT_DNA_BASES_UPPER,\n          complement_dict=sequence_utils.STRICT_DNA_COMPLEMENT_UPPER),\n      dict(\n          bases_set=sequence_utils.STRICT_DNA_BASES,\n          complement_dict=sequence_utils.STRICT_DNA_COMPLEMENT),\n      dict(\n          bases_set=sequence_utils.DNA_BASES_UPPER,\n          complement_dict=sequence_utils.DNA_COMPLEMENT_UPPER),\n      dict(\n          bases_set=sequence_utils.DNA_BASES,\n          complement_dict=sequence_utils.DNA_COMPLEMENT),\n  )\n  def test_base_set_definitions(self, bases_set, complement_dict):\n    """"""Tests that base set and complement dict definitions are consistent.""""""\n    self.assertEqual(bases_set, frozenset(complement_dict.keys()))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/util/struct_utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Struct proto utilities.\n\nThis class provides wrappers for conveniently interacting with protos defined\nin struct.proto, mostly ListValue and Value objects. It should primarily be used\nby variant_utils and variantcallutils rather than being used directly.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport types\n\nfrom third_party.nucleus.protos import struct_pb2\n\n# Field names of values defined in struct_pb2.Value.\n_BOOL_TYPE = \'bool_value\'\n_INT_TYPE = \'int_value\'\n_NUMBER_TYPE = \'number_value\'\n_STRING_TYPE = \'string_value\'\n\n\ndef _add_field_with_type(field_map, field_name, value, value_type):\n  """"""Adds values to a particular map field containing a ListValue.""""""\n  if not isinstance(value, (list, types.GeneratorType, tuple)):\n    value = [value]\n  struct_values = [struct_pb2.Value(**{value_type: v}) for v in value]\n  field_map[field_name].values.extend(struct_values)\n\n\ndef _set_field_with_type(field_map, field_name, value, value_type):\n  """"""Sets values to a particular map field containing a ListValue.""""""\n  if field_name in field_map:\n    del field_map[field_name]\n  _add_field_with_type(field_map, field_name, value, value_type)\n\n\ndef _get_field_with_type(field_map, field_name, is_single_field, value_type):\n  fields = [getattr(v, value_type) for v in field_map[field_name].values]\n  return fields[0] if is_single_field and fields else fields\n\n\ndef add_number_field(field_map, field_name, value):\n  """"""Appends the given number value(s) to field_map[field_name].\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to append value to.\n    value: The number value(s) to append to the field. This can be a single\n      number or a list of numbers.\n  """"""\n  _add_field_with_type(field_map, field_name, value, _NUMBER_TYPE)\n\n\ndef set_number_field(field_map, field_name, value):\n  """"""Sets field_map[field_name] with the given number value(s).\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to set.\n    value: The number value(s) to set the field to. This can be a single number\n      or a list of numbers.\n  """"""\n  _set_field_with_type(field_map, field_name, value, _NUMBER_TYPE)\n\n\ndef get_number_field(field_map, field_name, is_single_field=False):\n  """"""Returns the number value(s) stored in `field_map[field_name]`.\n\n  If the field_name is not present in field_map, the empty list is returned.\n\n  Args:\n    field_map: Map(str --> ListValue) of interest.\n    field_name: str. The name of the field to extract number values from.\n    is_single_field: bool. If True, return the first number value stored (it\n      should be the only one in the field). Otherwise, return the list of\n      numbers.\n\n  Returns:\n    The number value(s) stored in the field_map under this field_name.\n  """"""\n  return _get_field_with_type(field_map, field_name, is_single_field,\n                              _NUMBER_TYPE)\n\n\ndef add_int_field(field_map, field_name, value):\n  """"""Appends the given int value(s) to field_map[field_name].\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to append value to.\n    value: The int value(s) to append to the field. This can be a single\n      int or a list of ints.\n  """"""\n  _add_field_with_type(field_map, field_name, value, _INT_TYPE)\n\n\ndef set_int_field(field_map, field_name, value):\n  """"""Sets field_map[field_name] with the given int value(s).\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to set.\n    value: The int value(s) to set the field to. This can be a single int\n      or a list of ints.\n  """"""\n  _set_field_with_type(field_map, field_name, value, _INT_TYPE)\n\n\ndef get_int_field(field_map, field_name, is_single_field=False):\n  """"""Returns the int value(s) stored in `field_map[field_name]`.\n\n  If the field_name is not present in field_map, the empty list is returned.\n\n  Args:\n    field_map: Map(str --> ListValue) of interest.\n    field_name: str. The name of the field to extract int values from.\n    is_single_field: bool. If True, return the first int value stored (it\n      should be the only one in the field). Otherwise, return the list of\n      ints.\n\n  Returns:\n    The int value(s) stored in the field_map under this field_name.\n  """"""\n  return _get_field_with_type(field_map, field_name, is_single_field, _INT_TYPE)\n\n\ndef add_string_field(field_map, field_name, value):\n  """"""Appends the given string value(s) to field_map[field_name].\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to append value to.\n    value: The string value(s) to append to the field. This can be a single\n      string or a list of strings.\n  """"""\n  _add_field_with_type(field_map, field_name, value, _STRING_TYPE)\n\n\ndef set_string_field(field_map, field_name, value):\n  """"""Sets field_map[field_name] with the given string value(s).\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to set.\n    value: The int value(s) to set the field to. This can be a single string or\n      a list of strings.\n  """"""\n  _set_field_with_type(field_map, field_name, value, _STRING_TYPE)\n\n\ndef get_string_field(field_map, field_name, is_single_field=False):\n  """"""Returns the string value(s) stored in `field_map[field_name]`.\n\n  If the field_name is not present in field_map, the empty list is returned.\n\n  Args:\n    field_map: Map(str --> ListValue) of interest.\n    field_name: str. The name of the field to extract string values from.\n    is_single_field: bool. If True, return the first string value stored (it\n      should be the only one in the field). Otherwise, return the list of\n      strings.\n\n  Returns:\n    The string value(s) stored in the field_map under this field_name.\n  """"""\n  return _get_field_with_type(field_map, field_name, is_single_field,\n                              _STRING_TYPE)\n\n\ndef add_bool_field(field_map, field_name, value):\n  """"""Appends the given boolean value(s) to field_map[field_name].\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to append value to.\n    value: The boolean value(s) to append to the field. This can be a single\n      boolean or a list of booleans.\n  """"""\n  _add_field_with_type(field_map, field_name, value, _BOOL_TYPE)\n\n\ndef set_bool_field(field_map, field_name, value):\n  """"""Sets field_map[field_name] with the given boolean value(s).\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to set.\n    value: The boolean value(s) to set the field to. This can be a single\n      boolean or a list of booleans.\n  """"""\n  _set_field_with_type(field_map, field_name, value, _BOOL_TYPE)\n\n\ndef get_bool_field(field_map, field_name, is_single_field=False):\n  """"""Returns the bool value(s) stored in `field_map[field_name]`.\n\n  If the field_name is not present in field_map, the empty list is returned.\n\n  Args:\n    field_map: Map(str --> ListValue) of interest.\n    field_name: str. The name of the field to extract bool values from.\n    is_single_field: bool. If True, return the first bool value stored (it\n      should be the only one in the field). Otherwise, return the list of\n      bools.\n\n  Returns:\n    The bool value(s) stored in the field_map under this field_name.\n  """"""\n  return _get_field_with_type(field_map, field_name, is_single_field,\n                              _BOOL_TYPE)\n'"
third_party/nucleus/util/struct_utils_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for third_party.nucleus.util.struct_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport sys\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom third_party.nucleus.protos import struct_pb2\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import struct_utils\n\n\ndef _set_protomap_from_dict(d):\n  """"""Returns a proto Map(str --> ListValue) with the given fields set.\n\n  Args:\n    d: dict(str --> list(Value)). The data to populate.\n\n  Returns:\n    The protocol buffer-defined Map(str --> ListValue).\n  """"""\n  # We use a Variant as an intermediate data structure since it contains the\n  # desired output map types.\n  v = variants_pb2.Variant()\n  for key, values in d.items():\n    v.info[key].values.extend(values)\n  return v.info\n\n\ndef _wrapped_value_and_num(value):\n  """"""Returns a list containing value plus the list\'s length.""""""\n  if isinstance(value, (list, tuple)):\n    return value, len(value)\n  else:\n    return [value], 1\n\n\nclass StructUtilsTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=1, expected=[1]),\n      dict(initial_fields={}, value=[1], expected=[1]),\n      dict(initial_fields={}, value=[1, 2.5], expected=[1, 2.5]),\n      dict(initial_fields={\'field\': []}, value=[1, 2.5], expected=[1, 2.5]),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(number_value=5)]},\n          value=[1, 2.5],\n          expected=[5, 1, 2.5]),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(number_value=5),\n                  struct_pb2.Value(number_value=-3.3),\n              ]\n          },\n          value=[1, 2.5],\n          expected=[5, -3.3, 1, 2.5]),\n  )\n  def test_add_number_field(self, initial_fields, value, expected):\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.add_number_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertEqual([v.number_value for v in field_map[\'field\'].values],\n                     expected)\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=1),\n      dict(initial_fields={}, value=[1]),\n      dict(initial_fields={}, value=[1, 2.5]),\n      dict(initial_fields={\'field\': []}, value=[1, 2.5]),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(number_value=5)]},\n          value=[1, 2.5]),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(number_value=5),\n                  struct_pb2.Value(number_value=-3.3),\n              ]\n          },\n          value=[1, 2.5]),\n  )\n  def test_set_number_field(self, initial_fields, value):\n    list_value, num_values = _wrapped_value_and_num(value)\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.set_number_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertLen(field_map[\'field\'].values, num_values)\n    self.assertEqual([v.number_value for v in field_map[\'field\'].values],\n                     list_value)\n\n  @parameterized.parameters(\n      dict(value=[], is_single_field=False, expected=[]),\n      dict(value=[], is_single_field=True, expected=[]),\n      dict(value=[1.5], is_single_field=False, expected=[1.5]),\n      dict(value=[1.5], is_single_field=True, expected=1.5),\n      dict(value=[1.5, 2], is_single_field=False, expected=[1.5, 2]),\n      dict(value=[1.5, 2], is_single_field=True, expected=1.5),\n  )\n  def test_get_number_field(self, value, is_single_field, expected):\n    key = \'field\'\n    field_map = _set_protomap_from_dict({})\n    struct_utils.set_number_field(field_map, key, value)\n    actual = struct_utils.get_number_field(field_map, key, is_single_field)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=1, expected=[1]),\n      dict(initial_fields={}, value=[1], expected=[1]),\n      dict(initial_fields={}, value=[1, 2], expected=[1, 2]),\n      dict(initial_fields={\'field\': []}, value=[1, 2], expected=[1, 2]),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(int_value=5)]},\n          value=[1, 2],\n          expected=[5, 1, 2]),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(int_value=5),\n                  struct_pb2.Value(int_value=-3),\n              ]\n          },\n          value=[1, 2],\n          expected=[5, -3, 1, 2]),\n  )\n  def test_add_int_field(self, initial_fields, value, expected):\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.add_int_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertEqual([v.int_value for v in field_map[\'field\'].values], expected)\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=1),\n      dict(initial_fields={}, value=[1]),\n      dict(initial_fields={}, value=[1, 2]),\n      dict(initial_fields={\'field\': []}, value=[1, 2]),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(int_value=5)]},\n          value=[1, 2]),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(int_value=5),\n                  struct_pb2.Value(int_value=-3),\n              ]\n          },\n          value=[1, 2]),\n  )\n  def test_set_int_field(self, initial_fields, value):\n    list_value, num_values = _wrapped_value_and_num(value)\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.set_int_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertLen(field_map[\'field\'].values, num_values)\n    self.assertEqual([v.int_value for v in field_map[\'field\'].values],\n                     list_value)\n\n  @parameterized.parameters(\n      dict(value=[], is_single_field=False, expected=[]),\n      dict(value=[], is_single_field=True, expected=[]),\n      dict(value=[1], is_single_field=False, expected=[1]),\n      dict(value=[1], is_single_field=True, expected=1),\n      dict(value=[1, 2], is_single_field=False, expected=[1, 2]),\n      dict(value=[1, 2], is_single_field=True, expected=1),\n  )\n  def test_get_int_field(self, value, is_single_field, expected):\n    key = \'field\'\n    field_map = _set_protomap_from_dict({})\n    struct_utils.set_int_field(field_map, key, value)\n    actual = struct_utils.get_int_field(field_map, key, is_single_field)\n    self.assertEqual(actual, expected)\n    # Test long handling in Python 2\n    if sys.version_info.major < 3:\n      field_map = _set_protomap_from_dict({})\n      struct_utils.set_int_field(field_map, key, [long(v) for v in value])\n      actual = struct_utils.get_int_field(field_map, key, is_single_field)\n      self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=\'hello\', expected=[\'hello\']),\n      dict(initial_fields={}, value=[\'hello\'], expected=[\'hello\']),\n      dict(initial_fields={}, value=[\'a\', \'aah\'], expected=[\'a\', \'aah\']),\n      dict(\n          initial_fields={\'field\': []},\n          value=[\'bc\', \'de\'],\n          expected=[\'bc\', \'de\']),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(string_value=\'hi\')]},\n          value=[\'a\', \'z\'],\n          expected=[\'hi\', \'a\', \'z\']),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(string_value=\'abc\'),\n                  struct_pb2.Value(string_value=u\'def\'),\n              ]\n          },\n          value=[\'ug\', u\'h\'],\n          expected=[\'abc\', \'def\', \'ug\', \'h\']),\n  )\n  def test_add_string_field(self, initial_fields, value, expected):\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.add_string_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertEqual([v.string_value for v in field_map[\'field\'].values],\n                     expected)\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=\'hello\'),\n      dict(initial_fields={}, value=[\'hello\']),\n      dict(initial_fields={}, value=[\'a\', \'aah\']),\n      dict(initial_fields={\'field\': []}, value=[\'bc\', \'de\']),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(string_value=\'hi\')]},\n          value=[\'a\', \'z\']),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(string_value=\'abc\'),\n                  struct_pb2.Value(string_value=u\'def\'),\n              ]\n          },\n          value=[\'ug\', u\'h\']),\n  )\n  def test_set_string_field(self, initial_fields, value):\n    list_value, num_values = _wrapped_value_and_num(value)\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.set_string_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertLen(field_map[\'field\'].values, num_values)\n    self.assertEqual([v.string_value for v in field_map[\'field\'].values],\n                     list_value)\n\n  @parameterized.parameters(\n      dict(value=[], is_single_field=False, expected=[]),\n      dict(value=[], is_single_field=True, expected=[]),\n      dict(value=[\'hi\'], is_single_field=False, expected=[\'hi\']),\n      dict(value=[\'single\'], is_single_field=True, expected=\'single\'),\n      dict(value=[\'2\', \'f\'], is_single_field=False, expected=[\'2\', \'f\']),\n      dict(value=[\'two\', \'fields\'], is_single_field=True, expected=\'two\'),\n  )\n  def test_get_string_field(self, value, is_single_field, expected):\n    key = \'field\'\n    field_map = _set_protomap_from_dict({})\n    struct_utils.set_string_field(field_map, key, value)\n    actual = struct_utils.get_string_field(field_map, key, is_single_field)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=True, expected=[True]),\n      dict(initial_fields={}, value=[True], expected=[True]),\n      dict(initial_fields={}, value=[True, False], expected=[True, False]),\n      dict(\n          initial_fields={\'field\': []},\n          value=[False, True],\n          expected=[False, True]),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(bool_value=True)]},\n          value=[False, True],\n          expected=[True, False, True]),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(bool_value=False),\n                  struct_pb2.Value(bool_value=True),\n              ]\n          },\n          value=[True, True],\n          expected=[False, True, True, True]),\n  )\n  def test_add_bool_field(self, initial_fields, value, expected):\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.add_bool_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertEqual([v.bool_value for v in field_map[\'field\'].values],\n                     expected)\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=True),\n      dict(initial_fields={}, value=[True]),\n      dict(initial_fields={}, value=[False, True]),\n      dict(initial_fields={\'field\': []}, value=[True, False]),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(bool_value=True)]},\n          value=[True, False]),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(bool_value=False),\n                  struct_pb2.Value(bool_value=True),\n              ]\n          },\n          value=[True, False]),\n  )\n  def test_set_bool_field(self, initial_fields, value):\n    list_value, num_values = _wrapped_value_and_num(value)\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.set_bool_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertLen(field_map[\'field\'].values, num_values)\n    self.assertEqual([v.bool_value for v in field_map[\'field\'].values],\n                     list_value)\n\n  @parameterized.parameters(\n      dict(value=[], is_single_field=False, expected=[]),\n      dict(value=[], is_single_field=True, expected=[]),\n      dict(value=[True], is_single_field=False, expected=[True]),\n      dict(value=[True], is_single_field=True, expected=True),\n      dict(value=[True, False], is_single_field=False, expected=[True, False]),\n      dict(value=[False, True], is_single_field=True, expected=False),\n  )\n  def test_get_bool_field(self, value, is_single_field, expected):\n    key = \'field\'\n    field_map = _set_protomap_from_dict({})\n    struct_utils.set_bool_field(field_map, key, value)\n    actual = struct_utils.get_bool_field(field_map, key, is_single_field)\n    self.assertEqual(actual, expected)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/util/utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Utility functions for working with reads.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom third_party.nucleus.protos import range_pb2\nfrom third_party.nucleus.util.python import utils as utils_cpp\n\n\ndef read_overlaps_region(read, region):\n  """"""Returns True if read overlaps read.\n\n  This function is equivalent to calling:\n\n    `ranges.ranges_overlap(region, read_range(read))`\n\n  But is optimized for speed and memory performance in C++.\n\n  Args:\n    read: nucleus.genomics.v1.Read.\n    region: nucleus.genomics.v1.Range.\n\n  Returns:\n    True if read and region overlap (i.e, have the same reference_name and their\n    start/ends overlap at least one basepair).\n  """"""\n  return utils_cpp.read_overlaps_region(read, region)\n\n\ndef read_range(read):\n  """"""Creates a Range proto from the alignment of Read.\n\n  Args:\n    read: nucleus.genomics.v1.Read. The read to calculate the range for.\n\n  Returns:\n    A nucleus.genomics.v1.Range for read.\n  """"""\n  range_pb = range_pb2.Range()\n  utils_cpp.read_range(read, range_pb)\n  return range_pb\n\n\ndef read_end(read):\n  """"""Returns the read start + alignment length for Read read.""""""\n  return read_range(read).end\n\n\ndef reservoir_sample(iterable, k, random=None):\n  """"""Samples k elements with uniform probability from an iterable.\n\n  Selects a subset of k elements from n input elements with uniform probability\n  without needing to hold all n elements in memory at the same time. This\n  implementation has max space complexity O(min(k, n)), i.e., we allocate up to\n  min(k, n) elements to store the samples. This means that we only use ~n\n  elements when n is smaller than k, which can be important when k is large. If\n  n elements are added to this sampler, and n <= k, all n elements will be\n  retained. If n > k, each added element will be retained with a uniform\n  probability of k / n.\n\n  The order of the k retained samples from our n elements is undefined. In\n  particular that means that the elements in the returned list can occur in a\n  different order than they appeared in the iterable.\n\n  More details about reservoir sampling (and the specific algorithm used here\n  called Algorithm R) can be found on wikipedia:\n\n  https://en.wikipedia.org/wiki/Reservoir_sampling#Algorithm_R\n\n  Args:\n    iterable: Python iterable. The iterable to sample from.\n    k: int. The number of elements to sample.\n    random: A random number generator or None.\n\n  Returns:\n    A list containing the k sampled elements.\n\n  Raises:\n    ValueError: If k is negative.\n  """"""\n  if k < 0:\n    raise ValueError(\'k must be nonnegative, but got {}\'.format(k))\n  if random is None:\n    random = np.random\n  sample = []\n  for i, item in enumerate(iterable):\n    if len(sample) < k:\n      sample.append(item)\n    else:\n      j = random.randint(0, i + 1)\n      if j < k:\n        sample[j] = item\n  return sample\n'"
third_party/nucleus/util/utils_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for third_party.nucleus.util.utils.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport collections\n\nfrom absl.testing import absltest\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport numpy.testing as npt\n\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\nfrom third_party.nucleus.util import utils\n\n\nclass UtilsTest(parameterized.TestCase):\n\n  def test_read_range(self):\n    """"""Tests reads have their ranges calculated correctly.""""""\n    start = 10000001\n    read = test_utils.make_read(\n        \'AAACAG\',\n        chrom=\'chrX\',\n        start=start,\n        cigar=\'2M1I3M\',\n        quals=range(10, 16),\n        name=\'read1\')\n    self.assertEqual(\n        ranges.make_range(\'chrX\', start, start + 5), utils.read_range(read))\n    read = test_utils.make_read(\n        \'AAACAG\',\n        chrom=\'chrX\',\n        start=start,\n        cigar=\'2M16D3M\',\n        quals=range(10, 16),\n        name=\'read1\')\n    self.assertEqual(\n        ranges.make_range(\'chrX\', start, start + 5 + 16),\n        utils.read_range(read))\n\n  def test_read_end(self):\n    """"""Tests reads have their ends calculated correctly.""""""\n    start = 10000001\n    read = test_utils.make_read(\n        \'AAACAG\',\n        chrom=\'chrX\',\n        start=start,\n        cigar=\'2M1I3M\',\n        quals=range(10, 16),\n        name=\'read1\')\n    self.assertEqual(\n        start + 5, utils.read_end(read))\n    read = test_utils.make_read(\n        \'AAACAG\',\n        chrom=\'chrX\',\n        start=start,\n        cigar=\'2M16D3M\',\n        quals=range(10, 16),\n        name=\'read1\')\n    self.assertEqual(\n        start + 5 + 16,\n        utils.read_end(read))\n\n  def test_reservoir_sample_length(self):\n    """"""Tests samples have expected length.""""""\n    first_ten_ints = range(10)\n    # Test sampling with k > len(iterable).\n    self.assertEqual(len(utils.reservoir_sample(first_ten_ints, 11)), 10)\n    # Test sampling with k == len(iterable).\n    self.assertEqual(len(utils.reservoir_sample(first_ten_ints, 10)), 10)\n    # Test sampling with k < len(iterable).\n    self.assertEqual(len(utils.reservoir_sample(first_ten_ints, 9)), 9)\n    # Test sampling with k == 0.\n    self.assertEqual(len(utils.reservoir_sample(first_ten_ints, 0)), 0)\n    # Test sampling with k < 0 (bad args).\n    with self.assertRaises(ValueError):\n      utils.reservoir_sample(first_ten_ints, -1)\n\n  @parameterized.parameters(\n      (10, 0),\n      (1, 1),\n      (10, 1),\n      (1, 3),\n      (3, 3),\n      (6, 3),\n      (10, 3),\n  )\n  def test_reservoir_sample_frequency(self, iterable_size, k):\n    """"""Tests observed frequency is close to expected frequency.""""""\n    # Use a fixed random number so our test is deterministic.\n    random = np.random.RandomState(123456789)\n    n_replicates = 100000\n    counts = collections.Counter(\n        item\n        for _ in range(n_replicates)\n        for item in utils.reservoir_sample(range(iterable_size), k, random))\n    expected_frequency = min(k / float(iterable_size), 1.0)\n    for c in counts.values():\n      observed_frequency = c / float(n_replicates)\n      npt.assert_allclose(observed_frequency, expected_frequency, atol=0.01)\n\n  @parameterized.parameters(\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=4, e2=10, expected=False),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=3, e2=10, expected=False),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=2, e2=10, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=1, e2=10, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=0, e2=10, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=0, e2=1, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=0, e2=2, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=0, e2=3, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=1, e2=2, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=1, e2=3, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=2, e2=3, expected=True),\n      # dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=3, e2=3, expected=False),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=0, e2=4, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=1, e2=4, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr2\', s2=1, e2=4, expected=False),\n  )\n  def test_read_overlaps_region(self, ref1, s1, e1, ref2, s2, e2, expected):\n\n    def check_overlaps(chr1, start1, end1, chr2, start2, end2, expected):\n      nbp = end1 - start1\n      read = test_utils.make_read(\n          \'A\' * nbp, chrom=chr1, start=start1, cigar=\'{}M\'.format(nbp))\n      region = ranges.make_range(chr2, start2, end2)\n      self.assertEqual(utils.read_overlaps_region(read, region), expected)\n      # This check ensures we get the same result calling ranges.ranges_overlap.\n      self.assertEqual(\n          ranges.ranges_overlap(region, utils.read_range(read)), expected)\n\n    check_overlaps(ref1, s1, e1, ref2, s2, e2, expected)\n    check_overlaps(ref2, s2, e2, ref1, s1, e1, expected)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/util/variant_utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Variant utilities.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport itertools\n\nimport enum\nimport six\n\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import ranges\nfrom third_party.nucleus.util import variantcall_utils\nfrom third_party.nucleus.util import vcf_constants\n\n\ndef only_call(variant):\n  """"""Ensures the Variant has exactly one VariantCall, and returns it.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant. The variant of interest.\n\n  Returns:\n    The single nucleus.genomics.v1.VariantCall in the variant.\n\n  Raises:\n    ValueError: Not exactly one VariantCall is in the variant.\n  """"""\n  if len(variant.calls) != 1:\n    raise ValueError(\'Expected exactly one VariantCall in {}\'.format(variant))\n  return variant.calls[0]\n\n\ndef decode_variants(encoded_iter):\n  """"""Yields a genomics.Variant from encoded_iter.\n\n  Args:\n    encoded_iter: An iterable that produces binary encoded\n      nucleus.genomics.v1.Variant strings.\n\n  Yields:\n    A parsed nucleus.genomics.v1.Variant for each encoded element of\n    encoded_iter in order.\n  """"""\n  for encoded in encoded_iter:\n    yield variants_pb2.Variant.FromString(encoded)\n\n\ndef variant_position(variant):\n  """"""Returns a new Range at the start position of variant.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    A new Range with the same reference_name as variant and start but an end\n    that is start + 1. This produces a range that is the single basepair of the\n    start of variant, hence the name position.\n  """"""\n  return ranges.make_range(variant.reference_name, variant.start,\n                           variant.start + 1)\n\n\ndef variant_range(variant):\n  """"""Returns a new Range covering variant.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    A new Range with the same reference_name, start, and end as variant.\n  """"""\n  return ranges.make_range(variant.reference_name, variant.start, variant.end)\n\n\ndef variant_range_tuple(variant):\n  """"""Returns a new tuple of (reference_name, start, end) for the variant.\n\n  A common use case for this function is to sort variants by chromosomal\n  location, with usage like `sorted(variants, key=variant_range_tuple)`.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    A three-tuple with the same reference_name, start, and end as variant.\n  """"""\n  return (variant.reference_name, variant.start, variant.end)\n\n\n@enum.unique\nclass GenotypeType(enum.Enum):\n  """"""An enumeration of the types of genotypes.""""""\n  hom_ref = (\'homozygous reference\', [0, 0], 0)\n  het = (\'heterozygous\', [0, 1], 1)\n  hom_var = (\'homozygous non-reference\', [1, 1], 2)\n  no_call = (\'no call\', [-1, -1], -1)\n\n  def __init__(self, full_name, example_gt, class_id):\n    """"""Create a GenotypeType with the given name, GT and class_id.""""""\n    self.full_name = full_name\n    self.example_gt = example_gt\n    self.class_id = class_id\n\n\n@enum.unique\nclass VariantType(enum.Enum):\n  """"""An enumeration of the types of variants.""""""\n  # A variant.proto where there is no alt allele.\n  ref = 0\n  # A non-reference variant.proto where all ref and alt alleles are single\n  # basepairs.\n  snp = 1\n  # A non-reference variant.proto where at least one of ref or alt alleles are\n  # longer than 1 bp.\n  indel = 2\n\n\ndef format_filters(variant):\n  """"""Returns a human-readable string showing the filters applied to variant.\n\n  Returns a string with the filter field values of variant separated by commas.\n  If the filter field isn\'t set, returns vcf_constants.MISSING_FIELD (\'.\').\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    A string.\n  """"""\n  if variant.filter:\n    return \',\'.join(variant.filter)\n  else:\n    return vcf_constants.MISSING_FIELD\n\n\ndef format_alleles(variant):\n  """"""Gets a string representation of the variant\'s alleles.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    A string ref_bases/alt1,alt2 etc.\n  """"""\n  return \'{}/{}\'.format(variant.reference_bases, \',\'.join(\n      variant.alternate_bases))\n\n\ndef format_position(variant):\n  """"""Gets a string representation of the variant\'s position.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    A string chr:start + 1 (as start is zero-based).\n  """"""\n  return \'{}:{}\'.format(variant.reference_name, variant.start + 1)\n\n\ndef _non_excluded_alts(alts, exclude_alleles=None):\n  """"""Exclude any alts listed, by default: \'<*>\', \'.\', and \'<NON_REF>\'.\n\n  These alleles are sometimes listed in ALT column but they shouldn\'t be\n  analyzed and usually indicate reference blocks in formats like gVCF.\n\n  E.g. \'A\'->\'<*>\' is NOT an insertion, and \'A\'->\'.\' is NOT a SNP.\n\n  Args:\n    alts: a list of strings representing the alternate alleles.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    alts alleles except those in exclude_alleles, by default excluding the GVCF\n    \'<*>\' allele, the \'<NON_REF>\' symbolic allele, and \'.\' missing field by\n    default.\n  """"""\n  if exclude_alleles is None:\n    exclude_alleles = [\n        vcf_constants.GVCF_ALT_ALLELE, vcf_constants.SYMBOLIC_ALT_ALLELE,\n        vcf_constants.MISSING_FIELD\n    ]\n  return [a for a in alts if a not in exclude_alleles]\n\n\ndef is_snp(variant, exclude_alleles=None):\n  """"""Is variant a SNP?\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    True if all alleles of variant are 1 bp in length.\n  """"""\n  relevant_alts = _non_excluded_alts(variant.alternate_bases, exclude_alleles)\n  return (len(variant.reference_bases) == 1 and len(relevant_alts) >= 1 and\n          all(len(x) == 1 for x in relevant_alts))\n\n\ndef is_indel(variant, exclude_alleles=None):\n  """"""Is variant an indel?\n\n  An indel event is simply one where the size of at least one of the alleles\n  is > 1.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    True if the alleles in variant indicate an insertion/deletion event\n    occurs at this site.\n  """"""\n  relevant_alts = _non_excluded_alts(variant.alternate_bases, exclude_alleles)\n  if not relevant_alts:\n    return False\n  return (len(variant.reference_bases) > 1 or\n          any(len(alt) > 1 for alt in relevant_alts))\n\n\ndef is_biallelic(variant, exclude_alleles=None):\n  """"""Returns True if variant has exactly one alternate allele.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    True if the variant has exactly one alternate allele.\n  """"""\n  relevant_alts = _non_excluded_alts(variant.alternate_bases, exclude_alleles)\n  return len(relevant_alts) == 1\n\n\ndef is_multiallelic(variant, exclude_alleles=None):\n  """"""Does variant have multiple alt alleles?\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    True if variant has more than one alt allele.\n  """"""\n  relevant_alts = _non_excluded_alts(variant.alternate_bases, exclude_alleles)\n  return len(relevant_alts) > 1\n\n\ndef variant_is_insertion(variant, exclude_alleles=None):\n  """"""Are all the variant\'s alt alleles insertions?\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    True if variant has at least one alt allele and all alts are insertions.\n  """"""\n  relevant_alts = _non_excluded_alts(variant.alternate_bases, exclude_alleles)\n  if not relevant_alts:\n    return False\n  return all(\n      is_insertion(variant.reference_bases, alt) for alt in relevant_alts)\n\n\ndef variant_is_deletion(variant, exclude_alleles=None):\n  """"""Are all the variant\'s alt alleles deletions?\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    True if variant has at least one alt allele and all alts are deletions.\n  """"""\n  relevant_alts = _non_excluded_alts(variant.alternate_bases, exclude_alleles)\n  if not relevant_alts:\n    return False\n  return all(is_deletion(variant.reference_bases, alt) for alt in relevant_alts)\n\n\ndef is_ref(variant, exclude_alleles=None):\n  """"""Returns true if variant is a reference record.\n\n  Variant protos can encode sites that aren\'t actually mutations in the\n  sample. For example, the record ref=\'A\', alt=\'.\' indicates that there is\n  no mutation present (i.e., alt is the missing value).\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    True if there are no actual alternate alleles.\n  """"""\n  relevant_alts = _non_excluded_alts(variant.alternate_bases, exclude_alleles)\n  return not relevant_alts\n\n\ndef variant_type(variant):\n  """"""Gets the VariantType of variant.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    VariantType indicating the type of this variant.\n  """"""\n  if is_ref(variant):\n    return VariantType.ref\n  elif is_snp(variant):\n    return VariantType.snp\n  else:\n    return VariantType.indel\n\n\ndef is_transition(allele1, allele2):\n  """"""Is the pair of single bp alleles a transition?\n\n  Args:\n    allele1: A string of the first allele, must be 1 bp in length.\n    allele2: A string of the second allele, must be 1 bp in length.\n\n  Returns:\n    True if allele1/allele2 are a transition SNP.\n\n  Raises:\n    ValueError: if allele1 and allele2 are equal or aren\'t 1 bp in length.\n  """"""\n  if allele1 == allele2:\n    raise ValueError(\'Alleles must be unique:\', allele1, allele2)\n  if len(allele1) != 1:\n    raise ValueError(\'Alleles must be 1 bp in length.\', allele1)\n  if len(allele2) != 1:\n    raise ValueError(\'Alleles must be 1 bp in length.\', allele2)\n\n  alleles_set = {allele1, allele2}\n  return any(alleles_set == x for x in [{\'A\', \'G\'}, {\'C\', \'T\'}])\n\n\ndef is_insertion(ref, alt):\n  """"""Is alt an insertion w.r.t. ref?\n\n  Args:\n    ref: A string of the reference allele.\n    alt: A string of the alternative allele.\n\n  Returns:\n    True if alt is an insertion w.r.t. ref.\n  """"""\n  return len(ref) < len(alt)\n\n\ndef is_deletion(ref, alt):\n  """"""Is alt a deletion w.r.t. ref?\n\n  Args:\n    ref: A string of the reference allele.\n    alt: A string of the alternative allele.\n\n  Returns:\n    True if alt is a deletion w.r.t. ref.\n  """"""\n  return len(ref) > len(alt)\n\n\ndef has_insertion(variant):\n  """"""Does variant have an insertion?\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    True if the alleles in variant indicate an insertion event\n    occurs at this site.\n  """"""\n  ref = variant.reference_bases\n  return (is_indel(variant) and\n          any(is_insertion(ref, alt) for alt in variant.alternate_bases))\n\n\ndef has_deletion(variant):\n  """"""Does variant have a deletion?\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    True if the alleles in variant indicate an deletion event\n    occurs at this site.\n  """"""\n  ref = variant.reference_bases\n  return (is_indel(variant) and\n          any(is_deletion(ref, alt) for alt in variant.alternate_bases))\n\n\n@enum.unique\nclass AlleleMismatchType(enum.Enum):\n  """"""An enumeration of the types of allele mismatches we detect.""""""\n  # Duplicate alleles.\n  duplicate_eval_alleles = 1\n  duplicate_true_alleles = 2\n  # Truth has an allele that doesn\'t match any allele in eval.\n  unmatched_true_alleles = 3\n  # Eval has an allele that doesn\'t match any allele in truth.\n  unmatched_eval_alleles = 4\n\n\ndef allele_mismatches(evalv, truev):\n  """"""Determines the set of allele mismatch discordances between evalv and truev.\n\n  Compares the alleles present in evalv and truev to determine if there are any\n  disagreements between the set of called alleles in the two Variant protos. The\n  type of differences basically boil down to:\n\n  -- Are there duplicate alt alleles?\n  -- Can we find a matching allele in the truev for each allele in evalv, and\n    vice versa?\n\n  Two alleles A and B match when they would produce the same sequence of bases\n  in ref and alt haplotypes starting at the same position. So CA=>TA is the same\n  as C=>T (position is the same, replacing A by A is a noop) but AC=>AT isn\'t\n  the same as C=>T because the former event changes bases 1 bp further along in\n  the reference genome than the C=>T allele.\n\n  Args:\n    evalv: A nucleus.genomics.v1.Variant.\n    truev: A nucleus.genomics.v1.Variant.\n\n  Returns:\n    A set of AlleleMismatchType values.\n  """"""\n  unmatched_eval_alleles = []\n  # Use set removes duplicate alleles in truth and eval variants.\n  allele_matches = {alt: [] for alt in set(truev.alternate_bases)}\n  for eval_alt in set(evalv.alternate_bases):\n    # Loop over each possible alt allele, adding eval_alt to each matching alt\n    # allele.\n    found_match = False\n    for true_alt in allele_matches:\n      if (simplify_alleles(evalv.reference_bases, eval_alt) == simplify_alleles(\n          truev.reference_bases, true_alt)):\n        # We are a match to true_alt, so record that fact in allele_matches\n        allele_matches[true_alt].append(eval_alt)\n        found_match = True\n    if not found_match:\n      # We never found a match for eval_alt.\n      unmatched_eval_alleles.append(eval_alt)\n\n  # At this point we\'ve checked every alt against every eval allele, and are\n  # ready to summarize the differences using our AlleleMismatchType enum.\n  types = set()\n  if len(set(evalv.alternate_bases)) != len(evalv.alternate_bases):\n    types.add(AlleleMismatchType.duplicate_eval_alleles)\n  if len(set(truev.alternate_bases)) != len(truev.alternate_bases):\n    types.add(AlleleMismatchType.duplicate_true_alleles)\n  if unmatched_eval_alleles:\n    types.add(AlleleMismatchType.unmatched_eval_alleles)\n  if any(len(match) != 1 for match in six.itervalues(allele_matches)):\n    types.add(AlleleMismatchType.unmatched_true_alleles)\n  return types\n\n\ndef simplify_alleles(*alleles):\n  """"""Simplifies alleles by stripping off common postfix bases.\n\n  For example, simplify(""AC"", ""GC"") would produce the tuple ""A"", ""G"" as the ""C""\n  base is a common postfix of both alleles. But simplify(""AC"", ""GT"") would\n  produce ""AC"", ""GT"" as there is no common postfix.\n\n  Note this function will never simplify any allele down to the empty string. So\n  if alleles = [\'CACA\', \'CA\'], the longest common postfix is \'CA\' but we will\n  not produce [\'CA\', \'\'] as this is an invalid Variant allele encoding. Instead\n  we produce [\'CAC\', \'C\'].\n\n  Args:\n    *alleles: A tuple of bases, each as a string, to simplify.\n\n  Returns:\n    A tuple, one for each allele in alleles in order, with any common postfix\n    bases stripped off.\n  """"""\n\n  def all_the_same(items):\n    first = next(items)\n    return all(item == first for item in items)\n\n  # Loop over the alleles to determine the length of the shared postfix. Start\n  # at 1 so every allele, even after trimming the postfix, has at least len 1.\n  # For example, alleles = [\'ATT\', \'TT\'] reduces to [\'AT\', \'T\'] not [\'A\', \'\'].\n  shortest_allele_len = min(len(a) for a in alleles)\n  common_postfix_len = 0\n  for i in range(1, shortest_allele_len):\n    if not all_the_same(a[-i] for a in alleles):\n      break\n    common_postfix_len = i\n\n  if common_postfix_len:\n    return tuple(a[0:-common_postfix_len] for a in alleles)\n  else:\n    # Fast path for the case where there\'s no shared postfix.\n    return alleles\n\n\ndef is_filtered(variant):\n  """"""Returns True if variant has a non-PASS filter field, or False otherwise.""""""\n  return bool(variant.filter) and any(\n      f not in {\'PASS\', vcf_constants.MISSING_FIELD} for f in variant.filter)\n\n\ndef is_variant_call(variant,\n                    require_non_ref_genotype=True,\n                    no_calls_are_variant=False,\n                    call_indices=None,\n                    apply_filter=True):\n  """"""Is variant a non-reference call?\n\n  A Variant proto doesn\'t always imply that there\'s a variant present in the\n  genome. The call may not have alternate bases, may be filtered, may a have\n  hom-ref genotype, etc. This function looks for all of those configurations\n  and returns true iff the variant is asserting that a mutation is present\n  in the same.\n\n  Note that this code allows a variant without a calls field to be variant,\n  but one with a genotype call must have a non-reference genotype to be\n  considered variant (if require_non_ref_genotype is True, the default). If\n  False, a variant that passes all of the site-level requirements for being\n  a variant_call will return a True value, regardless of the genotypes, which\n  means that we\'ll consider a site with a sample with a hom-ref or no-call site\n  a variant call.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    require_non_ref_genotype: Should we require a site with a genotype call to\n      have a non-reference (het, hom-var) genotype for the site to be considered\n      a variant call?\n    no_calls_are_variant: If a site has genotypes, should we consider no_call\n      genotypes as being variant or not? e.g. -1/1 listed as ./. in VCF\n    call_indices: A list of 0-based indices. If specified, only the calls\n      at the given indices will be considered. The function will return\n      True if any of those calls are variant.\n    apply_filter: If set to True, will never treat this site as variant when\n      any filter other than PASS or . is set.\n\n  Returns:\n    True if variant is really a mutation call.\n  """"""\n  if is_ref(variant):\n    # No actual alt allele listed in ALT column\n    return False\n  elif apply_filter and is_filtered(variant):\n    # Anything other than PASS or . in FILTER column\n    return False\n  elif not variant.calls or not require_non_ref_genotype:\n    return True\n  # All tests after this point should only look at genotype-based fields, as\n  # we may have aborted out in the prev. line due to require_non_ref_genotype.\n  else:\n    # Check for non-ref genotypes and optionally no-call (-1) genotypes\n    if call_indices is None:\n      call_indices = range(len(variant.calls))\n    for i in call_indices:\n      for g in variant.calls[i].genotype:\n        if g > 0 or (no_calls_are_variant and g < 0):\n          return True\n    return False\n\n\ndef has_calls(variant):\n  """"""Does variant have any genotype calls?\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    True if variant has one or more VariantCalls.\n  """"""\n  return bool(variant.calls)\n\n\ndef genotype_type(variant):\n  """"""Gets the GenotypeType for variant.\n\n  If variant doesn\'t have genotypes, returns no_call. Otherwise\n  returns one of no_call, hom_ref, het, or hom_var depending on the\n  status of the genotypes in the call field of variant.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    A GenotypeType.\n\n  Raises:\n    ValueError: If variant has more than one call (i.e., is multi-sample).\n  """"""\n  if not has_calls(variant):\n    return GenotypeType.no_call\n  elif len(variant.calls) > 1:\n    raise ValueError(\'Unsupported: multiple genotypes found at\', variant)\n  else:\n    gt = set(only_call(variant).genotype)\n    if gt == {-1}:\n      return GenotypeType.no_call\n    elif gt == {0}:\n      return GenotypeType.hom_ref\n    elif len(gt) > 1:\n      return GenotypeType.het\n    else:\n      return GenotypeType.hom_var\n\n\ndef genotype_as_alleles(variant, call_ix=0):\n  """"""Gets genotype of the sample in variant as a list of actual alleles.\n\n  Returns the alleles specified by the genotype indices of variant.calls[0].\n  For example, if variant.reference_bases = \'A\' and variant.alternative_bases\n  = [\'C\'] and the genotypes are [0, 1], this function will return\n  [\'A\', \'C\'].\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    call_ix: int. The index into the calls attribute indicating which\n      VariantCall to return alleles for.\n\n  Returns:\n    A list of allele (string) from variant, one for each genotype in\n    variant.calls[call_ix], in order.\n\n  Raises:\n    ValueError: If variant doesn\'t have a call at the specified index.\n  """"""\n  if not 0 <= call_ix < len(variant.calls):\n    raise ValueError(\n        \'Unsupported: requesting call {} in variant with {} calls: {}\'.format(\n            call_ix, len(variant.calls), variant))\n  else:\n    # Genotypes are encoded as integers, where 0 is the reference allele,\n    # indices > 0 refer to alt alleles, and the no-call genotypes is encoded\n    # as -1 in the genotypes. This code relies on this encoding to quickly\n    # reference into the alleles by adding 1 to the genotype index.\n    alleles = ([vcf_constants.MISSING_FIELD, variant.reference_bases] +\n               list(variant.alternate_bases))\n    return [alleles[i + 1] for i in variant.calls[call_ix].genotype]\n\n\ndef unphase_all_genotypes(variant):\n  """"""Sorts genotype and removes phasing bit of all calls in variant.\n\n  This mutation is done in place rather than returning a different copy.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    The variant with unphased calls.\n  """"""\n  for c in variant.calls:\n    c.is_phased = False\n    c.genotype.sort()\n  return variant\n\n\ndef is_gvcf(variant):\n  """"""Returns true if variant encodes a standard gVCF reference block.\n\n  This means in practice that variant has a single alternate allele that is the\n  canonical gVCF allele vcf_constants.GVCF_ALT_ALLELE.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    Boolean. True if variant is a gVCF record, False otherwise.\n  """"""\n  return variant.alternate_bases == [vcf_constants.GVCF_ALT_ALLELE]\n\n\ndef _genotype_order_in_likelihoods(num_alts, ploidy=2):\n  """"""Yields tuples of `ploidy` ints for the given number of alt alleles.\n\n  https://samtools.github.io/hts-specs/VCFv4.1.pdf\n  ""If A is the allele in REF and B,C,... are the alleles as ordered in ALT,\n  the ordering of genotypes for the likelihoods is given by:\n  F(j/k) = (k*(k+1)/2)+j. In other words, for biallelic sites the ordering is:\n  AA,AB,BB; for triallelic sites the ordering is: AA,AB,BB,AC,BC,CC, etc.""\n  The biallelic sites in our case are 0/0, 0/1, 1/1.\n  The triallelic sites are 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.\n  This wiki page has more information that generalizes to different ploidy.\n  http://genome.sph.umich.edu/wiki/Relationship_between_Ploidy,_Alleles_and_Genotypes\n\n  Args:\n    num_alts: int. The number of alternate alleles at the site.\n    ploidy: int. The ploidy for which to return genotypes.\n\n  Yields:\n    Tuples of `ploidy` ints representing allele indices in the order they appear\n    in the corresponding genotype likelihood array.\n  """"""\n  if ploidy == 1:\n    for i in range(num_alts + 1):\n      yield (i,)\n  elif ploidy == 2:\n    for j in range(num_alts + 1):\n      for i in range(j + 1):\n        yield (i, j)\n  else:\n    raise NotImplementedError(\'Only haploid and diploid supported.\')\n\n\ndef genotype_ordering_in_likelihoods(variant):\n  """"""Yields (i, j, allele_i, allele_j) for the genotypes ordering in GLs.\n\n  https://samtools.github.io/hts-specs/VCFv4.1.pdf\n  ""If A is the allele in REF and B,C,... are the alleles as ordered in ALT,\n  the ordering of genotypes for the likelihoods is given by:\n  F(j/k) = (k*(k+1)/2)+j. In other words, for biallelic sites the ordering is:\n  AA,AB,BB; for triallelic sites the ordering is: AA,AB,BB,AC,BC,CC, etc.""\n  The biallelic sites in our case are 0/0, 0/1, 1/1.\n  The triallelic sites are 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.\n  This wiki page has more information that generalizes ot different ploidy.\n  http://genome.sph.umich.edu/wiki/Relationship_between_Ploidy,_Alleles_and_Genotypes\n\n  Currently this function only implements for diploid cases.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Yields:\n    allele indices and strings (i, j, allele_i, allele_j) in the correct order.\n  """"""\n  alleles = [variant.reference_bases] + list(variant.alternate_bases)\n  for i, j in _genotype_order_in_likelihoods(\n      len(variant.alternate_bases), ploidy=2):\n    yield i, j, alleles[i], alleles[j]\n\n\ndef genotype_likelihood(variant_call, allele_indices):\n  """"""Returns the genotype likelihood for the given allele indices.\n\n  Args:\n    variant_call: nucleus.genomics.v1.VariantCall. The VariantCall from\n      which to extract the genotype likelihood of the allele indices.\n    allele_indices: list(int). The list of allele indices for a given genotype.\n      E.g. diploid heterozygous alternate can be represented as [0, 1].\n\n  Returns:\n    The float value of the genotype likelihood of this set of alleles.\n  """"""\n  return variant_call.genotype_likelihood[genotype_likelihood_index(\n      allele_indices)]\n\n\ndef genotype_likelihood_index(allele_indices):\n  """"""Returns the genotype likelihood index for the given allele indices.\n\n  Args:\n    allele_indices: list(int). The list of allele indices for a given genotype.\n      E.g. diploid homozygous reference is represented as [0, 0].\n\n  Returns:\n    The index into the associated genotype likelihood array corresponding to\n    the likelihood of this list of alleles.\n\n  Raises:\n    NotImplementedError: The allele_indices are more than diploid.\n  """"""\n  if len(allele_indices) == 1:\n    # Haploid case.\n    return allele_indices[0]\n  elif len(allele_indices) == 2:\n    # Diploid case.\n    g1, g2 = sorted(allele_indices)\n    return g1 + (g2 * (g2 + 1) // 2)\n  else:\n    raise NotImplementedError(\n        \'Genotype likelihood index only supports haploid and diploid: {}\'.\n        format(allele_indices))\n\n\ndef allele_indices_for_genotype_likelihood_index(gl_index, ploidy=2):\n  """"""Returns a tuple of allele_indices corresponding to the given GL index.\n\n  This is the inverse function to `genotype_likelihood_index`.\n\n  Args:\n    gl_index: int. The index within a genotype likelihood array for which to\n      determine the associated alleles.\n    ploidy: int. The ploidy of the result.\n\n  Returns:\n    A tuple of `ploidy` ints representing the allele indices at this GL index.\n\n  Raises:\n    NotImplementedError: The requested allele indices are more than diploid.\n  """"""\n  if ploidy == 1:\n    return gl_index\n  elif ploidy == 2:\n    # redacted\n    # https://genome.sph.umich.edu/wiki/Relationship_between_Ploidy,_Alleles_and_Genotypes\n    # rather than creating all genotypes explicitly.\n    num_alts = 1\n    while genotype_likelihood_index([num_alts, num_alts]) < gl_index:\n      num_alts += 1\n    genotypes = list(_genotype_order_in_likelihoods(num_alts, ploidy=ploidy))\n    return genotypes[gl_index]\n  else:\n    raise NotImplementedError(\n        \'Allele calculations only supported for haploid and diploid.\')\n\n\ndef allele_indices_with_num_alts(variant, num_alts, ploidy=2):\n  """"""Returns a list of allele indices configurations with `num_alts` alternates.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant. The variant of interest, which\n      defines the candidate alternate alleles that can be used to generate\n      allele indices configurations.\n    num_alts: int in [0, `ploidy`]. The number of non-reference alleles for\n      which to create the allele indices configurations.\n    ploidy: int. The ploidy for which to return allele indices configurations.\n\n  Returns: A list of tuples. Each tuple is of length `ploidy` and represents the\n    allele indices of all `ploidy` genotypes that contain `num_alts`\n    non-reference alleles.\n\n  Raises:\n    ValueError: The domain of `num_alts` is invalid.\n    NotImplementedError: `ploidy` is not diploid.\n  """"""\n  if ploidy != 2:\n    raise NotImplementedError(\n        \'allele_indices_with_num_alts only supports diploid.\')\n  if not 0 <= num_alts <= ploidy:\n    raise ValueError(\n        \'Invalid number of alternate alleles requested: {} for ploidy {}\'.\n        format(num_alts, ploidy))\n\n  max_candidate_alt_ix = len(variant.alternate_bases)\n  if num_alts == 0:\n    return [(0, 0)]\n  elif num_alts == 1:\n    return [(0, i) for i in range(1, max_candidate_alt_ix + 1)]\n  else:\n    return [(i, j)\n            for i in range(1, max_candidate_alt_ix + 1)\n            for j in range(i, max_candidate_alt_ix + 1)]\n\n\ndef variants_overlap(variant1, variant2):\n  """"""Returns True if the range of variant1 and variant2 overlap.\n\n  This is equivalent to:\n\n    ranges_overlap(variant_range(variant1), variant_range(variant2))\n\n  Args:\n    variant1: nucleus.genomics.v1.Variant we want to compare for overlap.\n    variant2: nucleus.genomics.v1.Variant we want to compare for overlap.\n\n  Returns:\n    True if the variants overlap, False otherwise.\n  """"""\n  return ranges.ranges_overlap(variant_range(variant1), variant_range(variant2))\n\n\ndef variant_key(variant, sort_alleles=True):\n  """"""Gets a human-readable string key that is almost unique for Variant.\n\n  Gets a string key that contains key information about the variant, formatted\n  as:\n\n    reference_name:start+1:reference_bases->alternative_bases\n\n  where alternative bases is joined with a \'/\' for each entry in\n  alternative_bases. The start+1 is so we display the position, which starts at\n  1, and not the offset, which starts at 0.\n\n  For example, a Variant(reference_name=\'20\', start=10, reference_bases=\'AC\',\n  alternative_bases=[\'A\', \'ACC\']) would have a key of:\n\n    20:11:AC->A/ACC\n\n  The key is \'almost unique\' in that the reference_name + start + alleles should\n  generally occur once within a single VCF file, given the way the VCF\n  specification works.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant to make into a key.\n    sort_alleles: bool. If True, the alternative_bases of variant will be sorted\n      according to their lexicographic order. If False, the alternative_bases\n      will be displayed in their order in the Variant.\n\n  Returns:\n    A str.\n  """"""\n  alts = variant.alternate_bases\n  if sort_alleles:\n    alts = sorted(alts)\n  return \'{}:{}:{}->{}\'.format(variant.reference_name, variant.start + 1,\n                               variant.reference_bases, \'/\'.join(alts))\n\n\ndef sorted_variants(variants):\n  """"""Returns sorted(variants, key=variant_range_tuple).""""""\n  return sorted(variants, key=variant_range_tuple)\n\n\ndef variants_are_sorted(variants):\n  """"""Returns True if variants are sorted w.r.t. variant_range.\n\n  Args:\n    variants: list[nucleus.genomics.v1.Variant]. A list of Variant\n      protos that may or may not be sorted.\n\n  Returns:\n    True if variants are sorted, False otherwise.\n  """"""\n  def _pairwise(iterable):\n    a, b = itertools.tee(iterable)\n    next(b, None)\n    return six.moves.zip(a, b)\n\n  for r1, r2 in _pairwise(variant_range_tuple(v) for v in variants):\n    if r2 < r1:\n      return False\n  return True\n\n\ndef set_info(variant, field_name, value, vcf_object=None):\n  """"""Sets a field of the info map of the `Variant` to the given value(s).\n\n  `variant.info` is analogous to the INFO field of a VCF record.\n\n  Args:\n    variant: Variant proto. The Variant to modify.\n    field_name: str. The name of the field to set.\n    value: A single value or list of values to update the Variant with. The type\n      of the value is determined by the `vcf_object` if one is given, otherwise\n      is looked up based on the reserved INFO fields in the VCF specification.\n    vcf_object: (Optional) A VcfReader or VcfWriter object. If not None, the\n      type of the field is inferred from the associated VcfReader or VcfWriter\n      based on its name. Otherwise, the type is inferred if it is a reserved\n      field.\n  """"""\n  if vcf_object is None:\n    set_field_fn = vcf_constants.reserved_info_field_set_fn(field_name)\n  else:\n    set_field_fn = vcf_object.field_access_cache.info_field_set_fn(field_name)\n  set_field_fn(variant.info, field_name, value)\n\n\ndef get_info(variant, field_name, vcf_object=None):\n  """"""Returns the value of the `field_name` INFO field.\n\n  The `vcf_object` is used to determine the type of the resulting value. If it\n  is a single value or a Flag, that single value will be returned. Otherwise,\n  the list of values is returned.\n\n  Args:\n    variant: Variant proto. The Variant of interest.\n    field_name: str. The name of the field to retrieve values from.\n    vcf_object: (Optional) A VcfReader or VcfWriter object. If not None, the\n      type of the field is inferred from the associated VcfReader or VcfWriter\n      based on its name. Otherwise, the type is inferred if it is a reserved\n      field.\n  """"""\n  if vcf_object is None:\n    get_field_fn = vcf_constants.reserved_info_field_get_fn(field_name)\n  else:\n    get_field_fn = vcf_object.field_access_cache.info_field_get_fn(field_name)\n  return get_field_fn(variant.info, field_name)\n\n\ndef calc_ac(variant):\n  """"""Returns a list of alt counts based on variant.calls.""""""\n  counts = [0] * len(variant.alternate_bases)\n  for call in variant.calls:\n    for gt in call.genotype:\n      if gt > 0:\n        counts[gt - 1] += 1\n  return counts\n\n\ndef calc_an(variant):\n  """"""Returns the total number of alleles in called genotypes in variant.""""""\n  return sum(\n      len([1 for gt in call.genotype if gt > -1]) for call in variant.calls)\n\n\ndef is_singleton(variant):\n  """"""Returns True iff the variant has exactly one non-ref VariantCall.""""""\n  non_ref_count = 0\n  for c in variant.calls:\n    if variantcall_utils.has_variation(c):\n      non_ref_count += 1\n      if non_ref_count > 1:\n        return False\n  return non_ref_count == 1\n\n\ndef major_allele_frequency(variant):\n  """"""Returns the frequency of the most common allele in the variant.""""""\n  count_dict = collections.defaultdict(int)\n  for call in variant.calls:\n    for geno in call.genotype:\n      count_dict[geno] += 1\n  denom = sum(cnt for geno, cnt in count_dict.items() if geno >= 0)\n  if denom > 0:\n    numer = max(cnt for geno, cnt in count_dict.items() if geno >= 0)\n    return float(numer) / denom\n  else:\n    return 0\n'"
third_party/nucleus/util/variant_utils_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for variant_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport functools\nimport itertools\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\nimport six\n\nfrom third_party.nucleus.protos import struct_pb2\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\nfrom third_party.nucleus.util import struct_utils\nfrom third_party.nucleus.util import variant_utils\n\nNO_MISMATCH = set()\nEVAL_DUP = variant_utils.AlleleMismatchType.duplicate_eval_alleles\nTRUE_DUP = variant_utils.AlleleMismatchType.duplicate_true_alleles\nTRUE_MISS = variant_utils.AlleleMismatchType.unmatched_true_alleles\nEVAL_MISS = variant_utils.AlleleMismatchType.unmatched_eval_alleles\n\n\nclass VariantUtilsTests(parameterized.TestCase):\n\n  def test_only_call(self):\n    expected = variants_pb2.VariantCall(call_set_name=\'name\', genotype=[0, 1])\n    variant = variants_pb2.Variant(calls=[expected])\n    actual = variant_utils.only_call(variant)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      0,\n      2,\n      3,\n  )\n  def test_invalid_only_call(self, num_calls):\n    calls = [\n        variants_pb2.VariantCall(call_set_name=str(x)) for x in range(num_calls)\n    ]\n    variant = variants_pb2.Variant(calls=calls)\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Expected exactly one VariantCall\'):\n      variant_utils.only_call(variant)\n\n  def test_modify_only_call(self):\n    variant = variants_pb2.Variant(calls=[variants_pb2.VariantCall()])\n    call = variant_utils.only_call(variant)\n    call.call_set_name = \'name\'\n    call.genotype[:] = [0, 1]\n    self.assertLen(variant.calls, 1)\n    self.assertEqual(variant.calls[0].call_set_name, \'name\')\n    self.assertEqual(variant.calls[0].genotype, [0, 1])\n\n  def test_decode_variants(self):\n    variants = [\n        test_utils.make_variant(start=1),\n        test_utils.make_variant(start=2)\n    ]\n    encoded = [variant.SerializeToString() for variant in variants]\n    actual = variant_utils.decode_variants(encoded)\n    # We have an iterable, so actual isn\'t equal to variants.\n    self.assertNotEqual(actual, variants)\n    # Making actual a list now makes it equal.\n    self.assertEqual(list(actual), variants)\n\n  def test_variant_position_and_range(self):\n    v1 = test_utils.make_variant(chrom=\'1\', alleles=[\'A\', \'C\'], start=10)\n    v2 = test_utils.make_variant(chrom=\'1\', alleles=[\'AGCT\', \'C\'], start=10)\n    pos = ranges.make_range(\'1\', 10, 11)\n    range_ = ranges.make_range(\'1\', 10, 14)\n    v1_range_tuple = (\'1\', 10, 11)\n    v2_range_tuple = (\'1\', 10, 14)\n    self.assertEqual(pos, variant_utils.variant_position(v1))\n    self.assertEqual(pos, variant_utils.variant_position(v2))\n    self.assertEqual(pos, variant_utils.variant_range(v1))\n    self.assertEqual(range_, variant_utils.variant_range(v2))\n    self.assertEqual(v1_range_tuple, variant_utils.variant_range_tuple(v1))\n    self.assertEqual(v2_range_tuple, variant_utils.variant_range_tuple(v2))\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), \'A/C\'),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\']), \'A/C,T\'),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\']), \'A/AT\'),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\']), \'AT/A\'),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'CT\']), \'AT/A,CT\'),\n  )\n  def test_format_alleles(self, variant, expected):\n    self.assertEqual(variant_utils.format_alleles(variant), expected)\n\n  @parameterized.parameters(\n      (None, \'.\'),\n      ([\'.\'], \'.\'),\n      ([\'PASS\'], \'PASS\'),\n      ([\'FILTER1\', \'FILTER2\'], \'FILTER1,FILTER2\'),\n      ([\'FILTER1\', \'FILTER2\', \'FILTER3\'], \'FILTER1,FILTER2,FILTER3\'),\n  )\n  def test_format_filters(self, filters, expected):\n    variant = test_utils.make_variant(filters=filters)\n    if filters is None:\n      variant.ClearField(\'filter\')\n    self.assertEqual(variant_utils.format_filters(variant), expected)\n\n  @parameterized.parameters(\n      # variant => status if we require non_ref genotype / status if we don\'t.\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), True, True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=None), True, True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'AT\']), True, True),\n      (test_utils.make_variant(alleles=[\'A\']), False, False),\n      (test_utils.make_variant(alleles=[\'A\', \'.\']), False, False),\n      (test_utils.make_variant(filters=[\'FAIL\']), False, False),\n      (test_utils.make_variant(gt=[-1, -1]), False, True),\n      (test_utils.make_variant(gt=[0, 0]), False, True),\n      (test_utils.make_variant(gt=[0, 1]), True, True),\n      (test_utils.make_variant(gt=[1, 1]), True, True),\n      (test_utils.make_variant_multiple_calls(gts=[[0, 0], [0, 0]]), False,\n       True),\n      (test_utils.make_variant_multiple_calls(gts=[[0, 1], [0, 0]]), True,\n       True),\n      (test_utils.make_variant_multiple_calls(gts=[[-1, -1], [0, 0]]), False,\n       True),\n  )\n  def test_is_variant_call(self, variant, expected_req_non_ref,\n                           expected_any_genotype):\n    # Check that default call checks for genotypes.\n    self.assertEqual(\n        variant_utils.is_variant_call(variant), expected_req_non_ref)\n    # Ask explicitly for genotypes to be included.\n    self.assertEqual(\n        variant_utils.is_variant_call(variant, require_non_ref_genotype=True),\n        expected_req_non_ref)\n    # Don\'t require non_ref genotypes.\n    self.assertEqual(\n        variant_utils.is_variant_call(variant, require_non_ref_genotype=False),\n        expected_any_genotype)\n\n    with self.assertRaises(Exception):\n      variant_utils.is_variant_call(None)\n\n  @parameterized.parameters(\n      ([-1, 0], [\'PASS\'], False, False),\n      ([-1, 0], [], False, False),\n      ([-1, 1], [\'FAIL\'], False, True),\n      ([0, 0], [\'PASS\'], False, False),\n      ([0, 1], [\'VQSRTrancheSNP99.80to100.00\'], False, True),\n      ([0, 1], [\'PASS\'], True, True),\n      ([0, 1], [], True, True),\n      ([1, 1], [\'FAIL\'], False, True),\n      ([1, 1], [\'PASS\'], True, True),\n      ([1, 2], [], True, True),\n  )\n  def test_is_variant_call_apply_filter(self, genotype, filters,\n                                        expected_when_applied,\n                                        expected_when_not_applied):\n    variant = test_utils.make_variant(gt=genotype, filters=filters)\n    # The default is apply_filter=True.\n    self.assertEqual(\n        variant_utils.is_variant_call(variant), expected_when_applied)\n    self.assertEqual(\n        variant_utils.is_variant_call(variant, apply_filter=True),\n        expected_when_applied)\n    self.assertEqual(\n        variant_utils.is_variant_call(variant, apply_filter=False),\n        expected_when_not_applied)\n\n  def test_is_variant_call_no_calls_are_variant(self):\n\n    def check_is_variant(variant, expected, **kwargs):\n      self.assertEqual(\n          variant_utils.is_variant_call(variant, **kwargs), expected)\n\n    no_genotypes = test_utils.make_variant(gt=[])\n    no_call = test_utils.make_variant(gt=[-1, -1])\n    hom_ref = test_utils.make_variant(gt=[0, 0])\n    het = test_utils.make_variant(gt=[0, 1])\n    hom_var = test_utils.make_variant(gt=[1, 1])\n    mult1 = test_utils.make_variant_multiple_calls(gts=[[-1, -1], [0, 0]])\n    mult2 = test_utils.make_variant_multiple_calls(\n        gts=[[0, 0], [0, 1], [-1, -1]])\n\n    check_is_variant(no_genotypes, False, no_calls_are_variant=False)\n    check_is_variant(no_genotypes, False, no_calls_are_variant=False)\n    check_is_variant(no_call, False, no_calls_are_variant=False)\n    check_is_variant(no_call, True, no_calls_are_variant=True)\n    check_is_variant(hom_ref, False, no_calls_are_variant=False)\n    check_is_variant(hom_ref, False, no_calls_are_variant=True)\n    check_is_variant(het, True, no_calls_are_variant=False)\n    check_is_variant(het, True, no_calls_are_variant=True)\n    check_is_variant(hom_var, True, no_calls_are_variant=False)\n    check_is_variant(hom_var, True, no_calls_are_variant=True)\n    check_is_variant(mult1, False, no_calls_are_variant=False)\n    check_is_variant(mult1, True, no_calls_are_variant=True)\n    check_is_variant(mult2, False, call_indices=[0])\n    check_is_variant(mult2, True, call_indices=[1])\n    check_is_variant(mult2, True, call_indices=[0, 1])\n    check_is_variant(mult2, False, call_indices=[2])\n\n  @parameterized.parameters(\n      (test_utils.make_variant(filters=None), False),\n      (test_utils.make_variant(filters=[\'.\']), False),\n      (test_utils.make_variant(filters=[\'PASS\']), False),\n      (test_utils.make_variant(filters=[\'FAIL\']), True),\n      (test_utils.make_variant(filters=[\'FAIL1\', \'FAIL2\']), True),\n      # These two are not allowed in VCF, but worth testing our\n      # code\'s behavior\n      (test_utils.make_variant(filters=[\'FAIL1\', \'PASS\']), True),\n      (test_utils.make_variant(filters=[\'FAIL1\', \'.\']), True),\n  )\n  def test_is_filtered(self, variant, expected):\n    self.assertEqual(variant_utils.is_filtered(variant), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']),\n       variant_utils.VariantType.snp),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\']),\n       variant_utils.VariantType.snp),\n      (test_utils.make_variant(alleles=[\'A\']), variant_utils.VariantType.ref),\n      (test_utils.make_variant(alleles=[\'A\', \'.\']),\n       variant_utils.VariantType.ref),\n      (test_utils.make_variant(alleles=[\'A\', \'AC\']),\n       variant_utils.VariantType.indel),\n      (test_utils.make_variant(alleles=[\'AC\', \'A\']),\n       variant_utils.VariantType.indel),\n      (test_utils.make_variant(alleles=[\'A\', \'AC\', \'ACC\']),\n       variant_utils.VariantType.indel),\n      (test_utils.make_variant(alleles=[\'ACC\', \'AC\', \'A\']),\n       variant_utils.VariantType.indel),\n  )\n  def test_variant_type(self, variant, expected):\n    self.assertEqual(variant_utils.variant_type(variant), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(\'chr1\', 10), \'chr1:11\'),\n      (test_utils.make_variant(\'chr2\', 100), \'chr2:101\'),\n  )\n  def test_format_position(self, variant, expected):\n    self.assertEqual(variant_utils.format_position(variant), expected)\n\n  @parameterized.parameters(\n      ([\'C\', \'<*>\'], None, [\'C\']),\n      ([\'C\', \'.\'], None, [\'C\']),\n      ([\'C\', \'<NON_REF>\'], None, [\'C\']),\n      ([\'C\', \'<*>\', \'A\', \'T\'], None, [\'C\', \'A\', \'T\']),\n      ([\'C\', \'<*>\', \'A\', \'T\'], [], [\'C\', \'<*>\', \'A\', \'T\']),\n      ([\'C\'], None, [\'C\']),\n      ([\'TEST\'], [\'TEST\'], []),\n  )\n  def test_non_excluded_alts(self, alts, excluded, expected):\n    self.assertEqual(\n        variant_utils._non_excluded_alts(alts, exclude_alleles=excluded),\n        expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\']), False),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\']), False),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'CT\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'AT\']), False),\n      (test_utils.make_variant(alleles=[\'A\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'.\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'G\', \'C\', \'AT\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'G\', \'T\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'T\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'T\', \'G\', \'<*>\']), True),\n  )\n  def test_is_snp(self, variant, expected):\n    self.assertEqual(variant_utils.is_snp(variant), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<NON_REF>\']), [\'<NON_REF>\'\n                                                                 ], True),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'<NON_REF>\']), [\'<NON_REF>\'\n                                                                  ], False),\n      # <NON_REF> is excluded by default\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<NON_REF>\']), None, True),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'<NON_REF>\']), None, False),\n  )\n  def test_is_snp_symbolic_allele(self, variant, exclude_alleles, expected):\n    self.assertEqual(\n        variant_utils.is_snp(variant, exclude_alleles=exclude_alleles),\n        expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\']), True),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\']), True),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'CT\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'AT\']), True),\n      (test_utils.make_variant(alleles=[\'A\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'.\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'AT\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'AT\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<NON_REF>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'<NON_REF>\']), True),\n  )\n  def test_is_indel(self, variant, expected):\n    self.assertEqual(variant_utils.is_indel(variant), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<NON_REF>\']), [\'<NON_REF>\'],\n       False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'<NON_REF>\']), [\'<NON_REF>\'],\n       True),\n  )\n  def test_is_indel_symbolic_allele(self, variant, exclude_alleles, expected):\n    self.assertEqual(\n        variant_utils.is_indel(variant, exclude_alleles=exclude_alleles),\n        expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\']), False),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\']), False),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'CT\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'AT\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\', \'<*>\']), True),\n  )\n  def test_is_multiallelic(self, variant, expected):\n    self.assertEqual(variant_utils.is_multiallelic(variant), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\']), True),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\']), True),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'CT\']), False),\n      (test_utils.make_variant(alleles=[\'AT\']), False),\n      (test_utils.make_variant(alleles=[\'AT\', \'.\']), False),\n      (test_utils.make_variant(alleles=[\'AT\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\', \'<*>\']), False),\n  )\n  def test_is_biallelic(self, variant, expected):\n    self.assertEqual(variant_utils.is_biallelic(variant), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'AA\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'G\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'AG\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'A\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'AGG\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'.\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'<NON_REF>\']), False),\n      (test_utils.make_variant(alleles=[\'A\']), False),\n  )\n  def test_variant_is_insertion(self, variant, expected):\n    self.assertEqual(variant_utils.variant_is_insertion(variant), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'AG\', \'A\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'G\']), False),\n      (test_utils.make_variant(alleles=[\'AAG\', \'AC\', \'AG\']), True),\n      (test_utils.make_variant(alleles=[\'AAC\', \'ATG\', \'A\']), False),\n      (test_utils.make_variant(alleles=[\'AAT\', \'A\', \'AA\']), True),\n      (test_utils.make_variant(alleles=[\'AGA\', \'.\']), False),\n      (test_utils.make_variant(alleles=[\'AGTT\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'AGAGTCGACTGAT\', \'<NON_REF>\']), False),\n      (test_utils.make_variant(alleles=[\'AT\']), False),\n  )\n  def test_variant_is_deletion(self, variant, expected):\n    self.assertEqual(variant_utils.variant_is_deletion(variant), expected)\n\n  @parameterized.parameters(\n      ([\'A\', \'C\'], [\'A\', \'C\']),\n      ([\'AA\', \'CA\'], [\'A\', \'C\']),\n      ([\'AAG\', \'CAG\'], [\'A\', \'C\']),\n      ([\'AAGAG\', \'CAGAG\'], [\'A\', \'C\']),\n      ([\'AACAG\', \'CAGAG\'], [\'AAC\', \'CAG\']),\n      ([\'AACAC\', \'CAGAG\'], [\'AACAC\', \'CAGAG\']),\n      ([\'ACT\', \'A\'], [\'ACT\', \'A\']),\n      ([\'ACTCT\', \'ACT\'], [\'ACT\', \'A\']),\n      ([\'ACTCT\', \'A\'], [\'ACTCT\', \'A\']),\n      ([\'CAG\', \'GAG\'], [\'C\', \'G\']),\n      # Make sure we don\'t reduce an allele to nothing.\n      ([\'AT\', \'ATAT\'], [\'A\', \'ATA\']),\n      # Tests for multi-allelics.\n      # There\'s one extra T here.\n      ([\'ATT\', \'AT\', \'ATTT\'], [\'AT\', \'A\', \'ATT\']),\n      # Another single base postfix where we can remove a \'G\'.\n      ([\'CAG\', \'GAG\', \'TCG\'], [\'CA\', \'GA\', \'TC\']),\n      # There are two extra Ts to remove.\n      ([\'ATTT\', \'ATT\', \'ATTTT\'], [\'AT\', \'A\', \'ATT\']),\n      # One pair can simplify, but not the other, so nothing can reduce.\n      ([\'CAG\', \'GAG\', \'TCA\'], [\'CAG\', \'GAG\', \'TCA\']),\n      # Example from internal.\n      ([\'CGGCGG\', \'CGG\', \'CAACGG\'], [\'CGGC\', \'C\', \'CAAC\']),\n  )\n  def test_simplify_alleles(self, alleles, expected):\n    self.assertEqual(variant_utils.simplify_alleles(*alleles), tuple(expected))\n    self.assertEqual(\n        variant_utils.simplify_alleles(*reversed(alleles)),\n        tuple(reversed(expected)))\n\n  @parameterized.parameters(\n      ([\'A\', \'C\'], [\'A\', \'C\'], NO_MISMATCH),\n      ([\'A\', \'AC\'], [\'A\', \'AC\'], NO_MISMATCH),\n      ([\'AC\', \'A\'], [\'AC\', \'A\'], NO_MISMATCH),\n      ([\'AC\', \'A\', \'ACT\'], [\'AC\', \'A\', \'ACT\'], NO_MISMATCH),\n      ([\'AC\', \'A\', \'ACT\'], [\'AC\', \'ACT\', \'A\'], NO_MISMATCH),\n      # Alleles are incompatible, so we have mismatches in both directions.\n      ([\'A\', \'C\'], [\'A\', \'T\'], {TRUE_MISS, EVAL_MISS}),\n      ([\'A\', \'C\'], [\'G\', \'C\'], {TRUE_MISS, EVAL_MISS}),\n      # Missing alts specific to eval and truth.\n      ([\'A\', \'C\', \'G\'], [\'A\', \'C\'], {EVAL_MISS}),\n      ([\'A\', \'C\'], [\'A\', \'C\', \'G\'], {TRUE_MISS}),\n      # Duplicate alleles.\n      ([\'A\', \'C\', \'C\'], [\'A\', \'C\'], {EVAL_DUP}),\n      ([\'A\', \'C\'], [\'A\', \'C\', \'C\'], {TRUE_DUP}),\n      ([\'A\', \'C\', \'C\'], [\'A\', \'C\', \'C\'], {EVAL_DUP, TRUE_DUP}),\n      # Dups in truth, discordant alleles.\n      ([\'A\', \'C\'], [\'A\', \'G\', \'G\'], {TRUE_DUP, EVAL_MISS, TRUE_MISS}),\n      # Simplification of alleles does the right matching.\n      ([\'A\', \'C\'], [\'AA\', \'CA\'], NO_MISMATCH),  # trailing A.\n      # preceding A, doesn\'t simplify so it\'s a mismatch.\n      ([\'A\', \'C\'], [\'AA\', \'AC\'], {EVAL_MISS, TRUE_MISS}),\n      # both training preceding A, doesn\'t simplify, so mismatches\n      ([\'A\', \'C\'], [\'AAA\', \'ACA\'], {EVAL_MISS, TRUE_MISS}),\n      # # Eval has 1 of the two alt alleles, so no eval mismatch.\n      ([\'ACT\', \'A\'], [\'ACTCT\', \'ACT\', \'A\'], {TRUE_MISS}),\n      # Eval has extra unmatched alleles, so it\'s got a mismatch.\n      ([\'ACTCT\', \'ACT\', \'A\'], [\'ACT\', \'A\'], {EVAL_MISS}),\n  )\n  def test_allele_mismatch(self, a1, a2, expected):\n    v1 = test_utils.make_variant(alleles=a1)\n    v2 = test_utils.make_variant(alleles=a2)\n    self.assertEqual(variant_utils.allele_mismatches(v1, v2), expected)\n\n  @parameterized.parameters(\n      ([\'A\', \'C\'], False),\n      ([\'A\', \'G\'], True),\n      ([\'A\', \'T\'], False),\n      ([\'C\', \'G\'], False),\n      ([\'C\', \'T\'], True),\n      ([\'G\', \'T\'], False),\n  )\n  def test_is_transition(self, ordered_alleles, expected):\n    for alleles in [ordered_alleles, reversed(ordered_alleles)]:\n      self.assertEqual(variant_utils.is_transition(*alleles), expected)\n\n  def test_is_transition_raises_with_bad_args(self):\n    with self.assertRaises(ValueError):\n      variant_utils.is_transition(\'A\', \'A\')\n    with self.assertRaises(ValueError):\n      variant_utils.is_transition(\'A\', \'AA\')\n    with self.assertRaises(ValueError):\n      variant_utils.is_transition(\'AA\', \'A\')\n\n  @parameterized.parameters(\n      # alleles followed by is_insertion and is_deletion expectation\n      ([\'A\', \'C\'], False, False),\n      ([\'A\', \'AT\'], True, False),\n      ([\'A\', \'ATT\'], True, False),\n      ([\'AT\', \'A\'], False, True),\n      ([\'ATT\', \'A\'], False, True),\n      ([\'CAT\', \'TCA\'], False, False),\n\n      # These are examples where ref is not simplified, such as could occur\n      # a multi-allelic record, such as the following:\n      # alleles = AT, A, ATT, CT (1 deletion, 1 insertion, 1 SNP)\n      ([\'AT\', \'A\'], False, True),\n      ([\'AT\', \'ATT\'], True, False),\n      ([\'AT\', \'CT\'], False, False),\n  )\n  def test_is_insertion_deletion(self, alleles, is_insertion, is_deletion):\n    self.assertEqual(variant_utils.is_insertion(*alleles), is_insertion)\n    self.assertEqual(variant_utils.is_deletion(*alleles), is_deletion)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), False, False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\']), False, False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\']), True, False),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\']), False, True),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'ATT\']), True, True),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'CT\']), False, True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'AT\']), True, False),\n      (test_utils.make_variant(alleles=[\'A\']), False, False),\n      (test_utils.make_variant(alleles=[\'AGA\', \'.\']), False, False),\n  )\n  def test_has_insertion_deletion(self, variant, has_insertion, has_deletion):\n    self.assertEqual(variant_utils.has_insertion(variant), has_insertion)\n    self.assertEqual(variant_utils.has_deletion(variant), has_deletion)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\']), None, True),\n      (test_utils.make_variant(alleles=[\'A\', \'.\']), None, True),\n      # a gVCF reference block record is counted as ref\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\']), None, True),\n      # symbolic allele <NON_REF> practically counts as ref\n      (test_utils.make_variant(alleles=[\'A\', \'<NON_REF>\']), None, True),\n      (test_utils.make_variant(alleles=[\'A\', \'G\']), None, False),\n      (test_utils.make_variant(alleles=[\'A\', \'<NON_REF>\']), [\'.\'], False),\n  )\n  def test_is_ref(self, variant, excluded, expected):\n    self.assertEqual(variant_utils.is_ref(variant, excluded), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(gt=None), False),\n      (test_utils.make_variant(gt=[0, 0]), True),\n      (test_utils.make_variant(gt=[0, 1]), True),\n      (test_utils.make_variant(gt=[1, 1]), True),\n      (test_utils.make_variant(gt=[-1, -1]), True),\n      (variants_pb2.Variant(calls=[]), False),\n      (variants_pb2.Variant(\n          calls=[variants_pb2.VariantCall(call_set_name=\'no_geno\')]), True),\n      (variants_pb2.Variant(calls=[\n          variants_pb2.VariantCall(call_set_name=\'no_geno\'),\n          variants_pb2.VariantCall(call_set_name=\'no_geno2\'),\n      ]), True),\n  )\n  def test_has_calls(self, variant, expected):\n    self.assertEqual(variant_utils.has_calls(variant), expected)\n\n  def test_has_calls_raises_with_bad_inputs(self):\n    with self.assertRaises(Exception):\n      variant_utils.has_calls(None)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(gt=None), variant_utils.GenotypeType.no_call),\n      (test_utils.make_variant(gt=[-1, -1]),\n       variant_utils.GenotypeType.no_call),\n      (test_utils.make_variant(gt=[0, 0]), variant_utils.GenotypeType.hom_ref),\n      (test_utils.make_variant(gt=[0, 1]), variant_utils.GenotypeType.het),\n      (test_utils.make_variant(gt=[1, 0]), variant_utils.GenotypeType.het),\n      (test_utils.make_variant(gt=[0, 2]), variant_utils.GenotypeType.het),\n      (test_utils.make_variant(gt=[2, 0]), variant_utils.GenotypeType.het),\n      (test_utils.make_variant(gt=[1, 1]), variant_utils.GenotypeType.hom_var),\n      (test_utils.make_variant(gt=[1, 2]), variant_utils.GenotypeType.het),\n  )\n  def test_genotype_type(self, variant, expected):\n    self.assertEqual(variant_utils.genotype_type(variant), expected)\n\n  def test_genotype_type_raises_with_bad_args(self):\n    with self.assertRaises(Exception):\n      variant_utils.genotype_type(None)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[0, 0]), [\'A\', \'A\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[0, 1]), [\'A\', \'C\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[1, 0]), [\'C\', \'A\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[1, 1]), [\'C\', \'C\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\'], gt=[0, 0]), [\'A\', \'A\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\'], gt=[0, 1]), [\'A\', \'C\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\'], gt=[0, 2]), [\'A\', \'T\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\'], gt=[1, 2]), [\'C\', \'T\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\'], gt=[2, 1]), [\'T\', \'C\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\'], gt=[1, 1]), [\'C\', \'C\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\'], gt=[2, 2]), [\'T\', \'T\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[-1, -1]), [\'.\', \'.\']),\n  )\n  def test_genotype_as_alleles(self, variant, expected):\n    self.assertEqual(variant_utils.genotype_as_alleles(variant), expected)\n\n  def test_genotype_as_alleles_raises_with_bad_inputs(self):\n    with self.assertRaises(Exception):\n      variant_utils.genotype_as_alleles(None)\n    with self.assertRaises(Exception):\n      variant_utils.genotype_as_alleles(test_utils.make_variant(gt=None))\n    with self.assertRaises(Exception):\n      variant_utils.genotype_as_alleles(\n          test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[0, 0]), call_ix=1)\n    with self.assertRaises(Exception):\n      variant_utils.genotype_type(None)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[0, 1], is_phased=False),\n       test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[0, 1], is_phased=False)),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[1, 0], is_phased=True),\n       test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[0, 1], is_phased=False)),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[1, 1], is_phased=True),\n       test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[1, 1], is_phased=False)),\n      (test_utils.make_variant(\n          alleles=[\'A\', \'C\', \'T\'], gt=[2, 1], is_phased=True),\n       test_utils.make_variant(\n           alleles=[\'A\', \'C\', \'T\'], gt=[1, 2], is_phased=False)),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[-1, -1], is_phased=True),\n       test_utils.make_variant(\n           alleles=[\'A\', \'C\'], gt=[-1, -1], is_phased=False)),\n  )\n  def test_unphase_all_genotypes(self, variant, expected):\n    self.assertEqual(variant_utils.unphase_all_genotypes(variant), expected)\n\n  @parameterized.parameters(\n      # Ref without an alt isn\'t gVCF.\n      (test_utils.make_variant(alleles=[\'A\']), False),\n      # SNPs and indels aren\'t gVCF records.\n      (test_utils.make_variant(alleles=[\'A\', \'T\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\']), False),\n      (test_utils.make_variant(alleles=[\'AT\', \'T\']), False),\n      # These are gVCF records.\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\'], filters=\'PASS\'), True),\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\'], filters=\'FAIL\'), True),\n      # These are close but not exactly gVCFs.\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\', \'C\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'<*F>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'<CNV>\']), False),\n  )\n  def test_is_gvcf(self, variant, expected):\n    self.assertEqual(variant_utils.is_gvcf(variant), expected)\n\n  @parameterized.parameters(\n      # Variants with one ref and one alt allele.\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), [(0, 0, \'A\', \'A\'),\n                                                     (0, 1, \'A\', \'C\'),\n                                                     (1, 1, \'C\', \'C\')]),\n      # Variants with one ref and two alt alleles.\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'G\']), [(0, 0, \'A\', \'A\'),\n                                                          (0, 1, \'A\', \'C\'),\n                                                          (1, 1, \'C\', \'C\'),\n                                                          (0, 2, \'A\', \'G\'),\n                                                          (1, 2, \'C\', \'G\'),\n                                                          (2, 2, \'G\', \'G\')]),\n      # Variants with one ref and three alt alleles.\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'G\', \'T\']),\n       [(0, 0, \'A\', \'A\'), (0, 1, \'A\', \'C\'), (1, 1, \'C\', \'C\'), (0, 2, \'A\', \'G\'),\n        (1, 2, \'C\', \'G\'), (2, 2, \'G\', \'G\'), (0, 3, \'A\', \'T\'), (1, 3, \'C\', \'T\'),\n        (2, 3, \'G\', \'T\'), (3, 3, \'T\', \'T\')]),\n  )\n  def test_genotype_ordering_in_likelihoods(self, variant, expected):\n    self.assertEqual(\n        list(variant_utils.genotype_ordering_in_likelihoods(variant)), expected)\n\n  @parameterized.parameters(\n      # Haploid.\n      dict(gls=[0.], allele_indices=[0], expected=0.),\n      dict(gls=[-1, -2], allele_indices=[1], expected=-2),\n      dict(gls=[-1, -2, -3], allele_indices=[2], expected=-3),\n      # Diploid.\n      dict(gls=[0.], allele_indices=[0, 0], expected=0.),\n      dict(gls=[-1, -2, -3], allele_indices=[0, 0], expected=-1),\n      dict(gls=[-1, -2, -3], allele_indices=[0, 1], expected=-2),\n      dict(gls=[-1, -2, -3], allele_indices=[1, 0], expected=-2),\n      dict(gls=[-1, -2, -3], allele_indices=[1, 1], expected=-3),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[0, 0], expected=-1),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[0, 1], expected=-2),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[1, 0], expected=-2),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[1, 1], expected=-3),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[0, 2], expected=-4),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[2, 0], expected=-4),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[1, 2], expected=-5),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[2, 1], expected=-5),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[2, 2], expected=-6),\n      dict(gls=range(10), allele_indices=[0, 3], expected=6),\n      dict(gls=range(10), allele_indices=[1, 3], expected=7),\n      dict(gls=range(10), allele_indices=[2, 3], expected=8),\n      dict(gls=range(10), allele_indices=[3, 3], expected=9),\n  )\n  def test_genotype_likelihood(self, gls, allele_indices, expected):\n    variantcall = variants_pb2.VariantCall(genotype_likelihood=gls)\n    actual = variant_utils.genotype_likelihood(variantcall, allele_indices)\n    self.assertEqual(actual, expected)\n\n  def test_unsupported_genotype_likelihood(self):\n    variantcall = variants_pb2.VariantCall(genotype_likelihood=[-1, -2, -3])\n    with self.assertRaisesRegexp(NotImplementedError,\n                                 \'only supports haploid and diploid\'):\n      variant_utils.genotype_likelihood(variantcall, [0, 1, 1])\n\n  def test_haploid_allele_indices_for_genotype_likelihood_index(self):\n    for aix in six.moves.xrange(20):\n      allele_indices = (aix,)\n      ix = variant_utils.genotype_likelihood_index(allele_indices)\n      actual = variant_utils.allele_indices_for_genotype_likelihood_index(\n          ix, ploidy=1)\n      self.assertEqual(actual, aix)\n\n  def test_diploid_allele_indices_for_genotype_likelihood_index(self):\n    for aix in range(20):\n      for bix in range(20):\n        allele_indices = (aix, bix)\n        expected = tuple(sorted(allele_indices))\n        ix = variant_utils.genotype_likelihood_index(allele_indices)\n        actual = variant_utils.allele_indices_for_genotype_likelihood_index(\n            ix, ploidy=2)\n        self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(ploidy=-1),\n      dict(ploidy=0),\n      dict(ploidy=3),\n  )\n  def test_unsupported_allele_indices_for_genotype_likelihood_index(\n      self, ploidy):\n    with self.assertRaisesRegexp(NotImplementedError,\n                                 \'only supported for haploid and diploid\'):\n      variant_utils.allele_indices_for_genotype_likelihood_index(0, ploidy)\n\n  @parameterized.parameters(\n      dict(alt_bases=[], num_alts=0, expected=[(0, 0)]),\n      dict(alt_bases=[\'A\'], num_alts=0, expected=[(0, 0)]),\n      dict(alt_bases=[\'A\'], num_alts=1, expected=[(0, 1)]),\n      dict(alt_bases=[\'A\'], num_alts=2, expected=[(1, 1)]),\n      dict(alt_bases=[\'A\', \'C\'], num_alts=0, expected=[(0, 0)]),\n      dict(alt_bases=[\'A\', \'C\'], num_alts=1, expected=[(0, 1), (0, 2)]),\n      dict(alt_bases=[\'A\', \'C\'], num_alts=2, expected=[(1, 1), (1, 2), (2, 2)]),\n  )\n  def test_allele_indices_with_num_alts(self, alt_bases, num_alts, expected):\n    variant = variants_pb2.Variant(alternate_bases=alt_bases)\n    actual = variant_utils.allele_indices_with_num_alts(\n        variant, num_alts, ploidy=2)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(alt_bases=[\'A\'], num_alts=0, ploidy=1),\n      dict(alt_bases=[\'A\'], num_alts=0, ploidy=3),\n      dict(alt_bases=[\'A\'], num_alts=-1, ploidy=2),\n      dict(alt_bases=[\'A\'], num_alts=3, ploidy=2),\n  )\n  def test_invalid_allele_indices_with_num_alts(self, alt_bases, num_alts,\n                                                ploidy):\n    variant = variants_pb2.Variant(alternate_bases=alt_bases)\n    with self.assertRaises((NotImplementedError, ValueError)):\n      variant_utils.allele_indices_with_num_alts(variant, num_alts, ploidy)\n\n  def test_variants_overlap(self):\n    v1 = test_utils.make_variant(chrom=\'1\', alleles=[\'A\', \'C\'], start=10)\n    v2 = test_utils.make_variant(chrom=\'1\', alleles=[\'A\', \'C\'], start=20)\n    with mock.patch.object(ranges, \'ranges_overlap\') as mock_overlap:\n      mock_overlap.return_value = \'SENTINEL\'\n      self.assertEqual(variant_utils.variants_overlap(v1, v2), \'SENTINEL\')\n      mock_overlap.assert_called_once_with(\n          variant_utils.variant_range(v1), variant_utils.variant_range(v2))\n\n  @parameterized.parameters(\n      # Degenerate cases - no and one variant.\n      dict(sorted_variants=[],),\n      dict(sorted_variants=[\n          test_utils.make_variant(chrom=\'1\', start=10),\n      ],),\n      # Two variants on the same chromosome.\n      dict(\n          sorted_variants=[\n              test_utils.make_variant(chrom=\'1\', start=10),\n              test_utils.make_variant(chrom=\'1\', start=15),\n          ],),\n      # The first variant has start > the second, but it\'s on a later chrom.\n      dict(\n          sorted_variants=[\n              test_utils.make_variant(chrom=\'1\', start=15),\n              test_utils.make_variant(chrom=\'2\', start=10),\n          ],),\n      # Make sure the end is respected.\n      dict(\n          sorted_variants=[\n              test_utils.make_variant(chrom=\'1\', start=10),\n              test_utils.make_variant(chrom=\'1\', start=15),\n              test_utils.make_variant(chrom=\'1\', alleles=[\'AA\', \'A\'], start=15),\n          ],),\n      # Complex example with multiple chromosomes, ends, etc.\n      dict(\n          sorted_variants=[\n              test_utils.make_variant(chrom=\'1\', start=10),\n              test_utils.make_variant(chrom=\'2\', start=5),\n              test_utils.make_variant(chrom=\'2\', alleles=[\'AA\', \'A\'], start=5),\n              test_utils.make_variant(chrom=\'2\', start=6),\n              test_utils.make_variant(chrom=\'2\', start=10),\n              test_utils.make_variant(chrom=\'3\', start=2),\n          ],),\n  )\n  def test_sorted_variants(self, sorted_variants):\n    for permutation in itertools.permutations(\n        sorted_variants, r=len(sorted_variants)):\n\n      # Check that sorting the permutations produced sorted.\n      self.assertEqual(\n          variant_utils.sorted_variants(permutation), sorted_variants)\n\n      # Check that variants_are_sorted() is correct, which we detect if\n      # the range_tuples of permutation == the range_tuples of sorted_variants.\n      def _range_tuples(variants):\n        return [variant_utils.variant_range_tuple(v) for v in variants]\n\n      self.assertEqual(\n          variant_utils.variants_are_sorted(permutation),\n          _range_tuples(permutation) == _range_tuples(sorted_variants))\n\n  @parameterized.parameters(\n      dict(\n          variant=test_utils.make_variant(\n              chrom=\'1\', start=10, alleles=[\'A\', \'C\']),\n          expected_key=\'1:11:A->C\'),\n      dict(\n          variant=test_utils.make_variant(\n              chrom=\'1\', start=10, alleles=[\'A\', \'G\', \'C\']),\n          sort_alleles=True,\n          expected_key=\'1:11:A->C/G\'),\n      dict(\n          variant=test_utils.make_variant(\n              chrom=\'1\', start=10, alleles=[\'A\', \'G\', \'C\']),\n          sort_alleles=False,\n          expected_key=\'1:11:A->G/C\'),\n  )\n  def test_variant_key(self, variant, expected_key, sort_alleles=True):\n    self.assertEqual(\n        variant_utils.variant_key(variant, sort_alleles=sort_alleles),\n        expected_key)\n\n  @parameterized.parameters(\n      dict(\n          field_name=\'AD\',\n          value=[23, 25],\n          reader=None,\n          expected=[\n              struct_pb2.Value(int_value=23),\n              struct_pb2.Value(int_value=25)\n          ],\n      ),\n      dict(\n          field_name=\'AA\',\n          value=\'C\',\n          reader=True,\n          expected=[struct_pb2.Value(string_value=\'C\')],\n      ),\n  )\n  def test_set_info(self, field_name, value, reader, expected):\n    if reader is not None:\n      reader = mock.Mock()\n      reader.field_access_cache.info_field_set_fn.return_value = (\n          struct_utils.set_string_field)\n    variant = variants_pb2.Variant()\n    variant_utils.set_info(variant, field_name, value, reader)\n    actual = variant.info[field_name].values\n    self.assertEqual(len(actual), len(expected))\n    for actual_elem, expected_elem in zip(actual, expected):\n      self.assertEqual(actual_elem, expected_elem)\n\n  @parameterized.parameters(\n      dict(field_name=\'AD\', reader=None, expected=[23, 25]),\n      dict(field_name=\'AA\', reader=True, expected=\'C\'),\n      dict(field_name=\'1000G\', reader=None, expected=True),\n  )\n  def test_get_info(self, field_name, reader, expected):\n    if reader is not None:\n      reader = mock.Mock()\n      reader.field_access_cache.info_field_get_fn.return_value = (\n          functools.partial(\n              struct_utils.get_string_field, is_single_field=True))\n    variant = variants_pb2.Variant()\n    variant_utils.set_info(variant, \'AD\', [23, 25])\n    variant_utils.set_info(variant, \'AA\', \'C\')\n    variant_utils.set_info(variant, \'1000G\', True)\n    variant_utils.set_info(variant, \'DB\', False)\n    actual = variant_utils.get_info(variant, field_name, vcf_object=reader)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(alt_bases=[\'A\', \'T\'], calls=[[0, 0], [0, 1], [1, 2]],\n           expected=[2, 1]),\n      dict(alt_bases=[\'C\'], calls=[[0, 0], [0, 0]], expected=[0]),\n      dict(alt_bases=[], calls=[[0, 0], [0, 0], [0, 0]], expected=[]),\n  )\n  def test_calc_ac(self, alt_bases, calls, expected):\n    variant = variants_pb2.Variant()\n    variant.alternate_bases[:] = alt_bases\n    for gt in calls:\n      variant.calls.add().genotype[:] = gt\n    self.assertEqual(variant_utils.calc_ac(variant), expected)\n\n  @parameterized.parameters(\n      dict(calls=[[0, 0], [0, 1], [1, 2]], expected=6),\n      dict(calls=[[0, 0], [0, 0]], expected=4),\n      dict(calls=[[0, 0], [-1, -1], [0, -1]], expected=3),\n  )\n  def test_calc_an(self, calls, expected):\n    variant = variants_pb2.Variant()\n    for gt in calls:\n      variant.calls.add().genotype[:] = gt\n    self.assertEqual(variant_utils.calc_an(variant), expected)\n\n  @parameterized.parameters(\n      dict(calls=[], expected=False),\n      dict(calls=[[0, 0]], expected=False),\n      dict(calls=[[0, 1]], expected=True),\n      dict(calls=[[1, 1]], expected=True),\n      dict(calls=[[1, 2]], expected=True),\n      dict(calls=[[0, 0], [0, 1]], expected=True),\n      dict(calls=[[0, 0], [1, 1]], expected=True),\n      dict(calls=[[0, 1], [0, 1]], expected=False),\n      dict(calls=[[0, 2], [0, -1]], expected=True),\n      dict(calls=[[0, 0], [0, 1], [-1, -1]], expected=True),\n      dict(calls=[[0, 0], [0, 1], [0, 0]], expected=True),\n  )\n  def test_is_singleton(self, calls, expected):\n    variant = variants_pb2.Variant()\n    for gt in calls:\n      variant.calls.add().genotype[:] = gt\n    actual = variant_utils.is_singleton(variant)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(calls=[], expected=0.),\n      dict(calls=[[0, 0]], expected=1.),\n      dict(calls=[[0, 1]], expected=0.5),\n      dict(calls=[[1, 1]], expected=1.),\n      dict(calls=[[1, 2]], expected=0.5),\n      dict(calls=[[0, 0], [0, 1]], expected=0.75),\n      dict(calls=[[0, 0], [1, 1]], expected=0.5),\n      dict(calls=[[0, 1], [0, 1]], expected=0.5),\n      dict(calls=[[0, 2], [0, -1]], expected=2. / 3),\n      dict(calls=[[0, 0], [0, 1], [-1, -1]], expected=0.75),\n      dict(calls=[[0, 0], [0, 1], [0, 0]], expected=5. / 6),\n  )\n  def test_major_allele_frequency(self, calls, expected):\n    variant = variants_pb2.Variant()\n    for gt in calls:\n      variant.calls.add().genotype[:] = gt\n    actual = variant_utils.major_allele_frequency(variant)\n    self.assertAlmostEqual(actual, expected)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/util/variantcall_utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""VariantCall utilities.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom third_party.nucleus.protos import struct_pb2\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import struct_utils\nfrom third_party.nucleus.util import vcf_constants\n\n# Special-cased FORMAT fields that are first-class fields in the VariantCall.\n_GL = \'GL\'\n_GT = \'GT\'\n\n\ndef set_format(variant_call, field_name, value, vcf_object=None):\n  """"""Sets a field of the info map of the `VariantCall` to the given value(s).\n\n  `variant_call.info` is analogous to the FORMAT field of a VCF call.\n\n  Example usage:\n  with vcf.VcfReader(\'/path/to/my.vcf\') as vcf_reader:\n    for variant in vcf_reader:\n      first_call = variant.calls[0]\n      # Type can be inferred for reserved VCF fields.\n      set_format(first_call, \'AD\', 25)\n      # Specify the reader explicitly for unknown fields.\n      set_format(first_call, \'MYFIELD\', 30, vcf_reader)\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to modify.\n    field_name: str. The name of the field to set.\n    value: A single value or list of values to update the VariantCall with.\n      The type of the value is determined by the `vcf_object` if one is given,\n      otherwise is looked up based on the reserved FORMAT fields in the VCF\n      specification.\n    vcf_object: (Optional) A VcfReader or VcfWriter object. If not None, the\n      type of the field is inferred from the associated VcfReader or VcfWriter\n      based on its name. Otherwise, the type is inferred if it is a reserved\n      field.\n  """"""\n  if field_name == _GL:\n    set_gl(variant_call, value)\n    return\n  if field_name == _GT:\n    set_gt(variant_call, value)\n    return\n\n  if vcf_object is None:\n    set_field_fn = vcf_constants.reserved_format_field_set_fn(field_name)\n  else:\n    set_field_fn = vcf_object.field_access_cache.format_field_set_fn(field_name)\n  set_field_fn(variant_call.info, field_name, value)\n\n\ndef get_format(variant_call, field_name, vcf_object=None):\n  """"""Returns the value of the `field_name` FORMAT field.\n\n  The `vcf_object` is used to determine the type of the resulting value. If it\n  is a single value or a Flag, that single value will be returned. Otherwise,\n  the list of values is returned.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall of interest.\n    field_name: str. The name of the field to retrieve values from.\n    vcf_object: (Optional) A VcfReader or VcfWriter object. If not None, the\n      type of the field is inferred from the associated VcfReader or VcfWriter\n      based on its name. Otherwise, the type is inferred if it is a reserved\n      field.\n  """"""\n  if field_name == _GL:\n    return get_gl(variant_call)\n  if field_name == _GT:\n    return get_gt(variant_call)\n\n  if vcf_object is None:\n    get_field_fn = vcf_constants.reserved_format_field_get_fn(field_name)\n  else:\n    get_field_fn = vcf_object.field_access_cache.format_field_get_fn(field_name)\n  return get_field_fn(variant_call.info, field_name)\n\n\n# The following functions are convenience methods for getting/setting some\n# reserved FORMAT fields of a VariantCall as well as some non-reserved FORMAT\n# fields used by DeepVariant. Note that these functions will use the types of\n# each field as defined by the VCF 4.3 specification, mirrored in\n# vcf_constants.py, so if you have redefined any of these fields to have\n# different types these functions will not do what you want.\ndef set_ad(variant_call, ad):\n  """"""Sets the allele depth of the VariantCall.""""""\n  set_format(variant_call, \'AD\', ad)\n\n\ndef get_ad(variant_call):\n  """"""Gets the allele depth of the VariantCall.""""""\n  return get_format(variant_call, \'AD\')\n\n\ndef set_gl(variant_call, gl):\n  """"""Sets the genotype likelihoods of the VariantCall.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to modify.\n    gl: list(float). The list of genotype likelihoods for the VariantCall.\n  """"""\n  # Note: genotype_likelihood is extracted to a first-class field within\n  # VariantCall. Consequently, we just set its value directly here.\n  variant_call.genotype_likelihood[:] = gl\n\n\ndef get_gl(variant_call):\n  """"""Returns the genotype likelihoods of the VariantCall.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall for which to return GLs.\n\n  Returns:\n    A list of floats representing the genotype likelihoods of this call.\n  """"""\n  return variant_call.genotype_likelihood\n\n\ndef set_gt(variant_call, gt):\n  """"""Sets the genotypes of the VariantCall.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to modify.\n    gt: list(int). The list of genotypes for the VariantCall.\n  """"""\n  # Note: genotype is extracted to a first-class field within\n  # VariantCall. Consequently, we just set its value directly here.\n  variant_call.genotype[:] = gt\n\n\ndef get_gt(variant_call):\n  """"""Returns the genotypes of the VariantCall.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall for which to return GTs.\n\n  Returns:\n    A list of ints representing the genotype indices of this call.\n  """"""\n  return variant_call.genotype\n\n\ndef set_gq(variant_call, gq):\n  """"""Sets the genotype quality of the VariantCall.""""""\n  set_format(variant_call, \'GQ\', gq)\n\n\ndef get_gq(variant_call):\n  """"""Gets the genotype quality of the VariantCall.""""""\n  return get_format(variant_call, \'GQ\')\n\n\ndef set_min_dp(variant_call, min_dp):\n  """"""Sets the \'MIN_DP\' field of the VariantCall.""""""\n  struct_utils.set_int_field(variant_call.info, \'MIN_DP\', min_dp)\n\n\ndef get_min_dp(variant_call):\n  """"""Gets the \'MIN_DP\' field of the VariantCall.""""""\n  return struct_utils.get_int_field(\n      variant_call.info, \'MIN_DP\', is_single_field=True)\n\n\ndef has_genotypes(variant_call):\n  """"""Returns True iff the VariantCall has one or more called genotypes.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to evaluate.\n\n  Returns:\n    True if the VariantCall has one or more called genotypes, False otherwise.\n  """"""\n  return any(gt >= 0 for gt in variant_call.genotype)\n\n\ndef has_full_genotypes(variant_call):\n  """"""Returns True iff the VariantCall has only known genotypes.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to evaluate.\n\n  Returns:\n    True if all `genotype` fields are known genotypes.\n  """"""\n  return all(gt >= 0 for gt in variant_call.genotype)\n\n\ndef ploidy(variant_call):\n  """"""Returns the ploidy of the VariantCall.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to evaluate.\n\n  Returns:\n    The ploidy of the call (a non-negative integer).\n  """"""\n  # Unknown genotypes are represented as -1 in VariantCall protos. When\n  # a VCF is parsed that contains multiple ploidies in different samples,\n  # a separate padding value of -2**30 - 1 is inserted into the calls.\n  return sum(gt >= -1 for gt in variant_call.genotype)\n\n\ndef has_variation(variant_call):\n  """"""Returns True if and only if the call has a non-reference genotype.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to evaluate.\n\n  Returns:\n    True if and only if the call has a non-reference genotype.\n  """"""\n  return any(gt > 0 for gt in variant_call.genotype)\n\n\ndef is_heterozygous(variant_call):\n  """"""Returns True if and only if the call is heterozygous.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to evaluate.\n\n  Returns:\n    True if and only if the call is heterozygous.\n  """"""\n  return len({gt for gt in variant_call.genotype if gt >= 0}) >= 2\n'"
third_party/nucleus/util/variantcall_utils_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for variantcall_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport functools\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\n\nfrom third_party.nucleus.protos import struct_pb2\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import struct_utils\nfrom third_party.nucleus.util import variantcall_utils\n\n\n\n\nclass VariantcallUtilsTests(parameterized.TestCase):\n\n  def _assert_struct_lists_equal(self, actual, expected):\n    self.assertEqual(len(actual), len(expected))\n    for actual_elem, expected_elem in zip(actual, expected):\n      self.assertEqual(actual_elem, expected_elem)\n\n  @parameterized.parameters(\n      dict(\n          field_name=\'GP\',\n          value=[.1, .2, .7],\n          reader=None,\n          expected=[struct_pb2.Value(number_value=v) for v in [.1, .2, .7]]),\n      dict(\n          field_name=\'AD\',\n          value=[23],\n          reader=None,\n          expected=[struct_pb2.Value(int_value=23)]),\n      dict(\n          field_name=\'FT\',\n          value=[\'PASS\'],\n          reader=None,\n          expected=[struct_pb2.Value(string_value=\'PASS\')]),\n      dict(\n          field_name=\'FT\',\n          value=[\'PASS\'],\n          reader=True,\n          expected=[struct_pb2.Value(string_value=\'PASS\')]),\n  )\n  def test_set_format(self, field_name, value, reader, expected):\n    if reader is not None:\n      reader = mock.Mock()\n      reader.field_access_cache.format_field_set_fn.return_value = (\n          struct_utils.set_string_field)\n    call = variants_pb2.VariantCall()\n    variantcall_utils.set_format(call, field_name, value, reader)\n    actual = call.info[field_name].values\n    self._assert_struct_lists_equal(actual, expected)\n\n  @parameterized.parameters(\n      dict(field_name=\'GP\', reader=None, expected=[.1, .2, .7]),\n      dict(field_name=\'AD\', reader=None, expected=[55, 3]),\n      dict(field_name=\'DP\', reader=None, expected=58),\n      dict(field_name=\'GL\', reader=None, expected=[-1, -3, -5.5]),\n      dict(field_name=\'GT\', reader=None, expected=[0, 1]),\n      dict(field_name=\'FT\', reader=None, expected=\'LowQual\'),\n      dict(field_name=\'FT\', reader=True, expected=\'LowQual\'),\n  )\n  def test_get_format(self, field_name, reader, expected):\n    if reader is not None:\n      reader = mock.Mock()\n      reader.field_access_cache.format_field_get_fn.return_value = (\n          functools.partial(\n              struct_utils.get_string_field, is_single_field=True))\n\n    call = variants_pb2.VariantCall()\n    variantcall_utils.set_format(call, \'GP\', [.1, .2, .7])\n    variantcall_utils.set_format(call, \'AD\', [55, 3])\n    variantcall_utils.set_format(call, \'DP\', 58)\n    variantcall_utils.set_format(call, \'GL\', [-1, -3, -5.5])\n    variantcall_utils.set_format(call, \'GT\', [0, 1])\n    variantcall_utils.set_format(call, \'FT\', [\'LowQual\'])\n    actual = variantcall_utils.get_format(call, field_name, vcf_object=reader)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(\n          field_name=\'AD\',\n          setter=variantcall_utils.set_ad,\n          getter=variantcall_utils.get_ad,\n          values=[[1, 5], [30, 29]]),\n      dict(\n          field_name=\'GL\',\n          setter=variantcall_utils.set_gl,\n          getter=variantcall_utils.get_gl,\n          values=[[-1, -2, -3.3], [-0.001, -3, -10]]),\n      dict(\n          field_name=\'GQ\',\n          setter=variantcall_utils.set_gq,\n          getter=variantcall_utils.get_gq,\n          values=range(10)),\n      dict(\n          field_name=\'GT\',\n          setter=variantcall_utils.set_gt,\n          getter=variantcall_utils.get_gt,\n          values=[[0, 1], [1, 1], [1, 2]]),\n      dict(\n          field_name=\'MIN_DP\',\n          setter=variantcall_utils.set_min_dp,\n          getter=variantcall_utils.get_min_dp,\n          values=range(10)),\n  )\n  def test_variantcall_format_roundtrip(self, field_name, setter, getter,\n                                        values):\n    vc = variants_pb2.VariantCall()\n    self.assertNotIn(field_name, vc.info)\n    for value in values:\n      setter(vc, value)\n      if field_name not in [\'GT\', \'GL\']:\n        self.assertIn(field_name, vc.info)\n      actual = getter(vc)\n      self.assertEqual(actual, value)\n\n  @parameterized.parameters(\n      dict(genotype=[], expected=False),\n      dict(genotype=[-1], expected=False),\n      dict(genotype=[-1, -1], expected=False),\n      dict(genotype=[-1, -1073741825], expected=False),\n      dict(genotype=[-1, 0], expected=True),\n      dict(genotype=[0, 0], expected=True),\n      dict(genotype=[0, 1], expected=True),\n      dict(genotype=[0, 1, -1], expected=True),\n      dict(genotype=[-1, 0, -1073741825], expected=True),\n  )\n  def test_has_genotypes(self, genotype, expected):\n    call = variants_pb2.VariantCall(genotype=genotype)\n    actual = variantcall_utils.has_genotypes(call)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(genotype=[], expected=True),\n      dict(genotype=[-1], expected=False),\n      dict(genotype=[-1, -1073741825], expected=False),\n      dict(genotype=[-1, 0], expected=False),\n      dict(genotype=[0, 0], expected=True),\n      dict(genotype=[0, 1], expected=True),\n      dict(genotype=[0, 1, -1], expected=False),\n      dict(genotype=[1, 0, -1073741825], expected=False),\n  )\n  def test_has_full_genotypes(self, genotype, expected):\n    call = variants_pb2.VariantCall(genotype=genotype)\n    actual = variantcall_utils.has_full_genotypes(call)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(genotype=[], expected=0),\n      dict(genotype=[-1], expected=1),\n      dict(genotype=[-1, -1], expected=2),\n      dict(genotype=[-1, -1073741825], expected=1),\n      dict(genotype=[-1, 0], expected=2),\n      dict(genotype=[0, 0], expected=2),\n      dict(genotype=[0, 1], expected=2),\n      dict(genotype=[0, 1, -1], expected=3),\n      dict(genotype=[-1, 0, -1073741825], expected=2),\n  )\n  def test_ploidy(self, genotype, expected):\n    call = variants_pb2.VariantCall(genotype=genotype)\n    actual = variantcall_utils.ploidy(call)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(genotype=[], expected=False),\n      dict(genotype=[-1], expected=False),\n      dict(genotype=[-1, -1], expected=False),\n      dict(genotype=[-1, -1073741825], expected=False),\n      dict(genotype=[-1, 0], expected=False),\n      dict(genotype=[0, 0], expected=False),\n      dict(genotype=[0, 1], expected=True),\n      dict(genotype=[0, 1, -1], expected=True),\n      dict(genotype=[-1, 0, -1073741825], expected=False),\n  )\n  def test_has_variation(self, genotype, expected):\n    call = variants_pb2.VariantCall(genotype=genotype)\n    actual = variantcall_utils.has_variation(call)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(genotype=[], expected=False),\n      dict(genotype=[-1], expected=False),\n      dict(genotype=[-1, -1], expected=False),\n      dict(genotype=[-1, -1073741825], expected=False),\n      dict(genotype=[-1, 0], expected=False),\n      dict(genotype=[0, 0], expected=False),\n      dict(genotype=[0, 1], expected=True),\n      dict(genotype=[0, 2], expected=True),\n      dict(genotype=[1, 1], expected=False),\n      dict(genotype=[2, 2], expected=False),\n      dict(genotype=[1, 2], expected=True),\n      dict(genotype=[0, 1, -1], expected=True),\n      dict(genotype=[0, 1, 2], expected=True),\n      dict(genotype=[-1, 0, -1073741825], expected=False),\n  )\n  def test_is_heterozygous(self, genotype, expected):\n    call = variants_pb2.VariantCall(genotype=genotype)\n    actual = variantcall_utils.is_heterozygous(call)\n    self.assertEqual(actual, expected)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/util/vcf_constants.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Constants related to the VCF variant specification.\n\nSee the full specification at https://samtools.github.io/hts-specs/VCFv4.3.pdf\nfor details.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import struct_utils\n\n# The alternate allele string for reference (no alt).\nNO_ALT_ALLELE = \'.\'\n\n# The alternate allele string for the gVCF ""any"" alternate allele.\nGVCF_ALT_ALLELE = \'<*>\'\n\n# Older symbolic alt allele, similar in meaning to gVCF alt allele\nSYMBOLIC_ALT_ALLELE = \'<NON_REF>\'\n\n# The replacement field used for missing data.\nMISSING_FIELD = \'.\'\n\n# Valid types for INFO and FORMAT fields, as per the VCF 4.3 spec.\nCHARACTER_TYPE = \'Character\'\nFLAG_TYPE = \'Flag\'\nFLOAT_TYPE = \'Float\'\nINTEGER_TYPE = \'Integer\'\nSTRING_TYPE = \'String\'\n\n# Reserved FILTER field definitions.\nRESERVED_FILTER_FIELDS = [\n    variants_pb2.VcfFilterInfo(id=\'PASS\', description=\'All filters passed\'),\n]\n\n# Reserved INFO field definitions, as per the VCF 4.3 spec.\nRESERVED_INFO_FIELDS = [\n    variants_pb2.VcfInfo(\n        id=\'AA\', number=\'1\', type=STRING_TYPE, description=\'Ancestral allele\'),\n    variants_pb2.VcfInfo(\n        id=\'AC\',\n        number=\'A\',\n        type=INTEGER_TYPE,\n        description=\'Allele count in genotypes, for each ALT \'\n        \'allele, in the same order as listed\'),\n    variants_pb2.VcfInfo(\n        id=\'AD\',\n        number=\'R\',\n        type=INTEGER_TYPE,\n        description=\'Total read depth for each allele\'),\n    variants_pb2.VcfInfo(\n        id=\'ADF\',\n        number=\'R\',\n        type=INTEGER_TYPE,\n        description=\'Read depth for each allele on the forward \'\n        \'strand\'),\n    variants_pb2.VcfInfo(\n        id=\'ADR\',\n        number=\'R\',\n        type=INTEGER_TYPE,\n        description=\'Read depth for each allele on the reverse strand\'),\n    variants_pb2.VcfInfo(\n        id=\'AF\',\n        number=\'A\',\n        type=FLOAT_TYPE,\n        description=\'Allele frequency for each ALT allele in \'\n        \'the same order as listed (estimated from \'\n        \'primary data, not called genotypes)\'),\n    variants_pb2.VcfInfo(\n        id=\'AN\',\n        number=\'1\',\n        type=INTEGER_TYPE,\n        description=\'Total number of alleles in called genotypes\'),\n    variants_pb2.VcfInfo(\n        id=\'BQ\', number=\'1\', type=FLOAT_TYPE, description=\'RMS base quality\'),\n    variants_pb2.VcfInfo(\n        id=\'CIGAR\',\n        number=\'A\',\n        type=STRING_TYPE,\n        description=\'Cigar string describing how to align an \'\n        \'alternate allele to the reference allele\'),\n    variants_pb2.VcfInfo(\n        id=\'DB\', number=\'0\', type=FLAG_TYPE, description=\'dbSNP membership\'),\n    variants_pb2.VcfInfo(\n        id=\'DP\',\n        number=\'1\',\n        type=INTEGER_TYPE,\n        description=\'Combined depth across samples\'),\n    variants_pb2.VcfInfo(\n        id=\'END\',\n        number=\'1\',\n        type=INTEGER_TYPE,\n        description=\'End position (for use with symbolic alleles)\'),\n    variants_pb2.VcfInfo(\n        id=\'H2\', number=\'0\', type=FLAG_TYPE, description=\'HapMap2 membership\'),\n    variants_pb2.VcfInfo(\n        id=\'H3\', number=\'0\', type=FLAG_TYPE, description=\'HapMap3 membership\'),\n    # NOTE: In the VCF 4.3 spec, the type of \'MQ\' is listed as \'.\', even though\n    # that is not specified as a valid type. Because root mean square is\n    # typically a float value, we specify its type as FLOAT_TYPE.\n    variants_pb2.VcfInfo(\n        id=\'MQ\', number=\'1\', type=FLOAT_TYPE,\n        description=\'RMS mapping quality\'),\n    variants_pb2.VcfInfo(\n        id=\'MQ0\',\n        number=\'1\',\n        type=INTEGER_TYPE,\n        description=\'Number of MAPQ == 0 reads\'),\n    variants_pb2.VcfInfo(\n        id=\'NS\',\n        number=\'1\',\n        type=INTEGER_TYPE,\n        description=\'Number of samples with data\'),\n    # NOTE: In the VCF 4.3 spec, the type of \'SB\' is listed as \'.\', even though\n    # that is not specified as a valid type. Because strand bias is usually a\n    # numerical measurement (e.g. p-value of contingency table), we specify its\n    # type as FLOAT_TYPE.\n    variants_pb2.VcfInfo(\n        id=\'SB\', number=\'.\', type=FLOAT_TYPE, description=\'Strand bias\'),\n    variants_pb2.VcfInfo(\n        id=\'SOMATIC\',\n        number=\'0\',\n        type=FLAG_TYPE,\n        description=\'Somatic mutation (for cancer genomics)\'),\n    variants_pb2.VcfInfo(\n        id=\'VALIDATED\',\n        number=\'0\',\n        type=FLAG_TYPE,\n        description=\'Validated by follow-up experiment\'),\n    variants_pb2.VcfInfo(\n        id=\'1000G\',\n        number=\'0\',\n        type=FLAG_TYPE,\n        description=\'1000 Genomes membership\'),\n]\n\n# Reserved FORMAT field definitions, as per the VCF 4.3 spec.\nRESERVED_FORMAT_FIELDS = [\n    variants_pb2.VcfFormatInfo(\n        id=\'AD\',\n        number=\'R\',\n        type=INTEGER_TYPE,\n        description=\'Read depth for each allele\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'ADF\',\n        number=\'R\',\n        type=INTEGER_TYPE,\n        description=\'Read depth for each allele on the \'\n        \'forward strand\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'ADR\',\n        number=\'R\',\n        type=INTEGER_TYPE,\n        description=\'Read depth for each allele on the \'\n        \'reverse strand\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'DP\', number=\'1\', type=INTEGER_TYPE, description=\'Read depth\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'EC\',\n        number=\'A\',\n        type=INTEGER_TYPE,\n        description=\'Expected alternate allele counts\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'FT\',\n        number=\'1\',\n        type=STRING_TYPE,\n        description=\'Filter indicating if this genotype \'\n        \'was ""called""\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'GL\',\n        number=\'G\',\n        type=FLOAT_TYPE,\n        description=\'Genotype likelihoods\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'GP\',\n        number=\'G\',\n        type=FLOAT_TYPE,\n        description=\'Genotype posterior probabilities\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'GQ\',\n        number=\'1\',\n        type=INTEGER_TYPE,\n        description=\'Conditional genotype quality\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'GT\', number=\'1\', type=STRING_TYPE, description=\'Genotype\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'HQ\', number=\'2\', type=INTEGER_TYPE,\n        description=\'Haplotype quality\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'MQ\',\n        number=\'1\',\n        type=INTEGER_TYPE,\n        description=\'RMS mapping quality\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'PL\',\n        number=\'G\',\n        type=INTEGER_TYPE,\n        description=\'Phred-scaled genotype likelihoods \'\n        \'rounded to the closest integer\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'PQ\', number=\'1\', type=INTEGER_TYPE, description=\'Phasing quality\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'PS\', number=\'1\', type=INTEGER_TYPE, description=\'Phase set\'),\n]\n\n# Map from field type to the function used to set struct_pb2.Value elements\n# of that type.\nSET_FN_LOOKUP = {\n    INTEGER_TYPE: struct_utils.set_int_field,\n    FLOAT_TYPE: struct_utils.set_number_field,\n    STRING_TYPE: struct_utils.set_string_field,\n    CHARACTER_TYPE: struct_utils.set_string_field,\n    FLAG_TYPE: struct_utils.set_bool_field,\n}\n\n\ndef _get_reserved_field(field_id, reserved_fields):\n  """"""Returns the desired reserved field.\n\n  Args:\n    field_id: str. The id of the field to retrieve.\n    reserved_fields: list(fields). The reserved fields to search.\n\n  Returns:\n    The reserved field with the given `field_id`.\n\n  Raises:\n    ValueError: `field_id` is not a known reserved field.\n  """"""\n  matching_fields = [field for field in reserved_fields if field.id == field_id]\n  if not matching_fields:\n    raise ValueError(\'No reserved field with id `{}`\'.format(field_id))\n  return matching_fields[0]\n\n\ndef reserved_filter_field(field_id):\n  """"""Returns the reserved FILTER field with the given ID.""""""\n  return _get_reserved_field(field_id, RESERVED_FILTER_FIELDS)\n\n\ndef reserved_info_field(field_id):\n  """"""Returns the reserved INFO field with the given ID.""""""\n  return _get_reserved_field(field_id, RESERVED_INFO_FIELDS)\n\n\ndef reserved_format_field(field_id):\n  """"""Returns the reserved FORMAT field with the given ID.""""""\n  return _get_reserved_field(field_id, RESERVED_FORMAT_FIELDS)\n\n\ndef create_get_fn(value_type, number):\n  """"""Returns a callable that extracts the typed information from a ListValue.\n\n  Args:\n    value_type: str. The value type stored as defined in the VCF 4.3 spec.\n    number: str. The number of entries of this value as defined in the VCF spec.\n\n  Returns:\n    A callable that takes two inputs: A Map(str --> ListValue) and a string\n    field name and returns the associated typed value(s). The return value is\n    a list of typed values or a single typed value, depending on the expected\n    number of values returned.\n  """"""\n  is_single_field = (number == \'0\' or number == \'1\')\n  if value_type == CHARACTER_TYPE or value_type == STRING_TYPE:\n    return functools.partial(\n        struct_utils.get_string_field, is_single_field=is_single_field)\n  elif value_type == INTEGER_TYPE:\n    return functools.partial(\n        struct_utils.get_int_field, is_single_field=is_single_field)\n  elif value_type == FLOAT_TYPE:\n    return functools.partial(\n        struct_utils.get_number_field, is_single_field=is_single_field)\n  elif value_type == FLAG_TYPE:\n    return functools.partial(\n        struct_utils.get_bool_field, is_single_field=is_single_field)\n  else:\n    raise ValueError(\'Invalid value_type: {}\'.format(value_type))\n\n\n# Map from INFO field name to the function used to set struct_pb2.Value elements\n# of that field.\nRESERVED_INFO_FIELD_SET_FNS = {\n    info.id: SET_FN_LOOKUP[info.type]\n    for info in RESERVED_INFO_FIELDS\n}\n\n# Map from INFO field name to the function used to get struct_pb2.Value elements\n# of that field.\nRESERVED_INFO_FIELD_GET_FNS = {\n    info.id: create_get_fn(info.type, info.number)\n    for info in RESERVED_INFO_FIELDS\n}\n\n# Map from FORMAT field name to the function used to set struct_pb2.Value\n# elements of that field.\nRESERVED_FORMAT_FIELD_SET_FNS = {\n    fmt.id: SET_FN_LOOKUP[fmt.type]\n    for fmt in RESERVED_FORMAT_FIELDS\n}\n\n# Map from FORMAT field name to the function used to get struct_pb2.Value\n# elements of that field.\nRESERVED_FORMAT_FIELD_GET_FNS = {\n    fmt.id: create_get_fn(fmt.type, fmt.number)\n    for fmt in RESERVED_FORMAT_FIELDS\n}\n\n\ndef reserved_info_field_set_fn(field_name):\n  """"""Returns the callable that sets the proper field for the given field_name.\n\n  Args:\n    field_name: str. The field name of the reserved INFO field (e.g. \'MQ\').\n\n  Returns:\n    The callable that takes in a Map(str --> ListValue), field name, and value\n    and modifies the map to populate the field_name entry with the given value.\n\n  Raises:\n    ValueError: The field_name is not a known reserved INFO field.\n  """"""\n  try:\n    return RESERVED_INFO_FIELD_SET_FNS[field_name]\n  except KeyError:\n    raise ValueError(\'Unknown reserved INFO field: {}\'.format(field_name))\n\n\ndef reserved_info_field_get_fn(field_name):\n  """"""Returns the callable that gets the proper field for the given field_name.\n\n  Args:\n    field_name: str. The field name of the reserved INFO field (e.g. \'MQ\').\n\n  Returns:\n    The callable that takes in a Map(str --> ListValue), and field name and\n    returns the associated typed value(s).\n\n  Raises:\n    ValueError: The field_name is not a known reserved INFO field.\n  """"""\n  try:\n    return RESERVED_INFO_FIELD_GET_FNS[field_name]\n  except KeyError:\n    raise ValueError(\n        \'Unknown reserved INFO field to get: {}\'.format(field_name))\n\n\ndef reserved_format_field_set_fn(field_name):\n  """"""Returns the callable that sets the proper field for the given field_name.\n\n  Args:\n    field_name: str. The field name of the reserved FORMAT field (e.g. \'AD\').\n\n  Returns:\n    The callable that takes in a Map(str --> ListValue), field name, and value\n    and modifies the map to populate the field_name entry with the given value.\n\n  Raises:\n    ValueError: The field_name is not a known reserved FORMAT field.\n  """"""\n  try:\n    return RESERVED_FORMAT_FIELD_SET_FNS[field_name]\n  except KeyError:\n    raise ValueError(\'Unknown reserved FORMAT field: {}\'.format(field_name))\n\n\ndef reserved_format_field_get_fn(field_name):\n  """"""Returns the callable that gets the proper field for the given field_name.\n\n  Args:\n    field_name: str. The field name of the reserved FORMAT field (e.g. \'AD\').\n\n  Returns:\n    The callable that takes in a Map(str --> ListValue), and field name and\n    returns the associated typed value(s).\n\n  Raises:\n    ValueError: The field_name is not a known reserved FORMAT field.\n  """"""\n  try:\n    return RESERVED_FORMAT_FIELD_GET_FNS[field_name]\n  except KeyError:\n    raise ValueError(\n        \'Unknown reserved FORMAT field to get: {}\'.format(field_name))\n'"
third_party/nucleus/util/vcf_constants_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for third_party.nucleus.util.vcf_constants.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.util import struct_utils\nfrom third_party.nucleus.util import vcf_constants\n\n\nclass VcfConstantsTest(parameterized.TestCase):\n\n  def test_unique_reserved_filter(self):\n    num_reserved_filter = len(vcf_constants.RESERVED_FILTER_FIELDS)\n    unique_filt_ids = {filt.id for filt in vcf_constants.RESERVED_FILTER_FIELDS}\n    self.assertLen(unique_filt_ids, num_reserved_filter)\n\n  def test_unique_reserved_info(self):\n    num_reserved_info = len(vcf_constants.RESERVED_INFO_FIELDS)\n    unique_info_ids = {info.id for info in vcf_constants.RESERVED_INFO_FIELDS}\n    self.assertLen(unique_info_ids, num_reserved_info)\n\n  def test_unique_reserved_format(self):\n    num_reserved_format = len(vcf_constants.RESERVED_FORMAT_FIELDS)\n    unique_format_ids = {f.id for f in vcf_constants.RESERVED_FORMAT_FIELDS}\n    self.assertLen(unique_format_ids, num_reserved_format)\n\n  def test_get_reserved_filter(self):\n    filt = vcf_constants.reserved_filter_field(\'PASS\')\n    self.assertIsInstance(filt, variants_pb2.VcfFilterInfo)\n    self.assertEqual(filt.id, \'PASS\')\n    self.assertEqual(filt.description, \'All filters passed\')\n\n  @parameterized.parameters(\n      \'RefCall\',\n      \'LowQual\',\n      \'AD\',\n      \'DP\',\n      \'GT\',\n      \'GQ\',\n  )\n  def test_invalid_get_reserved_filter(self, field_id):\n    with self.assertRaisesRegexp(ValueError, \'No reserved field with id\'):\n      vcf_constants.reserved_filter_field(field_id)\n\n  @parameterized.parameters(\n      \'AA\',\n      \'AC\',\n      \'AD\',\n      \'ADF\',\n      \'END\',\n      \'H2\',\n  )\n  def test_get_reserved_info(self, field_id):\n    info = vcf_constants.reserved_info_field(field_id)\n    self.assertIsInstance(info, variants_pb2.VcfInfo)\n    self.assertEqual(info.id, field_id)\n\n  @parameterized.parameters(\n      \'PASS\',\n      \'GT\',\n      \'GQ\',\n      \'GL\',\n      \'FT\',\n  )\n  def test_invalid_get_reserved_info(self, field_id):\n    with self.assertRaisesRegexp(ValueError, \'No reserved field with id\'):\n      vcf_constants.reserved_info_field(field_id)\n\n  @parameterized.parameters(\n      \'AD\',\n      \'ADF\',\n      \'DP\',\n      \'GT\',\n      \'GQ\',\n      \'GL\',\n      \'FT\',\n      \'PL\',\n  )\n  def test_get_reserved_format(self, field_id):\n    fmt = vcf_constants.reserved_format_field(field_id)\n    self.assertIsInstance(fmt, variants_pb2.VcfFormatInfo)\n    self.assertEqual(fmt.id, field_id)\n\n  @parameterized.parameters(\n      \'PASS\',\n      \'AN\',\n      \'1000G\',\n      \'END\',\n      \'H2\',\n  )\n  def test_invalid_get_reserved_format(self, field_id):\n    with self.assertRaisesRegexp(ValueError, \'No reserved field with id\'):\n      vcf_constants.reserved_format_field(field_id)\n\n  @parameterized.parameters(\n      dict(\n          value_type=vcf_constants.CHARACTER_TYPE,\n          values=[\'a\'],\n          number=\'1\',\n          expected=\'a\'),\n      dict(\n          value_type=vcf_constants.CHARACTER_TYPE,\n          values=[\'b\'],\n          number=\'.\',\n          expected=[\'b\']),\n      dict(\n          value_type=vcf_constants.CHARACTER_TYPE,\n          values=[\'c\', \'d\'],\n          number=\'R\',\n          expected=[\'c\', \'d\']),\n      dict(\n          value_type=vcf_constants.FLAG_TYPE,\n          values=[True],\n          number=\'0\',\n          expected=True),\n      dict(\n          value_type=vcf_constants.FLOAT_TYPE,\n          values=[2.5],\n          number=\'1\',\n          expected=2.5),\n      dict(\n          value_type=vcf_constants.FLOAT_TYPE,\n          values=[2.5],\n          number=\'.\',\n          expected=[2.5]),\n      dict(\n          value_type=vcf_constants.FLOAT_TYPE,\n          values=[2.5, 3.5],\n          number=\'A\',\n          expected=[2.5, 3.5]),\n      dict(\n          value_type=vcf_constants.INTEGER_TYPE,\n          values=[2, 3, 4],\n          number=\'G\',\n          expected=[2, 3, 4]),\n      dict(\n          value_type=vcf_constants.STRING_TYPE,\n          values=[\'a\', \'bc\'],\n          number=\'.\',\n          expected=[\'a\', \'bc\']),\n  )\n  def test_create_get_fn(self, value_type, values, number, expected):\n    info = variants_pb2.Variant().info\n    set_fn = vcf_constants.SET_FN_LOOKUP[value_type]\n    set_fn(info, \'field\', values)\n    get_fn = vcf_constants.create_get_fn(value_type, number)\n    actual = get_fn(info, \'field\')\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(field=\'CIGAR\', expected=struct_utils.set_string_field),\n      dict(field=\'DP\', expected=struct_utils.set_int_field),\n      dict(field=\'MQ\', expected=struct_utils.set_number_field),\n      dict(field=\'SOMATIC\', expected=struct_utils.set_bool_field),\n  )\n  def test_reserved_info_field_set_fn(self, field, expected):\n    actual = vcf_constants.reserved_info_field_set_fn(field)\n    self.assertIs(actual, expected)\n\n  @parameterized.parameters(\n      dict(field=\'INVALID\'),\n      dict(field=\'EC\'),\n      dict(field=\'HQ\'),\n  )\n  def test_invalid_reserved_info_field_set_fn(self, field):\n    with self.assertRaisesRegexp(ValueError, \'Unknown reserved INFO field:\'):\n      vcf_constants.reserved_info_field_set_fn(field)\n\n  def test_reserved_info_field_get_fn(self):\n    info = variants_pb2.Variant().info\n    values = [\'C\']\n    struct_utils.set_string_field(info, \'AA\', values)\n    get_fn = vcf_constants.reserved_info_field_get_fn(\'AA\')\n    actual = get_fn(info, \'AA\')\n    self.assertEqual(actual, values[0])\n\n  @parameterized.parameters(\n      dict(field=\'INVALID\'),\n      dict(field=\'EC\'),\n      dict(field=\'HQ\'),\n  )\n  def test_invalid_reserved_info_field_get_fn(self, field):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Unknown reserved INFO field to get:\'):\n      vcf_constants.reserved_info_field_get_fn(field)\n\n  @parameterized.parameters(\n      dict(field=\'AD\', expected=struct_utils.set_int_field),\n      dict(field=\'GL\', expected=struct_utils.set_number_field),\n      dict(field=\'FT\', expected=struct_utils.set_string_field),\n  )\n  def test_reserved_format_field_set_fn(self, field, expected):\n    actual = vcf_constants.reserved_format_field_set_fn(field)\n    self.assertIs(actual, expected)\n\n  @parameterized.parameters(\n      dict(field=\'INVALID\'),\n      dict(field=\'CIGAR\'),\n      dict(field=\'H2\'),\n  )\n  def test_invalid_reserved_format_field_set_fn(self, field):\n    with self.assertRaisesRegexp(ValueError, \'Unknown reserved FORMAT field:\'):\n      vcf_constants.reserved_format_field_set_fn(field)\n\n  def test_reserved_format_field_get_fn(self):\n    info = variants_pb2.VariantCall().info\n    expected = [0.2, 0.5, 0.3]\n    struct_utils.set_number_field(info, \'GP\', expected[:])\n    get_fn = vcf_constants.reserved_format_field_get_fn(\'GP\')\n    actual = get_fn(info, \'GP\')\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(field=\'INVALID\'),\n      dict(field=\'CIGAR\'),\n      dict(field=\'H2\'),\n  )\n  def test_invalid_reserved_format_field_get_fn(self, field):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Unknown reserved FORMAT field to get:\'):\n      vcf_constants.reserved_format_field_get_fn(field)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/util/vis.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n# Lint as: python3\n""""""Utility functions for visualization and inspection of pileup examples.\n\nVisualization and inspection utility functions enable showing image-like array\ndata including those used in DeepVariant.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom IPython import display\nimport numpy as np\nfrom PIL import Image\nfrom PIL import ImageDraw\n\nfrom third_party.nucleus.protos import variants_pb2\n\nDEEPVARIANT_CHANNEL_NAMES = [\n    \'read base\', \'base quality\', \'mapping quality\', \'strand\',\n    \'read supports variant\', \'base differs from ref\'\n]\n\n\ndef get_image_array_from_example(example):\n  """"""Decode image/encoded and image/shape of an Example into a numpy array.\n\n  Parse image/encoded and image/shape features from a tensorflow Example and\n  decode the image into that shape.\n\n  Args:\n    example: a tensorflow Example containing features that include\n      ""image/encoded"" and ""image/shape""\n\n  Returns:\n    numpy array of dtype np.uint8.\n  """"""\n  features = example.features.feature\n  img = features[\'image/encoded\'].bytes_list.value[0]\n  shape = features[\'image/shape\'].int64_list.value[0:3]\n  return np.frombuffer(img, np.uint8).reshape(shape)\n\n\ndef split_3d_array_into_channels(arr):\n  """"""Split 3D array into a list of 2D arrays.\n\n  e.g. given a numpy array of shape (100, 200, 6), return a list of 6 channels,\n  each with shape (100, 200).\n\n  Args:\n    arr: a 3D numpy array.\n\n  Returns:\n    list of 2D numpy arrays.\n  """"""\n  return [arr[:, :, i] for i in range(arr.shape[-1])]\n\n\ndef channels_from_example(example):\n  """"""Extract image from an Example and return the list of channels.\n\n  Args:\n    example: a tensorflow Example containing features that include\n      ""image/encoded"" and ""image/shape""\n\n  Returns:\n    list of 2D numpy arrays, one for each channel.\n  """"""\n  image = get_image_array_from_example(example)\n  return split_3d_array_into_channels(image)\n\n\ndef convert_6_channels_to_rgb(channels):\n  """"""Convert 6-channel image from DeepVariant to RGB for quick visualization.\n\n  The 6 channels are: ""read base"", ""base quality"", ""mapping quality"", ""strand"",\n  ""supports variant"", ""base != reference"".\n\n  Args:\n    channels: a list of 6 numpy arrays.\n\n  Returns:\n    3D numpy array of 3 colors (Red, green, blue).\n  """"""\n  base = channels[0]\n  # qual is the minimum of base quality and mapping quality at each position\n  # 254 is the max value for quality scores because the SAM specification has\n  # 255 reserved for unavailable values.\n  qual = np.minimum(channels[1], channels[2])\n  strand = channels[3]\n  # alpha is <supports variant> * <base != reference>\n  alpha = np.multiply(channels[4] / 254.0, channels[5] / 254.0)\n  return np.multiply(np.stack([base, qual, strand]),\n                     alpha).astype(np.uint8).transpose([1, 2, 0])\n\n\ndef scale_colors_for_png(arr, vmin=0, vmax=255):\n  """"""Scale an array to integers between 0 and 255 to prep it for a PNG image.\n\n  Args:\n    arr: numpy array. Input array made up of integers or floats.\n    vmin: number. Minimum data value to map to 0. Values below this will be\n      clamped to this value and therefore become 0.\n    vmax: number. Maximum data value to map to 255. Values above this will be\n      clamped to this value and therefore become 255.\n\n  Returns:\n    numpy array of dtype np.uint8 (integers between 0 and 255).\n  """"""\n  if vmax == 0 or vmax <= vmin:\n    raise ValueError(\'vmin must be non-zero and higher than vmin.\')\n\n  # Careful not to modify the original array\n  scaled = np.copy(arr)\n\n  # Snap numbers in the array falling outside the range into the range,\n  # otherwise they will produce artifacts due to byte overflow\n  scaled[scaled > vmax] = vmax\n  scaled[scaled < vmin] = vmin\n\n  # Scale the input into the range of vmin to vmax\n  if vmin != 0 or vmax != 255:\n    scaled = ((scaled - vmin) / (vmax - vmin)) * 255\n  return scaled.astype(np.uint8)\n\n\ndef _get_image_type_from_array(arr):\n  """"""Find image type based on array dimensions.\n\n  Raises error on invalid image dimensions.\n  Args:\n    arr: numpy array. Input array.\n\n  Returns:\n    str. ""RGB"" or ""L"", meant for PIL.Image.fromarray.\n  """"""\n  if len(arr.shape) == 3 and arr.shape[2] == 3:\n    # 8-bit x 3 colors\n    return \'RGB\'\n  elif len(arr.shape) == 2:\n    # 8-bit, gray-scale\n    return \'L\'\n  else:\n    raise ValueError(\n        \'Input array must have either 2 dimensions or 3 dimensions where the \'\n        \'third dimension has 3 channels. i.e. arr.shape is (x,y) or (x,y,3). \'\n        \'Found shape {}.\'.format(arr.shape))\n\n\ndef autoscale_colors_for_png(arr, vmin=None, vmax=None):\n  """"""Adjust an array to prepare it for saving to an image.\n\n  Re-scale numbers in the input array to go from 0 to 255 to adapt them for a\n  PNG image.\n\n  Args:\n    arr: numpy array. Should be 2-dimensional or 3-dimensional where the third\n      dimension has 3 channels.\n    vmin: number (float or int). Minimum data value, which will correspond to\n      black in greyscale or lack of each color in RGB images. Default None takes\n      the minimum of the data from arr.\n    vmax: number (float or int). Maximum data value, which will correspond to\n      white in greyscale or full presence of each color in RGB images. Default\n      None takes the max of the data from arr.\n\n  Returns:\n    (modified numpy array, image_mode)\n  """"""\n  image_mode = _get_image_type_from_array(arr)\n\n  if vmin is None:\n    vmin = np.min(arr)\n  if vmax is None:\n    vmax = np.max(arr)\n\n  # In cases where all elements are the same, fix the vmax so that even though\n  # the whole image will be black, the user can at least see the shape\n  if vmin == vmax:\n    vmax = vmin + 1\n\n  scaled = scale_colors_for_png(arr, vmin=vmin, vmax=vmax)\n  return scaled, image_mode\n\n\ndef add_header(img, labels, mark_midpoints=True, header_height=20):\n  """"""Adds labels to the image, evenly distributed across the top.\n\n  This is primarily useful for showing the names of channels.\n\n  Args:\n    img: A PIL Image.\n    labels: list of strs. Labels for segments to write across the top.\n    mark_midpoints: bool. Whether to add a small vertical line marking the\n      center of each segment of the image.\n    header_height: int. Height of the header in pixels.\n\n  Returns:\n    A new PIL Image, taller than the original img and annotated.\n  """"""\n\n  # Create a taller image to make space for a header at the top.\n  new_height = header_height + img.size[1]\n  new_width = img.size[0]\n\n  if img.mode == \'RGB\':\n    placeholder_size = (new_height, new_width, 3)\n  else:\n    placeholder_size = (new_height, new_width)\n  placeholder = np.ones(placeholder_size, dtype=np.uint8) * 255\n\n  # Divide the image width into segments.\n  segment_width = img.size[0] / len(labels)\n\n  # Calculate midpoints for all segments.\n  midpoints = [int(segment_width * (i + 0.5)) for i in range(len(labels))]\n\n  if mark_midpoints:\n    # For each label, add a small line to mark the middle.\n    for x_position in midpoints:\n      placeholder[header_height - 5:header_height, x_position] = 0\n      # If image has an even width, it will need 2 pixels marked as the middle.\n      if segment_width % 2 == 0:\n        placeholder[header_height - 5:header_height, x_position + 1] = 0\n\n  bigger_img = Image.fromarray(placeholder, mode=img.mode)\n  # Place the original image inside the taller placeholder image.\n  bigger_img.paste(img, (0, header_height))\n\n  # Add a label for each segment.\n  draw = ImageDraw.Draw(bigger_img)\n  for i in range(len(labels)):\n    text = labels[i]\n    text_width = draw.textsize(text)[0]\n    # xy refers to the left top corner of the text, so to center the text on\n    # the midpoint, subtract half the text width from the midpoint position.\n    x_position = int(midpoints[i] - text_width / 2)\n    draw.text(xy=(x_position, 0), text=text, fill=\'black\')\n  return bigger_img\n\n\ndef save_to_png(arr,\n                path=None,\n                image_mode=None,\n                show=True,\n                labels=None,\n                scale=None):\n  """"""Make a PNG and show it from a numpy array of dtype=np.uint8.\n\n  Args:\n    arr: numpy array. Input array to save.\n    path: str. file path at which to save the image.\n    image_mode: ""RGB"" or ""L"". Leave as default=None to choose based on image\n      dimensions.\n    show: bool. Whether to display the image using IPython (for notebooks).\n    labels: list of str. Labels to show across the top of the image.\n    scale: integer. Number of pixels wide and tall to show each cell in the\n      array. This sizes up the image while keeping exactly the same number of\n      pixels for every cell in the array, preserving resolution and preventing\n      any interpolation or overlapping of pixels. Default None adapts to the\n      size of the image to multiply it up until a limit of 500 pixels, a\n      convenient size for use in notebooks. If saving to a file for automated\n      processing, scale=1 is recommended to keep output files small and simple\n      while still retaining all the information content.\n\n  Returns:\n    None. Saves an image at path and optionally shows it with IPython.display.\n  """"""\n  if image_mode is None:\n    image_mode = _get_image_type_from_array(arr)\n\n  img = Image.fromarray(arr, mode=image_mode)\n\n  if labels is not None:\n    img = add_header(img, labels)\n\n  if scale is None:\n    scale = max(1, int(500 / max(arr.shape)))\n\n  if scale != 1:\n    img = img.resize((img.size[0] * scale, img.size[1] * scale))\n\n  # Saving to a temporary file is needed even when showing in a notebook\n  if path is None:\n    path = \'/tmp/tmp.png\'\n  img.save(path)\n\n  # Show image (great for notebooks)\n  if show:\n    display.display(display.Image(path))\n\n\ndef array_to_png(arr,\n                 path=None,\n                 show=True,\n                 vmin=None,\n                 vmax=None,\n                 scale=None,\n                 labels=None):\n  """"""Save an array as a PNG image with PIL and show it.\n\n  Args:\n    arr: numpy array. Should be 2-dimensional or 3-dimensional where the third\n      dimension has 3 channels.\n    path: str. Path for the image output. Default is /tmp/tmp.png for quickly\n      showing the image in a notebook.\n    show: bool. Whether to show the image using IPython utilities, only works in\n      notebooks.\n    vmin: number. Minimum data value, which will correspond to black in\n      greyscale or lack of each color in RGB images. Default None takes the\n      minimum of the data from arr.\n    vmax: number. Maximum data value, which will correspond to white in\n      greyscale or full presence of each color in RGB images. Default None takes\n      the max of the data from arr.\n    scale: integer. Number of pixels wide and tall to show each cell in the\n      array. This sizes up the image while keeping exactly the same number of\n      pixels for every cell in the array, preserving resolution and preventing\n      any interpolation or overlapping of pixels. Default None adapts to the\n      size of the image to multiply it up until a limit of 500 pixels, a\n      convenient size for use in notebooks. If saving to a file for automated\n      processing, scale=1 is recommended to keep output files small and simple\n      while still retaining all the information content.\n    labels: list of str. Labels to show across the top of the image.\n\n  Returns:\n    None. Saves an image at path and optionally shows it with IPython.display.\n  """"""\n  scaled, image_mode = autoscale_colors_for_png(arr, vmin=vmin, vmax=vmax)\n  save_to_png(\n      scaled,\n      path=path,\n      show=show,\n      image_mode=image_mode,\n      labels=labels,\n      scale=scale)\n\n\ndef draw_deepvariant_pileup(example=None,\n                            channels=None,\n                            composite_type=None,\n                            annotated=True,\n                            labels=None,\n                            path=None,\n                            show=True,\n                            scale=None):\n  """"""Quick utility for showing a pileup example as channels or RGB.\n\n  Args:\n    example: A tensorflow Example containing image/encoded and image/shape\n      features. Will be parsed through channels_from_example. Ignored if\n      channels are provided directly. Either example OR channels is required.\n    channels: list of 2D arrays containing the data to draw. Either example OR\n      channels is required.\n    composite_type: str or None. Method for combining channels. One of\n      [None,""RGB""].\n    annotated: bool. Whether to add channel labels and mark midpoints.\n    labels: list of str. Which labels to add to the image. If annotated=True,\n      use default channels labels for DeepVariant.\n    path: str. Output file path for saving as an image. If None, just show plot.\n    show: bool. Whether to display the image for ipython notebooks. Set to False\n      to prevent extra output when running in bulk.\n    scale: integer. Multiplier to enlarge the image. Default: None, which will\n      set it automatically for a human-readable size. Set to 1 for no scaling.\n\n  Returns:\n    None. Saves an image at path and optionally shows it with IPython.display.\n  """"""\n  if example and not channels:\n    channels = channels_from_example(example)\n  elif not channels:\n    raise ValueError(\'Either example OR channels must be specified.\')\n\n  if composite_type is None:\n    img_array = np.concatenate(channels, axis=1)\n    if annotated and labels is None:\n      labels = DEEPVARIANT_CHANNEL_NAMES\n  elif composite_type == \'RGB\':\n    img_array = convert_6_channels_to_rgb(channels)\n    if annotated and labels is None:\n      labels = [\'\']  # Creates one midpoint with no label.\n  else:\n    raise ValueError(\n        ""Unrecognized composite_type: {}. Must be None or \'RGB\'"".format(\n            composite_type))\n\n  array_to_png(\n      img_array,\n      path=path,\n      show=show,\n      scale=scale,\n      labels=labels,\n      vmin=0,\n      vmax=254)\n\n\ndef variant_from_example(example):\n  """"""Extract Variant object from the \'variant/encoded\' feature of an Example.\n\n  Args:\n    example: a DeepVariant-style make_examples output example.\n\n  Returns:\n    A Nucleus Variant.\n  """"""\n  features = example.features.feature\n  var_string = features[\'variant/encoded\'].bytes_list.value[0]\n  return variants_pb2.Variant.FromString(var_string)\n\n\ndef locus_id_from_variant(variant):\n  """"""Create a locus ID of form ""chr:pos_ref"" from a Variant object.\n\n  Args:\n    variant: a nucleus variant.\n\n  Returns:\n    str.\n  """"""\n  return \'{}:{}_{}\'.format(variant.reference_name, variant.start,\n                           variant.reference_bases)\n\n\ndef alt_allele_indices_from_example(example):\n  """"""Extract indices of the particular alt allele(s) the example represents.\n\n  Args:\n    example: a DeepVariant make_examples output example.\n\n  Returns:\n    list of indices.\n  """"""\n  features = example.features.feature\n  val = features[\'alt_allele_indices/encoded\'].bytes_list.value[0]\n  # Extract the encoded proto into unsigned integers and convert to regular ints\n  mapped = [int(x) for x in np.frombuffer(val, dtype=np.uint8)]\n  # Format is [<field id + type>, <number of elements in array>, ...<array>].\n  # Extract the array only, leaving out the metadata.\n  return mapped[2:]\n\n\ndef alt_bases_from_indices(alt_allele_indices, alternate_bases):\n  """"""Get alt allele bases based on their indices.\n\n  e.g. one alt allele: [0], [""C""] => ""C""\n  or with two alt alleles: [0,2], [""C"", ""TT"", ""A""] => ""C-A""\n\n  Args:\n    alt_allele_indices: list of integers. Indices of the alt alleles for a\n      particular example.\n    alternate_bases: list of strings. All alternate alleles for the variant.\n\n  Returns:\n    str. Alt allele(s) at the indices, joined by \'-\' if more than 1.\n  """"""\n  alleles = [alternate_bases[i] for i in alt_allele_indices]\n  # Avoiding \'/\' to support use in file paths.\n  return \'-\'.join(alleles)\n\n\ndef alt_from_example(example):\n  """"""Get alt allele(s) from a DeepVariant example.\n\n  Args:\n    example: a DeepVariant make_examples output example\n\n  Returns:\n    str. The bases of the alt alleles, joined by a -.\n  """"""\n  variant = variant_from_example(example)\n  indices = alt_allele_indices_from_example(example)\n  return alt_bases_from_indices(indices, variant.alternate_bases)\n\n\ndef locus_id_with_alt(example):\n  """"""Get complete locus ID from a DeepVariant example.\n\n  Args:\n    example: a DeepVariant make_examples output example\n\n  Returns:\n    str in the form ""chr:pos_ref_alt.\n  """"""\n  variant = variant_from_example(example)\n  locus_id = locus_id_from_variant(variant)\n  alt = alt_from_example(example)\n  return \'{}_{}\'.format(locus_id, alt)\n'"
third_party/nucleus/util/vis_test.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n# Lint as: python3\n""""""Tests for third_party.nucleus.util.vis.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport glob\nimport os\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import vis\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.core.example import example_pb2\nfrom tensorflow.core.example import feature_pb2\n\n\ndef _bytes_feature(list_of_strings):\n  """"""Returns a bytes_list from a list of string / byte.""""""\n  return feature_pb2.Feature(\n      bytes_list=feature_pb2.BytesList(value=list_of_strings))\n\n\ndef _int_feature(list_of_ints):\n  """"""Returns a int64_list from a list of int / bool.""""""\n  return feature_pb2.Feature(\n      int64_list=feature_pb2.Int64List(value=list_of_ints))\n\n\ndef _image_array(shape):\n  return np.random.randint(255, size=shape, dtype=np.uint8)\n\n\ndef _mock_example_with_image(shape):\n  arr = _image_array(shape)\n  feature = {\n      \'image/encoded\': _bytes_feature([arr.tobytes()]),\n      \'image/shape\': _int_feature(shape)\n  }\n  return arr, example_pb2.Example(\n      features=feature_pb2.Features(feature=feature))\n\n\ndef _mock_example_with_variant_and_alt_allele_indices(\n    encoded_indices=b\'\\n\\x01\\x00\', alleles=(\'A\', \'C\')):\n  variant = test_utils.make_variant(chrom=\'X\', alleles=alleles, start=10)\n  feature = {\n      \'variant/encoded\': _bytes_feature([variant.SerializeToString()]),\n      \'alt_allele_indices/encoded\': _bytes_feature([encoded_indices])\n  }\n  return example_pb2.Example(features=feature_pb2.Features(feature=feature))\n\n\nclass VisTest(parameterized.TestCase):\n\n  def test_get_image_array_from_example(self):\n    shape = (3, 2, 4)\n    arr, example = _mock_example_with_image(shape)\n    decoded_image_array = vis.get_image_array_from_example(example)\n    self.assertTrue((arr == decoded_image_array).all())\n\n  @parameterized.parameters(((5, 4, 3),), ((10, 7, 5),))\n  def test_split_3d_array_into_channels(self, input_shape):\n    arr = np.random.random(input_shape)\n    output = vis.split_3d_array_into_channels(arr)\n    self.assertLen(output, input_shape[2])\n    for i in range(input_shape[2]):\n      self.assertEqual(output[i].shape, arr.shape[0:2])\n      self.assertTrue((output[i] == arr[:, :, i]).all())\n\n  def test_channels_from_example(self):\n    shape = (3, 2, 4)\n    arr, example = _mock_example_with_image(shape)\n    channels = vis.channels_from_example(example)\n    self.assertLen(channels, shape[2])\n    self.assertTrue((channels[0] == arr[:, :, 0]).all())\n\n  @parameterized.parameters(((4, 8), (4, 8, 3)), ((100, 20), (100, 20, 3)))\n  def test_convert_6_channels_to_rgb(self, input_shape, expected_output_shape):\n    channels = [np.random.random(input_shape) for _ in range(6)]\n    rgb = vis.convert_6_channels_to_rgb(channels)\n    self.assertEqual(rgb.shape, expected_output_shape)\n\n  @parameterized.parameters((None,), (\'RGB\',))\n  def test_draw_deepvariant_pileup_with_example_input(self, composite_type):\n    _, example = _mock_example_with_image((100, 10, 7))\n    # Testing that it runs without error\n    vis.draw_deepvariant_pileup(example=example, composite_type=composite_type)\n\n  @parameterized.parameters((None,), (\'RGB\',))\n  def test_draw_deepvariant_pileup_with_channels_input(self, composite_type):\n    channels = [_image_array((100, 221)) for _ in range(6)]\n    # Testing that it runs without error\n    vis.draw_deepvariant_pileup(\n        channels=channels, composite_type=composite_type)\n\n  @parameterized.parameters(\n      ([[0.0, 1], [5, 10]], 0, 10, [[0, 25], [127, 255]]),\n      ([[0.0, 0.1], [0.5, 1]], 0, 1, [[0, 25], [127, 255]]),\n      ([[0.0, 0.1], [0.5, 1]], 0, 0.5, [[0, 51], [255, 255]]),\n      ([[0.0, 0.1], [0.5, 1]], 0.5, 1, [[0, 0], [0, 255]]),\n      ([[0.0, 0.1], [0.5, 1]], -1, 1, [[127, 140], [191, 255]]),\n      ([[0.0, 0.1], [0.5, 1]], -1, 2, [[85, 93], [127, 170]]))\n  def test_scale_colors_for_png(self, arr, vmin, vmax, expected):\n    arr = np.array(arr)\n    scaled = vis.scale_colors_for_png(arr, vmin=vmin, vmax=vmax)\n    self.assertTrue((scaled == expected).all())\n\n  @parameterized.parameters(\n      ((100, 200), \'L\'),\n      ((100, 200, 3), \'RGB\'),\n  )\n  def test_autoscale_colors_for_png(self, shape, expected_image_mode):\n    arr = np.random.random(shape)\n    scaled, image_mode = vis.autoscale_colors_for_png(arr)\n    # Original array should be unchanged.\n    self.assertLess(np.max(arr), 1)\n    self.assertNotEqual(arr.dtype, np.uint8)\n    # Output values have been scaled up and the array\'s data type changed.\n    self.assertGreater(np.max(scaled), 1)\n    self.assertEqual(scaled.dtype, np.uint8)\n    self.assertEqual(image_mode, expected_image_mode)\n\n  @parameterized.parameters(\n      ((100, 200), \'L\'),\n      ((10, 1), \'L\'),\n      ((100, 200, 3), \'RGB\'),\n      ((10, 1, 3), \'RGB\'),\n      ((100, 200, 6), None),\n      ((100, 200, 3, 1), None),\n      ((100), None),\n  )\n  def test_get_image_type_from_array(self, shape, expected):\n    arr = _image_array(shape)\n    if expected is not None:\n      self.assertEqual(vis._get_image_type_from_array(arr), expected)\n    else:\n      self.assertRaisesWithPredicateMatch(\n          ValueError, lambda x: str(x).index(\'dimensions\') != -1,\n          vis.save_to_png, arr)\n\n  @parameterized.parameters(\n      ((100, 200, 3), True),\n      ((100, 200), True),\n      ((100, 200, 6), False),\n      ((100, 200, 3, 1), False),\n      ((100), False),\n  )\n  def test_save_to_png(self, shape, should_succeed):\n    arr = _image_array(shape)\n\n    if should_succeed:\n      temp_dir = self.create_tempdir().full_path\n      output_path = os.path.join(temp_dir, \'test.png\')\n      # check the file doesn\'t already exist before function runs\n      self.assertEmpty(glob.glob(output_path))\n      vis.save_to_png(arr, path=output_path)\n      self.assertLen(glob.glob(output_path), 1)\n    else:\n      self.assertRaisesWithPredicateMatch(\n          ValueError, lambda x: str(x).index(\'dimensions\') != -1,\n          vis.save_to_png, arr)\n\n  @parameterized.parameters(\n      ((100, 200, 3), True),\n      ((100, 200), True),\n      ((100, 200, 6), False),\n      ((100, 200, 3, 1), False),\n      ((100), False),\n  )\n  def test_array_to_png_works_with_floats(self, shape, should_succeed):\n    arr = np.random.random(shape)\n\n    if should_succeed:\n      temp_dir = self.create_tempdir().full_path\n      output_path = os.path.join(temp_dir, \'test.png\')\n      # Check the file doesn\'t already exist before function runs.\n      self.assertEmpty(glob.glob(output_path))\n      vis.array_to_png(arr, path=output_path)\n      self.assertLen(glob.glob(output_path), 1)\n    else:\n      self.assertRaisesWithPredicateMatch(\n          ValueError, lambda x: str(x).index(\'dimensions\') != -1,\n          vis.array_to_png, arr)\n\n  def test_variant_from_example(self):\n    example = _mock_example_with_variant_and_alt_allele_indices()\n    variant = vis.variant_from_example(example)\n    self.assertIsInstance(variant, variants_pb2.Variant)\n\n  @parameterized.parameters(\n      (b\'\\n\\x01\\x00\', [0]),\n      (b\'\\n\\x02\\x00\\x01\', [0, 1]),\n  )\n  def test_alt_allele_indices_from_example(self, encoded_indices, expected):\n    example = _mock_example_with_variant_and_alt_allele_indices(encoded_indices)\n    indices = vis.alt_allele_indices_from_example(example)\n    self.assertEqual(indices, expected)\n\n  @parameterized.parameters(\n      (\'chr1\', 100, \'G\', \'chr1:100_G\'),\n      (\'X\', 0, \'GACGT\', \'X:0_GACGT\'),\n  )\n  def test_locus_id_from_variant(self, chrom, pos, ref, expected):\n    variant = test_utils.make_variant(\n        chrom=chrom, alleles=[ref, \'A\'], start=pos)\n    locus_id = vis.locus_id_from_variant(variant)\n    self.assertEqual(locus_id, expected)\n\n  @parameterized.parameters(\n      (b\'\\n\\x01\\x00\', [\'A\', \'G\', \'GA\', \'AG\'], \'G\'),\n      (b\'\\n\\x02\\x00\\x01\', [\'C\', \'CA\', \'T\', \'TA\'], \'CA-T\'),\n      (b\'\\n\\x02\\x01\\x02\', [\'C\', \'CA\', \'T\', \'TA\'], \'T-TA\'),\n  )\n  def test_alt_from_example(self, encoded_indices, alleles, expected):\n    example = _mock_example_with_variant_and_alt_allele_indices(\n        encoded_indices=encoded_indices, alleles=alleles)\n    alt = vis.alt_from_example(example)\n    self.assertEqual(alt, expected)\n\n  @parameterized.parameters(\n      (b\'\\n\\x01\\x00\', [\'A\', \'G\', \'GA\', \'AG\'], \'X:10_A_G\'),\n      (b\'\\n\\x02\\x00\\x01\', [\'C\', \'CA\', \'T\', \'TA\'], \'X:10_C_CA-T\'),\n      (b\'\\n\\x02\\x01\\x02\', [\'C\', \'CA\', \'T\', \'TA\'], \'X:10_C_T-TA\'),\n  )\n  def test_locus_id_with_alt(self, encoded_indices, alleles, expected):\n    example = _mock_example_with_variant_and_alt_allele_indices(\n        encoded_indices=encoded_indices, alleles=alleles)\n    locus_id_with_alt = vis.locus_id_with_alt(example)\n    self.assertEqual(locus_id_with_alt, expected)\n\n  @parameterized.parameters(\n      ([0], [\'C\'], \'C\'),\n      ([0, 1], [\'C\', \'TT\'], \'C-TT\'),\n      ([3, 4], [\'C\', \'TT\', \'T\', \'G\', \'A\'], \'G-A\'),\n  )\n  def test_alt_bases_from_indices(self, indices, alternate_bases, expected):\n    alt = vis.alt_bases_from_indices(indices, alternate_bases)\n    self.assertEqual(alt, expected)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/vendor/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n'"
third_party/nucleus/io/python/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n'"
third_party/nucleus/io/python/bed_reader_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for bed_reader CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import clif_postproc\nfrom third_party.nucleus.io.python import bed_reader\nfrom third_party.nucleus.protos import bed_pb2\nfrom third_party.nucleus.testing import test_utils\n\n\nclass BedReaderTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.bed = test_utils.genomics_core_testdata(\'test_regions.bed\')\n    self.zipped_bed = test_utils.genomics_core_testdata(\'test_regions.bed.gz\')\n    self.options = bed_pb2.BedReaderOptions()\n    self.first = bed_pb2.BedRecord(\n        reference_name=\'chr1\',\n        start=10,\n        end=20,\n        name=\'first\',\n        score=100,\n        strand=bed_pb2.BedRecord.FORWARD_STRAND,\n        thick_start=12,\n        thick_end=18,\n        item_rgb=\'255,124,1\',\n        block_count=3,\n        block_sizes=\'2,6,2\',\n        block_starts=\'10,12,18\')\n\n  def test_bed_iterate(self):\n    with bed_reader.BedReader.from_file(self.bed, self.options) as reader:\n      self.assertEqual(reader.header.num_fields, 12)\n      iterable = reader.iterate()\n      self.assertIsInstance(iterable, clif_postproc.WrappedCppIterable)\n      actual = list(iterable)\n      self.assertLen(actual, 2)\n      self.assertEqual(actual[0], self.first)\n\n    zreader = bed_reader.BedReader.from_file(\n        self.zipped_bed,\n        bed_pb2.BedReaderOptions())\n    self.assertEqual(zreader.header.num_fields, 12)\n    with zreader:\n      ziterable = zreader.iterate()\n      self.assertIsInstance(ziterable, clif_postproc.WrappedCppIterable)\n      zactual = list(ziterable)\n      self.assertLen(zactual, 2)\n      self.assertEqual(zactual[0], self.first)\n\n  def test_from_file_raises_with_missing_bed(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Not found: Could not open missing.bed\'):\n      bed_reader.BedReader.from_file(\'missing.bed\', self.options)\n\n  def test_ops_on_closed_reader_raise(self):\n    reader = bed_reader.BedReader.from_file(self.bed, self.options)\n    with reader:\n      pass\n    # At this point the reader is closed.\n    with self.assertRaisesRegexp(ValueError, \'Cannot Iterate a closed\'):\n      reader.iterate()\n\n  @parameterized.parameters(\'malformed.bed\', \'malformed2.bed\')\n  def test_bed_iterate_raises_on_malformed_record(self, filename):\n    malformed = test_utils.genomics_core_testdata(filename)\n    reader = bed_reader.BedReader.from_file(malformed, self.options)\n    iterable = iter(reader.iterate())\n    self.assertIsNotNone(next(iterable))\n    with self.assertRaises(ValueError):\n      list(iterable)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/python/bed_writer_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for BedWriter CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom tensorflow.python.platform import gfile\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.io.python import bed_writer\nfrom third_party.nucleus.protos import bed_pb2\nfrom third_party.nucleus.testing import test_utils\n\n\n_DOUBLE_CLOSE_ERROR = \'Cannot close an already closed BedWriter\'\n_WRITE_TO_CLOSED_ERROR = \'Cannot write to closed BED stream\'\n\n\nclass WrapBedWriterTest(parameterized.TestCase):\n\n  def setUp(self):\n    out_fname = test_utils.test_tmpfile(\'output.bed\')\n    self.writer = bed_writer.BedWriter.to_file(\n        out_fname, bed_pb2.BedHeader(num_fields=12), bed_pb2.BedWriterOptions())\n    self.expected_bed_content = [\n        \'chr1\\t10\\t20\\tfirst\\t100\\t+\\t12\\t18\\t255,124,1\\t3\\t2,6,2\\t10,12,18\\n\',\n        \'chr1\\t100\\t200\\tsecond\\t250\\t.\\t120\\t180\\t252,122,12\\t2\\t35,40\\t\'\n        \'100,160\\n\'\n    ]\n    self.record = bed_pb2.BedRecord(\n        reference_name=\'chr1\', start=20, end=30, name=\'r\')\n\n  def test_writing_canned_records(self):\n    """"""Tests writing all the records that are \'canned\' in our tfrecord file.""""""\n    # This file is in TFRecord format.\n    tfrecord_file = test_utils.genomics_core_testdata(\n        \'test_regions.bed.tfrecord\')\n\n    header = bed_pb2.BedHeader(num_fields=12)\n    writer_options = bed_pb2.BedWriterOptions()\n    bed_records = list(\n        tfrecord.read_tfrecords(tfrecord_file, proto=bed_pb2.BedRecord))\n    out_fname = test_utils.test_tmpfile(\'output.bed\')\n    with bed_writer.BedWriter.to_file(out_fname, header,\n                                      writer_options) as writer:\n      for record in bed_records:\n        writer.write(record)\n\n    with gfile.Open(out_fname, \'r\') as f:\n      self.assertEqual(f.readlines(), self.expected_bed_content)\n\n  def test_context_manager(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.record))\n\n    # self.writer should be closed, so writing again will fail.\n    with self.assertRaisesRegexp(ValueError, _WRITE_TO_CLOSED_ERROR):\n      self.writer.write(self.record)\n\n  def test_double_context_manager(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.record))\n\n    with self.assertRaisesRegexp(ValueError, _DOUBLE_CLOSE_ERROR):\n      # Entering the closed writer should be fine.\n      with self.writer:\n        pass  # We want to raise an error on exit, so nothing to do in context.\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/python/fastq_reader_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for fastq_reader CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import clif_postproc\nfrom third_party.nucleus.io.python import fastq_reader\nfrom third_party.nucleus.protos import fastq_pb2\nfrom third_party.nucleus.testing import test_utils\n\n\nclass FastqReaderTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.fastq = test_utils.genomics_core_testdata(\'test_reads.fastq\')\n    self.options = fastq_pb2.FastqReaderOptions()\n\n  @parameterized.parameters(\'test_reads.fastq\', \'test_reads.fastq.gz\',\n                            \'test_reads.bgzip.fastq.gz\')\n  def test_fastq_iterate(self, filename):\n    path = test_utils.genomics_core_testdata(filename)\n    with fastq_reader.FastqReader.from_file(path, self.options) as reader:\n      iterable = reader.iterate()\n      self.assertIsInstance(iterable, clif_postproc.WrappedCppIterable)\n      self.assertEqual(test_utils.iterable_len(iterable), 4)\n\n  def test_from_file_raises_with_missing_fastq(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Not found: Could not open missing.fastq\'):\n      fastq_reader.FastqReader.from_file(\'missing.fastq\', self.options)\n\n  def test_ops_on_closed_reader_raise(self):\n    reader = fastq_reader.FastqReader.from_file(self.fastq, self.options)\n    with reader:\n      pass\n    # At this point the reader is closed.\n    with self.assertRaisesRegexp(ValueError, \'Cannot Iterate a closed\'):\n      reader.iterate()\n\n  @parameterized.parameters(\'malformed.fastq\', \'malformed2.fastq\')\n  def test_fastq_iterate_raises_on_malformed_record(self, filename):\n    malformed = test_utils.genomics_core_testdata(filename)\n    reader = fastq_reader.FastqReader.from_file(malformed, self.options)\n    iterable = iter(reader.iterate())\n    self.assertIsNotNone(next(iterable))\n    with self.assertRaises(ValueError):\n      list(iterable)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/python/fastq_writer_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for FastqWriter CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom tensorflow.python.platform import gfile\nfrom third_party.nucleus.io import fastq\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.io.python import fastq_writer\nfrom third_party.nucleus.protos import fastq_pb2\nfrom third_party.nucleus.testing import test_utils\n\n_DOUBLE_CLOSE_ERROR = \'Cannot close an already closed FastqWriter\'\n_WRITE_TO_CLOSED_ERROR = \'Cannot write to closed FASTQ stream\'\n\n\nclass WrapFastqWriterTest(parameterized.TestCase):\n\n  def setUp(self):\n    writer_options = fastq_pb2.FastqWriterOptions()\n    out_fname = test_utils.test_tmpfile(\'output.fastq\')\n    self.writer = fastq_writer.FastqWriter.to_file(out_fname, writer_options)\n    self.expected_fastq_content = [\n        \'@NODESC:header\\n\',\n        \'GATTACA\\n\',\n        \'+\\n\',\n        \'BB>B@FA\\n\',\n        \'@M01321:49:000000000-A6HWP:1:1101:17009:2216 1:N:0:1\\n\',\n        \'CGTTAGCGCAGGGGGCATCTTCACACTGGTGACAGGTAACCGCCGTAGTAAAGGTTCCGCCTTTCACT\\n\',\n        \'+\\n\',\n        \'AAAAABF@BBBDGGGG?FFGFGHBFBFBFABBBHGGGFHHCEFGGGGG?FGFFHEDG3EFGGGHEGHG\\n\',\n        \'@FASTQ contains multiple spaces in description\\n\',\n        \'CGGCTGGTCAGGCTGACATCGCCGCCGGCCTGCAGCGAGCCGCTGC\\n\',\n        \'+\\n\',\n        \'FAFAF;F/9;.:/;999B/9A.DFFF;-->.AAB/FC;9-@-=;=.\\n\',\n        \'@FASTQ_with_trailing_space\\n\',\n        \'CGG\\n\',\n        \'+\\n\',\n        \'FAD\\n\',\n    ]\n    self.record = fastq_pb2.FastqRecord(\n        id=\'ID\', description=\'desc\', sequence=\'ACGTAC\', quality=\'ABCDEF\')\n\n  def test_writing_canned_records(self):\n    """"""Tests writing all the variants that are \'canned\' in our tfrecord file.""""""\n    # This file is in TFRecord format.\n    tfrecord_file = test_utils.genomics_core_testdata(\n        \'test_reads.fastq.tfrecord\')\n\n    writer_options = fastq_pb2.FastqWriterOptions()\n    fastq_records = list(\n        tfrecord.read_tfrecords(tfrecord_file, proto=fastq_pb2.FastqRecord))\n    out_fname = test_utils.test_tmpfile(\'output.fastq\')\n    with fastq_writer.FastqWriter.to_file(out_fname, writer_options) as writer:\n      for record in fastq_records:\n        writer.write(record)\n\n    with gfile.Open(out_fname, \'r\') as f:\n      self.assertEqual(f.readlines(), self.expected_fastq_content)\n\n  def test_context_manager(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.record))\n\n    # self.writer should be closed, so writing again will fail.\n    with self.assertRaisesRegexp(ValueError, _WRITE_TO_CLOSED_ERROR):\n      self.writer.write(self.record)\n\n  def test_double_context_manager(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.record))\n\n    with self.assertRaisesRegexp(ValueError, _DOUBLE_CLOSE_ERROR):\n      # Entering the closed writer should be fine.\n      with self.writer:\n        pass  # We want to raise an error on exit, so nothing to do in context.\n\n\nclass WrapFastqWriterRoundTripTests(parameterized.TestCase):\n\n  @parameterized.parameters(\'test_reads.fastq\', \'test_reads.fastq.gz\')\n  def test_round_trip_fastq(self, test_datum_name):\n    # Round-trip FASTQ records through writing and reading:\n    # 1. Read records v1 from FastqReader;\n    # 2. Write v1 to fastq using our FastqWriter;\n    # 3. Read back in using FastqReader -- v2;\n    # 4. compare v1 and v2.\n    in_file = test_utils.genomics_core_testdata(test_datum_name)\n    out_file = test_utils.test_tmpfile(\'output_\' + test_datum_name)\n\n    v1_reader = fastq.FastqReader(in_file)\n    v1_records = list(v1_reader.iterate())\n    self.assertTrue(v1_records, \'Reader failed to find records\')\n\n    writer_options = fastq_pb2.FastqWriterOptions()\n\n    with fastq_writer.FastqWriter.to_file(out_file, writer_options) as writer:\n      for record in v1_records:\n        writer.write(record)\n\n    v2_reader = fastq.FastqReader(out_file)\n    v2_records = list(v2_reader.iterate())\n    self.assertEqual(v1_records, v2_records,\n                     \'Round-tripped FASTQ files not as expected\')\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/python/gff_reader_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for gff_reader CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import clif_postproc\nfrom third_party.nucleus.io.python import gff_reader\nfrom third_party.nucleus.protos import gff_pb2\nfrom third_party.nucleus.testing import test_utils\n\n\nclass GffReaderTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.options = gff_pb2.GffReaderOptions()\n    self.first = gff_pb2.GffRecord()\n    self.first.range.reference_name = \'ctg123\'\n    self.first.range.start = 999\n    self.first.range.end = 9000\n    self.first.source = \'GenBank\'\n    self.first.type = \'gene\'\n    self.first.score = 2.5\n    self.first.strand = gff_pb2.GffRecord.FORWARD_STRAND\n    self.first.phase = 0\n    self.first.attributes[\'ID\'] = \'gene00001\'\n    self.first.attributes[\'Name\'] = \'EDEN\'\n\n    self.second = gff_pb2.GffRecord()\n    self.second.range.reference_name = \'ctg123\'\n    self.second.range.start = 999\n    self.second.range.end = 1012\n    self.second.phase = -1\n    self.second.score = -float(\'inf\')\n\n  @parameterized.parameters(\'test_features.gff\', \'test_features.gff.gz\')\n  def test_gff_iterate(self, test_features_gff_filename):\n    file_path = test_utils.genomics_core_testdata(test_features_gff_filename)\n    with gff_reader.GffReader.from_file(file_path, self.options) as reader:\n      iterable = reader.iterate()\n      self.assertIsInstance(iterable, clif_postproc.WrappedCppIterable)\n      actual = list(iterable)\n      self.assertLen(actual, 2)\n      self.assertEqual(actual[0], self.first)\n      self.assertEqual(actual[1], self.second)\n\n  def test_from_file_raises_with_missing_gff(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Not found: Could not open missing.gff\'):\n      gff_reader.GffReader.from_file(\'missing.gff\', self.options)\n\n  def test_ops_on_closed_reader_raise(self):\n    file_path = test_utils.genomics_core_testdata(\'test_features.gff\')\n    reader = gff_reader.GffReader.from_file(file_path, self.options)\n    with reader:\n      pass\n    # At this point the reader is closed.\n    with self.assertRaisesRegexp(ValueError, \'Cannot Iterate a closed\'):\n      reader.iterate()\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/python/gff_writer_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for GffWriter CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.io.python import gff_writer\nfrom third_party.nucleus.protos import gff_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\n\n_DOUBLE_CLOSE_ERROR = \'Cannot close an already closed GffWriter\'\n_WRITE_TO_CLOSED_ERROR = \'Cannot write to closed GFF stream\'\n\n\nclass WrapGffWriterTest(parameterized.TestCase):\n\n  def setUp(self):\n    out_fname = test_utils.test_tmpfile(\'output.gff\')\n    self.writer = gff_writer.GffWriter.to_file(out_fname, gff_pb2.GffHeader(),\n                                               gff_pb2.GffWriterOptions())\n    self.expected_gff_content = open(\n        test_utils.genomics_core_testdata(\'test_features.gff\')).readlines()\n    self.header = gff_pb2.GffHeader(\n        sequence_regions=[ranges.make_range(\'ctg123\', 0, 1497228)])\n    self.record = gff_pb2.GffRecord(\n        range=ranges.make_range(\'ctg123\', 1000, 1100))\n\n  def test_writing_canned_records(self):\n    """"""Tests writing all the records that are \'canned\' in our tfrecord file.""""""\n    # This file is in TFRecord format.\n    tfrecord_file = test_utils.genomics_core_testdata(\n        \'test_features.gff.tfrecord\')\n    writer_options = gff_pb2.GffWriterOptions()\n    gff_records = list(\n        tfrecord.read_tfrecords(tfrecord_file, proto=gff_pb2.GffRecord))\n    out_fname = test_utils.test_tmpfile(\'output.gff\')\n    with gff_writer.GffWriter.to_file(out_fname, self.header,\n                                      writer_options) as writer:\n      for record in gff_records:\n        writer.write(record)\n\n    with open(out_fname) as f:\n      self.assertEqual(f.readlines(), self.expected_gff_content)\n\n  def test_context_manager(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.record))\n\n    # self.writer should be closed, so writing again will fail.\n    with self.assertRaisesRegexp(ValueError, _WRITE_TO_CLOSED_ERROR):\n      self.writer.write(self.record)\n\n  def test_double_close(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.record))\n\n    with self.assertRaisesRegexp(ValueError, _DOUBLE_CLOSE_ERROR):\n      # Entering the closed writer should be fine.\n      with self.writer:\n        pass  # We want to raise an error on exit, so nothing to do in context.\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/python/hts_verbose_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Tests for hts_verbose.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\n\nfrom third_party.nucleus.io.python import hts_verbose\n\n\nclass HtsVerbose(absltest.TestCase):\n\n  def test_set(self):\n    hts_verbose.set(hts_verbose.htsLogLevel.HTS_LOG_TRACE)\n    level = hts_verbose.get()\n    self.assertEqual(level, hts_verbose.htsLogLevel.HTS_LOG_TRACE)\n\n    hts_verbose.set(hts_verbose.htsLogLevel.HTS_LOG_INFO)\n    level = hts_verbose.get()\n    self.assertEqual(level, hts_verbose.htsLogLevel.HTS_LOG_INFO)\n\n    hts_verbose.set(hts_verbose.htsLogLevel[\'HTS_LOG_DEBUG\'])\n    level = hts_verbose.get()\n    self.assertEqual(level, hts_verbose.htsLogLevel.HTS_LOG_DEBUG)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/python/reference_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for GenomeReference CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io.python import reference\nfrom third_party.nucleus.protos import fasta_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\n\n\nclass WrapReferenceTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      (\'test.fasta\', False, \'TAACC\'),\n      (\'test.fasta\', True, \'TaaCC\'),\n      (\'test.fasta.gz\', False, \'TAACC\'),\n      (\'test.fasta.gz\', True, \'TaaCC\'))\n  def test_wrap(self, fasta_filename, keep_true_case, expected_bases):\n    chr_names = [\'chrM\', \'chr1\', \'chr2\']\n    chr_lengths = [100, 76, 121]\n    fasta = test_utils.genomics_core_testdata(fasta_filename)\n    fai = test_utils.genomics_core_testdata(fasta_filename + \'.fai\')\n    options = fasta_pb2.FastaReaderOptions(keep_true_case=keep_true_case)\n    with reference.IndexedFastaReader.from_file(fasta, fai, options) as ref:\n      self.assertEqual(ref.contig_names, chr_names)\n      self.assertEqual(ref.bases(ranges.make_range(\'chrM\', 22, 27)),\n                       expected_bases)\n\n      self.assertTrue(ref.is_valid_interval(ranges.make_range(\'chrM\', 1, 10)))\n      self.assertFalse(\n          ref.is_valid_interval(ranges.make_range(\'chrM\', 1, 100000)))\n\n      self.assertLen(ref.contigs, 3)\n      self.assertEqual([c.name for c in ref.contigs], chr_names)\n      self.assertEqual([c.n_bases for c in ref.contigs], chr_lengths)\n      for contig in ref.contigs:\n        self.assertEqual(ref.contig(contig.name), contig)\n        self.assertTrue(ref.has_contig(contig.name))\n        self.assertFalse(ref.has_contig(contig.name + \'.unknown\'))\n\n  @parameterized.parameters(\n      # The fasta and the FAI are both missing.\n      (\'missing.fasta\', \'missing.fasta.fai\'),\n      # The fasta is present but the FAI is missing.\n      (\'test.fasta\', \'missing.fasta.fai\'),\n      # The fasta is missing but the FAI is present.\n      (\'missing.fasta\', \'test.fasta.fai\'),\n  )\n  def test_from_file_raises_with_missing_inputs(self, fasta_filename,\n                                                fai_filename):\n    fasta = test_utils.genomics_core_testdata(fasta_filename)\n    fai = test_utils.genomics_core_testdata(fai_filename)\n    with self.assertRaisesRegexp(\n        ValueError,\n        \'Not found: could not load fasta and/or fai for fasta \' + fasta):\n      reference.IndexedFastaReader.from_file(fasta, fai,\n                                             fasta_pb2.FastaReaderOptions())\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/python/sam_reader_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for sam_reader CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom third_party.nucleus.io import clif_postproc\nfrom third_party.nucleus.io.python import sam_reader\nfrom third_party.nucleus.protos import reads_pb2\nfrom third_party.nucleus.protos import reference_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\n\n\nclass SamReaderTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.bam = test_utils.genomics_core_testdata(\'test.bam\')\n    self.options = reads_pb2.SamReaderOptions()\n\n  def test_bam_iterate(self):\n    reader = sam_reader.SamReader.from_file(\n        reads_path=self.bam, ref_path=\'\', options=self.options)\n    with reader:\n      iterable = reader.iterate()\n      self.assertIsInstance(iterable, clif_postproc.WrappedCppIterable)\n      self.assertEqual(test_utils.iterable_len(iterable), 106)\n\n  def test_bam_query(self):\n    reader = sam_reader.SamReader.from_file(\n        reads_path=self.bam, ref_path=\'\', options=self.options)\n    expected = [(ranges.parse_literal(\'chr20:10,000,000-10,000,100\'), 106),\n                (ranges.parse_literal(\'chr20:10,000,000-10,000,000\'), 45)]\n    with reader:\n      for interval, n_expected in expected:\n        with reader.query(interval) as iterable:\n          self.assertIsInstance(iterable, clif_postproc.WrappedCppIterable)\n          self.assertEqual(test_utils.iterable_len(iterable), n_expected)\n\n  def test_bam_samples(self):\n    reader = sam_reader.SamReader.from_file(\n        reads_path=self.bam, ref_path=\'\', options=self.options)\n    with reader:\n      self.assertLen(reader.header.read_groups, 1)\n      self.assertEqual(reader.header.read_groups[0].sample_id, \'NA12878\')\n\n  def test_sam_contigs(self):\n    reader = sam_reader.SamReader.from_file(\n        reads_path=self.bam, ref_path=\'\', options=self.options)\n    with reader:\n      self.assertEqual([\n          reference_pb2.ContigInfo(name=\'chrM\', pos_in_fasta=0, n_bases=16571),\n          reference_pb2.ContigInfo(\n              name=\'chr1\', pos_in_fasta=1, n_bases=249250621),\n          reference_pb2.ContigInfo(\n              name=\'chr2\', pos_in_fasta=2, n_bases=243199373),\n          reference_pb2.ContigInfo(\n              name=\'chr3\', pos_in_fasta=3, n_bases=198022430),\n          reference_pb2.ContigInfo(\n              name=\'chr4\', pos_in_fasta=4, n_bases=191154276),\n          reference_pb2.ContigInfo(\n              name=\'chr5\', pos_in_fasta=5, n_bases=180915260),\n          reference_pb2.ContigInfo(\n              name=\'chr6\', pos_in_fasta=6, n_bases=171115067),\n          reference_pb2.ContigInfo(\n              name=\'chr7\', pos_in_fasta=7, n_bases=159138663),\n          reference_pb2.ContigInfo(\n              name=\'chr8\', pos_in_fasta=8, n_bases=146364022),\n          reference_pb2.ContigInfo(\n              name=\'chr9\', pos_in_fasta=9, n_bases=141213431),\n          reference_pb2.ContigInfo(\n              name=\'chr10\', pos_in_fasta=10, n_bases=135534747),\n          reference_pb2.ContigInfo(\n              name=\'chr11\', pos_in_fasta=11, n_bases=135006516),\n          reference_pb2.ContigInfo(\n              name=\'chr12\', pos_in_fasta=12, n_bases=133851895),\n          reference_pb2.ContigInfo(\n              name=\'chr13\', pos_in_fasta=13, n_bases=115169878),\n          reference_pb2.ContigInfo(\n              name=\'chr14\', pos_in_fasta=14, n_bases=107349540),\n          reference_pb2.ContigInfo(\n              name=\'chr15\', pos_in_fasta=15, n_bases=102531392),\n          reference_pb2.ContigInfo(\n              name=\'chr16\', pos_in_fasta=16, n_bases=90354753),\n          reference_pb2.ContigInfo(\n              name=\'chr17\', pos_in_fasta=17, n_bases=81195210),\n          reference_pb2.ContigInfo(\n              name=\'chr18\', pos_in_fasta=18, n_bases=78077248),\n          reference_pb2.ContigInfo(\n              name=\'chr19\', pos_in_fasta=19, n_bases=59128983),\n          reference_pb2.ContigInfo(\n              name=\'chr20\', pos_in_fasta=20, n_bases=63025520),\n          reference_pb2.ContigInfo(\n              name=\'chr21\', pos_in_fasta=21, n_bases=48129895),\n          reference_pb2.ContigInfo(\n              name=\'chr22\', pos_in_fasta=22, n_bases=51304566),\n          reference_pb2.ContigInfo(\n              name=\'chrX\', pos_in_fasta=23, n_bases=155270560),\n          reference_pb2.ContigInfo(\n              name=\'chrY\', pos_in_fasta=24, n_bases=59373566),\n      ], list(reader.header.contigs))\n\n  def test_context_manager(self):\n    """"""Test that we can use context manager to do two queries in sequence.""""""\n    reader = sam_reader.SamReader.from_file(\n        reads_path=self.bam, ref_path=\'\', options=self.options)\n    region = ranges.parse_literal(\'chr20:10,000,000-10,000,100\')\n    with reader:\n      with reader.query(region) as query_iterable1:\n        self.assertIsNotNone(query_iterable1)\n        self.assertIsInstance(query_iterable1, clif_postproc.WrappedCppIterable)\n      with reader.query(region) as query_iterable2:\n        self.assertIsNotNone(query_iterable2)\n        self.assertIsInstance(query_iterable2, clif_postproc.WrappedCppIterable)\n\n  def test_from_file_raises_with_missing_bam(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Not found: Could not open missing.bam\'):\n      sam_reader.SamReader.from_file(\n          reads_path=\'missing.bam\', ref_path=\'\', options=self.options)\n\n  def test_ops_on_closed_reader_raise(self):\n    reader = sam_reader.SamReader.from_file(\n        reads_path=self.bam, ref_path=\'\', options=self.options)\n    with reader:\n      pass\n    # At this point the reader is closed.\n    with self.assertRaisesRegexp(ValueError, \'Cannot Iterate a closed\'):\n      reader.iterate()\n    with self.assertRaisesRegexp(ValueError, \'Cannot Query a closed\'):\n      reader.query(ranges.parse_literal(\'chr20:10,000,000-10,000,100\'))\n\n  @parameterized.parameters(\'test.sam\', \'unindexed.bam\')\n  def test_query_without_index_raises(self, unindexed_file_name):\n    path = test_utils.genomics_core_testdata(unindexed_file_name)\n    window = ranges.parse_literal(\'chr20:10,000,000-10,000,100\')\n    with sam_reader.SamReader.from_file(\n        reads_path=path, ref_path=\'\', options=self.options) as reader:\n      with self.assertRaisesRegexp(ValueError, \'Cannot query without an index\'):\n        reader.query(window)\n\n  def test_query_raises_with_bad_range(self):\n    with sam_reader.SamReader.from_file(\n        reads_path=self.bam, ref_path=\'\', options=self.options) as reader:\n      with self.assertRaisesRegexp(ValueError, \'Unknown reference_name\'):\n        reader.query(ranges.parse_literal(\'XXX:1-10\'))\n      with self.assertRaisesRegexp(ValueError, \'unknown reference interval\'):\n        reader.query(ranges.parse_literal(\'chr20:10-5\'))\n\n  def test_sam_iterate_raises_on_malformed_record(self):\n    malformed = test_utils.genomics_core_testdata(\'malformed.sam\')\n    reader = sam_reader.SamReader.from_file(\n        reads_path=malformed, ref_path=\'\', options=self.options)\n    iterable = iter(reader.iterate())\n    self.assertIsNotNone(next(iterable))\n    with self.assertRaises(ValueError):\n      list(iterable)\n\n  def test_headless_sam_raises(self):\n    headerless = test_utils.genomics_core_testdata(\'headerless.sam\')\n    reader = sam_reader.SamReader.from_file(\n        reads_path=headerless, ref_path=\'\', options=self.options)\n    iterable = iter(reader.iterate())\n    with self.assertRaises(ValueError):\n      next(iterable)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/python/vcf_reader_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for vcf_reader CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\n\nfrom third_party.nucleus.io.python import vcf_reader\nfrom third_party.nucleus.protos import reference_pb2\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.testing import test_utils\nfrom third_party.nucleus.util import ranges\n\nexpected_sites_contigs = [\n    reference_pb2.ContigInfo(name=\'chr1\', pos_in_fasta=0, n_bases=248956422),\n    reference_pb2.ContigInfo(name=\'chr2\', pos_in_fasta=1, n_bases=242193529),\n    reference_pb2.ContigInfo(name=\'chr3\', pos_in_fasta=2, n_bases=198295559),\n    reference_pb2.ContigInfo(name=\'chr4\', pos_in_fasta=3, n_bases=190214555),\n    reference_pb2.ContigInfo(name=\'chr5\', pos_in_fasta=4, n_bases=181538259),\n    reference_pb2.ContigInfo(name=\'chr6\', pos_in_fasta=5, n_bases=170805979),\n    reference_pb2.ContigInfo(name=\'chr7\', pos_in_fasta=6, n_bases=159345973),\n    reference_pb2.ContigInfo(name=\'chr8\', pos_in_fasta=7, n_bases=145138636),\n    reference_pb2.ContigInfo(name=\'chr9\', pos_in_fasta=8, n_bases=138394717),\n    reference_pb2.ContigInfo(name=\'chr10\', pos_in_fasta=9, n_bases=133797422),\n    reference_pb2.ContigInfo(name=\'chr11\', pos_in_fasta=10, n_bases=135086622),\n    reference_pb2.ContigInfo(name=\'chr12\', pos_in_fasta=11, n_bases=133275309),\n    reference_pb2.ContigInfo(name=\'chr13\', pos_in_fasta=12, n_bases=114364328),\n    reference_pb2.ContigInfo(name=\'chr14\', pos_in_fasta=13, n_bases=107043718),\n    reference_pb2.ContigInfo(name=\'chr15\', pos_in_fasta=14, n_bases=101991189),\n    reference_pb2.ContigInfo(name=\'chr16\', pos_in_fasta=15, n_bases=90338345),\n    reference_pb2.ContigInfo(name=\'chr17\', pos_in_fasta=16, n_bases=83257441),\n    reference_pb2.ContigInfo(name=\'chr18\', pos_in_fasta=17, n_bases=80373285),\n    reference_pb2.ContigInfo(name=\'chr19\', pos_in_fasta=18, n_bases=58617616),\n    reference_pb2.ContigInfo(name=\'chr20\', pos_in_fasta=19, n_bases=64444167),\n    reference_pb2.ContigInfo(name=\'chr21\', pos_in_fasta=20, n_bases=46709983),\n    reference_pb2.ContigInfo(name=\'chr22\', pos_in_fasta=21, n_bases=50818468),\n    reference_pb2.ContigInfo(name=\'chrX\', pos_in_fasta=22, n_bases=156040895),\n    reference_pb2.ContigInfo(name=\'chrY\', pos_in_fasta=23, n_bases=57227415),\n    reference_pb2.ContigInfo(name=\'chrM\', pos_in_fasta=24, n_bases=16569),\n]\n\n# pylint: disable=line-too-long\nexpected_samples_filters = [\n    variants_pb2.VcfFilterInfo(id=\'PASS\', description=\'All filters passed\'),\n    variants_pb2.VcfFilterInfo(id=\'LowQual\', description=\'Low\tquality\'),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL95.00to96.00\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod:\t0.9364\t<=\tx\t<\t1.0415\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL96.00to97.00\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod:\t0.8135\t<=\tx\t<\t0.9364\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL97.00to99.00\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod:\t0.323\t<=\tx\t<\t0.8135\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL99.00to99.50\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod:\t-0.1071\t<=\tx\t<\t0.323\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL99.50to99.90\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod:\t-1.845\t<=\tx\t<\t-0.1071\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL99.90to99.95\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod:\t-3.2441\t<=\tx\t<\t-1.845\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL99.95to100.00+\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod\t<\t-57172.0693\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL99.95to100.00\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod:\t-57172.0693\t<=\tx\t<\t-3.2441\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheSNP99.50to99.60\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tSNP\tmodel\tat\tVQS\tLod:\t-0.751\t<=\tx\t<\t-0.6681\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheSNP99.60to99.80\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tSNP\tmodel\tat\tVQS\tLod:\t-1.0839\t<=\tx\t<\t-0.751\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheSNP99.80to99.90\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tSNP\tmodel\tat\tVQS\tLod:\t-1.7082\t<=\tx\t<\t-1.0839\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheSNP99.90to99.95\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tSNP\tmodel\tat\tVQS\tLod:\t-3.0342\t<=\tx\t<\t-1.7082\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheSNP99.95to100.00+\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tSNP\tmodel\tat\tVQS\tLod\t<\t-40235.9641\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheSNP99.95to100.00\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tSNP\tmodel\tat\tVQS\tLod:\t-40235.9641\t<=\tx\t<\t-3.0342\'\n    )\n]\n\n# pylint: enable=line-too-long\n\n\nclass WrapVcfReaderTests(absltest.TestCase):\n\n  def setUp(self):\n    self.sites_vcf = test_utils.genomics_core_testdata(\'test_sites.vcf\')\n    self.samples_vcf = test_utils.genomics_core_testdata(\'test_samples.vcf.gz\')\n    self.options = variants_pb2.VcfReaderOptions()\n    self.sites_reader = vcf_reader.VcfReader.from_file(self.sites_vcf,\n                                                       self.options)\n    self.samples_reader = vcf_reader.VcfReader.from_file(\n        self.samples_vcf, self.options)\n\n  def test_vcf_iterate(self):\n    iterable = self.sites_reader.iterate()\n    self.assertEqual(test_utils.iterable_len(iterable), 5)\n\n  def test_vcf_header(self):\n    header = self.sites_reader.header\n    expected1 = variants_pb2.VcfStructuredExtra(\n        key=\'ALT\',\n        fields=[\n            variants_pb2.VcfExtra(key=\'ID\', value=\'NON_REF\'),\n            variants_pb2.VcfExtra(\n                key=\'Description\',\n                value=\'Represents\tany\tpossible\talternative\tallele\tat\tth\'\n                \'is\tlocation\')\n        ])\n    expected2 = variants_pb2.VcfStructuredExtra(\n        key=\'META\',\n        fields=[\n            variants_pb2.VcfExtra(key=\'ID\', value=\'TESTMETA\'),\n            variants_pb2.VcfExtra(key=\'Description\', value=\'blah\')\n        ])\n    self.assertLen(header.structured_extras, 2)\n    self.assertEqual(header.structured_extras[1], expected2)\n    self.assertEqual(header.structured_extras[0], expected1)\n\n  def test_vcf_contigs(self):\n    self.assertEqual(expected_sites_contigs,\n                     list(self.sites_reader.header.contigs))\n\n  def test_vcf_filters(self):\n    self.assertEqual(expected_samples_filters,\n                     list(self.samples_reader.header.filters))\n\n  def test_vcf_samples(self):\n    self.assertEqual(list(self.sites_reader.header.sample_names), [])\n    self.assertEqual(\n        list(self.samples_reader.header.sample_names), [\'NA12878_18_99\'])\n\n  def test_vcf_query(self):\n    range1 = ranges.parse_literal(\'chr3:100,000-500,000\')\n    iterable = self.samples_reader.query(range1)\n    self.assertEqual(test_utils.iterable_len(iterable), 4)\n\n  def test_vcf_from_string(self):\n    v = self.samples_reader.from_string(\n        \'chr3\\t370537\\trs142286746\\tC\\tCA,CAA\\t350.73\\tPASS\\t\'\n        \'AC=1,1;AF=0.500,0.500;AN=2;DB;DP=16;ExcessHet=3.0103;\'\n        \'FS=0.000;MLEAC=1,1;MLEAF=0.500,0.500;MQ=60.00;QD=26.98;\'\n        \'SOR=1.179;VQSLOD=2.88;culprit=FS\\tGT:AD:DP:GQ:PL\\t\'\n        \'1/2:0,6,7:13:99:388,188,149,140,0,116\')\n    self.assertEqual(v.reference_name, \'chr3\')\n    self.assertEqual(v.start, 370536)\n    self.assertEqual(list(v.names), [\'rs142286746\'])\n    self.assertEqual(v.reference_bases, \'C\')\n    self.assertEqual(list(v.alternate_bases), [\'CA\', \'CAA\'])\n    self.assertEqual(len(v.calls), 1)\n\n  def test_vcf_from_string_raises_on_bad_input(self):\n    with self.assertRaises(ValueError):\n      self.samples_reader.from_string(\'BAD NOT A VCF RECORD\\n;;\')\n\n  def test_from_file_raises_with_missing_source(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Not found: Could not open missing.vcf\'):\n      vcf_reader.VcfReader.from_file(\'missing.vcf\', self.options)\n\n  def test_ops_on_closed_reader_raise(self):\n    with self.samples_reader:\n      pass\n    # At this point the reader is closed.\n    with self.assertRaisesRegexp(ValueError, \'Cannot Iterate a closed\'):\n      self.samples_reader.iterate()\n    with self.assertRaisesRegexp(ValueError, \'Cannot Query a closed\'):\n      self.samples_reader.query(\n          ranges.parse_literal(\'chr1:10,000,000-10,000,100\'))\n\n  def test_query_on_unindexed_reader_raises(self):\n    window = ranges.parse_literal(\'chr1:10,000,000-10,000,100\')\n    unindexed_file = test_utils.genomics_core_testdata(\'test_samples.vcf\')\n    with vcf_reader.VcfReader.from_file(unindexed_file, self.options) as reader:\n      with self.assertRaisesRegexp(ValueError, \'Cannot query without an index\'):\n        reader.query(window)\n\n  def test_query_raises_with_bad_range(self):\n    with self.assertRaisesRegexp(ValueError, \'Unknown reference_name\'):\n      self.samples_reader.query(ranges.parse_literal(\'XXX:1-10\'))\n    with self.assertRaisesRegexp(ValueError, \'Malformed region\'):\n      self.samples_reader.query(ranges.parse_literal(\'chr1:0-5\'))\n    with self.assertRaisesRegexp(ValueError, \'Malformed region\'):\n      self.samples_reader.query(ranges.parse_literal(\'chr1:6-5\'))\n    with self.assertRaisesRegexp(ValueError, \'Malformed region\'):\n      self.samples_reader.query(ranges.parse_literal(\'chr1:10-5\'))\n\n  def test_context_manager(self):\n    with vcf_reader.VcfReader.from_file(self.sites_vcf, self.options) as f:\n      self.assertEqual(expected_sites_contigs, list(f.header.contigs))\n\n  # Commented out because we in fact don\'t detect the malformed VCF yet. It is\n  # unclear if it\'s even possible to detect the issue with the API provided by\n  # htslib.\n  # def test_vcf_iterate_raises_on_malformed_record(self):\n  #   malformed = test_utils.genomics_core_testdata(\'malformed.vcf\')\n  #   reader = vcf_reader.VcfReader.from_file(malformed, self.unindexed_options)\n  #   iterable = iter(reader.iterate())\n  #   self.assertIsNotNone(next(iterable))\n  #   with self.assertRaises(ValueError):\n  #     print(list(iterable))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/io/python/vcf_writer_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for VcfWriter CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nimport copy\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom tensorflow.python.platform import gfile\nfrom third_party.nucleus.io import tfrecord\nfrom third_party.nucleus.io import vcf\nfrom third_party.nucleus.io.python import vcf_writer\nfrom third_party.nucleus.protos import reference_pb2\nfrom third_party.nucleus.protos import variants_pb2\nfrom third_party.nucleus.testing import test_utils\n\n_DOUBLE_CLOSE_ERROR = \'Cannot close an already closed VcfWriter\'\n_WRITE_TO_CLOSED_ERROR = \'Cannot write to closed VCF stream\'\n_WRONG_NUMBER_OF_SAMPLES = (\n    \'Variant call count \\d+ must match number of samples \\d+\')\n_DISCORDANT_SAMPLE_NAMES_ERROR = (\n    \'Out-of-order call set names, or unrecognized call set name, with respect \'\n    \'to samples declared in VCF header.\')\n_UNKNOWN_CONTIG_ERROR = ""Record\'s reference name is not available in VCF header""\n_FILTER_NOT_FOUND_ERROR = \'Filter must be found in header\'\n\n\nclass WrapVcfWriterTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.out_fname = test_utils.test_tmpfile(\'output.vcf\')\n    self.header = variants_pb2.VcfHeader(\n        contigs=[\n            reference_pb2.ContigInfo(name=\'Chr1\', n_bases=50, pos_in_fasta=0),\n            reference_pb2.ContigInfo(name=\'Chr2\', n_bases=25, pos_in_fasta=1),\n        ],\n        sample_names=[\'Fido\', \'Spot\'],\n        formats=[\n            variants_pb2.VcfFormatInfo(\n                id=\'GT\', number=\'1\', type=\'String\', description=\'Genotype\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'GQ\',\n                number=\'1\',\n                type=\'Float\',\n                description=\'Genotype Quality\')\n        ],\n    )\n    self.options = variants_pb2.VcfWriterOptions()\n    self.writer = vcf_writer.VcfWriter.to_file(self.out_fname, self.header,\n                                               self.options)\n    self.variant = test_utils.make_variant(\n        chrom=\'Chr1\',\n        start=10,\n        alleles=[\'A\', \'C\'],\n    )\n    self.variant.calls.extend([\n        variants_pb2.VariantCall(genotype=[0, 0], call_set_name=\'Fido\'),\n        variants_pb2.VariantCall(genotype=[0, 1], call_set_name=\'Spot\'),\n    ])\n\n  def test_writing_canned_variants(self):\n    """"""Tests writing all the variants that are \'canned\' in our tfrecord file.""""""\n    # This file is in the TF record format\n    tfrecord_file = test_utils.genomics_core_testdata(\n        \'test_samples.vcf.golden.tfrecord\')\n\n    writer_options = variants_pb2.VcfWriterOptions()\n    header = variants_pb2.VcfHeader(\n        contigs=[\n            reference_pb2.ContigInfo(name=\'chr1\', n_bases=248956422),\n            reference_pb2.ContigInfo(name=\'chr2\', n_bases=242193529),\n            reference_pb2.ContigInfo(name=\'chr3\', n_bases=198295559),\n            reference_pb2.ContigInfo(name=\'chrX\', n_bases=156040895)\n        ],\n        sample_names=[\'NA12878_18_99\'],\n        filters=[\n            variants_pb2.VcfFilterInfo(\n                id=\'PASS\', description=\'All filters passed\'),\n            variants_pb2.VcfFilterInfo(id=\'LowQual\', description=\'\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL95.00to96.00\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL96.00to97.00\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL97.00to99.00\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL99.00to99.50\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL99.50to99.90\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL99.90to99.95\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL99.95to100.00+\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL99.95to100.00\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheSNP99.50to99.60\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheSNP99.60to99.80\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheSNP99.80to99.90\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheSNP99.90to99.95\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheSNP99.95to100.00+\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheSNP99.95to100.00\'),\n        ],\n        infos=[\n            variants_pb2.VcfInfo(\n                id=\'END\',\n                number=\'1\',\n                type=\'Integer\',\n                description=\'Stop position of the interval\')\n        ],\n        formats=[\n            variants_pb2.VcfFormatInfo(\n                id=\'GT\', number=\'1\', type=\'String\', description=\'Genotype\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'GQ\',\n                number=\'1\',\n                type=\'Integer\',\n                description=\'Genotype Quality\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'DP\',\n                number=\'1\',\n                type=\'Integer\',\n                description=\'Read depth of all passing filters reads.\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'MIN_DP\',\n                number=\'1\',\n                type=\'Integer\',\n                description=\'Minimum DP observed within the GVCF block.\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'AD\',\n                number=\'R\',\n                type=\'Integer\',\n                description=\n                \'Read depth of all passing filters reads for each allele.\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'VAF\',\n                number=\'A\',\n                type=\'Float\',\n                description=\'Variant allele fractions.\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'PL\',\n                number=\'G\',\n                type=\'Integer\',\n                description=\'Genotype likelihoods, Phred encoded\'),\n        ],\n    )\n    variant_records = list(\n        tfrecord.read_tfrecords(tfrecord_file, proto=variants_pb2.Variant))\n    out_fname = test_utils.test_tmpfile(\'output.vcf\')\n    with vcf_writer.VcfWriter.to_file(out_fname, header,\n                                      writer_options) as writer:\n      for record in variant_records[:5]:\n        writer.write(record)\n\n    # Check: are the variants written as expected?\n    # pylint: disable=line-too-long\n    expected_vcf_content = [\n        \'##fileformat=VCFv4.2\\n\',\n        \'##FILTER=<ID=PASS,Description=""All filters passed"">\\n\',\n        \'##FILTER=<ID=LowQual,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL95.00to96.00,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL96.00to97.00,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL97.00to99.00,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL99.00to99.50,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL99.50to99.90,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL99.90to99.95,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL99.95to100.00+,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL99.95to100.00,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheSNP99.50to99.60,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheSNP99.60to99.80,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheSNP99.80to99.90,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheSNP99.90to99.95,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheSNP99.95to100.00+,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheSNP99.95to100.00,Description="""">\\n\',\n        \'##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of \'\n        \'the interval"">\\n\',\n        \'##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">\\n\',\n        \'##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">\\n\',\n        \'##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth of all \'\n        \'passing filters reads."">\\n\',\n        \'##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP \'\n        \'observed within the GVCF block."">\\n\',\n        \'##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth of all \'\n        \'passing filters reads for each allele."">\\n\',\n        \'##FORMAT=<ID=VAF,Number=A,Type=Float,Description=\\""Variant allele \'\n        \'fractions."">\\n\',\n        \'##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Genotype \'\n        \'likelihoods, Phred encoded"">\\n\',\n        \'##contig=<ID=chr1,length=248956422>\\n\',\n        \'##contig=<ID=chr2,length=242193529>\\n\',\n        \'##contig=<ID=chr3,length=198295559>\\n\',\n        \'##contig=<ID=chrX,length=156040895>\\n\',\n        \'#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\tFORMAT\\tNA12878_18_99\\n\',\n        \'chr1\\t13613\\t.\\tT\\tA\\t39.88\\tVQSRTrancheSNP99.90to99.95\\t.\\tGT:GQ:DP:AD:PL\\t0/1:16:4:1,3:68,0,16\\n\',\n        \'chr1\\t13813\\t.\\tT\\tG\\t90.28\\tPASS\\t.\\tGT:GQ:DP:AD:PL\\t1/1:9:3:0,3:118,9,0\\n\',\n        \'chr1\\t13838\\trs28428499\\tC\\tT\\t62.74\\tPASS\\t.\\tGT:GQ:DP:AD:PL\\t1/1:6:2:0,2:90,6,0\\n\',\n        \'chr1\\t14397\\trs756427959\\tCTGT\\tC\\t37.73\\tPASS\\t.\\tGT:GQ:DP:AD:PL\\t0/1:75:5:3,2:75,0,152\\n\',\n        \'chr1\\t14522\\t.\\tG\\tA\\t49.77\\tVQSRTrancheSNP99.60to99.80\\t.\\tGT:GQ:DP:AD:PL\\t0/1:78:10:6,4:78,0,118\\n\'\n    ]\n    # pylint: enable=line-too-long\n\n    with gfile.Open(out_fname, \'r\') as f:\n      self.assertEqual(f.readlines(), expected_vcf_content)\n\n  def test_write_variant_is_ok(self):\n    self.assertIsNone(self.writer.write(self.variant))\n\n  def test_write_raises_with_unknown_contig(self):\n    with self.assertRaisesRegexp(ValueError, _UNKNOWN_CONTIG_ERROR):\n      self.variant.reference_name = \'BadChrom\'\n      self.writer.write(self.variant)\n\n  def test_write_raises_with_unknown_filter(self):\n    with self.assertRaisesRegexp(ValueError, _FILTER_NOT_FOUND_ERROR):\n      self.variant.filter[:] = [\'BadFilter\']\n      self.writer.write(self.variant)\n\n  @parameterized.parameters(\n      ([], _WRONG_NUMBER_OF_SAMPLES),\n      ([\'Spot\'], _WRONG_NUMBER_OF_SAMPLES),\n      ([\'Fido\'], _WRONG_NUMBER_OF_SAMPLES),\n      ([\'Unknown\', \'Fido\'], _DISCORDANT_SAMPLE_NAMES_ERROR),\n      ([\'Spot\', \'Unknown\'], _DISCORDANT_SAMPLE_NAMES_ERROR),\n      ([\'Spot\', \'Fido\'], _DISCORDANT_SAMPLE_NAMES_ERROR),  # Out of order.\n      ([\'Fido\', \'Spot\', \'Extra\'], _WRONG_NUMBER_OF_SAMPLES),\n  )\n  def test_write_raises_with_unknown_sample(self, sample_names, message):\n    with self.assertRaisesRegexp(ValueError, message):\n      del self.variant.calls[:]\n      for sample_name in sample_names:\n        self.variant.calls.add(genotype=[0, 0], call_set_name=sample_name)\n      self.writer.write(self.variant)\n\n  def test_context_manager(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.variant))\n\n    # self.writer should be closed, so writing again will fail.\n    with self.assertRaisesRegexp(ValueError, _WRITE_TO_CLOSED_ERROR):\n      self.writer.write(self.variant)\n\n  def test_double_context_manager(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.variant))\n\n    with self.assertRaisesRegexp(ValueError, _DOUBLE_CLOSE_ERROR):\n      # Entering the closed writer should be fine.\n      with self.writer:\n        pass  # We want to raise an error on exit, so nothing to do in context.\n\n\nclass WrapVcfWriterRoundTripTests(parameterized.TestCase):\n\n  @parameterized.parameters((\'test_samples.vcf\',), (\'test_samples.vcf.gz\',),\n                            (\'test_sites.vcf\',))\n  def test_round_trip_vcf(self, test_datum_name):\n    # Round-trip variants through writing and reading:\n    # 1. Read variants v1 from VcfReader;\n    # 2. Write v1 to vcf using our VcfWriter;\n    # 3. Read back in using VcfReader -- v2;\n    # 4. compare v1 and v2.\n    in_file = test_utils.genomics_core_testdata(test_datum_name)\n    out_file = test_utils.test_tmpfile(\'output_\' + test_datum_name)\n\n    v1_reader = vcf.VcfReader(in_file)\n    v1_records = list(v1_reader.iterate())\n    self.assertTrue(v1_records, \'Reader failed to find records\')\n\n    header = copy.deepcopy(v1_reader.header)\n    writer_options = variants_pb2.VcfWriterOptions()\n\n    with vcf_writer.VcfWriter.to_file(out_file, header,\n                                      writer_options) as writer:\n      for record in v1_records:\n        writer.write(record)\n\n    v2_reader = vcf.VcfReader(out_file)\n    v2_records = list(v2_reader.iterate())\n\n    self.assertEqual(v1_records, v2_records,\n                     \'Round-tripped variants not as expected\')\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/util/python/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n'"
third_party/nucleus/util/python/math_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n""""""Tests for Math CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\n\nfrom third_party.nucleus.util.python import math\n\n\nclass MathWrapTest(absltest.TestCase):\n\n  def test_one_minus_log10_prob_to_phred(self):\n    self.assertAlmostEqual(16.4277471723837,\n                           math.log10_ptrue_to_phred(-0.01, 1000))\n\n  def test_phred_to_prob(self):\n    self.assertEqual(0.1, math.phred_to_perror(10))\n\n  def test_phred_to_log10_prob(self):\n    self.assertEqual(-1, math.phred_to_log10_perror(10))\n\n  def test_prob_to_phred(self):\n    self.assertEqual(10.0, math.perror_to_phred(0.1))\n\n  def test_prob_to_rounded_phred(self):\n    self.assertEqual(10, math.perror_to_rounded_phred(0.1))\n\n  def test_prob_to_log10_prob(self):\n    self.assertEqual(-1, math.perror_to_log10_perror(0.1))\n\n  def test_log10_prob_to_phred(self):\n    self.assertEqual(10, math.log10_perror_to_phred(-1))\n\n  def test_log10_prob_to_rounded_phred(self):\n    self.assertEqual(10, math.log10_perror_to_rounded_phred(-1))\n\n  def test_log10_prob_to_prob(self):\n    self.assertEqual(0.1, math.log10_perror_to_perror(-1))\n\n  def test_zero_shift_log10_probs(self):\n    self.assertSequenceEqual([0, -1, -2],\n                             math.zero_shift_log10_probs([-1, -2, -3]))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
third_party/nucleus/vendor/python/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n'"
third_party/nucleus/vendor/python/statusor_examples_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice,\n#    this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from this\n#    software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nif \'google\' in sys.modules and \'google.protobuf\' not in sys.modules:\n  del sys.modules[\'google\']\n\n\nfrom absl.testing import absltest\nfrom third_party.nucleus.vendor.python import statusor_examples\n\n\nclass StatusorClifWrapTest(absltest.TestCase):\n\n  def test_make_int_ok(self):\n    self.assertEqual(statusor_examples.MakeIntOK(), 42)\n\n  def test_make_int_fail(self):\n    with self.assertRaisesRegexp(ValueError, \'Invalid argument: MakeIntFail\'):\n      statusor_examples.MakeIntFail()\n\n  def test_make_str_ok(self):\n    self.assertEqual(statusor_examples.MakeStrOK(), \'hello\')\n\n  # See CLIF wrapper for a discussion of why this is commented out.\n  # def test_make_str_ok_stripped_type(self):\n  #   self.assertEqual(statusor_examples.MakeStrOKStrippedType(), \'hello\')\n\n  def test_make_str_fail(self):\n    with self.assertRaisesRegexp(ValueError, \'Invalid argument: MakeStrFail\'):\n      statusor_examples.MakeStrFail()\n\n  def test_make_int_unique_ptr_ok(self):\n    self.assertEqual(statusor_examples.MakeIntUniquePtrOK(), 421)\n\n  def test_make_int_unique_ptr_fail(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Invalid argument: MakeIntUniquePtrFail\'):\n      statusor_examples.MakeIntUniquePtrFail()\n\n  def test_make_int_vector_ok(self):\n    self.assertEqual(statusor_examples.MakeIntVectorOK(), [1, 2, 42])\n\n  def test_make_int_vector_fail(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Invalid argument: MakeIntVectorFail\'):\n      statusor_examples.MakeIntVectorFail()\n\n  def test_returning_status_ok_returns_none(self):\n    self.assertEqual(statusor_examples.FuncReturningStatusOK(), None)\n\n  def test_returning_status_fail_raises(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Invalid argument: FuncReturningStatusFail\'):\n      statusor_examples.FuncReturningStatusFail()\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
