file_path,api_count,code
setup.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Setup script for Dopamine.\n\nThis script will install Dopamine as a Python module.\n\nSee: https://github.com/google/dopamine\n\n""""""\n\nfrom os import path\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nhere = path.abspath(path.dirname(__file__))\n\ninstall_requires = [\'gin-config >= 0.1.1\', \'absl-py >= 0.2.2\',\n                    \'opencv-python >= 3.4.1.15\',\n                    \'gym >= 0.10.5\', \'Pillow >= 5.4.1\']\ntests_require = [\'gin-config >= 0.1.1\', \'absl-py >= 0.2.2\',\n                 \'opencv-python >= 3.4.1.15\',\n                 \'gym >= 0.10.5\', \'mock >= 1.0.0\', \'Pillow >= 5.4.1\']\n\ndopamine_description = (\n    \'Dopamine: A framework for flexible Reinforcement Learning research\')\n\nsetup(\n    name=\'dopamine_rl\',\n    version=\'3.0.1\',\n    include_package_data=True,\n    packages=find_packages(exclude=[\'docs\']),  # Required\n    package_data={\'testdata\': [\'testdata/*.gin\']},\n    install_requires=install_requires,\n    tests_require=tests_require,\n    description=dopamine_description,\n    long_description=dopamine_description,\n    url=\'https://github.com/google/dopamine\',  # Optional\n    author=\'The Dopamine Team\',  # Optional\n    classifiers=[  # Optional\n        \'Development Status :: 4 - Beta\',\n\n        # Indicate who your project is intended for\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n\n        # Pick your license as you wish\n        \'License :: OSI Approved :: Apache Software License\',\n\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Mathematics\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n\n    ],\n    project_urls={  # Optional\n        \'Documentation\': \'https://github.com/google/dopamine\',\n        \'Bug Reports\': \'https://github.com/google/dopamine/issues\',\n        \'Source\': \'https://github.com/google/dopamine\',\n    },\n    license=\'Apache 2.0\',\n    keywords=\'dopamine reinforcement-learning python machine learning\'\n)\n'"
dopamine/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nname = \'dopamine\'\n'"
dopamine/agents/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
dopamine/colab/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
dopamine/colab/utils.py,4,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""This provides utilities for dealing with Dopamine data.\n\nSee: dopamine/common/logger.py .\n""""""\n\nimport itertools\nimport os\nimport pickle\nimport sys\n\n\n\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow.compat.v1 as tf\n\nFILE_PREFIX = \'log\'\nITERATION_PREFIX = \'iteration_\'\n\nALL_GAMES = [\'AirRaid\', \'Alien\', \'Amidar\', \'Assault\', \'Asterix\', \'Asteroids\',\n             \'Atlantis\', \'BankHeist\', \'BattleZone\', \'BeamRider\', \'Berzerk\',\n             \'Bowling\', \'Boxing\', \'Breakout\', \'Carnival\', \'Centipede\',\n             \'ChopperCommand\', \'CrazyClimber\', \'DemonAttack\', \'DoubleDunk\',\n             \'ElevatorAction\', \'Enduro\', \'FishingDerby\', \'Freeway\', \'Frostbite\',\n             \'Gopher\', \'Gravitar\', \'Hero\', \'IceHockey\', \'Jamesbond\',\n             \'JourneyEscape\', \'Kangaroo\', \'Krull\', \'KungFuMaster\',\n             \'MontezumaRevenge\', \'MsPacman\', \'NameThisGame\', \'Phoenix\',\n             \'Pitfall\', \'Pong\', \'Pooyan\', \'PrivateEye\', \'Qbert\', \'Riverraid\',\n             \'RoadRunner\', \'Robotank\', \'Seaquest\', \'Skiing\', \'Solaris\',\n             \'SpaceInvaders\', \'StarGunner\', \'Tennis\', \'TimePilot\', \'Tutankham\',\n             \'UpNDown\', \'Venture\', \'VideoPinball\', \'WizardOfWor\', \'YarsRevenge\',\n             \'Zaxxon\']\n\n\ndef load_baselines(base_dir, verbose=False):\n  """"""Reads in the baseline experimental data from a specified base directory.\n\n  Args:\n    base_dir: string, base directory where to read data from.\n    verbose: bool, whether to print warning messages.\n\n  Returns:\n    A dict containing pandas DataFrames for all available agents and games.\n  """"""\n  experimental_data = {}\n  for game in ALL_GAMES:\n    for agent in [\'dqn\', \'c51\', \'rainbow\', \'iqn\']:\n      game_data_file = os.path.join(base_dir, agent, \'{}.pkl\'.format(game))\n      if not tf.gfile.Exists(game_data_file):\n        if verbose:\n          # pylint: disable=superfluous-parens\n          print(\'Unable to load data for agent {} on game {}\'.format(agent,\n                                                                     game))\n          # pylint: enable=superfluous-parens\n        continue\n      with tf.gfile.Open(game_data_file, \'rb\') as f:\n        if sys.version_info.major >= 3:\n          # pylint: disable=unexpected-keyword-arg\n          single_agent_data = pickle.load(f, encoding=\'latin1\')\n          # pylint: enable=unexpected-keyword-arg\n        else:\n          single_agent_data = pickle.load(f)\n        single_agent_data[\'agent\'] = agent\n        # The dataframe rows are all read as \'objects\', which causes a\n        # ValueError when merging below. We cast the numerics to float64s to\n        # avoid this.\n        for field_name in single_agent_data.keys():\n          try:\n            single_agent_data[field_name] = (\n                single_agent_data[field_name].astype(np.float64))\n          except ValueError:\n            # This will catch any non-numerics that cannot be cast to float64.\n            continue\n        if game in experimental_data:\n          experimental_data[game] = experimental_data[game].merge(\n              single_agent_data, how=\'outer\')\n        else:\n          experimental_data[game] = single_agent_data\n  return experimental_data\n\n\ndef load_statistics(log_path, iteration_number=None, verbose=True):\n  """"""Reads in a statistics object from log_path.\n\n  Args:\n    log_path: string, provides the full path to the training/eval statistics.\n    iteration_number: The iteration number of the statistics object we want\n      to read. If set to None, load the latest version.\n    verbose: Whether to output information about the load procedure.\n\n  Returns:\n    data: The requested statistics object.\n    iteration: The corresponding iteration number.\n\n  Raises:\n    Exception: if data is not present.\n  """"""\n  # If no iteration is specified, we\'ll look for the most recent.\n  if iteration_number is None:\n    iteration_number = get_latest_iteration(log_path)\n\n  log_file = \'%s/%s_%d\' % (log_path, FILE_PREFIX, iteration_number)\n\n  if verbose:\n    # pylint: disable=superfluous-parens\n    print(\'Reading statistics from: {}\'.format(log_file))\n    # pylint: enable=superfluous-parens\n\n  with tf.gfile.Open(log_file, \'rb\') as f:\n    return pickle.load(f), iteration_number\n\n\ndef get_latest_file(path):\n  """"""Return the file named \'path_[0-9]*\' with the largest such number.\n\n  Args:\n    path: The base path (including directory and base name) to search.\n\n  Returns:\n    The latest file (in terms of given numbers).\n  """"""\n  try:\n    latest_iteration = get_latest_iteration(path)\n    return os.path.join(path, \'{}_{}\'.format(FILE_PREFIX, latest_iteration))\n  except ValueError:\n    return None\n\n\ndef get_latest_iteration(path):\n  """"""Return the largest iteration number corresponding to the given path.\n\n  Args:\n    path: The base path (including directory and base name) to search.\n\n  Returns:\n    The latest iteration number.\n\n  Raises:\n    ValueError: if there is not available log data at the given path.\n  """"""\n  glob = os.path.join(path, \'{}_[0-9]*\'.format(FILE_PREFIX))\n  log_files = tf.gfile.Glob(glob)\n\n  if not log_files:\n    raise ValueError(\'No log data found at {}\'.format(path))\n\n  def extract_iteration(x):\n    return int(x[x.rfind(\'_\') + 1:])\n\n  latest_iteration = max(extract_iteration(x) for x in log_files)\n  return latest_iteration\n\n\ndef summarize_data(data, summary_keys):\n  """"""Processes log data into a per-iteration summary.\n\n  Args:\n    data: Dictionary loaded by load_statistics describing the data. This\n      dictionary has keys iteration_0, iteration_1, ... describing per-iteration\n      data.\n    summary_keys: List of per-iteration data to be summarized.\n\n  Example:\n    data = load_statistics(...)\n    summarize_data(data, [\'train_episode_returns\',\n        \'eval_episode_returns\'])\n\n  Returns:\n    A dictionary mapping each key in returns_keys to a per-iteration summary.\n  """"""\n  summary = {}\n  latest_iteration_number = len(data.keys())\n  current_value = None\n\n  for key in summary_keys:\n    summary[key] = []\n    # Compute per-iteration average of the given key.\n    for i in range(latest_iteration_number):\n      iter_key = \'{}{}\'.format(ITERATION_PREFIX, i)\n      # We allow reporting the same value multiple times when data is missing.\n      # If there is no data for this iteration, use the previous\'.\n      if iter_key in data:\n        current_value = np.mean(data[iter_key][key])\n      summary[key].append(current_value)\n\n  return summary\n\n\ndef read_experiment(log_path,\n                    parameter_set=None,\n                    job_descriptor=\'\',\n                    iteration_number=None,\n                    summary_keys=(\'train_episode_returns\',\n                                  \'eval_episode_returns\'),\n                    verbose=False):\n  """"""Reads in a set of experimental results from log_path.\n\n  The provided parameter_set is an ordered_dict which\n    1) defines the parameters of this experiment,\n    2) defines the order in which they occur in the job descriptor.\n\n  The method reads all experiments of the form\n\n  ${log_path}/${job_descriptor}.format(params)/logs,\n\n  where params is constructed from the cross product of the elements in\n  the parameter_set.\n\n  For example:\n    parameter_set = collections.OrderedDict([\n        (\'game\', [\'Asterix\', \'Pong\']),\n        (\'epsilon\', [\'0\', \'0.1\'])\n    ])\n    read_experiment(\'/tmp/logs\', parameter_set, job_descriptor=\'{}_{}\')\n    Will try to read logs from:\n    - /tmp/logs/Asterix_0/logs\n    - /tmp/logs/Asterix_0.1/logs\n    - /tmp/logs/Pong_0/logs\n    - /tmp/logs/Pong_0.1/logs\n\n  Args:\n    log_path: string, base path specifying where results live.\n    parameter_set: An ordered_dict mapping parameter names to allowable values.\n    job_descriptor: A job descriptor string which is used to construct the full\n      path for each trial within an experiment.\n    iteration_number: Int, if not None determines the iteration number at which\n      we read in results.\n    summary_keys: Iterable of strings, iteration statistics to summarize.\n    verbose: If True, print out additional information.\n\n  Returns:\n    A Pandas dataframe containing experimental results.\n  """"""\n  keys = [] if parameter_set is None else list(parameter_set.keys())\n  # Extract parameter value lists, one per parameter.\n  ordered_values = [parameter_set[key] for key in keys]\n\n  column_names = keys + [\'iteration\'] + list(summary_keys)\n  num_parameter_settings = len([_ for _ in itertools.product(*ordered_values)])\n  expected_num_iterations = 200\n  expected_num_rows = num_parameter_settings * expected_num_iterations\n\n  # Create DataFrame with predicted number of rows.\n  data_frame = pd.DataFrame(index=np.arange(0, expected_num_rows),\n                            columns=column_names)\n  row_index = 0\n\n  # Now take their cross product. This generates tuples of the form\n  # (p1, p2, p3, ...) where p1, p2, p3 are parameter values for the first,\n  # second, etc. parameters as ordered in value_set.\n  for parameter_tuple in itertools.product(*ordered_values):\n    if job_descriptor is not None:\n      name = job_descriptor.format(*parameter_tuple)\n    else:\n      # Construct name for values.\n      name = \'-\'.join([keys[i] + \'_\' + str(parameter_tuple[i])\n                       for i in range(len(keys))])\n\n    experiment_path = \'{}/{}/logs\'.format(log_path, name)\n\n    raw_data, last_iteration = load_statistics(\n        experiment_path, iteration_number=iteration_number, verbose=verbose)\n\n    summary = summarize_data(raw_data, summary_keys)\n    for iteration in range(last_iteration + 1):\n      # The row contains all the parameters, the iteration, and finally the\n      # requested values.\n      row_data = (list(parameter_tuple) + [iteration] +\n                  [summary[key][iteration] for key in summary_keys])\n      data_frame.loc[row_index] = row_data\n\n      row_index += 1\n\n  # The dataframe rows are all read as \'objects\', which causes a\n  # ValueError when merging below. We cast the numerics to float64s to\n  # avoid this.\n  for field_name in data_frame.keys():\n    try:\n      data_frame[field_name] = data_frame[field_name].astype(np.float64)\n    except ValueError:\n      # This will catch any non-numerics that cannot be cast to float64.\n      continue\n\n  # Shed any unused rows.\n  return data_frame.drop(np.arange(row_index, expected_num_rows))\n'"
dopamine/discrete_domains/__init__.py,0,"b'# coding=utf-8\n""""""Copyright 2018 The Dopamine Authors.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n'"
dopamine/discrete_domains/atari_lib.py,80,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Atari-specific utilities including Atari-specific network architectures.\n\nThis includes a class implementing minimal Atari 2600 preprocessing, which\nis in charge of:\n  . Emitting a terminal signal when losing a life (optional).\n  . Frame skipping and color pooling.\n  . Resizing the image before it is provided to the agent.\n\n## Networks\nWe are subclassing keras.models.Model in our network definitions. Each network\nclass has two main functions: `.__init__` and `.call`. When we create our\nnetwork the `__init__` function is called and necessary layers are defined. Once\nwe create our network, we can create the output operations by doing `call`s to\nour network with different inputs. At each call, the same parameters will be\nused.\n\nMore information about keras.Model API can be found here:\nhttps://www.tensorflow.org/api_docs/python/tf/keras/models/Model\n\n## Network Types\nNetwork types are namedtuples that define the output signature of the networks\nused. Please use the appropriate signature as needed when defining new networks.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport math\n\n\nimport gin\nimport gym\nfrom gym.spaces.box import Box\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nimport cv2\nfrom tensorflow.contrib import layers as contrib_layers\nfrom tensorflow.contrib import slim as contrib_slim\n\n\nNATURE_DQN_OBSERVATION_SHAPE = (84, 84)  # Size of downscaled Atari 2600 frame.\nNATURE_DQN_DTYPE = tf.uint8  # DType of Atari 2600 observations.\nNATURE_DQN_STACK_SIZE = 4  # Number of frames in the state stack.\n\nDQNNetworkType = collections.namedtuple(\'dqn_network\', [\'q_values\'])\nRainbowNetworkType = collections.namedtuple(\n    \'c51_network\', [\'q_values\', \'logits\', \'probabilities\'])\nImplicitQuantileNetworkType = collections.namedtuple(\n    \'iqn_network\', [\'quantile_values\', \'quantiles\'])\n\n\n\n\n@gin.configurable\ndef create_atari_environment(game_name=None, sticky_actions=True):\n  """"""Wraps an Atari 2600 Gym environment with some basic preprocessing.\n\n  This preprocessing matches the guidelines proposed in Machado et al. (2017),\n  ""Revisiting the Arcade Learning Environment: Evaluation Protocols and Open\n  Problems for General Agents"".\n\n  The created environment is the Gym wrapper around the Arcade Learning\n  Environment.\n\n  The main choice available to the user is whether to use sticky actions or not.\n  Sticky actions, as prescribed by Machado et al., cause actions to persist\n  with some probability (0.25) when a new command is sent to the ALE. This\n  can be viewed as introducing a mild form of stochasticity in the environment.\n  We use them by default.\n\n  Args:\n    game_name: str, the name of the Atari 2600 domain.\n    sticky_actions: bool, whether to use sticky_actions as per Machado et al.\n\n  Returns:\n    An Atari 2600 environment with some standard preprocessing.\n  """"""\n  assert game_name is not None\n  game_version = \'v0\' if sticky_actions else \'v4\'\n  full_game_name = \'{}NoFrameskip-{}\'.format(game_name, game_version)\n  env = gym.make(full_game_name)\n  # Strip out the TimeLimit wrapper from Gym, which caps us at 100k frames. We\n  # handle this time limit internally instead, which lets us cap at 108k frames\n  # (30 minutes). The TimeLimit wrapper also plays poorly with saving and\n  # restoring states.\n  env = env.env\n  env = AtariPreprocessing(env)\n  return env\n\n\ndef nature_dqn_network(num_actions, network_type, state):\n  """"""The convolutional network used to compute the agent\'s Q-values.\n\n  Args:\n    num_actions: int, number of actions.\n    network_type: namedtuple, collection of expected values to return.\n    state: `tf.Tensor`, contains the agent\'s current state.\n\n  Returns:\n    net: _network_type object containing the tensors output by the network.\n  """"""\n  net = tf.cast(state, tf.float32)\n  net = tf.div(net, 255.)\n  net = contrib_slim.conv2d(net, 32, [8, 8], stride=4)\n  net = contrib_slim.conv2d(net, 64, [4, 4], stride=2)\n  net = contrib_slim.conv2d(net, 64, [3, 3], stride=1)\n  net = contrib_slim.flatten(net)\n  net = contrib_slim.fully_connected(net, 512)\n  q_values = contrib_slim.fully_connected(net, num_actions, activation_fn=None)\n  return network_type(q_values)\n\n\ndef rainbow_network(num_actions, num_atoms, support, network_type, state):\n  """"""The convolutional network used to compute agent\'s Q-value distributions.\n\n  Args:\n    num_actions: int, number of actions.\n    num_atoms: int, the number of buckets of the value function distribution.\n    support: tf.linspace, the support of the Q-value distribution.\n    network_type: namedtuple, collection of expected values to return.\n    state: `tf.Tensor`, contains the agent\'s current state.\n\n  Returns:\n    net: _network_type object containing the tensors output by the network.\n  """"""\n  weights_initializer = contrib_slim.variance_scaling_initializer(\n      factor=1.0 / np.sqrt(3.0), mode=\'FAN_IN\', uniform=True)\n\n  net = tf.cast(state, tf.float32)\n  net = tf.div(net, 255.)\n  net = contrib_slim.conv2d(\n      net, 32, [8, 8], stride=4, weights_initializer=weights_initializer)\n  net = contrib_slim.conv2d(\n      net, 64, [4, 4], stride=2, weights_initializer=weights_initializer)\n  net = contrib_slim.conv2d(\n      net, 64, [3, 3], stride=1, weights_initializer=weights_initializer)\n  net = contrib_slim.flatten(net)\n  net = contrib_slim.fully_connected(\n      net, 512, weights_initializer=weights_initializer)\n  net = contrib_slim.fully_connected(\n      net,\n      num_actions * num_atoms,\n      activation_fn=None,\n      weights_initializer=weights_initializer)\n\n  logits = tf.reshape(net, [-1, num_actions, num_atoms])\n  probabilities = contrib_layers.softmax(logits)\n  q_values = tf.reduce_sum(support * probabilities, axis=2)\n  return network_type(q_values, logits, probabilities)\n\n\ndef implicit_quantile_network(num_actions, quantile_embedding_dim,\n                              network_type, state, num_quantiles):\n  """"""The Implicit Quantile ConvNet.\n\n  Args:\n    num_actions: int, number of actions.\n    quantile_embedding_dim: int, embedding dimension for the quantile input.\n    network_type: namedtuple, collection of expected values to return.\n    state: `tf.Tensor`, contains the agent\'s current state.\n    num_quantiles: int, number of quantile inputs.\n\n  Returns:\n    net: _network_type object containing the tensors output by the network.\n  """"""\n  weights_initializer = contrib_slim.variance_scaling_initializer(\n      factor=1.0 / np.sqrt(3.0), mode=\'FAN_IN\', uniform=True)\n\n  state_net = tf.cast(state, tf.float32)\n  state_net = tf.div(state_net, 255.)\n  state_net = contrib_slim.conv2d(\n      state_net, 32, [8, 8], stride=4, weights_initializer=weights_initializer)\n  state_net = contrib_slim.conv2d(\n      state_net, 64, [4, 4], stride=2, weights_initializer=weights_initializer)\n  state_net = contrib_slim.conv2d(\n      state_net, 64, [3, 3], stride=1, weights_initializer=weights_initializer)\n  state_net = contrib_slim.flatten(state_net)\n  state_net_size = state_net.get_shape().as_list()[-1]\n  state_net_tiled = tf.tile(state_net, [num_quantiles, 1])\n\n  batch_size = state_net.get_shape().as_list()[0]\n  quantiles_shape = [num_quantiles * batch_size, 1]\n  quantiles = tf.random_uniform(\n      quantiles_shape, minval=0, maxval=1, dtype=tf.float32)\n\n  quantile_net = tf.tile(quantiles, [1, quantile_embedding_dim])\n  pi = tf.constant(math.pi)\n  quantile_net = tf.cast(tf.range(\n      1, quantile_embedding_dim + 1, 1), tf.float32) * pi * quantile_net\n  quantile_net = tf.cos(quantile_net)\n  quantile_net = contrib_slim.fully_connected(\n      quantile_net, state_net_size, weights_initializer=weights_initializer)\n  # Hadamard product.\n  net = tf.multiply(state_net_tiled, quantile_net)\n\n  net = contrib_slim.fully_connected(\n      net, 512, weights_initializer=weights_initializer)\n  quantile_values = contrib_slim.fully_connected(\n      net,\n      num_actions,\n      activation_fn=None,\n      weights_initializer=weights_initializer)\n\n  return network_type(quantile_values=quantile_values, quantiles=quantiles)\n\n\n@gin.configurable(blacklist=[\'variables\'])\ndef maybe_transform_variable_names(variables, legacy_checkpoint_load=False):\n  """"""Maps old variable names to the new ones.\n\n  The resulting dictionary can be passed to the tf.train.Saver to load\n  legacy checkpoints into Keras models.\n\n  Args:\n    variables: list, of all variables to be transformed.\n    legacy_checkpoint_load: bool, if True the variable names are mapped to\n        the legacy names as appeared in `tf.slim` based agents. Use this if\n        you want to load checkpoints saved before tf.keras.Model upgrade.\n  Returns:\n    dict or None, of <new_names, var>.\n  """"""\n  tf.logging.info(\'legacy_checkpoint_load: %s\', legacy_checkpoint_load)\n  if legacy_checkpoint_load:\n    name_map = {}\n    for var in variables:\n      new_name = var.op.name.replace(\'bias\', \'biases\')\n      new_name = new_name.replace(\'kernel\', \'weights\')\n      name_map[new_name] = var\n  else:\n    name_map = None\n  return name_map\n\n\nclass NatureDQNNetwork(tf.keras.Model):\n  """"""The convolutional network used to compute the agent\'s Q-values.""""""\n\n  def __init__(self, num_actions, name=None):\n    """"""Creates the layers used for calculating Q-values.\n\n    Args:\n      num_actions: int, number of actions.\n      name: str, used to create scope for network parameters.\n    """"""\n    super(NatureDQNNetwork, self).__init__(name=name)\n\n    self.num_actions = num_actions\n    # Defining layers.\n    activation_fn = tf.keras.activations.relu\n    # Setting names of the layers manually to make variable names more similar\n    # with tf.slim variable names/checkpoints.\n    self.conv1 = tf.keras.layers.Conv2D(32, [8, 8], strides=4, padding=\'same\',\n                                        activation=activation_fn, name=\'Conv\')\n    self.conv2 = tf.keras.layers.Conv2D(64, [4, 4], strides=2, padding=\'same\',\n                                        activation=activation_fn, name=\'Conv\')\n    self.conv3 = tf.keras.layers.Conv2D(64, [3, 3], strides=1, padding=\'same\',\n                                        activation=activation_fn, name=\'Conv\')\n    self.flatten = tf.keras.layers.Flatten()\n    self.dense1 = tf.keras.layers.Dense(512, activation=activation_fn,\n                                        name=\'fully_connected\')\n    self.dense2 = tf.keras.layers.Dense(num_actions, name=\'fully_connected\')\n\n  def call(self, state):\n    """"""Creates the output tensor/op given the state tensor as input.\n\n    See https://www.tensorflow.org/api_docs/python/tf/keras/Model for more\n    information on this. Note that tf.keras.Model implements `call` which is\n    wrapped by `__call__` function by tf.keras.Model.\n\n    Parameters created here will have scope according to the `name` argument\n    given at `.__init__()` call.\n    Args:\n      state: Tensor, input tensor.\n    Returns:\n      collections.namedtuple, output ops (graph mode) or output tensors (eager).\n    """"""\n    x = tf.cast(state, tf.float32)\n    x = tf.div(x, 255.)\n    x = self.conv1(x)\n    x = self.conv2(x)\n    x = self.conv3(x)\n    x = self.flatten(x)\n    x = self.dense1(x)\n\n    return DQNNetworkType(self.dense2(x))\n\n\nclass RainbowNetwork(tf.keras.Model):\n  """"""The convolutional network used to compute agent\'s return distributions.""""""\n\n  def __init__(self, num_actions, num_atoms, support, name=None):\n    """"""Creates the layers used calculating return distributions.\n\n    Args:\n      num_actions: int, number of actions.\n      num_atoms: int, the number of buckets of the value function distribution.\n      support: tf.linspace, the support of the Q-value distribution.\n      name: str, used to crete scope for network parameters.\n    """"""\n    super(RainbowNetwork, self).__init__(name=name)\n    activation_fn = tf.keras.activations.relu\n    self.num_actions = num_actions\n    self.num_atoms = num_atoms\n    self.support = support\n    self.kernel_initializer = tf.keras.initializers.VarianceScaling(\n        scale=1.0 / np.sqrt(3.0), mode=\'fan_in\', distribution=\'uniform\')\n    # Defining layers.\n    self.conv1 = tf.keras.layers.Conv2D(\n        32, [8, 8], strides=4, padding=\'same\', activation=activation_fn,\n        kernel_initializer=self.kernel_initializer, name=\'Conv\')\n    self.conv2 = tf.keras.layers.Conv2D(\n        64, [4, 4], strides=2, padding=\'same\', activation=activation_fn,\n        kernel_initializer=self.kernel_initializer, name=\'Conv\')\n    self.conv3 = tf.keras.layers.Conv2D(\n        64, [3, 3], strides=1, padding=\'same\', activation=activation_fn,\n        kernel_initializer=self.kernel_initializer, name=\'Conv\')\n    self.flatten = tf.keras.layers.Flatten()\n    self.dense1 = tf.keras.layers.Dense(\n        512, activation=activation_fn,\n        kernel_initializer=self.kernel_initializer, name=\'fully_connected\')\n    self.dense2 = tf.keras.layers.Dense(\n        num_actions * num_atoms, kernel_initializer=self.kernel_initializer,\n        name=\'fully_connected\')\n\n  def call(self, state):\n    """"""Creates the output tensor/op given the state tensor as input.\n\n    See https://www.tensorflow.org/api_docs/python/tf/keras/Model for more\n    information on this. Note that tf.keras.Model implements `call` which is\n    wrapped by `__call__` function by tf.keras.Model.\n\n    Args:\n      state: Tensor, input tensor.\n    Returns:\n      collections.namedtuple, output ops (graph mode) or output tensors (eager).\n    """"""\n    x = tf.cast(state, tf.float32)\n    x = tf.div(x, 255.)\n    x = self.conv1(x)\n    x = self.conv2(x)\n    x = self.conv3(x)\n    x = self.flatten(x)\n    x = self.dense1(x)\n    x = self.dense2(x)\n    logits = tf.reshape(x, [-1, self.num_actions, self.num_atoms])\n    probabilities = tf.keras.activations.softmax(logits)\n    q_values = tf.reduce_sum(self.support * probabilities, axis=2)\n    return RainbowNetworkType(q_values, logits, probabilities)\n\n\nclass ImplicitQuantileNetwork(tf.keras.Model):\n  """"""The Implicit Quantile Network (Dabney et al., 2018)..""""""\n\n  def __init__(self, num_actions, quantile_embedding_dim, name=None):\n    """"""Creates the layers used calculating quantile values.\n\n    Args:\n      num_actions: int, number of actions.\n      quantile_embedding_dim: int, embedding dimension for the quantile input.\n      name: str, used to create scope for network parameters.\n    """"""\n    super(ImplicitQuantileNetwork, self).__init__(name=name)\n    self.num_actions = num_actions\n    self.quantile_embedding_dim = quantile_embedding_dim\n    # We need the activation function during `call`, therefore set the field.\n    self.activation_fn = tf.keras.activations.relu\n    self.kernel_initializer = tf.keras.initializers.VarianceScaling(\n        scale=1.0 / np.sqrt(3.0), mode=\'fan_in\', distribution=\'uniform\')\n    # Defining layers.\n    self.conv1 = tf.keras.layers.Conv2D(\n        32, [8, 8], strides=4, padding=\'same\', activation=self.activation_fn,\n        kernel_initializer=self.kernel_initializer, name=\'Conv\')\n    self.conv2 = tf.keras.layers.Conv2D(\n        64, [4, 4], strides=2, padding=\'same\', activation=self.activation_fn,\n        kernel_initializer=self.kernel_initializer, name=\'Conv\')\n    self.conv3 = tf.keras.layers.Conv2D(\n        64, [3, 3], strides=1, padding=\'same\', activation=self.activation_fn,\n        kernel_initializer=self.kernel_initializer, name=\'Conv\')\n    self.flatten = tf.keras.layers.Flatten()\n    self.dense1 = tf.keras.layers.Dense(\n        512, activation=self.activation_fn,\n        kernel_initializer=self.kernel_initializer, name=\'fully_connected\')\n    self.dense2 = tf.keras.layers.Dense(\n        num_actions, kernel_initializer=self.kernel_initializer,\n        name=\'fully_connected\')\n\n  def call(self, state, num_quantiles):\n    """"""Creates the output tensor/op given the state tensor as input.\n\n    See https://www.tensorflow.org/api_docs/python/tf/keras/Model for more\n    information on this. Note that tf.keras.Model implements `call` which is\n    wrapped by `__call__` function by tf.keras.Model.\n\n    Args:\n      state: `tf.Tensor`, contains the agent\'s current state.\n      num_quantiles: int, number of quantile inputs.\n    Returns:\n      collections.namedtuple, that contains (quantile_values, quantiles).\n    """"""\n    batch_size = state.get_shape().as_list()[0]\n    x = tf.cast(state, tf.float32)\n    x = tf.div(x, 255.)\n    x = self.conv1(x)\n    x = self.conv2(x)\n    x = self.conv3(x)\n    x = self.flatten(x)\n    state_vector_length = x.get_shape().as_list()[-1]\n    state_net_tiled = tf.tile(x, [num_quantiles, 1])\n    quantiles_shape = [num_quantiles * batch_size, 1]\n    quantiles = tf.random_uniform(\n        quantiles_shape, minval=0, maxval=1, dtype=tf.float32)\n    quantile_net = tf.tile(quantiles, [1, self.quantile_embedding_dim])\n    pi = tf.constant(math.pi)\n    quantile_net = tf.cast(tf.range(\n        1, self.quantile_embedding_dim + 1, 1), tf.float32) * pi * quantile_net\n    quantile_net = tf.cos(quantile_net)\n    # Create the quantile layer in the first call. This is because\n    # number of output units depends on the input shape. Therefore, we can only\n    # create the layer during the first forward call, not during `.__init__()`.\n    if not hasattr(self, \'dense_quantile\'):\n      self.dense_quantile = tf.keras.layers.Dense(\n          state_vector_length, activation=self.activation_fn,\n          kernel_initializer=self.kernel_initializer)\n    quantile_net = self.dense_quantile(quantile_net)\n    x = tf.multiply(state_net_tiled, quantile_net)\n    x = self.dense1(x)\n    quantile_values = self.dense2(x)\n    return ImplicitQuantileNetworkType(quantile_values, quantiles)\n\n\n@gin.configurable\nclass AtariPreprocessing(object):\n  """"""A class implementing image preprocessing for Atari 2600 agents.\n\n  Specifically, this provides the following subset from the JAIR paper\n  (Bellemare et al., 2013) and Nature DQN paper (Mnih et al., 2015):\n\n    * Frame skipping (defaults to 4).\n    * Terminal signal when a life is lost (off by default).\n    * Grayscale and max-pooling of the last two frames.\n    * Downsample the screen to a square image (defaults to 84x84).\n\n  More generally, this class follows the preprocessing guidelines set down in\n  Machado et al. (2018), ""Revisiting the Arcade Learning Environment:\n  Evaluation Protocols and Open Problems for General Agents"".\n  """"""\n\n  def __init__(self, environment, frame_skip=4, terminal_on_life_loss=False,\n               screen_size=84):\n    """"""Constructor for an Atari 2600 preprocessor.\n\n    Args:\n      environment: Gym environment whose observations are preprocessed.\n      frame_skip: int, the frequency at which the agent experiences the game.\n      terminal_on_life_loss: bool, If True, the step() method returns\n        is_terminal=True whenever a life is lost. See Mnih et al. 2015.\n      screen_size: int, size of a resized Atari 2600 frame.\n\n    Raises:\n      ValueError: if frame_skip or screen_size are not strictly positive.\n    """"""\n    if frame_skip <= 0:\n      raise ValueError(\'Frame skip should be strictly positive, got {}\'.\n                       format(frame_skip))\n    if screen_size <= 0:\n      raise ValueError(\'Target screen size should be strictly positive, got {}\'.\n                       format(screen_size))\n\n    self.environment = environment\n    self.terminal_on_life_loss = terminal_on_life_loss\n    self.frame_skip = frame_skip\n    self.screen_size = screen_size\n\n    obs_dims = self.environment.observation_space\n    # Stores temporary observations used for pooling over two successive\n    # frames.\n    self.screen_buffer = [\n        np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8),\n        np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8)\n    ]\n\n    self.game_over = False\n    self.lives = 0  # Will need to be set by reset().\n\n  @property\n  def observation_space(self):\n    # Return the observation space adjusted to match the shape of the processed\n    # observations.\n    return Box(low=0, high=255, shape=(self.screen_size, self.screen_size, 1),\n               dtype=np.uint8)\n\n  @property\n  def action_space(self):\n    return self.environment.action_space\n\n  @property\n  def reward_range(self):\n    return self.environment.reward_range\n\n  @property\n  def metadata(self):\n    return self.environment.metadata\n\n  def close(self):\n    return self.environment.close()\n\n  def reset(self):\n    """"""Resets the environment.\n\n    Returns:\n      observation: numpy array, the initial observation emitted by the\n        environment.\n    """"""\n    self.environment.reset()\n    self.lives = self.environment.ale.lives()\n    self._fetch_grayscale_observation(self.screen_buffer[0])\n    self.screen_buffer[1].fill(0)\n    return self._pool_and_resize()\n\n  def render(self, mode):\n    """"""Renders the current screen, before preprocessing.\n\n    This calls the Gym API\'s render() method.\n\n    Args:\n      mode: Mode argument for the environment\'s render() method.\n        Valid values (str) are:\n          \'rgb_array\': returns the raw ALE image.\n          \'human\': renders to display via the Gym renderer.\n\n    Returns:\n      if mode=\'rgb_array\': numpy array, the most recent screen.\n      if mode=\'human\': bool, whether the rendering was successful.\n    """"""\n    return self.environment.render(mode)\n\n  def step(self, action):\n    """"""Applies the given action in the environment.\n\n    Remarks:\n\n      * If a terminal state (from life loss or episode end) is reached, this may\n        execute fewer than self.frame_skip steps in the environment.\n      * Furthermore, in this case the returned observation may not contain valid\n        image data and should be ignored.\n\n    Args:\n      action: The action to be executed.\n\n    Returns:\n      observation: numpy array, the observation following the action.\n      reward: float, the reward following the action.\n      is_terminal: bool, whether the environment has reached a terminal state.\n        This is true when a life is lost and terminal_on_life_loss, or when the\n        episode is over.\n      info: Gym API\'s info data structure.\n    """"""\n    accumulated_reward = 0.\n\n    for time_step in range(self.frame_skip):\n      # We bypass the Gym observation altogether and directly fetch the\n      # grayscale image from the ALE. This is a little faster.\n      _, reward, game_over, info = self.environment.step(action)\n      accumulated_reward += reward\n\n      if self.terminal_on_life_loss:\n        new_lives = self.environment.ale.lives()\n        is_terminal = game_over or new_lives < self.lives\n        self.lives = new_lives\n      else:\n        is_terminal = game_over\n\n      if is_terminal:\n        break\n      # We max-pool over the last two frames, in grayscale.\n      elif time_step >= self.frame_skip - 2:\n        t = time_step - (self.frame_skip - 2)\n        self._fetch_grayscale_observation(self.screen_buffer[t])\n\n    # Pool the last two observations.\n    observation = self._pool_and_resize()\n\n    self.game_over = game_over\n    return observation, accumulated_reward, is_terminal, info\n\n  def _fetch_grayscale_observation(self, output):\n    """"""Returns the current observation in grayscale.\n\n    The returned observation is stored in \'output\'.\n\n    Args:\n      output: numpy array, screen buffer to hold the returned observation.\n\n    Returns:\n      observation: numpy array, the current observation in grayscale.\n    """"""\n    self.environment.ale.getScreenGrayscale(output)\n    return output\n\n  def _pool_and_resize(self):\n    """"""Transforms two frames into a Nature DQN observation.\n\n    For efficiency, the transformation is done in-place in self.screen_buffer.\n\n    Returns:\n      transformed_screen: numpy array, pooled, resized screen.\n    """"""\n    # Pool if there are enough screens to do so.\n    if self.frame_skip > 1:\n      np.maximum(self.screen_buffer[0], self.screen_buffer[1],\n                 out=self.screen_buffer[0])\n\n    transformed_image = cv2.resize(self.screen_buffer[0],\n                                   (self.screen_size, self.screen_size),\n                                   interpolation=cv2.INTER_AREA)\n    int_image = np.asarray(transformed_image, dtype=np.uint8)\n    return np.expand_dims(int_image, axis=2)\n'"
dopamine/discrete_domains/checkpointer.py,12,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A checkpointing mechanism for Dopamine agents.\n\nThis Checkpointer expects a base directory where checkpoints for different\niterations are stored. Specifically, Checkpointer.save_checkpoint() takes in\nas input a dictionary \'data\' to be pickled to disk. At each iteration, we\nwrite a file called \'cpkt.#\', where # is the iteration number. The\nCheckpointer also cleans up old files, maintaining up to the CHECKPOINT_DURATION\nmost recent iterations.\n\nThe Checkpointer writes a sentinel file to indicate that checkpointing was\nglobally successful. This means that all other checkpointing activities\n(saving the Tensorflow graph, the replay buffer) should be performed *prior*\nto calling Checkpointer.save_checkpoint(). This allows the Checkpointer to\ndetect incomplete checkpoints.\n\n#### Example\n\nAfter running 10 iterations (numbered 0...9) with base_directory=\'/checkpoint\',\nthe following files will exist:\n```\n  /checkpoint/cpkt.6\n  /checkpoint/cpkt.7\n  /checkpoint/cpkt.8\n  /checkpoint/cpkt.9\n  /checkpoint/sentinel_checkpoint_complete.6\n  /checkpoint/sentinel_checkpoint_complete.7\n  /checkpoint/sentinel_checkpoint_complete.8\n  /checkpoint/sentinel_checkpoint_complete.9\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport pickle\n\nimport gin\nimport tensorflow.compat.v1 as tf\n\nCHECKPOINT_DURATION = 4\n\n\n@gin.configurable\ndef get_latest_checkpoint_number(base_directory,\n                                 override_number=None,\n                                 sentinel_file_identifier=\'checkpoint\'):\n  """"""Returns the version number of the latest completed checkpoint.\n\n  Args:\n    base_directory: str, directory in which to look for checkpoint files.\n    override_number: None or int, allows the user to manually override\n      the checkpoint number via a gin-binding.\n    sentinel_file_identifier: str, prefix used by checkpointer for naming\n      sentinel files.\n\n  Returns:\n    int, the iteration number of the latest checkpoint, or -1 if none was found.\n  """"""\n  if override_number is not None:\n    return override_number\n\n  sentinel = \'sentinel_{}_complete.*\'.format(sentinel_file_identifier)\n  glob = os.path.join(base_directory, sentinel)\n  def extract_iteration(x):\n    return int(x[x.rfind(\'.\') + 1:])\n  try:\n    checkpoint_files = tf.gfile.Glob(glob)\n  except tf.errors.NotFoundError:\n    return -1\n  try:\n    latest_iteration = max(extract_iteration(x) for x in checkpoint_files)\n    return latest_iteration\n  except ValueError:\n    return -1\n\n\nclass Checkpointer(object):\n  """"""Class for managing checkpoints for Dopamine agents.\n  """"""\n\n  def __init__(self, base_directory, checkpoint_file_prefix=\'ckpt\',\n               sentinel_file_identifier=\'checkpoint\', checkpoint_frequency=1):\n    """"""Initializes Checkpointer.\n\n    Args:\n      base_directory: str, directory where all checkpoints are saved/loaded.\n      checkpoint_file_prefix: str, prefix to use for naming checkpoint files.\n      sentinel_file_identifier: str, prefix to use for naming sentinel files.\n      checkpoint_frequency: int, the frequency at which to checkpoint.\n\n    Raises:\n      ValueError: if base_directory is empty, or not creatable.\n    """"""\n    if not base_directory:\n      raise ValueError(\'No path provided to Checkpointer.\')\n    self._checkpoint_file_prefix = checkpoint_file_prefix\n    self._sentinel_file_prefix = \'sentinel_{}_complete\'.format(\n        sentinel_file_identifier)\n    self._checkpoint_frequency = checkpoint_frequency\n    self._base_directory = base_directory\n    try:\n      tf.gfile.MakeDirs(base_directory)\n    except tf.errors.PermissionDeniedError:\n      # We catch the PermissionDeniedError and issue a more useful exception.\n      raise ValueError(\'Unable to create checkpoint path: {}.\'.format(\n          base_directory))\n\n  def _generate_filename(self, file_prefix, iteration_number):\n    """"""Returns a checkpoint filename from prefix and iteration number.""""""\n    filename = \'{}.{}\'.format(file_prefix, iteration_number)\n    return os.path.join(self._base_directory, filename)\n\n  def _save_data_to_file(self, data, filename):\n    """"""Saves the given \'data\' object to a file.""""""\n    with tf.gfile.GFile(filename, \'w\') as fout:\n      pickle.dump(data, fout)\n\n  def save_checkpoint(self, iteration_number, data):\n    """"""Saves a new checkpoint at the current iteration_number.\n\n    Args:\n      iteration_number: int, the current iteration number for this checkpoint.\n      data: Any (picklable) python object containing the data to store in the\n        checkpoint.\n    """"""\n    if iteration_number % self._checkpoint_frequency != 0:\n      return\n\n    filename = self._generate_filename(self._checkpoint_file_prefix,\n                                       iteration_number)\n    self._save_data_to_file(data, filename)\n    filename = self._generate_filename(self._sentinel_file_prefix,\n                                       iteration_number)\n    with tf.gfile.GFile(filename, \'wb\') as fout:\n      fout.write(\'done\')\n\n    self._clean_up_old_checkpoints(iteration_number)\n\n  def _clean_up_old_checkpoints(self, iteration_number):\n    """"""Removes sufficiently old checkpoints.""""""\n    # After writing a the checkpoint and sentinel file, we garbage collect files\n    # that are CHECKPOINT_DURATION * self._checkpoint_frequency versions old.\n    stale_iteration_number = iteration_number - (self._checkpoint_frequency *\n                                                 CHECKPOINT_DURATION)\n\n    if stale_iteration_number >= 0:\n      stale_file = self._generate_filename(self._checkpoint_file_prefix,\n                                           stale_iteration_number)\n      stale_sentinel = self._generate_filename(self._sentinel_file_prefix,\n                                               stale_iteration_number)\n      try:\n        tf.gfile.Remove(stale_file)\n        tf.gfile.Remove(stale_sentinel)\n      except tf.errors.NotFoundError:\n        # Ignore if file not found.\n        tf.logging.info(\'Unable to remove {} or {}.\'.format(stale_file,\n                                                            stale_sentinel))\n\n  def _load_data_from_file(self, filename):\n    if not tf.gfile.Exists(filename):\n      return None\n    with tf.gfile.GFile(filename, \'rb\') as fin:\n      return pickle.load(fin)\n\n  def load_checkpoint(self, iteration_number):\n    """"""Tries to reload a checkpoint at the selected iteration number.\n\n    Args:\n      iteration_number: The checkpoint iteration number to try to load.\n\n    Returns:\n      If the checkpoint files exist, two unpickled objects that were passed in\n        as data to save_checkpoint; returns None if the files do not exist.\n    """"""\n    checkpoint_file = self._generate_filename(self._checkpoint_file_prefix,\n                                              iteration_number)\n    return self._load_data_from_file(checkpoint_file)\n'"
dopamine/discrete_domains/gym_lib.py,42,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Gym-specific (non-Atari) utilities.\n\nSome network specifications specific to certain Gym environments are provided\nhere.\n\nIncludes a wrapper class around Gym environments. This class makes general Gym\nenvironments conformant with the API Dopamine is expecting.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport math\n\n\n\nfrom dopamine.discrete_domains import atari_lib\nimport gym\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nimport gin.tf\nfrom tensorflow.contrib import layers as contrib_layers\nfrom tensorflow.contrib import slim as contrib_slim\n\n\nCARTPOLE_MIN_VALS = np.array([-2.4, -5., -math.pi/12., -math.pi*2.])\nCARTPOLE_MAX_VALS = np.array([2.4, 5., math.pi/12., math.pi*2.])\nACROBOT_MIN_VALS = np.array([-1., -1., -1., -1., -5., -5.])\nACROBOT_MAX_VALS = np.array([1., 1., 1., 1., 5., 5.])\ngin.constant(\'gym_lib.CARTPOLE_OBSERVATION_SHAPE\', (4, 1))\ngin.constant(\'gym_lib.CARTPOLE_OBSERVATION_DTYPE\', tf.float64)\ngin.constant(\'gym_lib.CARTPOLE_STACK_SIZE\', 1)\ngin.constant(\'gym_lib.ACROBOT_OBSERVATION_SHAPE\', (6, 1))\ngin.constant(\'gym_lib.ACROBOT_OBSERVATION_DTYPE\', tf.float64)\ngin.constant(\'gym_lib.ACROBOT_STACK_SIZE\', 1)\n\n\n@gin.configurable\ndef create_gym_environment(environment_name=None, version=\'v0\'):\n  """"""Wraps a Gym environment with some basic preprocessing.\n\n  Args:\n    environment_name: str, the name of the environment to run.\n    version: str, version of the environment to run.\n\n  Returns:\n    A Gym environment with some standard preprocessing.\n  """"""\n  assert environment_name is not None\n  full_game_name = \'{}-{}\'.format(environment_name, version)\n  env = gym.make(full_game_name)\n  # Strip out the TimeLimit wrapper from Gym, which caps us at 200 steps.\n  env = env.env\n  # Wrap the returned environment in a class which conforms to the API expected\n  # by Dopamine.\n  env = GymPreprocessing(env)\n  return env\n\n\n@gin.configurable\nclass BasicDiscreteDomainNetwork(tf.keras.layers.Layer):\n  """"""The fully connected network used to compute the agent\'s Q-values.\n\n    This sub network used within various other models. Since it is an inner\n    block, we define it as a layer. These sub networks normalize their inputs to\n    lie in range [-1, 1], using min_/max_vals. It supports both DQN- and\n    Rainbow- style networks.\n    Attributes:\n      min_vals: float, minimum attainable values (must be same shape as\n        `state`).\n      max_vals: float, maximum attainable values (must be same shape as\n        `state`).\n      num_actions: int, number of actions.\n      num_atoms: int or None, if None will construct a DQN-style network,\n        otherwise will construct a Rainbow-style network.\n      name: str, used to create scope for network parameters.\n      activation_fn: function, passed to the layer constructors.\n  """"""\n\n  def __init__(self, min_vals, max_vals, num_actions,\n               num_atoms=None, name=None,\n               activation_fn=tf.keras.activations.relu):\n    super(BasicDiscreteDomainNetwork, self).__init__(name=name)\n    self.num_actions = num_actions\n    self.num_atoms = num_atoms\n    self.min_vals = min_vals\n    self.max_vals = max_vals\n    # Defining layers.\n    self.flatten = tf.keras.layers.Flatten()\n    self.dense1 = tf.keras.layers.Dense(512, activation=activation_fn,\n                                        name=\'fully_connected\')\n    self.dense2 = tf.keras.layers.Dense(512, activation=activation_fn,\n                                        name=\'fully_connected\')\n    if num_atoms is None:\n      self.last_layer = tf.keras.layers.Dense(num_actions,\n                                              name=\'fully_connected\')\n    else:\n      self.last_layer = tf.keras.layers.Dense(num_actions * num_atoms,\n                                              name=\'fully_connected\')\n\n  def call(self, state):\n    """"""Creates the output tensor/op given the state tensor as input.""""""\n    x = tf.cast(state, tf.float32)\n    x = self.flatten(x)\n    x -= self.min_vals\n    x /= self.max_vals - self.min_vals\n    x = 2.0 * x - 1.0  # Rescale in range [-1, 1].\n    x = self.dense1(x)\n    x = self.dense2(x)\n    x = self.last_layer(x)\n    return x\n\n\n@gin.configurable\ndef _basic_discrete_domain_network(min_vals, max_vals, num_actions, state,\n                                   num_atoms=None):\n  """"""Builds a basic network for discrete domains, rescaling inputs to [-1, 1].\n\n  Args:\n    min_vals: float, minimum attainable values (must be same shape as `state`).\n    max_vals: float, maximum attainable values (must be same shape as `state`).\n    num_actions: int, number of actions.\n    state: `tf.Tensor`, the state input.\n    num_atoms: int or None, if None will construct a DQN-style network,\n      otherwise will construct a Rainbow-style network.\n\n  Returns:\n    The Q-values for DQN-style agents or logits for Rainbow-style agents.\n  """"""\n  net = tf.cast(state, tf.float32)\n  net = contrib_slim.flatten(net)\n  net -= min_vals\n  net /= max_vals - min_vals\n  net = 2.0 * net - 1.0  # Rescale in range [-1, 1].\n  net = contrib_slim.fully_connected(net, 512)\n  net = contrib_slim.fully_connected(net, 512)\n  if num_atoms is None:\n    # We are constructing a DQN-style network.\n    return contrib_slim.fully_connected(net, num_actions, activation_fn=None)\n  else:\n    # We are constructing a Rainbow-style network.\n    return contrib_slim.fully_connected(\n        net, num_actions * num_atoms, activation_fn=None)\n\n\n@gin.configurable\nclass CartpoleDQNNetwork(tf.keras.Model):\n  """"""Keras DQN network for Cartpole.""""""\n\n  def __init__(self, num_actions, name=None):\n    """"""Builds the deep network used to compute the agent\'s Q-values.\n\n    It rescales the input features so they lie in range [-1, 1].\n\n    Args:\n      num_actions: int, number of actions.\n      name: str, used to create scope for network parameters.\n    """"""\n    super(CartpoleDQNNetwork, self).__init__(name=name)\n    self.net = BasicDiscreteDomainNetwork(\n        CARTPOLE_MIN_VALS, CARTPOLE_MAX_VALS, num_actions)\n\n  def call(self, state):\n    """"""Creates the output tensor/op given the state tensor as input.""""""\n    x = self.net(state)\n    return atari_lib.DQNNetworkType(x)\n\n\n@gin.configurable\ndef cartpole_dqn_network(num_actions, network_type, state):\n  """"""Builds the deep network used to compute the agent\'s Q-values.\n\n  It rescales the input features to a range that yields improved performance.\n\n  Args:\n    num_actions: int, number of actions.\n    network_type: namedtuple, collection of expected values to return.\n    state: `tf.Tensor`, contains the agent\'s current state.\n\n  Returns:\n    net: _network_type object containing the tensors output by the network.\n  """"""\n  q_values = _basic_discrete_domain_network(\n      CARTPOLE_MIN_VALS, CARTPOLE_MAX_VALS, num_actions, state)\n  return network_type(q_values)\n\n\nclass FourierBasis(object):\n  """"""Fourier Basis linear function approximation.\n\n  Requires the ranges for each dimension, and is thus able to use only sine or\n  cosine (and uses cosine). So, this has half the coefficients that a full\n  Fourier approximation would use.\n\n  Many thanks to Will Dabney (wdabney@) for this implementation.\n\n  From the paper:\n  G.D. Konidaris, S. Osentoski and P.S. Thomas. (2011)\n  Value Function Approximation in Reinforcement Learning using the Fourier Basis\n  """"""\n\n  def __init__(self, nvars, min_vals=0, max_vals=None, order=3):\n    self.order = order\n    self.min_vals = min_vals\n    self.max_vals = max_vals\n    terms = itertools.product(range(order + 1), repeat=nvars)\n\n    # Removing first iterate because it corresponds to the constant bias\n    self.multipliers = tf.constant(\n        [list(map(int, x)) for x in terms][1:], dtype=tf.float32)\n\n  def scale(self, values):\n    shifted = values - self.min_vals\n    if self.max_vals is None:\n      return shifted\n\n    return shifted / (self.max_vals - self.min_vals)\n\n  def compute_features(self, features):\n    # Important to rescale features to be between [0,1]\n    scaled = self.scale(features)\n    return tf.cos(np.pi * tf.matmul(scaled, self.multipliers, transpose_b=True))\n\n\n@gin.configurable\nclass FourierDQNNetwork(tf.keras.Model):\n  """"""Keras model for DQN.""""""\n\n  def __init__(self, min_vals, max_vals, num_actions, fourier_basis_order=3,\n               name=None):\n    """"""Builds the function approximator used to compute the agent\'s Q-values.\n\n    It uses the features of the FourierBasis class and a linear layer\n    without bias.\n\n    Value Function Approximation in Reinforcement Learning using the Fourier\n    Basis"", Konidaris, Osentoski and Thomas (2011).\n\n    Args:\n      min_vals: float, minimum attainable values (must be same shape as\n        `state`).\n      max_vals: float, maximum attainable values (must be same shape as\n        `state`).\n      num_actions: int, number of actions.\n      fourier_basis_order: int, order of the Fourier basis functions.\n      name: str, used to create scope for network parameters.\n    """"""\n    super(FourierDQNNetwork, self).__init__(name=name)\n    self.num_actions = num_actions\n    self.fourier_basis_order = fourier_basis_order\n    self.min_vals = min_vals\n    self.max_vals = max_vals\n    # Defining layers.\n    self.flatten = tf.keras.layers.Flatten()\n    self.last_layer = tf.keras.layers.Dense(num_actions, use_bias=False,\n                                            name=\'fully_connected\')\n\n  def call(self, state):\n    """"""Creates the output tensor/op given the state tensor as input.""""""\n    x = tf.cast(state, tf.float32)\n    x = self.flatten(x)\n    # Since FourierBasis needs the shape of the input, we can only initialize\n    # it during the first forward pass when we know the shape of the input.\n    if not hasattr(self, \'feature_generator\'):\n      self.feature_generator = FourierBasis(\n          x.get_shape().as_list()[-1],\n          self.min_vals,\n          self.max_vals,\n          order=self.fourier_basis_order)\n    x = self.feature_generator.compute_features(x)\n    x = self.last_layer(x)\n    return atari_lib.DQNNetworkType(x)\n\n\n@gin.configurable\ndef fourier_dqn_network(min_vals,\n                        max_vals,\n                        num_actions,\n                        state,\n                        fourier_basis_order=3):\n  """"""Builds the function approximator used to compute the agent\'s Q-values.\n\n  It uses FourierBasis features and a linear layer.\n\n  Args:\n    min_vals: float, minimum attainable values (must be same shape as `state`).\n    max_vals: float, maximum attainable values (must be same shape as `state`).\n    num_actions: int, number of actions.\n    state: `tf.Tensor`, contains the agent\'s current state.\n    fourier_basis_order: int, order of the Fourier basis functions.\n\n  Returns:\n    The Q-values for DQN-style agents or logits for Rainbow-style agents.\n  """"""\n  net = tf.cast(state, tf.float32)\n  net = contrib_slim.flatten(net)\n\n  # Feed state through Fourier basis.\n  feature_generator = FourierBasis(\n      net.get_shape().as_list()[-1],\n      min_vals,\n      max_vals,\n      order=fourier_basis_order)\n  net = feature_generator.compute_features(net)\n\n  # Q-values are always linear w.r.t. last layer.\n  q_values = contrib_slim.fully_connected(\n      net, num_actions, activation_fn=None, biases_initializer=None)\n  return q_values\n\n\n@gin.configurable\nclass CartpoleFourierDQNNetwork(FourierDQNNetwork):\n  """"""Keras network for fourier Cartpole.""""""\n\n  def __init__(self, num_actions, name=None):\n    """"""Builds the function approximator used to compute the agent\'s Q-values.\n\n    It uses the Fourier basis features and a linear function approximator.\n\n    Args:\n      num_actions: int, number of actions.\n      name: str, used to create scope for network parameters.\n    """"""\n    super(CartpoleFourierDQNNetwork, self).__init__(\n        CARTPOLE_MIN_VALS, CARTPOLE_MAX_VALS, num_actions, name=name)\n\n\ndef cartpole_fourier_dqn_network(num_actions, network_type, state):\n  """"""Builds the function approximator used to compute the agent\'s Q-values.\n\n  It uses the Fourier basis features and a linear function approximator.\n\n  Args:\n    num_actions: int, number of actions.\n    network_type: namedtuple, collection of expected values to return.\n    state: `tf.Tensor`, contains the agent\'s current state.\n\n  Returns:\n    net: _network_type object containing the tensors output by the network.\n  """"""\n  q_values = fourier_dqn_network(CARTPOLE_MIN_VALS, CARTPOLE_MAX_VALS,\n                                 num_actions, state)\n  return network_type(q_values)\n\n\n@gin.configurable\nclass CartpoleRainbowNetwork(tf.keras.Model):\n  """"""Keras Rainbow network for Cartpole.""""""\n\n  def __init__(self, num_actions, num_atoms, support, name=None):\n    """"""Builds the deep network used to compute the agent\'s Q-values.\n\n    It rescales the input features to a range that yields improved performance.\n\n    Args:\n      num_actions: int, number of actions.\n      num_atoms: int, the number of buckets of the value function distribution.\n      support: tf.linspace, the support of the Q-value distribution.\n      name: str, used to create scope for network parameters.\n    """"""\n    super(CartpoleRainbowNetwork, self).__init__(name=name)\n    self.net = BasicDiscreteDomainNetwork(\n        CARTPOLE_MIN_VALS, CARTPOLE_MAX_VALS, num_actions, num_atoms=num_atoms)\n    self.num_actions = num_actions\n    self.num_atoms = num_atoms\n    self.support = support\n\n  def call(self, state):\n    x = self.net(state)\n    logits = tf.reshape(x, [-1, self.num_actions, self.num_atoms])\n    probabilities = contrib_layers.softmax(logits)\n    q_values = tf.reduce_sum(self.support * probabilities, axis=2)\n    return atari_lib.RainbowNetworkType(q_values, logits, probabilities)\n\n\n@gin.configurable\ndef cartpole_rainbow_network(num_actions, num_atoms, support, network_type,\n                             state):\n  """"""Build the deep network used to compute the agent\'s Q-value distributions.\n\n  Args:\n    num_actions: int, number of actions.\n    num_atoms: int, the number of buckets of the value function distribution.\n    support: tf.linspace, the support of the Q-value distribution.\n    network_type: `namedtuple`, collection of expected values to return.\n    state: `tf.Tensor`, contains the agent\'s current state.\n\n  Returns:\n    net: _network_type object containing the tensors output by the network.\n  """"""\n  net = _basic_discrete_domain_network(\n      CARTPOLE_MIN_VALS, CARTPOLE_MAX_VALS, num_actions, state,\n      num_atoms=num_atoms)\n  logits = tf.reshape(net, [-1, num_actions, num_atoms])\n  probabilities = contrib_layers.softmax(logits)\n  q_values = tf.reduce_sum(support * probabilities, axis=2)\n  return network_type(q_values, logits, probabilities)\n\n\n@gin.configurable\nclass AcrobotDQNNetwork(tf.keras.Model):\n  """"""Keras DQN network for Acrobot.""""""\n\n  def __init__(self, num_actions, name=None):\n    """"""Builds the deep network used to compute the agent\'s Q-values.\n\n    It rescales the input features to a range that yields improved performance.\n\n    Args:\n      num_actions: int, number of actions.\n      name: str, used to create scope for network parameters.\n    """"""\n    super(AcrobotDQNNetwork, self).__init__(name=name)\n    self.net = BasicDiscreteDomainNetwork(\n        ACROBOT_MIN_VALS, ACROBOT_MAX_VALS, num_actions)\n\n  def call(self, state):\n    x = self.net(state)\n    return atari_lib.DQNNetworkType(x)\n\n\n@gin.configurable\ndef acrobot_dqn_network(num_actions, network_type, state):\n  """"""Builds the deep network used to compute the agent\'s Q-values.\n\n  It rescales the input features to a range that yields improved performance.\n\n  Args:\n    num_actions: int, number of actions.\n    network_type: namedtuple, collection of expected values to return.\n    state: `tf.Tensor`, contains the agent\'s current state.\n\n  Returns:\n    net: _network_type object containing the tensors output by the network.\n  """"""\n  q_values = _basic_discrete_domain_network(\n      ACROBOT_MIN_VALS, ACROBOT_MAX_VALS, num_actions, state)\n  return network_type(q_values)\n\n\n@gin.configurable\nclass AcrobotFourierDQNNetwork(FourierDQNNetwork):\n  """"""Keras fourier DQN network for Acrobot.""""""\n\n  def __init__(self, num_actions, name=None):\n    """"""Builds the function approximator used to compute the agent\'s Q-values.\n\n    It uses the Fourier basis features and a linear function approximator.\n\n    Args:\n      num_actions: int, number of actions.\n      name: str, used to create scope for network parameters.\n    """"""\n\n    super(AcrobotFourierDQNNetwork, self).__init__(\n        ACROBOT_MIN_VALS, ACROBOT_MAX_VALS, num_actions, name=name)\n\n\n@gin.configurable\ndef acrobot_fourier_dqn_network(num_actions, network_type, state):\n  """"""Builds the function approximator used to compute the agent\'s Q-values.\n\n  It uses the Fourier basis features and a linear function approximator.\n\n  Args:\n    num_actions: int, number of actions.\n    network_type: namedtuple, collection of expected values to return.\n    state: `tf.Tensor`, contains the agent\'s current state.\n\n  Returns:\n    net: _network_type object containing the tensors output by the network.\n  """"""\n  q_values = fourier_dqn_network(ACROBOT_MIN_VALS, ACROBOT_MAX_VALS,\n                                 num_actions, state)\n  return network_type(q_values)\n\n\n@gin.configurable\nclass AcrobotRainbowNetwork(tf.keras.Model):\n  """"""Keras Rainbow network for Acrobot.""""""\n\n  def __init__(self, num_actions, num_atoms, support, name=None):\n    """"""Builds the deep network used to compute the agent\'s Q-values.\n\n    It rescales the input features to a range that yields improved performance.\n\n    Args:\n      num_actions: int, number of actions.\n      num_atoms: int, the number of buckets of the value function distribution.\n      support: Tensor, the support of the Q-value distribution.\n      name: str, used to create scope for network parameters.\n    """"""\n    super(AcrobotRainbowNetwork, self).__init__(name=name)\n    self.net = BasicDiscreteDomainNetwork(\n        ACROBOT_MIN_VALS, ACROBOT_MAX_VALS, num_actions, num_atoms=num_atoms)\n    self.num_actions = num_actions\n    self.num_atoms = num_atoms\n    self.support = support\n\n  def call(self, state):\n    x = self.net(state)\n    logits = tf.reshape(x, [-1, self.num_actions, self.num_atoms])\n    probabilities = contrib_layers.softmax(logits)\n    q_values = tf.reduce_sum(self.support * probabilities, axis=2)\n    return atari_lib.RainbowNetworkType(q_values, logits, probabilities)\n\n\n@gin.configurable\ndef acrobot_rainbow_network(num_actions, num_atoms, support, network_type,\n                            state):\n  """"""Build the deep network used to compute the agent\'s Q-value distributions.\n\n  Args:\n    num_actions: int, number of actions.\n    num_atoms: int, the number of buckets of the value function distribution.\n    support: tf.linspace, the support of the Q-value distribution.\n    network_type: `namedtuple`, collection of expected values to return.\n    state: `tf.Tensor`, contains the agent\'s current state.\n\n  Returns:\n    net: _network_type object containing the tensors output by the network.\n  """"""\n  net = _basic_discrete_domain_network(\n      ACROBOT_MIN_VALS, ACROBOT_MAX_VALS, num_actions, state,\n      num_atoms=num_atoms)\n  logits = tf.reshape(net, [-1, num_actions, num_atoms])\n  probabilities = contrib_layers.softmax(logits)\n  q_values = tf.reduce_sum(support * probabilities, axis=2)\n  return network_type(q_values, logits, probabilities)\n\n\n@gin.configurable\nclass GymPreprocessing(object):\n  """"""A Wrapper class around Gym environments.""""""\n\n  def __init__(self, environment):\n    self.environment = environment\n    self.game_over = False\n\n  @property\n  def observation_space(self):\n    return self.environment.observation_space\n\n  @property\n  def action_space(self):\n    return self.environment.action_space\n\n  @property\n  def reward_range(self):\n    return self.environment.reward_range\n\n  @property\n  def metadata(self):\n    return self.environment.metadata\n\n  def reset(self):\n    return self.environment.reset()\n\n  def step(self, action):\n    observation, reward, game_over, info = self.environment.step(action)\n    self.game_over = game_over\n    return observation, reward, game_over, info\n'"
dopamine/discrete_domains/iteration_statistics.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A class for storing iteration-specific metrics.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass IterationStatistics(object):\n  """"""A class for storing iteration-specific metrics.\n\n  The internal format is as follows: we maintain a mapping from keys to lists.\n  Each list contains all the values corresponding to the given key.\n\n  For example, self.data_lists[\'train_episode_returns\'] might contain the\n    per-episode returns achieved during this iteration.\n\n  Attributes:\n    data_lists: dict mapping each metric_name (str) to a list of said metric\n      across episodes.\n  """"""\n\n  def __init__(self):\n    self.data_lists = {}\n\n  def append(self, data_pairs):\n    """"""Add the given values to their corresponding key-indexed lists.\n\n    Args:\n      data_pairs: A dictionary of key-value pairs to be recorded.\n    """"""\n    for key, value in data_pairs.items():\n      if key not in self.data_lists:\n        self.data_lists[key] = []\n      self.data_lists[key].append(value)\n'"
dopamine/discrete_domains/logger.py,9,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A lightweight logging mechanism for dopamine agents.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport pickle\nimport tensorflow.compat.v1 as tf\n\n\nCHECKPOINT_DURATION = 4\n\n\nclass Logger(object):\n  """"""Class for maintaining a dictionary of data to log.""""""\n\n  def __init__(self, logging_dir):\n    """"""Initializes Logger.\n\n    Args:\n      logging_dir: str, Directory to which logs are written.\n    """"""\n    # Dict used by logger to store data.\n    self.data = {}\n    self._logging_enabled = True\n\n    if not logging_dir:\n      tf.logging.info(\'Logging directory not specified, will not log.\')\n      self._logging_enabled = False\n      return\n    # Try to create logging directory.\n    try:\n      tf.gfile.MakeDirs(logging_dir)\n    except tf.errors.PermissionDeniedError:\n      # If it already exists, ignore exception.\n      pass\n    if not tf.gfile.Exists(logging_dir):\n      tf.logging.warning(\n          \'Could not create directory %s, logging will be disabled.\',\n          logging_dir)\n      self._logging_enabled = False\n      return\n    self._logging_dir = logging_dir\n\n  def __setitem__(self, key, value):\n    """"""This method will set an entry at key with value in the dictionary.\n\n    It will effectively overwrite any previous data at the same key.\n\n    Args:\n      key: str, indicating key where to write the entry.\n      value: A python object to store.\n    """"""\n    if self._logging_enabled:\n      self.data[key] = value\n\n  def _generate_filename(self, filename_prefix, iteration_number):\n    filename = \'{}_{}\'.format(filename_prefix, iteration_number)\n    return os.path.join(self._logging_dir, filename)\n\n  def log_to_file(self, filename_prefix, iteration_number):\n    """"""Save the pickled dictionary to a file.\n\n    Args:\n      filename_prefix: str, name of the file to use (without iteration\n        number).\n      iteration_number: int, the iteration number, appended to the end of\n        filename_prefix.\n    """"""\n    if not self._logging_enabled:\n      tf.logging.warning(\'Logging is disabled.\')\n      return\n    log_file = self._generate_filename(filename_prefix, iteration_number)\n    with tf.gfile.GFile(log_file, \'w\') as fout:\n      pickle.dump(self.data, fout, protocol=pickle.HIGHEST_PROTOCOL)\n    # After writing a checkpoint file, we garbage collect the log file\n    # that is CHECKPOINT_DURATION versions old.\n    stale_iteration_number = iteration_number - CHECKPOINT_DURATION\n    if stale_iteration_number >= 0:\n      stale_file = self._generate_filename(filename_prefix,\n                                           stale_iteration_number)\n      try:\n        tf.gfile.Remove(stale_file)\n      except tf.errors.NotFoundError:\n        # Ignore if file not found.\n        pass\n\n  def is_logging_enabled(self):\n    """"""Return if logging is enabled.""""""\n    return self._logging_enabled\n'"
dopamine/discrete_domains/run_experiment.py,24,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Module defining classes and helper methods for general agents.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\n\nfrom dopamine.agents.dqn import dqn_agent\nfrom dopamine.agents.implicit_quantile import implicit_quantile_agent\nfrom dopamine.agents.rainbow import rainbow_agent\nfrom dopamine.discrete_domains import atari_lib\nfrom dopamine.discrete_domains import checkpointer\nfrom dopamine.discrete_domains import iteration_statistics\nfrom dopamine.discrete_domains import logger\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nimport gin.tf\n\n\ndef load_gin_configs(gin_files, gin_bindings):\n  """"""Loads gin configuration files.\n\n  Args:\n    gin_files: list, of paths to the gin configuration files for this\n      experiment.\n    gin_bindings: list, of gin parameter bindings to override the values in\n      the config files.\n  """"""\n  gin.parse_config_files_and_bindings(gin_files,\n                                      bindings=gin_bindings,\n                                      skip_unknown=False)\n\n\n@gin.configurable\ndef create_agent(sess, environment, agent_name=None, summary_writer=None,\n                 debug_mode=False):\n  """"""Creates an agent.\n\n  Args:\n    sess: A `tf.Session` object for running associated ops.\n    environment: A gym environment (e.g. Atari 2600).\n    agent_name: str, name of the agent to create.\n    summary_writer: A Tensorflow summary writer to pass to the agent\n      for in-agent training statistics in Tensorboard.\n    debug_mode: bool, whether to output Tensorboard summaries. If set to true,\n      the agent will output in-episode statistics to Tensorboard. Disabled by\n      default as this results in slower training.\n\n  Returns:\n    agent: An RL agent.\n\n  Raises:\n    ValueError: If `agent_name` is not in supported list.\n  """"""\n  assert agent_name is not None\n  if not debug_mode:\n    summary_writer = None\n  if agent_name == \'dqn\':\n    return dqn_agent.DQNAgent(sess, num_actions=environment.action_space.n,\n                              summary_writer=summary_writer)\n  elif agent_name == \'rainbow\':\n    return rainbow_agent.RainbowAgent(\n        sess, num_actions=environment.action_space.n,\n        summary_writer=summary_writer)\n  elif agent_name == \'implicit_quantile\':\n    return implicit_quantile_agent.ImplicitQuantileAgent(\n        sess, num_actions=environment.action_space.n,\n        summary_writer=summary_writer)\n  else:\n    raise ValueError(\'Unknown agent: {}\'.format(agent_name))\n\n\n@gin.configurable\ndef create_runner(base_dir, schedule=\'continuous_train_and_eval\'):\n  """"""Creates an experiment Runner.\n\n  Args:\n    base_dir: str, base directory for hosting all subdirectories.\n    schedule: string, which type of Runner to use.\n\n  Returns:\n    runner: A `Runner` like object.\n\n  Raises:\n    ValueError: When an unknown schedule is encountered.\n  """"""\n  assert base_dir is not None\n  # Continuously runs training and evaluation until max num_iterations is hit.\n  if schedule == \'continuous_train_and_eval\':\n    return Runner(base_dir, create_agent)\n  # Continuously runs training until max num_iterations is hit.\n  elif schedule == \'continuous_train\':\n    return TrainRunner(base_dir, create_agent)\n  else:\n    raise ValueError(\'Unknown schedule: {}\'.format(schedule))\n\n\n@gin.configurable\nclass Runner(object):\n  """"""Object that handles running Dopamine experiments.\n\n  Here we use the term \'experiment\' to mean simulating interactions between the\n  agent and the environment and reporting some statistics pertaining to these\n  interactions.\n\n  A simple scenario to train a DQN agent is as follows:\n\n  ```python\n  import dopamine.discrete_domains.atari_lib\n  base_dir = \'/tmp/simple_example\'\n  def create_agent(sess, environment):\n    return dqn_agent.DQNAgent(sess, num_actions=environment.action_space.n)\n  runner = Runner(base_dir, create_agent, atari_lib.create_atari_environment)\n  runner.run()\n  ```\n  """"""\n\n  def __init__(self,\n               base_dir,\n               create_agent_fn,\n               create_environment_fn=atari_lib.create_atari_environment,\n               checkpoint_file_prefix=\'ckpt\',\n               logging_file_prefix=\'log\',\n               log_every_n=1,\n               num_iterations=200,\n               training_steps=250000,\n               evaluation_steps=125000,\n               max_steps_per_episode=27000):\n    """"""Initialize the Runner object in charge of running a full experiment.\n\n    Args:\n      base_dir: str, the base directory to host all required sub-directories.\n      create_agent_fn: A function that takes as args a Tensorflow session and an\n        environment, and returns an agent.\n      create_environment_fn: A function which receives a problem name and\n        creates a Gym environment for that problem (e.g. an Atari 2600 game).\n      checkpoint_file_prefix: str, the prefix to use for checkpoint files.\n      logging_file_prefix: str, prefix to use for the log files.\n      log_every_n: int, the frequency for writing logs.\n      num_iterations: int, the iteration number threshold (must be greater than\n        start_iteration).\n      training_steps: int, the number of training steps to perform.\n      evaluation_steps: int, the number of evaluation steps to perform.\n      max_steps_per_episode: int, maximum number of steps after which an episode\n        terminates.\n\n    This constructor will take the following actions:\n    - Initialize an environment.\n    - Initialize a `tf.Session`.\n    - Initialize a logger.\n    - Initialize an agent.\n    - Reload from the latest checkpoint, if available, and initialize the\n      Checkpointer object.\n    """"""\n    assert base_dir is not None\n    self._logging_file_prefix = logging_file_prefix\n    self._log_every_n = log_every_n\n    self._num_iterations = num_iterations\n    self._training_steps = training_steps\n    self._evaluation_steps = evaluation_steps\n    self._max_steps_per_episode = max_steps_per_episode\n    self._base_dir = base_dir\n    self._create_directories()\n    self._summary_writer = tf.summary.FileWriter(self._base_dir)\n\n    self._environment = create_environment_fn()\n    config = tf.ConfigProto(allow_soft_placement=True)\n    # Allocate only subset of the GPU memory as needed which allows for running\n    # multiple agents/workers on the same GPU.\n    config.gpu_options.allow_growth = True\n    # Set up a session and initialize variables.\n    self._sess = tf.Session(\'\', config=config)\n    self._agent = create_agent_fn(self._sess, self._environment,\n                                  summary_writer=self._summary_writer)\n    self._summary_writer.add_graph(graph=tf.get_default_graph())\n    self._sess.run(tf.global_variables_initializer())\n\n    self._initialize_checkpointer_and_maybe_resume(checkpoint_file_prefix)\n\n  def _create_directories(self):\n    """"""Create necessary sub-directories.""""""\n    self._checkpoint_dir = os.path.join(self._base_dir, \'checkpoints\')\n    self._logger = logger.Logger(os.path.join(self._base_dir, \'logs\'))\n\n  def _initialize_checkpointer_and_maybe_resume(self, checkpoint_file_prefix):\n    """"""Reloads the latest checkpoint if it exists.\n\n    This method will first create a `Checkpointer` object and then call\n    `checkpointer.get_latest_checkpoint_number` to determine if there is a valid\n    checkpoint in self._checkpoint_dir, and what the largest file number is.\n    If a valid checkpoint file is found, it will load the bundled data from this\n    file and will pass it to the agent for it to reload its data.\n    If the agent is able to successfully unbundle, this method will verify that\n    the unbundled data contains the keys,\'logs\' and \'current_iteration\'. It will\n    then load the `Logger`\'s data from the bundle, and will return the iteration\n    number keyed by \'current_iteration\' as one of the return values (along with\n    the `Checkpointer` object).\n\n    Args:\n      checkpoint_file_prefix: str, the checkpoint file prefix.\n\n    Returns:\n      start_iteration: int, the iteration number to start the experiment from.\n      experiment_checkpointer: `Checkpointer` object for the experiment.\n    """"""\n    self._checkpointer = checkpointer.Checkpointer(self._checkpoint_dir,\n                                                   checkpoint_file_prefix)\n    self._start_iteration = 0\n    # Check if checkpoint exists. Note that the existence of checkpoint 0 means\n    # that we have finished iteration 0 (so we will start from iteration 1).\n    latest_checkpoint_version = checkpointer.get_latest_checkpoint_number(\n        self._checkpoint_dir)\n    if latest_checkpoint_version >= 0:\n      experiment_data = self._checkpointer.load_checkpoint(\n          latest_checkpoint_version)\n      if self._agent.unbundle(\n          self._checkpoint_dir, latest_checkpoint_version, experiment_data):\n        if experiment_data is not None:\n          assert \'logs\' in experiment_data\n          assert \'current_iteration\' in experiment_data\n          self._logger.data = experiment_data[\'logs\']\n          self._start_iteration = experiment_data[\'current_iteration\'] + 1\n        tf.logging.info(\'Reloaded checkpoint and will start from iteration %d\',\n                        self._start_iteration)\n\n  def _initialize_episode(self):\n    """"""Initialization for a new episode.\n\n    Returns:\n      action: int, the initial action chosen by the agent.\n    """"""\n    initial_observation = self._environment.reset()\n    return self._agent.begin_episode(initial_observation)\n\n  def _run_one_step(self, action):\n    """"""Executes a single step in the environment.\n\n    Args:\n      action: int, the action to perform in the environment.\n\n    Returns:\n      The observation, reward, and is_terminal values returned from the\n        environment.\n    """"""\n    observation, reward, is_terminal, _ = self._environment.step(action)\n    return observation, reward, is_terminal\n\n  def _end_episode(self, reward):\n    """"""Finalizes an episode run.\n\n    Args:\n      reward: float, the last reward from the environment.\n    """"""\n    self._agent.end_episode(reward)\n\n  def _run_one_episode(self):\n    """"""Executes a full trajectory of the agent interacting with the environment.\n\n    Returns:\n      The number of steps taken and the total reward.\n    """"""\n    step_number = 0\n    total_reward = 0.\n\n    action = self._initialize_episode()\n    is_terminal = False\n\n    # Keep interacting until we reach a terminal state.\n    while True:\n      observation, reward, is_terminal = self._run_one_step(action)\n\n      total_reward += reward\n      step_number += 1\n\n      # Perform reward clipping.\n      reward = np.clip(reward, -1, 1)\n\n      if (self._environment.game_over or\n          step_number == self._max_steps_per_episode):\n        # Stop the run loop once we reach the true end of episode.\n        break\n      elif is_terminal:\n        # If we lose a life but the episode is not over, signal an artificial\n        # end of episode to the agent.\n        self._agent.end_episode(reward)\n        action = self._agent.begin_episode(observation)\n      else:\n        action = self._agent.step(reward, observation)\n\n    self._end_episode(reward)\n\n    return step_number, total_reward\n\n  def _run_one_phase(self, min_steps, statistics, run_mode_str):\n    """"""Runs the agent/environment loop until a desired number of steps.\n\n    We follow the Machado et al., 2017 convention of running full episodes,\n    and terminating once we\'ve run a minimum number of steps.\n\n    Args:\n      min_steps: int, minimum number of steps to generate in this phase.\n      statistics: `IterationStatistics` object which records the experimental\n        results.\n      run_mode_str: str, describes the run mode for this agent.\n\n    Returns:\n      Tuple containing the number of steps taken in this phase (int), the sum of\n        returns (float), and the number of episodes performed (int).\n    """"""\n    step_count = 0\n    num_episodes = 0\n    sum_returns = 0.\n\n    while step_count < min_steps:\n      episode_length, episode_return = self._run_one_episode()\n      statistics.append({\n          \'{}_episode_lengths\'.format(run_mode_str): episode_length,\n          \'{}_episode_returns\'.format(run_mode_str): episode_return\n      })\n      step_count += episode_length\n      sum_returns += episode_return\n      num_episodes += 1\n      # We use sys.stdout.write instead of tf.logging so as to flush frequently\n      # without generating a line break.\n      sys.stdout.write(\'Steps executed: {} \'.format(step_count) +\n                       \'Episode length: {} \'.format(episode_length) +\n                       \'Return: {}\\r\'.format(episode_return))\n      sys.stdout.flush()\n    return step_count, sum_returns, num_episodes\n\n  def _run_train_phase(self, statistics):\n    """"""Run training phase.\n\n    Args:\n      statistics: `IterationStatistics` object which records the experimental\n        results. Note - This object is modified by this method.\n\n    Returns:\n      num_episodes: int, The number of episodes run in this phase.\n      average_reward: The average reward generated in this phase.\n    """"""\n    # Perform the training phase, during which the agent learns.\n    self._agent.eval_mode = False\n    start_time = time.time()\n    number_steps, sum_returns, num_episodes = self._run_one_phase(\n        self._training_steps, statistics, \'train\')\n    average_return = sum_returns / num_episodes if num_episodes > 0 else 0.0\n    statistics.append({\'train_average_return\': average_return})\n    time_delta = time.time() - start_time\n    tf.logging.info(\'Average undiscounted return per training episode: %.2f\',\n                    average_return)\n    tf.logging.info(\'Average training steps per second: %.2f\',\n                    number_steps / time_delta)\n    return num_episodes, average_return\n\n  def _run_eval_phase(self, statistics):\n    """"""Run evaluation phase.\n\n    Args:\n      statistics: `IterationStatistics` object which records the experimental\n        results. Note - This object is modified by this method.\n\n    Returns:\n      num_episodes: int, The number of episodes run in this phase.\n      average_reward: float, The average reward generated in this phase.\n    """"""\n    # Perform the evaluation phase -- no learning.\n    self._agent.eval_mode = True\n    _, sum_returns, num_episodes = self._run_one_phase(\n        self._evaluation_steps, statistics, \'eval\')\n    average_return = sum_returns / num_episodes if num_episodes > 0 else 0.0\n    tf.logging.info(\'Average undiscounted return per evaluation episode: %.2f\',\n                    average_return)\n    statistics.append({\'eval_average_return\': average_return})\n    return num_episodes, average_return\n\n  def _run_one_iteration(self, iteration):\n    """"""Runs one iteration of agent/environment interaction.\n\n    An iteration involves running several episodes until a certain number of\n    steps are obtained. The interleaving of train/eval phases implemented here\n    are to match the implementation of (Mnih et al., 2015).\n\n    Args:\n      iteration: int, current iteration number, used as a global_step for saving\n        Tensorboard summaries.\n\n    Returns:\n      A dict containing summary statistics for this iteration.\n    """"""\n    statistics = iteration_statistics.IterationStatistics()\n    tf.logging.info(\'Starting iteration %d\', iteration)\n    num_episodes_train, average_reward_train = self._run_train_phase(\n        statistics)\n    num_episodes_eval, average_reward_eval = self._run_eval_phase(\n        statistics)\n\n    self._save_tensorboard_summaries(iteration, num_episodes_train,\n                                     average_reward_train, num_episodes_eval,\n                                     average_reward_eval)\n    return statistics.data_lists\n\n  def _save_tensorboard_summaries(self, iteration,\n                                  num_episodes_train,\n                                  average_reward_train,\n                                  num_episodes_eval,\n                                  average_reward_eval):\n    """"""Save statistics as tensorboard summaries.\n\n    Args:\n      iteration: int, The current iteration number.\n      num_episodes_train: int, number of training episodes run.\n      average_reward_train: float, The average training reward.\n      num_episodes_eval: int, number of evaluation episodes run.\n      average_reward_eval: float, The average evaluation reward.\n    """"""\n    summary = tf.Summary(value=[\n        tf.Summary.Value(tag=\'Train/NumEpisodes\',\n                         simple_value=num_episodes_train),\n        tf.Summary.Value(tag=\'Train/AverageReturns\',\n                         simple_value=average_reward_train),\n        tf.Summary.Value(tag=\'Eval/NumEpisodes\',\n                         simple_value=num_episodes_eval),\n        tf.Summary.Value(tag=\'Eval/AverageReturns\',\n                         simple_value=average_reward_eval)\n    ])\n    self._summary_writer.add_summary(summary, iteration)\n\n  def _log_experiment(self, iteration, statistics):\n    """"""Records the results of the current iteration.\n\n    Args:\n      iteration: int, iteration number.\n      statistics: `IterationStatistics` object containing statistics to log.\n    """"""\n    self._logger[\'iteration_{:d}\'.format(iteration)] = statistics\n    if iteration % self._log_every_n == 0:\n      self._logger.log_to_file(self._logging_file_prefix, iteration)\n\n  def _checkpoint_experiment(self, iteration):\n    """"""Checkpoint experiment data.\n\n    Args:\n      iteration: int, iteration number for checkpointing.\n    """"""\n    experiment_data = self._agent.bundle_and_checkpoint(self._checkpoint_dir,\n                                                        iteration)\n    if experiment_data:\n      experiment_data[\'current_iteration\'] = iteration\n      experiment_data[\'logs\'] = self._logger.data\n      self._checkpointer.save_checkpoint(iteration, experiment_data)\n\n  def run_experiment(self):\n    """"""Runs a full experiment, spread over multiple iterations.""""""\n    tf.logging.info(\'Beginning training...\')\n    if self._num_iterations <= self._start_iteration:\n      tf.logging.warning(\'num_iterations (%d) < start_iteration(%d)\',\n                         self._num_iterations, self._start_iteration)\n      return\n\n    for iteration in range(self._start_iteration, self._num_iterations):\n      statistics = self._run_one_iteration(iteration)\n      self._log_experiment(iteration, statistics)\n      self._checkpoint_experiment(iteration)\n\n\n@gin.configurable\nclass TrainRunner(Runner):\n  """"""Object that handles running experiments.\n\n  The `TrainRunner` differs from the base `Runner` class in that it does not\n  the evaluation phase. Checkpointing and logging for the train phase are\n  preserved as before.\n  """"""\n\n  def __init__(self, base_dir, create_agent_fn,\n               create_environment_fn=atari_lib.create_atari_environment):\n    """"""Initialize the TrainRunner object in charge of running a full experiment.\n\n    Args:\n      base_dir: str, the base directory to host all required sub-directories.\n      create_agent_fn: A function that takes as args a Tensorflow session and an\n        environment, and returns an agent.\n      create_environment_fn: A function which receives a problem name and\n        creates a Gym environment for that problem (e.g. an Atari 2600 game).\n    """"""\n    tf.logging.info(\'Creating TrainRunner ...\')\n    super(TrainRunner, self).__init__(base_dir, create_agent_fn,\n                                      create_environment_fn)\n    self._agent.eval_mode = False\n\n  def _run_one_iteration(self, iteration):\n    """"""Runs one iteration of agent/environment interaction.\n\n    An iteration involves running several episodes until a certain number of\n    steps are obtained. This method differs from the `_run_one_iteration` method\n    in the base `Runner` class in that it only runs the train phase.\n\n    Args:\n      iteration: int, current iteration number, used as a global_step for saving\n        Tensorboard summaries.\n\n    Returns:\n      A dict containing summary statistics for this iteration.\n    """"""\n    statistics = iteration_statistics.IterationStatistics()\n    num_episodes_train, average_reward_train = self._run_train_phase(\n        statistics)\n\n    self._save_tensorboard_summaries(iteration, num_episodes_train,\n                                     average_reward_train)\n    return statistics.data_lists\n\n  def _save_tensorboard_summaries(self, iteration, num_episodes,\n                                  average_reward):\n    """"""Save statistics as tensorboard summaries.""""""\n    summary = tf.Summary(value=[\n        tf.Summary.Value(tag=\'Train/NumEpisodes\', simple_value=num_episodes),\n        tf.Summary.Value(\n            tag=\'Train/AverageReturns\', simple_value=average_reward),\n    ])\n    self._summary_writer.add_summary(summary, iteration)\n'"
dopamine/discrete_domains/train.py,1,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nr""""""The entry point for running a Dopamine agent.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom absl import app\nfrom absl import flags\n\nfrom dopamine.discrete_domains import run_experiment\n\nimport tensorflow.compat.v1 as tf\n\n\nflags.DEFINE_string(\'base_dir\', None,\n                    \'Base directory to host all required sub-directories.\')\nflags.DEFINE_multi_string(\n    \'gin_files\', [], \'List of paths to gin configuration files (e.g.\'\n    \'""dopamine/agents/dqn/dqn.gin"").\')\nflags.DEFINE_multi_string(\n    \'gin_bindings\', [],\n    \'Gin bindings to override the values set in the config files \'\n    \'(e.g. ""DQNAgent.epsilon_train=0.1"",\'\n    \'      ""create_environment.game_name=""Pong"""").\')\n\nFLAGS = flags.FLAGS\n\n\ndef main(unused_argv):\n  """"""Main method.\n\n  Args:\n    unused_argv: Arguments (unused).\n  """"""\n  tf.logging.set_verbosity(tf.logging.INFO)\n  run_experiment.load_gin_configs(FLAGS.gin_files, FLAGS.gin_bindings)\n  runner = run_experiment.create_runner(FLAGS.base_dir)\n  runner.run_experiment()\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'base_dir\')\n  app.run(main)\n'"
dopamine/replay_memory/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
dopamine/replay_memory/circular_replay_buffer.py,27,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""The standard DQN replay memory.\n\nThis implementation is an out-of-graph replay memory + in-graph wrapper. It\nsupports vanilla n-step updates of the form typically found in the literature,\ni.e. where rewards are accumulated for n steps and the intermediate trajectory\nis not exposed to the agent. This does not allow, for example, performing\noff-policy corrections.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport gzip\nimport math\nimport os\nimport pickle\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nimport gin.tf\nfrom tensorflow.contrib import staging as contrib_staging\n\n# Defines a type describing part of the tuple returned by the replay\n# memory. Each element of the tuple is a tensor of shape [batch, ...] where\n# ... is defined the \'shape\' field of ReplayElement. The tensor type is\n# given by the \'type\' field. The \'name\' field is for convenience and ease of\n# debugging.\nReplayElement = (\n    collections.namedtuple(\'shape_type\', [\'name\', \'shape\', \'type\']))\n\n# A prefix that can not collide with variable names for checkpoint files.\nSTORE_FILENAME_PREFIX = \'$store$_\'\n\n# This constant determines how many iterations a checkpoint is kept for.\nCHECKPOINT_DURATION = 4\n\n\ndef invalid_range(cursor, replay_capacity, stack_size, update_horizon):\n  """"""Returns a array with the indices of cursor-related invalid transitions.\n\n  There are update_horizon + stack_size invalid indices:\n    - The update_horizon indices before the cursor, because we do not have a\n      valid N-step transition (including the next state).\n    - The stack_size indices on or immediately after the cursor.\n  If N = update_horizon, K = stack_size, and the cursor is at c, invalid\n  indices are:\n    c - N, c - N + 1, ..., c, c + 1, ..., c + K - 1.\n\n  It handles special cases in a circular buffer in the beginning and the end.\n\n  Args:\n    cursor: int, the position of the cursor.\n    replay_capacity: int, the size of the replay memory.\n    stack_size: int, the size of the stacks returned by the replay memory.\n    update_horizon: int, the agent\'s update horizon.\n  Returns:\n    np.array of size stack_size with the invalid indices.\n  """"""\n  assert cursor < replay_capacity\n  return np.array(\n      [(cursor - update_horizon + i) % replay_capacity\n       for i in range(stack_size + update_horizon)])\n\n\nclass OutOfGraphReplayBuffer(object):\n  """"""A simple out-of-graph Replay Buffer.\n\n  Stores transitions, state, action, reward, next_state, terminal (and any\n  extra contents specified) in a circular buffer and provides a uniform\n  transition sampling function.\n\n  When the states consist of stacks of observations storing the states is\n  inefficient. This class writes observations and constructs the stacked states\n  at sample time.\n\n  Attributes:\n    add_count: int, counter of how many transitions have been added (including\n      the blank ones at the beginning of an episode).\n    invalid_range: np.array, an array with the indices of cursor-related invalid\n      transitions\n  """"""\n\n  def __init__(self,\n               observation_shape,\n               stack_size,\n               replay_capacity,\n               batch_size,\n               update_horizon=1,\n               gamma=0.99,\n               max_sample_attempts=1000,\n               extra_storage_types=None,\n               observation_dtype=np.uint8,\n               terminal_dtype=np.uint8,\n               action_shape=(),\n               action_dtype=np.int32,\n               reward_shape=(),\n               reward_dtype=np.float32):\n    """"""Initializes OutOfGraphReplayBuffer.\n\n    Args:\n      observation_shape: tuple of ints.\n      stack_size: int, number of frames to use in state stack.\n      replay_capacity: int, number of transitions to keep in memory.\n      batch_size: int.\n      update_horizon: int, length of update (\'n\' in n-step update).\n      gamma: int, the discount factor.\n      max_sample_attempts: int, the maximum number of attempts allowed to\n        get a sample.\n      extra_storage_types: list of ReplayElements defining the type of the extra\n        contents that will be stored and returned by sample_transition_batch.\n      observation_dtype: np.dtype, type of the observations. Defaults to\n        np.uint8 for Atari 2600.\n      terminal_dtype: np.dtype, type of the terminals. Defaults to np.uint8 for\n        Atari 2600.\n      action_shape: tuple of ints, the shape for the action vector. Empty tuple\n        means the action is a scalar.\n      action_dtype: np.dtype, type of elements in the action.\n      reward_shape: tuple of ints, the shape of the reward vector. Empty tuple\n        means the reward is a scalar.\n      reward_dtype: np.dtype, type of elements in the reward.\n\n    Raises:\n      ValueError: If replay_capacity is too small to hold at least one\n        transition.\n    """"""\n    assert isinstance(observation_shape, tuple)\n    if replay_capacity < update_horizon + stack_size:\n      raise ValueError(\'There is not enough capacity to cover \'\n                       \'update_horizon and stack_size.\')\n\n    tf.logging.info(\n        \'Creating a %s replay memory with the following parameters:\',\n        self.__class__.__name__)\n    tf.logging.info(\'\\t observation_shape: %s\', str(observation_shape))\n    tf.logging.info(\'\\t observation_dtype: %s\', str(observation_dtype))\n    tf.logging.info(\'\\t terminal_dtype: %s\', str(terminal_dtype))\n    tf.logging.info(\'\\t stack_size: %d\', stack_size)\n    tf.logging.info(\'\\t replay_capacity: %d\', replay_capacity)\n    tf.logging.info(\'\\t batch_size: %d\', batch_size)\n    tf.logging.info(\'\\t update_horizon: %d\', update_horizon)\n    tf.logging.info(\'\\t gamma: %f\', gamma)\n\n    self._action_shape = action_shape\n    self._action_dtype = action_dtype\n    self._reward_shape = reward_shape\n    self._reward_dtype = reward_dtype\n    self._observation_shape = observation_shape\n    self._stack_size = stack_size\n    self._state_shape = self._observation_shape + (self._stack_size,)\n    self._replay_capacity = replay_capacity\n    self._batch_size = batch_size\n    self._update_horizon = update_horizon\n    self._gamma = gamma\n    self._observation_dtype = observation_dtype\n    self._terminal_dtype = terminal_dtype\n    self._max_sample_attempts = max_sample_attempts\n    if extra_storage_types:\n      self._extra_storage_types = extra_storage_types\n    else:\n      self._extra_storage_types = []\n    self._create_storage()\n    self.add_count = np.array(0)\n    self.invalid_range = np.zeros((self._stack_size))\n    # When the horizon is > 1, we compute the sum of discounted rewards as a dot\n    # product using the precomputed vector <gamma^0, gamma^1, ..., gamma^{n-1}>.\n    self._cumulative_discount_vector = np.array(\n        [math.pow(self._gamma, n) for n in range(update_horizon)],\n        dtype=np.float32)\n\n  def _create_storage(self):\n    """"""Creates the numpy arrays used to store transitions.\n    """"""\n    self._store = {}\n    for storage_element in self.get_storage_signature():\n      array_shape = [self._replay_capacity] + list(storage_element.shape)\n      self._store[storage_element.name] = np.empty(\n          array_shape, dtype=storage_element.type)\n\n  def get_add_args_signature(self):\n    """"""The signature of the add function.\n\n    Note - Derived classes may return a different signature.\n\n    Returns:\n      list of ReplayElements defining the type of the argument signature needed\n        by the add function.\n    """"""\n    return self.get_storage_signature()\n\n  def get_storage_signature(self):\n    """"""Returns a default list of elements to be stored in this replay memory.\n\n    Note - Derived classes may return a different signature.\n\n    Returns:\n      list of ReplayElements defining the type of the contents stored.\n    """"""\n    storage_elements = [\n        ReplayElement(\'observation\', self._observation_shape,\n                      self._observation_dtype),\n        ReplayElement(\'action\', self._action_shape, self._action_dtype),\n        ReplayElement(\'reward\', self._reward_shape, self._reward_dtype),\n        ReplayElement(\'terminal\', (), self._terminal_dtype)\n    ]\n\n    for extra_replay_element in self._extra_storage_types:\n      storage_elements.append(extra_replay_element)\n    return storage_elements\n\n  def _add_zero_transition(self):\n    """"""Adds a padding transition filled with zeros (Used in episode beginnings).\n    """"""\n    zero_transition = []\n    for element_type in self.get_add_args_signature():\n      zero_transition.append(\n          np.zeros(element_type.shape, dtype=element_type.type))\n    self._add(*zero_transition)\n\n  def add(self, observation, action, reward, terminal, *args):\n    """"""Adds a transition to the replay memory.\n\n    This function checks the types and handles the padding at the beginning of\n    an episode. Then it calls the _add function.\n\n    Since the next_observation in the transition will be the observation added\n    next there is no need to pass it.\n\n    If the replay memory is at capacity the oldest transition will be discarded.\n\n    Args:\n      observation: np.array with shape observation_shape.\n      action: int, the action in the transition.\n      reward: float, the reward received in the transition.\n      terminal: np.dtype, acts as a boolean indicating whether the transition\n                was terminal (1) or not (0).\n      *args: extra contents with shapes and dtypes according to\n        extra_storage_types.\n    """"""\n    self._check_add_types(observation, action, reward, terminal, *args)\n    if self.is_empty() or self._store[\'terminal\'][self.cursor() - 1] == 1:\n      for _ in range(self._stack_size - 1):\n        # Child classes can rely on the padding transitions being filled with\n        # zeros. This is useful when there is a priority argument.\n        self._add_zero_transition()\n    self._add(observation, action, reward, terminal, *args)\n\n  def _add(self, *args):\n    """"""Internal add method to add to the storage arrays.\n\n    Args:\n      *args: All the elements in a transition.\n    """"""\n    self._check_args_length(*args)\n    transition = {e.name: args[idx]\n                  for idx, e in enumerate(self.get_add_args_signature())}\n    self._add_transition(transition)\n\n  def _add_transition(self, transition):\n    """"""Internal add method to add transition dictionary to storage arrays.\n\n    Args:\n      transition: The dictionary of names and values of the transition\n                  to add to the storage.\n    """"""\n    cursor = self.cursor()\n    for arg_name in transition:\n      self._store[arg_name][cursor] = transition[arg_name]\n\n    self.add_count += 1\n    self.invalid_range = invalid_range(\n        self.cursor(), self._replay_capacity, self._stack_size,\n        self._update_horizon)\n\n  def _check_args_length(self, *args):\n    """"""Check if args passed to the add method have the same length as storage.\n\n    Args:\n      *args: Args for elements used in storage.\n\n    Raises:\n      ValueError: If args have wrong length.\n    """"""\n    if len(args) != len(self.get_add_args_signature()):\n      raise ValueError(\'Add expects {} elements, received {}\'.format(\n          len(self.get_add_args_signature()), len(args)))\n\n  def _check_add_types(self, *args):\n    """"""Checks if args passed to the add method match those of the storage.\n\n    Args:\n      *args: Args whose types need to be validated.\n\n    Raises:\n      ValueError: If args have wrong shape or dtype.\n    """"""\n    self._check_args_length(*args)\n    for arg_element, store_element in zip(args, self.get_add_args_signature()):\n      if isinstance(arg_element, np.ndarray):\n        arg_shape = arg_element.shape\n      elif isinstance(arg_element, tuple) or isinstance(arg_element, list):\n        # TODO(b/80536437). This is not efficient when arg_element is a list.\n        arg_shape = np.array(arg_element).shape\n      else:\n        # Assume it is scalar.\n        arg_shape = tuple()\n      store_element_shape = tuple(store_element.shape)\n      if arg_shape != store_element_shape:\n        raise ValueError(\'arg has shape {}, expected {}\'.format(\n            arg_shape, store_element_shape))\n\n  def is_empty(self):\n    """"""Is the Replay Buffer empty?""""""\n    return self.add_count == 0\n\n  def is_full(self):\n    """"""Is the Replay Buffer full?""""""\n    return self.add_count >= self._replay_capacity\n\n  def cursor(self):\n    """"""Index to the location where the next transition will be written.""""""\n    return self.add_count % self._replay_capacity\n\n  def get_range(self, array, start_index, end_index):\n    """"""Returns the range of array at the index handling wraparound if necessary.\n\n    Args:\n      array: np.array, the array to get the stack from.\n      start_index: int, index to the start of the range to be returned. Range\n        will wraparound if start_index is smaller than 0.\n      end_index: int, exclusive end index. Range will wraparound if end_index\n        exceeds replay_capacity.\n\n    Returns:\n      np.array, with shape [end_index - start_index, array.shape[1:]].\n    """"""\n    assert end_index > start_index, \'end_index must be larger than start_index\'\n    assert end_index >= 0\n    assert start_index < self._replay_capacity\n    if not self.is_full():\n      assert end_index <= self.cursor(), (\n          \'Index {} has not been added.\'.format(start_index))\n\n    # Fast slice read when there is no wraparound.\n    if start_index % self._replay_capacity < end_index % self._replay_capacity:\n      return_array = array[start_index:end_index, ...]\n    # Slow list read.\n    else:\n      indices = [(start_index + i) % self._replay_capacity\n                 for i in range(end_index - start_index)]\n      return_array = array[indices, ...]\n    return return_array\n\n  def get_observation_stack(self, index):\n    return self._get_element_stack(index, \'observation\')\n\n  def _get_element_stack(self, index, element_name):\n    state = self.get_range(self._store[element_name],\n                           index - self._stack_size + 1, index + 1)\n    # The stacking axis is 0 but the agent expects as the last axis.\n    return np.moveaxis(state, 0, -1)\n\n  def get_terminal_stack(self, index):\n    return self.get_range(self._store[\'terminal\'], index - self._stack_size + 1,\n                          index + 1)\n\n  def is_valid_transition(self, index):\n    """"""Checks if the index contains a valid transition.\n\n    Checks for collisions with the end of episodes and the current position\n    of the cursor.\n\n    Args:\n      index: int, the index to the state in the transition.\n\n    Returns:\n      Is the index valid: Boolean.\n\n    """"""\n    # Check the index is in the valid range\n    if index < 0 or index >= self._replay_capacity:\n      return False\n    if not self.is_full():\n      # The indices and next_indices must be smaller than the cursor.\n      if index >= self.cursor() - self._update_horizon:\n        return False\n      # The first few indices contain the padding states of the first episode.\n      if index < self._stack_size - 1:\n        return False\n\n    # Skip transitions that straddle the cursor.\n    if index in set(self.invalid_range):\n      return False\n\n    # If there are terminal flags in any other frame other than the last one\n    # the stack is not valid, so don\'t sample it.\n    if self.get_terminal_stack(index)[:-1].any():\n      return False\n\n    return True\n\n  def _create_batch_arrays(self, batch_size):\n    """"""Create a tuple of arrays with the type of get_transition_elements.\n\n    When using the WrappedReplayBuffer with staging enabled it is important to\n    create new arrays every sample because StaginArea keeps a pointer to the\n    returned arrays.\n\n    Args:\n      batch_size: (int) number of transitions returned. If None the default\n        batch_size will be used.\n\n    Returns:\n      Tuple of np.arrays with the shape and type of get_transition_elements.\n    """"""\n    transition_elements = self.get_transition_elements(batch_size)\n    batch_arrays = []\n    for element in transition_elements:\n      batch_arrays.append(np.empty(element.shape, dtype=element.type))\n    return tuple(batch_arrays)\n\n  def sample_index_batch(self, batch_size):\n    """"""Returns a batch of valid indices sampled uniformly.\n\n    Args:\n      batch_size: int, number of indices returned.\n\n    Returns:\n      list of ints, a batch of valid indices sampled uniformly.\n\n    Raises:\n      RuntimeError: If the batch was not constructed after maximum number of\n        tries.\n    """"""\n    if self.is_full():\n      # add_count >= self._replay_capacity > self._stack_size\n      min_id = self.cursor() - self._replay_capacity + self._stack_size - 1\n      max_id = self.cursor() - self._update_horizon\n    else:\n      # add_count < self._replay_capacity\n      min_id = self._stack_size - 1\n      max_id = self.cursor() - self._update_horizon\n      if max_id <= min_id:\n        raise RuntimeError(\'Cannot sample a batch with fewer than stack size \'\n                           \'({}) + update_horizon ({}) transitions.\'.\n                           format(self._stack_size, self._update_horizon))\n\n    indices = []\n    attempt_count = 0\n    while (len(indices) < batch_size and\n           attempt_count < self._max_sample_attempts):\n      index = np.random.randint(min_id, max_id) % self._replay_capacity\n      if self.is_valid_transition(index):\n        indices.append(index)\n      else:\n        attempt_count += 1\n    if len(indices) != batch_size:\n      raise RuntimeError(\n          \'Max sample attempts: Tried {} times but only sampled {}\'\n          \' valid indices. Batch size is {}\'.\n          format(self._max_sample_attempts, len(indices), batch_size))\n\n    return indices\n\n  def sample_transition_batch(self, batch_size=None, indices=None):\n    """"""Returns a batch of transitions (including any extra contents).\n\n    If get_transition_elements has been overridden and defines elements not\n    stored in self._store, an empty array will be returned and it will be\n    left to the child class to fill it. For example, for the child class\n    OutOfGraphPrioritizedReplayBuffer, the contents of the\n    sampling_probabilities are stored separately in a sum tree.\n\n    When the transition is terminal next_state_batch has undefined contents.\n\n    NOTE: This transition contains the indices of the sampled elements. These\n    are only valid during the call to sample_transition_batch, i.e. they may\n    be used by subclasses of this replay buffer but may point to different data\n    as soon as sampling is done.\n\n    Args:\n      batch_size: int, number of transitions returned. If None, the default\n        batch_size will be used.\n      indices: None or list of ints, the indices of every transition in the\n        batch. If None, sample the indices uniformly.\n\n    Returns:\n      transition_batch: tuple of np.arrays with the shape and type as in\n        get_transition_elements().\n\n    Raises:\n      ValueError: If an element to be sampled is missing from the replay buffer.\n    """"""\n    if batch_size is None:\n      batch_size = self._batch_size\n    if indices is None:\n      indices = self.sample_index_batch(batch_size)\n    assert len(indices) == batch_size\n\n    transition_elements = self.get_transition_elements(batch_size)\n    batch_arrays = self._create_batch_arrays(batch_size)\n    for batch_element, state_index in enumerate(indices):\n      trajectory_indices = [(state_index + j) % self._replay_capacity\n                            for j in range(self._update_horizon)]\n      trajectory_terminals = self._store[\'terminal\'][trajectory_indices]\n      is_terminal_transition = trajectory_terminals.any()\n      if not is_terminal_transition:\n        trajectory_length = self._update_horizon\n      else:\n        # np.argmax of a bool array returns the index of the first True.\n        trajectory_length = np.argmax(trajectory_terminals.astype(np.bool),\n                                      0) + 1\n      next_state_index = state_index + trajectory_length\n      trajectory_discount_vector = (\n          self._cumulative_discount_vector[:trajectory_length])\n      trajectory_rewards = self.get_range(self._store[\'reward\'], state_index,\n                                          next_state_index)\n\n      # Fill the contents of each array in the sampled batch.\n      assert len(transition_elements) == len(batch_arrays)\n      for element_array, element in zip(batch_arrays, transition_elements):\n        if element.name == \'state\':\n          element_array[batch_element] = self.get_observation_stack(state_index)\n        elif element.name == \'reward\':\n          # compute the discounted sum of rewards in the trajectory.\n          element_array[batch_element] = np.sum(\n              trajectory_discount_vector * trajectory_rewards, axis=0)\n        elif element.name == \'next_state\':\n          element_array[batch_element] = self.get_observation_stack(\n              (next_state_index) % self._replay_capacity)\n        elif element.name in (\'next_action\', \'next_reward\'):\n          element_array[batch_element] = (\n              self._store[element.name.lstrip(\'next_\')][(next_state_index) %\n                                                        self._replay_capacity])\n        elif element.name == \'terminal\':\n          element_array[batch_element] = is_terminal_transition\n        elif element.name == \'indices\':\n          element_array[batch_element] = state_index\n        elif element.name in self._store.keys():\n          element_array[batch_element] = (\n              self._store[element.name][state_index])\n        # We assume the other elements are filled in by the subclass.\n\n    return batch_arrays\n\n  def get_transition_elements(self, batch_size=None):\n    """"""Returns a \'type signature\' for sample_transition_batch.\n\n    Args:\n      batch_size: int, number of transitions returned. If None, the default\n        batch_size will be used.\n    Returns:\n      signature: A namedtuple describing the method\'s return type signature.\n    """"""\n    batch_size = self._batch_size if batch_size is None else batch_size\n\n    transition_elements = [\n        ReplayElement(\'state\', (batch_size,) + self._state_shape,\n                      self._observation_dtype),\n        ReplayElement(\'action\', (batch_size,) + self._action_shape,\n                      self._action_dtype),\n        ReplayElement(\'reward\', (batch_size,) + self._reward_shape,\n                      self._reward_dtype),\n        ReplayElement(\'next_state\', (batch_size,) + self._state_shape,\n                      self._observation_dtype),\n        ReplayElement(\'next_action\', (batch_size,) + self._action_shape,\n                      self._action_dtype),\n        ReplayElement(\'next_reward\', (batch_size,) + self._reward_shape,\n                      self._reward_dtype),\n        ReplayElement(\'terminal\', (batch_size,), self._terminal_dtype),\n        ReplayElement(\'indices\', (batch_size,), np.int32)\n    ]\n    for element in self._extra_storage_types:\n      transition_elements.append(\n          ReplayElement(element.name, (batch_size,) + tuple(element.shape),\n                        element.type))\n    return transition_elements\n\n  def _generate_filename(self, checkpoint_dir, name, suffix):\n    return os.path.join(checkpoint_dir, \'{}_ckpt.{}.gz\'.format(name, suffix))\n\n  def _return_checkpointable_elements(self):\n    """"""Return the dict of elements of the class for checkpointing.\n\n    Returns:\n      checkpointable_elements: dict containing all non private (starting with\n      _) members + all the arrays inside self._store.\n    """"""\n    checkpointable_elements = {}\n    for member_name, member in self.__dict__.items():\n      if member_name == \'_store\':\n        for array_name, array in self._store.items():\n          checkpointable_elements[STORE_FILENAME_PREFIX + array_name] = array\n      elif not member_name.startswith(\'_\'):\n        checkpointable_elements[member_name] = member\n    return checkpointable_elements\n\n  def save(self, checkpoint_dir, iteration_number):\n    """"""Save the OutOfGraphReplayBuffer attributes into a file.\n\n    This method will save all the replay buffer\'s state in a single file.\n\n    Args:\n      checkpoint_dir: str, the directory where numpy checkpoint files should be\n        saved.\n      iteration_number: int, iteration_number to use as a suffix in naming\n        numpy checkpoint files.\n    """"""\n    if not tf.gfile.Exists(checkpoint_dir):\n      return\n\n    checkpointable_elements = self._return_checkpointable_elements()\n\n    for attr in checkpointable_elements:\n      filename = self._generate_filename(checkpoint_dir, attr, iteration_number)\n      with tf.gfile.Open(filename, \'wb\') as f:\n        with gzip.GzipFile(fileobj=f) as outfile:\n          # Checkpoint the np arrays in self._store with np.save instead of\n          # pickling the dictionary is critical for file size and performance.\n          # STORE_FILENAME_PREFIX indicates that the variable is contained in\n          # self._store.\n          if attr.startswith(STORE_FILENAME_PREFIX):\n            array_name = attr[len(STORE_FILENAME_PREFIX):]\n            np.save(outfile, self._store[array_name], allow_pickle=False)\n          # Some numpy arrays might not be part of storage\n          elif isinstance(self.__dict__[attr], np.ndarray):\n            np.save(outfile, self.__dict__[attr], allow_pickle=False)\n          else:\n            pickle.dump(self.__dict__[attr], outfile)\n\n      # After writing a checkpoint file, we garbage collect the checkpoint file\n      # that is four versions old.\n      stale_iteration_number = iteration_number - CHECKPOINT_DURATION\n      if stale_iteration_number >= 0:\n        stale_filename = self._generate_filename(checkpoint_dir, attr,\n                                                 stale_iteration_number)\n        try:\n          tf.gfile.Remove(stale_filename)\n        except tf.errors.NotFoundError:\n          pass\n\n  def load(self, checkpoint_dir, suffix):\n    """"""Restores the object from bundle_dictionary and numpy checkpoints.\n\n    Args:\n      checkpoint_dir: str, the directory where to read the numpy checkpointed\n        files from.\n      suffix: str, the suffix to use in numpy checkpoint files.\n\n    Raises:\n      NotFoundError: If not all expected files are found in directory.\n    """"""\n    save_elements = self._return_checkpointable_elements()\n    # We will first make sure we have all the necessary files available to avoid\n    # loading a partially-specified (i.e. corrupted) replay buffer.\n    for attr in save_elements:\n      filename = self._generate_filename(checkpoint_dir, attr, suffix)\n      if not tf.gfile.Exists(filename):\n        raise tf.errors.NotFoundError(None, None,\n                                      \'Missing file: {}\'.format(filename))\n    # If we\'ve reached this point then we have verified that all expected files\n    # are available.\n    for attr in save_elements:\n      filename = self._generate_filename(checkpoint_dir, attr, suffix)\n      with tf.gfile.Open(filename, \'rb\') as f:\n        with gzip.GzipFile(fileobj=f) as infile:\n          if attr.startswith(STORE_FILENAME_PREFIX):\n            array_name = attr[len(STORE_FILENAME_PREFIX):]\n            self._store[array_name] = np.load(infile, allow_pickle=False)\n          elif isinstance(self.__dict__[attr], np.ndarray):\n            self.__dict__[attr] = np.load(infile, allow_pickle=False)\n          else:\n            self.__dict__[attr] = pickle.load(infile)\n\n\n@gin.configurable(blacklist=[\'observation_shape\', \'stack_size\',\n                             \'update_horizon\', \'gamma\'])\nclass WrappedReplayBuffer(object):\n  """"""Wrapper of OutOfGraphReplayBuffer with an in graph sampling mechanism.\n\n  Usage:\n    To add a transition:  call the add function.\n\n    To sample a batch:    Construct operations that depend on any of the\n                          tensors is the transition dictionary. Every sess.run\n                          that requires any of these tensors will sample a new\n                          transition.\n  """"""\n\n  def __init__(self,\n               observation_shape,\n               stack_size,\n               use_staging=True,\n               replay_capacity=1000000,\n               batch_size=32,\n               update_horizon=1,\n               gamma=0.99,\n               wrapped_memory=None,\n               max_sample_attempts=1000,\n               extra_storage_types=None,\n               observation_dtype=np.uint8,\n               terminal_dtype=np.uint8,\n               action_shape=(),\n               action_dtype=np.int32,\n               reward_shape=(),\n               reward_dtype=np.float32):\n    """"""Initializes WrappedReplayBuffer.\n\n    Args:\n      observation_shape: tuple of ints.\n      stack_size: int, number of frames to use in state stack.\n      use_staging: bool, when True it would use a staging area to prefetch\n        the next sampling batch.\n      replay_capacity: int, number of transitions to keep in memory.\n      batch_size: int.\n      update_horizon: int, length of update (\'n\' in n-step update).\n      gamma: int, the discount factor.\n      wrapped_memory: The \'inner\' memory data structure. If None,\n        it creates the standard DQN replay memory.\n      max_sample_attempts: int, the maximum number of attempts allowed to\n        get a sample.\n      extra_storage_types: list of ReplayElements defining the type of the extra\n        contents that will be stored and returned by sample_transition_batch.\n      observation_dtype: np.dtype, type of the observations. Defaults to\n        np.uint8 for Atari 2600.\n      terminal_dtype: np.dtype, type of the terminals. Defaults to np.uint8 for\n        Atari 2600.\n      action_shape: tuple of ints, the shape for the action vector. Empty tuple\n        means the action is a scalar.\n      action_dtype: np.dtype, type of elements in the action.\n      reward_shape: tuple of ints, the shape of the reward vector. Empty tuple\n        means the reward is a scalar.\n      reward_dtype: np.dtype, type of elements in the reward.\n\n    Raises:\n      ValueError: If update_horizon is not positive.\n      ValueError: If discount factor is not in [0, 1].\n    """"""\n    if replay_capacity < update_horizon + 1:\n      raise ValueError(\n          \'Update horizon ({}) should be significantly smaller \'\n          \'than replay capacity ({}).\'.format(update_horizon, replay_capacity))\n    if not update_horizon >= 1:\n      raise ValueError(\'Update horizon must be positive.\')\n    if not 0.0 <= gamma <= 1.0:\n      raise ValueError(\'Discount factor (gamma) must be in [0, 1].\')\n\n    self.batch_size = batch_size\n\n    # Mainly used to allow subclasses to pass self.memory.\n    if wrapped_memory is not None:\n      self.memory = wrapped_memory\n    else:\n      self.memory = OutOfGraphReplayBuffer(\n          observation_shape,\n          stack_size,\n          replay_capacity,\n          batch_size,\n          update_horizon,\n          gamma,\n          max_sample_attempts,\n          observation_dtype=observation_dtype,\n          terminal_dtype=terminal_dtype,\n          extra_storage_types=extra_storage_types,\n          action_shape=action_shape,\n          action_dtype=action_dtype,\n          reward_shape=reward_shape,\n          reward_dtype=reward_dtype)\n\n    self.create_sampling_ops(use_staging)\n\n  def add(self, observation, action, reward, terminal, *args):\n    """"""Adds a transition to the replay memory.\n\n    Since the next_observation in the transition will be the observation added\n    next there is no need to pass it.\n\n    If the replay memory is at capacity the oldest transition will be discarded.\n\n    Args:\n      observation: np.array with shape observation_shape.\n      action: int, the action in the transition.\n      reward: float, the reward received in the transition.\n      terminal: np.dtype, acts as a boolean indicating whether the transition\n                was terminal (1) or not (0).\n      *args: extra contents with shapes and dtypes according to\n        extra_storage_types.\n    """"""\n    self.memory.add(observation, action, reward, terminal, *args)\n\n  def create_sampling_ops(self, use_staging):\n    """"""Creates the ops necessary to sample from the replay buffer.\n\n    Creates the transition dictionary containing the sampling tensors.\n\n    Args:\n      use_staging: bool, when True it would use a staging area to prefetch\n        the next sampling batch.\n    """"""\n    with tf.name_scope(\'sample_replay\'):\n      with tf.device(\'/cpu:*\'):\n        transition_type = self.memory.get_transition_elements()\n        transition_tensors = tf.py_func(\n            self.memory.sample_transition_batch, [],\n            [return_entry.type for return_entry in transition_type],\n            name=\'replay_sample_py_func\')\n        self._set_transition_shape(transition_tensors, transition_type)\n        if use_staging:\n          transition_tensors = self._set_up_staging(transition_tensors)\n          self._set_transition_shape(transition_tensors, transition_type)\n\n        # Unpack sample transition into member variables.\n        self.unpack_transition(transition_tensors, transition_type)\n\n  def _set_transition_shape(self, transition, transition_type):\n    """"""Set shape for each element in the transition.\n\n    Args:\n      transition: tuple of tf.Tensors.\n      transition_type: tuple of ReplayElements descriving the shapes of the\n        respective tensors.\n    """"""\n    for element, element_type in zip(transition, transition_type):\n      element.set_shape(element_type.shape)\n\n  def _set_up_staging(self, transition):\n    """"""Sets up staging ops for prefetching the next transition.\n\n    This allows us to hide the py_func latency. To do so we use a staging area\n    to pre-fetch the next batch of transitions.\n\n    Args:\n      transition: tuple of tf.Tensors with shape\n        memory.get_transition_elements().\n\n    Returns:\n      prefetched_transition: tuple of tf.Tensors with shape\n        memory.get_transition_elements() that have been previously prefetched.\n    """"""\n    transition_type = self.memory.get_transition_elements()\n\n    # Create the staging area in CPU.\n    prefetch_area = contrib_staging.StagingArea(\n        [shape_with_type.type for shape_with_type in transition_type])\n\n    # Store prefetch op for tests, but keep it private -- users should not be\n    # calling _prefetch_batch.\n    self._prefetch_batch = prefetch_area.put(transition)\n    initial_prefetch = tf.cond(\n        tf.equal(prefetch_area.size(), 0),\n        lambda: prefetch_area.put(transition), tf.no_op)\n\n    # Every time a transition is sampled self.prefetch_batch will be\n    # called. If the staging area is empty, two put ops will be called.\n    with tf.control_dependencies([self._prefetch_batch, initial_prefetch]):\n      prefetched_transition = prefetch_area.get()\n\n    return prefetched_transition\n\n  def unpack_transition(self, transition_tensors, transition_type):\n    """"""Unpacks the given transition into member variables.\n\n    Args:\n      transition_tensors: tuple of tf.Tensors.\n      transition_type: tuple of ReplayElements matching transition_tensors.\n    """"""\n    self.transition = collections.OrderedDict()\n    for element, element_type in zip(transition_tensors, transition_type):\n      self.transition[element_type.name] = element\n\n    # TODO(bellemare): These are legacy and should probably be removed in\n    # future versions.\n    self.states = self.transition[\'state\']\n    self.actions = self.transition[\'action\']\n    self.rewards = self.transition[\'reward\']\n    self.next_states = self.transition[\'next_state\']\n    self.next_actions = self.transition[\'next_action\']\n    self.next_rewards = self.transition[\'next_reward\']\n    self.terminals = self.transition[\'terminal\']\n    self.indices = self.transition[\'indices\']\n\n  def save(self, checkpoint_dir, iteration_number):\n    """"""Save the underlying replay buffer\'s contents in a file.\n\n    Args:\n      checkpoint_dir: str, the directory where to read the numpy checkpointed\n        files from.\n      iteration_number: int, the iteration_number to use as a suffix in naming\n        numpy checkpoint files.\n    """"""\n    self.memory.save(checkpoint_dir, iteration_number)\n\n  def load(self, checkpoint_dir, suffix):\n    """"""Loads the replay buffer\'s state from a saved file.\n\n    Args:\n      checkpoint_dir: str, the directory where to read the numpy checkpointed\n        files from.\n      suffix: str, the suffix to use in numpy checkpoint files.\n    """"""\n    self.memory.load(checkpoint_dir, suffix)\n'"
dopamine/replay_memory/prioritized_replay_buffer.py,7,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""An implementation of Prioritized Experience Replay (PER).\n\nThis implementation is based on the paper ""Prioritized Experience Replay""\nby Tom Schaul et al. (2015). Many thanks to Tom Schaul, John Quan, and Matteo\nHessel for providing useful pointers on the algorithm and its implementation.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom dopamine.replay_memory import circular_replay_buffer\nfrom dopamine.replay_memory import sum_tree\nfrom dopamine.replay_memory.circular_replay_buffer import ReplayElement\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nimport gin.tf\n\n\nclass OutOfGraphPrioritizedReplayBuffer(\n    circular_replay_buffer.OutOfGraphReplayBuffer):\n  """"""An out-of-graph Replay Buffer for Prioritized Experience Replay.\n\n  See circular_replay_buffer.py for details.\n  """"""\n\n  def __init__(self,\n               observation_shape,\n               stack_size,\n               replay_capacity,\n               batch_size,\n               update_horizon=1,\n               gamma=0.99,\n               max_sample_attempts=1000,\n               extra_storage_types=None,\n               observation_dtype=np.uint8,\n               terminal_dtype=np.uint8,\n               action_shape=(),\n               action_dtype=np.int32,\n               reward_shape=(),\n               reward_dtype=np.float32):\n    """"""Initializes OutOfGraphPrioritizedReplayBuffer.\n\n    Args:\n      observation_shape: tuple of ints.\n      stack_size: int, number of frames to use in state stack.\n      replay_capacity: int, number of transitions to keep in memory.\n      batch_size: int.\n      update_horizon: int, length of update (\'n\' in n-step update).\n      gamma: int, the discount factor.\n      max_sample_attempts: int, the maximum number of attempts allowed to\n        get a sample.\n      extra_storage_types: list of ReplayElements defining the type of the extra\n        contents that will be stored and returned by sample_transition_batch.\n      observation_dtype: np.dtype, type of the observations. Defaults to\n        np.uint8 for Atari 2600.\n      terminal_dtype: np.dtype, type of the terminals. Defaults to np.uint8 for\n        Atari 2600.\n      action_shape: tuple of ints, the shape for the action vector. Empty tuple\n        means the action is a scalar.\n      action_dtype: np.dtype, type of elements in the action.\n      reward_shape: tuple of ints, the shape of the reward vector. Empty tuple\n        means the reward is a scalar.\n      reward_dtype: np.dtype, type of elements in the reward.\n    """"""\n    super(OutOfGraphPrioritizedReplayBuffer, self).__init__(\n        observation_shape=observation_shape,\n        stack_size=stack_size,\n        replay_capacity=replay_capacity,\n        batch_size=batch_size,\n        update_horizon=update_horizon,\n        gamma=gamma,\n        max_sample_attempts=max_sample_attempts,\n        extra_storage_types=extra_storage_types,\n        observation_dtype=observation_dtype,\n        terminal_dtype=terminal_dtype,\n        action_shape=action_shape,\n        action_dtype=action_dtype,\n        reward_shape=reward_shape,\n        reward_dtype=reward_dtype)\n\n    self.sum_tree = sum_tree.SumTree(replay_capacity)\n\n  def get_add_args_signature(self):\n    """"""The signature of the add function.\n\n    The signature is the same as the one for OutOfGraphReplayBuffer, with an\n    added priority.\n\n    Returns:\n      list of ReplayElements defining the type of the argument signature needed\n        by the add function.\n    """"""\n    parent_add_signature = super(OutOfGraphPrioritizedReplayBuffer,\n                                 self).get_add_args_signature()\n    add_signature = parent_add_signature + [\n        ReplayElement(\'priority\', (), np.float32)\n    ]\n    return add_signature\n\n  def _add(self, *args):\n    """"""Internal add method to add to the underlying memory arrays.\n\n    The arguments need to match add_arg_signature.\n\n    If priority is none, it is set to the maximum priority ever seen.\n\n    Args:\n      *args: All the elements in a transition.\n    """"""\n    self._check_args_length(*args)\n\n    # Use Schaul et al.\'s (2015) scheme of setting the priority of new elements\n    # to the maximum priority so far.\n    # Picks out \'priority\' from arguments and adds it to the sum_tree.\n    transition = {}\n    for i, element in enumerate(self.get_add_args_signature()):\n      if element.name == \'priority\':\n        priority = args[i]\n      else:\n        transition[element.name] = args[i]\n\n    self.sum_tree.set(self.cursor(), priority)\n    super(OutOfGraphPrioritizedReplayBuffer, self)._add_transition(transition)\n\n  def sample_index_batch(self, batch_size):\n    """"""Returns a batch of valid indices sampled as in Schaul et al. (2015).\n\n    Args:\n      batch_size: int, number of indices returned.\n\n    Returns:\n      list of ints, a batch of valid indices sampled uniformly.\n\n    Raises:\n      Exception: If the batch was not constructed after maximum number of tries.\n    """"""\n    # Sample stratified indices. Some of them might be invalid.\n    indices = self.sum_tree.stratified_sample(batch_size)\n    allowed_attempts = self._max_sample_attempts\n    for i in range(len(indices)):\n      if not self.is_valid_transition(indices[i]):\n        if allowed_attempts == 0:\n          raise RuntimeError(\n              \'Max sample attempts: Tried {} times but only sampled {}\'\n              \' valid indices. Batch size is {}\'.\n              format(self._max_sample_attempts, i, batch_size))\n        index = indices[i]\n        while not self.is_valid_transition(index) and allowed_attempts > 0:\n          # If index i is not valid keep sampling others. Note that this\n          # is not stratified.\n          index = self.sum_tree.sample()\n          allowed_attempts -= 1\n        indices[i] = index\n    return indices\n\n  def sample_transition_batch(self, batch_size=None, indices=None):\n    """"""Returns a batch of transitions with extra storage and the priorities.\n\n    The extra storage are defined through the extra_storage_types constructor\n    argument.\n\n    When the transition is terminal next_state_batch has undefined contents.\n\n    Args:\n      batch_size: int, number of transitions returned. If None, the default\n        batch_size will be used.\n      indices: None or list of ints, the indices of every transition in the\n        batch. If None, sample the indices uniformly.\n\n    Returns:\n      transition_batch: tuple of np.arrays with the shape and type as in\n        get_transition_elements().\n    """"""\n    transition = (super(OutOfGraphPrioritizedReplayBuffer, self).\n                  sample_transition_batch(batch_size, indices))\n    transition_elements = self.get_transition_elements(batch_size)\n    transition_names = [e.name for e in transition_elements]\n    probabilities_index = transition_names.index(\'sampling_probabilities\')\n    indices_index = transition_names.index(\'indices\')\n    indices = transition[indices_index]\n    # The parent returned an empty array for the probabilities. Fill it with the\n    # contents of the sum tree.\n    transition[probabilities_index][:] = self.get_priority(indices)\n    return transition\n\n  def set_priority(self, indices, priorities):\n    """"""Sets the priority of the given elements according to Schaul et al.\n\n    Args:\n      indices: np.array with dtype int32, of indices in range\n        [0, replay_capacity).\n      priorities: float, the corresponding priorities.\n    """"""\n    assert indices.dtype == np.int32, (\'Indices must be integers, \'\n                                       \'given: {}\'.format(indices.dtype))\n    for index, priority in zip(indices, priorities):\n      self.sum_tree.set(index, priority)\n\n  def get_priority(self, indices):\n    """"""Fetches the priorities correspond to a batch of memory indices.\n\n    For any memory location not yet used, the corresponding priority is 0.\n\n    Args:\n      indices: np.array with dtype int32, of indices in range\n        [0, replay_capacity).\n\n    Returns:\n      priorities: float, the corresponding priorities.\n    """"""\n    assert indices.shape, \'Indices must be an array.\'\n    assert indices.dtype == np.int32, (\'Indices must be int32s, \'\n                                       \'given: {}\'.format(indices.dtype))\n    batch_size = len(indices)\n    priority_batch = np.empty((batch_size), dtype=np.float32)\n    for i, memory_index in enumerate(indices):\n      priority_batch[i] = self.sum_tree.get(memory_index)\n    return priority_batch\n\n  def get_transition_elements(self, batch_size=None):\n    """"""Returns a \'type signature\' for sample_transition_batch.\n\n    Args:\n      batch_size: int, number of transitions returned. If None, the default\n        batch_size will be used.\n    Returns:\n      signature: A namedtuple describing the method\'s return type signature.\n    """"""\n    parent_transition_type = (\n        super(OutOfGraphPrioritizedReplayBuffer,\n              self).get_transition_elements(batch_size))\n    probablilities_type = [\n        ReplayElement(\'sampling_probabilities\', (batch_size,), np.float32)\n    ]\n    return parent_transition_type + probablilities_type\n\n\n@gin.configurable(blacklist=[\'observation_shape\', \'stack_size\',\n                             \'update_horizon\', \'gamma\'])\nclass WrappedPrioritizedReplayBuffer(\n    circular_replay_buffer.WrappedReplayBuffer):\n  """"""Wrapper of OutOfGraphPrioritizedReplayBuffer with in-graph sampling.\n\n  Usage:\n\n    * To add a transition:  Call the add function.\n\n    * To sample a batch:  Query any of the tensors in the transition dictionary.\n                          Every sess.run that requires any of these tensors will\n                          sample a new transition.\n  """"""\n\n  def __init__(self,\n               observation_shape,\n               stack_size,\n               use_staging=True,\n               replay_capacity=1000000,\n               batch_size=32,\n               update_horizon=1,\n               gamma=0.99,\n               wrapped_memory=None,\n               max_sample_attempts=1000,\n               extra_storage_types=None,\n               observation_dtype=np.uint8,\n               terminal_dtype=np.uint8,\n               action_shape=(),\n               action_dtype=np.int32,\n               reward_shape=(),\n               reward_dtype=np.float32):\n    """"""Initializes WrappedPrioritizedReplayBuffer.\n\n    Args:\n      observation_shape: tuple of ints.\n      stack_size: int, number of frames to use in state stack.\n      use_staging: bool, when True it would use a staging area to prefetch\n        the next sampling batch.\n      replay_capacity: int, number of transitions to keep in memory.\n      batch_size: int.\n      update_horizon: int, length of update (\'n\' in n-step update).\n      gamma: int, the discount factor.\n      wrapped_memory: The \'inner\' memory data structure. If None, use the\n        default prioritized replay.\n      max_sample_attempts: int, the maximum number of attempts allowed to\n        get a sample.\n      extra_storage_types: list of ReplayElements defining the type of the extra\n        contents that will be stored and returned by sample_transition_batch.\n      observation_dtype: np.dtype, type of the observations. Defaults to\n        np.uint8 for Atari 2600.\n      terminal_dtype: np.dtype, type of the terminals. Defaults to np.uint8 for\n        Atari 2600.\n      action_shape: tuple of ints, the shape for the action vector. Empty tuple\n        means the action is a scalar.\n      action_dtype: np.dtype, type of elements in the action.\n      reward_shape: tuple of ints, the shape of the reward vector. Empty tuple\n        means the reward is a scalar.\n      reward_dtype: np.dtype, type of elements in the reward.\n\n    Raises:\n      ValueError: If update_horizon is not positive.\n      ValueError: If discount factor is not in [0, 1].\n    """"""\n    if wrapped_memory is None:\n      wrapped_memory = OutOfGraphPrioritizedReplayBuffer(\n          observation_shape, stack_size, replay_capacity, batch_size,\n          update_horizon, gamma, max_sample_attempts,\n          extra_storage_types=extra_storage_types,\n          observation_dtype=observation_dtype)\n\n    super(WrappedPrioritizedReplayBuffer, self).__init__(\n        observation_shape,\n        stack_size,\n        use_staging,\n        replay_capacity,\n        batch_size,\n        update_horizon,\n        gamma,\n        wrapped_memory=wrapped_memory,\n        extra_storage_types=extra_storage_types,\n        observation_dtype=observation_dtype,\n        terminal_dtype=terminal_dtype,\n        action_shape=action_shape,\n        action_dtype=action_dtype,\n        reward_shape=reward_shape,\n        reward_dtype=reward_dtype)\n\n  def tf_set_priority(self, indices, priorities):\n    """"""Sets the priorities for the given indices.\n\n    Args:\n      indices: tf.Tensor with dtype int32 and shape [n].\n      priorities: tf.Tensor with dtype float and shape [n].\n\n    Returns:\n       A tf op setting the priorities for prioritized sampling.\n    """"""\n    return tf.py_func(\n        self.memory.set_priority, [indices, priorities], [],\n        name=\'prioritized_replay_set_priority_py_func\')\n\n  def tf_get_priority(self, indices):\n    """"""Gets the priorities for the given indices.\n\n    Args:\n      indices: tf.Tensor with dtype int32 and shape [n].\n\n    Returns:\n      priorities: tf.Tensor with dtype float and shape [n], the priorities at\n        the indices.\n    """"""\n    return tf.py_func(\n        self.memory.get_priority, [indices],\n        tf.float32,\n        name=\'prioritized_replay_get_priority_py_func\')\n'"
dopamine/replay_memory/sum_tree.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A sum tree data structure.\n\nUsed for prioritized experience replay. See prioritized_replay_buffer.py\nand Schaul et al. (2015).\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport random\n\nimport numpy as np\n\n\nclass SumTree(object):\n  """"""A sum tree data structure for storing replay priorities.\n\n  A sum tree is a complete binary tree whose leaves contain values called\n  priorities. Internal nodes maintain the sum of the priorities of all leaf\n  nodes in their subtree.\n\n  For capacity = 4, the tree may look like this:\n\n               +---+\n               |2.5|\n               +-+-+\n                 |\n         +-------+--------+\n         |                |\n       +-+-+            +-+-+\n       |1.5|            |1.0|\n       +-+-+            +-+-+\n         |                |\n    +----+----+      +----+----+\n    |         |      |         |\n  +-+-+     +-+-+  +-+-+     +-+-+\n  |0.5|     |1.0|  |0.5|     |0.5|\n  +---+     +---+  +---+     +---+\n\n  This is stored in a list of numpy arrays:\n  self.nodes = [ [2.5], [1.5, 1], [0.5, 1, 0.5, 0.5] ]\n\n  For conciseness, we allocate arrays as powers of two, and pad the excess\n  elements with zero values.\n\n  This is similar to the usual array-based representation of a complete binary\n  tree, but is a little more user-friendly.\n  """"""\n\n  def __init__(self, capacity):\n    """"""Creates the sum tree data structure for the given replay capacity.\n\n    Args:\n      capacity: int, the maximum number of elements that can be stored in this\n        data structure.\n\n    Raises:\n      ValueError: If requested capacity is not positive.\n    """"""\n    assert isinstance(capacity, int)\n    if capacity <= 0:\n      raise ValueError(\'Sum tree capacity should be positive. Got: {}\'.\n                       format(capacity))\n\n    self.nodes = []\n    tree_depth = int(math.ceil(np.log2(capacity)))\n    level_size = 1\n    for _ in range(tree_depth + 1):\n      nodes_at_this_depth = np.zeros(level_size)\n      self.nodes.append(nodes_at_this_depth)\n\n      level_size *= 2\n\n    self.max_recorded_priority = 1.0\n\n  def _total_priority(self):\n    """"""Returns the sum of all priorities stored in this sum tree.\n\n    Returns:\n      float, sum of priorities stored in this sum tree.\n    """"""\n    return self.nodes[0][0]\n\n  def sample(self, query_value=None):\n    """"""Samples an element from the sum tree.\n\n    Each element has probability p_i / sum_j p_j of being picked, where p_i is\n    the (positive) value associated with node i (possibly unnormalized).\n\n    Args:\n      query_value: float in [0, 1], used as the random value to select a\n      sample. If None, will select one randomly in [0, 1).\n\n    Returns:\n      int, a random element from the sum tree.\n\n    Raises:\n      Exception: If the sum tree is empty (i.e. its node values sum to 0), or if\n        the supplied query_value is larger than the total sum.\n    """"""\n    if self._total_priority() == 0.0:\n      raise Exception(\'Cannot sample from an empty sum tree.\')\n\n    if query_value and (query_value < 0. or query_value > 1.):\n      raise ValueError(\'query_value must be in [0, 1].\')\n\n    # Sample a value in range [0, R), where R is the value stored at the root.\n    query_value = random.random() if query_value is None else query_value\n    query_value *= self._total_priority()\n\n    # Now traverse the sum tree.\n    node_index = 0\n    for nodes_at_this_depth in self.nodes[1:]:\n      # Compute children of previous depth\'s node.\n      left_child = node_index * 2\n\n      left_sum = nodes_at_this_depth[left_child]\n      # Each subtree describes a range [0, a), where a is its value.\n      if query_value < left_sum:  # Recurse into left subtree.\n        node_index = left_child\n      else:  # Recurse into right subtree.\n        node_index = left_child + 1\n        # Adjust query to be relative to right subtree.\n        query_value -= left_sum\n\n    return node_index\n\n  def stratified_sample(self, batch_size):\n    """"""Performs stratified sampling using the sum tree.\n\n    Let R be the value at the root (total value of sum tree). This method will\n    divide [0, R) into batch_size segments, pick a random number from each of\n    those segments, and use that random number to sample from the sum_tree. This\n    is as specified in Schaul et al. (2015).\n\n    Args:\n      batch_size: int, the number of strata to use.\n    Returns:\n      list of batch_size elements sampled from the sum tree.\n\n    Raises:\n      Exception: If the sum tree is empty (i.e. its node values sum to 0).\n    """"""\n    if self._total_priority() == 0.0:\n      raise Exception(\'Cannot sample from an empty sum tree.\')\n\n    bounds = np.linspace(0., 1., batch_size + 1)\n    assert len(bounds) == batch_size + 1\n    segments = [(bounds[i], bounds[i+1]) for i in range(batch_size)]\n    query_values = [random.uniform(x[0], x[1]) for x in segments]\n    return [self.sample(query_value=x) for x in query_values]\n\n  def get(self, node_index):\n    """"""Returns the value of the leaf node corresponding to the index.\n\n    Args:\n      node_index: The index of the leaf node.\n    Returns:\n      The value of the leaf node.\n    """"""\n    return self.nodes[-1][node_index]\n\n  def set(self, node_index, value):\n    """"""Sets the value of a leaf node and updates internal nodes accordingly.\n\n    This operation takes O(log(capacity)).\n    Args:\n      node_index: int, the index of the leaf node to be updated.\n      value: float, the value which we assign to the node. This value must be\n        nonnegative. Setting value = 0 will cause the element to never be\n        sampled.\n\n    Raises:\n      ValueError: If the given value is negative.\n    """"""\n    if value < 0.0:\n      raise ValueError(\'Sum tree values should be nonnegative. Got {}\'.\n                       format(value))\n    self.max_recorded_priority = max(value, self.max_recorded_priority)\n\n    delta_value = value - self.nodes[-1][node_index]\n\n    # Now traverse back the tree, adjusting all sums along the way.\n    for nodes_at_this_depth in reversed(self.nodes):\n      # Note: Adding a delta leads to some tolerable numerical inaccuracies.\n      nodes_at_this_depth[node_index] += delta_value\n      node_index //= 2\n\n    assert node_index == 0, (\'Sum tree traversal failed, final node index \'\n                             \'is not 0.\')\n'"
dopamine/utils/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
dopamine/utils/agent_visualizer.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Code to visualize different aspects of an agent\'s behaviour.\n\nThis file defines the class AgentVisualizer, which allows one to combine\na number of Plotter objects into a series of single images, generated during\nagent interaction with the environment.\nIf requested, this class will combine the image files into a movie.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport subprocess\n\n\nimport gin\nimport numpy as np\nfrom PIL import Image\nimport pygame\n\n\n@gin.configurable\nclass AgentVisualizer(object):\n  """"""Code to visualize an agent\'s behaviour.""""""\n\n  def __init__(self,\n               record_path,\n               plotters,\n               screen_width=160,\n               screen_height=210,\n               render_rate=1,\n               file_types=(\'png\', \'\'),\n               filename_format=\'frame_{:06d}\'):\n    """"""Constructor for the AgentVisualizer class.\n\n    This class generates a series of images built by a set of Plotters. These\n    images are then saved to disk.\n\n    It can optionally generate a video by concatenating all the images with\n    ffmpeg.\n\n    Args:\n      record_path: str, path where to save files.\n      plotters: list of `Plotter` objects to draw.\n      screen_width: int, width of generated images.\n      screen_height: int, height of generated images.\n      render_rate: int, frame frequency at which to generate files.\n      file_types: list of str, specifies the file types to generate.\n      filename_format: str, format to use for saving files.\n    """"""\n    self.record_path = record_path\n    self.plotters = plotters\n    self.screen_width = screen_width\n    self.screen_height = screen_height\n    self.render_rate = render_rate\n    self.file_types = file_types\n    self.filename_format = filename_format\n    self.step = 0\n    self.record_frame = np.zeros((self.screen_height, self.screen_width, 3),\n                                 dtype=np.uint8)\n    # This is necessary to avoid a `pygame.error: No available video device`\n    # error.\n    os.environ[\'SDL_VIDEODRIVER\'] = \'dummy\'\n    pygame.init()\n    self.screen = pygame.display.set_mode((self.screen_width,\n                                           self.screen_height),\n                                          0, 32)\n\n  def visualize(self):\n    if self.step % self.render_rate == 0:\n      self.screen.fill((0, 0, 0))\n      for plotter in self.plotters:\n        self.screen = self.screen.copy()  # To avoid locked Surfaces issue.\n        self.screen.blit(plotter.draw(), (plotter.x, plotter.y))\n      self.save_frame()\n    self.step += 1\n\n  def save_frame(self):\n    """"""Save a frame to disk and generate a video, if enabled.""""""\n    screen_buffer = (\n        np.frombuffer(self.screen.get_buffer(), dtype=np.int32)\n        .reshape(self.screen_height, self.screen_width))\n    sb = screen_buffer[:, 0:self.screen_width]\n    self.record_frame[..., 2] = sb % 256\n    self.record_frame[..., 1] = (sb >> 8) % 256\n    self.record_frame[..., 0] = (sb >> 16) % 256\n    frame_number = self.step // self.render_rate\n    for file_type in self.file_types:\n      if not file_type:\n        continue\n      filename = (\n          self.filename_format.format(frame_number) + \'.{}\'.format(file_type))\n      im = Image.fromarray(self.record_frame)\n      im.save(os.path.join(self.record_path, filename))\n\n  def generate_video(self, video_file=\'video.mp4\'):\n    """"""Generates a video, requires \'png\' be in file_types.\n\n    Note that this will issue a `subprocess.call` to `ffmpeg`, so only use this\n    functionality with trusted paths.\n\n    Args:\n      video_file: str, name of video file to generate.\n    """"""\n    if \'png\' not in self.file_types:\n      return\n    os.chdir(self.record_path)\n    file_regex = self.filename_format.replace(\'{:\', \'%\').replace(\'}\', \'\')\n    file_regex += \'.png\'\n    subprocess.call([\'ffmpeg\', \'-r\', \'30\', \'-f\', \'image2\', \'-s\', \'1920x1080\',\n                     \'-i\', file_regex, \'-vcodec\', \'libx264\', \'-crf\', \'25\',\n                     \'-pix_fmt\', \'yuv420p\', video_file])\n'"
dopamine/utils/atari_plotter.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""AtariPlotter used for rendering Atari 2600 frames.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nfrom dopamine.utils import plotter\nimport gin\nimport numpy as np\nimport pygame\n\n\n@gin.configurable\nclass AtariPlotter(plotter.Plotter):\n  """"""A Plotter for rendering Atari 2600 frames.""""""\n\n  _defaults = {\n      \'x\': 0,\n      \'y\': 0,\n      \'width\': 160,\n      \'height\': 210,\n  }\n\n  def __init__(self, parameter_dict=None):\n    """"""Constructor for AtariPlotter.\n\n    Args:\n      parameter_dict: None or dict of parameter specifications for\n        visualization. If an expected parameter is present, its value will\n        be used, otherwise it will use defaults.\n    """"""\n    super(AtariPlotter, self).__init__(parameter_dict)\n    assert \'environment\' in self.parameters\n    self.game_surface = pygame.Surface((self.parameters[\'width\'],\n                                        self.parameters[\'height\']))\n\n  def draw(self):\n    """"""Render the Atari 2600 frame.\n\n    Returns:\n      object to be rendered by AgentVisualizer.\n    """"""\n    environment = self.parameters[\'environment\']\n    numpy_surface = np.frombuffer(self.game_surface.get_buffer(),\n                                  dtype=np.int32)\n    obs = environment.render(mode=\'rgb_array\').astype(np.int32)\n    obs = np.transpose(obs)\n    obs = np.swapaxes(obs, 1, 2)\n    obs = obs[2] | (obs[1] << 8) | (obs[0] << 16)\n    np.copyto(numpy_surface, obs.ravel())\n    return pygame.transform.scale(self.game_surface,\n                                  (self.parameters[\'width\'],\n                                   self.parameters[\'height\']))\n'"
dopamine/utils/bar_plotter.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""BarPlotter used for drawing bar plots.\n\nNote that a side effect of using this class is to change the font used by\nmatplotlib. Unless you\'re planning to use matplotlib elsewhere in your code,\nthis should be a non-issue.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nfrom dopamine.utils import plotter\nimport gin\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pygame\n\n\n# You can change this to use your own palette. A site with great examples is:\n# https://www.dtelepathy.com/blog/inspiration/24-flat-designs-with-compelling-color-palettes\nCOLORS = [\'b\', \'g\', \'r\', \'c\', \'m\', \'y\', \'k\', \'w\']\n\n\n@gin.configurable\nclass BarPlotter(plotter.Plotter):\n  """"""A Plotter for generating bar plots.""""""\n\n  _defaults = {\n      \'x\': 0,\n      \'y\': 0,\n      \'width\': 213,\n      \'height\': 210,\n      \'fontsize\': 30,\n      \'bg_color\': \'#f8f7f2\',\n      \'face_color\': \'#ffffff\',\n      \'colors\': COLORS,\n      \'max_width\': 500,\n      \'figsize\': (12, 9),\n      \'font\': {\'family\': \'Bitstream Vera Sans\',\n               \'weight\': \'regular\',\n               \'size\': 26},\n  }\n\n  def __init__(self, parameter_dict=None):\n    """"""Constructor for BarPlotter.\n\n    This expects a callable \'get_bar_data_fn\' in the parameters, which will\n    return a list of distributions for each of the actions.  Typically, this\n    will be a callback from the agent, which will return some useful information\n    about its performance.\n\n    Args:\n      parameter_dict: None or dict of parameter specifications for\n        visualization. If an expected parameter is present, its value will\n        be used, otherwise it will use defaults.\n    """"""\n    super(BarPlotter, self).__init__(parameter_dict)\n    assert \'get_bar_data_fn\' in self.parameters\n    self.fig = plt.figure(frameon=False, figsize=self.parameters[\'figsize\'])\n    self.plot = self.fig.add_subplot(111)\n    self.plot_surface = None\n    # This sets the font used by the plotter to be the desired font.\n    matplotlib.rc(\'font\', **self.parameters[\'font\'])\n\n  def draw(self):\n    """"""Draw the bar plot.\n\n    If `parameter_dict` contains a \'legend\' key pointing to a list of labels,\n    this will be used as the legend labels in the plot.\n\n    Returns:\n      object to be rendered by AgentVisualizer.\n    """"""\n    self._setup_plot()\n    num_colors = len(self.parameters[\'colors\'])\n    bar_data = self.parameters[\'get_bar_data_fn\']()\n    num_actions, num_bins = bar_data.shape\n    for i in range(num_actions):\n      self.plot.bar(np.arange(num_bins), bar_data[i],\n                    color=self.parameters[\'colors\'][i % num_colors])\n    if \'legend\' in self.parameters:\n      self.plot.legend(self.parameters[\'legend\'])\n    self.fig.canvas.draw()\n    # Now transfer to surface.\n    width, height = self.fig.canvas.get_width_height()\n    if self.plot_surface is None:\n      self.plot_surface = pygame.Surface((width, height))\n    plot_buffer = np.frombuffer(self.fig.canvas.buffer_rgba(), np.uint32)\n    surf_buffer = np.frombuffer(self.plot_surface.get_buffer(),\n                                dtype=np.int32)\n    np.copyto(surf_buffer, plot_buffer)\n    return pygame.transform.smoothscale(\n        self.plot_surface,\n        (self.parameters[\'width\'], self.parameters[\'height\']))\n'"
dopamine/utils/example_viz.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nr""""""Sample file to generate visualizations.\n\nTo run, point FLAGS.restore_checkpoint to the TensorFlow checkpoint of a\ntrained agent. As an example, you can download to `/tmp/checkpoints` the files\nlinked below:\n  # pylint: disable=line-too-long\n  * https://storage.cloud.google.com/download-dopamine-rl/colab/samples/rainbow/SpaceInvaders_v4/checkpoints/tf_ckpt-199.data-00000-of-00001\n  * https://storage.cloud.google.com/download-dopamine-rl/colab/samples/rainbow/SpaceInvaders_v4/checkpoints/tf_ckpt-199.index\n  * https://storage.cloud.google.com/download-dopamine-rl/colab/samples/rainbow/SpaceInvaders_v4/checkpoints/tf_ckpt-199.meta\n  # pylint: enable=line-too-long\n\nYou can then run the binary with:\n\n```\npython example_viz.py \\\n        --agent=\'rainbow\' \\\n        --game=\'SpaceInvaders\' \\\n        --num_steps=1000 \\\n        --root_dir=\'/tmp/dopamine\' \\\n        --restore_checkpoint=/tmp/checkpoints/colab_samples_rainbow_SpaceInvaders_v4_checkpoints_tf_ckpt-199\n```\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nfrom absl import app\nfrom absl import flags\nfrom dopamine.utils import example_viz_lib\n\nflags.DEFINE_string(\'agent\', \'dqn\', \'Agent to visualize.\')\nflags.DEFINE_string(\'game\', \'Breakout\', \'Game to visualize.\')\nflags.DEFINE_string(\'root_dir\', \'/tmp/dopamine/\', \'Root directory.\')\nflags.DEFINE_string(\'restore_checkpoint\', None,\n                    \'Path to checkpoint to restore for visualizing.\')\nflags.DEFINE_integer(\'num_steps\', 2000, \'Number of steps to run.\')\nflags.DEFINE_boolean(\n    \'use_legacy_checkpoint\', False,\n    \'Set to true if loading from a legacy (pre-Keras) checkpoint.\')\n\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n  example_viz_lib.run(agent=FLAGS.agent,\n                      game=FLAGS.game,\n                      num_steps=FLAGS.num_steps,\n                      root_dir=FLAGS.root_dir,\n                      restore_ckpt=FLAGS.restore_checkpoint,\n                      use_legacy_checkpoint=FLAGS.use_legacy_checkpoint)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
dopamine/utils/example_viz_lib.py,16,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Library used by example_viz.py to generate visualizations.\n\nThis file illustrates the following:\n  - How to subclass an existing agent to add visualization functionality.\n    - For DQN we visualize the cumulative rewards and the Q-values for each\n      action (MyDQNAgent).\n    - For Rainbow we visualize the cumulative rewards and the Q-value\n      distributions for each action (MyRainbowAgent).\n  - How to subclass Runner to run in eval mode, lay out the different subplots,\n    generate the visualizations, and compile them into a video (MyRunner).\n  - The function `run()` is the main entrypoint for running everything.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom dopamine.agents.dqn import dqn_agent\nfrom dopamine.agents.rainbow import rainbow_agent\nfrom dopamine.discrete_domains import atari_lib\nfrom dopamine.discrete_domains import iteration_statistics\nfrom dopamine.discrete_domains import run_experiment\nfrom dopamine.utils import agent_visualizer\nfrom dopamine.utils import atari_plotter\nfrom dopamine.utils import bar_plotter\nfrom dopamine.utils import line_plotter\nimport gin\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.contrib import slim as contrib_slim\n\n\nclass MyDQNAgent(dqn_agent.DQNAgent):\n  """"""Sample DQN agent to visualize Q-values and rewards.""""""\n\n  def __init__(self, sess, num_actions, summary_writer=None):\n    super(MyDQNAgent, self).__init__(sess, num_actions,\n                                     summary_writer=summary_writer)\n    self.q_values = [[] for _ in range(num_actions)]\n    self.rewards = []\n\n  def step(self, reward, observation):\n    self.rewards.append(reward)\n    return super(MyDQNAgent, self).step(reward, observation)\n\n  def _select_action(self):\n    action = super(MyDQNAgent, self)._select_action()\n    q_vals = self._sess.run(self._net_outputs.q_values,\n                            {self.state_ph: self.state})[0]\n    for i in range(len(q_vals)):\n      self.q_values[i].append(q_vals[i])\n    return action\n\n  def reload_checkpoint(self, checkpoint_path, use_legacy_checkpoint=False):\n    if use_legacy_checkpoint:\n      variables_to_restore = atari_lib.maybe_transform_variable_names(\n          tf.all_variables(), legacy_checkpoint_load=True)\n    else:\n      global_vars = set([x.name for x in tf.global_variables()])\n      ckpt_vars = [\n          \'{}:0\'.format(name)\n          for name, _ in tf.train.list_variables(checkpoint_path)\n      ]\n      include_vars = list(global_vars.intersection(set(ckpt_vars)))\n      variables_to_restore = contrib_slim.get_variables_to_restore(\n          include=include_vars)\n    if variables_to_restore:\n      reloader = tf.train.Saver(var_list=variables_to_restore)\n      reloader.restore(self._sess, checkpoint_path)\n      tf.logging.info(\'Done restoring from %s\', checkpoint_path)\n    else:\n      tf.logging.info(\'Nothing to restore!\')\n\n  def get_q_values(self):\n    return self.q_values\n\n  def get_rewards(self):\n    return [np.cumsum(self.rewards)]\n\n\nclass MyRainbowAgent(rainbow_agent.RainbowAgent):\n  """"""Sample Rainbow agent to visualize Q-values and rewards.""""""\n\n  def __init__(self, sess, num_actions, summary_writer=None):\n    super(MyRainbowAgent, self).__init__(sess, num_actions,\n                                         summary_writer=summary_writer)\n    self.rewards = []\n\n  def step(self, reward, observation):\n    self.rewards.append(reward)\n    return super(MyRainbowAgent, self).step(reward, observation)\n\n  def reload_checkpoint(self, checkpoint_path, use_legacy_checkpoint=False):\n    if use_legacy_checkpoint:\n      variables_to_restore = atari_lib.maybe_transform_variable_names(\n          tf.all_variables(), legacy_checkpoint_load=True)\n    else:\n      global_vars = set([x.name for x in tf.global_variables()])\n      ckpt_vars = [\n          \'{}:0\'.format(name)\n          for name, _ in tf.train.list_variables(checkpoint_path)\n      ]\n      include_vars = list(global_vars.intersection(set(ckpt_vars)))\n      variables_to_restore = contrib_slim.get_variables_to_restore(\n          include=include_vars)\n    if variables_to_restore:\n      reloader = tf.train.Saver(var_list=variables_to_restore)\n      reloader.restore(self._sess, checkpoint_path)\n      tf.logging.info(\'Done restoring from %s\', checkpoint_path)\n    else:\n      tf.logging.info(\'Nothing to restore!\')\n\n  def get_probabilities(self):\n    return self._sess.run(tf.squeeze(self._net_outputs.probabilities),\n                          {self.state_ph: self.state})\n\n  def get_rewards(self):\n    return [np.cumsum(self.rewards)]\n\n\nclass MyRunner(run_experiment.Runner):\n  """"""Sample Runner class to generate visualizations.""""""\n\n  def __init__(self, base_dir, trained_agent_ckpt_path, create_agent_fn,\n               use_legacy_checkpoint=False):\n    self._trained_agent_ckpt_path = trained_agent_ckpt_path\n    self._use_legacy_checkpoint = use_legacy_checkpoint\n    super(MyRunner, self).__init__(base_dir, create_agent_fn)\n\n  def _initialize_checkpointer_and_maybe_resume(self, checkpoint_file_prefix):\n    self._agent.reload_checkpoint(self._trained_agent_ckpt_path,\n                                  self._use_legacy_checkpoint)\n    self._start_iteration = 0\n\n  def _run_one_iteration(self, iteration):\n    statistics = iteration_statistics.IterationStatistics()\n    tf.logging.info(\'Starting iteration %d\', iteration)\n    _, _ = self._run_eval_phase(statistics)\n    return statistics.data_lists\n\n  def visualize(self, record_path, num_global_steps=500):\n    if not tf.gfile.Exists(record_path):\n      tf.gfile.MakeDirs(record_path)\n    self._agent.eval_mode = True\n\n    # Set up the game playback rendering.\n    atari_params = {\'environment\': self._environment}\n    atari_plot = atari_plotter.AtariPlotter(parameter_dict=atari_params)\n    # Plot the rewards received next to it.\n    reward_params = {\'x\': atari_plot.parameters[\'width\'],\n                     \'xlabel\': \'Timestep\',\n                     \'ylabel\': \'Reward\',\n                     \'title\': \'Rewards\',\n                     \'get_line_data_fn\': self._agent.get_rewards}\n    reward_plot = line_plotter.LinePlotter(parameter_dict=reward_params)\n    action_names = [\n        \'Action {}\'.format(x) for x in range(self._agent.num_actions)]\n    # Plot Q-values (DQN) or Q-value distributions (Rainbow).\n    q_params = {\'x\': atari_plot.parameters[\'width\'] // 2,\n                \'y\': atari_plot.parameters[\'height\'],\n                \'legend\': action_names}\n    if \'DQN\' in self._agent.__class__.__name__:\n      q_params[\'xlabel\'] = \'Timestep\'\n      q_params[\'ylabel\'] = \'Q-Value\'\n      q_params[\'title\'] = \'Q-Values\'\n      q_params[\'get_line_data_fn\'] = self._agent.get_q_values\n      q_plot = line_plotter.LinePlotter(parameter_dict=q_params)\n    else:\n      q_params[\'xlabel\'] = \'Return\'\n      q_params[\'ylabel\'] = \'Return probability\'\n      q_params[\'title\'] = \'Return distribution\'\n      q_params[\'get_bar_data_fn\'] = self._agent.get_probabilities\n      q_plot = bar_plotter.BarPlotter(parameter_dict=q_params)\n    screen_width = (\n        atari_plot.parameters[\'width\'] + reward_plot.parameters[\'width\'])\n    screen_height = (\n        atari_plot.parameters[\'height\'] + q_plot.parameters[\'height\'])\n    # Dimensions need to be divisible by 2:\n    if screen_width % 2 > 0:\n      screen_width += 1\n    if screen_height % 2 > 0:\n      screen_height += 1\n    visualizer = agent_visualizer.AgentVisualizer(\n        record_path=record_path, plotters=[atari_plot, reward_plot, q_plot],\n        screen_width=screen_width, screen_height=screen_height)\n    global_step = 0\n    while global_step < num_global_steps:\n      initial_observation = self._environment.reset()\n      action = self._agent.begin_episode(initial_observation)\n      while True:\n        observation, reward, is_terminal, _ = self._environment.step(action)\n        global_step += 1\n        visualizer.visualize()\n        if self._environment.game_over or global_step >= num_global_steps:\n          break\n        elif is_terminal:\n          self._agent.end_episode(reward)\n          action = self._agent.begin_episode(observation)\n        else:\n          action = self._agent.step(reward, observation)\n      self._end_episode(reward)\n    visualizer.generate_video()\n\n\ndef create_dqn_agent(sess, environment, summary_writer=None):\n  return MyDQNAgent(sess, num_actions=environment.action_space.n,\n                    summary_writer=summary_writer)\n\n\ndef create_rainbow_agent(sess, environment, summary_writer=None):\n  return MyRainbowAgent(sess, num_actions=environment.action_space.n,\n                        summary_writer=summary_writer)\n\n\ndef create_runner(base_dir, trained_agent_ckpt_path, agent=\'dqn\',\n                  use_legacy_checkpoint=False):\n  create_agent = create_dqn_agent if agent == \'dqn\' else create_rainbow_agent\n  return MyRunner(base_dir, trained_agent_ckpt_path, create_agent,\n                  use_legacy_checkpoint)\n\n\ndef run(agent, game, num_steps, root_dir, restore_ckpt, use_legacy_checkpoint):\n  """"""Main entrypoint for running and generating visualizations.\n\n  Args:\n    agent: str, agent type to use.\n    game: str, Atari 2600 game to run.\n    num_steps: int, number of steps to play game.\n    root_dir: str, root directory where files will be stored.\n    restore_ckpt: str, path to the checkpoint to reload.\n    use_legacy_checkpoint: bool, whether to restore from a legacy (pre-Keras)\n      checkpoint.\n  """"""\n  config = """"""\n  atari_lib.create_atari_environment.game_name = \'{}\'\n  WrappedReplayBuffer.replay_capacity = 300\n  """""".format(game)\n  base_dir = os.path.join(root_dir, \'agent_viz\', game, agent)\n  gin.parse_config(config)\n  runner = create_runner(base_dir, restore_ckpt, agent, use_legacy_checkpoint)\n  runner.visualize(os.path.join(base_dir, \'images\'), num_global_steps=num_steps)\n'"
dopamine/utils/line_plotter.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""LinePlotter used for drawing line plots.\n\nNote that a side effect of using this class is to change the font used by\nmatplotlib. Unless you\'re planning to use matplotlib elsewhere in your code,\nthis should be a non-issue.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nfrom dopamine.utils import plotter\nimport gin\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pygame\n\n\n# You can change this to use your own palette. A site with great examples is:\n# https://www.dtelepathy.com/blog/inspiration/24-flat-designs-with-compelling-color-palettes\nCOLORS = [\'b\', \'g\', \'r\', \'c\', \'m\', \'y\', \'k\', \'w\']\n\n\n@gin.configurable\nclass LinePlotter(plotter.Plotter):\n  """"""A Plotter for generating line plots.""""""\n\n  _defaults = {\n      \'x\': 0,\n      \'y\': 0,\n      \'width\': 213,\n      \'height\': 210,\n      \'fontsize\': 30,\n      \'bg_color\': \'#f8f7f2\',\n      \'face_color\': \'#ffffff\',\n      \'colors\': COLORS,\n      \'max_width\': 500,\n      \'figsize\': (12, 9),\n      \'font\': {\'family\': \'Bitstream Vera Sans\',\n               \'weight\': \'regular\',\n               \'size\': 26},\n      \'linewidth\': 5,\n  }\n\n  def __init__(self, parameter_dict=None):\n    """"""Constructor for LinePlotter.\n\n    This expects a callable \'get_line_data_fn\' in the parameters, which\n    will return a list of list of floats, each one representing a line\n    to be drawn. Typically, this will be a callback from the agent,\n    which will return some useful information about its performance.\n\n    Args:\n      parameter_dict: None or dict of parameter specifications for\n        visualization. If an expected parameter is present, its value will\n        be used, otherwise it will use defaults.\n    """"""\n    super(LinePlotter, self).__init__(parameter_dict)\n    assert \'get_line_data_fn\' in self.parameters\n    self.fig = plt.figure(frameon=False, figsize=self.parameters[\'figsize\'])\n    self.plot = self.fig.add_subplot(111)\n    self.plot_surface = None\n    matplotlib.rc(\'font\', **self.parameters[\'font\'])\n\n  def draw(self):\n    """"""Draw the line plot.\n\n    If `parameter_dict` contains a \'legend\' key pointing to a list of labels,\n    this will be used as the legend labels in the plot.\n\n    Returns:\n      object to be rendered by AgentVisualizer.\n    """"""\n    self._setup_plot()\n    num_colors = len(self.parameters[\'colors\'])\n    max_xlim = 0\n    line_data = self.parameters[\'get_line_data_fn\']()\n    for i in range(len(line_data)):\n      self.plot.plot(line_data[i],\n                     linewidth=self.parameters[\'linewidth\'],\n                     color=self.parameters[\'colors\'][i % num_colors])\n      max_xlim = max(max_xlim, len(line_data[i]))\n    min_xlim = max(0, max_xlim - self.parameters[\'max_width\'])\n    self.plot.set_xlim(min_xlim, max_xlim)\n    if \'legend\' in self.parameters:\n      self.plot.legend(self.parameters[\'legend\'])\n    self.fig.canvas.draw()\n    # Now transfer to surface.\n    width, height = self.fig.canvas.get_width_height()\n    if self.plot_surface is None:\n      self.plot_surface = pygame.Surface((width, height))\n    plot_buffer = np.frombuffer(self.fig.canvas.buffer_rgba(), np.uint32)\n    surf_buffer = np.frombuffer(self.plot_surface.get_buffer(),\n                                dtype=np.int32)\n    np.copyto(surf_buffer, plot_buffer)\n    return pygame.transform.smoothscale(\n        self.plot_surface,\n        (self.parameters[\'width\'], self.parameters[\'height\']))\n'"
dopamine/utils/plotter.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Base class for plotters.\n\nThis class provides the core functionality for Plotter objects. Specifically, it\ninitializes `self.parameters` with the values passed through the constructor or\nwith the provided defaults (specified in each child class), and specifies the\nabstract `draw()` method, which child classes will need to implement.\n\nThis class also provides a helper function `_setup_plot` for Plotters based on\nmatplotlib.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\n\nclass Plotter(object):\n  """"""Abstract base class for plotters.""""""\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, parameter_dict=None):\n    """"""Constructor for a Plotter, each child class must define _defaults.\n\n    It will ensure there are values for \'x\' and \'y\' in `self.parameters`. The\n    other key/values will come from either `parameter_dict` or, if not specified\n    there, from `self._defaults`.\n\n    Args:\n      parameter_dict: None or dict of parameter specifications for\n        visualization. If an expected parameter is present, its value will\n        be used, otherwise it will use defaults.\n    """"""\n    self.parameters = {\'x\': 0, \'y\': 0}\n    self.parameters.update(self._defaults)\n    self.parameters.update(parameter_dict)\n\n  def _setup_plot(self):\n    """"""Helpful common functionality when rendering matplotlib-style plots.""""""\n    self.plot.cla()  # Clear current figure.\n    self.fig.patch.set_facecolor(self.parameters[\'face_color\'])\n    try:\n      self.plot.set_facecolor(self.parameters[\'bg_color\'])\n    except AttributeError:\n      self.plot.set_axis_bgcolor(self.parameters[\'bg_color\'])\n    if \'xlabel\' in self.parameters:\n      self.plot.set_xlabel(self.parameters[\'xlabel\'],\n                           fontsize=self.parameters[\'fontsize\'] - 2)\n    if \'ylabel\' in self.parameters:\n      self.plot.set_ylabel(self.parameters[\'ylabel\'],\n                           fontsize=self.parameters[\'fontsize\'] - 2)\n    if \'title\' in self.parameters:\n      self.plot.set_title(self.parameters[\'title\'],\n                          fontsize=self.parameters[\'fontsize\'] + 2)\n    self.plot.tick_params(labelsize=self.parameters[\'fontsize\'])\n\n  @abc.abstractmethod\n  def draw(self):\n    """"""Draw a plot.\n\n    Returns:\n      object to be rendered by AgentVisualizer.\n    """"""\n    pass\n\n  @property\n  def x(self):\n    return self.parameters[\'x\']\n\n  @property\n  def y(self):\n    return self.parameters[\'y\']\n'"
dopamine/utils/test_utils.py,1,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Common testing utilities shared across agents.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nimport mock\nimport tensorflow.compat.v1 as tf\n\n\nclass MockReplayBuffer(object):\n  """"""Mock ReplayBuffer to verify the way the agent interacts with it.""""""\n\n  def __init__(self):\n    with tf.variable_scope(\'MockReplayBuffer\', reuse=tf.AUTO_REUSE):\n      self.add = mock.Mock()\n      self.memory = mock.Mock()\n      self.memory.add_count = 0\n'"
tests/dopamine/atari_init_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A simple test for validating that the Atari env initializes.""""""\n\nimport datetime\nimport os\nimport shutil\n\n\n\nfrom absl import flags\nfrom dopamine.discrete_domains import train\nimport tensorflow.compat.v1 as tf\n\n\nFLAGS = flags.FLAGS\n\n\nclass AtariInitTest(tf.test.TestCase):\n\n  def setUp(self):\n    FLAGS.base_dir = os.path.join(\n        \'/tmp/dopamine_tests\',\n        datetime.datetime.utcnow().strftime(\'run_%Y_%m_%d_%H_%M_%S\'))\n    FLAGS.gin_files = [\'dopamine/agents/dqn/configs/dqn.gin\']\n    # `num_iterations` set to zero to prevent runner execution.\n    FLAGS.gin_bindings = [\n        \'Runner.num_iterations=0\',\n        \'WrappedReplayBuffer.replay_capacity = 100\'  # To prevent OOM.\n    ]\n    FLAGS.alsologtostderr = True\n\n  def test_atari_init(self):\n    """"""Tests that a DQN agent is initialized.""""""\n    train.main([])\n    shutil.rmtree(FLAGS.base_dir)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
dopamine/agents/dqn/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
dopamine/agents/dqn/dqn_agent.py,49,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Compact implementation of a DQN agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\nimport random\n\n\n\nfrom dopamine.discrete_domains import atari_lib\nfrom dopamine.replay_memory import circular_replay_buffer\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nimport gin.tf\n\n\n# These are aliases which are used by other classes.\nNATURE_DQN_OBSERVATION_SHAPE = atari_lib.NATURE_DQN_OBSERVATION_SHAPE\nNATURE_DQN_DTYPE = atari_lib.NATURE_DQN_DTYPE\nNATURE_DQN_STACK_SIZE = atari_lib.NATURE_DQN_STACK_SIZE\nnature_dqn_network = atari_lib.NatureDQNNetwork\n\n\n@gin.configurable\ndef linearly_decaying_epsilon(decay_period, step, warmup_steps, epsilon):\n  """"""Returns the current epsilon for the agent\'s epsilon-greedy policy.\n\n  This follows the Nature DQN schedule of a linearly decaying epsilon (Mnih et\n  al., 2015). The schedule is as follows:\n    Begin at 1. until warmup_steps steps have been taken; then\n    Linearly decay epsilon from 1. to epsilon in decay_period steps; and then\n    Use epsilon from there on.\n\n  Args:\n    decay_period: float, the period over which epsilon is decayed.\n    step: int, the number of training steps completed so far.\n    warmup_steps: int, the number of steps taken before epsilon is decayed.\n    epsilon: float, the final value to which to decay the epsilon parameter.\n\n  Returns:\n    A float, the current epsilon value computed according to the schedule.\n  """"""\n  steps_left = decay_period + warmup_steps - step\n  bonus = (1.0 - epsilon) * steps_left / decay_period\n  bonus = np.clip(bonus, 0., 1. - epsilon)\n  return epsilon + bonus\n\n\n@gin.configurable\ndef identity_epsilon(unused_decay_period, unused_step, unused_warmup_steps,\n                     epsilon):\n  return epsilon\n\n\n@gin.configurable\nclass DQNAgent(object):\n  """"""An implementation of the DQN agent.""""""\n\n  def __init__(self,\n               sess,\n               num_actions,\n               observation_shape=atari_lib.NATURE_DQN_OBSERVATION_SHAPE,\n               observation_dtype=atari_lib.NATURE_DQN_DTYPE,\n               stack_size=atari_lib.NATURE_DQN_STACK_SIZE,\n               network=atari_lib.NatureDQNNetwork,\n               gamma=0.99,\n               update_horizon=1,\n               min_replay_history=20000,\n               update_period=4,\n               target_update_period=8000,\n               epsilon_fn=linearly_decaying_epsilon,\n               epsilon_train=0.01,\n               epsilon_eval=0.001,\n               epsilon_decay_period=250000,\n               tf_device=\'/cpu:*\',\n               eval_mode=False,\n               use_staging=True,\n               max_tf_checkpoints_to_keep=4,\n               optimizer=tf.train.RMSPropOptimizer(\n                   learning_rate=0.00025,\n                   decay=0.95,\n                   momentum=0.0,\n                   epsilon=0.00001,\n                   centered=True),\n               summary_writer=None,\n               summary_writing_frequency=500,\n               allow_partial_reload=False):\n    """"""Initializes the agent and constructs the components of its graph.\n\n    Args:\n      sess: `tf.Session`, for executing ops.\n      num_actions: int, number of actions the agent can take at any state.\n      observation_shape: tuple of ints describing the observation shape.\n      observation_dtype: tf.DType, specifies the type of the observations. Note\n        that if your inputs are continuous, you should set this to tf.float32.\n      stack_size: int, number of frames to use in state stack.\n      network: tf.Keras.Model, expecting 2 parameters: num_actions,\n        network_type. A call to this object will return an instantiation of the\n        network provided. The network returned can be run with different inputs\n        to create different outputs. See\n        dopamine.discrete_domains.atari_lib.NatureDQNNetwork as an example.\n      gamma: float, discount factor with the usual RL meaning.\n      update_horizon: int, horizon at which updates are performed, the \'n\' in\n        n-step update.\n      min_replay_history: int, number of transitions that should be experienced\n        before the agent begins training its value function.\n      update_period: int, period between DQN updates.\n      target_update_period: int, update period for the target network.\n      epsilon_fn: function expecting 4 parameters:\n        (decay_period, step, warmup_steps, epsilon). This function should return\n        the epsilon value used for exploration during training.\n      epsilon_train: float, the value to which the agent\'s epsilon is eventually\n        decayed during training.\n      epsilon_eval: float, epsilon used when evaluating the agent.\n      epsilon_decay_period: int, length of the epsilon decay schedule.\n      tf_device: str, Tensorflow device on which the agent\'s graph is executed.\n      eval_mode: bool, True for evaluation and False for training.\n      use_staging: bool, when True use a staging area to prefetch the next\n        training batch, speeding training up by about 30%.\n      max_tf_checkpoints_to_keep: int, the number of TensorFlow checkpoints to\n        keep.\n      optimizer: `tf.train.Optimizer`, for training the value function.\n      summary_writer: SummaryWriter object for outputting training statistics.\n        Summary writing disabled if set to None.\n      summary_writing_frequency: int, frequency with which summaries will be\n        written. Lower values will result in slower training.\n      allow_partial_reload: bool, whether we allow reloading a partial agent\n        (for instance, only the network parameters).\n    """"""\n    assert isinstance(observation_shape, tuple)\n    tf.logging.info(\'Creating %s agent with the following parameters:\',\n                    self.__class__.__name__)\n    tf.logging.info(\'\\t gamma: %f\', gamma)\n    tf.logging.info(\'\\t update_horizon: %f\', update_horizon)\n    tf.logging.info(\'\\t min_replay_history: %d\', min_replay_history)\n    tf.logging.info(\'\\t update_period: %d\', update_period)\n    tf.logging.info(\'\\t target_update_period: %d\', target_update_period)\n    tf.logging.info(\'\\t epsilon_train: %f\', epsilon_train)\n    tf.logging.info(\'\\t epsilon_eval: %f\', epsilon_eval)\n    tf.logging.info(\'\\t epsilon_decay_period: %d\', epsilon_decay_period)\n    tf.logging.info(\'\\t tf_device: %s\', tf_device)\n    tf.logging.info(\'\\t use_staging: %s\', use_staging)\n    tf.logging.info(\'\\t optimizer: %s\', optimizer)\n    tf.logging.info(\'\\t max_tf_checkpoints_to_keep: %d\',\n                    max_tf_checkpoints_to_keep)\n\n    self.num_actions = num_actions\n    self.observation_shape = tuple(observation_shape)\n    self.observation_dtype = observation_dtype\n    self.stack_size = stack_size\n    self.network = network\n    self.gamma = gamma\n    self.update_horizon = update_horizon\n    self.cumulative_gamma = math.pow(gamma, update_horizon)\n    self.min_replay_history = min_replay_history\n    self.target_update_period = target_update_period\n    self.epsilon_fn = epsilon_fn\n    self.epsilon_train = epsilon_train\n    self.epsilon_eval = epsilon_eval\n    self.epsilon_decay_period = epsilon_decay_period\n    self.update_period = update_period\n    self.eval_mode = eval_mode\n    self.training_steps = 0\n    self.optimizer = optimizer\n    self.summary_writer = summary_writer\n    self.summary_writing_frequency = summary_writing_frequency\n    self.allow_partial_reload = allow_partial_reload\n\n    with tf.device(tf_device):\n      # Create a placeholder for the state input to the DQN network.\n      # The last axis indicates the number of consecutive frames stacked.\n      state_shape = (1,) + self.observation_shape + (stack_size,)\n      self.state = np.zeros(state_shape)\n      self.state_ph = tf.placeholder(self.observation_dtype, state_shape,\n                                     name=\'state_ph\')\n      self._replay = self._build_replay_buffer(use_staging)\n\n      self._build_networks()\n\n      self._train_op = self._build_train_op()\n      self._sync_qt_ops = self._build_sync_op()\n\n    if self.summary_writer is not None:\n      # All tf.summaries should have been defined prior to running this.\n      self._merged_summaries = tf.summary.merge_all()\n    self._sess = sess\n\n    var_map = atari_lib.maybe_transform_variable_names(tf.all_variables())\n    self._saver = tf.train.Saver(var_list=var_map,\n                                 max_to_keep=max_tf_checkpoints_to_keep)\n\n    # Variables to be initialized by the agent once it interacts with the\n    # environment.\n    self._observation = None\n    self._last_observation = None\n\n  def _create_network(self, name):\n    """"""Builds the convolutional network used to compute the agent\'s Q-values.\n\n    Args:\n      name: str, this name is passed to the tf.keras.Model and used to create\n        variable scope under the hood by the tf.keras.Model.\n    Returns:\n      network: tf.keras.Model, the network instantiated by the Keras model.\n    """"""\n    network = self.network(self.num_actions, name=name)\n    return network\n\n  def _build_networks(self):\n    """"""Builds the Q-value network computations needed for acting and training.\n\n    These are:\n      self.online_convnet: For computing the current state\'s Q-values.\n      self.target_convnet: For computing the next state\'s target Q-values.\n      self._net_outputs: The actual Q-values.\n      self._q_argmax: The action maximizing the current state\'s Q-values.\n      self._replay_net_outputs: The replayed states\' Q-values.\n      self._replay_next_target_net_outputs: The replayed next states\' target\n        Q-values (see Mnih et al., 2015 for details).\n    """"""\n\n    # _network_template instantiates the model and returns the network object.\n    # The network object can be used to generate different outputs in the graph.\n    # At each call to the network, the parameters will be reused.\n    self.online_convnet = self._create_network(name=\'Online\')\n    self.target_convnet = self._create_network(name=\'Target\')\n    self._net_outputs = self.online_convnet(self.state_ph)\n    # TODO(bellemare): Ties should be broken. They are unlikely to happen when\n    # using a deep network, but may affect performance with a linear\n    # approximation scheme.\n    self._q_argmax = tf.argmax(self._net_outputs.q_values, axis=1)[0]\n    self._replay_net_outputs = self.online_convnet(self._replay.states)\n    self._replay_next_target_net_outputs = self.target_convnet(\n        self._replay.next_states)\n\n  def _build_replay_buffer(self, use_staging):\n    """"""Creates the replay buffer used by the agent.\n\n    Args:\n      use_staging: bool, if True, uses a staging area to prefetch data for\n        faster training.\n\n    Returns:\n      A WrapperReplayBuffer object.\n    """"""\n    return circular_replay_buffer.WrappedReplayBuffer(\n        observation_shape=self.observation_shape,\n        stack_size=self.stack_size,\n        use_staging=use_staging,\n        update_horizon=self.update_horizon,\n        gamma=self.gamma,\n        observation_dtype=self.observation_dtype.as_numpy_dtype)\n\n  def _build_target_q_op(self):\n    """"""Build an op used as a target for the Q-value.\n\n    Returns:\n      target_q_op: An op calculating the Q-value.\n    """"""\n    # Get the maximum Q-value across the actions dimension.\n    replay_next_qt_max = tf.reduce_max(\n        self._replay_next_target_net_outputs.q_values, 1)\n    # Calculate the Bellman target value.\n    #   Q_t = R_t + \\gamma^N * Q\'_t+1\n    # where,\n    #   Q\'_t+1 = \\argmax_a Q(S_t+1, a)\n    #          (or) 0 if S_t is a terminal state,\n    # and\n    #   N is the update horizon (by default, N=1).\n    return self._replay.rewards + self.cumulative_gamma * replay_next_qt_max * (\n        1. - tf.cast(self._replay.terminals, tf.float32))\n\n  def _build_train_op(self):\n    """"""Builds a training op.\n\n    Returns:\n      train_op: An op performing one step of training from replay data.\n    """"""\n    replay_action_one_hot = tf.one_hot(\n        self._replay.actions, self.num_actions, 1., 0., name=\'action_one_hot\')\n    replay_chosen_q = tf.reduce_sum(\n        self._replay_net_outputs.q_values * replay_action_one_hot,\n        reduction_indices=1,\n        name=\'replay_chosen_q\')\n\n    target = tf.stop_gradient(self._build_target_q_op())\n    loss = tf.losses.huber_loss(\n        target, replay_chosen_q, reduction=tf.losses.Reduction.NONE)\n    if self.summary_writer is not None:\n      with tf.variable_scope(\'Losses\'):\n        tf.summary.scalar(\'HuberLoss\', tf.reduce_mean(loss))\n    return self.optimizer.minimize(tf.reduce_mean(loss))\n\n  def _build_sync_op(self):\n    """"""Builds ops for assigning weights from online to target network.\n\n    Returns:\n      ops: A list of ops assigning weights from online to target network.\n    """"""\n    # Get trainable variables from online and target DQNs\n    sync_qt_ops = []\n    scope = tf.get_default_graph().get_name_scope()\n    trainables_online = tf.get_collection(\n        tf.GraphKeys.TRAINABLE_VARIABLES, scope=os.path.join(scope, \'Online\'))\n    trainables_target = tf.get_collection(\n        tf.GraphKeys.TRAINABLE_VARIABLES, scope=os.path.join(scope, \'Target\'))\n\n    for (w_online, w_target) in zip(trainables_online, trainables_target):\n      # Assign weights from online to target network.\n      sync_qt_ops.append(w_target.assign(w_online, use_locking=True))\n    return sync_qt_ops\n\n  def begin_episode(self, observation):\n    """"""Returns the agent\'s first action for this episode.\n\n    Args:\n      observation: numpy array, the environment\'s initial observation.\n\n    Returns:\n      int, the selected action.\n    """"""\n    self._reset_state()\n    self._record_observation(observation)\n\n    if not self.eval_mode:\n      self._train_step()\n\n    self.action = self._select_action()\n    return self.action\n\n  def step(self, reward, observation):\n    """"""Records the most recent transition and returns the agent\'s next action.\n\n    We store the observation of the last time step since we want to store it\n    with the reward.\n\n    Args:\n      reward: float, the reward received from the agent\'s most recent action.\n      observation: numpy array, the most recent observation.\n\n    Returns:\n      int, the selected action.\n    """"""\n    self._last_observation = self._observation\n    self._record_observation(observation)\n\n    if not self.eval_mode:\n      self._store_transition(self._last_observation, self.action, reward, False)\n      self._train_step()\n\n    self.action = self._select_action()\n    return self.action\n\n  def end_episode(self, reward):\n    """"""Signals the end of the episode to the agent.\n\n    We store the observation of the current time step, which is the last\n    observation of the episode.\n\n    Args:\n      reward: float, the last reward from the environment.\n    """"""\n    if not self.eval_mode:\n      self._store_transition(self._observation, self.action, reward, True)\n\n  def _select_action(self):\n    """"""Select an action from the set of available actions.\n\n    Chooses an action randomly with probability self._calculate_epsilon(), and\n    otherwise acts greedily according to the current Q-value estimates.\n\n    Returns:\n       int, the selected action.\n    """"""\n    if self.eval_mode:\n      epsilon = self.epsilon_eval\n    else:\n      epsilon = self.epsilon_fn(\n          self.epsilon_decay_period,\n          self.training_steps,\n          self.min_replay_history,\n          self.epsilon_train)\n    if random.random() <= epsilon:\n      # Choose a random action with probability epsilon.\n      return random.randint(0, self.num_actions - 1)\n    else:\n      # Choose the action with highest Q-value at the current state.\n      return self._sess.run(self._q_argmax, {self.state_ph: self.state})\n\n  def _train_step(self):\n    """"""Runs a single training step.\n\n    Runs a training op if both:\n      (1) A minimum number of frames have been added to the replay buffer.\n      (2) `training_steps` is a multiple of `update_period`.\n\n    Also, syncs weights from online to target network if training steps is a\n    multiple of target update period.\n    """"""\n    # Run a train op at the rate of self.update_period if enough training steps\n    # have been run. This matches the Nature DQN behaviour.\n    if self._replay.memory.add_count > self.min_replay_history:\n      if self.training_steps % self.update_period == 0:\n        self._sess.run(self._train_op)\n        if (self.summary_writer is not None and\n            self.training_steps > 0 and\n            self.training_steps % self.summary_writing_frequency == 0):\n          summary = self._sess.run(self._merged_summaries)\n          self.summary_writer.add_summary(summary, self.training_steps)\n\n      if self.training_steps % self.target_update_period == 0:\n        self._sess.run(self._sync_qt_ops)\n\n    self.training_steps += 1\n\n  def _record_observation(self, observation):\n    """"""Records an observation and update state.\n\n    Extracts a frame from the observation vector and overwrites the oldest\n    frame in the state buffer.\n\n    Args:\n      observation: numpy array, an observation from the environment.\n    """"""\n    # Set current observation. We do the reshaping to handle environments\n    # without frame stacking.\n    self._observation = np.reshape(observation, self.observation_shape)\n    # Swap out the oldest frame with the current frame.\n    self.state = np.roll(self.state, -1, axis=-1)\n    self.state[0, ..., -1] = self._observation\n\n  def _store_transition(self, last_observation, action, reward, is_terminal):\n    """"""Stores an experienced transition.\n\n    Executes a tf session and executes replay buffer ops in order to store the\n    following tuple in the replay buffer:\n      (last_observation, action, reward, is_terminal).\n\n    Pedantically speaking, this does not actually store an entire transition\n    since the next state is recorded on the following time step.\n\n    Args:\n      last_observation: numpy array, last observation.\n      action: int, the action taken.\n      reward: float, the reward.\n      is_terminal: bool, indicating if the current state is a terminal state.\n    """"""\n    self._replay.add(last_observation, action, reward, is_terminal)\n\n  def _reset_state(self):\n    """"""Resets the agent state by filling it with zeros.""""""\n    self.state.fill(0)\n\n  def bundle_and_checkpoint(self, checkpoint_dir, iteration_number):\n    """"""Returns a self-contained bundle of the agent\'s state.\n\n    This is used for checkpointing. It will return a dictionary containing all\n    non-TensorFlow objects (to be saved into a file by the caller), and it saves\n    all TensorFlow objects into a checkpoint file.\n\n    Args:\n      checkpoint_dir: str, directory where TensorFlow objects will be saved.\n      iteration_number: int, iteration number to use for naming the checkpoint\n        file.\n\n    Returns:\n      A dict containing additional Python objects to be checkpointed by the\n        experiment. If the checkpoint directory does not exist, returns None.\n    """"""\n    if not tf.gfile.Exists(checkpoint_dir):\n      return None\n    # Call the Tensorflow saver to checkpoint the graph.\n    self._saver.save(\n        self._sess,\n        os.path.join(checkpoint_dir, \'tf_ckpt\'),\n        global_step=iteration_number)\n    # Checkpoint the out-of-graph replay buffer.\n    self._replay.save(checkpoint_dir, iteration_number)\n    bundle_dictionary = {}\n    bundle_dictionary[\'state\'] = self.state\n    bundle_dictionary[\'training_steps\'] = self.training_steps\n    return bundle_dictionary\n\n  def unbundle(self, checkpoint_dir, iteration_number, bundle_dictionary):\n    """"""Restores the agent from a checkpoint.\n\n    Restores the agent\'s Python objects to those specified in bundle_dictionary,\n    and restores the TensorFlow objects to those specified in the\n    checkpoint_dir. If the checkpoint_dir does not exist, will not reset the\n      agent\'s state.\n\n    Args:\n      checkpoint_dir: str, path to the checkpoint saved by tf.Save.\n      iteration_number: int, checkpoint version, used when restoring the replay\n        buffer.\n      bundle_dictionary: dict, containing additional Python objects owned by\n        the agent.\n\n    Returns:\n      bool, True if unbundling was successful.\n    """"""\n    try:\n      # self._replay.load() will throw a NotFoundError if it does not find all\n      # the necessary files.\n      self._replay.load(checkpoint_dir, iteration_number)\n    except tf.errors.NotFoundError:\n      if not self.allow_partial_reload:\n        # If we don\'t allow partial reloads, we will return False.\n        return False\n      tf.logging.warning(\'Unable to reload replay buffer!\')\n    if bundle_dictionary is not None:\n      for key in self.__dict__:\n        if key in bundle_dictionary:\n          self.__dict__[key] = bundle_dictionary[key]\n    elif not self.allow_partial_reload:\n      return False\n    else:\n      tf.logging.warning(""Unable to reload the agent\'s parameters!"")\n    # Restore the agent\'s TensorFlow graph.\n    self._saver.restore(self._sess,\n                        os.path.join(checkpoint_dir,\n                                     \'tf_ckpt-{}\'.format(iteration_number)))\n    return True\n'"
dopamine/agents/implicit_quantile/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
dopamine/agents/implicit_quantile/implicit_quantile_agent.py,48,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""The implicit quantile networks (IQN) agent.\n\nThe agent follows the description given in ""Implicit Quantile Networks for\nDistributional RL"" (Dabney et. al, 2018).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom dopamine.agents.rainbow import rainbow_agent\nfrom dopamine.discrete_domains import atari_lib\nimport tensorflow.compat.v1 as tf\n\nimport gin.tf\n\n\n@gin.configurable\nclass ImplicitQuantileAgent(rainbow_agent.RainbowAgent):\n  """"""An extension of Rainbow to perform implicit quantile regression.""""""\n\n  def __init__(self,\n               sess,\n               num_actions,\n               network=atari_lib.ImplicitQuantileNetwork,\n               kappa=1.0,\n               num_tau_samples=32,\n               num_tau_prime_samples=32,\n               num_quantile_samples=32,\n               quantile_embedding_dim=64,\n               double_dqn=False,\n               summary_writer=None,\n               summary_writing_frequency=500):\n    """"""Initializes the agent and constructs the Graph.\n\n    Most of this constructor\'s parameters are IQN-specific hyperparameters whose\n    values are taken from Dabney et al. (2018).\n\n    Args:\n      sess: `tf.Session` object for running associated ops.\n      num_actions: int, number of actions the agent can take at any state.\n      network: tf.Keras.Model, expects three parameters:\n        (num_actions, quantile_embedding_dim, network_type). This class is used\n        to generate network instances that are used by the agent. Each\n        instantiation would have different set of variables. See\n        dopamine.discrete_domains.atari_lib.NatureDQNNetwork as an example.\n      kappa: float, Huber loss cutoff.\n      num_tau_samples: int, number of online quantile samples for loss\n        estimation.\n      num_tau_prime_samples: int, number of target quantile samples for loss\n        estimation.\n      num_quantile_samples: int, number of quantile samples for computing\n        Q-values.\n      quantile_embedding_dim: int, embedding dimension for the quantile input.\n      double_dqn: boolean, whether to perform double DQN style learning\n        as described in Van Hasselt et al.: https://arxiv.org/abs/1509.06461.\n      summary_writer: SummaryWriter object for outputting training statistics.\n        Summary writing disabled if set to None.\n      summary_writing_frequency: int, frequency with which summaries will be\n        written. Lower values will result in slower training.\n    """"""\n    self.kappa = kappa\n    # num_tau_samples = N below equation (3) in the paper.\n    self.num_tau_samples = num_tau_samples\n    # num_tau_prime_samples = N\' below equation (3) in the paper.\n    self.num_tau_prime_samples = num_tau_prime_samples\n    # num_quantile_samples = k below equation (3) in the paper.\n    self.num_quantile_samples = num_quantile_samples\n    # quantile_embedding_dim = n above equation (4) in the paper.\n    self.quantile_embedding_dim = quantile_embedding_dim\n    # option to perform double dqn.\n    self.double_dqn = double_dqn\n\n    super(ImplicitQuantileAgent, self).__init__(\n        sess=sess,\n        num_actions=num_actions,\n        network=network,\n        summary_writer=summary_writer,\n        summary_writing_frequency=summary_writing_frequency)\n\n  def _create_network(self, name):\n    r""""""Builds an Implicit Quantile ConvNet.\n\n    Args:\n      name: str, this name is passed to the tf.keras.Model and used to create\n        variable scope under the hood by the tf.keras.Model.\n    Returns:\n      network: tf.keras.Model, the network instantiated by the Keras model.\n    """"""\n    network = self.network(self.num_actions, self.quantile_embedding_dim,\n                           name=name)\n    return network\n\n  def _build_networks(self):\n    """"""Builds the IQN computations needed for acting and training.\n\n    These are:\n      self.online_convnet: For computing the current state\'s quantile values.\n      self.target_convnet: For computing the next state\'s target quantile\n        values.\n      self._net_outputs: The actual quantile values.\n      self._q_argmax: The action maximizing the current state\'s Q-values.\n      self._replay_net_outputs: The replayed states\' quantile values.\n      self._replay_next_target_net_outputs: The replayed next states\' target\n        quantile values.\n    """"""\n    self.online_convnet = self._create_network(name=\'Online\')\n    self.target_convnet = self._create_network(name=\'Target\')\n\n    # Compute the Q-values which are used for action selection in the current\n    # state.\n    self._net_outputs = self.online_convnet(self.state_ph,\n                                            self.num_quantile_samples)\n    # Shape of self._net_outputs.quantile_values:\n    # num_quantile_samples x num_actions.\n    # e.g. if num_actions is 2, it might look something like this:\n    # Vals for Quantile .2  Vals for Quantile .4  Vals for Quantile .6\n    #    [[0.1, 0.5],         [0.15, -0.3],          [0.15, -0.2]]\n    # Q-values = [(0.1 + 0.15 + 0.15)/3, (0.5 + 0.15 + -0.2)/3].\n    self._q_values = tf.reduce_mean(self._net_outputs.quantile_values, axis=0)\n    self._q_argmax = tf.argmax(self._q_values, axis=0)\n\n    self._replay_net_outputs = self.online_convnet(self._replay.states,\n                                                   self.num_tau_samples)\n    # Shape: (num_tau_samples x batch_size) x num_actions.\n    self._replay_net_quantile_values = self._replay_net_outputs.quantile_values\n    self._replay_net_quantiles = self._replay_net_outputs.quantiles\n\n    # Do the same for next states in the replay buffer.\n    self._replay_net_target_outputs = self.target_convnet(\n        self._replay.next_states, self.num_tau_prime_samples)\n    # Shape: (num_tau_prime_samples x batch_size) x num_actions.\n    vals = self._replay_net_target_outputs.quantile_values\n    self._replay_net_target_quantile_values = vals\n\n    # Compute Q-values which are used for action selection for the next states\n    # in the replay buffer. Compute the argmax over the Q-values.\n    if self.double_dqn:\n      outputs_action = self.online_convnet(self._replay.next_states,\n                                           self.num_quantile_samples)\n    else:\n      outputs_action = self.target_convnet(self._replay.next_states,\n                                           self.num_quantile_samples)\n\n    # Shape: (num_quantile_samples x batch_size) x num_actions.\n    target_quantile_values_action = outputs_action.quantile_values\n    # Shape: num_quantile_samples x batch_size x num_actions.\n    target_quantile_values_action = tf.reshape(target_quantile_values_action,\n                                               [self.num_quantile_samples,\n                                                self._replay.batch_size,\n                                                self.num_actions])\n    # Shape: batch_size x num_actions.\n    self._replay_net_target_q_values = tf.squeeze(tf.reduce_mean(\n        target_quantile_values_action, axis=0))\n    self._replay_next_qt_argmax = tf.argmax(\n        self._replay_net_target_q_values, axis=1)\n\n  def _build_target_quantile_values_op(self):\n    """"""Build an op used as a target for return values at given quantiles.\n\n    Returns:\n      An op calculating the target quantile return.\n    """"""\n    batch_size = tf.shape(self._replay.rewards)[0]\n    # Shape of rewards: (num_tau_prime_samples x batch_size) x 1.\n    rewards = self._replay.rewards[:, None]\n    rewards = tf.tile(rewards, [self.num_tau_prime_samples, 1])\n\n    is_terminal_multiplier = 1. - tf.to_float(self._replay.terminals)\n    # Incorporate terminal state to discount factor.\n    # size of gamma_with_terminal: (num_tau_prime_samples x batch_size) x 1.\n    gamma_with_terminal = self.cumulative_gamma * is_terminal_multiplier\n    gamma_with_terminal = tf.tile(gamma_with_terminal[:, None],\n                                  [self.num_tau_prime_samples, 1])\n\n    # Get the indices of the maximium Q-value across the action dimension.\n    # Shape of replay_next_qt_argmax: (num_tau_prime_samples x batch_size) x 1.\n\n    replay_next_qt_argmax = tf.tile(\n        self._replay_next_qt_argmax[:, None], [self.num_tau_prime_samples, 1])\n\n    # Shape of batch_indices: (num_tau_prime_samples x batch_size) x 1.\n    batch_indices = tf.cast(tf.range(\n        self.num_tau_prime_samples * batch_size)[:, None], tf.int64)\n\n    # Shape of batch_indexed_target_values:\n    # (num_tau_prime_samples x batch_size) x 2.\n    batch_indexed_target_values = tf.concat(\n        [batch_indices, replay_next_qt_argmax], axis=1)\n\n    # Shape of next_target_values: (num_tau_prime_samples x batch_size) x 1.\n    target_quantile_values = tf.gather_nd(\n        self._replay_net_target_quantile_values,\n        batch_indexed_target_values)[:, None]\n\n    return rewards + gamma_with_terminal * target_quantile_values\n\n  def _build_train_op(self):\n    """"""Builds a training op.\n\n    Returns:\n      train_op: An op performing one step of training from replay data.\n    """"""\n    batch_size = tf.shape(self._replay.rewards)[0]\n\n    target_quantile_values = tf.stop_gradient(\n        self._build_target_quantile_values_op())\n    # Reshape to self.num_tau_prime_samples x batch_size x 1 since this is\n    # the manner in which the target_quantile_values are tiled.\n    target_quantile_values = tf.reshape(target_quantile_values,\n                                        [self.num_tau_prime_samples,\n                                         batch_size, 1])\n    # Transpose dimensions so that the dimensionality is batch_size x\n    # self.num_tau_prime_samples x 1 to prepare for computation of\n    # Bellman errors.\n    # Final shape of target_quantile_values:\n    # batch_size x num_tau_prime_samples x 1.\n    target_quantile_values = tf.transpose(target_quantile_values, [1, 0, 2])\n\n    # Shape of indices: (num_tau_samples x batch_size) x 1.\n    # Expand dimension by one so that it can be used to index into all the\n    # quantiles when using the tf.gather_nd function (see below).\n    indices = tf.range(self.num_tau_samples * batch_size)[:, None]\n\n    # Expand the dimension by one so that it can be used to index into all the\n    # quantiles when using the tf.gather_nd function (see below).\n    reshaped_actions = self._replay.actions[:, None]\n    reshaped_actions = tf.tile(reshaped_actions, [self.num_tau_samples, 1])\n    # Shape of reshaped_actions: (num_tau_samples x batch_size) x 2.\n    reshaped_actions = tf.concat([indices, reshaped_actions], axis=1)\n\n    chosen_action_quantile_values = tf.gather_nd(\n        self._replay_net_quantile_values, reshaped_actions)\n    # Reshape to self.num_tau_samples x batch_size x 1 since this is the manner\n    # in which the quantile values are tiled.\n    chosen_action_quantile_values = tf.reshape(chosen_action_quantile_values,\n                                               [self.num_tau_samples,\n                                                batch_size, 1])\n    # Transpose dimensions so that the dimensionality is batch_size x\n    # self.num_tau_samples x 1 to prepare for computation of\n    # Bellman errors.\n    # Final shape of chosen_action_quantile_values:\n    # batch_size x num_tau_samples x 1.\n    chosen_action_quantile_values = tf.transpose(\n        chosen_action_quantile_values, [1, 0, 2])\n\n    # Shape of bellman_erors and huber_loss:\n    # batch_size x num_tau_prime_samples x num_tau_samples x 1.\n    bellman_errors = target_quantile_values[\n        :, :, None, :] - chosen_action_quantile_values[:, None, :, :]\n    # The huber loss (see Section 2.3 of the paper) is defined via two cases:\n    # case_one: |bellman_errors| <= kappa\n    # case_two: |bellman_errors| > kappa\n    huber_loss_case_one = tf.to_float(\n        tf.abs(bellman_errors) <= self.kappa) * 0.5 * bellman_errors ** 2\n    huber_loss_case_two = tf.to_float(\n        tf.abs(bellman_errors) > self.kappa) * self.kappa * (\n            tf.abs(bellman_errors) - 0.5 * self.kappa)\n    huber_loss = huber_loss_case_one + huber_loss_case_two\n\n    # Reshape replay_quantiles to batch_size x num_tau_samples x 1\n    replay_quantiles = tf.reshape(\n        self._replay_net_quantiles, [self.num_tau_samples, batch_size, 1])\n    replay_quantiles = tf.transpose(replay_quantiles, [1, 0, 2])\n\n    # Tile by num_tau_prime_samples along a new dimension. Shape is now\n    # batch_size x num_tau_prime_samples x num_tau_samples x 1.\n    # These quantiles will be used for computation of the quantile huber loss\n    # below (see section 2.3 of the paper).\n    replay_quantiles = tf.to_float(tf.tile(\n        replay_quantiles[:, None, :, :], [1, self.num_tau_prime_samples, 1, 1]))\n    # Shape: batch_size x num_tau_prime_samples x num_tau_samples x 1.\n    quantile_huber_loss = (tf.abs(replay_quantiles - tf.stop_gradient(\n        tf.to_float(bellman_errors < 0))) * huber_loss) / self.kappa\n    # Sum over current quantile value (num_tau_samples) dimension,\n    # average over target quantile value (num_tau_prime_samples) dimension.\n    # Shape: batch_size x num_tau_prime_samples x 1.\n    loss = tf.reduce_sum(quantile_huber_loss, axis=2)\n    # Shape: batch_size x 1.\n    loss = tf.reduce_mean(loss, axis=1)\n\n    # TODO(kumasaurabh): Add prioritized replay functionality here.\n    update_priorities_op = tf.no_op()\n    with tf.control_dependencies([update_priorities_op]):\n      if self.summary_writer is not None:\n        with tf.variable_scope(\'Losses\'):\n          tf.summary.scalar(\'QuantileLoss\', tf.reduce_mean(loss))\n      return self.optimizer.minimize(tf.reduce_mean(loss)), tf.reduce_mean(loss)\n'"
dopamine/agents/rainbow/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
dopamine/agents/rainbow/rainbow_agent.py,53,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Compact implementation of a simplified Rainbow agent.\n\nSpecifically, we implement the following components from Rainbow:\n\n  * n-step updates;\n  * prioritized replay; and\n  * distributional RL.\n\nThese three components were found to significantly impact the performance of\nthe Atari game-playing agent.\n\nFurthermore, our implementation does away with some minor hyperparameter\nchoices. Specifically, we\n\n  * keep the beta exponent fixed at beta=0.5, rather than increase it linearly;\n  * remove the alpha parameter, which was set to alpha=0.5 throughout the paper.\n\nDetails in ""Rainbow: Combining Improvements in Deep Reinforcement Learning"" by\nHessel et al. (2018).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom dopamine.agents.dqn import dqn_agent\nfrom dopamine.discrete_domains import atari_lib\nfrom dopamine.replay_memory import prioritized_replay_buffer\nimport tensorflow.compat.v1 as tf\n\nimport gin.tf\n\n\n@gin.configurable\nclass RainbowAgent(dqn_agent.DQNAgent):\n  """"""A compact implementation of a simplified Rainbow agent.""""""\n\n  def __init__(self,\n               sess,\n               num_actions,\n               observation_shape=dqn_agent.NATURE_DQN_OBSERVATION_SHAPE,\n               observation_dtype=dqn_agent.NATURE_DQN_DTYPE,\n               stack_size=dqn_agent.NATURE_DQN_STACK_SIZE,\n               network=atari_lib.RainbowNetwork,\n               num_atoms=51,\n               vmax=10.,\n               gamma=0.99,\n               update_horizon=1,\n               min_replay_history=20000,\n               update_period=4,\n               target_update_period=8000,\n               epsilon_fn=dqn_agent.linearly_decaying_epsilon,\n               epsilon_train=0.01,\n               epsilon_eval=0.001,\n               epsilon_decay_period=250000,\n               replay_scheme=\'prioritized\',\n               tf_device=\'/cpu:*\',\n               use_staging=True,\n               optimizer=tf.train.AdamOptimizer(\n                   learning_rate=0.00025, epsilon=0.0003125),\n               summary_writer=None,\n               summary_writing_frequency=500):\n    """"""Initializes the agent and constructs the components of its graph.\n\n    Args:\n      sess: `tf.Session`, for executing ops.\n      num_actions: int, number of actions the agent can take at any state.\n      observation_shape: tuple of ints or an int. If single int, the observation\n        is assumed to be a 2D square.\n      observation_dtype: tf.DType, specifies the type of the observations. Note\n        that if your inputs are continuous, you should set this to tf.float32.\n      stack_size: int, number of frames to use in state stack.\n      network: tf.Keras.Model, expects four parameters:\n        (num_actions, num_atoms, support, network_type).  This class is used to\n        generate network instances that are used by the agent. Each\n        instantiation would have different set of variables. See\n        dopamine.discrete_domains.atari_lib.RainbowNetwork as an example.\n      num_atoms: int, the number of buckets of the value function distribution.\n      vmax: float, the value distribution support is [-vmax, vmax].\n      gamma: float, discount factor with the usual RL meaning.\n      update_horizon: int, horizon at which updates are performed, the \'n\' in\n        n-step update.\n      min_replay_history: int, number of transitions that should be experienced\n        before the agent begins training its value function.\n      update_period: int, period between DQN updates.\n      target_update_period: int, update period for the target network.\n      epsilon_fn: function expecting 4 parameters:\n        (decay_period, step, warmup_steps, epsilon). This function should return\n        the epsilon value used for exploration during training.\n      epsilon_train: float, the value to which the agent\'s epsilon is eventually\n        decayed during training.\n      epsilon_eval: float, epsilon used when evaluating the agent.\n      epsilon_decay_period: int, length of the epsilon decay schedule.\n      replay_scheme: str, \'prioritized\' or \'uniform\', the sampling scheme of the\n        replay memory.\n      tf_device: str, Tensorflow device on which the agent\'s graph is executed.\n      use_staging: bool, when True use a staging area to prefetch the next\n        training batch, speeding training up by about 30%.\n      optimizer: `tf.train.Optimizer`, for training the value function.\n      summary_writer: SummaryWriter object for outputting training statistics.\n        Summary writing disabled if set to None.\n      summary_writing_frequency: int, frequency with which summaries will be\n        written. Lower values will result in slower training.\n    """"""\n    # We need this because some tools convert round floats into ints.\n    vmax = float(vmax)\n    self._num_atoms = num_atoms\n    self._support = tf.linspace(-vmax, vmax, num_atoms)\n    self._replay_scheme = replay_scheme\n    # TODO(b/110897128): Make agent optimizer attribute private.\n    self.optimizer = optimizer\n\n    dqn_agent.DQNAgent.__init__(\n        self,\n        sess=sess,\n        num_actions=num_actions,\n        observation_shape=observation_shape,\n        observation_dtype=observation_dtype,\n        stack_size=stack_size,\n        network=network,\n        gamma=gamma,\n        update_horizon=update_horizon,\n        min_replay_history=min_replay_history,\n        update_period=update_period,\n        target_update_period=target_update_period,\n        epsilon_fn=epsilon_fn,\n        epsilon_train=epsilon_train,\n        epsilon_eval=epsilon_eval,\n        epsilon_decay_period=epsilon_decay_period,\n        tf_device=tf_device,\n        use_staging=use_staging,\n        optimizer=self.optimizer,\n        summary_writer=summary_writer,\n        summary_writing_frequency=summary_writing_frequency)\n\n  def _create_network(self, name):\n    """"""Builds a convolutional network that outputs Q-value distributions.\n\n    Args:\n      name: str, this name is passed to the tf.keras.Model and used to create\n        variable scope under the hood by the tf.keras.Model.\n    Returns:\n      network: tf.keras.Model, the network instantiated by the Keras model.\n    """"""\n    network = self.network(self.num_actions, self._num_atoms, self._support,\n                           name=name)\n    return network\n\n  def _build_replay_buffer(self, use_staging):\n    """"""Creates the replay buffer used by the agent.\n\n    Args:\n      use_staging: bool, if True, uses a staging area to prefetch data for\n        faster training.\n\n    Returns:\n      A `WrappedPrioritizedReplayBuffer` object.\n\n    Raises:\n      ValueError: if given an invalid replay scheme.\n    """"""\n    if self._replay_scheme not in [\'uniform\', \'prioritized\']:\n      raise ValueError(\'Invalid replay scheme: {}\'.format(self._replay_scheme))\n    # Both replay schemes use the same data structure, but the \'uniform\' scheme\n    # sets all priorities to the same value (which yields uniform sampling).\n    return prioritized_replay_buffer.WrappedPrioritizedReplayBuffer(\n        observation_shape=self.observation_shape,\n        stack_size=self.stack_size,\n        use_staging=use_staging,\n        update_horizon=self.update_horizon,\n        gamma=self.gamma,\n        observation_dtype=self.observation_dtype.as_numpy_dtype)\n\n  def _build_target_distribution(self):\n    """"""Builds the C51 target distribution as per Bellemare et al. (2017).\n\n    First, we compute the support of the Bellman target, r + gamma Z\'. Where Z\'\n    is the support of the next state distribution:\n\n      * Evenly spaced in [-vmax, vmax] if the current state is nonterminal;\n      * 0 otherwise (duplicated num_atoms times).\n\n    Second, we compute the next-state probabilities, corresponding to the action\n    with highest expected value.\n\n    Finally we project the Bellman target (support + probabilities) onto the\n    original support.\n\n    Returns:\n      target_distribution: tf.tensor, the target distribution from the replay.\n    """"""\n    batch_size = self._replay.batch_size\n\n    # size of rewards: batch_size x 1\n    rewards = self._replay.rewards[:, None]\n\n    # size of tiled_support: batch_size x num_atoms\n    tiled_support = tf.tile(self._support, [batch_size])\n    tiled_support = tf.reshape(tiled_support, [batch_size, self._num_atoms])\n\n    # size of target_support: batch_size x num_atoms\n\n    is_terminal_multiplier = 1. - tf.cast(self._replay.terminals, tf.float32)\n    # Incorporate terminal state to discount factor.\n    # size of gamma_with_terminal: batch_size x 1\n    gamma_with_terminal = self.cumulative_gamma * is_terminal_multiplier\n    gamma_with_terminal = gamma_with_terminal[:, None]\n\n    target_support = rewards + gamma_with_terminal * tiled_support\n\n    # size of next_qt_argmax: 1 x batch_size\n    next_qt_argmax = tf.argmax(\n        self._replay_next_target_net_outputs.q_values, axis=1)[:, None]\n    batch_indices = tf.range(tf.to_int64(batch_size))[:, None]\n    # size of next_qt_argmax: batch_size x 2\n    batch_indexed_next_qt_argmax = tf.concat(\n        [batch_indices, next_qt_argmax], axis=1)\n\n    # size of next_probabilities: batch_size x num_atoms\n    next_probabilities = tf.gather_nd(\n        self._replay_next_target_net_outputs.probabilities,\n        batch_indexed_next_qt_argmax)\n\n    return project_distribution(target_support, next_probabilities,\n                                self._support)\n\n  def _build_train_op(self):\n    """"""Builds a training op.\n\n    Returns:\n      train_op: An op performing one step of training from replay data.\n    """"""\n    target_distribution = tf.stop_gradient(self._build_target_distribution())\n\n    # size of indices: batch_size x 1.\n    indices = tf.range(tf.shape(self._replay_net_outputs.logits)[0])[:, None]\n    # size of reshaped_actions: batch_size x 2.\n    reshaped_actions = tf.concat([indices, self._replay.actions[:, None]], 1)\n    # For each element of the batch, fetch the logits for its selected action.\n    chosen_action_logits = tf.gather_nd(self._replay_net_outputs.logits,\n                                        reshaped_actions)\n\n    loss = tf.nn.softmax_cross_entropy_with_logits(\n        labels=target_distribution,\n        logits=chosen_action_logits)\n\n    if self._replay_scheme == \'prioritized\':\n      # The original prioritized experience replay uses a linear exponent\n      # schedule 0.4 -> 1.0. Comparing the schedule to a fixed exponent of 0.5\n      # on 5 games (Asterix, Pong, Q*Bert, Seaquest, Space Invaders) suggested\n      # a fixed exponent actually performs better, except on Pong.\n      probs = self._replay.transition[\'sampling_probabilities\']\n      loss_weights = 1.0 / tf.sqrt(probs + 1e-10)\n      loss_weights /= tf.reduce_max(loss_weights)\n\n      # Rainbow and prioritized replay are parametrized by an exponent alpha,\n      # but in both cases it is set to 0.5 - for simplicity\'s sake we leave it\n      # as is here, using the more direct tf.sqrt(). Taking the square root\n      # ""makes sense"", as we are dealing with a squared loss.\n      # Add a small nonzero value to the loss to avoid 0 priority items. While\n      # technically this may be okay, setting all items to 0 priority will cause\n      # troubles, and also result in 1.0 / 0.0 = NaN correction terms.\n      update_priorities_op = self._replay.tf_set_priority(\n          self._replay.indices, tf.sqrt(loss + 1e-10))\n\n      # Weight the loss by the inverse priorities.\n      loss = loss_weights * loss\n    else:\n      update_priorities_op = tf.no_op()\n\n    with tf.control_dependencies([update_priorities_op]):\n      if self.summary_writer is not None:\n        with tf.variable_scope(\'Losses\'):\n          tf.summary.scalar(\'CrossEntropyLoss\', tf.reduce_mean(loss))\n      # Schaul et al. reports a slightly different rule, where 1/N is also\n      # exponentiated by beta. Not doing so seems more reasonable, and did not\n      # impact performance in our experiments.\n      return self.optimizer.minimize(tf.reduce_mean(loss)), loss\n\n  def _store_transition(self,\n                        last_observation,\n                        action,\n                        reward,\n                        is_terminal,\n                        priority=None):\n    """"""Stores a transition when in training mode.\n\n    Executes a tf session and executes replay buffer ops in order to store the\n    following tuple in the replay buffer (last_observation, action, reward,\n    is_terminal, priority).\n\n    Args:\n      last_observation: Last observation, type determined via observation_type\n        parameter in the replay_memory constructor.\n      action: An integer, the action taken.\n      reward: A float, the reward.\n      is_terminal: Boolean indicating if the current state is a terminal state.\n      priority: Float. Priority of sampling the transition. If None, the default\n        priority will be used. If replay scheme is uniform, the default priority\n        is 1. If the replay scheme is prioritized, the default priority is the\n        maximum ever seen [Schaul et al., 2015].\n    """"""\n    if priority is None:\n      if self._replay_scheme == \'uniform\':\n        priority = 1.\n      else:\n        priority = self._replay.memory.sum_tree.max_recorded_priority\n\n    if not self.eval_mode:\n      self._replay.add(last_observation, action, reward, is_terminal, priority)\n\n\ndef project_distribution(supports, weights, target_support,\n                         validate_args=False):\n  """"""Projects a batch of (support, weights) onto target_support.\n\n  Based on equation (7) in (Bellemare et al., 2017):\n    https://arxiv.org/abs/1707.06887\n  In the rest of the comments we will refer to this equation simply as Eq7.\n\n  This code is not easy to digest, so we will use a running example to clarify\n  what is going on, with the following sample inputs:\n\n    * supports =       [[0, 2, 4, 6, 8],\n                        [1, 3, 4, 5, 6]]\n    * weights =        [[0.1, 0.6, 0.1, 0.1, 0.1],\n                        [0.1, 0.2, 0.5, 0.1, 0.1]]\n    * target_support = [4, 5, 6, 7, 8]\n\n  In the code below, comments preceded with \'Ex:\' will be referencing the above\n  values.\n\n  Args:\n    supports: Tensor of shape (batch_size, num_dims) defining supports for the\n      distribution.\n    weights: Tensor of shape (batch_size, num_dims) defining weights on the\n      original support points. Although for the CategoricalDQN agent these\n      weights are probabilities, it is not required that they are.\n    target_support: Tensor of shape (num_dims) defining support of the projected\n      distribution. The values must be monotonically increasing. Vmin and Vmax\n      will be inferred from the first and last elements of this tensor,\n      respectively. The values in this tensor must be equally spaced.\n    validate_args: Whether we will verify the contents of the\n      target_support parameter.\n\n  Returns:\n    A Tensor of shape (batch_size, num_dims) with the projection of a batch of\n    (support, weights) onto target_support.\n\n  Raises:\n    ValueError: If target_support has no dimensions, or if shapes of supports,\n      weights, and target_support are incompatible.\n  """"""\n  target_support_deltas = target_support[1:] - target_support[:-1]\n  # delta_z = `\\Delta z` in Eq7.\n  delta_z = target_support_deltas[0]\n  validate_deps = []\n  supports.shape.assert_is_compatible_with(weights.shape)\n  supports[0].shape.assert_is_compatible_with(target_support.shape)\n  target_support.shape.assert_has_rank(1)\n  if validate_args:\n    # Assert that supports and weights have the same shapes.\n    validate_deps.append(\n        tf.Assert(\n            tf.reduce_all(tf.equal(tf.shape(supports), tf.shape(weights))),\n            [supports, weights]))\n    # Assert that elements of supports and target_support have the same shape.\n    validate_deps.append(\n        tf.Assert(\n            tf.reduce_all(\n                tf.equal(tf.shape(supports)[1], tf.shape(target_support))),\n            [supports, target_support]))\n    # Assert that target_support has a single dimension.\n    validate_deps.append(\n        tf.Assert(\n            tf.equal(tf.size(tf.shape(target_support)), 1), [target_support]))\n    # Assert that the target_support is monotonically increasing.\n    validate_deps.append(\n        tf.Assert(tf.reduce_all(target_support_deltas > 0), [target_support]))\n    # Assert that the values in target_support are equally spaced.\n    validate_deps.append(\n        tf.Assert(\n            tf.reduce_all(tf.equal(target_support_deltas, delta_z)),\n            [target_support]))\n\n  with tf.control_dependencies(validate_deps):\n    # Ex: `v_min, v_max = 4, 8`.\n    v_min, v_max = target_support[0], target_support[-1]\n    # Ex: `batch_size = 2`.\n    batch_size = tf.shape(supports)[0]\n    # `N` in Eq7.\n    # Ex: `num_dims = 5`.\n    num_dims = tf.shape(target_support)[0]\n    # clipped_support = `[\\hat{T}_{z_j}]^{V_max}_{V_min}` in Eq7.\n    # Ex: `clipped_support = [[[ 4.  4.  4.  6.  8.]]\n    #                         [[ 4.  4.  4.  5.  6.]]]`.\n    clipped_support = tf.clip_by_value(supports, v_min, v_max)[:, None, :]\n    # Ex: `tiled_support = [[[[ 4.  4.  4.  6.  8.]\n    #                         [ 4.  4.  4.  6.  8.]\n    #                         [ 4.  4.  4.  6.  8.]\n    #                         [ 4.  4.  4.  6.  8.]\n    #                         [ 4.  4.  4.  6.  8.]]\n    #                        [[ 4.  4.  4.  5.  6.]\n    #                         [ 4.  4.  4.  5.  6.]\n    #                         [ 4.  4.  4.  5.  6.]\n    #                         [ 4.  4.  4.  5.  6.]\n    #                         [ 4.  4.  4.  5.  6.]]]]`.\n    tiled_support = tf.tile([clipped_support], [1, 1, num_dims, 1])\n    # Ex: `reshaped_target_support = [[[ 4.]\n    #                                  [ 5.]\n    #                                  [ 6.]\n    #                                  [ 7.]\n    #                                  [ 8.]]\n    #                                 [[ 4.]\n    #                                  [ 5.]\n    #                                  [ 6.]\n    #                                  [ 7.]\n    #                                  [ 8.]]]`.\n    reshaped_target_support = tf.tile(target_support[:, None], [batch_size, 1])\n    reshaped_target_support = tf.reshape(reshaped_target_support,\n                                         [batch_size, num_dims, 1])\n    # numerator = `|clipped_support - z_i|` in Eq7.\n    # Ex: `numerator = [[[[ 0.  0.  0.  2.  4.]\n    #                     [ 1.  1.  1.  1.  3.]\n    #                     [ 2.  2.  2.  0.  2.]\n    #                     [ 3.  3.  3.  1.  1.]\n    #                     [ 4.  4.  4.  2.  0.]]\n    #                    [[ 0.  0.  0.  1.  2.]\n    #                     [ 1.  1.  1.  0.  1.]\n    #                     [ 2.  2.  2.  1.  0.]\n    #                     [ 3.  3.  3.  2.  1.]\n    #                     [ 4.  4.  4.  3.  2.]]]]`.\n    numerator = tf.abs(tiled_support - reshaped_target_support)\n    quotient = 1 - (numerator / delta_z)\n    # clipped_quotient = `[1 - numerator / (\\Delta z)]_0^1` in Eq7.\n    # Ex: `clipped_quotient = [[[[ 1.  1.  1.  0.  0.]\n    #                            [ 0.  0.  0.  0.  0.]\n    #                            [ 0.  0.  0.  1.  0.]\n    #                            [ 0.  0.  0.  0.  0.]\n    #                            [ 0.  0.  0.  0.  1.]]\n    #                           [[ 1.  1.  1.  0.  0.]\n    #                            [ 0.  0.  0.  1.  0.]\n    #                            [ 0.  0.  0.  0.  1.]\n    #                            [ 0.  0.  0.  0.  0.]\n    #                            [ 0.  0.  0.  0.  0.]]]]`.\n    clipped_quotient = tf.clip_by_value(quotient, 0, 1)\n    # Ex: `weights = [[ 0.1  0.6  0.1  0.1  0.1]\n    #                 [ 0.1  0.2  0.5  0.1  0.1]]`.\n    weights = weights[:, None, :]\n    # inner_prod = `\\sum_{j=0}^{N-1} clipped_quotient * p_j(x\', \\pi(x\'))`\n    # in Eq7.\n    # Ex: `inner_prod = [[[[ 0.1  0.6  0.1  0.  0. ]\n    #                      [ 0.   0.   0.   0.  0. ]\n    #                      [ 0.   0.   0.   0.1 0. ]\n    #                      [ 0.   0.   0.   0.  0. ]\n    #                      [ 0.   0.   0.   0.  0.1]]\n    #                     [[ 0.1  0.2  0.5  0.  0. ]\n    #                      [ 0.   0.   0.   0.1 0. ]\n    #                      [ 0.   0.   0.   0.  0.1]\n    #                      [ 0.   0.   0.   0.  0. ]\n    #                      [ 0.   0.   0.   0.  0. ]]]]`.\n    inner_prod = clipped_quotient * weights\n    # Ex: `projection = [[ 0.8 0.0 0.1 0.0 0.1]\n    #                    [ 0.8 0.1 0.1 0.0 0.0]]`.\n    projection = tf.reduce_sum(inner_prod, 3)\n    projection = tf.reshape(projection, [batch_size, num_dims])\n    return projection\n'"
tests/dopamine/discrete_domains/atari_lib_test.py,3,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for dopamine.discrete_domains.atari_lib.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\n\nfrom absl import flags\nfrom dopamine.discrete_domains import atari_lib\nimport gym\nimport mock\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nFLAGS = flags.FLAGS\n\n\nclass AtariLibTest(tf.test.TestCase):\n\n\n  def testCreateAtariEnvironmentWithoutGameName(self):\n    with self.assertRaises(AssertionError):\n      atari_lib.create_atari_environment()\n\n  @mock.patch.object(atari_lib, \'AtariPreprocessing\')\n  @mock.patch.object(gym, \'make\')\n  def testCreateAtariEnvironment(self, mock_gym_make, mock_atari_lib):\n    class MockGymEnv(object):\n\n      def __init__(self, env_name):\n        self.env = \'gym({})\'.format(env_name)\n\n    def fake_make_env(name):\n      return MockGymEnv(name)\n\n    mock_gym_make.side_effect = fake_make_env\n    # pylint: disable=unnecessary-lambda\n    mock_atari_lib.side_effect = lambda x: \'atari({})\'.format(x)\n    # pylint: enable=unnecessary-lambda\n    game_name = \'Test\'\n    env = atari_lib.create_atari_environment(game_name)\n    self.assertEqual(\'atari(gym(TestNoFrameskip-v0))\', env)\n\n\nclass MockALE(object):\n  """"""Mock internal ALE for testing.""""""\n\n  def __init__(self):\n    pass\n\n  def lives(self):\n    return 1\n\n  def getScreenGrayscale(self, screen):  # pylint: disable=invalid-name\n    screen.fill(self.screen_value)\n\n\nclass MockEnvironment(object):\n  """"""Mock environment for testing.""""""\n\n  def __init__(self, screen_size=10, max_steps=10):\n    self.max_steps = max_steps\n    self.screen_size = screen_size\n    self.ale = MockALE()\n    self.observation_space = np.empty((screen_size, screen_size))\n    self.game_over = False\n\n  def reset(self):\n    self.ale.screen_value = 10\n    self.num_steps = 0\n    return self.get_observation()\n\n  def get_observation(self):\n    observation = np.empty((self.screen_size, self.screen_size))\n    return self.ale.getScreenGrayscale(observation)\n\n  def step(self, action):\n    reward = -1. if action > 0 else 1.\n    self.num_steps += 1\n    is_terminal = self.num_steps >= self.max_steps\n\n    unused = 0\n    self.ale.screen_value -= 2\n    return (self.get_observation(), reward, is_terminal, unused)\n\n  def render(self, mode):\n    pass\n\n\nclass AtariPreprocessingTest(tf.test.TestCase):\n\n  def testResetPassesObservation(self):\n    env = MockEnvironment()\n    env = atari_lib.AtariPreprocessing(env, frame_skip=1, screen_size=16)\n    observation = env.reset()\n\n    self.assertEqual(observation.shape, (16, 16, 1))\n\n  def testTerminalPassedThrough(self):\n    max_steps = 10\n    env = MockEnvironment(max_steps=max_steps)\n    env = atari_lib.AtariPreprocessing(env, frame_skip=1)\n    env.reset()\n\n    # Make sure we get the right number of steps.\n    for _ in range(max_steps - 1):\n      _, _, is_terminal, _ = env.step(0)\n      self.assertFalse(is_terminal)\n\n    _, _, is_terminal, _ = env.step(0)\n    self.assertTrue(is_terminal)\n\n  def testFrameSkipAccumulatesReward(self):\n    frame_skip = 2\n    env = MockEnvironment()\n    env = atari_lib.AtariPreprocessing(env, frame_skip=frame_skip)\n    env.reset()\n\n    # Make sure we get the right number of steps. Reward is 1 when we\n    # pass in action 0.\n    _, reward, _, _ = env.step(0)\n    self.assertEqual(reward, frame_skip)\n\n  def testMaxFramePooling(self):\n    frame_skip = 2\n    env = MockEnvironment()\n    env = atari_lib.AtariPreprocessing(env, frame_skip=frame_skip)\n    env.reset()\n\n    # The first observation is 2, the second 0; max is 2.\n    observation, _, _, _ = env.step(0)\n    self.assertTrue((observation == 8).all())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/discrete_domains/checkpointer_test.py,8,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for dopamine.common.checkpointer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\n\n\n\nfrom absl import flags\nfrom dopamine.discrete_domains import checkpointer\nimport tensorflow.compat.v1 as tf\n\nFLAGS = flags.FLAGS\n\n\nclass CheckpointerTest(tf.test.TestCase):\n\n  def setUp(self):\n    self._test_subdir = os.path.join(\'/tmp/dopamine_tests\', \'checkpointing\')\n    shutil.rmtree(self._test_subdir, ignore_errors=True)\n    os.makedirs(self._test_subdir)\n\n  def testCheckpointingInitialization(self):\n    # Fails with empty directory.\n    with self.assertRaisesRegexp(ValueError,\n                                 \'No path provided to Checkpointer.\'):\n      checkpointer.Checkpointer(\'\')\n    # Fails with invalid directory.\n    invalid_dir = \'/does/not/exist\'\n    with self.assertRaisesRegexp(\n        ValueError, \'Unable to create checkpoint path: {}.\'.\n        format(invalid_dir)):\n      checkpointer.Checkpointer(invalid_dir)\n    # Succeeds with valid directory.\n    checkpointer.Checkpointer(\'/tmp/dopamine_tests\')\n    # This verifies initialization still works after the directory has already\n    # been created.\n    self.assertTrue(tf.gfile.Exists(\'/tmp/dopamine_tests\'))\n    checkpointer.Checkpointer(\'/tmp/dopamine_tests\')\n\n  def testLogToFileWithValidDirectoryDefaultPrefix(self):\n    exp_checkpointer = checkpointer.Checkpointer(self._test_subdir)\n    data = {\'data1\': 1, \'data2\': \'two\', \'data3\': (3, \'three\')}\n    iteration_number = 1729\n    exp_checkpointer.save_checkpoint(iteration_number, data)\n    loaded_data = exp_checkpointer.load_checkpoint(iteration_number)\n    self.assertEqual(data, loaded_data)\n    self.assertEqual(None,\n                     exp_checkpointer.load_checkpoint(iteration_number + 1))\n\n  def testLogToFileWithValidDirectoryCustomPrefix(self):\n    prefix = \'custom_prefix\'\n    exp_checkpointer = checkpointer.Checkpointer(self._test_subdir,\n                                                 checkpoint_file_prefix=prefix)\n    data = {\'data1\': 1, \'data2\': \'two\', \'data3\': (3, \'three\')}\n    iteration_number = 1729\n    exp_checkpointer.save_checkpoint(iteration_number, data)\n    loaded_data = exp_checkpointer.load_checkpoint(iteration_number)\n    self.assertEqual(data, loaded_data)\n    self.assertEqual(None,\n                     exp_checkpointer.load_checkpoint(iteration_number + 1))\n\n  def testLoadLatestCheckpointWithInvalidDir(self):\n    self.assertEqual(\n        -1, checkpointer.get_latest_checkpoint_number(\'/does/not/exist\'))\n\n  def testLoadLatestCheckpointWithEmptyDir(self):\n    self.assertEqual(\n        -1, checkpointer.get_latest_checkpoint_number(self._test_subdir))\n\n  def testLoadLatestCheckpointWithOverride(self):\n    override_number = 1729\n    self.assertEqual(\n        override_number,\n        checkpointer.get_latest_checkpoint_number(\n            \'/ignored\', override_number=override_number))\n\n  def testLoadLatestCheckpoint(self):\n    exp_checkpointer = checkpointer.Checkpointer(self._test_subdir)\n    first_iter = 1729\n    exp_checkpointer.save_checkpoint(first_iter, first_iter)\n    second_iter = first_iter + 1\n    exp_checkpointer.save_checkpoint(second_iter, second_iter)\n    self.assertEqual(\n        second_iter,\n        checkpointer.get_latest_checkpoint_number(self._test_subdir))\n\n  def testGarbageCollection(self):\n    custom_prefix = \'custom_prefix\'\n    exp_checkpointer = checkpointer.Checkpointer(\n        self._test_subdir, checkpoint_file_prefix=custom_prefix)\n    data = {\'data1\': 1, \'data2\': \'two\', \'data3\': (3, \'three\')}\n    deleted_log_files = 7\n    total_log_files = checkpointer.CHECKPOINT_DURATION + deleted_log_files\n    for iteration_number in range(total_log_files):\n      exp_checkpointer.save_checkpoint(iteration_number, data)\n    for iteration_number in range(total_log_files):\n      prefixes = [custom_prefix, \'sentinel_checkpoint_complete\']\n      for prefix in prefixes:\n        checkpoint_file = os.path.join(self._test_subdir, \'{}.{}\'.format(\n            prefix, iteration_number))\n        if iteration_number < deleted_log_files:\n          self.assertFalse(tf.gfile.Exists(checkpoint_file))\n        else:\n          self.assertTrue(tf.gfile.Exists(checkpoint_file))\n\n  def testGarbageCollectionWithCheckpointFrequency(self):\n    custom_prefix = \'custom_prefix\'\n    checkpoint_frequency = 3\n    exp_checkpointer = checkpointer.Checkpointer(\n        self._test_subdir, checkpoint_file_prefix=custom_prefix,\n        checkpoint_frequency=checkpoint_frequency)\n    data = {\'data1\': 1, \'data2\': \'two\', \'data3\': (3, \'three\')}\n    deleted_log_files = 6\n    total_log_files = (checkpointer.CHECKPOINT_DURATION *\n                       checkpoint_frequency) + deleted_log_files + 1\n\n    # The checkpoints will happen in iteration numbers 0,3,6,9,12,15,18.\n    # We are checking if checkpoints 0,3,6 are deleted.\n    for iteration_number in range(total_log_files):\n      exp_checkpointer.save_checkpoint(iteration_number,\n                                       data)\n    for iteration_number in range(total_log_files):\n      prefixes = [custom_prefix, \'sentinel_checkpoint_complete\']\n      for prefix in prefixes:\n        checkpoint_file = os.path.join(self._test_subdir, \'{}.{}\'.format(\n            prefix, iteration_number))\n        if iteration_number <= deleted_log_files:\n          self.assertFalse(tf.gfile.Exists(checkpoint_file))\n        else:\n          if iteration_number % checkpoint_frequency == 0:\n            self.assertTrue(tf.gfile.Exists(checkpoint_file))\n          else:\n            self.assertFalse(tf.gfile.Exists(checkpoint_file))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/discrete_domains/gym_lib_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for dopamine.discrete_domains.gym_lib.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom dopamine.discrete_domains import gym_lib\nimport tensorflow.compat.v1 as tf\n\n\nclass MockGymEnvironment(object):\n  """"""Mock environment for testing.""""""\n\n  def __init__(self):\n    self.observation_space = \'observation_space\'\n    self.action_space = \'action_space\'\n    self.reward_range = \'reward_range\'\n    self.metadata = \'metadata\'\n\n  def reset(self):\n    return \'reset\'\n\n  def step(self, unused_action):\n    return \'obs\', \'rew\', \'game_over\', \'info\'\n\n\nclass GymPreprocessingTest(tf.test.TestCase):\n\n  def testAll(self):\n    env = gym_lib.GymPreprocessing(MockGymEnvironment())\n    self.assertEqual(\'observation_space\', env.observation_space)\n    self.assertEqual(\'action_space\', env.action_space)\n    self.assertEqual(\'reward_range\', env.reward_range)\n    self.assertEqual(\'metadata\', env.metadata)\n    self.assertEqual(\'reset\', env.reset())\n    self.assertAllEqual([\'obs\', \'rew\', \'game_over\', \'info\'], env.step(0))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/discrete_domains/iteration_statistics_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for dopamine.common.iteration_statistics.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom dopamine.discrete_domains import iteration_statistics\nimport tensorflow.compat.v1 as tf\n\n\nclass IterationStatisticsTest(tf.test.TestCase):\n\n  def setUp(self):\n    pass\n\n  def testMissingValue(self):\n    statistics = iteration_statistics.IterationStatistics()\n    with self.assertRaises(KeyError):\n      _ = statistics.data_lists[\'missing_key\']\n\n  def testAddOneValue(self):\n    statistics = iteration_statistics.IterationStatistics()\n\n    # The statistics data structure should be empty a-priori.\n    self.assertEqual(0, len(statistics.data_lists))\n\n    statistics.append({\'key1\': 0})\n    # We should have exactly one list, containing one value.\n    self.assertEqual(1, len(statistics.data_lists))\n    self.assertEqual(1, len(statistics.data_lists[\'key1\']))\n    self.assertEqual(0, statistics.data_lists[\'key1\'][0])\n\n  def testAddManyValues(self):\n    my_pi = 3.14159\n\n    statistics = iteration_statistics.IterationStatistics()\n\n    # Add a number of items. Each item is added to the list corresponding to its\n    # given key.\n    statistics.append({\'rewards\': 0,\n                       \'nouns\': \'reinforcement\',\n                       \'angles\': my_pi})\n    # Add a second item to the \'nouns\' list.\n    statistics.append({\'nouns\': \'learning\'})\n\n    # There are three lists.\n    self.assertEqual(3, len(statistics.data_lists))\n    self.assertEqual(1, len(statistics.data_lists[\'rewards\']))\n    self.assertEqual(2, len(statistics.data_lists[\'nouns\']))\n    self.assertEqual(1, len(statistics.data_lists[\'angles\']))\n\n    self.assertEqual(0, statistics.data_lists[\'rewards\'][0])\n    self.assertEqual(\'reinforcement\', statistics.data_lists[\'nouns\'][0])\n    self.assertEqual(\'learning\', statistics.data_lists[\'nouns\'][1])\n    self.assertEqual(my_pi, statistics.data_lists[\'angles\'][0])\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/discrete_domains/logger_test.py,5,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for dopamine.logger.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport pickle\nimport shutil\n\n\nfrom absl import flags\nfrom dopamine.discrete_domains import logger\nimport tensorflow.compat.v1 as tf\n\nFLAGS = flags.FLAGS\n\n\nclass LoggerTest(tf.test.TestCase):\n\n  def setUp(self):\n    self._test_subdir = os.path.join(\'/tmp/dopamine_tests\', \'logging\')\n    shutil.rmtree(self._test_subdir, ignore_errors=True)\n    os.makedirs(self._test_subdir)\n\n  def testLoggingDisabledWithEmptyDirectory(self):\n    exp_logger = logger.Logger(\'\')\n    self.assertFalse(exp_logger.is_logging_enabled())\n\n  def testLoggingDisabledWithInvalidDirectory(self):\n    exp_logger = logger.Logger(\'/does/not/exist\')\n    self.assertFalse(exp_logger.is_logging_enabled())\n\n  def testLoggingEnabledWithValidDirectory(self):\n    exp_logger = logger.Logger(\'/tmp/dopamine_tests\')\n    self.assertTrue(exp_logger.is_logging_enabled())\n\n  def testSetEntry(self):\n    exp_logger = logger.Logger(\'/tmp/dopamine_tests\')\n    self.assertEqual(len(exp_logger.data.keys()), 0)\n    key = \'key\'\n    val = [1, 2, 3, 4]\n    exp_logger[key] = val\n    expected_dictionary = {}\n    expected_dictionary[key] = val\n    self.assertEqual(expected_dictionary, exp_logger.data)\n    # Calling __setitem__ with the same value should overwrite the previous\n    # value.\n    val = \'new value\'\n    exp_logger[key] = val\n    expected_dictionary[key] = val\n    self.assertEqual(expected_dictionary, exp_logger.data)\n\n  def testLogToFileWithInvalidDirectory(self):\n    exp_logger = logger.Logger(\'/does/not/exist\')\n    self.assertFalse(exp_logger.is_logging_enabled())\n    exp_logger.log_to_file(None, None)\n\n  def testLogToFileWithValidDirectory(self):\n    exp_logger = logger.Logger(self._test_subdir)\n    self.assertTrue(exp_logger.is_logging_enabled())\n    key = \'key\'\n    val = [1, 2, 3, 4]\n    exp_logger[key] = val\n    expected_dictionary = {}\n    expected_dictionary[key] = val\n    self.assertEqual(expected_dictionary, exp_logger.data)\n    iteration_number = 7\n    exp_logger.log_to_file(\'log\', iteration_number)\n    log_file = os.path.join(self._test_subdir,\n                            \'log_{}\'.format(iteration_number))\n    with tf.gfile.GFile(log_file, \'rb\') as f:\n      contents = f.read()\n    self.assertEqual(contents, pickle.dumps(expected_dictionary,\n                                            protocol=pickle.HIGHEST_PROTOCOL))\n\n  def testGarbageCollection(self):\n    exp_logger = logger.Logger(self._test_subdir)\n    self.assertTrue(exp_logger.is_logging_enabled())\n    key = \'key\'\n    val = [1, 2, 3, 4]\n    exp_logger[key] = val\n    expected_dictionary = {}\n    expected_dictionary[key] = val\n    self.assertEqual(expected_dictionary, exp_logger.data)\n    deleted_log_files = 7\n    total_log_files = logger.CHECKPOINT_DURATION + deleted_log_files\n    for iteration_number in range(total_log_files):\n      exp_logger.log_to_file(\'log\', iteration_number)\n    for iteration_number in range(total_log_files):\n      log_file = os.path.join(self._test_subdir,\n                              \'log_{}\'.format(iteration_number))\n      if iteration_number < deleted_log_files:\n        self.assertFalse(tf.gfile.Exists(log_file))\n      else:\n        self.assertTrue(tf.gfile.Exists(log_file))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/discrete_domains/run_experiment_test.py,5,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for dopamine.common.run_experiment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\n\n\n\nfrom absl import flags\nfrom dopamine.agents.dqn import dqn_agent\nfrom dopamine.agents.implicit_quantile import implicit_quantile_agent\nfrom dopamine.agents.rainbow import rainbow_agent\nfrom dopamine.discrete_domains import checkpointer\nfrom dopamine.discrete_domains import logger\nfrom dopamine.discrete_domains import run_experiment\nimport mock\nimport tensorflow.compat.v1 as tf\n\nimport gin.tf\n\nFLAGS = flags.FLAGS\n\n\ndef _create_mock_checkpointer():\n  mock_checkpointer = mock.Mock()\n  test_dictionary = {\'current_iteration\': 1729,\n                     \'logs\': \'logs\'}\n  mock_checkpointer.load_checkpoint.return_value = test_dictionary\n  return mock_checkpointer\n\n\nclass MockEnvironment(object):\n  """"""Mock environment for testing.""""""\n\n  def __init__(self, max_steps=10):\n    self._observation = 0\n    self.max_steps = max_steps\n    self.game_over = False\n\n  def reset(self):\n    self._observation = 0\n    return self._observation\n\n  def step(self, action):\n    self._observation += 1\n    reward_multiplier = -1 if action > 0 else 1\n    reward = self._observation * reward_multiplier\n    is_terminal = self._observation >= self.max_steps\n    self.game_over = is_terminal\n\n    unused = 0\n    return (self._observation, reward, is_terminal, unused)\n\n  def render(self, mode):\n    pass\n\n\nclass MockLogger(object):\n  """"""Class to mock the experiment logger.""""""\n\n  def __init__(self, test_cls=None, run_asserts=True, data=None):\n    self._test_cls = test_cls\n    self._run_asserts = run_asserts\n    self._iter = 0\n    self._calls_to_set = 0\n    self._calls_to_log = 0\n    self.data = data\n\n  def __setitem__(self, key, val):\n    if self._run_asserts:\n      self._test_cls.assertEqual(\'iteration_{:d}\'.format(self._iter), key)\n      self._test_cls.assertEqual(\'statistics\', val)\n      self._iter += 1\n    self._calls_to_set += 1\n\n  def log_to_file(self, filename_prefix, iteration_number):\n    if self._run_asserts:\n      self._test_cls.assertEqual(\n          \'prefix_{}\'.format(self._iter - 1),\n          \'{}_{}\'.format(filename_prefix, iteration_number))\n    self._calls_to_log += 1\n\n\nclass RunExperimentTest(tf.test.TestCase):\n\n  @mock.patch.object(gin, \'parse_config_files_and_bindings\')\n  def testLoadGinConfigs(self, mock_parse_config_files_and_bindings):\n    gin_files = [\'file1\', \'file2\', \'file3\']\n    gin_bindings = [\'binding1\', \'binding2\']\n    run_experiment.load_gin_configs(gin_files, gin_bindings)\n    self.assertEqual(1, mock_parse_config_files_and_bindings.call_count)\n    mock_args, mock_kwargs = mock_parse_config_files_and_bindings.call_args\n    self.assertEqual(gin_files, mock_args[0])\n    self.assertEqual(gin_bindings, mock_kwargs[\'bindings\'])\n    self.assertFalse(mock_kwargs[\'skip_unknown\'])\n\n  def testNoAgentName(self):\n    with self.assertRaises(AssertionError):\n      _ = run_experiment.create_agent(self.test_session(), mock.Mock())\n\n  @mock.patch.object(dqn_agent, \'DQNAgent\')\n  def testCreateDQNAgent(self, mock_dqn_agent):\n    def mock_fn(unused_sess, num_actions, summary_writer):\n      del summary_writer\n      return num_actions * 10\n\n    mock_dqn_agent.side_effect = mock_fn\n    environment = mock.Mock()\n    environment.action_space.n = 7\n    self.assertEqual(70, run_experiment.create_agent(self.test_session(),\n                                                     environment,\n                                                     agent_name=\'dqn\'))\n\n  @mock.patch.object(rainbow_agent, \'RainbowAgent\')\n  def testCreateRainbowAgent(self, mock_rainbow_agent):\n    def mock_fn(unused_sess, num_actions, summary_writer):\n      del summary_writer\n      return num_actions * 10\n\n    mock_rainbow_agent.side_effect = mock_fn\n    environment = mock.Mock()\n    environment.action_space.n = 7\n    self.assertEqual(70, run_experiment.create_agent(self.test_session(),\n                                                     environment,\n                                                     agent_name=\'rainbow\'))\n\n  @mock.patch.object(implicit_quantile_agent, \'ImplicitQuantileAgent\')\n  def testCreateImplicitQuantileAgent(self, mock_implicit_quantile_agent):\n    def mock_fn(unused_sess, num_actions, summary_writer):\n      del summary_writer\n      return num_actions * 10\n\n    mock_implicit_quantile_agent.side_effect = mock_fn\n    environment = mock.Mock()\n    environment.action_space.n = 7\n    self.assertEqual(70, run_experiment.create_agent(\n        self.test_session(), environment, agent_name=\'implicit_quantile\'))\n\n  def testCreateRunnerUnknown(self):\n    base_dir = \'/tmp\'\n    with self.assertRaisesRegexp(ValueError, \'Unknown schedule\'):\n      run_experiment.create_runner(base_dir,\n                                   \'Unknown schedule\')\n\n  @mock.patch.object(run_experiment, \'Runner\')\n  @mock.patch.object(run_experiment, \'create_agent\')\n  def testCreateRunner(self, mock_create_agent, mock_runner_constructor):\n    base_dir = \'/tmp\'\n    run_experiment.create_runner(base_dir)\n    self.assertEqual(1, mock_runner_constructor.call_count)\n    mock_args, _ = mock_runner_constructor.call_args\n    self.assertEqual(base_dir, mock_args[0])\n    self.assertEqual(mock_create_agent, mock_args[1])\n\n  @mock.patch.object(run_experiment, \'TrainRunner\')\n  @mock.patch.object(run_experiment, \'create_agent\')\n  def testCreateTrainRunner(self, mock_create_agent, mock_runner_constructor):\n    base_dir = \'/tmp\'\n    run_experiment.create_runner(base_dir,\n                                 schedule=\'continuous_train\')\n    self.assertEqual(1, mock_runner_constructor.call_count)\n    mock_args, _ = mock_runner_constructor.call_args\n    self.assertEqual(base_dir, mock_args[0])\n    self.assertEqual(mock_create_agent, mock_args[1])\n\n\nclass RunnerTest(tf.test.TestCase):\n\n  def _agent_step(self, reward, observation):\n    # We verify that rewards are clipped (and set by MockEnvironment as a\n    # function of observation)\n    expected_reward = 1 if observation % 2 else -1\n    self.assertEqual(expected_reward, reward)\n    return observation % 2\n\n  def setUp(self):\n    super(RunnerTest, self).setUp()\n    self._agent = mock.Mock()\n    self._agent.begin_episode.side_effect = lambda x: 0\n    self._agent.step.side_effect = self._agent_step\n    self._create_agent_fn = lambda x, y, summary_writer: self._agent\n    self._test_subdir = \'/tmp/dopamine_tests\'\n    shutil.rmtree(self._test_subdir, ignore_errors=True)\n    os.makedirs(self._test_subdir)\n\n  @mock.patch.object(checkpointer, \'get_latest_checkpoint_number\')\n  def testInitializeCheckpointingWithNoCheckpointFile(self, mock_get_latest):\n    mock_get_latest.return_value = -1\n    base_dir = \'/does/not/exist\'\n    with self.assertRaisesRegexp(tf.errors.PermissionDeniedError,\n                                 \'.*/does.*\'):\n      run_experiment.Runner(base_dir, self._create_agent_fn, mock.Mock)\n\n  @mock.patch.object(checkpointer, \'get_latest_checkpoint_number\')\n  @mock.patch.object(checkpointer, \'Checkpointer\')\n  @mock.patch.object(logger, \'Logger\')\n  def testInitializeCheckpointingWhenCheckpointUnbundleFails(\n      self, mock_logger_constructor, mock_checkpointer_constructor,\n      mock_get_latest):\n    mock_checkpointer = _create_mock_checkpointer()\n    mock_checkpointer_constructor.return_value = mock_checkpointer\n    latest_checkpoint = 7\n    mock_get_latest.return_value = latest_checkpoint\n    agent = mock.Mock()\n    agent.unbundle.return_value = False\n    mock_logger = mock.Mock()\n    mock_logger_constructor.return_value = mock_logger\n    runner = run_experiment.Runner(self._test_subdir,\n                                   lambda x, y, summary_writer: agent,\n                                   mock.Mock)\n    self.assertEqual(0, runner._start_iteration)\n    self.assertEqual(1, mock_checkpointer.load_checkpoint.call_count)\n    self.assertEqual(1, agent.unbundle.call_count)\n    mock_args, _ = agent.unbundle.call_args\n    self.assertEqual(\'{}/checkpoints\'.format(self._test_subdir), mock_args[0])\n    self.assertEqual(latest_checkpoint, mock_args[1])\n    expected_dictionary = {\'current_iteration\': 1729,\n                           \'logs\': \'logs\'}\n    self.assertDictEqual(expected_dictionary, mock_args[2])\n\n  @mock.patch.object(checkpointer, \'get_latest_checkpoint_number\')\n  def testInitializeCheckpointingWhenCheckpointUnbundleSucceeds(\n      self, mock_get_latest):\n    latest_checkpoint = 7\n    mock_get_latest.return_value = latest_checkpoint\n    logs_data = {\'a\': 1, \'b\': 2}\n    current_iteration = 1729\n    checkpoint_data = {\'current_iteration\': current_iteration,\n                       \'logs\': logs_data}\n    checkpoint_dir = os.path.join(self._test_subdir, \'checkpoints\')\n    checkpoint = checkpointer.Checkpointer(checkpoint_dir, \'ckpt\')\n    checkpoint.save_checkpoint(latest_checkpoint, checkpoint_data)\n    mock_agent = mock.Mock()\n    mock_agent.unbundle.return_value = True\n    runner = run_experiment.Runner(self._test_subdir,\n                                   lambda x, y, summary_writer: mock_agent,\n                                   mock.Mock)\n    expected_iteration = current_iteration + 1\n    self.assertEqual(expected_iteration, runner._start_iteration)\n    self.assertDictEqual(logs_data, runner._logger.data)\n    mock_agent.unbundle.assert_called_once_with(\n        checkpoint_dir, latest_checkpoint, checkpoint_data)\n\n  def testRunOneEpisode(self):\n    max_steps_per_episode = 11\n    environment = MockEnvironment()\n    runner = run_experiment.Runner(\n        self._test_subdir, self._create_agent_fn, lambda: environment,\n        max_steps_per_episode=max_steps_per_episode)\n    step_number, total_reward = runner._run_one_episode()\n    self.assertEqual(self._agent.step.call_count, environment.max_steps - 1)\n    self.assertEqual(self._agent.end_episode.call_count, 1)\n    self.assertEqual(environment.max_steps, step_number)\n    # Expected reward will be \\sum_{i=0}^{9} (-1)**i * i = -5\n    self.assertEqual(-5, total_reward)\n\n  def testRunOneEpisodeWithLowMaxSteps(self):\n    max_steps_per_episode = 2\n    environment = MockEnvironment()\n    runner = run_experiment.Runner(\n        self._test_subdir, self._create_agent_fn, lambda: environment,\n        max_steps_per_episode=max_steps_per_episode)\n    step_number, total_reward = runner._run_one_episode()\n    self.assertEqual(self._agent.step.call_count, max_steps_per_episode - 1)\n    self.assertEqual(self._agent.end_episode.call_count, 1)\n    self.assertEqual(max_steps_per_episode, step_number)\n    self.assertEqual(-1, total_reward)\n\n  def testRunOnePhase(self):\n    max_steps = 10\n    environment_steps = 2\n    environment = MockEnvironment(max_steps=environment_steps)\n    statistics = []\n    runner = run_experiment.Runner(\n        self._test_subdir, self._create_agent_fn, lambda: environment)\n    step_number, sum_returns, num_episodes = runner._run_one_phase(\n        max_steps, statistics, \'test\')\n    calls_to_run_episode = int(max_steps / environment_steps)\n    self.assertEqual(self._agent.step.call_count, calls_to_run_episode)\n    self.assertEqual(self._agent.end_episode.call_count, calls_to_run_episode)\n    self.assertEqual(max_steps, step_number)\n    self.assertEqual(-1 * calls_to_run_episode, sum_returns)\n    self.assertEqual(calls_to_run_episode, num_episodes)\n    expected_statistics = []\n    for _ in range(calls_to_run_episode):\n      expected_statistics.append({\n          \'test_episode_lengths\': 2,\n          \'test_episode_returns\': -1\n      })\n    self.assertEqual(len(expected_statistics), len(statistics))\n    for i in range(len(statistics)):\n      self.assertDictEqual(expected_statistics[i], statistics[i])\n\n  def testRunOneIteration(self):\n    environment_steps = 2\n    environment = MockEnvironment(max_steps=environment_steps)\n    training_steps = 20\n    evaluation_steps = 10\n    runner = run_experiment.Runner(\n        self._test_subdir, self._create_agent_fn, lambda: environment,\n        training_steps=training_steps,\n        evaluation_steps=evaluation_steps)\n    dictionary = runner._run_one_iteration(1)\n    train_calls = int(training_steps / environment_steps)\n    eval_calls = int(evaluation_steps / environment_steps)\n    expected_dictionary = {\n        \'train_episode_lengths\': [2 for _ in range(train_calls)],\n        \'train_episode_returns\': [-1 for _ in range(train_calls)],\n        \'train_average_return\': [-1],\n        \'eval_episode_lengths\': [2 for _ in range(eval_calls)],\n        \'eval_episode_returns\': [-1 for _ in range(eval_calls)],\n        \'eval_average_return\': [-1]\n    }\n    self.assertDictEqual(expected_dictionary, dictionary)\n\n  @mock.patch.object(logger, \'Logger\')\n  def testLogExperiment(self, mock_logger_constructor):\n    log_every_n = 2\n    logging_file_prefix = \'prefix\'\n    statistics = \'statistics\'\n    experiment_logger = MockLogger(test_cls=self)\n    mock_logger_constructor.return_value = experiment_logger\n    runner = run_experiment.Runner(\n        self._test_subdir, self._create_agent_fn, mock.Mock,\n        logging_file_prefix=logging_file_prefix,\n        log_every_n=log_every_n)\n    num_iterations = 10\n    for i in range(num_iterations):\n      runner._log_experiment(i, statistics)\n    self.assertEqual(num_iterations, experiment_logger._calls_to_set)\n    self.assertEqual((num_iterations / log_every_n),\n                     experiment_logger._calls_to_log)\n\n  @mock.patch.object(checkpointer, \'Checkpointer\')\n  @mock.patch.object(logger, \'Logger\')\n  def testCheckpointExperiment(self, mock_logger_constructor,\n                               mock_checkpointer_constructor):\n    checkpoint_dir = os.path.join(self._test_subdir, \'checkpoints\')\n    test_dict = {\'test\': 1}\n    iteration = 1729\n\n    def bundle_and_checkpoint(x, y):\n      self.assertEqual(checkpoint_dir, x)\n      self.assertEqual(iteration, y)\n      return test_dict\n\n    self._agent.bundle_and_checkpoint.side_effect = bundle_and_checkpoint\n    experiment_checkpointer = mock.Mock()\n    mock_checkpointer_constructor.return_value = experiment_checkpointer\n    logs_data = {\'one\': 1, \'two\': 2}\n    mock_logger = MockLogger(run_asserts=False, data=logs_data)\n    mock_logger_constructor.return_value = mock_logger\n    runner = run_experiment.Runner(\n        self._test_subdir, self._create_agent_fn, mock.Mock)\n    runner._checkpoint_experiment(iteration)\n    self.assertEqual(1, experiment_checkpointer.save_checkpoint.call_count)\n    mock_args, _ = experiment_checkpointer.save_checkpoint.call_args\n    self.assertEqual(iteration, mock_args[0])\n    test_dict[\'logs\'] = logs_data\n    test_dict[\'current_iteration\'] = iteration\n    self.assertDictEqual(test_dict, mock_args[1])\n\n  @mock.patch.object(checkpointer, \'Checkpointer\')\n  @mock.patch.object(logger, \'Logger\')\n  def testRunExperimentWithInconsistentRange(self, mock_logger_constructor,\n                                             mock_checkpointer_constructor):\n    experiment_logger = MockLogger()\n    mock_logger_constructor.return_value = experiment_logger\n    experiment_checkpointer = mock.Mock()\n    mock_checkpointer_constructor.return_value = experiment_checkpointer\n    runner = run_experiment.Runner(\n        self._test_subdir, self._create_agent_fn, mock.Mock,\n        num_iterations=0)\n    runner.run_experiment()\n    self.assertEqual(0, experiment_checkpointer.save_checkpoint.call_count)\n    self.assertEqual(0, experiment_logger._calls_to_set)\n    self.assertEqual(0, experiment_logger._calls_to_log)\n\n  @mock.patch.object(checkpointer, \'get_latest_checkpoint_number\')\n  @mock.patch.object(checkpointer, \'Checkpointer\')\n  @mock.patch.object(logger, \'Logger\')\n  def testRunExperiment(self, mock_logger_constructor,\n                        mock_checkpointer_constructor,\n                        mock_get_latest):\n    log_every_n = 1\n    environment = MockEnvironment()\n    experiment_logger = MockLogger(run_asserts=False)\n    mock_logger_constructor.return_value = experiment_logger\n    experiment_checkpointer = mock.Mock()\n    start_iteration = 1729\n    mock_get_latest.return_value = start_iteration\n    def load_checkpoint(_):\n      return {\'logs\': \'log_data\', \'current_iteration\': start_iteration - 1}\n\n    experiment_checkpointer.load_checkpoint.side_effect = load_checkpoint\n    mock_checkpointer_constructor.return_value = experiment_checkpointer\n    def bundle_and_checkpoint(x, y):\n      del x, y  # Unused.\n      return {\'test\': 1}\n\n    self._agent.bundle_and_checkpoint.side_effect = bundle_and_checkpoint\n    num_iterations = 10\n    self._agent.unbundle.return_value = True\n    end_iteration = start_iteration + num_iterations\n    runner = run_experiment.Runner(\n        self._test_subdir, self._create_agent_fn, lambda: environment,\n        log_every_n=log_every_n,\n        num_iterations=end_iteration,\n        training_steps=1,\n        evaluation_steps=1)\n    self.assertEqual(start_iteration, runner._start_iteration)\n    runner.run_experiment()\n    self.assertEqual(num_iterations,\n                     experiment_checkpointer.save_checkpoint.call_count)\n    self.assertEqual(num_iterations, experiment_logger._calls_to_set)\n    self.assertEqual(num_iterations, experiment_logger._calls_to_log)\n    glob_string = \'{}/events.out.tfevents.*\'.format(self._test_subdir)\n    self.assertGreater(len(tf.gfile.Glob(glob_string)), 0)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/replay_memory/circular_replay_buffer_test.py,15,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for circular_replay_buffer.py.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport shutil\n\n\n\nfrom absl import flags\nfrom dopamine.replay_memory import circular_replay_buffer\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\n\nFLAGS = flags.FLAGS\n\n# Default parameters used when creating the replay memory.\nOBSERVATION_SHAPE = (84, 84)\nOBS_DTYPE = np.uint8\nSTACK_SIZE = 4\nBATCH_SIZE = 32\n\n\nclass CheckpointableClass(object):\n\n  def __init__(self):\n    self.attribute = 0\n\n\nclass OutOfGraphReplayBufferTest(tf.test.TestCase):\n\n  def setUp(self):\n    self._test_subdir = os.path.join(\'/tmp/dopamine_tests\', \'replay\')\n    shutil.rmtree(self._test_subdir, ignore_errors=True)\n    os.makedirs(self._test_subdir)\n    num_dims = 10\n    self._test_observation = np.ones(num_dims) * 1\n    self._test_action = np.ones(num_dims) * 2\n    self._test_reward = np.ones(num_dims) * 3\n    self._test_terminal = np.ones(num_dims) * 4\n    self._test_add_count = np.array(7)\n    self._test_invalid_range = np.ones(num_dims)\n\n  def testWithNontupleObservationShape(self):\n    with self.assertRaises(AssertionError):\n      _ = circular_replay_buffer.OutOfGraphReplayBuffer(\n          observation_shape=84, stack_size=STACK_SIZE, replay_capacity=5,\n          batch_size=BATCH_SIZE)\n\n  def testConstructor(self):\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=5,\n        batch_size=BATCH_SIZE)\n    self.assertEqual(memory._observation_shape, OBSERVATION_SHAPE)\n    # Test with non square observation shape\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=(4, 20),\n        stack_size=STACK_SIZE,\n        replay_capacity=5,\n        batch_size=BATCH_SIZE)\n    self.assertEqual(memory._observation_shape, (4, 20))\n    self.assertEqual(memory.add_count, 0)\n    # Test with terminal datatype of np.int32\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        terminal_dtype=np.int32,\n        replay_capacity=5,\n        batch_size=BATCH_SIZE)\n    self.assertEqual(memory._terminal_dtype, np.int32)\n\n  def testAdd(self):\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=5,\n        batch_size=BATCH_SIZE)\n    self.assertEqual(memory.cursor(), 0)\n    zeros = np.zeros(OBSERVATION_SHAPE)\n    memory.add(zeros, 0, 0, 0)\n    # Check if the cursor moved STACK_SIZE -1 padding adds + 1, (the one above).\n    self.assertEqual(memory.cursor(), STACK_SIZE)\n\n  def testExtraAdd(self):\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=5,\n        batch_size=BATCH_SIZE,\n        extra_storage_types=[\n            circular_replay_buffer.ReplayElement(\'extra1\', [], np.float32),\n            circular_replay_buffer.ReplayElement(\'extra2\', [2], np.int8)\n        ])\n    self.assertEqual(memory.cursor(), 0)\n    zeros = np.zeros(OBSERVATION_SHAPE)\n    memory.add(zeros, 0, 0, 0, 0, [0, 0])\n\n    with self.assertRaisesRegexp(ValueError, \'Add expects\'):\n      memory.add(zeros, 0, 0, 0)\n    # Check if the cursor moved STACK_SIZE -1 zeros adds + 1, (the one above).\n    self.assertEqual(memory.cursor(), STACK_SIZE)\n\n  def testCheckAddTypes(self):\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=5,\n        batch_size=BATCH_SIZE,\n        extra_storage_types=[\n            circular_replay_buffer.ReplayElement(\'extra1\', [], np.float32),\n            circular_replay_buffer.ReplayElement(\'extra2\', [2], np.int8)\n        ])\n    zeros = np.zeros(OBSERVATION_SHAPE)\n\n    memory._check_add_types(zeros, 0, 0, 0, 0, [0, 0])\n\n    with self.assertRaisesRegexp(ValueError, \'Add expects\'):\n      memory._check_add_types(zeros, 0, 0, 0)\n\n  def testLowCapacity(self):\n    with self.assertRaisesRegexp(ValueError, \'There is not enough capacity\'):\n      circular_replay_buffer.OutOfGraphReplayBuffer(\n          observation_shape=OBSERVATION_SHAPE,\n          stack_size=10,\n          replay_capacity=10,\n          batch_size=BATCH_SIZE,\n          update_horizon=1,\n          gamma=1.0)\n\n    with self.assertRaisesRegexp(ValueError, \'There is not enough capacity\'):\n      circular_replay_buffer.OutOfGraphReplayBuffer(\n          observation_shape=OBSERVATION_SHAPE,\n          stack_size=5,\n          replay_capacity=10,\n          batch_size=BATCH_SIZE,\n          update_horizon=10,\n          gamma=1.0)\n\n    # We should be able to create a buffer that contains just enough for a\n    # transition.\n    circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=5,\n        replay_capacity=10,\n        batch_size=BATCH_SIZE,\n        update_horizon=5,\n        gamma=1.0)\n\n  def testGetRangeInvalidIndexOrder(self):\n    replay_capacity = 10\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=replay_capacity,\n        batch_size=BATCH_SIZE,\n        update_horizon=5,\n        gamma=1.0)\n    with self.assertRaisesRegexp(AssertionError,\n                                 \'end_index must be larger than start_index\'):\n      memory.get_range([], 2, 1)\n    with self.assertRaises(AssertionError):\n      # Negative end_index.\n      memory.get_range([], 1, -1)\n    with self.assertRaises(AssertionError):\n      # Start index beyond replay capacity.\n      memory.get_range([], replay_capacity, replay_capacity + 1)\n    with self.assertRaisesRegexp(AssertionError,\n                                 \'Index 1 has not been added.\'):\n      memory.get_range([], 1, 2)\n\n  def testGetRangeNoWraparound(self):\n    # Test the get_range function when the indices do not wrap around the\n    # circular buffer. In other words, start_index < end_index.\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=10,\n        batch_size=BATCH_SIZE,\n        update_horizon=5,\n        gamma=1.0)\n    for _ in range(10):\n      memory.add(\n          np.full(OBSERVATION_SHAPE, 0, dtype=OBS_DTYPE),\n          0, 2.0, 0)\n    # The constructed `array` will be:\n    # array([[ 1.,  1.,  1.,  1.,  1.],\n    #        [ 2.,  2.,  2.,  2.,  2.],\n    #        [ 3.,  3.,  3.,  3.,  3.],\n    #        [ 4.,  4.,  4.,  4.,  4.],\n    #        [ 5.,  5.,  5.,  5.,  5.],\n    #        [ 6.,  6.,  6.,  6.,  6.],\n    #        [ 7.,  7.,  7.,  7.,  7.],\n    #        [ 8.,  8.,  8.,  8.,  8.],\n    #        [ 9.,  9.,  9.,  9.,  9.],\n    #        [10., 10., 10., 10., 10.]])\n    array = np.arange(10).reshape(10, 1) + np.ones(5)\n    sliced_array = memory.get_range(array, 2, 5)\n    self.assertAllEqual(sliced_array, array[2:5])\n\n  def testGetRangeWithWraparound(self):\n    # Test the get_range function when the indices wrap around the circular\n    # buffer. In other words, start_index > end_index.\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=10,\n        batch_size=BATCH_SIZE,\n        update_horizon=5,\n        gamma=1.0)\n    for _ in range(10):\n      memory.add(\n          np.full(OBSERVATION_SHAPE, 0, dtype=OBS_DTYPE),\n          0, 2.0, 0)\n    # The constructed `array` will be:\n    # array([[ 1.,  1.,  1.,  1.,  1.],\n    #        [ 2.,  2.,  2.,  2.,  2.],\n    #        [ 3.,  3.,  3.,  3.,  3.],\n    #        [ 4.,  4.,  4.,  4.,  4.],\n    #        [ 5.,  5.,  5.,  5.,  5.],\n    #        [ 6.,  6.,  6.,  6.,  6.],\n    #        [ 7.,  7.,  7.,  7.,  7.],\n    #        [ 8.,  8.,  8.,  8.,  8.],\n    #        [ 9.,  9.,  9.,  9.,  9.],\n    #        [10., 10., 10., 10., 10.]])\n    array = np.arange(10).reshape(10, 1) + np.ones(5)\n    sliced_array = memory.get_range(array, 8, 12)\n    # We roll by two, since start_index == 8 and replay_capacity == 10, so the\n    # resulting indices used will be [8, 9, 0, 1].\n    rolled_array = np.roll(array, 2, axis=0)\n    self.assertAllEqual(sliced_array, rolled_array[:4])\n\n  def testNSteprewardum(self):\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=10,\n        batch_size=BATCH_SIZE,\n        update_horizon=5,\n        gamma=1.0)\n\n    for i in range(50):\n      memory.add(\n          np.full(OBSERVATION_SHAPE, i, dtype=OBS_DTYPE),\n          0, 2.0, 0)\n\n    for i in range(100):\n      batch = memory.sample_transition_batch()\n      # Make sure the total reward is reward per step x update_horizon.\n      self.assertEqual(batch[2][0], 10.0)\n\n  def testGetStack(self):\n    zero_stack = np.zeros(OBSERVATION_SHAPE + (4,), dtype=OBS_DTYPE)\n\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=50,\n        batch_size=BATCH_SIZE)\n    for i in range(11):\n      memory.add(\n          np.full(OBSERVATION_SHAPE, i, dtype=OBS_DTYPE),\n          0, 0, 0)\n\n    # ensure that the returned shapes are always correct\n    for i in range(3, memory.cursor()):\n      self.assertTrue(\n          memory.get_observation_stack(i).shape,\n          OBSERVATION_SHAPE + (4,))\n\n    # ensure that there is the necessary 0 padding\n    stack = memory.get_observation_stack(3)\n    self.assertTrue(np.array_equal(zero_stack, stack))\n\n    # ensure that after the padding the contents are properly stored\n    stack = memory.get_observation_stack(6)\n    for i in range(4):\n      self.assertTrue(\n          np.array_equal(np.full(OBSERVATION_SHAPE, i), stack[:, :, i]))\n\n  def testSampleTransitionBatch(self):\n    replay_capacity = 10\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=1,\n        replay_capacity=replay_capacity,\n        batch_size=2)\n    num_adds = 50  # The number of transitions to add to the memory.\n    for i in range(num_adds):\n      memory.add(\n          np.full(OBSERVATION_SHAPE, i, OBS_DTYPE), 0,\n          0, i % 4)  # Every 4 transitions is terminal.\n    # Test sampling with default batch size.\n    for i in range(1000):\n      batch = memory.sample_transition_batch()\n      self.assertEqual(batch[0].shape[0], 2)\n    # Test changing batch sizes.\n    for i in range(1000):\n      batch = memory.sample_transition_batch(BATCH_SIZE)\n      self.assertEqual(batch[0].shape[0], BATCH_SIZE)\n    # Verify we revert to default batch size.\n    for i in range(1000):\n      batch = memory.sample_transition_batch()\n      self.assertEqual(batch[0].shape[0], 2)\n\n    # Verify we can specify what indices to sample.\n    indices = [1, 2, 3, 5, 8]\n    expected_states = np.array([\n        np.full(OBSERVATION_SHAPE + (1,), i, dtype=OBS_DTYPE)\n        for i in indices\n    ])\n    expected_next_states = (expected_states + 1) % replay_capacity\n    # Because the replay buffer is circular, we can exactly compute what the\n    # states will be at the specified indices by doing a little mod math:\n    expected_states += num_adds - replay_capacity\n    expected_next_states += num_adds - replay_capacity\n    # This is replicating the formula that was used above to determine what\n    # transitions are terminal when adding observation (i % 4).\n    expected_terminal = np.array(\n        [min((x + num_adds - replay_capacity) % 4, 1) for x in indices])\n    batch = memory.sample_transition_batch(batch_size=len(indices),\n                                           indices=indices)\n    (states, action, reward, next_states, next_action, next_reward, terminal,\n     indices_batch) = batch\n    self.assertAllEqual(states, expected_states)\n    self.assertAllEqual(action, np.zeros(len(indices)))\n    self.assertAllEqual(reward, np.zeros(len(indices)))\n    self.assertAllEqual(next_action, np.zeros(len(indices)))\n    self.assertAllEqual(next_reward, np.zeros(len(indices)))\n    self.assertAllEqual(next_states, expected_next_states)\n    self.assertAllEqual(terminal, expected_terminal)\n    self.assertAllEqual(indices_batch, indices)\n\n  def testSampleTransitionBatchExtra(self):\n    replay_capacity = 10\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=1,\n        replay_capacity=replay_capacity,\n        batch_size=2,\n        extra_storage_types=[\n            circular_replay_buffer.ReplayElement(\'extra1\', [], np.float32),\n            circular_replay_buffer.ReplayElement(\'extra2\', [2], np.int8)\n        ])\n    num_adds = 50  # The number of transitions to add to the memory.\n    for i in range(num_adds):\n      memory.add(\n          np.full(OBSERVATION_SHAPE, i, dtype=OBS_DTYPE),\n          0, 0, i % 4, 0, [0, 0])  # Every 4 transitions is terminal.\n    # Test sampling with default batch size.\n    for i in range(1000):\n      batch = memory.sample_transition_batch()\n      self.assertEqual(batch[0].shape[0], 2)\n    # Test changing batch sizes.\n    for i in range(1000):\n      batch = memory.sample_transition_batch(BATCH_SIZE)\n      self.assertEqual(batch[0].shape[0], BATCH_SIZE)\n    # Verify we revert to default batch size.\n    for i in range(1000):\n      batch = memory.sample_transition_batch()\n      self.assertEqual(batch[0].shape[0], 2)\n\n    # Verify we can specify what indices to sample.\n    indices = [1, 2, 3, 5, 8]\n    expected_states = np.array([\n        np.full(OBSERVATION_SHAPE + (1,), i, dtype=OBS_DTYPE)\n        for i in indices\n    ])\n    expected_next_states = (expected_states + 1) % replay_capacity\n    # Because the replay buffer is circular, we can exactly compute what the\n    # states will be at the specified indices by doing a little mod math:\n    expected_states += num_adds - replay_capacity\n    expected_next_states += num_adds - replay_capacity\n    # This is replicating the formula that was used above to determine what\n    # transitions are terminal when adding observation (i % 4).\n    expected_terminal = np.array(\n        [min((x + num_adds - replay_capacity) % 4, 1) for x in indices])\n    expected_extra2 = np.zeros([len(indices), 2])\n    batch = memory.sample_transition_batch(\n        batch_size=len(indices), indices=indices)\n    (states, action, reward, next_states, next_action, next_reward, terminal,\n     indices_batch, extra1, extra2) = batch\n    self.assertAllEqual(states, expected_states)\n    self.assertAllEqual(action, np.zeros(len(indices)))\n    self.assertAllEqual(reward, np.zeros(len(indices)))\n    self.assertAllEqual(next_action, np.zeros(len(indices)))\n    self.assertAllEqual(next_reward, np.zeros(len(indices)))\n    self.assertAllEqual(next_states, expected_next_states)\n    self.assertAllEqual(terminal, expected_terminal)\n    self.assertAllEqual(indices_batch, indices)\n    self.assertAllEqual(extra1, np.zeros(len(indices)))\n    self.assertAllEqual(extra2, expected_extra2)\n\n  def testSamplingWithterminalInTrajectory(self):\n    replay_capacity = 10\n    update_horizon = 3\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=1,\n        replay_capacity=replay_capacity,\n        batch_size=2,\n        update_horizon=update_horizon,\n        gamma=1.0)\n    for i in range(replay_capacity):\n      memory.add(\n          np.full(OBSERVATION_SHAPE, i, dtype=OBS_DTYPE),\n          i * 2,  # action\n          i,  # reward\n          1 if i == 3 else 0)  # terminal\n    indices = [2, 3, 4]\n    batch = memory.sample_transition_batch(batch_size=len(indices),\n                                           indices=indices)\n    states, action, reward, _, _, _, terminal, indices_batch = batch\n    expected_states = np.array([\n        np.full(OBSERVATION_SHAPE + (1,), i, dtype=OBS_DTYPE)\n        for i in indices\n    ])\n    # The reward in the replay buffer will be (an asterisk marks the terminal\n    # state):\n    #   [0 1 2 3* 4 5 6 7 8 9]\n    # Since we\'re setting the update_horizon to 3, the accumulated trajectory\n    # reward starting at each of the replay buffer positions will be:\n    #   [3 6 5 3 15 18 21 24]\n    # Since indices = [2, 3, 4], our expected reward are [5, 3, 15].\n    expected_reward = np.array([5, 3, 15])\n    # Because update_horizon = 3, both indices 2 and 3 include terminal.\n    expected_terminal = np.array([1, 1, 0])\n    self.assertAllEqual(states, expected_states)\n    self.assertAllEqual(action, np.array(indices) * 2)\n    self.assertAllEqual(reward, expected_reward)\n    self.assertAllEqual(terminal, expected_terminal)\n    self.assertAllEqual(indices_batch, indices)\n\n  def testInvalidRange(self):\n    # The correct range accounts for the automatically applied padding (3 blanks\n    # each episode.\n\n    invalid_range = circular_replay_buffer.invalid_range(\n        cursor=6, replay_capacity=10, stack_size=4, update_horizon=1)\n    correct_invalid_range = [5, 6, 7, 8, 9]\n    self.assertAllClose(correct_invalid_range, invalid_range)\n\n    invalid_range = circular_replay_buffer.invalid_range(\n        cursor=9, replay_capacity=10, stack_size=4, update_horizon=1)\n    correct_invalid_range = [8, 9, 0, 1, 2]\n    self.assertAllClose(correct_invalid_range, invalid_range)\n\n    invalid_range = circular_replay_buffer.invalid_range(\n        cursor=0, replay_capacity=10, stack_size=4, update_horizon=1)\n    correct_invalid_range = [9, 0, 1, 2, 3]\n    self.assertAllClose(correct_invalid_range, invalid_range)\n\n    invalid_range = circular_replay_buffer.invalid_range(\n        cursor=6, replay_capacity=10, stack_size=4, update_horizon=3)\n    correct_invalid_range = [3, 4, 5, 6, 7, 8, 9]\n    self.assertAllClose(correct_invalid_range, invalid_range)\n\n  def testIsTransitionValid(self):\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=10,\n        batch_size=2)\n\n    memory.add(\n        np.full(OBSERVATION_SHAPE, 0, dtype=OBS_DTYPE), 0, 0, 0)\n    memory.add(\n        np.full(OBSERVATION_SHAPE, 0, dtype=OBS_DTYPE), 0, 0, 0)\n    memory.add(\n        np.full(OBSERVATION_SHAPE, 0, dtype=OBS_DTYPE), 0, 0, 1)\n\n    # These valids account for the automatically applied padding (3 blanks each\n    # episode.\n    correct_valids = [0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n    # The cursor is:                    ^\\\n    for i in range(10):\n      self.assertEqual(correct_valids[i], memory.is_valid_transition(i),\n                       \'Index %i should be %s\' % (i, bool(correct_valids[i])))\n\n  def testSave(self):\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=5,\n        batch_size=BATCH_SIZE)\n    memory.observation = self._test_observation\n    memory.action = self._test_action\n    memory.reward = self._test_reward\n    memory.terminal = self._test_terminal\n    current_iteration = 5\n    stale_iteration = (\n        current_iteration - circular_replay_buffer.CHECKPOINT_DURATION)\n    memory.save(self._test_subdir, stale_iteration)\n    for attr in memory.__dict__:\n      if attr.startswith(\'_\'):\n        continue\n      stale_filename = os.path.join(self._test_subdir, \'{}_ckpt.{}.gz\'.format(\n          attr, stale_iteration))\n      self.assertTrue(tf.gfile.Exists(stale_filename))\n\n    memory.save(self._test_subdir, current_iteration)\n    for attr in memory.__dict__:\n      if attr.startswith(\'_\'):\n        continue\n      filename = os.path.join(self._test_subdir, \'{}_ckpt.{}.gz\'.format(\n          attr, current_iteration))\n      self.assertTrue(tf.gfile.Exists(filename))\n      # The stale version file should have been deleted.\n      self.assertFalse(tf.gfile.Exists(stale_filename))\n\n  def testSaveNonNDArrayAttributes(self):\n    """"""Tests checkpointing an attribute which is not a numpy array.""""""\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=5,\n        batch_size=BATCH_SIZE)\n\n    # Add some non-numpy data: an int, a string, an object.\n    memory.dummy_attribute_1 = 4753849\n    memory.dummy_attribute_2 = \'String data\'\n    memory.dummy_attribute_3 = CheckpointableClass()\n\n    current_iteration = 5\n    stale_iteration = (\n        current_iteration - circular_replay_buffer.CHECKPOINT_DURATION)\n    memory.save(self._test_subdir, stale_iteration)\n    for attr in memory.__dict__:\n      if attr.startswith(\'_\'):\n        continue\n      stale_filename = os.path.join(self._test_subdir, \'{}_ckpt.{}.gz\'.format(\n          attr, stale_iteration))\n      self.assertTrue(tf.gfile.Exists(stale_filename))\n\n    memory.save(self._test_subdir, current_iteration)\n    for attr in memory.__dict__:\n      if attr.startswith(\'_\'):\n        continue\n      filename = os.path.join(self._test_subdir, \'{}_ckpt.{}.gz\'.format(\n          attr, current_iteration))\n      self.assertTrue(tf.gfile.Exists(filename))\n      # The stale version file should have been deleted.\n      self.assertFalse(tf.gfile.Exists(stale_filename))\n\n  def testLoadFromNonexistentDirectory(self):\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=5,\n        batch_size=BATCH_SIZE)\n    # We are trying to load from a non-existent directory, so a NotFoundError\n    # will be raised.\n    with self.assertRaises(tf.errors.NotFoundError):\n      memory.load(\'/does/not/exist\', \'3\')\n    self.assertNotEqual(memory._store[\'observation\'], self._test_observation)\n    self.assertNotEqual(memory._store[\'action\'], self._test_action)\n    self.assertNotEqual(memory._store[\'reward\'], self._test_reward)\n    self.assertNotEqual(memory._store[\'terminal\'], self._test_terminal)\n    self.assertNotEqual(memory.add_count, self._test_add_count)\n    self.assertNotEqual(memory.invalid_range, self._test_invalid_range)\n\n  def testPartialLoadFails(self):\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=5,\n        batch_size=BATCH_SIZE)\n    self.assertNotEqual(memory._store[\'observation\'], self._test_observation)\n    self.assertNotEqual(memory._store[\'action\'], self._test_action)\n    self.assertNotEqual(memory._store[\'reward\'], self._test_reward)\n    self.assertNotEqual(memory._store[\'terminal\'], self._test_terminal)\n    self.assertNotEqual(memory.add_count, self._test_add_count)\n    numpy_arrays = {\n        \'observation\': self._test_observation,\n        \'action\': self._test_action,\n        \'terminal\': self._test_terminal,\n        \'add_count\': self._test_add_count,\n        \'invalid_range\': self._test_invalid_range\n    }\n    for attr in numpy_arrays:\n      filename = os.path.join(self._test_subdir, \'{}_ckpt.3.gz\'.format(attr))\n      with tf.gfile.Open(filename, \'w\') as f:\n        with gzip.GzipFile(fileobj=f) as outfile:\n          np.save(outfile, numpy_arrays[attr], allow_pickle=False)\n    # We are are missing the reward file, so a NotFoundError will be raised.\n    with self.assertRaises(tf.errors.NotFoundError):\n      memory.load(self._test_subdir, \'3\')\n    # Since we are missing the reward file, it should not have loaded any of\n    # the other files.\n    self.assertNotEqual(memory._store[\'observation\'], self._test_observation)\n    self.assertNotEqual(memory._store[\'action\'], self._test_action)\n    self.assertNotEqual(memory._store[\'reward\'], self._test_reward)\n    self.assertNotEqual(memory._store[\'terminal\'], self._test_terminal)\n    self.assertNotEqual(memory.add_count, self._test_add_count)\n    self.assertNotEqual(memory.invalid_range, self._test_invalid_range)\n\n  def testLoad(self):\n    memory = circular_replay_buffer.OutOfGraphReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=5,\n        batch_size=BATCH_SIZE)\n    self.assertNotEqual(memory._store[\'observation\'], self._test_observation)\n    self.assertNotEqual(memory._store[\'action\'], self._test_action)\n    self.assertNotEqual(memory._store[\'reward\'], self._test_reward)\n    self.assertNotEqual(memory._store[\'terminal\'], self._test_terminal)\n    self.assertNotEqual(memory.add_count, self._test_add_count)\n    self.assertNotEqual(memory.invalid_range, self._test_invalid_range)\n    store_prefix = \'$store$_\'\n    numpy_arrays = {\n        store_prefix + \'observation\': self._test_observation,\n        store_prefix + \'action\': self._test_action,\n        store_prefix + \'reward\': self._test_reward,\n        store_prefix + \'terminal\': self._test_terminal,\n        \'add_count\': self._test_add_count,\n        \'invalid_range\': self._test_invalid_range\n    }\n    for attr in numpy_arrays:\n      filename = os.path.join(self._test_subdir, \'{}_ckpt.3.gz\'.format(attr))\n      with tf.gfile.Open(filename, \'w\') as f:\n        with gzip.GzipFile(fileobj=f) as outfile:\n          np.save(outfile, numpy_arrays[attr], allow_pickle=False)\n    memory.load(self._test_subdir, \'3\')\n    self.assertAllClose(memory._store[\'observation\'], self._test_observation)\n    self.assertAllClose(memory._store[\'action\'], self._test_action)\n    self.assertAllClose(memory._store[\'reward\'], self._test_reward)\n    self.assertAllClose(memory._store[\'terminal\'], self._test_terminal)\n    self.assertEqual(memory.add_count, self._test_add_count)\n    self.assertAllClose(memory.invalid_range, self._test_invalid_range)\n\n\nclass WrappedReplayBufferTest(tf.test.TestCase):\n\n  def setUp(self):\n    self._test_subdir = os.path.join(\'/tmp/dopamine_tests\', \'wrapped_replay\')\n    shutil.rmtree(self._test_subdir, ignore_errors=True)\n    os.makedirs(self._test_subdir)\n    num_dims = 10\n    self._test_observation = np.ones(num_dims) * 3\n    self._test_action = np.ones(num_dims) * 5\n    self._test_reward = np.ones(num_dims) * 7\n    self._test_terminal = np.ones(num_dims) * 11\n    self._test_add_count = np.array(7)\n    self._test_invalid_range = np.ones(num_dims)\n\n  def testConstructorCapacityNotLargeEnough(self):\n    with self.assertRaisesRegexp(\n        ValueError,\n        r\'Update horizon \\(5\\) should be significantly \'\n        r\'smaller than replay capacity \\(5\\)\\.\'):\n      circular_replay_buffer.WrappedReplayBuffer(\n          observation_shape=OBSERVATION_SHAPE,\n          stack_size=STACK_SIZE,\n          replay_capacity=5,\n          update_horizon=5)\n\n  def testConstructorWithZeroUpdateHorizon(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 r\'Update horizon must be positive\\.\'):\n      circular_replay_buffer.WrappedReplayBuffer(\n          observation_shape=OBSERVATION_SHAPE,\n          stack_size=STACK_SIZE,\n          update_horizon=0)\n\n  def testConstructorWithOutOfBoundsDiscountFactor(self):\n    exception_string = r\'Discount factor \\(gamma\\) must be in \\[0, 1\\]\\.\'\n    with self.assertRaisesRegexp(ValueError, exception_string):\n      circular_replay_buffer.WrappedReplayBuffer(\n          observation_shape=OBSERVATION_SHAPE, stack_size=STACK_SIZE, gamma=-1)\n    with self.assertRaisesRegexp(ValueError, exception_string):\n      circular_replay_buffer.WrappedReplayBuffer(\n          observation_shape=OBSERVATION_SHAPE, stack_size=STACK_SIZE, gamma=1.1)\n\n  def testConstructorWithExtraStorageTypes(self):\n    circular_replay_buffer.WrappedReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        extra_storage_types=[\n            circular_replay_buffer.ReplayElement(\'extra1\', [], np.float32),\n            circular_replay_buffer.ReplayElement(\'extra2\', [2], np.int8)\n        ])\n\n  def _verify_sampled_trajectories(self, batch):\n    (states, action, reward, next_states, next_action, next_reward, terminal,\n     indices) = batch.values()\n    # Because we\'ve added BATCH_SIZE * 2 observation, using the enumerator to\n    # fill the respective observation, all observation will be np.full arrays\n    # where the values are in [0, BATCH_SIZE * 2). Because we\'re sampling we can\n    # deterministically predict what values will be sampled, we can ensure that\n    # they will all be in that range by setting a ""midpoint"" observation (with\n    # value BATCH_SIZE), and verifying that all observation values are near this\n    # midpoint, with tolerance BATCH_SIZE.\n    midpoint_observation = np.full(\n        (BATCH_SIZE,) + OBSERVATION_SHAPE + (STACK_SIZE,),\n        BATCH_SIZE,\n        dtype=OBS_DTYPE)\n    self.assertAllClose(states, midpoint_observation, rtol=BATCH_SIZE)\n    self.assertAllClose(next_states, midpoint_observation, rtol=BATCH_SIZE)\n    self.assertAllClose(action, np.ones(BATCH_SIZE) * 2)\n    self.assertAllClose(reward, np.ones(BATCH_SIZE))\n    self.assertAllClose(next_action, np.ones(BATCH_SIZE) * 2)\n    self.assertAllClose(next_reward, np.ones(BATCH_SIZE))\n    self.assertAllClose(terminal, np.zeros(BATCH_SIZE))\n    self.assertAllClose(indices, np.ones(BATCH_SIZE) * BATCH_SIZE,\n                        rtol=BATCH_SIZE)\n\n  def testConstructorWithNoStaging(self):\n    replay = circular_replay_buffer.WrappedReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=100,\n        batch_size=BATCH_SIZE,\n        use_staging=False)\n    with self.test_session() as sess:\n      for i in range(BATCH_SIZE * 2):\n        observation = np.full(OBSERVATION_SHAPE, i, dtype=OBS_DTYPE)\n        replay.add(observation, 2, 1, 0)\n    self._verify_sampled_trajectories(sess.run(replay.transition))\n\n  def testConstructorWithStaging(self):\n    replay = circular_replay_buffer.WrappedReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=100,\n        batch_size=BATCH_SIZE,\n        use_staging=True)\n    # When staging is on, replay._prefetch_batch tries to prefetch transitions\n    # for efficient sampling. Since no transitions have been added, this raises\n    # an error.\n    with self.assertRaisesOpError(\n        \'Cannot sample a batch with fewer than stack size\'):\n      self.evaluate(replay._prefetch_batch)\n    with self.test_session() as sess:\n      for i in range(BATCH_SIZE * 2):\n        observation = np.full(OBSERVATION_SHAPE, i, dtype=OBS_DTYPE)\n        replay.add(observation, 2, 1, 0)\n      sess.run(replay._prefetch_batch)\n    self._verify_sampled_trajectories(sess.run(replay.transition))\n\n  def testWrapperSave(self):\n    replay = circular_replay_buffer.WrappedReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=5,\n        batch_size=BATCH_SIZE)\n    replay.memory.observation = self._test_observation\n    replay.memory.action = self._test_action\n    replay.memory.reward = self._test_reward\n    replay.memory.terminal = self._test_terminal\n    replay.memory.add_count = self._test_add_count\n    replay.memory.invalid_range = self._test_invalid_range\n    replay.save(self._test_subdir, 3)\n    for attr in replay.memory.__dict__:\n      if attr.startswith(\'_\'):\n        continue\n      filename = os.path.join(self._test_subdir, \'{}_ckpt.3.gz\'.format(attr))\n      self.assertTrue(tf.gfile.Exists(filename))\n\n  def testWrapperLoad(self):\n    replay = circular_replay_buffer.WrappedReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE,\n        replay_capacity=5,\n        batch_size=BATCH_SIZE)\n    self.assertNotEqual(replay.memory._store[\'observation\'],\n                        self._test_observation)\n    self.assertNotEqual(replay.memory._store[\'action\'], self._test_action)\n    self.assertNotEqual(replay.memory._store[\'reward\'], self._test_reward)\n    self.assertNotEqual(replay.memory._store[\'terminal\'], self._test_terminal)\n    self.assertNotEqual(replay.memory.add_count, self._test_add_count)\n    self.assertNotEqual(replay.memory.invalid_range, self._test_invalid_range)\n    store_prefix = \'$store$_\'\n    numpy_arrays = {\n        store_prefix + \'observation\': self._test_observation,\n        store_prefix + \'action\': self._test_action,\n        store_prefix + \'reward\': self._test_reward,\n        store_prefix + \'terminal\': self._test_terminal,\n        \'add_count\': self._test_add_count,\n        \'invalid_range\': self._test_invalid_range\n    }\n    for attr in numpy_arrays:\n      filename = os.path.join(self._test_subdir, \'{}_ckpt.3.gz\'.format(attr))\n      with tf.gfile.Open(filename, \'w\') as f:\n        with gzip.GzipFile(fileobj=f) as outfile:\n          np.save(outfile, numpy_arrays[attr], allow_pickle=False)\n    replay.load(self._test_subdir, \'3\')\n    self.assertAllClose(replay.memory._store[\'observation\'],\n                        self._test_observation)\n    self.assertAllClose(replay.memory._store[\'action\'], self._test_action)\n    self.assertAllClose(replay.memory._store[\'reward\'], self._test_reward)\n    self.assertAllClose(replay.memory._store[\'terminal\'], self._test_terminal)\n    self.assertEqual(replay.memory.add_count, self._test_add_count)\n    self.assertAllClose(replay.memory.invalid_range, self._test_invalid_range)\n\n  def testDefaultObsDataType(self):\n    # Tests that the default data type for observations is np.uint8 for\n    # integration with Atari 2600.\n    replay = circular_replay_buffer.WrappedReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE, replay_capacity=10)\n    self.assertEqual(replay.memory._store[\'observation\'].dtype, np.uint8)\n\n  def testCustomObsDataType(self):\n    # Tests that observation store is initialized with the correct data type\n    # when an observation_dtype argument is passed to the constructor.\n    replay = circular_replay_buffer.WrappedReplayBuffer(\n        observation_shape=OBSERVATION_SHAPE,\n        stack_size=STACK_SIZE, replay_capacity=10, observation_dtype=np.int32)\n    self.assertEqual(replay.memory._store[\'observation\'].dtype, np.int32)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/replay_memory/prioritized_replay_buffer_test.py,3,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for prioritzed replay memory.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom dopamine.replay_memory import prioritized_replay_buffer\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\n\n# Default parameters used when creating the replay memory.\nSCREEN_SIZE = (84, 84)\nSTACK_SIZE = 4\nBATCH_SIZE = 32\nREPLAY_CAPACITY = 100\n\n\nclass OutOfGraphPrioritizedReplayBufferTest(tf.test.TestCase):\n\n  def create_default_memory(self):\n    return prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer(\n        SCREEN_SIZE,\n        STACK_SIZE,\n        REPLAY_CAPACITY,\n        BATCH_SIZE,\n        max_sample_attempts=10)  # For faster tests.\n\n  def add_blank(self, memory, action=0, reward=0.0, terminal=0, priority=1.0):\n    """"""Adds a replay transition with a blank observation.\n\n    Allows setting action, reward, terminal.\n    Args:\n      memory: The replay memory.\n      action: Integer.\n      reward: Float.\n      terminal: Integer (0 or 1).\n      priority: Float. Defults to standard priority of 1.\n\n    Returns:\n      Index of the transition just added.\n    """"""\n    dummy = np.zeros(SCREEN_SIZE)\n    memory.add(dummy, action, reward, terminal, priority)\n    index = (memory.cursor() - 1) % REPLAY_CAPACITY\n    return index\n\n  def testAddWithAndWithoutPriority(self):\n    memory = self.create_default_memory()\n    self.assertEqual(memory.cursor(), 0)\n    zeros = np.zeros(SCREEN_SIZE)\n\n    self.add_blank(memory)\n    self.assertEqual(memory.cursor(), STACK_SIZE)\n    self.assertEqual(memory.add_count, STACK_SIZE)\n\n    # Check that the prioritized replay buffer expects an additional argument\n    # for priority.\n    with self.assertRaisesRegexp(ValueError, \'Add expects\'):\n      memory.add(zeros, 0, 0, 0)\n\n  def testDummyScreensAddedToNewMemory(self):\n    memory = self.create_default_memory()\n    index = self.add_blank(memory)\n    for i in range(index):\n      self.assertEqual(memory.sum_tree.get(i), 0.0)\n\n  def testGetPriorityWithInvalidIndices(self):\n    memory = self.create_default_memory()\n    index = self.add_blank(memory)\n    with self.assertRaises(AssertionError, msg=\'Indices must be an array.\'):\n      memory.get_priority(index)\n    with self.assertRaises(AssertionError,\n                           msg=\'Indices must be int32s, given: int64\'):\n      memory.get_priority(np.array([index]))\n\n  def testSetAndGetPriority(self):\n    memory = self.create_default_memory()\n    batch_size = 7\n    indices = np.zeros(batch_size, dtype=np.int32)\n    for index in range(batch_size):\n      indices[index] = self.add_blank(memory)\n    priorities = np.arange(batch_size)\n    memory.set_priority(indices, priorities)\n    # We send the indices in reverse order and verify the priorities come back\n    # in that same order.\n    fetched_priorities = memory.get_priority(np.flip(indices, 0))\n    for i in range(batch_size):\n      self.assertEqual(priorities[i], fetched_priorities[batch_size - 1 - i])\n\n  def testNewElementHasHighPriority(self):\n    memory = self.create_default_memory()\n    index = self.add_blank(memory)\n    self.assertEqual(\n        memory.get_priority(np.array([index], dtype=np.int32))[0],\n        1.0)\n\n  def testLowPriorityElementNotFrequentlySampled(self):\n    memory = self.create_default_memory()\n    # Add an item and set its priority to 0.\n    self.add_blank(memory, terminal=0, priority=0.0)\n    # Now add a few new items.\n    for _ in range(3):\n      self.add_blank(memory, terminal=1)\n    # This test should always pass.\n    for _ in range(100):\n      _, _, _, _, _, _, terminals, _, _ = (\n          memory.sample_transition_batch(batch_size=2))\n      # Ensure all terminals are set to 1.\n      self.assertTrue((terminals == 1).all())\n\n  def testSampleIndexBatchTooManyFailedRetries(self):\n    memory = self.create_default_memory()\n    # Only adding a single observation is not enough to be able to sample\n    # (as it both straddles the cursor and does not pass the\n    # `index >= self.cursor() - self._update_horizon` check in\n    # circular_replay_buffer.py).\n    self.add_blank(memory)\n    with self.assertRaises(\n        RuntimeError,\n        msg=\'Max sample attempts: Tried 10 times but only sampled 1 valid \'\n            \'indices. Batch size is 2\'):\n      memory.sample_index_batch(2)\n\n  def testSampleIndexBatch(self):\n    memory = prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer(\n        SCREEN_SIZE,\n        STACK_SIZE,\n        REPLAY_CAPACITY,\n        BATCH_SIZE,\n        max_sample_attempts=REPLAY_CAPACITY)\n    # This will ensure we end up with cursor == 1.\n    for _ in range(REPLAY_CAPACITY - STACK_SIZE + 2):\n      self.add_blank(memory)\n    self.assertEqual(memory.cursor(), 1)\n    samples = memory.sample_index_batch(REPLAY_CAPACITY)\n    # Because cursor == 1, the invalid range as set by circular_replay_buffer.py\n    # will be # [0, 1, 2, 3], resulting in all samples being in\n    # [STACK_SIZE, REPLAY_CAPACITY - 1].\n    for sample in samples:\n      self.assertGreaterEqual(sample, STACK_SIZE)\n      self.assertLessEqual(sample, REPLAY_CAPACITY - 1)\n\n\nclass WrappedPrioritizedReplayBufferTest(tf.test.TestCase):\n  """"""Tests the Tensorflow wrapper around the Python replay memory.\n  """"""\n\n  def create_default_memory(self):\n    return prioritized_replay_buffer.WrappedPrioritizedReplayBuffer(\n        SCREEN_SIZE,\n        STACK_SIZE,\n        use_staging=False,\n        replay_capacity=REPLAY_CAPACITY,\n        batch_size=BATCH_SIZE,\n        max_sample_attempts=10)  # For faster tests.\n\n  def add_blank(self, replay):\n    replay.add(np.zeros(SCREEN_SIZE), 0, 0, 0, 1.0)\n\n  def testSetAndGetPriority(self):\n    replay = self.create_default_memory()\n\n    batch_size = 7\n    with self.test_session() as sess:\n      indices = np.zeros(batch_size, dtype=np.int32)\n      for index in range(batch_size):\n        self.add_blank(replay)\n        indices[index] = replay.memory.cursor() - 1\n\n      priorities = np.arange(batch_size)\n      sess.run(replay.tf_set_priority(indices, priorities))\n      # We send the indices in reverse order and verify the priorities come back\n      # in that same order.\n      fetched_priorities = sess.run(replay.tf_get_priority(np.flip(indices, 0)))\n      for i in range(batch_size):\n        self.assertEqual(priorities[i],\n                         fetched_priorities[batch_size - 1 - i])\n\n  def testSampleBatch(self):\n    replay = self.create_default_memory()\n\n    num_data = 64\n    with self.test_session() as sess:\n      for _ in range(num_data):\n        self.add_blank(replay)\n      probabilities = sess.run(replay.transition[\'sampling_probabilities\'])\n      for prob in probabilities:\n        self.assertEqual(prob, 1.0)\n\n  def testConstructorWithExtraStorageTypes(self):\n    prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer(\n        SCREEN_SIZE,\n        STACK_SIZE,\n        REPLAY_CAPACITY,\n        BATCH_SIZE,\n        extra_storage_types=[\n            prioritized_replay_buffer.ReplayElement(\'extra1\', [], np.float32),\n            prioritized_replay_buffer.ReplayElement(\'extra2\', [2], np.int8)\n        ])\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/replay_memory/sum_tree_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for dopamine.agents.rainbow.sum_tree.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\n\n\n\nfrom dopamine.replay_memory import sum_tree\nimport tensorflow.compat.v1 as tf\n\n\nclass SumTreeTest(tf.test.TestCase):\n\n  def setUp(self):\n    self._tree = sum_tree.SumTree(capacity=100)\n\n  def testNegativeCapacity(self):\n    with self.assertRaises(ValueError,\n                           msg=\'Sum tree capacity should be positive. Got: -1\'):\n      sum_tree.SumTree(capacity=-1)\n\n  def testSetNegativeValue(self):\n    with self.assertRaises(ValueError,\n                           msg=\'Sum tree values should be nonnegative. Got -1\'):\n      self._tree.set(node_index=0, value=-1)\n\n  def testSmallCapacityConstructor(self):\n    tree = sum_tree.SumTree(capacity=1)\n    self.assertEqual(len(tree.nodes), 1)\n    tree = sum_tree.SumTree(capacity=2)\n    self.assertEqual(len(tree.nodes), 2)\n\n  def testSetValueSmallCapacity(self):\n    tree = sum_tree.SumTree(capacity=1)\n    tree.set(0, 1.5)\n    self.assertEqual(tree.get(0), 1.5)\n\n  def testSetValue(self):\n    self._tree.set(node_index=0, value=1.0)\n    self.assertEqual(self._tree.get(0), 1.0)\n\n    # Validate that all nodes on the leftmost branch have value 1.\n    for level in self._tree.nodes:\n      self.assertEqual(level[0], 1.0)\n      nodes_at_this_depth = len(level)\n      for i in range(1, nodes_at_this_depth):\n        self.assertEqual(level[i], 0.0)\n\n  def testCapacityGreaterThanRequested(self):\n    self.assertGreaterEqual(len(self._tree.nodes[-1]), 100)\n\n  def testSampleFromEmptyTree(self):\n    with self.assertRaises(Exception,\n                           msg=\'Cannot sample from an empty sum tree.\'):\n      self._tree.sample()\n\n  def testSampleWithInvalidQueryValue(self):\n    self._tree.set(node_index=5, value=1.0)\n    with self.assertRaises(ValueError, msg=\'query_value must be in [0, 1].\'):\n      self._tree.sample(query_value=-0.1)\n    with self.assertRaises(ValueError, msg=\'query_value must be in [0, 1].\'):\n      self._tree.sample(query_value=1.1)\n\n  def testSampleSingleton(self):\n    self._tree.set(node_index=5, value=1.0)\n    item = self._tree.sample()\n\n    self.assertEqual(item, 5)\n\n  def testSamplePairWithUnevenProbabilities(self):\n    self._tree.set(node_index=2, value=1.0)\n    self._tree.set(node_index=3, value=3.0)\n\n    for _ in range(10000):\n      random.seed(1)\n      self.assertEqual(self._tree.sample(), 2)\n\n  def testSamplePairWithUnevenProbabilitiesWithQueryValue(self):\n    self._tree.set(node_index=2, value=1.0)\n    self._tree.set(node_index=3, value=3.0)\n\n    for _ in range(10000):\n      self.assertEqual(self._tree.sample(query_value=0.1), 2)\n\n  def testSamplingWithSeedDoesNotAffectFutureCalls(self):\n    # Setting the seed here will set a deterministic random value r, which will\n    # be used when sampling from the tree. Since it is scalled up by the total\n    # sum value of the tree, M, we can see that r\' * M + m = M, where:\n    #   - M  = total sum value of the tree (total_value)\n    #   - m  = value of node 3 (max_value)\n    #   - r\' = r + delta\n    # We can then solve for M: M = m / (1 - r\'), and we can set the value of the\n    # node 2 to r\' * M + delta, which will guarantee that\n    # r * M < r\' * M + delta, thereby guaranteeing that node 2 will always get\n    # picked.\n    seed = 1\n    random.seed(seed)\n    deterministic_random_value = random.random()\n    max_value = 100\n    delta = 0.01\n    total_value = max_value / (1 - deterministic_random_value - delta)\n    min_value = deterministic_random_value * total_value + delta\n    self._tree.set(node_index=2, value=min_value)\n    self._tree.set(node_index=3, value=max_value)\n    for _ in range(10000):\n      random.seed(seed)\n      self.assertEqual(self._tree.sample(), 2)\n    # The above loop demonstrated that there is 0 probability that node 3 gets\n    # selected. The loop below demonstrates that this probability is no longer\n    # 0 when the seed is not set explicitly. There is a very low probability\n    # that node 2 gets selected, but to avoid flakiness, we simply assert that\n    # node 3 gets selected most of the time.\n    counts = {2: 0, 3: 0}\n    for _ in range(10000):\n      counts[self._tree.sample()] += 1\n    self.assertLess(counts[2], counts[3])\n\n  def testStratifiedSamplingFromEmptyTree(self):\n    with self.assertRaises(Exception,\n                           msg=\'Cannot sample from an empty sum tree.\'):\n      self._tree.stratified_sample(5)\n\n  def testStratifiedSampling(self):\n    k = 32\n    for i in range(k):\n      self._tree.set(node_index=i, value=1)\n    samples = self._tree.stratified_sample(k)\n    self.assertEqual(len(samples), k)\n    for i in range(k):\n      self.assertEqual(samples[i], i)\n\n  def testMaxRecordedProbability(self):\n    k = 32\n    self._tree.set(node_index=0, value=0)\n    self.assertEqual(self._tree.max_recorded_priority, 1)\n    for i in range(1, k):\n      self._tree.set(node_index=i, value=i)\n      self.assertEqual(self._tree.max_recorded_priority, i)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/tests/gin_config_test.py,31,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for establishing correct dependency injection with gin.""""""\n\nimport datetime\nimport os\nimport shutil\n\n\n\nfrom absl import flags\nfrom dopamine.discrete_domains import atari_lib\nfrom dopamine.discrete_domains import run_experiment\nimport tensorflow.compat.v1 as tf\n\nimport gin.tf\n\nFLAGS = flags.FLAGS\n\n\nclass GinConfigTest(tf.test.TestCase):\n  """"""Tests for configuring Atari agents using gin.\n\n  """"""\n\n  def setUp(self):\n    self._base_dir = os.path.join(\n        \'/tmp/dopamine_tests\',\n        datetime.datetime.utcnow().strftime(\'run_%Y_%m_%d_%H_%M_%S\'))\n    self._checkpoint_dir = os.path.join(self._base_dir, \'checkpoints\')\n    self._logging_dir = os.path.join(self._base_dir, \'logs\')\n    self._videos_dir = os.path.join(self._base_dir, \'videos\')\n    gin.clear_config()\n\n  def testDefaultGinDqn(self):\n    """"""Test DQNAgent configuration using the default gin config.""""""\n    tf.logging.info(\'####### Training the DQN agent #####\')\n    tf.logging.info(\'####### DQN base_dir: {}\'.format(self._base_dir))\n    gin_files = [\'dopamine/agents/dqn/configs/dqn.gin\']\n    gin_bindings = [\n        \'WrappedReplayBuffer.replay_capacity = 100\',  # To prevent OOM.\n        ""create_agent.agent_name = \'dqn\'""\n    ]\n    run_experiment.load_gin_configs(gin_files, gin_bindings)\n    runner = run_experiment.Runner(self._base_dir, run_experiment.create_agent,\n                                   atari_lib.create_atari_environment)\n    self.assertIsInstance(runner._agent.optimizer, tf.train.RMSPropOptimizer)\n    self.assertNear(0.00025, runner._agent.optimizer._learning_rate, 0.0001)\n    shutil.rmtree(self._base_dir)\n\n  def testOverrideRunnerParams(self):\n    """"""Test DQNAgent configuration using the default gin config.""""""\n    tf.logging.info(\'####### Training the DQN agent #####\')\n    tf.logging.info(\'####### DQN base_dir: {}\'.format(self._base_dir))\n    gin_files = [\'dopamine/agents/dqn/configs/dqn.gin\']\n    gin_bindings = [\n        \'TrainRunner.base_dir = ""{}""\'.format(self._base_dir),\n        \'Runner.log_every_n = 1729\',\n        \'WrappedReplayBuffer.replay_capacity = 100\',  # To prevent OOM.\n        ""create_agent.agent_name = \'dqn\'""\n    ]\n    run_experiment.load_gin_configs(gin_files, gin_bindings)\n    runner = run_experiment.TrainRunner(\n        create_agent_fn=run_experiment.create_agent,\n        create_environment_fn=atari_lib.create_atari_environment)\n    self.assertEqual(runner._base_dir, self._base_dir)\n    self.assertEqual(runner._log_every_n, 1729)\n    shutil.rmtree(self._base_dir)\n\n  def testDefaultGinRmspropDqn(self):\n    """"""Test DQNAgent configuration overridden with RMSPropOptimizer.""""""\n    tf.logging.info(\'####### Training the DQN agent #####\')\n    tf.logging.info(\'####### DQN base_dir: {}\'.format(self._base_dir))\n    gin_files = [\'dopamine/agents/dqn/configs/dqn.gin\']\n    gin_bindings = [\n        \'DQNAgent.optimizer = @tf.train.RMSPropOptimizer()\',\n        \'tf.train.RMSPropOptimizer.learning_rate = 100\',\n        \'WrappedReplayBuffer.replay_capacity = 100\',  # To prevent OOM.\n        ""create_agent.agent_name = \'dqn\'""\n    ]\n    run_experiment.load_gin_configs(gin_files, gin_bindings)\n    runner = run_experiment.Runner(self._base_dir, run_experiment.create_agent,\n                                   atari_lib.create_atari_environment)\n    self.assertIsInstance(runner._agent.optimizer, tf.train.RMSPropOptimizer)\n    self.assertEqual(100, runner._agent.optimizer._learning_rate)\n    shutil.rmtree(self._base_dir)\n\n  def testOverrideGinDqn(self):\n    """"""Test DQNAgent configuration overridden with AdamOptimizer.""""""\n    tf.logging.info(\'####### Training the DQN agent #####\')\n    tf.logging.info(\'####### DQN base_dir: {}\'.format(self._base_dir))\n    gin_files = [\'dopamine/agents/dqn/configs/dqn.gin\']\n    gin_bindings = [\n        \'DQNAgent.optimizer = @tf.train.AdamOptimizer()\',\n        \'tf.train.AdamOptimizer.learning_rate = 100\',\n        \'WrappedReplayBuffer.replay_capacity = 100\',  # To prevent OOM.\n        ""create_agent.agent_name = \'dqn\'""\n    ]\n\n    run_experiment.load_gin_configs(gin_files, gin_bindings)\n    runner = run_experiment.Runner(self._base_dir, run_experiment.create_agent,\n                                   atari_lib.create_atari_environment)\n    self.assertIsInstance(runner._agent.optimizer, tf.train.AdamOptimizer)\n    self.assertEqual(100, runner._agent.optimizer._lr)\n    shutil.rmtree(self._base_dir)\n\n  def testDefaultGinRainbow(self):\n    """"""Test RainbowAgent default configuration using default gin.""""""\n    tf.logging.info(\'####### Training the RAINBOW agent #####\')\n    tf.logging.info(\'####### RAINBOW base_dir: {}\'.format(self._base_dir))\n    gin_files = [\n        \'dopamine/agents/rainbow/configs/rainbow.gin\'\n    ]\n    gin_bindings = [\n        \'WrappedReplayBuffer.replay_capacity = 100\',  # To prevent OOM.\n        ""create_agent.agent_name = \'rainbow\'""\n    ]\n    run_experiment.load_gin_configs(gin_files, gin_bindings)\n    runner = run_experiment.Runner(self._base_dir, run_experiment.create_agent,\n                                   atari_lib.create_atari_environment)\n    self.assertIsInstance(runner._agent.optimizer, tf.train.AdamOptimizer)\n    self.assertNear(0.0000625, runner._agent.optimizer._lr, 0.0001)\n    shutil.rmtree(self._base_dir)\n\n  def testOverrideGinRainbow(self):\n    """"""Test RainbowAgent configuration overridden with RMSPropOptimizer.""""""\n    tf.logging.info(\'####### Training the RAINBOW agent #####\')\n    tf.logging.info(\'####### RAINBOW base_dir: {}\'.format(self._base_dir))\n    gin_files = [\n        \'dopamine/agents/rainbow/configs/rainbow.gin\',\n    ]\n    gin_bindings = [\n        \'RainbowAgent.optimizer = @tf.train.RMSPropOptimizer()\',\n        \'tf.train.RMSPropOptimizer.learning_rate = 100\',\n        \'WrappedReplayBuffer.replay_capacity = 100\',  # To prevent OOM.\n        ""create_agent.agent_name = \'rainbow\'""\n    ]\n    run_experiment.load_gin_configs(gin_files, gin_bindings)\n    runner = run_experiment.Runner(self._base_dir, run_experiment.create_agent,\n                                   atari_lib.create_atari_environment)\n    self.assertIsInstance(runner._agent.optimizer, tf.train.RMSPropOptimizer)\n    self.assertEqual(100, runner._agent.optimizer._learning_rate)\n    shutil.rmtree(self._base_dir)\n\n  def testDefaultDQNConfig(self):\n    """"""Verify the default DQN configuration.""""""\n    run_experiment.load_gin_configs(\n        [\'dopamine/agents/dqn/configs/dqn.gin\'], [])\n    agent = run_experiment.create_agent(\n        self.test_session(),\n        atari_lib.create_atari_environment(game_name=\'Pong\'))\n    self.assertEqual(agent.gamma, 0.99)\n    self.assertEqual(agent.update_horizon, 1)\n    self.assertEqual(agent.min_replay_history, 20000)\n    self.assertEqual(agent.update_period, 4)\n    self.assertEqual(agent.target_update_period, 8000)\n    self.assertEqual(agent.epsilon_train, 0.01)\n    self.assertEqual(agent.epsilon_eval, 0.001)\n    self.assertEqual(agent.epsilon_decay_period, 250000)\n    self.assertEqual(agent._replay.memory._replay_capacity, 1000000)\n    self.assertEqual(agent._replay.memory._batch_size, 32)\n\n  def testDefaultC51Config(self):\n    """"""Verify the default C51 configuration.""""""\n    run_experiment.load_gin_configs(\n        [\'dopamine/agents/rainbow/configs/c51.gin\'], [])\n    agent = run_experiment.create_agent(\n        self.test_session(),\n        atari_lib.create_atari_environment(game_name=\'Pong\'))\n    self.assertEqual(agent._num_atoms, 51)\n    support = self.evaluate(agent._support)\n    self.assertEqual(min(support), -10.)\n    self.assertEqual(max(support), 10.)\n    self.assertEqual(len(support), 51)\n    self.assertEqual(agent.gamma, 0.99)\n    self.assertEqual(agent.update_horizon, 1)\n    self.assertEqual(agent.min_replay_history, 20000)\n    self.assertEqual(agent.update_period, 4)\n    self.assertEqual(agent.target_update_period, 8000)\n    self.assertEqual(agent.epsilon_train, 0.01)\n    self.assertEqual(agent.epsilon_eval, 0.001)\n    self.assertEqual(agent.epsilon_decay_period, 250000)\n    self.assertEqual(agent._replay.memory._replay_capacity, 1000000)\n    self.assertEqual(agent._replay.memory._batch_size, 32)\n\n  def testDefaultRainbowConfig(self):\n    """"""Verify the default Rainbow configuration.""""""\n    run_experiment.load_gin_configs(\n        [\'dopamine/agents/rainbow/configs/rainbow.gin\'], [])\n    agent = run_experiment.create_agent(\n        self.test_session(),\n        atari_lib.create_atari_environment(game_name=\'Pong\'))\n    self.assertEqual(agent._num_atoms, 51)\n    support = self.evaluate(agent._support)\n    self.assertEqual(min(support), -10.)\n    self.assertEqual(max(support), 10.)\n    self.assertEqual(len(support), 51)\n    self.assertEqual(agent.gamma, 0.99)\n    self.assertEqual(agent.update_horizon, 3)\n    self.assertEqual(agent.min_replay_history, 20000)\n    self.assertEqual(agent.update_period, 4)\n    self.assertEqual(agent.target_update_period, 8000)\n    self.assertEqual(agent.epsilon_train, 0.01)\n    self.assertEqual(agent.epsilon_eval, 0.001)\n    self.assertEqual(agent.epsilon_decay_period, 250000)\n    self.assertEqual(agent._replay.memory._replay_capacity, 1000000)\n    self.assertEqual(agent._replay.memory._batch_size, 32)\n\n  def testDefaultGinImplicitQuantileIcml(self):\n    """"""Test default ImplicitQuantile configuration using ICML gin.""""""\n    tf.logging.info(\'###### Training the Implicit Quantile agent #####\')\n    self._base_dir = os.path.join(\n        \'/tmp/dopamine_tests\',\n        datetime.datetime.utcnow().strftime(\'run_%Y_%m_%d_%H_%M_%S\'))\n    tf.logging.info(\'###### IQN base dir: {}\'.format(self._base_dir))\n    gin_files = [\'dopamine/agents/\'\n                 \'implicit_quantile/configs/implicit_quantile_icml.gin\']\n    gin_bindings = [\n        \'Runner.num_iterations=0\',\n    ]\n    run_experiment.load_gin_configs(gin_files, gin_bindings)\n    runner = run_experiment.Runner(self._base_dir, run_experiment.create_agent,\n                                   atari_lib.create_atari_environment)\n    self.assertEqual(1000000, runner._agent._replay.memory._replay_capacity)\n    shutil.rmtree(self._base_dir)\n\n  def testOverrideGinImplicitQuantileIcml(self):\n    """"""Test ImplicitQuantile configuration overriding using ICML gin.""""""\n    tf.logging.info(\'###### Training the Implicit Quantile agent #####\')\n    self._base_dir = os.path.join(\n        \'/tmp/dopamine_tests\',\n        datetime.datetime.utcnow().strftime(\'run_%Y_%m_%d_%H_%M_%S\'))\n    tf.logging.info(\'###### IQN base dir: {}\'.format(self._base_dir))\n    gin_files = [\'dopamine/agents/implicit_quantile/configs/\'\n                 \'implicit_quantile_icml.gin\']\n    gin_bindings = [\n        \'Runner.num_iterations=0\',\n        \'WrappedPrioritizedReplayBuffer.replay_capacity = 1000\',\n    ]\n    run_experiment.load_gin_configs(gin_files, gin_bindings)\n    runner = run_experiment.Runner(self._base_dir, run_experiment.create_agent,\n                                   atari_lib.create_atari_environment)\n    self.assertEqual(1000, runner._agent._replay.memory._replay_capacity)\n    shutil.rmtree(self._base_dir)\n\n  def testOverrideGinImplicitQuantile(self):\n    """"""Test ImplicitQuantile configuration overriding using IQN gin.""""""\n    tf.logging.info(\'###### Training the Implicit Quantile agent #####\')\n    self._base_dir = os.path.join(\n        \'/tmp/dopamine_tests\',\n        datetime.datetime.utcnow().strftime(\'run_%Y_%m_%d_%H_%M_%S\'))\n    tf.logging.info(\'###### IQN base dir: {}\'.format(self._base_dir))\n    gin_files = [\'dopamine/agents/implicit_quantile/configs/\'\n                 \'implicit_quantile.gin\']\n    gin_bindings = [\n        \'Runner.num_iterations=0\',\n        \'WrappedPrioritizedReplayBuffer.replay_capacity = 1000\',\n    ]\n    run_experiment.load_gin_configs(gin_files, gin_bindings)\n    runner = run_experiment.Runner(self._base_dir, run_experiment.create_agent,\n                                   atari_lib.create_atari_environment)\n    self.assertEqual(1000, runner._agent._replay.memory._replay_capacity)\n    shutil.rmtree(self._base_dir)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/tests/integration_test.py,6,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""End to end integration tests for Dopamine package.""""""\n\nimport datetime\nimport os\nimport shutil\n\n\n\nfrom absl import flags\nfrom dopamine.discrete_domains import train\nimport tensorflow.compat.v1 as tf\n\nimport gin.tf\n\n\nFLAGS = flags.FLAGS\n\n\nclass AtariIntegrationTest(tf.test.TestCase):\n  """"""Tests for Atari environment with various agents.\n\n  """"""\n\n  def setUp(self):\n    FLAGS.base_dir = os.path.join(\n        \'/tmp/dopamine_tests\',\n        datetime.datetime.utcnow().strftime(\'run_%Y_%m_%d_%H_%M_%S\'))\n    self._checkpoint_dir = os.path.join(FLAGS.base_dir, \'checkpoints\')\n    self._logging_dir = os.path.join(FLAGS.base_dir, \'logs\')\n    FLAGS.alsologtostderr = True\n    gin.clear_config()\n\n  def quickDqnFlags(self):\n    """"""Assign flags for a quick run of DQNAgent.""""""\n    FLAGS.gin_files = [\'dopamine/agents/dqn/configs/dqn.gin\']\n    FLAGS.gin_bindings = [\n        \'Runner.training_steps=100\',\n        \'Runner.evaluation_steps=10\',\n        \'Runner.num_iterations=1\',\n        \'Runner.max_steps_per_episode=100\',\n        \'dqn_agent.DQNAgent.min_replay_history=500\',\n        \'WrappedReplayBuffer.replay_capacity=100\'\n    ]\n\n  def quickRainbowFlags(self):\n    """"""Assign flags for a quick run of RainbowAgent.""""""\n    FLAGS.gin_files = [\n        \'dopamine/agents/rainbow/configs/rainbow.gin\'\n    ]\n    FLAGS.gin_bindings = [\n        \'Runner.training_steps=100\',\n        \'Runner.evaluation_steps=10\',\n        \'Runner.num_iterations=1\',\n        \'Runner.max_steps_per_episode=100\',\n        ""rainbow_agent.RainbowAgent.replay_scheme=\'prioritized\'"",\n        \'rainbow_agent.RainbowAgent.min_replay_history=500\',\n        \'WrappedReplayBuffer.replay_capacity=100\'\n    ]\n\n  def verifyFilesCreated(self, base_dir):\n    """"""Verify that files have been created.""""""\n    # Check checkpoint files\n    self.assertTrue(\n        os.path.exists(os.path.join(self._checkpoint_dir, \'ckpt.0\')))\n    self.assertTrue(\n        os.path.exists(os.path.join(self._checkpoint_dir, \'checkpoint\')))\n    self.assertTrue(\n        os.path.exists(\n            os.path.join(self._checkpoint_dir,\n                         \'sentinel_checkpoint_complete.0\')))\n    # Check log files\n    self.assertTrue(os.path.exists(os.path.join(self._logging_dir, \'log_0\')))\n\n  def testIntegrationDqn(self):\n    """"""Test the DQN agent.""""""\n    tf.logging.info(\'####### Training the DQN agent #####\')\n    tf.logging.info(\'####### DQN base_dir: {}\'.format(FLAGS.base_dir))\n    self.quickDqnFlags()\n    train.main([])\n    self.verifyFilesCreated(FLAGS.base_dir)\n    shutil.rmtree(FLAGS.base_dir)\n\n  def testIntegrationRainbow(self):\n    """"""Test the rainbow agent.""""""\n    tf.logging.info(\'####### Training the Rainbow agent #####\')\n    tf.logging.info(\'####### Rainbow base_dir: {}\'.format(FLAGS.base_dir))\n    self.quickRainbowFlags()\n    train.main([])\n    self.verifyFilesCreated(FLAGS.base_dir)\n    shutil.rmtree(FLAGS.base_dir)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/tests/train_runner_integration_test.py,4,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""End to end tests for TrainRunner.""""""\n\nimport datetime\nimport os\nimport shutil\n\n\n\nfrom absl import flags\n\nfrom dopamine.discrete_domains import train\nimport tensorflow.compat.v1 as tf\n\nFLAGS = flags.FLAGS\n\n\nclass TrainRunnerIntegrationTest(tf.test.TestCase):\n  """"""Tests for Atari environment with various agents.\n\n  """"""\n\n  def setUp(self):\n    FLAGS.base_dir = os.path.join(\n        \'/tmp/dopamine_tests\',\n        datetime.datetime.utcnow().strftime(\'run_%Y_%m_%d_%H_%M_%S\'))\n    self._checkpoint_dir = os.path.join(FLAGS.base_dir, \'checkpoints\')\n    self._logging_dir = os.path.join(FLAGS.base_dir, \'logs\')\n\n  def quickDqnFlags(self):\n    """"""Assign flags for a quick run of DQN agent.""""""\n    FLAGS.gin_files = [\'dopamine/agents/dqn/configs/dqn.gin\']\n    FLAGS.gin_bindings = [\n        ""create_runner.schedule=\'continuous_train\'"",\n        \'Runner.training_steps=100\',\n        \'Runner.evaluation_steps=10\',\n        \'Runner.num_iterations=1\',\n        \'Runner.max_steps_per_episode=100\',\n        \'dqn_agent.DQNAgent.min_replay_history=500\',\n        \'WrappedReplayBuffer.replay_capacity=100\'\n    ]\n    FLAGS.alsologtostderr = True\n\n  def verifyFilesCreated(self, base_dir):\n    """"""Verify that files have been created.""""""\n    # Check checkpoint files\n    self.assertTrue(\n        os.path.exists(os.path.join(self._checkpoint_dir, \'ckpt.0\')))\n    self.assertTrue(\n        os.path.exists(os.path.join(self._checkpoint_dir, \'checkpoint\')))\n    self.assertTrue(\n        os.path.exists(\n            os.path.join(self._checkpoint_dir,\n                         \'sentinel_checkpoint_complete.0\')))\n    # Check log files\n    self.assertTrue(os.path.exists(os.path.join(self._logging_dir, \'log_0\')))\n\n  def testIntegrationDqn(self):\n    """"""Test the DQN agent.""""""\n    tf.logging.info(\'####### Training the DQN agent #####\')\n    tf.logging.info(\'####### DQN base_dir: {}\'.format(FLAGS.base_dir))\n    self.quickDqnFlags()\n    train.main([])\n    self.verifyFilesCreated(FLAGS.base_dir)\n    shutil.rmtree(FLAGS.base_dir)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/utils/agent_visualizer_test.py,3,"b'# coding=utf-8\n# Copyright 2019 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for dopamine.utils.agent_visualizer.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\n\n\n\nfrom absl import flags\nfrom dopamine.utils.agent_visualizer import AgentVisualizer\nfrom dopamine.utils.line_plotter import LinePlotter\nimport numpy as np\nfrom PIL import Image\nimport tensorflow.compat.v1 as tf\n\n\nFLAGS = flags.FLAGS\n\n\nclass AgentVisualizerTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(AgentVisualizerTest, self).setUp()\n    self._test_subdir = os.path.join(\'/tmp/dopamine_tests\', \'agent_visualizer\')\n    shutil.rmtree(self._test_subdir, ignore_errors=True)\n    os.makedirs(self._test_subdir)\n\n  def test_agent_visualizer_save_frame(self):\n    parameter_dict = LinePlotter._defaults.copy()\n    parameter_dict[\'get_line_data_fn\'] = lambda: [[1, 2, 3]]\n    plotter = LinePlotter(parameter_dict=parameter_dict)\n\n    agent_visualizer = AgentVisualizer(self._test_subdir, [plotter])\n    agent_visualizer.save_frame()\n\n    frame_filename = os.path.join(self._test_subdir, \'frame_000000.png\')\n    self.assertTrue(tf.gfile.Exists(frame_filename))\n\n    im = Image.open(frame_filename)\n    im_arr = np.array(im)\n    self.assertTrue(np.array_equal(im_arr, agent_visualizer.record_frame))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/agents/dqn/dqn_agent_test.py,31,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for dopamine.agents.dqn_agent.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\n\n\n\nfrom absl import flags\nfrom dopamine.agents.dqn import dqn_agent\nfrom dopamine.discrete_domains import atari_lib\nfrom dopamine.utils import test_utils\nimport mock\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nimport gin.tf\n\nFLAGS = flags.FLAGS\n\n\nclass DQNAgentTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(DQNAgentTest, self).setUp()\n    self._test_subdir = os.path.join(\'/tmp/dopamine_tests\', \'ckpts\')\n    shutil.rmtree(self._test_subdir, ignore_errors=True)\n    os.makedirs(self._test_subdir)\n    self.num_actions = 4\n    self.min_replay_history = 6\n    self.update_period = 2\n    self.target_update_period = 4\n    self.epsilon_decay_period = 90\n    self.epsilon_train = 0.05\n    self.observation_shape = dqn_agent.NATURE_DQN_OBSERVATION_SHAPE\n    self.observation_dtype = dqn_agent.NATURE_DQN_DTYPE\n    self.stack_size = dqn_agent.NATURE_DQN_STACK_SIZE\n    self.zero_state = np.zeros(\n        (1,) + self.observation_shape + (self.stack_size,))\n    gin.bind_parameter(\'WrappedReplayBuffer.replay_capacity\', 100)\n\n  def _create_test_agent(self, sess, allow_partial_reload=False):\n    stack_size = self.stack_size\n\n    # This dummy network allows us to deterministically anticipate that\n    # action 0 will be selected by an argmax.\n    class MockDQNNetwork(tf.keras.Model):\n      """"""The Keras network used in tests.""""""\n\n      def __init__(self, num_actions, **kwargs):\n        # This weights_initializer gives action 0 a higher weight, ensuring\n        # that it gets picked by the argmax.\n        super(MockDQNNetwork, self).__init__(**kwargs)\n        weights_initializer = np.tile(\n            np.arange(num_actions, 0, -1), (stack_size, 1))\n        self.layer = tf.keras.layers.Dense(\n            num_actions,\n            kernel_initializer=tf.constant_initializer(weights_initializer),\n            bias_initializer=tf.ones_initializer())\n\n      def call(self, state):\n        inputs = tf.constant(\n            np.zeros((state.shape[0], stack_size)), dtype=tf.float32)\n        return atari_lib.DQNNetworkType(self.layer((inputs)))\n\n    agent = dqn_agent.DQNAgent(\n        sess=sess,\n        network=MockDQNNetwork,\n        observation_shape=self.observation_shape,\n        observation_dtype=self.observation_dtype,\n        stack_size=self.stack_size,\n        num_actions=self.num_actions,\n        min_replay_history=self.min_replay_history,\n        epsilon_fn=lambda w, x, y, z: 0.0,  # No exploration.\n        update_period=self.update_period,\n        target_update_period=self.target_update_period,\n        epsilon_eval=0.0,  # No exploration during evaluation.\n        allow_partial_reload=allow_partial_reload)\n    # This ensures non-random action choices (since epsilon_eval = 0.0) and\n    # skips the train_step.\n    agent.eval_mode = True\n    sess.run(tf.global_variables_initializer())\n    return agent\n\n  def testCreateAgentWithDefaults(self):\n    # Verifies that we can create and train an agent with the default values.\n    with tf.Session() as sess:\n      agent = dqn_agent.DQNAgent(sess, num_actions=4)\n      sess.run(tf.global_variables_initializer())\n      observation = np.ones([84, 84, 1])\n      agent.begin_episode(observation)\n      agent.step(reward=1, observation=observation)\n      agent.end_episode(reward=1)\n\n  def testBeginEpisode(self):\n    """"""Test the functionality of agent.begin_episode.\n\n    Specifically, the action returned and its effect on state.\n    """"""\n    with tf.Session() as sess:\n      agent = self._create_test_agent(sess)\n      # We fill up the state with 9s. On calling agent.begin_episode the state\n      # should be reset to all 0s.\n      agent.state.fill(9)\n      first_observation = np.ones(self.observation_shape + (1,))\n      self.assertEqual(agent.begin_episode(first_observation), 0)\n      # When the all-1s observation is received, it will be placed at the end of\n      # the state.\n      expected_state = self.zero_state\n      expected_state[:, :, :, -1] = np.ones((1,) + self.observation_shape)\n      self.assertAllEqual(agent.state, expected_state)\n      self.assertAllEqual(agent._observation, first_observation[:, :, 0])\n      # No training happens in eval mode.\n      self.assertEqual(agent.training_steps, 0)\n\n      # This will now cause training to happen.\n      agent.eval_mode = False\n      # Having a low replay memory add_count will prevent any of the\n      # train/prefetch/sync ops from being called.\n      agent._replay.memory.add_count = 0\n      second_observation = np.ones(self.observation_shape + (1,)) * 2\n      agent.begin_episode(second_observation)\n      # The agent\'s state will be reset, so we will only be left with the all-2s\n      # observation.\n      expected_state[:, :, :, -1] = np.full((1,) + self.observation_shape, 2)\n      self.assertAllEqual(agent.state, expected_state)\n      self.assertAllEqual(agent._observation, second_observation[:, :, 0])\n      # training_steps is incremented since we set eval_mode to False.\n      self.assertEqual(agent.training_steps, 1)\n\n  def testStepEval(self):\n    """"""Test the functionality of agent.step() in eval mode.\n\n    Specifically, the action returned, and confirm no training is happening.\n    """"""\n    with tf.Session() as sess:\n      agent = self._create_test_agent(sess)\n      base_observation = np.ones(self.observation_shape + (1,))\n      # This will reset state and choose a first action.\n      agent.begin_episode(base_observation)\n      # We mock the replay buffer to verify how the agent interacts with it.\n      agent._replay = test_utils.MockReplayBuffer()\n      self.evaluate(tf.global_variables_initializer())\n\n      expected_state = self.zero_state\n      num_steps = 10\n      for step in range(1, num_steps + 1):\n        # We make observation a multiple of step for testing purposes (to\n        # uniquely identify each observation).\n        observation = base_observation * step\n        self.assertEqual(agent.step(reward=1, observation=observation), 0)\n        stack_pos = step - num_steps - 1\n        if stack_pos >= -self.stack_size:\n          expected_state[:, :, :, stack_pos] = np.full(\n              (1,) + self.observation_shape, step)\n      self.assertAllEqual(agent.state, expected_state)\n      self.assertAllEqual(\n          agent._last_observation,\n          np.ones(self.observation_shape) * (num_steps - 1))\n      self.assertAllEqual(agent._observation, observation[:, :, 0])\n      # No training happens in eval mode.\n      self.assertEqual(agent.training_steps, 0)\n      # No transitions are added in eval mode.\n      self.assertEqual(agent._replay.add.call_count, 0)\n\n  def testStepTrain(self):\n    """"""Test the functionality of agent.step() in train mode.\n\n    Specifically, the action returned, and confirm training is happening.\n    """"""\n    with tf.Session() as sess:\n      agent = self._create_test_agent(sess)\n      agent.eval_mode = False\n      base_observation = np.ones(self.observation_shape + (1,))\n      # We mock the replay buffer to verify how the agent interacts with it.\n      agent._replay = test_utils.MockReplayBuffer()\n      self.evaluate(tf.global_variables_initializer())\n      # This will reset state and choose a first action.\n      agent.begin_episode(base_observation)\n      observation = base_observation\n\n      expected_state = self.zero_state\n      num_steps = 10\n      for step in range(1, num_steps + 1):\n        # We make observation a multiple of step for testing purposes (to\n        # uniquely identify each observation).\n        last_observation = observation\n        observation = base_observation * step\n        self.assertEqual(agent.step(reward=1, observation=observation), 0)\n        stack_pos = step - num_steps - 1\n        if stack_pos >= -self.stack_size:\n          expected_state[:, :, :, stack_pos] = np.full(\n              (1,) + self.observation_shape, step)\n        self.assertEqual(agent._replay.add.call_count, step)\n        mock_args, _ = agent._replay.add.call_args\n        self.assertAllEqual(last_observation[:, :, 0], mock_args[0])\n        self.assertAllEqual(0, mock_args[1])  # Action selected.\n        self.assertAllEqual(1, mock_args[2])  # Reward received.\n        self.assertFalse(mock_args[3])  # is_terminal\n      self.assertAllEqual(agent.state, expected_state)\n      self.assertAllEqual(\n          agent._last_observation,\n          np.full(self.observation_shape, num_steps - 1))\n      self.assertAllEqual(agent._observation, observation[:, :, 0])\n      # We expect one more than num_steps because of the call to begin_episode.\n      self.assertEqual(agent.training_steps, num_steps + 1)\n      self.assertEqual(agent._replay.add.call_count, num_steps)\n\n      agent.end_episode(reward=1)\n      self.assertEqual(agent._replay.add.call_count, num_steps + 1)\n      mock_args, _ = agent._replay.add.call_args\n      self.assertAllEqual(observation[:, :, 0], mock_args[0])\n      self.assertAllEqual(0, mock_args[1])  # Action selected.\n      self.assertAllEqual(1, mock_args[2])  # Reward received.\n      self.assertTrue(mock_args[3])  # is_terminal\n\n  def testNonTupleObservationShape(self):\n    with self.assertRaises(AssertionError):\n      self.observation_shape = 84\n      with tf.Session() as sess:\n        _ = self._create_test_agent(sess)\n\n  def _testCustomShapes(self, shape, dtype, stack_size):\n    self.observation_shape = shape\n    self.observation_dtype = dtype\n    self.stack_size = stack_size\n    self.zero_state = np.zeros((1,) + shape + (stack_size,))\n    with tf.Session() as sess:\n      agent = self._create_test_agent(sess)\n      agent.eval_mode = False\n      base_observation = np.ones(self.observation_shape + (1,))\n      # We mock the replay buffer to verify how the agent interacts with it.\n      agent._replay = test_utils.MockReplayBuffer()\n      self.evaluate(tf.global_variables_initializer())\n      # This will reset state and choose a first action.\n      agent.begin_episode(base_observation)\n      observation = base_observation\n\n      expected_state = self.zero_state\n      num_steps = 10\n      for step in range(1, num_steps + 1):\n        # We make observation a multiple of step for testing purposes (to\n        # uniquely identify each observation).\n        last_observation = observation\n        observation = base_observation * step\n        self.assertEqual(agent.step(reward=1, observation=observation), 0)\n        stack_pos = step - num_steps - 1\n        if stack_pos >= -self.stack_size:\n          expected_state[..., stack_pos] = np.full(\n              (1,) + self.observation_shape, step)\n        self.assertEqual(agent._replay.add.call_count, step)\n        mock_args, _ = agent._replay.add.call_args\n        self.assertAllEqual(last_observation[..., 0], mock_args[0])\n        self.assertAllEqual(0, mock_args[1])  # Action selected.\n        self.assertAllEqual(1, mock_args[2])  # Reward received.\n        self.assertFalse(mock_args[3])  # is_terminal\n      self.assertAllEqual(agent.state, expected_state)\n      self.assertAllEqual(\n          agent._last_observation,\n          np.full(self.observation_shape, num_steps - 1))\n      self.assertAllEqual(agent._observation, observation[..., 0])\n      # We expect one more than num_steps because of the call to begin_episode.\n      self.assertEqual(agent.training_steps, num_steps + 1)\n      self.assertEqual(agent._replay.add.call_count, num_steps)\n\n      agent.end_episode(reward=1)\n      self.assertEqual(agent._replay.add.call_count, num_steps + 1)\n      mock_args, _ = agent._replay.add.call_args\n      self.assertAllEqual(observation[..., 0], mock_args[0])\n      self.assertAllEqual(0, mock_args[1])  # Action selected.\n      self.assertAllEqual(1, mock_args[2])  # Reward received.\n      self.assertTrue(mock_args[3])  # is_terminal\n\n  def testStepTrainCustomObservationShapes(self):\n    custom_shapes = [(1,), (4, 4), (6, 1), (1, 6), (1, 1, 6), (6, 6, 6, 6)]\n    for shape in custom_shapes:\n      self._testCustomShapes(shape, tf.uint8, 1)\n\n  def testStepTrainCustomTypes(self):\n    custom_types = [tf.float32, tf.uint8, tf.int64]\n    for dtype in custom_types:\n      self._testCustomShapes((4, 4), dtype, 1)\n\n  def testStepTrainCustomStackSizes(self):\n    custom_stack_sizes = [1, 4, 8]\n    for stack_size in custom_stack_sizes:\n      self._testCustomShapes((4, 4), tf.uint8, stack_size)\n\n  def testLinearlyDecayingEpsilon(self):\n    """"""Test the functionality of the linearly_decaying_epsilon function.""""""\n    decay_period = 100\n    warmup_steps = 6\n    epsilon = 0.1\n    steps_schedule = [\n        (0, 1.0),  # step < warmup_steps\n        (16, 0.91),  # bonus = 0.9 * 90 / 100 = 0.81\n        (decay_period + warmup_steps + 1, epsilon)]  # step > decay+warmup\n    for step, expected_epsilon in steps_schedule:\n      self.assertNear(dqn_agent.linearly_decaying_epsilon(decay_period,\n                                                          step,\n                                                          warmup_steps,\n                                                          epsilon),\n                      expected_epsilon, 0.01)\n\n  def testBundlingWithNonexistentDirectory(self):\n    with tf.Session() as sess:\n      agent = self._create_test_agent(sess)\n      self.assertEqual(None, agent.bundle_and_checkpoint(\'/does/not/exist\', 1))\n\n  def testUnbundlingWithFailingReplayBuffer(self):\n    with tf.Session() as sess:\n      agent = self._create_test_agent(sess)\n      bundle = {}\n      # The ReplayBuffer will throw an exception since it is not able to load\n      # the expected files, which will cause the unbundle() method to return\n      # False.\n      self.assertFalse(agent.unbundle(self._test_subdir, 1729, bundle))\n\n  def testUnbundlingWithNoBundleDictionary(self):\n    with tf.Session() as sess:\n      agent = self._create_test_agent(sess)\n      agent._replay = mock.Mock()\n      self.assertFalse(agent.unbundle(self._test_subdir, 1729, None))\n\n  def testPartialUnbundling(self):\n    with tf.Session() as sess:\n      agent = self._create_test_agent(sess, allow_partial_reload=True)\n      # These values don\'t reflect the actual types of these attributes, but are\n      # used merely for facility of testing.\n      agent.state = \'state\'\n      agent.training_steps = \'training_steps\'\n      iteration_number = 1729\n      _ = agent.bundle_and_checkpoint(self._test_subdir, iteration_number)\n      # Both the ReplayBuffer and bundle_dictionary checks will fail,\n      # but this will be ignored since we\'re allowing partial reloads.\n      self.assertTrue(agent.unbundle(self._test_subdir, 1729, None))\n\n  def testBundling(self):\n    with tf.Session() as sess:\n      agent = self._create_test_agent(sess)\n      # These values don\'t reflect the actual types of these attributes, but are\n      # used merely for facility of testing.\n      agent.state = \'state\'\n      agent._replay = mock.Mock()\n      agent.training_steps = \'training_steps\'\n      iteration_number = 1729\n      bundle = agent.bundle_and_checkpoint(self._test_subdir, iteration_number)\n      keys = [\'state\', \'training_steps\']\n      for key in keys:\n        self.assertIn(key, bundle)\n        self.assertEqual(key, bundle[key])\n\n      agent.unbundle(self._test_subdir, iteration_number, bundle)\n      for key in keys:\n        self.assertEqual(key, agent.__dict__[key])\n\n  def testSyncOpWithNameScopes(self):\n    num_agents = 5\n    with tf.Session() as sess:\n      agents = []\n      for i in range(num_agents):\n        with tf.name_scope(\'agent_{}\'.format(i)):\n          agents.append(self._create_test_agent(sess))\n\n      # Checks that the agents have two ops each.\n      ops = []\n      for agent in agents:\n        self.assertLen(agent._sync_qt_ops, 2)\n        ops.extend(agent._sync_qt_ops)\n\n      # Assigns the bias of the first agent to 0.\n      ops.append(agents[0].online_convnet.layer.bias.assign(tf.zeros(4)))\n      # Runs twice to make sure zeros are propagated to online and target.\n      sess.run(ops)\n      sess.run(ops)\n\n      biases_0 = sess.run([\n          agents[0].target_convnet.layer.bias,\n          agents[0].online_convnet.layer.bias])\n      self.assertAllEqual(biases_0[0], tf.zeros(4))\n      self.assertAllEqual(biases_0[1], biases_0[0])\n\n      # Checks that other agents have not been affected.\n      biases = sess.run([agt.target_convnet.layer.bias for agt in agents[1:]])\n      for bias in biases:\n        self.assertNotAllEqual(bias, biases_0[0])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/agents/implicit_quantile/implicit_quantile_agent_test.py,12,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for implicit quantile agent.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom dopamine.agents.dqn import dqn_agent\nfrom dopamine.agents.implicit_quantile import implicit_quantile_agent\nfrom dopamine.discrete_domains import atari_lib\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\n\nclass ImplicitQuantileAgentTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(ImplicitQuantileAgentTest, self).setUp()\n    self._num_actions = 4\n    self.observation_shape = dqn_agent.NATURE_DQN_OBSERVATION_SHAPE\n    self.observation_dtype = dqn_agent.NATURE_DQN_DTYPE\n    self.stack_size = dqn_agent.NATURE_DQN_STACK_SIZE\n    self.ones_state = np.ones(\n        (1,) + self.observation_shape + (self.stack_size,))\n\n  def _create_test_agent(self, sess):\n    # This dummy network allows us to deterministically anticipate that the\n    # state-action-quantile outputs will be equal to sum of the\n    # corresponding quantile inputs.\n    # State/Quantile shapes will be k x 1, (N x batch_size) x 1,\n    # or (N\' x batch_size) x 1.\n\n    class MockImplicitQuantileNetwork(tf.keras.Model):\n      """"""Custom tf.keras.Model used in tests.""""""\n\n      def __init__(self, num_actions, quantile_embedding_dim, **kwargs):\n        # This weights_initializer gives action 0 a higher weight, ensuring\n        # that it gets picked by the argmax.\n        super(MockImplicitQuantileNetwork, self).__init__(**kwargs)\n        self.num_actions = num_actions\n        self.layer = tf.keras.layers.Dense(\n            self.num_actions, kernel_initializer=tf.ones_initializer(),\n            bias_initializer=tf.zeros_initializer())\n\n      def call(self, state, num_quantiles):\n        batch_size = state.get_shape().as_list()[0]\n        inputs = tf.constant(\n            np.ones((batch_size*num_quantiles, self.num_actions)),\n            dtype=tf.float32)\n        quantiles_shape = [num_quantiles * batch_size, 1]\n        quantiles = tf.ones(quantiles_shape)\n        return atari_lib.ImplicitQuantileNetworkType(self.layer(inputs),\n                                                     quantiles)\n\n    agent = implicit_quantile_agent.ImplicitQuantileAgent(\n        sess=sess,\n        network=MockImplicitQuantileNetwork,\n        num_actions=self._num_actions,\n        kappa=1.0,\n        num_tau_samples=2,\n        num_tau_prime_samples=3,\n        num_quantile_samples=4)\n    # This ensures non-random action choices (since epsilon_eval = 0.0) and\n    # skips the train_step.\n    agent.eval_mode = True\n    sess.run(tf.global_variables_initializer())\n    return agent\n\n  def testCreateAgentWithDefaults(self):\n    # Verifies that we can create and train an agent with the default values.\n    with self.test_session(use_gpu=False) as sess:\n      agent = implicit_quantile_agent.ImplicitQuantileAgent(sess, num_actions=4)\n      sess.run(tf.global_variables_initializer())\n      observation = np.ones([84, 84, 1])\n      agent.begin_episode(observation)\n      agent.step(reward=1, observation=observation)\n      agent.end_episode(reward=1)\n\n  def testShapes(self):\n    with self.test_session(use_gpu=False) as sess:\n      agent = self._create_test_agent(sess)\n\n      # Replay buffer batch size:\n      self.assertEqual(agent._replay.batch_size, 32)\n\n      # quantile values, q-values, q-argmax at sample action time:\n      self.assertEqual(agent._net_outputs.quantile_values.shape[0],\n                       agent.num_quantile_samples)\n      self.assertEqual(agent._net_outputs.quantile_values.shape[1],\n                       agent.num_actions)\n      self.assertEqual(agent._q_values.shape[0], agent.num_actions)\n\n      # Check the setting of num_actions.\n      self.assertEqual(self._num_actions, agent.num_actions)\n\n      # input quantiles, quantile values, and output q-values at loss\n      # computation time.\n      self.assertEqual(agent._replay_net_quantile_values.shape[0],\n                       agent.num_tau_samples * agent._replay.batch_size)\n      self.assertEqual(agent._replay_net_quantile_values.shape[1],\n                       agent.num_actions)\n\n      self.assertEqual(agent._replay_net_target_quantile_values.shape[0],\n                       agent.num_tau_prime_samples * agent._replay.batch_size)\n      self.assertEqual(agent._replay_net_target_quantile_values.shape[1],\n                       agent.num_actions)\n\n      self.assertEqual(agent._replay_net_target_q_values.shape[0],\n                       agent._replay.batch_size)\n      self.assertEqual(agent._replay_net_target_q_values.shape[1],\n                       agent.num_actions)\n\n  def test_q_value_computation(self):\n    with self.test_session(use_gpu=False) as sess:\n      agent = self._create_test_agent(sess)\n      quantiles = np.ones(agent.num_quantile_samples)\n      q_value = np.sum(quantiles)\n      quantiles = quantiles.reshape([agent.num_quantile_samples, 1])\n      state = self.ones_state\n      feed_dict = {agent.state_ph: state}\n\n      q_values, q_argmax = sess.run([agent._q_values, agent._q_argmax],\n                                    feed_dict)\n\n      q_values_arr = np.ones([agent.num_actions]) * q_value\n      self.assertAllEqual(q_values, q_values_arr)\n      self.assertEqual(q_argmax, 0)\n\n      q_values_target = sess.run(agent._replay_net_target_q_values, feed_dict)\n\n      batch_size = agent._replay.batch_size\n\n      for i in range(batch_size):\n        for j in range(agent.num_actions):\n          self.assertEqual(q_values_target[i][j], q_values[j])\n\n  def test_replay_quantile_value_computation(self):\n    with self.test_session(use_gpu=False) as sess:\n      agent = self._create_test_agent(sess)\n\n      replay_quantile_vals, replay_target_quantile_vals = sess.run(\n          [agent._replay_net_quantile_values,\n           agent._replay_net_target_quantile_values])\n\n      batch_size = agent._replay.batch_size\n      replay_quantile_vals = replay_quantile_vals.reshape([\n          agent.num_tau_samples, batch_size, agent.num_actions])\n      replay_target_quantile_vals = replay_target_quantile_vals.reshape([\n          agent.num_tau_prime_samples, batch_size, agent.num_actions])\n      for i in range(agent.num_tau_samples):\n        for j in range(agent._replay.batch_size):\n          self.assertEqual(replay_quantile_vals[i][j][0], agent.num_actions)\n\n      for i in range(agent.num_tau_prime_samples):\n        for j in range(agent._replay.batch_size):\n          self.assertEqual(replay_target_quantile_vals[i][j][0],\n                           agent.num_actions)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/dopamine/agents/rainbow/rainbow_agent_test.py,107,"b'# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for dopamine.agents.rainbow.rainbow_agent.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom dopamine.agents.dqn import dqn_agent\nfrom dopamine.agents.rainbow import rainbow_agent\nfrom dopamine.discrete_domains import atari_lib\nfrom dopamine.utils import test_utils\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\n\nclass ProjectDistributionTest(tf.test.TestCase):\n\n  def testInconsistentSupportsAndWeightsParameters(self):\n    supports = tf.constant([[0, 2, 4, 6, 8], [3, 4, 5, 6, 7]], dtype=tf.float32)\n    weights = tf.constant(\n        [[0.1, 0.2, 0.3, 0.2], [0.1, 0.2, 0.3, 0.2]], dtype=tf.float32)\n    target_support = tf.constant([4, 5, 6, 7, 8], dtype=tf.float32)\n    with self.assertRaisesRegexp(ValueError, \'are incompatible\'):\n      rainbow_agent.project_distribution(supports, weights, target_support)\n\n  def testInconsistentSupportsAndWeightsWithPlaceholders(self):\n    supports = [[0, 2, 4, 6, 8], [3, 4, 5, 6, 7]]\n    supports_ph = tf.placeholder(tf.float32, None)\n    weights = [[0.1, 0.2, 0.3, 0.2], [0.1, 0.2, 0.3, 0.2]]\n    weights_ph = tf.placeholder(tf.float32, None)\n    target_support = [4, 5, 6, 7, 8]\n    target_support_ph = tf.placeholder(tf.float32, None)\n    projection = rainbow_agent.project_distribution(\n        supports_ph, weights_ph, target_support_ph, validate_args=True)\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      with self.assertRaisesRegexp(tf.errors.InvalidArgumentError,\n                                   \'assertion failed\'):\n        sess.run(\n            projection,\n            feed_dict={\n                supports_ph: supports,\n                weights_ph: weights,\n                target_support_ph: target_support\n            })\n\n  def testInconsistentSupportsAndTargetSupportParameters(self):\n    supports = tf.constant([[0, 2, 4, 6, 8], [3, 4, 5, 6, 7]], dtype=tf.float32)\n    weights = tf.constant(\n        [[0.1, 0.2, 0.3, 0.2, 0.2], [0.1, 0.2, 0.3, 0.2, 0.2]],\n        dtype=tf.float32)\n    target_support = tf.constant([4, 5, 6], dtype=tf.float32)\n    with self.assertRaisesRegexp(ValueError, \'are incompatible\'):\n      rainbow_agent.project_distribution(supports, weights, target_support)\n\n  def testInconsistentSupportsAndTargetSupportWithPlaceholders(self):\n    supports = [[0, 2, 4, 6, 8], [3, 4, 5, 6, 7]]\n    supports_ph = tf.placeholder(tf.float32, None)\n    weights = [[0.1, 0.2, 0.3, 0.2, 0.2], [0.1, 0.2, 0.3, 0.2, 0.2]]\n    weights_ph = tf.placeholder(tf.float32, None)\n    target_support = [4, 5, 6]\n    target_support_ph = tf.placeholder(tf.float32, None)\n    projection = rainbow_agent.project_distribution(\n        supports_ph, weights_ph, target_support_ph, validate_args=True)\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      with self.assertRaisesRegexp(tf.errors.InvalidArgumentError,\n                                   \'assertion failed\'):\n        sess.run(\n            projection,\n            feed_dict={\n                supports_ph: supports,\n                weights_ph: weights,\n                target_support_ph: target_support\n            })\n\n  def testZeroDimensionalTargetSupport(self):\n    supports = tf.constant([[0, 2, 4, 6, 8], [3, 4, 5, 6, 7]], dtype=tf.float32)\n    weights = tf.constant(\n        [[0.1, 0.2, 0.3, 0.2, 0.2], [0.1, 0.2, 0.3, 0.2, 0.2]],\n        dtype=tf.float32)\n    target_support = tf.constant(3, dtype=tf.float32)\n    with self.assertRaisesRegexp(ValueError, \'Index out of range\'):\n      rainbow_agent.project_distribution(supports, weights, target_support)\n\n  def testZeroDimensionalTargetSupportWithPlaceholders(self):\n    supports = [[0, 2, 4, 6, 8], [3, 4, 5, 6, 7]]\n    supports_ph = tf.placeholder(tf.float32, None)\n    weights = [[0.1, 0.2, 0.3, 0.2, 0.2], [0.1, 0.2, 0.3, 0.2, 0.2]]\n    weights_ph = tf.placeholder(tf.float32, None)\n    target_support = 3\n    target_support_ph = tf.placeholder(tf.float32, None)\n    projection = rainbow_agent.project_distribution(\n        supports_ph, weights_ph, target_support_ph, validate_args=True)\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      with (self.assertRaises(tf.errors.InvalidArgumentError)):\n        sess.run(\n            projection,\n            feed_dict={\n                supports_ph: supports,\n                weights_ph: weights,\n                target_support_ph: target_support\n            })\n\n  def testMultiDimensionalTargetSupport(self):\n    supports = tf.constant([[0, 2, 4, 6, 8], [3, 4, 5, 6, 7]], dtype=tf.float32)\n    weights = tf.constant(\n        [[0.1, 0.2, 0.3, 0.2, 0.2], [0.1, 0.2, 0.3, 0.2, 0.2]],\n        dtype=tf.float32)\n    target_support = tf.constant([[3]], dtype=tf.float32)\n    with self.assertRaisesRegexp(ValueError, \'out of bounds\'):\n      rainbow_agent.project_distribution(supports, weights, target_support)\n\n  def testMultiDimensionalTargetSupportWithPlaceholders(self):\n    supports = [[0, 2, 4, 6, 8], [3, 4, 5, 6, 7]]\n    supports_ph = tf.placeholder(tf.float32, None)\n    weights = [[0.1, 0.2, 0.3, 0.2, 0.2], [0.1, 0.2, 0.3, 0.2, 0.2]]\n    weights_ph = tf.placeholder(tf.float32, None)\n    target_support = [[3]]\n    target_support_ph = tf.placeholder(tf.float32, None)\n    projection = rainbow_agent.project_distribution(\n        supports_ph, weights_ph, target_support_ph, validate_args=True)\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      with (self.assertRaises(tf.errors.InvalidArgumentError)):\n        sess.run(\n            projection,\n            feed_dict={\n                supports_ph: supports,\n                weights_ph: weights,\n                target_support_ph: target_support\n            })\n\n  def testProjectWithNonMonotonicTargetSupport(self):\n    supports = tf.constant([[0, 2, 4, 6, 8], [3, 4, 5, 6, 7]], dtype=tf.float32)\n    weights = tf.constant(\n        [[0.1, 0.2, 0.3, 0.2, 0.2], [0.1, 0.2, 0.3, 0.2, 0.2]],\n        dtype=tf.float32)\n    target_support = tf.constant([8, 7, 6, 5, 4], dtype=tf.float32)\n    projection = rainbow_agent.project_distribution(\n        supports, weights, target_support, validate_args=True)\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      with self.assertRaisesRegexp(tf.errors.InvalidArgumentError,\n                                   \'assertion failed\'):\n        sess.run(projection)\n\n  def testProjectNewSupportHasInconsistentDeltask(self):\n    supports = tf.constant([[0, 2, 4, 6, 8], [3, 4, 5, 6, 7]], dtype=tf.float32)\n    weights = tf.constant(\n        [[0.1, 0.2, 0.3, 0.2, 0.2], [0.1, 0.2, 0.3, 0.2, 0.2]],\n        dtype=tf.float32)\n    target_support = tf.constant([3, 4, 6, 7, 8], dtype=tf.float32)\n    projection = rainbow_agent.project_distribution(\n        supports, weights, target_support, validate_args=True)\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      with self.assertRaisesRegexp(tf.errors.InvalidArgumentError,\n                                   \'assertion failed\'):\n        sess.run(projection)\n\n  def testProjectSingleIdenticalDistribution(self):\n    supports = tf.constant([[0, 1, 2, 3, 4]], dtype=tf.float32)\n    expected_weights = [0.1, 0.2, 0.1, 0.3, 0.3]\n    weights = tf.constant([expected_weights], dtype=tf.float32)\n    target_support = tf.constant([0, 1, 2, 3, 4], dtype=tf.float32)\n    projection = rainbow_agent.project_distribution(supports, weights,\n                                                    target_support)\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      projection_ = sess.run(projection)\n      self.assertAllClose([expected_weights], projection_)\n\n  def testProjectSingleDifferentDistribution(self):\n    supports = tf.constant([[0, 1, 2, 3, 4]], dtype=tf.float32)\n    weights = tf.constant([[0.1, 0.2, 0.1, 0.3, 0.3]], dtype=tf.float32)\n    target_support = tf.constant([3, 4, 5, 6, 7], dtype=tf.float32)\n    projection = rainbow_agent.project_distribution(supports, weights,\n                                                    target_support)\n    expected_projection = [[0.7, 0.3, 0.0, 0.0, 0.0]]\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      projection_ = sess.run(projection)\n      self.assertAllClose(expected_projection, projection_)\n\n  def testProjectFromNonMonotonicSupport(self):\n    supports = tf.constant([[4, 3, 2, 1, 0]], dtype=tf.float32)\n    weights = tf.constant([[0.1, 0.2, 0.1, 0.3, 0.3]], dtype=tf.float32)\n    target_support = tf.constant([3, 4, 5, 6, 7], dtype=tf.float32)\n    projection = rainbow_agent.project_distribution(supports, weights,\n                                                    target_support)\n    expected_projection = [[0.9, 0.1, 0.0, 0.0, 0.0]]\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      projection_ = sess.run(projection)\n      self.assertAllClose(expected_projection, projection_)\n\n  def testExampleFromCodeComments(self):\n    supports = tf.constant([[0, 2, 4, 6, 8], [1, 3, 4, 5, 6]], dtype=tf.float32)\n    weights = tf.constant(\n        [[0.1, 0.6, 0.1, 0.1, 0.1], [0.1, 0.2, 0.5, 0.1, 0.1]],\n        dtype=tf.float32)\n    target_support = tf.constant([4, 5, 6, 7, 8], dtype=tf.float32)\n    projection = rainbow_agent.project_distribution(supports, weights,\n                                                    target_support)\n    expected_projections = [[0.8, 0.0, 0.1, 0.0, 0.1],\n                            [0.8, 0.1, 0.1, 0.0, 0.0]]\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      projection_ = sess.run(projection)\n      self.assertAllClose(expected_projections, projection_)\n\n  def testProjectBatchOfDifferentDistributions(self):\n    supports = tf.constant(\n        [[0, 2, 4, 6, 8], [0, 1, 2, 3, 4], [3, 4, 5, 6, 7]], dtype=tf.float32)\n    weights = tf.constant(\n        [[0.1, 0.2, 0.3, 0.2, 0.2], [0.1, 0.2, 0.1, 0.3, 0.3],\n         [0.1, 0.2, 0.3, 0.2, 0.2]],\n        dtype=tf.float32)\n    target_support = tf.constant([3, 4, 5, 6, 7], dtype=tf.float32)\n    projection = rainbow_agent.project_distribution(supports, weights,\n                                                    target_support)\n    expected_projections = [[0.3, 0.3, 0.0, 0.2,\n                             0.2], [0.7, 0.3, 0.0, 0.0, 0.0],\n                            [0.1, 0.2, 0.3, 0.2, 0.2]]\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      projection_ = sess.run(projection)\n      self.assertAllClose(expected_projections, projection_)\n\n  def testUsingPlaceholders(self):\n    supports = [[0, 2, 4, 6, 8], [0, 1, 2, 3, 4], [3, 4, 5, 6, 7]]\n    supports_ph = tf.placeholder(tf.float32, None)\n    weights = [[0.1, 0.2, 0.3, 0.2, 0.2], [0.1, 0.2, 0.1, 0.3, 0.3],\n               [0.1, 0.2, 0.3, 0.2, 0.2]]\n    weights_ph = tf.placeholder(tf.float32, None)\n    target_support = [3, 4, 5, 6, 7]\n    target_support_ph = tf.placeholder(tf.float32, None)\n    projection = rainbow_agent.project_distribution(supports_ph, weights_ph,\n                                                    target_support_ph)\n    expected_projections = [[0.3, 0.3, 0.0, 0.2,\n                             0.2], [0.7, 0.3, 0.0, 0.0, 0.0],\n                            [0.1, 0.2, 0.3, 0.2, 0.2]]\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      projection_ = sess.run(\n          projection,\n          feed_dict={\n              supports_ph: supports,\n              weights_ph: weights,\n              target_support_ph: target_support\n          })\n      self.assertAllClose(expected_projections, projection_)\n\n  def testProjectBatchOfDifferentDistributionsWithLargerDelta(self):\n    supports = tf.constant(\n        [[0, 2, 4, 6, 8], [8, 9, 10, 12, 14]], dtype=tf.float32)\n    weights = tf.constant(\n        [[0.1, 0.2, 0.2, 0.2, 0.3], [0.1, 0.2, 0.4, 0.1, 0.2]],\n        dtype=tf.float32)\n    target_support = tf.constant([0, 4, 8, 12, 16], dtype=tf.float32)\n    projection = rainbow_agent.project_distribution(supports, weights,\n                                                    target_support)\n    expected_projections = [[0.2, 0.4, 0.4, 0.0, 0.0],\n                            [0.0, 0.0, 0.45, 0.45, 0.1]]\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      projection_ = sess.run(projection)\n      self.assertAllClose(expected_projections, projection_)\n\n\nclass RainbowAgentTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(RainbowAgentTest, self).setUp()\n    self._num_actions = 4\n    self._num_atoms = 5\n    self._vmax = 7.\n    self._min_replay_history = 32\n    self._epsilon_decay_period = 90\n    self.observation_shape = dqn_agent.NATURE_DQN_OBSERVATION_SHAPE\n    self.observation_dtype = dqn_agent.NATURE_DQN_DTYPE\n    self.stack_size = dqn_agent.NATURE_DQN_STACK_SIZE\n    self.zero_state = np.zeros(\n        (1,) + self.observation_shape + (self.stack_size,))\n\n  def _create_test_agent(self, sess):\n    stack_size = self.stack_size\n    # This dummy network allows us to deterministically anticipate that\n    # action 0 will be selected by an argmax.\n\n    # In Rainbow we are dealing with a distribution over Q-values,\n    # which are represented as num_atoms bins, ranging from -vmax to vmax.\n    # The output layer will have num_actions * num_atoms elements,\n    # so each group of num_atoms weights represent the logits for a\n    # particular action. By setting 1s everywhere, except for the first\n    # num_atoms (representing the logits for the first action), which are\n    # set to np.arange(num_atoms), we are ensuring that the first action\n    # places higher weight on higher Q-values; this results in the first\n    # action being chosen.\n    class MockRainbowNetwork(tf.keras.Model):\n      """"""Custom tf.keras.Model used in tests.""""""\n\n      def __init__(self, num_actions, num_atoms, support, **kwargs):\n        super(MockRainbowNetwork, self).__init__(**kwargs)\n        self.num_actions = num_actions\n        self.num_atoms = num_atoms\n        self.support = support\n        first_row = np.tile(np.ones(self.num_atoms), self.num_actions - 1)\n        first_row = np.concatenate((np.arange(self.num_atoms), first_row))\n        bottom_rows = np.tile(\n            np.ones(self.num_actions * self.num_atoms), (stack_size - 1, 1))\n        weights_initializer = np.concatenate(([first_row], bottom_rows))\n        self.layer = tf.keras.layers.Dense(\n            self.num_actions * self.num_atoms,\n            kernel_initializer=tf.constant_initializer(weights_initializer),\n            bias_initializer=tf.ones_initializer())\n\n      def call(self, state):\n        inputs = tf.constant(\n            np.zeros((state.shape[0], stack_size)), dtype=tf.float32)\n        net = self.layer(inputs)\n        logits = tf.reshape(net, [-1, self.num_actions, self.num_atoms])\n        probabilities = tf.keras.activations.softmax(logits)\n        qs = tf.reduce_sum(self.support * probabilities, axis=2)\n        return atari_lib.RainbowNetworkType(qs, logits, probabilities)\n\n    agent = rainbow_agent.RainbowAgent(\n        sess=sess,\n        network=MockRainbowNetwork,\n        num_actions=self._num_actions,\n        num_atoms=self._num_atoms,\n        vmax=self._vmax,\n        min_replay_history=self._min_replay_history,\n        epsilon_fn=lambda w, x, y, z: 0.0,  # No exploration.\n        epsilon_eval=0.0,\n        epsilon_decay_period=self._epsilon_decay_period)\n    # This ensures non-random action choices (since epsilon_eval = 0.0) and\n    # skips the train_step.\n    agent.eval_mode = True\n    sess.run(tf.global_variables_initializer())\n    return agent\n\n  def testCreateAgentWithDefaults(self):\n    # Verifies that we can create and train an agent with the default values.\n    with tf.Session() as sess:\n      agent = rainbow_agent.RainbowAgent(sess, num_actions=4)\n      sess.run(tf.global_variables_initializer())\n      observation = np.ones([84, 84, 1])\n      agent.begin_episode(observation)\n      agent.step(reward=1, observation=observation)\n      agent.end_episode(reward=1)\n\n  def testShapesAndValues(self):\n    with tf.Session() as sess:\n      agent = self._create_test_agent(sess)\n      self.assertEqual(agent._support.shape[0], self._num_atoms)\n      self.assertEqual(\n          self.evaluate(tf.reduce_min(agent._support)), -self._vmax)\n      self.assertEqual(self.evaluate(tf.reduce_max(agent._support)), self._vmax)\n      self.assertEqual(agent._net_outputs.logits.shape,\n                       (1, self._num_actions, self._num_atoms))\n      self.assertEqual(agent._net_outputs.probabilities.shape,\n                       agent._net_outputs.logits.shape)\n      self.assertEqual(agent._replay_net_outputs.logits.shape[1],\n                       self._num_actions)\n      self.assertEqual(agent._replay_net_outputs.logits.shape[2],\n                       self._num_atoms)\n      self.assertEqual(agent._replay_next_target_net_outputs.logits.shape[1],\n                       self._num_actions)\n      self.assertEqual(agent._replay_next_target_net_outputs.logits.shape[2],\n                       self._num_atoms)\n      self.assertEqual(agent._net_outputs.q_values.shape,\n                       (1, self._num_actions))\n\n  def testBeginEpisode(self):\n    """"""Tests the functionality of agent.begin_episode.\n\n    Specifically, the action returned and its effect on the state.\n    """"""\n    with tf.Session() as sess:\n      agent = self._create_test_agent(sess)\n      # We fill up the state with 9s. On calling agent.begin_episode the state\n      # should be reset to all 0s.\n      agent.state.fill(9)\n      first_observation = np.ones(self.observation_shape + (1,))\n      self.assertEqual(agent.begin_episode(first_observation), 0)\n      # When the all-1s observation is received, it will be placed at the end of\n      # the state.\n      expected_state = self.zero_state\n      expected_state[:, :, :, -1] = np.ones((1,) + self.observation_shape)\n      self.assertAllEqual(agent.state, expected_state)\n      self.assertAllEqual(agent._observation, first_observation[:, :, 0])\n      # No training happens in eval mode.\n      self.assertEqual(agent.training_steps, 0)\n\n      # This will now cause training to happen.\n      agent.eval_mode = False\n      # Having a low replay memory add_count will prevent any of the\n      # train/prefetch/sync ops from being called.\n      agent._replay.memory.add_count = 0\n      second_observation = np.ones(self.observation_shape + (1,)) * 2\n      agent.begin_episode(second_observation)\n      # The agent\'s state will be reset, so we will only be left with the all-2s\n      # observation.\n      expected_state[:, :, :, -1] = np.full((1,) + self.observation_shape, 2)\n      self.assertAllEqual(agent.state, expected_state)\n      self.assertAllEqual(agent._observation, second_observation[:, :, 0])\n      # training_steps is incremented since we set eval_mode to False.\n      self.assertEqual(agent.training_steps, 1)\n\n  def testStepEval(self):\n    """"""Tests the functionality of agent.step() in eval mode.\n\n    Specifically, the action returned, and confirms that no training happens.\n    """"""\n    with tf.Session() as sess:\n      agent = self._create_test_agent(sess)\n      base_observation = np.ones(self.observation_shape + (1,))\n      # This will reset state and choose a first action.\n      agent.begin_episode(base_observation)\n      # We mock the replay buffer to verify how the agent interacts with it.\n      agent._replay = test_utils.MockReplayBuffer()\n      self.evaluate(tf.global_variables_initializer())\n\n      expected_state = self.zero_state\n      num_steps = 10\n      for step in range(1, num_steps + 1):\n        # We make observation a multiple of step for testing purposes (to\n        # uniquely identify each observation).\n        observation = base_observation * step\n        self.assertEqual(agent.step(reward=1, observation=observation), 0)\n        stack_pos = step - num_steps - 1\n        if stack_pos >= -self.stack_size:\n          expected_state[:, :, :, stack_pos] = np.full(\n              (1,) + self.observation_shape, step)\n      self.assertAllEqual(agent.state, expected_state)\n      self.assertAllEqual(\n          agent._last_observation,\n          np.ones(self.observation_shape) * (num_steps - 1))\n      self.assertAllEqual(agent._observation, observation[:, :, 0])\n      # No training happens in eval mode.\n      self.assertEqual(agent.training_steps, 0)\n      # No transitions are added in eval mode.\n      self.assertEqual(agent._replay.add.call_count, 0)\n\n  def testStepTrain(self):\n    """"""Test the functionality of agent.step() in train mode.\n\n    Specifically, the action returned, and confirms training is happening.\n    """"""\n    with tf.Session() as sess:\n      agent = self._create_test_agent(sess)\n      agent.eval_mode = False\n      base_observation = np.ones(self.observation_shape + (1,))\n      # We mock the replay buffer to verify how the agent interacts with it.\n      agent._replay = test_utils.MockReplayBuffer()\n      self.evaluate(tf.global_variables_initializer())\n      # This will reset state and choose a first action.\n      agent.begin_episode(base_observation)\n\n      expected_state = self.zero_state\n      num_steps = 10\n      for step in range(1, num_steps + 1):\n        # We make observation a multiple of step for testing purposes (to\n        # uniquely identify each observation).\n        observation = base_observation * step\n        self.assertEqual(agent.step(reward=1, observation=observation), 0)\n        stack_pos = step - num_steps - 1\n        if stack_pos >= -self.stack_size:\n          expected_state[:, :, :, stack_pos] = np.full(\n              (1,) + self.observation_shape, step)\n      self.assertAllEqual(agent.state, expected_state)\n      self.assertAllEqual(\n          agent._last_observation,\n          np.full(self.observation_shape, num_steps - 1))\n      self.assertAllEqual(agent._observation, observation[:, :, 0])\n      # We expect one more than num_steps because of the call to begin_episode.\n      self.assertEqual(agent.training_steps, num_steps + 1)\n      self.assertEqual(agent._replay.add.call_count, num_steps)\n\n      agent.end_episode(reward=1)\n      self.assertEqual(agent._replay.add.call_count, num_steps + 1)\n\n  def testStoreTransitionWithUniformSampling(self):\n    with tf.Session() as sess:\n      agent = rainbow_agent.RainbowAgent(\n          sess, num_actions=4, replay_scheme=\'uniform\')\n      dummy_frame = np.zeros((84, 84))\n      # Adding transitions with default, 10., default priorities.\n      agent._store_transition(dummy_frame, 0, 0, False)\n      agent._store_transition(dummy_frame, 0, 0, False, 10.)\n      agent._store_transition(dummy_frame, 0, 0, False)\n      returned_priorities = agent._replay.memory.get_priority(\n          np.arange(self.stack_size - 1, self.stack_size + 2, dtype=np.int32))\n      expected_priorities = [1., 10., 1.]\n      self.assertAllEqual(returned_priorities, expected_priorities)\n\n  def testStoreTransitionWithPrioritizedSamplingy(self):\n    with tf.Session() as sess:\n      agent = rainbow_agent.RainbowAgent(\n          sess, num_actions=4, replay_scheme=\'prioritized\')\n      dummy_frame = np.zeros((84, 84))\n      # Adding transitions with default, 10., default priorities.\n      agent._store_transition(dummy_frame, 0, 0, False)\n      agent._store_transition(dummy_frame, 0, 0, False, 10.)\n      agent._store_transition(dummy_frame, 0, 0, False)\n      returned_priorities = agent._replay.memory.get_priority(\n          np.arange(self.stack_size - 1, self.stack_size + 2, dtype=np.int32))\n      expected_priorities = [1., 10., 10.]\n      self.assertAllEqual(returned_priorities, expected_priorities)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
