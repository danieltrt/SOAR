file_path,api_count,code
setup.py,0,"b'""""""Edward2.\n\nEdward2 is a probabilistic programming language in Python. It extends the NumPy\nor TensorFlow ecosystem so that one can declare models as probabilistic programs\nand manipulate a model\'s computation for flexible training, latent variable\ninference, and prediction.\n\nSee more details in the [`README.md`](https://github.com/google/edward2).\n""""""\n\nimport os\nimport sys\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n# To enable importing version.py directly, we add its path to sys.path.\nversion_path = os.path.join(os.path.dirname(__file__), \'edward2\')\nsys.path.append(version_path)\nfrom version import __version__  # pylint: disable=g-import-not-at-top\n\nsetup(\n    name=\'edward2\',\n    version=\'0.0.1\',\n    description=\'Edward2\',\n    author=\'Edward2 Team\',\n    author_email=\'trandustin@google.com\',\n    url=\'http://github.com/google/edward2\',\n    license=\'Apache 2.0\',\n    packages=find_packages(),\n    install_requires=[],\n    extras_require={\n        \'numpy\': [\'numpy>=1.7\',\n                  \'scipy>=1.0.0\'],\n        \'tensorflow\': [\'tensorflow>=2.0.0a0\',\n                       \'tensorflow-probability>=0.8.0\'],\n        \'tf-nightly\': [\'tf-nightly\',\n                       \'tfp-nightly\'],\n        \'tests\': [\n            \'absl-py>=0.5.0\',\n            \'matplotlib>=2.0.0\',\n            \'pylint>=1.9.0\',\n            \'tensorflow-datasets>=1.3.0\',\n        ],\n    },\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n    ],\n    keywords=\'probabilistic programming tensorflow machine learning\',\n)\n'"
edward2/__init__.py,0,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Edward2 probabilistic programming language.\n\nFor user guides, see:\n\n+ [Overview](\n   https://github.com/google/edward2/blob/master/README.md)\n+ [Upgrading from Edward to Edward2](\n   https://github.com/google/edward2/blob/master/Upgrading_From_Edward_To_Edward2.md)\n\n""""""\n\nimport warnings\nfrom edward2 import numpy\nfrom edward2 import tensorflow\nfrom edward2.tensorflow import *  # pylint: disable=wildcard-import\n\n_allowed_symbols = [\n    ""numpy"",\n    ""tensorflow"",\n]\n# By default, `import edward2 as ed` uses the TensorFlow backend\'s namespace.\nfor name in dir(tensorflow):\n  _allowed_symbols.append(name)\n\ntry:\n  from tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\nexcept ImportError:\n  __all__ = _allowed_symbols\n  try:\n    import numpy as np  # pylint: disable=g-import-not-at-top,unused-import\n  except ImportError:\n    warnings.warn(""Neither NumPy nor TensorFlow backends are available for ""\n                  ""Edward2."")\nelse:\n  remove_undocumented(__name__, _allowed_symbols)\n'"
edward2/trace.py,0,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tracing mechanism for controlling the execution of programs.""""""\n\nimport contextlib\nimport functools\nimport threading\n\n\nclass _TracerStack(threading.local):\n  """"""A thread-local stack of tracers.""""""\n\n  def __init__(self):\n    super(_TracerStack, self).__init__()\n    self.stack = [lambda f, *args, **kwargs: f(*args, **kwargs)]\n\n\n_tracer_stack = _TracerStack()\n\n\n@contextlib.contextmanager\ndef trace(tracer):\n  """"""Python context manager for tracing.\n\n  Upon entry, a trace context manager pushes an tracer onto a\n  thread-local stack. Upon exiting, it pops the tracer from the stack.\n\n  Args:\n    tracer: Function which takes a callable `f` and inputs `*args`, `**kwargs`.\n\n  Yields:\n    None.\n\n  #### Examples\n\n  Tracing controls the execution of Edward programs. Below we illustrate\n  how to set the value of a specific random variable within a program.\n\n  ```python\n  import edward2 as ed\n\n  def model():\n    return ed.Poisson(rate=1.5, name=""y"")\n\n  def tracer(f, *args, **kwargs):\n    if kwargs.get(""name"") == ""y"":\n      kwargs[""value""] = 42\n    return ed.traceable(f)(*args, **kwargs)\n\n  with ed.trace(tracer):\n    y = model()\n\n  assert y == 42\n  ```\n\n  Wrapping `f` as `traceable` allows tracers down the stack to\n  additionally modify this operation. Since the operation `f()` is not wrapped\n  by default, we could have called it directly. Refer also to the example in\n  `get_next_tracer()` for more details on nested tracers.\n  """"""\n  try:\n    _tracer_stack.stack.append(tracer)\n    yield\n  finally:\n    _tracer_stack.stack.pop()\n\n\n@contextlib.contextmanager\ndef get_next_tracer():\n  """"""Yields the top-most tracer on the thread-local trace stack.\n\n  Operations may be traced by multiple nested tracers. Once reached,\n  an operation can be forwarded through nested tracers until resolved.\n  To allow for nesting, implement tracers by re-wrapping their first\n  argument (`f`) as an `traceable`. To avoid nesting, manipulate the\n  computation without using `traceable`.\n\n  This function allows for nesting by manipulating the thread-local tracer\n  stack, so that operations are traced in the order of tracer nesting.\n\n  #### Examples\n\n  ```python\n  import edward2 as ed\n\n  def model():\n    x = ed.Normal(loc=0., scale=1., name=""x"")\n    y = ed.Normal(loc=x, scale=1., name=""y"")\n    return x + y\n\n  def double(f, *args, **kwargs):\n    return 2. * ed.traceable(f)(*args, **kwargs)\n\n  def set_y(f, *args, **kwargs):\n    if kwargs.get(""name"") == ""y"":\n      kwargs[""value""] = 0.42\n    return ed.traceable(f)(*args, **kwargs)\n\n  with ed.trace(double):\n    with ed.trace(set_y):\n      z = model()\n  ```\n\n  This will firstly put `double` on the stack, and then `set_y`,\n  resulting in the stack:\n  (TOP) set_y -> double -> apply (BOTTOM)\n\n  The execution of `model` is then (top lines are current stack state):\n  1) (TOP) set_y -> double -> apply (BOTTOM);\n  `ed.Normal(0., 1., ""x"")` is traced by `set_y`, and as the name is not ""y""\n  the operation is simply forwarded to the next tracer on the stack.\n\n  2) (TOP) double -> apply (BOTTOM);\n  `ed.Normal(0., 1., ""x"")` is traced by `double`, to produce\n  `2*ed.Normal(0., 1., ""x"")`, with the operation being forwarded down the stack.\n\n  3) (TOP) apply (BOTTOM);\n  `ed.Normal(0., 1., ""x"")` is traced by `apply`, which simply calls the\n  constructor.\n\n  (At this point, the nested calls to `get_next_tracer()`, produced by\n  forwarding operations, exit, and the current stack is again:\n  (TOP) set_y -> double -> apply (BOTTOM))\n\n  4) (TOP) set_y -> double -> apply (BOTTOM);\n  `ed.Normal(0., 1., ""y"")` is traced by `set_y`,\n  the value of `y` is set to 0.42 and the operation is forwarded down the stack.\n\n  5) (TOP) double -> apply (BOTTOM);\n  `ed.Normal(0., 1., ""y"")` is traced by `double`, to produce\n  `2*ed.Normal(0., 1., ""y"")`, with the operation being forwarded down the stack.\n\n  6) (TOP) apply (BOTTOM);\n  `ed.Normal(0., 1., ""y"")` is traced by `apply`, which simply calls the\n  constructor.\n\n  The final values for `x` and `y` inside of `model()` are tensors where `x` is\n  a random draw from Normal(0., 1.) doubled, and `y` is a constant 0.84, thus\n  z = 2 * Normal(0., 1.) + 0.84.\n  """"""\n  try:\n    tracer = _tracer_stack.stack.pop()\n    yield tracer\n  finally:\n    _tracer_stack.stack.append(tracer)\n\n\ndef traceable(func):\n  """"""Decorator that wraps `func` so that its execution is traced.\n\n  The wrapper passes `func` to the tracer for the current thread.\n\n  If there is no next tracer, we perform an ""immediate"" call to `func`.\n  That is, `func` terminates without forwarding its execution to another\n  tracer.\n\n  Args:\n    func: Function to wrap.\n\n  Returns:\n    The decorated function.\n  """"""\n  @functools.wraps(func)\n  def func_wrapped(*args, **kwargs):\n    with get_next_tracer() as tracer:\n      return tracer(func, *args, **kwargs)\n\n  return func_wrapped\n'"
edward2/trace_test.py,2,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tracing.""""""\n\nfrom absl.testing import parameterized\nimport edward2 as ed\nimport tensorflow as tf\n\n\nclass TraceTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.parameters(\n      {""cls"": ed.Normal, ""value"": 2., ""kwargs"": {""loc"": 0.5, ""scale"": 1.}},\n      {""cls"": ed.Bernoulli, ""value"": 1, ""kwargs"": {""logits"": 0.}},\n  )\n  def testTrace(self, cls, value, kwargs):\n    def tracer(f, *fargs, **fkwargs):\n      name = fkwargs.get(""name"", None)\n      if name == ""rv2"":\n        fkwargs[""value""] = value\n      return f(*fargs, **fkwargs)\n    rv1 = cls(value=value, name=""rv1"", **kwargs)\n    with ed.trace(tracer):\n      rv2 = cls(name=""rv2"", **kwargs)\n    self.assertEqual(rv1, value)\n    self.assertEqual(rv2, value)\n\n  def testTrivialTracerPreservesLogJoint(self):\n    def trivial_tracer(fn, *args, **kwargs):\n      # A tracer that does nothing.\n      return ed.traceable(fn)(*args, **kwargs)\n\n    def model():\n      return ed.Normal(0., 1., name=""x"")\n\n    def transformed_model():\n      with ed.trace(trivial_tracer):\n        model()\n\n    log_joint = ed.make_log_joint_fn(model)\n    log_joint_transformed = ed.make_log_joint_fn(transformed_model)\n    self.assertEqual(log_joint(x=5.), log_joint_transformed(x=5.))\n\n  def testTraceForwarding(self):\n    def double(f, *args, **kwargs):\n      return 2. * ed.traceable(f)(*args, **kwargs)\n\n    def set_xy(f, *args, **kwargs):\n      if kwargs.get(""name"") == ""x"":\n        kwargs[""value""] = 1.\n      if kwargs.get(""name"") == ""y"":\n        kwargs[""value""] = 0.42\n      return ed.traceable(f)(*args, **kwargs)\n\n    def model():\n      x = ed.Normal(loc=0., scale=1., name=""x"")\n      y = ed.Normal(loc=x, scale=1., name=""y"")\n      return x + y\n\n    with ed.trace(set_xy):\n      with ed.trace(double):\n        z = model()\n\n    value = 2. * 1. + 2. * 0.42\n    self.assertAlmostEqual(z, value, places=5)\n\n  def testTraceNonForwarding(self):\n    def double(f, *args, **kwargs):\n      self.assertEqual(""yes"", ""no"")\n      return 2. * f(*args, **kwargs)\n\n    def set_xy(f, *args, **kwargs):\n      if kwargs.get(""name"") == ""x"":\n        kwargs[""value""] = 1.\n      if kwargs.get(""name"") == ""y"":\n        kwargs[""value""] = 0.42\n      return f(*args, **kwargs)\n\n    def model():\n      x = ed.Normal(loc=0., scale=1., name=""x"")\n      y = ed.Normal(loc=x, scale=1., name=""y"")\n      return x + y\n\n    with ed.trace(double):\n      with ed.trace(set_xy):\n        z = model()\n\n    value = 1. + 0.42\n    self.assertAlmostEqual(z, value, places=5)\n\n  def testTraceException(self):\n    def f():\n      raise NotImplementedError()\n    def tracer(f, *fargs, **fkwargs):\n      return f(*fargs, **fkwargs)\n\n    with ed.get_next_tracer() as top_tracer:\n      old_tracer = top_tracer\n\n    with self.assertRaises(NotImplementedError):\n      with ed.trace(tracer):\n        f()\n\n    with ed.get_next_tracer() as top_tracer:\n      new_tracer = top_tracer\n\n    self.assertEqual(old_tracer, new_tracer)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
edward2/tracers.py,5,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tracing operations.\n\nThis file collects common tracing operations, i.e., traces that each control the\nexecution of programs in a specific manner. For example, \'condition\' traces a\nprogram and fixes the value of random variables; and \'tape\' traces the program\nand records the executed random variables onto an ordered dictionary.\n""""""\n\nimport collections\nimport contextlib\nfrom edward2.trace import trace\nfrom edward2.trace import traceable\n\n\n@contextlib.contextmanager\ndef condition(**model_kwargs):\n  """"""Context manager for setting the values of random variables.\n\n  Args:\n    **model_kwargs: dict of str to Tensor. Keys are the names of random variable\n      in the model. Values are Tensors to set their corresponding value to.\n\n  Yields:\n    None.\n\n  #### Examples\n\n  `condition` is typically used for binding observations to random variables\n  in the model, or equivalently binding posterior samples to random variables\n  in the model. This lets one compute likelihoods or prior densities.\n\n  ```python\n  import edward2 as ed\n\n  def probabilistic_matrix_factorization():\n    users = ed.Normal(0., 1., sample_shape=[5000, 128], name=""users"")\n    items = ed.Normal(0., 1., sample_shape=[7500, 128], name=""items"")\n    ratings = ed.Normal(loc=tf.matmul(users, items, transpose_b=True),\n                        scale=0.1,\n                        name=""ratings"")\n    return ratings\n\n  users = tf.zeros([5000, 128])\n  items = tf.zeros([7500, 128])\n  with ed.condition(users=users, items=items):\n    ratings = probabilistic_matrix_factorization()\n\n  # Compute the likelihood given latent user preferences and item attributes set\n  # to zero matrices, p(data | users=0, items=0).\n  ratings.distribution.log_prob(data)\n  ```\n  """"""\n  def _condition(f, *args, **kwargs):\n    """"""Sets random variable values to its aligned value.""""""\n    name = kwargs.get(""name"")\n    if name in model_kwargs:\n      kwargs[""value""] = model_kwargs[name]\n    return traceable(f)(*args, **kwargs)\n\n  with trace(_condition):\n    yield\n\n\n@contextlib.contextmanager\ndef tape():\n  """"""Context manager for recording traceable executions onto a tape.\n\n  Similar to `tf.GradientTape`, operations are recorded if they are executed\n  within this context manager. In addition, the operation must be registered\n  (decorated) as `ed.traceable`.\n\n  Yields:\n    tape: OrderedDict where operations are recorded in sequence. Keys are\n      the `name` keyword argument to the operation (typically, a random\n      variable\'s `name`) and values are the corresponding output of the\n      operation. If the operation has no name, it is not recorded.\n\n  #### Examples\n\n  ```python\n  import edward2 as ed\n\n  def probabilistic_matrix_factorization():\n    users = ed.Normal(0., 1., sample_shape=[5000, 128], name=""users"")\n    items = ed.Normal(0., 1., sample_shape=[7500, 128], name=""items"")\n    ratings = ed.Normal(loc=tf.matmul(users, items, transpose_b=True),\n                        scale=0.1,\n                        name=""ratings"")\n    return ratings\n\n  with ed.tape() as model_tape:\n    ratings = probabilistic_matrix_factorization()\n\n  assert model_tape[""users""].shape == (5000, 128)\n  assert model_tape[""items""].shape == (7500, 128)\n  assert model_tape[""ratings""] == ratings\n  ```\n\n  """"""\n  tape_data = collections.OrderedDict({})\n\n  def record(f, *args, **kwargs):\n    """"""Records execution to a tape.""""""\n    name = kwargs.get(""name"")\n    output = traceable(f)(*args, **kwargs)\n    if name:\n      tape_data[name] = output\n    return output\n\n  with trace(record):\n    yield tape_data\n'"
edward2/tracers_test.py,4,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tracers.""""""\n\nimport edward2 as ed\nimport tensorflow as tf\n\n\nclass TracersTest(tf.test.TestCase):\n\n  def testCondition(self):\n    tf.random.set_seed(358758)\n    def model():\n      x = ed.Normal(loc=-5., scale=1e-8, name=""x"")\n      y = ed.Normal(loc=x, scale=1e-8, name=""y"")\n      return x, y\n\n    with ed.condition(x=5.):\n      x, y = model()\n\n    self.assertEqual(x, 5.)\n    self.assertAllClose(tf.convert_to_tensor(y), 5., atol=1e-3)\n\n  def testTape(self):\n    def model():\n      x = ed.Normal(loc=0., scale=1., name=""x"")\n      y = ed.Normal(loc=x, scale=1., name=""y"")\n      return x + y\n\n    with ed.tape() as model_tape:\n      output = model()\n\n    self.assertEqual(list(model_tape.keys()), [""x"", ""y""])\n    expected_value = model_tape[""x""] + model_tape[""y""]\n    actual_value = output\n    self.assertEqual(expected_value, actual_value)\n\n  def testTapeNoName(self):\n    def model():\n      x = ed.Normal(loc=0., scale=1., name=""x"")\n      y = ed.Normal(loc=x, scale=1.)\n      return x + y\n\n    with ed.tape() as model_tape:\n      _ = model()\n\n    self.assertEqual(list(model_tape.keys()), [""x""])\n\n  def testTapeOuterForwarding(self):\n    def double(f, *args, **kwargs):\n      return 2. * ed.traceable(f)(*args, **kwargs)\n\n    def model():\n      x = ed.Normal(loc=0., scale=1., name=""x"")\n      y = ed.Normal(loc=x, scale=1., name=""y"")\n      return x + y\n\n    with ed.tape() as model_tape:\n      with ed.trace(double):\n        output = model()\n\n    self.assertEqual(list(model_tape.keys()), [""x"", ""y""])\n    expected_value = 2. * model_tape[""x""] + 2. * model_tape[""y""]\n    actual_value = output\n    self.assertEqual(expected_value, actual_value)\n\n  def testTapeInnerForwarding(self):\n    def double(f, *args, **kwargs):\n      return 2. * ed.traceable(f)(*args, **kwargs)\n\n    def model():\n      x = ed.Normal(loc=0., scale=1., name=""x"")\n      y = ed.Normal(loc=x, scale=1., name=""y"")\n      return x + y\n\n    with ed.trace(double):\n      with ed.tape() as model_tape:\n        output = model()\n\n    self.assertEqual(list(model_tape.keys()), [""x"", ""y""])\n    expected_value = model_tape[""x""] + model_tape[""y""]\n    actual_value = output\n    self.assertEqual(expected_value, actual_value)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
edward2/version.py,0,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Package version.""""""\n\n__version__ = \'0.0.1\'\nVERSION = __version__\n'"
baselines/cifar/batchensemble.py,40,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""BatchEnsemble Wide ResNet 28-10 on CIFAR-10 and CIFAR-100.""""""\n\nimport functools\nimport os\nimport time\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport batchensemble_model  # local file import\nimport utils  # local file import\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nflags.DEFINE_integer(\'ensemble_size\', 4, \'Size of ensemble.\')\nflags.DEFINE_integer(\'per_core_batch_size\', 64,\n                     \'Batch size per TPU core/GPU. The number of new \'\n                     \'datapoints gathered per batch is this number divided by \'\n                     \'ensemble_size (we tile the batch by that # of times).\')\nflags.DEFINE_float(\'random_sign_init\', -0.5,\n                   \'Use random sign init for fast weights.\')\nflags.DEFINE_integer(\'seed\', 0, \'Random seed.\')\nflags.DEFINE_float(\'fast_weight_lr_multiplier\', 1.0,\n                   \'fast weights lr multiplier.\')\nflags.DEFINE_float(\'train_proportion\', default=1.0,\n                   help=\'only use a proportion of training set.\')\nflags.DEFINE_float(\'base_learning_rate\', 0.1,\n                   \'Base learning rate when total training batch size is 128.\')\nflags.DEFINE_integer(\'lr_warmup_epochs\', 1,\n                     \'Number of epochs for a linear warmup to the initial \'\n                     \'learning rate. Use 0 to do no warmup.\')\nflags.DEFINE_float(\'lr_decay_ratio\', 0.2, \'Amount to decay learning rate.\')\nflags.DEFINE_list(\'lr_decay_epochs\', [\'80\', \'160\', \'180\'],\n                  \'Epochs to decay learning rate by.\')\nflags.DEFINE_float(\'l2\', 3e-4, \'L2 coefficient.\')\nflags.DEFINE_enum(\'dataset\', \'cifar10\',\n                  enum_values=[\'cifar10\', \'cifar100\'],\n                  help=\'Dataset.\')\n# TODO(ghassen): consider adding CIFAR-100-C to TFDS.\nflags.DEFINE_string(\'cifar100_c_path\', None,\n                    \'Path to the TFRecords files for CIFAR-100-C. Only valid \'\n                    \'(and required) if dataset is cifar100 and corruptions.\')\nflags.DEFINE_integer(\'corruptions_interval\', 250,\n                     \'Number of epochs between evaluating on the corrupted \'\n                     \'test data. Use -1 to never evaluate.\')\nflags.DEFINE_integer(\'checkpoint_interval\', 25,\n                     \'Number of epochs between saving checkpoints. Use -1 to \'\n                     \'never save checkpoints.\')\nflags.DEFINE_integer(\'num_bins\', 15, \'Number of bins for ECE.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/cifar\',\n                    \'The directory where the model weights and \'\n                    \'training/evaluation summaries are stored.\')\nflags.DEFINE_integer(\'train_epochs\', 250, \'Number of training epochs.\')\n\n# Accelerator flags.\nflags.DEFINE_bool(\'use_gpu\', False, \'Whether to run on GPU or otherwise TPU.\')\nflags.DEFINE_bool(\'use_bfloat16\', False, \'Whether to use mixed precision.\')\nflags.DEFINE_integer(\'num_cores\', 8, \'Number of TPU cores or number of GPUs.\')\nflags.DEFINE_string(\'tpu\', None,\n                    \'Name of the TPU. Only used if use_gpu is False.\')\nFLAGS = flags.FLAGS\n\n\ndef main(argv):\n  del argv  # unused arg\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  logging.info(\'Saving checkpoints at %s\', FLAGS.output_dir)\n  tf.random.set_seed(FLAGS.seed)\n\n  ds_info = tfds.builder(FLAGS.dataset).info\n  per_core_batch_size = FLAGS.per_core_batch_size // FLAGS.ensemble_size\n  batch_size = per_core_batch_size * FLAGS.num_cores\n  # Train_proportion is a float so need to convert steps_per_epoch to int.\n  steps_per_epoch = int((ds_info.splits[\'train\'].num_examples *\n                         FLAGS.train_proportion) // batch_size)\n  steps_per_eval = ds_info.splits[\'test\'].num_examples // batch_size\n  num_classes = ds_info.features[\'label\'].num_classes\n\n  if FLAGS.use_gpu:\n    logging.info(\'Use GPU\')\n    strategy = tf.distribute.MirroredStrategy()\n  else:\n    logging.info(\'Use TPU at %s\',\n                 FLAGS.tpu if FLAGS.tpu is not None else \'local\')\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n  train_input_fn = utils.load_input_fn(\n      split=tfds.Split.TRAIN,\n      name=FLAGS.dataset,\n      batch_size=per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16,\n      proportion=FLAGS.train_proportion)\n  clean_test_input_fn = utils.load_input_fn(\n      split=tfds.Split.TEST,\n      name=FLAGS.dataset,\n      batch_size=per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  train_dataset = strategy.experimental_distribute_datasets_from_function(\n      train_input_fn)\n  test_datasets = {\n      \'clean\': strategy.experimental_distribute_datasets_from_function(\n          clean_test_input_fn),\n  }\n  if FLAGS.corruptions_interval > 0:\n    if FLAGS.dataset == \'cifar10\':\n      load_c_input_fn = utils.load_cifar10_c_input_fn\n    else:\n      load_c_input_fn = functools.partial(utils.load_cifar100_c_input_fn,\n                                          path=FLAGS.cifar100_c_path)\n    corruption_types, max_intensity = utils.load_corrupted_test_info(\n        FLAGS.dataset)\n    for corruption in corruption_types:\n      for intensity in range(1, max_intensity + 1):\n        input_fn = load_c_input_fn(\n            corruption_name=corruption,\n            corruption_intensity=intensity,\n            batch_size=per_core_batch_size,\n            use_bfloat16=FLAGS.use_bfloat16)\n        test_datasets[\'{0}_{1}\'.format(corruption, intensity)] = (\n            strategy.experimental_distribute_datasets_from_function(input_fn))\n\n  if FLAGS.use_bfloat16:\n    policy = tf.keras.mixed_precision.experimental.Policy(\'mixed_bfloat16\')\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n\n  summary_writer = tf.summary.create_file_writer(\n      os.path.join(FLAGS.output_dir, \'summaries\'))\n\n  with strategy.scope():\n    logging.info(\'Building Keras model\')\n    model = batchensemble_model.wide_resnet(\n        input_shape=ds_info.features[\'image\'].shape,\n        depth=28,\n        width_multiplier=10,\n        num_classes=num_classes,\n        ensemble_size=FLAGS.ensemble_size,\n        random_sign_init=FLAGS.random_sign_init,\n        l2=FLAGS.l2)\n    logging.info(\'Model input shape: %s\', model.input_shape)\n    logging.info(\'Model output shape: %s\', model.output_shape)\n    logging.info(\'Model number of weights: %s\', model.count_params())\n    # Linearly scale learning rate and the decay epochs by vanilla settings.\n    base_lr = FLAGS.base_learning_rate * batch_size / 128\n    lr_decay_epochs = [(int(start_epoch_str) * FLAGS.train_epochs) // 200\n                       for start_epoch_str in FLAGS.lr_decay_epochs]\n    lr_schedule = utils.LearningRateSchedule(\n        steps_per_epoch,\n        base_lr,\n        decay_ratio=FLAGS.lr_decay_ratio,\n        decay_epochs=lr_decay_epochs,\n        warmup_epochs=FLAGS.lr_warmup_epochs)\n    optimizer = tf.keras.optimizers.SGD(lr_schedule,\n                                        momentum=0.9,\n                                        nesterov=True)\n    metrics = {\n        \'train/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'train/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'train/loss\': tf.keras.metrics.Mean(),\n        \'train/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'test/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'test/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n    }\n    for i in range(FLAGS.ensemble_size):\n      metrics[\'test/nll_member_{}\'.format(i)] = tf.keras.metrics.Mean()\n      metrics[\'test/accuracy_member_{}\'.format(i)] = (\n          tf.keras.metrics.SparseCategoricalAccuracy())\n    if FLAGS.corruptions_interval > 0:\n      corrupt_metrics = {}\n      for intensity in range(1, max_intensity + 1):\n        for corruption in corruption_types:\n          dataset_name = \'{0}_{1}\'.format(corruption, intensity)\n          corrupt_metrics[\'test/nll_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.Mean())\n          corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.SparseCategoricalAccuracy())\n          corrupt_metrics[\'test/ece_{}\'.format(dataset_name)] = (\n              ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins))\n\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n    initial_epoch = 0\n    if latest_checkpoint:\n      # checkpoint.restore must be within a strategy.scope() so that optimizer\n      # slot variables are mirrored.\n      checkpoint.restore(latest_checkpoint)\n      logging.info(\'Loaded checkpoint %s\', latest_checkpoint)\n      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n\n  @tf.function\n  def train_step(iterator):\n    """"""Training StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      images = tf.tile(images, [FLAGS.ensemble_size, 1, 1, 1])\n      labels = tf.tile(labels, [FLAGS.ensemble_size])\n\n      with tf.GradientTape() as tape:\n        logits = model(images, training=True)\n        if FLAGS.use_bfloat16:\n          logits = tf.cast(logits, tf.float32)\n        negative_log_likelihood = tf.reduce_mean(\n            tf.keras.losses.sparse_categorical_crossentropy(labels,\n                                                            logits,\n                                                            from_logits=True))\n        l2_loss = sum(model.losses)\n        loss = negative_log_likelihood + l2_loss\n        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n        scaled_loss = loss / strategy.num_replicas_in_sync\n\n      grads = tape.gradient(scaled_loss, model.trainable_variables)\n\n      # Separate learning rate implementation.\n      if FLAGS.fast_weight_lr_multiplier != 1.0:\n        grads_and_vars = []\n        for grad, var in zip(grads, model.trainable_variables):\n          # Apply different learning rate on the fast weight approximate\n          # posterior/prior parameters. This is excludes BN and slow weights,\n          # but pay caution to the naming scheme.\n          if (\'batch_norm\' not in var.name and \'kernel\' not in var.name):\n            grads_and_vars.append((grad * FLAGS.fast_weight_lr_multiplier, var))\n          else:\n            grads_and_vars.append((grad, var))\n        optimizer.apply_gradients(grads_and_vars)\n      else:\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n      probs = tf.nn.softmax(logits)\n      metrics[\'train/ece\'].update_state(labels, probs)\n      metrics[\'train/loss\'].update_state(loss)\n      metrics[\'train/negative_log_likelihood\'].update_state(\n          negative_log_likelihood)\n      metrics[\'train/accuracy\'].update_state(labels, logits)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  @tf.function\n  def test_step(iterator, dataset_name):\n    """"""Evaluation StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      images = tf.tile(images, [FLAGS.ensemble_size, 1, 1, 1])\n      logits = model(images, training=False)\n      if FLAGS.use_bfloat16:\n        logits = tf.cast(logits, tf.float32)\n      probs = tf.nn.softmax(logits)\n      per_probs = tf.split(probs,\n                           num_or_size_splits=FLAGS.ensemble_size,\n                           axis=0)\n      for i in range(FLAGS.ensemble_size):\n        member_probs = per_probs[i]\n        member_loss = tf.keras.losses.sparse_categorical_crossentropy(\n            labels, member_probs)\n        metrics[\'test/nll_member_{}\'.format(i)].update_state(member_loss)\n        metrics[\'test/accuracy_member_{}\'.format(i)].update_state(labels,\n                                                                  member_probs)\n\n      probs = tf.reduce_mean(per_probs, axis=0)\n      negative_log_likelihood = tf.reduce_mean(\n          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n      if dataset_name == \'clean\':\n        metrics[\'test/negative_log_likelihood\'].update_state(\n            negative_log_likelihood)\n        metrics[\'test/accuracy\'].update_state(labels, probs)\n        metrics[\'test/ece\'].update_state(labels, probs)\n      else:\n        corrupt_metrics[\'test/nll_{}\'.format(dataset_name)].update_state(\n            negative_log_likelihood)\n        corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n        corrupt_metrics[\'test/ece_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  train_iterator = iter(train_dataset)\n  start_time = time.time()\n  for epoch in range(initial_epoch, FLAGS.train_epochs):\n    logging.info(\'Starting to run epoch: %s\', epoch)\n    for step in range(steps_per_epoch):\n      train_step(train_iterator)\n\n      current_step = epoch * steps_per_epoch + (step + 1)\n      max_steps = steps_per_epoch * FLAGS.train_epochs\n      time_elapsed = time.time() - start_time\n      steps_per_sec = float(current_step) / time_elapsed\n      eta_seconds = (max_steps - current_step) / steps_per_sec\n      message = (\'{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. \'\n                 \'ETA: {:.0f} min. Time elapsed: {:.0f} min\'.format(\n                     current_step / max_steps,\n                     epoch + 1,\n                     FLAGS.train_epochs,\n                     steps_per_sec,\n                     eta_seconds / 60,\n                     time_elapsed / 60))\n      if step % 20 == 0:\n        logging.info(message)\n\n    datasets_to_evaluate = {\'clean\': test_datasets[\'clean\']}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      datasets_to_evaluate = test_datasets\n    for dataset_name, test_dataset in datasets_to_evaluate.items():\n      test_iterator = iter(test_dataset)\n      logging.info(\'Testing on dataset %s\', dataset_name)\n      for step in range(steps_per_eval):\n        if step % 20 == 0:\n          logging.info(\'Starting to run eval step %s of epoch: %s\', step,\n                       epoch)\n        test_step(test_iterator, dataset_name)\n      logging.info(\'Done with testing on %s\', dataset_name)\n\n    corrupt_results = {}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      corrupt_results = utils.aggregate_corrupt_metrics(corrupt_metrics,\n                                                        corruption_types,\n                                                        max_intensity)\n\n    logging.info(\'Train Loss: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'train/loss\'].result(),\n                 metrics[\'train/accuracy\'].result() * 100)\n    logging.info(\'Test NLL: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'test/negative_log_likelihood\'].result(),\n                 metrics[\'test/accuracy\'].result() * 100)\n    for i in range(FLAGS.ensemble_size):\n      logging.info(\'Member %d Test Loss: %.4f, Accuracy: %.2f%%\',\n                   i, metrics[\'test/nll_member_{}\'.format(i)].result(),\n                   metrics[\'test/accuracy_member_{}\'.format(i)].result() * 100)\n    total_results = {name: metric.result() for name, metric in metrics.items()}\n    total_results.update(corrupt_results)\n    with summary_writer.as_default():\n      for name, result in total_results.items():\n        tf.summary.scalar(name, result, step=epoch + 1)\n\n    for metric in metrics.values():\n      metric.reset_states()\n\n    if (FLAGS.checkpoint_interval > 0 and\n        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n      checkpoint_name = checkpoint.save(\n          os.path.join(FLAGS.output_dir, \'checkpoint\'))\n      logging.info(\'Saved checkpoint to %s\', checkpoint_name)\n\n  final_checkpoint_name = checkpoint.save(\n      os.path.join(FLAGS.output_dir, \'checkpoint\'))\n  logging.info(\'Saved last checkpoint to %s\', final_checkpoint_name)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/cifar/batchensemble_model.py,26,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""BatchEnsemble for a Wide ResNet architecture.""""""\n\nimport functools\nimport edward2 as ed\nimport tensorflow as tf\n\n\nBatchNormalization = functools.partial(  # pylint: disable=invalid-name\n    tf.keras.layers.BatchNormalization,\n    epsilon=1e-5,  # using epsilon and momentum defaults from Torch\n    momentum=0.9)\nConv2DBatchEnsemble = functools.partial(  # pylint: disable=invalid-name\n    ed.layers.Conv2DBatchEnsemble,\n    kernel_size=3,\n    padding=\'same\',\n    use_bias=False,\n    kernel_initializer=\'he_normal\')\n\n\ndef make_sign_initializer(random_sign_init):\n  if random_sign_init > 0:\n    return ed.initializers.RandomSign(random_sign_init)\n  else:\n    return tf.keras.initializers.RandomNormal(mean=1.0,\n                                              stddev=-random_sign_init)\n\n\ndef basic_block(inputs, filters, strides, ensemble_size, random_sign_init, l2):\n  """"""Basic residual block of two 3x3 convs.\n\n  Args:\n    inputs: tf.Tensor.\n    filters: Number of filters for Conv2D.\n    strides: Stride dimensions for Conv2D.\n    ensemble_size: Number of ensemble members.\n    random_sign_init: Probability of 1 in random sign init.\n    l2: L2 regularization coefficient.\n\n  Returns:\n    tf.Tensor.\n  """"""\n  x = inputs\n  y = inputs\n  y = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(l2),\n                         gamma_regularizer=tf.keras.regularizers.l2(l2))(y)\n  y = tf.keras.layers.Activation(\'relu\')(y)\n  y = Conv2DBatchEnsemble(\n      filters,\n      strides=strides,\n      alpha_initializer=make_sign_initializer(random_sign_init),\n      gamma_initializer=make_sign_initializer(random_sign_init),\n      kernel_regularizer=tf.keras.regularizers.l2(l2),\n      ensemble_size=ensemble_size)(y)\n  y = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(l2),\n                         gamma_regularizer=tf.keras.regularizers.l2(l2))(y)\n  y = tf.keras.layers.Activation(\'relu\')(y)\n  y = Conv2DBatchEnsemble(\n      filters,\n      strides=1,\n      alpha_initializer=make_sign_initializer(random_sign_init),\n      gamma_initializer=make_sign_initializer(random_sign_init),\n      kernel_regularizer=tf.keras.regularizers.l2(l2),\n      ensemble_size=ensemble_size)(y)\n  if not x.shape.is_compatible_with(y.shape):\n    x = Conv2DBatchEnsemble(\n        filters,\n        kernel_size=1,\n        strides=strides,\n        alpha_initializer=make_sign_initializer(random_sign_init),\n        gamma_initializer=make_sign_initializer(random_sign_init),\n        kernel_regularizer=tf.keras.regularizers.l2(l2),\n        ensemble_size=ensemble_size)(x)\n  x = tf.keras.layers.add([x, y])\n  return x\n\n\ndef group(inputs, filters, strides, num_blocks, **kwargs):\n  """"""Group of residual blocks.""""""\n  x = basic_block(inputs, filters=filters, strides=strides, **kwargs)\n  for _ in range(num_blocks - 1):\n    x = basic_block(x, filters=filters, strides=1, **kwargs)\n  return x\n\n\ndef wide_resnet(input_shape, depth, width_multiplier, num_classes,\n                ensemble_size, random_sign_init, l2):\n  """"""Builds Wide ResNet.\n\n  Following Zagoruyko and Komodakis (2016), it accepts a width multiplier on the\n  number of filters. Using three groups of residual blocks, the network maps\n  spatial features of size 32x32 -> 16x16 -> 8x8.\n\n  Args:\n    input_shape: tf.Tensor.\n    depth: Total number of convolutional layers. ""n"" in WRN-n-k. It differs from\n      He et al. (2015)\'s notation which uses the maximum depth of the network\n      counting non-conv layers like dense.\n    width_multiplier: Integer to multiply the number of typical filters by. ""k""\n      in WRN-n-k.\n    num_classes: Number of output classes.\n    ensemble_size: Number of ensemble members.\n    random_sign_init: Probability of 1 in random sign init.\n    l2: L2 regularization coefficient.\n\n  Returns:\n    tf.keras.Model.\n  """"""\n  if (depth - 4) % 6 != 0:\n    raise ValueError(\'depth should be 6n+4 (e.g., 16, 22, 28, 40).\')\n  num_blocks = (depth - 4) // 6\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  x = Conv2DBatchEnsemble(\n      16,\n      strides=1,\n      alpha_initializer=make_sign_initializer(random_sign_init),\n      gamma_initializer=make_sign_initializer(random_sign_init),\n      kernel_regularizer=tf.keras.regularizers.l2(l2),\n      ensemble_size=ensemble_size)(inputs)\n  for strides, filters in zip([1, 2, 2], [16, 32, 64]):\n    x = group(x,\n              filters=filters * width_multiplier,\n              strides=strides,\n              num_blocks=num_blocks,\n              random_sign_init=random_sign_init,\n              ensemble_size=ensemble_size,\n              l2=l2)\n\n  x = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(l2),\n                         gamma_regularizer=tf.keras.regularizers.l2(l2))(x)\n  x = tf.keras.layers.Activation(\'relu\')(x)\n  x = tf.keras.layers.AveragePooling2D(pool_size=8)(x)\n  x = tf.keras.layers.Flatten()(x)\n  x = ed.layers.DenseBatchEnsemble(\n      num_classes,\n      alpha_initializer=make_sign_initializer(random_sign_init),\n      gamma_initializer=make_sign_initializer(random_sign_init),\n      activation=None,\n      kernel_initializer=\'he_normal\',\n      kernel_regularizer=tf.keras.regularizers.l2(l2),\n      bias_regularizer=tf.keras.regularizers.l2(l2),\n      ensemble_size=ensemble_size)(x)\n  return tf.keras.Model(inputs=inputs, outputs=x)\n'"
baselines/cifar/batchensemble_model_test.py,10,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for BatchEnsemble.""""""\n\nimport batchensemble_model  # local file import\nimport tensorflow as tf\n\n\nclass BatchEnsembleModelTest(tf.test.TestCase):\n\n  def testWideResnet(self):\n    tf.random.set_seed(83922)\n    dataset_size = 10\n    batch_size = 4  # must be divisible by ensemble_size\n    input_shape = (32, 32, 1)\n    num_classes = 2\n\n    features = tf.random.normal((dataset_size,) + input_shape)\n    coeffs = tf.random.normal([tf.reduce_prod(input_shape), num_classes])\n    net = tf.reshape(features, [dataset_size, -1])\n    logits = tf.matmul(net, coeffs)\n    labels = tf.random.categorical(logits, 1)\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n    dataset = dataset.repeat().shuffle(dataset_size).batch(batch_size)\n\n    model = batchensemble_model.wide_resnet(input_shape=input_shape,\n                                            depth=10,\n                                            width_multiplier=1,\n                                            num_classes=num_classes,\n                                            ensemble_size=2,\n                                            random_sign_init=-0.5,\n                                            l2=0.)\n    model.compile(\n        \'adam\',\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n    history = model.fit(dataset,\n                        steps_per_epoch=dataset_size // batch_size,\n                        epochs=2)\n\n    loss_history = history.history[\'loss\']\n    self.assertAllGreaterEqual(loss_history, 0.)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
baselines/cifar/deterministic.py,65,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Wide ResNet 28-10 on CIFAR-10/100 trained with maximum likelihood.\n\nHyperparameters differ slightly from the original paper\'s code\n(https://github.com/szagoruyko/wide-residual-networks) as TensorFlow uses, for\nexample, l2 instead of weight decay, and a different parameterization for SGD\'s\nmomentum.\n""""""\n\nimport functools\nimport os\nimport time\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport utils  # local file import\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nflags.DEFINE_integer(\'seed\', 42, \'Random seed.\')\nflags.DEFINE_integer(\'per_core_batch_size\', 64, \'Batch size per TPU core/GPU.\')\nflags.DEFINE_float(\'base_learning_rate\', 0.1,\n                   \'Base learning rate when total batch size is 128. It is \'\n                   \'scaled by the ratio of the total batch size to 128.\')\nflags.DEFINE_integer(\'lr_warmup_epochs\', 1,\n                     \'Number of epochs for a linear warmup to the initial \'\n                     \'learning rate. Use 0 to do no warmup.\')\nflags.DEFINE_float(\'lr_decay_ratio\', 0.2, \'Amount to decay learning rate.\')\nflags.DEFINE_list(\'lr_decay_epochs\', [\'60\', \'120\', \'160\'],\n                  \'Epochs to decay learning rate by.\')\nflags.DEFINE_float(\'l2\', 2e-4, \'L2 regularization coefficient.\')\nflags.DEFINE_enum(\'dataset\', \'cifar10\',\n                  enum_values=[\'cifar10\', \'cifar100\'],\n                  help=\'Dataset.\')\n# TODO(ghassen): consider adding CIFAR-100-C to TFDS.\nflags.DEFINE_string(\'cifar100_c_path\', None,\n                    \'Path to the TFRecords files for CIFAR-100-C. Only valid \'\n                    \'(and required) if dataset is cifar100 and corruptions.\')\nflags.DEFINE_integer(\'corruptions_interval\', 200,\n                     \'Number of epochs between evaluating on the corrupted \'\n                     \'test data. Use -1 to never evaluate.\')\nflags.DEFINE_integer(\'checkpoint_interval\', 25,\n                     \'Number of epochs between saving checkpoints. Use -1 to \'\n                     \'never save checkpoints.\')\nflags.DEFINE_integer(\'num_bins\', 15, \'Number of bins for ECE.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/cifar\', \'Output directory.\')\nflags.DEFINE_integer(\'train_epochs\', 200, \'Number of training epochs.\')\n\n# Accelerator flags.\nflags.DEFINE_bool(\'use_gpu\', False, \'Whether to run on GPU or otherwise TPU.\')\nflags.DEFINE_bool(\'use_bfloat16\', False, \'Whether to use mixed precision.\')\nflags.DEFINE_integer(\'num_cores\', 8, \'Number of TPU cores or number of GPUs.\')\nflags.DEFINE_string(\'tpu\', None,\n                    \'Name of the TPU. Only used if use_gpu is False.\')\nFLAGS = flags.FLAGS\n\nBatchNormalization = functools.partial(  # pylint: disable=invalid-name\n    tf.keras.layers.BatchNormalization,\n    epsilon=1e-5,  # using epsilon and momentum defaults from Torch\n    momentum=0.9)\nConv2D = functools.partial(  # pylint: disable=invalid-name\n    tf.keras.layers.Conv2D,\n    kernel_size=3,\n    padding=\'same\',\n    use_bias=False,\n    kernel_initializer=\'he_normal\')\n\n\ndef basic_block(inputs, filters, strides, l2, version):\n  """"""Basic residual block of two 3x3 convs.\n\n  Args:\n    inputs: tf.Tensor.\n    filters: Number of filters for Conv2D.\n    strides: Stride dimensions for Conv2D.\n    l2: L2 regularization coefficient.\n    version: 1, indicating the original ordering from He et al. (2015); or 2,\n      indicating the preactivation ordering from He et al. (2016).\n\n  Returns:\n    tf.Tensor.\n  """"""\n  x = inputs\n  y = inputs\n  if version == 2:\n    y = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(l2),\n                           gamma_regularizer=tf.keras.regularizers.l2(l2))(y)\n    y = tf.keras.layers.Activation(\'relu\')(y)\n  y = Conv2D(filters,\n             strides=strides,\n             kernel_regularizer=tf.keras.regularizers.l2(l2))(y)\n  y = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(l2),\n                         gamma_regularizer=tf.keras.regularizers.l2(l2))(y)\n  y = tf.keras.layers.Activation(\'relu\')(y)\n  y = Conv2D(filters,\n             strides=1,\n             kernel_regularizer=tf.keras.regularizers.l2(l2))(y)\n  if version == 1:\n    y = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(l2),\n                           gamma_regularizer=tf.keras.regularizers.l2(l2))(y)\n  if not x.shape.is_compatible_with(y.shape):\n    x = Conv2D(filters,\n               kernel_size=1,\n               strides=strides,\n               kernel_regularizer=tf.keras.regularizers.l2(l2))(x)\n  x = tf.keras.layers.add([x, y])\n  if version == 1:\n    x = tf.keras.layers.Activation(\'relu\')(x)\n  return x\n\n\ndef group(inputs, filters, strides, num_blocks, l2, version):\n  """"""Group of residual blocks.""""""\n  x = basic_block(inputs, filters=filters, strides=strides, l2=l2,\n                  version=version)\n  for _ in range(num_blocks - 1):\n    x = basic_block(x, filters=filters, strides=1, l2=l2, version=version)\n  return x\n\n\ndef wide_resnet(input_shape, depth, width_multiplier, num_classes, l2, version):\n  """"""Builds Wide ResNet.\n\n  Following Zagoruyko and Komodakis (2016), it accepts a width multiplier on the\n  number of filters. Using three groups of residual blocks, the network maps\n  spatial features of size 32x32 -> 16x16 -> 8x8.\n\n  Args:\n    input_shape: tf.Tensor.\n    depth: Total number of convolutional layers. ""n"" in WRN-n-k. It differs from\n      He et al. (2015)\'s notation which uses the maximum depth of the network\n      counting non-conv layers like dense.\n    width_multiplier: Integer to multiply the number of typical filters by. ""k""\n      in WRN-n-k.\n    num_classes: Number of output classes.\n    l2: L2 regularization coefficient.\n    version: 1, indicating the original ordering from He et al. (2015); or 2,\n      indicating the preactivation ordering from He et al. (2016).\n\n  Returns:\n    tf.keras.Model.\n  """"""\n  if (depth - 4) % 6 != 0:\n    raise ValueError(\'depth should be 6n+4 (e.g., 16, 22, 28, 40).\')\n  num_blocks = (depth - 4) // 6\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  x = Conv2D(16,\n             strides=1,\n             kernel_regularizer=tf.keras.regularizers.l2(l2))(inputs)\n  if version == 1:\n    x = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(l2),\n                           gamma_regularizer=tf.keras.regularizers.l2(l2))(x)\n    x = tf.keras.layers.Activation(\'relu\')(x)\n  x = group(x,\n            filters=16 * width_multiplier,\n            strides=1,\n            num_blocks=num_blocks,\n            l2=l2,\n            version=version)\n  x = group(x,\n            filters=32 * width_multiplier,\n            strides=2,\n            num_blocks=num_blocks,\n            l2=l2,\n            version=version)\n  x = group(x,\n            filters=64 * width_multiplier,\n            strides=2,\n            num_blocks=num_blocks,\n            l2=l2,\n            version=version)\n  if version == 2:\n    x = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(l2),\n                           gamma_regularizer=tf.keras.regularizers.l2(l2))(x)\n    x = tf.keras.layers.Activation(\'relu\')(x)\n  x = tf.keras.layers.AveragePooling2D(pool_size=8)(x)\n  x = tf.keras.layers.Flatten()(x)\n  x = tf.keras.layers.Dense(\n      num_classes,\n      kernel_initializer=\'he_normal\',\n      kernel_regularizer=tf.keras.regularizers.l2(l2),\n      bias_regularizer=tf.keras.regularizers.l2(l2))(x)\n  return tf.keras.Model(inputs=inputs, outputs=x)\n\n\ndef main(argv):\n  del argv  # unused arg\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  logging.info(\'Saving checkpoints at %s\', FLAGS.output_dir)\n  tf.random.set_seed(FLAGS.seed)\n\n  if FLAGS.use_gpu:\n    logging.info(\'Use GPU\')\n    strategy = tf.distribute.MirroredStrategy()\n  else:\n    logging.info(\'Use TPU at %s\',\n                 FLAGS.tpu if FLAGS.tpu is not None else \'local\')\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n  train_input_fn = utils.load_input_fn(\n      split=tfds.Split.TRAIN,\n      name=FLAGS.dataset,\n      batch_size=FLAGS.per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  clean_test_input_fn = utils.load_input_fn(\n      split=tfds.Split.TEST,\n      name=FLAGS.dataset,\n      batch_size=FLAGS.per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  train_dataset = strategy.experimental_distribute_datasets_from_function(\n      train_input_fn)\n  test_datasets = {\n      \'clean\': strategy.experimental_distribute_datasets_from_function(\n          clean_test_input_fn),\n  }\n  if FLAGS.corruptions_interval > 0:\n    if FLAGS.dataset == \'cifar10\':\n      load_c_input_fn = utils.load_cifar10_c_input_fn\n    else:\n      load_c_input_fn = functools.partial(utils.load_cifar100_c_input_fn,\n                                          path=FLAGS.cifar100_c_path)\n    corruption_types, max_intensity = utils.load_corrupted_test_info(\n        FLAGS.dataset)\n    for corruption in corruption_types:\n      for intensity in range(1, max_intensity + 1):\n        input_fn = load_c_input_fn(\n            corruption_name=corruption,\n            corruption_intensity=intensity,\n            batch_size=FLAGS.per_core_batch_size,\n            use_bfloat16=FLAGS.use_bfloat16)\n        test_datasets[\'{0}_{1}\'.format(corruption, intensity)] = (\n            strategy.experimental_distribute_datasets_from_function(input_fn))\n\n  ds_info = tfds.builder(FLAGS.dataset).info\n  batch_size = FLAGS.per_core_batch_size * FLAGS.num_cores\n  steps_per_epoch = ds_info.splits[\'train\'].num_examples // batch_size\n  steps_per_eval = ds_info.splits[\'test\'].num_examples // batch_size\n  num_classes = ds_info.features[\'label\'].num_classes\n\n  if FLAGS.use_bfloat16:\n    policy = tf.keras.mixed_precision.experimental.Policy(\'mixed_bfloat16\')\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n\n  summary_writer = tf.summary.create_file_writer(\n      os.path.join(FLAGS.output_dir, \'summaries\'))\n\n  with strategy.scope():\n    logging.info(\'Building ResNet model\')\n    model = wide_resnet(input_shape=ds_info.features[\'image\'].shape,\n                        depth=28,\n                        width_multiplier=10,\n                        num_classes=num_classes,\n                        l2=FLAGS.l2,\n                        version=2)\n    logging.info(\'Model input shape: %s\', model.input_shape)\n    logging.info(\'Model output shape: %s\', model.output_shape)\n    logging.info(\'Model number of weights: %s\', model.count_params())\n    # Linearly scale learning rate and the decay epochs by vanilla settings.\n    base_lr = FLAGS.base_learning_rate * batch_size / 128\n    lr_decay_epochs = [(int(start_epoch_str) * FLAGS.train_epochs) // 200\n                       for start_epoch_str in FLAGS.lr_decay_epochs]\n    lr_schedule = utils.LearningRateSchedule(\n        steps_per_epoch,\n        base_lr,\n        decay_ratio=FLAGS.lr_decay_ratio,\n        decay_epochs=lr_decay_epochs,\n        warmup_epochs=FLAGS.lr_warmup_epochs)\n    optimizer = tf.keras.optimizers.SGD(lr_schedule,\n                                        momentum=0.9,\n                                        nesterov=True)\n    metrics = {\n        \'train/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'train/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'train/loss\': tf.keras.metrics.Mean(),\n        \'train/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'test/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'test/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n    }\n    if FLAGS.corruptions_interval > 0:\n      corrupt_metrics = {}\n      for intensity in range(1, max_intensity + 1):\n        for corruption in corruption_types:\n          dataset_name = \'{0}_{1}\'.format(corruption, intensity)\n          corrupt_metrics[\'test/nll_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.Mean())\n          corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.SparseCategoricalAccuracy())\n          corrupt_metrics[\'test/ece_{}\'.format(dataset_name)] = (\n              ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins))\n\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n    initial_epoch = 0\n    if latest_checkpoint:\n      # checkpoint.restore must be within a strategy.scope() so that optimizer\n      # slot variables are mirrored.\n      checkpoint.restore(latest_checkpoint)\n      logging.info(\'Loaded checkpoint %s\', latest_checkpoint)\n      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n\n  @tf.function\n  def train_step(iterator):\n    """"""Training StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      with tf.GradientTape() as tape:\n        logits = model(images, training=True)\n        if FLAGS.use_bfloat16:\n          logits = tf.cast(logits, tf.float32)\n        negative_log_likelihood = tf.reduce_mean(\n            tf.keras.losses.sparse_categorical_crossentropy(labels,\n                                                            logits,\n                                                            from_logits=True))\n        l2_loss = sum(model.losses)\n        loss = negative_log_likelihood + l2_loss\n        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n        scaled_loss = loss / strategy.num_replicas_in_sync\n\n      grads = tape.gradient(scaled_loss, model.trainable_variables)\n      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n      probs = tf.nn.softmax(logits)\n      metrics[\'train/ece\'].update_state(labels, probs)\n      metrics[\'train/loss\'].update_state(loss)\n      metrics[\'train/negative_log_likelihood\'].update_state(\n          negative_log_likelihood)\n      metrics[\'train/accuracy\'].update_state(labels, logits)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  @tf.function\n  def test_step(iterator, dataset_name):\n    """"""Evaluation StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      logits = model(images, training=False)\n      if FLAGS.use_bfloat16:\n        logits = tf.cast(logits, tf.float32)\n      probs = tf.nn.softmax(logits)\n      negative_log_likelihood = tf.reduce_mean(\n          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n\n      if dataset_name == \'clean\':\n        metrics[\'test/negative_log_likelihood\'].update_state(\n            negative_log_likelihood)\n        metrics[\'test/accuracy\'].update_state(labels, probs)\n        metrics[\'test/ece\'].update_state(labels, probs)\n      else:\n        corrupt_metrics[\'test/nll_{}\'.format(dataset_name)].update_state(\n            negative_log_likelihood)\n        corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n        corrupt_metrics[\'test/ece_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  train_iterator = iter(train_dataset)\n  start_time = time.time()\n  for epoch in range(initial_epoch, FLAGS.train_epochs):\n    logging.info(\'Starting to run epoch: %s\', epoch)\n    for step in range(steps_per_epoch):\n      train_step(train_iterator)\n\n      current_step = epoch * steps_per_epoch + (step + 1)\n      max_steps = steps_per_epoch * FLAGS.train_epochs\n      time_elapsed = time.time() - start_time\n      steps_per_sec = float(current_step) / time_elapsed\n      eta_seconds = (max_steps - current_step) / steps_per_sec\n      message = (\'{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. \'\n                 \'ETA: {:.0f} min. Time elapsed: {:.0f} min\'.format(\n                     current_step / max_steps,\n                     epoch + 1,\n                     FLAGS.train_epochs,\n                     steps_per_sec,\n                     eta_seconds / 60,\n                     time_elapsed / 60))\n      if step % 20 == 0:\n        logging.info(message)\n\n    datasets_to_evaluate = {\'clean\': test_datasets[\'clean\']}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      datasets_to_evaluate = test_datasets\n    for dataset_name, test_dataset in datasets_to_evaluate.items():\n      test_iterator = iter(test_dataset)\n      logging.info(\'Testing on dataset %s\', dataset_name)\n      for step in range(steps_per_eval):\n        if step % 20 == 0:\n          logging.info(\'Starting to run eval step %s of epoch: %s\', step,\n                       epoch)\n        test_step(test_iterator, dataset_name)\n      logging.info(\'Done with testing on %s\', dataset_name)\n\n    corrupt_results = {}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      corrupt_results = utils.aggregate_corrupt_metrics(corrupt_metrics,\n                                                        corruption_types,\n                                                        max_intensity)\n\n    logging.info(\'Train Loss: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'train/loss\'].result(),\n                 metrics[\'train/accuracy\'].result() * 100)\n    logging.info(\'Test NLL: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'test/negative_log_likelihood\'].result(),\n                 metrics[\'test/accuracy\'].result() * 100)\n    total_results = {name: metric.result() for name, metric in metrics.items()}\n    total_results.update(corrupt_results)\n    with summary_writer.as_default():\n      for name, result in total_results.items():\n        tf.summary.scalar(name, result, step=epoch + 1)\n\n    for metric in metrics.values():\n      metric.reset_states()\n\n    if (FLAGS.checkpoint_interval > 0 and\n        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n      checkpoint_name = checkpoint.save(\n          os.path.join(FLAGS.output_dir, \'checkpoint\'))\n      logging.info(\'Saved checkpoint to %s\', checkpoint_name)\n\n  final_checkpoint_name = checkpoint.save(\n      os.path.join(FLAGS.output_dir, \'checkpoint\'))\n  logging.info(\'Saved last checkpoint to %s\', final_checkpoint_name)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/cifar/deterministic_test.py,10,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for WRN 28-10.""""""\n\nimport deterministic  # local file import\nimport tensorflow as tf\n\n\nclass DeterministicTest(tf.test.TestCase):\n\n  def testWideResnet(self):\n    tf.random.set_seed(83922)\n    dataset_size = 10\n    batch_size = 5\n    input_shape = (32, 32, 1)\n    num_classes = 2\n\n    features = tf.random.normal((dataset_size,) + input_shape)\n    coeffs = tf.random.normal([tf.reduce_prod(input_shape), num_classes])\n    net = tf.reshape(features, [dataset_size, -1])\n    logits = tf.matmul(net, coeffs)\n    labels = tf.random.categorical(logits, 1)\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n    dataset = dataset.repeat().shuffle(dataset_size).batch(batch_size)\n\n    model = deterministic.wide_resnet(input_shape=input_shape,\n                                      depth=10,\n                                      width_multiplier=1,\n                                      num_classes=num_classes,\n                                      l2=0.,\n                                      version=2)\n    model.compile(\n        \'adam\',\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n    history = model.fit(dataset,\n                        steps_per_epoch=dataset_size // batch_size,\n                        epochs=2)\n\n    loss_history = history.history[\'loss\']\n    self.assertAllGreaterEqual(loss_history, 0.)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
baselines/cifar/dropout.py,62,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Wide ResNet 28-10 with Monte Carlo dropout on CIFAR-10.""""""\n\nimport functools\nimport os\nimport time\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport utils  # local file import\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nflags.DEFINE_integer(\'seed\', 42, \'Random seed.\')\nflags.DEFINE_integer(\'per_core_batch_size\', 64, \'Batch size per TPU core/GPU.\')\nflags.DEFINE_float(\'base_learning_rate\', 0.05,\n                   \'Base learning rate when total batch size is 128. It is \'\n                   \'scaled by the ratio of the total batch size to 128.\')\nflags.DEFINE_integer(\'lr_warmup_epochs\', 1,\n                     \'Number of epochs for a linear warmup to the initial \'\n                     \'learning rate. Use 0 to do no warmup.\')\nflags.DEFINE_float(\'lr_decay_ratio\', 0.2, \'Amount to decay learning rate.\')\nflags.DEFINE_list(\'lr_decay_epochs\', [\'60\', \'120\', \'160\'],\n                  \'Epochs to decay learning rate by.\')\nflags.DEFINE_float(\'l2\', 3e-4, \'L2 regularization coefficient.\')\nflags.DEFINE_float(\'dropout_rate\', 0.1, \'Dropout rate.\')\nflags.DEFINE_integer(\'num_dropout_samples\', 1,\n                     \'Number of dropout samples to use for prediction.\')\n\nflags.DEFINE_enum(\'dataset\', \'cifar10\',\n                  enum_values=[\'cifar10\', \'cifar100\'],\n                  help=\'Dataset.\')\n# TODO(ghassen): consider adding CIFAR-100-C to TFDS.\nflags.DEFINE_string(\'cifar100_c_path\', None,\n                    \'Path to the TFRecords files for CIFAR-100-C. Only valid \'\n                    \'(and required) if dataset is cifar100 and corruptions.\')\nflags.DEFINE_integer(\'corruptions_interval\', 50,\n                     \'Number of epochs between evaluating on the corrupted \'\n                     \'test data. Use -1 to never evaluate.\')\nflags.DEFINE_integer(\'checkpoint_interval\', -1,\n                     \'Number of epochs between saving checkpoints. Use -1 to \'\n                     \'never save checkpoints.\')\nflags.DEFINE_integer(\'num_bins\', 15, \'Number of bins for ECE.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/cifar\', \'Output directory.\')\nflags.DEFINE_integer(\'train_epochs\', 200, \'Number of training epochs.\')\n\n# Accelerator flags.\nflags.DEFINE_bool(\'use_gpu\', False, \'Whether to run on GPU or otherwise TPU.\')\nflags.DEFINE_bool(\'use_bfloat16\', False, \'Whether to use mixed precision.\')\nflags.DEFINE_integer(\'num_cores\', 8, \'Number of TPU cores or number of GPUs.\')\nflags.DEFINE_string(\'tpu\', None,\n                    \'Name of the TPU. Only used if use_gpu is False.\')\nFLAGS = flags.FLAGS\n\nBatchNormalization = functools.partial(  # pylint: disable=invalid-name\n    tf.keras.layers.BatchNormalization,\n    epsilon=1e-5,  # using epsilon and momentum defaults from Torch\n    momentum=0.9)\nConv2D = functools.partial(  # pylint: disable=invalid-name\n    tf.keras.layers.Conv2D,\n    kernel_size=3,\n    padding=\'same\',\n    use_bias=False,\n    kernel_initializer=\'he_normal\')\n\n\ndef basic_block(inputs, filters, strides, l2, dropout_rate):\n  """"""Basic residual block of two 3x3 convs.\n\n  Args:\n    inputs: tf.Tensor.\n    filters: Number of filters for Conv2D.\n    strides: Stride dimensions for Conv2D.\n    l2: L2 regularization coefficient.\n    dropout_rate: Dropout rate.\n\n  Returns:\n    tf.Tensor.\n  """"""\n  x = inputs\n  y = inputs\n  y = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(l2),\n                         gamma_regularizer=tf.keras.regularizers.l2(l2))(y)\n  y = tf.keras.layers.Activation(\'relu\')(y)\n  y = Conv2D(filters,\n             strides=strides,\n             kernel_regularizer=tf.keras.regularizers.l2(l2))(y)\n  y = tf.keras.layers.Dropout(dropout_rate)(y, training=True)\n  y = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(l2),\n                         gamma_regularizer=tf.keras.regularizers.l2(l2))(y)\n  y = tf.keras.layers.Activation(\'relu\')(y)\n  y = Conv2D(filters,\n             strides=1,\n             kernel_regularizer=tf.keras.regularizers.l2(l2))(y)\n  if not x.shape.is_compatible_with(y.shape):\n    x = Conv2D(filters,\n               kernel_size=1,\n               strides=strides,\n               kernel_regularizer=tf.keras.regularizers.l2(l2))(x)\n  x = tf.keras.layers.add([x, y])\n  return x\n\n\ndef group(inputs, filters, strides, num_blocks, l2, dropout_rate):\n  """"""Group of residual blocks.""""""\n  x = basic_block(inputs, filters=filters, strides=strides, l2=l2,\n                  dropout_rate=dropout_rate)\n  for _ in range(num_blocks - 1):\n    x = basic_block(x, filters=filters, strides=1, l2=l2,\n                    dropout_rate=dropout_rate)\n  return x\n\n\ndef wide_resnet(input_shape, depth, width_multiplier, num_classes, l2,\n                dropout_rate):\n  """"""Builds Wide ResNet.\n\n  Following Zagoruyko and Komodakis (2016), it accepts a width multiplier on the\n  number of filters. Using three groups of residual blocks, the network maps\n  spatial features of size 32x32 -> 16x16 -> 8x8.\n\n  Args:\n    input_shape: tf.Tensor.\n    depth: Total number of convolutional layers. ""n"" in WRN-n-k. It differs from\n      He et al. (2015)\'s notation which uses the maximum depth of the network\n      counting non-conv layers like dense.\n    width_multiplier: Integer to multiply the number of typical filters by. ""k""\n      in WRN-n-k.\n    num_classes: Number of output classes.\n    l2: L2 regularization coefficient.\n    dropout_rate: Dropout rate.\n\n  Returns:\n    tf.keras.Model.\n  """"""\n  if (depth - 4) % 6 != 0:\n    raise ValueError(\'depth should be 6n+4 (e.g., 16, 22, 28, 40).\')\n  num_blocks = (depth - 4) // 6\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  x = Conv2D(16,\n             strides=1,\n             kernel_regularizer=tf.keras.regularizers.l2(l2))(inputs)\n  x = group(x,\n            filters=16 * width_multiplier,\n            strides=1,\n            num_blocks=num_blocks,\n            l2=l2,\n            dropout_rate=dropout_rate)\n  x = group(x,\n            filters=32 * width_multiplier,\n            strides=2,\n            num_blocks=num_blocks,\n            l2=l2,\n            dropout_rate=dropout_rate)\n  x = group(x,\n            filters=64 * width_multiplier,\n            strides=2,\n            num_blocks=num_blocks,\n            l2=l2,\n            dropout_rate=dropout_rate)\n  x = BatchNormalization(beta_regularizer=tf.keras.regularizers.l2(l2),\n                         gamma_regularizer=tf.keras.regularizers.l2(l2))(x)\n  x = tf.keras.layers.Activation(\'relu\')(x)\n  x = tf.keras.layers.AveragePooling2D(pool_size=8)(x)\n  x = tf.keras.layers.Flatten()(x)\n  x = tf.keras.layers.Dense(\n      num_classes,\n      kernel_initializer=\'he_normal\',\n      kernel_regularizer=tf.keras.regularizers.l2(l2),\n      bias_regularizer=tf.keras.regularizers.l2(l2))(x)\n  return tf.keras.Model(inputs=inputs, outputs=x)\n\n\ndef main(argv):\n  del argv  # unused arg\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  logging.info(\'Saving checkpoints at %s\', FLAGS.output_dir)\n  tf.random.set_seed(FLAGS.seed)\n\n  if FLAGS.use_gpu:\n    logging.info(\'Use GPU\')\n    strategy = tf.distribute.MirroredStrategy()\n  else:\n    logging.info(\'Use TPU at %s\',\n                 FLAGS.tpu if FLAGS.tpu is not None else \'local\')\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n  train_input_fn = utils.load_input_fn(\n      split=tfds.Split.TRAIN,\n      name=FLAGS.dataset,\n      batch_size=FLAGS.per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  clean_test_input_fn = utils.load_input_fn(\n      split=tfds.Split.TEST,\n      name=FLAGS.dataset,\n      batch_size=FLAGS.per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  train_dataset = strategy.experimental_distribute_datasets_from_function(\n      train_input_fn)\n  test_datasets = {\n      \'clean\': strategy.experimental_distribute_datasets_from_function(\n          clean_test_input_fn),\n  }\n  if FLAGS.corruptions_interval > 0:\n    if FLAGS.dataset == \'cifar10\':\n      load_c_input_fn = utils.load_cifar10_c_input_fn\n    else:\n      load_c_input_fn = functools.partial(utils.load_cifar100_c_input_fn,\n                                          path=FLAGS.cifar100_c_path)\n    corruption_types, max_intensity = utils.load_corrupted_test_info(\n        FLAGS.dataset)\n    for corruption in corruption_types:\n      for intensity in range(1, max_intensity + 1):\n        input_fn = load_c_input_fn(\n            corruption_name=corruption,\n            corruption_intensity=intensity,\n            batch_size=FLAGS.per_core_batch_size,\n            use_bfloat16=FLAGS.use_bfloat16)\n        test_datasets[\'{0}_{1}\'.format(corruption, intensity)] = (\n            strategy.experimental_distribute_datasets_from_function(input_fn))\n\n  ds_info = tfds.builder(FLAGS.dataset).info\n  batch_size = FLAGS.per_core_batch_size * FLAGS.num_cores\n  steps_per_epoch = ds_info.splits[\'train\'].num_examples // batch_size\n  steps_per_eval = ds_info.splits[\'test\'].num_examples // batch_size\n  num_classes = ds_info.features[\'label\'].num_classes\n\n  if FLAGS.use_bfloat16:\n    policy = tf.keras.mixed_precision.experimental.Policy(\'mixed_bfloat16\')\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n\n  summary_writer = tf.summary.create_file_writer(\n      os.path.join(FLAGS.output_dir, \'summaries\'))\n\n  with strategy.scope():\n    logging.info(\'Building ResNet model\')\n    model = wide_resnet(input_shape=ds_info.features[\'image\'].shape,\n                        depth=28,\n                        width_multiplier=10,\n                        num_classes=num_classes,\n                        l2=FLAGS.l2,\n                        dropout_rate=FLAGS.dropout_rate)\n    logging.info(\'Model input shape: %s\', model.input_shape)\n    logging.info(\'Model output shape: %s\', model.output_shape)\n    logging.info(\'Model number of weights: %s\', model.count_params())\n    # Linearly scale learning rate and the decay epochs by vanilla settings.\n    base_lr = FLAGS.base_learning_rate * batch_size / 128\n    lr_decay_epochs = [(int(start_epoch_str) * FLAGS.train_epochs) // 200\n                       for start_epoch_str in FLAGS.lr_decay_epochs]\n    lr_schedule = utils.LearningRateSchedule(\n        steps_per_epoch,\n        base_lr,\n        decay_ratio=FLAGS.lr_decay_ratio,\n        decay_epochs=lr_decay_epochs,\n        warmup_epochs=FLAGS.lr_warmup_epochs)\n    optimizer = tf.keras.optimizers.SGD(lr_schedule,\n                                        momentum=0.9,\n                                        nesterov=True)\n    metrics = {\n        \'train/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'train/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'train/loss\': tf.keras.metrics.Mean(),\n        \'train/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'test/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'test/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n    }\n    if FLAGS.corruptions_interval > 0:\n      corrupt_metrics = {}\n      for intensity in range(1, max_intensity + 1):\n        for corruption in corruption_types:\n          dataset_name = \'{0}_{1}\'.format(corruption, intensity)\n          corrupt_metrics[\'test/nll_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.Mean())\n          corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.SparseCategoricalAccuracy())\n          corrupt_metrics[\'test/ece_{}\'.format(dataset_name)] = (\n              ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins))\n\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n    initial_epoch = 0\n    if latest_checkpoint:\n      # checkpoint.restore must be within a strategy.scope() so that optimizer\n      # slot variables are mirrored.\n      checkpoint.restore(latest_checkpoint)\n      logging.info(\'Loaded checkpoint %s\', latest_checkpoint)\n      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n\n  @tf.function\n  def train_step(iterator):\n    """"""Training StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      with tf.GradientTape() as tape:\n        logits = model(images, training=True)\n        if FLAGS.use_bfloat16:\n          logits = tf.cast(logits, tf.float32)\n        negative_log_likelihood = tf.reduce_mean(\n            tf.keras.losses.sparse_categorical_crossentropy(labels,\n                                                            logits,\n                                                            from_logits=True))\n        l2_loss = sum(model.losses)\n        loss = negative_log_likelihood + l2_loss\n        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n        scaled_loss = loss / strategy.num_replicas_in_sync\n\n      grads = tape.gradient(scaled_loss, model.trainable_variables)\n      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n      probs = tf.nn.softmax(logits)\n      metrics[\'train/ece\'].update_state(labels, probs)\n      metrics[\'train/loss\'].update_state(loss)\n      metrics[\'train/negative_log_likelihood\'].update_state(\n          negative_log_likelihood)\n      metrics[\'train/accuracy\'].update_state(labels, logits)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  @tf.function\n  def test_step(iterator, dataset_name):\n    """"""Evaluation StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n\n      logits_list = []\n      for _ in range(FLAGS.num_dropout_samples):\n        logits = model(images, training=False)\n        if FLAGS.use_bfloat16:\n          logits = tf.cast(logits, tf.float32)\n        logits_list.append(logits)\n\n      # Logits dimension is (num_samples, batch_size, num_classes).\n      logits_list = tf.stack(logits_list, axis=0)\n      probs_list = tf.nn.softmax(logits_list)\n      probs = tf.reduce_mean(probs_list, axis=0)\n\n      negative_log_likelihood = tf.reduce_mean(\n          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n\n      if dataset_name == \'clean\':\n        metrics[\'test/negative_log_likelihood\'].update_state(\n            negative_log_likelihood)\n        metrics[\'test/accuracy\'].update_state(labels, probs)\n        metrics[\'test/ece\'].update_state(labels, probs)\n      else:\n        corrupt_metrics[\'test/nll_{}\'.format(dataset_name)].update_state(\n            negative_log_likelihood)\n        corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n        corrupt_metrics[\'test/ece_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  train_iterator = iter(train_dataset)\n  start_time = time.time()\n  for epoch in range(initial_epoch, FLAGS.train_epochs):\n    logging.info(\'Starting to run epoch: %s\', epoch)\n    for step in range(steps_per_epoch):\n      train_step(train_iterator)\n\n      current_step = epoch * steps_per_epoch + (step + 1)\n      max_steps = steps_per_epoch * FLAGS.train_epochs\n      time_elapsed = time.time() - start_time\n      steps_per_sec = float(current_step) / time_elapsed\n      eta_seconds = (max_steps - current_step) / steps_per_sec\n      message = (\'{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. \'\n                 \'ETA: {:.0f} min. Time elapsed: {:.0f} min\'.format(\n                     current_step / max_steps,\n                     epoch + 1,\n                     FLAGS.train_epochs,\n                     steps_per_sec,\n                     eta_seconds / 60,\n                     time_elapsed / 60))\n      if step % 20 == 0:\n        logging.info(message)\n\n    datasets_to_evaluate = {\'clean\': test_datasets[\'clean\']}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      datasets_to_evaluate = test_datasets\n    for dataset_name, test_dataset in datasets_to_evaluate.items():\n      test_iterator = iter(test_dataset)\n      logging.info(\'Testing on dataset %s\', dataset_name)\n      for step in range(steps_per_eval):\n        if step % 20 == 0:\n          logging.info(\'Starting to run eval step %s of epoch: %s\', step,\n                       epoch)\n        test_step(test_iterator, dataset_name)\n      logging.info(\'Done with testing on %s\', dataset_name)\n\n    corrupt_results = {}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      corrupt_results = utils.aggregate_corrupt_metrics(corrupt_metrics,\n                                                        corruption_types,\n                                                        max_intensity)\n\n    logging.info(\'Train Loss: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'train/loss\'].result(),\n                 metrics[\'train/accuracy\'].result() * 100)\n    logging.info(\'Test NLL: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'test/negative_log_likelihood\'].result(),\n                 metrics[\'test/accuracy\'].result() * 100)\n    total_results = {name: metric.result() for name, metric in metrics.items()}\n    total_results.update(corrupt_results)\n    with summary_writer.as_default():\n      for name, result in total_results.items():\n        tf.summary.scalar(name, result, step=epoch + 1)\n\n    for metric in metrics.values():\n      metric.reset_states()\n\n    if (FLAGS.checkpoint_interval > 0 and\n        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n      checkpoint_name = checkpoint.save(\n          os.path.join(FLAGS.output_dir, \'checkpoint\'))\n      logging.info(\'Saved checkpoint to %s\', checkpoint_name)\n  final_checkpoint_name = checkpoint.save(\n      os.path.join(FLAGS.output_dir, \'checkpoint\'))\n  logging.info(\'Saved last checkpoint to %s\', final_checkpoint_name)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/cifar/dropout_test.py,10,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for WRN 28-10 with Monte Carlo dropout.""""""\n\nimport dropout  # local file import\nimport tensorflow as tf\n\n\nclass DropoutTest(tf.test.TestCase):\n\n  def testWideResnet(self):\n    tf.random.set_seed(83922)\n    dataset_size = 15\n    batch_size = 5\n    input_shape = (32, 32, 1)\n    num_classes = 2\n\n    features = tf.random.normal((dataset_size,) + input_shape)\n    coeffs = tf.random.normal([tf.reduce_prod(input_shape), num_classes])\n    net = tf.reshape(features, [dataset_size, -1])\n    logits = tf.matmul(net, coeffs)\n    labels = tf.random.categorical(logits, 1)\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n    dataset = dataset.repeat().shuffle(dataset_size).batch(batch_size)\n\n    model = dropout.wide_resnet(input_shape=input_shape,\n                                depth=10,\n                                width_multiplier=1,\n                                num_classes=num_classes,\n                                l2=0.,\n                                dropout_rate=0.01)\n    model.compile(\n        \'adam\',\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n    history = model.fit(dataset,\n                        steps_per_epoch=dataset_size // batch_size,\n                        epochs=2)\n\n    loss_history = history.history[\'loss\']\n    self.assertAllGreaterEqual(loss_history, 0.)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
baselines/cifar/ensemble.py,35,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Ensemble on CIFAR.\n\nThis script only performs evaluation, not training. We recommend training\nensembles by launching independent runs of `deterministic.py` over different\nseeds.\n""""""\n\nimport functools\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport deterministic  # local file import\nimport utils  # local file import\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# TODO(trandustin): We inherit\n# FLAGS.{dataset,per_core_batch_size,output_dir,seed} from deterministic. This\n# is not intuitive, which suggests we need to either refactor to avoid importing\n# from a binary or duplicate the model definition here.\nflags.DEFINE_string(\'checkpoint_dir\', None,\n                    \'The directory where the model weights are stored.\')\nflags.mark_flag_as_required(\'checkpoint_dir\')\nFLAGS = flags.FLAGS\n\n\ndef ensemble_negative_log_likelihood(labels, logits):\n  """"""Negative log-likelihood for ensemble.\n\n  For each datapoint (x,y), the ensemble\'s negative log-likelihood is:\n\n  ```\n  -log p(y|x) = -log sum_{m=1}^{ensemble_size} exp(log p(y|x,theta_m)) +\n                log ensemble_size.\n  ```\n\n  Args:\n    labels: tf.Tensor of shape [...].\n    logits: tf.Tensor of shape [ensemble_size, ..., num_classes].\n\n  Returns:\n    tf.Tensor of shape [...].\n  """"""\n  labels = tf.cast(labels, tf.int32)\n  logits = tf.convert_to_tensor(logits)\n  ensemble_size = float(logits.shape[0])\n  nll = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      tf.broadcast_to(labels[tf.newaxis, ...], tf.shape(logits)[:-1]),\n      logits)\n  return -tf.reduce_logsumexp(-nll, axis=0) + tf.math.log(ensemble_size)\n\n\ndef gibbs_cross_entropy(labels, logits):\n  """"""Average cross entropy for ensemble members (Gibbs cross entropy).\n\n  For each datapoint (x,y), the ensemble\'s Gibbs cross entropy is:\n\n  ```\n  GCE = - (1/ensemble_size) sum_{m=1}^ensemble_size log p(y|x,theta_m).\n  ```\n\n  The Gibbs cross entropy approximates the average cross entropy of a single\n  model drawn from the (Gibbs) ensemble.\n\n  Args:\n    labels: tf.Tensor of shape [...].\n    logits: tf.Tensor of shape [ensemble_size, ..., num_classes].\n\n  Returns:\n    tf.Tensor of shape [...].\n  """"""\n  labels = tf.cast(labels, tf.int32)\n  logits = tf.convert_to_tensor(logits)\n  nll = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      tf.broadcast_to(labels[tf.newaxis, ...], tf.shape(logits)[:-1]),\n      logits)\n  return tf.reduce_mean(nll, axis=0)\n\n\ndef main(argv):\n  del argv  # unused arg\n  if not FLAGS.use_gpu:\n    raise ValueError(\'Only GPU is currently supported.\')\n  if FLAGS.num_cores > 1:\n    raise ValueError(\'Only a single accelerator is currently supported.\')\n  tf.random.set_seed(FLAGS.seed)\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n\n  ds_info = tfds.builder(FLAGS.dataset).info\n  batch_size = FLAGS.per_core_batch_size * FLAGS.num_cores\n  steps_per_eval = ds_info.splits[\'test\'].num_examples // batch_size\n  num_classes = ds_info.features[\'label\'].num_classes\n\n  dataset_input_fn = utils.load_input_fn(\n      split=tfds.Split.TEST,\n      name=FLAGS.dataset,\n      batch_size=FLAGS.per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  test_datasets = {\'clean\': dataset_input_fn()}\n  corruption_types, max_intensity = utils.load_corrupted_test_info(\n      FLAGS.dataset)\n  for name in corruption_types:\n    for intensity in range(1, max_intensity + 1):\n      dataset_name = \'{0}_{1}\'.format(name, intensity)\n      if FLAGS.dataset == \'cifar10\':\n        load_c_dataset = utils.load_cifar10_c_input_fn\n      else:\n        load_c_dataset = functools.partial(\n            utils.load_cifar100_c_input_fn, path=FLAGS.cifar100_c_path)\n      corrupted_input_fn = load_c_dataset(\n          corruption_name=name,\n          corruption_intensity=intensity,\n          batch_size=FLAGS.per_core_batch_size,\n          use_bfloat16=FLAGS.use_bfloat16)\n      test_datasets[dataset_name] = corrupted_input_fn()\n\n  model = deterministic.wide_resnet(\n      input_shape=ds_info.features[\'image\'].shape,\n      depth=28,\n      width_multiplier=10,\n      num_classes=num_classes,\n      l2=0.,\n      version=2)\n  logging.info(\'Model input shape: %s\', model.input_shape)\n  logging.info(\'Model output shape: %s\', model.output_shape)\n  logging.info(\'Model number of weights: %s\', model.count_params())\n\n  # Search for checkpoints from their index file; then remove the index suffix.\n  ensemble_filenames = tf.io.gfile.glob(os.path.join(FLAGS.checkpoint_dir,\n                                                     \'**/*.index\'))\n  ensemble_filenames = [filename[:-6] for filename in ensemble_filenames]\n  ensemble_size = len(ensemble_filenames)\n  logging.info(\'Ensemble size: %s\', ensemble_size)\n  logging.info(\'Ensemble number of weights: %s\',\n               ensemble_size * model.count_params())\n  logging.info(\'Ensemble filenames: %s\', str(ensemble_filenames))\n  checkpoint = tf.train.Checkpoint(model=model)\n\n  # Write model predictions to files.\n  num_datasets = len(test_datasets)\n  for m, ensemble_filename in enumerate(ensemble_filenames):\n    checkpoint.restore(ensemble_filename)\n    for n, (name, test_dataset) in enumerate(test_datasets.items()):\n      filename = \'{dataset}_{member}.npy\'.format(dataset=name, member=m)\n      filename = os.path.join(FLAGS.output_dir, filename)\n      if not tf.io.gfile.exists(filename):\n        logits = []\n        test_iterator = iter(test_dataset)\n        for _ in range(steps_per_eval):\n          features, _ = next(test_iterator)  # pytype: disable=attribute-error\n          logits.append(model(features, training=False))\n\n        logits = tf.concat(logits, axis=0)\n        with tf.io.gfile.GFile(filename, \'w\') as f:\n          np.save(f, logits.numpy())\n      percent = (m * num_datasets + (n + 1)) / (ensemble_size * num_datasets)\n      message = (\'{:.1%} completion for prediction: ensemble member {:d}/{:d}. \'\n                 \'Dataset {:d}/{:d}\'.format(percent,\n                                            m + 1,\n                                            ensemble_size,\n                                            n + 1,\n                                            num_datasets))\n      logging.info(message)\n\n  metrics = {\n      \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n      \'test/gibbs_cross_entropy\': tf.keras.metrics.Mean(),\n      \'test/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n      \'test/ece\': ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins),\n  }\n  corrupt_metrics = {}\n  for name in test_datasets:\n    corrupt_metrics[\'test/nll_{}\'.format(name)] = tf.keras.metrics.Mean()\n    corrupt_metrics[\'test/accuracy_{}\'.format(name)] = (\n        tf.keras.metrics.SparseCategoricalAccuracy())\n    corrupt_metrics[\'test/ece_{}\'.format(name)] = (\n        ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins))\n\n  # Evaluate model predictions.\n  for n, (name, test_dataset) in enumerate(test_datasets.items()):\n    logits_dataset = []\n    for m in range(ensemble_size):\n      filename = \'{dataset}_{member}.npy\'.format(dataset=name, member=m)\n      filename = os.path.join(FLAGS.output_dir, filename)\n      with tf.io.gfile.GFile(filename, \'rb\') as f:\n        logits_dataset.append(np.load(f))\n\n    logits_dataset = tf.convert_to_tensor(logits_dataset)\n    test_iterator = iter(test_dataset)\n    for step in range(steps_per_eval):\n      _, labels = next(test_iterator)  # pytype: disable=attribute-error\n      logits = logits_dataset[:, (step*batch_size):((step+1)*batch_size)]\n      labels = tf.cast(labels, tf.int32)\n      negative_log_likelihood = tf.reduce_mean(\n          ensemble_negative_log_likelihood(labels, logits))\n      per_probs = tf.nn.softmax(logits)\n      probs = tf.reduce_mean(per_probs, axis=0)\n      if name == \'clean\':\n        gibbs_ce = tf.reduce_mean(gibbs_cross_entropy(labels, logits))\n        metrics[\'test/negative_log_likelihood\'].update_state(\n            negative_log_likelihood)\n        metrics[\'test/gibbs_cross_entropy\'].update_state(gibbs_ce)\n        metrics[\'test/accuracy\'].update_state(labels, probs)\n        metrics[\'test/ece\'].update_state(labels, probs)\n      else:\n        corrupt_metrics[\'test/nll_{}\'.format(name)].update_state(\n            negative_log_likelihood)\n        corrupt_metrics[\'test/accuracy_{}\'.format(name)].update_state(\n            labels, probs)\n        corrupt_metrics[\'test/ece_{}\'.format(name)].update_state(\n            labels, probs)\n\n    message = (\'{:.1%} completion for evaluation: dataset {:d}/{:d}\'.format(\n        (n + 1) / num_datasets, n + 1, num_datasets))\n    logging.info(message)\n\n  corrupt_results = utils.aggregate_corrupt_metrics(corrupt_metrics,\n                                                    corruption_types,\n                                                    max_intensity)\n  total_results = {name: metric.result() for name, metric in metrics.items()}\n  total_results.update(corrupt_results)\n  logging.info(\'Metrics: %s\', total_results)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/cifar/utils.py,51,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities for CIFAR-10 and CIFAR-100.""""""\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n\ndef load_cifar100_c_input_fn(corruption_name,\n                             corruption_intensity,\n                             batch_size,\n                             use_bfloat16,\n                             path,\n                             drop_remainder=True,\n                             normalize=True):\n  """"""Loads CIFAR-100-C dataset.""""""\n  if use_bfloat16:\n    dtype = tf.bfloat16\n  else:\n    dtype = tf.float32\n  filename = path + \'{0}-{1}.tfrecords\'.format(corruption_name,\n                                               corruption_intensity)\n  def preprocess(serialized_example):\n    """"""Preprocess a serialized example for CIFAR100-C.""""""\n    features = tf.io.parse_single_example(\n        serialized_example,\n        features={\n            \'image\': tf.io.FixedLenFeature([], tf.string),\n            \'label\': tf.io.FixedLenFeature([], tf.int64),\n        })\n    image = tf.io.decode_raw(features[\'image\'], tf.uint8)\n    image = tf.cast(tf.reshape(image, [32, 32, 3]), dtype)\n    image = tf.image.convert_image_dtype(image, dtype)\n    image = image / 255  # to convert into the [0, 1) range\n    if normalize:\n      mean = tf.constant([0.4914, 0.4822, 0.4465], dtype=dtype)\n      std = tf.constant([0.2023, 0.1994, 0.2010], dtype=dtype)\n      image = (image - mean) / std\n    else:\n      # Normalize per-image using mean/stddev computed across pixels.\n      image = tf.image.per_image_standardization(image)\n    label = tf.cast(features[\'label\'], dtype)\n    return image, label\n\n  def input_fn(ctx=None):\n    """"""Returns a locally sharded (i.e., per-core) dataset batch.""""""\n    dataset = tf.data.TFRecordDataset(filename, buffer_size=16 * 1000 * 1000)\n    dataset = dataset.map(\n        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    if ctx and ctx.num_input_pipelines > 1:\n      dataset = dataset.shard(ctx.num_input_pipelines, ctx.input_pipeline_id)\n    return dataset\n  return input_fn\n\n\ndef load_cifar10_c_input_fn(corruption_name,\n                            corruption_intensity,\n                            batch_size,\n                            use_bfloat16,\n                            drop_remainder=True,\n                            normalize=True):\n  """"""Loads CIFAR-10-C dataset.""""""\n  if use_bfloat16:\n    dtype = tf.bfloat16\n  else:\n    dtype = tf.float32\n  corruption = corruption_name + \'_\' + str(corruption_intensity)\n  def preprocess(image, label):\n    image = tf.image.convert_image_dtype(image, dtype)\n    if normalize:\n      mean = tf.constant([0.4914, 0.4822, 0.4465], dtype=dtype)\n      std = tf.constant([0.2023, 0.1994, 0.2010], dtype=dtype)\n      image = (image - mean) / std\n    label = tf.cast(label, dtype)\n    return image, label\n\n  def input_fn(ctx=None):\n    """"""Returns a locally sharded (i.e., per-core) dataset batch.""""""\n    dataset = tfds.load(name=\'cifar10_corrupted/{}\'.format(corruption),\n                        split=tfds.Split.TEST,\n                        as_supervised=True)\n    dataset = dataset.map(\n        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    if ctx and ctx.num_input_pipelines > 1:\n      dataset = dataset.shard(ctx.num_input_pipelines, ctx.input_pipeline_id)\n    return dataset\n  return input_fn\n\n\n# TODO(ghassen,trandustin): Push this metadata upstream to TFDS.\ndef load_corrupted_test_info(dataset):\n  """"""Loads information for CIFAR-10-C.""""""\n  if dataset == \'cifar10\':\n    corruption_types = [\n        \'gaussian_noise\',\n        \'shot_noise\',\n        \'impulse_noise\',\n        \'defocus_blur\',\n        \'frosted_glass_blur\',\n        \'motion_blur\',\n        \'zoom_blur\',\n        \'snow\',\n        \'frost\',\n        \'fog\',\n        \'brightness\',\n        \'contrast\',\n        \'elastic\',\n        \'pixelate\',\n        \'jpeg_compression\',\n    ]\n  else:\n    corruption_types = [\n        \'brightness\',\n        \'contrast\',\n        \'defocus_blur\',\n        \'elastic_transform\',\n        \'fog\',\n        \'frost\',\n        \'glass_blur\',  # Called frosted_glass_blur in CIFAR-10.\n        \'gaussian_blur\',\n        \'gaussian_noise\',\n        \'impulse_noise\',\n        \'jpeg_compression\',\n        \'pixelate\',\n        \'saturate\',\n        \'shot_noise\',\n        \'spatter\',\n        \'speckle_noise\',  # Does not exist for CIFAR-10.\n        \'zoom_blur\',\n    ]\n  max_intensity = 5\n  return corruption_types, max_intensity\n\n\ndef load_input_fn(split,\n                  batch_size,\n                  name,\n                  use_bfloat16,\n                  normalize=True,\n                  drop_remainder=True,\n                  proportion=1.0):\n  """"""Loads CIFAR dataset for training or testing.\n\n  Args:\n    split: tfds.Split.\n    batch_size: The global batch size to use.\n    name: A string indicates whether it is cifar10 or cifar100.\n    use_bfloat16: data type, bfloat16 precision or float32.\n    normalize: Whether to apply mean-std normalization on features.\n    drop_remainder: bool.\n    proportion: float, the proportion of dataset to be used.\n\n  Returns:\n    Input function which returns a locally-sharded dataset batch.\n  """"""\n  if use_bfloat16:\n    dtype = tf.bfloat16\n  else:\n    dtype = tf.float32\n  ds_info = tfds.builder(name).info\n  image_shape = ds_info.features[\'image\'].shape\n  dataset_size = ds_info.splits[\'train\'].num_examples\n\n  def preprocess(image, label):\n    """"""Image preprocessing function.""""""\n    if split == tfds.Split.TRAIN:\n      image = tf.image.resize_with_crop_or_pad(\n          image, image_shape[0] + 4, image_shape[1] + 4)\n      image = tf.image.random_crop(image, image_shape)\n      image = tf.image.random_flip_left_right(image)\n\n    image = tf.image.convert_image_dtype(image, dtype)\n    if normalize:\n      mean = tf.constant([0.4914, 0.4822, 0.4465], dtype=dtype)\n      std = tf.constant([0.2023, 0.1994, 0.2010], dtype=dtype)\n      image = (image - mean) / std\n    label = tf.cast(label, dtype)\n    return image, label\n\n  def input_fn(ctx=None):\n    """"""Returns a locally sharded (i.e., per-core) dataset batch.""""""\n    if proportion == 1.0:\n      dataset = tfds.load(name, split=split, as_supervised=True)\n    else:\n      new_name = \'{}:3.*.*\'.format(name)\n      if split == tfds.Split.TRAIN:\n        new_split = \'train[:{}%]\'.format(int(100 * proportion))\n      else:\n        new_split = \'test[:{}%]\'.format(int(100 * proportion))\n      dataset = tfds.load(new_name, split=new_split, as_supervised=True)\n    if split == tfds.Split.TRAIN:\n      dataset = dataset.shuffle(buffer_size=dataset_size).repeat()\n\n    dataset = dataset.map(preprocess,\n                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    if ctx and ctx.num_input_pipelines > 1:\n      dataset = dataset.shard(ctx.num_input_pipelines, ctx.input_pipeline_id)\n    return dataset\n  return input_fn\n\n\nclass LearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n  """"""Learning rate schedule.\n\n  It starts with a linear warmup to the initial learning rate over\n  `warmup_epochs`. This is found to be helpful for large batch size training\n  (Goyal et al., 2018). The learning rate\'s value then uses the initial\n  learning rate, and decays by a multiplier at the start of each epoch in\n  `decay_epochs`. The stepwise decaying schedule follows He et al. (2015).\n  """"""\n\n  def __init__(self,\n               steps_per_epoch,\n               initial_learning_rate,\n               decay_ratio,\n               decay_epochs,\n               warmup_epochs):\n    super(LearningRateSchedule, self).__init__()\n    self.steps_per_epoch = steps_per_epoch\n    self.initial_learning_rate = initial_learning_rate\n    self.decay_ratio = decay_ratio\n    self.decay_epochs = decay_epochs\n    self.warmup_epochs = warmup_epochs\n\n  def __call__(self, step):\n    lr_epoch = tf.cast(step, tf.float32) / self.steps_per_epoch\n    learning_rate = self.initial_learning_rate\n    if self.warmup_epochs >= 1:\n      learning_rate *= lr_epoch / self.warmup_epochs\n    decay_epochs = [self.warmup_epochs] + self.decay_epochs\n    for index, start_epoch in enumerate(decay_epochs):\n      learning_rate = tf.where(\n          lr_epoch >= start_epoch,\n          self.initial_learning_rate * self.decay_ratio**index,\n          learning_rate)\n    return learning_rate\n\n  def get_config(self):\n    return {\n        \'steps_per_epoch\': self.steps_per_epoch,\n        \'initial_learning_rate\': self.initial_learning_rate,\n    }\n\n\ndef aggregate_corrupt_metrics(metrics,\n                              corruption_types,\n                              max_intensity,\n                              log_fine_metrics=False,\n                              corrupt_diversity=None,\n                              output_dir=None):\n  """"""Aggregates metrics across intensities and corruption types.\n\n  Args:\n    metrics: Dictionary of tf.keras.metrics to be aggregated.\n    corruption_types: List of corruption types.\n    max_intensity: Int, of maximum intensity.\n    log_fine_metrics: Bool, whether log fine metrics to main training script.\n    corrupt_diversity: Dictionary of diversity metrics on corrupted datasets.\n    output_dir: Str, the path to save the aggregated results.\n\n  Returns:\n    Dictionary of aggregated results.\n\n  """"""\n  diversity_keys = [\'disagreement\', \'cosine_similarity\', \'average_kl\']\n  results = {\n      \'test/nll_mean_corrupted\': 0.,\n      \'test/accuracy_mean_corrupted\': 0.,\n      \'test/ece_mean_corrupted\': 0.,\n      \'test/member_acc_mean_corrupted\': 0.,\n      \'test/member_ece_mean_corrupted\': 0.\n  }\n  fine_metrics_results = {}\n  if corrupt_diversity is not None:\n    for key in diversity_keys:\n      results[\'corrupt_diversity/{}_mean_corrupted\'.format(key)] = 0.\n\n  for intensity in range(1, max_intensity + 1):\n    ece = np.zeros(len(corruption_types))\n    nll = np.zeros(len(corruption_types))\n    acc = np.zeros(len(corruption_types))\n    member_acc = np.zeros(len(corruption_types))\n    member_ece = np.zeros(len(corruption_types))\n    disagreement = np.zeros(len(corruption_types))\n    cosine_similarity = np.zeros(len(corruption_types))\n    average_kl = np.zeros(len(corruption_types))\n\n    for i in range(len(corruption_types)):\n      dataset_name = \'{0}_{1}\'.format(corruption_types[i], intensity)\n      nll[i] = metrics[\'test/nll_{}\'.format(dataset_name)].result()\n      acc[i] = metrics[\'test/accuracy_{}\'.format(dataset_name)].result()\n      ece[i] = metrics[\'test/ece_{}\'.format(dataset_name)].result()\n      if \'test/member_acc_mean_{}\'.format(dataset_name) in metrics.keys():\n        member_acc[i] = metrics[\'test/member_acc_mean_{}\'.format(\n            dataset_name)].result()\n      else:\n        member_acc[i] = 0.\n      if \'test/member_ece_mean_{}\'.format(dataset_name) in metrics.keys():\n        member_ece[i] = metrics[\'test/member_ece_mean_{}\'.format(\n            dataset_name)].result()\n        member_ece[i] = 0.\n      if corrupt_diversity is not None:\n        disagreement[i] = (\n            corrupt_diversity[\'corrupt_diversity/disagreement_{}\'.format(\n                dataset_name)].result())\n        # Normalize the corrupt disagreement by its error rate.\n        error = 1 - acc[i] + tf.keras.backend.epsilon()\n        cosine_similarity[i] = (\n            corrupt_diversity[\'corrupt_diversity/cosine_similarity_{}\'.format(\n                dataset_name)].result()) / error\n        average_kl[i] = (\n            corrupt_diversity[\'corrupt_diversity/average_kl_{}\'.format(\n                dataset_name)].result())\n      if log_fine_metrics or output_dir is not None:\n        fine_metrics_results[\'test/nll_{}\'.format(dataset_name)] = nll[i]\n        fine_metrics_results[\'test/accuracy_{}\'.format(dataset_name)] = acc[i]\n        fine_metrics_results[\'test/ece_{}\'.format(dataset_name)] = ece[i]\n        if corrupt_diversity is not None:\n          fine_metrics_results[\'corrupt_diversity/disagreement_{}\'.format(\n              dataset_name)] = disagreement[i]\n          fine_metrics_results[\'corrupt_diversity/cosine_similarity_{}\'.format(\n              dataset_name)] = cosine_similarity[i]\n          fine_metrics_results[\'corrupt_diversity/average_kl_{}\'.format(\n              dataset_name)] = average_kl[i]\n    avg_nll = np.mean(nll)\n    avg_accuracy = np.mean(acc)\n    avg_ece = np.mean(ece)\n    avg_member_acc = np.mean(member_acc)\n    avg_member_ece = np.mean(member_ece)\n    results[\'test/nll_mean_{}\'.format(intensity)] = avg_nll\n    results[\'test/accuracy_mean_{}\'.format(intensity)] = avg_accuracy\n    results[\'test/ece_mean_{}\'.format(intensity)] = avg_ece\n    results[\'test/nll_median_{}\'.format(intensity)] = np.median(nll)\n    results[\'test/accuracy_median_{}\'.format(intensity)] = np.median(acc)\n    results[\'test/ece_median_{}\'.format(intensity)] = np.median(ece)\n    results[\'test/member_acc_mean_{}\'.format(intensity)] = avg_member_acc\n    results[\'test/member_ece_mean_{}\'.format(intensity)] = avg_member_ece\n    results[\'test/nll_mean_corrupted\'] += avg_nll\n    results[\'test/accuracy_mean_corrupted\'] += avg_accuracy\n    results[\'test/ece_mean_corrupted\'] += avg_ece\n    results[\'test/member_acc_mean_corrupted\'] += avg_member_acc\n    results[\'test/member_ece_mean_corrupted\'] += avg_member_ece\n    if corrupt_diversity is not None:\n      avg_diversity_metrics = [np.mean(disagreement), np.mean(\n          cosine_similarity), np.mean(average_kl)]\n      for key, avg in zip(diversity_keys, avg_diversity_metrics):\n        results[\'corrupt_diversity/{}_mean_{}\'.format(\n            key, intensity)] = avg\n        results[\'corrupt_diversity/{}_mean_corrupted\'.format(key)] += avg\n\n  results[\'test/nll_mean_corrupted\'] /= max_intensity\n  results[\'test/accuracy_mean_corrupted\'] /= max_intensity\n  results[\'test/ece_mean_corrupted\'] /= max_intensity\n  results[\'test/member_acc_mean_corrupted\'] /= max_intensity\n  results[\'test/member_ece_mean_corrupted\'] /= max_intensity\n  if corrupt_diversity is not None:\n    for key in diversity_keys:\n      results[\'corrupt_diversity/{}_mean_corrupted\'.format(\n          key)] /= max_intensity\n\n  fine_metrics_results.update(results)\n  if output_dir is not None:\n    save_file_name = os.path.join(output_dir, \'corrupt_metrics.npz\')\n    with tf.io.gfile.GFile(save_file_name, \'w\') as f:\n      np.save(f, fine_metrics_results)\n\n  if log_fine_metrics:\n    return fine_metrics_results\n  else:\n    return results\n\n\ndef double_fault(logits_1, logits_2, labels):\n  """"""Double fault [1] is the number of examples both classifiers predict wrong.\n\n  Args:\n    logits_1: tf.Tensor.\n    logits_2: tf.Tensor.\n    labels: tf.Tensor.\n\n  Returns:\n    Scalar double-fault diversity metric.\n\n  ## References\n\n  [1] Kuncheva, Ludmila I., and Christopher J. Whitaker. ""Measures of diversity\n      in classifier ensembles and their relationship with the ensemble\n      accuracy."" Machine learning 51.2 (2003): 181-207.\n  """"""\n  preds_1 = tf.cast(tf.argmax(logits_1, axis=-1), labels.dtype)\n  preds_2 = tf.cast(tf.argmax(logits_2, axis=-1), labels.dtype)\n\n  fault_1_idx = tf.squeeze(tf.where(preds_1 != labels))\n  fault_1_idx = tf.cast(fault_1_idx, tf.int32)\n\n  preds_2_at_idx = tf.gather(preds_2, fault_1_idx)\n  labels_at_idx = tf.gather(labels, fault_1_idx)\n\n  double_faults = preds_2_at_idx != labels_at_idx\n  double_faults = tf.cast(double_faults, tf.float32)\n  return tf.reduce_mean(double_faults)\n'"
baselines/cifar/variational_inference.py,60,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Variational inference for Wide ResNet 28-10 on CIFAR-10 and CIFAR-100.\n\nThis script performs variational inference with a few notable techniques:\n\n1. Normal prior whose mean is tied at the variational posterior\'s. This makes\n   the KL penalty only penalize the weight posterior\'s standard deviation and\n   not its mean. The prior\'s standard deviation is a fixed hyperparameter.\n2. Fully factorized normal variational distribution (Blundell et al., 2015).\n3. Flipout for lower-variance gradients in convolutional layers and the final\n   dense layer (Wen et al., 2018).\n4. KL annealing (Bowman et al., 2015).\n""""""\n\nimport functools\nimport os\nimport time\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport utils  # local file import\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nflags.DEFINE_integer(\'seed\', 42, \'Random seed.\')\nflags.DEFINE_integer(\'per_core_batch_size\', 64, \'Batch size per TPU core/GPU.\')\nflags.DEFINE_float(\'base_learning_rate\', 0.1,\n                   \'Base learning rate when total batch size is 128. It is \'\n                   \'scaled by the ratio of the total batch size to 128.\')\nflags.DEFINE_integer(\'lr_warmup_epochs\', 1,\n                     \'Number of epochs for a linear warmup to the initial \'\n                     \'learning rate. Use 0 to do no warmup.\')\nflags.DEFINE_float(\'lr_decay_ratio\', 0.2, \'Amount to decay learning rate.\')\nflags.DEFINE_list(\'lr_decay_epochs\', [\'60\', \'120\', \'160\'],\n                  \'Epochs to decay learning rate by.\')\nflags.DEFINE_integer(\'kl_annealing_epochs\', 200,\n                     \'Number of epoch over which to anneal the KL term to 1.\')\nflags.DEFINE_float(\'l2\', 4e-4, \'L2 regularization coefficient.\')\nflags.DEFINE_float(\'prior_stddev\', 0.1, \'Fixed stddev for weight prior.\')\nflags.DEFINE_float(\'stddev_init\', 1e-3,\n                   \'Initialization of posterior standard deviation parameters.\')\nflags.DEFINE_enum(\'dataset\', \'cifar10\',\n                  enum_values=[\'cifar10\', \'cifar100\'],\n                  help=\'Dataset.\')\n# TODO(ghassen): consider adding CIFAR-100-C to TFDS.\nflags.DEFINE_string(\'cifar100_c_path\', None,\n                    \'Path to the TFRecords files for CIFAR-100-C. Only valid \'\n                    \'(and required) if dataset is cifar100 and corruptions.\')\nflags.DEFINE_integer(\'corruptions_interval\', 250,\n                     \'Number of epochs between evaluating on the corrupted \'\n                     \'test data. Use -1 to never evaluate.\')\nflags.DEFINE_integer(\'checkpoint_interval\', 25,\n                     \'Number of epochs between saving checkpoints. Use -1 to \'\n                     \'never save checkpoints.\')\nflags.DEFINE_integer(\'num_bins\', 15, \'Number of bins for ECE.\')\nflags.DEFINE_integer(\'num_eval_samples\', 5,\n                     \'Number of samples per example during evaluation. Only \'\n                     \'used for corrupted predicitons.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/cifar\', \'Output directory.\')\nflags.DEFINE_integer(\'train_epochs\', 250, \'Number of training epochs.\')\n\n# Accelerator flags.\nflags.DEFINE_bool(\'use_gpu\', False, \'Whether to run on GPU or otherwise TPU.\')\nflags.DEFINE_bool(\'use_bfloat16\', False, \'Whether to use mixed precision.\')\nflags.DEFINE_integer(\'num_cores\', 8, \'Number of TPU cores or number of GPUs.\')\nflags.DEFINE_string(\'tpu\', None,\n                    \'Name of the TPU. Only used if use_gpu is False.\')\nFLAGS = flags.FLAGS\n\nBatchNormalization = functools.partial(  # pylint: disable=invalid-name\n    tf.keras.layers.BatchNormalization,\n    epsilon=1e-5,  # using epsilon and momentum defaults from Torch\n    momentum=0.9)\nConv2DFlipout = functools.partial(  # pylint: disable=invalid-name\n    ed.layers.Conv2DFlipout,\n    kernel_size=3,\n    padding=\'same\',\n    use_bias=False)\n\n\nclass NormalKLDivergenceWithTiedMean(tf.keras.regularizers.Regularizer):\n  """"""KL with normal prior whose mean is fixed at the variational posterior\'s.""""""\n\n  def __init__(self, stddev=1., scale_factor=1.):\n    """"""Constructs regularizer.""""""\n    self.stddev = stddev\n    self.scale_factor = scale_factor\n\n  def __call__(self, x):\n    """"""Computes regularization given an ed.Normal random variable as input.""""""\n    if not isinstance(x, ed.RandomVariable):\n      raise ValueError(\'Input must be an ed.RandomVariable.\')\n    prior = ed.Independent(\n        ed.Normal(loc=x.distribution.mean(), scale=self.stddev).distribution,\n        reinterpreted_batch_ndims=len(x.distribution.event_shape))\n    regularization = x.distribution.kl_divergence(prior.distribution)\n    return self.scale_factor * regularization\n\n  def get_config(self):\n    return {\n        \'stddev\': self.stddev,\n        \'scale_factor\': self.scale_factor,\n    }\n\n\ndef basic_block(inputs, filters, strides, prior_stddev, dataset_size,\n                stddev_init):\n  """"""Basic residual block of two 3x3 convs.\n\n  Args:\n    inputs: tf.Tensor.\n    filters: Number of filters for Conv2D.\n    strides: Stride dimensions for Conv2D.\n    prior_stddev: Fixed standard deviation for weight prior.\n    dataset_size: Dataset size to properly scale the KL.\n    stddev_init: float to initialize variational posterior stddev parameters.\n\n  Returns:\n    tf.Tensor.\n  """"""\n  x = inputs\n  y = inputs\n  y = BatchNormalization()(y)\n  y = tf.keras.layers.Activation(\'relu\')(y)\n  y = Conv2DFlipout(\n      filters,\n      strides=strides,\n      kernel_initializer=ed.initializers.TrainableHeNormal(\n          stddev_initializer=tf.keras.initializers.TruncatedNormal(\n              mean=np.log(np.expm1(stddev_init)), stddev=0.1)),\n      kernel_regularizer=NormalKLDivergenceWithTiedMean(\n          stddev=prior_stddev, scale_factor=1./dataset_size))(y)\n  y = BatchNormalization()(y)\n  y = tf.keras.layers.Activation(\'relu\')(y)\n  y = Conv2DFlipout(\n      filters,\n      strides=1,\n      kernel_initializer=ed.initializers.TrainableHeNormal(\n          stddev_initializer=tf.keras.initializers.TruncatedNormal(\n              mean=np.log(np.expm1(stddev_init)), stddev=0.1)),\n      kernel_regularizer=NormalKLDivergenceWithTiedMean(\n          stddev=prior_stddev, scale_factor=1./dataset_size))(y)\n  if not x.shape.is_compatible_with(y.shape):\n    x = Conv2DFlipout(\n        filters,\n        kernel_size=1,\n        strides=strides,\n        kernel_initializer=ed.initializers.TrainableHeNormal(\n            stddev_initializer=tf.keras.initializers.TruncatedNormal(\n                mean=np.log(np.expm1(stddev_init)), stddev=0.1)),\n        kernel_regularizer=NormalKLDivergenceWithTiedMean(\n            stddev=prior_stddev, scale_factor=1./dataset_size))(x)\n  x = tf.keras.layers.add([x, y])\n  return x\n\n\ndef group(inputs, filters, strides, num_blocks, **kwargs):\n  """"""Group of residual blocks.""""""\n  x = basic_block(inputs, filters=filters, strides=strides, **kwargs)\n  for _ in range(num_blocks - 1):\n    x = basic_block(x, filters=filters, strides=1, **kwargs)\n  return x\n\n\ndef wide_resnet(input_shape, depth, width_multiplier, num_classes,\n                prior_stddev, dataset_size,\n                stddev_init):\n  """"""Builds Wide ResNet.\n\n  Following Zagoruyko and Komodakis (2016), it accepts a width multiplier on the\n  number of filters. Using three groups of residual blocks, the network maps\n  spatial features of size 32x32 -> 16x16 -> 8x8.\n\n  Args:\n    input_shape: tf.Tensor.\n    depth: Total number of convolutional layers. ""n"" in WRN-n-k. It differs from\n      He et al. (2015)\'s notation which uses the maximum depth of the network\n      counting non-conv layers like dense.\n    width_multiplier: Integer to multiply the number of typical filters by. ""k""\n      in WRN-n-k.\n    num_classes: Number of output classes.\n    prior_stddev: Fixed standard deviation for weight prior.\n    dataset_size: Dataset size to properly scale the KL.\n    stddev_init: float to initialize variational posterior stddev parameters.\n\n  Returns:\n    tf.keras.Model.\n  """"""\n  if (depth - 4) % 6 != 0:\n    raise ValueError(\'depth should be 6n+4 (e.g., 16, 22, 28, 40).\')\n  num_blocks = (depth - 4) // 6\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  x = Conv2DFlipout(\n      16,\n      strides=1,\n      kernel_initializer=ed.initializers.TrainableHeNormal(\n          stddev_initializer=tf.keras.initializers.TruncatedNormal(\n              mean=np.log(np.expm1(stddev_init)), stddev=0.1)),\n      kernel_regularizer=NormalKLDivergenceWithTiedMean(\n          stddev=prior_stddev, scale_factor=1./dataset_size))(inputs)\n  for strides, filters in zip([1, 2, 2], [16, 32, 64]):\n    x = group(x,\n              filters=filters * width_multiplier,\n              strides=strides,\n              num_blocks=num_blocks,\n              prior_stddev=prior_stddev,\n              dataset_size=dataset_size,\n              stddev_init=stddev_init)\n\n  x = BatchNormalization()(x)\n  x = tf.keras.layers.Activation(\'relu\')(x)\n  x = tf.keras.layers.AveragePooling2D(pool_size=8)(x)\n  x = tf.keras.layers.Flatten()(x)\n  x = ed.layers.DenseFlipout(\n      num_classes,\n      kernel_initializer=ed.initializers.TrainableHeNormal(\n          stddev_initializer=tf.keras.initializers.TruncatedNormal(\n              mean=np.log(np.expm1(stddev_init)), stddev=0.1)),\n      kernel_regularizer=NormalKLDivergenceWithTiedMean(\n          stddev=prior_stddev, scale_factor=1./dataset_size))(x)\n  return tf.keras.Model(inputs=inputs, outputs=x)\n\n\ndef main(argv):\n  del argv  # unused arg\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  logging.info(\'Saving checkpoints at %s\', FLAGS.output_dir)\n  tf.random.set_seed(FLAGS.seed)\n\n  if FLAGS.use_gpu:\n    logging.info(\'Use GPU\')\n    strategy = tf.distribute.MirroredStrategy()\n  else:\n    logging.info(\'Use TPU at %s\',\n                 FLAGS.tpu if FLAGS.tpu is not None else \'local\')\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n  train_input_fn = utils.load_input_fn(\n      split=tfds.Split.TRAIN,\n      name=FLAGS.dataset,\n      batch_size=FLAGS.per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  clean_test_input_fn = utils.load_input_fn(\n      split=tfds.Split.TEST,\n      name=FLAGS.dataset,\n      batch_size=FLAGS.per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  train_dataset = strategy.experimental_distribute_datasets_from_function(\n      train_input_fn)\n\n  test_datasets = {\n      \'clean\': strategy.experimental_distribute_datasets_from_function(\n          clean_test_input_fn),\n  }\n  if FLAGS.corruptions_interval > 0:\n    if FLAGS.dataset == \'cifar10\':\n      load_c_input_fn = utils.load_cifar10_c_input_fn\n    else:\n      load_c_input_fn = functools.partial(utils.load_cifar100_c_input_fn,\n                                          path=FLAGS.cifar100_c_path)\n    corruption_types, max_intensity = utils.load_corrupted_test_info(\n        FLAGS.dataset)\n    for corruption in corruption_types:\n      for intensity in range(1, max_intensity + 1):\n        input_fn = load_c_input_fn(\n            corruption_name=corruption,\n            corruption_intensity=intensity,\n            batch_size=FLAGS.per_core_batch_size,\n            use_bfloat16=FLAGS.use_bfloat16)\n        test_datasets[\'{0}_{1}\'.format(corruption, intensity)] = (\n            strategy.experimental_distribute_datasets_from_function(input_fn))\n\n  ds_info = tfds.builder(FLAGS.dataset).info\n  batch_size = FLAGS.per_core_batch_size * FLAGS.num_cores\n  train_dataset_size = ds_info.splits[\'train\'].num_examples\n  steps_per_epoch = train_dataset_size // batch_size\n  steps_per_eval = ds_info.splits[\'test\'].num_examples // batch_size\n  num_classes = ds_info.features[\'label\'].num_classes\n\n  if FLAGS.use_bfloat16:\n    policy = tf.keras.mixed_precision.experimental.Policy(\'mixed_bfloat16\')\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n\n  summary_writer = tf.summary.create_file_writer(\n      os.path.join(FLAGS.output_dir, \'summaries\'))\n\n  with strategy.scope():\n    logging.info(\'Building ResNet model\')\n    model = wide_resnet(input_shape=ds_info.features[\'image\'].shape,\n                        depth=28,\n                        width_multiplier=10,\n                        num_classes=num_classes,\n                        prior_stddev=FLAGS.prior_stddev,\n                        dataset_size=train_dataset_size,\n                        stddev_init=FLAGS.stddev_init)\n    logging.info(\'Model input shape: %s\', model.input_shape)\n    logging.info(\'Model output shape: %s\', model.output_shape)\n    logging.info(\'Model number of weights: %s\', model.count_params())\n    # Linearly scale learning rate and the decay epochs by vanilla settings.\n    base_lr = FLAGS.base_learning_rate * batch_size / 128\n    lr_decay_epochs = [(int(start_epoch_str) * FLAGS.train_epochs) // 200\n                       for start_epoch_str in FLAGS.lr_decay_epochs]\n    lr_schedule = utils.LearningRateSchedule(\n        steps_per_epoch,\n        base_lr,\n        decay_ratio=FLAGS.lr_decay_ratio,\n        decay_epochs=lr_decay_epochs,\n        warmup_epochs=FLAGS.lr_warmup_epochs)\n    optimizer = tf.keras.optimizers.SGD(lr_schedule,\n                                        momentum=0.9,\n                                        nesterov=True)\n    metrics = {\n        \'train/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'train/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'train/loss\': tf.keras.metrics.Mean(),\n        \'train/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        \'train/kl\': tf.keras.metrics.Mean(),\n        \'train/kl_scale\': tf.keras.metrics.Mean(),\n        \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'test/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'test/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n    }\n    if FLAGS.corruptions_interval > 0:\n      corrupt_metrics = {}\n      for intensity in range(1, max_intensity + 1):\n        for corruption in corruption_types:\n          dataset_name = \'{0}_{1}\'.format(corruption, intensity)\n          corrupt_metrics[\'test/nll_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.Mean())\n          corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.SparseCategoricalAccuracy())\n          corrupt_metrics[\'test/ece_{}\'.format(dataset_name)] = (\n              ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins))\n\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n    initial_epoch = 0\n    if latest_checkpoint:\n      # checkpoint.restore must be within a strategy.scope() so that optimizer\n      # slot variables are mirrored.\n      checkpoint.restore(latest_checkpoint)\n      logging.info(\'Loaded checkpoint %s\', latest_checkpoint)\n      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n\n  @tf.function\n  def train_step(iterator):\n    """"""Training StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      with tf.GradientTape() as tape:\n        logits = model(images, training=True)\n        if FLAGS.use_bfloat16:\n          logits = tf.cast(logits, tf.float32)\n        negative_log_likelihood = tf.reduce_mean(\n            tf.keras.losses.sparse_categorical_crossentropy(labels,\n                                                            logits,\n                                                            from_logits=True))\n\n        filtered_variables = []\n        for var in model.trainable_variables:\n          # Apply l2 on the BN parameters and bias terms. This\n          # excludes only fast weight approximate posterior/prior parameters,\n          # but pay caution to their naming scheme.\n          if \'batch_norm\' in var.name or \'bias\' in var.name:\n            filtered_variables.append(tf.reshape(var, (-1,)))\n\n        l2_loss = FLAGS.l2 * 2 * tf.nn.l2_loss(\n            tf.concat(filtered_variables, axis=0))\n        kl = sum(model.losses)\n        kl_scale = tf.cast(optimizer.iterations + 1, kl.dtype)\n        kl_scale /= steps_per_epoch * FLAGS.kl_annealing_epochs\n        kl_scale = tf.minimum(1., kl_scale)\n        kl_loss = kl_scale * kl\n\n        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n        loss = negative_log_likelihood + l2_loss + kl_loss\n        scaled_loss = loss / strategy.num_replicas_in_sync\n\n      grads = tape.gradient(scaled_loss, model.trainable_variables)\n      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n      probs = tf.nn.softmax(logits)\n      metrics[\'train/ece\'].update_state(labels, probs)\n      metrics[\'train/loss\'].update_state(loss)\n      metrics[\'train/negative_log_likelihood\'].update_state(\n          negative_log_likelihood)\n      metrics[\'train/kl\'].update_state(kl)\n      metrics[\'train/kl_scale\'].update_state(kl_scale)\n      metrics[\'train/accuracy\'].update_state(labels, logits)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  @tf.function\n  def test_step(iterator, dataset_name):\n    """"""Evaluation StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      # TODO(trandustin): Use more eval samples only on corrupted predictions;\n      # it\'s expensive but a one-time compute if scheduled post-training.\n      if FLAGS.num_eval_samples > 1 and dataset_name != \'clean\':\n        logits = tf.stack([model(images, training=False)\n                           for _ in range(FLAGS.num_eval_samples)], axis=0)\n      else:\n        logits = model(images, training=False)\n      if FLAGS.use_bfloat16:\n        logits = tf.cast(logits, tf.float32)\n      probs = tf.nn.softmax(logits)\n      if FLAGS.num_eval_samples > 1 and dataset_name != \'clean\':\n        probs = tf.reduce_mean(probs, axis=0)\n      negative_log_likelihood = tf.reduce_mean(\n          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n\n      if dataset_name == \'clean\':\n        metrics[\'test/negative_log_likelihood\'].update_state(\n            negative_log_likelihood)\n        metrics[\'test/accuracy\'].update_state(labels, probs)\n        metrics[\'test/ece\'].update_state(labels, probs)\n      else:\n        corrupt_metrics[\'test/nll_{}\'.format(dataset_name)].update_state(\n            negative_log_likelihood)\n        corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n        corrupt_metrics[\'test/ece_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  train_iterator = iter(train_dataset)\n  start_time = time.time()\n  for epoch in range(initial_epoch, FLAGS.train_epochs):\n    logging.info(\'Starting to run epoch: %s\', epoch)\n    for step in range(steps_per_epoch):\n      train_step(train_iterator)\n\n      current_step = epoch * steps_per_epoch + (step + 1)\n      max_steps = steps_per_epoch * FLAGS.train_epochs\n      time_elapsed = time.time() - start_time\n      steps_per_sec = float(current_step) / time_elapsed\n      eta_seconds = (max_steps - current_step) / steps_per_sec\n      message = (\'{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. \'\n                 \'ETA: {:.0f} min. Time elapsed: {:.0f} min\'.format(\n                     current_step / max_steps,\n                     epoch + 1,\n                     FLAGS.train_epochs,\n                     steps_per_sec,\n                     eta_seconds / 60,\n                     time_elapsed / 60))\n      if step % 20 == 0:\n        logging.info(message)\n\n    datasets_to_evaluate = {\'clean\': test_datasets[\'clean\']}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      datasets_to_evaluate = test_datasets\n    for dataset_name, test_dataset in datasets_to_evaluate.items():\n      test_iterator = iter(test_dataset)\n      logging.info(\'Testing on dataset %s\', dataset_name)\n      for step in range(steps_per_eval):\n        if step % 20 == 0:\n          logging.info(\'Starting to run eval step %s of epoch: %s\', step,\n                       epoch)\n        test_step(test_iterator, dataset_name)\n      logging.info(\'Done with testing on %s\', dataset_name)\n\n    corrupt_results = {}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      corrupt_results = utils.aggregate_corrupt_metrics(corrupt_metrics,\n                                                        corruption_types,\n                                                        max_intensity)\n\n    logging.info(\'Train Loss: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'train/loss\'].result(),\n                 metrics[\'train/accuracy\'].result() * 100)\n    logging.info(\'Test NLL: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'test/negative_log_likelihood\'].result(),\n                 metrics[\'test/accuracy\'].result() * 100)\n    total_results = {name: metric.result() for name, metric in metrics.items()}\n    total_results.update(corrupt_results)\n    with summary_writer.as_default():\n      for name, result in total_results.items():\n        tf.summary.scalar(name, result, step=epoch + 1)\n\n    for metric in metrics.values():\n      metric.reset_states()\n\n    if (FLAGS.checkpoint_interval > 0 and\n        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n      checkpoint_name = checkpoint.save(\n          os.path.join(FLAGS.output_dir, \'checkpoint\'))\n      logging.info(\'Saved checkpoint to %s\', checkpoint_name)\n\n  final_checkpoint_name = checkpoint.save(\n      os.path.join(FLAGS.output_dir, \'checkpoint\'))\n  logging.info(\'Saved last checkpoint to %s\', final_checkpoint_name)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/cifar/variational_inference_test.py,10,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for variational inference.""""""\n\nimport variational_inference  # local file import\nimport tensorflow as tf\n\n\nclass VariationalInferenceTest(tf.test.TestCase):\n\n  def testResNetV1(self):\n    tf.random.set_seed(83922)\n    dataset_size = 10\n    batch_size = 5\n    input_shape = (32, 32, 1)\n    num_classes = 2\n\n    features = tf.random.normal((dataset_size,) + input_shape)\n    coeffs = tf.random.normal([tf.reduce_prod(input_shape), num_classes])\n    net = tf.reshape(features, [dataset_size, -1])\n    logits = tf.matmul(net, coeffs)\n    labels = tf.random.categorical(logits, 1)\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n    dataset = dataset.repeat().shuffle(dataset_size).batch(batch_size)\n\n    model = variational_inference.wide_resnet(input_shape=input_shape,\n                                              depth=10,\n                                              width_multiplier=1,\n                                              num_classes=num_classes,\n                                              prior_stddev=0.1,\n                                              dataset_size=dataset_size,\n                                              stddev_init=1e-3)\n    model.compile(\n        \'adam\',\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n    history = model.fit(dataset,\n                        steps_per_epoch=dataset_size // batch_size,\n                        epochs=2)\n\n    loss_history = history.history[\'loss\']\n    self.assertAllGreaterEqual(loss_history, 0.)\n    self.assertGreater(loss_history[0], loss_history[-1])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
baselines/clinc_intent/deterministic.py,34,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""TextCNN trained with maximum likelihood.""""""\n\nimport os\nimport time\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport deterministic_model as cnn_model  # local file import\nimport deterministic_model_bert as bert_model  # local file import\n\nimport numpy as np\nimport tensorflow as tf\nimport uncertainty_baselines as ub\n\n# Data flags\nflags.DEFINE_string(\n    \'dataset_dir\', None,\n    \'Directory containing the TFRecord datasets and the tokenizer for Clinc \'\n    \'Intent Detection Data.\')\n\n# Model flags\nflags.DEFINE_string(\'model_family\', \'textcnn\',\n                    \'Types of model to use. Can be either TextCNN or BERT.\')\n\n# Model flags, TextCNN\nflags.DEFINE_list(\'filter_sizes\', [\'3\', \'4\', \'5\'],\n                  \'Comma-separated filter sizes.\')\nflags.DEFINE_integer(\'num_filters\', 64, \'Number of filters per filter size.\')\nflags.DEFINE_integer(\'embedding_size\', 300,\n                     \'Dimensionality of character embedding.\')\nflags.DEFINE_float(\'dropout_rate\', 0.2,\n                   \'Fraction of units to drop in the convolutional layers.\')\nflags.DEFINE_float(\'l2\', 1e-4,\n                   \'L2 regularization coefficient for the output layer.\')\nflags.DEFINE_string(\'word_embedding_dir\', None,\n                    \'Directory to word embedding npy file.\')\n\n# Model flags, BERT.\nflags.DEFINE_string(\n    \'bert_dir\', None,\n    \'Directory to BERT pre-trained checkpoints and config files.\')\nflags.DEFINE_string(\n    \'bert_ckpt_dir\', None, \'Directory to BERT pre-trained checkpoints. \'\n    \'If None then then default to {bert_dir}/bert_model.ckpt.\')\nflags.DEFINE_string(\n    \'bert_config_dir\', None, \'Directory to BERT config files. \'\n    \'If None then then default to {bert_dir}/bert_config.json.\')\n\n# Optimization and evaluation flags\nflags.DEFINE_integer(\'seed\', 42, \'Random seed.\')\nflags.DEFINE_integer(\'per_core_batch_size\', 64, \'Batch size per TPU core/GPU.\')\nflags.DEFINE_float(\n    \'base_learning_rate\', 0.0005,\n    \'Base learning rate when total batch size is 128. It is \'\n    \'scaled by the ratio of the total batch size to 128.\')\nflags.DEFINE_integer(\n    \'checkpoint_interval\', 25,\n    \'Number of epochs between saving checkpoints. Use -1 to \'\n    \'never save checkpoints.\')\nflags.DEFINE_integer(\'evaluation_interval\', 50,\n                     \'Number of epochs between evaluation.\')\nflags.DEFINE_integer(\'num_bins\', 15, \'Number of bins for ECE.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/clinc_intent\', \'Output directory.\')\nflags.DEFINE_integer(\'train_epochs\', 1000, \'Number of training epochs.\')\nflags.DEFINE_float(\n    \'warmup_proportion\', 0.1,\n    \'Proportion of training to perform linear learning rate warmup for. \'\n    \'E.g., 0.1 = 10% of training.\')\n\n# Accelerator flags.\nflags.DEFINE_bool(\'use_gpu\', False, \'Whether to run on GPU or otherwise TPU.\')\nflags.DEFINE_bool(\'use_bfloat16\', False, \'Whether to use mixed precision.\')\nflags.DEFINE_integer(\'num_cores\', 8, \'Number of TPU cores or number of GPUs.\')\nflags.DEFINE_string(\'tpu\', None,\n                    \'Name of the TPU. Only used if use_gpu is False.\')\n\nFLAGS = flags.FLAGS\n\n\ndef resolve_bert_ckpt_and_config_dir(bert_dir, bert_config_dir, bert_ckpt_dir):\n  """"""Resolves BERT checkpoint and config file directories.""""""\n\n  missing_ckpt_or_config_dir = not (bert_ckpt_dir and bert_config_dir)\n  if missing_ckpt_or_config_dir:\n    if not bert_dir:\n      raise ValueError(\'bert_dir cannot be empty.\')\n\n    if not bert_config_dir:\n      bert_config_dir = os.path.join(bert_dir, \'bert_config.json\')\n\n    if not bert_ckpt_dir:\n      bert_ckpt_dir = os.path.join(bert_dir, \'bert_model.ckpt\')\n  return bert_config_dir, bert_ckpt_dir\n\n\ndef create_feature_and_label(inputs, feature_size, model_family):\n  """"""Creates features and labels from model inputs.""""""\n  if model_family.lower() == \'bert\':\n    features, labels = bert_model.create_feature_and_label(inputs, feature_size)\n  else:\n    features = inputs[\'features\']\n    labels = inputs[\'labels\']\n  return features, labels\n\n\ndef main(argv):\n  del argv  # unused arg\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  logging.info(\'Saving checkpoints at %s\', FLAGS.output_dir)\n  tf.random.set_seed(FLAGS.seed)\n\n  if FLAGS.use_gpu:\n    logging.info(\'Use GPU\')\n    strategy = tf.distribute.MirroredStrategy()\n  else:\n    logging.info(\'Use TPU at %s\',\n                 FLAGS.tpu if FLAGS.tpu is not None else \'local\')\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n  ind_dataset_builder = ub.datasets.ClincIntentDetectionDataset(\n      batch_size=FLAGS.per_core_batch_size,\n      eval_batch_size=FLAGS.per_core_batch_size,\n      dataset_dir=FLAGS.dataset_dir,\n      data_mode=\'ind\')\n  ood_dataset_builder = ub.datasets.ClincIntentDetectionDataset(\n      batch_size=FLAGS.per_core_batch_size,\n      eval_batch_size=FLAGS.per_core_batch_size,\n      dataset_dir=FLAGS.dataset_dir,\n      data_mode=\'ood\')\n\n  dataset_builders = {\'clean\': ind_dataset_builder, \'ood\': ood_dataset_builder}\n\n  train_dataset = ind_dataset_builder.build(split=ub.datasets.base.Split.TRAIN)\n\n  ds_info = ind_dataset_builder.info\n  feature_size = ds_info[\'feature_size\']\n  # num_classes is number of valid intents plus out-of-scope intent\n  num_classes = ds_info[\'num_classes\'] + 1\n  # vocab_size is total number of valid tokens plus the out-of-vocabulary token.\n  vocab_size = ind_dataset_builder.tokenizer.num_words + 1\n\n  batch_size = FLAGS.per_core_batch_size * FLAGS.num_cores\n  steps_per_epoch = ds_info[\'num_train_examples\'] // batch_size\n\n  test_datasets = {}\n  steps_per_eval = {}\n  for dataset_name, dataset_builder in dataset_builders.items():\n    test_datasets[dataset_name] = dataset_builder.build(\n        split=ub.datasets.base.Split.TEST)\n    steps_per_eval[dataset_name] = (\n        dataset_builder.info[\'num_test_examples\'] // batch_size)\n\n  if FLAGS.use_bfloat16:\n    policy = tf.keras.mixed_precision.experimental.Policy(\'mixed_bfloat16\')\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n\n  summary_writer = tf.summary.create_file_writer(\n      os.path.join(FLAGS.output_dir, \'summaries\'))\n\n  premade_embedding_array = None\n  if FLAGS.word_embedding_dir:\n    with tf.io.gfile.GFile(FLAGS.word_embedding_dir, \'rb\') as embedding_file:\n      premade_embedding_array = np.load(embedding_file)\n\n  with strategy.scope():\n    logging.info(\'Building %s model\', FLAGS.model_family)\n    if FLAGS.model_family.lower() == \'textcnn\':\n      model = cnn_model.textcnn(\n          filter_sizes=[int(x) for x in FLAGS.filter_sizes],\n          num_filters=FLAGS.num_filters,\n          num_classes=num_classes,\n          feature_size=feature_size,\n          vocab_size=vocab_size,\n          embed_size=FLAGS.embedding_size,\n          dropout_rate=FLAGS.dropout_rate,\n          l2=FLAGS.l2,\n          premade_embedding_arr=premade_embedding_array)\n      optimizer = tf.keras.optimizers.Adam(FLAGS.base_learning_rate)\n    elif FLAGS.model_family.lower() == \'bert\':\n      bert_config_dir, bert_ckpt_dir = resolve_bert_ckpt_and_config_dir(\n          FLAGS.bert_dir, FLAGS.bert_config_dir, FLAGS.bert_ckpt_dir)\n      bert_config = bert_model.create_config(bert_config_dir)\n      model, bert_encoder = bert_model.create_model(\n          num_classes=num_classes,\n          feature_size=feature_size,\n          bert_config=bert_config)\n      optimizer = bert_model.create_optimizer(\n          FLAGS.base_learning_rate,\n          steps_per_epoch=steps_per_epoch,\n          epochs=FLAGS.train_epochs,\n          warmup_proportion=FLAGS.warmup_proportion)\n    else:\n      raise ValueError(\'model_family ({}) can only be TextCNN or BERT.\'.format(\n          FLAGS.model_family))\n\n    logging.info(\'Model input shape: %s\', model.input_shape)\n    logging.info(\'Model output shape: %s\', model.output_shape)\n    logging.info(\'Model number of weights: %s\', model.count_params())\n\n    metrics = {\n        \'train/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'train/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'train/loss\': tf.keras.metrics.Mean(),\n        \'train/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'test/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'test/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n    }\n\n    for dataset_name, test_dataset in test_datasets.items():\n      if dataset_name != \'clean\':\n        metrics.update({\n            \'test/nll_{}\'.format(dataset_name):\n                tf.keras.metrics.Mean(),\n            \'test/accuracy_{}\'.format(dataset_name):\n                tf.keras.metrics.SparseCategoricalAccuracy(),\n            \'test/ece_{}\'.format(dataset_name):\n                ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins)\n        })\n\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n    initial_epoch = 0\n    if latest_checkpoint:\n      # checkpoint.restore must be within a strategy.scope() so that optimizer\n      # slot variables are mirrored.\n      checkpoint.restore(latest_checkpoint)\n      logging.info(\'Loaded checkpoint %s\', latest_checkpoint)\n      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n    elif FLAGS.model_family.lower() == \'bert\':\n      # load BERT from initial checkpoint\n      bert_checkpoint = tf.train.Checkpoint(model=bert_encoder)\n      bert_checkpoint.restore(\n          bert_ckpt_dir).assert_existing_objects_matched()\n      logging.info(\'Loaded BERT checkpoint %s\', bert_ckpt_dir)\n\n  @tf.function\n  def train_step(iterator):\n    """"""Training StepFn.""""""\n\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      features, labels = create_feature_and_label(\n          inputs, feature_size, model_family=FLAGS.model_family)\n\n      with tf.GradientTape() as tape:\n        # Set learning phase to enable dropout etc during training.\n        logits = model(features, training=True)\n        if FLAGS.use_bfloat16:\n          logits = tf.cast(logits, tf.float32)\n        negative_log_likelihood = tf.reduce_mean(\n            tf.keras.losses.sparse_categorical_crossentropy(\n                labels, logits, from_logits=True))\n        l2_loss = sum(model.losses)\n        loss = negative_log_likelihood + l2_loss\n        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n        scaled_loss = loss / strategy.num_replicas_in_sync\n\n      grads = tape.gradient(scaled_loss, model.trainable_variables)\n      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n      probs = tf.nn.softmax(logits)\n      metrics[\'train/ece\'].update_state(labels, probs)\n      metrics[\'train/loss\'].update_state(loss)\n      metrics[\'train/negative_log_likelihood\'].update_state(\n          negative_log_likelihood)\n      metrics[\'train/accuracy\'].update_state(labels, logits)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  @tf.function\n  def test_step(iterator, dataset_name):\n    """"""Evaluation StepFn.""""""\n\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      features, labels = create_feature_and_label(\n          inputs, feature_size, model_family=FLAGS.model_family)\n\n      # Set learning phase to disable dropout etc during eval.\n      logits = model(features, training=False)\n      if FLAGS.use_bfloat16:\n        logits = tf.cast(logits, tf.float32)\n      probs = tf.nn.softmax(logits)\n      negative_log_likelihood = tf.reduce_mean(\n          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n\n      if dataset_name == \'clean\':\n        metrics[\'test/negative_log_likelihood\'].update_state(\n            negative_log_likelihood)\n        metrics[\'test/accuracy\'].update_state(labels, probs)\n        metrics[\'test/ece\'].update_state(labels, probs)\n      else:\n        metrics[\'test/nll_{}\'.format(dataset_name)].update_state(\n            negative_log_likelihood)\n        metrics[\'test/accuracy_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n        metrics[\'test/ece_{}\'.format(dataset_name)].update_state(labels, probs)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  train_iterator = iter(train_dataset)\n  start_time = time.time()\n  for epoch in range(initial_epoch, FLAGS.train_epochs):\n    logging.info(\'Starting to run epoch: %s\', epoch)\n    for step in range(steps_per_epoch):\n      train_step(train_iterator)\n\n      current_step = epoch * steps_per_epoch + (step + 1)\n      max_steps = steps_per_epoch * FLAGS.train_epochs\n      time_elapsed = time.time() - start_time\n      steps_per_sec = float(current_step) / time_elapsed\n      eta_seconds = (max_steps - current_step) / steps_per_sec\n      message = (\'{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. \'\n                 \'ETA: {:.0f} min. Time elapsed: {:.0f} min\'.format(\n                     current_step / max_steps, epoch + 1, FLAGS.train_epochs,\n                     steps_per_sec, eta_seconds / 60, time_elapsed / 60))\n      if step % 20 == 0:\n        logging.info(message)\n\n    if epoch % FLAGS.evaluation_interval == 0:\n      for dataset_name, test_dataset in test_datasets.items():\n        test_iterator = iter(test_dataset)\n        logging.info(\'Testing on dataset %s\', dataset_name)\n        for step in range(steps_per_eval[dataset_name]):\n          if step % 20 == 0:\n            logging.info(\'Starting to run eval step %s of epoch: %s\', step,\n                         epoch)\n          test_step(test_iterator, dataset_name)\n        logging.info(\'Done with testing on %s\', dataset_name)\n\n      logging.info(\'Train Loss: %.4f, Accuracy: %.2f%%\',\n                   metrics[\'train/loss\'].result(),\n                   metrics[\'train/accuracy\'].result() * 100)\n      logging.info(\'Test NLL: %.4f, Accuracy: %.2f%%\',\n                   metrics[\'test/negative_log_likelihood\'].result(),\n                   metrics[\'test/accuracy\'].result() * 100)\n      total_results = {\n          name: metric.result() for name, metric in metrics.items()\n      }\n      with summary_writer.as_default():\n        for name, result in total_results.items():\n          tf.summary.scalar(name, result, step=epoch + 1)\n\n    for metric in metrics.values():\n      metric.reset_states()\n\n    if (FLAGS.checkpoint_interval > 0 and\n        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n      checkpoint_name = checkpoint.save(\n          os.path.join(FLAGS.output_dir, \'checkpoint\'))\n      logging.info(\'Saved checkpoint to %s\', checkpoint_name)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/clinc_intent/deterministic_model.py,21,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""TextCNN model.\n\nA standard 1D CNN model for sentence classification.\n\n## References\n[1]: Yoon Kim. Convolutional Neural Networks for Sentence Classification.\n     In _Empirical Methods in Natural Language Processing_, 2014.\n     https://www.aclweb.org/anthology/D14-1181/\n""""""\n\nimport tensorflow as tf\n\n\ndef embedding_block(inputs,\n                    vocab_size,\n                    feature_size,\n                    embed_size,\n                    premade_embedding_arr=None):\n  """"""Creates Embedding Layer.\n\n  Args:\n    inputs: (tf.Tensor) Input sentence in token indices format, shape\n      (batch_size, feature_size).\n    vocab_size: (int) Static size of vocabulary.\n    feature_size: (int) Static size of input feature.\n    embed_size: (int) Static size of hidden dimension of the embedding output.\n    premade_embedding_arr: (np.ndarray) Pre-made word embedding in numpy array\n      format, shape (vocab_size, embed_size).\n\n  Raises:\n    (ValueError): If shape of premade_embedding_arr is not\n      (vocab_size, embed_size).\n\n  Returns:\n    (tf.Tensor) shape (batch_size, feature_size, embed_size).\n  """"""\n  # Make initializer.\n  if premade_embedding_arr is not None:\n    premade_vocab_size, premade_embed_size = premade_embedding_arr.shape\n    if premade_vocab_size != vocab_size or premade_embed_size != embed_size:\n      raise ValueError(""\'premade_embedding_arr\' should have size ({}, {}). ""\n                       \'Observed ({}, {})\'.format(vocab_size, embed_size,\n                                                  premade_vocab_size,\n                                                  premade_embed_size))\n    embed_init = tf.keras.initializers.Constant(premade_embedding_arr)\n  else:\n    embed_init = tf.keras.initializers.RandomUniform(minval=-1, maxval=1)\n\n  # Define layer.\n  embedding_layer = tf.keras.layers.Embedding(\n      vocab_size,\n      embed_size,\n      input_length=feature_size,\n      embeddings_initializer=embed_init,\n      name=\'embedding\')\n  return embedding_layer(inputs)\n\n\ndef conv_pooled_block(inputs, num_filters, filter_size, feature_size,\n                      embed_size):\n  """"""Creates convolution layer with max pooling.\n\n  Args:\n    inputs: (tf.Tensor) Input tensor, shape (batch_size, feature_size,\n      embed_size).\n    num_filters: (int) Number of filters to apply to input.\n    filter_size: (int) Static size of the convolutional filter.\n    feature_size: (int) Static size of input feature.\n    embed_size: (int) Static size of hidden dimension of the text embedding.\n\n  Returns:\n    (tf.Tensor) shape (batch_size, 1, num_filters).\n  """"""\n  filter_shape = (filter_size, embed_size)\n  max_pool_shape = (feature_size - filter_size + 1, 1)\n\n  conv_layer = tf.keras.layers.Conv2D(\n      num_filters,\n      filter_shape,\n      strides=(1, 1),\n      padding=\'valid\',\n      data_format=\'channels_last\',\n      activation=\'relu\',\n      kernel_initializer=\'glorot_normal\',\n      bias_initializer=tf.keras.initializers.constant(0.1),\n      name=\'convolution_{:d}\'.format(filter_size))\n\n  # Max pooling over sentence positions for each filter.\n  maxpool_layer = tf.keras.layers.MaxPool2D(\n      pool_size=max_pool_shape,\n      strides=(1, 1),\n      padding=\'valid\',\n      data_format=\'channels_last\',\n      name=\'max_pooling_{:d}\'.format(filter_size))\n\n  conv = conv_layer(inputs)\n  return maxpool_layer(conv)\n\n\ndef textcnn(filter_sizes,\n            num_filters,\n            num_classes,\n            feature_size,\n            vocab_size,\n            embed_size,\n            dropout_rate,\n            l2,\n            premade_embedding_arr=None):\n  """"""Builds TextCNN model.\n\n  Args:\n    filter_sizes: (list) A list specifying the sizes of the convolutional\n      filters.\n    num_filters: (int) Number of filters to apply to input.\n    num_classes: (int) Number of output classes.\n    feature_size: (int) Static size of input feature.\n    vocab_size: (int) Static size of vocabulary.\n    embed_size: (int) Static size of hidden dimension of the embedding output.\n    dropout_rate: (float) Fraction of the convolutional output units to drop.\n    l2: (float) Strength of L2 regularization for wieghts in the output dense\n      layer.\n    premade_embedding_arr: (np.ndarray) Pre-made word embedding in numpy array\n      format, shape (vocab_size, embed_size).\n\n  Returns:\n    (tf.keras.Model) The TextCNN model.\n  """"""\n  inputs = tf.keras.Input(shape=(feature_size,))\n\n  # Prepare word embedding for convolutional layers.\n  embed = embedding_block(\n      inputs,\n      vocab_size,\n      feature_size,\n      embed_size,\n      premade_embedding_arr=premade_embedding_arr)\n  embed = tf.keras.layers.Reshape((feature_size, embed_size, 1),\n                                  name=\'add_channel\')(\n                                      embed)\n\n  # Evaluate and gather conv layer output for each filter size.\n  pool_outputs = []\n  for filter_size in filter_sizes:\n    pool = conv_pooled_block(embed, num_filters, filter_size, feature_size,\n                             embed_size)\n    pool_outputs.append(pool)\n\n  pool_outputs = tf.keras.layers.concatenate(\n      pool_outputs, axis=-1, name=\'concatenate\')\n\n  # Flatten and apply dropout.\n  flat_outputs = tf.keras.layers.Flatten(\n      data_format=\'channels_last\', name=\'flatten\')(\n          pool_outputs)\n  flat_outputs = tf.keras.layers.Dropout(\n      dropout_rate, name=\'dropout\')(\n          flat_outputs)\n\n  # Dense output.\n  dense_output_layer = tf.keras.layers.Dense(\n      num_classes,\n      activation=None,\n      kernel_initializer=\'glorot_normal\',\n      bias_initializer=tf.keras.initializers.constant(0.1),\n      kernel_regularizer=tf.keras.regularizers.l2(l2),\n      bias_regularizer=tf.keras.regularizers.l2(l2),\n      name=\'dense\')\n  outputs = dense_output_layer(flat_outputs)\n\n  return tf.keras.Model(inputs=inputs, outputs=outputs)\n'"
baselines/clinc_intent/deterministic_model_bert.py,3,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""The Bidirectional Encoder Representations from Transformers (BERT) model.""""""\n\nimport json\nimport tensorflow as tf\n\nfrom tensorflow_models.official.nlp import optimization\nfrom tensorflow_models.official.nlp.bert import bert_models\nfrom tensorflow_models.official.nlp.bert import configs\n\n\n\ndef create_config(config_dir):\n  """"""Load a BERT config object from directory.""""""\n  with tf.io.gfile.GFile(config_dir) as config_file:\n    bert_config = json.load(config_file)\n  return configs.BertConfig(**bert_config)\n\n\ndef create_feature_and_label(inputs, feature_size):\n  """"""Creates features and labels for a BERT model.""""""\n  input_token_ids = inputs[\'features\']\n  labels = inputs[\'labels\']\n  num_tokens = inputs[\'num_tokens\']\n\n  input_mask = tf.sequence_mask(num_tokens, feature_size, dtype=tf.int32)\n  type_id = tf.sequence_mask(num_tokens, feature_size, dtype=tf.int32)\n  features = [input_token_ids, input_mask, type_id]\n\n  return features, labels\n\n\ndef create_optimizer(initial_lr,\n                     steps_per_epoch,\n                     epochs,\n                     warmup_proportion,\n                     end_lr=0.0,\n                     optimizer_type=\'adamw\'):\n  """"""Creates a BERT optimizer with learning rate schedule.""""""\n  num_train_steps = steps_per_epoch * epochs\n  num_warmup_steps = int(num_train_steps * warmup_proportion)\n  return optimization.create_optimizer(\n      initial_lr,\n      num_train_steps,\n      num_warmup_steps,\n      end_lr=end_lr,\n      optimizer_type=optimizer_type)\n\n\ndef create_model(num_classes, feature_size, bert_config):\n  """"""Creates a BERT classifier model.""""""\n  # TODO(jereliu): Point to a locally implemented BERT for v2.\n  return bert_models.classifier_model(\n      bert_config=bert_config,\n      num_labels=num_classes,\n      max_seq_length=feature_size,\n  )\n'"
baselines/clinc_intent/ensemble.py,37,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Ensemble on CLINC Intent Detection.\n\nThis script only performs evaluation, not training. We recommend training\nensembles by launching independent runs of `deterministic.py` over different\nseeds.\n""""""\n\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\n# import clinc_intent.deterministic to inhere its flags\nimport deterministic  # pylint:disable=unused-import  # local file import\nimport deterministic_model as cnn_model  # local file import\nimport deterministic_model_bert as bert_model  # local file import\n\nimport numpy as np\nimport tensorflow as tf\nimport uncertainty_baselines as ub\n\n# TODO(trandustin): We inherit\n# FLAGS.{dataset,per_core_batch_size,output_dir,seed} from deterministic. This\n# is not intuitive, which suggests we need to either refactor to avoid importing\n# from a binary or duplicate the model definition here.\n\n# Model flags\nflags.DEFINE_string(\'checkpoint_dir\', None,\n                    \'The directory where the model weights are stored.\')\nflags.mark_flag_as_required(\'checkpoint_dir\')\nFLAGS = flags.FLAGS\n\n\ndef ensemble_negative_log_likelihood(labels, logits):\n  """"""Negative log-likelihood for ensemble.\n\n  For each datapoint (x,y), the ensemble\'s negative log-likelihood is:\n\n  ```\n  -log p(y|x) = -log sum_{m=1}^{ensemble_size} exp(log p(y|x,theta_m)) +\n                log ensemble_size.\n  ```\n\n  Args:\n    labels: tf.Tensor of shape [...].\n    logits: tf.Tensor of shape [ensemble_size, ..., num_classes].\n\n  Returns:\n    tf.Tensor of shape [...].\n  """"""\n  labels = tf.cast(labels, tf.int32)\n  logits = tf.convert_to_tensor(logits)\n  ensemble_size = float(logits.shape[0])\n  nll = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      tf.broadcast_to(labels[tf.newaxis, ...],\n                      tf.shape(logits)[:-1]), logits)\n  return -tf.reduce_logsumexp(-nll, axis=0) + tf.math.log(ensemble_size)\n\n\ndef gibbs_cross_entropy(labels, logits):\n  """"""Average cross entropy for ensemble members (Gibbs cross entropy).\n\n  For each datapoint (x,y), the ensemble\'s Gibbs cross entropy is:\n\n  ```\n  GCE = - (1/ensemble_size) sum_{m=1}^ensemble_size log p(y|x,theta_m).\n  ```\n\n  The Gibbs cross entropy approximates the average cross entropy of a single\n  model drawn from the (Gibbs) ensemble.\n\n  Args:\n    labels: tf.Tensor of shape [...].\n    logits: tf.Tensor of shape [ensemble_size, ..., num_classes].\n\n  Returns:\n    tf.Tensor of shape [...].\n  """"""\n  labels = tf.cast(labels, tf.int32)\n  logits = tf.convert_to_tensor(logits)\n  nll = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      tf.broadcast_to(labels[tf.newaxis, ...],\n                      tf.shape(logits)[:-1]), logits)\n  return tf.reduce_mean(nll, axis=0)\n\n\ndef main(argv):\n  del argv  # unused arg\n  if not FLAGS.use_gpu:\n    raise ValueError(\'Only GPU is currently supported.\')\n  if FLAGS.num_cores > 1:\n    raise ValueError(\'Only a single accelerator is currently supported.\')\n  tf.random.set_seed(FLAGS.seed)\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n\n  ind_dataset_builder = ub.datasets.ClincIntentDetectionDataset(\n      batch_size=FLAGS.per_core_batch_size,\n      eval_batch_size=FLAGS.per_core_batch_size,\n      dataset_dir=FLAGS.dataset_dir,\n      data_mode=\'ind\')\n  ood_dataset_builder = ub.datasets.ClincIntentDetectionDataset(\n      batch_size=FLAGS.per_core_batch_size,\n      eval_batch_size=FLAGS.per_core_batch_size,\n      dataset_dir=FLAGS.dataset_dir,\n      data_mode=\'ood\')\n\n  dataset_builders = {\'clean\': ind_dataset_builder, \'ood\': ood_dataset_builder}\n\n  ds_info = ind_dataset_builder.info\n  feature_size = ds_info[\'feature_size\']\n  # num_classes is number of valid intents plus out-of-scope intent\n  num_classes = ds_info[\'num_classes\'] + 1\n  # vocab_size is total number of valid tokens plus the out-of-vocabulary token.\n  vocab_size = ind_dataset_builder.tokenizer.num_words + 1\n\n  batch_size = FLAGS.per_core_batch_size * FLAGS.num_cores\n\n  test_datasets = {}\n  steps_per_eval = {}\n  for dataset_name, dataset_builder in dataset_builders.items():\n    test_datasets[dataset_name] = dataset_builder.build(\n        split=ub.datasets.base.Split.TEST)\n    steps_per_eval[dataset_name] = (\n        dataset_builder.info[\'num_test_examples\'] // batch_size)\n\n  if FLAGS.model_family.lower() == \'textcnn\':\n    model = cnn_model.textcnn(\n        filter_sizes=FLAGS.filter_sizes,\n        num_filters=FLAGS.num_filters,\n        num_classes=num_classes,\n        feature_size=feature_size,\n        vocab_size=vocab_size,\n        embed_size=FLAGS.embedding_size,\n        dropout_rate=FLAGS.dropout_rate,\n        l2=FLAGS.l2)\n  elif FLAGS.model_family.lower() == \'bert\':\n    bert_config_dir, _ = deterministic.resolve_bert_ckpt_and_config_dir(\n        FLAGS.bert_dir, FLAGS.bert_config_dir, FLAGS.bert_ckpt_dir)\n    bert_config = bert_model.create_config(bert_config_dir)\n    model, _ = bert_model.create_model(num_classes=num_classes,\n                                       feature_size=feature_size,\n                                       bert_config=bert_config)\n  else:\n    raise ValueError(\'model_family ({}) can only be TextCNN or BERT.\'.format(\n        FLAGS.model_family))\n\n  logging.info(\'Model input shape: %s\', model.input_shape)\n  logging.info(\'Model output shape: %s\', model.output_shape)\n  logging.info(\'Model number of weights: %s\', model.count_params())\n\n  # Search for checkpoints from their index file; then remove the index suffix.\n  ensemble_filenames = tf.io.gfile.glob(\n      os.path.join(FLAGS.checkpoint_dir, \'**/*.index\'))\n  ensemble_filenames = [filename[:-6] for filename in ensemble_filenames]\n  ensemble_size = len(ensemble_filenames)\n  logging.info(\'Ensemble size: %s\', ensemble_size)\n  logging.info(\'Ensemble number of weights: %s\',\n               ensemble_size * model.count_params())\n  logging.info(\'Ensemble filenames: %s\', str(ensemble_filenames))\n  checkpoint = tf.train.Checkpoint(model=model)\n\n  # Write model predictions to files.\n  num_datasets = len(test_datasets)\n  for m, ensemble_filename in enumerate(ensemble_filenames):\n    checkpoint.restore(ensemble_filename)\n    for n, (name, test_dataset) in enumerate(test_datasets.items()):\n      filename = \'{dataset}_{member}.npy\'.format(dataset=name, member=m)\n      filename = os.path.join(FLAGS.output_dir, filename)\n      if not tf.io.gfile.exists(filename):\n        logits = []\n        test_iterator = iter(test_dataset)\n        for _ in range(steps_per_eval[name]):\n          inputs = next(test_iterator)\n          features, _ = deterministic.create_feature_and_label(\n              inputs, feature_size, model_family=FLAGS.model_family)\n          logits.append(model(features, training=False))\n\n        logits = tf.concat(logits, axis=0)\n        with tf.io.gfile.GFile(filename, \'w\') as f:\n          np.save(f, logits.numpy())\n      percent = (m * num_datasets + (n + 1)) / (ensemble_size * num_datasets)\n      message = (\'{:.1%} completion for prediction: ensemble member {:d}/{:d}. \'\n                 \'Dataset {:d}/{:d}\'.format(percent, m + 1, ensemble_size,\n                                            n + 1, num_datasets))\n      logging.info(message)\n\n  metrics = {\n      \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n      \'test/gibbs_cross_entropy\': tf.keras.metrics.Mean(),\n      \'test/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n      \'test/ece\': ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins),\n  }\n\n  for dataset_name, test_dataset in test_datasets.items():\n    if dataset_name != \'clean\':\n      metrics.update({\n          \'test/nll_{}\'.format(dataset_name):\n              tf.keras.metrics.Mean(),\n          \'test/accuracy_{}\'.format(dataset_name):\n              tf.keras.metrics.SparseCategoricalAccuracy(),\n          \'test/ece_{}\'.format(dataset_name):\n              ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins)\n      })\n\n  # Evaluate model predictions.\n  for n, (name, test_dataset) in enumerate(test_datasets.items()):\n    logits_dataset = []\n    for m in range(ensemble_size):\n      filename = \'{dataset}_{member}.npy\'.format(dataset=name, member=m)\n      filename = os.path.join(FLAGS.output_dir, filename)\n      with tf.io.gfile.GFile(filename, \'rb\') as f:\n        logits_dataset.append(np.load(f))\n\n    logits_dataset = tf.convert_to_tensor(logits_dataset)\n    test_iterator = iter(test_dataset)\n    for step in range(steps_per_eval[name]):\n      inputs = next(test_iterator)\n      _, labels = deterministic.create_feature_and_label(\n          inputs, feature_size, model_family=FLAGS.model_family)\n      logits = logits_dataset[:, (step * batch_size):((step + 1) * batch_size)]\n      labels = tf.cast(labels, tf.int32)\n      negative_log_likelihood = tf.reduce_mean(\n          ensemble_negative_log_likelihood(labels, logits))\n      per_probs = tf.nn.softmax(logits)\n      probs = tf.reduce_mean(per_probs, axis=0)\n      if name == \'clean\':\n        gibbs_ce = tf.reduce_mean(gibbs_cross_entropy(labels, logits))\n        metrics[\'test/negative_log_likelihood\'].update_state(\n            negative_log_likelihood)\n        metrics[\'test/gibbs_cross_entropy\'].update_state(gibbs_ce)\n        metrics[\'test/accuracy\'].update_state(labels, probs)\n        metrics[\'test/ece\'].update_state(labels, probs)\n      else:\n        metrics[\'test/nll_{}\'.format(name)].update_state(\n            negative_log_likelihood)\n        metrics[\'test/accuracy_{}\'.format(name)].update_state(\n            labels, probs)\n        metrics[\'test/ece_{}\'.format(name)].update_state(labels, probs)\n\n    message = (\'{:.1%} completion for evaluation: dataset {:d}/{:d}\'.format(\n        (n + 1) / num_datasets, n + 1, num_datasets))\n    logging.info(message)\n\n  total_results = {name: metric.result() for name, metric in metrics.items()}\n  logging.info(\'Metrics: %s\', total_results)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/imagenet/batchensemble.py,53,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Batch Ensemble ResNet-50.""""""\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport batchensemble_model  # local file import\nimport utils  # local file import\nimport tensorflow as tf\n\nflags.DEFINE_integer(\'ensemble_size\', 4, \'Size of ensemble.\')\nflags.DEFINE_integer(\'per_core_batch_size\', 128, \'Batch size per TPU core/GPU.\')\nflags.DEFINE_float(\'random_sign_init\', -0.5,\n                   \'Use random sign init for fast weights.\')\nflags.DEFINE_integer(\'seed\', 0, \'Random seed.\')\nflags.DEFINE_float(\'base_learning_rate\', 0.1,\n                   \'Base learning rate when train batch size is 256.\')\nflags.DEFINE_float(\'l2\', 1e-4, \'L2 coefficient.\')\nflags.DEFINE_float(\'fast_weight_lr_multiplier\', 0.5,\n                   \'fast weights lr multiplier.\')\nflags.DEFINE_string(\'data_dir\', None, \'Path to training and testing data.\')\nflags.mark_flag_as_required(\'data_dir\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/imagenet\',\n                    \'The directory where the model weights and \'\n                    \'training/evaluation summaries are stored.\')\nflags.DEFINE_integer(\'train_epochs\', 135, \'Number of training epochs.\')\nflags.DEFINE_integer(\'corruptions_interval\', 135,\n                     \'Number of epochs between evaluating on the corrupted \'\n                     \'test data. Use -1 to never evaluate.\')\nflags.DEFINE_integer(\'checkpoint_interval\', 27,\n                     \'Number of epochs between saving checkpoints. Use -1 to \'\n                     \'never save checkpoints.\')\nflags.DEFINE_string(\'alexnet_errors_path\', None,\n                    \'Path to AlexNet corruption errors file.\')\nflags.DEFINE_integer(\'num_bins\', 15, \'Number of bins for ECE computation.\')\nflags.DEFINE_bool(\'use_ensemble_bn\', False, \'Whether to use ensemble bn.\')\n\n# Accelerator flags.\nflags.DEFINE_bool(\'use_gpu\', False, \'Whether to run on GPU or otherwise TPU.\')\nflags.DEFINE_bool(\'use_bfloat16\', True, \'Whether to use mixed precision.\')\nflags.DEFINE_integer(\'num_cores\', 32, \'Number of TPU cores or number of GPUs.\')\nflags.DEFINE_string(\'tpu\', None,\n                    \'Name of the TPU. Only used if use_gpu is False.\')\nFLAGS = flags.FLAGS\n\n# Number of images in ImageNet-1k train dataset.\nAPPROX_IMAGENET_TRAIN_IMAGES = 1281167\n# Number of images in eval dataset.\nIMAGENET_VALIDATION_IMAGES = 50000\nNUM_CLASSES = 1000\n\n_LR_SCHEDULE = [    # (multiplier, epoch to start) tuples\n    (1.0, 5), (0.1, 30), (0.01, 60), (0.001, 80)\n]\n\n\ndef main(argv):\n  del argv  # unused arg\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  logging.info(\'Saving checkpoints at %s\', FLAGS.output_dir)\n  tf.random.set_seed(FLAGS.seed)\n\n  per_core_batch_size = FLAGS.per_core_batch_size // FLAGS.ensemble_size\n  batch_size = per_core_batch_size * FLAGS.num_cores\n  steps_per_epoch = APPROX_IMAGENET_TRAIN_IMAGES // batch_size\n  steps_per_eval = IMAGENET_VALIDATION_IMAGES // batch_size\n\n  if FLAGS.use_gpu:\n    logging.info(\'Use GPU\')\n    strategy = tf.distribute.MirroredStrategy()\n  else:\n    logging.info(\'Use TPU at %s\',\n                 FLAGS.tpu if FLAGS.tpu is not None else \'local\')\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n  imagenet_train = utils.ImageNetInput(\n      is_training=True,\n      data_dir=FLAGS.data_dir,\n      batch_size=per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  imagenet_eval = utils.ImageNetInput(\n      is_training=False,\n      data_dir=FLAGS.data_dir,\n      batch_size=per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  test_datasets = {\n      \'clean\':\n          strategy.experimental_distribute_datasets_from_function(\n              imagenet_eval.input_fn),\n  }\n  if FLAGS.corruptions_interval > 0:\n    corruption_types, max_intensity = utils.load_corrupted_test_info()\n    for name in corruption_types:\n      for intensity in range(1, max_intensity + 1):\n        dataset_name = \'{0}_{1}\'.format(name, intensity)\n        corrupt_input_fn = utils.corrupt_test_input_fn(\n            batch_size=per_core_batch_size,\n            corruption_name=name,\n            corruption_intensity=intensity,\n            use_bfloat16=FLAGS.use_bfloat16)\n        test_datasets[dataset_name] = (\n            strategy.experimental_distribute_datasets_from_function(\n                corrupt_input_fn))\n\n  train_dataset = strategy.experimental_distribute_datasets_from_function(\n      imagenet_train.input_fn)\n\n  if FLAGS.use_bfloat16:\n    policy = tf.keras.mixed_precision.experimental.Policy(\'mixed_bfloat16\')\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n\n  summary_writer = tf.summary.create_file_writer(\n      os.path.join(FLAGS.output_dir, \'summaries\'))\n\n  with strategy.scope():\n    logging.info(\'Building Keras ResNet-50 model\')\n    model = batchensemble_model.ensemble_resnet50(\n        input_shape=(224, 224, 3),\n        num_classes=NUM_CLASSES,\n        ensemble_size=FLAGS.ensemble_size,\n        random_sign_init=FLAGS.random_sign_init,\n        use_ensemble_bn=FLAGS.use_ensemble_bn)\n    logging.info(\'Model input shape: %s\', model.input_shape)\n    logging.info(\'Model output shape: %s\', model.output_shape)\n    logging.info(\'Model number of weights: %s\', model.count_params())\n    # Scale learning rate and decay epochs by vanilla settings.\n    base_lr = FLAGS.base_learning_rate * batch_size / 256\n    learning_rate = utils.LearningRateSchedule(steps_per_epoch,\n                                               base_lr,\n                                               FLAGS.train_epochs,\n                                               _LR_SCHEDULE)\n    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate,\n                                        momentum=0.9,\n                                        nesterov=True)\n    metrics = {\n        \'train/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'train/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'train/loss\': tf.keras.metrics.Mean(),\n        \'train/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'test/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'test/ece\': ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins)\n    }\n    if FLAGS.corruptions_interval > 0:\n      corrupt_metrics = {}\n      for intensity in range(1, max_intensity + 1):\n        for corruption in corruption_types:\n          dataset_name = \'{0}_{1}\'.format(corruption, intensity)\n          corrupt_metrics[\'test/nll_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.Mean())\n          corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.SparseCategoricalAccuracy())\n          corrupt_metrics[\'test/ece_{}\'.format(dataset_name)] = (\n              ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins))\n\n    test_diversity = {}\n    training_diversity = {}\n    for i in range(FLAGS.ensemble_size):\n      metrics[\'test/nll_member_{}\'.format(i)] = tf.keras.metrics.Mean()\n      metrics[\'test/accuracy_member_{}\'.format(i)] = (\n          tf.keras.metrics.SparseCategoricalAccuracy())\n    test_diversity = {\n        \'test/disagreement\': tf.keras.metrics.Mean(),\n        \'test/average_kl\': tf.keras.metrics.Mean(),\n        \'test/cosine_similarity\': tf.keras.metrics.Mean(),\n    }\n    training_diversity = {\n        \'train/disagreement\': tf.keras.metrics.Mean(),\n        \'train/average_kl\': tf.keras.metrics.Mean(),\n        \'train/cosine_similarity\': tf.keras.metrics.Mean(),\n    }\n\n    logging.info(\'Finished building Keras ResNet-50 model\')\n\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n    initial_epoch = 0\n    if latest_checkpoint:\n      # checkpoint.restore must be within a strategy.scope() so that optimizer\n      # slot variables are mirrored.\n      checkpoint.restore(latest_checkpoint)\n      logging.info(\'Loaded checkpoint %s\', latest_checkpoint)\n      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n\n  @tf.function\n  def train_step(iterator):\n    """"""Training StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      images = tf.tile(images, [FLAGS.ensemble_size, 1, 1, 1])\n      labels = tf.tile(labels, [FLAGS.ensemble_size])\n\n      with tf.GradientTape() as tape:\n        logits = model(images, training=True)\n        if FLAGS.use_bfloat16:\n          logits = tf.cast(logits, tf.float32)\n\n        probs = tf.nn.softmax(logits)\n        per_probs = tf.reshape(\n            probs, tf.concat([[FLAGS.ensemble_size, -1], probs.shape[1:]], 0))\n        diversity_results = ed.metrics.average_pairwise_diversity(\n            per_probs, FLAGS.ensemble_size)\n\n        negative_log_likelihood = tf.reduce_mean(\n            tf.keras.losses.sparse_categorical_crossentropy(labels,\n                                                            logits,\n                                                            from_logits=True))\n        filtered_variables = []\n        for var in model.trainable_variables:\n          # Apply l2 on the slow weights and bias terms. This excludes BN\n          # parameters and fast weight approximate posterior/prior parameters,\n          # but pay caution to their naming scheme.\n          if \'kernel\' in var.name or \'bias\' in var.name:\n            filtered_variables.append(tf.reshape(var, (-1,)))\n\n        l2_loss = FLAGS.l2 * 2 * tf.nn.l2_loss(\n            tf.concat(filtered_variables, axis=0))\n        loss = negative_log_likelihood + l2_loss\n        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n        scaled_loss = loss / strategy.num_replicas_in_sync\n\n      grads = tape.gradient(scaled_loss, model.trainable_variables)\n\n      # Separate learning rate implementation.\n      if FLAGS.fast_weight_lr_multiplier != 1.0:\n        grads_and_vars = []\n        for grad, var in zip(grads, model.trainable_variables):\n          # Apply different learning rate on the fast weights. This excludes BN\n          # and slow weights, but pay caution to the naming scheme.\n          if (\'batch_norm\' not in var.name and \'kernel\' not in var.name):\n            grads_and_vars.append((grad * FLAGS.fast_weight_lr_multiplier,\n                                   var))\n          else:\n            grads_and_vars.append((grad, var))\n        optimizer.apply_gradients(grads_and_vars)\n      else:\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n      metrics[\'train/ece\'].update_state(labels, probs)\n      metrics[\'train/loss\'].update_state(loss)\n      metrics[\'train/negative_log_likelihood\'].update_state(\n          negative_log_likelihood)\n      metrics[\'train/accuracy\'].update_state(labels, logits)\n      for k, v in diversity_results.items():\n        training_diversity[\'train/\' + k].update_state(v)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  @tf.function\n  def test_step(iterator, dataset_name):\n    """"""Evaluation StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      images = tf.tile(images, [FLAGS.ensemble_size, 1, 1, 1])\n      logits = model(images, training=False)\n      if FLAGS.use_bfloat16:\n        logits = tf.cast(logits, tf.float32)\n      probs = tf.nn.softmax(logits)\n\n      if dataset_name == \'clean\':\n        per_probs_tensor = tf.reshape(\n            probs, tf.concat([[FLAGS.ensemble_size, -1], probs.shape[1:]], 0))\n        diversity_results = ed.metrics.average_pairwise_diversity(\n            per_probs_tensor, FLAGS.ensemble_size)\n        for k, v in diversity_results.items():\n          test_diversity[\'test/\' + k].update_state(v)\n\n      per_probs = tf.split(\n          probs, num_or_size_splits=FLAGS.ensemble_size, axis=0)\n      probs = tf.reduce_mean(per_probs, axis=0)\n\n      negative_log_likelihood = tf.reduce_mean(\n          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n\n      if dataset_name == \'clean\':\n        for i in range(FLAGS.ensemble_size):\n          member_probs = per_probs[i]\n          member_loss = tf.keras.losses.sparse_categorical_crossentropy(\n              labels, member_probs)\n          metrics[\'test/nll_member_{}\'.format(i)].update_state(member_loss)\n          metrics[\'test/accuracy_member_{}\'.format(i)].update_state(\n              labels, member_probs)\n\n        metrics[\'test/negative_log_likelihood\'].update_state(\n            negative_log_likelihood)\n        metrics[\'test/accuracy\'].update_state(labels, probs)\n        metrics[\'test/ece\'].update_state(labels, probs)\n      else:\n        corrupt_metrics[\'test/nll_{}\'.format(dataset_name)].update_state(\n            negative_log_likelihood)\n        corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n        corrupt_metrics[\'test/ece_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  train_iterator = iter(train_dataset)\n  start_time = time.time()\n  for epoch in range(initial_epoch, FLAGS.train_epochs):\n    logging.info(\'Starting to run epoch: %s\', epoch)\n    for step in range(steps_per_epoch):\n      train_step(train_iterator)\n\n      current_step = epoch * steps_per_epoch + (step + 1)\n      max_steps = steps_per_epoch * FLAGS.train_epochs\n      time_elapsed = time.time() - start_time\n      steps_per_sec = float(current_step) / time_elapsed\n      eta_seconds = (max_steps - current_step) / steps_per_sec\n      message = (\'{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. \'\n                 \'ETA: {:.0f} min. Time elapsed: {:.0f} min\'.format(\n                     current_step / max_steps,\n                     epoch + 1,\n                     FLAGS.train_epochs,\n                     steps_per_sec,\n                     eta_seconds / 60,\n                     time_elapsed / 60))\n      if step % 20 == 0:\n        logging.info(message)\n\n    datasets_to_evaluate = {\'clean\': test_datasets[\'clean\']}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      datasets_to_evaluate = test_datasets\n    for dataset_name, test_dataset in datasets_to_evaluate.items():\n      test_iterator = iter(test_dataset)\n      logging.info(\'Testing on dataset %s\', dataset_name)\n      for step in range(steps_per_eval):\n        if step % 20 == 0:\n          logging.info(\'Starting to run eval step %s of epoch: %s\', step,\n                       epoch)\n        test_step(test_iterator, dataset_name)\n      logging.info(\'Done with testing on %s\', dataset_name)\n\n    corrupt_results = {}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      corrupt_results = utils.aggregate_corrupt_metrics(\n          corrupt_metrics, corruption_types, max_intensity,\n          FLAGS.alexnet_errors_path)\n\n    logging.info(\'Train Loss: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'train/loss\'].result(),\n                 metrics[\'train/accuracy\'].result() * 100)\n    logging.info(\'Test NLL: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'test/negative_log_likelihood\'].result(),\n                 metrics[\'test/accuracy\'].result() * 100)\n    for i in range(FLAGS.ensemble_size):\n      logging.info(\'Member %d Test Loss: %.4f, Accuracy: %.2f%%\',\n                   i, metrics[\'test/nll_member_{}\'.format(i)].result(),\n                   metrics[\'test/accuracy_member_{}\'.format(i)].result() * 100)\n\n    total_metrics = metrics.copy()\n    total_metrics.update(training_diversity)\n    total_metrics.update(test_diversity)\n    total_results = {name: metric.result()\n                     for name, metric in total_metrics.items()}\n    total_results.update(corrupt_results)\n    with summary_writer.as_default():\n      for name, result in total_results.items():\n        tf.summary.scalar(name, result, step=epoch + 1)\n\n    for _, metric in total_metrics.items():\n      metric.reset_states()\n\n    if (FLAGS.checkpoint_interval > 0 and\n        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n      checkpoint_name = checkpoint.save(os.path.join(\n          FLAGS.output_dir, \'checkpoint\'))\n      logging.info(\'Saved checkpoint to %s\', checkpoint_name)\n\n  final_checkpoint_name = checkpoint.save(\n      os.path.join(FLAGS.output_dir, \'checkpoint\'))\n  logging.info(\'Saved last checkpoint to %s\', final_checkpoint_name)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/imagenet/batchensemble_model.py,16,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Batch Ensemble ResNet50.""""""\n\nimport functools\nimport string\nimport edward2 as ed\nimport tensorflow as tf\n\n# Use batch normalization defaults from Pytorch.\nBATCH_NORM_DECAY = 0.9\nBATCH_NORM_EPSILON = 1e-5\n\nEnsembleBatchNormalization = functools.partial(  # pylint: disable=invalid-name\n    ed.layers.EnsembleSyncBatchNorm,\n    epsilon=BATCH_NORM_EPSILON,\n    momentum=BATCH_NORM_DECAY)\nBatchNormalization = functools.partial(  # pylint: disable=invalid-name\n    tf.keras.layers.BatchNormalization,\n    epsilon=BATCH_NORM_EPSILON,\n    momentum=BATCH_NORM_DECAY)\n\n\ndef make_random_sign_initializer(random_sign_init):\n  if random_sign_init > 0:\n    initializer = ed.initializers.RandomSign(random_sign_init)\n  else:\n    initializer = tf.keras.initializers.RandomNormal(mean=1.0,\n                                                     stddev=-random_sign_init)\n  return initializer\n\n\ndef bottleneck_block(inputs,\n                     filters,\n                     stage,\n                     block,\n                     strides,\n                     ensemble_size,\n                     random_sign_init,\n                     use_ensemble_bn):\n  """"""Residual block with 1x1 -> 3x3 -> 1x1 convs in main path.\n\n  Note that strides appear in the second conv (3x3) rather than the first (1x1).\n  This is also known as ""ResNet v1.5"" as it differs from He et al. (2015)\n  (http://torch.ch/blog/2016/02/04/resnets.html).\n\n  Args:\n    inputs: tf.Tensor.\n    filters: list of integers, the filters of 3 conv layer at main path\n    stage: integer, current stage label, used for generating layer names\n    block: \'a\',\'b\'..., current block label, used for generating layer names\n    strides: Strides for the second conv layer in the block.\n    ensemble_size: the ensemble size, when it is one, it goes back to the\n        single model case.\n    random_sign_init: whether uses random sign initializer to initializer\n        the fast weights.\n    use_ensemble_bn: whether to use ensemble batch norm.\n\n  Returns:\n    tf.Tensor.\n  """"""\n  filters1, filters2, filters3 = filters\n  conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n  bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n  x = ed.layers.Conv2DBatchEnsemble(\n      filters1,\n      kernel_size=1,\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      alpha_initializer=make_random_sign_initializer(random_sign_init),\n      gamma_initializer=make_random_sign_initializer(random_sign_init),\n      name=conv_name_base + \'2a\',\n      ensemble_size=ensemble_size)(inputs)\n  if use_ensemble_bn:\n    x = EnsembleBatchNormalization(\n        ensemble_size=ensemble_size,\n        name=bn_name_base+\'2a\')(x)\n  else:\n    x = BatchNormalization(name=bn_name_base+\'2a\')(x)\n  x = tf.keras.layers.Activation(\'relu\')(x)\n\n  x = ed.layers.Conv2DBatchEnsemble(\n      filters2,\n      kernel_size=3,\n      strides=strides,\n      padding=\'same\',\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      alpha_initializer=make_random_sign_initializer(random_sign_init),\n      gamma_initializer=make_random_sign_initializer(random_sign_init),\n      name=conv_name_base + \'2b\',\n      ensemble_size=ensemble_size)(x)\n  if use_ensemble_bn:\n    x = EnsembleBatchNormalization(\n        ensemble_size=ensemble_size,\n        name=bn_name_base+\'2b\')(x)\n  else:\n    x = BatchNormalization(name=bn_name_base+\'2b\')(x)\n  x = tf.keras.layers.Activation(\'relu\')(x)\n\n  x = ed.layers.Conv2DBatchEnsemble(\n      filters3,\n      kernel_size=1,\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      alpha_initializer=make_random_sign_initializer(random_sign_init),\n      gamma_initializer=make_random_sign_initializer(random_sign_init),\n      name=conv_name_base + \'2c\',\n      ensemble_size=ensemble_size)(x)\n  if use_ensemble_bn:\n    x = EnsembleBatchNormalization(\n        ensemble_size=ensemble_size,\n        name=bn_name_base+\'2c\')(x)\n  else:\n    x = BatchNormalization(name=bn_name_base+\'2c\')(x)\n\n  shortcut = inputs\n  if not x.shape.is_compatible_with(shortcut.shape):\n    shortcut = ed.layers.Conv2DBatchEnsemble(\n        filters3,\n        kernel_size=1,\n        strides=strides,\n        use_bias=False,\n        kernel_initializer=\'he_normal\',\n        alpha_initializer=make_random_sign_initializer(random_sign_init),\n        gamma_initializer=make_random_sign_initializer(random_sign_init),\n        name=conv_name_base + \'1\',\n        ensemble_size=ensemble_size)(inputs)\n    if use_ensemble_bn:\n      shortcut = EnsembleBatchNormalization(\n          ensemble_size=ensemble_size,\n          name=bn_name_base+\'1\')(shortcut)\n    else:\n      shortcut = BatchNormalization(name=bn_name_base+\'1\')(shortcut)\n\n  x = tf.keras.layers.add([x, shortcut])\n  x = tf.keras.layers.Activation(\'relu\')(x)\n  return x\n\n\ndef group(inputs, filters, num_blocks, stage, strides,\n          ensemble_size, random_sign_init, use_ensemble_bn):\n  """"""Group of residual blocks.""""""\n  bottleneck_block_ = functools.partial(bottleneck_block,\n                                        filters=filters,\n                                        stage=stage,\n                                        ensemble_size=ensemble_size,\n                                        random_sign_init=random_sign_init,\n                                        use_ensemble_bn=use_ensemble_bn)\n  blocks = string.ascii_lowercase\n  x = bottleneck_block_(inputs, block=blocks[0], strides=strides)\n  for i in range(num_blocks - 1):\n    x = bottleneck_block_(x, block=blocks[i + 1], strides=1)\n  return x\n\n\ndef ensemble_resnet50(input_shape,\n                      num_classes,\n                      ensemble_size,\n                      random_sign_init,\n                      use_ensemble_bn):\n  """"""Builds BatchEnsemble ResNet50.\n\n  Using strided conv, pooling, four groups of residual blocks, and pooling, the\n  network maps spatial features of size 224x224 -> 112x112 -> 56x56 -> 28x28 ->\n  14x14 -> 7x7 (Table 1 of He et al. (2015)).\n\n  Args:\n    input_shape: Shape tuple of input excluding batch dimension.\n    num_classes: Number of output classes.\n    ensemble_size: Ensemble size.\n    random_sign_init: float, probability of RandomSign initializer.\n    use_ensemble_bn: whether to use ensemble batch norm.\n\n  Returns:\n    tf.keras.Model.\n  """"""\n  group_ = functools.partial(group,\n                             ensemble_size=ensemble_size,\n                             use_ensemble_bn=use_ensemble_bn,\n                             random_sign_init=random_sign_init)\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  x = tf.keras.layers.ZeroPadding2D(padding=3, name=\'conv1_pad\')(inputs)\n  x = ed.layers.Conv2DBatchEnsemble(\n      64,\n      kernel_size=7,\n      strides=2,\n      padding=\'valid\',\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      alpha_initializer=make_random_sign_initializer(random_sign_init),\n      gamma_initializer=make_random_sign_initializer(random_sign_init),\n      name=\'conv1\',\n      ensemble_size=ensemble_size)(x)\n  if use_ensemble_bn:\n    x = EnsembleBatchNormalization(\n        ensemble_size=ensemble_size,\n        name=\'bn_conv1\')(x)\n  else:\n    x = BatchNormalization(name=\'bn_conv1\')(x)\n  x = tf.keras.layers.Activation(\'relu\')(x)\n  x = tf.keras.layers.MaxPooling2D(3, strides=(2, 2), padding=\'same\')(x)\n  x = group_(x, [64, 64, 256], stage=2, num_blocks=3, strides=1)\n  x = group_(x, [128, 128, 512], stage=3, num_blocks=4, strides=2)\n  x = group_(x, [256, 256, 1024], stage=4, num_blocks=6, strides=2)\n  x = group_(x, [512, 512, 2048], stage=5, num_blocks=3, strides=2)\n  x = tf.keras.layers.GlobalAveragePooling2D(name=\'avg_pool\')(x)\n  x = ed.layers.DenseBatchEnsemble(\n      num_classes,\n      ensemble_size=ensemble_size,\n      kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n      alpha_initializer=make_random_sign_initializer(random_sign_init),\n      gamma_initializer=make_random_sign_initializer(random_sign_init),\n      activation=None,\n      name=\'fc1000\')(x)\n  return tf.keras.Model(inputs=inputs, outputs=x, name=\'resnet50\')\n'"
baselines/imagenet/deterministic.py,35,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""ResNet-50 on ImageNet trained with maximum likelihood and gradient descent.\n""""""\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport deterministic_model  # local file import\nimport utils  # local file import\nimport tensorflow as tf\n\nflags.DEFINE_integer(\'per_core_batch_size\', 128, \'Batch size per TPU core/GPU.\')\nflags.DEFINE_integer(\'seed\', 0, \'Random seed.\')\nflags.DEFINE_float(\'base_learning_rate\', 0.1,\n                   \'Base learning rate when train batch size is 256.\')\nflags.DEFINE_float(\'l2\', 1e-4, \'L2 coefficient.\')\nflags.DEFINE_string(\'data_dir\', None, \'Path to training and testing data.\')\nflags.mark_flag_as_required(\'data_dir\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/imagenet\',\n                    \'The directory where the model weights and \'\n                    \'training/evaluation summaries are stored.\')\nflags.DEFINE_integer(\'train_epochs\', 90, \'Number of training epochs.\')\nflags.DEFINE_integer(\'corruptions_interval\', 90,\n                     \'Number of epochs between evaluating on the corrupted \'\n                     \'test data. Use -1 to never evaluate.\')\nflags.DEFINE_integer(\'checkpoint_interval\', 25,\n                     \'Number of epochs between saving checkpoints. Use -1 to \'\n                     \'never save checkpoints.\')\nflags.DEFINE_string(\'alexnet_errors_path\', None,\n                    \'Path to AlexNet corruption errors file.\')\nflags.DEFINE_integer(\'num_bins\', 15, \'Number of bins for ECE computation.\')\n\n# Accelerator flags.\nflags.DEFINE_bool(\'use_gpu\', False, \'Whether to run on GPU or otherwise TPU.\')\nflags.DEFINE_bool(\'use_bfloat16\', True, \'Whether to use mixed precision.\')\nflags.DEFINE_integer(\'num_cores\', 32, \'Number of TPU cores or number of GPUs.\')\nflags.DEFINE_string(\'tpu\', None,\n                    \'Name of the TPU. Only used if use_gpu is False.\')\nFLAGS = flags.FLAGS\n\n# Number of images in ImageNet-1k train dataset.\nAPPROX_IMAGENET_TRAIN_IMAGES = 1281167\n# Number of images in eval dataset.\nIMAGENET_VALIDATION_IMAGES = 50000\nNUM_CLASSES = 1000\n\n_LR_SCHEDULE = [    # (multiplier, epoch to start) tuples\n    (1.0, 5), (0.1, 30), (0.01, 60), (0.001, 80)\n]\n\n\ndef main(argv):\n  del argv  # unused arg\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  logging.info(\'Saving checkpoints at %s\', FLAGS.output_dir)\n  tf.random.set_seed(FLAGS.seed)\n\n  batch_size = FLAGS.per_core_batch_size * FLAGS.num_cores\n  steps_per_epoch = APPROX_IMAGENET_TRAIN_IMAGES // batch_size\n  steps_per_eval = IMAGENET_VALIDATION_IMAGES // batch_size\n\n  if FLAGS.use_gpu:\n    logging.info(\'Use GPU\')\n    strategy = tf.distribute.MirroredStrategy()\n  else:\n    logging.info(\'Use TPU at %s\',\n                 FLAGS.tpu if FLAGS.tpu is not None else \'local\')\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n  imagenet_train = utils.ImageNetInput(\n      is_training=True,\n      data_dir=FLAGS.data_dir,\n      batch_size=FLAGS.per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  imagenet_eval = utils.ImageNetInput(\n      is_training=False,\n      data_dir=FLAGS.data_dir,\n      batch_size=FLAGS.per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  test_datasets = {\n      \'clean\':\n          strategy.experimental_distribute_datasets_from_function(\n              imagenet_eval.input_fn)\n  }\n  if FLAGS.corruptions_interval > 0:\n    corruption_types, max_intensity = utils.load_corrupted_test_info()\n    for name in corruption_types:\n      for intensity in range(1, max_intensity + 1):\n        dataset_name = \'{0}_{1}\'.format(name, intensity)\n        corrupt_input_fn = utils.corrupt_test_input_fn(\n            batch_size=FLAGS.per_core_batch_size,\n            corruption_name=name,\n            corruption_intensity=intensity,\n            use_bfloat16=FLAGS.use_bfloat16)\n        test_datasets[dataset_name] = (\n            strategy.experimental_distribute_datasets_from_function(\n                corrupt_input_fn))\n\n  train_dataset = strategy.experimental_distribute_datasets_from_function(\n      imagenet_train.input_fn)\n\n  if FLAGS.use_bfloat16:\n    policy = tf.keras.mixed_precision.experimental.Policy(\'mixed_bfloat16\')\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n\n  with strategy.scope():\n    logging.info(\'Building Keras ResNet-50 model\')\n    model = deterministic_model.resnet50(input_shape=(224, 224, 3),\n                                         num_classes=NUM_CLASSES)\n    logging.info(\'Model input shape: %s\', model.input_shape)\n    logging.info(\'Model output shape: %s\', model.output_shape)\n    logging.info(\'Model number of weights: %s\', model.count_params())\n    # Scale learning rate and decay epochs by vanilla settings.\n    base_lr = FLAGS.base_learning_rate * batch_size / 256\n    learning_rate = utils.LearningRateSchedule(steps_per_epoch,\n                                               base_lr,\n                                               FLAGS.train_epochs,\n                                               _LR_SCHEDULE)\n    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate,\n                                        momentum=0.9,\n                                        nesterov=True)\n    metrics = {\n        \'train/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'train/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'train/loss\': tf.keras.metrics.Mean(),\n        \'train/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'test/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'test/ece\': ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins)\n    }\n    if FLAGS.corruptions_interval > 0:\n      corrupt_metrics = {}\n      for intensity in range(1, max_intensity + 1):\n        for corruption in corruption_types:\n          dataset_name = \'{0}_{1}\'.format(corruption, intensity)\n          corrupt_metrics[\'test/nll_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.Mean())\n          corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.SparseCategoricalAccuracy())\n          corrupt_metrics[\'test/ece_{}\'.format(dataset_name)] = (\n              ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins))\n\n    logging.info(\'Finished building Keras ResNet-50 model\')\n\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n    initial_epoch = 0\n    if latest_checkpoint:\n      # checkpoint.restore must be within a strategy.scope() so that optimizer\n      # slot variables are mirrored.\n      checkpoint.restore(latest_checkpoint)\n      logging.info(\'Loaded checkpoint %s\', latest_checkpoint)\n      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n\n  summary_writer = tf.summary.create_file_writer(\n      os.path.join(FLAGS.output_dir, \'summaries\'))\n\n  @tf.function\n  def train_step(iterator):\n    """"""Training StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      with tf.GradientTape() as tape:\n        logits = model(images, training=True)\n        if FLAGS.use_bfloat16:\n          logits = tf.cast(logits, tf.float32)\n\n        negative_log_likelihood = tf.reduce_mean(\n            tf.keras.losses.sparse_categorical_crossentropy(labels,\n                                                            logits,\n                                                            from_logits=True))\n        filtered_variables = []\n        for var in model.trainable_variables:\n          # Apply l2 on the weights. This excludes BN parameters and biases, but\n          # pay caution to their naming scheme.\n          if \'kernel\' in var.name or \'bias\' in var.name:\n            filtered_variables.append(tf.reshape(var, (-1,)))\n\n        l2_loss = FLAGS.l2 * 2 * tf.nn.l2_loss(\n            tf.concat(filtered_variables, axis=0))\n        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n        loss = negative_log_likelihood + l2_loss\n        scaled_loss = loss / strategy.num_replicas_in_sync\n\n      grads = tape.gradient(scaled_loss, model.trainable_variables)\n      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n      probs = tf.nn.softmax(logits)\n      metrics[\'train/ece\'].update_state(labels, probs)\n      metrics[\'train/loss\'].update_state(loss)\n      metrics[\'train/negative_log_likelihood\'].update_state(\n          negative_log_likelihood)\n      metrics[\'train/accuracy\'].update_state(labels, logits)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  @tf.function\n  def test_step(iterator, dataset_name):\n    """"""Evaluation StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      logits = model(images, training=False)\n      if FLAGS.use_bfloat16:\n        logits = tf.cast(logits, tf.float32)\n\n      negative_log_likelihood = tf.reduce_mean(\n          tf.keras.losses.sparse_categorical_crossentropy(labels,\n                                                          logits,\n                                                          from_logits=True))\n      probs = tf.nn.softmax(logits)\n      if dataset_name == \'clean\':\n        metrics[\'test/negative_log_likelihood\'].update_state(\n            negative_log_likelihood)\n        metrics[\'test/accuracy\'].update_state(labels, probs)\n        metrics[\'test/ece\'].update_state(labels, probs)\n      else:\n        corrupt_metrics[\'test/nll_{}\'.format(dataset_name)].update_state(\n            negative_log_likelihood)\n        corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n        corrupt_metrics[\'test/ece_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  train_iterator = iter(train_dataset)\n  start_time = time.time()\n  for epoch in range(initial_epoch, FLAGS.train_epochs):\n    logging.info(\'Starting to run epoch: %s\', epoch)\n    for step in range(steps_per_epoch):\n      train_step(train_iterator)\n\n      current_step = epoch * steps_per_epoch + (step + 1)\n      max_steps = steps_per_epoch * FLAGS.train_epochs\n      time_elapsed = time.time() - start_time\n      steps_per_sec = float(current_step) / time_elapsed\n      eta_seconds = (max_steps - current_step) / steps_per_sec\n      message = (\'{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. \'\n                 \'ETA: {:.0f} min. Time elapsed: {:.0f} min\'.format(\n                     current_step / max_steps,\n                     epoch + 1,\n                     FLAGS.train_epochs,\n                     steps_per_sec,\n                     eta_seconds / 60,\n                     time_elapsed / 60))\n      if step % 20 == 0:\n        logging.info(message)\n\n    datasets_to_evaluate = {\'clean\': test_datasets[\'clean\']}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      datasets_to_evaluate = test_datasets\n    for dataset_name, test_dataset in datasets_to_evaluate.items():\n      test_iterator = iter(test_dataset)\n      logging.info(\'Testing on dataset %s\', dataset_name)\n      for step in range(steps_per_eval):\n        if step % 20 == 0:\n          logging.info(\'Starting to run eval step %s of epoch: %s\', step,\n                       epoch)\n        test_step(test_iterator, dataset_name)\n      logging.info(\'Done with testing on %s\', dataset_name)\n\n    corrupt_results = {}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      corrupt_results = utils.aggregate_corrupt_metrics(\n          corrupt_metrics, corruption_types, max_intensity,\n          FLAGS.alexnet_errors_path)\n\n    logging.info(\'Train Loss: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'train/loss\'].result(),\n                 metrics[\'train/accuracy\'].result() * 100)\n    logging.info(\'Test NLL: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'test/negative_log_likelihood\'].result(),\n                 metrics[\'test/accuracy\'].result() * 100)\n    total_results = {name: metric.result() for name, metric in metrics.items()}\n    total_results.update(corrupt_results)\n    with summary_writer.as_default():\n      for name, result in total_results.items():\n        tf.summary.scalar(name, result, step=epoch + 1)\n\n    for metric in metrics.values():\n      metric.reset_states()\n\n    if (FLAGS.checkpoint_interval > 0 and\n        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n      checkpoint_name = checkpoint.save(os.path.join(\n          FLAGS.output_dir, \'checkpoint\'))\n      logging.info(\'Saved checkpoint to %s\', checkpoint_name)\n\n  final_checkpoint_name = checkpoint.save(\n      os.path.join(FLAGS.output_dir, \'checkpoint\'))\n  logging.info(\'Saved last checkpoint to %s\', final_checkpoint_name)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/imagenet/deterministic_model.py,25,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""ResNet50 model.""""""\n\nimport string\nimport tensorflow as tf\n\n# Use batch normalization defaults from Pytorch.\nBATCH_NORM_DECAY = 0.9\nBATCH_NORM_EPSILON = 1e-5\n\n\ndef bottleneck_block(inputs,\n                     filters,\n                     stage,\n                     block,\n                     strides):\n  """"""Residual block with 1x1 -> 3x3 -> 1x1 convs in main path.\n\n  Note that strides appear in the second conv (3x3) rather than the first (1x1).\n  This is also known as ""ResNet v1.5"" as it differs from He et al. (2015)\n  (http://torch.ch/blog/2016/02/04/resnets.html).\n\n  Args:\n    inputs: tf.Tensor.\n    filters: list of integers, the filters of 3 conv layer at main path\n    stage: integer, current stage label, used for generating layer names\n    block: \'a\',\'b\'..., current block label, used for generating layer names\n    strides: Strides for the second conv layer in the block.\n\n  Returns:\n    tf.Tensor.\n  """"""\n  filters1, filters2, filters3 = filters\n  conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n  bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n  x = tf.keras.layers.Conv2D(\n      filters1,\n      kernel_size=1,\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      name=conv_name_base + \'2a\')(inputs)\n  x = tf.keras.layers.BatchNormalization(\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=bn_name_base + \'2a\')(x)\n  x = tf.keras.layers.Activation(\'relu\')(x)\n\n  x = tf.keras.layers.Conv2D(\n      filters2,\n      kernel_size=3,\n      strides=strides,\n      padding=\'same\',\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      name=conv_name_base + \'2b\')(x)\n  x = tf.keras.layers.BatchNormalization(\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=bn_name_base + \'2b\')(x)\n  x = tf.keras.layers.Activation(\'relu\')(x)\n\n  x = tf.keras.layers.Conv2D(\n      filters3,\n      kernel_size=1,\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      name=conv_name_base + \'2c\')(x)\n  x = tf.keras.layers.BatchNormalization(\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=bn_name_base + \'2c\')(x)\n\n  shortcut = inputs\n  if not x.shape.is_compatible_with(shortcut.shape):\n    shortcut = tf.keras.layers.Conv2D(\n        filters3,\n        kernel_size=1,\n        use_bias=False,\n        strides=strides,\n        kernel_initializer=\'he_normal\',\n        name=conv_name_base + \'1\')(shortcut)\n    shortcut = tf.keras.layers.BatchNormalization(\n        momentum=BATCH_NORM_DECAY,\n        epsilon=BATCH_NORM_EPSILON,\n        name=bn_name_base + \'1\')(shortcut)\n\n  x = tf.keras.layers.add([x, shortcut])\n  x = tf.keras.layers.Activation(\'relu\')(x)\n  return x\n\n\ndef group(inputs, filters, num_blocks, stage, strides):\n  blocks = string.ascii_lowercase\n  x = bottleneck_block(inputs, filters, stage, block=blocks[0], strides=strides)\n  for i in range(num_blocks - 1):\n    x = bottleneck_block(x, filters, stage, block=blocks[i + 1], strides=1)\n  return x\n\n\ndef resnet50(input_shape, num_classes):\n  """"""Builds ResNet50.\n\n  Using strided conv, pooling, four groups of residual blocks, and pooling, the\n  network maps spatial features of size 224x224 -> 112x112 -> 56x56 -> 28x28 ->\n  14x14 -> 7x7 (Table 1 of He et al. (2015)).\n\n  Args:\n    input_shape: Shape tuple of input excluding batch dimension.\n    num_classes: Number of output classes.\n\n  Returns:\n    tf.keras.Model.\n  """"""\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  x = tf.keras.layers.ZeroPadding2D(padding=3, name=\'conv1_pad\')(inputs)\n  x = tf.keras.layers.Conv2D(\n      64,\n      kernel_size=7,\n      strides=2,\n      padding=\'valid\',\n      use_bias=False,\n      kernel_initializer=\'he_normal\',\n      name=\'conv1\')(x)\n  x = tf.keras.layers.BatchNormalization(\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=\'bn_conv1\')(x)\n  x = tf.keras.layers.Activation(\'relu\')(x)\n  x = tf.keras.layers.MaxPooling2D(3, strides=2, padding=\'same\')(x)\n  x = group(x, [64, 64, 256], stage=2, num_blocks=3, strides=1)\n  x = group(x, [128, 128, 512], stage=3, num_blocks=4, strides=2)\n  x = group(x, [256, 256, 1024], stage=4, num_blocks=6, strides=2)\n  x = group(x, [512, 512, 2048], stage=5, num_blocks=3, strides=2)\n  x = tf.keras.layers.GlobalAveragePooling2D(name=\'avg_pool\')(x)\n  x = tf.keras.layers.Dense(\n      num_classes,\n      activation=None,\n      kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n      name=\'fc1000\')(x)\n  return tf.keras.Model(inputs=inputs, outputs=x, name=\'resnet50\')\n'"
baselines/imagenet/efficientnet.py,45,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""EfficientNet.""""""\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport efficientnet_model  # local file import\nimport utils  # local file import\nimport tensorflow as tf\n\n# ~312.78 steps per epoch for 4x4 TPU; per_core_batch_size=128; 350 epochs;\n\n# General model flags\nflags.DEFINE_enum(\'model_name\',\n                  default=\'efficientnet-b0\',\n                  enum_values=[\'efficientnet-b0\', \'efficientnet-b1\',\n                               \'efficientnet-b2\', \'efficientnet-b3\'],\n                  help=\'Efficientnet model name.\')\nflags.DEFINE_integer(\'per_core_batch_size\', 128, \'Batch size per TPU core/GPU.\')\nflags.DEFINE_integer(\'seed\', 0, \'Random seed.\')\nflags.DEFINE_float(\'base_learning_rate\', 0.016,\n                   \'Base learning rate when train batch size is 256.\')\nflags.DEFINE_float(\'l2\', 5e-6, \'L2 coefficient.\')\nflags.DEFINE_string(\'data_dir\', \'\', \'Path to training and testing data.\')\nflags.mark_flag_as_required(\'data_dir\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/imagenet\',\n                    \'The directory where the model weights and \'\n                    \'training/evaluation summaries are stored.\')\nflags.DEFINE_integer(\'train_epochs\', 350, \'Number of training epochs.\')\nflags.DEFINE_integer(\'checkpoint_interval\', 15,\n                     \'Number of epochs between saving checkpoints. Use -1 to \'\n                     \'never save checkpoints.\')\nflags.DEFINE_integer(\'evaluation_interval\', 5, \'How many epochs to run test.\')\nflags.DEFINE_string(\'alexnet_errors_path\', None,\n                    \'Path to AlexNet corruption errors file.\')\nflags.DEFINE_float(\'label_smoothing\', 0.1, \'label smoothing constant.\')\nflags.DEFINE_integer(\'num_bins\', 15, \'Number of bins for ECE computation.\')\nflags.DEFINE_float(\'moving_average_decay\', 0., \'moving average decay.\')\n\n# Accelerator flags.\nflags.DEFINE_bool(\'use_gpu\', False, \'Whether to run on GPU or otherwise TPU.\')\nflags.DEFINE_bool(\'use_bfloat16\', True, \'Whether to use mixed precision.\')\nflags.DEFINE_integer(\'num_cores\', 32, \'Number of TPU cores or number of GPUs.\')\nflags.DEFINE_string(\'tpu\', None,\n                    \'Name of the TPU. Only used if use_gpu is False.\')\nFLAGS = flags.FLAGS\n\n# Number of images in ImageNet-1k train dataset.\nAPPROX_IMAGENET_TRAIN_IMAGES = 1281167\nIMAGENET_VALIDATION_IMAGES = 50000\nNUM_CLASSES = 1000\n\n\ndef main(argv):\n  del argv  # unused arg\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  logging.info(\'Saving checkpoints at %s\', FLAGS.output_dir)\n  tf.random.set_seed(FLAGS.seed)\n\n  batch_size = FLAGS.per_core_batch_size * FLAGS.num_cores\n  steps_per_epoch = APPROX_IMAGENET_TRAIN_IMAGES // batch_size\n  steps_per_eval = IMAGENET_VALIDATION_IMAGES // batch_size\n  logging.info(\'Saving checkpoints at %s\', FLAGS.output_dir)\n\n  if FLAGS.use_gpu:\n    logging.info(\'Use GPU\')\n    strategy = tf.distribute.MirroredStrategy()\n  else:\n    logging.info(\'Use TPU at %s\',\n                 FLAGS.tpu if FLAGS.tpu is not None else \'local\')\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n  width_coefficient, depth_coefficient, input_image_size, dropout_rate = (\n      efficientnet_model.efficientnet_params(FLAGS.model_name))\n  imagenet_train = utils.ImageNetInput(\n      is_training=True,\n      use_bfloat16=FLAGS.use_bfloat16,\n      data_dir=FLAGS.data_dir,\n      batch_size=FLAGS.per_core_batch_size,\n      image_size=input_image_size,\n      normalize_input=True,\n      one_hot=True)\n  imagenet_eval = utils.ImageNetInput(\n      is_training=False,\n      use_bfloat16=FLAGS.use_bfloat16,\n      data_dir=FLAGS.data_dir,\n      batch_size=batch_size,\n      image_size=input_image_size,\n      normalize_input=True,\n      one_hot=True)\n  train_dataset = strategy.experimental_distribute_datasets_from_function(\n      imagenet_train.input_fn)\n  test_datasets = {\n      \'clean\':\n          strategy.experimental_distribute_dataset(imagenet_eval.input_fn()),\n  }\n  train_iterator = iter(train_dataset)\n  test_iterator = iter(test_datasets[\'clean\'])\n\n  if FLAGS.use_bfloat16:\n    policy = tf.keras.mixed_precision.experimental.Policy(\'mixed_bfloat16\')\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n\n  summary_writer = tf.summary.create_file_writer(\n      os.path.join(FLAGS.output_dir, \'summaries\'))\n\n  with strategy.scope():\n    logging.info(\'Building %s model\', FLAGS.model_name)\n    model = efficientnet_model.Model(width_coefficient,\n                                     depth_coefficient,\n                                     dropout_rate)\n\n    scaled_lr = FLAGS.base_learning_rate * (batch_size / 256.0)\n    # Decay epoch is 2.4, warmup epoch is 5 according to the Efficientnet paper.\n    decay_steps = steps_per_epoch * 2.4\n    warmup_step = steps_per_epoch * 5\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        scaled_lr, decay_steps, decay_rate=0.97, staircase=True)\n    learning_rate = utils.WarmupDecaySchedule(lr_schedule, warmup_step)\n    optimizer = tf.keras.optimizers.RMSprop(\n        learning_rate, rho=0.9, momentum=0.9, epsilon=0.001)\n    if FLAGS.moving_average_decay > 0:\n      optimizer = utils.MovingAverage(\n          optimizer,\n          average_decay=FLAGS.moving_average_decay)\n      optimizer.shadow_copy(model)\n\n    metrics = {\n        \'train/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'train/accuracy\': tf.keras.metrics.CategoricalAccuracy(),\n        \'train/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        \'train/loss\': tf.keras.metrics.Mean(),\n        \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'test/accuracy\': tf.keras.metrics.CategoricalAccuracy(),\n        \'test/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n    }\n    logging.info(\'Finished building %s model\', FLAGS.model_name)\n\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n    initial_epoch = 0\n    if latest_checkpoint:\n      # checkpoint.restore must be within a strategy.scope() so that optimizer\n      # slot variables are mirrored.\n      checkpoint.restore(latest_checkpoint)\n      logging.info(\'Loaded checkpoint %s\', latest_checkpoint)\n      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n\n  def train_step(inputs):\n    """"""Build `step_fn` for efficientnet learning.""""""\n    images, labels = inputs\n\n    num_replicas = tf.cast(strategy.num_replicas_in_sync, tf.float32)\n    l2_coeff = tf.cast(FLAGS.l2, tf.float32)\n\n    with tf.GradientTape() as tape:\n      logits = model(images, training=True)\n      logits = tf.cast(logits, tf.float32)\n      negative_log_likelihood = tf.reduce_mean(\n          tf.keras.losses.categorical_crossentropy(\n              labels,\n              logits,\n              from_logits=True,\n              label_smoothing=FLAGS.label_smoothing))\n\n      def _is_batch_norm(v):\n        """"""Decide whether a variable belongs to `batch_norm`.""""""\n        keywords = [\'batchnorm\', \'batch_norm\', \'bn\']\n        return any([k in v.name.lower() for k in keywords])\n\n      l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in model.trainable_weights\n                          if not _is_batch_norm(v)])\n      loss = negative_log_likelihood + l2_coeff * l2_loss\n      scaled_loss = loss / num_replicas\n\n    gradients = tape.gradient(scaled_loss, model.trainable_weights)\n    # MovingAverage optimizer automatically updates avg when applying gradients.\n    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n\n    sparse_labels = tf.cast(\n        tf.math.argmax(labels, axis=-1, output_type=tf.int32), tf.float32)\n    probs = tf.nn.softmax(logits)\n    metrics[\'train/loss\'].update_state(loss)\n    metrics[\'train/negative_log_likelihood\'].update_state(\n        negative_log_likelihood)\n    metrics[\'train/accuracy\'].update_state(labels, logits)\n    metrics[\'train/ece\'].update_state(sparse_labels, probs)\n\n    step_info = {\n        \'loss/negative_log_likelihood\': negative_log_likelihood / num_replicas,\n        \'loss/total_loss\': scaled_loss,\n    }\n    return step_info\n\n  def eval_step(inputs):\n    """"""A single step.""""""\n    images, labels = inputs\n    logits = model(images, training=False)\n    logits = tf.cast(logits, tf.float32)\n    negative_log_likelihood = tf.reduce_mean(\n        tf.keras.losses.categorical_crossentropy(\n            labels, logits, from_logits=True))\n    sparse_labels = tf.cast(\n        tf.math.argmax(labels, axis=-1, output_type=tf.int32), tf.float32)\n    probs = tf.nn.softmax(logits)\n    metrics[\'test/negative_log_likelihood\'].update_state(\n        negative_log_likelihood)\n    metrics[\'test/accuracy\'].update_state(labels, logits)\n    metrics[\'test/ece\'].update_state(sparse_labels, probs)\n\n  @tf.function\n  def epoch_fn(should_eval):\n    """"""Build `epoch_fn` for training and potential eval.""""""\n    for _ in tf.range(tf.cast(steps_per_epoch, tf.int32)):\n      info = strategy.run(train_step, args=(next(train_iterator),))\n\n      optim_step = optimizer.iterations\n      if optim_step % tf.cast(100, optim_step.dtype) == 0:\n        for k, v in info.items():\n          v_reduce = strategy.reduce(tf.distribute.ReduceOp.SUM, v, None)\n          tf.summary.scalar(k, v_reduce, optim_step)\n        tf.summary.scalar(\'loss/lr\', learning_rate(optim_step), optim_step)\n        summary_writer.flush()\n\n    if should_eval:\n      if isinstance(optimizer, utils.MovingAverage):\n        optimizer.swap_weights(strategy)\n      for _ in tf.range(tf.cast(steps_per_eval, tf.int32)):\n        strategy.run(eval_step, args=(next(test_iterator),))\n      if isinstance(optimizer, utils.MovingAverage):\n        optimizer.swap_weights(strategy)\n\n  # Main training loop.\n  start_time = time.time()\n  with summary_writer.as_default():\n    for epoch in range(initial_epoch, FLAGS.train_epochs):\n      logging.info(\'Starting to run epoch: %s\', epoch)\n      should_eval = (epoch % FLAGS.evaluation_interval == 0)\n      epoch_start_time = time.time()\n      # Pass tf constant to avoid re-tracing.\n      epoch_fn(tf.constant(should_eval))\n      epoch_time = time.time() - epoch_start_time\n      example_per_secs = (steps_per_epoch * batch_size) / epoch_time\n      if not should_eval:\n        tf.summary.scalar(\n            \'examples_per_secs\', example_per_secs, optimizer.iterations)\n        summary_writer.flush()\n\n      current_step = (epoch + 1) * steps_per_epoch\n      max_steps = steps_per_epoch * FLAGS.train_epochs\n      time_elapsed = time.time() - start_time\n      steps_per_sec = float(current_step) / time_elapsed\n      eta_seconds = (max_steps - current_step) / steps_per_sec\n      message = (\'{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. \'\n                 \'ETA: {:.0f} min. Time elapsed: {:.0f} min\'.format(\n                     current_step / max_steps,\n                     epoch + 1,\n                     FLAGS.train_epochs,\n                     steps_per_sec,\n                     eta_seconds / 60,\n                     time_elapsed / 60))\n      logging.info(message)\n\n      logging.info(\'Train Loss: %.4f, Accuracy: %.2f%%\',\n                   metrics[\'train/loss\'].result(),\n                   metrics[\'train/accuracy\'].result() * 100)\n\n      if should_eval:\n        logging.info(\'Test NLL: %.4f, Accuracy: %.2f%%\',\n                     metrics[\'test/negative_log_likelihood\'].result(),\n                     metrics[\'test/accuracy\'].result() * 100)\n\n      total_metrics = metrics.copy()\n      total_results = {name: metric.result()\n                       for name, metric in total_metrics.items()}\n      total_results.update({\'lr\': learning_rate(optimizer.iterations)})\n      with summary_writer.as_default():\n        for name, result in total_results.items():\n          if should_eval or \'test\' not in name:\n            tf.summary.scalar(name, result, step=epoch + 1)\n\n      for metric in metrics.values():\n        metric.reset_states()\n\n      if (FLAGS.checkpoint_interval > 0 and\n          (epoch + 1) % FLAGS.checkpoint_interval == 0):\n        checkpoint_name = checkpoint.save(os.path.join(\n            FLAGS.output_dir, \'checkpoint\'))\n        logging.info(\'Saved checkpoint to %s\', checkpoint_name)\n\n  final_checkpoint_name = checkpoint.save(\n      os.path.join(FLAGS.output_dir, \'checkpoint\'))\n  logging.info(\'Saved last checkpoint to %s\', final_checkpoint_name)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/imagenet/efficientnet_be.py,51,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""EfficientNet with BatchEnsemble.""""""\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport efficientnet_be_model  # local file import\nimport utils  # local file import\nimport tensorflow as tf\n\n# ~312.78 steps per epoch for 4x4 TPU; per_core_batch_size=128; 350 epochs;\n\n# TODO(trandustin): Tune results.\n# General model flags\nflags.DEFINE_enum(\'model_name\',\n                  default=\'efficientnet-b0\',\n                  enum_values=[\'efficientnet-b0\', \'efficientnet-b1\',\n                               \'efficientnet-b2\', \'efficientnet-b3\'],\n                  help=\'Efficientnet model name.\')\nflags.DEFINE_integer(\'ensemble_size\', 4, \'Size of ensemble.\')\nflags.DEFINE_integer(\'per_core_batch_size\', 128, \'Batch size per TPU core/GPU.\')\nflags.DEFINE_float(\'random_sign_init\', -0.5,\n                   \'Use random sign init for fast weights.\')\nflags.DEFINE_integer(\'seed\', 0, \'Random seed.\')\nflags.DEFINE_float(\'base_learning_rate\', 0.016,\n                   \'Base learning rate when train batch size is 256.\')\nflags.DEFINE_float(\'fast_weight_lr_multiplier\', 0.5,\n                   \'fast weights lr multiplier.\')\nflags.DEFINE_float(\'l2\', 5e-6, \'L2 coefficient.\')\nflags.DEFINE_string(\'data_dir\', \'\', \'Path to training and testing data.\')\nflags.mark_flag_as_required(\'data_dir\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/imagenet\',\n                    \'The directory where the model weights and \'\n                    \'training/evaluation summaries are stored.\')\nflags.DEFINE_integer(\'train_epochs\', 350, \'Number of training epochs.\')\nflags.DEFINE_integer(\'checkpoint_interval\', 15,\n                     \'Number of epochs between saving checkpoints. Use -1 to \'\n                     \'never save checkpoints.\')\nflags.DEFINE_integer(\'evaluation_interval\', 5, \'How many epochs to run test.\')\nflags.DEFINE_string(\'alexnet_errors_path\', None,\n                    \'Path to AlexNet corruption errors file.\')\nflags.DEFINE_float(\'label_smoothing\', 0.1, \'label smoothing constant.\')\nflags.DEFINE_integer(\'num_bins\', 15, \'Number of bins for ECE computation.\')\n\n# Accelerator flags.\nflags.DEFINE_bool(\'use_gpu\', False, \'Whether to run on GPU or otherwise TPU.\')\nflags.DEFINE_bool(\'use_bfloat16\', True, \'Whether to use mixed precision.\')\nflags.DEFINE_integer(\'num_cores\', 32, \'Number of TPU cores or number of GPUs.\')\nflags.DEFINE_string(\'tpu\', None,\n                    \'Name of the TPU. Only used if use_gpu is False.\')\nFLAGS = flags.FLAGS\n\n# Number of images in ImageNet-1k train dataset.\nAPPROX_IMAGENET_TRAIN_IMAGES = 1281167\nIMAGENET_VALIDATION_IMAGES = 50000\nNUM_CLASSES = 1000\n\n\ndef main(argv):\n  del argv  # unused arg\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  logging.info(\'Saving checkpoints at %s\', FLAGS.output_dir)\n  tf.random.set_seed(FLAGS.seed)\n\n  per_core_batch_size = FLAGS.per_core_batch_size // FLAGS.ensemble_size\n  batch_size = per_core_batch_size * FLAGS.num_cores\n  steps_per_epoch = APPROX_IMAGENET_TRAIN_IMAGES // batch_size\n  steps_per_eval = IMAGENET_VALIDATION_IMAGES // batch_size\n\n  if FLAGS.use_gpu:\n    logging.info(\'Use GPU\')\n    strategy = tf.distribute.MirroredStrategy()\n  else:\n    logging.info(\'Use TPU at %s\',\n                 FLAGS.tpu if FLAGS.tpu is not None else \'local\')\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n  width_coefficient, depth_coefficient, input_image_size, dropout_rate = (\n      efficientnet_be_model.efficientnet_params(FLAGS.model_name))\n  imagenet_train = utils.ImageNetInput(\n      is_training=True,\n      use_bfloat16=FLAGS.use_bfloat16,\n      data_dir=FLAGS.data_dir,\n      batch_size=FLAGS.per_core_batch_size,\n      image_size=input_image_size,\n      normalize_input=True,\n      one_hot=True)\n  imagenet_eval = utils.ImageNetInput(\n      is_training=False,\n      use_bfloat16=FLAGS.use_bfloat16,\n      data_dir=FLAGS.data_dir,\n      batch_size=batch_size,\n      image_size=input_image_size,\n      normalize_input=True,\n      one_hot=True)\n  train_dataset = strategy.experimental_distribute_datasets_from_function(\n      imagenet_train.input_fn)\n  test_datasets = {\n      \'clean\':\n          strategy.experimental_distribute_dataset(imagenet_eval.input_fn()),\n  }\n  train_iterator = iter(train_dataset)\n  test_iterator = iter(test_datasets[\'clean\'])\n\n  if FLAGS.use_bfloat16:\n    policy = tf.keras.mixed_precision.experimental.Policy(\'mixed_bfloat16\')\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n\n  summary_writer = tf.summary.create_file_writer(\n      os.path.join(FLAGS.output_dir, \'summaries\'))\n\n  with strategy.scope():\n    logging.info(\'Building %s model\', FLAGS.model_name)\n    model = efficientnet_be_model.Model(\n        width_coefficient,\n        depth_coefficient,\n        dropout_rate,\n        ensemble_size=FLAGS.ensemble_size,\n        random_sign_init=FLAGS.random_sign_init)\n\n    scaled_lr = FLAGS.base_learning_rate * (batch_size / 256.0)\n    # Decay epoch is 2.4, warmup epoch is 5 according to the Efficientnet paper.\n    decay_steps = steps_per_epoch * 2.4\n    warmup_step = steps_per_epoch * 5\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        scaled_lr, decay_steps, decay_rate=0.97, staircase=True)\n    learning_rate = utils.WarmupDecaySchedule(lr_schedule, warmup_step)\n    optimizer = tf.keras.optimizers.RMSprop(\n        learning_rate, rho=0.9, momentum=0.9, epsilon=0.001)\n\n    metrics = {\n        \'train/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'train/accuracy\': tf.keras.metrics.CategoricalAccuracy(),\n        \'train/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        \'train/loss\': tf.keras.metrics.Mean(),\n        \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'test/accuracy\': tf.keras.metrics.CategoricalAccuracy(),\n        \'test/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n    }\n    logging.info(\'Finished building %s model\', FLAGS.model_name)\n\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n    initial_epoch = 0\n    if latest_checkpoint:\n      # checkpoint.restore must be within a strategy.scope() so that optimizer\n      # slot variables are mirrored.\n      checkpoint.restore(latest_checkpoint)\n      logging.info(\'Loaded checkpoint %s\', latest_checkpoint)\n      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n\n  def train_step(inputs):\n    """"""Build `step_fn` for efficientnet learning.""""""\n    images, labels = inputs\n    images = tf.tile(images, [FLAGS.ensemble_size, 1, 1, 1])\n    labels = tf.tile(labels, [FLAGS.ensemble_size, 1])\n\n    num_replicas = tf.cast(strategy.num_replicas_in_sync, tf.float32)\n    l2_coeff = tf.cast(FLAGS.l2, tf.float32)\n\n    with tf.GradientTape() as tape:\n      logits = model(images, training=True)\n      logits = tf.cast(logits, tf.float32)\n      negative_log_likelihood = tf.reduce_mean(\n          tf.keras.losses.categorical_crossentropy(\n              labels,\n              logits,\n              from_logits=True,\n              label_smoothing=FLAGS.label_smoothing))\n\n      filtered_variables = []\n      for var in model.trainable_variables:\n        # Apply l2 on the slow weights and bias terms. This excludes BN\n        # parameters and fast weight approximate posterior/prior parameters,\n        # but pay caution to their naming scheme.\n        if \'kernel\' in var.name or \'bias\' in var.name:\n          filtered_variables.append(tf.reshape(var, (-1,)))\n\n      l2_loss = FLAGS.l2 * 2 * tf.nn.l2_loss(\n          tf.concat(filtered_variables, axis=0))\n      loss = negative_log_likelihood + l2_coeff * l2_loss\n      scaled_loss = loss / num_replicas\n\n    grads = tape.gradient(scaled_loss, model.trainable_weights)\n\n    # Separate learning rate implementation.\n    if FLAGS.fast_weight_lr_multiplier != 1.0:\n      grads_and_vars = []\n      for grad, var in zip(grads, model.trainable_variables):\n        # Apply different learning rate on the fast weights. This excludes BN\n        # and slow weights, but pay caution to the naming scheme.\n        if (\'batch_norm\' not in var.name and \'kernel\' not in var.name):\n          grads_and_vars.append((grad * FLAGS.fast_weight_lr_multiplier,\n                                 var))\n        else:\n          grads_and_vars.append((grad, var))\n      optimizer.apply_gradients(grads_and_vars)\n    else:\n      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n    sparse_labels = tf.cast(\n        tf.math.argmax(labels, axis=-1, output_type=tf.int32), tf.float32)\n    probs = tf.nn.softmax(logits)\n    metrics[\'train/loss\'].update_state(loss)\n    metrics[\'train/negative_log_likelihood\'].update_state(\n        negative_log_likelihood)\n    metrics[\'train/accuracy\'].update_state(labels, logits)\n    metrics[\'train/ece\'].update_state(sparse_labels, probs)\n\n    step_info = {\n        \'loss/negative_log_likelihood\': negative_log_likelihood / num_replicas,\n        \'loss/total_loss\': scaled_loss,\n    }\n    return step_info\n\n  def eval_step(inputs):\n    """"""A single step.""""""\n    images, labels = inputs\n    images = tf.tile(images, [FLAGS.ensemble_size, 1, 1, 1])\n    logits = model(images, training=False)\n    logits = tf.cast(logits, tf.float32)\n    probs = tf.nn.softmax(logits)\n    per_probs = tf.split(\n        probs, num_or_size_splits=FLAGS.ensemble_size, axis=0)\n    probs = tf.reduce_mean(per_probs, axis=0)\n\n    negative_log_likelihood = tf.reduce_mean(\n        tf.keras.losses.categorical_crossentropy(labels, probs))\n    sparse_labels = tf.cast(\n        tf.math.argmax(labels, axis=-1, output_type=tf.int32), tf.float32)\n    metrics[\'test/negative_log_likelihood\'].update_state(\n        negative_log_likelihood)\n    metrics[\'test/accuracy\'].update_state(labels, probs)\n    metrics[\'test/ece\'].update_state(sparse_labels, probs)\n\n  @tf.function\n  def epoch_fn(should_eval):\n    """"""Build `epoch_fn` for training and potential eval.""""""\n    for _ in tf.range(tf.cast(steps_per_epoch, tf.int32)):\n      info = strategy.run(train_step, args=(next(train_iterator),))\n\n      optim_step = optimizer.iterations\n      if optim_step % tf.cast(100, optim_step.dtype) == 0:\n        for k, v in info.items():\n          v_reduce = strategy.reduce(tf.distribute.ReduceOp.SUM, v, None)\n          tf.summary.scalar(k, v_reduce, optim_step)\n        tf.summary.scalar(\'loss/lr\', learning_rate(optim_step), optim_step)\n        summary_writer.flush()\n\n    if should_eval:\n      for _ in tf.range(tf.cast(steps_per_eval, tf.int32)):\n        strategy.run(eval_step, args=(next(test_iterator),))\n\n  # Main training loop.\n  start_time = time.time()\n  with summary_writer.as_default():\n    for epoch in range(initial_epoch, FLAGS.train_epochs):\n      logging.info(\'Starting to run epoch: %s\', epoch)\n      should_eval = (epoch % FLAGS.evaluation_interval == 0)\n      # Pass tf constant to avoid re-tracing.\n      epoch_fn(tf.constant(should_eval))\n\n      current_step = (epoch + 1) * steps_per_epoch\n      max_steps = steps_per_epoch * FLAGS.train_epochs\n      time_elapsed = time.time() - start_time\n      steps_per_sec = float(current_step) / time_elapsed\n      eta_seconds = (max_steps - current_step) / steps_per_sec\n      message = (\'{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. \'\n                 \'ETA: {:.0f} min. Time elapsed: {:.0f} min\'.format(\n                     current_step / max_steps,\n                     epoch + 1,\n                     FLAGS.train_epochs,\n                     steps_per_sec,\n                     eta_seconds / 60,\n                     time_elapsed / 60))\n      logging.info(message)\n\n      logging.info(\'Train Loss: %.4f, Accuracy: %.2f%%\',\n                   metrics[\'train/loss\'].result(),\n                   metrics[\'train/accuracy\'].result() * 100)\n\n      if should_eval:\n        logging.info(\'Test NLL: %.4f, Accuracy: %.2f%%\',\n                     metrics[\'test/negative_log_likelihood\'].result(),\n                     metrics[\'test/accuracy\'].result() * 100)\n\n      total_metrics = metrics.copy()\n      total_results = {name: metric.result()\n                       for name, metric in total_metrics.items()}\n      total_results.update({\'lr\': learning_rate(optimizer.iterations)})\n      with summary_writer.as_default():\n        for name, result in total_results.items():\n          if should_eval or \'test\' not in name:\n            tf.summary.scalar(name, result, step=epoch + 1)\n\n      for metric in metrics.values():\n        metric.reset_states()\n\n      if (FLAGS.checkpoint_interval > 0 and\n          (epoch + 1) % FLAGS.checkpoint_interval == 0):\n        checkpoint_name = checkpoint.save(os.path.join(\n            FLAGS.output_dir, \'checkpoint\'))\n        logging.info(\'Saved checkpoint to %s\', checkpoint_name)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/imagenet/efficientnet_be_model.py,11,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""EfficientNet model with BatchEnsemble.""""""\n\nimport collections\nimport functools\nimport math\nimport edward2 as ed\nimport utils  # local file import\nimport tensorflow as tf\n\nBlockArgs = collections.namedtuple(\'BlockArgs\', [\n    \'kernel_size\',\n    \'num_repeat\',\n    \'input_filters\',\n    \'output_filters\',\n    \'expand_ratio\',\n    \'strides\',\n    \'se_ratio\',\n])\n\n\ndef efficientnet_params(model_name):\n  """"""Get efficientnet params based on model name.""""""\n  params_dict = {\n      # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n      \'efficientnet-b0\': (1.0, 1.0, 224, 0.2),\n      \'efficientnet-b1\': (1.0, 1.1, 240, 0.2),\n      \'efficientnet-b2\': (1.1, 1.2, 260, 0.3),\n      \'efficientnet-b3\': (1.2, 1.4, 300, 0.3),\n      \'efficientnet-b4\': (1.4, 1.8, 380, 0.4),\n      \'efficientnet-b5\': (1.6, 2.2, 456, 0.4),\n      \'efficientnet-b6\': (1.8, 2.6, 528, 0.5),\n      \'efficientnet-b7\': (2.0, 3.1, 600, 0.5),\n      \'efficientnet-b8\': (2.2, 3.6, 672, 0.5),\n      \'efficientnet-l2\': (4.3, 5.3, 800, 0.5),\n  }\n  return params_dict[model_name]\n\n\ndef round_filters(filters, width_coefficient, depth_divisor, min_depth):\n  """"""Round number of filters based on depth multiplier.""""""\n  filters *= width_coefficient\n  min_depth = min_depth or depth_divisor\n  new_filters = max(\n      min_depth,\n      int(filters + depth_divisor / 2) // depth_divisor * depth_divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_filters < 0.9 * filters:\n    new_filters += depth_divisor\n  return int(new_filters)\n\n\ndef round_repeats(repeats, depth_coefficient):\n  """"""Round number of filters based on depth multiplier.""""""\n  return int(math.ceil(depth_coefficient * repeats))\n\n\ndef make_sign_initializer(random_sign_init):\n  if random_sign_init > 0:\n    initializer = ed.initializers.RandomSign(random_sign_init)\n  else:\n    initializer = tf.keras.initializers.RandomNormal(mean=1.0,\n                                                     stddev=-random_sign_init)\n  return initializer\n\n\nclass MBConvBlock(tf.keras.layers.Layer):\n  """"""A class of MBConv: Mobile Inverted Residual Bottleneck.""""""\n\n  def __init__(self,\n               block_args,\n               ensemble_size,\n               random_sign_init,\n               batch_norm_momentum,\n               batch_norm_epsilon,\n               batch_norm,\n               data_format,\n               relu_fn,\n               use_se,\n               clip_projection_output):\n    """"""Initializes a MBConv block.\n\n    Args:\n      block_args: BlockArgs, arguments to create a Block.\n      ensemble_size: Size of ensemble.\n      random_sign_init: Probability/stddev for fast weight initialization.\n      batch_norm_momentum: Momentum for batch normalization.\n      batch_norm_epsilon: Epsilon for batch normalization.\n      batch_norm: Batch norm layer.\n      data_format: Image data format.\n      relu_fn: Activation.\n      use_se: Whether to use squeeze and excitation layers.\n      clip_projection_output: Whether to clip projected conv outputs.\n    """"""\n    super(MBConvBlock, self).__init__()\n    self._block_args = block_args\n    self._ensemble_size = ensemble_size\n    self._random_sign_init = random_sign_init\n    self._batch_norm_momentum = batch_norm_momentum\n    self._batch_norm_epsilon = batch_norm_epsilon\n    self._batch_norm = batch_norm\n    self._data_format = data_format\n    if self._data_format == \'channels_first\':\n      self._channel_axis = 1\n      self._spatial_dims = [2, 3]\n    else:\n      self._channel_axis = -1\n      self._spatial_dims = [1, 2]\n\n    self._relu_fn = relu_fn\n    self._has_se = (\n        use_se and self._block_args.se_ratio is not None and\n        0 < self._block_args.se_ratio <= 1)\n    self._clip_projection_output = clip_projection_output\n    self._build()\n\n  def _build(self):\n    """"""Builds block according to the arguments.""""""\n    filters = self._block_args.input_filters * self._block_args.expand_ratio\n    kernel_size = self._block_args.kernel_size\n    self._expand_conv = ed.layers.Conv2DBatchEnsemble(\n        filters=filters,\n        kernel_size=[1, 1],\n        alpha_initializer=make_sign_initializer(self._random_sign_init),\n        gamma_initializer=make_sign_initializer(self._random_sign_init),\n        ensemble_size=self._ensemble_size,\n        strides=[1, 1],\n        kernel_initializer=utils.conv_kernel_initializer,\n        padding=\'same\',\n        data_format=self._data_format,\n        use_bias=False)\n    self._bn0 = self._batch_norm(\n        axis=self._channel_axis,\n        momentum=self._batch_norm_momentum,\n        epsilon=self._batch_norm_epsilon)\n    self._depthwise_conv = ed.layers.DepthwiseConv2DBatchEnsemble(\n        kernel_size=[kernel_size, kernel_size],\n        alpha_initializer=make_sign_initializer(self._random_sign_init),\n        gamma_initializer=make_sign_initializer(self._random_sign_init),\n        ensemble_size=self._ensemble_size,\n        strides=self._block_args.strides,\n        depthwise_initializer=utils.conv_kernel_initializer,\n        padding=\'same\',\n        data_format=self._data_format,\n        use_bias=False)\n    self._bn1 = self._batch_norm(\n        axis=self._channel_axis,\n        momentum=self._batch_norm_momentum,\n        epsilon=self._batch_norm_epsilon)\n    if self._has_se:\n      num_reduced_filters = max(\n          1, int(self._block_args.input_filters * self._block_args.se_ratio))\n      self._se_reduce = ed.layers.Conv2DBatchEnsemble(\n          num_reduced_filters,\n          kernel_size=[1, 1],\n          alpha_initializer=make_sign_initializer(self._random_sign_init),\n          gamma_initializer=make_sign_initializer(self._random_sign_init),\n          ensemble_size=self._ensemble_size,\n          strides=[1, 1],\n          kernel_initializer=utils.conv_kernel_initializer,\n          padding=\'same\',\n          data_format=self._data_format,\n          use_bias=True)\n      self._se_expand = ed.layers.Conv2DBatchEnsemble(\n          filters,\n          kernel_size=[1, 1],\n          alpha_initializer=make_sign_initializer(self._random_sign_init),\n          gamma_initializer=make_sign_initializer(self._random_sign_init),\n          ensemble_size=self._ensemble_size,\n          strides=[1, 1],\n          kernel_initializer=utils.conv_kernel_initializer,\n          padding=\'same\',\n          data_format=self._data_format,\n          use_bias=True)\n\n    filters = self._block_args.output_filters\n    self._project_conv = ed.layers.Conv2DBatchEnsemble(\n        filters=filters,\n        kernel_size=[1, 1],\n        alpha_initializer=make_sign_initializer(self._random_sign_init),\n        gamma_initializer=make_sign_initializer(self._random_sign_init),\n        ensemble_size=self._ensemble_size,\n        strides=[1, 1],\n        kernel_initializer=utils.conv_kernel_initializer,\n        padding=\'same\',\n        data_format=self._data_format,\n        use_bias=False)\n    self._bn2 = self._batch_norm(\n        axis=self._channel_axis,\n        momentum=self._batch_norm_momentum,\n        epsilon=self._batch_norm_epsilon)\n\n  def call(self, inputs, training=True, survival_prob=None):\n    """"""Implementation of call().\n\n    Args:\n      inputs: the inputs tensor.\n      training: boolean, whether the model is constructed for training.\n      survival_prob: float, between 0 to 1, drop connect rate.\n\n    Returns:\n      A output tensor.\n    """"""\n    x = inputs\n    if self._block_args.expand_ratio != 1:\n      x = self._relu_fn(self._bn0(self._expand_conv(x), training=training))\n    x = self._relu_fn(self._bn1(self._depthwise_conv(x), training=training))\n\n    if self._has_se:\n      se_tensor = tf.reduce_mean(\n          x, self._spatial_dims, keepdims=True)\n      se_tensor = self._se_expand(self._relu_fn(self._se_reduce(se_tensor)))\n      x = tf.sigmoid(se_tensor) * x\n\n    x = self._bn2(self._project_conv(x), training=training)\n    # Add identity so that quantization-aware training can insert quantization\n    # ops correctly.\n    x = tf.identity(x)\n    if self._clip_projection_output:\n      x = tf.clip_by_value(x, -6, 6)\n    if all(\n        s == 1 for s in self._block_args.strides\n    ) and self._block_args.input_filters == self._block_args.output_filters:\n      if survival_prob:\n        x = utils.drop_connect(x, training, survival_prob)\n      x = tf.add(x, inputs)\n    return x\n\n\nclass Model(tf.keras.Model):\n  """"""EfficientNet.""""""\n\n  def __init__(self,\n               width_coefficient,\n               depth_coefficient,\n               dropout_rate,\n               ensemble_size,\n               random_sign_init,\n               batch_norm_momentum=0.99,\n               batch_norm_epsilon=1e-3,\n               survival_prob=0.8,\n               data_format=\'channels_last\',\n               num_classes=1000,\n               depth_divisor=8,\n               min_depth=None,\n               relu_fn=tf.nn.swish,\n               batch_norm=utils.SyncBatchNorm,  # TPU-specific requirement.\n               use_se=True,\n               clip_projection_output=False):\n    """"""Initializes model instance.\n\n    Args:\n      width_coefficient: Coefficient to scale width.\n      depth_coefficient: Coefficient to scale depth.\n      dropout_rate: Dropout rate.\n      ensemble_size: Size of ensemble.\n      random_sign_init: Probability/stddev for fast weight initialization.\n      batch_norm_momentum: Momentum for batch normalization.\n      batch_norm_epsilon: Epsilon for batch normalization.\n      survival_prob: float, survival probability for stochastic depth.\n      data_format: Image data format.\n      num_classes: Number of output classes.\n      depth_divisor: Divisor to divide filters per conv when rounding.\n      min_depth: Minimum depth per conv when rounding filters.\n      relu_fn: Activation.\n      batch_norm: Batch norm layer.\n      use_se: Whether to use squeeze and excitation layers.\n      clip_projection_output: Whether to clip projected conv outputs.\n    """"""\n    super(Model, self).__init__()\n    self._width_coefficient = width_coefficient\n    self._depth_coefficient = depth_coefficient\n    self._dropout_rate = dropout_rate\n    self._ensemble_size = ensemble_size\n    self._random_sign_init = random_sign_init\n    self._batch_norm_momentum = batch_norm_momentum\n    self._batch_norm_epsilon = batch_norm_epsilon\n    self._survival_prob = survival_prob\n    self._data_format = data_format\n    self._num_classes = num_classes\n    self._depth_divisor = depth_divisor\n    self._min_depth = min_depth\n    self._relu_fn = relu_fn\n    self._batch_norm = batch_norm\n    self._use_se = use_se\n    self._clip_projection_output = clip_projection_output\n    self._build()\n\n  def _build(self):\n    """"""Builds a model.""""""\n    if self._data_format == \'channels_first\':\n      channel_axis = 1\n      self._spatial_dims = [2, 3]\n    else:\n      channel_axis = -1\n      self._spatial_dims = [1, 2]\n\n    self._conv_stem = ed.layers.Conv2DBatchEnsemble(\n        filters=round_filters(32,\n                              self._width_coefficient,\n                              self._depth_divisor,\n                              self._min_depth),\n        kernel_size=[3, 3],\n        alpha_initializer=make_sign_initializer(self._random_sign_init),\n        gamma_initializer=make_sign_initializer(self._random_sign_init),\n        ensemble_size=self._ensemble_size,\n        strides=[2, 2],\n        kernel_initializer=utils.conv_kernel_initializer,\n        padding=\'same\',\n        data_format=self._data_format,\n        use_bias=False)\n    self._bn0 = self._batch_norm(\n        axis=channel_axis,\n        momentum=self._batch_norm_momentum,\n        epsilon=self._batch_norm_epsilon)\n\n    Block = functools.partial(  # pylint: disable=invalid-name\n        MBConvBlock,\n        ensemble_size=self._ensemble_size,\n        random_sign_init=self._random_sign_init,\n        batch_norm_momentum=self._batch_norm_momentum,\n        batch_norm_epsilon=self._batch_norm_epsilon,\n        batch_norm=self._batch_norm,\n        data_format=self._data_format,\n        relu_fn=self._relu_fn,\n        use_se=self._use_se,\n        clip_projection_output=self._clip_projection_output)\n    self._blocks = []\n    blocks_args = [\n        BlockArgs(kernel_size=3,\n                  num_repeat=1,\n                  input_filters=32,\n                  output_filters=16,\n                  expand_ratio=1,\n                  strides=[1, 1],\n                  se_ratio=0.25),\n        BlockArgs(kernel_size=3,\n                  num_repeat=2,\n                  input_filters=16,\n                  output_filters=24,\n                  expand_ratio=6,\n                  strides=[2, 2],\n                  se_ratio=0.25),\n        BlockArgs(kernel_size=5,\n                  num_repeat=2,\n                  input_filters=24,\n                  output_filters=40,\n                  expand_ratio=6,\n                  strides=[2, 2],\n                  se_ratio=0.25),\n        BlockArgs(kernel_size=3,\n                  num_repeat=3,\n                  input_filters=40,\n                  output_filters=80,\n                  expand_ratio=6,\n                  strides=[2, 2],\n                  se_ratio=0.25),\n        BlockArgs(kernel_size=5,\n                  num_repeat=3,\n                  input_filters=80,\n                  output_filters=112,\n                  expand_ratio=6,\n                  strides=[1, 1],\n                  se_ratio=0.25),\n        BlockArgs(kernel_size=5,\n                  num_repeat=4,\n                  input_filters=112,\n                  output_filters=192,\n                  expand_ratio=6,\n                  strides=[2, 2],\n                  se_ratio=0.25),\n        BlockArgs(kernel_size=3,\n                  num_repeat=1,\n                  input_filters=192,\n                  output_filters=320,\n                  expand_ratio=6,\n                  strides=[1, 1],\n                  se_ratio=0.25),\n    ]\n    for block_args in blocks_args:\n      # Update block input and output filters based on depth multiplier.\n      input_filters = round_filters(block_args.input_filters,\n                                    self._width_coefficient,\n                                    self._depth_divisor,\n                                    self._min_depth)\n      output_filters = round_filters(block_args.output_filters,\n                                     self._width_coefficient,\n                                     self._depth_divisor,\n                                     self._min_depth)\n      repeats = round_repeats(block_args.num_repeat,\n                              self._depth_coefficient)\n      block_args = block_args._replace(\n          input_filters=input_filters,\n          output_filters=output_filters,\n          num_repeat=repeats)\n      self._blocks.append(Block(block_args))\n\n      if block_args.num_repeat > 1:\n        # pylint: disable=protected-access\n        block_args = block_args._replace(\n            input_filters=block_args.output_filters, strides=[1, 1])\n        # pylint: enable=protected-access\n      for _ in range(block_args.num_repeat - 1):\n        self._blocks.append(Block(block_args))\n\n    self._conv_head = ed.layers.Conv2DBatchEnsemble(\n        filters=round_filters(1280,\n                              self._width_coefficient,\n                              self._depth_divisor,\n                              self._min_depth),\n        kernel_size=[1, 1],\n        alpha_initializer=make_sign_initializer(self._random_sign_init),\n        gamma_initializer=make_sign_initializer(self._random_sign_init),\n        ensemble_size=self._ensemble_size,\n        strides=[1, 1],\n        kernel_initializer=utils.conv_kernel_initializer,\n        padding=\'same\',\n        use_bias=False)\n    self._bn1 = self._batch_norm(\n        axis=channel_axis,\n        momentum=self._batch_norm_momentum,\n        epsilon=self._batch_norm_epsilon)\n    self._avg_pooling = tf.keras.layers.GlobalAveragePooling2D(\n        data_format=self._data_format)\n    if self._dropout_rate > 0:\n      self._dropout = tf.keras.layers.Dropout(self._dropout_rate)\n    else:\n      self._dropout = None\n    self._fc = ed.layers.DenseBatchEnsemble(\n        self._num_classes,\n        alpha_initializer=make_sign_initializer(self._random_sign_init),\n        gamma_initializer=make_sign_initializer(self._random_sign_init),\n        ensemble_size=self._ensemble_size,\n        kernel_initializer=utils.dense_kernel_initializer)\n\n  def call(self, inputs, training=True):\n    """"""Implementation of call().\n\n    Args:\n      inputs: input tensors.\n      training: boolean, whether the model is constructed for training.\n\n    Returns:\n      output tensors.\n    """"""\n    outputs = self._relu_fn(\n        self._bn0(self._conv_stem(inputs), training=training))\n\n    for idx, block in enumerate(self._blocks):\n      survival_prob = self._survival_prob\n      if survival_prob:\n        drop_rate = 1.0 - survival_prob\n        survival_prob = 1.0 - drop_rate * float(idx) / len(self._blocks)\n      outputs = block.call(\n          outputs, training=training, survival_prob=survival_prob)\n\n    outputs = self._relu_fn(\n        self._bn1(self._conv_head(outputs), training=training))\n    outputs = self._avg_pooling(outputs)\n    if self._dropout:\n      outputs = self._dropout(outputs, training=training)\n    outputs = self._fc(outputs)\n    return outputs\n'"
baselines/imagenet/efficientnet_model.py,18,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""EfficientNet model adopted from official estimator version for tf2.\n\n[1] Mingxing Tan, Quoc V. Le\n  EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\n  ICML\'19, https://arxiv.org/abs/1905.11946.\n""""""\n\nimport collections\nimport functools\nimport math\nimport utils  # local file import\nimport tensorflow as tf\n\nBlockArgs = collections.namedtuple(\'BlockArgs\', [\n    \'kernel_size\',\n    \'num_repeat\',\n    \'input_filters\',\n    \'output_filters\',\n    \'expand_ratio\',\n    \'strides\',\n    \'se_ratio\',\n])\n\n\ndef efficientnet_params(model_name):\n  """"""Get efficientnet params based on model name.""""""\n  params_dict = {\n      # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n      \'efficientnet-b0\': (1.0, 1.0, 224, 0.2),\n      \'efficientnet-b1\': (1.0, 1.1, 240, 0.2),\n      \'efficientnet-b2\': (1.1, 1.2, 260, 0.3),\n      \'efficientnet-b3\': (1.2, 1.4, 300, 0.3),\n      \'efficientnet-b4\': (1.4, 1.8, 380, 0.4),\n      \'efficientnet-b5\': (1.6, 2.2, 456, 0.4),\n      \'efficientnet-b6\': (1.8, 2.6, 528, 0.5),\n      \'efficientnet-b7\': (2.0, 3.1, 600, 0.5),\n      \'efficientnet-b8\': (2.2, 3.6, 672, 0.5),\n      \'efficientnet-l2\': (4.3, 5.3, 800, 0.5),\n  }\n  return params_dict[model_name]\n\n\ndef round_filters(filters, width_coefficient, depth_divisor, min_depth):\n  """"""Round number of filters based on depth multiplier.""""""\n  filters *= width_coefficient\n  min_depth = min_depth or depth_divisor\n  new_filters = max(\n      min_depth,\n      int(filters + depth_divisor / 2) // depth_divisor * depth_divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_filters < 0.9 * filters:\n    new_filters += depth_divisor\n  return int(new_filters)\n\n\ndef round_repeats(repeats, depth_coefficient):\n  """"""Round number of filters based on depth multiplier.""""""\n  return int(math.ceil(depth_coefficient * repeats))\n\n\nclass MBConvBlock(tf.keras.layers.Layer):\n  """"""A class of MBConv: Mobile Inverted Residual Bottleneck.""""""\n\n  def __init__(self,\n               block_args,\n               batch_norm_momentum,\n               batch_norm_epsilon,\n               batch_norm,\n               data_format,\n               relu_fn,\n               use_se,\n               clip_projection_output):\n    """"""Initializes a MBConv block.\n\n    Args:\n      block_args: BlockArgs, arguments to create a Block.\n      batch_norm_momentum: Momentum for batch normalization.\n      batch_norm_epsilon: Epsilon for batch normalization.\n      batch_norm: Batch norm layer.\n      data_format: Image data format.\n      relu_fn: Activation.\n      use_se: Whether to use squeeze and excitation layers.\n      clip_projection_output: Whether to clip projected conv outputs.\n    """"""\n    super(MBConvBlock, self).__init__()\n    self._block_args = block_args\n    self._batch_norm_momentum = batch_norm_momentum\n    self._batch_norm_epsilon = batch_norm_epsilon\n    self._batch_norm = batch_norm\n    self._data_format = data_format\n    if self._data_format == \'channels_first\':\n      self._channel_axis = 1\n      self._spatial_dims = [2, 3]\n    else:\n      self._channel_axis = -1\n      self._spatial_dims = [1, 2]\n\n    self._relu_fn = relu_fn\n    self._has_se = (\n        use_se and self._block_args.se_ratio is not None and\n        0 < self._block_args.se_ratio <= 1)\n    self._clip_projection_output = clip_projection_output\n    self._build()\n\n  def _build(self):\n    """"""Builds block according to the arguments.""""""\n    filters = self._block_args.input_filters * self._block_args.expand_ratio\n    kernel_size = self._block_args.kernel_size\n    self._expand_conv = tf.keras.layers.Conv2D(\n        filters=filters,\n        kernel_size=[1, 1],\n        strides=[1, 1],\n        kernel_initializer=utils.conv_kernel_initializer,\n        padding=\'same\',\n        data_format=self._data_format,\n        use_bias=False)\n    self._bn0 = self._batch_norm(\n        axis=self._channel_axis,\n        momentum=self._batch_norm_momentum,\n        epsilon=self._batch_norm_epsilon)\n    self._depthwise_conv = tf.keras.layers.DepthwiseConv2D(\n        kernel_size=[kernel_size, kernel_size],\n        strides=self._block_args.strides,\n        depthwise_initializer=utils.conv_kernel_initializer,\n        padding=\'same\',\n        data_format=self._data_format,\n        use_bias=False)\n    self._bn1 = self._batch_norm(\n        axis=self._channel_axis,\n        momentum=self._batch_norm_momentum,\n        epsilon=self._batch_norm_epsilon)\n    if self._has_se:\n      num_reduced_filters = max(\n          1, int(self._block_args.input_filters * self._block_args.se_ratio))\n      self._se_reduce = tf.keras.layers.Conv2D(\n          num_reduced_filters,\n          kernel_size=[1, 1],\n          strides=[1, 1],\n          kernel_initializer=utils.conv_kernel_initializer,\n          padding=\'same\',\n          data_format=self._data_format,\n          use_bias=True)\n      self._se_expand = tf.keras.layers.Conv2D(\n          filters,\n          kernel_size=[1, 1],\n          strides=[1, 1],\n          kernel_initializer=utils.conv_kernel_initializer,\n          padding=\'same\',\n          data_format=self._data_format,\n          use_bias=True)\n\n    filters = self._block_args.output_filters\n    self._project_conv = tf.keras.layers.Conv2D(\n        filters=filters,\n        kernel_size=[1, 1],\n        strides=[1, 1],\n        kernel_initializer=utils.conv_kernel_initializer,\n        padding=\'same\',\n        data_format=self._data_format,\n        use_bias=False)\n    self._bn2 = self._batch_norm(\n        axis=self._channel_axis,\n        momentum=self._batch_norm_momentum,\n        epsilon=self._batch_norm_epsilon)\n\n  def call(self, inputs, training=True, survival_prob=None):\n    """"""Implementation of call().\n\n    Args:\n      inputs: the inputs tensor.\n      training: boolean, whether the model is constructed for training.\n      survival_prob: float, between 0 to 1, drop connect rate.\n\n    Returns:\n      A output tensor.\n    """"""\n    x = inputs\n    if self._block_args.expand_ratio != 1:\n      x = self._relu_fn(self._bn0(self._expand_conv(x), training=training))\n    x = self._relu_fn(self._bn1(self._depthwise_conv(x), training=training))\n\n    if self._has_se:\n      se_tensor = tf.reduce_mean(\n          x, self._spatial_dims, keepdims=True)\n      se_tensor = self._se_expand(self._relu_fn(self._se_reduce(se_tensor)))\n      x = tf.sigmoid(se_tensor) * x\n\n    x = self._bn2(self._project_conv(x), training=training)\n    # Add identity so that quantization-aware training can insert quantization\n    # ops correctly.\n    x = tf.identity(x)\n    if self._clip_projection_output:\n      x = tf.clip_by_value(x, -6, 6)\n    if all(\n        s == 1 for s in self._block_args.strides\n    ) and self._block_args.input_filters == self._block_args.output_filters:\n      if survival_prob:\n        x = utils.drop_connect(x, training, survival_prob)\n      x = tf.add(x, inputs)\n    return x\n\n\nclass Model(tf.keras.Model):\n  """"""EfficientNet.""""""\n\n  def __init__(self,\n               width_coefficient,\n               depth_coefficient,\n               dropout_rate,\n               batch_norm_momentum=0.99,\n               batch_norm_epsilon=1e-3,\n               survival_prob=0.8,\n               data_format=\'channels_last\',\n               num_classes=1000,\n               depth_divisor=8,\n               min_depth=None,\n               relu_fn=tf.nn.swish,\n               batch_norm=utils.SyncBatchNorm,  # TPU-specific requirement.\n               use_se=True,\n               clip_projection_output=False):\n    """"""Initializes model instance.\n\n    Args:\n      width_coefficient: Coefficient to scale width.\n      depth_coefficient: Coefficient to scale depth.\n      dropout_rate: Dropout rate.\n      batch_norm_momentum: Momentum for batch normalization.\n      batch_norm_epsilon: Epsilon for batch normalization.\n      survival_prob: float, survival probability for stochastic depth.\n      data_format: Image data format.\n      num_classes: Number of output classes.\n      depth_divisor: Divisor to divide filters per conv when rounding.\n      min_depth: Minimum depth per conv when rounding filters.\n      relu_fn: Activation.\n      batch_norm: Batch norm layer.\n      use_se: Whether to use squeeze and excitation layers.\n      clip_projection_output: Whether to clip projected conv outputs.\n    """"""\n    super(Model, self).__init__()\n    self._width_coefficient = width_coefficient\n    self._depth_coefficient = depth_coefficient\n    self._dropout_rate = dropout_rate\n    self._batch_norm_momentum = batch_norm_momentum\n    self._batch_norm_epsilon = batch_norm_epsilon\n    self._survival_prob = survival_prob\n    self._data_format = data_format\n    self._num_classes = num_classes\n    self._depth_divisor = depth_divisor\n    self._min_depth = min_depth\n    self._relu_fn = relu_fn\n    self._batch_norm = batch_norm\n    self._use_se = use_se\n    self._clip_projection_output = clip_projection_output\n    self._build()\n\n  def _build(self):\n    """"""Builds a model.""""""\n    if self._data_format == \'channels_first\':\n      channel_axis = 1\n      self._spatial_dims = [2, 3]\n    else:\n      channel_axis = -1\n      self._spatial_dims = [1, 2]\n\n    self._conv_stem = tf.keras.layers.Conv2D(\n        filters=round_filters(32,\n                              self._width_coefficient,\n                              self._depth_divisor,\n                              self._min_depth),\n        kernel_size=[3, 3],\n        strides=[2, 2],\n        kernel_initializer=utils.conv_kernel_initializer,\n        padding=\'same\',\n        data_format=self._data_format,\n        use_bias=False)\n    self._bn0 = self._batch_norm(\n        axis=channel_axis,\n        momentum=self._batch_norm_momentum,\n        epsilon=self._batch_norm_epsilon)\n\n    Block = functools.partial(  # pylint: disable=invalid-name\n        MBConvBlock,\n        batch_norm_momentum=self._batch_norm_momentum,\n        batch_norm_epsilon=self._batch_norm_epsilon,\n        batch_norm=self._batch_norm,\n        data_format=self._data_format,\n        relu_fn=self._relu_fn,\n        use_se=self._use_se,\n        clip_projection_output=self._clip_projection_output)\n    self._blocks = []\n    blocks_args = [\n        BlockArgs(kernel_size=3,\n                  num_repeat=1,\n                  input_filters=32,\n                  output_filters=16,\n                  expand_ratio=1,\n                  strides=[1, 1],\n                  se_ratio=0.25),\n        BlockArgs(kernel_size=3,\n                  num_repeat=2,\n                  input_filters=16,\n                  output_filters=24,\n                  expand_ratio=6,\n                  strides=[2, 2],\n                  se_ratio=0.25),\n        BlockArgs(kernel_size=5,\n                  num_repeat=2,\n                  input_filters=24,\n                  output_filters=40,\n                  expand_ratio=6,\n                  strides=[2, 2],\n                  se_ratio=0.25),\n        BlockArgs(kernel_size=3,\n                  num_repeat=3,\n                  input_filters=40,\n                  output_filters=80,\n                  expand_ratio=6,\n                  strides=[2, 2],\n                  se_ratio=0.25),\n        BlockArgs(kernel_size=5,\n                  num_repeat=3,\n                  input_filters=80,\n                  output_filters=112,\n                  expand_ratio=6,\n                  strides=[1, 1],\n                  se_ratio=0.25),\n        BlockArgs(kernel_size=5,\n                  num_repeat=4,\n                  input_filters=112,\n                  output_filters=192,\n                  expand_ratio=6,\n                  strides=[2, 2],\n                  se_ratio=0.25),\n        BlockArgs(kernel_size=3,\n                  num_repeat=1,\n                  input_filters=192,\n                  output_filters=320,\n                  expand_ratio=6,\n                  strides=[1, 1],\n                  se_ratio=0.25),\n    ]\n    for block_args in blocks_args:\n      # Update block input and output filters based on depth multiplier.\n      input_filters = round_filters(block_args.input_filters,\n                                    self._width_coefficient,\n                                    self._depth_divisor,\n                                    self._min_depth)\n      output_filters = round_filters(block_args.output_filters,\n                                     self._width_coefficient,\n                                     self._depth_divisor,\n                                     self._min_depth)\n      repeats = round_repeats(block_args.num_repeat,\n                              self._depth_coefficient)\n      block_args = block_args._replace(\n          input_filters=input_filters,\n          output_filters=output_filters,\n          num_repeat=repeats)\n      self._blocks.append(Block(block_args))\n\n      if block_args.num_repeat > 1:\n        # pylint: disable=protected-access\n        block_args = block_args._replace(\n            input_filters=block_args.output_filters, strides=[1, 1])\n        # pylint: enable=protected-access\n      for _ in range(block_args.num_repeat - 1):\n        self._blocks.append(Block(block_args))\n\n    self._conv_head = tf.keras.layers.Conv2D(\n        filters=round_filters(1280,\n                              self._width_coefficient,\n                              self._depth_divisor,\n                              self._min_depth),\n        kernel_size=[1, 1],\n        strides=[1, 1],\n        kernel_initializer=utils.conv_kernel_initializer,\n        padding=\'same\',\n        use_bias=False)\n    self._bn1 = self._batch_norm(\n        axis=channel_axis,\n        momentum=self._batch_norm_momentum,\n        epsilon=self._batch_norm_epsilon)\n    self._avg_pooling = tf.keras.layers.GlobalAveragePooling2D(\n        data_format=self._data_format)\n    if self._dropout_rate > 0:\n      self._dropout = tf.keras.layers.Dropout(self._dropout_rate)\n    else:\n      self._dropout = None\n    self._fc = tf.keras.layers.Dense(\n        self._num_classes,\n        kernel_initializer=utils.dense_kernel_initializer)\n\n  def call(self, inputs, training=True):\n    """"""Implementation of call().\n\n    Args:\n      inputs: input tensors.\n      training: boolean, whether the model is constructed for training.\n\n    Returns:\n      output tensors.\n    """"""\n    outputs = self._relu_fn(\n        self._bn0(self._conv_stem(inputs), training=training))\n\n    for idx, block in enumerate(self._blocks):\n      survival_prob = self._survival_prob\n      if survival_prob:\n        drop_rate = 1.0 - survival_prob\n        survival_prob = 1.0 - drop_rate * float(idx) / len(self._blocks)\n      outputs = block.call(\n          outputs, training=training, survival_prob=survival_prob)\n\n    outputs = self._relu_fn(\n        self._bn1(self._conv_head(outputs), training=training))\n    outputs = self._avg_pooling(outputs)\n    if self._dropout:\n      outputs = self._dropout(outputs, training=training)\n    outputs = self._fc(outputs)\n    return outputs\n'"
baselines/imagenet/ensemble.py,35,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Ensemble on ImageNet.\n\nThis script only performs evaluation, not training. We recommend training\nensembles by launching independent runs of `deterministic.py` over different\nseeds.\n""""""\n\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport deterministic_model  # local file import\nimport utils  # local file import\nimport numpy as np\nimport tensorflow as tf\n\nflags.DEFINE_integer(\'per_core_batch_size\', 512, \'Batch size per TPU core/GPU.\')\nflags.DEFINE_integer(\'seed\', 0, \'Random seed.\')\nflags.DEFINE_string(\'data_dir\', None, \'Path to training and testing data.\')\nflags.mark_flag_as_required(\'data_dir\')\nflags.DEFINE_string(\'checkpoint_dir\', None,\n                    \'The directory where the model weights are stored.\')\nflags.mark_flag_as_required(\'checkpoint_dir\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/imagenet\',\n                    \'The directory where to save predictions.\')\nflags.DEFINE_string(\'alexnet_errors_path\', None,\n                    \'Path to AlexNet corruption errors file.\')\nflags.DEFINE_integer(\'num_bins\', 15, \'Number of bins for ECE computation.\')\n\n# Accelerator flags.\nflags.DEFINE_bool(\'use_gpu\', True, \'Whether to run on GPU or otherwise TPU.\')\nflags.DEFINE_integer(\'num_cores\', 1, \'Number of TPU cores or number of GPUs.\')\nflags.DEFINE_string(\'tpu\', None,\n                    \'Name of the TPU. Only used if use_gpu is False.\')\nFLAGS = flags.FLAGS\n\n# Number of images in eval dataset.\nIMAGENET_VALIDATION_IMAGES = 50000\nNUM_CLASSES = 1000\n\n\ndef ensemble_negative_log_likelihood(labels, logits):\n  """"""Negative log-likelihood for ensemble.\n\n  For each datapoint (x,y), the ensemble\'s negative log-likelihood is:\n\n  ```\n  -log p(y|x) = -log sum_{m=1}^{ensemble_size} exp(log p(y|x,theta_m)) +\n                log ensemble_size.\n  ```\n\n  Args:\n    labels: tf.Tensor of shape [...].\n    logits: tf.Tensor of shape [ensemble_size, ..., num_classes].\n\n  Returns:\n    tf.Tensor of shape [...].\n  """"""\n  labels = tf.cast(labels, tf.int32)\n  logits = tf.convert_to_tensor(logits)\n  ensemble_size = float(logits.shape[0])\n  nll = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      tf.broadcast_to(labels[tf.newaxis, ...], tf.shape(logits)[:-1]),\n      logits)\n  return -tf.reduce_logsumexp(-nll, axis=0) + tf.math.log(ensemble_size)\n\n\ndef gibbs_cross_entropy(labels, logits):\n  """"""Average cross entropy for ensemble members (Gibbs cross entropy).\n\n  For each datapoint (x,y), the ensemble\'s Gibbs cross entropy is:\n\n  ```\n  GCE = - (1/ensemble_size) sum_{m=1}^ensemble_size log p(y|x,theta_m).\n  ```\n\n  The Gibbs cross entropy approximates the average cross entropy of a single\n  model drawn from the (Gibbs) ensemble.\n\n  Args:\n    labels: tf.Tensor of shape [...].\n    logits: tf.Tensor of shape [ensemble_size, ..., num_classes].\n\n  Returns:\n    tf.Tensor of shape [...].\n  """"""\n  labels = tf.cast(labels, tf.int32)\n  logits = tf.convert_to_tensor(logits)\n  nll = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      tf.broadcast_to(labels[tf.newaxis, ...], tf.shape(logits)[:-1]),\n      logits)\n  return tf.reduce_mean(nll, axis=0)\n\n\ndef main(argv):\n  del argv  # unused arg\n  if not FLAGS.use_gpu:\n    raise ValueError(\'Only GPU is currently supported.\')\n  if FLAGS.num_cores > 1:\n    raise ValueError(\'Only a single accelerator is currently supported.\')\n  tf.random.set_seed(FLAGS.seed)\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n\n  batch_size = FLAGS.per_core_batch_size * FLAGS.num_cores\n  steps_per_eval = IMAGENET_VALIDATION_IMAGES // batch_size\n\n  dataset_test = utils.ImageNetInput(\n      is_training=False,\n      data_dir=FLAGS.data_dir,\n      batch_size=FLAGS.per_core_batch_size,\n      use_bfloat16=False).input_fn()\n  test_datasets = {\'clean\': dataset_test}\n  corruption_types, max_intensity = utils.load_corrupted_test_info()\n  for name in corruption_types:\n    for intensity in range(1, max_intensity + 1):\n      dataset_name = \'{0}_{1}\'.format(name, intensity)\n      test_datasets[dataset_name] = utils.load_corrupted_test_dataset(\n          name=name,\n          intensity=intensity,\n          batch_size=FLAGS.per_core_batch_size,\n          drop_remainder=True,\n          use_bfloat16=False)\n\n  model = deterministic_model.resnet50(input_shape=(224, 224, 3),\n                                       num_classes=NUM_CLASSES)\n\n  logging.info(\'Model input shape: %s\', model.input_shape)\n  logging.info(\'Model output shape: %s\', model.output_shape)\n  logging.info(\'Model number of weights: %s\', model.count_params())\n  # Search for checkpoints from their index file; then remove the index suffix.\n  ensemble_filenames = tf.io.gfile.glob(os.path.join(FLAGS.checkpoint_dir,\n                                                     \'**/*.index\'))\n  ensemble_filenames = [filename[:-6] for filename in ensemble_filenames]\n  ensemble_size = len(ensemble_filenames)\n  logging.info(\'Ensemble size: %s\', ensemble_size)\n  logging.info(\'Ensemble number of weights: %s\',\n               ensemble_size * model.count_params())\n  logging.info(\'Ensemble filenames: %s\', str(ensemble_filenames))\n  checkpoint = tf.train.Checkpoint(model=model)\n\n  # Write model predictions to files.\n  num_datasets = len(test_datasets)\n  for m, ensemble_filename in enumerate(ensemble_filenames):\n    checkpoint.restore(ensemble_filename)\n    for n, (name, test_dataset) in enumerate(test_datasets.items()):\n      filename = \'{dataset}_{member}.npy\'.format(dataset=name, member=m)\n      filename = os.path.join(FLAGS.output_dir, filename)\n      if not tf.io.gfile.exists(filename):\n        logits = []\n        test_iterator = iter(test_dataset)\n        for _ in range(steps_per_eval):\n          features, _ = next(test_iterator)  # pytype: disable=attribute-error\n          logits.append(model(features, training=False))\n\n        logits = tf.concat(logits, axis=0)\n        with tf.io.gfile.GFile(filename, \'w\') as f:\n          np.save(f, logits.numpy())\n      percent = (m * num_datasets + (n + 1)) / (ensemble_size * num_datasets)\n      message = (\'{:.1%} completion for prediction: ensemble member {:d}/{:d}. \'\n                 \'Dataset {:d}/{:d}\'.format(percent,\n                                            m + 1,\n                                            ensemble_size,\n                                            n + 1,\n                                            num_datasets))\n      logging.info(message)\n\n  metrics = {\n      \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n      \'test/gibbs_cross_entropy\': tf.keras.metrics.Mean(),\n      \'test/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n      \'test/ece\': ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins),\n  }\n  corrupt_metrics = {}\n  for name in test_datasets:\n    corrupt_metrics[\'test/nll_{}\'.format(name)] = tf.keras.metrics.Mean()\n    corrupt_metrics[\'test/accuracy_{}\'.format(name)] = (\n        tf.keras.metrics.SparseCategoricalAccuracy())\n    corrupt_metrics[\'test/ece_{}\'.format(\n        name)] = ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins)\n\n  # Evaluate model predictions.\n  for n, (name, test_dataset) in enumerate(test_datasets.items()):\n    logits_dataset = []\n    for m in range(ensemble_size):\n      filename = \'{dataset}_{member}.npy\'.format(dataset=name, member=m)\n      filename = os.path.join(FLAGS.output_dir, filename)\n      with tf.io.gfile.GFile(filename, \'rb\') as f:\n        logits_dataset.append(np.load(f))\n\n    logits_dataset = tf.convert_to_tensor(logits_dataset)\n    test_iterator = iter(test_dataset)\n    for step in range(steps_per_eval):\n      _, labels = next(test_iterator)  # pytype: disable=attribute-error\n      logits = logits_dataset[:, (step*batch_size):((step+1)*batch_size)]\n      labels = tf.cast(tf.reshape(labels, [-1]), tf.int32)\n      negative_log_likelihood = tf.reduce_mean(\n          ensemble_negative_log_likelihood(labels, logits))\n      per_probs = tf.nn.softmax(logits)\n      probs = tf.reduce_mean(per_probs, axis=0)\n      if name == \'clean\':\n        gibbs_ce = tf.reduce_mean(gibbs_cross_entropy(labels, logits))\n        metrics[\'test/negative_log_likelihood\'].update_state(\n            negative_log_likelihood)\n        metrics[\'test/gibbs_cross_entropy\'].update_state(gibbs_ce)\n        metrics[\'test/accuracy\'].update_state(labels, probs)\n        metrics[\'test/ece\'].update_state(labels, probs)\n      else:\n        corrupt_metrics[\'test/nll_{}\'.format(name)].update_state(\n            negative_log_likelihood)\n        corrupt_metrics[\'test/accuracy_{}\'.format(name)].update_state(\n            labels, probs)\n        corrupt_metrics[\'test/ece_{}\'.format(name)].update_state(\n            labels, probs)\n\n    message = (\'{:.1%} completion for evaluation: dataset {:d}/{:d}\'.format(\n        (n + 1) / num_datasets, n + 1, num_datasets))\n    logging.info(message)\n\n  corrupt_results = utils.aggregate_corrupt_metrics(corrupt_metrics,\n                                                    corruption_types,\n                                                    max_intensity,\n                                                    FLAGS.alexnet_errors_path)\n  total_results = {name: metric.result() for name, metric in metrics.items()}\n  total_results.update(corrupt_results)\n  logging.info(\'Metrics: %s\', total_results)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/imagenet/utils.py,122,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities for ImageNet.""""""\n\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow.compat.v1 as tf\nimport tensorflow_datasets as tfds\n\nIMAGE_SIZE = 224\nCROP_PADDING = 32\n\n# ImageNet statistics. Used to normalize the input to Efficientnet.\nIMAGENET_MEAN = np.array([[[0.485, 0.456, 0.406]]], np.float32) * 255.\nIMAGENET_STDDEV = np.array([[[0.229, 0.224, 0.225]]], np.float32) * 255.\n\n\n# TODO(trandustin): Refactor similar to CIFAR code.\nclass LearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n  """"""Resnet learning rate schedule.""""""\n\n  def __init__(self, steps_per_epoch, initial_learning_rate, num_epochs,\n               schedule):\n    super(LearningRateSchedule, self).__init__()\n    self.num_epochs = num_epochs\n    self.steps_per_epoch = steps_per_epoch\n    self.initial_learning_rate = initial_learning_rate\n    self.schedule = schedule\n\n  def __call__(self, step):\n    lr_epoch = tf.cast(step, tf.float32) / self.steps_per_epoch\n    warmup_lr_multiplier, warmup_end_epoch = self.schedule[0]\n    # Scale learning rate schedule by total epochs at vanilla settings.\n    warmup_end_epoch = (warmup_end_epoch * self.num_epochs) // 90\n    learning_rate = (\n        self.initial_learning_rate * warmup_lr_multiplier * lr_epoch /\n        warmup_end_epoch)\n    for mult, start_epoch in self.schedule:\n      start_epoch = (start_epoch * self.num_epochs) // 90\n      learning_rate = tf.where(lr_epoch >= start_epoch,\n                               self.initial_learning_rate * mult, learning_rate)\n    return learning_rate\n\n  def get_config(self):\n    return {\n        \'steps_per_epoch\': self.steps_per_epoch,\n        \'initial_learning_rate\': self.initial_learning_rate,\n        \'num_epochs\': self.num_epochs,\n        \'schedule\': self.schedule,\n    }\n\n\nclass WarmupDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n  """"""A wrapper for LearningRateSchedule that includes warmup steps.""""""\n\n  def __init__(self, lr_schedule, warmup_steps):\n    """"""Add warmup decay to a learning rate schedule.\n\n    Args:\n      lr_schedule: base learning rate scheduler\n      warmup_steps: number of warmup steps\n\n    """"""\n    super(WarmupDecaySchedule, self).__init__()\n    self._lr_schedule = lr_schedule\n    self._warmup_steps = warmup_steps\n\n  def __call__(self, step):\n    lr = self._lr_schedule(step)\n    if self._warmup_steps:\n      initial_learning_rate = tf.convert_to_tensor(\n          self._lr_schedule.initial_learning_rate, name=\'initial_learning_rate\')\n      dtype = initial_learning_rate.dtype\n      global_step_recomp = tf.cast(step, dtype)\n      warmup_steps = tf.cast(self._warmup_steps, dtype)\n      warmup_lr = initial_learning_rate * global_step_recomp / warmup_steps\n      lr = tf.cond(global_step_recomp < warmup_steps,\n                   lambda: warmup_lr,\n                   lambda: lr)\n    return lr\n\n  def get_config(self):\n    config = self._lr_schedule.get_config()\n    config.update({\n        \'warmup_steps\': self._warmup_steps,\n    })\n    return config\n\n\ndef distorted_bounding_box_crop(image_bytes,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image_bytes: `Tensor` of binary image data.\n    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n        where each coordinate is [0, 1) and the coordinates are arranged\n        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n        image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n        area of the image must contain at least this fraction of any bounding\n        box supplied.\n    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n        image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `float`s. The cropped area of the image\n        must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n        region of the image of the specified constraints. After `max_attempts`\n        failures, return the entire image.\n    scope: Optional `str` for name scope.\n  Returns:\n    (cropped image `Tensor`, distorted bbox `Tensor`).\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image_bytes, bbox]):\n    decoded = image_bytes.dtype != tf.string\n    shape = (tf.shape(image_bytes) if decoded\n             else tf.image.extract_jpeg_shape(image_bytes))\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        shape,\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n    target_height, target_width, _ = tf.unstack(bbox_size)\n    if decoded:\n      image = tf.image.crop_to_bounding_box(\n          image_bytes,\n          offset_height=offset_y,\n          offset_width=offset_x,\n          target_height=target_height,\n          target_width=target_width)\n    else:\n      crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n      image = tf.image.decode_and_crop_jpeg(image_bytes,\n                                            crop_window,\n                                            channels=3)\n    return image\n\n\ndef _at_least_x_are_equal(a, b, x):\n  """"""At least `x` of `a` and `b` `Tensors` are equal.""""""\n  match = tf.equal(a, b)\n  match = tf.cast(match, tf.int32)\n  return tf.greater_equal(tf.reduce_sum(match), x)\n\n\ndef _resize_image(image, image_size, method=None):\n  if method is not None:\n    return tf.image.resize([image], [image_size, image_size], method)[0]\n  return tf.image.resize_bicubic([image], [image_size, image_size])[0]\n\n\ndef _decode_and_random_crop(image_bytes, image_size, resize_method=None):\n  """"""Make a random crop of image_size.""""""\n  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n  image = distorted_bounding_box_crop(\n      image_bytes,\n      bbox,\n      min_object_covered=0.1,\n      aspect_ratio_range=(3. / 4, 4. / 3.),\n      area_range=(0.08, 1.0),\n      max_attempts=10,\n      scope=None)\n  original_shape = tf.image.extract_jpeg_shape(image_bytes)\n  bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)\n\n  image = tf.cond(\n      bad,\n      lambda: _decode_and_center_crop(image_bytes, image_size),\n      lambda: _resize_image(image, image_size, resize_method))\n\n  return image\n\n\ndef _decode_and_center_crop(image_bytes, image_size, resize_method=None):\n  """"""Crops to center of image with padding then scales by image_size.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size, or\n      an already decoded image.\n    image_size: Image height/width dimension.\n    resize_method: Resize method.\n\n  Returns:\n    A decoded and cropped image Tensor.\n  """"""\n  decoded = image_bytes.dtype != tf.string\n  shape = (tf.shape(image_bytes) if decoded\n           else tf.image.extract_jpeg_shape(image_bytes))\n  image_height = shape[0]\n  image_width = shape[1]\n\n  padded_center_crop_size = tf.cast(\n      ((image_size / (image_size + CROP_PADDING)) *\n       tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n      tf.int32)\n\n  offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n  offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n  crop_window = tf.stack([offset_height, offset_width,\n                          padded_center_crop_size, padded_center_crop_size])\n  if decoded:\n    image = tf.image.crop_to_bounding_box(\n        image_bytes,\n        offset_height=offset_height,\n        offset_width=offset_width,\n        target_height=padded_center_crop_size,\n        target_width=padded_center_crop_size)\n  else:\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n\n  image = _resize_image(image, image_size, resize_method)\n\n  return image\n\n\ndef preprocess_for_train(image_bytes,\n                         use_bfloat16,\n                         image_size=IMAGE_SIZE,\n                         resize_method=None):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size, or\n      an already decoded image.\n    use_bfloat16: `bool` for whether to use bfloat16.\n    image_size: image size/resolution in Efficientnet.\n    resize_method: resize method. If none, use bicubic.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n  image = tf.image.random_flip_left_right(image)\n  image = tf.reshape(image, [image_size, image_size, 3])\n  image = tf.image.convert_image_dtype(\n      image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n  return image\n\n\ndef preprocess_for_eval(image_bytes,\n                        use_bfloat16,\n                        image_size=IMAGE_SIZE,\n                        resize_method=None):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size, or\n      an already decoded image.\n    use_bfloat16: `bool` for whether to use bfloat16.\n    image_size: image size.\n    resize_method: if None, use bicubic.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n  image = tf.reshape(image, [image_size, image_size, 3])\n  image = tf.image.convert_image_dtype(\n      image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n  return image\n\n\ndef preprocess_image(image_bytes,\n                     is_training=False,\n                     use_bfloat16=False,\n                     image_size=IMAGE_SIZE,\n                     resize_method=None):\n  """"""Preprocesses the given image.\n\n  Args:\n    image_bytes: `Tensor` representing an image binary of arbitrary size, or\n      an already decoded image.\n    is_training: `bool` for whether the preprocessing is for training.\n    use_bfloat16: `bool` for whether to use bfloat16.\n    image_size: image size.\n    resize_method: if None, use bicubic.\n\n  Returns:\n    A preprocessed image `Tensor`.\n  """"""\n  if is_training:\n    return preprocess_for_train(image_bytes, use_bfloat16,\n                                image_size, resize_method)\n  else:\n    return preprocess_for_eval(image_bytes, use_bfloat16,\n                               image_size, resize_method)\n\n\nclass ImageNetInput(object):\n  """"""Generates ImageNet input_fn for training or evaluation.\n\n  The training data is assumed to be in TFRecord format with keys as specified\n  in the dataset_parser below, sharded across 1024 files, named sequentially:\n      train-00000-of-01024\n      train-00001-of-01024\n      ...\n      train-01023-of-01024\n\n  The validation data is in the same format but sharded in 128 files.\n\n  The format of the data required is created by the script at:\n      https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py\n\n  Attributes:\n    is_training: `bool` for whether the input is for training.\n    data_dir: `str` for the directory of the training and validation data.\n    use_bfloat16: If True, use bfloat16 precision; else use float32.\n    image_size: `int` for image size (both width and height).\n    normalize_input: `bool` for normalizing the input. Enable in Efficientnet.\n    one_hot: `bool` for using one-hot label. Enable in Efficientnet.\n    drop_remainder: `bool` for dropping the remainder when batching.\n    batch_size: The global batch size to use.\n    image_preprocessing_fn: Image preprocessing function.\n    resize_method: If None, use bicubic in default.\n  """"""\n\n  def __init__(self,\n               is_training,\n               data_dir,\n               batch_size,\n               image_size=224,\n               normalize_input=False,\n               one_hot=False,\n               drop_remainder=True,\n               use_bfloat16=False,\n               resize_method=None):\n    self.image_preprocessing_fn = preprocess_image\n    self.is_training = is_training\n    self.use_bfloat16 = use_bfloat16\n    self.drop_remainder = drop_remainder\n    self.data_dir = data_dir\n    self.batch_size = batch_size\n    self.image_size = image_size\n    self.normalize_input = normalize_input\n    self.one_hot = one_hot\n    self.resize_method = resize_method\n\n  def dataset_parser(self, value):\n    """"""Parse an ImageNet record from a serialized string Tensor.""""""\n    keys_to_features = {\n        \'image/encoded\':\n            tf.FixedLenFeature((), tf.string, \'\'),\n        \'image/format\':\n            tf.FixedLenFeature((), tf.string, \'jpeg\'),\n        \'image/class/label\':\n            tf.FixedLenFeature([], tf.int64, -1),\n        \'image/class/text\':\n            tf.FixedLenFeature([], tf.string, \'\'),\n        \'image/object/bbox/xmin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\':\n            tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/class/label\':\n            tf.VarLenFeature(dtype=tf.int64),\n    }\n\n    parsed = tf.parse_single_example(value, keys_to_features)\n    image_bytes = tf.reshape(parsed[\'image/encoded\'], shape=[])\n\n    image = self.image_preprocessing_fn(\n        image_bytes=image_bytes,\n        is_training=self.is_training,\n        use_bfloat16=self.use_bfloat16,\n        image_size=self.image_size,\n        resize_method=self.resize_method)\n\n    # Subtract one so that labels are in [0, 1000), and cast to float32 for\n    # Keras model.\n    if self.one_hot:\n      # TODO(ywenxu): The number of classes is hard coded for now.\n      label = tf.cast(parsed[\'image/class/label\'], tf.int32) - 1\n      label = tf.one_hot(label, 1000, dtype=tf.float32)\n    else:\n      label = tf.cast(parsed[\'image/class/label\'], dtype=tf.int32) - 1\n      label = tf.cast(label, tf.float32)\n\n    if self.normalize_input:\n      mean = np.reshape(IMAGENET_MEAN, [1, 1, 3])\n      stddev = np.reshape(IMAGENET_STDDEV, [1, 1, 3])\n      image = (tf.cast(image, tf.float32) - mean) / stddev\n      if self.use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    return image, label\n\n  def input_fn(self, ctx=None):\n    """"""Input function which provides a single batch for train or eval.\n\n    Args:\n      ctx: Input context.\n\n    Returns:\n      A `tf.data.Dataset` object.\n    """"""\n    # Shuffle the filenames to ensure better randomization.\n    file_pattern = os.path.join(\n        self.data_dir, \'train-*\' if self.is_training else \'validation-*\')\n    dataset = tf.data.Dataset.list_files(file_pattern, shuffle=self.is_training)\n\n    if ctx and ctx.num_input_pipelines > 1:\n      dataset = dataset.shard(ctx.num_input_pipelines, ctx.input_pipeline_id)\n\n    # Evaluation dataset can also be repeat as long as steps_per_eval is set.\n    dataset = dataset.repeat()\n\n    def fetch_dataset(filename):\n      buffer_size = 8 * 1024 * 1024     # 8 MiB per file\n      dataset = tf.data.TFRecordDataset(filename, buffer_size=buffer_size)\n      return dataset\n\n    # Read the data from disk in parallel\n    dataset = dataset.interleave(\n        fetch_dataset, cycle_length=16,\n        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n    if self.is_training:\n      dataset = dataset.shuffle(1024)\n\n    dataset = dataset.map(self.dataset_parser, tf.data.experimental.AUTOTUNE)\n    dataset = dataset.batch(\n        batch_size=self.batch_size, drop_remainder=self.drop_remainder)\n\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n    # Optimize dataset performance.\n    options = tf.data.Options()\n    options.experimental_optimization.parallel_batch = True\n    options.experimental_optimization.map_fusion = True\n    options.experimental_optimization.map_vectorization.enabled = True\n    options.experimental_optimization.map_parallelization = True\n    dataset = dataset.with_options(options)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n    return dataset\n\n\ndef load_corrupted_test_dataset(batch_size,\n                                name,\n                                intensity,\n                                drop_remainder=True,\n                                use_bfloat16=False):\n  """"""Loads an ImageNet-C dataset.""""""\n  corruption = name + \'_\' + str(intensity)\n\n  dataset = tfds.load(\n      name=\'imagenet2012_corrupted/{}\'.format(corruption),\n      split=tfds.Split.VALIDATION,\n      decoders={\n          \'image\': tfds.decode.SkipDecoding(),\n      },\n      with_info=False,\n      as_supervised=True)\n\n  def preprocess(image, label):\n    image = tf.reshape(image, shape=[])\n    image = preprocess_for_eval(image, use_bfloat16)\n    label = tf.cast(label, dtype=tf.float32)\n    return image, label\n\n  dataset = dataset.map(\n      preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n  return dataset\n\n\ndef corrupt_test_input_fn(corruption_name,\n                          corruption_intensity,\n                          batch_size,\n                          drop_remainder=True,\n                          use_bfloat16=False):\n  """"""Generates a distributed input_fn for ImageNet-C datasets.""""""\n  def test_input_fn(ctx):\n    """"""Sets up local (per-core) corrupted dataset batching.""""""\n    dataset = load_corrupted_test_dataset(\n        batch_size=batch_size,\n        name=corruption_name,\n        intensity=corruption_intensity,\n        drop_remainder=drop_remainder,\n        use_bfloat16=use_bfloat16)\n    if ctx and ctx.num_input_pipelines > 1:\n      dataset = dataset.shard(ctx.num_input_pipelines, ctx.input_pipeline_id)\n    return dataset\n\n  return test_input_fn\n\n\n# TODO(ghassen,trandustin): Push this metadata upstream to TFDS.\ndef load_corrupted_test_info():\n  """"""Loads information for ImageNet-C.""""""\n  corruption_types = [\n      \'gaussian_noise\',\n      \'shot_noise\',\n      \'impulse_noise\',\n      \'defocus_blur\',\n      \'glass_blur\',\n      \'motion_blur\',\n      \'zoom_blur\',\n      \'snow\',\n      \'frost\',\n      \'fog\',\n      \'brightness\',\n      \'contrast\',\n      \'elastic_transform\',\n      \'pixelate\',\n      \'jpeg_compression\',\n  ]\n  max_intensity = 5\n  return corruption_types, max_intensity\n\n\ndef aggregate_corrupt_metrics(metrics,\n                              corruption_types,\n                              max_intensity,\n                              alexnet_errors_path=None,\n                              fine_metrics=False):\n  """"""Aggregates metrics across intensities and corruption types.""""""\n  results = {\'test/nll_mean_corrupted\': 0.,\n             \'test/accuracy_mean_corrupted\': 0.,\n             \'test/ece_mean_corrupted\': 0.}\n  for intensity in range(1, max_intensity + 1):\n    ece = np.zeros(len(corruption_types))\n    nll = np.zeros(len(corruption_types))\n    acc = np.zeros(len(corruption_types))\n    for i in range(len(corruption_types)):\n      dataset_name = \'{0}_{1}\'.format(corruption_types[i], intensity)\n      nll[i] = metrics[\'test/nll_{}\'.format(dataset_name)].result()\n      acc[i] = metrics[\'test/accuracy_{}\'.format(dataset_name)].result()\n      ece[i] = metrics[\'test/ece_{}\'.format(dataset_name)].result()\n      if fine_metrics:\n        results[\'test/nll_{}\'.format(dataset_name)] = nll[i]\n        results[\'test/accuracy_{}\'.format(dataset_name)] = acc[i]\n        results[\'test/ece_{}\'.format(dataset_name)] = ece[i]\n    avg_nll = np.mean(nll)\n    avg_accuracy = np.mean(acc)\n    avg_ece = np.mean(ece)\n    results[\'test/nll_mean_{}\'.format(intensity)] = avg_nll\n    results[\'test/accuracy_mean_{}\'.format(intensity)] = avg_accuracy\n    results[\'test/ece_mean_{}\'.format(intensity)] = avg_ece\n    results[\'test/nll_median_{}\'.format(intensity)] = np.median(nll)\n    results[\'test/accuracy_median_{}\'.format(intensity)] = np.median(acc)\n    results[\'test/ece_median_{}\'.format(intensity)] = np.median(ece)\n    results[\'test/nll_mean_corrupted\'] += avg_nll\n    results[\'test/accuracy_mean_corrupted\'] += avg_accuracy\n    results[\'test/ece_mean_corrupted\'] += avg_ece\n\n  results[\'test/nll_mean_corrupted\'] /= max_intensity\n  results[\'test/accuracy_mean_corrupted\'] /= max_intensity\n  results[\'test/ece_mean_corrupted\'] /= max_intensity\n\n  if alexnet_errors_path:\n    with tf.io.gfile.GFile(alexnet_errors_path, \'r\') as f:\n      df = pd.read_csv(f, index_col=\'intensity\').transpose()\n    alexnet_errors = df.to_dict()\n    corrupt_error = {}\n    for corruption in corruption_types:\n      alexnet_normalization = alexnet_errors[corruption][\'average\']\n      errors = np.zeros(max_intensity)\n      for index in range(max_intensity):\n        dataset_name = \'{0}_{1}\'.format(corruption, index + 1)\n        errors[index] = 1. - metrics[\'test/accuracy_{}\'.format(\n            dataset_name)].result()\n      average_error = np.mean(errors)\n      corrupt_error[corruption] = average_error / alexnet_normalization\n      results[\'test/corruption_error_{}\'.format(\n          corruption)] = 100 * corrupt_error[corruption]\n    results[\'test/mCE\'] = 100 * np.mean(list(corrupt_error.values()))\n  return results\n\n\n# TODO(ywenxu): Check out `tf.keras.layers.experimental.SyncBatchNormalization.\n# SyncBatchNorm on TPU. Orginal authored by hyhieu.\nclass SyncBatchNorm(tf.keras.layers.Layer):\n  """"""BatchNorm that averages over ALL replicas. Only works for `NHWC` inputs.""""""\n\n  def __init__(self, axis=3, momentum=0.99, epsilon=0.001,\n               trainable=True, name=\'batch_norm\', **kwargs):\n    super(SyncBatchNorm, self).__init__(\n        trainable=trainable, name=name, **kwargs)\n    self.axis = axis\n    self.momentum = momentum\n    self.epsilon = epsilon\n\n  def build(self, input_shape):\n    """"""Build function.""""""\n    dim = input_shape[-1]\n    shape = [dim]\n\n    self.gamma = self.add_weight(\n        name=\'gamma\',\n        shape=shape,\n        dtype=self.dtype,\n        initializer=\'ones\',\n        trainable=True)\n\n    self.beta = self.add_weight(\n        name=\'beta\',\n        shape=shape,\n        dtype=self.dtype,\n        initializer=\'zeros\',\n        trainable=True)\n\n    self.moving_mean = self.add_weight(\n        name=\'moving_mean\',\n        shape=shape,\n        dtype=self.dtype,\n        initializer=\'zeros\',\n        synchronization=tf.VariableSynchronization.ON_READ,\n        trainable=False,\n        aggregation=tf.VariableAggregation.MEAN)\n\n    self.moving_variance = self.add_weight(\n        name=\'moving_variance\',\n        shape=shape,\n        dtype=self.dtype,\n        initializer=\'ones\',\n        synchronization=tf.VariableSynchronization.ON_READ,\n        trainable=False,\n        aggregation=tf.VariableAggregation.MEAN)\n\n  def _get_mean_and_variance(self, x):\n    """"""Cross-replica mean and variance.""""""\n    replica_context = tf.distribute.get_replica_context()\n    num_replicas_in_sync = replica_context.num_replicas_in_sync\n    if num_replicas_in_sync <= 8:\n      group_assignment = None\n      num_replicas_per_group = tf.cast(num_replicas_in_sync, tf.float32)\n    else:\n      num_replicas_per_group = max(8, num_replicas_in_sync // 8)\n      group_assignment = np.arange(num_replicas_in_sync, dtype=np.int32)\n      group_assignment = group_assignment.reshape([-1, num_replicas_per_group])\n      group_assignment = group_assignment.tolist()\n      num_replicas_per_group = tf.cast(num_replicas_per_group, tf.float32)\n\n    mean = tf.reduce_mean(x, axis=[0, 1, 2])\n    mean = tf.cast(mean, tf.float32)\n    mean = tf.tpu.cross_replica_sum(mean, group_assignment)\n    mean = mean / num_replicas_per_group\n\n    # Var[x] = E[x^2] - E[x]^2\n    mean_sq = tf.reduce_mean(tf.square(x), axis=[0, 1, 2])\n    mean_sq = tf.cast(mean_sq, tf.float32)\n    mean_sq = tf.tpu.cross_replica_sum(mean_sq, group_assignment)\n    mean_sq = mean_sq / num_replicas_per_group\n    variance = mean_sq - tf.square(mean)\n\n    def _assign(moving, normal):\n      decay = tf.cast(1. - self.momentum, tf.float32)\n      diff = tf.cast(moving, tf.float32) - tf.cast(normal, tf.float32)\n      return moving.assign_sub(decay * diff)\n\n    self.add_update(_assign(self.moving_mean, mean))\n    self.add_update(_assign(self.moving_variance, variance))\n\n    # TODO(ywenxu): Assuming bfloat16. Fix for non bfloat16 case.\n    mean = tf.cast(mean, tf.bfloat16)\n    variance = tf.cast(variance, tf.bfloat16)\n\n    return mean, variance\n\n  def call(self, inputs, training):\n    """"""Call function.""""""\n    if training:\n      mean, variance = self._get_mean_and_variance(inputs)\n    else:\n      mean, variance = self.moving_mean, self.moving_variance\n    x = tf.nn.batch_normalization(\n        inputs,\n        mean=mean,\n        variance=variance,\n        offset=self.beta,\n        scale=self.gamma,\n        variance_epsilon=tf.cast(self.epsilon, variance.dtype),\n    )\n    return x\n\n\ndef drop_connect(inputs, is_training, survival_prob):\n  """"""Drop the entire conv with given survival probability.""""""\n  # ""Deep Networks with Stochastic Depth"", https://arxiv.org/pdf/1603.09382.pdf\n  if not is_training:\n    return inputs\n\n  # Compute tensor.\n  batch_size = tf.shape(inputs)[0]\n  random_tensor = survival_prob\n  random_tensor += tf.random.uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)\n  binary_tensor = tf.floor(random_tensor)\n  # Unlike conventional way that multiply survival_prob at test time, here we\n  # divide survival_prob at training time, such that no addition compute is\n  # needed at test time.\n  output = tf.math.divide(inputs, survival_prob) * binary_tensor\n  return output\n\n\ndef conv_kernel_initializer(shape, dtype=None, partition_info=None):\n  """"""Initialization for convolutional kernels.\n\n  The main difference with tf.variance_scaling_initializer is that\n  tf.variance_scaling_initializer uses a truncated normal with an uncorrected\n  standard deviation, whereas here we use a normal distribution. Similarly,\n  tf.initializers.variance_scaling uses a truncated normal with\n  a corrected standard deviation.\n\n  Args:\n    shape: shape of variable\n    dtype: dtype of variable\n    partition_info: unused\n\n  Returns:\n    an initialization for the variable\n  """"""\n  del partition_info\n  kernel_height, kernel_width, _, out_filters = shape\n  fan_out = int(kernel_height * kernel_width * out_filters)\n  return tf.random.normal(\n      shape, mean=0.0, stddev=np.sqrt(2.0 / fan_out), dtype=dtype)\n\n\ndef dense_kernel_initializer(shape, dtype=None, partition_info=None):\n  """"""Initialization for dense kernels.\n\n  This initialization is equal to\n    tf.variance_scaling_initializer(scale=1.0/3.0, mode=\'fan_out\',\n                                    distribution=\'uniform\').\n  It is written out explicitly here for clarity.\n\n  Args:\n    shape: shape of variable\n    dtype: dtype of variable\n    partition_info: unused\n\n  Returns:\n    an initialization for the variable\n  """"""\n  del partition_info\n  init_range = 1.0 / np.sqrt(shape[1])\n  return tf.random.uniform(shape, -init_range, init_range, dtype=dtype)\n\n\nclass MovingAverage(tf.keras.optimizers.Optimizer):\n  """"""Optimizer that computes a moving average of the variables.\n\n  Empirically it has been found that using the moving average of the trained\n  parameters of a deep network is better than using its trained parameters\n  directly. This optimizer allows you to compute this moving average and swap\n  the variables at save time so that any code outside of the training loop\n  will use by default the average values instead of the original ones.\n\n  Example of usage for training:\n  ```python\n  opt = tf.keras.optimizers.SGD(learning_rate)\n  opt = MovingAverage(opt)\n\n  opt.shadow_copy(model)\n  ```\n\n  At test time, swap the shadow variables to evaluate on the averaged weights:\n  ```python\n  opt.swap_weights()\n  # Test eval the model here\n  opt.swap_weights()\n  ```\n  """"""\n\n  def __init__(self,\n               optimizer,\n               average_decay=0.99,\n               start_step=0,\n               dynamic_decay=True,\n               name=\'moving_average\',\n               **kwargs):\n    """"""Construct a new MovingAverage optimizer.\n\n    Args:\n      optimizer: `tf.keras.optimizers.Optimizer` that will be\n        used to compute and apply gradients.\n      average_decay: float. Decay to use to maintain the moving averages\n        of trained variables.\n      start_step: int. What step to start the moving average.\n      dynamic_decay: bool. Whether to change the decay based on the number\n        of optimizer updates. Decay will start at 0.1 and gradually increase\n        up to `average_decay` after each optimizer update. This behavior is\n        similar to `tf.train.ExponentialMovingAverage` in TF 1.x.\n      name: Optional name for the operations created when applying\n        gradients. Defaults to ""moving_average"".\n      **kwargs: keyword arguments. Allowed to be {`clipnorm`,\n        `clipvalue`, `lr`, `decay`}.\n    """"""\n    super(MovingAverage, self).__init__(name, **kwargs)\n    self._optimizer = optimizer\n    self._average_decay = average_decay\n    self._start_step = tf.constant(start_step, tf.float32)\n    self._dynamic_decay = dynamic_decay\n\n  def shadow_copy(self, model):\n    """"""Creates shadow variables for the given model weights.""""""\n    for var in model.weights:\n      self.add_slot(var, \'average\', initializer=\'zeros\')\n    self._average_weights = [\n        self.get_slot(var, \'average\') for var in model.weights\n    ]\n    self._model_weights = model.weights\n\n  @property\n  def has_shadow_copy(self):\n    """"""Whether this optimizer has created shadow variables.""""""\n    return self._model_weights is not None\n\n  def _create_slots(self, var_list):\n    self._optimizer._create_slots(var_list=var_list)  # pylint: disable=protected-access\n\n  def apply_gradients(self, grads_and_vars, name=None):\n    result = self._optimizer.apply_gradients(grads_and_vars, name)\n    self.update_average(self._optimizer.iterations)\n    return result\n\n  @tf.function\n  def update_average(self, step):\n    step = tf.cast(step, tf.float32)\n    if step < self._start_step:\n      decay = tf.constant(0., tf.float32)\n    elif self._dynamic_decay:\n      decay = step - self._start_step\n      decay = tf.minimum(self._average_decay, (1. + decay) / (10. + decay))\n    else:\n      decay = self._average_decay\n\n    def _apply_moving(v_moving, v_normal):\n      diff = v_moving - v_normal\n      v_moving.assign_sub(tf.cast(1. - decay, v_moving.dtype) * diff)\n      return v_moving\n\n    def _update(strategy, v_moving_and_v_normal):\n      for v_moving, v_normal in v_moving_and_v_normal:\n        strategy.extended.update(v_moving, _apply_moving, args=(v_normal,))\n\n    ctx = tf.distribute.get_replica_context()\n    return ctx.merge_call(_update, args=(zip(self._average_weights,\n                                             self._model_weights),))\n\n  def swap_weights(self, strategy):\n    """"""Swap the average and moving weights.\n\n    This is a convenience method to allow one to evaluate the averaged weights\n    at test time. Loads the weights stored in `self._average` into the model,\n    keeping a copy of the original model weights. Swapping twice will return\n    the original weights.\n\n    Args:\n      strategy: tf.distribute.Strategy to be used.\n    """"""\n    strategy.run(self._swap_weights, args=())\n\n  def _swap_weights(self):\n    def fn_0(a, b):\n      a.assign_add(b)\n      return a\n    def fn_1(b, a):\n      b.assign(a - b)\n      return b\n    def fn_2(a, b):\n      a.assign_sub(b)\n      return a\n\n    def swap(strategy, a_and_b):\n      """"""Swap `a` and `b` and mirror to all devices.""""""\n      for a, b in a_and_b:\n        strategy.extended.update(a, fn_0, args=(b,))  # a = a + b\n        strategy.extended.update(b, fn_1, args=(a,))  # b = a - b\n        strategy.extended.update(a, fn_2, args=(b,))  # a = a - b\n\n    ctx = tf.distribute.get_replica_context()\n    return ctx.merge_call(\n        swap, args=(zip(self._average_weights, self._model_weights),))\n\n  def assign_average_vars(self, var_list):\n    """"""Assign variables in var_list with their respective averages.\n\n    Args:\n      var_list: List of model variables to be assigned to their average.\n    Returns:\n      assign_op: The op corresponding to the assignment operation of\n        variables to their average.\n    """"""\n    assign_op = tf.group([\n        var.assign(self.get_slot(var, \'average\')) for var in var_list\n        if var.trainable\n    ])\n    return assign_op\n\n  def _create_hypers(self):\n    self._optimizer._create_hypers()  # pylint: disable=protected-access\n\n  def _prepare(self, var_list):\n    return self._optimizer._prepare(var_list=var_list)  # pylint: disable=protected-access\n\n  @property\n  def iterations(self):\n    return self._optimizer.iterations\n\n  @iterations.setter\n  def iterations(self, variable):\n    self._optimizer.iterations = variable\n\n  @property\n  def weights(self):\n    return self._optimizer.weights\n\n  # pylint: disable=protected-access\n  @property\n  def lr(self):\n    return self._optimizer._get_hyper(\'learning_rate\')\n\n  @lr.setter\n  def lr(self, lr):\n    self._optimizer._set_hyper(\'learning_rate\', lr)\n\n  @property\n  def learning_rate(self):\n    return self._optimizer._get_hyper(\'learning_rate\')\n\n  @learning_rate.setter\n  def learning_rate(self, learning_rate):  # pylint: disable=redefined-outer-name\n    self._optimizer._set_hyper(\'learning_rate\', learning_rate)\n\n  def _resource_apply_dense(self, grad, var):\n    return self._optimizer._resource_apply_dense(grad, var)\n\n  def _resource_apply_sparse(self, grad, var, indices):\n    return self._optimizer._resource_apply_sparse(grad, var, indices)\n\n  def _resource_apply_sparse_duplicate_indices(self, grad, var, indices):\n    return self._optimizer._resource_apply_sparse_duplicate_indices(\n        grad, var, indices)\n  # pylint: enable=protected-access\n\n  def get_config(self):\n    config = {\n        \'optimizer\': tf.keras.optimizers.serialize(self._optimizer),\n        \'average_decay\': self._average_decay,\n        \'start_step\': self._start_step,\n        \'dynamic_decay\': self._dynamic_decay,\n    }\n    base_config = super(MovingAverage, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n\n  @classmethod\n  def from_config(cls, config, custom_objects=None):\n    optimizer = tf.keras.optimizers.deserialize(\n        config.pop(\'optimizer\'),\n        custom_objects=custom_objects,\n    )\n    return cls(optimizer, **config)\n'"
baselines/mnist/deterministic.py,23,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""LeNet-5 on (Fashion) MNIST.""""""\n\nimport os\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport utils  # local file import\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tf1\n\nflags.DEFINE_enum(\'dataset\', \'mnist\',\n                  enum_values=[\'mnist\', \'fashion_mnist\'],\n                  help=\'Name of the image dataset.\')\nflags.DEFINE_integer(\'ensemble_size\', 1, \'Number of ensemble members.\')\nflags.DEFINE_boolean(\'bootstrap\', False,\n                     \'Sample the training set for bootstrapping.\')\nflags.DEFINE_integer(\'training_steps\', 5000, \'Training steps.\')\nflags.DEFINE_integer(\'batch_size\', 256, \'Batch size.\')\nflags.DEFINE_float(\'learning_rate\', 0.001, \'Learning rate.\')\nflags.DEFINE_integer(\'validation_freq\', 5, \'Validation frequency in steps.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/det_training\',\n                    \'The directory where the model weights and \'\n                    \'training/evaluation summaries are stored.\')\nflags.DEFINE_integer(\'seed\', 0, \'Random seed.\')\nFLAGS = flags.FLAGS\n\n\ndef lenet5(input_shape, num_classes):\n  """"""Builds LeNet5.""""""\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  conv1 = tf.keras.layers.Conv2D(6,\n                                 kernel_size=5,\n                                 padding=\'SAME\',\n                                 activation=\'relu\')(inputs)\n  pool1 = tf.keras.layers.MaxPooling2D(pool_size=[2, 2],\n                                       strides=[2, 2],\n                                       padding=\'SAME\')(conv1)\n  conv2 = tf.keras.layers.Conv2D(16,\n                                 kernel_size=5,\n                                 padding=\'SAME\',\n                                 activation=\'relu\')(pool1)\n  pool2 = tf.keras.layers.MaxPooling2D(pool_size=[2, 2],\n                                       strides=[2, 2],\n                                       padding=\'SAME\')(conv2)\n  conv3 = tf.keras.layers.Conv2D(120,\n                                 kernel_size=5,\n                                 padding=\'SAME\',\n                                 activation=tf.nn.relu)(pool2)\n  flatten = tf.keras.layers.Flatten()(conv3)\n  dense1 = tf.keras.layers.Dense(84, activation=tf.nn.relu)(flatten)\n  logits = tf.keras.layers.Dense(num_classes)(dense1)\n  outputs = tf.keras.layers.Lambda(lambda x: ed.Categorical(logits=x))(logits)\n  return tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\ndef main(argv):\n  del argv  # unused arg\n  np.random.seed(FLAGS.seed)\n  tf.random.set_seed(FLAGS.seed)\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  tf1.disable_v2_behavior()\n\n  session = tf1.Session()\n  x_train, y_train, x_test, y_test = utils.load(FLAGS.dataset, session)\n  n_train = x_train.shape[0]\n  num_classes = int(np.amax(y_train)) + 1\n\n  ensemble_filenames = []\n  for i in range(FLAGS.ensemble_size):\n    # TODO(trandustin): We re-build the graph for each ensemble member. This\n    # is due to an unknown bug where the variables are otherwise not\n    # re-initialized to be random. While this is inefficient in graph mode, I\'m\n    # keeping this for now as we\'d like to move to eager mode anyways.\n    model = lenet5(x_train.shape[1:], num_classes)\n\n    def negative_log_likelihood(y, rv_y):\n      del rv_y  # unused arg\n      return -model.output.distribution.log_prob(tf.squeeze(y))  # pylint: disable=cell-var-from-loop\n\n    def accuracy(y_true, y_sample):\n      del y_sample  # unused arg\n      return tf.equal(\n          tf.argmax(input=model.output.distribution.logits, axis=1),  # pylint: disable=cell-var-from-loop\n          tf.cast(tf.squeeze(y_true), tf.int64))\n\n    def log_likelihood(y_true, y_sample):\n      del y_sample  # unused arg\n      return model.output.distribution.log_prob(tf.squeeze(y_true))  # pylint: disable=cell-var-from-loop\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(lr=FLAGS.learning_rate),\n        loss=negative_log_likelihood,\n        metrics=[log_likelihood, accuracy])\n    member_dir = os.path.join(FLAGS.output_dir, \'member_\' + str(i))\n    tensorboard = tf1.keras.callbacks.TensorBoard(\n        log_dir=member_dir,\n        update_freq=FLAGS.batch_size * FLAGS.validation_freq)\n\n    if FLAGS.bootstrap:\n      inds = np.random.choice(n_train, n_train, replace=True)\n      x_sampled = x_train[inds]\n      y_sampled = y_train[inds]\n\n    model.fit(\n        x=x_train if not FLAGS.bootstrap else x_sampled,\n        y=y_train if not FLAGS.bootstrap else y_sampled,\n        batch_size=FLAGS.batch_size,\n        epochs=(FLAGS.batch_size * FLAGS.training_steps) // n_train,\n        validation_data=(x_test, y_test),\n        validation_freq=max(\n            (FLAGS.validation_freq * FLAGS.batch_size) // n_train, 1),\n        verbose=1,\n        callbacks=[tensorboard])\n\n    member_filename = os.path.join(member_dir, \'model.weights\')\n    ensemble_filenames.append(member_filename)\n    model.save_weights(member_filename)\n\n  labels = tf.keras.layers.Input(shape=y_train.shape[1:])\n  ll = tf.keras.backend.function([model.input, labels], [\n      model.output.distribution.log_prob(tf.squeeze(labels)),\n      model.output.distribution.logits,\n  ])\n\n  ensemble_metrics_vals = {\n      \'train\': utils.ensemble_metrics(\n          x_train, y_train, model, ll, weight_files=ensemble_filenames),\n      \'test\': utils.ensemble_metrics(\n          x_test, y_test, model, ll, weight_files=ensemble_filenames),\n  }\n\n  for split, metrics in ensemble_metrics_vals.items():\n    logging.info(split)\n    for metric_name in metrics:\n      logging.info(\'%s: %s\', metric_name, metrics[metric_name])\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/mnist/utils.py,1,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities for (Fashion) MNIST.""""""\n\nimport numpy as np\nimport scipy\nimport tensorflow_datasets as tfds\n\n\ndef load(name, session):\n  """"""Loads dataset (mnist, fashion_mnist) as numpy array.""""""\n  train_ds = tfds.load(\n      name, split=tfds.Split.TRAIN, batch_size=-1, as_supervised=True)\n  x_train, y_train = session.run(train_ds)\n  test_ds = tfds.load(\n      name, split=tfds.Split.TEST, batch_size=-1, as_supervised=True)\n  x_test, y_test = session.run(test_ds)\n  # Scale to the [0, 1] interval.\n  x_min, x_max = np.amin(x_train), np.amax(x_train)\n  x_train = (x_train - x_min) / (x_max + 1e-10)\n  x_test = (x_test - x_min) / (x_max + 1e-10)\n  return x_train, y_train, x_test, y_test\n\n\ndef one_hot(a, num_classes):\n  return np.squeeze(np.eye(num_classes)[a.reshape(-1)])\n\n\ndef brier_score(y, p):\n  """"""Compute the Brier score.\n\n  Brier Score: see\n  https://www.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf,\n  page 363, Example 1\n\n  Args:\n    y: one-hot encoding of the true classes, size (?, num_classes)\n    p: numpy array, size (?, num_classes)\n       containing the output predicted probabilities\n  Returns:\n    bs: Brier score.\n  """"""\n  return np.mean(np.power(p - y, 2))\n\n\ndef calibration(y, p_mean, num_bins=10):\n  """"""Compute the calibration.\n\n  References:\n  https://arxiv.org/abs/1706.04599\n  https://arxiv.org/abs/1807.00263\n\n  Args:\n    y: one-hot encoding of the true classes, size (?, num_classes)\n    p_mean: numpy array, size (?, num_classes)\n           containing the mean output predicted probabilities\n    num_bins: number of bins\n\n  Returns:\n    ece: Expected Calibration Error\n    mce: Maximum Calibration Error\n  """"""\n  # Compute for every test sample x, the predicted class.\n  class_pred = np.argmax(p_mean, axis=1)\n  # and the confidence (probability) associated with it.\n  conf = np.max(p_mean, axis=1)\n  # Convert y from one-hot encoding to the number of the class\n  y = np.argmax(y, axis=1)\n  # Storage\n  acc_tab = np.zeros(num_bins)  # empirical (true) confidence\n  mean_conf = np.zeros(num_bins)  # predicted confidence\n  nb_items_bin = np.zeros(num_bins)  # number of items in the bins\n  tau_tab = np.linspace(0, 1, num_bins+1)  # confidence bins\n  for i in np.arange(num_bins):  # iterate over the bins\n    # select the items where the predicted max probability falls in the bin\n    # [tau_tab[i], tau_tab[i + 1)]\n    sec = (tau_tab[i + 1] > conf) & (conf >= tau_tab[i])\n    nb_items_bin[i] = np.sum(sec)  # Number of items in the bin\n    # select the predicted classes, and the true classes\n    class_pred_sec, y_sec = class_pred[sec], y[sec]\n    # average of the predicted max probabilities\n    mean_conf[i] = np.mean(conf[sec]) if nb_items_bin[i] > 0 else np.nan\n    # compute the empirical confidence\n    acc_tab[i] = np.mean(\n        class_pred_sec == y_sec) if nb_items_bin[i] > 0 else np.nan\n\n  # Cleaning\n  mean_conf = mean_conf[nb_items_bin > 0]\n  acc_tab = acc_tab[nb_items_bin > 0]\n  nb_items_bin = nb_items_bin[nb_items_bin > 0]\n\n  # Expected Calibration Error\n  ece = np.average(\n      np.absolute(mean_conf - acc_tab),\n      weights=nb_items_bin.astype(np.float) / np.sum(nb_items_bin))\n  # Maximum Calibration Error\n  mce = np.max(np.absolute(mean_conf - acc_tab))\n  return ece, mce\n\n\ndef ensemble_metrics(x,\n                     y,\n                     model,\n                     log_likelihood_fn,\n                     n_samples=1,\n                     weight_files=None):\n  """"""Evaluate metrics of an ensemble.\n\n  Args:\n    x: numpy array of inputs\n    y: numpy array of labels\n    model: tf.keras.Model.\n    log_likelihood_fn: keras function of log likelihood. For classification\n      tasks, log_likelihood_fn(...)[1] should return the logits\n    n_samples: number of Monte Carlo samples to draw per ensemble member (each\n      weight file).\n    weight_files: to draw samples from multiple weight sets, specify a list of\n      weight files to load. These files must have been generated through\n      keras\'s model.save_weights(...).\n\n  Returns:\n    metrics_dict: dictionary containing the metrics\n  """"""\n  if weight_files is None:\n    ensemble_logprobs = [log_likelihood_fn([x, y])[0] for _ in range(n_samples)]\n    metric_values = [model.evaluate(x, y, verbose=0)\n                     for _ in range(n_samples)]\n    ensemble_logits = [log_likelihood_fn([x, y])[1] for _ in range(n_samples)]\n  else:\n    ensemble_logprobs = []\n    metric_values = []\n    ensemble_logits = []\n    for filename in weight_files:\n      model.load_weights(filename)\n      ensemble_logprobs.extend([log_likelihood_fn([x, y])[0]\n                                for _ in range(n_samples)])\n      ensemble_logits.extend([log_likelihood_fn([x, y])[1]\n                              for _ in range(n_samples)])\n      metric_values.extend([model.evaluate(x, y, verbose=0)\n                            for _ in range(n_samples)])\n\n  metric_values = np.mean(np.array(metric_values), axis=0)\n  results = {}\n  for m, name in zip(metric_values, model.metrics_names):\n    results[name] = m\n\n  ensemble_logprobs = np.array(ensemble_logprobs)\n  probabilistic_log_likelihood = np.mean(\n      scipy.special.logsumexp(\n          np.sum(ensemble_logprobs, axis=2)\n          if len(ensemble_logprobs.shape) > 2 else ensemble_logprobs,\n          b=1. / ensemble_logprobs.shape[0],\n          axis=0),\n      axis=0)\n  results[\'probabilistic_log_likelihood\'] = probabilistic_log_likelihood\n\n  ensemble_logits = np.array(ensemble_logits)\n  probs = np.mean(scipy.special.softmax(ensemble_logits, axis=2), axis=0)\n  class_pred = np.argmax(probs, axis=1)\n  probabilistic_accuracy = np.mean(np.equal(y, class_pred))\n  results[\'probabilistic_accuracy\'] = probabilistic_accuracy\n  results[\'ece\'], results[\'mce\'] = calibration(\n      one_hot(y, probs.shape[1]), probs)\n  results[\'brier_score\'] = brier_score(one_hot(y, probs.shape[1]), probs)\n  return results\n'"
baselines/mnist/variational_inference.py,34,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Variational inference for LeNet5 on (Fashion) MNIST.""""""\n\nimport os\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport utils  # local file import\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tf1\nimport tensorflow_probability as tfp\n\nflags.DEFINE_enum(\'dataset\', \'mnist\',\n                  enum_values=[\'mnist\', \'fashion_mnist\'],\n                  help=\'Name of the image dataset.\')\nflags.DEFINE_integer(\'training_steps\', 30000, \'Training steps.\')\nflags.DEFINE_integer(\'batch_size\', 256, \'Batch size.\')\nflags.DEFINE_float(\'learning_rate\', 0.001, \'Learning rate.\')\nflags.DEFINE_float(\'learning_rate_for_sampling\', 0.00001, \'Learning rate.\')\nflags.DEFINE_integer(\'auxiliary_sampling_frequency\', 100,\n                     \'Steps between sampling auxiliary variables.\')\nflags.DEFINE_float(\'auxiliary_variance_ratio\', 0.7,\n                   \'Variance ratio of the auxiliary variables wrt the prior.\')\nflags.DEFINE_integer(\'n_auxiliary_variables\', 5,\n                     \'Number of auxiliary variables.\')\nflags.DEFINE_float(\'mean_field_init_untransformed_scale\', -7,\n                   \'Initial scale (before softplus) for mean field.\')\nflags.DEFINE_integer(\'ensemble_size\', 10, \'Number of ensemble components.\')\nflags.DEFINE_integer(\'validation_freq\', 5, \'Validation frequency in steps.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/uci\',\n                    \'The directory where the model weights and \'\n                    \'training/evaluation summaries are stored.\')\nflags.DEFINE_integer(\'seed\', 0, \'Random seed.\')\nFLAGS = flags.FLAGS\n\n\n# TODO(trandustin): Remove need for this boilerplate code.\ndef mean_field_fn(empirical_bayes=False,\n                  initializer=tf1.initializers.he_normal()):\n  """"""Constructors for Gaussian prior and posterior distributions.\n\n  Args:\n    empirical_bayes (bool): Whether to train the variance of the prior or not.\n    initializer (tf1.initializer): Initializer for the posterior means.\n  Returns:\n    prior, posterior (tfp.distribution): prior and posterior\n    to be fed into a Bayesian Layer.\n  """"""\n\n  def prior(dtype, shape, name, trainable, add_variable_fn):\n    """"""Returns the prior distribution (tfp.distributions.Independent).""""""\n    softplus_inverse_scale = np.log(np.exp(1.) - 1.)\n\n    istrainable = add_variable_fn(\n        name=name + \'_istrainable\',\n        shape=(),\n        initializer=tf1.constant_initializer(1.),\n        dtype=dtype,\n        trainable=False)\n\n    untransformed_scale = add_variable_fn(\n        name=name + \'_untransformed_scale\',\n        shape=(),\n        initializer=tf1.constant_initializer(softplus_inverse_scale),\n        dtype=dtype,\n        trainable=empirical_bayes and trainable)\n    scale = (\n        np.finfo(dtype.as_numpy_dtype).eps +\n        tf.nn.softplus(untransformed_scale * istrainable + (1. - istrainable) *\n                       tf1.stop_gradient(untransformed_scale)))\n    loc = add_variable_fn(\n        name=name + \'_loc\',\n        shape=shape,\n        initializer=tf1.constant_initializer(0.),\n        dtype=dtype,\n        trainable=False)\n    dist = tfp.distributions.Normal(loc=loc, scale=scale)\n    dist.istrainable = istrainable\n    dist.untransformed_scale = untransformed_scale\n    batch_ndims = tf1.size(input=dist.batch_shape_tensor())\n    return tfp.distributions.Independent(dist,\n                                         reinterpreted_batch_ndims=batch_ndims)\n\n  def posterior(dtype, shape, name, trainable, add_variable_fn):\n    """"""Returns the posterior distribution (tfp.distributions.Independent).""""""\n    untransformed_scale = add_variable_fn(\n        name=name + \'_untransformed_scale\',\n        shape=shape,\n        initializer=tf1.initializers.random_normal(\n            mean=FLAGS.mean_field_init_untransformed_scale, stddev=0.1),\n        dtype=dtype,\n        trainable=trainable)\n    scale = (\n        np.finfo(dtype.as_numpy_dtype).eps +\n        tf.nn.softplus(untransformed_scale))\n    loc = add_variable_fn(\n        name=name + \'_loc\',\n        shape=shape,\n        initializer=initializer,\n        dtype=dtype,\n        trainable=trainable)\n    dist = tfp.distributions.Normal(loc=loc, scale=scale)\n    dist.untransformed_scale = untransformed_scale\n    batch_ndims = tf1.size(input=dist.batch_shape_tensor())\n    return tfp.distributions.Independent(dist,\n                                         reinterpreted_batch_ndims=batch_ndims)\n\n  return prior, posterior\n\n\ndef sample_auxiliary_op(prior, posterior, aux_variance_ratio):\n  r""""""Sample the auxiliary variable and calculate the conditionals.\n\n  Given a gaussian prior $$\\mathcal{N}(\\mu_z, \\sigma^2_z)$$\n  Define auxiliary variables $$z=a_1+a_2$$ with $$a_1=\\mathcal{N}(0,\n  \\sigma_{a_1}^2)$$ and $$a_2=\\mathcal{N}(\\mu_z, \\sigma_{a_2}^2)$$ with\n  $$\\frac{\\sigma_{a_1}^2}{\\sigma^2_z}=$$aux_variance_ratio and\n  $$\\sigma_{a_1}^2+\\sigma_{a_2}^2=\\sigma_z^2$$.\n  From this, we can calculate the posterior of a1 and the conditional of z.\n\n  Conditional:\n  $$p(a_1|z) =  \\mathcal{N}(z \\frac{\\sigma_{a_1}^2}{\\sigma_{z}^2},\n  \\frac{\\sigma_{a_1}^2\\sigma_{a_2}^2}{\\sigma_z^2})$$\n\n  Posterior of $$a_1$$:\n  $$q(a_1) =\\mathcal{N}(\\mu_{q(z)} \\frac{\\sigma_{a_1}^2}{\\sigma_{z}^2},\n  \\frac{\\sigma_{q(z)}^2\\sigma_{a_1}^4}{\\sigma_{z}^4} +\n  \\frac{\\sigma_{a_1}^2\\sigma_{a_2}^2}{\\sigma_{z}^2})$$\n\n  Conditional posterior:\n  $$q(z|a_1)=\\frac{q(a_1|z)q(z)}{q(a_1)}$$\n\n  $$q(z|a_1)=\\mathcal{N}(\\frac{a_1\\sigma^2_{q(z)}\\sigma^2_{z} +\n  \\mu_{q(z)}\\sigma^2_{a_2}\\sigma^2_{z}}{\\sigma^2_{q(z)}\\sigma^2_{a_1} +\n  \\sigma^2_z\\sigma^2_{a_2}},\n  \\frac{\\sigma^2_{q(z)}\\sigma^2_z\\sigma^2_{a_2}}{\\sigma^2_{a_1}\\sigma^2_{q(z)} +\n  \\sigma^2_{z}\\sigma^2_{a_2}})$$.\n\n  Args:\n    prior: The prior distribution. Must be parameterized by loc and\n      untransformed_scale, with the transformation being the softplus function.\n    posterior: The posterior distribution. Must be parameterized by loc and\n      untransformed_scale, with the transformation being the softplus function.\n    aux_variance_ratio: Ratio of the variance of the auxiliary variable and the\n      prior. The mean of the auxiliary variable is at 0.\n\n  Returns:\n    sampling_op: Tensorflow operation that executes the sampling.\n    log_density_ratio: Tensor containing the density ratio of the auxiliary\n    variable.\n  """"""\n  if aux_variance_ratio > 1. or aux_variance_ratio < 0.:\n    raise ValueError(\n        \'The ratio of the variance of the auxiliary variable must be between 0 \'\n        \'and 1.\'\n    )\n\n  p_a1_loc = tf.zeros_like(prior.loc)\n  p_a1_scale = tf.math.sqrt(prior.scale**2 * aux_variance_ratio)\n  p_a1 = tfp.distributions.Normal(loc=p_a1_loc, scale=p_a1_scale)\n  p_a2_loc = prior.loc\n  p_a2_scale = tf.math.sqrt(prior.scale**2 - p_a1_scale**2)\n  # q(a1)\n  a1_loc = (posterior.loc - prior.loc) * p_a1_scale**2 / prior.scale**2\n  a1_scale = tf.math.sqrt(\n      (posterior.scale**2 * p_a1_scale**2 / prior.scale**2 + p_a2_scale**2) *\n      p_a1_scale**2 / prior.scale**2)\n  q_a1 = tfp.distributions.Normal(loc=a1_loc, scale=a1_scale)\n  a1 = q_a1.sample()\n\n  # q(z|a1)\n  z_a1_loc = prior.loc + (\n      (posterior.loc - prior.loc) * p_a2_scale**2 * prior.scale**2 +\n      a1 * posterior.scale**2 * prior.scale**2) / (\n          prior.scale**2 * p_a2_scale**2 + posterior.scale**2 * p_a1_scale**2)\n  z_a1_scale = tf.math.sqrt(\n      (posterior.scale**2 * p_a2_scale**2 * prior.scale**2) /\n      (prior.scale**2 * p_a2_scale**2 + p_a1_scale**2 * posterior.scale**2))\n\n  with tf1.control_dependencies([\n      q_a1.loc, q_a1.scale, p_a1.loc, p_a1.scale, a1, p_a2_loc, p_a2_scale,\n      z_a1_loc, z_a1_scale\n  ]):\n    log_density_ratio = q_a1.log_prob(a1) - p_a1.log_prob(a1)\n    prior_update = [\n        prior.loc.assign(a1 + p_a2_loc),\n        prior.untransformed_scale.assign(tfp.math.softplus_inverse(p_a2_scale))\n    ]\n    posterior_update = [\n        posterior.loc.assign(z_a1_loc),\n        posterior.untransformed_scale.assign(\n            tfp.math.softplus_inverse(z_a1_scale))\n    ]\n  return [prior_update, posterior_update], tf.reduce_sum(log_density_ratio)\n\n\ndef lenet5(n_examples, input_shape, num_classes):\n  """"""Builds Bayesian LeNet5.""""""\n  p_fn, q_fn = mean_field_fn(empirical_bayes=True)\n  def normalized_kl_fn(q, p, _):\n    return q.kl_divergence(p) / tf.cast(n_examples, tf.float32)\n\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  conv1 = tfp.layers.Convolution2DFlipout(\n      6,\n      kernel_size=5,\n      padding=\'SAME\',\n      activation=tf.nn.relu,\n      kernel_prior_fn=p_fn,\n      kernel_posterior_fn=q_fn,\n      bias_prior_fn=p_fn,\n      bias_posterior_fn=q_fn,\n      kernel_divergence_fn=normalized_kl_fn,\n      bias_divergence_fn=normalized_kl_fn)(inputs)\n  pool1 = tf.keras.layers.MaxPooling2D(pool_size=[2, 2],\n                                       strides=[2, 2],\n                                       padding=\'SAME\')(conv1)\n  conv2 = tfp.layers.Convolution2DFlipout(\n      16,\n      kernel_size=5,\n      padding=\'SAME\',\n      activation=tf.nn.relu,\n      kernel_prior_fn=p_fn,\n      kernel_posterior_fn=q_fn,\n      bias_prior_fn=p_fn,\n      bias_posterior_fn=q_fn,\n      kernel_divergence_fn=normalized_kl_fn,\n      bias_divergence_fn=normalized_kl_fn)(pool1)\n  pool2 = tf.keras.layers.MaxPooling2D(pool_size=[2, 2],\n                                       strides=[2, 2],\n                                       padding=\'SAME\')(conv2)\n  conv3 = tfp.layers.Convolution2DFlipout(\n      120,\n      kernel_size=5,\n      padding=\'SAME\',\n      activation=tf.nn.relu,\n      kernel_prior_fn=p_fn,\n      kernel_posterior_fn=q_fn,\n      bias_prior_fn=p_fn,\n      bias_posterior_fn=q_fn,\n      kernel_divergence_fn=normalized_kl_fn,\n      bias_divergence_fn=normalized_kl_fn)(pool2)\n  flatten = tf.keras.layers.Flatten()(conv3)\n  dense1 = tfp.layers.DenseLocalReparameterization(\n      84,\n      activation=tf.nn.relu,\n      kernel_prior_fn=p_fn,\n      kernel_posterior_fn=q_fn,\n      bias_prior_fn=p_fn,\n      bias_posterior_fn=q_fn,\n      kernel_divergence_fn=normalized_kl_fn,\n      bias_divergence_fn=normalized_kl_fn)(flatten)\n  dense2 = tfp.layers.DenseLocalReparameterization(\n      num_classes,\n      kernel_prior_fn=p_fn,\n      kernel_posterior_fn=q_fn,\n      bias_prior_fn=p_fn,\n      bias_posterior_fn=q_fn,\n      kernel_divergence_fn=normalized_kl_fn,\n      bias_divergence_fn=normalized_kl_fn)(dense1)\n  outputs = tf.keras.layers.Lambda(lambda x: ed.Categorical(logits=x))(dense2)\n  return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n\n\ndef get_losses_and_metrics(model, n_train):\n  """"""Define the losses and metrics for the model.""""""\n\n  def negative_log_likelihood(y, rv_y):\n    del rv_y  # unused arg\n    return -model.output.distribution.log_prob(tf.squeeze(y))\n\n  def accuracy(y_true, y_sample):\n    del y_sample  # unused arg\n    return tf.equal(\n        tf.argmax(input=model.output.distribution.logits, axis=1),\n        tf.cast(tf.squeeze(y_true), tf.int64))\n\n  def log_likelihood(y_true, y_sample):\n    """"""Expected conditional log-likelihood.""""""\n    del y_sample  # unused arg\n    return model.output.distribution.log_prob(tf.squeeze(y_true))\n\n  def kl(y_true, y_sample):\n    """"""KL-divergence.""""""\n    del y_true  # unused arg\n    del y_sample  # unused arg\n    sampling_cost = sum(\n        [l.kl_cost_weight + l.kl_cost_bias for l in model.layers])\n    return sum(model.losses) * n_train + sampling_cost\n\n  def elbo(y_true, y_sample):\n    return log_likelihood(y_true, y_sample) * n_train - kl(y_true, y_sample)\n\n  return negative_log_likelihood, accuracy, log_likelihood, kl, elbo\n\n\ndef main(argv):\n  del argv  # unused arg\n  np.random.seed(FLAGS.seed)\n  tf.random.set_seed(FLAGS.seed)\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  tf1.disable_v2_behavior()\n\n  session = tf1.Session()\n  with session.as_default():\n    x_train, y_train, x_test, y_test = utils.load(FLAGS.dataset, session)\n    n_train = x_train.shape[0]\n\n    num_classes = int(np.amax(y_train)) + 1\n    model = lenet5(n_train, x_train.shape[1:], num_classes)\n    for l in model.layers:\n      l.kl_cost_weight = l.add_weight(\n          name=\'kl_cost_weight\',\n          shape=(),\n          initializer=tf.constant_initializer(0.),\n          trainable=False)\n      l.kl_cost_bias = l.add_variable(\n          name=\'kl_cost_bias\',\n          shape=(),\n          initializer=tf.constant_initializer(0.),\n          trainable=False)\n\n    [negative_log_likelihood,\n     accuracy,\n     log_likelihood,\n     kl,\n     elbo] = get_losses_and_metrics(model, n_train)\n    metrics = [elbo, log_likelihood, kl, accuracy]\n    tensorboard = tf1.keras.callbacks.TensorBoard(\n        log_dir=FLAGS.output_dir,\n        update_freq=FLAGS.batch_size * FLAGS.validation_freq)\n\n    def fit_fn(model,\n               steps,\n               initial_epoch):\n      return model.fit(\n          x=x_train,\n          y=y_train,\n          batch_size=FLAGS.batch_size,\n          epochs=initial_epoch + (FLAGS.batch_size * steps) // n_train,\n          initial_epoch=initial_epoch,\n          validation_data=(x_test, y_test),\n          validation_freq=max(\n              (FLAGS.validation_freq * FLAGS.batch_size) // n_train, 1),\n          verbose=1,\n          callbacks=[tensorboard])\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(lr=float(FLAGS.learning_rate)),\n        loss=negative_log_likelihood,\n        metrics=metrics)\n    session.run(tf1.initialize_all_variables())\n\n    train_epochs = (FLAGS.training_steps * FLAGS.batch_size) // n_train\n    fit_fn(model, FLAGS.training_steps, initial_epoch=0)\n\n    labels = tf.keras.layers.Input(shape=y_train.shape[1:])\n    ll = tf.keras.backend.function([model.input, labels], [\n        model.output.distribution.log_prob(tf.squeeze(labels)),\n        model.output.distribution.logits\n    ])\n\n    base_metrics = [\n        utils.ensemble_metrics(x_train, y_train, model, ll, n_samples=10),\n        utils.ensemble_metrics(x_test, y_test, model, ll, n_samples=10)\n    ]\n    model_dir = os.path.join(FLAGS.output_dir, \'models\')\n    tf.io.gfile.makedirs(model_dir)\n    base_model_filename = os.path.join(model_dir, \'base_model.weights\')\n    model.save_weights(base_model_filename)\n\n    # Train base model further for comparison.\n    fit_fn(\n        model,\n        FLAGS.n_auxiliary_variables * FLAGS.auxiliary_sampling_frequency *\n        FLAGS.ensemble_size,\n        initial_epoch=train_epochs)\n\n    overtrained_metrics = [\n        utils.ensemble_metrics(x_train, y_train, model, ll, n_samples=10),\n        utils.ensemble_metrics(x_test, y_test, model, ll, n_samples=10)\n    ]\n\n    # Perform refined VI.\n    sample_op = []\n    for l in model.layers:\n      if isinstance(l, tfp.layers.DenseLocalReparameterization) or isinstance(\n          l, tfp.layers.Convolution2DFlipout):\n        weight_op, weight_cost = sample_auxiliary_op(\n            l.kernel_prior.distribution, l.kernel_posterior.distribution,\n            FLAGS.auxiliary_variance_ratio)\n        sample_op.append(weight_op)\n        sample_op.append(l.kl_cost_weight.assign_add(weight_cost))\n        # Fix the variance of the prior\n        session.run(l.kernel_prior.distribution.istrainable.assign(0.))\n        if hasattr(l.bias_prior, \'distribution\'):\n          bias_op, bias_cost = sample_auxiliary_op(\n              l.bias_prior.distribution, l.bias_posterior.distribution,\n              FLAGS.auxiliary_variance_ratio)\n          sample_op.append(bias_op)\n          sample_op.append(l.kl_cost_bias.assign_add(bias_cost))\n          # Fix the variance of the prior\n          session.run(l.bias_prior.distribution.istrainable.assign(0.))\n\n    ensemble_filenames = []\n    for i in range(FLAGS.ensemble_size):\n      model.load_weights(base_model_filename)\n      for j in range(FLAGS.n_auxiliary_variables):\n        session.run(sample_op)\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(\n                # The learning rate is proportional to the scale of the prior.\n                lr=float(FLAGS.learning_rate_for_sampling *\n                         np.sqrt(1. - FLAGS.auxiliary_variance_ratio)**j)),\n            loss=negative_log_likelihood,\n            metrics=metrics)\n        fit_fn(\n            model,\n            FLAGS.auxiliary_sampling_frequency,\n            initial_epoch=train_epochs)\n      ensemble_filename = os.path.join(\n          model_dir, \'ensemble_component_\' + str(i) + \'.weights\')\n      ensemble_filenames.append(ensemble_filename)\n      model.save_weights(ensemble_filename)\n\n    auxiliary_metrics = [\n        utils.ensemble_metrics(\n            x_train,\n            y_train,\n            model,\n            ll,\n            weight_files=ensemble_filenames, n_samples=10),\n        utils.ensemble_metrics(\n            x_test,\n            y_test,\n            model,\n            ll,\n            weight_files=ensemble_filenames, n_samples=10)\n    ]\n\n    for metrics, name in [(base_metrics, \'Base model\'),\n                          (overtrained_metrics, \'Overtrained model\'),\n                          (auxiliary_metrics, \'Auxiliary sampling\')]:\n      logging.info(name)\n      for metrics_dict, split in [(metrics[0], \'train\'),\n                                  (metrics[1], \'test\')]:\n        logging.info(split)\n        for metric_name in metrics_dict:\n          logging.info(\'%s: %s\', metric_name, metrics_dict[metric_name])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/uci/deterministic.py,22,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""MLP on UCI data trained with maximum likelihood and gradient descent.""""""\n\nimport os\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport utils  # local file import\nimport numpy as np\nimport tensorflow as tf\n\nflags.DEFINE_enum(\'dataset\', \'boston_housing\',\n                  enum_values=[\'boston_housing\',\n                               \'concrete_strength\',\n                               \'energy_efficiency\',\n                               \'naval_propulsion\',\n                               \'kin8nm\',\n                               \'power_plant\',\n                               \'protein_structure\',\n                               \'wine\',\n                               \'yacht_hydrodynamics\'],\n                  help=\'Name of the UCI dataset.\')\nflags.DEFINE_integer(\'ensemble_size\', 1, \'Number of ensemble members.\')\nflags.DEFINE_boolean(\'bootstrap\', False,\n                     \'Sample the training set for bootstrapping.\')\nflags.DEFINE_integer(\'training_steps\', 2500, \'Training steps.\')\nflags.DEFINE_integer(\'batch_size\', 256, \'Batch size.\')\nflags.DEFINE_float(\'learning_rate\', 0.001, \'Learning rate.\')\nflags.DEFINE_float(\'epsilon\', 0.,\n                   \'Epsilon for adversarial training. It is given as a ratio \'\n                   \'of the input range (e.g the adjustment is 2.55 if input \'\n                   \'range is [0,255]). Set to 0. for no adversarial training.\')\nflags.DEFINE_integer(\'validation_freq\', 5, \'Validation frequency in steps.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/uci\',\n                    \'The directory where the model weights and \'\n                    \'training/evaluation summaries are stored.\')\nflags.DEFINE_integer(\'seed\', 0,\n                     \'Random seed. Note train/test splits are random and also \'\n                     \'based on this seed.\')\nFLAGS = flags.FLAGS\n\n\n# TODO(trandustin): Change to act like InputLayer or just swap to a Normal\n# custom layer with a scale tf.Variable.\nclass VariableInputLayer(tf.keras.layers.Layer):\n  """"""Layer as an entry point into a Model, and which is learnable.""""""\n\n  def __init__(self,\n               input_shape,\n               initializer=\'zeros\',\n               regularizer=None,\n               constraint=None,\n               **kwargs):\n    self.shape = input_shape\n    self.initializer = ed.initializers.get(initializer)\n    self.regularizer = ed.regularizers.get(regularizer)\n    self.constraint = ed.constraints.get(constraint)\n    super(VariableInputLayer, self).__init__(**kwargs)\n\n  def build(self, input_shape):\n    del input_shape  # unused arg\n    self.variable = self.add_weight(shape=self.shape,\n                                    name=\'variable\',\n                                    initializer=self.initializer,\n                                    regularizer=self.regularizer,\n                                    constraint=None)\n    self.built = True\n\n  def call(self, inputs):\n    del inputs  # unused arg\n    variable = tf.convert_to_tensor(self.variable)\n    if self.constraint is not None:\n      variable = self.constraint(variable)\n    return variable\n\n  def get_config(self):\n    config = {\n        \'input_shape\': self.shape,\n        \'initializer\': ed.initializers.serialize(self.initializer),\n        \'regularizer\': ed.regularizers.serialize(self.regularizer),\n        \'constraint\': ed.constraints.serialize(self.constraint),\n    }\n    base_config = super(VariableInputLayer, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n\n\ndef multilayer_perceptron(input_shape, output_scaler=1.):\n  """"""Builds a single hidden layer feedforward network.\n\n  Args:\n    input_shape: tf.TensorShape.\n    output_scaler: Float to scale mean predictions. Training is faster and more\n      stable when both the inputs and outputs are normalized. To not affect\n      metrics such as RMSE and NLL, the outputs need to be scaled back\n      (de-normalized, but the mean doesn\'t matter), using output_scaler.\n\n  Returns:\n    tf.keras.Model.\n  """"""\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  hidden = tf.keras.layers.Dense(50, activation=\'relu\')(inputs)\n  loc = tf.keras.layers.Dense(1, activation=None)(hidden)\n  loc = tf.keras.layers.Lambda(lambda x: x * output_scaler)(loc)\n  # The variable layer must depend on a symbolic input tensor.\n  scale = VariableInputLayer((), constraint=\'softplus\')(inputs)\n  outputs = tf.keras.layers.Lambda(lambda x: ed.Normal(loc=x[0], scale=x[1]))(\n      (loc, scale))\n  return tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\ndef make_adversarial_loss_fn(model):\n  """"""Returns loss function with adversarial training.""""""\n  def loss_fn(x, y):\n    """"""Loss function with adversarial training.""""""\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      predictions = model(x)\n      loss = -tf.reduce_mean(predictions.distribution.log_prob(y))\n    dx = tape.gradient(loss, x)\n    # Assume the training data is normalized.\n    adv_inputs = x + FLAGS.epsilon * tf.math.sign(tf.stop_gradient(dx))\n    adv_predictions = model(adv_inputs)\n    adv_loss = -tf.reduce_mean(adv_predictions.distribution.log_prob(y))\n    return 0.5 * loss + 0.5 * adv_loss\n  return loss_fn\n\n\ndef main(argv):\n  del argv  # unused arg\n  np.random.seed(FLAGS.seed)\n  tf.random.set_seed(FLAGS.seed)\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n\n  x_train, y_train, x_test, y_test = utils.load(FLAGS.dataset)\n  n_train = x_train.shape[0]\n  ensemble_filenames = []\n  for i in range(FLAGS.ensemble_size):\n    model = multilayer_perceptron(\n        x_train.shape[1:], np.std(y_train, axis=0) + tf.keras.backend.epsilon())\n    if FLAGS.epsilon:\n      loss_fn = make_adversarial_loss_fn(model)\n      optimizer = tf.keras.optimizers.Adam(lr=FLAGS.learning_rate)\n    else:\n      def negative_log_likelihood(y_true, y_pred):\n        return -y_pred.distribution.log_prob(y_true)\n      model.compile(optimizer=tf.keras.optimizers.Adam(lr=FLAGS.learning_rate),\n                    loss=negative_log_likelihood)\n\n    member_dir = os.path.join(FLAGS.output_dir, \'member_\' + str(i))\n    tensorboard = tf.keras.callbacks.TensorBoard(\n        log_dir=member_dir,\n        update_freq=FLAGS.batch_size * FLAGS.validation_freq)\n    if FLAGS.epsilon:\n      for epoch in range((FLAGS.batch_size * FLAGS.training_steps) // n_train):\n        logging.info(\'Epoch %s\', epoch)\n        for j in range(n_train // FLAGS.batch_size):\n          perm = np.random.permutation(n_train)\n          with tf.GradientTape() as tape:\n            loss = loss_fn(x_train[perm[j:j + FLAGS.batch_size]],\n                           y_train[perm[j:j + FLAGS.batch_size]])\n          grads = tape.gradient(loss, model.trainable_weights)\n          optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    else:\n      if FLAGS.bootstrap:\n        inds = np.random.choice(n_train, n_train, replace=True)\n        x_sampled = x_train[inds]\n        y_sampled = y_train[inds]\n\n      model.fit(\n          x=x_train if not FLAGS.bootstrap else x_sampled,\n          y=y_train if not FLAGS.bootstrap else y_sampled,\n          batch_size=FLAGS.batch_size,\n          epochs=(FLAGS.batch_size * FLAGS.training_steps) // n_train,\n          validation_data=(x_test, y_test),\n          validation_freq=max(\n              (FLAGS.validation_freq * FLAGS.batch_size) // n_train, 1),\n          verbose=0,\n          callbacks=[tensorboard])\n\n    member_filename = os.path.join(member_dir, \'model.weights\')\n    ensemble_filenames.append(member_filename)\n    model.save_weights(member_filename)\n\n  # TODO(trandustin): Move this into utils.ensemble_metrics. It\'s currently\n  # separate so that VI can use utils.ensemble_metrics while in TF1.\n  def ll(arg):\n    features, labels = arg\n    predictions = model(features)\n    log_prob = predictions.distribution.log_prob(labels)\n    error = predictions.distribution.loc - labels\n    return [log_prob, error]\n\n  ensemble_metrics_vals = {\n      \'train\': utils.ensemble_metrics(\n          x_train, y_train, model, ll, weight_files=ensemble_filenames),\n      \'test\': utils.ensemble_metrics(\n          x_test, y_test, model, ll, weight_files=ensemble_filenames),\n  }\n\n  for split, metrics in ensemble_metrics_vals.items():\n    logging.info(split)\n    for metric_name in metrics:\n      logging.info(\'%s: %s\', metric_name, metrics[metric_name])\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
baselines/uci/utils.py,3,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities for UCI datasets.""""""\n\nimport collections\nimport os\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport tensorflow as tf\n\n\nclass DataSpec(collections.namedtuple(\n    \'UCIDataSpec\', \'path,desc,label,excluded\')):\n\n  __slots__ = []\n\n\n# TODO(trandustin): Avoid hard-coding directory string so it\'s user-specified.\nUCI_BASE_DIR = \'/tmp/uci_datasets\'\nDATA_SPECS = {\n    \'boston_housing\': DataSpec(\n        path=os.path.join(UCI_BASE_DIR, \'boston_housing.csv\'),\n        desc=(\'The Boston housing data was collected in 1978 and each of the \'\n              \'506 entries represent aggregated data about 14 features for \'\n              \'homes from various suburbs in Boston, Massachusetts.\'),\n        label=\'MEDV\',\n        excluded=[]),\n    \'concrete_strength\': DataSpec(\n        path=os.path.join(UCI_BASE_DIR, \'concrete_strength.csv\'),\n        desc=(\'The Boston housing data was collected in 1978 and each of the \'\n              \'506 entries represent aggregated data about 14 features for \'\n              \'homes from various suburbs in Boston, Massachusetts.\'),\n        label=\'concrete_compressive_strength\',\n        excluded=[]),\n    \'energy_efficiency\': DataSpec(\n        path=os.path.join(UCI_BASE_DIR, \'energy_efficiency.csv\'),\n        desc=(\'This study looked into assessing the heating load and cooling \'\n              \'load requirements of buildings (that is, energy efficiency) as \'\n              \'a function of building parameters. **Heating load only**.\'),\n        label=\'Y1\',\n        excluded=[\'Y2\']),\n    \'naval_propulsion\': DataSpec(\n        path=os.path.join(UCI_BASE_DIR, \'naval_propulsion.csv\'),\n        desc=(\'Data have been generated from a sophisticated simulator of a \'\n              \'Gas Turbines (GT), mounted on a Frigate characterized by a \'\n              \'Combined Diesel eLectric And Gas (CODLAG) propulsion plant \'\n              \'type. **GT Turbine decay state coefficient only**\'),\n        label=\'GT Turbine decay state coefficient\',\n        excluded=[\'GT Compressor decay state coefficient\']),\n    \'kin8nm\': DataSpec(\n        path=os.path.join(UCI_BASE_DIR, \'kin8nm.csv\'),\n        desc=(\'This is data set is concerned with the forward kinematics of \'\n              \'an 8 link robot arm. Among the existing variants of this data \'\n              \'set we have used the variant 8nm, which is known to be highly \'\n              \'non-linear and medium noisy.\'),\n        label=\'y\',\n        excluded=[]),\n    \'power_plant\': DataSpec(\n        path=os.path.join(UCI_BASE_DIR, \'power_plant.csv\'),\n        desc=(\'The Boston housing data was collected in 1978 and each of the \'\n              \'506 entries represent aggregated data about 14 features for \'\n              \'homes from various suburbs in Boston, Massachusetts.\'),\n        label=\'PE\',\n        excluded=[]),\n    \'protein_structure\': DataSpec(\n        path=os.path.join(UCI_BASE_DIR, \'protein_structure.csv\'),\n        desc=(\'This is a data set of Physicochemical Properties of Protein \'\n              \'Tertiary Structure. The data set is taken from CASP 5-9. There \'\n              \'are 45730 decoys and size varying from 0 to 21 armstrong.\'),\n        label=\'RMSD\',\n        excluded=[]),\n    \'wine\': DataSpec(\n        path=os.path.join(UCI_BASE_DIR, \'wine.csv\'),\n        desc=(\'The dataset is related to red variant of the Portuguese \'\n              \'""Vinho Verde"" wine. **NB contains red wine examples only**\'),\n        label=\'quality\',\n        excluded=[]),\n    \'yacht_hydrodynamics\': DataSpec(\n        path=os.path.join(UCI_BASE_DIR, \'yacht_hydrodynamics.csv\'),\n        desc=(\'Delft data set, used to predict the hydodynamic performance of \'\n              \'sailing yachts from dimensions and velocity.\'),\n        label=\'Residuary resistance per unit weight of displacement\',\n        excluded=[])\n}\n\n\ndef get_uci_data(name):\n  """"""Returns an array of features and a vector of labels for dataset `name`.""""""\n  spec = DATA_SPECS.get(name)\n  if spec is None:\n    raise ValueError(\'Unknown dataset: {}. Available datasets:\\n{}\'.format(\n        name, \'\\n\'.join(DATA_SPECS.keys())))\n  with tf.io.gfile.GFile(spec.path) as f:\n    df = pd.read_csv(f)\n  labels = df.pop(spec.label).as_matrix().astype(np.float32)\n  for ex in spec.excluded:\n    _ = df.pop(ex)\n  features = df.as_matrix().astype(np.float32)\n  return features, labels\n\n\ndef load(name):\n  """"""Loads dataset as numpy array.""""""\n  x, y = get_uci_data(name)\n  if len(y.shape) == 1:\n    y = y[:, None]\n  train_test_split = 0.8\n  random_permutation = np.random.permutation(x.shape[0])\n  n_train = int(x.shape[0] * train_test_split)\n  train_ind = random_permutation[:n_train]\n  test_ind = random_permutation[n_train:]\n  x_train, y_train = x[train_ind, :], y[train_ind, :]\n  x_test, y_test = x[test_ind, :], y[test_ind, :]\n\n  x_mean, x_std = np.mean(x_train, axis=0), np.std(x_train, axis=0)\n  y_mean = np.mean(y_train, axis=0)\n  epsilon = tf.keras.backend.epsilon()\n  x_train = (x_train - x_mean) / (x_std + epsilon)\n  x_test = (x_test - x_mean) / (x_std + epsilon)\n  y_train, y_test = y_train - y_mean, y_test - y_mean\n  return x_train, y_train, x_test, y_test\n\n\ndef ensemble_metrics(x,\n                     y,\n                     model,\n                     log_likelihood_fn,\n                     n_samples=1,\n                     weight_files=None):\n  """"""Evaluate metrics of an ensemble.\n\n  Args:\n    x: numpy array of inputs\n    y: numpy array of labels\n    model: tf.keras.Model.\n    log_likelihood_fn: function which takes tuple of x, y and returns batched\n      tuple output of the log prob and mean error.\n    n_samples: number of Monte Carlo samples to draw per ensemble member (each\n      weight file).\n    weight_files: to draw samples from multiple weight sets, specify a list of\n      weight files to load. These files must have been generated through\n      keras\'s model.save_weights(...).\n\n  Returns:\n    metrics_dict: dictionary containing the metrics\n  """"""\n  if weight_files is None:\n    ensemble_logprobs = [log_likelihood_fn([x, y])[0] for _ in range(n_samples)]\n    ensemble_error = [log_likelihood_fn([x, y])[1] for _ in range(n_samples)]\n  else:\n    ensemble_logprobs = []\n    ensemble_error = []\n    for filename in weight_files:\n      model.load_weights(filename)\n      ensemble_logprobs.extend([\n          log_likelihood_fn([x, y])[0] for _ in range(n_samples)])\n      ensemble_error.extend([\n          log_likelihood_fn([x, y])[1] for _ in range(n_samples)])\n\n  results = {}\n  ensemble_logprobs = np.array(ensemble_logprobs)\n  results[\'log_likelihood\'] = np.mean(ensemble_logprobs)\n  results[\'mse\'] = np.mean(np.square(ensemble_error))\n  probabilistic_log_likelihood = np.mean(\n      scipy.special.logsumexp(\n          np.sum(ensemble_logprobs, axis=2)\n          if len(ensemble_logprobs.shape) > 2 else ensemble_logprobs,\n          b=1. / ensemble_logprobs.shape[0],\n          axis=0),\n      axis=0)\n  results[\'probabilistic_log_likelihood\'] = probabilistic_log_likelihood\n  ensemble_error = np.stack([np.array(l) for l in ensemble_error])\n  results[\'probabilistic_mse\'] = np.mean(\n      np.square(np.mean(ensemble_error, axis=0)))\n  return results\n'"
baselines/uci/variational_inference.py,28,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Variational inference for MLP on UCI data.""""""\n\nimport os\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nimport utils  # local file import\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tf1\nimport tensorflow_probability as tfp\n\nflags.DEFINE_enum(\'dataset\', \'boston_housing\',\n                  enum_values=[\'boston_housing\',\n                               \'concrete_strength\',\n                               \'energy_efficiency\',\n                               \'naval_propulsion\',\n                               \'kin8nm\',\n                               \'power_plant\',\n                               \'protein_structure\',\n                               \'wine\',\n                               \'yacht_hydrodynamics\'],\n                  help=\'Name of the UCI dataset.\')\nflags.DEFINE_integer(\'training_steps\', 30000, \'Training steps.\')\nflags.DEFINE_integer(\'batch_size\', 256, \'Batch size.\')\nflags.DEFINE_float(\'learning_rate\', 0.001, \'Learning rate.\')\nflags.DEFINE_float(\'learning_rate_for_sampling\', 0.00001, \'Learning rate.\')\nflags.DEFINE_integer(\'auxiliary_sampling_frequency\', 100,\n                     \'Steps between sampling auxiliary variables.\')\nflags.DEFINE_float(\'auxiliary_variance_ratio\', 0.7,\n                   \'Variance ratio of the auxiliary variables wrt the prior.\')\nflags.DEFINE_integer(\'n_auxiliary_variables\', 5,\n                     \'Number of auxiliary variables.\')\nflags.DEFINE_float(\'mean_field_init_untransformed_scale\', -7,\n                   \'Initial scale (before softplus) for mean field.\')\nflags.DEFINE_integer(\'ensemble_size\', 10, \'Number of ensemble components.\')\nflags.DEFINE_integer(\'validation_freq\', 5, \'Validation frequency in steps.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/uci\',\n                    \'The directory where the model weights and \'\n                    \'training/evaluation summaries are stored.\')\nflags.DEFINE_integer(\'seed\', 0, \'Random seed.\')\nFLAGS = flags.FLAGS\n\n\n# TODO(trandustin): Remove need for this boilerplate code.\ndef mean_field_fn(empirical_bayes=False,\n                  initializer=tf1.initializers.he_normal()):\n  """"""Constructors for Gaussian prior and posterior distributions.\n\n  Args:\n    empirical_bayes (bool): Whether to train the variance of the prior or not.\n    initializer (tf1.initializer): Initializer for the posterior means.\n  Returns:\n    prior, posterior (tfp.distribution): prior and posterior\n    to be fed into a Bayesian Layer.\n  """"""\n\n  def prior(dtype, shape, name, trainable, add_variable_fn):\n    """"""Returns the prior distribution (tfp.distributions.Independent).""""""\n    softplus_inverse_scale = np.log(np.exp(1.) - 1.)\n\n    istrainable = add_variable_fn(\n        name=name + \'_istrainable\',\n        shape=(),\n        initializer=tf1.constant_initializer(1.),\n        dtype=dtype,\n        trainable=False)\n\n    untransformed_scale = add_variable_fn(\n        name=name + \'_untransformed_scale\',\n        shape=(),\n        initializer=tf1.constant_initializer(softplus_inverse_scale),\n        dtype=dtype,\n        trainable=empirical_bayes and trainable)\n    scale = (\n        np.finfo(dtype.as_numpy_dtype).eps +\n        tf.nn.softplus(untransformed_scale * istrainable + (1. - istrainable) *\n                       tf1.stop_gradient(untransformed_scale)))\n    loc = add_variable_fn(\n        name=name + \'_loc\',\n        shape=shape,\n        initializer=tf1.constant_initializer(0.),\n        dtype=dtype,\n        trainable=False)\n    dist = tfp.distributions.Normal(loc=loc, scale=scale)\n    dist.istrainable = istrainable\n    dist.untransformed_scale = untransformed_scale\n    batch_ndims = tf1.size(input=dist.batch_shape_tensor())\n    return tfp.distributions.Independent(dist,\n                                         reinterpreted_batch_ndims=batch_ndims)\n\n  def posterior(dtype, shape, name, trainable, add_variable_fn):\n    """"""Returns the posterior distribution (tfp.distributions.Independent).""""""\n    untransformed_scale = add_variable_fn(\n        name=name + \'_untransformed_scale\',\n        shape=shape,\n        initializer=tf1.initializers.random_normal(\n            mean=FLAGS.mean_field_init_untransformed_scale, stddev=0.1),\n        dtype=dtype,\n        trainable=trainable)\n    scale = (\n        np.finfo(dtype.as_numpy_dtype).eps +\n        tf.nn.softplus(untransformed_scale))\n    loc = add_variable_fn(\n        name=name + \'_loc\',\n        shape=shape,\n        initializer=initializer,\n        dtype=dtype,\n        trainable=trainable)\n    dist = tfp.distributions.Normal(loc=loc, scale=scale)\n    dist.untransformed_scale = untransformed_scale\n    batch_ndims = tf1.size(input=dist.batch_shape_tensor())\n    return tfp.distributions.Independent(dist,\n                                         reinterpreted_batch_ndims=batch_ndims)\n\n  return prior, posterior\n\n\ndef sample_auxiliary_op(prior, posterior, aux_variance_ratio):\n  r""""""Sample the auxiliary variable and calculate the conditionals.\n\n  Given a gaussian prior $$\\mathcal{N}(\\mu_z, \\sigma^2_z)$$\n  Define auxiliary variables $$z=a_1+a_2$$ with $$a_1=\\mathcal{N}(0,\n  \\sigma_{a_1}^2)$$ and $$a_2=\\mathcal{N}(\\mu_z, \\sigma_{a_2}^2)$$ with\n  $$\\frac{\\sigma_{a_1}^2}{\\sigma^2_z}=$$aux_variance_ratio and\n  $$\\sigma_{a_1}^2+\\sigma_{a_2}^2=\\sigma_z^2$$.\n  From this, we can calculate the posterior of a1 and the conditional of z.\n\n  Conditional:\n  $$p(a_1|z) =  \\mathcal{N}(z \\frac{\\sigma_{a_1}^2}{\\sigma_{z}^2},\n  \\frac{\\sigma_{a_1}^2\\sigma_{a_2}^2}{\\sigma_z^2})$$\n\n  Posterior of $$a_1$$:\n  $$q(a_1) =\\mathcal{N}(\\mu_{q(z)} \\frac{\\sigma_{a_1}^2}{\\sigma_{z}^2},\n  \\frac{\\sigma_{q(z)}^2\\sigma_{a_1}^4}{\\sigma_{z}^4} +\n  \\frac{\\sigma_{a_1}^2\\sigma_{a_2}^2}{\\sigma_{z}^2})$$\n\n  Conditional posterior:\n  $$q(z|a_1)=\\frac{q(a_1|z)q(z)}{q(a_1)}$$\n\n  $$q(z|a_1)=\\mathcal{N}(\\frac{a_1\\sigma^2_{q(z)}\\sigma^2_{z} +\n  \\mu_{q(z)}\\sigma^2_{a_2}\\sigma^2_{z}}{\\sigma^2_{q(z)}\\sigma^2_{a_1} +\n  \\sigma^2_z\\sigma^2_{a_2}},\n  \\frac{\\sigma^2_{q(z)}\\sigma^2_z\\sigma^2_{a_2}}{\\sigma^2_{a_1}\\sigma^2_{q(z)} +\n  \\sigma^2_{z}\\sigma^2_{a_2}})$$.\n\n  Args:\n    prior: The prior distribution. Must be parameterized by loc and\n      untransformed_scale, with the transformation being the softplus function.\n    posterior: The posterior distribution. Must be parameterized by loc and\n      untransformed_scale, with the transformation being the softplus function.\n    aux_variance_ratio: Ratio of the variance of the auxiliary variable and the\n      prior. The mean of the auxiliary variable is at 0.\n\n  Returns:\n    sampling_op: Tensorflow operation that executes the sampling.\n    log_density_ratio: Tensor containing the density ratio of the auxiliary\n    variable.\n  """"""\n  if aux_variance_ratio > 1. or aux_variance_ratio < 0.:\n    raise ValueError(\n        \'The ratio of the variance of the auxiliary variable must be between 0 \'\n        \'and 1.\'\n    )\n\n  p_a1_loc = tf.zeros_like(prior.loc)\n  p_a1_scale = tf.math.sqrt(prior.scale**2 * aux_variance_ratio)\n  p_a1 = tfp.distributions.Normal(loc=p_a1_loc, scale=p_a1_scale)\n  p_a2_loc = prior.loc\n  p_a2_scale = tf.math.sqrt(prior.scale**2 - p_a1_scale**2)\n  # q(a1)\n  a1_loc = (posterior.loc - prior.loc) * p_a1_scale**2 / prior.scale**2\n  a1_scale = tf.math.sqrt(\n      (posterior.scale**2 * p_a1_scale**2 / prior.scale**2 + p_a2_scale**2) *\n      p_a1_scale**2 / prior.scale**2)\n  q_a1 = tfp.distributions.Normal(loc=a1_loc, scale=a1_scale)\n  a1 = q_a1.sample()\n\n  # q(z|a1)\n  z_a1_loc = prior.loc + (\n      (posterior.loc - prior.loc) * p_a2_scale**2 * prior.scale**2 +\n      a1 * posterior.scale**2 * prior.scale**2) / (\n          prior.scale**2 * p_a2_scale**2 + posterior.scale**2 * p_a1_scale**2)\n  z_a1_scale = tf.math.sqrt(\n      (posterior.scale**2 * p_a2_scale**2 * prior.scale**2) /\n      (prior.scale**2 * p_a2_scale**2 + p_a1_scale**2 * posterior.scale**2))\n\n  with tf1.control_dependencies([\n      q_a1.loc, q_a1.scale, p_a1.loc, p_a1.scale, a1, p_a2_loc, p_a2_scale,\n      z_a1_loc, z_a1_scale\n  ]):\n    log_density_ratio = q_a1.log_prob(a1) - p_a1.log_prob(a1)\n    prior_update = [\n        prior.loc.assign(a1 + p_a2_loc),\n        prior.untransformed_scale.assign(tfp.math.softplus_inverse(p_a2_scale))\n    ]\n    posterior_update = [\n        posterior.loc.assign(z_a1_loc),\n        posterior.untransformed_scale.assign(\n            tfp.math.softplus_inverse(z_a1_scale))\n    ]\n  return [prior_update, posterior_update], tf.reduce_sum(log_density_ratio)\n\n\ndef multilayer_perceptron(n_examples, input_shape, output_scaler=1.):\n  """"""Builds a single hidden layer Bayesian feedforward network.\n\n  Args:\n    n_examples: Number of examples in training set.\n    input_shape: tf.TensorShape.\n    output_scaler: Float to scale mean predictions. Training is faster and more\n      stable when both the inputs and outputs are normalized. To not affect\n      metrics such as RMSE and NLL, the outputs need to be scaled back\n      (de-normalized, but the mean doesn\'t matter), using output_scaler.\n\n  Returns:\n    tf.keras.Model.\n  """"""\n  p_fn, q_fn = mean_field_fn(empirical_bayes=True)\n  def normalized_kl_fn(q, p, _):\n    return q.kl_divergence(p) / tf.cast(n_examples, tf.float32)\n\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  hidden = tfp.layers.DenseLocalReparameterization(\n      50,\n      activation=\'relu\',\n      kernel_prior_fn=p_fn,\n      kernel_posterior_fn=q_fn,\n      bias_prior_fn=p_fn,\n      bias_posterior_fn=q_fn,\n      kernel_divergence_fn=normalized_kl_fn,\n      bias_divergence_fn=normalized_kl_fn)(inputs)\n  loc = tfp.layers.DenseLocalReparameterization(\n      1,\n      activation=None,\n      kernel_prior_fn=p_fn,\n      kernel_posterior_fn=q_fn,\n      bias_prior_fn=p_fn,\n      bias_posterior_fn=q_fn,\n      kernel_divergence_fn=normalized_kl_fn,\n      bias_divergence_fn=normalized_kl_fn)(hidden)\n  loc = tf.keras.layers.Lambda(lambda x: x * output_scaler)(loc)\n  scale = tfp.layers.VariableLayer(\n      shape=(), initializer=tf.keras.initializers.Constant(-3.))(loc)\n  scale = tf.keras.layers.Activation(\'softplus\')(scale)\n  outputs = tf.keras.layers.Lambda(lambda x: ed.Normal(loc=x[0], scale=x[1]))(\n      (loc, scale))\n  return tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\ndef get_losses_and_metrics(model, n_train):\n  """"""Define the losses and metrics for the model.""""""\n\n  def negative_log_likelihood(y, rv_y):\n    del rv_y  # unused arg\n    return -model.output.distribution.log_prob(y)\n\n  def mse(y_true, y_sample):\n    """"""Mean-squared error.""""""\n    del y_sample  # unused arg\n    return tf.math.square(model.output.distribution.loc - y_true)\n\n  def log_likelihood(y_true, y_sample):\n    del y_sample  # unused arg\n    return model.output.distribution.log_prob(y_true)\n\n  def kl(y_true, y_sample):\n    """"""KL-divergence.""""""\n    del y_true  # unused arg\n    del y_sample  # unused arg\n    sampling_cost = sum(\n        [l.kl_cost_weight + l.kl_cost_bias for l in model.layers])\n    return sum(model.losses) * n_train + sampling_cost\n\n  def elbo(y_true, y_sample):\n    return log_likelihood(y_true, y_sample) * n_train - kl(y_true, y_sample)\n\n  return negative_log_likelihood, mse, log_likelihood, kl, elbo\n\n\ndef main(argv):\n  del argv  # unused arg\n  np.random.seed(FLAGS.seed)\n  tf.random.set_seed(FLAGS.seed)\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  tf1.disable_v2_behavior()\n\n  session = tf1.Session()\n  with session.as_default():\n    x_train, y_train, x_test, y_test = utils.load(FLAGS.dataset)\n    n_train = x_train.shape[0]\n\n    model = multilayer_perceptron(\n        n_train,\n        x_train.shape[1:],\n        np.std(y_train) + tf.keras.backend.epsilon())\n    for l in model.layers:\n      l.kl_cost_weight = l.add_weight(\n          name=\'kl_cost_weight\',\n          shape=(),\n          initializer=tf.constant_initializer(0.),\n          trainable=False)\n      l.kl_cost_bias = l.add_variable(\n          name=\'kl_cost_bias\',\n          shape=(),\n          initializer=tf.constant_initializer(0.),\n          trainable=False)\n\n    [negative_log_likelihood,\n     mse,\n     log_likelihood,\n     kl,\n     elbo] = get_losses_and_metrics(model, n_train)\n    metrics = [elbo, log_likelihood, kl, mse]\n\n    tensorboard = tf1.keras.callbacks.TensorBoard(\n        log_dir=FLAGS.output_dir,\n        update_freq=FLAGS.batch_size * FLAGS.validation_freq)\n\n    def fit_fn(model,\n               steps,\n               initial_epoch):\n      return model.fit(\n          x=x_train,\n          y=y_train,\n          batch_size=FLAGS.batch_size,\n          epochs=initial_epoch + (FLAGS.batch_size * steps) // n_train,\n          initial_epoch=initial_epoch,\n          validation_data=(x_test, y_test),\n          validation_freq=max(\n              (FLAGS.validation_freq * FLAGS.batch_size) // n_train, 1),\n          verbose=1,\n          callbacks=[tensorboard])\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(lr=float(FLAGS.learning_rate)),\n        loss=negative_log_likelihood,\n        metrics=metrics)\n    session.run(tf1.initialize_all_variables())\n\n    train_epochs = (FLAGS.training_steps * FLAGS.batch_size) // n_train\n    fit_fn(model, FLAGS.training_steps, initial_epoch=0)\n\n    labels = tf.keras.layers.Input(shape=y_train.shape[1:])\n    ll = tf.keras.backend.function(\n        [model.input, labels],\n        [model.output.distribution.log_prob(labels),\n         model.output.distribution.loc - labels])\n\n    base_metrics = [\n        utils.ensemble_metrics(x_train, y_train, model, ll),\n        utils.ensemble_metrics(x_test, y_test, model, ll),\n    ]\n    model_dir = os.path.join(FLAGS.output_dir, \'models\')\n    tf.io.gfile.makedirs(model_dir)\n    base_model_filename = os.path.join(model_dir, \'base_model.weights\')\n    model.save_weights(base_model_filename)\n\n    # Train base model further for comparison.\n    fit_fn(\n        model,\n        FLAGS.n_auxiliary_variables * FLAGS.auxiliary_sampling_frequency *\n        FLAGS.ensemble_size,\n        initial_epoch=train_epochs)\n\n    overtrained_metrics = [\n        utils.ensemble_metrics(x_train, y_train, model, ll),\n        utils.ensemble_metrics(x_test, y_test, model, ll),\n    ]\n\n    # Perform refined VI.\n    sample_op = []\n    for l in model.layers:\n      if hasattr(l, \'kernel_prior\'):\n        weight_op, weight_cost = sample_auxiliary_op(\n            l.kernel_prior.distribution, l.kernel_posterior.distribution,\n            FLAGS.auxiliary_variance_ratio)\n        sample_op.append(weight_op)\n        sample_op.append(l.kl_cost_weight.assign_add(weight_cost))\n        # Fix the variance of the prior\n        session.run(l.kernel_prior.distribution.istrainable.assign(0.))\n        if hasattr(l.bias_prior, \'distribution\'):\n          bias_op, bias_cost = sample_auxiliary_op(\n              l.bias_prior.distribution, l.bias_posterior.distribution,\n              FLAGS.auxiliary_variance_ratio)\n          sample_op.append(bias_op)\n          sample_op.append(l.kl_cost_bias.assign_add(bias_cost))\n          # Fix the variance of the prior\n          session.run(l.bias_prior.distribution.istrainable.assign(0.))\n\n    ensemble_filenames = []\n    for i in range(FLAGS.ensemble_size):\n      model.load_weights(base_model_filename)\n      for j in range(FLAGS.n_auxiliary_variables):\n        session.run(sample_op)\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(\n                # The learning rate is proportional to the scale of the prior.\n                lr=float(FLAGS.learning_rate_for_sampling *\n                         np.sqrt(1. - FLAGS.auxiliary_variance_ratio)**j)),\n            loss=negative_log_likelihood,\n            metrics=metrics)\n        fit_fn(\n            model,\n            FLAGS.auxiliary_sampling_frequency,\n            initial_epoch=train_epochs)\n      ensemble_filename = os.path.join(\n          model_dir, \'ensemble_component_\' + str(i) + \'.weights\')\n      ensemble_filenames.append(ensemble_filename)\n      model.save_weights(ensemble_filename)\n\n    auxiliary_metrics = [\n        utils.ensemble_metrics(\n            x_train,\n            y_train,\n            model,\n            ll,\n            weight_files=ensemble_filenames),\n        utils.ensemble_metrics(\n            x_test,\n            y_test,\n            model,\n            ll,\n            weight_files=ensemble_filenames),\n    ]\n\n    for metrics, name in [(base_metrics, \'Base model\'),\n                          (overtrained_metrics, \'Overtrained model\'),\n                          (auxiliary_metrics, \'Auxiliary sampling\')]:\n      logging.info(name)\n      for metrics_dict, split in [(metrics[0], \'train\'),\n                                  (metrics[1], \'test\')]:\n        logging.info(split)\n        for metric_name in metrics_dict:\n          logging.info(\'%s: %s\', metric_name, metrics_dict[metric_name])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
edward2/numpy/__init__.py,0,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Edward2 probabilistic programming language with NumPy backend.""""""\n\n# Make the NumPy backend be optional. The namespace is empty if NumPy\n# is not available.\n# pylint: disable=g-import-not-at-top\ntry:\n  import numpy as np  # pylint: disable=unused-import\n  from scipy import stats\nexcept ImportError:\n  pass\nelse:\n  from edward2.numpy import generated_random_variables\n  from edward2.numpy.generated_random_variables import *  # pylint: disable=wildcard-import\n  from edward2.numpy.program_transformations import make_log_joint_fn\n  from edward2.trace import get_next_tracer\n  from edward2.trace import trace\n  from edward2.trace import traceable\n  from edward2.tracers import condition\n  from edward2.tracers import tape\n  from edward2.version import __version__\n  from edward2.version import VERSION\n\n  _allowed_symbols = [\n      ""condition"",\n      ""get_next_tracer"",\n      ""make_log_joint_fn"",\n      ""tape"",\n      ""trace"",\n      ""traceable"",\n      ""__version__"",\n      ""VERSION"",\n  ]\n  for name in dir(generated_random_variables):\n    if name in sorted(dir(stats)):\n      _allowed_symbols.append(name)\n\n  try:\n    from tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n  except ImportError:\n    __all__ = _allowed_symbols\n  else:\n    remove_undocumented(__name__, _allowed_symbols)\n# pylint: enable=g-import-not-at-top\n'"
edward2/numpy/generated_random_variables.py,0,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Automatically generated random variables.""""""\n\nfrom edward2.trace import traceable\nimport scipy.stats\n\n# Note a vanilla Edward2-like PPL in SciPy would introduce a RandomVariable\n# abstraction: it wraps SciPy frozen distributions and calls `rvs` to associate\n# the RandomVariable with a sampled value. SciPy distributions already enable\n# parameters as input to `rvs`. Therefore instead of introducing a new\n# abstraction, we just wrap `rvs`. This enables the same manipulations.\n__all__ = []\n_globals = globals()\nfor candidate_name in sorted(dir(scipy.stats)):\n  candidate = getattr(scipy.stats, candidate_name)\n  if isinstance(candidate, (scipy.stats._multivariate.multi_rv_generic,  # pylint: disable=protected-access\n                            scipy.stats.rv_continuous,\n                            scipy.stats.rv_discrete,\n                            scipy.stats.rv_histogram)):\n    candidate.rvs = traceable(candidate.rvs)\n    _globals[candidate_name] = candidate\n    __all__.append(candidate_name)\n\n_HAS_DYNAMIC_ATTRIBUTES = True\n'"
edward2/numpy/generated_random_variables_test.py,0,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for generated random variables.""""""\n\nfrom absl.testing import absltest\nimport edward2.numpy as ed\nimport scipy.stats\n\n\nclass GeneratedRandomVariablesTest(absltest.TestCase):\n\n  def testBernoulli(self):\n    self.assertEqual(ed.bernoulli.__doc__, scipy.stats.bernoulli.__doc__)\n    self.assertEqual(ed.bernoulli.logpmf(0, p=0.2),\n                     scipy.stats.bernoulli.logpmf(0, p=0.2))\n\nif __name__ == ""__main__"":\n  absltest.main()\n'"
edward2/numpy/program_transformations.py,1,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Transformations of Edward2 programs.""""""\n\nimport inspect\nimport sys\nfrom edward2.trace import trace\nimport numpy as np\n\n\ndef make_log_joint_fn(model):\n  """"""Takes Edward2 probabilistic program and returns its log joint function.\n\n  Args:\n    model: Python callable which executes the generative process of a\n      computable probability distribution using Edward2 random variables.\n\n  Returns:\n    A log-joint probability function. Its inputs are `model`\'s original inputs\n    and random variables which appear during the program execution. Its output\n    is a scalar `np.ndarray`.\n\n  #### Examples\n\n  Below we define Bayesian logistic regression as an Edward2 program, which\n  represents the model\'s generative process. We apply `make_log_joint_fn` in\n  order to alternatively represent the model in terms of its joint probability\n  function.\n\n  ```python\n  import edward2.numpy as ed\n\n  def model(X):\n    beta = ed.norm.rvs(loc=0., scale=0.1, size=X.shape[1])\n    loc = np.einsum(\'ij,j->i\', X, beta)\n    y = ed.norm.rvs(loc=loc, scale=1.)\n    return y\n\n  log_joint = ed.make_log_joint_fn(model)\n\n  X = np.random.normal(size=[3, 2])\n  beta = np.random.normal(size=[2])\n  y = np.random.normal(size=[3])\n  out = log_joint(X, beta, y)\n  ```\n\n  One can use kwargs in `log_joint` if `rvs` are given `name` kwargs.\n\n  ```python\n  def model(X):\n    beta = ed.norm.rvs(loc=0., scale=0.1, size=X.shape[1], name=""beta"")\n    loc = np.einsum(\'ij,j->i\', X, beta)\n    y = ed.norm.rvs(loc=loc, scale=1., name=""y"")\n    return y\n\n  log_joint = ed.make_log_joint_fn(model)\n  out = log_joint(X, y=y, beta=beta)\n  ```\n\n  #### Notes\n\n  For implementation, we make several requirements:\n\n  1. A random variable\'s `rvs` method has the same kwargs as scipy.stats\'\n    `logpmf`/`logpdf` up to `size` and `random_state`.\n  2. User must use explicit kwargs (no positional arguments) when specifying\n     `size` and `random_state` in the `rvs` method.\n     TODO(trandustin): Relax this requirement.\n  """"""\n  def log_joint_fn(*args, **kwargs):\n    """"""Log-probability of inputs according to a joint probability distribution.\n\n    Args:\n      *args: Positional arguments. They are the model\'s original inputs and can\n        alternatively be specified as part of `kwargs`.\n      **kwargs: Keyword arguments, where for each key-value pair `k` and `v`,\n        `v` is passed as a `value` to the random variable(s) whose keyword\n        argument `name` during construction is equal to `k`.\n\n    Returns:\n      Scalar `np.ndarray`, which represents the model\'s log-probability summed\n      over all Edward2 random variables and their dimensions.\n\n    Raises:\n      TypeError: If a random variable in the model has no specified value in\n        `**kwargs`.\n    """"""\n    log_probs = []\n    args_counter = []\n\n    def tracer(rv_call, *rv_args, **rv_kwargs):\n      """"""Overrides a random variable\'s `value` and accumulates its log-prob.""""""\n      if len(args) - len(args_counter) > 0:\n        value = args[len(args_counter)]\n        args_counter.append(0)\n      else:\n        # Set value to keyword argument indexed by `name` (an input tensor).\n        rv_name = rv_kwargs.get(""name"")\n        if rv_name is None:\n          raise KeyError(""Random variable call {} has no name in its arguments.""\n                         .format(rv_call.im_class.__name__))\n        value = kwargs.get(rv_name)\n        if value is None:\n          raise LookupError(""Keyword argument specifying value for {} is ""\n                            ""missing."".format(rv_name))\n      if sys.version_info < (3,):\n        cls = rv_call.im_class\n      else:\n        cls = rv_call.__self__.__class__\n      log_prob_fn = getattr(cls, ""logpdf"", getattr(cls, ""logpmf"", None))\n      rv_kwargs.pop(""size"", None)\n      rv_kwargs.pop(""random_state"", None)\n      rv_kwargs.pop(""name"", None)\n      log_prob = np.sum(log_prob_fn(cls(), value, *rv_args, **rv_kwargs))\n      log_probs.append(log_prob)\n      return value\n\n    args, model_args, model_kwargs = _get_function_inputs(\n        model, *args, **kwargs)\n    with trace(tracer):\n      model(*model_args, **model_kwargs)\n    log_prob = sum(log_probs)\n    return log_prob\n  return log_joint_fn\n\n\ndef _get_function_inputs(f, *args, **kwargs):\n  """"""Filters inputs to be compatible with function `f`\'s signature.\n\n  Args:\n    f: Function according to whose input signature we filter arguments.\n    *args: Keyword arguments to filter according to `f`.\n    **kwargs: Keyword arguments to filter according to `f`.\n\n  Returns:\n    New original args, args of f, kwargs of f.\n  """"""\n  if hasattr(f, ""_func""):  # functions returned by tf.make_template\n    argspec = inspect.getargspec(f._func)  # pylint: disable=protected-access\n  else:\n    argspec = inspect.getargspec(f)\n\n  fkwargs = {}\n  for k, v in kwargs.items():\n    if k in argspec.args:\n      fkwargs[k] = v\n      kwargs.pop(k)\n  num_args = len(argspec.args) - len(fkwargs)\n  fargs = args[:num_args]\n  new_args = args[num_args:]\n  return new_args, fargs, fkwargs\n'"
edward2/numpy/program_transformations_test.py,0,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for program transformations.""""""\n\nfrom absl.testing import absltest\nimport edward2.numpy as ed\nimport numpy as np\n\n\nclass ProgramTransformationsTest(absltest.TestCase):\n\n  def testMakeLogJointUnconditional(self):\n    """"""Test `make_log_joint` works on unconditional model.""""""\n    def normal_normal_model():\n      loc = ed.norm.rvs(loc=0., scale=1., name=\'loc\')\n      x = ed.norm.rvs(loc=loc, scale=0.5, size=5, name=\'x\')\n      return x\n\n    log_joint = ed.make_log_joint_fn(normal_normal_model)\n\n    x = np.random.normal(size=5)\n    loc = 0.3\n\n    value = log_joint(loc=loc, x=x)\n    true_value = np.sum(ed.norm.logpdf(loc, loc=0., scale=1.))\n    true_value += np.sum(ed.norm.logpdf(x, loc=loc, scale=0.5))\n    self.assertAlmostEqual(value, true_value)\n\n  def testMakeLogJointConditional(self):\n    """"""Test `make_log_joint` works on conditional model.""""""\n    def linear_regression(features, prior_precision):\n      beta = ed.norm.rvs(loc=0.,\n                         scale=1. / np.sqrt(prior_precision),\n                         size=features.shape[1],\n                         name=\'beta\')\n      loc = np.einsum(\'ij,j->i\', features, beta)\n      y = ed.norm.rvs(loc=loc, scale=1., name=\'y\')\n      return y\n\n    log_joint = ed.make_log_joint_fn(linear_regression)\n\n    features = np.random.normal(size=[3, 2])\n    prior_precision = 0.5\n    beta = np.random.normal(size=[2])\n    y = np.random.normal(size=[3])\n\n    true_value = np.sum(ed.norm.logpdf(\n        beta, loc=0., scale=1. / np.sqrt(prior_precision)))\n    loc = np.einsum(\'ij,j->i\', features, beta)\n    true_value += np.sum(ed.norm.logpdf(y, loc=loc, scale=1.))\n\n    # Test args as input.\n    value = log_joint(features, prior_precision, beta, y)\n    self.assertAlmostEqual(value, true_value)\n\n    # Test kwargs as input.\n    value = log_joint(features, prior_precision, y=y, beta=beta)\n    self.assertAlmostEqual(value, true_value)\n\nif __name__ == \'__main__\':\n  np.random.seed(8327)\n  absltest.main()\n'"
edward2/tensorflow/__init__.py,0,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Edward2 probabilistic programming language with TensorFlow backend.""""""\n\n# Make the TensorFlow backend be optional. The namespace is empty if\n# TensorFlow is not available.\n# pylint: disable=g-import-not-at-top\ntry:\n  import tensorflow as tf  # pylint: disable=unused-import\n  from tensorflow_probability import distributions\nexcept ImportError:\n  pass\nelse:\n  from edward2.tensorflow import generated_random_variables\n  from edward2.tensorflow import constraints\n  from edward2.tensorflow import initializers\n  from edward2.tensorflow import layers\n  from edward2.tensorflow import metrics\n  from edward2.tensorflow import regularizers\n  from edward2.tensorflow.generated_random_variables import *  # pylint: disable=wildcard-import\n  from edward2.tensorflow.generated_random_variables import make_random_variable\n  from edward2.tensorflow.program_transformations import make_log_joint_fn\n  from edward2.tensorflow.random_variable import RandomVariable\n  from edward2.tensorflow.transformed_random_variable import TransformedRandomVariable\n  from edward2.trace import get_next_tracer\n  from edward2.trace import trace\n  from edward2.trace import traceable\n  from edward2.tracers import condition\n  from edward2.tracers import tape\n  from edward2.version import __version__\n  from edward2.version import VERSION\n\n  from tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n  _allowed_symbols = [\n      ""RandomVariable"",\n      ""TransformedRandomVariable"",\n      ""condition"",\n      ""constraints"",\n      ""get_next_tracer"",\n      ""initializers"",\n      ""layers"",\n      ""make_log_joint_fn"",\n      ""make_random_variable"",\n      ""metrics"",\n      ""regularizers"",\n      ""tape"",\n      ""trace"",\n      ""traceable"",\n      ""__version__"",\n      ""VERSION"",\n  ]\n  for name in dir(generated_random_variables):\n    if name in sorted(dir(distributions)):\n      _allowed_symbols.append(name)\n\n  remove_undocumented(__name__, _allowed_symbols)\n# pylint: enable=g-import-not-at-top\n'"
edward2/tensorflow/constraints.py,16,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Constraints.\n\nOne subtlety is how Bayesian Layers uses `tf.keras.constraints`. Typically,\nKeras constraints are used with projected gradient descent, where one performs\nunconstrained optimization and then applies a projection (the constraint) after\neach gradient update. To stay in line with probabilistic literature, trainable\ninitializers, such as variational distributions for the weight initializer,\napply constraints on the `tf.Variables` themselves (i.e., a constrained\nparameterization) and do not apply projections during optimization.\n""""""\n\nimport tensorflow as tf\n\n\nclass Exp(tf.keras.constraints.Constraint):\n  """"""Exp constraint.""""""\n\n  def __init__(self, epsilon=tf.keras.backend.epsilon()):\n    self.epsilon = epsilon\n\n  def __call__(self, w):\n    return tf.exp(w) + self.epsilon\n\n  def get_config(self):\n    return {\'epsilon\': self.epsilon}\n\n\nclass Positive(tf.keras.constraints.Constraint):\n  """"""Positive constraint.""""""\n\n  def __init__(self, epsilon=tf.keras.backend.epsilon()):\n    self.epsilon = epsilon\n\n  def __call__(self, w):\n    return tf.maximum(w, self.epsilon)\n\n  def get_config(self):\n    return {\'epsilon\': self.epsilon}\n\n\nclass Softplus(tf.keras.constraints.Constraint):\n  """"""Softplus constraint.""""""\n\n  def __init__(self, epsilon=tf.keras.backend.epsilon()):\n    self.epsilon = epsilon\n\n  def __call__(self, w):\n    return tf.nn.softplus(w) + self.epsilon\n\n  def get_config(self):\n    return {\'epsilon\': self.epsilon}\n\n\n# Compatibility aliases, following tf.keras\n\n# pylint: disable=invalid-name\nexp = Exp\npositive = Positive\nsoftplus = Softplus\n# pylint: enable=invalid-name\n\n# Utility functions, following tf.keras\n\n\ndef serialize(initializer):\n  return tf.keras.utils.serialize_keras_object(initializer)\n\n\ndef deserialize(config, custom_objects=None):\n  return tf.keras.utils.deserialize_keras_object(\n      config,\n      module_objects=globals(),\n      custom_objects=custom_objects,\n      printable_module_name=\'constraints\')\n\n\ndef get(identifier, value=None):\n  """"""Getter for loading from strings; falls back to Keras as needed.""""""\n  if value is None:\n    value = identifier\n  if identifier is None:\n    return None\n  elif isinstance(identifier, dict):\n    try:\n      return deserialize(identifier)\n    except ValueError:\n      pass\n  elif isinstance(identifier, str):\n    config = {\'class_name\': str(identifier), \'config\': {}}\n    try:\n      return deserialize(config)\n    except ValueError:\n      pass\n  elif callable(identifier):\n    return identifier\n  return tf.keras.constraints.get(value)\n'"
edward2/tensorflow/constraints_test.py,4,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for Keras-style constraints.""""""\n\nfrom absl.testing import parameterized\nimport edward2 as ed\nimport tensorflow as tf\n\n\nclass ConstraintsTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.parameters(\n      {\'name\': \'exp\'},\n      {\'name\': \'positive\'},\n      {\'name\': \'softplus\'},\n  )\n  def testPositiveConstraint(self, name):\n    weight = tf.random.normal((3,))\n    constraint = ed.constraints.get(name)\n    constrained_weight = constraint(weight)\n    self.assertAllGreater(constrained_weight, 0.)\n\n  def testConstraintsGet(self):\n    self.assertIsInstance(ed.constraints.get(\'positive\'),\n                          ed.constraints.Positive)\n    self.assertIsInstance(ed.constraints.get(\'non_neg\'),\n                          tf.keras.constraints.NonNeg)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
edward2/tensorflow/generated_random_variables.py,0,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Automatically generated random variables.""""""\n\nimport functools\nimport inspect\nimport re\n\nfrom edward2.tensorflow.random_variable import RandomVariable\nfrom edward2.trace import traceable\nimport tensorflow_probability as tfp\n\n\ndef expand_docstring(**kwargs):\n  """"""Decorator to programmatically expand the docstring.\n\n  Args:\n    **kwargs: Keyword arguments to set. For each key-value pair `k` and `v`,\n      the key is found as `${k}` in the docstring and replaced with `v`.\n\n  Returns:\n    Decorated function.\n  """"""\n  def _fn_wrapped(fn):\n    """"""Original function with modified `__doc__` attribute.""""""\n    doc = inspect.cleandoc(fn.__doc__)\n    for k, v in kwargs.items():\n      # Capture each ${k} reference to replace with v.\n      # We wrap the replacement in a function so no backslash escapes\n      # are processed.\n      pattern = r""\\$\\{"" + str(k) + r""\\}""\n      doc = re.sub(pattern, lambda match: v, doc)  # pylint: disable=cell-var-from-loop\n    fn.__doc__ = doc\n    return fn\n  return _fn_wrapped\n\n\ndef make_random_variable(distribution_cls):\n  """"""Factory function to make random variable given distribution class.""""""\n  @traceable\n  @functools.wraps(distribution_cls, assigned=(""__module__"", ""__name__""))\n  @expand_docstring(cls=distribution_cls.__name__,\n                    doc=inspect.cleandoc(\n                        distribution_cls.__init__.__doc__ if\n                        distribution_cls.__init__.__doc__ is not None else """"))\n  def func(*args, **kwargs):\n    # pylint: disable=g-doc-args\n    """"""Create a random variable for ${cls}.\n\n    See ${cls} for more details.\n\n    Returns:\n      RandomVariable.\n\n    #### Original Docstring for Distribution\n\n    ${doc}\n    """"""\n    # pylint: enable=g-doc-args\n    sample_shape = kwargs.pop(""sample_shape"", ())\n    value = kwargs.pop(""value"", None)\n    return RandomVariable(distribution=distribution_cls(*args, **kwargs),\n                          sample_shape=sample_shape,\n                          value=value)\n  return func\n\n\n__all__ = [""make_random_variable""]\n_globals = globals()\nfor candidate_name in sorted(dir(tfp.distributions)):\n  candidate = getattr(tfp.distributions, candidate_name)\n  if (inspect.isclass(candidate) and\n      candidate != tfp.distributions.Distribution and\n      issubclass(candidate, tfp.distributions.Distribution)):\n\n    _globals[candidate_name] = make_random_variable(candidate)\n    __all__.append(candidate_name)\n\n_HAS_DYNAMIC_ATTRIBUTES = True\n'"
edward2/tensorflow/generated_random_variables_test.py,3,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for generated random variables.""""""\n\nimport inspect\nfrom absl.testing import parameterized\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n\nclass GeneratedRandomVariablesTest(parameterized.TestCase, tf.test.TestCase):\n\n  def testBernoulliDoc(self):\n    self.assertGreater(len(ed.Bernoulli.__doc__), 0)\n    self.assertIn(\n        inspect.cleandoc(tfp.distributions.Bernoulli.__init__.__doc__),\n        ed.Bernoulli.__doc__)\n    self.assertEqual(ed.Bernoulli.__name__, ""Bernoulli"")\n\n  @parameterized.named_parameters(\n      {""testcase_name"": ""1d_rv_1d_event"", ""logits"": np.zeros(1), ""n"": [1]},\n      {""testcase_name"": ""1d_rv_5d_event"", ""logits"": np.zeros(1), ""n"": [5]},\n      {""testcase_name"": ""5d_rv_1d_event"", ""logits"": np.zeros(5), ""n"": [1]},\n      {""testcase_name"": ""5d_rv_5d_event"", ""logits"": np.zeros(5), ""n"": [5]},\n  )\n  def testBernoulliLogProb(self, logits, n):\n    rv = ed.Bernoulli(logits)\n    dist = tfp.distributions.Bernoulli(logits)\n    x = rv.distribution.sample(n)\n    self.assertAllEqual(rv.distribution.log_prob(x), dist.log_prob(x))\n\n  @parameterized.named_parameters(\n      {""testcase_name"": ""0d_rv_0d_sample"",\n       ""logits"": 0.,\n       ""n"": 1},\n      {""testcase_name"": ""0d_rv_1d_sample"",\n       ""logits"": 0.,\n       ""n"": [1]},\n      {""testcase_name"": ""1d_rv_1d_sample"",\n       ""logits"": np.array([0.]),\n       ""n"": [1]},\n      {""testcase_name"": ""1d_rv_5d_sample"",\n       ""logits"": np.array([0.]),\n       ""n"": [5]},\n      {""testcase_name"": ""2d_rv_1d_sample"",\n       ""logits"": np.array([-0.2, 0.8]),\n       ""n"": [1]},\n      {""testcase_name"": ""2d_rv_5d_sample"",\n       ""logits"": np.array([-0.2, 0.8]),\n       ""n"": [5]},\n  )\n  def testBernoulliSample(self, logits, n):\n    rv = ed.Bernoulli(logits)\n    dist = tfp.distributions.Bernoulli(logits)\n    self.assertEqual(rv.distribution.sample(n).shape, dist.sample(n).shape)\n\n  @parameterized.named_parameters(\n      {""testcase_name"": ""0d_bernoulli"",\n       ""rv_cls"": ed.Bernoulli,\n       ""sample_shape"": [],\n       ""batch_shape"": [],\n       ""event_shape"": [],\n       ""probs"": 0.5},\n      {""testcase_name"": ""2d_bernoulli"",\n       ""rv_cls"": ed.Bernoulli,\n       ""sample_shape"": [],\n       ""batch_shape"": [2, 3],\n       ""event_shape"": [],\n       ""logits"": np.zeros([2, 3])},\n      {""testcase_name"": ""2x0d_bernoulli"",\n       ""rv_cls"": ed.Bernoulli,\n       ""sample_shape"": [2],\n       ""batch_shape"": [],\n       ""event_shape"": [],\n       ""probs"": 0.5},\n      {""testcase_name"": ""2x1d_bernoulli"",\n       ""rv_cls"": ed.Bernoulli,\n       ""sample_shape"": [2, 1],\n       ""batch_shape"": [],\n       ""event_shape"": [],\n       ""probs"": 0.5},\n      {""testcase_name"": ""3d_dirichlet"",\n       ""rv_cls"": ed.Dirichlet,\n       ""sample_shape"": [],\n       ""batch_shape"": [],\n       ""event_shape"": [3],\n       ""concentration"": np.ones(3)},\n      {""testcase_name"": ""2x3d_dirichlet"",\n       ""rv_cls"": ed.Dirichlet,\n       ""sample_shape"": [],\n       ""batch_shape"": [2],\n       ""event_shape"": [3],\n       ""concentration"": np.ones([2, 3])},\n      {""testcase_name"": ""1x3d_dirichlet"",\n       ""rv_cls"": ed.Dirichlet,\n       ""sample_shape"": [1],\n       ""batch_shape"": [],\n       ""event_shape"": [3],\n       ""concentration"": np.ones(3)},\n      {""testcase_name"": ""2x1x3d_dirichlet"",\n       ""rv_cls"": ed.Dirichlet,\n       ""sample_shape"": [2, 1],\n       ""batch_shape"": [],\n       ""event_shape"": [3],\n       ""concentration"": np.ones(3)},\n  )\n  def testShape(self, rv_cls, sample_shape, batch_shape, event_shape, **kwargs):\n    rv = rv_cls(sample_shape=sample_shape, **kwargs)\n    self.assertEqual(rv.shape, sample_shape + batch_shape + event_shape)\n    self.assertEqual(rv.sample_shape, sample_shape)\n    self.assertEqual(rv.distribution.batch_shape, batch_shape)\n    self.assertEqual(rv.distribution.event_shape, event_shape)\n\n  @parameterized.parameters(\n      {""cls"": ed.Normal, ""value"": 2, ""loc"": 0.5, ""scale"": 1.0},\n      {""cls"": ed.Normal, ""value"": [2], ""loc"": [0.5], ""scale"": [1.0]},\n      {""cls"": ed.Poisson, ""value"": 2, ""rate"": 0.5},\n  )\n  def testValueShapeAndDtype(self, cls, value, **kwargs):\n    rv = cls(value=value, **kwargs)\n    value_shape = rv.value.shape\n    expected_shape = rv.sample_shape.concatenate(\n        rv.distribution.batch_shape).concatenate(rv.distribution.event_shape)\n    self.assertEqual(value_shape, expected_shape)\n    self.assertEqual(rv.distribution.dtype, rv.value.dtype)\n\n  @parameterized.parameters(\n      {""cls"": ed.Normal, ""value"": 2, ""loc"": [0.5, 0.5], ""scale"": 1.0},\n      {""cls"": ed.Normal, ""value"": 2, ""loc"": [0.5], ""scale"": [1.0]},\n      {""cls"": ed.Normal, ""value"": np.zeros([10, 3]), ""loc"": [0.5, 0.5],\n       ""scale"": [1.0, 1.0]},\n  )\n  def testValueMismatchRaises(self, cls, value, **kwargs):\n    with self.assertRaises(ValueError):\n      cls(value=tf.convert_to_tensor(value), **kwargs)\n\n  def testMakeRandomVariable(self):\n    """"""Tests that manual wrapping is the same as the built-in solution.""""""\n    custom_normal = ed.make_random_variable(tfp.distributions.Normal)\n\n    def model_builtin():\n      return ed.Normal(1., 0.1, name=""x"")\n\n    def model_wrapped():\n      return custom_normal(1., 0.1, name=""x"")\n\n    log_joint_builtin = ed.make_log_joint_fn(model_builtin)\n    log_joint_wrapped = ed.make_log_joint_fn(model_wrapped)\n    self.assertEqual(log_joint_builtin(x=7.), log_joint_wrapped(x=7.))\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
edward2/tensorflow/initializers.py,38,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Initializers.\n\nThis module extends `tf.keras.initializers` with the notion of ""trainable\ninitializers"", where initializers to weights and biases in `tf.keras.layers` may\nthemselves carry parameters. For example, consider a weight initializer which\nreturns a variational distribution: this is reified as an `ed.RandomVariable`\nparameterized by `tf.Variables`.\n\nOne subtlety is how `tf.keras.constraints` are used on the parameters of\ntrainable initializers. Typically, Keras constraints are used with projected\ngradient descent, where one performs unconstrained optimization and then applies\na projection (the constraint) after each gradient update. To stay in line with\nprobabilistic literature, trainable initializers apply constraints on the\n`tf.Variables` themselves (i.e., a constrained parameterization) and do not\napply projections during optimization.\n""""""\n\nimport math\n\nfrom edward2.tensorflow import constraints\nfrom edward2.tensorflow import generated_random_variables\nfrom edward2.tensorflow import regularizers\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n\ndef get_condconv_initializer(initializer, num_experts, expert_shape):\n  """"""Wraps the initializer to correctly initialize CondConv variables.\n\n  CondConv initializes biases and kernels in a num_experts x num_params\n  matrix for efficient computation. This wrapper ensures that each expert\n  is correctly initialized with the given initializer before being flattened\n  into the correctly shaped CondConv variable.\n\n  Arguments:\n    initializer: The initializer to apply for each individual expert.\n    num_experts: The number of experts to be initialized.\n    expert_shape: The original shape of each individual expert.\n\n  Returns:\n    The initializer for the num_experts x num_params CondConv variable.\n  """"""\n  def condconv_initializer(expected_shape, dtype=None, partition=None):\n    """"""CondConv initializer function.""""""\n    num_params = np.prod(expert_shape)\n    if (len(expected_shape) != 2 or expected_shape[0] != num_experts or\n        expected_shape[1] != num_params):\n      raise (ValueError(\n          \'CondConv variables must have shape [num_experts, num_params]\'))\n    flattened_kernels = []\n    for _ in range(num_experts):\n      kernel = initializer(expert_shape, dtype, partition)\n      flattened_kernels.append(tf.reshape(kernel, [-1]))\n    return tf.stack(flattened_kernels)\n\n  return condconv_initializer\n\n\n# From `tensorflow/python/ops/init_ops.py`\ndef _compute_fans(shape):\n  """"""Computes the number of input and output units for a weight shape.\n\n  Args:\n    shape: Integer shape tuple or TF tensor shape.\n\n  Returns:\n    A tuple of scalars (fan_in, fan_out).\n  """"""\n  if len(shape) < 1:  # Just to avoid errors for constants.\n    fan_in = fan_out = 1\n  elif len(shape) == 1:\n    fan_in = fan_out = shape[0]\n  elif len(shape) == 2:\n    fan_in = shape[0]\n    fan_out = shape[1]\n  else:\n    # Assuming convolution kernels (2D, 3D, or more).\n    # kernel shape: (..., input_depth, depth)\n    receptive_field_size = 1.\n    for dim in shape[:-2]:\n      receptive_field_size *= dim\n    fan_in = shape[-2] * receptive_field_size\n    fan_out = shape[-1] * receptive_field_size\n  return fan_in, fan_out\n\n\nclass ScaledNormalStdDev(tf.keras.initializers.VarianceScaling):\n  """"""Initializer capable of adapting its scale to the shape of weights tensors.\n\n  This initializes the standard deviation parameter of a Trainable Normal\n  distribution with a scale based on the shape of the weights tensor.\n  Additionally, A small amount of noise will be added to break weigh symmetry.\n\n  With `distribution=""truncated_normal"" or ""untruncated_normal""`, the standard\n  deviation (after truncation, if used) is `stddev = sqrt(scale / n)`, where n\n  is:\n    - number of input units in the weight tensor, if mode = ""fan_in""\n    - number of output units, if mode = ""fan_out""\n    - average of the numbers of input and output units, if mode = ""fan_avg""\n  """"""\n\n  def __init__(self,\n               scale=1.0,\n               mode=\'fan_in\',\n               distribution=\'untruncated_normal\',\n               seed=None):\n    """"""Constructs the initializer.\n\n    Args:\n      scale: Scaling factor (positive float).\n      mode: One of ""fan_in"", ""fan_out"", ""fan_avg"".\n      distribution: Random distribution to use. One of ""truncated_normal"", or\n        ""untruncated_normal"".\n      seed: A Python integer. Used to create random seeds. See\n        `tf.set_random_seed`\n        for behavior.\n\n    Raises:\n      ValueError: In case of an invalid value for the ""scale"", mode"" or\n        ""distribution"" arguments.\n    """"""\n    distribution = distribution.lower()\n    if distribution not in {\'truncated_normal\', \'untruncated_normal\'}:\n      raise ValueError(\'Invalid `distribution` argument:\', distribution)\n    super(ScaledNormalStdDev, self).__init__(scale=scale, mode=mode,\n                                             distribution=distribution,\n                                             seed=seed)\n\n  def __call__(self, shape, dtype=None):\n    if dtype is None:\n      dtype = self.dtype\n    scale = self.scale\n    scale_shape = shape\n    fan_in, fan_out = _compute_fans(scale_shape)\n    if self.mode == \'fan_in\':\n      scale /= max(1., fan_in)\n    elif self.mode == \'fan_out\':\n      scale /= max(1., fan_out)\n    else:\n      scale /= max(1., (fan_in + fan_out) / 2.)\n    if self.distribution == \'truncated_normal\':\n      # constant from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n      stddev = math.sqrt(scale) / .87962566103423978\n    else:  # self.distribution == \'untruncated_normal\':\n      stddev = math.sqrt(scale)\n    return tf.random.truncated_normal(shape, mean=stddev, stddev=stddev*0.1,\n                                      dtype=dtype, seed=self.seed)\n\n\nclass TrainableDeterministic(tf.keras.layers.Layer):\n  """"""Deterministic point-wise initializer with trainable location.""""""\n\n  def __init__(self,\n               loc_initializer=\'glorot_uniform\',\n               loc_regularizer=None,\n               loc_constraint=None,\n               seed=None,\n               **kwargs):\n    """"""Constructs the initializer.""""""\n    super(TrainableDeterministic, self).__init__(**kwargs)\n    self.loc_initializer = get(loc_initializer)\n    self.loc_regularizer = regularizers.get(loc_regularizer)\n    self.loc_constraint = constraints.get(loc_constraint)\n    self.seed = seed\n\n  def build(self, shape, dtype=None):\n    if dtype is None:\n      dtype = self.dtype\n\n    self.loc = self.add_weight(\n        \'loc\',\n        shape=shape,\n        initializer=self.loc_initializer,\n        regularizer=self.loc_regularizer,\n        constraint=None,\n        dtype=dtype,\n        trainable=True)\n    self.built = True\n\n  def __call__(self, shape, dtype=None):\n    if not self.built:\n      self.build(shape, dtype)\n    loc = self.loc\n    if self.loc_constraint:\n      loc = self.loc_constraint(loc)\n    return generated_random_variables.Independent(\n        generated_random_variables.Deterministic(loc=loc).distribution,\n        reinterpreted_batch_ndims=len(shape))\n\n  def get_config(self):\n    return {\n        \'loc_initializer\':\n            serialize(self.loc_initializer),\n        \'loc_regularizer\':\n            regularizers.serialize(self.loc_regularizer),\n        \'loc_constraint\':\n            constraints.serialize(self.loc_constraint),\n        \'seed\': self.seed,\n    }\n\n\nclass TrainableHalfCauchy(tf.keras.layers.Layer):\n  """"""Half-Cauchy distribution initializer with trainable parameters.""""""\n\n  def __init__(self,\n               loc_initializer=tf.keras.initializers.TruncatedNormal(\n                   stddev=1e-5),\n               scale_initializer=tf.keras.initializers.TruncatedNormal(\n                   mean=-3., stddev=0.1),\n               loc_regularizer=None,\n               scale_regularizer=None,\n               loc_constraint=None,\n               scale_constraint=\'softplus\',\n               seed=None,\n               **kwargs):\n    """"""Constructs the initializer.""""""\n    super(TrainableHalfCauchy, self).__init__(**kwargs)\n    self.loc_initializer = get(loc_initializer)\n    self.scale_initializer = get(scale_initializer)\n    self.loc_regularizer = regularizers.get(loc_regularizer)\n    self.scale_regularizer = regularizers.get(scale_regularizer)\n    self.loc_constraint = constraints.get(loc_constraint)\n    self.scale_constraint = constraints.get(scale_constraint)\n    self.seed = seed\n\n  def build(self, shape, dtype=None):\n    if dtype is None:\n      dtype = self.dtype\n\n    self.loc = self.add_weight(\n        \'loc\',\n        shape=shape,\n        initializer=self.loc_initializer,\n        regularizer=self.loc_regularizer,\n        constraint=None,\n        dtype=dtype,\n        trainable=True)\n    self.scale = self.add_weight(\n        \'scale\',\n        shape=shape,\n        initializer=self.scale_initializer,\n        regularizer=self.scale_regularizer,\n        constraint=None,\n        dtype=dtype,\n        trainable=True)\n    self.built = True\n\n  def __call__(self, shape, dtype=None):\n    if not self.built:\n      self.build(shape, dtype)\n    loc = self.loc\n    if self.loc_constraint:\n      loc = self.loc_constraint(loc)\n    scale = self.scale\n    if self.scale_constraint:\n      scale = self.scale_constraint(scale)\n    return generated_random_variables.Independent(\n        generated_random_variables.HalfCauchy(loc=loc,\n                                              scale=scale).distribution,\n        reinterpreted_batch_ndims=len(shape))\n\n  def get_config(self):\n    return {\n        \'loc_initializer\':\n            serialize(self.loc_initializer),\n        \'scale_initializer\':\n            serialize(self.scale_initializer),\n        \'loc_regularizer\':\n            regularizers.serialize(self.loc_regularizer),\n        \'scale_regularizer\':\n            regularizers.serialize(self.scale_regularizer),\n        \'loc_constraint\':\n            constraints.serialize(self.loc_constraint),\n        \'scale_constraint\':\n            constraints.serialize(self.scale_constraint),\n        \'seed\': self.seed,\n    }\n\n\nclass TrainableCauchy(tf.keras.layers.Layer):\n  """"""Cauchy distribution initializer with trainable parameters.""""""\n\n  def __init__(\n      self,\n      loc_initializer=tf.keras.initializers.TruncatedNormal(stddev=1e-5),\n      scale_initializer=tf.keras.initializers.TruncatedNormal(\n          mean=-3., stddev=0.1),\n      loc_regularizer=None,\n      scale_regularizer=None,\n      loc_constraint=None,\n      scale_constraint=\'softplus\',\n      seed=None,\n      **kwargs):\n    """"""Constructs the initializer.""""""\n    super(TrainableCauchy, self).__init__(**kwargs)\n    self.loc_initializer = get(loc_initializer)\n    self.scale_initializer = get(scale_initializer)\n    self.loc_regularizer = regularizers.get(loc_regularizer)\n    self.scale_regularizer = regularizers.get(scale_regularizer)\n    self.loc_constraint = constraints.get(loc_constraint)\n    self.scale_constraint = constraints.get(scale_constraint)\n    self.seed = seed\n\n  def build(self, shape, dtype=None):\n    if dtype is None:\n      dtype = self.dtype\n\n    self.loc = self.add_weight(\n        \'loc\',\n        shape=shape,\n        initializer=self.loc_initializer,\n        regularizer=self.loc_regularizer,\n        constraint=None,\n        dtype=dtype,\n        trainable=True)\n    self.scale = self.add_weight(\n        \'scale\',\n        shape=shape,\n        initializer=self.scale_initializer,\n        regularizer=self.scale_regularizer,\n        constraint=None,\n        dtype=dtype,\n        trainable=True)\n    self.built = True\n\n  def __call__(self, shape, dtype=None):\n    if not self.built:\n      self.build(shape, dtype)\n    loc = self.loc\n    if self.loc_constraint:\n      loc = self.loc_constraint(loc)\n    scale = self.scale\n    if self.scale_constraint:\n      scale = self.scale_constraint(scale)\n    return generated_random_variables.Independent(\n        generated_random_variables.Cauchy(loc=loc, scale=scale).distribution,\n        reinterpreted_batch_ndims=len(shape))\n\n  def get_config(self):\n    return {\n        \'loc_initializer\': serialize(self.loc_initializer),\n        \'scale_initializer\': serialize(self.scale_initializer),\n        \'loc_regularizer\': regularizers.serialize(self.loc_regularizer),\n        \'scale_regularizer\': regularizers.serialize(self.scale_regularizer),\n        \'loc_constraint\': constraints.serialize(self.loc_constraint),\n        \'scale_constraint\': constraints.serialize(self.scale_constraint),\n        \'seed\': self.seed,\n    }\n\n\nclass TrainableLogNormal(tf.keras.layers.Layer):\n  """"""Random log normal op as an initializer with trainable loc and scale.""""""\n\n  def __init__(self,\n               loc_initializer=tf.keras.initializers.TruncatedNormal(\n                   stddev=1e-5),\n               scale_initializer=tf.keras.initializers.TruncatedNormal(\n                   mean=-3., stddev=0.1),\n               loc_regularizer=None,\n               scale_regularizer=None,\n               loc_constraint=None,\n               scale_constraint=\'softplus\',\n               seed=None,\n               **kwargs):\n    """"""Constructs the initializer.""""""\n    super(TrainableLogNormal, self).__init__(**kwargs)\n    self.loc_initializer = get(loc_initializer)\n    self.scale_initializer = get(scale_initializer)\n    self.loc_regularizer = regularizers.get(loc_regularizer)\n    self.scale_regularizer = regularizers.get(scale_regularizer)\n    self.loc_constraint = constraints.get(loc_constraint)\n    self.scale_constraint = constraints.get(scale_constraint)\n    self.seed = seed\n\n  def build(self, shape, dtype=None):\n    if dtype is None:\n      dtype = self.dtype\n\n    self.loc = self.add_weight(\n        \'loc\',\n        shape=shape,\n        initializer=self.loc_initializer,\n        regularizer=self.loc_regularizer,\n        constraint=None,\n        dtype=dtype,\n        trainable=True)\n    self.scale = self.add_weight(\n        \'scale\',\n        shape=shape,\n        initializer=self.scale_initializer,\n        regularizer=self.scale_regularizer,\n        constraint=None,\n        dtype=dtype,\n        trainable=True)\n    self.built = True\n\n  def __call__(self, shape, dtype=None):\n    if not self.built:\n      self.build(shape, dtype)\n    loc = self.loc\n    if self.loc_constraint:\n      loc = self.loc_constraint(loc)\n    scale = self.scale\n    if self.scale_constraint:\n      scale = self.scale_constraint(scale)\n    return generated_random_variables.Independent(\n        generated_random_variables.LogNormal(loc=loc, scale=scale).distribution,\n        reinterpreted_batch_ndims=len(shape))\n\n  def get_config(self):\n    return {\n        \'loc_initializer\':\n            serialize(self.loc_initializer),\n        \'scale_initializer\':\n            serialize(self.scale_initializer),\n        \'loc_regularizer\':\n            regularizers.serialize(self.loc_regularizer),\n        \'scale_regularizer\':\n            regularizers.serialize(self.scale_regularizer),\n        \'loc_constraint\':\n            constraints.serialize(self.loc_constraint),\n        \'scale_constraint\':\n            constraints.serialize(self.scale_constraint),\n        \'seed\': self.seed,\n    }\n\n\nclass TrainableNormal(tf.keras.layers.Layer):\n  """"""Random normal op as an initializer with trainable mean and stddev.""""""\n\n  def __init__(self,\n               mean_initializer=tf.keras.initializers.TruncatedNormal(\n                   stddev=1e-5),\n               stddev_initializer=tf.keras.initializers.TruncatedNormal(\n                   mean=-3., stddev=0.1),\n               mean_regularizer=None,\n               stddev_regularizer=None,\n               mean_constraint=None,\n               stddev_constraint=\'softplus\',\n               seed=None,\n               **kwargs):\n    """"""Constructs the initializer.""""""\n    super(TrainableNormal, self).__init__(**kwargs)\n    self.mean_initializer = get(mean_initializer)\n    self.stddev_initializer = get(stddev_initializer)\n    self.mean_regularizer = regularizers.get(mean_regularizer)\n    self.stddev_regularizer = regularizers.get(stddev_regularizer)\n    self.mean_constraint = constraints.get(mean_constraint)\n    self.stddev_constraint = constraints.get(stddev_constraint)\n    self.seed = seed\n\n  def build(self, shape, dtype=None):\n    if dtype is None:\n      dtype = self.dtype\n\n    self.mean = self.add_weight(\n        \'mean\',\n        shape=shape,\n        initializer=self.mean_initializer,\n        regularizer=self.mean_regularizer,\n        constraint=None,\n        dtype=dtype,\n        trainable=True)\n    self.stddev = self.add_weight(\n        \'stddev\',\n        shape=shape,\n        initializer=self.stddev_initializer,\n        regularizer=self.stddev_regularizer,\n        constraint=None,\n        dtype=dtype,\n        trainable=True)\n    self.built = True\n\n  def __call__(self, shape, dtype=None):\n    if not self.built:\n      self.build(shape, dtype)\n    mean = self.mean\n    if self.mean_constraint:\n      mean = self.mean_constraint(mean)\n    stddev = self.stddev\n    if self.stddev_constraint:\n      stddev = self.stddev_constraint(stddev)\n    return generated_random_variables.Independent(\n        generated_random_variables.Normal(loc=mean, scale=stddev).distribution,\n        reinterpreted_batch_ndims=len(shape))\n\n  def get_config(self):\n    return {\n        \'mean_initializer\':\n            serialize(self.mean_initializer),\n        \'stddev_initializer\':\n            serialize(self.stddev_initializer),\n        \'mean_regularizer\':\n            regularizers.serialize(self.mean_regularizer),\n        \'stddev_regularizer\':\n            regularizers.serialize(self.stddev_regularizer),\n        \'mean_constraint\':\n            constraints.serialize(self.mean_constraint),\n        \'stddev_constraint\':\n            constraints.serialize(self.stddev_constraint),\n        \'seed\': self.seed,\n    }\n\n\nclass TrainableHeNormal(TrainableNormal):\n  """"""Trainable normal initialized per He et al. 2015, given a ReLU nonlinearity.\n\n  The distribution is initialized to a Normal scaled by `sqrt(2 / fan_in)`,\n  where `fan_in` is the number of input units. A ReLU nonlinearity is assumed\n  for this initialization scheme.\n\n  References:\n    He K, Zhang X, Ren S, Sun J. Delving deep into rectifiers: Surpassing\n    human-level performance on imagenet classification. In Proceedings of the\n    IEEE international conference on computer vision 2015 (pp. 1026-1034).\n    https://arxiv.org/abs/1502.01852\n  """"""\n\n  def __init__(self, seed=None, **kwargs):\n    super(TrainableHeNormal, self).__init__(\n        mean_initializer=tf.keras.initializers.he_normal(seed),\n        seed=seed,\n        **kwargs)\n\n  def get_config(self):\n    return {\n        \'seed\': self.seed,\n    }\n\n\nclass TrainableGlorotNormal(TrainableNormal):\n  """"""Trainable normal initialized per Glorot and Bengio, 2010.\n\n  The distribution is initialized to a Normal scaled by `sqrt(2 / fan_in +\n  fan_out)`, where `fan_in` is the number of input units and `fan_out` is the\n  number of output units.\n\n  References:\n    Glorot X, Bengio Y. Understanding the difficulty of training deep\n    feedforward neural networks. In Proceedings of the thirteenth international\n    conference on artificial intelligence and statistics 2010 Mar 31 (pp.\n    249-256). http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n  """"""\n\n  def __init__(self, seed=None, **kwargs):\n    super(TrainableGlorotNormal, self).__init__(\n        mean_initializer=tf.keras.initializers.GlorotNormal(seed),\n        seed=seed,\n        **kwargs)\n\n  def get_config(self):\n    return {\n        \'seed\': self.seed,\n    }\n\n\nclass TrainableNormalSharedStddev(TrainableNormal):\n  """"""Random normal op as an initializer with trainable mean and stddev.\n\n  The stddev parameter is a scalar shared across the weight. This enables, e.g.,\n  learnable dropout rates per-layer during Gaussian variational dropout rather\n  than learnable dropout rates per-weight.\n  """"""\n\n  def build(self, shape, dtype=None):\n    if dtype is None:\n      dtype = self.dtype\n\n    self.mean = self.add_weight(\n        \'mean\',\n        shape=shape,\n        initializer=self.mean_initializer,\n        regularizer=self.mean_regularizer,\n        constraint=None,\n        dtype=dtype,\n        trainable=True)\n    self.stddev = self.add_weight(\n        \'stddev\',\n        shape=(),\n        initializer=self.stddev_initializer,\n        regularizer=self.stddev_regularizer,\n        constraint=None,\n        dtype=dtype,\n        trainable=True)\n    self.built = True\n\n\nclass TrainableNormalFixedStddev(tf.keras.layers.Layer):\n  """"""Random normal op as an initializer with trainable mean and fixed stddev.""""""\n\n  def __init__(self,\n               stddev=1.,\n               mean_initializer=tf.keras.initializers.TruncatedNormal(\n                   stddev=1e-5),\n               mean_regularizer=None,\n               mean_constraint=None,\n               seed=None,\n               **kwargs):\n    """"""Constructs the initializer.""""""\n    super(TrainableNormalFixedStddev, self).__init__(**kwargs)\n    self.stddev = stddev\n    self.mean_initializer = get(mean_initializer)\n    self.mean_regularizer = regularizers.get(mean_regularizer)\n    self.mean_constraint = constraints.get(mean_constraint)\n    self.seed = seed\n\n  def build(self, shape, dtype=None):\n    if dtype is None:\n      dtype = self.dtype\n    self.mean = self.add_weight(\n        \'mean\',\n        shape=shape,\n        initializer=self.mean_initializer,\n        regularizer=self.mean_regularizer,\n        constraint=None,\n        dtype=dtype,\n        trainable=True)\n    self.built = True\n\n  def __call__(self, shape, dtype=None):\n    if not self.built:\n      self.build(shape, dtype)\n    mean = self.mean\n    if self.mean_constraint:\n      mean = self.mean_constraint(mean)\n    return generated_random_variables.Independent(\n        generated_random_variables.Normal(loc=mean,\n                                          scale=self.stddev).distribution,\n        reinterpreted_batch_ndims=len(shape))\n\n  def get_config(self):\n    return {\n        \'stddev\': self.stddev,\n        \'mean_initializer\': serialize(self.mean_initializer),\n        \'mean_regularizer\': regularizers.serialize(self.mean_regularizer),\n        \'mean_constraint\': constraints.serialize(self.mean_constraint),\n        \'seed\': self.seed,\n    }\n\n\nclass RandomSign(tf.keras.initializers.Initializer):\n  """"""Initializer that generates tensors initialized to +/- 1.\n\n  Attributes:\n    probs: probability of +1.\n    dtype: tensorflow dtype.\n    seed: A Python integer. Used to create random seeds. See\n      `tf.set_random_seed`\n  """"""\n\n  def __init__(self, probs=1.0, seed=None, dtype=tf.float32):\n    self.probs = probs\n    self.seed = seed\n    self.dtype = dtype\n\n  def __call__(self, shape, dtype=None, partition_info=None):\n    if dtype is None:\n      dtype = self.dtype\n    bernoulli = tfp.distributions.Bernoulli(probs=self.probs,\n                                            dtype=dtype)\n    return 2. * bernoulli.sample(shape, self.seed) - 1.\n\n  def get_config(self):\n    return {\n        \'dtype\': self.dtype.name,\n        \'seed\': self.seed,\n        \'probs\': self.probs\n    }\n\n\nclass TrainableMixtureOfDeltas(tf.keras.layers.Layer):\n  """"""Mixture of deltas as an initializer with trainable locations.""""""\n\n  def __init__(self,\n               num_components=5,\n               loc_initializer=tf.keras.initializers.he_normal(),\n               loc_regularizer=None,\n               loc_constraint=None,\n               seed=None,\n               **kwargs):\n    """"""Constructs the initializer.""""""\n    super(TrainableMixtureOfDeltas, self).__init__(**kwargs)\n    self.num_components = num_components\n    self.loc_initializer = get(loc_initializer)\n    self.loc_regularizer = regularizers.get(loc_regularizer)\n    self.loc_constraint = constraints.get(loc_constraint)\n    self.seed = seed\n\n  def build(self, shape, dtype=None):\n    if dtype is None:\n      dtype = self.dtype\n\n    self.loc = self.add_weight(\n        \'loc\',\n        shape=list(shape) + [self.num_components],\n        initializer=self.loc_initializer,\n        regularizer=self.loc_regularizer,\n        constraint=None,\n        dtype=dtype,\n        trainable=True)\n    self.built = True\n\n  def __call__(self, shape, dtype=None):\n    if not self.built:\n      self.build(shape, dtype)\n    loc = self.loc\n    if self.loc_constraint:\n      loc = self.loc_constraint(loc)\n    return generated_random_variables.Independent(\n        generated_random_variables.MixtureSameFamily(\n            mixture_distribution=generated_random_variables.Categorical(\n                probs=tf.broadcast_to(\n                    [[1/self.num_components]*self.num_components],\n                    list(shape) + [self.num_components])).distribution,\n            components_distribution=generated_random_variables.Deterministic(\n                loc=loc).distribution\n        ).distribution,\n        reinterpreted_batch_ndims=len(shape))\n\n  def get_config(self):\n    return {\n        \'num_components\': self.num_components,\n        \'loc_initializer\':\n            serialize(self.loc_initializer),\n        \'loc_regularizer\':\n            regularizers.serialize(self.loc_regularizer),\n        \'loc_constraint\':\n            constraints.serialize(self.loc_constraint),\n        \'seed\': self.seed,\n    }\n\n\n# Compatibility aliases, following tf.keras\n\n# pylint: disable=invalid-name\nscaled_normal_std_dev = ScaledNormalStdDev\ntrainable_deterministic = TrainableDeterministic\ntrainable_half_cauchy = TrainableHalfCauchy\ntrainable_cauchy = TrainableCauchy\ntrainable_normal = TrainableNormal\ntrainable_he_normal = TrainableHeNormal\ntrainable_glorot_normal = TrainableGlorotNormal\ntrainable_log_normal = TrainableLogNormal\ntrainable_normal_shared_stddev = TrainableNormalSharedStddev\ntrainable_normal_fixed_stddev = TrainableNormalFixedStddev\ntrainable_mixture_of_deltas = TrainableMixtureOfDeltas\nrandom_sign = RandomSign\n# pylint: enable=invalid-name\n\n# Utility functions, following tf.keras\n\n\ndef serialize(initializer):\n  return tf.keras.utils.serialize_keras_object(initializer)\n\n\ndef deserialize(config, custom_objects=None):\n  return tf.keras.utils.deserialize_keras_object(\n      config,\n      module_objects=globals(),\n      custom_objects=custom_objects,\n      printable_module_name=\'initializers\')\n\n\ndef get(identifier, value=None):\n  """"""Getter for loading from strings; falls back to Keras as needed.""""""\n  if value is None:\n    value = identifier\n  if identifier is None:\n    return None\n  elif isinstance(identifier, dict):\n    try:\n      return deserialize(identifier)\n    except ValueError:\n      pass\n  elif isinstance(identifier, str):\n    config = {\'class_name\': str(identifier), \'config\': {}}\n    try:\n      return deserialize(config)\n    except ValueError:\n      pass\n  elif callable(identifier):\n    return identifier\n  return tf.keras.initializers.get(value)\n'"
edward2/tensorflow/initializers_test.py,20,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for Keras-style initializers.""""""\n\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass InitializersTest(tf.test.TestCase):\n\n  def testTrainableDeterministic(self):\n    tf.random.set_seed(345689)\n    shape = (100,)\n    initializer = ed.initializers.get(\'trainable_deterministic\')\n    rv = initializer(shape)\n    # Get distribution of rv -> get distribution of Independent.\n    loc = rv.distribution.distribution.loc\n    atol = np.sqrt(6/sum(shape)) + 1e-8\n    self.assertAllClose(tf.convert_to_tensor(loc),\n                        np.zeros(shape),\n                        atol=atol)\n\n    self.assertEqual(rv.shape, shape)\n\n  def testTrainableHalfCauchy(self):\n    tf.random.set_seed(2832)\n    shape = (3,)\n    initializer = ed.initializers.get(\'trainable_half_cauchy\')\n    half_cauchy = initializer(shape)\n    # Get distribution of rv -> get distribution of Independent.\n    loc = half_cauchy.distribution.distribution.loc\n    scale = half_cauchy.distribution.distribution.scale\n    self.assertAllClose(tf.convert_to_tensor(loc), np.zeros(shape), atol=1e-4)\n    target_scale = np.log(1. + np.exp(-3.))\n    self.assertAllClose(tf.convert_to_tensor(scale),\n                        target_scale * np.ones(shape),\n                        atol=5e-2)\n\n    self.assertAllEqual(half_cauchy.shape, shape)\n    self.assertAllGreaterEqual(half_cauchy.value, 0.)\n\n  def testTrainableLogNormal(self):\n    tf.random.set_seed(345689)\n    shape = (100,)\n    initializer = ed.initializers.get(\'trainable_log_normal\')\n    log_normal = initializer(shape)\n    # Get distribution of rv -> get distribution of Independent.\n    loc = log_normal.distribution.distribution.loc\n    scale = log_normal.distribution.distribution.scale\n    self.assertAllClose(tf.convert_to_tensor(loc), np.zeros(shape), atol=1e-4)\n    target_scale = np.log(1. + np.exp(-3.))\n    self.assertAllClose(tf.convert_to_tensor(scale),\n                        target_scale * np.ones(shape),\n                        atol=5e-2)\n\n    self.assertAllGreater(tf.convert_to_tensor(log_normal), 0.)\n    self.assertEqual(log_normal.shape, shape)\n\n  def testTrainableNormal(self):\n    tf.random.set_seed(345689)\n    shape = (100,)\n    initializer = ed.initializers.get(\'trainable_normal\')\n    normal = initializer(shape)\n    # Get distribution of rv -> get distribution of Independent.\n    loc = normal.distribution.distribution.loc\n    scale = normal.distribution.distribution.scale\n    self.assertAllClose(tf.convert_to_tensor(loc), np.zeros(shape), atol=1e-4)\n    target_scale = np.log(1. + np.exp(-3.))\n    self.assertAllClose(tf.convert_to_tensor(scale),\n                        target_scale * np.ones(shape),\n                        atol=5e-2)\n    self.assertEqual(normal.shape, shape)\n\n  def testTrainableMixtureOfDeltas(self):\n    tf.random.set_seed(345689)\n    shape = (100,)\n    num_components = 5\n    initializer = ed.initializers.TrainableMixtureOfDeltas(num_components)\n    mixture_shape = list(shape) + [num_components]\n    rv = initializer(shape)\n    # Get distribution of rv -> get distribution of Independent.\n    probs = rv.distribution.distribution.mixture_distribution.probs\n    loc = rv.distribution.distribution.components_distribution.loc\n    self.assertAllClose(\n        probs,\n        tf.broadcast_to([[1/num_components]*num_components], mixture_shape),\n        atol=1e-4)\n    self.assertAllClose(tf.convert_to_tensor(loc), tf.zeros_like(loc), atol=1.)\n    self.assertEqual(rv.shape, shape)\n\n  def testInitializersGet(self):\n    self.assertIsInstance(ed.initializers.get(\'trainable_normal\'),\n                          ed.initializers.TrainableNormal)\n    # This is working correctly, but the test won\'t pass currently because TF\n    # isn\'t consistent (yet).  Specifically, tf.keras.initializers.get(\'zeros\')\n    # returns a certain class while tf.keras.initializers.zeros (or Zeros)\n    # currently returns v2 of that class.\n    # self.assertIsInstance(ed.initializers.get(\'zeros\'),\n    #                       tf.keras.initializers.Zeros().__class__)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
edward2/tensorflow/metrics.py,58,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Metrics (Keras-style).""""""\n\nimport itertools\nimport tensorflow as tf\n\n\nclass ExpectedCalibrationError(tf.keras.metrics.Metric):\n  """"""Expected Calibration Error.\n\n  Expected calibration error (Guo et al., 2017, Naeini et al., 2015) is a scalar\n  measure of calibration for probabilistic models. Calibration is defined as the\n  level to which the accuracy over a set of predicted decisions and true\n  outcomes associated with a given predicted probability level matches the\n  predicted probability. A perfectly calibrated model would be correct `p`% of\n  the time for all examples for which the predicted probability was `p`%, over\n  all values of `p`.\n\n  This metric can be computed as follows. First, cut up the probability space\n  interval [0, 1] into some number of bins. Then, for each example, store the\n  predicted class (based on a threshold of 0.5 in the binary case and the max\n  probability in the multiclass case), the predicted probability corresponding\n  to the predicted class, and the true label into the corresponding bin based on\n  the predicted probability. Then, for each bin, compute the average predicted\n  probability (""confidence""), the accuracy of the predicted classes, and the\n  absolute difference between the confidence and the accuracy (""calibration\n  error""). Expected calibration error can then be computed as a weighted average\n  calibration error over all bins, weighted based on the number of examples per\n  bin.\n\n  Perfect calibration under this setup is when, for all bins, the average\n  predicted probability matches the accuracy, and thus the expected calibration\n  error equals zero. In the limit as the number of bins goes to infinity, the\n  predicted probability would be equal to the accuracy for all possible\n  probabilities.\n\n  References:\n    1. Guo, C., Pleiss, G., Sun, Y. & Weinberger, K. Q. On Calibration of Modern\n       Neural Networks. in International Conference on Machine Learning (ICML)\n       cs.LG, (Cornell University Library, 2017).\n    2. Naeini, M. P., Cooper, G. F. & Hauskrecht, M. Obtaining Well Calibrated\n       Probabilities Using Bayesian Binning. Proc Conf AAAI Artif Intell 2015,\n       2901-2907 (2015).\n  """"""\n\n  _setattr_tracking = False  # Automatic tracking breaks some unit tests\n\n  def __init__(self, num_bins=15, name=None, dtype=None):\n    """"""Constructs an expected calibration error metric.\n\n    Args:\n      num_bins: Number of bins to maintain over the interval [0, 1].\n      name: Name of this metric.\n      dtype: Data type.\n    """"""\n    super(ExpectedCalibrationError, self).__init__(name, dtype)\n    self.num_bins = num_bins\n\n    self.correct_sums = self.add_weight(\n        \'correct_sums\', shape=(num_bins,), initializer=tf.zeros_initializer)\n    self.prob_sums = self.add_weight(\n        \'prob_sums\', shape=(num_bins,), initializer=tf.zeros_initializer)\n    self.counts = self.add_weight(\n        \'counts\', shape=(num_bins,), initializer=tf.zeros_initializer)\n\n  @tf.function\n  def update_state(self, labels, probabilities, **kwargs):\n    """"""Updates this metric.\n\n    This will flatten the labels and probabilities, and then compute the ECE\n    over all predictions.\n\n    Args:\n      labels: Tensor of shape [..., ] of class labels in [0, k-1].\n      probabilities: Tensor of shape [..., ], [..., 1] or [..., k] of normalized\n        probabilities associated with the True class in the binary case, or with\n        each of k classes in the multiclass case.\n      **kwargs: Other potential keywords, which will be ignored by this method.\n    """"""\n    del kwargs  # unused\n    labels = tf.convert_to_tensor(labels)\n    probabilities = tf.cast(probabilities, self.dtype)\n\n    # Flatten labels to [N, ] and probabilities to [N, 1] or [N, k].\n    if tf.rank(labels) != 1:\n      labels = tf.reshape(labels, [-1])\n    if tf.rank(probabilities) != 2 or (tf.shape(probabilities)[0] !=\n                                       tf.shape(labels)[0]):\n      probabilities = tf.reshape(probabilities, [tf.shape(labels)[0], -1])\n    # Extend any probabilities of shape [N, 1] to shape [N, 2].\n    # NOTE: XLA does not allow for different shapes in the branches of a\n    # conditional statement. Therefore, explicit indexing is used.\n    given_k = tf.shape(probabilities)[-1]\n    k = tf.math.maximum(2, given_k)\n    probabilities = tf.cond(\n        given_k < 2,\n        lambda: tf.concat([1. - probabilities, probabilities], axis=-1)[:, -k:],\n        lambda: probabilities)\n\n    pred_labels = tf.math.argmax(probabilities, axis=-1)\n    pred_probs = tf.math.reduce_max(probabilities, axis=-1)\n    correct_preds = tf.math.equal(pred_labels,\n                                  tf.cast(labels, pred_labels.dtype))\n    correct_preds = tf.cast(correct_preds, self.dtype)\n\n    bin_indices = tf.histogram_fixed_width_bins(\n        pred_probs, tf.constant([0., 1.], self.dtype), nbins=self.num_bins)\n    batch_correct_sums = tf.math.unsorted_segment_sum(\n        data=tf.cast(correct_preds, self.dtype),\n        segment_ids=bin_indices,\n        num_segments=self.num_bins)\n    batch_prob_sums = tf.math.unsorted_segment_sum(data=pred_probs,\n                                                   segment_ids=bin_indices,\n                                                   num_segments=self.num_bins)\n    batch_counts = tf.math.unsorted_segment_sum(data=tf.ones_like(bin_indices),\n                                                segment_ids=bin_indices,\n                                                num_segments=self.num_bins)\n    batch_counts = tf.cast(batch_counts, self.dtype)\n    self.correct_sums.assign_add(batch_correct_sums)\n    self.prob_sums.assign_add(batch_prob_sums)\n    self.counts.assign_add(batch_counts)\n\n  def result(self):\n    """"""Computes the expected calibration error.""""""\n    non_empty = tf.math.not_equal(self.counts, 0)\n    correct_sums = tf.boolean_mask(self.correct_sums, non_empty)\n    prob_sums = tf.boolean_mask(self.prob_sums, non_empty)\n    counts = tf.boolean_mask(self.counts, non_empty)\n    accs = correct_sums / counts\n    confs = prob_sums / counts\n    total_count = tf.reduce_sum(counts)\n    return tf.reduce_sum(counts / total_count * tf.abs(accs - confs))\n\n  def reset_states(self):\n    """"""Resets all of the metric state variables.\n\n    This function is called between epochs/steps,\n    when a metric is evaluated during training.\n    """"""\n    tf.keras.backend.batch_set_value([(v, [0.,]*self.num_bins) for v in\n                                      self.variables])\n\n\n# TODO(ghassen): disagreement and double_fault could be extended beyond pairs.\ndef disagreement(logits_1, logits_2):\n  """"""Disagreement between the predictions of two classifiers.""""""\n  preds_1 = tf.argmax(logits_1, axis=-1, output_type=tf.int32)\n  preds_2 = tf.argmax(logits_2, axis=-1, output_type=tf.int32)\n  return tf.reduce_mean(tf.cast(preds_1 != preds_2, tf.float32))\n\n\ndef logit_kl_divergence(logits_1, logits_2):\n  """"""Average KL divergence between logit distributions of two classifiers.""""""\n  probs_1 = tf.nn.softmax(logits_1)\n  probs_2 = tf.nn.softmax(logits_2)\n  vals = kl_divergence(probs_1, probs_2)\n  return tf.reduce_mean(vals)\n\n\ndef kl_divergence(p, q):\n  """"""Generalized KL divergence [1] for unnormalized distributions.\n\n  Args:\n    p: tf.Tensor.\n    q: tf.Tensor\n\n  Returns:\n    tf.Tensor of the Kullback-Leibler divergences per example.\n\n  ## References\n\n  [1] Lee, Daniel D., and H. Sebastian Seung. ""Algorithms for non-negative\n  matrix factorization."" Advances in neural information processing systems.\n  2001.\n  """"""\n  return tf.reduce_sum(p * tf.math.log(p / q) - p + q, axis=-1)\n\n\ndef lp_distance(x, y, p=1):\n  """"""l_p distance.""""""\n  diffs_abs = tf.abs(x - y)\n  summation = tf.reduce_sum(tf.math.pow(diffs_abs, p), axis=-1)\n  return tf.reduce_mean(tf.math.pow(summation, 1./p), axis=-1)\n\n\ndef cosine_distance(x, y):\n  """"""Cosine distance between vectors x and y.""""""\n  x_norm = tf.math.sqrt(tf.reduce_sum(tf.pow(x, 2), axis=-1))\n  x_norm = tf.reshape(x_norm, (-1, 1))\n  y_norm = tf.math.sqrt(tf.reduce_sum(tf.pow(y, 2), axis=-1))\n  y_norm = tf.reshape(y_norm, (-1, 1))\n  normalized_x = x / x_norm\n  normalized_y = y / y_norm\n  return tf.reduce_mean(tf.reduce_sum(normalized_x * normalized_y, axis=-1))\n\n\n# TODO(ghassen): we could extend this to take an arbitrary list of metric fns.\ndef average_pairwise_diversity(probs, num_models, error=None):\n  """"""Average pairwise distance computation across models.""""""\n  if probs.shape[0] != num_models:\n    raise ValueError(\'The number of models {0} does not match \'\n                     \'the probs length {1}\'.format(num_models, probs.shape[0]))\n\n  pairwise_disagreement = []\n  pairwise_kl_divergence = []\n  pairwise_cosine_distance = []\n  for pair in list(itertools.combinations(range(num_models), 2)):\n    probs_1 = probs[pair[0]]\n    probs_2 = probs[pair[1]]\n    pairwise_disagreement.append(disagreement(probs_1, probs_2))\n    pairwise_kl_divergence.append(\n        tf.reduce_mean(kl_divergence(probs_1, probs_2)))\n    pairwise_cosine_distance.append(cosine_distance(probs_1, probs_2))\n\n  # TODO(ghassen): we could also return max and min pairwise metrics.\n  average_disagreement = tf.reduce_mean(tf.stack(pairwise_disagreement))\n  if error is not None:\n    average_disagreement /= (error + tf.keras.backend.epsilon())\n  average_kl_divergence = tf.reduce_mean(tf.stack(pairwise_kl_divergence))\n  average_cosine_distance = tf.reduce_mean(tf.stack(pairwise_cosine_distance))\n\n  return {\n      \'disagreement\': average_disagreement,\n      \'average_kl\': average_kl_divergence,\n      \'cosine_similarity\': average_cosine_distance\n  }\n'"
edward2/tensorflow/metrics_test.py,17,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for metrics.""""""\n\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass ExpectedCalibrationErrorTest(tf.test.TestCase):\n\n  def testBinaryClassification(self):\n    num_bins = 10\n    pred_probs = np.array([0.51, 0.45, 0.39, 0.66, 0.68, 0.29, 0.81, 0.85])\n    # max_pred_probs: [0.51, 0.55, 0.61, 0.66, 0.68, 0.71, 0.81, 0.85]\n    # pred_class: [1, 0, 0, 1, 1, 0, 1, 1]\n    labels = np.array([0., 0., 0., 1., 0., 1., 1., 1.])\n    n = len(pred_probs)\n\n    # Bins for the max predicted probabilities are (0, 0.1), [0.1, 0.2), ...,\n    # [0.9, 1) and are numbered starting at zero.\n    bin_counts = np.array([0, 0, 0, 0, 0, 2, 3, 1, 2, 0])\n    bin_correct_sums = np.array([0, 0, 0, 0, 0, 1, 2, 0, 2, 0])\n    bin_prob_sums = np.array([0, 0, 0, 0, 0, 0.51 + 0.55, 0.61 + 0.66 + 0.68,\n                              0.71, 0.81 + 0.85, 0])\n\n    correct_ece = 0.\n    bin_accs = np.array([0.] * num_bins)\n    bin_confs = np.array([0.] * num_bins)\n    for i in range(num_bins):\n      if bin_counts[i] > 0:\n        bin_accs[i] = bin_correct_sums[i] / bin_counts[i]\n        bin_confs[i] = bin_prob_sums[i] / bin_counts[i]\n        correct_ece += bin_counts[i] / n * abs(bin_accs[i] - bin_confs[i])\n\n    metric = ed.metrics.ExpectedCalibrationError(\n        num_bins, name=\'ECE\', dtype=tf.float64)\n    self.assertEqual(len(metric.variables), 3)\n\n    ece1 = metric(labels, pred_probs)\n    self.assertAllClose(ece1, correct_ece)\n\n    actual_bin_counts = tf.convert_to_tensor(metric.counts)\n    actual_bin_correct_sums = tf.convert_to_tensor(metric.correct_sums)\n    actual_bin_prob_sums = tf.convert_to_tensor(metric.prob_sums)\n    self.assertAllEqual(bin_counts, actual_bin_counts)\n    self.assertAllEqual(bin_correct_sums, actual_bin_correct_sums)\n    self.assertAllClose(bin_prob_sums, actual_bin_prob_sums)\n\n    # Test various types of input shapes.\n    metric.reset_states()\n    metric.update_state(labels[:2], pred_probs[:2])\n    metric.update_state(labels[2:6].reshape(2, 2),\n                        pred_probs[2:6].reshape(2, 2))\n    metric.update_state(labels[6:7], pred_probs[6:7])\n    ece2 = metric(labels[7:, np.newaxis], pred_probs[7:, np.newaxis])\n    ece3 = metric.result()\n    self.assertAllClose(ece2, ece3)\n    self.assertAllClose(ece3, correct_ece)\n\n    actual_bin_counts = tf.convert_to_tensor(metric.counts)\n    actual_bin_correct_sums = tf.convert_to_tensor(metric.correct_sums)\n    actual_bin_prob_sums = tf.convert_to_tensor(metric.prob_sums)\n    self.assertAllEqual(bin_counts, actual_bin_counts)\n    self.assertAllEqual(bin_correct_sums, actual_bin_correct_sums)\n    self.assertAllClose(bin_prob_sums, actual_bin_prob_sums)\n\n  def testBinaryClassificationKerasModel(self):\n    num_bins = 10\n    pred_probs = np.array([0.51, 0.45, 0.39, 0.66, 0.68, 0.29, 0.81, 0.85])\n    # max_pred_probs: [0.51, 0.55, 0.61, 0.66, 0.68, 0.71, 0.81, 0.85]\n    # pred_class: [1, 0, 0, 1, 1, 0, 1, 1]\n    labels = np.array([0., 0., 0., 1., 0., 1., 1., 1.])\n    n = len(pred_probs)\n\n    # Bins for the max predicted probabilities are (0, 0.1), [0.1, 0.2), ...,\n    # [0.9, 1) and are numbered starting at zero.\n    bin_counts = [0, 0, 0, 0, 0, 2, 3, 1, 2, 0]\n    bin_correct_sums = [0, 0, 0, 0, 0, 1, 2, 0, 2, 0]\n    bin_prob_sums = [0, 0, 0, 0, 0, 0.51 + 0.55, 0.61 + 0.66 + 0.68, 0.71,\n                     0.81 + 0.85, 0]\n\n    correct_ece = 0.\n    bin_accs = [0.] * num_bins\n    bin_confs = [0.] * num_bins\n    for i in range(num_bins):\n      if bin_counts[i] > 0:\n        bin_accs[i] = bin_correct_sums[i] / bin_counts[i]\n        bin_confs[i] = bin_prob_sums[i] / bin_counts[i]\n        correct_ece += bin_counts[i] / n * abs(bin_accs[i] - bin_confs[i])\n\n    metric = ed.metrics.ExpectedCalibrationError(num_bins, name=\'ECE\')\n    self.assertEqual(len(metric.variables), 3)\n\n    model = tf.keras.models.Sequential([tf.keras.layers.Lambda(lambda x: 1*x)])\n    model.compile(loss=\'binary_crossentropy\', optimizer=\'sgd\', metrics=[metric])\n    outputs = model.predict(pred_probs)\n    self.assertAllClose(pred_probs.reshape([n, 1]), outputs)\n    _, ece = model.evaluate(pred_probs, labels)\n    self.assertAllClose(ece, correct_ece)\n\n    actual_bin_counts = tf.convert_to_tensor(metric.counts)\n    actual_bin_correct_sums = tf.convert_to_tensor(metric.correct_sums)\n    actual_bin_prob_sums = tf.convert_to_tensor(metric.prob_sums)\n    self.assertAllEqual(bin_counts, actual_bin_counts)\n    self.assertAllEqual(bin_correct_sums, actual_bin_correct_sums)\n    self.assertAllClose(bin_prob_sums, actual_bin_prob_sums)\n\n  def testMulticlassClassification(self):\n    num_bins = 10\n    pred_probs = [\n        [0.31, 0.32, 0.27],\n        [0.37, 0.33, 0.30],\n        [0.30, 0.31, 0.39],\n        [0.61, 0.38, 0.01],\n        [0.10, 0.65, 0.25],\n        [0.91, 0.05, 0.04],\n    ]\n    # max_pred_probs: [0.32, 0.37, 0.39, 0.61, 0.65, 0.91]\n    # pred_class: [1, 0, 2, 0, 1, 0]\n    labels = [1., 0, 0., 1., 0., 0.]\n    n = len(pred_probs)\n\n    # Bins for the max predicted probabilities are (0, 0.1), [0.1, 0.2), ...,\n    # [0.9, 1) and are numbered starting at zero.\n    bin_counts = [0, 0, 0, 3, 0, 0, 2, 0, 0, 1]\n    bin_correct_sums = [0, 0, 0, 2, 0, 0, 0, 0, 0, 1]\n    bin_prob_sums = [0, 0, 0, 0.32 + 0.37 + 0.39, 0, 0, 0.61 + 0.65, 0, 0, 0.91]\n\n    correct_ece = 0.\n    bin_accs = [0.] * num_bins\n    bin_confs = [0.] * num_bins\n    for i in range(num_bins):\n      if bin_counts[i] > 0:\n        bin_accs[i] = bin_correct_sums[i] / bin_counts[i]\n        bin_confs[i] = bin_prob_sums[i] / bin_counts[i]\n        correct_ece += bin_counts[i] / n * abs(bin_accs[i] - bin_confs[i])\n\n    metric = ed.metrics.ExpectedCalibrationError(\n        num_bins, name=\'ECE\', dtype=tf.float64)\n    self.assertEqual(len(metric.variables), 3)\n\n    metric.update_state(labels[:4], pred_probs[:4])\n    ece1 = metric(labels[4:], pred_probs[4:])\n    ece2 = metric.result()\n    self.assertAllClose(ece1, ece2)\n    self.assertAllClose(ece2, correct_ece)\n\n    actual_bin_counts = tf.convert_to_tensor(metric.counts)\n    actual_bin_correct_sums = tf.convert_to_tensor(metric.correct_sums)\n    actual_bin_prob_sums = tf.convert_to_tensor(metric.prob_sums)\n    self.assertAllEqual(bin_counts, actual_bin_counts)\n    self.assertAllEqual(bin_correct_sums, actual_bin_correct_sums)\n    self.assertAllClose(bin_prob_sums, actual_bin_prob_sums)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
edward2/tensorflow/program_transformations.py,8,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Transformations of Edward2 programs.""""""\n\nimport inspect\nfrom edward2.trace import trace\nimport tensorflow as tf\n\n\ndef make_log_joint_fn(model):\n  """"""Takes Edward probabilistic program and returns its log joint function.\n\n  Args:\n    model: Python callable which executes the generative process of a\n      computable probability distribution using `ed.RandomVariable`s.\n\n  Returns:\n    A log-joint probability function. Its inputs are `model`\'s original inputs\n    and random variables which appear during the program execution. Its output\n    is a scalar tf.Tensor.\n\n  #### Examples\n\n  Below we define Bayesian logistic regression as an Edward program,\n  representing the model\'s generative process. We apply `make_log_joint_fn` in\n  order to represent the model in terms of its joint probability function.\n\n  ```python\n  import edward2 as ed\n\n  def logistic_regression(features):\n    coeffs = ed.Normal(loc=0., scale=1.,\n                       sample_shape=features.shape[1], name=""coeffs"")\n    outcomes = ed.Bernoulli(logits=tf.tensordot(features, coeffs, [[1], [0]]),\n                            name=""outcomes"")\n    return outcomes\n\n  log_joint = ed.make_log_joint_fn(logistic_regression)\n\n  features = tf.random.normal([3, 2])\n  coeffs_value = tf.random.normal([2])\n  outcomes_value = tf.round(tf.random.uniform([3]))\n  output = log_joint(features, coeffs=coeffs_value, outcomes=outcomes_value)\n  ```\n\n  """"""\n  def log_joint_fn(*args, **kwargs):\n    """"""Log-probability of inputs according to a joint probability distribution.\n\n    Args:\n      *args: Positional arguments. They are the model\'s original inputs and can\n        alternatively be specified as part of `kwargs`.\n      **kwargs: Keyword arguments, where for each key-value pair `k` and `v`,\n        `v` is passed as a `value` to the random variable(s) whose keyword\n        argument `name` during construction is equal to `k`.\n\n    Returns:\n      Scalar tf.Tensor, which represents the model\'s log-probability summed\n      over all Edward random variables and their dimensions.\n\n    Raises:\n      TypeError: If a random variable in the model has no specified value in\n        `**kwargs`.\n    """"""\n    log_probs = []\n\n    def tracer(rv_constructor, *rv_args, **rv_kwargs):\n      """"""Overrides a random variable\'s `value` and accumulates its log-prob.""""""\n      # Set value to keyword argument indexed by `name` (an input tensor).\n      rv_name = rv_kwargs.get(""name"")\n      if rv_name is None:\n        raise KeyError(""Random variable constructor {} has no name ""\n                       ""in its arguments."".format(rv_constructor.__name__))\n      value = kwargs.get(rv_name)\n      if value is None:\n        raise LookupError(""Keyword argument specifying value for {} is ""\n                          ""missing."".format(rv_name))\n      rv_kwargs[""value""] = value\n\n      rv = rv_constructor(*rv_args, **rv_kwargs)\n      log_prob = tf.reduce_sum(rv.distribution.log_prob(rv.value))\n      log_probs.append(log_prob)\n      return rv\n\n    model_kwargs = _get_function_inputs(model, kwargs)\n    with trace(tracer):\n      model(*args, **model_kwargs)\n    log_prob = sum(log_probs)\n    return log_prob\n  return log_joint_fn\n\n\ndef _get_function_inputs(f, src_kwargs):\n  """"""Filters inputs to be compatible with function `f`\'s signature.\n\n  Args:\n    f: Function according to whose input signature we filter arguments.\n    src_kwargs: Keyword arguments to filter according to `f`.\n\n  Returns:\n    kwargs: Dict of key-value pairs in `src_kwargs` which exist in `f`\'s\n      signature.\n  """"""\n  if hasattr(f, ""_func""):  # functions returned by tf.make_template\n    f = f._func  # pylint: disable=protected-access\n\n  try:  # getargspec was deprecated in Python 3.6\n    argspec = inspect.getfullargspec(f)  # pytype: disable=module-attr\n  except AttributeError:\n    argspec = inspect.getargspec(f)\n\n  fkwargs = {k: v for k, v in src_kwargs.items() if k in argspec.args}\n  return fkwargs\n'"
edward2/tensorflow/program_transformations_test.py,27,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for program transformations.""""""\n\nimport edward2 as ed\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\ntfd = tfp.distributions\n\n\nclass ProgramTransformationsTest(tf.test.TestCase):\n\n  def testMakeLogJointFnUnconditional(self):\n    """"""Test `make_log_joint_fn` on unconditional Edward program.""""""\n    def normal_with_unknown_mean():\n      loc = ed.Normal(loc=0., scale=1., name=""loc"")\n      x = ed.Normal(loc=loc, scale=0.5, sample_shape=5, name=""x"")\n      return x\n\n    def true_log_joint(loc, x):\n      log_prob = tf.reduce_sum(tfd.Normal(loc=0., scale=1.).log_prob(loc))\n      log_prob += tf.reduce_sum(tfd.Normal(loc=loc, scale=0.5).log_prob(x))\n      return log_prob\n\n    loc_value = 0.3\n    x_value = tf.random.normal([5])\n\n    log_joint = ed.make_log_joint_fn(normal_with_unknown_mean)\n    actual_log_prob = true_log_joint(loc_value, x_value)\n    expected_log_prob = log_joint(loc=loc_value, x=x_value)\n\n    with self.assertRaises(LookupError):\n      _ = log_joint(loc=loc_value)\n\n    self.assertEqual(actual_log_prob, expected_log_prob)\n\n  def testMakeLogJointFnConditional(self):\n    """"""Test `make_log_joint_fn` on conditional Edward program.""""""\n    def linear_regression(features, prior_precision):\n      w = ed.Normal(loc=0.,\n                    scale=tf.math.rsqrt(prior_precision),\n                    sample_shape=features.shape[1],\n                    name=""w"")\n      y = ed.Normal(loc=tf.tensordot(features, w, [[1], [0]]),\n                    scale=1.,\n                    name=""y"")\n      return y\n\n    features = tf.random.normal([3, 2])\n    prior_precision = 0.5\n    w_value = tf.random.normal([2])\n    y_value = tf.random.normal([3])\n\n    def true_log_joint(features, prior_precision, w, y):\n      log_prob = tf.reduce_sum(tfd.Normal(\n          loc=0.,\n          scale=tf.math.rsqrt(prior_precision)).log_prob(w))\n      log_prob += tf.reduce_sum(tfd.Normal(\n          loc=tf.tensordot(features, w, [[1], [0]]),\n          scale=1.).log_prob(y))\n      return log_prob\n\n    log_joint = ed.make_log_joint_fn(linear_regression)\n    actual_log_prob = true_log_joint(\n        features, prior_precision, w_value, y_value)\n    expected_log_prob = log_joint(\n        features, prior_precision, y=y_value, w=w_value)\n\n    with self.assertRaises(LookupError):\n      _ = log_joint(features, prior_precision, w=w_value)\n\n    self.assertEqual(actual_log_prob, expected_log_prob)\n\n  def testMakeLogJointFnDynamic(self):\n    """"""Test `make_log_joint_fn` on Edward program with stochastic control flow.\n\n    This verifies that Edward\'s program transformation is done by tracing the\n    execution at runtime (and not purely by static analysis). In particular,\n    the execution is controlled by random variable outcomes, which in turn is\n    controlled by the log-joint\'s inputs.\n    """"""\n    def mixture_of_real_and_int():\n      loc = ed.Normal(loc=0., scale=1., name=""loc"")\n      flip = ed.Bernoulli(probs=0.5, name=""flip"")\n      if tf.equal(flip, 1):\n        x = ed.Normal(loc=loc, scale=0.5, sample_shape=5, name=""x"")\n      else:\n        x = ed.Poisson(rate=tf.nn.softplus(loc), sample_shape=3, name=""x"")\n      return x\n\n    def true_log_joint(loc, flip, x):\n      log_prob = tf.reduce_sum(tfd.Normal(loc=0., scale=1.).log_prob(loc))\n      log_prob += tf.reduce_sum(tfd.Bernoulli(probs=0.5).log_prob(flip))\n      if tf.equal(flip, 1):\n        log_prob += tf.reduce_sum(tfd.Normal(loc=loc, scale=0.5).log_prob(x))\n      else:\n        log_prob += tf.reduce_sum(\n            tfd.Poisson(rate=tf.nn.softplus(loc)).log_prob(x))\n      return log_prob\n\n    loc_value = 0.3\n    flip_value = tf.constant(1)\n    x_value = tf.random.normal([5])\n\n    log_joint = ed.make_log_joint_fn(mixture_of_real_and_int)\n    actual_log_prob = true_log_joint(loc_value, flip_value, x_value)\n    expected_log_prob = log_joint(loc=loc_value, flip=flip_value, x=x_value)\n    self.assertEqual(actual_log_prob, expected_log_prob)\n\n    loc_value = 1.2\n    flip_value = tf.constant(0)\n    x_value = tf.random.normal([3])\n\n    actual_log_prob = true_log_joint(loc_value, flip_value, x_value)\n    expected_log_prob = log_joint(loc=loc_value, flip=flip_value, x=x_value)\n    self.assertEqual(actual_log_prob, expected_log_prob)\n\n  def testMakeLogJointFnError(self):\n    """"""Test `make_log_joint_fn` raises errors when `name`(s) not supplied.""""""\n    def normal_with_unknown_mean():\n      loc = ed.Normal(loc=0., scale=1., name=""loc"")\n      x = ed.Normal(loc=loc, scale=0.5, sample_shape=5)\n      return x\n\n    loc_value = 0.3\n    x_value = tf.random.normal([5])\n\n    log_joint = ed.make_log_joint_fn(normal_with_unknown_mean)\n\n    with self.assertRaises(KeyError):\n      _ = log_joint(loc=loc_value, x=x_value)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
edward2/tensorflow/random_variable.py,22,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Random variable.""""""\n\nimport functools\nimport tensorflow as tf\n\n\nclass RandomVariable(object):\n  """"""Class for random variables.\n\n  `RandomVariable` encapsulates properties of a random variable, namely, its\n  distribution, sample shape, and (optionally overridden) value. Its `value`\n  property is a `tf.Tensor`, which embeds the `RandomVariable` object into the\n  TensorFlow graph. `RandomVariable` also features operator overloading,\n  enabling idiomatic usage as if one were operating on `tf.Tensor`s.\n\n  The random variable\'s shape is given by\n\n  `sample_shape + distribution.batch_shape + distribution.event_shape`,\n\n  where `sample_shape` is an optional argument describing the shape of\n  independent, identical draws from the distribution (default is `()`, meaning\n  a single draw); `distribution.batch_shape` describes the shape of\n  independent-but-not-identical draws (determined by the shape of the\n  distribution\'s parameters); and `distribution.event_shape` describes the\n  shape of dependent dimensions (e.g., `Normal` has scalar `event_shape`;\n  `Dirichlet` has vector `event_shape`).\n\n  #### Examples\n\n  ```python\n  import edward2 as ed\n  import tensorflow_probability as tfp\n\n  z1 = tf.constant([[1.0, -0.8], [0.3, -1.0]])\n  z2 = tf.constant([[0.9, 0.2], [2.0, -0.1]])\n  x = ed.RandomVariable(tfp.distributions.Bernoulli(logits=tf.matmul(z1, z2)))\n\n  loc = ed.RandomVariable(tfp.distributions.Normal(0., 1.))\n  x = ed.RandomVariable(tfp.distributions.Normal(loc, 1.), sample_shape=50)\n  assert x.shape.as_list() == [50]\n  assert x.sample_shape.as_list() == [50]\n  assert x.distribution.batch_shape.as_list() == []\n  assert x.distribution.event_shape.as_list() == []\n  ```\n  """"""\n\n  def __init__(self,\n               distribution,\n               sample_shape=(),\n               value=None):\n    """"""Create a new random variable.\n\n    Args:\n      distribution: Distribution of the random variable. At minimum, the\n        distribution object must have the attributes `dtype`, `batch_shape`,\n        `event_shape`, `sample`, and `name`.\n      sample_shape: tf.TensorShape of samples to draw from the random variable.\n        Default is `()` corresponding to a single sample.\n      value: tf.Tensor to associate with random variable. Must have shape\n        `sample_shape + distribution.batch_shape + distribution.event_shape`.\n        Default is to sample from random variable according to `sample_shape`.\n\n    Raises:\n      ValueError: `value` has incompatible shape with\n        `sample_shape + distribution.batch_shape + distribution.event_shape`.\n    """"""\n    self._distribution = distribution\n    self._sample_shape = sample_shape\n    if isinstance(value, tf.Tensor):\n      value_shape = value.shape\n      expected_value_shape = self.sample_shape.concatenate(\n          self.distribution.batch_shape).concatenate(\n              self.distribution.event_shape)\n      if not value_shape.is_compatible_with(expected_value_shape):\n        raise ValueError(\n            ""Incompatible shape for initialization argument \'value\'. ""\n            ""Expected %s, got %s."" % (expected_value_shape, value_shape))\n    self._value = value\n\n  @property\n  def distribution(self):\n    """"""Distribution of random variable.""""""\n    return self._distribution\n\n  @property\n  def dtype(self):\n    """"""`Dtype` of elements in this random variable.""""""\n    return self.value.dtype\n\n  @property\n  def sample_shape(self):\n    """"""Sample shape of random variable as a `TensorShape`.""""""\n    if isinstance(self._sample_shape, tf.Tensor):\n      return tf.TensorShape(tf.get_static_value(self._sample_shape))\n    return tf.TensorShape(self._sample_shape)\n\n  def sample_shape_tensor(self, name=""sample_shape_tensor""):\n    """"""Sample shape of random variable as a 1-D `Tensor`.\n\n    Args:\n      name: name to give to the op\n\n    Returns:\n      sample_shape: `Tensor`.\n    """"""\n    with tf.name_scope(name):\n      if isinstance(self._sample_shape, tf.Tensor):\n        return self._sample_shape\n      return tf.convert_to_tensor(self.sample_shape.as_list(), dtype=tf.int32)\n\n  @property\n  def shape(self):\n    """"""Shape of random variable.""""""\n    return self.value.shape\n\n  @property\n  def value(self):\n    """"""Get tensor that the random variable corresponds to.""""""\n    if self._value is None:\n      try:\n        self._value = self.distribution.sample(self.sample_shape_tensor())\n      except NotImplementedError:\n        raise NotImplementedError(\n            ""sample is not implemented for {0}. You must either pass in the ""\n            ""value argument or implement sample for {0}.""\n            .format(self.distribution.__class__.__name__))\n    else:\n      self._value = tf.cast(self._value, self.distribution.dtype)\n    return self._value\n\n  def __str__(self):\n    name = _numpy_text(self.value)\n    return ""RandomVariable(\\""%s\\""%s%s%s)"" % (\n        name,\n        "", shape=%s"" % self.shape if self.shape.ndims is not None else """",\n        "", dtype=%s"" % self.dtype.name if self.dtype else """",\n        "", device=%s"" % self.value.device if self.value.device else """")\n\n  def __repr__(self):\n    string = ""ed.RandomVariable \'%s\' shape=%s dtype=%s"" % (\n        self.distribution.name, self.shape, self.dtype.name)\n    if hasattr(self.value, ""numpy""):\n      string += "" numpy=%s"" % _numpy_text(self.value, is_repr=True)\n    return ""<%s>"" % string\n\n  def __getitem__(self, slices):\n    value = self.value.__getitem__(slices)\n    if self.sample_shape.as_list():\n      # Return an indexed Tensor instead of RandomVariable if sample_shape is\n      # non-scalar. Sample shapes can complicate how to index the distribution.\n      return value\n    try:\n      distribution = self.distribution.__getitem__(slices)\n    except (tf.errors.InvalidArgumentError, ValueError):\n      return value\n    else:\n      return RandomVariable(distribution, value=value)\n\n  def __hash__(self):\n    return id(self)\n\n  def __eq__(self, other):\n    return id(self) == id(other)\n\n  def __ne__(self, other):\n    return not self == other\n\n  def numpy(self):\n    """"""Value as NumPy array.""""""\n    return self.value.numpy()\n\n  def get_shape(self):\n    """"""Get shape of random variable.""""""\n    return self.shape\n\n  # This enables the RandomVariable\'s overloaded ""right"" binary operators to\n  # run when the left operand is an ndarray, because it accords the\n  # RandomVariable class higher priority than an ndarray, or a numpy matrix.\n  __array_priority__ = 100\n\n\ndef _numpy_text(tensor, is_repr=False):\n  """"""Human-readable representation of a tensor\'s numpy value.""""""\n  if tensor.dtype.is_numpy_compatible:\n    text = repr(tensor.numpy()) if is_repr else str(tensor.numpy())\n  else:\n    text = ""<unprintable>""\n  if ""\\n"" in text:\n    text = ""\\n"" + text\n  return text\n\n\ndef _overload_operator(cls, op):\n  """"""Defer an operator overload to `tf.Tensor`.\n\n  We pull the operator out of tf.Tensor dynamically to avoid ordering issues.\n\n  Args:\n    cls: Class to overload operator.\n    op: Python string representing the operator name.\n  """"""\n  @functools.wraps(getattr(tf.Tensor, op))\n  def _run_op(a, *args):\n    return getattr(tf.Tensor, op)(a.value, *args)\n\n  setattr(cls, op, _run_op)\n\n\ndef _tensor_conversion_function(v, dtype=None, name=None, as_ref=False):\n  del name, as_ref  # unused\n  if dtype and not dtype.is_compatible_with(v.dtype):\n    raise ValueError(\n        ""Incompatible type conversion requested to type \'%s\' for variable ""\n        ""of type \'%s\'"" % (dtype.name, v.dtype.name))\n  return v.value\n\n\nfor operator in tf.Tensor.OVERLOADABLE_OPERATORS.difference(\n    {""__getitem__""}).union({""__iter__"", ""__bool__"", ""__nonzero__""}):\n  _overload_operator(RandomVariable, operator)\n\ntf.register_tensor_conversion_function(  # enable tf.convert_to_tensor\n    RandomVariable, _tensor_conversion_function)\n'"
edward2/tensorflow/random_variable_test.py,12,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for random variable.""""""\n\nimport re\nfrom absl.testing import parameterized\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n\nclass FakeDistribution(tfp.distributions.Distribution):\n  """"""Fake distribution class for testing.""""""\n\n  def __init__(self):\n    super(FakeDistribution, self).__init__(\n        dtype=None,\n        reparameterization_type=tfp.distributions.FULLY_REPARAMETERIZED,\n        validate_args=False,\n        allow_nan_stats=True)\n\n\nclass RandomVariableTest(parameterized.TestCase, tf.test.TestCase):\n\n  def testConstructor(self):\n    x = ed.RandomVariable(tfp.distributions.Poisson(rate=np.ones([2, 5])),\n                          value=np.ones([2, 5]))\n    self.assertAllEqual(tf.convert_to_tensor(x), x.value)\n    with self.assertRaises(ValueError):\n      _ = ed.RandomVariable(tfp.distributions.Bernoulli(probs=0.5),\n                            value=tf.zeros([2, 5], dtype=tf.int32))\n    x = ed.RandomVariable(FakeDistribution())\n    with self.assertRaises(NotImplementedError):\n      _ = x.value\n\n  def testGradientsFirstOrder(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0., 1.))\n    def f(x):\n      return 2. * x\n    with tf.GradientTape() as tape:\n      tape.watch(x.value)\n      y = f(x)\n    z = tape.gradient(y, [x.value])[0]\n    self.assertEqual(z, 2.)\n\n  def testGradientsSecondOrder(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    def f(x):\n      return 2 * (x ** 2)\n    with tf.GradientTape() as tape2:\n      tape2.watch(x.value)\n      with tf.GradientTape() as tape:\n        tape.watch(x.value)\n        y = f(x)\n      z = tape.gradient(y, [x.value])[0]\n    z = tape2.gradient(z, [x.value])[0]\n    self.assertEqual(z, 4.0)\n\n  def testStr(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0), value=1.234)\n    pattern = ""RandomVariable(\\""1.234\\"", shape=(), dtype=float32""\n    regexp = re.escape(pattern)\n    self.assertRegexpMatches(str(x), regexp)\n\n  def testRepr(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0), value=1.234)\n    string = (""<ed.RandomVariable \'{name}\' shape=() ""\n              ""dtype=float32 numpy=1.234>"".format(name=x.distribution.name))\n    self.assertEqual(repr(x), string)\n\n  def testNumpy(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0), value=1.23)\n    self.assertEqual(x, tf.constant(1.23))\n\n  def testOperatorsAdd(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = x + y\n    z_value = x.value + y\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsRadd(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = y + x\n    z_value = y + x.value\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsSub(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = x - y\n    z_value = x.value - y\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsRsub(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = y - x\n    z_value = y - x.value\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsMul(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = x * y\n    z_value = x.value * y\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsRmul(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = y * x\n    z_value = y * x.value\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsDiv(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = x / y\n    z_value = x.value / y\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsRdiv(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = y / x\n    z_value = y / x.value\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsFloordiv(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = x // y\n    z_value = x.value // y\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsRfloordiv(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = y // x\n    z_value = y // x.value\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsMod(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = x % y\n    z_value = x.value % y\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsRmod(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = y % x\n    z_value = y % x.value\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsLt(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = x < y\n    z_value = x.value < y\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsLe(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = x <= y\n    z_value = x.value <= y\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsGt(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = x > y\n    z_value = x.value > y\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsGe(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = x >= y\n    z_value = x.value >= y\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsGetitem(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(tf.random.normal([3, 4]),\n                                                   1.))\n    z = x[0:2, 2:3]\n    z_value = x.value[0:2, 2:3]\n    self.assertIsInstance(z, ed.RandomVariable)\n    self.assertAllEqual(z.distribution.mean(), x.distribution.mean()[0:2, 2:3])\n    self.assertAllEqual(tf.convert_to_tensor(z), z_value)\n\n  def testOperatorsPow(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = x ** y\n    z_value = x.value ** y\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsRpow(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    z = y ** x\n    z_value = y ** x.value\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsNeg(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    z = -x\n    z_value = -x.value\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsAbs(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    z = abs(x)\n    z_value = abs(x.value)\n    self.assertAllEqual(z, z_value)\n\n  def testOperatorsHash(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    self.assertNotEqual(hash(x), hash(y))\n    self.assertEqual(hash(x), id(x))\n\n  # TODO(trandustin): Re-enable test.\n  # def testOperatorsEq(self):\n  #   x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n  #   self.assertEqual(x, x)\n\n  def testOperatorsNe(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = 5.0\n    self.assertNotEqual(x, y)\n\n  def testOperatorsBoolNonzero(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    with self.assertRaises(TypeError):\n      _ = not x\n\n  def testArrayPriority(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 1.0))\n    y = np.array(5.0, dtype=np.float32)\n    z = y / x\n    z_value = y / x.value\n    self.assertAllEqual(z, z_value)\n\n  def testConvertToTensor(self):\n    x = ed.RandomVariable(tfp.distributions.Normal(0.0, 0.1))\n    with self.assertRaises(ValueError):\n      _ = tf.convert_to_tensor(x, dtype=tf.int32)\n\n  @parameterized.parameters(\n      {""probs"": 0.5,\n       ""sample_shape"": [],\n       ""batch_shape"": [],\n       ""event_shape"": []},\n      {""probs"": np.zeros([2, 3]) + 0.5,\n       ""sample_shape"": [],\n       ""batch_shape"": [2, 3],\n       ""event_shape"": []},\n      {""probs"": 0.5,\n       ""sample_shape"": [2],\n       ""batch_shape"": [],\n       ""event_shape"": []},\n      {""probs"": 0.5,\n       ""sample_shape"": [2],\n       ""batch_shape"": [],\n       ""event_shape"": []},\n      {""probs"": 0.5,\n       ""sample_shape"": [2, 4],\n       ""batch_shape"": [],\n       ""event_shape"": []},\n  )\n  def testShape(self, probs, sample_shape, batch_shape, event_shape):\n    rv = ed.RandomVariable(tfp.distributions.Bernoulli(probs=probs),\n                           sample_shape=sample_shape)\n    self.assertEqual(rv.shape, sample_shape + batch_shape + event_shape)\n    self.assertEqual(rv.shape, rv.shape)\n    self.assertEqual(rv.sample_shape, sample_shape)\n    self.assertEqual(rv.distribution.batch_shape, batch_shape)\n    self.assertEqual(rv.distribution.event_shape, event_shape)\n\n  def testRandomTensorSample(self):\n    num_samples = tf.cast(tfp.distributions.Poisson(rate=5.).sample(), tf.int32)\n    _ = ed.RandomVariable(tfp.distributions.Normal(loc=0.0, scale=1.0),\n                          sample_shape=num_samples)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
edward2/tensorflow/regularizers.py,36,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Regularizers.\n\nThis module extends `tf.keras.regularizers` with two features:\n\n1. Regularizers which compute using any weight random variables\' distribution.\nFor example, consider a regularizer which computes an analytic KL\ndivergence given an input ed.Normal weight.\n2. ""Trainable regularizers"" are regularizers which may themselves carry\nparameters. For example, consider a weight regularizer which computes a\nKL divergence from the weights towards a learnable prior.\n\nOne subtlety is how `tf.keras.constraints` are used on the parameters of\ntrainable regularizers. Typically, Keras constraints are used with projected\ngradient descent, where one performs unconstrained optimization and then applies\na projection (the constraint) after each gradient update. To stay in line with\nprobabilistic literature, trainable regularizers apply constraints on the\n`tf.Variables` themselves (i.e., a constrained parameterization) and do not\napply projections during optimization.\n""""""\n\nfrom edward2.tensorflow import constraints\nfrom edward2.tensorflow import generated_random_variables\nfrom edward2.tensorflow import random_variable\nimport tensorflow as tf\n\n\nclass CauchyKLDivergence(tf.keras.regularizers.Regularizer):\n  """"""KL divergence regularizer from an input to the Cauchy distribution.""""""\n\n  def __init__(self, loc=0., scale=1., scale_factor=1.):\n    """"""Constructs regularizer where default uses the standard Cauchy.""""""\n    self.loc = loc\n    self.scale = scale\n    self.scale_factor = scale_factor\n\n  def __call__(self, x):\n    """"""Computes regularization using an unbiased Monte Carlo estimate.""""""\n    prior = generated_random_variables.Independent(\n        generated_random_variables.Cauchy(\n            loc=tf.broadcast_to(self.loc, x.distribution.event_shape),\n            scale=tf.broadcast_to(self.scale, x.distribution.event_shape)\n        ).distribution,\n        reinterpreted_batch_ndims=len(x.distribution.event_shape))\n    negative_entropy = x.distribution.log_prob(x)\n    cross_entropy = -prior.distribution.log_prob(x)\n    return self.scale_factor * (negative_entropy + cross_entropy)\n\n  def get_config(self):\n    return {\n        \'loc\': self.loc,\n        \'scale\': self.scale,\n        \'scale_factor\': self.scale_factor,\n    }\n\n\nclass HalfCauchyKLDivergence(tf.keras.regularizers.Regularizer):\n  """"""KL divergence regularizer from an input to the half-Cauchy distribution.""""""\n\n  def __init__(self, loc=0., scale=1., scale_factor=1.):\n    """"""Constructs regularizer where default uses the standard half-Cauchy.""""""\n    self.loc = loc\n    self.scale = scale\n    self.scale_factor = scale_factor\n\n  def __call__(self, x):\n    """"""Computes regularization using an unbiased Monte Carlo estimate.""""""\n    prior = generated_random_variables.Independent(\n        generated_random_variables.HalfCauchy(\n            loc=tf.broadcast_to(self.loc, x.distribution.event_shape),\n            scale=tf.broadcast_to(self.scale, x.distribution.event_shape)\n        ).distribution,\n        reinterpreted_batch_ndims=len(x.distribution.event_shape))\n    negative_entropy = x.distribution.log_prob(x)\n    cross_entropy = -prior.distribution.log_prob(x)\n    return self.scale_factor * (negative_entropy + cross_entropy)\n\n  def get_config(self):\n    return {\n        \'loc\': self.loc,\n        \'scale\': self.scale,\n        \'scale_factor\': self.scale_factor,\n    }\n\n\nclass LogUniformKLDivergence(tf.keras.regularizers.Regularizer):\n  """"""KL divergence regularizer from an input to the log-uniform distribution.""""""\n\n  def __init__(self, scale_factor=1.):\n    """"""Constructs regularizer.""""""\n    self.scale_factor = scale_factor\n\n  def __call__(self, x):\n    """"""Computes regularization given an ed.Normal random variable as input.""""""\n    if not isinstance(x, random_variable.RandomVariable):\n      raise ValueError(\'Input must be an ed.RandomVariable (for correct math, \'\n                       \'an ed.Normal random variable).\')\n    # Clip magnitude of dropout rate, where we get the dropout rate alpha from\n    # the additive parameterization (Molchanov et al., 2017): for weight ~\n    # Normal(mu, sigma**2), the variance `sigma**2 = alpha * mu**2`.\n    mean = x.distribution.mean()\n    log_variance = tf.math.log(x.distribution.variance())\n    log_alpha = log_variance - tf.math.log(tf.square(mean) +\n                                           tf.keras.backend.epsilon())\n    log_alpha = tf.clip_by_value(log_alpha, -8., 8.)\n\n    # Set magic numbers for cubic polynomial approx. (Molchanov et al., 2017).\n    k1 = 0.63576\n    k2 = 1.8732\n    k3 = 1.48695\n    c = -k1\n    output = tf.reduce_sum(k1 * tf.nn.sigmoid(k2 + k3 * log_alpha) +\n                           -0.5 * tf.math.log1p(tf.exp(-log_alpha)) + c)\n    return self.scale_factor * output\n\n  def get_config(self):\n    return {\n        \'scale_factor\': self.scale_factor,\n    }\n\n\nclass LogNormalKLDivergence(tf.keras.regularizers.Regularizer):\n  """"""KL divergence regularizer from an input to the log normal distribution.""""""\n\n  def __init__(self, loc=0., scale=1., scale_factor=1.):\n    """"""Constructs regularizer where default is a KL towards a std log normal.""""""\n    self.loc = loc\n    self.scale = scale\n    self.scale_factor = scale_factor\n\n  def __call__(self, x):\n    """"""Computes regularization given an input ed.RandomVariable.""""""\n    if not isinstance(x, random_variable.RandomVariable):\n      raise ValueError(\'Input must be an ed.RandomVariable.\')\n    prior = generated_random_variables.Independent(\n        generated_random_variables.LogNormal(\n            loc=tf.broadcast_to(self.loc, x.distribution.event_shape),\n            scale=tf.broadcast_to(self.scale, x.distribution.event_shape)\n        ).distribution,\n        reinterpreted_batch_ndims=len(x.distribution.event_shape))\n    regularization = x.distribution.kl_divergence(prior.distribution)\n    return self.scale_factor * regularization\n\n  def get_config(self):\n    return {\n        \'loc\': self.loc,\n        \'scale\': self.scale,\n        \'scale_factor\': self.scale_factor,\n    }\n\n\nclass NormalKLDivergence(tf.keras.regularizers.Regularizer):\n  """"""KL divergence regularizer from an input to the normal distribution.""""""\n\n  def __init__(self, mean=0., stddev=1., scale_factor=1.):\n    """"""Constructs regularizer where default is a KL towards the std normal.""""""\n    self.mean = mean\n    self.stddev = stddev\n    self.scale_factor = scale_factor\n\n  def __call__(self, x):\n    """"""Computes regularization given an input ed.RandomVariable.""""""\n    if not isinstance(x, random_variable.RandomVariable):\n      raise ValueError(\'Input must be an ed.RandomVariable.\')\n    prior = generated_random_variables.Independent(\n        generated_random_variables.Normal(\n            loc=tf.broadcast_to(self.mean, x.distribution.event_shape),\n            scale=tf.broadcast_to(self.stddev, x.distribution.event_shape)\n        ).distribution,\n        reinterpreted_batch_ndims=len(x.distribution.event_shape))\n    regularization = x.distribution.kl_divergence(prior.distribution)\n    return self.scale_factor * regularization\n\n  def get_config(self):\n    return {\n        \'mean\': self.mean,\n        \'stddev\': self.stddev,\n        \'scale_factor\': self.scale_factor,\n    }\n\n\nclass NormalEmpiricalBayesKLDivergence(NormalKLDivergence):\n  """"""Normal prior with distribution on variance and using empirical Bayes.\n\n  This regularizer uses a hierachical prior that shares a variance distribution\n  across all weight elements (Wu et al., 2018):\n\n  ```\n  p(variance) = InverseGamma(variance | variance_concentration, variance_scale)\n  p(weight[i, j]) = Normal(weight[i, j] | mean, variance),\n  ```\n\n  where `variance_concentration`, `variance_scale`, and `mean` are fixed. Given\n  an input random variable q(weight), the regularizer computes a KL divergence\n  towards the prior distribution p(weight, variance). The variance is fixed at\n  the value variance*:\n\n  ```\n  R(weight)\n  = KL( q(weight) deterministic(variance | variance*) || p(weight, scale) )\n  = E [ log q(weight) + log deterministic(variance | variance*) -\n        log p(weight, scale) ]\n  = E [ log q(weight) - log p(weight | variance*) ] - log p(variance*)\n  = KL( q(weight) || p(weight | variance*) ) - log p(variance*).\n  ```\n\n  We use Wu et al. (2018)\'s closed-form solution for variance*. The estimate is\n  approximate if the input random variable is not normally distributed.\n  """"""\n\n  def __init__(self,\n               mean=0.,\n               variance_concentration=2.01,\n               variance_scale=0.101,\n               scale_factor=1.):\n    """"""Constructs regularizer.""""""\n    self.variance_concentration = variance_concentration\n    self.variance_scale = variance_scale\n    super(NormalEmpiricalBayesKLDivergence, self).__init__(\n        mean=mean,\n        stddev=None,  # to be estimated at each call\n        scale_factor=scale_factor)\n\n  def __call__(self, x):\n    """"""Computes regularization given an input ed.RandomVariable.""""""\n    if not isinstance(x, random_variable.RandomVariable):\n      raise ValueError(\'Input must be an ed.RandomVariable.\')\n    # variance = (tr( sigma_q + mu_q mu_q^T ) + 2*beta) / (omega + 2*alpha + 2)\n    trace_covariance = tf.reduce_sum(x.distribution.variance())\n    trace_mean_outer_product = tf.reduce_sum(x.distribution.mean()**2)\n    num_weights = tf.cast(tf.reduce_prod(x.shape), x.dtype)\n    variance = ((trace_covariance + trace_mean_outer_product) +\n                2. * self.variance_scale)\n    variance /= num_weights + 2. * self.variance_concentration + 2.\n    self.stddev = tf.sqrt(variance)\n\n    variance_prior = generated_random_variables.InverseGamma(\n        self.variance_concentration, self.variance_scale)\n    regularization = super(NormalEmpiricalBayesKLDivergence, self).__call__(x)\n    regularization -= (self.scale_factor *\n                       variance_prior.distribution.log_prob(variance))\n    return regularization\n\n  def get_config(self):\n    return {\n        \'mean\': self.mean,\n        \'variance_concentration\': self.variance_concentration,\n        \'variance_scale\': self.variance_scale,\n        \'scale_factor\': self.scale_factor,\n    }\n\n\nclass TrainableNormalKLDivergenceStdDev(tf.keras.layers.Layer):\n  """"""Normal KL divergence with trainable stddev parameter.""""""\n\n  def __init__(self,\n               mean=0.,\n               stddev_initializer=tf.keras.initializers.TruncatedNormal(\n                   mean=0.5413248, stddev=0.1),  # mean=softplus_inverse(1.)\n               stddev_regularizer=None,\n               stddev_constraint=\'softplus\',\n               scale_factor=1.,\n               seed=None,\n               **kwargs):\n    super(TrainableNormalKLDivergenceStdDev, self).__init__(**kwargs)\n    self.mean = mean\n    self.stddev_initializer = tf.keras.initializers.get(stddev_initializer)\n    self.stddev_regularizer = get(stddev_regularizer)\n    self.stddev_constraint = constraints.get(stddev_constraint)\n    self.scale_factor = scale_factor\n\n  def build(self, input_shape):\n    self.stddev = self.add_weight(\n        \'stddev\',\n        shape=input_shape,\n        initializer=self.stddev_initializer,\n        regularizer=self.stddev_regularizer,\n        constraint=None,\n        dtype=self.dtype,\n        trainable=True)\n    self.built = True\n\n  def call(self, inputs):\n    """"""Computes regularization given an input ed.RandomVariable.""""""\n    if not isinstance(inputs, random_variable.RandomVariable):\n      raise ValueError(\'Input must be an ed.RandomVariable.\')\n    stddev = self.stddev\n    if self.stddev_constraint:\n      stddev = self.stddev_constraint(stddev)\n    prior = generated_random_variables.Independent(\n        generated_random_variables.Normal(\n            loc=self.mean, scale=stddev).distribution,\n        reinterpreted_batch_ndims=len(inputs.distribution.event_shape))\n    regularization = inputs.distribution.kl_divergence(prior.distribution)\n    return self.scale_factor * regularization\n\n  def get_config(self):\n    return {\n        \'loc\': self.loc,\n        \'stddev_initializer\':\n            tf.keras.initializers.serialize(self.stddev_initializer),\n        \'stddev_regularizer\': serialize(self.stddev_regularizer),\n        \'stddev_constraint\': constraints.serialize(self.stddev_constraint),\n        \'scale_factor\': self.scale_factor,\n        \'seed\': self.seed,\n    }\n\n\nclass UniformKLDivergence(tf.keras.regularizers.Regularizer):\n  """"""KL divergence regularizer from an input to a uniform distribution.\n\n  This regularizer computes the negative entropy of the input variable, which\n  yields a value that is proportional to the KL divergence up to an additive\n  constant. Assumes a uniform distribution over the support of the input random\n  variable.\n  """"""\n\n  def __init__(self, scale_factor=1.):\n    """"""Constructs regularizer.""""""\n    self.scale_factor = scale_factor\n\n  def __call__(self, x):\n    """"""Computes regularization given an input ed.RandomVariable.""""""\n    if not isinstance(x, random_variable.RandomVariable):\n      raise ValueError(\'Input must be an ed.RandomVariable.\')\n    return self.scale_factor * -x.distribution.entropy()\n\n  def get_config(self):\n    return {\n        \'scale_factor\': self.scale_factor,\n    }\n\n\n# Compatibility aliases, following tf.keras\n\n# pylint: disable=invalid-name\ncauchy_kl_divergence = CauchyKLDivergence\nhalf_cauchy_kl_divergence = HalfCauchyKLDivergence\nlog_normal_kl_divergence = LogNormalKLDivergence\nlog_uniform_kl_divergence = LogUniformKLDivergence\nnormal_kl_divergence = NormalKLDivergence\nnormal_empirical_bayes_kl_divergence = NormalEmpiricalBayesKLDivergence\ntrainable_normal_kl_divergence_stddev = TrainableNormalKLDivergenceStdDev\nuniform_kl_divergence = UniformKLDivergence\n# pylint: enable=invalid-name\n\n# Utility functions, following tf.keras\n\n\ndef serialize(initializer):\n  return tf.keras.utils.serialize_keras_object(initializer)\n\n\ndef deserialize(config, custom_objects=None):\n  return tf.keras.utils.deserialize_keras_object(\n      config,\n      module_objects=globals(),\n      custom_objects=custom_objects,\n      printable_module_name=\'regularizers\')\n\n\ndef get(identifier, value=None):\n  """"""Getter for loading from strings; falls back to Keras as needed.""""""\n  if value is None:\n    value = identifier\n  if identifier in (None, \'\'):\n    return None\n  elif isinstance(identifier, dict):\n    try:\n      return deserialize(identifier)\n    except ValueError:\n      pass\n  elif isinstance(identifier, str):\n    config = {\'class_name\': str(identifier), \'config\': {}}\n    try:\n      return deserialize(config)\n    except ValueError:\n      pass\n  elif callable(identifier):\n    return identifier\n  return tf.keras.regularizers.get(value)\n'"
edward2/tensorflow/regularizers_test.py,16,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for Keras-style regularizers.""""""\n\nimport itertools\nfrom absl.testing import parameterized\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass RegularizersTest(parameterized.TestCase, tf.test.TestCase):\n\n  def testCauchyKLDivergence(self):\n    shape = (3,)\n    regularizer = ed.regularizers.get(\'cauchy_kl_divergence\')\n    variational_posterior = ed.Independent(\n        ed.Normal(loc=tf.zeros(shape), scale=1.).distribution,\n        reinterpreted_batch_ndims=1)\n    kl = regularizer(variational_posterior)\n    # KL uses a single-sample estimate, which is not necessarily >0. We only\n    # check shape.\n    self.assertEqual(kl.shape, ())\n\n  def testHalfCauchyKLDivergence(self):\n    shape = (3,)\n    regularizer = ed.regularizers.get(\'half_cauchy_kl_divergence\')\n    variational_posterior = ed.Independent(\n        ed.LogNormal(loc=tf.zeros(shape), scale=1.).distribution,\n        reinterpreted_batch_ndims=1)\n    kl = regularizer(variational_posterior)\n    # KL uses a single-sample estimate, which is not necessarily >0. We only\n    # check shape.\n    self.assertEqual(kl.shape, ())\n\n  def testLogNormalKLDivergence(self):\n    shape = (3,)\n    regularizer = ed.regularizers.get(\'log_normal_kl_divergence\')\n    variational_posterior = ed.Independent(\n        ed.LogNormal(loc=tf.zeros(shape), scale=1.).distribution,\n        reinterpreted_batch_ndims=1)\n    kl = regularizer(variational_posterior)\n    self.assertGreaterEqual(kl, 0.)\n\n    dataset_size = 100\n    scale_factor = 1. / dataset_size\n    regularizer = ed.regularizers.LogNormalKLDivergence(\n        scale_factor=scale_factor)\n    scaled_kl = regularizer(variational_posterior)\n    self.assertEqual(scale_factor * kl, scaled_kl)\n\n  def testNormalKLDivergence(self):\n    shape = (3,)\n    regularizer = ed.regularizers.get(\'normal_kl_divergence\')\n    variational_posterior = ed.Independent(\n        ed.Normal(loc=tf.zeros(shape), scale=1.).distribution,\n        reinterpreted_batch_ndims=1)\n    kl = regularizer(variational_posterior)\n    self.assertGreaterEqual(kl, 0.)\n\n    dataset_size = 100\n    scale_factor = 1. / dataset_size\n    regularizer = ed.regularizers.NormalKLDivergence(scale_factor=scale_factor)\n    scaled_kl = regularizer(variational_posterior)\n    self.assertEqual(scale_factor * kl, scaled_kl)\n\n  @parameterized.parameters(\n      itertools.product(\n          [0.1, 1.0, 2.0],\n          [0.1, 1.0, 2.0]))\n  def testNormalEmpiricalBayesKLDivergence(self, gen_stddev, eb_prior_stddev):\n    """"""Tests ed.regularizers.NormalEmpiricalBayesKLDivergence.\n\n    Check that EB KL estimate should always be smaller but close to the true\n    generating Normal-InverseGamma KL due to it being explicitly optimized.\n\n    Args:\n      gen_stddev: Standard deviation of the generating normal distribution.\n      eb_prior_stddev: Standard deviation of the EB hyperprior.\n    """"""\n    tf.random.set_seed(89323)\n    shape = (99, 101)\n    gen_mean = 0.\n    eb_prior_mean = eb_prior_stddev**2\n    cvar = (eb_prior_mean / eb_prior_stddev)**2\n    variance_concentration = cvar + 2.\n    variance_scale = eb_prior_mean*(cvar + 1.)\n    weight = ed.Independent(\n        ed.Normal(gen_mean + tf.zeros(shape), gen_stddev).distribution,\n        reinterpreted_batch_ndims=len(shape))\n\n    # Compute KL(q(w)|| N(w|gen_stddev)) - log IG(gen_stddev**2) under a fixed\n    # setting of the prior stddev.\n    normal_regularizer = ed.regularizers.NormalKLDivergence(mean=gen_mean,\n                                                            stddev=gen_stddev)\n    kl = normal_regularizer(weight)\n    kl -= tf.reduce_sum(\n        ed.InverseGamma(variance_concentration,\n                        variance_scale).distribution.log_prob(gen_stddev**2))\n\n    eb_regularizer = ed.regularizers.NormalEmpiricalBayesKLDivergence(\n        mean=gen_mean,\n        variance_concentration=variance_concentration,\n        variance_scale=variance_scale)\n    eb_kl = eb_regularizer(weight)\n    # Normalize comparison by total number of weights. (Note this also scales\n    # the IG log prob.)\n    kl /= float(np.prod(shape))\n    eb_kl /= float(np.prod(shape))\n    self.assertGreaterEqual(kl, eb_kl)\n    self.assertAlmostEqual(kl.numpy(), eb_kl.numpy(), delta=0.05,\n                           msg=\'Parameters score KL=%.6f on generating \'\n                           \'Normal-IG KL and KL=%.6f on EB-fitted KL, \'\n                           \'too much difference.\' % (kl, eb_kl))\n\n  def testNormalEmpiricalBayesKLDivergenceTFFunction(self):\n    """"""Checks that KL evaluates properly multiple times when compiled.""""""\n    shape = (3,)\n    regularizer = ed.regularizers.get(\'normal_empirical_bayes_kl_divergence\')\n    regularizer_compiled = tf.function(regularizer)\n    weights_one = ed.Independent(\n        ed.Normal(loc=tf.zeros(shape), scale=1.).distribution,\n        reinterpreted_batch_ndims=len(shape))\n    kl_one = regularizer(weights_one).numpy()\n    kl_one_c = regularizer_compiled(weights_one).numpy()\n\n    weights_two = ed.Independent(\n        ed.Normal(loc=5. + tf.zeros(shape), scale=1.).distribution,\n        reinterpreted_batch_ndims=len(shape))\n    kl_two = regularizer(weights_two).numpy()\n    kl_two_c = regularizer_compiled(weights_two).numpy()\n\n    self.assertAllClose(kl_one, kl_one_c)\n    self.assertAllClose(kl_two, kl_two_c)\n    self.assertNotAlmostEqual(kl_one_c, kl_two_c)\n\n  def testTrainableNormalKLDivergenceStddev(self):\n    tf.random.set_seed(83271)\n    shape = (3,)\n    regularizer = ed.regularizers.get(\'trainable_normal_kl_divergence_stddev\')\n    variational_posterior = ed.Independent(\n        ed.Normal(loc=tf.zeros(shape), scale=1.).distribution,\n        reinterpreted_batch_ndims=1)\n    kl = regularizer(variational_posterior)\n    self.assertGreaterEqual(kl, 0.)\n\n    prior_stddev = regularizer.stddev_constraint(regularizer.stddev)\n    self.assertAllClose(prior_stddev, np.ones(prior_stddev.shape),\n                        atol=0.1)\n\n  def testUniformKLDivergence(self):\n    shape = (3,)\n    regularizer = ed.regularizers.get(\'uniform_kl_divergence\')\n    variational_posterior = ed.Independent(\n        ed.Normal(loc=tf.zeros(shape), scale=1.).distribution,\n        reinterpreted_batch_ndims=1)\n    kl = regularizer(variational_posterior)\n    self.assertNotEqual(kl, 0.)\n\n    dataset_size = 100\n    scale_factor = 1. / dataset_size\n    regularizer = ed.regularizers.UniformKLDivergence(scale_factor=scale_factor)\n    scaled_kl = regularizer(variational_posterior)\n    self.assertAlmostEqual(scale_factor * kl, scaled_kl)\n\n  def testRegularizersGet(self):\n    self.assertIsInstance(ed.regularizers.get(\'normal_kl_divergence\'),\n                          ed.regularizers.NormalKLDivergence)\n    self.assertIsInstance(ed.regularizers.get(\'l2\'), tf.keras.regularizers.L2)\n    self.assertIsNone(ed.regularizers.get(\'\'))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
edward2/tensorflow/transformed_random_variable.py,4,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Reversible layers.""""""\n\nfrom edward2 import trace\nfrom edward2.tensorflow import random_variable\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n\nclass TransformedDistribution(tfp.distributions.Distribution):\n  """"""Distribution of f(x), where x ~ p(x) and f is reversible.""""""\n\n  def __init__(self, base, reversible_layer, name=None):\n    """"""Constructs a transformed distribution.\n\n    Args:\n      base: Base distribution.\n      reversible_layer: Callable with methods `reverse` and `log_det_jacobian`.\n      name: Name for scoping operations in the class.\n    """"""\n    self.base = base\n    self.reversible_layer = reversible_layer\n    if name is None:\n      name = reversible_layer.name + base.name\n    super(TransformedDistribution, self).__init__(\n        base.dtype,\n        base.reparameterization_type,\n        base.validate_args,\n        base.allow_nan_stats,\n        parameters=dict(locals()),\n        name=name)\n\n  def _event_shape_tensor(self):\n    return self.base.event_shape_tensor()\n\n  def _event_shape(self):\n    return self.base.event_shape\n\n  def _batch_shape_tensor(self):\n    return self.base.batch_shape_tensor()\n\n  def _batch_shape(self):\n    return self.base.batch_shape\n\n  def __getitem__(self, slices):\n    overrides = {\'base\': self.base[slices]}\n    return self.copy(**overrides)\n\n  def _call_sample_n(self, sample_shape, seed, name, **kwargs):\n    x = self.base.sample(sample_shape, seed, **kwargs)\n    y = self.reversible_layer(x)\n    return y\n\n  def _log_prob(self, value):\n    x = self.reversible_layer.reverse(value)\n    log_det_jacobian = self.reversible_layer.log_det_jacobian(value)\n    return self.base.log_prob(x) + log_det_jacobian\n\n  def _prob(self, value):\n    if not hasattr(self.base, \'_prob\'):\n      return tf.exp(self.log_prob(value))\n    x = self.reversible_layer.reverse(value)\n    log_det_jacobian = self.reversible_layer.log_det_jacobian(value)\n    return self.base.prob(x) * tf.exp(log_det_jacobian)\n\n  def _log_cdf(self, value):\n    x = self.reversible_layer.reverse(value)\n    return self.base.log_cdf(x)\n\n  def _cdf(self, value):\n    x = self.reversible_layer.reverse(value)\n    return self.base.cdf(x)\n\n  def _log_survival_function(self, value):\n    x = self.reversible_layer.reverse(value)\n    return self.base.log_survival_function(x)\n\n  def _survival_function(self, value):\n    x = self.reversible_layer.reverse(value)\n    return self.base.survival_function(x)\n\n  def _quantile(self, value):\n    inverse_cdf = self.base.quantile(value)\n    return self.reversible_layer(inverse_cdf)\n\n  def _entropy(self):\n    dummy = tf.zeros(\n        tf.concat([self.batch_shape_tensor(), self.event_shape_tensor()], 0),\n        dtype=self.dtype)\n    log_det_jacobian = self.reversible_layer.log_det_jacobian(dummy)\n    entropy = self.base.entropy() - log_det_jacobian\n    return entropy\n\n\n@trace.traceable\ndef TransformedRandomVariable(rv,  # pylint: disable=invalid-name\n                              reversible_layer,\n                              name=None,\n                              sample_shape=(),\n                              value=None):\n  """"""Random variable for f(x), where x ~ p(x) and f is reversible.""""""\n  return random_variable.RandomVariable(\n      distribution=TransformedDistribution(rv.distribution,\n                                           reversible_layer,\n                                           name=name),\n      sample_shape=sample_shape,\n      value=value)\n'"
edward2/tensorflow/transformed_random_variable_test.py,6,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for transformed random variables.""""""\n\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass TransformedRandomVariableTest(tf.test.TestCase):\n\n  def testTransformedRandomVariable(self):\n    class Exp(tf.keras.layers.Layer):\n      """"""Exponential activation function for reversible networks.""""""\n\n      def __call__(self, inputs, *args, **kwargs):\n        if not isinstance(inputs, ed.RandomVariable):\n          return super(Exp, self).__call__(inputs, *args, **kwargs)\n        return ed.TransformedRandomVariable(inputs, self)\n\n      def call(self, inputs):\n        return tf.exp(inputs)\n\n      def reverse(self, inputs):\n        return tf.math.log(inputs)\n\n      def log_det_jacobian(self, inputs):\n        return -tf.math.log(inputs)\n\n    x = ed.Normal(0., 1.)\n    y = Exp()(x)\n    y_sample = y.distribution.sample()\n    y_log_prob = y.distribution.log_prob(y_sample)\n    self.assertGreater(y_sample, 0.)\n    self.assertTrue(np.isfinite(y_log_prob))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
examples/no_u_turn_sampler/__init__.py,0,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""No-U-Turn Sampler.""""""\n\nfrom .nuts import kernel  # local file import\n\nfrom tensorflow.python.util.all_util import remove_undocumented\n\n_allowed_symbols = [\n    ""kernel"",\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
examples/no_u_turn_sampler/logistic_regression.py,12,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Trains a Bayesian logistic regression model using No-U-Turn Sampler.""""""\n\nimport cProfile\nimport functools\nimport os\nimport pstats\nimport time\n\nfrom absl import app\nfrom absl import flags\nimport edward2 as ed\nimport no_u_turn_sampler  # local file import\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt  # pylint: disable=g-import-not-at-top\nimport numpy as np\nimport sklearn.datasets\nimport tensorflow as tf\n\nflags.DEFINE_integer(""max_steps"",\n                     default=5,\n                     help=""Number of training steps to run."")\nflags.DEFINE_string(""model_dir"",\n                    default=os.path.join(os.getenv(""TEST_TMPDIR"", ""/tmp""),\n                                         ""logistic_regression/""),\n                    help=""Path to write plots to."")\nflags.DEFINE_bool(""fake_data"",\n                  default=None,\n                  help=""If True, uses fake data. Defaults to real data."")\n\nFLAGS = flags.FLAGS\n\n\ndef profile(func):\n  """"""Decorator for profiling the execution of a function.""""""\n  @functools.wraps(func)\n  def func_wrapped(*args, **kwargs):\n    """"""Function which wraps original function with start/stop profiling.""""""\n    pr = cProfile.Profile()\n    pr.enable()\n    start = time.time()\n    output = func(*args, **kwargs)\n    print(""Elapsed"", time.time() - start)\n    pr.disable()\n    ps = pstats.Stats(pr).sort_stats(""cumulative"")\n    ps.print_stats()\n    return output\n  return func_wrapped\n\n\ndef logistic_regression(features):\n  """"""Bayesian logistic regression, which returns labels given features.""""""\n  coeffs = ed.MultivariateNormalDiag(\n      loc=tf.zeros(features.shape[1]), name=""coeffs"")\n  labels = ed.Bernoulli(\n      logits=tf.tensordot(features, coeffs, [[1], [0]]), name=""labels"")\n  return labels\n\n\ndef covertype():\n  """"""Builds the Covertype data set.""""""\n  data = sklearn.datasets.fetch_covtype()\n  features = data.data  # pytype: disable=attribute-error\n  labels = data.target  # pytype: disable=attribute-error\n\n  # Normalize features and append a column of ones for the intercept.\n  features -= features.mean(0)\n  features /= features.std(0)\n  features = np.hstack([features, np.ones([features.shape[0], 1])])\n  features = tf.cast(features, dtype=tf.float32)\n\n  # Binarize outcomes on whether it is a specific category.\n  _, counts = np.unique(labels, return_counts=True)\n  specific_category = np.argmax(counts)\n  labels = (labels == specific_category)\n  labels = tf.cast(labels, dtype=tf.int32)\n  return features, labels\n\n\ndef main(argv):\n  del argv  # unused\n  if tf.io.gfile.isdir(FLAGS.model_dir):\n    tf.logging.warning(\n        ""Warning: deleting old log directory at {}"".format(FLAGS.model_dir))\n    tf.io.gfile.rmtree(FLAGS.model_dir)\n  tf.io.gfile.makedirs(FLAGS.model_dir)\n\n  print(""GPU(s) available"", tf.test.is_gpu_available())\n\n  if FLAGS.fake_data:\n    features = tf.random.normal([20, 55])\n    labels = tf.random.uniform([20], minval=0, maxval=2, dtype=tf.int32)\n  else:\n    features, labels = covertype()\n  print(""Data set size"", features.shape[0])\n  print(""Number of features"", features.shape[1])\n\n  log_joint = ed.make_log_joint_fn(logistic_regression)\n  def target_log_prob_fn(coeffs):\n    return log_joint(features=features, coeffs=coeffs, labels=labels)\n\n  # Initialize using a sample from 20 steps of NUTS. It is roughly a MAP\n  # estimate and is written explicitly to avoid differences in warm-starts\n  # between different implementations (e.g., Stan, PyMC3).\n  coeffs = tf.constant(\n      [+2.03420663e+00, -3.53567265e-02, -1.49223924e-01, -3.07049364e-01,\n       -1.00028366e-01, -1.46827862e-01, -1.64167881e-01, -4.20344204e-01,\n       +9.47479829e-02, -1.12681836e-02, +2.64442056e-01, -1.22087866e-01,\n       -6.00568838e-02, -3.79419506e-01, -1.06668741e-01, -2.97053963e-01,\n       -2.05253899e-01, -4.69537191e-02, -2.78072730e-02, -1.43250525e-01,\n       -6.77954629e-02, -4.34899796e-03, +5.90927452e-02, +7.23133609e-02,\n       +1.38526391e-02, -1.24497898e-01, -1.50733739e-02, -2.68872194e-02,\n       -1.80925727e-02, +3.47936489e-02, +4.03552800e-02, -9.98773426e-03,\n       +6.20188080e-02, +1.15002751e-01, +1.32145107e-01, +2.69109547e-01,\n       +2.45785132e-01, +1.19035013e-01, -2.59744357e-02, +9.94279515e-04,\n       +3.39266285e-02, -1.44057125e-02, -6.95222765e-02, -7.52013028e-02,\n       +1.21171586e-01, +2.29205526e-02, +1.47308692e-01, -8.34354162e-02,\n       -9.34122875e-02, -2.97472421e-02, -3.03937674e-01, -1.70958012e-01,\n       -1.59496680e-01, -1.88516974e-01, -1.20889175e+00])\n\n  # Initialize step size via result of 50 warmup steps from Stan.\n  step_size = 0.00167132\n\n  kernel = profile(no_u_turn_sampler.kernel)\n  coeffs_samples = []\n  target_log_prob = None\n  grads_target_log_prob = None\n  for step in range(FLAGS.max_steps):\n    print(""Step"", step)\n    [\n        [coeffs],\n        target_log_prob,\n        grads_target_log_prob,\n    ] = kernel(target_log_prob_fn=target_log_prob_fn,\n               current_state=[coeffs],\n               step_size=[step_size],\n               seed=step,\n               current_target_log_prob=target_log_prob,\n               current_grads_target_log_prob=grads_target_log_prob)\n    coeffs_samples.append(coeffs)\n\n  for coeffs_sample in coeffs_samples:\n    plt.plot(coeffs_sample.numpy())\n\n  filename = os.path.join(FLAGS.model_dir, ""coeffs_samples.png"")\n  plt.savefig(filename)\n  print(""Figure saved as"", filename)\n  plt.close()\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
examples/no_u_turn_sampler/nuts.py,22,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""No U-Turn Sampler via an Eager-only single-chain implementation.\n\nThe implementation uses minimal abstractions and data structures: it applies\nPython callables, lists, and Tensors. It closely follows [1; Algorithm 3] in\nthat there exists a ""build tree"" function that recursively builds the No-U-Turn\nSampler trajectory. The path length is set adaptively; the step size is fixed.\n\nFuture work may abstract this code as part of a Markov chain Monte Carlo\nlibrary.\n\n#### References\n\n[1]: Matthew D. Hoffman, Andrew Gelman. The No-U-Turn Sampler: Adaptively\n     Setting Path Lengths in Hamiltonian Monte Carlo.\n     In _Journal of Machine Learning Research_, 15(1):1593-1623, 2014.\n     http://jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf\n""""""\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n\ndef kernel(target_log_prob_fn,\n           current_state,\n           step_size,\n           seed=None,\n           current_target_log_prob=None,\n           current_grads_target_log_prob=None,\n           name=""nuts_kernel""):\n  """"""Simulates a No-U-Turn Sampler (NUTS) trajectory.\n\n  Args:\n    target_log_prob_fn: Python callable which takes an argument like\n      `*current_state` and returns its (possibly unnormalized) log-density under\n      the target distribution.\n    current_state: List of `Tensor`s representing the states to simulate from.\n    step_size: List of `Tensor`s representing the step sizes for the leapfrog\n      integrator. Must have same shape as `current_state`.\n    seed: Integer to seed the random number generator.\n    current_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at the `current_state`.\n    current_grads_target_log_prob: List of `Tensor`s representing gradient of\n      `current_target_log_prob` with respect to `current_state`. Must have same\n      shape as `current_state`.\n    name: A name for the operation.\n\n  Returns:\n    next_state: List of `Tensor`s representing the next states of the NUTS\n      trajectory. Has same shape as `current_state`.\n    next_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at `next_state`.\n    next_grads_target_log_prob: List of `Tensor`s representing the gradient of\n      `next_target_log_prob` with respect to `next_state`.\n\n  Raises:\n    NotImplementedError: If the execution mode is not eager.\n  """"""\n  if not tf.executing_eagerly():\n    raise NotImplementedError(""`kernel` is only available in Eager mode."")\n\n  with tf.name_scope(name):\n    with tf.name_scope(""initialize""):\n      current_state = [tf.convert_to_tensor(s) for s in current_state]\n      step_size = [tf.convert_to_tensor(s) for s in step_size]\n      if (current_target_log_prob is None or\n          current_grads_target_log_prob is None):\n        with tf.GradientTape() as tape:\n          tape.watch(current_state)\n          current_target_log_prob = target_log_prob_fn(*current_state)\n        current_grads_target_log_prob = tape.gradient(current_target_log_prob,\n                                                      current_state)\n        if any(grad is None for grad in current_grads_target_log_prob):\n          raise ValueError(""Gradient is None for a state."")\n\n      seed_stream = tfp.util.SeedStream(seed, ""nuts_kernel"")\n      current_momentum = []\n      for state_tensor in current_state:\n        momentum_tensor = tf.random.normal(shape=tf.shape(state_tensor),\n                                           dtype=state_tensor.dtype,\n                                           seed=seed_stream())\n        current_momentum.append(momentum_tensor)\n\n      # Draw a slice variable u ~ Uniform(0, p(initial state, initial\n      # momentum)) and compute log u. For numerical stability, we perform this\n      # in log space where log u = log (u\' * p(...)) = log u\' + log\n      # p(...) and u\' ~ Uniform(0, 1).\n      log_slice_sample = tf.math.log(tf.random.uniform([], seed=seed_stream()))\n      log_slice_sample += _log_joint(current_target_log_prob,\n                                     current_momentum)\n\n      # Initialize loop variables. It comprises a collection of information\n      # about a ""reverse"" state, a collection of information about a ""forward""\n      # state, a collection of information about the next state,\n      # the trajectory\'s tree depth, the number of candidate states, and\n      # whether to continue the trajectory.\n      reverse_state = current_state\n      reverse_target_log_prob = current_target_log_prob\n      reverse_grads_target_log_prob = current_grads_target_log_prob\n      reverse_momentum = current_momentum\n      forward_state = current_state\n      forward_target_log_prob = current_target_log_prob\n      forward_grads_target_log_prob = current_grads_target_log_prob\n      forward_momentum = current_momentum\n      next_state = current_state\n      next_target_log_prob = current_target_log_prob\n      next_grads_target_log_prob = current_grads_target_log_prob\n      depth = 0\n      num_states = 1\n      continue_trajectory = True\n\n    while continue_trajectory:\n      # Grow the No-U-Turn Sampler trajectory by choosing a random direction and\n      # simulating Hamiltonian dynamics in that direction. This extends either\n      # the forward or reverse state.\n      direction = tfp.math.random_rademacher([], seed=seed_stream())\n      if direction < 0:\n        [\n            reverse_state,\n            reverse_target_log_prob,\n            reverse_grads_target_log_prob,\n            reverse_momentum,\n            _,\n            _,\n            _,\n            _,\n            next_state_in_subtree,\n            next_target_log_prob_in_subtree,\n            next_grads_target_log_prob_in_subtree,\n            num_states_in_subtree,\n            continue_trajectory,\n        ] = _build_tree(\n            target_log_prob_fn=target_log_prob_fn,\n            current_state=reverse_state,\n            current_target_log_prob=reverse_target_log_prob,\n            current_grads_target_log_prob=reverse_grads_target_log_prob,\n            current_momentum=reverse_momentum,\n            direction=direction,\n            depth=depth,\n            step_size=step_size,\n            log_slice_sample=log_slice_sample,\n            seed=seed_stream())\n      else:\n        [\n            _,\n            _,\n            _,\n            _,\n            forward_state,\n            forward_target_log_prob,\n            forward_grads_target_log_prob,\n            forward_momentum,\n            next_state_in_subtree,\n            next_target_log_prob_in_subtree,\n            next_grads_target_log_prob_in_subtree,\n            num_states_in_subtree,\n            continue_trajectory,\n        ] = _build_tree(\n            target_log_prob_fn=target_log_prob_fn,\n            current_state=forward_state,\n            current_target_log_prob=forward_target_log_prob,\n            current_grads_target_log_prob=forward_grads_target_log_prob,\n            current_momentum=forward_momentum,\n            direction=direction,\n            depth=depth,\n            step_size=step_size,\n            log_slice_sample=log_slice_sample,\n            seed=seed_stream())\n\n      if continue_trajectory:\n        # If the built tree did not terminate, accept the tree\'s next state\n        # with a certain probability.\n        accept_state_in_subtree = _random_bernoulli(\n            [],\n            probs=tf.minimum(1., num_states_in_subtree / num_states),\n            dtype=tf.bool,\n            seed=seed_stream())\n        if accept_state_in_subtree:\n          next_state = next_state_in_subtree\n          next_target_log_prob = next_target_log_prob_in_subtree\n          next_grads_target_log_prob = next_grads_target_log_prob_in_subtree\n\n      # Continue the NUTS trajectory if the tree-building did not terminate, and\n      # if the reverse-most and forward-most states do not exhibit a U-turn.\n      has_no_u_turn = tf.logical_and(\n          _has_no_u_turn(forward_state, reverse_state, forward_momentum),\n          _has_no_u_turn(forward_state, reverse_state, reverse_momentum))\n      continue_trajectory = continue_trajectory and has_no_u_turn\n      num_states += num_states_in_subtree\n      depth += 1\n\n    return next_state, next_target_log_prob, next_grads_target_log_prob\n\n\ndef _build_tree(target_log_prob_fn,\n                current_state,\n                current_target_log_prob,\n                current_grads_target_log_prob,\n                current_momentum,\n                direction,\n                depth,\n                step_size,\n                log_slice_sample,\n                max_simulation_error=1000.,\n                seed=None):\n  """"""Builds a tree at a given tree depth and at a given state.\n\n  The `current` state is immediately adjacent to, but outside of,\n  the subtrajectory spanned by the returned `forward` and `reverse` states.\n\n  Args:\n    target_log_prob_fn: Python callable which takes an argument like\n      `*current_state` and returns its (possibly unnormalized) log-density under\n      the target distribution.\n    current_state: List of `Tensor`s representing the current states of the\n      NUTS trajectory.\n    current_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at the `current_state`.\n    current_grads_target_log_prob: List of `Tensor`s representing gradient of\n      `current_target_log_prob` with respect to `current_state`. Must have same\n      shape as `current_state`.\n    current_momentum: List of `Tensor`s representing the momentums of\n      `current_state`. Must have same shape as `current_state`.\n    direction: int that is either -1 or 1. It determines whether to perform\n      leapfrog integration backwards (reverse) or forward in time respectively.\n    depth: non-negative int that indicates how deep of a tree to build.\n      Each call to `_build_tree` takes `2**depth` leapfrog steps.\n    step_size: List of `Tensor`s representing the step sizes for the leapfrog\n      integrator. Must have same shape as `current_state`.\n    log_slice_sample: The log of an auxiliary slice variable. It is used\n      together with `max_simulation_error` to avoid simulating trajectories with\n      too much numerical error.\n    max_simulation_error: Maximum simulation error to tolerate before\n      terminating the trajectory. Simulation error is the\n      `log_slice_sample` minus the log-joint probability at the simulated state.\n    seed: Integer to seed the random number generator.\n\n  Returns:\n    reverse_state: List of `Tensor`s representing the ""reverse"" states of the\n      NUTS trajectory. Has same shape as `current_state`.\n    reverse_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at the `reverse_state`.\n    reverse_grads_target_log_prob: List of `Tensor`s representing gradient of\n      `reverse_target_log_prob` with respect to `reverse_state`. Has same shape\n      as `reverse_state`.\n    reverse_momentum: List of `Tensor`s representing the momentums of\n      `reverse_state`. Has same shape as `reverse_state`.\n    forward_state: List of `Tensor`s representing the ""forward"" states of the\n      NUTS trajectory. Has same shape as `current_state`.\n    forward_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at the `forward_state`.\n    forward_grads_target_log_prob: List of `Tensor`s representing gradient of\n      `forward_target_log_prob` with respect to `forward_state`. Has same shape\n      as `forward_state`.\n    forward_momentum: List of `Tensor`s representing the momentums of\n      `forward_state`. Has same shape as `forward_state`.\n    next_state: List of `Tensor`s representing the next states of the NUTS\n      trajectory. Has same shape as `current_state`.\n    next_target_log_prob: Scalar `Tensor` representing the value of\n      `target_log_prob_fn` at `next_state`.\n    next_grads_target_log_prob: List of `Tensor`s representing the gradient of\n      `next_target_log_prob` with respect to `next_state`.\n    num_states: Number of acceptable candidate states in the subtree. A state is\n      acceptable if it is ""in the slice"", that is, if its log-joint probability\n      with its momentum is greater than `log_slice_sample`.\n    continue_trajectory: bool determining whether to continue the simulation\n      trajectory. The trajectory is continued if no U-turns are encountered\n      within the built subtree, and if the log-probability accumulation due to\n      integration error does not exceed `max_simulation_error`.\n  """"""\n  if depth == 0:  # base case\n    # Take a leapfrog step. Terminate the tree-building if the simulation\n    # error from the leapfrog integrator is too large. States discovered by\n    # continuing the simulation are likely to have very low probability.\n    [\n        next_state,\n        next_target_log_prob,\n        next_grads_target_log_prob,\n        next_momentum,\n    ] = _leapfrog(\n        target_log_prob_fn=target_log_prob_fn,\n        current_state=current_state,\n        current_grads_target_log_prob=current_grads_target_log_prob,\n        current_momentum=current_momentum,\n        step_size=direction * step_size)\n    next_log_joint = _log_joint(next_target_log_prob, next_momentum)\n    num_states = tf.cast(next_log_joint > log_slice_sample, dtype=tf.int32)\n    continue_trajectory = (next_log_joint >\n                           log_slice_sample - max_simulation_error)\n    return [\n        next_state,\n        next_target_log_prob,\n        next_grads_target_log_prob,\n        next_momentum,\n        next_state,\n        next_target_log_prob,\n        next_grads_target_log_prob,\n        next_momentum,\n        next_state,\n        next_target_log_prob,\n        next_grads_target_log_prob,\n        num_states,\n        continue_trajectory,\n    ]\n\n  # Build a tree at the current state.\n  seed_stream = tfp.util.SeedStream(seed, ""build_tree"")\n  [\n      reverse_state,\n      reverse_target_log_prob,\n      reverse_grads_target_log_prob,\n      reverse_momentum,\n      forward_state,\n      forward_target_log_prob,\n      forward_grads_target_log_prob,\n      forward_momentum,\n      next_state,\n      next_target_log_prob,\n      next_grads_target_log_prob,\n      num_states,\n      continue_trajectory,\n  ] = _build_tree(target_log_prob_fn=target_log_prob_fn,\n                  current_state=current_state,\n                  current_target_log_prob=current_target_log_prob,\n                  current_grads_target_log_prob=current_grads_target_log_prob,\n                  current_momentum=current_momentum,\n                  direction=direction,\n                  depth=depth - 1,\n                  step_size=step_size,\n                  log_slice_sample=log_slice_sample,\n                  seed=seed_stream())\n  if continue_trajectory:\n    # If the just-built subtree did not terminate, build a second subtree at\n    # the forward or reverse state, as appropriate.\n    if direction < 0:\n      [\n          reverse_state,\n          reverse_target_log_prob,\n          reverse_grads_target_log_prob,\n          reverse_momentum,\n          _,\n          _,\n          _,\n          _,\n          far_state,\n          far_target_log_prob,\n          far_grads_target_log_prob,\n          far_num_states,\n          far_continue_trajectory,\n      ] = _build_tree(\n          target_log_prob_fn=target_log_prob_fn,\n          current_state=reverse_state,\n          current_target_log_prob=reverse_target_log_prob,\n          current_grads_target_log_prob=reverse_grads_target_log_prob,\n          current_momentum=reverse_momentum,\n          direction=direction,\n          depth=depth - 1,\n          step_size=step_size,\n          log_slice_sample=log_slice_sample,\n          seed=seed_stream())\n    else:\n      [\n          _,\n          _,\n          _,\n          _,\n          forward_state,\n          forward_target_log_prob,\n          forward_grads_target_log_prob,\n          forward_momentum,\n          far_state,\n          far_target_log_prob,\n          far_grads_target_log_prob,\n          far_num_states,\n          far_continue_trajectory,\n      ] = _build_tree(\n          target_log_prob_fn=target_log_prob_fn,\n          current_state=forward_state,\n          current_target_log_prob=forward_target_log_prob,\n          current_grads_target_log_prob=forward_grads_target_log_prob,\n          current_momentum=forward_momentum,\n          direction=direction,\n          depth=depth - 1,\n          step_size=step_size,\n          log_slice_sample=log_slice_sample,\n          seed=seed_stream())\n\n    # Propose either `next_state` (which came from the first subtree and so is\n    # nearby) or the new forward/reverse state (which came from the second\n    # subtree and so is far away).\n    num_states += far_num_states\n    accept_far_state = _random_bernoulli(\n        [],\n        probs=far_num_states / num_states,\n        dtype=tf.bool,\n        seed=seed_stream())\n    if accept_far_state:\n      next_state = far_state\n      next_target_log_prob = far_target_log_prob\n      next_grads_target_log_prob = far_grads_target_log_prob\n\n    # Continue the NUTS trajectory if the far subtree did not terminate either,\n    # and if the reverse-most and forward-most states do not exhibit a U-turn.\n    has_no_u_turn = tf.logical_and(\n        _has_no_u_turn(forward_state, reverse_state, forward_momentum),\n        _has_no_u_turn(forward_state, reverse_state, reverse_momentum))\n    continue_trajectory = far_continue_trajectory and has_no_u_turn\n\n  return [\n      reverse_state,\n      reverse_target_log_prob,\n      reverse_grads_target_log_prob,\n      reverse_momentum,\n      forward_state,\n      forward_target_log_prob,\n      forward_grads_target_log_prob,\n      forward_momentum,\n      next_state,\n      next_target_log_prob,\n      next_grads_target_log_prob,\n      num_states,\n      continue_trajectory,\n  ]\n\n\ndef _has_no_u_turn(state_one, state_two, momentum):\n  """"""If two given states and momentum do not exhibit a U-turn pattern.""""""\n  dot_product = sum([tf.reduce_sum((s1 - s2) * m)\n                     for s1, s2, m in zip(state_one, state_two, momentum)])\n  return dot_product > 0\n\n\ndef _leapfrog(target_log_prob_fn,\n              current_state,\n              current_grads_target_log_prob,\n              current_momentum,\n              step_size):\n  """"""Runs one step of leapfrog integration.""""""\n  mid_momentum = [\n      m + 0.5 * step * g for m, step, g in\n      zip(current_momentum, step_size, current_grads_target_log_prob)]\n  next_state = [\n      s + step * m for s, step, m in\n      zip(current_state, step_size, mid_momentum)]\n  with tf.GradientTape() as tape:\n    tape.watch(next_state)\n    next_target_log_prob = target_log_prob_fn(*next_state)\n  next_grads_target_log_prob = tape.gradient(next_target_log_prob,\n                                             next_state)\n  if any(grad is None for grad in next_grads_target_log_prob):\n    raise ValueError(""Gradient is None for a state."")\n  next_momentum = [\n      m + 0.5 * step * g for m, step, g in\n      zip(mid_momentum, step_size, next_grads_target_log_prob)]\n  return [\n      next_state,\n      next_target_log_prob,\n      next_grads_target_log_prob,\n      next_momentum,\n  ]\n\n\ndef _log_joint(current_target_log_prob, current_momentum):\n  """"""Log-joint probability given a state\'s log-probability and momentum.""""""\n  momentum_log_prob = -sum([tf.reduce_sum(0.5 * (m ** 2.))\n                            for m in current_momentum])\n  return current_target_log_prob + momentum_log_prob\n\n\ndef _random_bernoulli(shape,\n                      probs,\n                      dtype=tf.int32,\n                      seed=None,\n                      name=""random_bernoulli""):\n  """"""Returns samples from a Bernoulli distribution.""""""\n  with tf.name_scope(name):\n    probs = tf.convert_to_tensor(probs)\n    random_uniform = tf.random.uniform(shape, dtype=probs.dtype, seed=seed)\n    return tf.cast(tf.less(random_uniform, probs), dtype)\n'"
examples/no_u_turn_sampler/nuts_test.py,19,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests of the No U-Turn Sampler.\n\nThe majority of the tests are based on visually checking plots. For now,\nwe only test that the plots have no runtime errors (that is, they type-check).\n""""""\n\nimport os\nfrom absl.testing import parameterized\nimport no_u_turn_sampler  # local file import\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt  # pylint: disable=g-import-not-at-top\nimport numpy as np\nimport scipy.stats\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\ntfb = tfp.bijectors\ntfd = tfp.distributions\n\n\ndef plot_with_expectation(samples, dist, model_dir, suffix):\n  """"""Comparison histogram of samples and a line for the expected density.""""""\n  _, bins, _ = plt.hist(samples, bins=30, label=""observed"")\n  bin_width = bins[1] - bins[0]\n  xs = np.linspace(bins[0], bins[-1], 500)\n  pdfs = dist.pdf(xs) * bin_width * len(samples)\n  plt.plot(xs, pdfs, label=""analytic"")\n  plt.legend()\n  ks_stat, pval = scipy.stats.kstest(sorted(samples), dist.cdf)\n  plt.title(""K-S stat: {}\\np-value: {}"".format(ks_stat, pval))\n  savefig(model_dir, suffix)\n  plt.close()\n\n\ndef savefig(model_dir, suffix):\n  """"""Saves figure given suffix.""""""\n  filename = os.path.join(model_dir, suffix)\n  plt.savefig(filename)\n  print(""Figure saved in"", filename)\n\n\nclass NutsTest(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    super(NutsTest, self).setUp()\n    self.model_dir = os.path.join(os.getenv(""TEST_TMPDIR"", ""/tmp""), ""test/"")\n    tf.io.gfile.makedirs(self.model_dir)\n\n  def testOneStepFromOrigin(self):\n    def target_log_prob_fn(event):\n      return tfd.Normal(loc=0., scale=1.).log_prob(event)\n\n    samples = []\n    for seed in range(10):\n      [state], _, _ = no_u_turn_sampler.kernel(\n          target_log_prob_fn=target_log_prob_fn,\n          current_state=[0.],\n          step_size=[0.3],\n          seed=seed)\n      samples.append(state)\n\n    samples = np.array(samples)\n    plt.hist(samples, bins=30)\n    savefig(self.model_dir, ""one_step_from_origin.png"")\n    plt.close()\n\n  def testReproducibility(self):\n    def target_log_prob_fn(event):\n      return tfd.Normal(loc=0., scale=1.).log_prob(event)\n\n    tf.random.set_seed(4)\n    xs = no_u_turn_sampler.kernel(\n        target_log_prob_fn=target_log_prob_fn,\n        current_state=[0.],\n        step_size=[0.3],\n        seed=3)\n    tf.random.set_seed(4)\n    ys = no_u_turn_sampler.kernel(\n        target_log_prob_fn=target_log_prob_fn,\n        current_state=[0.],\n        step_size=[0.3],\n        seed=3)\n    for x, y in zip(xs, ys):\n      self.assertAllEqual(x, y)\n\n  def testNormal(self):\n    def target_log_prob_fn(event):\n      return tfd.Normal(loc=0., scale=1.).log_prob(event)\n\n    rng = np.random.RandomState(seed=7)\n    states = tf.cast(rng.normal(size=10), dtype=tf.float32)\n    tf.random.set_seed(2)\n    samples = []\n    for seed, state in enumerate(states):\n      [state], _, _ = no_u_turn_sampler.kernel(\n          target_log_prob_fn=target_log_prob_fn,\n          current_state=[state],\n          step_size=[0.3],\n          seed=seed)\n      samples.append(state)\n\n    samples = np.array(samples)\n    plot_with_expectation(samples,\n                          dist=scipy.stats.norm(0, 1),\n                          model_dir=self.model_dir,\n                          suffix=""one_step_posterior_conservation_normal.png"")\n\n  def testLogitBeta(self):\n    def target_log_prob_fn(event):\n      return tfd.TransformedDistribution(\n          distribution=tfd.Beta(concentration0=1.0, concentration1=3.0),\n          bijector=tfb.Invert(tfb.Sigmoid())).log_prob(event)\n\n    states = tfd.TransformedDistribution(\n        distribution=tfd.Beta(concentration0=1.0, concentration1=3.0),\n        bijector=tfb.Invert(tfb.Sigmoid())).sample(10, seed=7)\n    plt.hist(states.numpy(), bins=30)\n    savefig(self.model_dir, ""logit_beta_start_positions.png"")\n    plt.close()\n\n    samples = []\n    for seed, state in enumerate(states):\n      [state], _, _ = no_u_turn_sampler.kernel(\n          target_log_prob_fn=target_log_prob_fn,\n          current_state=[state],\n          step_size=[0.3],\n          seed=seed)\n      samples.append(state)\n\n    samples = np.array(samples)\n    plt.hist(samples, bins=30)\n    savefig(self.model_dir, ""one_step_logit_beta_posterior_conservation.png"")\n    plt.close()\n\n    _ = scipy.stats.ks_2samp(samples.flatten(), states.numpy().flatten())\n\n  def testMultivariateNormal2d(self):\n    def target_log_prob_fn(event):\n      return tfd.MultivariateNormalFullCovariance(\n          loc=tf.zeros(2), covariance_matrix=tf.eye(2)).log_prob(event)\n\n    rng = np.random.RandomState(seed=7)\n    states = tf.cast(rng.normal(size=[10, 2]), dtype=tf.float32)\n    samples = []\n    for seed, state in enumerate(states):\n      [state], _, _ = no_u_turn_sampler.kernel(\n          target_log_prob_fn=target_log_prob_fn,\n          current_state=[state],\n          step_size=[0.3],\n          seed=seed)\n      samples.append(state)\n\n    samples = tf.stack(samples).numpy()\n    plt.scatter(samples[:, 0], samples[:, 1])\n    savefig(self.model_dir, ""one_step_posterior_conservation_2d.png"")\n    plt.close()\n    plot_with_expectation(samples[:, 0],\n                          dist=scipy.stats.norm(0, 1),\n                          model_dir=self.model_dir,\n                          suffix=""one_step_posterior_conservation_2d_dim_0.png"")\n    plot_with_expectation(samples[:, 1],\n                          dist=scipy.stats.norm(0, 1),\n                          model_dir=self.model_dir,\n                          suffix=""one_step_posterior_conservation_2d_dim_1.png"")\n\n  def testSkewedMultivariateNormal2d(self):\n    def target_log_prob_fn(event):\n      return tfd.MultivariateNormalFullCovariance(\n          loc=tf.zeros(2),\n          covariance_matrix=tf.linalg.diag([1., 10.])).log_prob(event)\n\n    rng = np.random.RandomState(seed=7)\n    states = tf.cast(rng.normal(scale=[1.0, 10.0], size=[10, 2]), tf.float32)\n    plt.scatter(states[:, 0], states[:, 1])\n    savefig(self.model_dir, ""skewed_start_positions_2d.png"")\n    plt.close()\n\n    samples = []\n    for seed, state in enumerate(states):\n      [state], _, _ = no_u_turn_sampler.kernel(\n          target_log_prob_fn=target_log_prob_fn,\n          current_state=[state],\n          step_size=[0.3],\n          seed=seed)\n      samples.append(state)\n\n    samples = tf.stack(samples).numpy()\n    plt.scatter(samples[:, 0], samples[:, 1])\n    savefig(self.model_dir, ""one_step_skewed_posterior_conservation_2d.png"")\n    plt.close()\n    plot_with_expectation(\n        samples[:, 0],\n        dist=scipy.stats.norm(0, 1),\n        model_dir=self.model_dir,\n        suffix=""one_step_skewed_posterior_conservation_2d_dim_0.png"")\n    plot_with_expectation(\n        samples[:, 1],\n        dist=scipy.stats.norm(0, 10),\n        model_dir=self.model_dir,\n        suffix=""one_step_skewed_posterior_conservation_2d_dim_1.png"")\n\n  @parameterized.parameters(\n      (3, 10,),\n      (5, 10,),\n  )\n  def testMultivariateNormalNd(self, event_size, num_samples):\n    def target_log_prob_fn(event):\n      return tfd.MultivariateNormalFullCovariance(\n          loc=tf.zeros(event_size),\n          covariance_matrix=tf.eye(event_size)).log_prob(event)\n\n    state = tf.zeros(event_size)\n    samples = []\n    for seed in range(num_samples):\n      [state], _, _ = no_u_turn_sampler.kernel(\n          target_log_prob_fn=target_log_prob_fn,\n          current_state=[state],\n          step_size=[0.3],\n          seed=seed)\n      npstate = state.numpy()\n      samples.append([npstate[0], npstate[1]])\n\n    samples = np.array(samples)\n    plt.scatter(samples[:, 0], samples[:, 1])\n    savefig(self.model_dir,\n            ""projection_chain_{}d_normal_{}_steps.png"".format(\n                event_size, num_samples))\n    plt.close()\n\n    target_samples = tfd.MultivariateNormalFullCovariance(\n        loc=tf.zeros(event_size),\n        covariance_matrix=tf.eye(event_size)).sample(\n            num_samples, seed=4).numpy()\n    plt.scatter(target_samples[:, 0], target_samples[:, 1])\n    savefig(self.model_dir,\n            ""projection_independent_{}d_normal_{}_samples.png"".format(\n                event_size, num_samples))\n    plt.close()\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
experimental/attentive_uncertainty/attention.py,50,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Implements the attention layers.\n""""""\n\nimport tensorflow.compat.v1 as tf\n\n\ndef uniform_attention(q, v):\n  """"""Uniform attention. Equivalent to np.\n\n  Args:\n    q: queries. Tensor of shape [B, m, d_k].\n    v: values. Tensor of shape [B, n, d_v].\n\n  Returns:\n    tensor of shape [B, m, d_v].\n  """"""\n  total_points = tf.shape(q)[1]\n  rep = tf.reduce_mean(v, axis=1, keepdims=True)  # [B, 1, d_v]\n  rep = tf.tile(rep, [1, total_points, 1])\n  return rep\n\n\ndef laplace_attention(q, k,\n                      v=None,\n                      scale=1.,\n                      normalize=True,\n                      weights_only=False,\n                      hard=False):\n  """"""Computes laplace exponential attention.\n\n  Args:\n    q: queries. Tensor of shape [B, m, d_k].\n    k: keys. Tensor of shape [B, n, d_k].\n    v: values. Tensor of shape [B, n, d_v]. Can be None if weights_only=True.\n    scale: Attn hyperparam that scales the L1 distance. Tensor of shape [B].\n    normalize: Boolean that determines whether weights sum to 1.\n    weights_only: Boolean which returns attention weights if True.\n    hard: Returns one-hot argmax weights instead of softmax if True.\n\n  Returns:\n    tensor of shape [B, m, d_v].\n  """"""\n  k = tf.expand_dims(k, axis=1)  # [B, 1, n, d_k]\n  q = tf.expand_dims(q, axis=2)  # [B, m, 1, d_k]\n  d_k = tf.shape(q)[-1]\n  scale = tf.reshape(scale * tf.sqrt(tf.cast(d_k, tf.float32)), [-1, 1, 1, 1])\n  unnorm_weights = - tf.abs((k - q) / scale)  # [B, m, n, d_k]\n  unnorm_weights = tf.reduce_sum(unnorm_weights, axis=-1)  # [B, m, n]\n  if normalize:\n    weight_fn = tf.nn.softmax\n  else:\n    weight_fn = lambda x: 1 + tf.tanh(x)\n\n  weights = weight_fn(unnorm_weights)  # [B, m, n]\n  if hard:\n    weights = tf.one_hot(\n        tf.math.argmax(weights, axis=-1),\n        depth=tf.shape(k)[2],\n        axis=-1)\n  if weights_only:\n    return weights\n  rep = tf.einsum(\'bik,bkj->bij\', weights, v)  # [B, m, d_v]\n  return rep\n\n\ndef squared_exponential_attention(q, k,\n                                  v=None,\n                                  scale=1.,\n                                  normalize=True,\n                                  weights_only=False,\n                                  hard=False):\n  """"""Computes squared exponential attention.\n\n  Args:\n    q: queries. Tensor of shape [B, m, d_k].\n    k: keys. Tensor of shape [B, n, d_k].\n    v: values. Tensor of shape [B, n, d_v]. Can be None if weights_only=True.\n    scale: Attn hyperparam that scales the L1 distance. Tensor of shape [B].\n    normalize: Boolean that determines whether weights sum to 1.\n    weights_only: Boolean which returns attention weights if True.\n    hard: Returns one-hot argmax weights instead of softmax if True.\n\n  Returns:\n    tensor of shape [B, m, d_v].\n  """"""\n  k = tf.expand_dims(k, axis=1)  # [B, 1, n, d_k]\n  q = tf.expand_dims(q, axis=2)  # [B, m, 1, d_k]\n  scale = tf.reshape(scale, [-1, 1, 1])  # [B, 1, 1]\n  unnorm_weights = tf.exp(-0.5 * tf.reduce_sum(tf.square(k - q), axis=-1)\n                          / tf.square(scale))  # [B, m, n, d_k]\n  if normalize:\n    weight_fn = tf.nn.softmax\n  else:\n    weight_fn = lambda x: x\n  weights = weight_fn(unnorm_weights)  # [B, m, n]\n  if hard:\n    weights = tf.one_hot(\n        tf.math.argmax(weights, axis=-1),\n        depth=tf.shape(k)[2],\n        axis=-1)\n  if weights_only:\n    return weights\n  rep = tf.einsum(\'bik,bkj->bij\', weights, v)  # [B, m, d_v]\n  return rep\n\n\ndef dot_product_attention(q, k,\n                          v=None,\n                          scale=1.,\n                          normalize=True,\n                          weights_only=False,\n                          hard=False):\n  """"""Computes dot product attention.\n\n  Args:\n    q: queries. Tensor of shape [B, m, d_k].\n    k: keys. Tensor of shape [B, n, d_k].\n    v: values. Tensor of shape [B, n, d_v]. Can be None if weights_only=True.\n    scale: Attn hyperparam that scales the dot product. Tensor of shape [B].\n    normalize: Boolean that determines whether weights sum to 1.\n    weights_only: Boolean which returns attention weights if True.\n    hard: Returns one-hot argmax weights instead of softmax if True.\n\n  Returns:\n    tensor of shape [B, m, d_v].\n  """"""\n  d_k = tf.shape(q)[-1]\n  scale = tf.reshape(scale * tf.sqrt(tf.cast(d_k, tf.float32)),\n                     [-1, 1, 1])  # [B, 1, 1]\n  unnorm_weights = tf.einsum(\'bjk,bik->bij\', k, q) / scale  # [B, m, n]\n  if normalize:\n    weight_fn = tf.nn.softmax\n  else:\n    weight_fn = tf.sigmoid\n  weights = weight_fn(unnorm_weights)  # [B, m, n]\n  if hard:\n    weights = tf.one_hot(\n        tf.math.argmax(weights, axis=-1),\n        depth=tf.shape(k)[1],\n        axis=-1)\n  if weights_only:\n    return weights\n  rep = tf.einsum(\'bik,bkj->bij\', weights, v)  # [B, m, d_v]\n  return rep\n\n\ndef multihead_attention(projection_nets,\n                        q,\n                        k,\n                        v,\n                        num_heads=8,\n                        scale=1.,\n                        normalize=True):\n  """"""Computes multi-head attention.\n\n  Args:\n    projection_nets: List of lists of projection_nets for q, k, v, dot_product\n      output for each head.\n    q: queries. Tensor of  shape [B, m, d_k].\n    k: keys. Tensor of shape [B, n, d_k].\n    v: values. Tensor of shape [B, n, d_v].\n    num_heads: number of heads. Should divide d_v.\n    scale: Attn hyperparam that scales the dot product. Tensor of shape [B].\n    normalize: Boolean that determines whether weights sum to 1.\n\n  Returns:\n    tensor of shape [B, m, d_v].\n  """"""\n  rep = tf.constant(0.0)\n  for h in range(num_heads):\n    q_net, k_net, v_net, r_net = projection_nets[h]\n    o = dot_product_attention(\n        q_net(q),\n        k_net(k),\n        v_net(v),\n        scale=scale,\n        normalize=normalize)\n    rep += r_net(o)\n  return rep\n\n\nclass AttentionLayer(tf.keras.layers.Layer):\n  """"""The Attention module.""""""\n\n  def __init__(self,\n               att_type,\n               scale=1.,\n               normalize=True,\n               num_heads=8):\n    """"""Create attention module.\n\n    Takes in context inputs, target inputs and\n    representations of each context input/output pair\n    to output an aggregated representation of the context data.\n    Args:\n      att_type: type of attention. One of the following:\n          [\'uniform\',\'laplace\',\'dot_product\',\'multihead\']\n      scale: scale of attention.\n      normalize: Boolean determining whether to:\n          1. apply softmax to weights so that they sum to 1 across context pts\n          2. apply custom transformation to have weights in [0,1].\n      num_heads: number of heads for multihead.\n    """"""\n    super(AttentionLayer, self).__init__()\n    self._type = att_type\n    self._scale = scale\n    self._normalize = normalize\n    if self._type == \'multihead\':\n      self._num_heads = num_heads\n\n  def build(self, input_shape):\n    assert isinstance(input_shape, list)\n    d_k, d_v = input_shape\n\n    if self._type == \'multihead\':\n      num_heads = self._num_heads\n      head_size = int(d_v / num_heads)\n      key_initializer = tf.random_normal_initializer(stddev=d_k**-0.5)\n      value_initializer = tf.random_normal_initializer(stddev=d_v**-0.5)\n      self.multihead_nets = []\n\n      for h in range(num_heads):\n        query_net = tf.keras.Sequential(\n            [tf.keras.layers.InputLayer([None, d_k]),\n             tf.keras.layers.Conv1D(head_size, 1,\n                                    kernel_initializer=key_initializer,\n                                    name=\'wq%d\' % h, use_bias=False,\n                                    padding=\'VALID\')])\n        key_net = tf.keras.Sequential(\n            [tf.keras.layers.InputLayer([None, d_k]),\n             tf.keras.layers.Conv1D(head_size, 1,\n                                    kernel_initializer=key_initializer,\n                                    name=\'wk%d\' % h, use_bias=False,\n                                    padding=\'VALID\')])\n        value_net = tf.keras.Sequential(\n            [tf.keras.layers.InputLayer([None, d_v]),\n             tf.keras.layers.Conv1D(head_size, 1,\n                                    kernel_initializer=key_initializer,\n                                    name=\'wv%d\' % h, use_bias=False,\n                                    padding=\'VALID\')])\n        rep_net = tf.keras.Sequential(\n            [tf.keras.layers.InputLayer([None, head_size]),\n             tf.keras.layers.Conv1D(d_v, 1,\n                                    kernel_initializer=value_initializer,\n                                    name=\'wo%d\' % h, use_bias=False,\n                                    padding=\'VALID\')])\n        self.multihead_nets.append([query_net, key_net, value_net, rep_net])\n\n    super(AttentionLayer, self).build(input_shape)\n\n  def __call__(self, q, k, v):\n    """"""Apply attention to create aggregated representation of v.\n\n    Args:\n      q: Tensor of shape [B, m, d_x].\n      k: Tensor of shape [B, n, d_x].\n      v: Tensor of shape [B, n, d].\n\n    Returns:\n      Tensor of shape [B, m, d].\n\n    Raises:\n      NameError: The argument for type was invalid.\n    """"""\n\n    if self._type == \'uniform\':\n      rep = uniform_attention(q, v)\n    elif self._type == \'laplace\':\n      rep = laplace_attention(q, k, v, self._scale, self._normalize)\n    elif self._type == \'dot_product\':\n      rep = dot_product_attention(q, k, v, self._scale, self._normalize)\n    elif self._type == \'multihead\':\n      rep = multihead_attention(\n          self.multihead_nets,\n          q,\n          k,\n          v,\n          self._num_heads,\n          self._scale,\n          self._normalize)\n    else:\n      raise NameError((""\'att_type\' not among [\'uniform\',\'laplace\',\'dot_product\'""\n                       "",\'multihead\']""))\n\n    return rep\n'"
experimental/attentive_uncertainty/generalized_neural_process.py,15,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Regression model for generalized neural processes.\n""""""\n\nimport edward2 as ed\nfrom experimental.attentive_uncertainty import attention  # local file import\nfrom experimental.attentive_uncertainty import layers  # local file import\nfrom experimental.attentive_uncertainty import utils  # local file import\n\nimport tensorflow.compat.v1 as tf\n\neps = tf.keras.backend.epsilon()\n\n\nclass Regressor(tf.keras.Model):\n  r""""""Generalized neural process regressor.\n\n  A generalized neural process (GNP) expresses the following generative process\n\n  ```\n  z ~ p(z | global_latent_layer(C))\n  zi ~ p(zi | local_latent_layer(z, xi, C))\n  yi ~ p(yi | decoder(z, zi, xi, C))\n  ```\n\n  Maximizing the marginal likelihood is intractable and SNPs maximize the\n  evidence lower bound obtained via the following variational distributions\n\n  ```\n  z ~ q(z | global_latent_layer(T))\n  zi ~ q(zi | local_latent_layer(z, xi, T))\n  ```\n\n  Note that the global_latent_net and local_latent_net parameters are shared.\n  Different instantiations of GNP differ in the particular forms of\n  conditioning they use; in particular, what ancestors to condition on and how\n  to condition (via attention or not).\n  """"""\n\n  def __init__(self,\n               input_dim,\n               output_dim,\n               x_encoder_net_sizes=None,\n               x_y_encoder_net_sizes=None,\n               heteroskedastic_net_sizes=None,\n               global_latent_net_sizes=None,\n               local_latent_net_sizes=None,\n               decoder_net_sizes=None,\n               att_type=\'multihead\',\n               att_heads=8,\n               model_type=\'fully_connected\',\n               activation=tf.nn.relu,\n               output_activation=None,\n               model_path=None,\n               data_uncertainty=True,\n               beta=1.,\n               temperature=1.):\n    """"""Initializes the generalized neural process regressor.\n\n    D below denotes:\n    - Context dataset C during decoding phase\n    - Target dataset T during encoding phase\n\n    Args:\n      input_dim: (int) Dimensionality of covariates x.\n      output_dim: (int) Dimensionality of labels y.\n      x_encoder_net_sizes: (list of ints) Hidden layer sizes for network\n        featurizing x.\n      x_y_encoder_net_sizes: (list of ints) Hidden layer sizes for network\n        featurizing D.\n      heteroskedastic_net_sizes: (list of ints) Hidden layer sizes for network\n      that maps x to heteroskedastic variance.\n      global_latent_net_sizes: (list of ints) Hidden layer sizes for network\n        that maps D to mean and variance of predictive p(z | D).\n      local_latent_net_sizes: (list of ints) Hidden layer sizes for network\n        that maps xi, z, D to mean and variance of predictive p(zi | z, xi, D).\n      decoder_net_sizes: (list of ints) Hidden layer sizes for network that maps\n        xi, z, zi, D to mean and variance of predictive p(yi | z, zi, xi, D).\n      att_type: (string) Attention type for freeform attention.\n      att_heads: (int) Number of heads in case att_type=\'multihead\'.\n      model_type: (string) One of \'fully_connected\', \'cnp\', \'acnp\', \'acns\',\n        \'np\', \'anp\'.\n      activation: (callable) Non-linearity used for all neural networks.\n      output_activation: (callable) Non-linearity for predictive mean.\n      model_path: (string) File path for best early-stopped model.\n      data_uncertainty: (boolean) True if data uncertainty is explicit.\n      beta: (float) Scaling factor for global kl loss.\n      temperature: (float) Inverse scaling factor for temperature.\n\n    Raises:\n      ValueError: If model_type is unrecognized.\n    """"""\n    if (model_type not in\n        [\'np\', \'anp\', \'acns\', \'fully_connected\', \'cnp\', \'acnp\']):\n      raise ValueError(\'Unrecognized model type: %s\'% model_type)\n\n    super(Regressor, self).__init__()\n    self._input_dim = input_dim\n    self._output_dim = output_dim\n    self.model_type = model_type\n    self._output_activation = output_activation\n    self._data_uncertainty = data_uncertainty\n    self.beta = tf.constant(beta)\n    self.temperature = temperature\n\n    self._global_latent_layer = None\n    self._local_latent_layer = None\n    self._decoder_layer = None\n    self._dataset_encoding_layer = None\n    self._x_encoder = None\n    self._heteroskedastic_net = None\n    self._homoskedastic_net = None\n\n    contains_global = [\'np\', \'anp\', \'acns\', \'fully_connected\']\n    contains_local = [\'acns\', \'fully_connected\']\n\n    x_dim = input_dim\n    if x_encoder_net_sizes is not None:\n      self._x_encoder = utils.mlp_block(\n          input_dim,\n          x_encoder_net_sizes,\n          activation)\n      x_dim = x_encoder_net_sizes[-1]\n\n    x_y_net = None\n    self_dataset_attention = None\n    if x_y_encoder_net_sizes is not None:\n      x_y_net = utils.mlp_block(\n          x_dim + output_dim,\n          x_y_encoder_net_sizes,\n          activation)\n      dataset_encoding_dim = x_y_encoder_net_sizes[-1]\n    else:\n      # Use self-attention.\n      dataset_encoding_dim = x_dim + output_dim\n      self_dataset_attention = attention.AttentionLayer(\n          att_type=att_type, num_heads=att_heads)\n      self_dataset_attention.build([x_dim, x_dim])\n\n    self._dataset_encoding_layer = layers.DatasetEncodingLayer(\n        x_y_net,\n        self_dataset_attention)\n    self._cross_dataset_attention = attention.AttentionLayer(\n        att_type=att_type, num_heads=att_heads, scale=self.temperature)\n    self._cross_dataset_attention.build([x_dim, dataset_encoding_dim])\n\n    if model_type in contains_global:\n      global_latent_net = utils.mlp_block(\n          dataset_encoding_dim,\n          global_latent_net_sizes,\n          activation)\n      self._global_latent_layer = layers.GlobalLatentLayer(global_latent_net)\n      global_latent_dim = global_latent_net_sizes[-1]//2\n\n    if model_type in contains_local:\n      local_input_dim = global_latent_dim + dataset_encoding_dim\n      local_latent_net = utils.mlp_block(\n          local_input_dim,\n          local_latent_net_sizes,\n          activation)\n      self._local_latent_layer = layers.LocalLatentLayer(local_latent_net)\n      local_latent_dim = local_latent_net_sizes[-1]//2\n\n      separate_prior_net = (model_type != \'fully_connected\')\n      if separate_prior_net:\n        local_latent_net = utils.mlp_block(\n            global_latent_dim,\n            local_latent_net_sizes,\n            activation)\n        self._prior_local_latent_layer = layers.LocalLatentLayer(\n            local_latent_net)\n      else:\n        self._prior_local_latent_layer = self._local_latent_layer\n\n    if decoder_net_sizes is not None:\n      decoder_input_dim = x_dim\n      if model_type == \'cnp\' or model_type == \'acnp\':  # depend on C\n        decoder_input_dim += dataset_encoding_dim\n      elif model_type == \'np\':  # depend on z\n        decoder_input_dim += global_latent_dim\n      elif model_type == \'anp\':  # depend on z, C\n        decoder_input_dim += dataset_encoding_dim + global_latent_dim\n      elif model_type == \'acns\':\n        decoder_input_dim += dataset_encoding_dim + local_latent_dim\n      elif model_type == \'fully_connected\':\n        decoder_input_dim += (dataset_encoding_dim + global_latent_dim\n                              + local_latent_dim)\n      decoder_net = utils.mlp_block(\n          decoder_input_dim,\n          decoder_net_sizes,\n          activation)\n      self._decoder_layer = layers.DecoderLayer(\n          decoder_net,\n          model_type,\n          output_activation)\n\n    if data_uncertainty:\n      if heteroskedastic_net_sizes is not None:\n        self._heteroskedastic_net = utils.mlp_block(\n            x_dim,\n            heteroskedastic_net_sizes,\n            activation)\n      else:\n        self._homoskedastic_net = layers.DataNoise()\n        self._homoskedastic_net.build(None)\n\n    if model_path:\n      self.load_weights(model_path)\n\n  def call(self, context_x, context_y, target_x, target_y=None):\n    if self._x_encoder is not None:\n      context_x = self._x_encoder(context_x)\n      target_x = self._x_encoder(target_x)\n\n    if self._data_uncertainty:\n      if self._heteroskedastic_net is None:\n        data_var = tf.nn.softplus(self._homoskedastic_net(None))\n      else:\n        data_var = tf.nn.softplus(self._heteroskedastic_net(target_x))\n    else:\n      data_var = 0.\n\n    context_x_y_encodings = self._dataset_encoding_layer(context_x, context_y)\n    if target_y is None:\n      target_x_y_encodings = context_x_y_encodings\n    else:\n      target_x_y_encodings = self._dataset_encoding_layer(target_x, target_y)\n\n    avg_context_dataset_encodings = tf.reduce_mean(\n        context_x_y_encodings, axis=1, keepdims=True)\n    avg_target_dataset_encodings = tf.reduce_mean(\n        target_x_y_encodings, axis=1, keepdims=True)\n\n    global_z_prior = None\n    global_z_posterior = None\n    if self._global_latent_layer is not None:\n      global_z_prior = self._global_latent_layer(avg_context_dataset_encodings)\n      global_z_posterior = self._global_latent_layer(\n          avg_target_dataset_encodings)\n      global_z_kl = self.beta * global_z_posterior.distribution.kl_divergence(\n          global_z_prior.distribution)\n    else:\n      global_z_kl = tf.constant(0., shape=(1, 1))\n    self.add_loss(lambda: global_z_kl)\n    cross_attentive_encodings = None\n    if self.model_type not in [\'cnp\', \'np\']:\n      cross_attentive_encodings = self._cross_dataset_attention(\n          target_x, context_x, context_x_y_encodings)\n\n    posterior_x_y_encodings = None\n    prior_x_y_encodings = None\n    if self.model_type == \'fully_connected\':\n      prior_x_y_encodings = cross_attentive_encodings\n      posterior_x_y_encodings = self._cross_dataset_attention(\n          target_x, target_x, target_x_y_encodings)\n    else:\n      posterior_x_y_encodings = target_x_y_encodings\n    if self.model_type == \'cnp\':\n      cross_attentive_encodings = tf.tile(\n          avg_context_dataset_encodings,\n          [1, tf.shape(target_x)[1], 1])\n\n    local_z_prior = None\n    local_z_posterior = None\n    num_targets = tf.shape(target_x)[1]\n    if self._local_latent_layer is not None:\n      local_z_prior = self._prior_local_latent_layer(\n          global_z_prior,\n          num_targets,\n          prior_x_y_encodings)\n      if target_y is None:\n        local_z_posterior = local_z_prior\n      else:\n        local_z_posterior = self._local_latent_layer(\n            global_z_posterior,\n            num_targets,\n            posterior_x_y_encodings)\n      local_z_kl = local_z_posterior.distribution.kl_divergence(\n          local_z_prior.distribution)\n    else:\n      local_z_kl = tf.constant(0., shape=(1, 1, 1))\n    self.add_loss(lambda: local_z_kl)\n\n    predictive = self._decoder_layer(\n        target_x,\n        cross_attentive_encodings,\n        local_z_posterior,\n        global_z_posterior)\n\n    posterior_predictive_mean = predictive.distribution.mean()\n    posterior_predictive_std = tf.sqrt(\n        tf.square(predictive.distribution.stddev()) + data_var + eps)\n    posterior_predictive = ed.Normal(loc=posterior_predictive_mean,\n                                     scale=posterior_predictive_std)\n\n    return posterior_predictive\n'"
experimental/attentive_uncertainty/layers.py,34,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utility layers for structured and generalized neural processes.\n""""""\n\nimport edward2 as ed\nimport tensorflow.compat.v1 as tf\n\neps = tf.keras.backend.epsilon()\n\n\nclass DataNoise(tf.keras.layers.Layer):\n  """"""Creates a variable for modeling homoskedastic noise.""""""\n\n  def build(self, input_shape=None):\n    self.untransformed_data_var = self.add_weight(\n        shape=input_shape,\n        initializer=tf.random_normal_initializer(),\n        dtype=tf.float32,\n        name=\'data_noise\')\n\n  def call(self, inputs):\n    return self.untransformed_data_var\n\n\nclass DatasetEncodingLayer(tf.keras.layers.Layer):\n  """"""Encodes a dataset of (x, y) pairs into embeddings via a shared network.""""""\n\n  def __init__(self,\n               net,\n               dataset_attention):\n    super(DatasetEncodingLayer, self).__init__()\n    self._net = net\n    self._dataset_attention = dataset_attention\n\n  def call(self, x, y):\n    dataset = tf.concat([x, y], axis=-1)\n    if self._net is not None:\n      x_y_encodings = self._net(dataset)\n    else:\n      # Use self-attention.\n      x_y_encodings = self._dataset_attention(dataset, dataset, dataset)\n\n    return x_y_encodings\n\n\nclass GlobalLatentLayer(tf.keras.layers.Layer):\n  """"""Maps embedded (x, y) points to a single stochastic embedding.""""""\n\n  def __init__(self, net):\n    super(GlobalLatentLayer, self).__init__()\n    self._net = net\n\n  def call(self, avg_dataset_encodings):\n    logits = self._net(avg_dataset_encodings)\n    mean, untransformed_std = tf.split(logits, 2, axis=-1)\n    std = tf.nn.softplus(untransformed_std) + eps\n    return ed.Normal(loc=mean, scale=std)\n\n\nclass LocalLatentLayer(tf.keras.layers.Layer):\n  """"""Maps conditioning inputs to a per-point stochastic embedding.""""""\n\n  def __init__(self, net):\n    super(LocalLatentLayer, self).__init__()\n    self._net = net\n\n  def call(self,\n           sampled_global_latents,\n           num_targets,\n           local_x_y_encodings):\n    inputs = tf.tile(sampled_global_latents, [1, num_targets, 1])\n    if local_x_y_encodings is not None:\n      inputs = tf.concat([inputs, local_x_y_encodings], axis=-1)\n    logits = self._net(inputs)\n    mean, untransformed_std = tf.split(logits, 2, axis=-1)\n    std = tf.nn.softplus(untransformed_std)\n    return ed.Normal(loc=mean, scale=std)\n\n\nclass DecoderLayer(tf.keras.layers.Layer):\n  """"""Maps conditioning inputs to a per-point predictive distribution.""""""\n\n  def __init__(self,\n               net,\n               model_type,\n               output_activation=None):\n    super(DecoderLayer, self).__init__()\n    self._net = net\n    self._model_type = model_type\n    self._output_activation = output_activation\n\n  def call(self,\n           unlabelled_x,\n           attentive_encodings,\n           sampled_local_latents,\n           sampled_global_latents):\n    inputs = unlabelled_x\n    if self._model_type in [\'cnp\', \'acnp\', \'anp\', \'acns\', \'fully_connected\']:\n      inputs = tf.concat([inputs, attentive_encodings], axis=-1)\n    if self._model_type in [\'acns\', \'fully_connected\']:\n      inputs = tf.concat([inputs, sampled_local_latents], axis=-1)\n    if self._model_type in [\'np\', \'anp\', \'fully_connected\']:\n      tiled_global_latents = tf.tile(\n          sampled_global_latents,\n          [1, tf.shape(unlabelled_x)[1], 1])\n      inputs = tf.concat([inputs, tiled_global_latents], axis=-1)\n    logits = self._net(inputs)\n    mean, untransformed_std = tf.split(logits, 2, axis=-1)\n    if self._output_activation is not None:\n      mean = self._output_activation(mean)\n    std = tf.nn.softplus(untransformed_std)\n    return ed.Normal(loc=mean, scale=std)\n\n\nclass SNPLocalLatentLayer(tf.keras.layers.Layer):\n  """"""Maps each datapoint (and global conditioning) to stochastic embedding.""""""\n\n  def __init__(self,\n               net,\n               uncertainty_type,\n               mean_att_type,\n               scale_att_type_1,\n               scale_att_type_2,\n               output_activation=None):\n    super(SNPLocalLatentLayer, self).__init__()\n    self._net = net\n    self._uncertainty_type = uncertainty_type\n    self._attention_mean = mean_att_type\n    self._attention_scale_1 = scale_att_type_1\n    self._attention_scale_2 = scale_att_type_2\n    self._output_activation = output_activation\n\n  def call(self,\n           unlabelled_x,\n           labelled_x,\n           labelled_y,\n           sampled_global_latents=None,\n           attentive_encodings=None,\n           lengthscale_1=1.,\n           lengthscale_2=1.):\n\n    def _get_mean_var(inputs):\n      logits = self._net(inputs)\n      mean, untransformed_var = tf.split(logits, 2, axis=-1)\n      if self._output_activation is not None:\n        mean = self._output_activation(mean)\n      var = tf.nn.softplus(untransformed_var)\n      return mean, var\n\n    tiled_unlabelled_dataset_encoding = tf.tile(\n        sampled_global_latents,\n        [1, tf.shape(unlabelled_x)[1], 1])\n    tiled_labelled_dataset_encoding = tf.tile(\n        sampled_global_latents,\n        [1, tf.shape(labelled_x)[1], 1])\n\n    if self._uncertainty_type == \'attentive_gp\':\n      if self._net is not None:\n        unlabelled_inputs = tf.concat(\n            [unlabelled_x, tiled_unlabelled_dataset_encoding], axis=-1)\n        global_unlabelled_mean, _ = _get_mean_var(unlabelled_inputs)\n        labelled_inputs = tf.concat(\n            [labelled_x, tiled_labelled_dataset_encoding], axis=-1)\n        global_labelled_mean, _ = _get_mean_var(labelled_inputs)\n      else:\n        global_unlabelled_mean = 0.\n        global_labelled_mean = 0.\n\n      mean = global_unlabelled_mean + self._attention_mean(\n          unlabelled_x,\n          labelled_x,\n          labelled_y-global_labelled_mean,\n          normalize=True,\n          scale=lengthscale_1)\n      k_xx = 1.\n      k_xd = self._attention_scale_1(\n          unlabelled_x,\n          labelled_x,\n          scale=lengthscale_2,\n          normalize=True,\n          weights_only=True)\n      w_xd = self._attention_scale_2(\n          unlabelled_x,\n          labelled_x,\n          scale=lengthscale_1,\n          normalize=True,\n          weights_only=True)\n      var = k_xx - tf.reduce_sum(\n          w_xd * k_xd, axis=-1, keepdims=True)\n    else:\n      inputs = tf.concat([unlabelled_x,\n                          tiled_unlabelled_dataset_encoding,\n                          attentive_encodings], axis=-1)\n      mean, var = _get_mean_var(inputs)\n\n    std = tf.sqrt(var + eps)\n    return ed.Normal(loc=mean, scale=std)\n'"
experimental/attentive_uncertainty/regressor.py,12,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Regression model for structured neural processes.\n""""""\n\nimport edward2 as ed\nfrom experimental.attentive_uncertainty import attention  # local file import\nfrom experimental.attentive_uncertainty import layers  # local file import\nfrom experimental.attentive_uncertainty import utils  # local file import\n\nimport tensorflow.compat.v1 as tf\n\neps = tf.keras.backend.epsilon()\n\n\nclass Regressor(tf.keras.Model):\n  r""""""Structured neural process regressor.\n\n  A structured neural process (SNP) expresses the following generative process\n\n  ```\n  z ~ p(z | global_latent_layer(C))\n  zi ~ p(zi | local_latent_layer(z, xi, C))\n  yi ~ p(yi | zi, heteroskedastic_net(xi)/homoskedastic_net(None))\n  ```\n\n  Maximizing the marginal likelihood is intractable and SNPs maximize the\n  evidence lower bound obtained via the following variational distributions\n\n  ```\n  z ~ q(z | global_latent_layer(T))\n  zi ~ q(zi | local_latent_layer(z, xi, T)\n  ```\n\n  Note that the global_latent_net and local_latent_net parameters are used\n  for inference as well for parameter efficiency.\n  """"""\n\n  def __init__(self,\n               input_dim,\n               output_dim=1,\n               x_encoder_sizes=None,\n               x_y_encoder_sizes=None,\n               heteroskedastic_net_sizes=None,\n               global_latent_net_sizes=None,\n               local_latent_net_sizes=None,\n               att_type=\'multihead\',\n               att_heads=8,\n               uncertainty_type=\'attentive_freeform\',\n               mean_att_type=attention.laplace_attention,\n               scale_att_type_1=attention.squared_exponential_attention,\n               scale_att_type_2=attention.squared_exponential_attention,\n               activation=tf.nn.relu,\n               output_activation=None,\n               model_path=None,\n               data_uncertainty=True,\n               local_variational=True):\n    """"""Initializes the structured neural process regressor.\n\n    D below denotes:\n    - Context dataset C during decoding phase\n    - Target dataset T during encoding phase\n\n    Args:\n      input_dim: (int) Dimensionality of covariates x.\n      output_dim: (int) Dimensionality of labels y.\n      x_encoder_sizes: (list of ints) Hidden layer sizes for featurizing x.\n      x_y_encoder_sizes: (list of ints) Hidden layer sizes for featurizing C/D.\n      heteroskedastic_net_sizes: (list of ints) Hidden layer sizes for network\n      that maps x to heteroskedastic variance.\n      global_latent_net_sizes: (list of ints) Hidden layer sizes for network\n        that maps D to mean and variance of predictive p(z | D).\n      local_latent_net_sizes: (list of ints) Hidden layer sizes for network\n        that maps xi, z, D to mean and variance of predictive p(z_i | z, xi, D).\n      att_type: (string) Attention type for freeform attention.\n      att_heads: (int) Number of heads in case att_type=\'multihead\'.\n      uncertainty_type: (string) One of \'attentive_gp\', \'attentive_freeform\'.\n        Default is \'attentive_freeform\' which does not impose structure on\n        posterior mean, std.\n      mean_att_type: (call) Attention for mean of predictive p(zi | z, x, D).\n      scale_att_type_1: (call) Attention for std of predictive p(zi | z, x, D).\n      scale_att_type_2: (call) Attention for std of predictive p(zi | z, x, D).\n      activation: (callable) Non-linearity used for all neural networks.\n      output_activation: (callable) Non-linearity for predictive mean.\n      model_path: (string) File path for best early-stopped model.\n      data_uncertainty: (boolean) True if data uncertainty is explicit.\n      local_variational: (boolean) True if VI performed on local latents.\n    """"""\n    super(Regressor, self).__init__()\n    self._input_dim = input_dim\n    self._output_dim = output_dim\n    self._uncertainty_type = uncertainty_type\n    self._output_activation = output_activation\n    self._data_uncertainty = data_uncertainty\n    self.local_variational = local_variational\n    self._global_latent_layer = None\n    self._local_latent_layer = None\n    self._dataset_encoding_layer = None\n    self._x_encoder = None\n    self._heteroskedastic_net = None\n    self._homoskedastic_net = None\n\n    x_dim = input_dim\n    if x_encoder_sizes is not None:\n      self._x_encoder = utils.mlp_block(\n          input_dim,\n          x_encoder_sizes,\n          activation)\n      x_dim = x_encoder_sizes[-1]\n\n    x_y_net = None\n    self_dataset_attention = None\n    if x_y_encoder_sizes is not None:\n      x_y_net = utils.mlp_block(\n          x_dim + output_dim,\n          x_y_encoder_sizes,\n          activation)\n      dataset_encoding_dim = x_y_encoder_sizes[-1]\n    else:\n      # Use self-attention.\n      dataset_encoding_dim = x_dim + output_dim\n      self_dataset_attention = attention.AttentionLayer(\n          att_type=att_type, num_heads=att_heads)\n      self_dataset_attention.build([x_dim, x_dim])\n\n    self._dataset_encoding_layer = layers.DatasetEncodingLayer(\n        x_y_net,\n        self_dataset_attention)\n    self._cross_dataset_attention = attention.AttentionLayer(\n        att_type=att_type, num_heads=att_heads)\n    self._cross_dataset_attention.build([x_dim, dataset_encoding_dim])\n\n    local_latent_dim = x_dim\n    if global_latent_net_sizes is not None:\n      global_latent_net = utils.mlp_block(\n          dataset_encoding_dim,\n          global_latent_net_sizes,\n          activation)\n      self._global_latent_layer = layers.GlobalLatentLayer(global_latent_net)\n      local_latent_dim += global_latent_net_sizes[-1]//2\n\n    if local_latent_net_sizes is not None:\n      # Freeform uncertainty directly attends to dataset encoding.\n      if uncertainty_type == \'attentive_freeform\':\n        local_latent_dim += dataset_encoding_dim\n\n      local_latent_net = utils.mlp_block(\n          local_latent_dim,\n          local_latent_net_sizes,\n          activation)\n      self._local_latent_layer = layers.SNPLocalLatentLayer(\n          local_latent_net,\n          uncertainty_type,\n          mean_att_type,\n          scale_att_type_1,\n          scale_att_type_2,\n          output_activation)\n\n    if data_uncertainty:\n      if heteroskedastic_net_sizes is not None:\n        self._heteroskedastic_net = utils.mlp_block(\n            x_dim,\n            heteroskedastic_net_sizes,\n            activation)\n      else:\n        self._homoskedastic_net = layers.DataNoise()\n        self._homoskedastic_net.build(None)\n\n    if model_path:\n      self.load_weights(model_path)\n\n  def call(self, context_x, context_y, target_x, target_y=None):\n    if self._x_encoder is not None:\n      context_x = self._x_encoder(context_x)\n      target_x = self._x_encoder(target_x)\n\n    if self._data_uncertainty:\n      if self._heteroskedastic_net is None:\n        data_var = tf.nn.softplus(self._homoskedastic_net(None))\n      else:\n        data_var = tf.nn.softplus(self._heteroskedastic_net(target_x))\n    else:\n      data_var = 0.\n\n    context_x_y_encodings = self._dataset_encoding_layer(context_x, context_y)\n    if target_y is None:\n      target_x_y_encodings = context_x_y_encodings\n    else:\n      target_x_y_encodings = self._dataset_encoding_layer(target_x, target_y)\n    avg_context_dataset_encodings = tf.reduce_mean(\n        context_x_y_encodings, axis=1, keepdims=True)\n    avg_target_dataset_encodings = tf.reduce_mean(\n        target_x_y_encodings, axis=1, keepdims=True)\n\n    global_z_prior = None\n    global_z_posterior = None\n    if self._global_latent_layer is not None:\n      global_z_prior = self._global_latent_layer(avg_context_dataset_encodings)\n      global_z_posterior = self._global_latent_layer(\n          avg_target_dataset_encodings)\n      global_z_kl = global_z_posterior.distribution.kl_divergence(\n          global_z_prior.distribution)\n      self.add_loss(lambda: global_z_kl)\n\n    self_attentive_encodings = None\n    cross_attentive_encodings = None\n    if self._uncertainty_type == \'attentive_freeform\':\n      cross_attentive_encodings = self._cross_dataset_attention(\n          target_x, context_x, context_x_y_encodings)\n      if target_y is None:\n        self_attentive_encodings = cross_attentive_encodings\n      else:\n        self_attentive_encodings = self._cross_dataset_attention(\n            target_x, target_x, target_x_y_encodings)\n\n    # TODO(adityagrover): Variational inference for hyperparameters.\n    lengthscale_1 = 1.\n    lengthscale_2 = 1.\n    local_z_prior = self._local_latent_layer(\n        target_x,\n        context_x,\n        context_y,\n        global_z_prior,\n        cross_attentive_encodings,\n        lengthscale_1=lengthscale_1,\n        lengthscale_2=lengthscale_2)\n\n    if self.local_variational:\n      if target_y is None:\n        local_z_posterior = local_z_prior\n      else:\n        local_z_posterior = self._local_latent_layer(\n            target_x,\n            target_x,\n            target_y,\n            global_z_posterior,\n            self_attentive_encodings,\n            lengthscale_1=lengthscale_1,\n            lengthscale_2=lengthscale_2)\n\n      local_z_kl = local_z_posterior.distribution.kl_divergence(\n          local_z_prior.distribution)\n      self.add_loss(lambda: local_z_kl)\n\n      posterior_predictive_mean = local_z_posterior.distribution.mean()\n      posterior_predictive_std = tf.sqrt(\n          tf.square(local_z_posterior.distribution.stddev()) + data_var + eps)\n      posterior_predictive = ed.Normal(loc=posterior_predictive_mean,\n                                       scale=posterior_predictive_std)\n      output_predictive = posterior_predictive\n    else:\n      prior_predictive_mean = local_z_prior.distribution.mean()\n      prior_predictive_std = tf.sqrt(\n          tf.square(local_z_prior.distribution.stddev()) + data_var + eps)\n      prior_predictive = ed.Normal(loc=prior_predictive_mean,\n                                   scale=prior_predictive_std)\n      # With no variational inference, local_z_kl term is zero.\n      self.add_loss(lambda: tf.constant(0., shape=(1, 1, 1)))\n      output_predictive = prior_predictive\n\n    return output_predictive\n'"
experimental/attentive_uncertainty/utils.py,23,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Defined utility functions for attentive uncertainty models.\n""""""\n\nimport tensorflow.compat.v1 as tf\n\n\ndef mse(y_true, y_pred_dist, reduction=\'mean\'):\n  """"""Returns the mean squared error for a predictive distribution.\n\n  Args:\n    y_true: (float) Tensor of target labels.\n    y_pred_dist: An edward2 distribution object.\n    reduction: (string) Either \'sum\' or \'mean\'.\n  """"""\n  if reduction == \'sum\':\n    return tf.reduce_sum(\n        tf.squared_difference(y_true,\n                              y_pred_dist.distribution.mean()))\n  else:\n    return tf.losses.mean_squared_error(y_true,\n                                        y_pred_dist.distribution.mean())\n\n\ndef nll(y_true, y_pred_dist, reduction=\'mean\'):\n  """"""Returns the negative log-likelihood of a model w.r.t. true targets.\n\n  Args:\n    y_true: (float) Tensor of target labels.\n    y_pred_dist: An edward2 distribution object.\n    reduction: (string) Either \'sum\' or \'mean\'.\n  """"""\n  log_p = y_pred_dist.distribution.log_prob(y_true)\n  if reduction == \'sum\':\n    return -tf.reduce_sum(log_p)\n  else:\n    return -tf.reduce_mean(log_p)\n\n\ndef mlp_block(in_dim, hidden_sizes, activation=tf.nn.relu):\n  """"""Return keras sequential MLP object for the final axis of a 2/3D tensor.\n\n  Args:\n    in_dim: (int) Input dimension for final axis.\n    hidden_sizes: (list of ints) An iterable containing the output sizes of the\n      MLP as defined in `basic.Linear`.\n    activation: (callable) Activation applied to all but the final layer.\n\n  Returns:\n    tensor of shape [B, n, d_out] where d_out = hidden_sizes[-1]\n  """"""\n\n  net = tf.keras.Sequential([tf.keras.layers.InputLayer(in_dim)])\n  for size in hidden_sizes[:-1]:\n    net.add(tf.keras.layers.Dense(size, activation=activation))\n  net.add(tf.keras.layers.Dense(hidden_sizes[-1], activation=None))\n  return net\n\n\n@tf.function\ndef train_step(model, data, optimizer_config, is_mse=False):\n  """"""Applies gradient updates and returns appropriate metrics.\n\n  Args:\n    model: An instance of SNP Regressor.\n    data: A 5-tuple consisting of context_x, context_y, target_x, target_y,\n      unseen_targets (i.e., target_x-context_x).\n    optimizer_config: A dictionary with two keys: an \'optimizer\' object and\n      a \'max_grad_norm\' for clipping gradients.\n    is_mse: Use mse (fixed variance) if True else use nll.\n\n  Returns:\n    nll_term: Negative log-likelihood assigned by model to unseen targets.\n    mse_term: Mean squared error of model for unseen targets.\n    local_kl: KL loss for latent variables of unseen targets.\n    global_kl: KL loss for global latent variable.\n  """"""\n  context_x, context_y, target_x, target_y, unseen_targets = data\n  num_context = tf.shape(context_x)[1]\n  with tf.GradientTape() as tape:\n    prediction = model(\n        context_x,\n        context_y,\n        target_x,\n        target_y)\n    unseen_predictions = prediction[:, num_context:]\n    nll_term = nll(unseen_targets, unseen_predictions)\n    mse_term = mse(unseen_targets, unseen_predictions)\n    loss = mse_term if is_mse else nll_term\n    if model.local_variational:\n      local_kl = tf.reduce_mean(\n          tf.reduce_sum(model.losses[-1][:, num_context:], axis=[1, 2]))\n    else:\n      local_kl = 0.\n    global_kl = tf.reduce_mean(tf.reduce_sum(model.losses[-2], axis=-1))\n    loss += local_kl + global_kl\n  gradients = tape.gradient(loss, model.trainable_variables)\n  max_grad_norm = optimizer_config[\'max_grad_norm\']\n  optimizer = optimizer_config[\'optimizer\']\n  clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm)\n  optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n  return nll_term, mse_term, local_kl, global_kl\n\n\n@tf.function\ndef train_gnp_step(model, data, optimizer_config, is_mse=False):\n  """"""Applies gradient updates and returns appropriate metrics.\n\n  Args:\n    model: An instance of GNP Regressor.\n    data: A 5-tuple consisting of context_x, context_y, target_x, target_y,\n      unseen_targets (i.e., target_x-context_x).\n    optimizer_config: A dictionary with two keys: an \'optimizer\' object and\n      a \'max_grad_norm\' for clipping gradients.\n    is_mse: Use mse (fixed variance) if True else use nll.\n\n  Returns:\n    nll_term: Negative log-likelihood assigned by model to unseen targets.\n    mse_term: Mean squared error of model for unseen targets.\n    local_kl: KL loss for latent variables of unseen targets.\n    global_kl: KL loss for global latent variable.\n  """"""\n  context_x, context_y, target_x, target_y, unseen_targets = data\n  num_context = tf.shape(context_x)[1]\n  with tf.GradientTape() as tape:\n    prediction = model(\n        context_x,\n        context_y,\n        target_x,\n        target_y)\n    unseen_predictions = prediction[:, num_context:]\n    nll_term = nll(unseen_targets, unseen_predictions)\n    mse_term = mse(unseen_targets, unseen_predictions)\n    loss = mse_term if is_mse else nll_term\n    local_kl = tf.reduce_mean(\n        tf.reduce_sum(model.losses[-1][:, num_context:], axis=[1, 2]))\n    global_kl = tf.reduce_mean(tf.reduce_sum(model.losses[-2], axis=-1))\n    loss += local_kl + global_kl\n  gradients = tape.gradient(loss, model.trainable_variables)\n  max_grad_norm = optimizer_config[\'max_grad_norm\']\n  optimizer = optimizer_config[\'optimizer\']\n  clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm)\n  optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n  return nll_term, mse_term, local_kl, global_kl\n\n'"
experimental/auxiliary_sampling/compute_metrics.py,1,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Compute metrics of an ensemble model.""""""\n\nimport numpy as np\nimport scipy\n\n\ndef one_hot(a, num_classes):\n  return np.squeeze(np.eye(num_classes)[a.reshape(-1)])\n\n\ndef brier_score(y, p):\n  """"""Compute the Brier score.\n\n  Brier Score: see\n  https://www.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf,\n  page 363, Example 1\n\n  Args:\n    y: one-hot encoding of the true classes, size (?, num_classes)\n    p: numpy array, size (?, num_classes)\n       containing the output predicted probabilities\n  Returns:\n    bs: Brier score.\n  """"""\n  return np.mean(np.power(p - y, 2))\n\n\ndef calibration(y, p_mean, num_bins=10):\n  """"""Compute the calibration.\n\n  References:\n  https://arxiv.org/abs/1706.04599\n  https://arxiv.org/abs/1807.00263\n\n  Args:\n    y: one-hot encoding of the true classes, size (?, num_classes)\n    p_mean: numpy array, size (?, num_classes)\n           containing the mean output predicted probabilities\n    num_bins: number of bins\n\n  Returns:\n    ece: Expected Calibration Error\n    mce: Maximum Calibration Error\n  """"""\n  # Compute for every test sample x, the predicted class.\n  class_pred = np.argmax(p_mean, axis=1)\n  # and the confidence (probability) associated with it.\n  conf = np.max(p_mean, axis=1)\n  # Convert y from one-hot encoding to the number of the class\n  y = np.argmax(y, axis=1)\n  # Storage\n  acc_tab = np.zeros(num_bins)  # empirical (true) confidence\n  mean_conf = np.zeros(num_bins)  # predicted confidence\n  nb_items_bin = np.zeros(num_bins)  # number of items in the bins\n  tau_tab = np.linspace(0, 1, num_bins+1)  # confidence bins\n  for i in np.arange(num_bins):  # iterate over the bins\n    # select the items where the predicted max probability falls in the bin\n    # [tau_tab[i], tau_tab[i + 1)]\n    sec = (tau_tab[i + 1] > conf) & (conf >= tau_tab[i])\n    nb_items_bin[i] = np.sum(sec)  # Number of items in the bin\n    # select the predicted classes, and the true classes\n    class_pred_sec, y_sec = class_pred[sec], y[sec]\n    # average of the predicted max probabilities\n    mean_conf[i] = np.mean(conf[sec]) if nb_items_bin[i] > 0 else np.nan\n    # compute the empirical confidence\n    acc_tab[i] = np.mean(\n        class_pred_sec == y_sec) if nb_items_bin[i] > 0 else np.nan\n\n  # Cleaning\n  mean_conf = mean_conf[nb_items_bin > 0]\n  acc_tab = acc_tab[nb_items_bin > 0]\n  nb_items_bin = nb_items_bin[nb_items_bin > 0]\n\n  # Expected Calibration Error\n  ece = np.average(\n      np.absolute(mean_conf - acc_tab),\n      weights=nb_items_bin.astype(np.float) / np.sum(nb_items_bin))\n  # Maximum Calibration Error\n  mce = np.max(np.absolute(mean_conf - acc_tab))\n  return ece, mce\n\n\ndef ensemble_metrics(x,\n                     y,\n                     model,\n                     log_likelihood_fn,\n                     n_samples=1,\n                     weight_files=None):\n  """"""Evaluate metrics of a Bayesian ensemble.\n\n  Args:\n    x: numpy array of inputs\n    y: numpy array of labels\n    model: tf.keras.Model.\n    log_likelihood_fn: keras function of log likelihood. For classification\n      tasks, log_likelihood_fn(...)[1] should return the logits\n    n_samples: number of Monte Carlo samples to draw per ensemble member (each\n      weight file).\n    weight_files: to draw samples from multiple weight sets, specify a list of\n      weight files to load. These files must have been generated through\n      keras\'s model.save_weights(...).\n\n  Returns:\n    metrics_dict: dictionary containing the metrics\n  """"""\n  if weight_files is None:\n    ensemble_logprobs = [log_likelihood_fn([x, y])[0] for _ in range(n_samples)]\n    metric_values = [model.evaluate(x, y, verbose=0)\n                     for _ in range(n_samples)]\n    ensemble_logits = [log_likelihood_fn([x, y])[1] for _ in range(n_samples)]\n  else:\n    ensemble_logprobs = []\n    metric_values = []\n    ensemble_logits = []\n    for filename in weight_files:\n      model.load_weights(filename)\n      ensemble_logprobs.extend([log_likelihood_fn([x, y])[0]\n                                for _ in range(n_samples)])\n      ensemble_logits.extend([log_likelihood_fn([x, y])[1]\n                              for _ in range(n_samples)])\n      metric_values.extend([model.evaluate(x, y, verbose=0)\n                            for _ in range(n_samples)])\n\n  metric_values = np.mean(np.array(metric_values), axis=0)\n  results = {}\n  for m, name in zip(metric_values, model.metrics_names):\n    results[name] = m\n\n  ensemble_logprobs = np.array(ensemble_logprobs)\n  bayesian_mll = np.mean(\n      scipy.special.logsumexp(\n          np.sum(ensemble_logprobs, axis=2)\n          if len(ensemble_logprobs.shape) > 2 else ensemble_logprobs,\n          b=1. / ensemble_logprobs.shape[0],\n          axis=0),\n      axis=0)\n  results[\'bayesian_mll\'] = bayesian_mll\n\n  ensemble_logits = np.array(ensemble_logits)\n  probs = np.mean(scipy.special.softmax(ensemble_logits, axis=2), axis=0)\n  class_pred = np.argmax(probs, axis=1)\n  bayesian_accuracy = np.mean(np.equal(y, class_pred))\n  results[\'bayesian_accuracy\'] = bayesian_accuracy\n  results[\'ece\'], results[\'mce\'] = calibration(\n      one_hot(y, probs.shape[1]), probs)\n  results[\'brier_score\'] = brier_score(one_hot(y, probs.shape[1]), probs)\n  return results\n'"
experimental/auxiliary_sampling/datasets.py,0,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Dataset loading utility.""""""\n\nimport numpy as np\nimport tensorflow_datasets as tfds\n\n\ndef load(session):\n  """"""Load cifar10 as numpy array.""""""\n  train_ds = tfds.load(\n      \'cifar10\', split=tfds.Split.TRAIN, batch_size=-1, as_supervised=True)\n  x_train, y_train = session.run(train_ds)\n  test_ds = tfds.load(\n      \'cifar10\', split=tfds.Split.TEST, batch_size=-1, as_supervised=True)\n  x_test, y_test = session.run(test_ds)\n  # Standardize data.\n  x_mean = np.mean(x_train, axis=(0, 1, 2))\n  x_std = np.std(x_train, axis=(0, 1, 2))\n  x_train = (x_train - x_mean) / (x_std + 1e-10)\n  x_test = (x_test - x_mean) / (x_std + 1e-10)\n  return x_train, y_train, x_test, y_test\n'"
experimental/auxiliary_sampling/lenet5.py,11,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Build a Convolutional Bayesian neural network.""""""\n\nimport edward2 as ed\nfrom experimental.auxiliary_sampling.sampling import mean_field_fn  # local file import\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n\ndef lenet5(n_examples, input_shape, num_classes):\n  """"""Builds Bayesian LeNet5.""""""\n  p_fn, q_fn = mean_field_fn(empirical_bayes=True)\n  def normalized_kl_fn(q, p, _):\n    return q.kl_divergence(p) / tf.cast(n_examples, tf.float32)\n\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  conv1 = tfp.layers.Convolution2DFlipout(\n      6,\n      kernel_size=5,\n      padding=\'SAME\',\n      activation=tf.nn.relu,\n      kernel_prior_fn=p_fn,\n      kernel_posterior_fn=q_fn,\n      bias_prior_fn=p_fn,\n      bias_posterior_fn=q_fn,\n      kernel_divergence_fn=normalized_kl_fn,\n      bias_divergence_fn=normalized_kl_fn)(inputs)\n  pool1 = tf.keras.layers.MaxPooling2D(pool_size=[2, 2],\n                                       strides=[2, 2],\n                                       padding=\'SAME\')(conv1)\n  conv2 = tfp.layers.Convolution2DFlipout(\n      16,\n      kernel_size=5,\n      padding=\'SAME\',\n      activation=tf.nn.relu,\n      kernel_prior_fn=p_fn,\n      kernel_posterior_fn=q_fn,\n      bias_prior_fn=p_fn,\n      bias_posterior_fn=q_fn,\n      kernel_divergence_fn=normalized_kl_fn,\n      bias_divergence_fn=normalized_kl_fn)(pool1)\n  pool2 = tf.keras.layers.MaxPooling2D(pool_size=[2, 2],\n                                       strides=[2, 2],\n                                       padding=\'SAME\')(conv2)\n  conv3 = tfp.layers.Convolution2DFlipout(\n      120,\n      kernel_size=5,\n      padding=\'SAME\',\n      activation=tf.nn.relu,\n      kernel_prior_fn=p_fn,\n      kernel_posterior_fn=q_fn,\n      bias_prior_fn=p_fn,\n      bias_posterior_fn=q_fn,\n      kernel_divergence_fn=normalized_kl_fn,\n      bias_divergence_fn=normalized_kl_fn)(pool2)\n  flatten = tf.keras.layers.Flatten()(conv3)\n  dense1 = tfp.layers.DenseLocalReparameterization(\n      84,\n      activation=tf.nn.relu,\n      kernel_prior_fn=p_fn,\n      kernel_posterior_fn=q_fn,\n      bias_prior_fn=p_fn,\n      bias_posterior_fn=q_fn,\n      kernel_divergence_fn=normalized_kl_fn,\n      bias_divergence_fn=normalized_kl_fn)(flatten)\n  dense2 = tfp.layers.DenseLocalReparameterization(\n      num_classes,\n      kernel_prior_fn=p_fn,\n      kernel_posterior_fn=q_fn,\n      bias_prior_fn=p_fn,\n      bias_posterior_fn=q_fn,\n      kernel_divergence_fn=normalized_kl_fn,\n      bias_divergence_fn=normalized_kl_fn)(dense1)\n  outputs = tf.keras.layers.Lambda(lambda x: ed.Categorical(logits=x))(dense2)\n  return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n'"
experimental/auxiliary_sampling/res_net.py,6,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Build a Convolutional Bayesian neural network.""""""\n\nimport functools\n\nfrom absl import logging\nimport edward2 as ed\nfrom experimental.auxiliary_sampling.sampling import mean_field_fn  # local file import\n\nimport tensorflow.compat.v1 as tf\nimport tensorflow_probability as tfp\n\nkeras = tf.keras\n\n\ndef _resnet_layer(inputs,\n                  num_filters=16,\n                  kernel_size=3,\n                  strides=1,\n                  activation=\'relu\',\n                  depth=20,\n                  batchnorm=False,\n                  conv_first=True,\n                  variational=False,\n                  n_examples=None):\n  """"""2D Convolution-Batch Normalization-Activation stack builder.\n\n  Args:\n    inputs (tensor): input tensor from input image or previous layer\n    num_filters (int): Conv2D number of filters\n    kernel_size (int): Conv2D square kernel dimensions\n    strides (int): Conv2D square stride dimensions\n    activation (string): Activation function string.\n    depth (int): ResNet depth; used for initialization scale.\n    batchnorm (bool): whether to include batch normalization\n    conv_first (bool): conv-bn-activation (True) or bn-activation-conv (False)\n    variational (bool): Whether to use a variational convolutional layer.\n    n_examples (int): Number of examples per epoch for variational KL.\n\n  Returns:\n      x (tensor): tensor as input to the next layer\n  """"""\n  if variational:\n\n    def fixup_init(shape, dtype=None):\n      """"""Fixup initialization; see https://arxiv.org/abs/1901.09321.""""""\n      return keras.initializers.he_normal()(\n          shape, dtype=dtype) * depth**(-1 / 4)\n\n    p_fn, q_fn = mean_field_fn(empirical_bayes=True, initializer=fixup_init)\n\n    def normalized_kl_fn(q, p, _):\n      return tfp.distributions.kl_divergence(q, p) / tf.to_float(n_examples)\n\n    conv = tfp.layers.Convolution2DFlipout(\n        num_filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=\'same\',\n        kernel_prior_fn=p_fn,\n        kernel_posterior_fn=q_fn,\n        kernel_divergence_fn=normalized_kl_fn)\n  else:\n    conv = keras.layers.Conv2D(\n        num_filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=\'same\',\n        kernel_initializer=\'he_normal\',\n        kernel_regularizer=keras.regularizers.l2(1e-4))\n\n  def apply_conv(net):\n    return conv(net)\n\n  x = inputs\n  x = apply_conv(x) if conv_first else x\n  if batchnorm:\n    x = keras.layers.BatchNormalization()(x)\n  x = keras.layers.Activation(activation)(x) if activation is not None else x\n  x = x if conv_first else apply_conv(x)\n  return x\n\n\ndef build_resnet_v1(input_layer,\n                    depth,\n                    variational,\n                    batchnorm,\n                    n_examples):\n  """"""ResNet Version 1 Model builder.\n\n  Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n  Last ReLU is after the shortcut connection.\n  At the beginning of each stage, the feature map size is halved (downsampled)\n  by a convolutional layer with strides=2, while the number of filters is\n  doubled. Within each stage, the layers have the same number filters and the\n  same number of filters.\n  Features maps sizes:\n  stage 0: 32x32, 16\n  stage 1: 16x16, 32\n  stage 2:  8x8,  64\n  The Number of parameters is approx:\n  ResNet20 0.27M\n  ResNet32 0.46M\n  ResNet44 0.66M\n  ResNet56 0.85M\n  ResNet110 1.7M\n\n  Args:\n    input_layer (tensor): keras.layers.InputLayer instance.\n    depth (int): number of core convolutional layers. It should be 6n+2 (eg 20,\n      32, 44).\n    variational (str): \'none\', \'hybrid\', \'full\'. whether to use variational\n      inference for zero, some, or all layers.\n    batchnorm (bool): use of batchnorm layers.\n    n_examples (int): number of training points.\n\n  Returns:\n     output: the output tensor of the Network.\n  """"""\n  if (depth - 2) % 6 != 0:\n    raise ValueError(\'depth should be 6n+2 (eg 20, 32, 44 in [a])\')\n  # Start model definition.\n  num_filters = 16\n  num_res_blocks = int((depth - 2) / 6)\n\n  activation = \'selu\' if variational else \'relu\'\n  resnet_layer = functools.partial(\n      _resnet_layer,\n      depth=depth,\n      n_examples=n_examples)\n\n  logging.info(\'Starting ResNet build.\')\n  x = resnet_layer(inputs=input_layer,\n                   activation=activation)\n  # Instantiate the stack of residual units\n  for stack in range(3):\n    for res_block in range(num_res_blocks):\n      logging.info(\'Starting ResNet stack #%d block #%d.\', stack, res_block)\n      strides = 1\n      if stack > 0 and res_block == 0:  # first layer but not first stack\n        strides = 2  # downsample\n      y = resnet_layer(\n          inputs=x,\n          num_filters=num_filters,\n          strides=strides,\n          activation=activation,\n          variational=True if variational == \'full\' else False,\n          batchnorm=batchnorm)\n      y = resnet_layer(\n          inputs=y,\n          num_filters=num_filters,\n          activation=None,\n          variational=True if variational in (\'hybrid\', \'full\') else False,\n          batchnorm=batchnorm)\n      if stack > 0 and res_block == 0:  # first layer but not first stack\n        # linear projection residual shortcut connection to match changed dims\n        x = resnet_layer(\n            inputs=x,\n            num_filters=num_filters,\n            kernel_size=1,\n            strides=strides,\n            activation=None,\n            batchnorm=False)\n      x = keras.layers.add([x, y])\n      x = keras.layers.Activation(activation)(x)\n    num_filters *= 2\n\n  # Add classifier on top.\n  # v1 does not use BN after last shortcut connection-ReLU\n  x = keras.layers.AveragePooling2D(pool_size=8)(x)\n  return keras.layers.Flatten()(x)\n\n\ndef res_net(n_examples,\n            input_shape,\n            num_classes,\n            batchnorm=False,\n            variational=\'full\'):\n  """"""Wrapper for build_resnet_v1.\n\n  Args:\n    n_examples (int): number of training points.\n    input_shape (list): input shape.\n    num_classes (int): number of classes (CIFAR10 has 10).\n    batchnorm (bool): use of batchnorm layers.\n    variational (str): \'none\', \'hybrid\', \'full\'. whether to use variational\n      inference for zero, some, or all layers.\n\n  Returns:\n      model (Model): Keras model instance whose output is a\n        tfp.distributions.Categorical distribution.\n  """"""\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  x = build_resnet_v1(\n      inputs,\n      depth=20,\n      variational=variational,\n      batchnorm=batchnorm,\n      n_examples=n_examples)\n\n  p_fn, q_fn = mean_field_fn(empirical_bayes=True)\n\n  def normalized_kl_fn(q, p, _):\n    return tfp.distributions.kl_divergence(q, p) / tf.to_float(n_examples)\n\n  logits = tfp.layers.DenseLocalReparameterization(\n      num_classes,\n      kernel_prior_fn=p_fn,\n      kernel_posterior_fn=q_fn,\n      bias_prior_fn=p_fn,\n      bias_posterior_fn=q_fn,\n      kernel_divergence_fn=normalized_kl_fn,\n      bias_divergence_fn=normalized_kl_fn)(x)\n  outputs = tf.keras.layers.Lambda(lambda x: ed.Categorical(logits=x))(logits)\n  return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n'"
experimental/auxiliary_sampling/run_training.py,17,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Variational inference for LeNet5 or ResNet-20 on CIFAR-10.""""""\n\nimport os\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom experimental.auxiliary_sampling import datasets  # local file import\nfrom experimental.auxiliary_sampling.compute_metrics import ensemble_metrics  # local file import\nfrom experimental.auxiliary_sampling.lenet5 import lenet5  # local file import\nfrom experimental.auxiliary_sampling.res_net import res_net  # local file import\nfrom experimental.auxiliary_sampling.sampling import sample_auxiliary_op  # local file import\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tf1\nimport tensorflow_probability as tfp\n\nflags.DEFINE_integer(\'training_steps\', 30000, \'Training steps.\')\nflags.DEFINE_integer(\'batch_size\', 256, \'Batch size.\')\nflags.DEFINE_float(\'learning_rate\', 0.001, \'Learning rate.\')\nflags.DEFINE_float(\'learning_rate_for_sampling\', 0.00001, \'Learning rate.\')\nflags.DEFINE_integer(\'auxiliary_sampling_frequency\', 100,\n                     \'Steps between sampling auxiliary variables.\')\nflags.DEFINE_float(\'auxiliary_variance_ratio\', 0.7,\n                   \'Variance ratio of the auxiliary variables wrt the prior.\')\nflags.DEFINE_integer(\'n_auxiliary_variables\', 5,\n                     \'Number of auxiliary variables.\')\nflags.DEFINE_integer(\'ensemble_size\', 10, \'Number of ensemble components.\')\nflags.DEFINE_integer(\'validation_freq\', 5, \'Validation frequency in steps.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/uci\',\n                    \'The directory where the model weights and \'\n                    \'training/evaluation summaries are stored.\')\nflags.DEFINE_integer(\'seed\', 0, \'Random seed.\')\nflags.DEFINE_boolean(\'resnet\', False, \'Use a ResNet for image classification.\' +\n                     \'The default is to use the LeNet5 arhitecture.\' +\n                     \'Currently only supported on cifar10.\')\nflags.DEFINE_boolean(\'hybrid\', False, \'Use mix of deterministic and Bayesian.\' +\n                     \'Only applies when resnet is True.\')\nflags.DEFINE_boolean(\'batchnorm\', False,\n                     \'Use batchnorm. Only applies when resnet is True.\')\nflags.DEFINE_boolean(\n    \'data_augmentation\', False,\n    \'Use data augmentation. Only applies when resnet is True.\')\nFLAGS = flags.FLAGS\n\n\ndef get_losses_and_metrics(model, n_train):\n  """"""Define the losses and metrics for the model.""""""\n\n  def negative_log_likelihood(y, rv_y):\n    del rv_y  # unused arg\n    return -model.output.distribution.log_prob(tf.squeeze(y))\n\n  def accuracy(y_true, y_sample):\n    del y_sample  # unused arg\n    return tf.equal(\n        tf.argmax(input=model.output.distribution.logits, axis=1),\n        tf.cast(tf.squeeze(y_true), tf.int64))\n\n  def log_likelihood(y_true, y_sample):\n    """"""Expected conditional log-likelihood.""""""\n    del y_sample  # unused arg\n    return model.output.distribution.log_prob(tf.squeeze(y_true))\n\n  def kl(y_true, y_sample):\n    """"""KL-divergence.""""""\n    del y_true  # unused arg\n    del y_sample  # unused arg\n    sampling_cost = sum(\n        [l.kl_cost_weight + l.kl_cost_bias for l in model.layers])\n    return sum(model.losses) * n_train + sampling_cost\n\n  def elbo(y_true, y_sample):\n    return log_likelihood(y_true, y_sample) * n_train - kl(y_true, y_sample)\n\n  return negative_log_likelihood, accuracy, log_likelihood, kl, elbo\n\n\ndef main(argv):\n  del argv  # unused arg\n  np.random.seed(FLAGS.seed)\n  tf.random.set_seed(FLAGS.seed)\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  tf1.disable_v2_behavior()\n\n  session = tf1.Session()\n  with session.as_default():\n    x_train, y_train, x_test, y_test = datasets.load(session)\n    n_train = x_train.shape[0]\n\n    num_classes = int(np.amax(y_train)) + 1\n    if not FLAGS.resnet:\n      model = lenet5(n_train, x_train.shape[1:], num_classes)\n    else:\n      datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n          rotation_range=90,\n          width_shift_range=0.1,\n          height_shift_range=0.1,\n          horizontal_flip=True)\n      datagen.fit(x_train)\n      model = res_net(n_train,\n                      x_train.shape[1:],\n                      num_classes,\n                      batchnorm=FLAGS.batchnorm,\n                      variational=\'hybrid\' if FLAGS.hybrid else \'full\')\n\n      def schedule_fn(epoch):\n        """"""Learning rate schedule function.""""""\n        rate = FLAGS.learning_rate\n        if epoch > 180:\n          rate *= 0.5e-3\n        elif epoch > 160:\n          rate *= 1e-3\n        elif epoch > 120:\n          rate *= 1e-2\n        elif epoch > 80:\n          rate *= 1e-1\n        return float(rate)\n\n      lr_callback = tf.keras.callbacks.LearningRateScheduler(schedule_fn)\n\n    for l in model.layers:\n      l.kl_cost_weight = l.add_weight(\n          name=\'kl_cost_weight\',\n          shape=(),\n          initializer=tf.constant_initializer(0.),\n          trainable=False)\n      l.kl_cost_bias = l.add_variable(\n          name=\'kl_cost_bias\',\n          shape=(),\n          initializer=tf.constant_initializer(0.),\n          trainable=False)\n\n    [negative_log_likelihood,\n     accuracy,\n     log_likelihood,\n     kl,\n     elbo] = get_losses_and_metrics(model, n_train)\n\n    metrics = [elbo, log_likelihood, kl, accuracy]\n\n    tensorboard = tf1.keras.callbacks.TensorBoard(\n        log_dir=FLAGS.output_dir,\n        update_freq=FLAGS.batch_size * FLAGS.validation_freq)\n    if FLAGS.resnet:\n      callbacks = [tensorboard, lr_callback]\n    else:\n      callbacks = [tensorboard]\n\n    if not FLAGS.resnet or not FLAGS.data_augmentation:\n\n      def fit_fn(model,\n                 steps,\n                 initial_epoch=0,\n                 with_lr_schedule=FLAGS.resnet):\n        return model.fit(\n            x=x_train,\n            y=y_train,\n            batch_size=FLAGS.batch_size,\n            epochs=initial_epoch + (FLAGS.batch_size * steps) // n_train,\n            initial_epoch=initial_epoch,\n            validation_data=(x_test, y_test),\n            validation_freq=(\n                (FLAGS.validation_freq * FLAGS.batch_size) // n_train),\n            verbose=1,\n            callbacks=callbacks if with_lr_schedule else [tensorboard])\n    else:\n\n      def fit_fn(model,\n                 steps,\n                 initial_epoch=0,\n                 with_lr_schedule=FLAGS.resnet):\n        return model.fit_generator(\n            datagen.flow(x_train, y_train, batch_size=FLAGS.batch_size),\n            epochs=initial_epoch + (FLAGS.batch_size * steps) // n_train,\n            initial_epoch=initial_epoch,\n            steps_per_epoch=n_train // FLAGS.batch_size,\n            validation_data=(x_test, y_test),\n            validation_freq=max(\n                (FLAGS.validation_freq * FLAGS.batch_size) // n_train, 1),\n            verbose=1,\n            callbacks=callbacks if with_lr_schedule else [tensorboard])\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(lr=float(FLAGS.learning_rate)),\n        loss=negative_log_likelihood,\n        metrics=metrics)\n    session.run(tf1.initialize_all_variables())\n\n    train_epochs = (FLAGS.training_steps * FLAGS.batch_size) // n_train\n    fit_fn(model, FLAGS.training_steps)\n\n    labels = tf.keras.layers.Input(shape=y_train.shape[1:])\n    ll = tf.keras.backend.function([model.input, labels], [\n        model.output.distribution.log_prob(tf.squeeze(labels)),\n        model.output.distribution.logits\n    ])\n\n    base_metrics = [\n        ensemble_metrics(x_train, y_train, model, ll),\n        ensemble_metrics(x_test, y_test, model, ll)\n    ]\n    model_dir = os.path.join(FLAGS.output_dir, \'models\')\n    tf.io.gfile.makedirs(model_dir)\n    base_model_filename = os.path.join(model_dir, \'base_model.weights\')\n    model.save_weights(base_model_filename)\n\n    # Train base model further for comparison.\n    fit_fn(\n        model,\n        FLAGS.n_auxiliary_variables * FLAGS.auxiliary_sampling_frequency *\n        FLAGS.ensemble_size,\n        initial_epoch=train_epochs)\n\n    overtrained_metrics = [\n        ensemble_metrics(x_train, y_train, model, ll),\n        ensemble_metrics(x_test, y_test, model, ll)\n    ]\n\n    # Perform refined VI.\n    sample_op = []\n    for l in model.layers:\n      if isinstance(l, tfp.layers.DenseLocalReparameterization) or isinstance(\n          l, tfp.layers.Convolution2DFlipout):\n        weight_op, weight_cost = sample_auxiliary_op(\n            l.kernel_prior.distribution, l.kernel_posterior.distribution,\n            FLAGS.auxiliary_variance_ratio)\n        sample_op.append(weight_op)\n        sample_op.append(l.kl_cost_weight.assign_add(weight_cost))\n        # Fix the variance of the prior\n        session.run(l.kernel_prior.distribution.istrainable.assign(0.))\n        if hasattr(l.bias_prior, \'distribution\'):\n          bias_op, bias_cost = sample_auxiliary_op(\n              l.bias_prior.distribution, l.bias_posterior.distribution,\n              FLAGS.auxiliary_variance_ratio)\n          sample_op.append(bias_op)\n          sample_op.append(l.kl_cost_bias.assign_add(bias_cost))\n          # Fix the variance of the prior\n          session.run(l.bias_prior.distribution.istrainable.assign(0.))\n\n    ensemble_filenames = []\n    for i in range(FLAGS.ensemble_size):\n      model.load_weights(base_model_filename)\n      for j in range(FLAGS.n_auxiliary_variables):\n        session.run(sample_op)\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(\n                # The learning rate is proportional to the scale of the prior.\n                lr=float(FLAGS.learning_rate_for_sampling *\n                         np.sqrt(1. - FLAGS.auxiliary_variance_ratio)**j)),\n            loss=negative_log_likelihood,\n            metrics=metrics)\n        fit_fn(\n            model,\n            FLAGS.auxiliary_sampling_frequency,\n            initial_epoch=train_epochs,\n            with_lr_schedule=False)\n      ensemble_filename = os.path.join(\n          model_dir, \'ensemble_component_\' + str(i) + \'.weights\')\n      ensemble_filenames.append(ensemble_filename)\n      model.save_weights(ensemble_filename)\n\n    auxiliary_metrics = [\n        ensemble_metrics(\n            x_train,\n            y_train,\n            model,\n            ll,\n            weight_files=ensemble_filenames),\n        ensemble_metrics(\n            x_test,\n            y_test,\n            model,\n            ll,\n            weight_files=ensemble_filenames)\n    ]\n\n    for metrics, name in [(base_metrics, \'Base model\'),\n                          (overtrained_metrics, \'Overtrained model\'),\n                          (auxiliary_metrics, \'Auxiliary sampling\')]:\n      logging.info(name)\n      for metrics_dict, split in [(metrics[0], \'Training\'),\n                                  (metrics[1], \'Testing\')]:\n        logging.info(split)\n        for metric_name in metrics_dict:\n          logging.info(\'%s: %s\', metric_name, metrics_dict[metric_name])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
experimental/auxiliary_sampling/sampling.py,8,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities for the auxiliary sampling algorithm.""""""\n\nfrom absl import flags\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tf1\nimport tensorflow_probability as tfp\n\ntfd = tfp.distributions\n\nflags.DEFINE_float(""mean_field_init_untransformed_scale"", -7,\n                   ""Initial scale (before softplus) for mean field."")\nFLAGS = flags.FLAGS\n\n\ndef mean_field_fn(empirical_bayes=False,\n                  initializer=tf1.initializers.he_normal()):\n  """"""Constructors for Gaussian prior and posterior distributions.\n\n  Args:\n    empirical_bayes (bool): Whether to train the variance of the prior or not.\n    initializer (tf1.initializer): Initializer for the posterior means.\n  Returns:\n    prior, posterior (tfp.distribution): prior and posterior\n    to be fed into a Bayesian Layer.\n  """"""\n\n  def prior(dtype, shape, name, trainable, add_variable_fn):\n    """"""Returns the prior distribution (tfp.distributions.Independent).""""""\n    softplus_inverse_scale = np.log(np.exp(1.) - 1.)\n\n    istrainable = add_variable_fn(\n        name=name + ""_istrainable"",\n        shape=(),\n        initializer=tf1.constant_initializer(1.),\n        dtype=dtype,\n        trainable=False)\n\n    untransformed_scale = add_variable_fn(\n        name=name + ""_untransformed_scale"",\n        shape=(),\n        initializer=tf1.constant_initializer(softplus_inverse_scale),\n        dtype=dtype,\n        trainable=empirical_bayes and trainable)\n    scale = (\n        np.finfo(dtype.as_numpy_dtype).eps +\n        tf.nn.softplus(untransformed_scale * istrainable + (1. - istrainable) *\n                       tf1.stop_gradient(untransformed_scale)))\n    loc = add_variable_fn(\n        name=name + ""_loc"",\n        shape=shape,\n        initializer=tf1.constant_initializer(0.),\n        dtype=dtype,\n        trainable=False)\n    dist = tfp.distributions.Normal(loc=loc, scale=scale)\n    dist.istrainable = istrainable\n    dist.untransformed_scale = untransformed_scale\n    batch_ndims = tf1.size(input=dist.batch_shape_tensor())\n    return tfp.distributions.Independent(dist,\n                                         reinterpreted_batch_ndims=batch_ndims)\n\n  def posterior(dtype, shape, name, trainable, add_variable_fn):\n    """"""Returns the posterior distribution (tfp.distributions.Independent).""""""\n    untransformed_scale = add_variable_fn(\n        name=name + ""_untransformed_scale"",\n        shape=shape,\n        initializer=tf1.compat.v1.initializers.random_normal(\n            mean=FLAGS.mean_field_init_untransformed_scale, stddev=0.1),\n        dtype=dtype,\n        trainable=trainable)\n    scale = (\n        np.finfo(dtype.as_numpy_dtype).eps +\n        tf.nn.softplus(untransformed_scale))\n    loc = add_variable_fn(\n        name=name + ""_loc"",\n        shape=shape,\n        initializer=initializer,\n        dtype=dtype,\n        trainable=trainable)\n    dist = tfp.distributions.Normal(loc=loc, scale=scale)\n    dist.untransformed_scale = untransformed_scale\n    batch_ndims = tf1.size(input=dist.batch_shape_tensor())\n    return tfp.distributions.Independent(dist,\n                                         reinterpreted_batch_ndims=batch_ndims)\n\n  return prior, posterior\n\n\ndef sample_auxiliary_op(prior, posterior, aux_variance_ratio):\n  r""""""Sample the auxiliary variable and calculate the conditionals.\n\n  Given a gaussian prior $$\\mathcal{N}(\\mu_z, \\sigma^2_z)$$\n  Define auxiliary variables $$z=a_1+a_2$$ with $$a_1=\\mathcal{N}(0,\n  \\sigma_{a_1}^2)$$ and $$a_2=\\mathcal{N}(\\mu_z, \\sigma_{a_2}^2)$$ with\n  $$\\frac{\\sigma_{a_1}^2}{\\sigma^2_z}=$$aux_variance_ratio and\n  $$\\sigma_{a_1}^2+\\sigma_{a_2}^2=\\sigma_z^2$$.\n  From this, we can calculate the posterior of a1 and the conditional of z.\n\n  Conditional:\n  $$p(a_1|z) =  \\mathcal{N}(z \\frac{\\sigma_{a_1}^2}{\\sigma_{z}^2},\n  \\frac{\\sigma_{a_1}^2\\sigma_{a_2}^2}{\\sigma_z^2})$$\n\n  Posterior of $$a_1$$:\n  $$q(a_1) =\\mathcal{N}(\\mu_{q(z)} \\frac{\\sigma_{a_1}^2}{\\sigma_{z}^2},\n  \\frac{\\sigma_{q(z)}^2\\sigma_{a_1}^4}{\\sigma_{z}^4} +\n  \\frac{\\sigma_{a_1}^2\\sigma_{a_2}^2}{\\sigma_{z}^2})$$\n\n  Conditional posterior:\n  $$q(z|a_1)=\\frac{q(a_1|z)q(z)}{q(a_1)}$$\n\n  $$q(z|a_1)=\\mathcal{N}(\\frac{a_1\\sigma^2_{q(z)}\\sigma^2_{z} +\n  \\mu_{q(z)}\\sigma^2_{a_2}\\sigma^2_{z}}{\\sigma^2_{q(z)}\\sigma^2_{a_1} +\n  \\sigma^2_z\\sigma^2_{a_2}},\n  \\frac{\\sigma^2_{q(z)}\\sigma^2_z\\sigma^2_{a_2}}{\\sigma^2_{a_1}\\sigma^2_{q(z)} +\n  \\sigma^2_{z}\\sigma^2_{a_2}})$$.\n\n  Args:\n    prior: The prior distribution. Must be parameterized by loc and\n      untransformed_scale, with the transformation being the softplus function.\n    posterior: The posterior distribution. Must be parameterized by loc and\n      untransformed_scale, with the transformation being the softplus function.\n    aux_variance_ratio: Ratio of the variance of the auxiliary variable and the\n      prior. The mean of the auxiliary variable is at 0.\n\n  Returns:\n    sampling_op: Tensorflow operation that executes the sampling.\n    log_density_ratio: Tensor containing the density ratio of the auxiliary\n    variable.\n  """"""\n  if aux_variance_ratio > 1. or aux_variance_ratio < 0.:\n    raise ValueError(\n        ""The ratio of the variance of the auxiliary variable must be between 0 ""\n        ""and 1.""\n    )\n\n  p_a1_loc = tf.zeros_like(prior.loc)\n  p_a1_scale = tf.math.sqrt(prior.scale**2 * aux_variance_ratio)\n  p_a1 = tfp.distributions.Normal(loc=p_a1_loc, scale=p_a1_scale)\n  p_a2_loc = prior.loc\n  p_a2_scale = tf.math.sqrt(prior.scale**2 - p_a1_scale**2)\n  # q(a1)\n  a1_loc = (posterior.loc - prior.loc) * p_a1_scale**2 / prior.scale**2\n  a1_scale = tf.math.sqrt(\n      (posterior.scale**2 * p_a1_scale**2 / prior.scale**2 + p_a2_scale**2) *\n      p_a1_scale**2 / prior.scale**2)\n  q_a1 = tfp.distributions.Normal(loc=a1_loc, scale=a1_scale)\n  a1 = q_a1.sample()\n\n  # q(z|a1)\n  z_a1_loc = prior.loc + (\n      (posterior.loc - prior.loc) * p_a2_scale**2 * prior.scale**2 +\n      a1 * posterior.scale**2 * prior.scale**2) / (\n          prior.scale**2 * p_a2_scale**2 + posterior.scale**2 * p_a1_scale**2)\n  z_a1_scale = tf.math.sqrt(\n      (posterior.scale**2 * p_a2_scale**2 * prior.scale**2) /\n      (prior.scale**2 * p_a2_scale**2 + p_a1_scale**2 * posterior.scale**2))\n\n  with tf1.control_dependencies([\n      q_a1.loc, q_a1.scale, p_a1.loc, p_a1.scale, a1, p_a2_loc, p_a2_scale,\n      z_a1_loc, z_a1_scale\n  ]):\n    log_density_ratio = q_a1.log_prob(a1) - p_a1.log_prob(a1)\n    prior_update = [\n        prior.loc.assign(a1 + p_a2_loc),\n        prior.untransformed_scale.assign(tfp.math.softplus_inverse(p_a2_scale))\n    ]\n    posterior_update = [\n        posterior.loc.assign(z_a1_loc),\n        posterior.untransformed_scale.assign(\n            tfp.math.softplus_inverse(z_a1_scale))\n    ]\n  return [prior_update, posterior_update], tf.reduce_sum(log_density_ratio)\n'"
experimental/auxiliary_sampling/sampling_test.py,13,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Tests for sampling.""""""\n\nfrom experimental.auxiliary_sampling import sampling  # local file import\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nimport tensorflow_probability as tfp\n\ntfd = tfp.distributions\n\n\nclass SamplingTest(tf.test.TestCase):\n\n  def _softplus_inverse_np(self, x):\n    return np.log(np.exp(x) - 1.)\n\n  def test_mean_field_fn(self):\n    p_fn, q_fn = sampling.mean_field_fn()\n    layer = tfp.layers.DenseLocalReparameterization(\n        100,\n        kernel_prior_fn=p_fn,\n        kernel_posterior_fn=q_fn,\n        bias_prior_fn=p_fn,\n        bias_posterior_fn=q_fn)\n    self.assertIsInstance(layer, tfp.layers.DenseLocalReparameterization)\n\n  def test_sample_auxiliary_op(self):\n    p_fn, q_fn = sampling.mean_field_fn()\n    p = p_fn(tf.float32, (), \'test_prior\', True, tf.get_variable).distribution\n    q = q_fn(tf.float32, (), \'test_posterior\', True,\n             tf.get_variable).distribution\n\n    # Test benign auxiliary variable\n    sample_op, _ = sampling.sample_auxiliary_op(p, q, 1e-10)\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n    p.loc.load(1., session=sess)\n    p.untransformed_scale.load(self._softplus_inverse_np(1.), session=sess)\n    q.loc.load(1.1, session=sess)\n    q.untransformed_scale.load(self._softplus_inverse_np(0.5), session=sess)\n    print(sess.run(q.scale))\n\n    sess.run(sample_op)\n\n    tolerance = 0.0001\n    self.assertLess(np.abs(sess.run(p.scale) - 1.), tolerance)\n    self.assertLess(np.abs(sess.run(p.loc) - 1.), tolerance)\n    self.assertLess(np.abs(sess.run(q.scale) - 0.5), tolerance)\n    self.assertLess(np.abs(sess.run(q.loc) - 1.1), tolerance)\n\n    # Test fully determining auxiliary variable\n    sample_op, _ = sampling.sample_auxiliary_op(p, q, 1. - 1e-10)\n    sess.run(tf.initialize_all_variables())\n    p.loc.load(1., session=sess)\n    p.untransformed_scale.load(self._softplus_inverse_np(1.), session=sess)\n    q.loc.load(1.1, session=sess)\n    q.untransformed_scale.load(self._softplus_inverse_np(.5), session=sess)\n\n    sess.run(sample_op)\n\n    self.assertLess(np.abs(sess.run(q.loc) - sess.run(p.loc)), tolerance)\n    self.assertLess(sess.run(p.scale), tolerance)\n    self.assertLess(sess.run(q.scale), tolerance)\n\n    # Test delta posterior\n    sample_op, _ = sampling.sample_auxiliary_op(p, q, 0.5)\n    sess.run(tf.initialize_all_variables())\n    p.loc.load(1., session=sess)\n    p.untransformed_scale.load(self._softplus_inverse_np(1.), session=sess)\n    q.loc.load(1.1, session=sess)\n    q.untransformed_scale.load(self._softplus_inverse_np(1e-10), session=sess)\n\n    sess.run(sample_op)\n\n    self.assertLess(np.abs(sess.run(q.loc) - 1.1), tolerance)\n    self.assertLess(sess.run(q.scale), tolerance)\n\n    # Test prior is posterior\n    sample_op, _ = sampling.sample_auxiliary_op(p, q, 0.5)\n    sess.run(tf.initialize_all_variables())\n    p.loc.load(1., session=sess)\n    p.untransformed_scale.load(self._softplus_inverse_np(1.), session=sess)\n    q.loc.load(1., session=sess)\n    q.untransformed_scale.load(self._softplus_inverse_np(1.), session=sess)\n\n    sess.run(sample_op)\n\n    self.assertLess(np.abs(sess.run(q.loc - p.loc)), tolerance)\n    self.assertLess(np.abs(sess.run(q.scale - p.scale)), tolerance)\n\n\ndef softplus_inverse_test(self):\n  sess = tf.Session()\n  self.assertEqual(sess.run(sampling.softplus_inverse(tf.nn.softplus(5.))), 5.)\n\n\nif __name__ == \'__main__\':\n  tf.disable_v2_behavior()\n  tf.test.main()\n'"
experimental/rank1_bnns/cifar.py,51,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Wide ResNet 28-10 with rank-1 distributions on CIFAR-10 and CIFAR-100.""""""\nimport functools\nimport os\nimport time\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nfrom baselines.cifar import utils  # local file import\nfrom experimental.rank1_bnns import cifar_model  # local file import\nfrom experimental.rank1_bnns import refining  # local file import\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nflags.DEFINE_integer(\'kl_annealing_epochs\', 200,\n                     \'Number of epoch over which to anneal the KL term to 1.\')\nflags.DEFINE_string(\'alpha_initializer\', \'trainable_normal\',\n                    \'Initializer name for the alpha parameters.\')\nflags.DEFINE_string(\'gamma_initializer\', \'trainable_normal\',\n                    \'Initializer name for the gamma parameters.\')\nflags.DEFINE_string(\'alpha_regularizer\', \'normal_kl_divergence\',\n                    \'Regularizer name for the alpha parameters.\')\nflags.DEFINE_string(\'gamma_regularizer\', \'normal_kl_divergence\',\n                    \'Regularizer name for the gamma parameters.\')\nflags.DEFINE_boolean(\'use_additive_perturbation\', False,\n                     \'Use additive perturbations instead of multiplicative.\')\nflags.DEFINE_float(\'dropout_rate\', 1e-3,\n                   \'Dropout rate. Only used if alpha/gamma initializers are, \'\n                   \'e.g., trainable normal.\')\nflags.DEFINE_float(\'prior_mean\', 1., \'Prior mean.\')\nflags.DEFINE_float(\'prior_stddev\', 0.1,\n                   \'Prior stddev. Sort of like a prior on dropout rate, where \'\n                   \'it encourages defaulting/shrinking to this value.\')\n\nflags.DEFINE_integer(\'ensemble_size\', 4, \'Size of ensemble.\')\nflags.DEFINE_integer(\'per_core_batch_size\', 64,\n                     \'Batch size per TPU core/GPU. The number of new \'\n                     \'datapoints gathered per batch is this number divided by \'\n                     \'ensemble_size (we tile the batch by that # of times).\')\nflags.DEFINE_float(\'random_sign_init\', 0.5,\n                   \'Use random sign init for fast weights.\')\nflags.DEFINE_integer(\'seed\', 0, \'Random seed.\')\nflags.DEFINE_float(\'fast_weight_lr_multiplier\', 1.0,\n                   \'fast weights lr multiplier.\')\nflags.DEFINE_float(\'base_learning_rate\', 0.1,\n                   \'Base learning rate when total training batch size is 128.\')\nflags.DEFINE_integer(\'lr_warmup_epochs\', 1,\n                     \'Number of epochs for a linear warmup to the initial \'\n                     \'learning rate. Use 0 to do no warmup.\')\nflags.DEFINE_float(\'lr_decay_ratio\', 0.2, \'Amount to decay learning rate.\')\nflags.DEFINE_list(\'lr_decay_epochs\', [\'80\', \'160\', \'180\'],\n                  \'Epochs to decay learning rate by.\')\nflags.DEFINE_float(\'l2\', 1e-4, \'L2 coefficient.\')\nflags.DEFINE_enum(\'dataset\', \'cifar10\',\n                  enum_values=[\'cifar10\', \'cifar100\'],\n                  help=\'Dataset.\')\n# TODO(ghassen): consider adding CIFAR-100-C to TFDS.\nflags.DEFINE_string(\'cifar100_c_path\',\n                    \'\',\n                    \'Path to the TFRecords files for CIFAR-100-C. Only valid \'\n                    \'(and required) if dataset is cifar100 and corruptions.\')\nflags.DEFINE_integer(\'corruptions_interval\', 250,\n                     \'Number of epochs between evaluating on the corrupted \'\n                     \'test data. Use -1 to never evaluate.\')\nflags.DEFINE_integer(\'checkpoint_interval\', 25,\n                     \'Number of epochs between saving checkpoints. Use -1 to \'\n                     \'never save checkpoints.\')\nflags.DEFINE_integer(\'num_bins\', 15, \'Number of bins for ECE.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/cifar\',\n                    \'The directory where the model weights and \'\n                    \'training/evaluation summaries are stored.\')\nflags.DEFINE_integer(\'train_epochs\', 250, \'Number of training epochs.\')\n\nflags.DEFINE_integer(\'num_eval_samples\', 1,\n                     \'Number of model predictions to sample per example at \'\n                     \'eval time.\')\n# Refinement flags.\nflags.DEFINE_integer(\'refining_epochs\', 0,\n                     \'Number of refining epochs. At the default 0 epochs,\'\n                     \'no refining takes place.\')\nflags.DEFINE_integer(\'num_auxiliary_variables\', 10,\n                     \'Number of auxiliary variables.\')\nflags.DEFINE_float(\'auxiliary_variance_ratio\', 0.5,\n                   \'The variance ratio of each auxiliary variable and the \'\n                   \'prior. The prior variance is reduced by this ratio after \'\n                   \'sampling each auxiliary variable.\')\nflags.DEFINE_float(\'refining_learning_rate\', 0.005,\n                   \'Learning rate during the refining phase.\')\nflags.DEFINE_bool(\'freeze_weights_during_refining\', True,\n                  \'Freeze the weight matrices during the refining phase.\')\n\n\n# Accelerator flags.\nflags.DEFINE_bool(\'use_gpu\', False, \'Whether to run on GPU or otherwise TPU.\')\nflags.DEFINE_bool(\'use_bfloat16\', False, \'Whether to use mixed precision.\')\nflags.DEFINE_integer(\'num_cores\', 8, \'Number of TPU cores or number of GPUs.\')\nflags.DEFINE_string(\'tpu\', None,\n                    \'Name of the TPU. Only used if use_gpu is False.\')\nFLAGS = flags.FLAGS\n\n\ndef main(argv):\n  del argv  # unused arg\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  logging.info(\'Saving checkpoints at %s\', FLAGS.output_dir)\n  tf.random.set_seed(FLAGS.seed)\n\n  if FLAGS.use_gpu:\n    logging.info(\'Use GPU\')\n    strategy = tf.distribute.MirroredStrategy()\n  else:\n    logging.info(\'Use TPU at %s\',\n                 FLAGS.tpu if FLAGS.tpu is not None else \'local\')\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n  train_input_fn = utils.load_input_fn(\n      split=tfds.Split.TRAIN,\n      name=FLAGS.dataset,\n      batch_size=FLAGS.per_core_batch_size // FLAGS.ensemble_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  clean_test_input_fn = utils.load_input_fn(\n      split=tfds.Split.TEST,\n      name=FLAGS.dataset,\n      batch_size=FLAGS.per_core_batch_size // FLAGS.ensemble_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  train_dataset = strategy.experimental_distribute_datasets_from_function(\n      train_input_fn)\n  test_datasets = {\n      \'clean\': strategy.experimental_distribute_datasets_from_function(\n          clean_test_input_fn),\n  }\n  if FLAGS.corruptions_interval > 0:\n    if FLAGS.dataset == \'cifar10\':\n      load_c_input_fn = utils.load_cifar10_c_input_fn\n    else:\n      load_c_input_fn = functools.partial(utils.load_cifar100_c_input_fn,\n                                          path=FLAGS.cifar100_c_path)\n    corruption_types, max_intensity = utils.load_corrupted_test_info(\n        FLAGS.dataset)\n    for corruption in corruption_types:\n      for intensity in range(1, max_intensity + 1):\n        input_fn = load_c_input_fn(\n            corruption_name=corruption,\n            corruption_intensity=intensity,\n            batch_size=FLAGS.per_core_batch_size // FLAGS.ensemble_size,\n            use_bfloat16=FLAGS.use_bfloat16)\n        test_datasets[\'{0}_{1}\'.format(corruption, intensity)] = (\n            strategy.experimental_distribute_datasets_from_function(input_fn))\n\n  ds_info = tfds.builder(FLAGS.dataset).info\n  batch_size = ((FLAGS.per_core_batch_size // FLAGS.ensemble_size) *\n                FLAGS.num_cores)\n  train_dataset_size = ds_info.splits[\'train\'].num_examples\n  steps_per_epoch = train_dataset_size // batch_size\n  steps_per_eval = ds_info.splits[\'test\'].num_examples // batch_size\n  num_classes = ds_info.features[\'label\'].num_classes\n\n  if FLAGS.use_bfloat16:\n    policy = tf.keras.mixed_precision.experimental.Policy(\'mixed_bfloat16\')\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n\n  summary_writer = tf.summary.create_file_writer(\n      os.path.join(FLAGS.output_dir, \'summaries\'))\n\n  with strategy.scope():\n    logging.info(\'Building Keras model\')\n    model = cifar_model.wide_resnet(\n        input_shape=ds_info.features[\'image\'].shape,\n        depth=28,\n        width_multiplier=10,\n        num_classes=num_classes,\n        alpha_initializer=FLAGS.alpha_initializer,\n        gamma_initializer=FLAGS.gamma_initializer,\n        alpha_regularizer=FLAGS.alpha_regularizer,\n        gamma_regularizer=FLAGS.gamma_regularizer,\n        use_additive_perturbation=FLAGS.use_additive_perturbation,\n        ensemble_size=FLAGS.ensemble_size,\n        random_sign_init=FLAGS.random_sign_init,\n        dropout_rate=FLAGS.dropout_rate,\n        prior_mean=FLAGS.prior_mean,\n        prior_stddev=FLAGS.prior_stddev)\n    logging.info(\'Model input shape: %s\', model.input_shape)\n    logging.info(\'Model output shape: %s\', model.output_shape)\n    logging.info(\'Model number of weights: %s\', model.count_params())\n    # Linearly scale learning rate and the decay epochs by vanilla settings.\n    base_lr = FLAGS.base_learning_rate * batch_size / 128\n    lr_decay_epochs = [(int(start_epoch_str) * FLAGS.train_epochs) // 200\n                       for start_epoch_str in FLAGS.lr_decay_epochs]\n    lr_schedule = refining.LearningRateScheduleWithRefining(\n        steps_per_epoch,\n        base_lr,\n        decay_ratio=FLAGS.lr_decay_ratio,\n        decay_epochs=lr_decay_epochs,\n        warmup_epochs=FLAGS.lr_warmup_epochs,\n        train_epochs=FLAGS.train_epochs,\n        refining_learning_rate=FLAGS.refining_learning_rate)\n    optimizer = tf.keras.optimizers.SGD(lr_schedule,\n                                        momentum=0.9,\n                                        nesterov=True)\n    metrics = {\n        \'train/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'train/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'train/loss\': tf.keras.metrics.Mean(),\n        \'train/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        \'train/kl\': tf.keras.metrics.Mean(),\n        \'train/kl_scale\': tf.keras.metrics.Mean(),\n        \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'test/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'test/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n    }\n    if FLAGS.ensemble_size > 1:\n      for i in range(FLAGS.ensemble_size):\n        metrics[\'test/nll_member_{}\'.format(i)] = tf.keras.metrics.Mean()\n        metrics[\'test/accuracy_member_{}\'.format(i)] = (\n            tf.keras.metrics.SparseCategoricalAccuracy())\n    if FLAGS.corruptions_interval > 0:\n      corrupt_metrics = {}\n      for intensity in range(1, max_intensity + 1):\n        for corruption in corruption_types:\n          dataset_name = \'{0}_{1}\'.format(corruption, intensity)\n          corrupt_metrics[\'test/nll_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.Mean())\n          corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.SparseCategoricalAccuracy())\n          corrupt_metrics[\'test/ece_{}\'.format(dataset_name)] = (\n              ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins))\n\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n    initial_epoch = 0\n    if latest_checkpoint:\n      # checkpoint.restore must be within a strategy.scope() so that optimizer\n      # slot variables are mirrored.\n      checkpoint.restore(latest_checkpoint)\n      logging.info(\'Loaded checkpoint %s\', latest_checkpoint)\n      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n\n  @tf.function\n  def train_step(iterator):\n    """"""Training StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      if FLAGS.ensemble_size > 1:\n        images = tf.tile(images, [FLAGS.ensemble_size, 1, 1, 1])\n        labels = tf.tile(labels, [FLAGS.ensemble_size])\n\n      with tf.GradientTape() as tape:\n        logits = model(images, training=True)\n        if FLAGS.use_bfloat16:\n          logits = tf.cast(logits, tf.float32)\n        negative_log_likelihood = tf.reduce_mean(\n            tf.keras.losses.sparse_categorical_crossentropy(labels,\n                                                            logits,\n                                                            from_logits=True))\n        filtered_variables = []\n        for var in model.trainable_variables:\n          # Apply l2 on the BN parameters and bias terms. This\n          # excludes only fast weight approximate posterior/prior parameters,\n          # but pay caution to their naming scheme.\n          if (\'kernel\' in var.name or\n              \'batch_norm\' in var.name or\n              \'bias\' in var.name):\n            filtered_variables.append(tf.reshape(var, (-1,)))\n\n        l2_loss = FLAGS.l2 * 2 * tf.nn.l2_loss(\n            tf.concat(filtered_variables, axis=0))\n        kl = sum(model.losses) / train_dataset_size\n        kl_scale = tf.cast(optimizer.iterations + 1, kl.dtype)\n        kl_scale /= steps_per_epoch * FLAGS.kl_annealing_epochs\n        kl_scale = tf.minimum(1., kl_scale)\n        kl_loss = kl_scale * kl\n\n        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n        loss = negative_log_likelihood + l2_loss + kl_loss\n        scaled_loss = loss / strategy.num_replicas_in_sync\n\n      grads = tape.gradient(scaled_loss, model.trainable_variables)\n\n      # Separate learning rate implementation.\n      if FLAGS.fast_weight_lr_multiplier != 1.0:\n        grads_and_vars = []\n        for grad, var in zip(grads, model.trainable_variables):\n          # Apply different learning rate on the fast weight approximate\n          # posterior/prior parameters. This is excludes BN and slow weights,\n          # but pay caution to the naming scheme.\n          if (\'kernel\' not in var.name and\n              \'batch_norm\' not in var.name and\n              \'bias\' not in var.name):\n            grads_and_vars.append((grad * FLAGS.fast_weight_lr_multiplier, var))\n          else:\n            grads_and_vars.append((grad, var))\n        optimizer.apply_gradients(grads_and_vars)\n      else:\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n      probs = tf.nn.softmax(logits)\n      metrics[\'train/ece\'].update_state(labels, probs)\n      metrics[\'train/loss\'].update_state(loss)\n      metrics[\'train/negative_log_likelihood\'].update_state(\n          negative_log_likelihood)\n      metrics[\'train/kl\'].update_state(kl)\n      metrics[\'train/kl_scale\'].update_state(kl_scale)\n      metrics[\'train/accuracy\'].update_state(labels, logits)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  @tf.function\n  def test_step(iterator, dataset_name):\n    """"""Evaluation StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      if FLAGS.ensemble_size > 1:\n        images = tf.tile(images, [FLAGS.ensemble_size, 1, 1, 1])\n      logits = tf.reshape(\n          [model(images, training=False)\n           for _ in range(FLAGS.num_eval_samples)],\n          [FLAGS.num_eval_samples, FLAGS.ensemble_size, -1, num_classes])\n      if FLAGS.use_bfloat16:\n        logits = tf.cast(logits, tf.float32)\n      probs = tf.nn.softmax(logits)\n\n      if FLAGS.ensemble_size > 1:\n        per_probs = tf.reduce_mean(probs, axis=0)  # marginalize samples\n        for i in range(FLAGS.ensemble_size):\n          member_probs = per_probs[i]\n          member_loss = tf.keras.losses.sparse_categorical_crossentropy(\n              labels, member_probs)\n          metrics[\'test/nll_member_{}\'.format(i)].update_state(member_loss)\n          metrics[\'test/accuracy_member_{}\'.format(i)].update_state(\n              labels, member_probs)\n\n      # Negative log marginal likelihood computed in a numerically-stable way.\n      labels_broadcasted = tf.broadcast_to(\n          labels,\n          [FLAGS.num_eval_samples, FLAGS.ensemble_size, labels.shape[0]])\n      log_likelihoods = -tf.keras.losses.sparse_categorical_crossentropy(\n          labels_broadcasted, logits, from_logits=True)\n      negative_log_likelihood = tf.reduce_mean(\n          -tf.reduce_logsumexp(log_likelihoods, axis=[0, 1]) +\n          tf.math.log(float(FLAGS.num_eval_samples * FLAGS.ensemble_size)))\n      probs = tf.math.reduce_mean(probs, axis=[0, 1])  # marginalize\n\n      if dataset_name == \'clean\':\n        metrics[\'test/negative_log_likelihood\'].update_state(\n            negative_log_likelihood)\n        metrics[\'test/accuracy\'].update_state(labels, probs)\n        metrics[\'test/ece\'].update_state(labels, probs)\n      else:\n        corrupt_metrics[\'test/nll_{}\'.format(dataset_name)].update_state(\n            negative_log_likelihood)\n        corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n        corrupt_metrics[\'test/ece_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  train_iterator = iter(train_dataset)\n  start_time = time.time()\n  for epoch in range(initial_epoch, FLAGS.train_epochs + FLAGS.refining_epochs):\n    logging.info(\'Starting to run epoch: %s\', epoch)\n    if epoch in np.linspace(FLAGS.train_epochs,\n                            FLAGS.train_epochs + FLAGS.refining_epochs,\n                            FLAGS.num_auxiliary_variables,\n                            dtype=int):\n      logging.info(\'Sampling auxiliary variables with ratio %f\',\n                   FLAGS.auxiliary_variance_ratio)\n      refining.sample_rank1_auxiliaries(model, FLAGS.auxiliary_variance_ratio)\n      if FLAGS.freeze_weights_during_refining:\n        refining.freeze_rank1_weights(model)\n\n    for step in range(steps_per_epoch):\n      train_step(train_iterator)\n\n      current_step = epoch * steps_per_epoch + (step + 1)\n      max_steps = steps_per_epoch * (FLAGS.train_epochs + FLAGS.refining_epochs)\n      time_elapsed = time.time() - start_time\n      steps_per_sec = float(current_step) / time_elapsed\n      eta_seconds = (max_steps - current_step) / steps_per_sec\n      message = (\'{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. \'\n                 \'ETA: {:.0f} min. Time elapsed: {:.0f} min\'.format(\n                     current_step / max_steps,\n                     epoch + 1,\n                     FLAGS.train_epochs + FLAGS.refining_epochs,\n                     steps_per_sec,\n                     eta_seconds / 60,\n                     time_elapsed / 60))\n      if step % 20 == 0:\n        logging.info(message)\n\n    datasets_to_evaluate = {\'clean\': test_datasets[\'clean\']}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      datasets_to_evaluate = test_datasets\n    for dataset_name, test_dataset in datasets_to_evaluate.items():\n      test_iterator = iter(test_dataset)\n      logging.info(\'Testing on dataset %s\', dataset_name)\n      for step in range(steps_per_eval):\n        if step % 20 == 0:\n          logging.info(\'Starting to run eval step %s of epoch: %s\', step,\n                       epoch)\n        test_step(test_iterator, dataset_name)\n      logging.info(\'Done with testing on %s\', dataset_name)\n\n    corrupt_results = {}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      corrupt_results = utils.aggregate_corrupt_metrics(corrupt_metrics,\n                                                        corruption_types,\n                                                        max_intensity)\n\n    logging.info(\'Train Loss: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'train/loss\'].result(),\n                 metrics[\'train/accuracy\'].result() * 100)\n    logging.info(\'Test NLL: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'test/negative_log_likelihood\'].result(),\n                 metrics[\'test/accuracy\'].result() * 100)\n    if FLAGS.ensemble_size > 1:\n      for i in range(FLAGS.ensemble_size):\n        logging.info(\'Member %d Test Loss: %.4f, Accuracy: %.2f%%\',\n                     i, metrics[\'test/nll_member_{}\'.format(i)].result(),\n                     metrics[\'test/accuracy_member_{}\'.format(i)].result()*100)\n    total_results = {name: metric.result() for name, metric in metrics.items()}\n    total_results.update(corrupt_results)\n    with summary_writer.as_default():\n      for name, result in total_results.items():\n        tf.summary.scalar(name, result, step=epoch + 1)\n\n    for metric in metrics.values():\n      metric.reset_states()\n\n    if (FLAGS.checkpoint_interval > 0 and\n        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n      checkpoint_name = checkpoint.save(\n          os.path.join(FLAGS.output_dir, \'checkpoint\'))\n      logging.info(\'Saved checkpoint to %s\', checkpoint_name)\n\n  final_checkpoint_name = checkpoint.save(\n      os.path.join(FLAGS.output_dir, \'checkpoint\'))\n  logging.info(\'Saved last checkpoint to %s\', final_checkpoint_name)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
experimental/rank1_bnns/cifar_model.py,13,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Wide ResNet architecture with rank-1 distributions.""""""\nimport functools\nfrom experimental.rank1_bnns import rank1_bnn_layers  # local file import\nfrom experimental.rank1_bnns import utils  # local file import\nimport tensorflow as tf\n\nBatchNormalization = functools.partial(  # pylint: disable=invalid-name\n    tf.keras.layers.BatchNormalization,\n    epsilon=1e-5,  # using epsilon and momentum defaults from Torch\n    momentum=0.9)\nConv2DRank1 = functools.partial(  # pylint: disable=invalid-name\n    rank1_bnn_layers.Conv2DRank1,\n    kernel_size=3,\n    padding=\'same\',\n    use_bias=False,\n    kernel_initializer=\'he_normal\')\n\n\ndef basic_block(inputs,\n                filters,\n                strides,\n                alpha_initializer,\n                gamma_initializer,\n                alpha_regularizer,\n                gamma_regularizer,\n                use_additive_perturbation,\n                ensemble_size,\n                random_sign_init,\n                dropout_rate,\n                prior_mean,\n                prior_stddev):\n  """"""Basic residual block of two 3x3 convs.\n\n  Args:\n    inputs: tf.Tensor.\n    filters: Number of filters for Conv2D.\n    strides: Stride dimensions for Conv2D.\n    alpha_initializer: The initializer for the alpha parameters.\n    gamma_initializer: The initializer for the gamma parameters.\n    alpha_regularizer: The regularizer for the alpha parameters.\n    gamma_regularizer: The regularizer for the gamma parameters.\n    use_additive_perturbation: Whether or not to use additive perturbations\n      instead of multiplicative perturbations.\n    ensemble_size: Number of ensemble members.\n    random_sign_init: Value used to initialize trainable deterministic\n      initializers, as applicable. Values greater than zero result in\n      initialization to a random sign vector, where random_sign_init is the\n      probability of a 1 value. Values less than zero result in initialization\n      from a Gaussian with mean 1 and standard deviation equal to\n      -random_sign_init.\n    dropout_rate: Dropout rate.\n    prior_mean: Mean of the prior.\n    prior_stddev: Standard deviation of the prior.\n\n  Returns:\n    tf.Tensor.\n  """"""\n  x = inputs\n  y = inputs\n  y = BatchNormalization()(y)\n  y = tf.keras.layers.Activation(\'relu\')(y)\n  y = Conv2DRank1(\n      filters,\n      strides=strides,\n      alpha_initializer=utils.make_initializer(alpha_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      gamma_initializer=utils.make_initializer(gamma_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      alpha_regularizer=utils.make_regularizer(\n          alpha_regularizer, prior_mean, prior_stddev),\n      gamma_regularizer=utils.make_regularizer(\n          gamma_regularizer, prior_mean, prior_stddev),\n      use_additive_perturbation=use_additive_perturbation,\n      ensemble_size=ensemble_size)(y)\n  y = BatchNormalization()(y)\n  y = tf.keras.layers.Activation(\'relu\')(y)\n  y = Conv2DRank1(\n      filters,\n      strides=1,\n      alpha_initializer=utils.make_initializer(alpha_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      gamma_initializer=utils.make_initializer(gamma_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      alpha_regularizer=utils.make_regularizer(\n          alpha_regularizer, prior_mean, prior_stddev),\n      gamma_regularizer=utils.make_regularizer(\n          gamma_regularizer, prior_mean, prior_stddev),\n      use_additive_perturbation=use_additive_perturbation,\n      ensemble_size=ensemble_size)(y)\n  if not x.shape.is_compatible_with(y.shape):\n    x = Conv2DRank1(\n        filters,\n        kernel_size=1,\n        strides=strides,\n        alpha_initializer=utils.make_initializer(alpha_initializer,\n                                                 random_sign_init,\n                                                 dropout_rate),\n        gamma_initializer=utils.make_initializer(gamma_initializer,\n                                                 random_sign_init,\n                                                 dropout_rate),\n        alpha_regularizer=utils.make_regularizer(\n            alpha_regularizer, prior_mean, prior_stddev),\n        gamma_regularizer=utils.make_regularizer(\n            gamma_regularizer, prior_mean, prior_stddev),\n        use_additive_perturbation=use_additive_perturbation,\n        ensemble_size=ensemble_size)(x)\n  x = tf.keras.layers.add([x, y])\n  return x\n\n\ndef group(inputs, filters, strides, num_blocks, **kwargs):\n  """"""Group of residual blocks.""""""\n  x = basic_block(inputs, filters=filters, strides=strides, **kwargs)\n  for _ in range(num_blocks - 1):\n    x = basic_block(x, filters=filters, strides=1, **kwargs)\n  return x\n\n\ndef wide_resnet(input_shape,\n                depth,\n                width_multiplier,\n                num_classes,\n                alpha_initializer,\n                gamma_initializer,\n                alpha_regularizer,\n                gamma_regularizer,\n                use_additive_perturbation,\n                ensemble_size,\n                random_sign_init,\n                dropout_rate,\n                prior_mean,\n                prior_stddev):\n  """"""Builds Wide ResNet.\n\n  Following Zagoruyko and Komodakis (2016), it accepts a width multiplier on the\n  number of filters. Using three groups of residual blocks, the network maps\n  spatial features of size 32x32 -> 16x16 -> 8x8.\n\n  Args:\n    input_shape: tf.Tensor.\n    depth: Total number of convolutional layers. ""n"" in WRN-n-k. It differs from\n      He et al. (2015)\'s notation which uses the maximum depth of the network\n      counting non-conv layers like dense.\n    width_multiplier: Integer to multiply the number of typical filters by. ""k""\n      in WRN-n-k.\n    num_classes: Number of output classes.\n    alpha_initializer: The initializer for the alpha parameters.\n    gamma_initializer: The initializer for the gamma parameters.\n    alpha_regularizer: The regularizer for the alpha parameters.\n    gamma_regularizer: The regularizer for the gamma parameters.\n    use_additive_perturbation: Whether or not to use additive perturbations\n      instead of multiplicative perturbations.\n    ensemble_size: Number of ensemble members.\n    random_sign_init: Value used to initialize trainable deterministic\n      initializers, as applicable. Values greater than zero result in\n      initialization to a random sign vector, where random_sign_init is the\n      probability of a 1 value. Values less than zero result in initialization\n      from a Gaussian with mean 1 and standard deviation equal to\n      -random_sign_init.\n    dropout_rate: Dropout rate.\n    prior_mean: Mean of the prior.\n    prior_stddev: Standard deviation of the prior.\n\n  Returns:\n    tf.keras.Model.\n  """"""\n  if (depth - 4) % 6 != 0:\n    raise ValueError(\'depth should be 6n+4 (e.g., 16, 22, 28, 40).\')\n  num_blocks = (depth - 4) // 6\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  x = Conv2DRank1(\n      16,\n      strides=1,\n      alpha_initializer=utils.make_initializer(alpha_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      gamma_initializer=utils.make_initializer(gamma_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      alpha_regularizer=utils.make_regularizer(\n          alpha_regularizer, prior_mean, prior_stddev),\n      gamma_regularizer=utils.make_regularizer(\n          gamma_regularizer, prior_mean, prior_stddev),\n      use_additive_perturbation=use_additive_perturbation,\n      ensemble_size=ensemble_size)(inputs)\n  for strides, filters in zip([1, 2, 2], [16, 32, 64]):\n    x = group(x,\n              filters=filters * width_multiplier,\n              strides=strides,\n              num_blocks=num_blocks,\n              alpha_initializer=alpha_initializer,\n              gamma_initializer=gamma_initializer,\n              alpha_regularizer=alpha_regularizer,\n              gamma_regularizer=gamma_regularizer,\n              use_additive_perturbation=use_additive_perturbation,\n              ensemble_size=ensemble_size,\n              random_sign_init=random_sign_init,\n              dropout_rate=dropout_rate,\n              prior_mean=prior_mean,\n              prior_stddev=prior_stddev)\n\n  x = BatchNormalization()(x)\n  x = tf.keras.layers.Activation(\'relu\')(x)\n  x = tf.keras.layers.AveragePooling2D(pool_size=8)(x)\n  x = tf.keras.layers.Flatten()(x)\n  x = rank1_bnn_layers.DenseRank1(\n      num_classes,\n      alpha_initializer=utils.make_initializer(alpha_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      gamma_initializer=utils.make_initializer(gamma_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      kernel_initializer=\'he_normal\',\n      activation=None,\n      alpha_regularizer=utils.make_regularizer(\n          alpha_regularizer, prior_mean, prior_stddev),\n      gamma_regularizer=utils.make_regularizer(\n          gamma_regularizer, prior_mean, prior_stddev),\n      use_additive_perturbation=use_additive_perturbation,\n      ensemble_size=ensemble_size)(x)\n  return tf.keras.Model(inputs=inputs, outputs=x)\n'"
experimental/rank1_bnns/imagenet.py,58,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""ResNet-50 with rank-1 distributions on ImageNet.""""""\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nfrom baselines.imagenet import utils  # local file import\nfrom experimental.rank1_bnns import imagenet_model  # local file import\nimport tensorflow as tf\n\nflags.DEFINE_integer(\'kl_annealing_epochs\', 90,\n                     \'Number of epochs over which to anneal the KL term to 1.\')\nflags.DEFINE_string(\'alpha_initializer\', \'trainable_normal\',\n                    \'Initializer name for the alpha parameters.\')\nflags.DEFINE_string(\'gamma_initializer\', \'trainable_normal\',\n                    \'Initializer name for the gamma parameters.\')\nflags.DEFINE_string(\'alpha_regularizer\', \'normal_kl_divergence\',\n                    \'Regularizer name for the alpha parameters.\')\nflags.DEFINE_string(\'gamma_regularizer\', \'normal_kl_divergence\',\n                    \'Regularizer name for the gamma parameters.\')\nflags.DEFINE_boolean(\'use_additive_perturbation\', False,\n                     \'Use additive perturbations instead of multiplicative.\')\n\n# General model flags\nflags.DEFINE_integer(\'ensemble_size\', 4, \'Size of ensemble.\')\nflags.DEFINE_integer(\'per_core_batch_size\', 128, \'Batch size per TPU core/GPU.\')\nflags.DEFINE_float(\'random_sign_init\', 0.75,\n                   \'Use random sign init for fast weights.\')\nflags.DEFINE_integer(\'seed\', 0, \'Random seed.\')\nflags.DEFINE_float(\'base_learning_rate\', 0.1,\n                   \'Base learning rate when train batch size is 256.\')\nflags.DEFINE_float(\'dropout_rate\', 1e-3,\n                   \'Dropout rate. Only used if alpha/gamma initializers are, \'\n                   \'e.g., trainable normal with a fixed stddev.\')\nflags.DEFINE_float(\'prior_stddev\', 0.05,\n                   \'Prior stddev. Sort of like a prior on dropout rate, where \'\n                   \'it encourages defaulting/shrinking to this value.\')\nflags.DEFINE_float(\'l2\', 1e-4, \'L2 coefficient.\')\nflags.DEFINE_float(\'fast_weight_lr_multiplier\', 1.0,\n                   \'fast weights lr multiplier.\')\nflags.DEFINE_string(\'data_dir\', None, \'Path to training and testing data.\')\nflags.mark_flag_as_required(\'data_dir\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/imagenet\',\n                    \'The directory where the model weights and \'\n                    \'training/evaluation summaries are stored.\')\nflags.DEFINE_integer(\'train_epochs\', 135, \'Number of training epochs.\')\nflags.DEFINE_integer(\'corruptions_interval\', 135,\n                     \'Number of epochs between evaluating on the corrupted \'\n                     \'test data. Use -1 to never evaluate.\')\nflags.DEFINE_integer(\'checkpoint_interval\', 27,\n                     \'Number of epochs between saving checkpoints. Use -1 to \'\n                     \'never save checkpoints.\')\nflags.DEFINE_string(\'alexnet_errors_path\', None,\n                    \'Path to AlexNet corruption errors file.\')\nflags.DEFINE_integer(\'num_bins\', 15, \'Number of bins for ECE computation.\')\n\nflags.DEFINE_integer(\'num_eval_samples\', 1,\n                     \'Number of model predictions to sample per example at \'\n                     \'eval time.\')\n\n# Accelerator flags.\nflags.DEFINE_bool(\'use_gpu\', False, \'Whether to run on GPU or otherwise TPU.\')\nflags.DEFINE_bool(\'use_bfloat16\', True, \'Whether to use mixed precision.\')\nflags.DEFINE_integer(\'num_cores\', 32, \'Number of TPU cores or number of GPUs.\')\nflags.DEFINE_string(\'tpu\', None,\n                    \'Name of the TPU. Only used if use_gpu is False.\')\nFLAGS = flags.FLAGS\n\n# Number of images in ImageNet-1k train dataset.\nAPPROX_IMAGENET_TRAIN_IMAGES = 1281167\n# Number of images in eval dataset.\nIMAGENET_VALIDATION_IMAGES = 50000\nNUM_CLASSES = 1000\n\n_LR_SCHEDULE = [    # (multiplier, epoch to start) tuples\n    (1.0, 5), (0.1, 30), (0.01, 60), (0.001, 80)\n]\n\n\ndef main(argv):\n  del argv  # unused arg\n  tf.random.set_seed(FLAGS.seed)\n\n  per_core_batch_size = FLAGS.per_core_batch_size // FLAGS.ensemble_size\n  batch_size = per_core_batch_size * FLAGS.num_cores\n  steps_per_epoch = APPROX_IMAGENET_TRAIN_IMAGES // batch_size\n  steps_per_eval = IMAGENET_VALIDATION_IMAGES // batch_size\n\n  logging.info(\'Saving checkpoints at %s\', FLAGS.output_dir)\n\n  if FLAGS.use_gpu:\n    logging.info(\'Use GPU\')\n    strategy = tf.distribute.MirroredStrategy()\n  else:\n    logging.info(\'Use TPU at %s\',\n                 FLAGS.tpu if FLAGS.tpu is not None else \'local\')\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n  imagenet_train = utils.ImageNetInput(\n      is_training=True,\n      data_dir=FLAGS.data_dir,\n      batch_size=per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  imagenet_eval = utils.ImageNetInput(\n      is_training=False,\n      data_dir=FLAGS.data_dir,\n      batch_size=per_core_batch_size,\n      use_bfloat16=FLAGS.use_bfloat16)\n  test_datasets = {\n      \'clean\':\n          strategy.experimental_distribute_datasets_from_function(\n              imagenet_eval.input_fn),\n  }\n  if FLAGS.corruptions_interval > 0:\n    corruption_types, max_intensity = utils.load_corrupted_test_info()\n    for name in corruption_types:\n      for intensity in range(1, max_intensity + 1):\n        dataset_name = \'{0}_{1}\'.format(name, intensity)\n        corrupt_input_fn = utils.corrupt_test_input_fn(\n            batch_size=per_core_batch_size,\n            corruption_name=name,\n            corruption_intensity=intensity,\n            use_bfloat16=FLAGS.use_bfloat16)\n        test_datasets[dataset_name] = (\n            strategy.experimental_distribute_datasets_from_function(\n                corrupt_input_fn))\n\n  train_dataset = strategy.experimental_distribute_datasets_from_function(\n      imagenet_train.input_fn)\n\n  if FLAGS.use_bfloat16:\n    policy = tf.keras.mixed_precision.experimental.Policy(\'mixed_bfloat16\')\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n\n  summary_writer = tf.summary.create_file_writer(\n      os.path.join(FLAGS.output_dir, \'summaries\'))\n\n  with strategy.scope():\n    logging.info(\'Building Keras ResNet-50 model\')\n    model = imagenet_model.rank1_resnet50(\n        input_shape=(224, 224, 3),\n        num_classes=NUM_CLASSES,\n        alpha_initializer=FLAGS.alpha_initializer,\n        gamma_initializer=FLAGS.gamma_initializer,\n        alpha_regularizer=FLAGS.alpha_regularizer,\n        gamma_regularizer=FLAGS.gamma_regularizer,\n        use_additive_perturbation=FLAGS.use_additive_perturbation,\n        ensemble_size=FLAGS.ensemble_size,\n        random_sign_init=FLAGS.random_sign_init,\n        dropout_rate=FLAGS.dropout_rate,\n        prior_stddev=FLAGS.prior_stddev,\n        use_tpu=not FLAGS.use_gpu)\n    logging.info(\'Model input shape: %s\', model.input_shape)\n    logging.info(\'Model output shape: %s\', model.output_shape)\n    logging.info(\'Model number of weights: %s\', model.count_params())\n    # Scale learning rate and decay epochs by vanilla settings.\n    base_lr = FLAGS.base_learning_rate * batch_size / 256\n    learning_rate = utils.LearningRateSchedule(steps_per_epoch,\n                                               base_lr,\n                                               FLAGS.train_epochs,\n                                               _LR_SCHEDULE)\n    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate,\n                                        momentum=0.9,\n                                        nesterov=True)\n    metrics = {\n        \'train/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'train/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'train/loss\': tf.keras.metrics.Mean(),\n        \'train/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        \'train/kl\': tf.keras.metrics.Mean(),\n        \'train/kl_scale\': tf.keras.metrics.Mean(),\n        \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'test/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'test/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n    }\n    if FLAGS.corruptions_interval > 0:\n      corrupt_metrics = {}\n      for intensity in range(1, max_intensity + 1):\n        for corruption in corruption_types:\n          dataset_name = \'{0}_{1}\'.format(corruption, intensity)\n          corrupt_metrics[\'test/nll_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.Mean())\n          corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.SparseCategoricalAccuracy())\n          corrupt_metrics[\'test/ece_{}\'.format(dataset_name)] = (\n              ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins))\n\n    test_diversity = {}\n    training_diversity = {}\n    if FLAGS.ensemble_size > 1:\n      for i in range(FLAGS.ensemble_size):\n        metrics[\'test/nll_member_{}\'.format(i)] = tf.keras.metrics.Mean()\n        metrics[\'test/accuracy_member_{}\'.format(i)] = (\n            tf.keras.metrics.SparseCategoricalAccuracy())\n      test_diversity = {\n          \'test/disagreement\': tf.keras.metrics.Mean(),\n          \'test/average_kl\': tf.keras.metrics.Mean(),\n          \'test/cosine_similarity\': tf.keras.metrics.Mean(),\n      }\n      training_diversity = {\n          \'train/disagreement\': tf.keras.metrics.Mean(),\n          \'train/average_kl\': tf.keras.metrics.Mean(),\n          \'train/cosine_similarity\': tf.keras.metrics.Mean(),\n      }\n\n    logging.info(\'Finished building Keras ResNet-50 model\')\n\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n    initial_epoch = 0\n    if latest_checkpoint:\n      # checkpoint.restore must be within a strategy.scope() so that optimizer\n      # slot variables are mirrored.\n      checkpoint.restore(latest_checkpoint)\n      logging.info(\'Loaded checkpoint %s\', latest_checkpoint)\n      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n\n  @tf.function\n  def train_step(iterator):\n    """"""Training StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      if FLAGS.ensemble_size > 1:\n        images = tf.tile(images, [FLAGS.ensemble_size, 1, 1, 1])\n        labels = tf.tile(labels, [FLAGS.ensemble_size])\n\n      with tf.GradientTape() as tape:\n        logits = model(images, training=True)\n        if FLAGS.use_bfloat16:\n          logits = tf.cast(logits, tf.float32)\n\n        probs = tf.nn.softmax(logits)\n        if FLAGS.ensemble_size > 1:\n          per_probs = tf.reshape(\n              probs, tf.concat([[FLAGS.ensemble_size, -1], probs.shape[1:]], 0))\n          diversity_results = ed.metrics.average_pairwise_diversity(\n              per_probs, FLAGS.ensemble_size)\n\n        negative_log_likelihood = tf.reduce_mean(\n            tf.keras.losses.sparse_categorical_crossentropy(labels,\n                                                            logits,\n                                                            from_logits=True))\n        filtered_variables = []\n        for var in model.trainable_variables:\n          # Apply l2 on the BN parameters and bias terms. This\n          # excludes only fast weight approximate posterior/prior parameters,\n          # but pay caution to their naming scheme.\n          if (\'kernel\' in var.name or\n              \'batch_norm\' in var.name or\n              \'bias\' in var.name):\n            filtered_variables.append(tf.reshape(var, (-1,)))\n\n        l2_loss = FLAGS.l2 * 2 * tf.nn.l2_loss(\n            tf.concat(filtered_variables, axis=0))\n        kl = sum(model.losses) / APPROX_IMAGENET_TRAIN_IMAGES\n        kl_scale = tf.cast(optimizer.iterations + 1, kl.dtype)\n        kl_scale /= steps_per_epoch * FLAGS.kl_annealing_epochs\n        kl_scale = tf.minimum(1., kl_scale)\n        kl_loss = kl_scale * kl\n        loss = negative_log_likelihood + l2_loss + kl_loss\n        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n        scaled_loss = loss / strategy.num_replicas_in_sync\n\n      grads = tape.gradient(scaled_loss, model.trainable_variables)\n\n      # Separate learning rate implementation.\n      if FLAGS.fast_weight_lr_multiplier != 1.0:\n        grads_and_vars = []\n        for grad, var in zip(grads, model.trainable_variables):\n          # Apply different learning rate on the fast weights. This excludes BN\n          # and slow weights, but pay caution to the naming scheme.\n          if (\'batch_norm\' not in var.name and \'kernel\' not in var.name):\n            grads_and_vars.append((grad * FLAGS.fast_weight_lr_multiplier,\n                                   var))\n          else:\n            grads_and_vars.append((grad, var))\n        optimizer.apply_gradients(grads_and_vars)\n      else:\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n      metrics[\'train/ece\'].update_state(labels, probs)\n      metrics[\'train/loss\'].update_state(loss)\n      metrics[\'train/negative_log_likelihood\'].update_state(\n          negative_log_likelihood)\n      metrics[\'train/kl\'].update_state(kl)\n      metrics[\'train/kl_scale\'].update_state(kl_scale)\n      metrics[\'train/accuracy\'].update_state(labels, logits)\n      if FLAGS.ensemble_size > 1:\n        for k, v in diversity_results.items():\n          training_diversity[\'train/\' + k].update_state(v)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  @tf.function\n  def test_step(iterator, dataset_name):\n    """"""Evaluation StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      if FLAGS.ensemble_size > 1:\n        images = tf.tile(images, [FLAGS.ensemble_size, 1, 1, 1])\n      logits = tf.reshape(\n          [model(images, training=False)\n           for _ in range(FLAGS.num_eval_samples)],\n          [FLAGS.num_eval_samples, FLAGS.ensemble_size, -1, NUM_CLASSES])\n      if FLAGS.use_bfloat16:\n        logits = tf.cast(logits, tf.float32)\n      all_probs = tf.nn.softmax(logits)\n      probs = tf.math.reduce_mean(all_probs, axis=[0, 1])  # marginalize\n\n      # Negative log marginal likelihood computed in a numerically-stable way.\n      labels_broadcasted = tf.broadcast_to(\n          labels,\n          [FLAGS.num_eval_samples, FLAGS.ensemble_size, labels.shape[0]])\n      log_likelihoods = -tf.keras.losses.sparse_categorical_crossentropy(\n          labels_broadcasted, logits, from_logits=True)\n      negative_log_likelihood = tf.reduce_mean(\n          -tf.reduce_logsumexp(log_likelihoods, axis=[0, 1]) +\n          tf.math.log(float(FLAGS.num_eval_samples * FLAGS.ensemble_size)))\n\n      if dataset_name == \'clean\':\n        if FLAGS.ensemble_size > 1:\n          per_probs = tf.reduce_mean(all_probs, axis=0)  # marginalize samples\n          diversity_results = ed.metrics.average_pairwise_diversity(\n              per_probs, FLAGS.ensemble_size)\n          for k, v in diversity_results.items():\n            test_diversity[\'test/\' + k].update_state(v)\n          for i in range(FLAGS.ensemble_size):\n            member_probs = per_probs[i]\n            member_loss = tf.keras.losses.sparse_categorical_crossentropy(\n                labels, member_probs)\n            metrics[\'test/nll_member_{}\'.format(i)].update_state(member_loss)\n            metrics[\'test/accuracy_member_{}\'.format(i)].update_state(\n                labels, member_probs)\n\n        metrics[\'test/negative_log_likelihood\'].update_state(\n            negative_log_likelihood)\n        metrics[\'test/accuracy\'].update_state(labels, probs)\n        metrics[\'test/ece\'].update_state(labels, probs)\n      else:\n        corrupt_metrics[\'test/nll_{}\'.format(dataset_name)].update_state(\n            negative_log_likelihood)\n        corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n        corrupt_metrics[\'test/ece_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  train_iterator = iter(train_dataset)\n  start_time = time.time()\n\n  for epoch in range(initial_epoch, FLAGS.train_epochs):\n    logging.info(\'Starting to run epoch: %s\', epoch)\n    for step in range(steps_per_epoch):\n      train_step(train_iterator)\n\n      current_step = epoch * steps_per_epoch + (step + 1)\n      max_steps = steps_per_epoch * FLAGS.train_epochs\n      time_elapsed = time.time() - start_time\n      steps_per_sec = float(current_step) / time_elapsed\n      eta_seconds = (max_steps - current_step) / steps_per_sec\n      message = (\'{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. \'\n                 \'ETA: {:.0f} min. Time elapsed: {:.0f} min\'.format(\n                     current_step / max_steps,\n                     epoch + 1,\n                     FLAGS.train_epochs,\n                     steps_per_sec,\n                     eta_seconds / 60,\n                     time_elapsed / 60))\n      if step % 20 == 0:\n        logging.info(message)\n\n    datasets_to_evaluate = {\'clean\': test_datasets[\'clean\']}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      datasets_to_evaluate = test_datasets\n    for dataset_name, test_dataset in datasets_to_evaluate.items():\n      logging.info(\'Testing on dataset %s\', dataset_name)\n      test_iterator = iter(test_dataset)\n      for step in range(steps_per_eval):\n        if step % 20 == 0:\n          logging.info(\'Starting to run eval step %s of epoch: %s\', step,\n                       epoch)\n        test_step(test_iterator, dataset_name)\n      logging.info(\'Done with testing on %s\', dataset_name)\n\n    corrupt_results = {}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      corrupt_results = utils.aggregate_corrupt_metrics(\n          corrupt_metrics, corruption_types, max_intensity,\n          FLAGS.alexnet_errors_path)\n\n    logging.info(\'Train Loss: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'train/loss\'].result(),\n                 metrics[\'train/accuracy\'].result() * 100)\n    logging.info(\'Test NLL: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'test/negative_log_likelihood\'].result(),\n                 metrics[\'test/accuracy\'].result() * 100)\n\n    for i in range(FLAGS.ensemble_size):\n      logging.info(\'Member %d Test Loss: %.4f, Accuracy: %.2f%%\',\n                   i, metrics[\'test/nll_member_{}\'.format(i)].result(),\n                   metrics[\'test/accuracy_member_{}\'.format(i)].result() * 100)\n\n    total_metrics = metrics.copy()\n    total_metrics.update(training_diversity)\n    total_metrics.update(test_diversity)\n    total_results = {name: metric.result()\n                     for name, metric in total_metrics.items()}\n    total_results.update(corrupt_results)\n    with summary_writer.as_default():\n      for name, result in total_results.items():\n        tf.summary.scalar(name, result, step=epoch + 1)\n\n    for metric in total_metrics.values():\n      metric.reset_states()\n\n    if (FLAGS.checkpoint_interval > 0 and\n        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n      checkpoint_name = checkpoint.save(os.path.join(\n          FLAGS.output_dir, \'checkpoint\'))\n      logging.info(\'Saved checkpoint to %s\', checkpoint_name)\n\n  final_checkpoint_name = checkpoint.save(\n      os.path.join(FLAGS.output_dir, \'checkpoint\'))\n  logging.info(\'Saved last checkpoint to %s\', final_checkpoint_name)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
experimental/rank1_bnns/imagenet_model.py,14,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""ResNet-50 with rank-1 distributions.""""""\nimport functools\nimport string\nimport edward2 as ed\nfrom experimental.rank1_bnns import rank1_bnn_layers  # local file import\nfrom experimental.rank1_bnns import utils  # local file import\nimport tensorflow as tf\n\n# Use batch normalization defaults from Pytorch.\nBATCH_NORM_DECAY = 0.9\nBATCH_NORM_EPSILON = 1e-5\n\n\ndef bottleneck_block(inputs,\n                     filters,\n                     stage,\n                     block,\n                     strides,\n                     alpha_initializer,\n                     gamma_initializer,\n                     alpha_regularizer,\n                     gamma_regularizer,\n                     use_additive_perturbation,\n                     ensemble_size,\n                     random_sign_init,\n                     dropout_rate,\n                     prior_stddev,\n                     use_tpu):\n  """"""Residual block with 1x1 -> 3x3 -> 1x1 convs in main path.\n\n  Note that strides appear in the second conv (3x3) rather than the first (1x1).\n  This is also known as ""ResNet v1.5"" as it differs from He et al. (2015)\n  (http://torch.ch/blog/2016/02/04/resnets.html).\n\n  Args:\n    inputs: tf.Tensor.\n    filters: list of integers, the filters of 3 conv layer at main path\n    stage: integer, current stage label, used for generating layer names\n    block: \'a\',\'b\'..., current block label, used for generating layer names\n    strides: Strides for the second conv layer in the block.\n    alpha_initializer: The initializer for the alpha parameters.\n    gamma_initializer: The initializer for the gamma parameters.\n    alpha_regularizer: The regularizer for the alpha parameters.\n    gamma_regularizer: The regularizer for the gamma parameters.\n    use_additive_perturbation: Whether or not to use additive perturbations\n      instead of multiplicative perturbations.\n    ensemble_size: Number of ensemble members.\n    random_sign_init: Value used to initialize trainable deterministic\n      initializers, as applicable. Values greater than zero result in\n      initialization to a random sign vector, where random_sign_init is the\n      probability of a 1 value. Values less than zero result in initialization\n      from a Gaussian with mean 1 and standard deviation equal to\n      -random_sign_init.\n    dropout_rate: Dropout rate.\n    prior_stddev: Standard deviation of the prior.\n    use_tpu: whether the model runs on TPU.\n\n  Returns:\n    tf.Tensor.\n  """"""\n  filters1, filters2, filters3 = filters\n  conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n  bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n  x = rank1_bnn_layers.Conv2DRank1(\n      filters1,\n      kernel_size=1,\n      use_bias=False,\n      alpha_initializer=utils.make_initializer(alpha_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      gamma_initializer=utils.make_initializer(gamma_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      kernel_initializer=\'he_normal\',\n      alpha_regularizer=utils.make_regularizer(\n          alpha_regularizer, 1., prior_stddev),\n      gamma_regularizer=utils.make_regularizer(\n          gamma_regularizer, 1., prior_stddev),\n      use_additive_perturbation=use_additive_perturbation,\n      name=conv_name_base + \'2a\',\n      ensemble_size=ensemble_size)(inputs)\n  x = ed.layers.ensemble_batchnorm(\n      x,\n      ensemble_size=ensemble_size,\n      use_tpu=use_tpu,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=bn_name_base+\'2a\')\n  x = tf.keras.layers.Activation(\'relu\')(x)\n\n  x = rank1_bnn_layers.Conv2DRank1(\n      filters2,\n      kernel_size=3,\n      strides=strides,\n      padding=\'same\',\n      use_bias=False,\n      alpha_initializer=utils.make_initializer(alpha_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      gamma_initializer=utils.make_initializer(gamma_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      kernel_initializer=\'he_normal\',\n      alpha_regularizer=utils.make_regularizer(\n          alpha_regularizer, 1., prior_stddev),\n      gamma_regularizer=utils.make_regularizer(\n          gamma_regularizer, 1., prior_stddev),\n      use_additive_perturbation=use_additive_perturbation,\n      name=conv_name_base + \'2b\',\n      ensemble_size=ensemble_size)(x)\n  x = ed.layers.ensemble_batchnorm(\n      x,\n      ensemble_size=ensemble_size,\n      use_tpu=use_tpu,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=bn_name_base+\'2b\')\n  x = tf.keras.layers.Activation(\'relu\')(x)\n\n  x = rank1_bnn_layers.Conv2DRank1(\n      filters3,\n      kernel_size=1,\n      use_bias=False,\n      alpha_initializer=utils.make_initializer(alpha_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      gamma_initializer=utils.make_initializer(gamma_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      kernel_initializer=\'he_normal\',\n      alpha_regularizer=utils.make_regularizer(\n          alpha_regularizer, 1., prior_stddev),\n      gamma_regularizer=utils.make_regularizer(\n          gamma_regularizer, 1., prior_stddev),\n      use_additive_perturbation=use_additive_perturbation,\n      name=conv_name_base + \'2c\',\n      ensemble_size=ensemble_size)(x)\n  x = ed.layers.ensemble_batchnorm(\n      x,\n      ensemble_size=ensemble_size,\n      use_tpu=use_tpu,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=bn_name_base+\'2c\')\n\n  shortcut = inputs\n  if not x.shape.is_compatible_with(shortcut.shape):\n    shortcut = rank1_bnn_layers.Conv2DRank1(\n        filters3,\n        kernel_size=1,\n        strides=strides,\n        use_bias=False,\n        alpha_initializer=utils.make_initializer(alpha_initializer,\n                                                 random_sign_init,\n                                                 dropout_rate),\n        gamma_initializer=utils.make_initializer(gamma_initializer,\n                                                 random_sign_init,\n                                                 dropout_rate),\n        kernel_initializer=\'he_normal\',\n        alpha_regularizer=utils.make_regularizer(\n            alpha_regularizer, 1., prior_stddev),\n        gamma_regularizer=utils.make_regularizer(\n            gamma_regularizer, 1., prior_stddev),\n        use_additive_perturbation=use_additive_perturbation,\n        name=conv_name_base + \'1\',\n        ensemble_size=ensemble_size)(inputs)\n    shortcut = ed.layers.ensemble_batchnorm(\n        shortcut,\n        ensemble_size=ensemble_size,\n        use_tpu=use_tpu,\n        momentum=BATCH_NORM_DECAY,\n        epsilon=BATCH_NORM_EPSILON,\n        name=bn_name_base+\'1\')\n\n  x = tf.keras.layers.add([x, shortcut])\n  x = tf.keras.layers.Activation(\'relu\')(x)\n  return x\n\n\ndef group(inputs,\n          filters,\n          num_blocks,\n          stage,\n          strides,\n          alpha_initializer,\n          gamma_initializer,\n          alpha_regularizer,\n          gamma_regularizer,\n          use_additive_perturbation,\n          ensemble_size,\n          random_sign_init,\n          dropout_rate,\n          prior_stddev,\n          use_tpu):\n  """"""Group of residual blocks.""""""\n  bottleneck_block_ = functools.partial(\n      bottleneck_block,\n      filters=filters,\n      stage=stage,\n      alpha_initializer=alpha_initializer,\n      gamma_initializer=gamma_initializer,\n      alpha_regularizer=alpha_regularizer,\n      gamma_regularizer=gamma_regularizer,\n      use_additive_perturbation=use_additive_perturbation,\n      ensemble_size=ensemble_size,\n      random_sign_init=random_sign_init,\n      dropout_rate=dropout_rate,\n      prior_stddev=prior_stddev,\n      use_tpu=use_tpu)\n  blocks = string.ascii_lowercase\n  x = bottleneck_block_(inputs, block=blocks[0], strides=strides)\n  for i in range(num_blocks - 1):\n    x = bottleneck_block_(x, block=blocks[i + 1], strides=1)\n  return x\n\n\ndef rank1_resnet50(input_shape,\n                   num_classes,\n                   alpha_initializer,\n                   gamma_initializer,\n                   alpha_regularizer,\n                   gamma_regularizer,\n                   use_additive_perturbation,\n                   ensemble_size,\n                   random_sign_init,\n                   dropout_rate,\n                   prior_stddev,\n                   use_tpu):\n  """"""Builds ResNet50 with rank 1 priors.\n\n  Using strided conv, pooling, four groups of residual blocks, and pooling, the\n  network maps spatial features of size 224x224 -> 112x112 -> 56x56 -> 28x28 ->\n  14x14 -> 7x7 (Table 1 of He et al. (2015)).\n\n  Args:\n    input_shape: Shape tuple of input excluding batch dimension.\n    num_classes: Number of output classes.\n    alpha_initializer: The initializer for the alpha parameters.\n    gamma_initializer: The initializer for the gamma parameters.\n    alpha_regularizer: The regularizer for the alpha parameters.\n    gamma_regularizer: The regularizer for the gamma parameters.\n    use_additive_perturbation: Whether or not to use additive perturbations\n      instead of multiplicative perturbations.\n    ensemble_size: Number of ensemble members.\n    random_sign_init: Value used to initialize trainable deterministic\n      initializers, as applicable. Values greater than zero result in\n      initialization to a random sign vector, where random_sign_init is the\n      probability of a 1 value. Values less than zero result in initialization\n      from a Gaussian with mean 1 and standard deviation equal to\n      -random_sign_init.\n    dropout_rate: Dropout rate.\n    prior_stddev: Standard deviation of the prior.\n    use_tpu: whether the model runs on TPU.\n\n  Returns:\n    tf.keras.Model.\n  """"""\n  group_ = functools.partial(\n      group,\n      alpha_initializer=alpha_initializer,\n      gamma_initializer=gamma_initializer,\n      alpha_regularizer=alpha_regularizer,\n      gamma_regularizer=gamma_regularizer,\n      use_additive_perturbation=use_additive_perturbation,\n      ensemble_size=ensemble_size,\n      random_sign_init=random_sign_init,\n      dropout_rate=dropout_rate,\n      prior_stddev=prior_stddev,\n      use_tpu=use_tpu)\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  x = tf.keras.layers.ZeroPadding2D(padding=3, name=\'conv1_pad\')(inputs)\n  x = rank1_bnn_layers.Conv2DRank1(\n      64,\n      kernel_size=7,\n      strides=2,\n      padding=\'valid\',\n      use_bias=False,\n      alpha_initializer=utils.make_initializer(alpha_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      gamma_initializer=utils.make_initializer(gamma_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      kernel_initializer=\'he_normal\',\n      alpha_regularizer=utils.make_regularizer(\n          alpha_regularizer, 1., prior_stddev),\n      gamma_regularizer=utils.make_regularizer(\n          gamma_regularizer, 1., prior_stddev),\n      use_additive_perturbation=use_additive_perturbation,\n      name=\'conv1\',\n      ensemble_size=ensemble_size)(x)\n  x = ed.layers.ensemble_batchnorm(\n      x,\n      ensemble_size=ensemble_size,\n      use_tpu=use_tpu,\n      momentum=BATCH_NORM_DECAY,\n      epsilon=BATCH_NORM_EPSILON,\n      name=\'bn_conv1\')\n  x = tf.keras.layers.Activation(\'relu\')(x)\n  x = tf.keras.layers.MaxPooling2D(3, strides=(2, 2), padding=\'same\')(x)\n  x = group_(x, [64, 64, 256], stage=2, num_blocks=3, strides=1)\n  x = group_(x, [128, 128, 512], stage=3, num_blocks=4, strides=2)\n  x = group_(x, [256, 256, 1024], stage=4, num_blocks=6, strides=2)\n  x = group_(x, [512, 512, 2048], stage=5, num_blocks=3, strides=2)\n  x = tf.keras.layers.GlobalAveragePooling2D(name=\'avg_pool\')(x)\n  x = rank1_bnn_layers.DenseRank1(\n      num_classes,\n      alpha_initializer=utils.make_initializer(alpha_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      gamma_initializer=utils.make_initializer(gamma_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.01),\n      alpha_regularizer=utils.make_regularizer(\n          alpha_regularizer, 1., prior_stddev),\n      gamma_regularizer=utils.make_regularizer(\n          gamma_regularizer, 1., prior_stddev),\n      use_additive_perturbation=use_additive_perturbation,\n      ensemble_size=ensemble_size,\n      activation=None,\n      name=\'fc1000\')(x)\n  return tf.keras.Model(inputs=inputs, outputs=x, name=\'resnet50\')\n'"
experimental/rank1_bnns/rank1_bnn_layers.py,111,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Rank-1 BNN layers.""""""\nimport edward2 as ed\nimport tensorflow as tf\n\n\n@ed.layers.utils.add_weight\nclass DenseRank1(tf.keras.layers.Layer):\n  """"""A rank-1 priors dense layer.\n\n  The argument ensemble_size selects the number of mixture components over all\n  weights, i.e., an ensemble of size `ensemble_size`. The layer performs a\n  forward pass by enumeration, returning a forward pass under each mixture\n  component. It takes an input tensor of shape\n  [ensemble_size*examples_per_model, input_dim] and returns an output tensor of\n  shape [ensemble_size*examples_per_model, units].\n\n  To use a different batch for each mixture, take a minibatch of size\n  ensemble_size*examples_per_model. To use the same batch for each mixture, get\n  a minibatch of size examples_per_model and tile it by ensemble_size before\n  applying any ensemble layers.\n  """"""\n\n  def __init__(self,\n               units,\n               activation=None,\n               use_bias=True,\n               alpha_initializer=\'trainable_normal\',\n               gamma_initializer=\'trainable_normal\',\n               kernel_initializer=\'glorot_uniform\',\n               bias_initializer=\'zeros\',\n               alpha_regularizer=\'normal_kl_divergence\',\n               gamma_regularizer=\'normal_kl_divergence\',\n               kernel_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               alpha_constraint=None,\n               gamma_constraint=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               use_additive_perturbation=False,\n               min_perturbation_value=-10,\n               max_perturbation_value=10,\n               ensemble_size=1,\n               **kwargs):\n    super().__init__(**kwargs)\n    self.units = units\n    self.activation = tf.keras.activations.get(activation)\n    self.use_bias = use_bias\n    self.alpha_initializer = ed.initializers.get(alpha_initializer)\n    self.gamma_initializer = ed.initializers.get(gamma_initializer)\n    self.bias_initializer = ed.initializers.get(bias_initializer)\n    self.alpha_regularizer = ed.regularizers.get(alpha_regularizer)\n    self.gamma_regularizer = ed.regularizers.get(gamma_regularizer)\n    self.bias_regularizer = ed.regularizers.get(bias_regularizer)\n    self.alpha_constraint = ed.constraints.get(alpha_constraint)\n    self.gamma_constraint = ed.constraints.get(gamma_constraint)\n    self.bias_constraint = ed.constraints.get(bias_constraint)\n    self.use_additive_perturbation = use_additive_perturbation\n    self.min_perturbation_value = min_perturbation_value\n    self.max_perturbation_value = max_perturbation_value\n    self.ensemble_size = ensemble_size\n    self.dense = tf.keras.layers.Dense(\n        units=units,\n        use_bias=False,\n        activation=None,\n        kernel_initializer=kernel_initializer,\n        kernel_regularizer=kernel_regularizer,\n        activity_regularizer=activity_regularizer,\n        kernel_constraint=kernel_constraint)\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    input_dim = input_shape[-1]\n    self.alpha = self.add_weight(\n        name=\'alpha\',\n        shape=[self.ensemble_size, input_dim],\n        initializer=self.alpha_initializer,\n        regularizer=self.alpha_regularizer,\n        constraint=self.alpha_constraint,\n        trainable=True,\n        dtype=self.dtype)\n    self.gamma = self.add_weight(\n        name=\'gamma\',\n        shape=[self.ensemble_size, self.units],\n        initializer=self.gamma_initializer,\n        regularizer=self.gamma_regularizer,\n        constraint=self.gamma_constraint,\n        trainable=True,\n        dtype=self.dtype)\n    if self.use_bias:\n      self.bias = self.add_weight(\n          name=\'bias\',\n          shape=[self.ensemble_size, self.units],\n          initializer=self.bias_initializer,\n          regularizer=self.bias_regularizer,\n          constraint=self.bias_constraint,\n          trainable=True,\n          dtype=self.dtype)\n      self.bias_shape = self.bias.shape\n    else:\n      self.bias = None\n      self.bias_shape = None\n    self.alpha_shape = self.alpha.shape\n    self.gamma_shape = self.gamma.shape\n    self.built = True\n\n  def call(self, inputs):\n    batch_size = tf.shape(inputs)[0]\n    input_dim = self.alpha_shape[-1]\n    examples_per_model = batch_size // self.ensemble_size\n    # NOTE: This restricts this layer from being called on tensors of ndim > 2.\n    inputs = tf.reshape(\n        inputs, [self.ensemble_size, examples_per_model, input_dim])\n\n    # Sample parameters for each example.\n    if isinstance(self.alpha_initializer, tf.keras.layers.Layer):\n      alpha = tf.clip_by_value(\n          self.alpha_initializer(\n              self.alpha_shape,\n              self.dtype).distribution.sample(examples_per_model),\n          self.min_perturbation_value,\n          self.max_perturbation_value)\n      alpha = tf.transpose(alpha, [1, 0, 2])\n    else:\n      alpha = tf.expand_dims(self.alpha, 1)\n    if isinstance(self.gamma_initializer, tf.keras.layers.Layer):\n      gamma = tf.clip_by_value(\n          self.gamma_initializer(\n              self.gamma_shape,\n              self.dtype).distribution.sample(examples_per_model),\n          self.min_perturbation_value,\n          self.max_perturbation_value)\n      gamma = tf.transpose(gamma, [1, 0, 2])\n    else:\n      gamma = tf.expand_dims(self.gamma, 1)\n\n    if self.use_additive_perturbation:\n      outputs = self.dense(inputs + alpha) + gamma\n    else:\n      outputs = self.dense(inputs * alpha) * gamma\n\n    if self.use_bias:\n      if isinstance(self.bias_initializer, tf.keras.layers.Layer):\n        bias = self.bias_initializer(\n            self.bias_shape, self.dtype).distribution.sample(examples_per_model)\n        bias = tf.transpose(bias, [1, 0, 2])\n      else:\n        bias = tf.expand_dims(self.bias, 1)\n      outputs += bias\n\n    if self.activation is not None:\n      outputs = self.activation(outputs)\n    outputs = tf.reshape(outputs, [batch_size, self.units])\n    return outputs\n\n  def get_config(self):\n    config = {\n        \'activation\': tf.keras.activations.serialize(self.activation),\n        \'use_bias\': self.use_bias,\n        \'alpha_initializer\': ed.initializers.serialize(self.alpha_initializer),\n        \'gamma_initializer\': ed.initializers.serialize(self.gamma_initializer),\n        \'bias_initializer\': ed.initializers.serialize(self.bias_initializer),\n        \'alpha_regularizer\': ed.regularizers.serialize(self.alpha_regularizer),\n        \'gamma_regularizer\': ed.regularizers.serialize(self.gamma_regularizer),\n        \'bias_regularizer\': ed.regularizers.serialize(self.bias_regularizer),\n        \'alpha_constraint\': ed.constraints.serialize(self.alpha_constraint),\n        \'gamma_constraint\': ed.constraints.serialize(self.gamma_constraint),\n        \'bias_constraint\': ed.constraints.serialize(self.bias_constraint),\n        \'use_additive_perturbation\': self.use_additive_perturbation,\n        \'ensemble_size\': self.ensemble_size,\n    }\n    base_config = super().get_config()\n    dense_config = self.dense.get_config()\n    return dict(\n        list(base_config.items()) +\n        list(dense_config.items()) +\n        list(config.items()))\n\n\n@ed.layers.utils.add_weight\nclass Conv2DRank1(tf.keras.layers.Layer):\n  """"""A rank-1 priors 2D convolution layer.\n\n  The argument ensemble_size selects the number of mixture components over all\n  weights, i.e., an ensemble of size `ensemble_size`. The layer performs a\n  forward pass by enumeration, returning a forward pass under each mixture\n  component. It takes an input tensor of shape\n  [ensemble_size*examples_per_model,] + input_shape and returns an output tensor\n  of shape [ensemble_size*examples_per_model,] + output_shape.\n\n  To use a different batch for each mixture, take a minibatch of size\n  ensemble_size*examples_per_model. To use the same batch for each mixture, get\n  a minibatch of size examples_per_model and tile it by ensemble_size before\n  applying any ensemble layers.\n  """"""\n\n  def __init__(self,\n               filters,\n               kernel_size,\n               strides=(1, 1),\n               padding=\'valid\',\n               data_format=None,\n               dilation_rate=(1, 1),\n               activation=None,\n               use_bias=True,\n               alpha_initializer=\'trainable_normal\',\n               gamma_initializer=\'trainable_normal\',\n               kernel_initializer=\'glorot_uniform\',\n               bias_initializer=\'zeros\',\n               alpha_regularizer=\'normal_kl_divergence\',\n               gamma_regularizer=\'normal_kl_divergence\',\n               kernel_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               alpha_constraint=None,\n               gamma_constraint=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               use_additive_perturbation=False,\n               min_perturbation_value=-10,\n               max_perturbation_value=10,\n               ensemble_size=1,\n               **kwargs):\n    super().__init__(**kwargs)\n    self.filters = filters\n    self.kernel_size = kernel_size\n    self.activation = tf.keras.activations.get(activation)\n    self.use_bias = use_bias\n    self.alpha_initializer = ed.initializers.get(alpha_initializer)\n    self.gamma_initializer = ed.initializers.get(gamma_initializer)\n    self.bias_initializer = ed.initializers.get(bias_initializer)\n    self.alpha_regularizer = ed.regularizers.get(alpha_regularizer)\n    self.gamma_regularizer = ed.regularizers.get(gamma_regularizer)\n    self.bias_regularizer = ed.regularizers.get(bias_regularizer)\n    self.alpha_constraint = ed.constraints.get(alpha_constraint)\n    self.gamma_constraint = ed.constraints.get(gamma_constraint)\n    self.bias_constraint = ed.constraints.get(bias_constraint)\n    self.use_additive_perturbation = use_additive_perturbation\n    self.min_perturbation_value = min_perturbation_value\n    self.max_perturbation_value = max_perturbation_value\n    self.ensemble_size = ensemble_size\n    self.conv2d = tf.keras.layers.Conv2D(\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        data_format=data_format,\n        activation=None,\n        use_bias=False,\n        kernel_initializer=kernel_initializer,\n        kernel_regularizer=kernel_regularizer,\n        activity_regularizer=activity_regularizer,\n        kernel_constraint=kernel_constraint)\n    self.data_format = self.conv2d.data_format\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    if self.data_format == \'channels_first\':\n      input_channel = input_shape[1]\n    elif self.data_format == \'channels_last\':\n      input_channel = input_shape[-1]\n\n    self.alpha = self.add_weight(\n        \'alpha\',\n        shape=[self.ensemble_size, input_channel],\n        initializer=self.alpha_initializer,\n        regularizer=self.alpha_regularizer,\n        constraint=self.alpha_constraint,\n        trainable=True,\n        dtype=self.dtype)\n    self.gamma = self.add_weight(\n        \'gamma\',\n        shape=[self.ensemble_size, self.filters],\n        initializer=self.gamma_initializer,\n        regularizer=self.gamma_regularizer,\n        constraint=self.gamma_constraint,\n        trainable=True,\n        dtype=self.dtype)\n    if self.use_bias:\n      self.bias = self.add_weight(\n          name=\'bias\',\n          shape=[self.ensemble_size, self.filters],\n          initializer=self.bias_initializer,\n          regularizer=self.bias_regularizer,\n          constraint=self.bias_constraint,\n          trainable=True,\n          dtype=self.dtype)\n      self.bias_shape = self.bias.shape\n    else:\n      self.bias = None\n      self.bias_shape = None\n    self.alpha_shape = self.alpha.shape\n    self.gamma_shape = self.gamma.shape\n    self.built = True\n\n  def call(self, inputs):\n    axis_change = -1 if self.data_format == \'channels_first\' else 1\n    batch_size = tf.shape(inputs)[0]\n    input_dim = self.alpha.shape[-1]\n    examples_per_model = batch_size // self.ensemble_size\n\n    # Sample parameters for each example.\n    if isinstance(self.alpha_initializer, tf.keras.layers.Layer):\n      alpha = tf.clip_by_value(\n          self.alpha_initializer(\n              self.alpha_shape,\n              self.dtype).distribution.sample(examples_per_model),\n          self.min_perturbation_value,\n          self.max_perturbation_value)\n      alpha = tf.transpose(alpha, [1, 0, 2])\n    else:\n      alpha = tf.tile(self.alpha, [1, examples_per_model])\n    if isinstance(self.gamma_initializer, tf.keras.layers.Layer):\n      gamma = tf.clip_by_value(\n          self.gamma_initializer(\n              self.gamma_shape,\n              self.dtype).distribution.sample(examples_per_model),\n          self.min_perturbation_value,\n          self.max_perturbation_value)\n      gamma = tf.transpose(gamma, [1, 0, 2])\n    else:\n      gamma = tf.tile(self.gamma, [1, examples_per_model])\n\n    alpha = tf.reshape(alpha, [batch_size, input_dim])\n    alpha = tf.expand_dims(alpha, axis=axis_change)\n    alpha = tf.expand_dims(alpha, axis=axis_change)\n    gamma = tf.reshape(gamma, [batch_size, self.filters])\n    gamma = tf.expand_dims(gamma, axis=axis_change)\n    gamma = tf.expand_dims(gamma, axis=axis_change)\n\n    if self.use_additive_perturbation:\n      outputs = self.conv2d(inputs + alpha) + gamma\n    else:\n      outputs = self.conv2d(inputs * alpha) * gamma\n\n    if self.use_bias:\n      if isinstance(self.bias_initializer, tf.keras.layers.Layer):\n        bias = self.bias_initializer(\n            self.bias_shape, self.dtype).distribution.sample(examples_per_model)\n        bias = tf.transpose(bias, [1, 0, 2])\n      else:\n        bias = tf.tile(self.bias, [1, examples_per_model])\n      bias = tf.reshape(bias, [batch_size, -1])\n      bias = tf.expand_dims(bias, axis=axis_change)\n      bias = tf.expand_dims(bias, axis=axis_change)\n      outputs += bias\n\n    if self.activation is not None:\n      outputs = self.activation(outputs)\n    return outputs\n\n  def get_config(self):\n    config = {\n        \'activation\': tf.keras.activations.serialize(self.activation),\n        \'use_bias\': self.use_bias,\n        \'alpha_initializer\': ed.initializers.serialize(self.alpha_initializer),\n        \'gamma_initializer\': ed.initializers.serialize(self.gamma_initializer),\n        \'bias_initializer\': ed.initializers.serialize(self.bias_initializer),\n        \'alpha_regularizer\': ed.regularizers.serialize(self.alpha_regularizer),\n        \'gamma_regularizer\': ed.regularizers.serialize(self.gamma_regularizer),\n        \'bias_regularizer\': ed.regularizers.serialize(self.bias_regularizer),\n        \'alpha_constraint\': ed.constraints.serialize(self.alpha_constraint),\n        \'gamma_constraint\': ed.constraints.serialize(self.gamma_constraint),\n        \'bias_constraint\': ed.constraints.serialize(self.bias_constraint),\n        \'use_additive_perturbation\': self.use_additive_perturbation,\n        \'ensemble_size\': self.ensemble_size,\n    }\n    base_config = super().get_config()\n    conv_config = self.conv2d.get_config()\n    return dict(\n        list(base_config.items()) +\n        list(conv_config.items()) +\n        list(config.items()))\n\n\n@ed.layers.utils.add_weight\nclass Conv1DRank1(tf.keras.layers.Layer):\n  """"""A rank-1 priors 1D convolution layer.\n\n  The argument ensemble_size selects the number of mixture components over all\n  weights, i.e., an ensemble of size `ensemble_size`. The layer performs a\n  forward pass by enumeration, returning a forward pass under each mixture\n  component. It takes an input tensor of shape\n  [ensemble_size*examples_per_model,] + input_shape and returns an output tensor\n  of shape [ensemble_size*examples_per_model,] + output_shape.\n\n  To use a different batch for each mixture, take a minibatch of size\n  ensemble_size*examples_per_model. To use the same batch for each mixture, get\n  a minibatch of size examples_per_model and tile it by ensemble_size before\n  applying any ensemble layers.\n  """"""\n\n  def __init__(self,\n               filters,\n               kernel_size,\n               strides=1,\n               padding=\'valid\',\n               data_format=\'channels_last\',\n               dilation_rate=1,\n               activation=None,\n               use_bias=True,\n               alpha_initializer=\'trainable_normal\',\n               gamma_initializer=\'trainable_normal\',\n               kernel_initializer=\'glorot_uniform\',\n               bias_initializer=\'zeros\',\n               alpha_regularizer=\'normal_kl_divergence\',\n               gamma_regularizer=\'normal_kl_divergence\',\n               kernel_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               alpha_constraint=None,\n               gamma_constraint=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               use_additive_perturbation=False,\n               min_perturbation_value=-10,\n               max_perturbation_value=10,\n               ensemble_size=1,\n               **kwargs):\n    super().__init__(**kwargs)\n    self.filters = filters\n    self.kernel_size = kernel_size\n    self.activation = tf.keras.activations.get(activation)\n    self.use_bias = use_bias\n    self.alpha_initializer = ed.initializers.get(alpha_initializer)\n    self.gamma_initializer = ed.initializers.get(gamma_initializer)\n    self.bias_initializer = ed.initializers.get(bias_initializer)\n    self.alpha_regularizer = ed.regularizers.get(alpha_regularizer)\n    self.gamma_regularizer = ed.regularizers.get(gamma_regularizer)\n    self.bias_regularizer = ed.regularizers.get(bias_regularizer)\n    self.alpha_constraint = ed.constraints.get(alpha_constraint)\n    self.gamma_constraint = ed.constraints.get(gamma_constraint)\n    self.bias_constraint = ed.constraints.get(bias_constraint)\n    self.use_additive_perturbation = use_additive_perturbation\n    self.min_perturbation_value = min_perturbation_value\n    self.max_perturbation_value = max_perturbation_value\n    self.ensemble_size = ensemble_size\n    self.conv1d = tf.keras.layers.Conv1D(\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        data_format=data_format,\n        dilation_rate=dilation_rate,\n        activation=None,\n        use_bias=False,\n        kernel_initializer=kernel_initializer,\n        kernel_regularizer=kernel_regularizer,\n        activity_regularizer=activity_regularizer,\n        kernel_constraint=kernel_constraint)\n    self.data_format = self.conv1d.data_format\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    if self.data_format == \'channels_first\':\n      input_channel = input_shape[1]\n    elif self.data_format == \'channels_last\':\n      input_channel = input_shape[-1]\n\n    self.alpha = self.add_weight(\n        \'alpha\',\n        shape=[self.ensemble_size, input_channel],\n        initializer=self.alpha_initializer,\n        regularizer=self.alpha_regularizer,\n        constraint=self.alpha_constraint,\n        trainable=True,\n        dtype=self.dtype)\n    self.gamma = self.add_weight(\n        \'gamma\',\n        shape=[self.ensemble_size, self.filters],\n        initializer=self.gamma_initializer,\n        regularizer=self.gamma_regularizer,\n        constraint=self.gamma_constraint,\n        trainable=True,\n        dtype=self.dtype)\n    if self.use_bias:\n      self.bias = self.add_weight(\n          name=\'bias\',\n          shape=[self.ensemble_size, self.filters],\n          initializer=self.bias_initializer,\n          regularizer=self.bias_regularizer,\n          constraint=self.bias_constraint,\n          trainable=True,\n          dtype=self.dtype)\n      self.bias_shape = self.bias.shape\n    else:\n      self.bias = None\n      self.bias_shape = None\n    self.alpha_shape = self.alpha.shape\n    self.gamma_shape = self.gamma.shape\n    self.built = True\n\n  def call(self, inputs):\n    axis_change = -1 if self.data_format == \'channels_first\' else 1\n    batch_size = tf.shape(inputs)[0]\n    input_dim = self.alpha_shape[-1]\n    examples_per_model = batch_size // self.ensemble_size\n\n    # Sample parameters for each example.\n    if isinstance(self.alpha_initializer, tf.keras.layers.Layer):\n      alpha = tf.clip_by_value(\n          self.alpha_initializer(\n              self.alpha_shape,\n              self.dtype).distribution.sample(examples_per_model),\n          self.min_perturbation_value,\n          self.max_perturbation_value)\n      alpha = tf.transpose(alpha, [1, 0, 2])\n    else:\n      alpha = tf.tile(self.alpha, [1, examples_per_model])\n    if isinstance(self.gamma_initializer, tf.keras.layers.Layer):\n      gamma = tf.clip_by_value(\n          self.gamma_initializer(\n              self.gamma_shape,\n              self.dtype).distribution.sample(examples_per_model),\n          self.min_perturbation_value,\n          self.max_perturbation_value)\n      gamma = tf.transpose(gamma, [1, 0, 2])\n    else:\n      gamma = tf.tile(self.gamma, [1, examples_per_model])\n\n    alpha = tf.reshape(alpha, [batch_size, input_dim])\n    alpha = tf.expand_dims(alpha, axis=axis_change)\n    gamma = tf.reshape(gamma, [batch_size, self.filters])\n    gamma = tf.expand_dims(gamma, axis=axis_change)\n\n    if self.use_additive_perturbation:\n      outputs = self.conv1d(inputs + alpha) + gamma\n    else:\n      outputs = self.conv1d(inputs * alpha) * gamma\n\n    if self.use_bias:\n      if isinstance(self.bias_initializer, tf.keras.layers.Layer):\n        bias = self.bias_initializer(\n            self.bias_shape, self.dtype).distribution.sample(examples_per_model)\n        bias = tf.transpose(bias, [1, 0, 2])\n      else:\n        bias = tf.tile(self.bias, [1, examples_per_model])\n      bias = tf.reshape(bias, [batch_size, self.filters])\n      bias = tf.expand_dims(bias, axis=axis_change)\n      outputs += bias\n    if self.activation is not None:\n      outputs = self.activation(outputs)\n    return outputs\n\n  def get_config(self):\n    config = {\n        \'activation\': tf.keras.activations.serialize(self.activation),\n        \'use_bias\': self.use_bias,\n        \'alpha_initializer\': ed.initializers.serialize(self.alpha_initializer),\n        \'gamma_initializer\': ed.initializers.serialize(self.gamma_initializer),\n        \'bias_initializer\': ed.initializers.serialize(self.bias_initializer),\n        \'alpha_regularizer\': ed.regularizers.serialize(self.alpha_regularizer),\n        \'gamma_regularizer\': ed.regularizers.serialize(self.gamma_regularizer),\n        \'bias_regularizer\': ed.regularizers.serialize(self.bias_regularizer),\n        \'alpha_constraint\': ed.constraints.serialize(self.alpha_constraint),\n        \'gamma_constraint\': ed.constraints.serialize(self.gamma_constraint),\n        \'bias_constraint\': ed.constraints.serialize(self.bias_constraint),\n        \'use_additive_perturbation\': self.use_additive_perturbation,\n        \'ensemble_size\': self.ensemble_size,\n    }\n    base_config = super().get_config()\n    conv_config = self.conv1d.get_config()\n    return dict(\n        list(base_config.items()) +\n        list(conv_config.items()) +\n        list(config.items()))\n\n\n@ed.layers.utils.add_weight\nclass LSTMCellRank1(tf.keras.layers.LSTMCell):\n  """"""A rank-1 priors LSTM cell.\n\n  The layer computes a variational Bayesian approximation to the distribution\n  over LSTM cell functions,\n\n  ```\n  p(outputs | inputs) = int lstm_cell(inputs; weights, bias) p(weights, bias)\n    dweights dbias,\n  ```\n\n  where the weights consist of both input and recurrent weights.\n\n  It does this with a stochastic forward pass, sampling from learnable\n  distributions on the kernel, recurrent kernel, and bias. Gradients with\n  respect to the distributions\' learnable parameters backpropagate via\n  reparameterization.  Minimizing cross-entropy plus the layer\'s losses performs\n  variational minimum description length, i.e., it minimizes an upper bound to\n  the negative marginal likelihood.\n\n  This layer uses the rank-1 setup whereby the priors and variational\n  distributions are over a rank-1 subspace of weights, where each weight matrix\n  is decomposed into the product of a matrix and the outer product of two\n  vectors, alpha and gamma. Rank-1 priors posit distributions over the alpha and\n  gamma vectors.\n\n  The argument ensemble_size selects the number of mixture components over all\n  weights, i.e., an ensemble of size `ensemble_size`. The layer performs a\n  forward pass by enumeration, returning a forward pass under each mixture\n  component. It takes an input tensor of shape\n  [ensemble_size*examples_per_model, input_dim] and returns an output tensor of\n  shape [ensemble_size*examples_per_model, units].\n\n  To use a different batch for each mixture, take a minibatch of size\n  ensemble_size*examples_per_model. To use the same batch for each mixture, get\n  a minibatch of size examples_per_model and tile it by ensemble_size before\n  applying any ensemble layers.\n  """"""\n\n  def __init__(self,\n               units,\n               activation=\'tanh\',\n               recurrent_activation=\'sigmoid\',\n               use_bias=True,\n               alpha_initializer=\'trainable_normal\',\n               gamma_initializer=\'trainable_normal\',\n               kernel_initializer=\'glorot_uniform\',\n               recurrent_alpha_initializer=\'trainable_normal\',\n               recurrent_gamma_initializer=\'trainable_normal\',\n               recurrent_initializer=\'orthogonal\',\n               bias_initializer=\'zeros\',\n               unit_forget_bias=True,\n               alpha_regularizer=\'normal_kl_divergence\',\n               gamma_regularizer=\'normal_kl_divergence\',\n               kernel_regularizer=None,\n               recurrent_alpha_regularizer=\'normal_kl_divergence\',\n               recurrent_gamma_regularizer=\'normal_kl_divergence\',\n               recurrent_regularizer=None,\n               bias_regularizer=None,\n               alpha_constraint=None,\n               gamma_constraint=None,\n               kernel_constraint=None,\n               recurrent_alpha_constraint=None,\n               recurrent_gamma_constraint=None,\n               recurrent_constraint=None,\n               bias_constraint=None,\n               dropout=0.,\n               recurrent_dropout=0.,\n               implementation=2,\n               use_additive_perturbation=False,\n               ensemble_size=1,\n               **kwargs):\n    """"""Initializes an LSTM cell layer.\n\n    Args:\n      units: Positive integer, dimensionality of the output space.\n      activation: Activation function to use. If you pass `None`, no activation\n        is applied (ie. ""linear"" activation is `a(x) = x`).\n      recurrent_activation: Activation function to use for the recurrent step.\n         If you pass `None`, no activation is applied (ie. ""linear"" activation\n         is `a(x) = x`).\n      use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n      alpha_initializer: Initializer for the rank-1 weights vector applied to\n        the inputs before the `kernel` is applied.\n      gamma_initializer: Initializer for the rank-1 weights vector applied to\n        after the `kernel` is applied.\n      kernel_initializer: Initializer for the `kernel` weights matrix, used for\n        the linear transformation of the inputs.\n      recurrent_alpha_initializer: Initializer for the rank-1 weights vector\n        applied to the recurrent state before the `recurrent_kernel` is applied.\n      recurrent_gamma_initializer: Initializer for the rank-1 weights vector\n        applied after the `recurrent_kernel` is applied.\n      recurrent_initializer: Initializer for the `recurrent_kernel` weights\n        matrix, used for the linear transformation of the recurrent state.\n      bias_initializer: Initializer for the bias vector.\n      unit_forget_bias: Boolean (default `True`). If True, add 1 to the bias of\n        the forget gate at initialization. Setting it to true will also force\n        `bias_initializer=""zeros""`. This is recommended in [Jozefowicz et\n          al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n      alpha_regularizer: Regularizer function applied to the `alpha` weights\n        vector.\n      gamma_regularizer: Regularizer function applied to the `gamma` weights\n        vector.\n      kernel_regularizer: Regularizer function applied to the `kernel` weights\n        matrix.\n      recurrent_alpha_regularizer: Regularizer function applied to the\n        `recurrent_alpha` weights vector.\n      recurrent_gamma_regularizer: Regularizer function applied to the\n        `recurrent_gamma` weights vector.\n      recurrent_regularizer: Regularizer function applied to the\n        `recurrent_kernel` weights matrix.\n      bias_regularizer: Regularizer function applied to the bias vector.\n      alpha_constraint: Constraint function applied to the `alpha` weights\n        vector.\n      gamma_constraint: Constraint function applied to the `gamma` weights\n        vector.\n      kernel_constraint: Constraint function applied to the `kernel` weights\n        matrix.\n      recurrent_alpha_constraint: Constraint function applied to the\n        `recurrent_alpha` weights vector.\n      recurrent_gamma_constraint: Constraint function applied to the\n        `recurrent_gamma` weights vector.\n      recurrent_constraint: Constraint function applied to the\n        `recurrent_kernel` weights matrix.\n      bias_constraint: Constraint function applied to the bias vector.\n      dropout: Float between 0 and 1. Fraction of the units to drop for the\n        linear transformation of the inputs.\n      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n        for the linear transformation of the recurrent state.\n      implementation: Implementation mode, either 1 or 2.\n        Mode 1 will structure its operations as a larger number of smaller dot\n        products and additions, whereas mode 2 (default) will batch them into\n        fewer, larger operations. These modes will have different performance\n        profiles on different hardware and for different applications.\n      use_additive_perturbation: Whether or not to use additive interactions vs.\n        multiplicative actions.\n      ensemble_size: Number of ensemble members, or equivalently, number of\n        mixture components.\n      **kwargs: Any additional arguments to pass to the superclasses.\n    """"""\n    self.alpha_initializer = ed.initializers.get(alpha_initializer)\n    self.gamma_initializer = ed.initializers.get(gamma_initializer)\n    self.recurrent_alpha_initializer = ed.initializers.get(\n        recurrent_alpha_initializer)\n    self.recurrent_gamma_initializer = ed.initializers.get(\n        recurrent_gamma_initializer)\n    self.alpha_regularizer = ed.regularizers.get(alpha_regularizer)\n    self.gamma_regularizer = ed.regularizers.get(gamma_regularizer)\n    self.recurrent_alpha_regularizer = ed.regularizers.get(\n        recurrent_alpha_regularizer)\n    self.recurrent_gamma_regularizer = ed.regularizers.get(\n        recurrent_gamma_regularizer)\n    self.alpha_constraint = ed.constraints.get(alpha_constraint)\n    self.gamma_constraint = ed.constraints.get(gamma_constraint)\n    self.recurrent_alpha_constraint = ed.constraints.get(\n        recurrent_alpha_constraint)\n    self.recurrent_gamma_constraint = ed.constraints.get(\n        recurrent_gamma_constraint)\n    self.use_additive_perturbation = use_additive_perturbation\n    self.ensemble_size = ensemble_size\n    self.sampled_weights = False\n    super().__init__(\n        units=units,\n        activation=activation,\n        recurrent_activation=recurrent_activation,\n        use_bias=use_bias,\n        kernel_initializer=kernel_initializer,\n        recurrent_initializer=recurrent_initializer,\n        bias_initializer=ed.initializers.get(bias_initializer),\n        unit_forget_bias=unit_forget_bias,\n        kernel_regularizer=kernel_regularizer,\n        recurrent_regularizer=recurrent_regularizer,\n        bias_regularizer=ed.regularizers.get(bias_regularizer),\n        kernel_constraint=kernel_constraint,\n        recurrent_constraint=recurrent_constraint,\n        bias_constraint=ed.constraints.get(bias_constraint),\n        dropout=dropout,\n        recurrent_dropout=recurrent_dropout,\n        implementation=implementation,\n        **kwargs)\n\n  def build(self, input_shape):\n    """"""Creates the variables of the layer.""""""\n    input_shape = tf.TensorShape(input_shape)\n    input_dim = input_shape[-1]\n    self.alpha = self.add_weight(\n        name=\'alpha\',\n        shape=[self.ensemble_size, input_dim],\n        initializer=self.alpha_initializer,\n        regularizer=self.alpha_regularizer,\n        constraint=self.alpha_constraint,\n        trainable=True,\n        dtype=self.dtype)\n    self.gamma = self.add_weight(\n        name=\'gamma\',\n        shape=[self.ensemble_size, self.units * 4],\n        initializer=self.gamma_initializer,\n        regularizer=self.gamma_regularizer,\n        constraint=self.gamma_constraint,\n        trainable=True,\n        dtype=self.dtype)\n    self.kernel = self.add_weight(\n        shape=(input_dim, self.units * 4),\n        name=\'kernel\',\n        initializer=self.kernel_initializer,\n        regularizer=self.kernel_regularizer,\n        constraint=self.kernel_constraint)\n    self.recurrent_alpha = self.add_weight(\n        name=\'recurrent_alpha\',\n        shape=[self.ensemble_size, self.units],\n        initializer=self.recurrent_alpha_initializer,\n        regularizer=self.recurrent_alpha_regularizer,\n        constraint=self.recurrent_alpha_constraint,\n        trainable=True,\n        dtype=self.dtype)\n    self.recurrent_gamma = self.add_weight(\n        name=\'recurrent_gamma\',\n        shape=[self.ensemble_size, self.units * 4],\n        initializer=self.recurrent_gamma_initializer,\n        regularizer=self.recurrent_gamma_regularizer,\n        constraint=self.recurrent_gamma_constraint,\n        trainable=True,\n        dtype=self.dtype)\n    self.recurrent_kernel = self.add_weight(\n        shape=(self.units, self.units * 4),\n        name=\'recurrent_kernel\',\n        initializer=self.recurrent_initializer,\n        regularizer=self.recurrent_regularizer,\n        constraint=self.recurrent_constraint)\n\n    if self.use_bias:\n      if (self.unit_forget_bias and not isinstance(self.bias_initializer,\n                                                   tf.keras.layers.Layer)):\n        def bias_initializer(_, *args, **kwargs):\n          return tf.concat([\n              self.bias_initializer([self.ensemble_size, self.units], *args,\n                                    **kwargs),\n              tf.keras.initializers.Ones()([self.ensemble_size, self.units],\n                                           *args, **kwargs),\n              self.bias_initializer([self.ensemble_size, self.units * 2],\n                                    *args, **kwargs),\n          ], axis=1)\n      else:\n        bias_initializer = self.bias_initializer\n      self.bias = self.add_weight(\n          shape=[self.ensemble_size, self.units * 4],\n          name=\'bias\',\n          initializer=bias_initializer,\n          regularizer=self.bias_regularizer,\n          constraint=self.bias_constraint)\n      self.bias_shape = self.bias.shape\n    self.alpha_shape = self.alpha.shape\n    self.gamma_shape = self.gamma.shape\n    self.recurrent_alpha_shape = self.recurrent_alpha.shape\n    self.recurrent_gamma_shape = self.recurrent_gamma.shape\n\n  def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n    """"""Computes carry and output using split kernels.""""""\n    x_i, x_f, x_c, x_o = x\n    h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\n    rec_k_i, rec_k_f, rec_k_c, rec_k_o = tf.split(\n        self.recurrent_kernel, num_or_size_splits=4, axis=1)\n    rec_alpha = self.recurrent_alpha_sample\n    rec_gamma_i, rec_gamma_f, rec_gamma_c, rec_gamma_o = tf.split(\n        self.recurrent_gamma_sample, num_or_size_splits=4, axis=1)\n    if self.use_additive_perturbation:\n      rec_i = tf.linalg.matmul(h_tm1_i + rec_alpha, rec_k_i) + rec_gamma_i\n      rec_f = tf.linalg.matmul(h_tm1_f + rec_alpha, rec_k_f) + rec_gamma_f\n      rec_c = tf.linalg.matmul(h_tm1_c + rec_alpha, rec_k_c) + rec_gamma_c\n      rec_o = tf.linalg.matmul(h_tm1_o + rec_alpha, rec_k_o) + rec_gamma_o\n    else:\n      rec_i = tf.linalg.matmul(h_tm1_i * rec_alpha, rec_k_i) * rec_gamma_i\n      rec_f = tf.linalg.matmul(h_tm1_f * rec_alpha, rec_k_f) * rec_gamma_f\n      rec_c = tf.linalg.matmul(h_tm1_c * rec_alpha, rec_k_c) * rec_gamma_c\n      rec_o = tf.linalg.matmul(h_tm1_o * rec_alpha, rec_k_o) * rec_gamma_o\n    i = self.recurrent_activation(x_i + rec_i)\n    f = self.recurrent_activation(x_f + rec_f)\n    c = f * c_tm1 + i * self.activation(x_c + rec_c)\n    o = self.recurrent_activation(x_o + rec_o)\n    return c, o\n\n  def _sample_weights(self, inputs=None, batch_size=None, dtype=None):\n    """"""Samples any rank-1 weight tensor if the initializer is itself a layer.""""""\n    if inputs is not None:\n      batch_size = tf.shape(inputs)[0]\n    examples_per_model = batch_size // self.ensemble_size\n\n    # Sample parameters for each input example.\n    def sample(weight_variable, weight_initializer, shape):\n      if isinstance(weight_initializer, tf.keras.layers.Layer):\n        weights = weight_initializer(\n            shape, self.dtype).distribution.sample(examples_per_model)\n        weights = tf.transpose(weights, [1, 0, 2])\n      else:\n        weights = tf.tile(weight_variable, [1, examples_per_model])\n      return weights\n\n    alpha = sample(self.alpha, self.alpha_initializer, self.alpha_shape)\n    gamma = sample(self.gamma, self.gamma_initializer, self.gamma_shape)\n    recurrent_alpha = sample(self.recurrent_alpha,\n                             self.recurrent_alpha_initializer,\n                             self.recurrent_alpha_shape)\n    recurrent_gamma = sample(self.recurrent_gamma,\n                             self.recurrent_gamma_initializer,\n                             self.recurrent_gamma_shape)\n\n    self.alpha_sample = tf.reshape(alpha, [batch_size, -1])\n    self.gamma_sample = tf.reshape(gamma, [batch_size, -1])\n    self.recurrent_alpha_sample = tf.reshape(recurrent_alpha, [batch_size, -1])\n    self.recurrent_gamma_sample = tf.reshape(recurrent_gamma, [batch_size, -1])\n    if self.use_bias:\n      bias = sample(self.bias, self.bias_initializer, self.bias_shape)\n      self.bias_sample = tf.reshape(bias, [batch_size, -1])\n    self.sampled_weights = True\n\n  def call(self, inputs, states, training=None):\n    """"""Executes the forward pass of the layer.""""""\n    batch_size = tf.shape(inputs)[0]\n    if not self.sampled_weights:\n      self._sample_weights(batch_size=batch_size)\n\n    alpha = self.alpha_sample\n    gamma = self.gamma_sample\n    recurrent_alpha = self.recurrent_alpha_sample\n    recurrent_gamma = self.recurrent_gamma_sample\n    if self.use_bias:\n      bias = self.bias_sample\n\n    h_tm1 = states[0]  # previous memory state\n    c_tm1 = states[1]  # previous carry state\n\n    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n        h_tm1, training, count=4)\n\n    if self.implementation == 1:\n      if 0 < self.dropout < 1.:\n        inputs_i = inputs * dp_mask[0]\n        inputs_f = inputs * dp_mask[1]\n        inputs_c = inputs * dp_mask[2]\n        inputs_o = inputs * dp_mask[3]\n      else:\n        inputs_i = inputs\n        inputs_f = inputs\n        inputs_c = inputs\n        inputs_o = inputs\n      k_i, k_f, k_c, k_o = tf.split(self.kernel, num_or_size_splits=4, axis=1)\n      gamma_i, gamma_f, gamma_c, gamma_o = tf.split(\n          gamma, num_or_size_splits=4, axis=1)\n      if self.use_additive_perturbation:\n        x_i = tf.linalg.matmul(inputs_i + alpha, k_i) + gamma_i\n        x_f = tf.linalg.matmul(inputs_f + alpha, k_f) + gamma_f\n        x_c = tf.linalg.matmul(inputs_c + alpha, k_c) + gamma_c\n        x_o = tf.linalg.matmul(inputs_o + alpha, k_o) + gamma_o\n      else:\n        x_i = tf.linalg.matmul(inputs_i * alpha, k_i) * gamma_i\n        x_f = tf.linalg.matmul(inputs_f * alpha, k_f) * gamma_f\n        x_c = tf.linalg.matmul(inputs_c * alpha, k_c) * gamma_c\n        x_o = tf.linalg.matmul(inputs_o * alpha, k_o) * gamma_o\n      if self.use_bias:\n        b_i, b_f, b_c, b_o = tf.split(bias, num_or_size_splits=4, axis=1)\n        x_i += b_i\n        x_f += b_f\n        x_c += b_c\n        x_o += b_o\n\n      if 0 < self.recurrent_dropout < 1.:\n        h_tm1_i = h_tm1 * rec_dp_mask[0]\n        h_tm1_f = h_tm1 * rec_dp_mask[1]\n        h_tm1_c = h_tm1 * rec_dp_mask[2]\n        h_tm1_o = h_tm1 * rec_dp_mask[3]\n      else:\n        h_tm1_i = h_tm1\n        h_tm1_f = h_tm1\n        h_tm1_c = h_tm1\n        h_tm1_o = h_tm1\n      x = (x_i, x_f, x_c, x_o)\n      h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)\n      c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)\n    else:\n      if 0. < self.dropout < 1.:\n        inputs = inputs * dp_mask[0]\n      if self.use_additive_perturbation:\n        z = tf.linalg.matmul(inputs + alpha, self.kernel) + gamma\n        z += tf.linalg.matmul(\n            h_tm1 + recurrent_alpha, self.recurrent_kernel) + recurrent_gamma\n      else:\n        z = tf.linalg.matmul(inputs * alpha, self.kernel) * gamma\n        z += tf.linalg.matmul(\n            h_tm1 * recurrent_alpha, self.recurrent_kernel) * recurrent_gamma\n      if self.use_bias:\n        z += bias\n\n      z = tf.split(z, num_or_size_splits=4, axis=1)\n      c, o = self._compute_carry_and_output_fused(z, c_tm1)\n\n    h = o * self.activation(c)\n    return h, [h, c]\n\n  def get_config(self):\n    """"""Returns the configuration for the layer.""""""\n    config = {\n        \'units\': self.units,\n        \'activation\': tf.keras.activations.serialize(self.activation),\n        \'recurrent_activation\': tf.keras.activations.serialize(\n            self.recurrent_activation),\n        \'use_bias\': self.use_bias,\n        \'alpha_initializer\': ed.initializers.serialize(self.alpha_initializer),\n        \'gamma_initializer\': ed.initializers.serialize(self.gamma_initializer),\n        \'kernel_initializer\': ed.initializers.serialize(\n            self.kernel_initializer),\n        \'recurrent_alpha_initializer\': ed.initializers.serialize(\n            self.recurrent_alpha_initializer),\n        \'recurrent_gamma_initializer\': ed.initializers.serialize(\n            self.recurrent_gamma_initializer),\n        \'recurrent_initializer\': ed.initializers.serialize(\n            self.recurrent_initializer),\n        \'bias_initializer\': ed.initializers.serialize(self.bias_initializer),\n        \'unit_forget_bias\': self.unit_forget_bias,\n        \'alpha_regularizer\': ed.regularizers.serialize(self.alpha_regularizer),\n        \'gamma_regularizer\': ed.regularizers.serialize(self.gamma_regularizer),\n        \'kernel_regularizer\': ed.regularizers.serialize(\n            self.kernel_regularizer),\n        \'recurrent_alpha_regularizer\': ed.regularizers.serialize(\n            self.recurrent_alpha_regularizer),\n        \'recurrent_gamma_regularizer\': ed.regularizers.serialize(\n            self.recurrent_gamma_regularizer),\n        \'recurrent_regularizer\': ed.regularizers.serialize(\n            self.recurrent_regularizer),\n        \'bias_regularizer\': ed.regularizers.serialize(self.bias_regularizer),\n        \'alpha_constraint\': ed.constraints.serialize(self.alpha_constraint),\n        \'gamma_constraint\': ed.constraints.serialize(self.gamma_constraint),\n        \'kernel_constraint\': ed.constraints.serialize(self.kernel_constraint),\n        \'recurrent_alpha_constraint\': ed.constraints.serialize(\n            self.recurrent_alpha_constraint),\n        \'recurrent_gamma_constraint\': ed.constraints.serialize(\n            self.recurrent_gamma_constraint),\n        \'recurrent_constraint\': ed.constraints.serialize(\n            self.recurrent_constraint),\n        \'bias_constraint\': ed.constraints.serialize(self.bias_constraint),\n        \'dropout\': self.dropout,\n        \'recurrent_dropout\': self.recurrent_dropout,\n        \'implementation\': self.implementation,\n        \'use_additive_perturbation\': self.use_additive_perturbation,\n        \'ensemble_size\': self.ensemble_size,\n    }\n    base_config = super().get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n\n  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n    """"""Gets the initial state and side-effect sampling of stochastic weights.""""""\n    if self.built:\n      self._sample_weights(inputs, batch_size, dtype)\n    return super().get_initial_state(\n        inputs=inputs, batch_size=batch_size, dtype=dtype)\n\n  def set_weights(self, *args, **kwargs):\n    """"""Sets the weights of the layer, from Numpy arrays.""""""\n    self.sampled_weights = False\n    super().set_weights(*args, **kwargs)\n'"
experimental/rank1_bnns/rank1_bnn_layers_test.py,54,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Tests for rank-1 BNN layers.""""""\nimport itertools\n\nfrom absl.testing import parameterized\nfrom experimental.rank1_bnns import rank1_bnn_layers  # local file import\nimport numpy as np\nimport tensorflow as tf\n\n\nclass Rank1PriorLayersTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.parameters(\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\',\n       \'bias_initializer\': \'zeros\'},\n      {\'alpha_initializer\': \'trainable_deterministic\',\n       \'gamma_initializer\': \'trainable_deterministic\',\n       \'bias_initializer\': \'trainable_deterministic\'},\n  )\n  def testDenseRank1BatchEnsemble(self,\n                                  alpha_initializer,\n                                  gamma_initializer,\n                                  bias_initializer):\n    tf.keras.backend.set_learning_phase(1)  # training time\n    ensemble_size = 3\n    examples_per_model = 4\n    input_dim = 5\n    output_dim = 5\n    inputs = tf.random.normal([examples_per_model, input_dim])\n    batched_inputs = tf.tile(inputs, [ensemble_size, 1])\n    layer = rank1_bnn_layers.DenseRank1(\n        output_dim,\n        alpha_initializer=alpha_initializer,\n        gamma_initializer=gamma_initializer,\n        bias_initializer=bias_initializer,\n        alpha_regularizer=None,\n        gamma_regularizer=None,\n        activation=None,\n        ensemble_size=ensemble_size)\n\n    output = layer(batched_inputs)\n    manual_output = [\n        layer.dense(inputs*layer.alpha[i]) * layer.gamma[i] + layer.bias[i]\n        for i in range(ensemble_size)]\n    manual_output = tf.concat(manual_output, axis=0)\n\n    expected_shape = (ensemble_size*examples_per_model, output_dim)\n    self.assertEqual(output.shape, expected_shape)\n    self.assertAllClose(output, manual_output)\n\n  @parameterized.parameters(\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'zeros\',\n       \'gamma_initializer\': \'zeros\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'zeros\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'zeros\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'zeros\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'zeros\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': False,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': False,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': False,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': False,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 4},\n  )\n  def testDenseRank1AlphaGamma(self,\n                               alpha_initializer,\n                               gamma_initializer,\n                               all_close,\n                               use_additive_perturbation,\n                               ensemble_size):\n    tf.keras.backend.set_learning_phase(1)  # training time\n    inputs = np.random.rand(5*ensemble_size, 12).astype(np.float32)\n    model = rank1_bnn_layers.DenseRank1(\n        4,\n        ensemble_size=ensemble_size,\n        alpha_initializer=alpha_initializer,\n        gamma_initializer=gamma_initializer,\n        activation=None)\n    outputs1 = model(inputs)\n    outputs2 = model(inputs)\n    self.assertEqual(outputs1.shape, (5*ensemble_size, 4))\n    if all_close:\n      self.assertAllClose(outputs1, outputs2)\n    else:\n      self.assertNotAllClose(outputs1, outputs2)\n    model.get_config()\n\n  def testDenseRank1Model(self):\n    inputs = np.random.rand(3, 4, 4, 1).astype(np.float32)\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(3,\n                               kernel_size=2,\n                               padding=\'SAME\',\n                               activation=tf.nn.relu),\n        tf.keras.layers.Flatten(),\n        rank1_bnn_layers.DenseRank1(2, activation=None),\n    ])\n    outputs = model(inputs, training=True)\n    self.assertEqual(outputs.shape, (3, 2))\n    self.assertLen(model.losses, 2)\n\n  @parameterized.parameters(\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\'},\n      {\'alpha_initializer\': \'trainable_deterministic\',\n       \'gamma_initializer\': \'trainable_deterministic\'},\n  )\n  def testConv2DRank1BatchEnsemble(self, alpha_initializer, gamma_initializer):\n    tf.keras.backend.set_learning_phase(1)  # training time\n    ensemble_size = 3\n    examples_per_model = 4\n    input_dim = 5\n    output_dim = 5\n    inputs = tf.random.normal([examples_per_model, 4, 4, input_dim])\n    batched_inputs = tf.tile(inputs, [ensemble_size, 1, 1, 1])\n    layer = rank1_bnn_layers.Conv2DRank1(\n        output_dim,\n        kernel_size=2,\n        padding=\'same\',\n        alpha_initializer=alpha_initializer,\n        gamma_initializer=gamma_initializer,\n        alpha_regularizer=None,\n        gamma_regularizer=None,\n        activation=None,\n        ensemble_size=ensemble_size)\n\n    output = layer(batched_inputs)\n    manual_output = [\n        layer.conv2d(inputs*layer.alpha[i]) * layer.gamma[i] + layer.bias[i]\n        for i in range(ensemble_size)]\n    manual_output = tf.concat(manual_output, axis=0)\n    self.assertEqual(output.shape,\n                     (ensemble_size*examples_per_model, 4, 4, output_dim))\n    self.assertAllClose(output, manual_output)\n\n  @parameterized.parameters(\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'zeros\',\n       \'gamma_initializer\': \'zeros\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'zeros\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'zeros\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'zeros\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'zeros\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': False,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': False,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': False,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': False,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 4},\n  )\n  def testConv2DRank1AlphaGamma(self,\n                                alpha_initializer,\n                                gamma_initializer,\n                                all_close,\n                                use_additive_perturbation,\n                                ensemble_size):\n    tf.keras.backend.set_learning_phase(1)  # training time\n    inputs = np.random.rand(5*ensemble_size, 4, 4, 12).astype(np.float32)\n    model = rank1_bnn_layers.Conv2DRank1(\n        4,\n        kernel_size=2,\n        alpha_initializer=alpha_initializer,\n        gamma_initializer=gamma_initializer,\n        activation=None)\n    outputs1 = model(inputs)\n    outputs2 = model(inputs)\n    self.assertEqual(outputs1.shape, (5*ensemble_size, 3, 3, 4))\n    if all_close:\n      self.assertAllClose(outputs1, outputs2)\n    else:\n      self.assertNotAllClose(outputs1, outputs2)\n    model.get_config()\n\n  def testConv2DRank1Model(self):\n    inputs = np.random.rand(3, 4, 4, 1).astype(np.float32)\n    model = tf.keras.Sequential([\n        rank1_bnn_layers.Conv2DRank1(3,\n                                     kernel_size=2,\n                                     padding=\'SAME\',\n                                     activation=tf.nn.relu),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(2, activation=None),\n    ])\n    outputs = model(inputs, training=True)\n    self.assertEqual(outputs.shape, (3, 2))\n    self.assertLen(model.losses, 2)\n\n  @parameterized.parameters(\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\'},\n      {\'alpha_initializer\': \'trainable_deterministic\',\n       \'gamma_initializer\': \'trainable_deterministic\'},\n  )\n  def testConv1DRank1BatchEnsemble(self, alpha_initializer, gamma_initializer):\n    tf.keras.backend.set_learning_phase(1)  # training time\n    ensemble_size = 3\n    examples_per_model = 4\n    input_dim = 5\n    output_dim = 5\n    inputs = tf.random.normal([examples_per_model, 4, input_dim])\n    batched_inputs = tf.tile(inputs, [ensemble_size, 1, 1])\n    layer = rank1_bnn_layers.Conv1DRank1(\n        output_dim,\n        kernel_size=2,\n        padding=\'same\',\n        alpha_initializer=alpha_initializer,\n        gamma_initializer=gamma_initializer,\n        alpha_regularizer=None,\n        gamma_regularizer=None,\n        activation=None,\n        ensemble_size=ensemble_size)\n\n    output = layer(batched_inputs)\n    manual_output = [\n        layer.conv1d(inputs*layer.alpha[i]) * layer.gamma[i] + layer.bias[i]\n        for i in range(ensemble_size)]\n    manual_output = tf.concat(manual_output, axis=0)\n    self.assertEqual(output.shape,\n                     (ensemble_size*examples_per_model, 4, output_dim))\n    self.assertAllClose(output, manual_output)\n\n  @parameterized.parameters(\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'he_normal\',\n       \'gamma_initializer\': \'he_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'zeros\',\n       \'gamma_initializer\': \'zeros\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'zeros\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'zeros\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'zeros\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'zeros\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': True,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': False,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': False,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': False,\n       \'use_additive_perturbation\': False,\n       \'ensemble_size\': 4},\n      {\'alpha_initializer\': \'trainable_normal\',\n       \'gamma_initializer\': \'trainable_normal\',\n       \'all_close\': False,\n       \'use_additive_perturbation\': True,\n       \'ensemble_size\': 4},\n  )\n  def testConv1DRank1AlphaGamma(self,\n                                alpha_initializer,\n                                gamma_initializer,\n                                all_close,\n                                use_additive_perturbation,\n                                ensemble_size):\n    tf.keras.backend.set_learning_phase(1)  # training time\n    inputs = np.random.rand(5*ensemble_size, 4, 12).astype(np.float32)\n    model = rank1_bnn_layers.Conv1DRank1(\n        4,\n        kernel_size=2,\n        alpha_initializer=alpha_initializer,\n        gamma_initializer=gamma_initializer,\n        activation=None)\n    outputs1 = model(inputs)\n    outputs2 = model(inputs)\n    self.assertEqual(outputs1.shape, (5*ensemble_size, 3, 4))\n    if all_close:\n      self.assertAllClose(outputs1, outputs2)\n    else:\n      self.assertNotAllClose(outputs1, outputs2)\n    model.get_config()\n\n  def testConv1DRank1Model(self):\n    inputs = np.random.rand(3, 4, 1).astype(np.float32)\n    model = tf.keras.Sequential([\n        rank1_bnn_layers.Conv1DRank1(3,\n                                     kernel_size=2,\n                                     padding=\'SAME\',\n                                     activation=tf.nn.relu),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(2, activation=None),\n    ])\n    outputs = model(inputs, training=True)\n    self.assertEqual(outputs.shape, (3, 2))\n    self.assertLen(model.losses, 2)\n\n  @parameterized.parameters(\n      itertools.chain(\n          itertools.product(\n              (\'he_normal\',), (\'he_normal\',), (\'he_normal\',), (\'he_normal\',),\n              (\'he_normal\',), (True, False), (1, 2), (True, False)),\n          itertools.product(\n              (\'trainable_deterministic\',), (\'trainable_deterministic\',),\n              (\'trainable_deterministic\',), (\'trainable_deterministic\',),\n              (\'trainable_deterministic\',), (True, False), (1, 2),\n              (True, False)))\n  )\n  def testLSTMCellRank1BatchEnsemble(self, alpha_initializer, gamma_initializer,\n                                     recurrent_alpha_initializer,\n                                     recurrent_gamma_initializer,\n                                     bias_initializer, use_bias, implementation,\n                                     use_additive_perturbation):\n    tf.keras.backend.set_learning_phase(1)  # training time\n    ensemble_size = 4\n    examples_per_model = 4\n    input_dim = 5\n    output_dim = 5\n    inputs = tf.random.normal([examples_per_model, input_dim])\n    batched_inputs = tf.tile(inputs, [ensemble_size, 1])\n    layer = rank1_bnn_layers.LSTMCellRank1(\n        output_dim,\n        use_bias=use_bias,\n        alpha_initializer=alpha_initializer,\n        gamma_initializer=gamma_initializer,\n        recurrent_alpha_initializer=recurrent_alpha_initializer,\n        recurrent_gamma_initializer=recurrent_gamma_initializer,\n        bias_initializer=bias_initializer,\n        alpha_regularizer=None,\n        gamma_regularizer=None,\n        recurrent_alpha_regularizer=None,\n        recurrent_gamma_regularizer=None,\n        implementation=implementation,\n        use_additive_perturbation=use_additive_perturbation,\n        ensemble_size=ensemble_size)\n    h0 = tf.random.normal([examples_per_model, output_dim])\n    c0 = tf.random.normal([examples_per_model, output_dim])\n\n    def compute_rank1_lstm_cell(i):\n      if use_additive_perturbation:\n        ifgo = tf.linalg.matmul(\n            inputs + layer.alpha[i], layer.kernel) + layer.gamma[i]\n        ifgo += tf.linalg.matmul(\n            h0 + layer.recurrent_alpha[i],\n            layer.recurrent_kernel) + layer.recurrent_gamma[i]\n      else:\n        ifgo = tf.linalg.matmul(\n            inputs * layer.alpha[i], layer.kernel) * layer.gamma[i]\n        ifgo += tf.linalg.matmul(\n            h0 * layer.recurrent_alpha[i],\n            layer.recurrent_kernel) * layer.recurrent_gamma[i]\n      if use_bias:\n        ifgo += layer.bias[i]\n      i, f, g, o = tf.split(ifgo, num_or_size_splits=4, axis=1)\n      i = tf.nn.sigmoid(i)\n      f = tf.nn.sigmoid(f)\n      g = tf.nn.tanh(g)\n      o = tf.nn.sigmoid(o)\n      c = f*c0 + i*g\n      h = o * tf.nn.tanh(c)\n      return h\n\n    h0_batched = tf.tile(h0, [ensemble_size, 1])\n    c0_batched = tf.tile(c0, [ensemble_size, 1])\n    outputs, _ = layer(batched_inputs, (h0_batched, c0_batched))\n    manual_outputs = tf.concat(\n        [compute_rank1_lstm_cell(i) for i in range(ensemble_size)], axis=0)\n\n    expected_shape = (ensemble_size*examples_per_model, output_dim)\n    self.assertEqual(outputs.shape, expected_shape)\n    self.assertAllClose(outputs, manual_outputs)\n\n    layer2 = rank1_bnn_layers.LSTMCellRank1.from_config(layer.get_config())\n    layer2(batched_inputs, (h0_batched, c0_batched))  # force initialization\n    layer2.set_weights(layer.get_weights())\n    outputs2, _ = layer2(batched_inputs, (h0_batched, c0_batched))\n    self.assertAllClose(outputs, outputs2)\n\n  @parameterized.parameters(\n      list(itertools.product(\n          (\'he_normal\', \'trainable_normal\',),\n          (\'he_normal\', \'trainable_normal\',),\n          (\'he_normal\', \'trainable_normal\',),\n          (\'he_normal\', \'trainable_normal\',),\n          (1, 2), (True, False)))\n  )\n  def testLSTMCellRank1AlphaGamma(self, alpha_initializer, gamma_initializer,\n                                  recurrent_alpha_initializer,\n                                  recurrent_gamma_initializer,\n                                  implementation, use_additive_perturbation):\n    tf.keras.backend.set_learning_phase(1)  # training time\n    ensemble_size = 4\n    batch_size = 5 * ensemble_size\n    output_dim = 4\n    inputs = np.random.rand(batch_size, 12).astype(np.float32)\n    layer = rank1_bnn_layers.LSTMCellRank1(\n        output_dim,\n        alpha_initializer=alpha_initializer,\n        gamma_initializer=gamma_initializer,\n        recurrent_alpha_initializer=recurrent_alpha_initializer,\n        recurrent_gamma_initializer=recurrent_gamma_initializer,\n        ensemble_size=ensemble_size)\n    h0 = tf.random.normal([batch_size, output_dim])\n    c0 = tf.random.normal([batch_size, output_dim])\n    outputs1, _ = layer(inputs, (h0, c0))\n    layer._sample_weights(inputs)\n    outputs2, _ = layer(inputs, (h0, c0))\n    self.assertEqual(outputs1.shape, (batch_size, output_dim))\n    all_close = \'trainable_normal\' not in [alpha_initializer, gamma_initializer,\n                                           recurrent_alpha_initializer,\n                                           recurrent_gamma_initializer]\n    if all_close:\n      self.assertAllClose(outputs1, outputs2, rtol=1e-4)\n    else:\n      self.assertNotAllClose(outputs1, outputs2)\n\n  @parameterized.parameters(\n      list(itertools.product((1, 4), (1, 2), (True, False), (True, False)))\n  )\n  def testLSTMCellRank1Model(self, ensemble_size, implementation,\n                             use_additive_perturbation, use_bias):\n    batch_size = 2 * ensemble_size\n    timesteps = 3\n    input_dim = 12\n    hidden_size = 10\n    inputs = np.random.rand(batch_size, timesteps, input_dim).astype(np.float32)\n    cell = rank1_bnn_layers.LSTMCellRank1(\n        hidden_size, use_bias=use_bias, implementation=implementation,\n        use_additive_perturbation=use_additive_perturbation,\n        ensemble_size=ensemble_size)\n    model = tf.keras.Sequential([\n        tf.keras.layers.RNN(cell, return_sequences=True)\n    ])\n\n    outputs1 = model(inputs)\n    outputs2 = model(inputs)\n    state = (tf.zeros([1, hidden_size]), tf.zeros([1, hidden_size]))\n    outputs3 = []\n    for t in range(timesteps):\n      out, state = cell(inputs[:, t, :], state)\n      outputs3.append(out)\n    outputs3 = tf.stack(outputs3, axis=1)\n    self.assertEqual(outputs1.shape, (batch_size, timesteps, hidden_size))\n    self.assertEqual(outputs3.shape, (batch_size, timesteps, hidden_size))\n    # NOTE: `cell.sample_weights` should have been called at the beginning of\n    # each call, so these should be different.\n    self.assertNotAllClose(outputs1, outputs2)\n    # NOTE: We didn\'t call `cell.sample_weights` again before computing\n    # `outputs3`, so the cell should have had the same weights as it did\n    # during computation of `outputs2`, and thus yielded the same output\n    # tensor.\n    self.assertAllClose(outputs2, outputs3)\n    self.assertLen(model.losses, 4)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
experimental/rank1_bnns/refining.py,16,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Utilities for sampling.""""""\nfrom baselines.cifar import utils  # local file import\nfrom experimental.rank1_bnns import rank1_bnn_layers  # local file import\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n\ndef get_auxiliary_posterior(posterior_mean,\n                            posterior_scale,\n                            prior_mean,\n                            prior_scale,\n                            auxiliary_scale):\n  """"""Calculates the posterior of an additive Gaussian auxiliary variable.""""""\n  # q(a)=\\int p(a|w)q(w)dw\n  prior_var = tf.math.pow(prior_scale, 2)\n  posterior_var = tf.math.pow(posterior_scale, 2)\n  auxiliary_var = tf.math.pow(auxiliary_scale, 2)\n  aux_div_prior_var = auxiliary_var / prior_var\n  auxiliary_posterior_mean = (posterior_mean - prior_mean) * aux_div_prior_var\n  auxiliary_posterior_var = (posterior_var * tf.math.pow(auxiliary_var, 2)\n                             / tf.math.pow(prior_var, 2)\n                             + aux_div_prior_var * (prior_var - auxiliary_var))\n  return auxiliary_posterior_mean, tf.sqrt(auxiliary_posterior_var)\n\n\ndef get_conditional_prior(prior_mean,\n                          prior_scale,\n                          auxiliary_scale,\n                          auxiliary_sample):\n  """"""Calculates the conditional prior given an auxiliary variable.""""""\n  # p(w|a)\n  prior_var = tf.math.pow(prior_scale, 2)\n  auxiliary_var = tf.math.pow(auxiliary_scale, 2)\n  return prior_mean + auxiliary_sample, tf.sqrt(prior_var - auxiliary_var)\n\n\ndef get_conditional_posterior(posterior_mean,\n                              posterior_scale,\n                              prior_mean,\n                              prior_scale,\n                              auxiliary_scale,\n                              auxiliary_sample):\n  """"""Calculates the conditional posterior given an additive auxiliary variable.""""""\n  # q(w|a)\\propto p(a|w)q(w)\n  prior_var = tf.math.pow(prior_scale, 2)\n  posterior_var = tf.math.pow(posterior_scale, 2)\n  auxiliary_var = tf.math.pow(auxiliary_scale, 2)\\\n\n  cond_x_prior_var = (prior_var - auxiliary_var) * prior_var\n  aux_x_post_var = auxiliary_var * posterior_var\n  denom = cond_x_prior_var + aux_x_post_var\n  conditional_mean = (prior_mean + (auxiliary_sample * posterior_var *\n                                    prior_var + (posterior_mean - prior_mean)\n                                    * cond_x_prior_var) / denom)\n  conditional_var = posterior_var * cond_x_prior_var / denom\n  return conditional_mean, tf.sqrt(conditional_var)\n\n\ndef sample_rank1_auxiliaries(model, auxiliary_var_ratio):\n  """"""Samples additive Gaussian auxiliary variables for the model.\n\n  For every rank1 BNN layer, then it samples additive Gaussian auxiliary\n  variables for alpha and gamma. It is assumed that the priors and posteriors\n  of alpha and gamma are both Gaussians.\n\n  Args:\n    model: Keras model.\n    auxiliary_var_ratio: The ratio of the variance of the auxiliary variable\n      to the variance of the prior. (0 < auxiliary_var_ratio < 1)\n  """"""\n  for layer in model.layers:\n    if (isinstance(layer, rank1_bnn_layers.DenseRank1) or\n        isinstance(layer, rank1_bnn_layers.Conv2DRank1)):\n      for initializer, regularizer in [(layer.alpha_initializer,\n                                        layer.alpha_regularizer),\n                                       (layer.gamma_initializer,\n                                        layer.gamma_regularizer)]:\n        posterior_mean = initializer.mean\n        unconstrained_posterior_scale = initializer.stddev\n        posterior_scale = initializer.stddev_constraint(\n            unconstrained_posterior_scale)\n        prior_mean = regularizer.mean\n        prior_scale = regularizer.stddev\n        auxiliary_scale_ratio = np.sqrt(auxiliary_var_ratio)\n        auxiliary_scale = tf.cast(auxiliary_scale_ratio * prior_scale,\n                                  dtype=posterior_mean.dtype)\n        a_mean, a_scale = get_auxiliary_posterior(posterior_mean,\n                                                  posterior_scale,\n                                                  prior_mean,\n                                                  prior_scale,\n                                                  auxiliary_scale)\n        auxiliary_sample = tfp.distributions.Normal(loc=a_mean,\n                                                    scale=a_scale).sample()\n        new_posterior_mean, new_posterior_scale = get_conditional_posterior(\n            posterior_mean,\n            posterior_scale,\n            prior_mean,\n            prior_scale,\n            auxiliary_scale,\n            auxiliary_sample)\n        new_prior_mean, new_prior_scale = get_conditional_prior(\n            prior_mean,\n            prior_scale,\n            auxiliary_scale,\n            auxiliary_sample)\n        posterior_mean.assign(new_posterior_mean)\n        unconstrained_posterior_scale.assign(\n            tfp.math.softplus_inverse(new_posterior_scale))\n        regularizer.mean = new_prior_mean.numpy()\n        regularizer.stddev = new_prior_scale.numpy()\n\n\ndef freeze_rank1_weights(model):\n  """"""Freeze the weight matrices of the rank1 BNN layers.""""""\n  for layer in model.layers:\n    if isinstance(layer, rank1_bnn_layers.DenseRank1):\n      layer.dense.trainable = False\n    elif isinstance(layer, rank1_bnn_layers.Conv2DRank1):\n      layer.conv2d.trainable = False\n\n\nclass LearningRateScheduleWithRefining(utils.LearningRateSchedule):\n  """"""Learning rate schedule that includes the refining phase.""""""\n\n  def __init__(self,\n               steps_per_epoch,\n               initial_learning_rate,\n               decay_ratio,\n               decay_epochs,\n               warmup_epochs,\n               train_epochs,\n               refining_learning_rate):\n    super(LearningRateScheduleWithRefining,\n          self).__init__(steps_per_epoch,\n                         initial_learning_rate,\n                         decay_ratio,\n                         decay_epochs,\n                         warmup_epochs)\n    self.train_epochs = train_epochs\n    self.refining_learning_rate = refining_learning_rate\n\n  def __call__(self, step):\n    lr_epoch = tf.cast(step, tf.float32) / self.steps_per_epoch\n    return tf.where(lr_epoch >= self.train_epochs,\n                    self.refining_learning_rate,\n                    super(LearningRateScheduleWithRefining,\n                          self).__call__(step))\n'"
experimental/rank1_bnns/resnet_cifar_main.py,73,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""ResNet-32x4 with rank-1 distributions.""""""\nimport functools\nimport itertools\nimport os\nimport time\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport edward2 as ed\nfrom baselines.cifar import utils  # local file import\nfrom experimental.rank1_bnns import resnet_cifar_model  # local file import\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# ~24.4 steps per epoch for 4x4 TPU; per_core_batch_size=64; 300 epochs;\n# so 2/3 of training time.\nflags.DEFINE_integer(\'kl_annealing_steps\', int(24.4 * 200),\n                     \'Number of steps over which to anneal the KL term to 1.\')\nflags.DEFINE_string(\'alpha_initializer\', \'trainable_deterministic\',\n                    \'Initializer name for the alpha parameters.\')\nflags.DEFINE_string(\'gamma_initializer\', \'trainable_deterministic\',\n                    \'Initializer name for the gamma parameters.\')\nflags.DEFINE_string(\'alpha_regularizer\', None,\n                    \'Regularizer name for the alpha parameters.\')\nflags.DEFINE_string(\'gamma_regularizer\', None,\n                    \'Regularizer name for the gamma parameters.\')\nflags.DEFINE_boolean(\'use_additive_perturbation\', False,\n                     \'Use additive perturbations instead of multiplicative.\')\nflags.DEFINE_integer(\'num_train_samples\', 1,\n                     \'Number of samples per example during training.\')\nflags.DEFINE_integer(\'num_eval_samples\', 1,\n                     \'Number of samples per example during evaluation.\')\n\n# General model flags\nflags.DEFINE_integer(\'ensemble_size\', 4, \'Size of ensemble.\')\nflags.DEFINE_bool(\n    \'member_sampling\', default=False,\n    help=(\'Whether or not to sample a single ensemble member per step with \'\n          \'which to compute the loss and derivatives.\'))\nflags.DEFINE_integer(\'per_core_batch_size\', 64, \'Batch size per TPU core/GPU.\')\nflags.DEFINE_float(\'random_sign_init\', -0.5,\n                   \'Use random sign init for fast weights.\')\nflags.DEFINE_integer(\'seed\', 0, \'Random seed.\')\nflags.DEFINE_float(\'fast_weight_lr_multiplier\', 0.5,\n                   \'fast weights lr multiplier.\')\nflags.DEFINE_bool(\'version2\', True, \'Use ensemble version2.\')\nflags.DEFINE_bool(\n    \'expected_probs\', default=False,\n    help=(\'Whether or not to compute the loss over the per-example average of \'\n          \'the predicted probabilities across the ensemble members.\'))\nflags.DEFINE_float(\'base_learning_rate\', 0.1,\n                   \'Base learning rate when total training batch size is 128.\')\nflags.DEFINE_integer(\'lr_warmup_epochs\', 1,\n                     \'Number of epochs for a linear warmup to the initial \'\n                     \'learning rate. Use 0 to do no warmup.\')\nflags.DEFINE_float(\'lr_decay_ratio\', 0.1, \'Amount to decay learning rate.\')\nflags.DEFINE_list(\'lr_decay_epochs\', [\'80\', \'160\', \'180\'],\n                  \'Epochs to decay learning rate by.\')\nflags.DEFINE_float(\'dropout_rate\', 0.,\n                   \'Dropout rate. Only used if alpha/gamma initializers are, \'\n                   \'e.g., trainable normal with a fixed stddev.\')\nflags.DEFINE_float(\'l2\', 2e-4, \'L2 coefficient.\')\nflags.DEFINE_enum(\'dataset\', \'cifar10\',\n                  enum_values=[\'cifar10\', \'cifar100\'],\n                  help=\'Dataset.\')\n# TODO(ghassen): consider adding CIFAR-100-C to TFDS.\nflags.DEFINE_string(\'cifar100_c_path\',\n                    \'\',\n                    \'Path to the TFRecords files for CIFAR-100-C. Only valid \'\n                    \'(and required) if dataset is cifar100 and corruptions.\')\nflags.DEFINE_integer(\'corruptions_interval\', 250,\n                     \'Number of epochs between evaluating on the corrupted \'\n                     \'test data. Use -1 to never evaluate.\')\nflags.DEFINE_integer(\'checkpoint_interval\', 25,\n                     \'Number of epochs between saving checkpoints. Use -1 to \'\n                     \'never save checkpoints.\')\nflags.DEFINE_integer(\'num_bins\', 15, \'Number of bins for ECE.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/cifar\',\n                    \'The directory where the model weights and \'\n                    \'training/evaluation summaries are stored.\')\nflags.DEFINE_integer(\'train_epochs\', 250, \'Number of training epochs.\')\n\n# Accelerator flags.\nflags.DEFINE_bool(\'use_gpu\', False, \'Whether to run on GPU or otherwise TPU.\')\nflags.DEFINE_bool(\'use_bfloat16\', False, \'Whether to use mixed precision.\')\nflags.DEFINE_integer(\'num_cores\', 8, \'Number of TPU cores or number of GPUs.\')\nflags.DEFINE_string(\'tpu\', None,\n                    \'Name of the TPU. Only used if use_gpu is False.\')\nFLAGS = flags.FLAGS\n\n\ndef main(argv):\n  del argv  # Unused arg.\n\n  tf.random.set_seed(FLAGS.seed)\n\n  if FLAGS.version2:\n    per_core_bs_train = FLAGS.per_core_batch_size // (FLAGS.ensemble_size *\n                                                      FLAGS.num_train_samples)\n    per_core_bs_eval = FLAGS.per_core_batch_size // (FLAGS.ensemble_size *\n                                                     FLAGS.num_eval_samples)\n  else:\n    per_core_bs_train = FLAGS.per_core_batch_size // FLAGS.num_train_samples\n    per_core_bs_eval = FLAGS.per_core_batch_size // FLAGS.num_eval_samples\n  batch_size_train = per_core_bs_train * FLAGS.num_cores\n  batch_size_eval = per_core_bs_eval * FLAGS.num_cores\n\n  logging.info(\'Saving checkpoints at %s\', FLAGS.output_dir)\n\n  if FLAGS.use_gpu:\n    logging.info(\'Use GPU\')\n    strategy = tf.distribute.MirroredStrategy()\n  else:\n    logging.info(\'Use TPU at %s\',\n                 FLAGS.tpu if FLAGS.tpu is not None else \'local\')\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n  train_input_fn = utils.load_input_fn(\n      split=tfds.Split.TRAIN,\n      name=FLAGS.dataset,\n      batch_size=per_core_bs_train,\n      use_bfloat16=FLAGS.use_bfloat16,\n      normalize=False)\n  clean_test_input_fn = utils.load_input_fn(\n      split=tfds.Split.TEST,\n      name=FLAGS.dataset,\n      batch_size=per_core_bs_eval,\n      use_bfloat16=FLAGS.use_bfloat16,\n      normalize=False)\n  train_dataset = strategy.experimental_distribute_datasets_from_function(\n      train_input_fn)\n  test_datasets = {\n      \'clean\': strategy.experimental_distribute_datasets_from_function(\n          clean_test_input_fn),\n  }\n  if FLAGS.corruptions_interval > 0:\n    if FLAGS.dataset == \'cifar10\':\n      load_c_input_fn = utils.load_cifar10_c_input_fn\n    else:\n      load_c_input_fn = functools.partial(utils.load_cifar100_c_input_fn,\n                                          path=FLAGS.cifar100_c_path)\n    corruption_types, max_intensity = utils.load_corrupted_test_info(\n        FLAGS.dataset)\n    for corruption in corruption_types:\n      for intensity in range(1, max_intensity + 1):\n        input_fn = load_c_input_fn(\n            corruption_name=corruption,\n            corruption_intensity=intensity,\n            batch_size=per_core_bs_eval,\n            use_bfloat16=FLAGS.use_bfloat16,\n            normalize=False)\n        test_datasets[\'{0}_{1}\'.format(corruption, intensity)] = (\n            strategy.experimental_distribute_datasets_from_function(input_fn))\n\n  ds_info = tfds.builder(FLAGS.dataset).info\n  train_dataset_size = ds_info.splits[\'train\'].num_examples\n  test_dataset_size = ds_info.splits[\'test\'].num_examples\n  num_classes = ds_info.features[\'label\'].num_classes\n\n  steps_per_epoch = train_dataset_size // batch_size_train\n  steps_per_eval = test_dataset_size // batch_size_eval\n\n  if FLAGS.use_bfloat16:\n    policy = tf.keras.mixed_precision.experimental.Policy(\'mixed_bfloat16\')\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n\n  summary_writer = tf.summary.create_file_writer(\n      os.path.join(FLAGS.output_dir, \'summaries\'))\n\n  with strategy.scope():\n    logging.info(\'Building Keras ResNet-32 model\')\n    model = resnet_cifar_model.rank1_resnet_v1(\n        input_shape=ds_info.features[\'image\'].shape,\n        depth=32,\n        num_classes=num_classes,\n        width_multiplier=4,\n        alpha_initializer=FLAGS.alpha_initializer,\n        gamma_initializer=FLAGS.gamma_initializer,\n        alpha_regularizer=FLAGS.alpha_regularizer,\n        gamma_regularizer=FLAGS.gamma_regularizer,\n        use_additive_perturbation=FLAGS.use_additive_perturbation,\n        ensemble_size=FLAGS.ensemble_size,\n        random_sign_init=FLAGS.random_sign_init,\n        dropout_rate=FLAGS.dropout_rate)\n    logging.info(model.summary())\n    base_lr = FLAGS.base_learning_rate * batch_size_train / 128\n    lr_decay_epochs = [(int(start_epoch_str) * FLAGS.train_epochs) // 200\n                       for start_epoch_str in FLAGS.lr_decay_epochs]\n    lr_schedule = utils.LearningRateSchedule(\n        steps_per_epoch,\n        base_lr,\n        decay_ratio=FLAGS.lr_decay_ratio,\n        decay_epochs=lr_decay_epochs,\n        warmup_epochs=FLAGS.lr_warmup_epochs)\n    optimizer = tf.keras.optimizers.SGD(\n        lr_schedule, momentum=0.9, nesterov=True)\n    metrics = {\n        \'train/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'train/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'train/loss\': tf.keras.metrics.Mean(),\n        \'train/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        \'test/negative_log_likelihood\': tf.keras.metrics.Mean(),\n        \'test/accuracy\': tf.keras.metrics.SparseCategoricalAccuracy(),\n        \'test/ece\': ed.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        \'test/loss\': tf.keras.metrics.Mean(),\n    }\n    if FLAGS.corruptions_interval > 0:\n      corrupt_metrics = {}\n      for intensity in range(1, max_intensity + 1):\n        for corruption in corruption_types:\n          dataset_name = \'{0}_{1}\'.format(corruption, intensity)\n          corrupt_metrics[\'test/nll_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.Mean())\n          corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)] = (\n              tf.keras.metrics.SparseCategoricalAccuracy())\n          corrupt_metrics[\'test/ece_{}\'.format(dataset_name)] = (\n              ed.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins))\n\n    test_diversity = {}\n    training_diversity = {}\n    if FLAGS.ensemble_size > 1:\n      for i in range(FLAGS.ensemble_size):\n        metrics[\'test/nll_member_{}\'.format(i)] = tf.keras.metrics.Mean()\n        metrics[\'test/accuracy_member_{}\'.format(i)] = (\n            tf.keras.metrics.SparseCategoricalAccuracy())\n      test_diversity = {\n          \'test/disagreement\': tf.keras.metrics.Mean(),\n          \'test/average_kl\': tf.keras.metrics.Mean(),\n          \'test/cosine_similarity\': tf.keras.metrics.Mean(),\n      }\n      training_diversity = {\n          \'train/disagreement\': tf.keras.metrics.Mean(),\n          \'train/average_kl\': tf.keras.metrics.Mean(),\n          \'train/cosine_similarity\': tf.keras.metrics.Mean(),\n      }\n\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n    initial_epoch = 0\n    if latest_checkpoint:\n      # checkpoint.restore must be within a strategy.scope() so that optimizer\n      # slot variables are mirrored.\n      checkpoint.restore(latest_checkpoint)\n      logging.info(\'Loaded checkpoint %s\', latest_checkpoint)\n      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n\n  @tf.function\n  def train_step(iterator):\n    """"""Training StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      if FLAGS.version2 and FLAGS.ensemble_size > 1:\n        images = tf.tile(images, [FLAGS.ensemble_size, 1, 1, 1])\n        if not (FLAGS.member_sampling or FLAGS.expected_probs):\n          labels = tf.tile(labels, [FLAGS.ensemble_size])\n\n      if FLAGS.num_train_samples > 1:\n        images = tf.tile(images, [FLAGS.num_train_samples, 1, 1, 1])\n\n      with tf.GradientTape() as tape:\n        logits = model(images, training=True)\n        probs = tf.nn.softmax(logits)\n        # Diversity evaluation.\n        if FLAGS.version2 and FLAGS.ensemble_size > 1:\n          per_probs = tf.reshape(\n              probs, tf.concat([[FLAGS.ensemble_size, -1], probs.shape[1:]], 0))\n\n          diversity_results = ed.metrics.average_pairwise_diversity(\n              per_probs, FLAGS.ensemble_size)\n\n        if FLAGS.num_train_samples > 1:\n          probs = tf.reshape(probs,\n                             tf.concat([[FLAGS.num_train_samples, -1],\n                                        probs.shape[1:]], 0))\n          probs = tf.reduce_mean(probs, 0)\n\n        if FLAGS.member_sampling and FLAGS.version2 and FLAGS.ensemble_size > 1:\n          idx = tf.random.uniform([], maxval=FLAGS.ensemble_size,\n                                  dtype=tf.int64)\n          idx_one_hot = tf.expand_dims(tf.one_hot(idx, FLAGS.ensemble_size,\n                                                  dtype=probs.dtype), 0)\n          probs_shape = probs.shape\n          probs = tf.reshape(probs, [FLAGS.ensemble_size, -1])\n          probs = tf.matmul(idx_one_hot, probs)\n          probs = tf.reshape(probs, tf.concat([[-1], probs_shape[1:]], 0))\n\n        elif FLAGS.expected_probs and FLAGS.version2 and FLAGS.ensemble_size > 1:\n          probs = tf.reshape(probs,\n                             tf.concat([[FLAGS.ensemble_size, -1],\n                                        probs.shape[1:]], 0))\n          probs = tf.reduce_mean(probs, 0)\n\n        negative_log_likelihood = tf.reduce_mean(\n            tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n\n        filtered_variables = []\n        for var in model.trainable_variables:\n          # Apply l2 on the slow weights and bias terms. This excludes BN\n          # parameters and fast weight approximate posterior/prior parameters,\n          # but pay caution to their naming scheme.\n          if \'kernel\' in var.name or \'bias\' in var.name:\n            filtered_variables.append(tf.reshape(var, (-1,)))\n\n        l2_loss = FLAGS.l2 * 2 * tf.nn.l2_loss(\n            tf.concat(filtered_variables, axis=0))\n        kl = sum(model.losses) / train_dataset_size\n        kl_scale = tf.cast(optimizer.iterations + 1, kl.dtype)\n        kl_scale /= FLAGS.kl_annealing_steps\n        kl_scale = tf.minimum(1., kl_scale)\n        kl_loss = kl_scale * kl\n\n        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n        loss = negative_log_likelihood + l2_loss + kl_loss\n        scaled_loss = loss / strategy.num_replicas_in_sync\n\n      grads = tape.gradient(scaled_loss, model.trainable_variables)\n\n      # Separate learning rate implementation.\n      grad_list = []\n      if FLAGS.fast_weight_lr_multiplier != 1.0:\n        grads_and_vars = list(zip(grads, model.trainable_variables))\n        for vec, var in grads_and_vars:\n          # Apply different learning rate on the fast weight approximate\n          # posterior/prior parameters. This is excludes BN and slow weights,\n          # but pay caution to the naming scheme.\n          if (\'batch_norm\' not in var.name and \'kernel\' not in var.name):\n            grad_list.append((vec * FLAGS.fast_weight_lr_multiplier, var))\n          else:\n            grad_list.append((vec, var))\n        optimizer.apply_gradients(grad_list)\n      else:\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n      metrics[\'train/ece\'].update_state(labels, probs)\n      metrics[\'train/loss\'].update_state(loss)\n      metrics[\'train/negative_log_likelihood\'].update_state(\n          negative_log_likelihood)\n      metrics[\'train/accuracy\'].update_state(labels, probs)\n      if FLAGS.version2 and FLAGS.ensemble_size > 1:\n        for k, v in diversity_results.items():\n          training_diversity[\'train/\' + k].update_state(v)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  @tf.function\n  def test_step(iterator, dataset_name):\n    """"""Evaluation StepFn.""""""\n    def step_fn(inputs):\n      """"""Per-Replica StepFn.""""""\n      images, labels = inputs\n      if FLAGS.ensemble_size > 1:\n        images = tf.tile(images, [FLAGS.ensemble_size, 1, 1, 1])\n      if FLAGS.num_eval_samples > 1:\n        images = tf.tile(images, [FLAGS.num_eval_samples, 1, 1, 1])\n      logits = model(images, training=False)\n      probs = tf.nn.softmax(logits)\n\n      if FLAGS.num_eval_samples > 1:\n        probs = tf.reshape(probs,\n                           tf.concat([[FLAGS.num_eval_samples, -1],\n                                      probs.shape[1:]], 0))\n        probs = tf.reduce_mean(probs, 0)\n\n      if FLAGS.ensemble_size > 1:\n        per_probs = tf.split(probs,\n                             num_or_size_splits=FLAGS.ensemble_size,\n                             axis=0)\n        if dataset_name == \'clean\':\n          per_probs_tensor = tf.reshape(\n              probs, tf.concat([[FLAGS.ensemble_size, -1], probs.shape[1:]], 0))\n          diversity_results = ed.metrics.average_pairwise_diversity(\n              per_probs_tensor, FLAGS.ensemble_size)\n\n          for k, v in diversity_results.items():\n            test_diversity[\'test/\' + k].update_state(v)\n\n          for i in range(FLAGS.ensemble_size):\n            member_probs = per_probs[i]\n            member_nll = tf.keras.losses.sparse_categorical_crossentropy(\n                labels, member_probs)\n            metrics[\'test/nll_member_{}\'.format(i)].update_state(member_nll)\n            metrics[\'test/accuracy_member_{}\'.format(i)].update_state(\n                labels, member_probs)\n\n        probs = tf.reduce_mean(per_probs, axis=0)\n\n      negative_log_likelihood = tf.reduce_mean(\n          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n      filtered_variables = []\n      for var in model.trainable_variables:\n        if \'kernel\' in var.name or \'bias\' in var.name:\n          filtered_variables.append(tf.reshape(var, (-1,)))\n\n      kl = sum(model.losses) / test_dataset_size\n      l2_loss = kl + FLAGS.l2 * 2 * tf.nn.l2_loss(\n          tf.concat(filtered_variables, axis=0))\n      loss = negative_log_likelihood + l2_loss\n      if dataset_name == \'clean\':\n        metrics[\'test/negative_log_likelihood\'].update_state(\n            negative_log_likelihood)\n        metrics[\'test/accuracy\'].update_state(labels, probs)\n        metrics[\'test/ece\'].update_state(labels, probs)\n        metrics[\'test/loss\'].update_state(loss)\n      else:\n        corrupt_metrics[\'test/nll_{}\'.format(dataset_name)].update_state(\n            negative_log_likelihood)\n        corrupt_metrics[\'test/accuracy_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n        corrupt_metrics[\'test/ece_{}\'.format(dataset_name)].update_state(\n            labels, probs)\n\n    strategy.run(step_fn, args=(next(iterator),))\n\n  train_iterator = iter(train_dataset)\n  start_time = time.time()\n  for epoch in range(initial_epoch, FLAGS.train_epochs):\n    logging.info(\'Starting to run epoch: %s\', epoch)\n    for step in range(steps_per_epoch):\n      train_step(train_iterator)\n\n      current_step = epoch * steps_per_epoch + (step + 1)\n      max_steps = steps_per_epoch * FLAGS.train_epochs\n      time_elapsed = time.time() - start_time\n      steps_per_sec = float(current_step) / time_elapsed\n      eta_seconds = (max_steps - current_step) / steps_per_sec\n      message = (\'{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. \'\n                 \'ETA: {:.0f} min. Time elapsed: {:.0f} min\'.format(\n                     current_step / max_steps,\n                     epoch + 1,\n                     FLAGS.train_epochs,\n                     steps_per_sec,\n                     eta_seconds / 60,\n                     time_elapsed / 60))\n      work_unit.set_notes(message)\n      if step % 20 == 0:\n        logging.info(message)\n\n    datasets_to_evaluate = {\'clean\': test_datasets[\'clean\']}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      datasets_to_evaluate = test_datasets\n    for dataset_name, test_dataset in datasets_to_evaluate.items():\n      test_iterator = iter(test_dataset)\n      logging.info(\'Testing on dataset %s\', dataset_name)\n      for step in range(steps_per_eval):\n        if step % 20 == 0:\n          logging.info(\'Starting to run eval step %s of epoch: %s\', step,\n                       epoch)\n        test_step(test_iterator, dataset_name)\n      logging.info(\'Done with testing on %s\', dataset_name)\n\n    corrupt_results = {}\n    if (FLAGS.corruptions_interval > 0 and\n        (epoch + 1) % FLAGS.corruptions_interval == 0):\n      corrupt_results = utils.aggregate_corrupt_metrics(corrupt_metrics,\n                                                        corruption_types,\n                                                        max_intensity)\n\n    logging.info(\'Train Loss: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'train/loss\'].result(),\n                 metrics[\'train/accuracy\'].result() * 100)\n    logging.info(\'Test NLL: %.4f, Accuracy: %.2f%%\',\n                 metrics[\'test/negative_log_likelihood\'].result(),\n                 metrics[\'test/accuracy\'].result() * 100)\n    for i in range(FLAGS.ensemble_size):\n      logging.info(\'Member %d Test Loss: %.4f, Accuracy: %.2f%%\',\n                   i, metrics[\'test/nll_member_{}\'.format(i)].result(),\n                   metrics[\'test/accuracy_member_{}\'.format(i)].result() * 100)\n    total_metrics = itertools.chain(metrics.items(),\n                                    training_diversity.items(),\n                                    test_diversity.items())\n    total_results = {name: metric.result() for name, metric in total_metrics}\n    total_results.update(corrupt_results)\n    with summary_writer.as_default():\n      for name, result in total_results.items():\n        tf.summary.scalar(name, result, step=epoch + 1)\n\n    for name, result in total_results.items():\n      name = name.replace(\'/\', \'_\')\n      if \'negative_log_likelihood\' in name:\n        # Plots sort WIDs from high-to-low so look at maximization objectives.\n        name = name.replace(\'negative_log_likelihood\', \'log_likelihood\')\n        result = -result\n      objective = work_unit.get_measurement_series(name)\n      objective.create_measurement(result, epoch + 1)\n\n    for _, metric in total_metrics:\n      metric.reset_states()\n    summary_writer.flush()\n\n    if (FLAGS.checkpoint_interval > 0 and\n        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n      checkpoint_name = checkpoint.save(\n          os.path.join(FLAGS.output_dir, \'checkpoint\'))\n      logging.info(\'Saved checkpoint to %s\', checkpoint_name)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
experimental/rank1_bnns/resnet_cifar_model.py,13,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""ResNet-32x4 with rank-1 distributions on CIFAR-10 and CIFAR-100.\n\n# Reference:\n- [Deep Residual Learning for Image Recognition](\n    https://arxiv.org/abs/1512.03385)\nAdapted from code contributed by BigMoyan.\n""""""\nimport functools\nfrom experimental.rank1_bnns import rank1_bnn_layers  # local file import\nfrom experimental.rank1_bnns import utils  # local file import\nimport tensorflow as tf\n\nBATCH_NORM_DECAY = 0.9\nBATCH_NORM_EPSILON = 1e-5\n\n\ndef rank1_resnet_layer(inputs,\n                       filters,\n                       kernel_size,\n                       strides,\n                       activation,\n                       alpha_initializer,\n                       gamma_initializer,\n                       alpha_regularizer,\n                       gamma_regularizer,\n                       use_additive_perturbation,\n                       ensemble_size,\n                       random_sign_init,\n                       dropout_rate):\n  """"""Bayesian rank-1 2D Convolution-Batch Norm-Activation stack builder.\n\n  Args:\n    inputs: tf.Tensor.\n    filters: Number of filters for Conv2D.\n    kernel_size: Kernel dimensions for Conv2D.\n    strides: Stride dimensinons for Conv2D.\n    activation: tf.keras.activations.Activation.\n    alpha_initializer: The initializer for the alpha parameters.\n    gamma_initializer: The initializer for the gamma parameters.\n    alpha_regularizer: The regularizer for the alpha parameters.\n    gamma_regularizer: The regularizer for the gamma parameters.\n    use_additive_perturbation: Whether or not to use additive perturbations\n      instead of multiplicative perturbations.\n    ensemble_size: Number of ensemble members.\n    random_sign_init: Value used to initialize trainable deterministic\n      initializers, as applicable. Values greater than zero result in\n      initialization to a random sign vector, where random_sign_init is the\n      probability of a 1 value. Values less than zero result in initialization\n      from a Gaussian with mean 1 and standard deviation equal to\n      -random_sign_init.\n    dropout_rate: Dropout rate.\n\n  Returns:\n    tf.Tensor.\n  """"""\n  x = inputs\n  x = rank1_bnn_layers.Conv2DRank1(\n      filters,\n      kernel_size=kernel_size,\n      strides=strides,\n      padding=\'same\',\n      use_bias=False,\n      alpha_initializer=utils.make_initializer(alpha_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      gamma_initializer=utils.make_initializer(gamma_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      kernel_initializer=\'he_normal\',\n      alpha_regularizer=alpha_regularizer,\n      gamma_regularizer=gamma_regularizer,\n      use_additive_perturbation=use_additive_perturbation,\n      ensemble_size=ensemble_size)(x)\n  x = tf.keras.layers.BatchNormalization(epsilon=BATCH_NORM_EPSILON,\n                                         momentum=BATCH_NORM_DECAY)(x)\n  if activation is not None:\n    x = tf.keras.layers.Activation(activation)(x)\n  return x\n\n\ndef rank1_resnet_v1(input_shape,\n                    depth,\n                    num_classes,\n                    width_multiplier,\n                    alpha_initializer,\n                    gamma_initializer,\n                    alpha_regularizer,\n                    gamma_regularizer,\n                    use_additive_perturbation,\n                    ensemble_size,\n                    random_sign_init,\n                    dropout_rate):\n  """"""Builds Bayesian rank-1 prior ResNet v1.\n\n  Args:\n    input_shape: tf.Tensor.\n    depth: ResNet depth.\n    num_classes: Number of output classes.\n    width_multiplier: Integer to multiply the number of typical filters by.\n    alpha_initializer: The initializer for the alpha parameters.\n    gamma_initializer: The initializer for the gamma parameters.\n    alpha_regularizer: The regularizer for the alpha parameters.\n    gamma_regularizer: The regularizer for the gamma parameters.\n    use_additive_perturbation: Whether or not to use additive perturbations\n      instead of multiplicative perturbations.\n    ensemble_size: Number of ensemble members.\n    random_sign_init: Value used to initialize trainable deterministic\n      initializers, as applicable. Values greater than zero result in\n      initialization to a random sign vector, where random_sign_init is the\n      probability of a 1 value. Values less than zero result in initialization\n      from a Gaussian with mean 1 and standard deviation equal to\n      -random_sign_init.\n    dropout_rate: Dropout rate.\n\n  Returns:\n    tf.keras.Model.\n  """"""\n  if (depth - 2) % 6 != 0:\n    raise ValueError(\'depth should be 6n+2 (e.g., 20, 32, 44).\')\n  filters = 16 * width_multiplier\n  num_res_blocks = int((depth - 2) / 6)\n\n  resnet_layer = functools.partial(\n      rank1_resnet_layer,\n      alpha_initializer=alpha_initializer,\n      gamma_initializer=gamma_initializer,\n      alpha_regularizer=alpha_regularizer,\n      gamma_regularizer=gamma_regularizer,\n      use_additive_perturbation=use_additive_perturbation,\n      ensemble_size=ensemble_size,\n      random_sign_init=random_sign_init,\n      dropout_rate=dropout_rate)\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  x = resnet_layer(inputs,\n                   filters=filters,\n                   kernel_size=3,\n                   strides=1,\n                   activation=\'relu\')\n  for stack in range(3):\n    for res_block in range(num_res_blocks):\n      strides = 1\n      if stack > 0 and res_block == 0:  # first layer but not first stack\n        strides = 2  # downsample\n      y = resnet_layer(\n          x,\n          filters=filters,\n          kernel_size=3,\n          strides=strides,\n          activation=\'relu\')\n      y = resnet_layer(\n          y,\n          filters=filters,\n          kernel_size=3,\n          strides=1,\n          activation=None)\n      if stack > 0 and res_block == 0:  # first layer but not first stack\n        # linear projection residual shortcut connection to match\n        # changed dims\n        x = resnet_layer(\n            x,\n            filters=filters,\n            kernel_size=1,\n            strides=strides,\n            activation=None)\n      x = tf.keras.layers.add([x, y])\n      x = tf.keras.layers.Activation(\'relu\')(x)\n    filters *= 2\n\n  # v1 does not use BN after last shortcut connection-ReLU\n  x = tf.keras.layers.AveragePooling2D(pool_size=8)(x)\n  x = tf.keras.layers.Flatten()(x)\n  x = rank1_bnn_layers.DenseRank1(\n      num_classes,\n      activation=None,\n      alpha_initializer=utils.make_initializer(alpha_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      gamma_initializer=utils.make_initializer(gamma_initializer,\n                                               random_sign_init,\n                                               dropout_rate),\n      kernel_initializer=\'he_normal\',\n      alpha_regularizer=alpha_regularizer,\n      gamma_regularizer=gamma_regularizer,\n      use_additive_perturbation=use_additive_perturbation,\n      ensemble_size=ensemble_size)(x)\n  model = tf.keras.Model(inputs=inputs, outputs=x)\n  return model\n'"
experimental/rank1_bnns/resnet_cifar_model_test.py,10,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Tests for Rank-1 ResNet-32x4.""""""\nfrom absl.testing import parameterized\nfrom experimental.rank1_bnns import resnet_cifar_model  # local file import\nimport tensorflow as tf\n\n\nclass ResnetCifarModelTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.parameters(\n      {\'alpha_initializer\': \'trainable_normal_fixed_stddev\',\n       \'gamma_initializer\': \'trainable_normal_fixed_stddev\',\n       \'random_sign_init\': 0.5,\n       \'ensemble_size\': 1},\n      {\'alpha_initializer\': \'trainable_deterministic\',\n       \'gamma_initializer\': \'trainable_deterministic\',\n       \'random_sign_init\': 0.5,\n       \'ensemble_size\': 2},\n      {\'alpha_initializer\': \'trainable_deterministic\',\n       \'gamma_initializer\': \'trainable_deterministic\',\n       \'random_sign_init\': -0.5,\n       \'ensemble_size\': 2},\n  )\n  def testRank1ResNetV1(self,\n                        alpha_initializer,\n                        gamma_initializer,\n                        random_sign_init,\n                        ensemble_size):\n    tf.random.set_seed(83922)\n    dataset_size = 10\n    batch_size = 6\n    input_shape = (32, 32, 2)  # TODO(dusenberrymw): (32, 32, 1) doesn\'t work...\n    num_classes = 2\n\n    features = tf.random.normal((dataset_size,) + input_shape)\n    coeffs = tf.random.normal([tf.reduce_prod(input_shape), num_classes])\n    net = tf.reshape(features, [dataset_size, -1])\n    logits = tf.matmul(net, coeffs)\n    labels = tf.random.categorical(logits, 1)\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n    dataset = dataset.repeat().shuffle(dataset_size).batch(batch_size)\n\n    model = resnet_cifar_model.rank1_resnet_v1(\n        input_shape=input_shape,\n        depth=8,\n        num_classes=num_classes,\n        width_multiplier=1,\n        alpha_initializer=alpha_initializer,\n        gamma_initializer=gamma_initializer,\n        alpha_regularizer=None,\n        gamma_regularizer=None,\n        use_additive_perturbation=False,\n        ensemble_size=ensemble_size,\n        random_sign_init=-0.5,\n        dropout_rate=0.)\n    model.compile(\n        \'adam\',\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n    history = model.fit(dataset,\n                        steps_per_epoch=dataset_size // batch_size,\n                        epochs=2)\n\n    loss_history = history.history[\'loss\']\n    self.assertAllGreaterEqual(loss_history, 0.)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
experimental/rank1_bnns/utils.py,9,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Utilities.""""""\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\ndef make_sign_initializer(random_sign_init):\n  if random_sign_init > 0:\n    return ed.initializers.RandomSign(random_sign_init)\n  else:\n    return tf.keras.initializers.RandomNormal(mean=1.0,\n                                              stddev=-random_sign_init)\n\n\ndef make_initializer(initializer, random_sign_init, dropout_rate):\n  """"""Builds initializer with specific mean and/or stddevs.""""""\n  if initializer == \'trainable_deterministic\':\n    return ed.initializers.TrainableDeterministic(\n        loc_initializer=make_sign_initializer(random_sign_init))\n  elif initializer == \'trainable_half_cauchy\':\n    stddev_init = np.log(np.expm1(np.sqrt(dropout_rate / (1. - dropout_rate))))\n    return ed.initializers.TrainableHalfCauchy(\n        loc_initializer=make_sign_initializer(random_sign_init),\n        scale_initializer=tf.keras.initializers.Constant(stddev_init),\n        scale_constraint=\'softplus\')\n  elif initializer == \'trainable_cauchy\':\n    stddev_init = np.log(np.expm1(np.sqrt(dropout_rate / (1. - dropout_rate))))\n    return ed.initializers.TrainableCauchy(\n        loc_initializer=make_sign_initializer(random_sign_init),\n        scale_initializer=tf.keras.initializers.Constant(stddev_init),\n        scale_constraint=\'softplus\')\n  elif initializer == \'trainable_normal\':\n    stddev_init = np.log(np.expm1(np.sqrt(dropout_rate / (1. - dropout_rate))))\n    return ed.initializers.TrainableNormal(\n        mean_initializer=make_sign_initializer(random_sign_init),\n        stddev_initializer=tf.keras.initializers.TruncatedNormal(\n            mean=stddev_init, stddev=0.1),\n        stddev_constraint=\'softplus\')\n  elif initializer == \'trainable_log_normal\':\n    stddev_init = np.log(np.expm1(np.sqrt(dropout_rate / (1. - dropout_rate))))\n    return ed.initializers.TrainableLogNormal(\n        loc_initializer=make_sign_initializer(random_sign_init),\n        scale_initializer=tf.keras.initializers.TruncatedNormal(\n            mean=stddev_init, stddev=0.1),\n        scale_constraint=\'softplus\')\n  elif initializer == \'trainable_normal_fixed_stddev\':\n    return ed.initializers.TrainableNormalFixedStddev(\n        stddev=tf.sqrt(dropout_rate / (1. - dropout_rate)),\n        mean_initializer=make_sign_initializer(random_sign_init))\n  elif initializer == \'trainable_normal_shared_stddev\':\n    stddev_init = np.log(np.expm1(np.sqrt(dropout_rate / (1. - dropout_rate))))\n    return ed.initializers.TrainableNormalSharedStddev(\n        mean_initializer=make_sign_initializer(random_sign_init),\n        stddev_initializer=tf.keras.initializers.Constant(stddev_init),\n        stddev_constraint=\'softplus\')\n  return initializer\n\n\nclass NormalKLDivergenceWithTiedMean(tf.keras.regularizers.Regularizer):\n  """"""KL with normal prior whose mean is fixed at the variational posterior\'s.""""""\n\n  def __init__(self, stddev=1., scale_factor=1.):\n    """"""Constructs regularizer.""""""\n    self.stddev = stddev\n    self.scale_factor = scale_factor\n\n  def __call__(self, x):\n    """"""Computes regularization given an ed.Normal random variable as input.""""""\n    if not isinstance(x, ed.RandomVariable):\n      raise ValueError(\'Input must be an ed.RandomVariable.\')\n    prior = ed.Independent(\n        ed.Normal(loc=x.distribution.mean(), scale=self.stddev).distribution,\n        reinterpreted_batch_ndims=len(x.distribution.event_shape))\n    regularization = x.distribution.kl_divergence(prior.distribution)\n    return self.scale_factor * regularization\n\n  def get_config(self):\n    return {\n        \'stddev\': self.stddev,\n        \'scale_factor\': self.scale_factor,\n    }\n\n\ndef make_regularizer(regularizer, mean, stddev):\n  """"""Builds regularizer with specific mean and/or stddevs.""""""\n  if regularizer == \'normal_kl_divergence\':\n    return ed.regularizers.NormalKLDivergence(mean=mean, stddev=stddev)\n  elif regularizer == \'log_normal_kl_divergence\':\n    return ed.regularizers.LogNormalKLDivergence(\n        loc=tf.math.log(1.), scale=stddev)\n  elif regularizer == \'normal_kl_divergence_with_tied_mean\':\n    return NormalKLDivergenceWithTiedMean(stddev=stddev)\n  elif regularizer == \'cauchy_kl_divergence\':\n    return ed.regularizers.CauchyKLDivergence(loc=mean, scale=stddev)\n  elif regularizer == \'normal_empirical_bayes_kl_divergence\':\n    return ed.regularizers.NormalEmpiricalBayesKLDivergence(mean=mean)\n  elif regularizer == \'trainable_normal_kl_divergence_stddev\':\n    return ed.regularizers.TrainableNormalKLDivergenceStdDev(mean=mean)\n  return regularizer\n'"
edward2/tensorflow/layers/__init__.py,0,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Layers.""""""\n\nfrom edward2.tensorflow.layers import utils\nfrom edward2.tensorflow.layers.bayesian_linear_model import BayesianLinearModel\nfrom edward2.tensorflow.layers.convolutional import CondConv2D\nfrom edward2.tensorflow.layers.convolutional import Conv1DBatchEnsemble\nfrom edward2.tensorflow.layers.convolutional import Conv1DFlipout\nfrom edward2.tensorflow.layers.convolutional import Conv1DReparameterization\nfrom edward2.tensorflow.layers.convolutional import Conv2DBatchEnsemble\nfrom edward2.tensorflow.layers.convolutional import Conv2DFlipout\nfrom edward2.tensorflow.layers.convolutional import Conv2DHierarchical\nfrom edward2.tensorflow.layers.convolutional import Conv2DReparameterization\nfrom edward2.tensorflow.layers.convolutional import Conv2DVariationalDropout\nfrom edward2.tensorflow.layers.convolutional import DepthwiseCondConv2D\nfrom edward2.tensorflow.layers.convolutional import DepthwiseConv2DBatchEnsemble\nfrom edward2.tensorflow.layers.dense import DenseBatchEnsemble\nfrom edward2.tensorflow.layers.dense import DenseDVI\nfrom edward2.tensorflow.layers.dense import DenseFlipout\nfrom edward2.tensorflow.layers.dense import DenseHierarchical\nfrom edward2.tensorflow.layers.dense import DenseReparameterization\nfrom edward2.tensorflow.layers.dense import DenseVariationalDropout\nfrom edward2.tensorflow.layers.discrete_flows import DiscreteAutoregressiveFlow\nfrom edward2.tensorflow.layers.discrete_flows import DiscreteBipartiteFlow\nfrom edward2.tensorflow.layers.discrete_flows import Reverse\nfrom edward2.tensorflow.layers.discrete_flows import SinkhornAutoregressiveFlow\nfrom edward2.tensorflow.layers.embeddings import EmbeddingReparameterization\nfrom edward2.tensorflow.layers.gaussian_process import ExponentiatedQuadratic\nfrom edward2.tensorflow.layers.gaussian_process import GaussianProcess\nfrom edward2.tensorflow.layers.gaussian_process import LinearKernel\nfrom edward2.tensorflow.layers.gaussian_process import SparseGaussianProcess\nfrom edward2.tensorflow.layers.gaussian_process import Zeros\nfrom edward2.tensorflow.layers.made import MADE\nfrom edward2.tensorflow.layers.neural_process import Attention\nfrom edward2.tensorflow.layers.neural_process import NeuralProcess\nfrom edward2.tensorflow.layers.noise import NCPCategoricalPerturb\nfrom edward2.tensorflow.layers.noise import NCPNormalOutput\nfrom edward2.tensorflow.layers.noise import NCPNormalPerturb\nfrom edward2.tensorflow.layers.normalization import ActNorm\nfrom edward2.tensorflow.layers.normalization import ensemble_batchnorm\nfrom edward2.tensorflow.layers.normalization import EnsembleSyncBatchNorm\nfrom edward2.tensorflow.layers.recurrent import LSTMCellFlipout\nfrom edward2.tensorflow.layers.recurrent import LSTMCellReparameterization\nfrom edward2.tensorflow.layers.stochastic_output import MixtureLogistic\n\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    ""ActNorm"",\n    ""Attention"",\n    ""BayesianLinearModel"",\n    ""CondConv2D"",\n    ""Conv1DBatchEnsemble"",\n    ""Conv2DBatchEnsemble"",\n    ""Conv1DFlipout"",\n    ""Conv2DFlipout"",\n    ""Conv2DHierarchical"",\n    ""Conv1DReparameterization"",\n    ""Conv2DReparameterization"",\n    ""Conv2DVariationalDropout"",\n    ""DenseBatchEnsemble"",\n    ""DenseDVI"",\n    ""DenseFlipout"",\n    ""DenseHierarchical"",\n    ""DenseReparameterization"",\n    ""DenseVariationalDropout"",\n    ""DepthwiseCondConv2D"",\n    ""DepthwiseConv2DBatchEnsemble"",\n    ""DiscreteAutoregressiveFlow"",\n    ""DiscreteBipartiteFlow"",\n    ""ExponentiatedQuadratic"",\n    ""EmbeddingReparameterization"",\n    ""EnsembleSyncBatchNorm"",\n    ""GaussianProcess"",\n    ""LinearKernel"",\n    ""LSTMCellFlipout"",\n    ""LSTMCellReparameterization"",\n    ""MADE"",\n    ""MixtureLogistic"",\n    ""NCPCategoricalPerturb"",\n    ""NCPNormalOutput"",\n    ""NCPNormalPerturb"",\n    ""NeuralProcess"",\n    ""Reverse"",\n    ""SinkhornAutoregressiveFlow"",\n    ""SparseGaussianProcess"",\n    ""Zeros"",\n    ""ensemble_batchnorm"",\n    ""utils"",\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
edward2/tensorflow/layers/bayesian_linear_model.py,12,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Bayesian linear model.""""""\n\nfrom edward2.tensorflow import generated_random_variables\n\nimport tensorflow as tf\n\n\nclass BayesianLinearModel(tf.keras.Model):\n  r""""""Bayesian linear model with standard normal prior over its coefficients.\n\n  A forward pass computes the mean of the exact predictive distribution\n\n  ```none\n  p(outputs | inputs) = \\int Normal(outputs | coeffs * inputs, noise_variance)\n                             Normal(coeffs | 0, 1) dweights dbias.\n  ```\n\n  It takes a Tensor of shape [batch_size, input_dim] as input and returns a\n  Normal random variable of shape [batch_size] representing its outputs.\n  After `fit()`, the forward pass computes the exact posterior predictive\n  distribution.\n  """"""\n\n  def __init__(self, noise_variance, **kwargs):\n    super(BayesianLinearModel, self).__init__(**kwargs)\n    self.noise_variance = noise_variance\n    self.coeffs_precision_tril_op = None\n    self.coeffs_mean = None\n\n  def call(self, inputs):\n    if self.coeffs_mean is None and self.coeffs_precision_tril_op is None:\n      # p(mean(ynew) | xnew) = Normal(ynew | mean = 0, variance = xnew xnew^T)\n      predictive_mean = 0.\n      predictive_variance = tf.reduce_sum(tf.square(inputs), axis=-1)\n    else:\n      # p(mean(ynew) | xnew, x, y) = Normal(ynew |\n      #   mean = xnew (1/noise_variance) (1/noise_variance x^T x + I)^{-1}x^T y,\n      #   variance = xnew (1/noise_variance x^T x + I)^{-1} xnew^T)\n      predictive_mean = tf.einsum(\'nm,m->n\', inputs, self.coeffs_mean)\n      predictive_covariance = tf.matmul(\n          inputs,\n          self.coeffs_precision_tril_op.solve(\n              self.coeffs_precision_tril_op.solve(inputs, adjoint_arg=True),\n              adjoint=True))\n      predictive_variance = tf.linalg.tensor_diag_part(predictive_covariance)\n    return generated_random_variables.Normal(loc=predictive_mean,\n                                             scale=tf.sqrt(predictive_variance))\n\n  def fit(self, x=None, y=None):\n    # p(coeffs | x, y) = Normal(coeffs |\n    #   mean = (1/noise_variance) (1/noise_variance x^T x + I)^{-1} x^T y,\n    #   covariance = (1/noise_variance x^T x + I)^{-1})\n    # TODO(trandustin): We newly fit the data at each call. Extend to do\n    # Bayesian updating.\n    kernel_matrix = tf.matmul(x, x, transpose_a=True) / self.noise_variance\n    coeffs_precision = tf.linalg.set_diag(\n        kernel_matrix, tf.linalg.diag_part(kernel_matrix) + 1.)\n    coeffs_precision_tril = tf.linalg.cholesky(coeffs_precision)\n    self.coeffs_precision_tril_op = tf.linalg.LinearOperatorLowerTriangular(\n        coeffs_precision_tril)\n    self.coeffs_mean = self.coeffs_precision_tril_op.solvevec(\n        self.coeffs_precision_tril_op.solvevec(tf.einsum(\'nm,n->m\', x, y)),\n        adjoint=True) / self.noise_variance\n    # TODO(trandustin): To be fully Keras-compatible, return History object.\n    return\n'"
edward2/tensorflow/layers/bayesian_linear_model_test.py,8,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for Bayesian linear models.""""""\n\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass BayesianLinearModelTest(tf.test.TestCase):\n\n  def testBayesianLinearModel(self):\n    """"""Tests that model makes reasonable predictions.""""""\n    np.random.seed(42)\n    train_batch_size = 5\n    test_batch_size = 2\n    num_features = 3\n    noise_variance = 0.01\n    coeffs = tf.range(num_features, dtype=tf.float32)\n    features = tf.cast(np.random.randn(train_batch_size, num_features),\n                       dtype=tf.float32)\n    noise = tf.cast(np.random.randn(train_batch_size), dtype=tf.float32)\n    labels = (tf.tensordot(features, coeffs, [[-1], [0]])\n              + noise_variance * noise)\n\n    model = ed.layers.BayesianLinearModel(noise_variance=noise_variance)\n    model.fit(features, labels)\n\n    test_features = np.random.randn(test_batch_size, num_features).astype(\n        np.float32)\n    test_labels = tf.tensordot(test_features, coeffs, [[-1], [0]])\n    outputs = model(test_features)\n    test_predictions = outputs.distribution.mean()\n    test_predictions_variance = outputs.distribution.variance()\n\n    self.assertEqual(test_predictions.shape, (test_batch_size,))\n    self.assertEqual(test_predictions_variance.shape, (test_batch_size,))\n    self.assertAllClose(test_predictions, test_labels, atol=0.1)\n    self.assertAllLessEqual(test_predictions_variance, noise_variance)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
edward2/tensorflow/layers/convolutional.py,126,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Uncertainty-based convolutional layers.""""""\n\nimport functools\nfrom edward2.tensorflow import constraints\nfrom edward2.tensorflow import generated_random_variables\nfrom edward2.tensorflow import initializers\nfrom edward2.tensorflow import random_variable\nfrom edward2.tensorflow import regularizers\nfrom edward2.tensorflow.layers import utils\n\nimport tensorflow as tf\n\n\n@utils.add_weight\nclass Conv2DReparameterization(tf.keras.layers.Conv2D):\n  """"""2D convolution layer (e.g. spatial convolution over images).\n\n  The layer computes a variational Bayesian approximation to the distribution\n  over convolutional layers,\n\n  ```\n  p(outputs | inputs) = int conv2d(inputs; weights, bias) p(weights, bias)\n    dweights dbias.\n  ```\n\n  It does this with a stochastic forward pass, sampling from learnable\n  distributions on the kernel and bias. Gradients with respect to the\n  distributions\' learnable parameters backpropagate via reparameterization.\n  Minimizing cross-entropy plus the layer\'s losses performs variational\n  minimum description length, i.e., it minimizes an upper bound to the negative\n  marginal likelihood.\n  """"""\n\n  def __init__(self,\n               filters,\n               kernel_size,\n               strides=(1, 1),\n               padding=\'valid\',\n               data_format=None,\n               dilation_rate=(1, 1),\n               activation=None,\n               use_bias=True,\n               kernel_initializer=\'trainable_normal\',\n               bias_initializer=\'zeros\',\n               kernel_regularizer=\'normal_kl_divergence\',\n               bias_regularizer=None,\n               activity_regularizer=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               **kwargs):\n    super(Conv2DReparameterization, self).__init__(\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        data_format=data_format,\n        dilation_rate=dilation_rate,\n        activation=activation,\n        use_bias=use_bias,\n        kernel_initializer=initializers.get(kernel_initializer),\n        bias_initializer=initializers.get(bias_initializer),\n        kernel_regularizer=regularizers.get(kernel_regularizer),\n        bias_regularizer=regularizers.get(bias_regularizer),\n        activity_regularizer=regularizers.get(activity_regularizer),\n        kernel_constraint=constraints.get(kernel_constraint),\n        bias_constraint=constraints.get(bias_constraint),\n        **kwargs)\n\n  def call_weights(self):\n    """"""Calls any weights if the initializer is itself a layer.""""""\n    if isinstance(self.kernel_initializer, tf.keras.layers.Layer):\n      self.kernel = self.kernel_initializer(self.kernel.shape, self.dtype)\n    if isinstance(self.bias_initializer, tf.keras.layers.Layer):\n      self.bias = self.bias_initializer(self.bias.shape, self.dtype)\n\n  def call(self, *args, **kwargs):\n    self.call_weights()\n    kwargs.pop(\'training\', None)\n    return super(Conv2DReparameterization, self).call(*args, **kwargs)\n\n\n@utils.add_weight\nclass Conv1DReparameterization(tf.keras.layers.Conv1D):\n  """"""1D convolution layer (e.g. temporal convolution over sequences).\n\n  The layer computes a variational Bayesian approximation to the distribution\n  over convolutional layers,\n\n  ```\n  p(outputs | inputs) = int conv1d(inputs; weights, bias) p(weights, bias)\n    dweights dbias.\n  ```\n\n  It does this with a stochastic forward pass, sampling from learnable\n  distributions on the kernel and bias. Gradients with respect to the\n  distributions\' learnable parameters backpropagate via reparameterization.\n  Minimizing cross-entropy plus the layer\'s losses performs variational\n  minimum description length, i.e., it minimizes an upper bound to the negative\n  marginal likelihood.\n  """"""\n\n  def __init__(self,\n               filters,\n               kernel_size,\n               strides=1,\n               padding=\'valid\',\n               data_format=None,\n               dilation_rate=1,\n               activation=None,\n               use_bias=True,\n               kernel_initializer=\'trainable_normal\',\n               bias_initializer=\'zeros\',\n               kernel_regularizer=\'normal_kl_divergence\',\n               bias_regularizer=None,\n               activity_regularizer=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               **kwargs):\n    super(Conv1DReparameterization, self).__init__(\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        data_format=data_format,\n        dilation_rate=dilation_rate,\n        activation=activation,\n        use_bias=use_bias,\n        kernel_initializer=initializers.get(kernel_initializer),\n        bias_initializer=initializers.get(bias_initializer),\n        kernel_regularizer=regularizers.get(kernel_regularizer),\n        bias_regularizer=regularizers.get(bias_regularizer),\n        activity_regularizer=regularizers.get(activity_regularizer),\n        kernel_constraint=constraints.get(kernel_constraint),\n        bias_constraint=constraints.get(bias_constraint),\n        **kwargs)\n\n  def call_weights(self):\n    """"""Calls any weights if the initializer is itself a layer.""""""\n    if isinstance(self.kernel_initializer, tf.keras.layers.Layer):\n      self.kernel = self.kernel_initializer(self.kernel.shape, self.dtype)\n    if isinstance(self.bias_initializer, tf.keras.layers.Layer):\n      self.bias = self.bias_initializer(self.bias.shape, self.dtype)\n\n  def call(self, *args, **kwargs):\n    self.call_weights()\n    kwargs.pop(\'training\', None)\n    return super(Conv1DReparameterization, self).call(*args, **kwargs)\n\n\nclass Conv2DFlipout(Conv2DReparameterization):\n  """"""2D convolution layer (e.g. spatial convolution over images).\n\n  The layer computes a variational Bayesian approximation to the distribution\n  over convolutional layers,\n\n  ```\n  p(outputs | inputs) = int conv2d(inputs; weights, bias) p(weights, bias)\n    dweights dbias.\n  ```\n\n  It does this with a stochastic forward pass, sampling from learnable\n  distributions on the kernel and bias. Gradients with respect to the\n  distributions\' learnable parameters backpropagate via reparameterization.\n  Minimizing cross-entropy plus the layer\'s losses performs variational\n  minimum description length, i.e., it minimizes an upper bound to the negative\n  marginal likelihood.\n\n  This layer uses the Flipout estimator (Wen et al., 2018) for integrating with\n  respect to the `kernel`. Namely, it applies\n  pseudo-independent weight perturbations via independent sign flips for each\n  example, enabling variance reduction over independent weight perturbations.\n  For this estimator to work, the `kernel` random variable must be able\n  to decompose as a sum of its mean and a perturbation distribution; the\n  perturbation distribution must be independent across weight elements and\n  symmetric around zero (for example, a fully factorized Gaussian).\n  """"""\n\n  def call(self, inputs):\n    if not isinstance(self.kernel, random_variable.RandomVariable):\n      return super(Conv2DFlipout, self).call(inputs)\n    self.call_weights()\n    outputs = self._apply_kernel(inputs)\n    if self.use_bias:\n      if self.data_format == \'channels_first\':\n        outputs = tf.nn.bias_add(outputs, self.bias, data_format=\'NCHW\')\n      else:\n        outputs = tf.nn.bias_add(outputs, self.bias, data_format=\'NHWC\')\n    if self.activation is not None:\n      outputs = self.activation(outputs)\n    return outputs\n\n  def _apply_kernel(self, inputs):\n    input_shape = tf.shape(inputs)\n    batch_dim = input_shape[0]\n    if self._convolution_op is None:\n      padding = self.padding\n      if self.padding == \'causal\':\n        padding = \'valid\'\n      if not isinstance(padding, (list, tuple)):\n        padding = padding.upper()\n      self._convolution_op = functools.partial(\n          tf.nn.convolution,\n          strides=self.strides,\n          padding=padding,\n          data_format=\'NHWC\' if self.data_format == \'channels_last\' else \'NCHW\',\n          dilations=self.dilation_rate)\n\n    if self.data_format == \'channels_first\':\n      channels = input_shape[1]\n      sign_input_shape = [batch_dim, channels, 1, 1]\n      sign_output_shape = [batch_dim, self.filters, 1, 1]\n    else:\n      channels = input_shape[-1]\n      sign_input_shape = [batch_dim, 1, 1, channels]\n      sign_output_shape = [batch_dim, 1, 1, self.filters]\n    sign_input = tf.cast(2 * tf.random.uniform(sign_input_shape,\n                                               minval=0,\n                                               maxval=2,\n                                               dtype=tf.int32) - 1,\n                         inputs.dtype)\n    sign_output = tf.cast(2 * tf.random.uniform(sign_output_shape,\n                                                minval=0,\n                                                maxval=2,\n                                                dtype=tf.int32) - 1,\n                          inputs.dtype)\n    kernel_mean = self.kernel.distribution.mean()\n    perturbation = self.kernel - kernel_mean\n    outputs = self._convolution_op(inputs, kernel_mean)\n    outputs += self._convolution_op(inputs * sign_input,\n                                    perturbation) * sign_output\n    return outputs\n\n\nclass Conv1DFlipout(Conv1DReparameterization):\n  """"""1D convolution layer (e.g. temporal convolution over sequences).\n\n  The layer computes a variational Bayesian approximation to the distribution\n  over convolutional layers,\n\n  ```\n  p(outputs | inputs) = int conv1d(inputs; weights, bias) p(weights, bias)\n    dweights dbias.\n  ```\n\n  It does this with a stochastic forward pass, sampling from learnable\n  distributions on the kernel and bias. Gradients with respect to the\n  distributions\' learnable parameters backpropagate via reparameterization.\n  Minimizing cross-entropy plus the layer\'s losses performs variational\n  minimum description length, i.e., it minimizes an upper bound to the negative\n  marginal likelihood.\n\n  This layer uses the Flipout estimator (Wen et al., 2018) for integrating with\n  respect to the `kernel`. Namely, it applies\n  pseudo-independent weight perturbations via independent sign flips for each\n  example, enabling variance reduction over independent weight perturbations.\n  For this estimator to work, the `kernel` random variable must be able\n  to decompose as a sum of its mean and a perturbation distribution; the\n  perturbation distribution must be independent across weight elements and\n  symmetric around zero (for example, a fully factorized Gaussian).\n  """"""\n\n  def call(self, inputs):\n    if not isinstance(self.kernel, random_variable.RandomVariable):\n      return super(Conv1DFlipout, self).call(inputs)\n    self.call_weights()\n    outputs = self._apply_kernel(inputs)\n    if self.use_bias:\n      if self.data_format == \'channels_first\':\n        outputs = tf.nn.bias_add(outputs, self.bias, data_format=\'NCW\')\n      else:\n        outputs = tf.nn.bias_add(outputs, self.bias, data_format=\'NWC\')\n    if self.activation is not None:\n      outputs = self.activation(outputs)\n    return outputs\n\n  def _apply_kernel(self, inputs):\n    input_shape = tf.shape(inputs)\n    batch_dim = input_shape[0]\n    if self._convolution_op is None:\n      padding = self.padding\n      if self.padding == \'causal\':\n        padding = \'valid\'\n      if not isinstance(padding, (list, tuple)):\n        padding = padding.upper()\n      self._convolution_op = functools.partial(\n          tf.nn.convolution,\n          strides=self.strides,\n          padding=padding,\n          data_format=\'NWC\' if self.data_format == \'channels_last\' else \'NCW\',\n          dilations=self.dilation_rate)\n\n    if self.data_format == \'channels_first\':\n      channels = input_shape[1]\n      sign_input_shape = [batch_dim, channels, 1]\n      sign_output_shape = [batch_dim, self.filters, 1]\n    else:\n      channels = input_shape[-1]\n      sign_input_shape = [batch_dim, 1, channels]\n      sign_output_shape = [batch_dim, 1, self.filters]\n    sign_input = tf.cast(\n        2 * tf.random.uniform(\n            sign_input_shape, minval=0, maxval=2, dtype=tf.int32) - 1,\n        inputs.dtype)\n    sign_output = tf.cast(\n        2 * tf.random.uniform(\n            sign_output_shape, minval=0, maxval=2, dtype=tf.int32) - 1,\n        inputs.dtype)\n    kernel_mean = self.kernel.distribution.mean()\n    perturbation = self.kernel - kernel_mean\n    outputs = self._convolution_op(inputs, kernel_mean)\n    outputs += self._convolution_op(inputs * sign_input,\n                                    perturbation) * sign_output\n    return outputs\n\n\nclass Conv2DHierarchical(Conv2DFlipout):\n  """"""2D convolution layer with hierarchical distributions.\n\n  The layer computes a variational Bayesian approximation to the distribution\n  over convolutional layers, and where the distribution over weights\n  involves a hierarchical distribution with hidden unit noise coupling vectors\n  of the kernel weight matrix (Louizos et al., 2017),\n\n  ```\n  p(outputs | inputs) = int conv2d(inputs; new_kernel, bias) p(kernel,\n    local_scales, global_scale, bias) dkernel dlocal_scales dglobal_scale dbias.\n  ```\n\n  It does this with a stochastic forward pass, sampling from learnable\n  distributions on the kernel and bias. The kernel is written in non-centered\n  parameterization where\n\n  ```\n  new_kernel[i, j] = kernel[i, j] * local_scale[j] * global_scale.\n  ```\n\n  That is, there is ""local"" multiplicative noise which couples weights for each\n  output filter. There is also a ""global"" multiplicative noise which couples the\n  entire weight matrix. By default, the weights are normally distributed and the\n  local and global noises are half-Cauchy distributed; this makes the kernel a\n  horseshoe distribution (Carvalho et al., 2009; Polson and Scott, 2012).\n\n  The estimation uses Flipout for variance reduction with respect to sampling\n  the full weights. Gradients with respect to the distributions\' learnable\n  parameters backpropagate via reparameterization. Minimizing cross-entropy\n  plus the layer\'s losses performs variational minimum description length,\n  i.e., it minimizes an upper bound to the negative marginal likelihood.\n  """"""\n\n  def __init__(self,\n               filters,\n               kernel_size,\n               strides=(1, 1),\n               padding=\'valid\',\n               data_format=None,\n               dilation_rate=(1, 1),\n               activation=None,\n               use_bias=True,\n               kernel_initializer=\'trainable_normal\',\n               bias_initializer=\'zeros\',\n               local_scale_initializer=\'trainable_half_cauchy\',\n               global_scale_initializer=\'trainable_half_cauchy\',\n               kernel_regularizer=\'normal_kl_divergence\',\n               bias_regularizer=None,\n               local_scale_regularizer=\'half_cauchy_kl_divergence\',\n               global_scale_regularizer=regularizers.HalfCauchyKLDivergence(\n                   scale=1e-5),\n               activity_regularizer=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               local_scale_constraint=\'softplus\',\n               global_scale_constraint=\'softplus\',\n               **kwargs):\n    self.local_scale_initializer = initializers.get(local_scale_initializer)\n    self.global_scale_initializer = initializers.get(global_scale_initializer)\n    self.local_scale_regularizer = regularizers.get(local_scale_regularizer)\n    self.global_scale_regularizer = regularizers.get(global_scale_regularizer)\n    self.local_scale_constraint = constraints.get(local_scale_constraint)\n    self.global_scale_constraint = constraints.get(global_scale_constraint)\n    super(Conv2DHierarchical, self).__init__(\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        data_format=data_format,\n        dilation_rate=dilation_rate,\n        activation=activation,\n        use_bias=use_bias,\n        kernel_initializer=initializers.get(kernel_initializer),\n        bias_initializer=initializers.get(bias_initializer),\n        kernel_regularizer=regularizers.get(kernel_regularizer),\n        bias_regularizer=regularizers.get(bias_regularizer),\n        activity_regularizer=regularizers.get(activity_regularizer),\n        kernel_constraint=constraints.get(kernel_constraint),\n        bias_constraint=constraints.get(bias_constraint),\n        **kwargs)\n\n  def build(self, input_shape):\n    self.local_scale = self.add_weight(\n        shape=(self.filters,),\n        name=\'local_scale\',\n        initializer=self.local_scale_initializer,\n        regularizer=self.local_scale_regularizer,\n        constraint=self.local_scale_constraint)\n    self.global_scale = self.add_weight(\n        shape=(),\n        name=\'global_scale\',\n        initializer=self.global_scale_initializer,\n        regularizer=self.global_scale_regularizer,\n        constraint=self.global_scale_constraint)\n    super(Conv2DHierarchical, self).build(input_shape)\n\n  def call_weights(self):\n    """"""Calls any weights if the initializer is itself a layer.""""""\n    if isinstance(self.local_scale_initializer, tf.keras.layers.Layer):\n      self.local_scale = self.local_scale_initializer(self.local_scale.shape,\n                                                      self.dtype)\n    if isinstance(self.global_scale_initializer, tf.keras.layers.Layer):\n      self.global_scale = self.global_scale_initializer(self.global_scale.shape,\n                                                        self.dtype)\n    super(Conv2DHierarchical, self).call_weights()\n\n  def _apply_kernel(self, inputs):\n    outputs = super(Conv2DHierarchical, self)._apply_kernel(inputs)\n    if self.data_format == \'channels_first\':\n      local_scale = tf.reshape(self.local_scale, [1, -1, 1, 1])\n    else:\n      local_scale = tf.reshape(self.local_scale, [1, 1, 1, -1])\n    # TODO(trandustin): Figure out what to set local/global scales to at test\n    # time. Means don\'t exist for Half-Cauchy approximate posteriors.\n    outputs *= local_scale * self.global_scale\n    return outputs\n\n\nclass Conv2DVariationalDropout(Conv2DReparameterization):\n  """"""2D convolution layer with variational dropout (Kingma et al., 2015).\n\n  Implementation follows the additive parameterization of\n  Molchanov et al. (2017).\n  """"""\n\n  def __init__(self,\n               filters,\n               kernel_size,\n               strides=(1, 1),\n               padding=\'valid\',\n               data_format=None,\n               dilation_rate=(1, 1),\n               activation=None,\n               use_bias=True,\n               kernel_initializer=\'trainable_normal\',\n               bias_initializer=\'zeros\',\n               kernel_regularizer=\'log_uniform_kl_divergence\',\n               bias_regularizer=None,\n               activity_regularizer=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               **kwargs):\n    super(Conv2DVariationalDropout, self).__init__(\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        data_format=data_format,\n        dilation_rate=dilation_rate,\n        activation=activation,\n        use_bias=use_bias,\n        kernel_initializer=initializers.get(kernel_initializer),\n        bias_initializer=initializers.get(bias_initializer),\n        kernel_regularizer=regularizers.get(kernel_regularizer),\n        bias_regularizer=regularizers.get(bias_regularizer),\n        activity_regularizer=regularizers.get(activity_regularizer),\n        kernel_constraint=constraints.get(kernel_constraint),\n        bias_constraint=constraints.get(bias_constraint),\n        **kwargs)\n\n  def call(self, inputs, training=None):\n    if not isinstance(self.kernel, random_variable.RandomVariable):\n      return super(Conv2DVariationalDropout, self).call(inputs)\n    self.call_weights()\n    if training is None:\n      training = tf.keras.backend.learning_phase()\n    if self._convolution_op is None:\n      padding = self.padding\n      if self.padding == \'causal\':\n        padding = \'valid\'\n      if not isinstance(padding, (list, tuple)):\n        padding = padding.upper()\n      self._convolution_op = functools.partial(\n          tf.nn.convolution,\n          strides=self.strides,\n          padding=padding,\n          data_format=\'NHWC\' if self.data_format == \'channels_last\' else \'NCHW\',\n          dilations=self.dilation_rate)\n\n    def dropped_inputs():\n      """"""Forward pass with dropout.""""""\n      # Clip magnitude of dropout rate, where we get the dropout rate alpha from\n      # the additive parameterization (Molchanov et al., 2017): for weight ~\n      # Normal(mu, sigma**2), the variance `sigma**2 = alpha * mu**2`.\n      mean = self.kernel.distribution.mean()\n      log_variance = tf.math.log(self.kernel.distribution.variance())\n      log_alpha = log_variance - tf.math.log(tf.square(mean) +\n                                             tf.keras.backend.epsilon())\n      log_alpha = tf.clip_by_value(log_alpha, -8., 8.)\n      log_variance = log_alpha + tf.math.log(tf.square(mean) +\n                                             tf.keras.backend.epsilon())\n\n      means = self._convolution_op(inputs, mean)\n      stddevs = tf.sqrt(\n          self._convolution_op(tf.square(inputs), tf.exp(log_variance)) +\n          tf.keras.backend.epsilon())\n      if self.use_bias:\n        if self.data_format == \'channels_first\':\n          means = tf.nn.bias_add(means, self.bias, data_format=\'NCHW\')\n        else:\n          means = tf.nn.bias_add(means, self.bias, data_format=\'NHWC\')\n      outputs = generated_random_variables.Normal(loc=means, scale=stddevs)\n      if self.activation is not None:\n        outputs = self.activation(outputs)\n      return outputs\n\n    # Following tf.keras.Dropout, only apply variational dropout if training\n    # flag is True.\n    training_value = utils.smart_constant_value(training)\n    if training_value is not None:\n      if training_value:\n        return dropped_inputs()\n      else:\n        return super(Conv2DVariationalDropout, self).call(inputs)\n    return tf.cond(\n        pred=training,\n        true_fn=dropped_inputs,\n        false_fn=lambda: super(Conv2DVariationalDropout, self).call(inputs))\n\n\nclass Conv2DBatchEnsemble(tf.keras.layers.Layer):\n  """"""A batch ensemble convolutional layer.""""""\n\n  def __init__(self,\n               filters,\n               kernel_size,\n               rank=1,\n               ensemble_size=4,\n               alpha_initializer=\'ones\',\n               gamma_initializer=\'ones\',\n               strides=(1, 1),\n               padding=\'valid\',\n               data_format=None,\n               activation=None,\n               use_bias=True,\n               kernel_initializer=\'glorot_uniform\',\n               bias_initializer=\'zeros\',\n               kernel_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               **kwargs):\n    super(Conv2DBatchEnsemble, self).__init__(**kwargs)\n    self.rank = rank\n    self.ensemble_size = ensemble_size\n    self.alpha_initializer = initializers.get(alpha_initializer)\n    self.gamma_initializer = initializers.get(gamma_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n    self.bias_constraint = constraints.get(bias_constraint)\n    self.activation = tf.keras.activations.get(activation)\n    self.use_bias = use_bias\n    self.conv2d = tf.keras.layers.Conv2D(\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        data_format=data_format,\n        activation=None,\n        use_bias=False,\n        kernel_initializer=kernel_initializer,\n        bias_initializer=None,\n        kernel_regularizer=kernel_regularizer,\n        bias_regularizer=None,\n        activity_regularizer=activity_regularizer,\n        kernel_constraint=kernel_constraint,\n        bias_constraint=None)\n    self.filters = self.conv2d.filters\n    self.kernel_size = self.conv2d.kernel_size\n    self.data_format = self.conv2d.data_format\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    if self.data_format == \'channels_first\':\n      input_channel = input_shape[1]\n    elif self.data_format == \'channels_last\':\n      input_channel = input_shape[-1]\n\n    if self.rank > 1:\n      alpha_shape = [self.rank, self.ensemble_size, input_channel]\n      gamma_shape = [self.rank, self.ensemble_size, self.filters]\n    else:\n      alpha_shape = [self.ensemble_size, input_channel]\n      gamma_shape = [self.ensemble_size, self.filters]\n    self.alpha = self.add_weight(\n        \'alpha\',\n        shape=alpha_shape,\n        initializer=self.alpha_initializer,\n        trainable=True,\n        dtype=self.dtype)\n    self.gamma = self.add_weight(\n        \'gamma\',\n        shape=gamma_shape,\n        initializer=self.gamma_initializer,\n        trainable=True,\n        dtype=self.dtype)\n    if self.use_bias:\n      self.bias = self.add_weight(\n          name=\'bias\',\n          shape=[self.ensemble_size, self.filters],\n          initializer=self.bias_initializer,\n          regularizer=self.bias_regularizer,\n          constraint=self.bias_constraint,\n          trainable=True,\n          dtype=self.dtype)\n    else:\n      self.bias = None\n    self.built = True\n\n  def call(self, inputs):\n    input_dim = self.alpha.shape[-1]\n    batch_size = tf.shape(inputs)[0]\n    examples_per_model = batch_size // self.ensemble_size\n    # TODO(ywenxu): Merge the following two cases.\n    if self.rank > 1:\n      # TODO(ywenxu): Check whether the following works in channels_last case.\n      axis_change = -1 if self.data_format == \'channels_first\' else 2\n      alpha = tf.reshape(tf.tile(self.alpha, [1, 1, examples_per_model]),\n                         [self.rank, batch_size, input_dim])\n      gamma = tf.reshape(tf.tile(self.gamma, [1, 1, examples_per_model]),\n                         [self.rank, batch_size, self.filters])\n\n      alpha = tf.expand_dims(alpha, axis=axis_change)\n      alpha = tf.expand_dims(alpha, axis=axis_change)\n      gamma = tf.expand_dims(gamma, axis=axis_change)\n      gamma = tf.expand_dims(gamma, axis=axis_change)\n\n      perturb_inputs = tf.expand_dims(inputs, 0) * alpha\n      perturb_inputs = tf.reshape(perturb_inputs, tf.concat(\n          [[-1], perturb_inputs.shape[2:]], 0))\n      outputs = self.conv2d(perturb_inputs)\n\n      outputs = tf.reshape(outputs, tf.concat(\n          [[self.rank, -1], outputs.shape[1:]], 0))\n      outputs = tf.reduce_sum(outputs * gamma, axis=0)\n    else:\n      axis_change = -1 if self.data_format == \'channels_first\' else 1\n      alpha = tf.reshape(tf.tile(self.alpha, [1, examples_per_model]),\n                         [batch_size, input_dim])\n      gamma = tf.reshape(tf.tile(self.gamma, [1, examples_per_model]),\n                         [batch_size, self.filters])\n      alpha = tf.expand_dims(alpha, axis=axis_change)\n      alpha = tf.expand_dims(alpha, axis=axis_change)\n      gamma = tf.expand_dims(gamma, axis=axis_change)\n      gamma = tf.expand_dims(gamma, axis=axis_change)\n      outputs = self.conv2d(inputs*alpha) * gamma\n\n    if self.use_bias:\n      bias = tf.reshape(tf.tile(self.bias, [1, examples_per_model]),\n                        [batch_size, self.filters])\n      bias = tf.expand_dims(bias, axis=axis_change)\n      bias = tf.expand_dims(bias, axis=axis_change)\n      outputs += bias\n\n    if self.activation is not None:\n      outputs = self.activation(outputs)\n    return outputs\n\n  def compute_output_shape(self, input_shape):\n    return self.conv2d.compute_output_shape(input_shape)\n\n  def get_config(self):\n    config = {\n        \'ensemble_size\': self.ensemble_size,\n        \'alpha_initializer\': initializers.serialize(self.alpha_initializer),\n        \'gamma_initializer\': initializers.serialize(self.gamma_initializer),\n        \'bias_initializer\': initializers.serialize(self.bias_initializer),\n        \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\n        \'bias_constraint\': constraints.serialize(self.bias_constraint),\n        \'activation\': tf.keras.activations.serialize(self.activation),\n        \'use_bias\': self.use_bias,\n    }\n    new_config = super(Conv2DBatchEnsemble, self).get_config()\n    new_config.update(self.conv2d.get_config())\n    new_config.update(config)\n    return new_config\n\n\nclass Conv1DBatchEnsemble(tf.keras.layers.Layer):\n  """"""A batch ensemble convolutional layer.""""""\n\n  def __init__(self,\n               filters,\n               kernel_size,\n               ensemble_size=4,\n               alpha_initializer=\'ones\',\n               gamma_initializer=\'ones\',\n               strides=1,\n               padding=\'valid\',\n               data_format=\'channels_last\',\n               activation=None,\n               use_bias=True,\n               kernel_initializer=\'glorot_uniform\',\n               bias_initializer=\'zeros\',\n               kernel_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               **kwargs):\n    super(Conv1DBatchEnsemble, self).__init__(**kwargs)\n    self.ensemble_size = ensemble_size\n    self.alpha_initializer = initializers.get(alpha_initializer)\n    self.gamma_initializer = initializers.get(gamma_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n    self.bias_constraint = constraints.get(bias_constraint)\n    self.activation = tf.keras.activations.get(activation)\n    self.use_bias = use_bias\n    self.conv1d = tf.keras.layers.Conv1D(\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        data_format=data_format,\n        activation=None,\n        use_bias=False,\n        kernel_initializer=kernel_initializer,\n        bias_initializer=None,\n        kernel_regularizer=kernel_regularizer,\n        bias_regularizer=None,\n        activity_regularizer=activity_regularizer,\n        kernel_constraint=kernel_constraint,\n        bias_constraint=None)\n    self.filters = self.conv1d.filters\n    self.kernel_size = self.conv1d.kernel_size\n    self.data_format = self.conv1d.data_format\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    if self.data_format == \'channels_first\':\n      input_channel = input_shape[1]\n    elif self.data_format == \'channels_last\':\n      input_channel = input_shape[-1]\n\n    self.alpha = self.add_weight(\n        \'alpha\',\n        shape=[self.ensemble_size, input_channel],\n        initializer=self.alpha_initializer,\n        trainable=True,\n        dtype=self.dtype)\n    self.gamma = self.add_weight(\n        \'gamma\',\n        shape=[self.ensemble_size, self.filters],\n        initializer=self.gamma_initializer,\n        trainable=True,\n        dtype=self.dtype)\n    if self.use_bias:\n      self.bias = self.add_weight(\n          name=\'bias\',\n          shape=[self.ensemble_size, self.filters],\n          initializer=self.bias_initializer,\n          regularizer=self.bias_regularizer,\n          constraint=self.bias_constraint,\n          trainable=True,\n          dtype=self.dtype)\n    else:\n      self.bias = None\n    self.built = True\n\n  def call(self, inputs):\n    axis_change = -1 if self.data_format == \'channels_first\' else 1\n    batch_size = tf.shape(inputs)[0]\n    input_dim = self.alpha.shape[-1]\n    examples_per_model = batch_size // self.ensemble_size\n    alpha = tf.reshape(tf.tile(self.alpha, [1, examples_per_model]),\n                       [batch_size, input_dim])\n    gamma = tf.reshape(tf.tile(self.gamma, [1, examples_per_model]),\n                       [batch_size, self.filters])\n    alpha = tf.expand_dims(alpha, axis=axis_change)\n    gamma = tf.expand_dims(gamma, axis=axis_change)\n    outputs = self.conv1d(inputs*alpha) * gamma\n\n    if self.use_bias:\n      bias = tf.reshape(tf.tile(self.bias, [1, examples_per_model]),\n                        [batch_size, self.filters])\n      bias = tf.expand_dims(bias, axis=axis_change)\n      outputs += bias\n\n    if self.activation is not None:\n      outputs = self.activation(outputs)\n    return outputs\n\n  def compute_output_shape(self, input_shape):\n    return self.conv1d.compute_output_shape(input_shape)\n\n  def get_config(self):\n    config = {\n        \'ensemble_size\': self.ensemble_size,\n        \'alpha_initializer\': initializers.serialize(self.alpha_initializer),\n        \'gamma_initializer\': initializers.serialize(self.gamma_initializer),\n        \'bias_initializer\': initializers.serialize(self.bias_initializer),\n        \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\n        \'bias_constraint\': constraints.serialize(self.bias_constraint),\n        \'activation\': tf.keras.activations.serialize(self.activation),\n        \'use_bias\': self.use_bias,\n    }\n    new_config = super(Conv1DBatchEnsemble, self).get_config()\n    new_config.update(self.conv1d.get_config())\n    new_config.update(config)\n    return new_config\n\n\n@utils.add_weight\nclass CondConv2D(tf.keras.layers.Conv2D):\n  """"""2D conditional convolution layer (e.g. spatial convolution over images).\n\n  This layer extends the base 2D convolution layer to compute example-dependent\n  parameters. A CondConv2D layer has \'num_experts` kernels and biases. It\n  computes a kernel and bias for each example as a weighted sum of experts\n  using the input example-dependent routing weights, then applies the 2D\n  convolution to each example.\n\n  Attributes:\n    filters: Integer, the dimensionality of the output space (i.e. the number of\n      output filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the height\n      and width of the 2D convolution window. Can be a single integer to specify\n      the same value for all spatial dimensions.\n    num_experts: The number of expert kernels and biases in the CondConv layer.\n    strides: An integer or tuple/list of 2 integers, specifying the strides of\n      the convolution along the height and width. Can be a single integer to\n      specify the same value for all spatial dimensions. Specifying any stride\n      value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n    padding: one of `""valid""` or `""same""` (case-insensitive).\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n      The ordering of the dimensions in the inputs. `channels_last` corresponds\n      to inputs with shape `(batch, height, width, channels)` while\n      `channels_first` corresponds to inputs with shape `(batch, channels,\n      height, width)`. It defaults to the `image_data_format` value found in\n      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n      it will be ""channels_last"".\n    dilation_rate: an integer or tuple/list of 2 integers, specifying the\n      dilation rate to use for dilated convolution. Can be a single integer to\n      specify the same value for all spatial dimensions. Currently, specifying\n      any `dilation_rate` value != 1 is incompatible with specifying any stride\n      value != 1.\n    activation: Activation function to use. If you don\'t specify anything, no\n      activation is applied\n      (ie. ""linear"" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix.\n    bias_initializer: Initializer for the bias vector.\n    kernel_regularizer: Regularizer function applied to the `kernel` weights\n      matrix.\n    bias_regularizer: Regularizer function applied to the bias vector.\n    activity_regularizer: Regularizer function applied to the output of the\n      layer (its ""activation"")..\n    kernel_constraint: Constraint function applied to the kernel matrix.\n    bias_constraint: Constraint function applied to the bias vector.\n\n  Input shape:\n    4D tensor with shape: `(samples, channels, rows, cols)` if\n      data_format=\'channels_first\'\n    or 4D tensor with shape: `(samples, rows, cols, channels)` if\n      data_format=\'channels_last\'.\n\n  Output shape:\n    4D tensor with shape: `(samples, filters, new_rows, new_cols)` if\n      data_format=\'channels_first\'\n    or 4D tensor with shape: `(samples, new_rows, new_cols, filters)` if\n      data_format=\'channels_last\'. `rows` and `cols` values might have changed\n      due to padding.\n  """"""\n\n  def __init__(self,\n               filters,\n               kernel_size,\n               num_experts,\n               strides=(1, 1),\n               padding=\'valid\',\n               data_format=None,\n               dilation_rate=(1, 1),\n               activation=None,\n               use_bias=True,\n               kernel_initializer=\'glorot_uniform\',\n               bias_initializer=\'zeros\',\n               kernel_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               **kwargs):\n    super(CondConv2D, self).__init__(\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        data_format=data_format,\n        dilation_rate=dilation_rate,\n        activation=activation,\n        use_bias=use_bias,\n        kernel_initializer=kernel_initializer,\n        bias_initializer=bias_initializer,\n        kernel_regularizer=kernel_regularizer,\n        bias_regularizer=bias_regularizer,\n        activity_regularizer=activity_regularizer,\n        kernel_constraint=kernel_constraint,\n        bias_constraint=bias_constraint,\n        **kwargs)\n    if num_experts < 1:\n      raise ValueError(\'A CondConv layer must have at least one expert.\')\n    self.num_experts = num_experts\n    if self.data_format == \'channels_first\':\n      self.converted_data_format = \'NCHW\'\n    else:\n      self.converted_data_format = \'NHWC\'\n\n  def build(self, input_shape):\n    if len(input_shape) != 4:\n      raise ValueError(\n          \'Inputs to `CondConv2D` should have rank 4. \'\n          \'Received input shape:\', str(input_shape))\n    input_shape = tf.TensorShape(input_shape)\n    channel_axis = self._get_channel_axis()\n    if input_shape.dims[channel_axis].value is None:\n      raise ValueError(\'The channel dimension of the inputs \'\n                       \'should be defined. Found `None`.\')\n    input_dim = int(input_shape[channel_axis])\n\n    self.kernel_shape = self.kernel_size + (input_dim, self.filters)\n    kernel_num_params = 1\n    for kernel_dim in self.kernel_shape:\n      kernel_num_params *= kernel_dim\n    condconv_kernel_shape = (self.num_experts, kernel_num_params)\n    self.condconv_kernel = self.add_weight(\n        name=\'condconv_kernel\',\n        shape=condconv_kernel_shape,\n        initializer=initializers.get_condconv_initializer(\n            self.kernel_initializer,\n            self.num_experts,\n            self.kernel_shape),\n        regularizer=self.kernel_regularizer,\n        constraint=self.kernel_constraint,\n        trainable=True,\n        dtype=self.dtype)\n\n    if self.use_bias:\n      self.bias_shape = (self.filters,)\n      condconv_bias_shape = (self.num_experts, self.filters)\n      self.condconv_bias = self.add_weight(\n          name=\'condconv_bias\',\n          shape=condconv_bias_shape,\n          initializer=initializers.get_condconv_initializer(\n              self.bias_initializer,\n              self.num_experts,\n              self.bias_shape),\n          regularizer=self.bias_regularizer,\n          constraint=self.bias_constraint,\n          trainable=True,\n          dtype=self.dtype)\n    else:\n      self.bias = None\n\n    self.input_spec = tf.keras.layers.InputSpec(\n        ndim=self.rank + 2, axes={channel_axis: input_dim})\n\n    self.built = True\n\n  def call(self, inputs, routing_weights):\n    # Compute example dependent kernels\n    kernels = tf.matmul(routing_weights, self.condconv_kernel)\n    batch_size = inputs.shape[0].value\n    inputs = tf.split(inputs, batch_size, 0)\n    kernels = tf.split(kernels, batch_size, 0)\n    # Apply example-dependent convolution to each example in the batch\n    outputs_list = []\n    # TODO(ywenxu): Check out tf.vectorized_map.\n    for input_tensor, kernel in zip(inputs, kernels):\n      kernel = tf.reshape(kernel, self.kernel_shape)\n      outputs_list.append(\n          tf.nn.convolution(\n              input_tensor,\n              kernel,\n              strides=self.strides,\n              padding=self._get_padding_op(),\n              dilations=self.dilation_rate,\n              data_format=self.converted_data_format))\n    outputs = tf.concat(outputs_list, 0)\n\n    if self.use_bias:\n      # Compute example-dependent biases\n      biases = tf.matmul(routing_weights, self.condconv_bias)\n      outputs = tf.split(outputs, batch_size, 0)\n      biases = tf.split(biases, batch_size, 0)\n      # Add example-dependent bias to each example in the batch\n      bias_outputs_list = []\n      for output, bias in zip(outputs, biases):\n        bias = tf.squeeze(bias, axis=0)\n        bias_outputs_list.append(\n            tf.nn.bias_add(output, bias,\n                           data_format=self.converted_data_format))\n      outputs = tf.concat(bias_outputs_list, 0)\n\n    if self.activation is not None:\n      return self.activation(outputs)\n    return outputs\n\n  def get_config(self):\n    config = {\'num_experts\': self.num_experts}\n    base_config = super(CondConv2D, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n\n  def _get_channel_axis(self):\n    if self.data_format == \'channels_first\':\n      return 1\n    else:\n      return -1\n\n  def _get_padding_op(self):\n    if self.padding == \'causal\':\n      op_padding = \'valid\'\n    else:\n      op_padding = self.padding\n    if not isinstance(op_padding, (list, tuple)):\n      op_padding = op_padding.upper()\n    return op_padding\n\n\n@utils.add_weight\nclass DepthwiseCondConv2D(tf.keras.layers.DepthwiseConv2D):\n  """"""Depthwise separable 2D conditional convolution layer.\n\n  This layer extends the base depthwise 2D convolution layer to compute\n  example-dependent parameters. A DepthwiseCondConv2D layer has \'num_experts`\n  kernels and biases. It computes a kernel and bias for each example as a\n  weighted sum of experts using the input example-dependent routing weights,\n  then applies the depthwise convolution to each example.\n\n  Attributes:\n    kernel_size: An integer or tuple/list of 2 integers, specifying the height\n      and width of the 2D convolution window. Can be a single integer to specify\n      the same value for all spatial dimensions.\n    num_experts: The number of expert kernels and biases in the\n      DepthwiseCondConv2D layer.\n    strides: An integer or tuple/list of 2 integers, specifying the strides of\n      the convolution along the height and width. Can be a single integer to\n      specify the same value for all spatial dimensions. Specifying any stride\n      value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n    padding: one of `\'valid\'` or `\'same\'` (case-insensitive).\n    depth_multiplier: The number of depthwise convolution output channels for\n      each input channel. The total number of depthwise convolution output\n      channels will be equal to `filters_in * depth_multiplier`.\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n      The ordering of the dimensions in the inputs. `channels_last` corresponds\n      to inputs with shape `(batch, height, width, channels)` while\n      `channels_first` corresponds to inputs with shape `(batch, channels,\n      height, width)`. It defaults to the `image_data_format` value found in\n      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n      it will be \'channels_last\'.\n    activation: Activation function to use. If you don\'t specify anything, no\n      activation is applied\n      (ie. \'linear\' activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    depthwise_initializer: Initializer for the depthwise kernel matrix.\n    bias_initializer: Initializer for the bias vector.\n    depthwise_regularizer: Regularizer function applied to the depthwise kernel\n      matrix.\n    bias_regularizer: Regularizer function applied to the bias vector.\n    activity_regularizer: Regularizer function applied to the output of the\n      layer (its \'activation\').\n    depthwise_constraint: Constraint function applied to the depthwise kernel\n      matrix.\n    bias_constraint: Constraint function applied to the bias vector.\n\n  Input shape:\n    4D tensor with shape: `[batch, channels, rows, cols]` if\n      data_format=\'channels_first\'\n    or 4D tensor with shape: `[batch, rows, cols, channels]` if\n      data_format=\'channels_last\'.\n\n  Output shape:\n    4D tensor with shape: `[batch, filters, new_rows, new_cols]` if\n      data_format=\'channels_first\'\n    or 4D tensor with shape: `[batch, new_rows, new_cols, filters]` if\n      data_format=\'channels_last\'. `rows` and `cols` values might have changed\n      due to padding.\n  """"""\n\n  def __init__(self,\n               kernel_size,\n               num_experts,\n               strides=(1, 1),\n               padding=\'valid\',\n               depth_multiplier=1,\n               data_format=None,\n               activation=None,\n               use_bias=True,\n               depthwise_initializer=\'glorot_uniform\',\n               bias_initializer=\'zeros\',\n               depthwise_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               depthwise_constraint=None,\n               bias_constraint=None,\n               **kwargs):\n    super(DepthwiseCondConv2D, self).__init__(\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        depth_multiplier=depth_multiplier,\n        data_format=data_format,\n        activation=activation,\n        use_bias=use_bias,\n        depthwise_initializer=depthwise_initializer,\n        bias_initializer=bias_initializer,\n        depthwise_regularizer=depthwise_regularizer,\n        bias_regularizer=bias_regularizer,\n        activity_regularizer=activity_regularizer,\n        depthwise_constraint=depthwise_constraint,\n        bias_constraint=bias_constraint,\n        **kwargs)\n    if num_experts < 1:\n      raise ValueError(\'A CondConv layer must have at least one expert.\')\n    self.num_experts = num_experts\n    if self.data_format == \'channels_first\':\n      self.converted_data_format = \'NCHW\'\n    else:\n      self.converted_data_format = \'NHWC\'\n\n  def build(self, input_shape):\n    if len(input_shape) < 4:\n      raise ValueError(\n          \'Inputs to `DepthwiseCondConv2D` should have rank 4. \'\n          \'Received input shape:\', str(input_shape))\n    input_shape = tf.TensorShape(input_shape)\n    if self.data_format == \'channels_first\':\n      channel_axis = 1\n    else:\n      channel_axis = 3\n    if input_shape.dims[channel_axis].value is None:\n      raise ValueError(\'The channel dimension of the inputs to \'\n                       \'`DepthwiseConv2D` \'\n                       \'should be defined. Found `None`.\')\n    input_dim = int(input_shape[channel_axis])\n    self.depthwise_kernel_shape = (self.kernel_size[0], self.kernel_size[1],\n                                   input_dim, self.depth_multiplier)\n\n    depthwise_kernel_num_params = 1\n    for dim in self.depthwise_kernel_shape:\n      depthwise_kernel_num_params *= dim\n    depthwise_condconv_kernel_shape = (self.num_experts,\n                                       depthwise_kernel_num_params)\n\n    self.depthwise_condconv_kernel = self.add_weight(\n        shape=depthwise_condconv_kernel_shape,\n        initializer=initializers.get_condconv_initializer(\n            self.depthwise_initializer,\n            self.num_experts,\n            self.depthwise_kernel_shape),\n        name=\'depthwise_condconv_kernel\',\n        regularizer=self.depthwise_regularizer,\n        constraint=self.depthwise_constraint,\n        trainable=True)\n\n    if self.use_bias:\n      bias_dim = input_dim * self.depth_multiplier\n      self.bias_shape = (bias_dim,)\n      condconv_bias_shape = (self.num_experts, bias_dim)\n      self.condconv_bias = self.add_weight(\n          name=\'condconv_bias\',\n          shape=condconv_bias_shape,\n          initializer=initializers.get_condconv_initializer(\n              self.bias_initializer,\n              self.num_experts,\n              self.bias_shape),\n          regularizer=self.bias_regularizer,\n          constraint=self.bias_constraint,\n          trainable=True,\n          dtype=self.dtype)\n    else:\n      self.bias = None\n    # Set input spec.\n    self.input_spec = tf.keras.layers.InputSpec(\n        ndim=4, axes={channel_axis: input_dim})\n    self.built = True\n\n  def call(self, inputs, routing_weights):\n    # Compute example dependent depthwise kernels\n    depthwise_kernels = tf.matmul(routing_weights,\n                                  self.depthwise_condconv_kernel)\n    batch_size = inputs.shape[0].value\n    inputs = tf.split(inputs, batch_size, 0)\n    depthwise_kernels = tf.split(depthwise_kernels, batch_size, 0)\n    # Apply example-dependent depthwise convolution to each example in the batch\n    outputs_list = []\n    if self.data_format == \'channels_first\':\n      converted_strides = (1, 1) + self.strides\n    else:\n      converted_strides = (1,) + self.strides + (1,)\n    for input_tensor, depthwise_kernel in zip(inputs, depthwise_kernels):\n      depthwise_kernel = tf.reshape(depthwise_kernel,\n                                    self.depthwise_kernel_shape)\n      outputs_list.append(\n          tf.nn.depthwise_conv2d(\n              input_tensor,\n              depthwise_kernel,\n              strides=converted_strides,\n              padding=self.padding.upper(),\n              dilations=self.dilation_rate,\n              data_format=self.converted_data_format))\n    outputs = tf.concat(outputs_list, 0)\n\n    if self.use_bias:\n      # Compute example-dependent biases\n      biases = tf.matmul(routing_weights, self.condconv_bias)\n      outputs = tf.split(outputs, batch_size, 0)\n      biases = tf.split(biases, batch_size, 0)\n      # Add example-dependent bias to each example in the batch\n      bias_outputs_list = []\n      for output, bias in zip(outputs, biases):\n        bias = tf.squeeze(bias, axis=0)\n        bias_outputs_list.append(\n            tf.nn.bias_add(output, bias,\n                           data_format=self.converted_data_format))\n      outputs = tf.concat(bias_outputs_list, 0)\n\n    if self.activation is not None:\n      return self.activation(outputs)\n\n    return outputs\n\n  def get_config(self):\n    config = {\'num_experts\': self.num_experts}\n    base_config = super(DepthwiseCondConv2D, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n\n\nclass DepthwiseConv2DBatchEnsemble(tf.keras.layers.Layer):\n  """"""Batch ensemble of depthwise separable 2D convolutions.""""""\n\n  def __init__(self,\n               kernel_size,\n               ensemble_size=4,\n               alpha_initializer=\'ones\',\n               gamma_initializer=\'ones\',\n               strides=(1, 1),\n               padding=\'valid\',\n               depth_multiplier=1,\n               data_format=None,\n               activation=None,\n               use_bias=True,\n               depthwise_initializer=\'glorot_uniform\',\n               bias_initializer=\'zeros\',\n               depthwise_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               depthwise_constraint=None,\n               bias_constraint=None,\n               **kwargs):\n    super(DepthwiseConv2DBatchEnsemble, self).__init__(**kwargs)\n    self.ensemble_size = ensemble_size\n    self.alpha_initializer = initializers.get(alpha_initializer)\n    self.gamma_initializer = initializers.get(gamma_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n    self.bias_constraint = constraints.get(bias_constraint)\n    self.activation = tf.keras.activations.get(activation)\n    self.use_bias = use_bias\n    self.conv2d = tf.keras.layers.DepthwiseConv2D(\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        depth_multiplier=depth_multiplier,\n        data_format=data_format,\n        activation=None,\n        use_bias=False,\n        depthwise_initializer=depthwise_initializer,\n        bias_initializer=None,\n        depthwise_regularizer=depthwise_regularizer,\n        bias_regularizer=None,\n        activity_regularizer=activity_regularizer,\n        depthwise_constraint=depthwise_constraint,\n        bias_constraint=None)\n    self.kernel_size = self.conv2d.kernel_size\n    self.depth_multiplier = self.conv2d.depth_multiplier\n    self.data_format = self.conv2d.data_format\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    if self.data_format == \'channels_first\':\n      input_channel = input_shape[1]\n    elif self.data_format == \'channels_last\':\n      input_channel = input_shape[-1]\n\n    filters = input_channel * self.depth_multiplier\n    self.alpha = self.add_weight(\n        \'alpha\',\n        shape=[self.ensemble_size, input_channel],\n        initializer=self.alpha_initializer,\n        trainable=True,\n        dtype=self.dtype)\n    self.gamma = self.add_weight(\n        \'gamma\',\n        shape=[self.ensemble_size, filters],\n        initializer=self.gamma_initializer,\n        trainable=True,\n        dtype=self.dtype)\n    if self.use_bias:\n      self.bias = self.add_weight(\n          name=\'bias\',\n          shape=[self.ensemble_size, filters],\n          initializer=self.bias_initializer,\n          regularizer=self.bias_regularizer,\n          constraint=self.bias_constraint,\n          trainable=True,\n          dtype=self.dtype)\n    else:\n      self.bias = None\n    self.built = True\n\n  def call(self, inputs):\n    axis_change = -1 if self.data_format == \'channels_first\' else 1\n    batch_size = tf.shape(inputs)[0]\n    input_dim = self.alpha.shape[-1]\n    filters = self.gamma.shape[-1]\n    examples_per_model = batch_size // self.ensemble_size\n    alpha = tf.reshape(tf.tile(self.alpha, [1, examples_per_model]),\n                       [batch_size, input_dim])\n    gamma = tf.reshape(tf.tile(self.gamma, [1, examples_per_model]),\n                       [batch_size, filters])\n    alpha = tf.expand_dims(alpha, axis=axis_change)\n    alpha = tf.expand_dims(alpha, axis=axis_change)\n    gamma = tf.expand_dims(gamma, axis=axis_change)\n    gamma = tf.expand_dims(gamma, axis=axis_change)\n    outputs = self.conv2d(inputs*alpha) * gamma\n\n    if self.use_bias:\n      bias = tf.reshape(tf.tile(self.bias, [1, examples_per_model]),\n                        [batch_size, filters])\n      bias = tf.expand_dims(bias, axis=axis_change)\n      bias = tf.expand_dims(bias, axis=axis_change)\n      outputs += bias\n\n    if self.activation is not None:\n      outputs = self.activation(outputs)\n    return outputs\n\n  def compute_output_shape(self, input_shape):\n    return self.conv2d.compute_output_shape(input_shape)\n\n  def get_config(self):\n    config = {\n        \'ensemble_size\': self.ensemble_size,\n        \'alpha_initializer\': initializers.serialize(self.alpha_initializer),\n        \'gamma_initializer\': initializers.serialize(self.gamma_initializer),\n        \'bias_initializer\': initializers.serialize(self.bias_initializer),\n        \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\n        \'bias_constraint\': constraints.serialize(self.bias_constraint),\n        \'activation\': tf.keras.activations.serialize(self.activation),\n        \'use_bias\': self.use_bias,\n    }\n    new_config = super(DepthwiseConv2DBatchEnsemble, self).get_config()\n    new_config.update(self.conv2d.get_config())\n    new_config.update(config)\n    return new_config\n'"
edward2/tensorflow/layers/convolutional_test.py,16,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for Bayesian convolutional layers.""""""\n\nimport functools\nfrom absl.testing import parameterized\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass ConvolutionalTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.parameters(\n      {""layer"": ed.layers.Conv2DFlipout,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": True},\n      {""layer"": ed.layers.Conv2DFlipout,\n       ""kernel_initializer"": ""trainable_normal"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": False},\n      {""layer"": ed.layers.Conv2DFlipout,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""trainable_normal"",\n       ""all_close"": False},\n      {""layer"": ed.layers.Conv2DHierarchical,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": True},\n      {""layer"": ed.layers.Conv2DHierarchical,\n       ""kernel_initializer"": ""trainable_normal"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": False},\n      {""layer"": ed.layers.Conv2DHierarchical,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""trainable_normal"",\n       ""all_close"": False},\n      {""layer"": ed.layers.Conv2DReparameterization,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": True},\n      {""layer"": ed.layers.Conv2DReparameterization,\n       ""kernel_initializer"": ""trainable_normal"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": False},\n      {""layer"": ed.layers.Conv2DReparameterization,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""trainable_normal"",\n       ""all_close"": False},\n      {""layer"": ed.layers.Conv2DVariationalDropout,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": True},\n      {""layer"": ed.layers.Conv2DVariationalDropout,\n       ""kernel_initializer"": ""trainable_normal"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": False},\n      {""layer"": ed.layers.Conv2DVariationalDropout,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""trainable_normal"",\n       ""all_close"": False},\n  )\n  def testConv2DKernel(self,\n                       layer,\n                       kernel_initializer,\n                       bias_initializer,\n                       all_close):\n    tf.keras.backend.set_learning_phase(1)  # training time\n    inputs = np.random.rand(5, 4, 4, 12).astype(np.float32)\n    model = layer(4,\n                  kernel_size=2,\n                  kernel_initializer=kernel_initializer,\n                  bias_initializer=bias_initializer,\n                  activation=""relu"")\n    outputs1 = model(inputs)\n    outputs2 = model(inputs)\n    self.assertEqual(outputs1.shape, (5, 3, 3, 4))\n    self.assertAllGreaterEqual(outputs1, 0.)\n    if all_close:\n      self.assertAllClose(outputs1, outputs2)\n    else:\n      self.assertNotAllClose(outputs1, outputs2)\n    model.get_config()\n\n  @parameterized.parameters(\n      {""layer"": ed.layers.Conv2DFlipout},\n      {""layer"": ed.layers.Conv2DHierarchical},\n      {""layer"": ed.layers.Conv2DReparameterization},\n      {""layer"": ed.layers.Conv2DVariationalDropout},\n  )\n  def testConv2DModel(self, layer):\n    inputs = np.random.rand(3, 4, 4, 1).astype(np.float32)\n    model = tf.keras.Sequential([\n        layer(3, kernel_size=2, padding=""SAME"", activation=""relu""),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(2, activation=None),\n    ])\n    outputs = model(inputs, training=True)\n    self.assertEqual(outputs.shape, (3, 2))\n    if layer == ed.layers.Conv2DHierarchical:\n      self.assertLen(model.losses, 3)\n    else:\n      self.assertLen(model.losses, 1)\n\n  @parameterized.parameters(\n      {""layer"": ed.layers.Conv1DFlipout,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": True},\n      {""layer"": ed.layers.Conv1DFlipout,\n       ""kernel_initializer"": ""trainable_normal"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": False},\n      {""layer"": ed.layers.Conv1DFlipout,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""trainable_normal"",\n       ""all_close"": False},\n      {""layer"": ed.layers.Conv1DReparameterization,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": True},\n      {""layer"": ed.layers.Conv1DReparameterization,\n       ""kernel_initializer"": ""trainable_normal"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": False},\n      {""layer"": ed.layers.Conv1DReparameterization,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""trainable_normal"",\n       ""all_close"": False},\n  )\n  def testConv1DKernel(self, layer, kernel_initializer, bias_initializer,\n                       all_close):\n    tf.keras.backend.set_learning_phase(1)  # training time\n    inputs = np.random.rand(5, 4, 12).astype(np.float32)\n    model = layer(\n        4,\n        kernel_size=2,\n        kernel_initializer=kernel_initializer,\n        bias_initializer=bias_initializer,\n        activation=""relu"")\n    outputs1 = model(inputs)\n    outputs2 = model(inputs)\n    self.assertEqual(outputs1.shape, (5, 3, 4))\n    self.assertAllGreaterEqual(outputs1, 0.)\n    if all_close:\n      self.assertAllClose(outputs1, outputs2)\n    else:\n      self.assertNotAllClose(outputs1, outputs2)\n    model.get_config()\n\n  @parameterized.parameters(\n      {""layer"": ed.layers.Conv1DFlipout},\n      {""layer"": ed.layers.Conv1DReparameterization},\n  )\n  def testConv1DModel(self, layer):\n    inputs = np.random.rand(3, 4, 1).astype(np.float32)\n    model = tf.keras.Sequential([\n        layer(3, kernel_size=2, padding=""SAME"", activation=""relu""),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(2, activation=None),\n    ])\n    outputs = model(inputs, training=True)\n    self.assertEqual(outputs.shape, (3, 2))\n    self.assertLen(model.losses, 1)\n\n  @parameterized.parameters(\n      {""layer_cls"": ed.layers.Conv2DBatchEnsemble},\n      {""layer_cls"": ed.layers.DepthwiseConv2DBatchEnsemble},\n  )\n  def testConv2DBatchEnsemble(self, layer_cls):\n    """"""Tests that vectorized implementation is same as for loop.""""""\n    ensemble_size = 2\n    examples_per_model = 3\n    channels = 5\n    inputs = tf.random.normal([examples_per_model, 4, 4, channels])\n    if layer_cls == ed.layers.Conv2DBatchEnsemble:\n      layer_cls = functools.partial(layer_cls, filters=channels)\n    layer = layer_cls(\n        kernel_size=2,\n        ensemble_size=ensemble_size,\n        activation=None)\n\n    batch_inputs = tf.tile(inputs, [ensemble_size, 1, 1, 1])\n    batch_outputs = layer(batch_inputs)\n    loop_outputs = [\n        layer.conv2d(inputs*layer.alpha[i]) * layer.gamma[i] + layer.bias[i]\n        for i in range(ensemble_size)]\n    loop_outputs = tf.concat(loop_outputs, axis=0)\n\n    expected_shape = (ensemble_size * examples_per_model, 3, 3, channels)\n    self.assertEqual(batch_outputs.shape, expected_shape)\n    self.assertAllClose(batch_outputs, loop_outputs)\n\n  def testConv1DBatchEnsemble(self):\n    """"""Tests that vectorized implementation is same as for loop.""""""\n    ensemble_size = 2\n    examples_per_model = 3\n    channels = 5\n    inputs = tf.random.normal([examples_per_model, 4, channels])\n    layer = ed.layers.Conv1DBatchEnsemble(\n        filters=channels,\n        kernel_size=2,\n        ensemble_size=ensemble_size,\n        activation=None)\n    batch_inputs = tf.tile(inputs, [ensemble_size, 1, 1])\n    batch_outputs = layer(batch_inputs)\n    loop_outputs = [\n        layer.conv1d(inputs*layer.alpha[i]) * layer.gamma[i] + layer.bias[i]\n        for i in range(ensemble_size)]\n    loop_outputs = tf.concat(loop_outputs, axis=0)\n\n    expected_shape = (ensemble_size * examples_per_model, 3, channels)\n    self.assertEqual(batch_outputs.shape, expected_shape)\n    self.assertAllClose(batch_outputs, loop_outputs)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
edward2/tensorflow/layers/dense.py,87,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Uncertainty-based dense layers.""""""\n\nimport math\nfrom edward2.tensorflow import constraints\nfrom edward2.tensorflow import generated_random_variables\nfrom edward2.tensorflow import initializers\nfrom edward2.tensorflow import random_variable\nfrom edward2.tensorflow import regularizers\nfrom edward2.tensorflow.layers import utils\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n\n@utils.add_weight\nclass DenseReparameterization(tf.keras.layers.Dense):\n  """"""Bayesian densely-connected layer estimated via reparameterization.\n\n  The layer computes a variational Bayesian approximation to the distribution\n  over densely-connected layers,\n\n  ```\n  p(outputs | inputs) = int dense(inputs; weights, bias) p(weights, bias)\n    dweights dbias.\n  ```\n\n  It does this with a stochastic forward pass, sampling from learnable\n  distributions on the kernel and bias. Gradients with respect to the\n  distributions\' learnable parameters backpropagate via reparameterization.\n  Minimizing cross-entropy plus the layer\'s losses performs variational\n  minimum description length, i.e., it minimizes an upper bound to the negative\n  marginal likelihood.\n  """"""\n\n  def __init__(self,\n               units,\n               activation=None,\n               use_bias=True,\n               kernel_initializer=\'trainable_normal\',\n               bias_initializer=\'zero\',\n               kernel_regularizer=\'normal_kl_divergence\',\n               bias_regularizer=None,\n               activity_regularizer=None,\n               **kwargs):\n    super(DenseReparameterization, self).__init__(\n        units=units,\n        activation=activation,\n        use_bias=use_bias,\n        kernel_initializer=initializers.get(kernel_initializer),\n        bias_initializer=initializers.get(bias_initializer),\n        kernel_regularizer=regularizers.get(kernel_regularizer),\n        bias_regularizer=regularizers.get(bias_regularizer),\n        activity_regularizer=regularizers.get(activity_regularizer),\n        **kwargs)\n\n  def call_weights(self):\n    """"""Calls any weights if the initializer is itself a layer.""""""\n    if isinstance(self.kernel_initializer, tf.keras.layers.Layer):\n      self.kernel = self.kernel_initializer(self.kernel.shape, self.dtype)\n    if isinstance(self.bias_initializer, tf.keras.layers.Layer):\n      self.bias = self.bias_initializer(self.bias.shape, self.dtype)\n\n  def call(self, *args, **kwargs):\n    self.call_weights()\n    kwargs.pop(\'training\', None)\n    return super(DenseReparameterization, self).call(*args, **kwargs)\n\n\nclass DenseDVI(DenseReparameterization):\n  """"""Densely-connected layer with deterministic VI (Wu et al., 2018).\n\n  This layer computes a variational inference approximation via first and second\n  moments. It is accurate if the kernel and bias initializers return factorized\n  normal random variables and the number of units is sufficiently large. The\n  advantage is that the forward pass is deterministic, reducing variance of\n  gradients during training. The disadvantage is an O(features^2*units) compute\n  and O(features^2 + features*units) memory complexity. In comparison,\n  DenseReparameterization has O(features*units) compute and memory complexity.\n\n  #### Examples\n\n  Below implements deterministic variational inference for Bayesian\n  feedforward network regression. We use the exact expected log-likelihood from\n  Wu et al. (2018), Eq. 8. Assume 2-D real-valued tensors of `features` and\n  `labels` of shapes `[batch_size, num_features]` and `[batch_size, 1]`\n  respectively.\n\n  ```python\n  model = tf.keras.Sequential([\n      ed.layers.DenseDVI(256, activation=tf.nn.relu),\n      ed.layers.DenseDVI(256, activation=tf.nn.relu),\n      ed.layers.DenseDVI(1, activation=None),\n  ])\n\n  # Run training loop.\n  num_steps = 1000\n  for _ in range(num_steps):\n    with tf.GradientTape() as tape:\n      locs = model(features)\n      nll = 0.5 * tf.reduce_mean(locs.distribution.variance() +\n                                 (labels - locs.distribution.mean())**2)\n      kl = sum(model.losses) / total_dataset_size\n      loss = nll + kl\n    gradients = tape.gradient(loss, model.variables)  # use any optimizer here\n  ```\n\n  For evaluation, feed in data and use, e.g., `predictions.distribution.mean()`\n  to make predictions via the posterior predictive distribution.\n\n  ```python\n  predictions = ed.Normal(loc=locs.distribution.mean(),\n                          scale=locs.distribution.variance() + 1.)\n  ```\n  """"""\n\n  def call(self, inputs):\n    if (not isinstance(inputs, random_variable.RandomVariable) and\n        not isinstance(self.kernel, random_variable.RandomVariable) and\n        not isinstance(self.bias, random_variable.RandomVariable)):\n      return super(DenseDVI, self).call(inputs)\n    self.call_weights()\n    inputs_mean, inputs_variance, inputs_covariance = get_moments(inputs)\n    kernel_mean, kernel_variance, _ = get_moments(self.kernel)\n    if self.use_bias:\n      bias_mean, _, bias_covariance = get_moments(self.bias)\n\n    # E[outputs] = E[inputs] * E[kernel] + E[bias]\n    mean = tf.tensordot(inputs_mean, kernel_mean, [[-1], [0]])\n    if self.use_bias:\n      mean = tf.nn.bias_add(mean, bias_mean)\n\n    # Cov = E[inputs**2] Cov(kernel) + E[W]^T Cov(inputs) E[W] + Cov(bias)\n    # For first term, assume Cov(kernel) = 0 on off-diagonals so we only\n    # compute diagonal term.\n    covariance_diag = tf.tensordot(inputs_variance + inputs_mean**2,\n                                   kernel_variance, [[-1], [0]])\n    # Compute quadratic form E[W]^T Cov E[W] from right-to-left. First is\n    #  [..., features, features], [features, units] -> [..., features, units].\n    cov_w = tf.tensordot(inputs_covariance, kernel_mean, [[-1], [0]])\n    # Next is [..., features, units], [features, units] -> [..., units, units].\n    w_cov_w = tf.tensordot(cov_w, kernel_mean, [[-2], [0]])\n    covariance = w_cov_w\n    if self.use_bias:\n      covariance += bias_covariance\n    covariance = tf.linalg.set_diag(\n        covariance, tf.linalg.diag_part(covariance) + covariance_diag)\n\n    if self.activation in (tf.keras.activations.relu, tf.nn.relu):\n      # Compute activation\'s moments with variable names from Wu et al. (2018).\n      variance = tf.linalg.diag_part(covariance)\n      scale = tf.sqrt(variance)\n      mu = mean / (scale + tf.keras.backend.epsilon())\n      mean = scale * soft_relu(mu)\n\n      pairwise_variances = (tf.expand_dims(variance, -1) *\n                            tf.expand_dims(variance, -2))  # [..., units, units]\n      rho = covariance / tf.sqrt(pairwise_variances +\n                                 tf.keras.backend.epsilon())\n      rho = tf.clip_by_value(rho,\n                             -1. / (1. + tf.keras.backend.epsilon()),\n                             1. / (1. + tf.keras.backend.epsilon()))\n      s = covariance / (rho + tf.keras.backend.epsilon())\n      mu1 = tf.expand_dims(mu, -1)  # [..., units, 1]\n      mu2 = tf.linalg.matrix_transpose(mu1)  # [..., 1, units]\n      a = (soft_relu(mu1) * soft_relu(mu2) +\n           rho * tfp.distributions.Normal(0., 1.).cdf(mu1) *\n           tfp.distributions.Normal(0., 1.).cdf(mu2))\n      gh = tf.asinh(rho)\n      bar_rho = tf.sqrt(1. - rho**2)\n      gr = gh + rho / (1. + bar_rho)\n      # Include numerically stable versions of gr and rho when multiplying or\n      # dividing them. The sign of gr*rho and rho/gr is always positive.\n      safe_gr = tf.abs(gr) + 0.5 * tf.keras.backend.epsilon()\n      safe_rho = tf.abs(rho) + tf.keras.backend.epsilon()\n      exp_negative_q = gr / (2. * math.pi) * tf.exp(\n          -safe_rho / (2. * safe_gr * (1 + bar_rho)) +\n          (gh - rho) / (safe_gr * safe_rho) * mu1 * mu2)\n      covariance = s * (a + exp_negative_q)\n    elif self.activation not in (tf.keras.activations.linear, None):\n      raise NotImplementedError(\'Activation is {}. Deterministic variational \'\n                                \'inference is only available if activation is \'\n                                \'ReLU or None.\'.format(self.activation))\n\n    return generated_random_variables.MultivariateNormalFullCovariance(\n        mean, covariance)\n\n\ndef get_moments(x):\n  """"""Gets first and second moments of input.""""""\n  if isinstance(x, random_variable.RandomVariable):\n    mean = x.distribution.mean()\n    variance = x.distribution.variance()\n    try:\n      covariance = x.distribution.covariance()\n    except NotImplementedError:\n      covariance = tf.zeros(x.shape.concatenate(x.shape[-1]), dtype=x.dtype)\n      covariance = tf.linalg.set_diag(covariance, variance)\n  else:\n    mean = x\n    variance = tf.zeros_like(x)\n    covariance = tf.zeros(x.shape.concatenate(x.shape[-1]), dtype=x.dtype)\n  return mean, variance, covariance\n\n\ndef soft_relu(x):\n  return (tfp.distributions.Normal(0., 1.).prob(x) +\n          x * tfp.distributions.Normal(0., 1.).cdf(x))\n\n\nclass DenseFlipout(DenseReparameterization):\n  """"""Bayesian densely-connected layer estimated via Flipout (Wen et al., 2018).\n\n  The layer computes a variational Bayesian approximation to the distribution\n  over densely-connected layers,\n\n  ```\n  p(outputs | inputs) = int dense(inputs; weights, bias) p(weights, bias)\n    dweights dbias.\n  ```\n\n  It does this with a stochastic forward pass, sampling from learnable\n  distributions on the kernel and bias. Gradients with respect to the\n  distributions\' learnable parameters backpropagate via reparameterization.\n  Minimizing cross-entropy plus the layer\'s losses performs variational\n  minimum description length, i.e., it minimizes an upper bound to the negative\n  marginal likelihood.\n\n  This layer uses the Flipout estimator (Wen et al., 2018) for integrating with\n  respect to the `kernel`. Namely, it applies\n  pseudo-independent weight perturbations via independent sign flips for each\n  example, enabling variance reduction over independent weight perturbations.\n  For this estimator to work, the `kernel` random variable must be able\n  to decompose as a sum of its mean and a perturbation distribution; the\n  perturbation distribution must be independent across weight elements and\n  symmetric around zero (for example, a fully factorized Gaussian).\n  """"""\n\n  def call(self, inputs):\n    if not isinstance(self.kernel, random_variable.RandomVariable):\n      return super(DenseFlipout, self).call(inputs)\n    self.call_weights()\n    input_shape = tf.shape(inputs)\n    output_shape = tf.concat([input_shape[:-1], [self.units]], 0)\n    sign_input = tf.cast(2 * tf.random.uniform(input_shape,\n                                               minval=0,\n                                               maxval=2,\n                                               dtype=tf.int32) - 1,\n                         inputs.dtype)\n    sign_output = tf.cast(2 * tf.random.uniform(output_shape,\n                                                minval=0,\n                                                maxval=2,\n                                                dtype=inputs.dtype) - 1,\n                          inputs.dtype)\n    kernel_mean = self.kernel.distribution.mean()\n    perturbation = self.kernel - kernel_mean\n    if inputs.shape.ndims <= 2:\n      outputs = tf.matmul(inputs, kernel_mean)\n      outputs += tf.matmul(inputs * sign_input, perturbation) * sign_output\n    else:\n      outputs = tf.tensordot(inputs, kernel_mean, [[-1], [0]])\n      outputs += tf.tensordot(inputs * sign_input,\n                              perturbation,\n                              [[-1], [0]]) * sign_output\n    if self.use_bias:\n      outputs = tf.nn.bias_add(outputs, self.bias)\n    if self.activation is not None:\n      outputs = self.activation(outputs)\n    return outputs\n\n\nclass DenseVariationalDropout(DenseReparameterization):\n  """"""Densely-connected layer with variational dropout (Kingma et al., 2015).\n\n  Implementation follows the additive parameterization of\n  Molchanov et al. (2017).\n  """"""\n\n  def __init__(self,\n               units,\n               activation=None,\n               use_bias=True,\n               kernel_initializer=\'trainable_normal\',\n               bias_initializer=\'zero\',\n               kernel_regularizer=\'log_uniform_kl_divergence\',\n               bias_regularizer=None,\n               activity_regularizer=None,\n               **kwargs):\n    super(DenseVariationalDropout, self).__init__(\n        units=units,\n        activation=activation,\n        use_bias=use_bias,\n        kernel_initializer=initializers.get(kernel_initializer),\n        bias_initializer=initializers.get(bias_initializer),\n        kernel_regularizer=regularizers.get(kernel_regularizer),\n        bias_regularizer=regularizers.get(bias_regularizer),\n        activity_regularizer=regularizers.get(activity_regularizer),\n        **kwargs)\n\n  def call(self, inputs, training=None):\n    if not isinstance(self.kernel, random_variable.RandomVariable):\n      return super(DenseVariationalDropout, self).call(inputs)\n    self.call_weights()\n    if training is None:\n      training = tf.keras.backend.learning_phase()\n\n    def dropped_inputs():\n      """"""Forward pass with dropout.""""""\n      # Clip magnitude of dropout rate, where we get the dropout rate alpha from\n      # the additive parameterization (Molchanov et al., 2017): for weight ~\n      # Normal(mu, sigma**2), the variance `sigma**2 = alpha * mu**2`.\n      mean = self.kernel.distribution.mean()\n      log_variance = tf.math.log(self.kernel.distribution.variance())\n      log_alpha = log_variance - tf.math.log(tf.square(mean) +\n                                             tf.keras.backend.epsilon())\n      log_alpha = tf.clip_by_value(log_alpha, -8., 8.)\n      log_variance = log_alpha + tf.math.log(tf.square(mean) +\n                                             tf.keras.backend.epsilon())\n\n      if inputs.shape.ndims <= 2:\n        means = tf.matmul(inputs, mean)\n        stddevs = tf.sqrt(\n            tf.matmul(tf.square(inputs), tf.exp(log_variance)) +\n            tf.keras.backend.epsilon())\n      else:\n        means = tf.tensordot(inputs, mean, [[-1], [0]])\n        stddevs = tf.sqrt(\n            tf.tensordot(tf.square(inputs), tf.exp(log_variance), [[-1], [0]]) +\n            tf.keras.backend.epsilon())\n      if self.use_bias:\n        means = tf.nn.bias_add(means, self.bias)\n      outputs = generated_random_variables.Normal(loc=means, scale=stddevs)\n      if self.activation is not None:\n        outputs = self.activation(outputs)\n      return outputs\n\n    # Following tf.keras.Dropout, only apply variational dropout if training\n    # flag is True.\n    training_value = utils.smart_constant_value(training)\n    if training_value is not None:\n      if training_value:\n        return dropped_inputs()\n      else:\n        return super(DenseVariationalDropout, self).call(inputs)\n    return tf.cond(\n        pred=training,\n        true_fn=dropped_inputs,\n        false_fn=lambda: super(DenseVariationalDropout, self).call(inputs))\n\n\nclass DenseHierarchical(DenseVariationalDropout):\n  """"""Bayesian densely-connected layer with hierarchical distributions.\n\n  The layer computes a variational Bayesian approximation to the distribution\n  over densely-connected layers, and where the distribution over weights\n  involves a hierarchical distribution with hidden unit noise coupling vectors\n  of the kernel weight matrix (Louizos et al., 2017),\n\n  ```\n  p(outputs | inputs) = int dense(inputs; new_kernel, bias) p(kernel,\n    local_scales, global_scale, bias) dkernel dlocal_scales dglobal_scale dbias.\n  ```\n\n  It does this with a stochastic forward pass, sampling from learnable\n  distributions on the kernel and bias. The kernel is written in non-centered\n  parameterization where\n\n  ```\n  new_kernel[i, j] = kernel[i, j] * local_scale[i] * global_scale.\n  ```\n\n  That is, there is ""local"" multiplicative noise which couples weights for each\n  input neuron. There is also a ""global"" multiplicative noise which couples the\n  entire weight matrix. By default, the weights are normally distributed and the\n  local and global noises are half-Cauchy distributed; this makes the kernel a\n  horseshoe distribution (Carvalho et al., 2009; Polson and Scott, 2012).\n\n  The estimation uses local reparameterization to avoid sampling the full\n  weights. Gradients with respect to the distributions\' learnable parameters\n  backpropagate via reparameterization. Minimizing cross-entropy plus the\n  layer\'s losses performs variational minimum description length, i.e., it\n  minimizes an upper bound to the negative marginal likelihood.\n  """"""\n\n  def __init__(self,\n               units,\n               activation=None,\n               use_bias=True,\n               kernel_initializer=\'trainable_normal\',\n               bias_initializer=\'zero\',\n               local_scale_initializer=\'trainable_half_cauchy\',\n               global_scale_initializer=\'trainable_half_cauchy\',\n               kernel_regularizer=\'normal_kl_divergence\',\n               bias_regularizer=None,\n               local_scale_regularizer=\'half_cauchy_kl_divergence\',\n               global_scale_regularizer=regularizers.HalfCauchyKLDivergence(\n                   scale=1e-5),\n               activity_regularizer=None,\n               local_scale_constraint=\'softplus\',\n               global_scale_constraint=\'softplus\',\n               **kwargs):\n    self.local_scale_initializer = initializers.get(local_scale_initializer)\n    self.global_scale_initializer = initializers.get(global_scale_initializer)\n    self.local_scale_regularizer = regularizers.get(local_scale_regularizer)\n    self.global_scale_regularizer = regularizers.get(global_scale_regularizer)\n    self.local_scale_constraint = constraints.get(local_scale_constraint)\n    self.global_scale_constraint = constraints.get(global_scale_constraint)\n    super(DenseHierarchical, self).__init__(\n        units=units,\n        activation=activation,\n        use_bias=use_bias,\n        kernel_initializer=initializers.get(kernel_initializer),\n        bias_initializer=initializers.get(bias_initializer),\n        kernel_regularizer=regularizers.get(kernel_regularizer),\n        bias_regularizer=regularizers.get(bias_regularizer),\n        activity_regularizer=regularizers.get(activity_regularizer),\n        **kwargs)\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    input_dim = input_shape[-1]\n    self.local_scale = self.add_weight(\n        shape=(input_dim,),\n        name=\'local_scale\',\n        initializer=self.local_scale_initializer,\n        regularizer=self.local_scale_regularizer,\n        constraint=self.local_scale_constraint)\n    self.global_scale = self.add_weight(\n        shape=(),\n        name=\'global_scale\',\n        initializer=self.global_scale_initializer,\n        regularizer=self.global_scale_regularizer,\n        constraint=self.global_scale_constraint)\n    super(DenseHierarchical, self).build(input_shape)\n\n  def call_weights(self):\n    """"""Calls any weights if the initializer is itself a layer.""""""\n    if isinstance(self.local_scale_initializer, tf.keras.layers.Layer):\n      self.local_scale = self.local_scale_initializer(self.local_scale.shape,\n                                                      self.dtype)\n    if isinstance(self.global_scale_initializer, tf.keras.layers.Layer):\n      self.global_scale = self.global_scale_initializer(self.global_scale.shape,\n                                                        self.dtype)\n    super(DenseHierarchical, self).call_weights()\n\n  def call(self, inputs, training=None):\n    self.call_weights()\n    # TODO(trandustin): Figure out what to set local/global scales to at test\n    # time. Means don\'t exist for Half-Cauchy approximate posteriors.\n    inputs *= self.local_scale[tf.newaxis, :] * self.global_scale\n    return super(DenseHierarchical, self).call(inputs, training=training)\n\n\nclass DenseBatchEnsemble(tf.keras.layers.Layer):\n  """"""A batch ensemble dense layer.""""""\n\n  def __init__(self,\n               units,\n               rank=1,\n               ensemble_size=4,\n               activation=None,\n               use_bias=True,\n               alpha_initializer=\'ones\',\n               gamma_initializer=\'ones\',\n               kernel_initializer=\'glorot_uniform\',\n               bias_initializer=\'zeros\',\n               kernel_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               **kwargs):\n    super(DenseBatchEnsemble, self).__init__(**kwargs)\n    self.rank = rank\n    self.ensemble_size = ensemble_size\n    self.activation = tf.keras.activations.get(activation)\n    self.use_bias = use_bias\n    self.alpha_initializer = initializers.get(alpha_initializer)\n    self.gamma_initializer = initializers.get(gamma_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n    self.bias_constraint = constraints.get(bias_constraint)\n    self.dense = tf.keras.layers.Dense(\n        units=units,\n        use_bias=False,\n        activation=None,\n        kernel_initializer=kernel_initializer,\n        bias_initializer=None,\n        kernel_regularizer=kernel_regularizer,\n        bias_regularizer=None,\n        activity_regularizer=activity_regularizer,\n        kernel_constraint=kernel_constraint,\n        bias_constraint=None)\n    self.units = self.dense.units\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    input_dim = input_shape[-1]\n    if self.rank > 1:\n      alpha_shape = [self.rank, self.ensemble_size, input_dim]\n      gamma_shape = [self.rank, self.ensemble_size, self.units]\n    else:\n      alpha_shape = [self.ensemble_size, input_dim]\n      gamma_shape = [self.ensemble_size, self.units]\n\n    self.alpha = self.add_weight(\n        name=\'alpha\',\n        shape=alpha_shape,\n        initializer=self.alpha_initializer,\n        trainable=True,\n        dtype=self.dtype)\n    self.gamma = self.add_weight(\n        name=\'gamma\',\n        shape=gamma_shape,\n        initializer=self.gamma_initializer,\n        trainable=True,\n        dtype=self.dtype)\n    if self.use_bias:\n      self.bias = self.add_weight(\n          name=\'bias\',\n          shape=[self.ensemble_size, self.units],\n          initializer=self.bias_initializer,\n          regularizer=self.bias_regularizer,\n          constraint=self.bias_constraint,\n          trainable=True,\n          dtype=self.dtype)\n    else:\n      self.bias = None\n    self.built = True\n\n  def call(self, inputs):\n    batch_size = tf.shape(inputs)[0]\n    input_dim = self.alpha.shape[-1]\n    examples_per_model = batch_size // self.ensemble_size\n\n    # TODO(ywenxu): Merge the following two cases.\n    if self.rank > 1:\n      input_dim = self.alpha.shape[-1]\n      alpha = tf.reshape(tf.tile(self.alpha, [1, 1, examples_per_model]),\n                         [self.rank, batch_size, input_dim])\n      gamma = tf.reshape(tf.tile(self.gamma, [1, 1, examples_per_model]),\n                         [self.rank, batch_size, self.units])\n      perturb_inputs = tf.expand_dims(inputs, 0) * alpha\n      outputs = self.dense(perturb_inputs)\n      outputs = tf.reduce_sum(outputs * gamma, axis=0)\n      outputs = tf.reshape(\n          outputs, [self.ensemble_size, examples_per_model, -1])\n    else:\n      inputs = tf.reshape(\n          inputs, [self.ensemble_size, examples_per_model, input_dim])\n      alpha = tf.expand_dims(self.alpha, 1)\n      gamma = tf.expand_dims(self.gamma, 1)\n      outputs = self.dense(inputs * alpha) * gamma\n\n    if self.use_bias:\n      bias = tf.expand_dims(self.bias, 1)\n      outputs += bias\n\n    if self.activation is not None:\n      outputs = self.activation(outputs)\n    outputs = tf.reshape(outputs, [batch_size, self.units])\n    return outputs\n\n  def compute_output_shape(self, input_shape):\n    return self.dense.compute_output_shape(input_shape)\n\n  def get_config(self):\n    config = {\n        \'ensemble_size\': self.ensemble_size,\n        \'activation\': tf.keras.activations.serialize(self.activation),\n        \'use_bias\': self.use_bias,\n        \'alpha_initializer\': initializers.serialize(self.alpha_initializer),\n        \'gamma_initializer\': initializers.serialize(self.gamma_initializer),\n        \'bias_initializer\': initializers.serialize(self.bias_initializer),\n        \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\n        \'bias_constraint\': constraints.serialize(self.bias_constraint),\n    }\n    new_config = super(DenseBatchEnsemble, self).get_config()\n    new_config.update(self.dense.get_config())\n    new_config.update(config)\n    return new_config\n'"
edward2/tensorflow/layers/dense_test.py,39,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for Bayesian dense layers.""""""\n\nfrom absl.testing import parameterized\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass DenseTest(parameterized.TestCase, tf.test.TestCase):\n\n  def testTrainableNormalStddevConstraint(self):\n    layer = ed.layers.DenseReparameterization(\n        100, kernel_initializer=""trainable_normal"")\n    inputs = tf.random.normal([1, 1])\n    _ = layer(inputs)\n    stddev = layer.kernel.distribution.stddev()\n    self.assertAllGreater(stddev, 0.)\n\n  @parameterized.parameters(\n      {""layer"": ed.layers.DenseDVI,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": True},\n      {""layer"": ed.layers.DenseDVI,\n       ""kernel_initializer"": ""trainable_normal"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": False},\n      {""layer"": ed.layers.DenseDVI,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""trainable_normal"",\n       ""all_close"": False},\n      {""layer"": ed.layers.DenseFlipout,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": True},\n      {""layer"": ed.layers.DenseFlipout,\n       ""kernel_initializer"": ""trainable_normal"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": False},\n      {""layer"": ed.layers.DenseFlipout,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""trainable_normal"",\n       ""all_close"": False},\n      {""layer"": ed.layers.DenseReparameterization,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": True},\n      {""layer"": ed.layers.DenseReparameterization,\n       ""kernel_initializer"": ""trainable_normal"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": False},\n      {""layer"": ed.layers.DenseReparameterization,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""trainable_normal"",\n       ""all_close"": False},\n      {""layer"": ed.layers.DenseVariationalDropout,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": True},\n      {""layer"": ed.layers.DenseVariationalDropout,\n       ""kernel_initializer"": ""trainable_normal"",\n       ""bias_initializer"": ""zeros"",\n       ""all_close"": False},\n      {""layer"": ed.layers.DenseVariationalDropout,\n       ""kernel_initializer"": ""zeros"",\n       ""bias_initializer"": ""trainable_normal"",\n       ""all_close"": False},\n  )\n  def testDenseKernel(self,\n                      layer,\n                      kernel_initializer,\n                      bias_initializer,\n                      all_close):\n    tf.keras.backend.set_learning_phase(1)  # training time\n    inputs = np.random.rand(5, 3, 12).astype(np.float32)\n    model = layer(4,\n                  kernel_initializer=kernel_initializer,\n                  bias_initializer=bias_initializer,\n                  activation=tf.nn.relu)\n    outputs1 = tf.convert_to_tensor(model(inputs))\n    outputs2 = tf.convert_to_tensor(model(inputs))\n    self.assertEqual(outputs1.shape, (5, 3, 4))\n    if layer != ed.layers.DenseDVI:\n      self.assertAllGreaterEqual(outputs1, 0.)\n    if all_close:\n      self.assertAllClose(outputs1, outputs2)\n    else:\n      self.assertNotAllClose(outputs1, outputs2)\n    model.get_config()\n\n  @parameterized.parameters(\n      {""layer"": ed.layers.DenseDVI},\n      {""layer"": ed.layers.DenseFlipout},\n      {""layer"": ed.layers.DenseReparameterization},\n      {""layer"": ed.layers.DenseVariationalDropout},\n  )\n  def testDenseMean(self, layer):\n    """"""Tests that forward pass can use other values, e.g., posterior mean.""""""\n    tf.keras.backend.set_learning_phase(0)  # test time\n    def take_mean(f, *args, **kwargs):\n      """"""Sets random variable value to its mean.""""""\n      rv = f(*args, **kwargs)\n      rv._value = rv.distribution.mean()\n      return rv\n    inputs = np.random.rand(5, 3, 7).astype(np.float32)\n    model = layer(4, activation=tf.nn.relu, use_bias=False)\n    outputs1 = tf.convert_to_tensor(model(inputs))\n    with ed.trace(take_mean):\n      outputs2 = tf.convert_to_tensor(model(inputs))\n    self.assertEqual(outputs1.shape, (5, 3, 4))\n    self.assertNotAllClose(outputs1, outputs2)\n    if layer != ed.layers.DenseDVI:\n      self.assertAllClose(outputs2, np.zeros((5, 3, 4)), atol=1e-4)\n\n  @parameterized.parameters(\n      {""layer"": ed.layers.DenseDVI},\n      {""layer"": ed.layers.DenseFlipout},\n      {""layer"": ed.layers.DenseReparameterization},\n      {""layer"": ed.layers.DenseVariationalDropout},\n      {""layer"": ed.layers.DenseHierarchical},\n  )\n  def testDenseLoss(self, layer):\n    tf.keras.backend.set_learning_phase(1)  # training time\n    features = np.random.rand(5, 12).astype(np.float32)\n    labels = np.random.rand(5, 10).astype(np.float32)\n    model = layer(10)\n\n    # Imagine this is the 1st epoch.\n    with tf.GradientTape(persistent=True) as tape:\n      predictions = model(features)  # first call forces build\n      model(features)  # ensure robustness after multiple calls\n      nll = tf.keras.losses.mean_squared_error(labels, predictions)\n      kl = sum(model.losses)\n\n    variables = [model.kernel_initializer.mean, model.kernel_initializer.stddev]\n    for v in variables:\n      # Note in TF 2.0, checking membership (v in model.weights) raises an error\n      # for lists of differently shaped Tensors.\n      self.assertTrue(any(v is weight for weight in model.weights))\n\n    # This will be fine, since the layer was built inside this tape, and thus\n    # the distribution init ops were inside this tape.\n    grads = tape.gradient(nll, variables)\n    for grad in grads:\n      self.assertIsNotNone(grad)\n    grads = tape.gradient(kl, variables)\n    for grad in grads:\n      self.assertIsNotNone(grad)\n\n    # Imagine this is the 2nd epoch.\n    with tf.GradientTape(persistent=True) as tape:\n      predictions = model(features)  # build is not called\n      nll = tf.keras.losses.mean_squared_error(labels, predictions)\n      kl = sum(model.losses)\n\n    variables = [model.kernel_initializer.mean, model.kernel_initializer.stddev]\n    for v in variables:\n      # Note in TF 2.0, checking membership (v in model.weights) raises an error\n      # for lists of differently shaped Tensors.\n      self.assertTrue(any(v is weight for weight in model.weights))\n\n    # This would fail, since the layer was built inside the tape from the 1st\n    # epoch, and thus the distribution init ops were inside that tape instead of\n    # this tape. By using a callable for the variable, this will no longer fail.\n    grads = tape.gradient(nll, variables)\n    for grad in grads:\n      self.assertIsNotNone(grad)\n    grads = tape.gradient(kl, variables)\n    for grad in grads:\n      self.assertIsNotNone(grad)\n\n  @parameterized.parameters(\n      {""layer"": ed.layers.DenseDVI},\n      {""layer"": ed.layers.DenseFlipout},\n      {""layer"": ed.layers.DenseReparameterization},\n      {""layer"": ed.layers.DenseVariationalDropout},\n      {""layer"": ed.layers.DenseHierarchical},\n  )\n  def testDenseModel(self, layer):\n    inputs = np.random.rand(3, 4, 4, 1).astype(np.float32)\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(3,\n                               kernel_size=2,\n                               padding=""SAME"",\n                               activation=tf.nn.relu),\n        tf.keras.layers.Flatten(),\n        layer(2, activation=None),\n    ])\n    outputs = model(inputs, training=True)\n    self.assertEqual(outputs.shape, (3, 2))\n    if layer == ed.layers.DenseHierarchical:\n      self.assertLen(model.losses, 3)\n    else:\n      self.assertLen(model.losses, 1)\n\n  @parameterized.parameters(\n      {""layer"": ed.layers.DenseDVI},\n      {""layer"": ed.layers.DenseFlipout},\n      {""layer"": ed.layers.DenseReparameterization},\n      {""layer"": ed.layers.DenseVariationalDropout},\n      {""layer"": ed.layers.DenseHierarchical},\n  )\n  def testDenseSubclass(self, layer):\n    class DenseSubclass(layer):\n      pass\n\n    inputs = np.random.rand(3, 4, 4, 1).astype(np.float32)\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(3,\n                               kernel_size=2,\n                               padding=""SAME"",\n                               activation=tf.nn.relu),\n        tf.keras.layers.Flatten(),\n        DenseSubclass(2, activation=None),\n    ])\n    outputs = model(inputs, training=True)\n    self.assertEqual(outputs.shape, (3, 2))\n    if layer == ed.layers.DenseHierarchical:\n      self.assertLen(model.losses, 3)\n    else:\n      self.assertLen(model.losses, 1)\n\n  def testDenseDVIIsDeterministic(self):\n    """"""Tests that DenseDVI network has a deterministic loss function.""""""\n    features = np.random.rand(3, 2).astype(np.float32)\n    labels = np.random.rand(3, 1).astype(np.float32)\n    model = tf.keras.Sequential([\n        ed.layers.DenseDVI(5, activation=tf.nn.relu),\n        ed.layers.DenseDVI(1, activation=None),\n    ])\n    def loss_fn(features, labels):\n      outputs = model(features, training=True)\n      nll = -tf.reduce_sum(outputs.distribution.log_prob(labels))\n      kl = sum(model.losses)\n      return nll + kl\n    self.assertEqual(loss_fn(features, labels), loss_fn(features, labels))\n\n  def testDenseDVIMoments(self):\n    """"""Verifies DenseDVI\'s moments empirically with samples.""""""\n    tf.random.set_seed(377269)\n    batch_size = 3\n    num_features = 5\n    units = 128\n    num_samples = 50000\n    inputs = tf.cast(np.random.rand(batch_size, num_features), dtype=tf.float32)\n    layer = ed.layers.DenseDVI(units, activation=tf.nn.relu)\n\n    outputs1 = layer(inputs)\n    mean1 = outputs1.distribution.mean()\n    covariance1 = outputs1.distribution.covariance()\n\n    kernel_samples = layer.kernel.distribution.sample(num_samples)\n    outputs2 = layer.activation(\n        tf.einsum(""bd,sdu->sbu"", inputs, kernel_samples) +\n        tf.reshape(layer.bias, [1, 1, units]))\n    mean2 = tf.reduce_mean(outputs2, axis=0)\n    centered_outputs2 = tf.transpose(a=outputs2 - mean2, perm=[1, 2, 0])\n    covariance2 = tf.matmul(centered_outputs2,\n                            centered_outputs2,\n                            transpose_b=True) / float(num_samples)\n\n    # Check % of mismatches is not too high according to heuristic thresholds.\n    num_mismatches = np.sum(np.abs(mean1 - mean2) > 5e-3)\n    percent_mismatches = num_mismatches / float(batch_size * units)\n    self.assertLessEqual(percent_mismatches, 0.05)\n    num_mismatches = np.sum(np.abs(covariance1 - covariance2) > 5e-3)\n    percent_mismatches = num_mismatches / float(batch_size * units * units)\n    self.assertLessEqual(percent_mismatches, 0.05)\n\n  def testDenseBatchEnsemble(self):\n    """"""Tests that vectorized implementation is same as for loop.""""""\n    tf.keras.backend.set_learning_phase(1)  # training time\n    ensemble_size = 3\n    examples_per_model = 4\n    input_dim = 5\n    output_dim = 5\n    inputs = tf.random.normal([examples_per_model, input_dim])\n    layer = ed.layers.DenseBatchEnsemble(\n        output_dim,\n        alpha_initializer=""he_normal"",\n        gamma_initializer=""he_normal"",\n        activation=None,\n        ensemble_size=ensemble_size)\n\n    batch_inputs = tf.tile(inputs, [ensemble_size, 1])\n    batch_outputs = layer(batch_inputs)\n    loop_outputs = [\n        layer.dense(inputs*layer.alpha[i]) * layer.gamma[i] + layer.bias[i]\n        for i in range(ensemble_size)]\n    loop_outputs = tf.concat(loop_outputs, axis=0)\n\n    expected_shape = (ensemble_size * examples_per_model, output_dim)\n    self.assertEqual(batch_outputs.shape, expected_shape)\n    self.assertAllClose(batch_outputs, loop_outputs)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
edward2/tensorflow/layers/discrete_flows.py,72,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Reversible layers.""""""\n\nfrom edward2.tensorflow import random_variable\nfrom edward2.tensorflow import transformed_random_variable\nfrom edward2.tensorflow.layers import utils\nimport tensorflow as tf\n\n\n# TODO(trandustin): Move Reverse to another module(?).\nclass Reverse(tf.keras.layers.Layer):\n  """"""Swaps the forward and reverse transformations of a layer.""""""\n\n  def __init__(self, reversible_layer, **kwargs):\n    super(Reverse, self).__init__(**kwargs)\n    if not hasattr(reversible_layer, \'reverse\'):\n      raise ValueError(\'Layer passed-in has not implemented ""reverse"" method: \'\n                       \'{}\'.format(reversible_layer))\n    self.call = reversible_layer.reverse\n    self.reverse = reversible_layer.call\n\n\nclass DiscreteAutoregressiveFlow(tf.keras.layers.Layer):\n  """"""A discrete reversible layer.\n\n  The flow takes as input a one-hot Tensor of shape `[..., length, vocab_size]`.\n  The flow returns a Tensor of same shape and dtype. (To enable gradients, the\n  input must have float dtype.)\n\n  For the forward pass, the flow computes in serial:\n\n  ```none\n  outputs = []\n  for t in range(length):\n    new_inputs = [outputs, inputs[..., t, :]]\n    net = layer(new_inputs)\n    loc, scale = tf.split(net, 2, axis=-1)\n    loc = tf.argmax(loc, axis=-1)\n    scale = tf.argmax(scale, axis=-1)\n    new_outputs = (((inputs - loc) * inverse(scale)) % vocab_size)[..., -1, :]\n    outputs.append(new_outputs)\n  ```\n\n  For the reverse pass, the flow computes in parallel:\n\n  ```none\n  net = layer(inputs)\n  loc, scale = tf.split(net, 2, axis=-1)\n  loc = tf.argmax(loc, axis=-1)\n  scale = tf.argmax(scale, axis=-1)\n  outputs = (loc + scale * inputs) % vocab_size\n  ```\n\n  The modular arithmetic happens in one-hot space.\n\n  If `x` is a discrete random variable, the induced probability mass function on\n  the outputs `y = flow(x)` is\n\n  ```none\n  p(y) = p(flow.reverse(y)).\n  ```\n\n  The location-only transform is always invertible ([integers modulo\n  `vocab_size` form an additive group](\n  https://en.wikipedia.org/wiki/Modular_arithmetic)). The transform with a scale\n  is invertible if the scale and `vocab_size` are coprime (see\n  [prime fields](https://en.wikipedia.org/wiki/Finite_field)).\n  """"""\n\n  def __init__(self, layer, temperature, **kwargs):\n    """"""Constructs flow.\n\n    Args:\n      layer: Two-headed masked network taking the inputs and returning a\n        real-valued Tensor of shape `[..., length, 2*vocab_size]`.\n        Alternatively, `layer` may return a Tensor of shape\n        `[..., length, vocab_size]` to be used as the location transform; the\n        scale transform will be hard-coded to 1.\n      temperature: Positive value determining bias of gradient estimator.\n      **kwargs: kwargs of parent class.\n    """"""\n    super(DiscreteAutoregressiveFlow, self).__init__(**kwargs)\n    self.layer = layer\n    self.temperature = temperature\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    self.vocab_size = input_shape[-1]\n    if self.vocab_size is None:\n      raise ValueError(\'The last dimension of the inputs to \'\n                       \'`DiscreteAutoregressiveFlow` should be defined. Found \'\n                       \'`None`.\')\n    self.built = True\n\n  def __call__(self, inputs, *args, **kwargs):\n    if not isinstance(inputs, random_variable.RandomVariable):\n      return super(DiscreteAutoregressiveFlow, self).__call__(\n          inputs, *args, **kwargs)\n    return transformed_random_variable.TransformedRandomVariable(inputs, self)\n\n  def call(self, inputs, **kwargs):\n    """"""Forward pass for left-to-right autoregressive generation.""""""\n    inputs = tf.convert_to_tensor(inputs)\n    length = inputs.shape[-2]\n    if length is None:\n      raise NotImplementedError(\'length dimension must be known.\')\n    # Form initial sequence tensor of shape [..., 1, vocab_size]. In a loop, we\n    # incrementally build a Tensor of shape [..., t, vocab_size] as t grows.\n    outputs = self._initial_call(inputs[..., 0, :], length, **kwargs)\n    # TODO(trandustin): Use tf.while_loop. Unrolling is memory-expensive for big\n    # models and not valid for variable lengths.\n    for t in range(1, length):\n      outputs = self._per_timestep_call(outputs,\n                                        inputs[..., t, :],\n                                        length,\n                                        t,\n                                        **kwargs)\n    return outputs\n\n  def _initial_call(self, new_inputs, length, **kwargs):\n    """"""Returns Tensor of shape [..., 1, vocab_size].\n\n    Args:\n      new_inputs: Tensor of shape [..., vocab_size], the new input to generate\n        its output.\n      length: Length of final desired sequence.\n      **kwargs: Optional keyword arguments to layer.\n    """"""\n    inputs = new_inputs[..., tf.newaxis, :]\n    # TODO(trandustin): To handle variable lengths, extend MADE to subset its\n    # input and output layer weights rather than pad inputs.\n    batch_ndims = inputs.shape.ndims - 2\n    padded_inputs = tf.pad(\n        inputs, paddings=[[0, 0]] * batch_ndims + [[0, length - 1], [0, 0]])\n    net = self.layer(padded_inputs, **kwargs)\n    if net.shape[-1] == 2 * self.vocab_size:\n      loc, scale = tf.split(net, 2, axis=-1)\n      loc = loc[..., 0:1, :]\n      loc = tf.cast(utils.one_hot_argmax(loc, self.temperature), inputs.dtype)\n      scale = scale[..., 0:1, :]\n      scale = tf.cast(utils.one_hot_argmax(scale, self.temperature),\n                      inputs.dtype)\n      inverse_scale = utils.multiplicative_inverse(scale, self.vocab_size)\n      shifted_inputs = utils.one_hot_minus(inputs, loc)\n      outputs = utils.one_hot_multiply(shifted_inputs, inverse_scale)\n    elif net.shape[-1] == self.vocab_size:\n      loc = net\n      loc = loc[..., 0:1, :]\n      loc = tf.cast(utils.one_hot_argmax(loc, self.temperature), inputs.dtype)\n      outputs = utils.one_hot_minus(inputs, loc)\n    else:\n      raise ValueError(\'Output of layer does not have compatible dimensions.\')\n    return outputs\n\n  def _per_timestep_call(self,\n                         current_outputs,\n                         new_inputs,\n                         length,\n                         timestep,\n                         **kwargs):\n    """"""Returns Tensor of shape [..., timestep+1, vocab_size].\n\n    Args:\n      current_outputs: Tensor of shape [..., timestep, vocab_size], the so-far\n        generated sequence Tensor.\n      new_inputs: Tensor of shape [..., vocab_size], the new input to generate\n        its output given current_outputs.\n      length: Length of final desired sequence.\n      timestep: Current timestep.\n      **kwargs: Optional keyword arguments to layer.\n    """"""\n    inputs = tf.concat([current_outputs,\n                        new_inputs[..., tf.newaxis, :]], axis=-2)\n    # TODO(trandustin): To handle variable lengths, extend MADE to subset its\n    # input and output layer weights rather than pad inputs.\n    batch_ndims = inputs.shape.ndims - 2\n    padded_inputs = tf.pad(\n        inputs,\n        paddings=[[0, 0]] * batch_ndims + [[0, length - timestep - 1], [0, 0]])\n    net = self.layer(padded_inputs, **kwargs)\n    if net.shape[-1] == 2 * self.vocab_size:\n      loc, scale = tf.split(net, 2, axis=-1)\n      loc = loc[..., :(timestep+1), :]\n      loc = tf.cast(utils.one_hot_argmax(loc, self.temperature), inputs.dtype)\n      scale = scale[..., :(timestep+1), :]\n      scale = tf.cast(utils.one_hot_argmax(scale, self.temperature),\n                      inputs.dtype)\n      inverse_scale = utils.multiplicative_inverse(scale, self.vocab_size)\n      shifted_inputs = utils.one_hot_minus(inputs, loc)\n      new_outputs = utils.one_hot_multiply(shifted_inputs, inverse_scale)\n    elif net.shape[-1] == self.vocab_size:\n      loc = net\n      loc = loc[..., :(timestep+1), :]\n      loc = tf.cast(utils.one_hot_argmax(loc, self.temperature), inputs.dtype)\n      new_outputs = utils.one_hot_minus(inputs, loc)\n    else:\n      raise ValueError(\'Output of layer does not have compatible dimensions.\')\n    outputs = tf.concat([current_outputs, new_outputs[..., -1:, :]], axis=-2)\n    if not tf.executing_eagerly():\n      outputs.set_shape([None] * batch_ndims + [timestep+1, self.vocab_size])\n    return outputs\n\n  def reverse(self, inputs, **kwargs):\n    """"""Reverse pass returning the inverse autoregressive transformation.""""""\n    if not self.built:\n      self._maybe_build(inputs)\n\n    net = self.layer(inputs, **kwargs)\n    if net.shape[-1] == 2 * self.vocab_size:\n      loc, scale = tf.split(net, 2, axis=-1)\n      scale = tf.cast(utils.one_hot_argmax(scale, self.temperature),\n                      inputs.dtype)\n      scaled_inputs = utils.one_hot_multiply(inputs, scale)\n    elif net.shape[-1] == self.vocab_size:\n      loc = net\n      scaled_inputs = inputs\n    else:\n      raise ValueError(\'Output of layer does not have compatible dimensions.\')\n    loc = tf.cast(utils.one_hot_argmax(loc, self.temperature), inputs.dtype)\n    outputs = utils.one_hot_add(loc, scaled_inputs)\n    return outputs\n\n  def log_det_jacobian(self, inputs):\n    return tf.cast(0, inputs.dtype)\n\n\nclass DiscreteBipartiteFlow(tf.keras.layers.Layer):\n  """"""A discrete reversible layer.\n\n  The flow takes as input a one-hot Tensor of shape `[..., length, vocab_size]`.\n  The flow returns a Tensor of same shape and dtype. (To enable gradients, the\n  input must have float dtype.)\n\n  For the forward pass, the flow computes:\n\n  ```none\n  net = layer(mask * inputs)\n  loc, scale = tf.split(net, 2, axis=-1)\n  loc = tf.argmax(loc, axis=-1)\n  scale = tf.argmax(scale, axis=-1)\n  outputs = ((inputs - (1-mask) * loc) * (1-mask) * inverse(scale)) % vocab_size\n  ```\n\n  For the reverse pass, the flow computes:\n\n  ```none\n  net = layer(mask * inputs)\n  loc, scale = tf.split(net, 2, axis=-1)\n  loc = tf.argmax(loc, axis=-1)\n  scale = tf.argmax(scale, axis=-1)\n  outputs = ((1-mask) * loc + (1-mask) * scale * inputs) % vocab_size\n  ```\n\n  The modular arithmetic happens in one-hot space.\n\n  If `x` is a discrete random variable, the induced probability mass function on\n  the outputs `y = flow(x)` is\n\n  ```none\n  p(y) = p(flow.reverse(y)).\n  ```\n\n  The location-only transform is always invertible ([integers modulo\n  `vocab_size` form an additive group](\n  https://en.wikipedia.org/wiki/Modular_arithmetic)). The transform with a scale\n  is invertible if the scale and `vocab_size` are coprime (see\n  [prime fields](https://en.wikipedia.org/wiki/Finite_field)).\n  """"""\n\n  def __init__(self, layer, mask, temperature, **kwargs):\n    """"""Constructs flow.\n\n    Args:\n      layer: Two-headed masked network taking the inputs and returning a\n        real-valued Tensor of shape `[..., length, 2*vocab_size]`.\n        Alternatively, `layer` may return a Tensor of shape\n        `[..., length, vocab_size]` to be used as the location transform; the\n        scale transform will be hard-coded to 1.\n      mask: binary Tensor of shape `[length]` forming the bipartite assignment.\n      temperature: Positive value determining bias of gradient estimator.\n      **kwargs: kwargs of parent class.\n    """"""\n    super(DiscreteBipartiteFlow, self).__init__(**kwargs)\n    self.layer = layer\n    self.mask = mask\n    self.temperature = temperature\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    self.vocab_size = input_shape[-1]\n    if self.vocab_size is None:\n      raise ValueError(\'The last dimension of the inputs to \'\n                       \'`DiscreteBipartiteFlow` should be defined. Found \'\n                       \'`None`.\')\n    self.built = True\n\n  def __call__(self, inputs, *args, **kwargs):\n    if not isinstance(inputs, random_variable.RandomVariable):\n      return super(DiscreteBipartiteFlow, self).__call__(\n          inputs, *args, **kwargs)\n    return transformed_random_variable.TransformedRandomVariable(inputs, self)\n\n  def call(self, inputs, **kwargs):\n    """"""Forward pass for bipartite generation.""""""\n    inputs = tf.convert_to_tensor(inputs)\n    batch_ndims = inputs.shape.ndims - 2\n    mask = tf.reshape(tf.cast(self.mask, inputs.dtype),\n                      [1] * batch_ndims + [-1, 1])\n    masked_inputs = mask * inputs\n    net = self.layer(masked_inputs, **kwargs)\n    if net.shape[-1] == 2 * self.vocab_size:\n      loc, scale = tf.split(net, 2, axis=-1)\n      loc = tf.cast(utils.one_hot_argmax(loc, self.temperature), inputs.dtype)\n      scale = tf.cast(utils.one_hot_argmax(scale, self.temperature),\n                      inputs.dtype)\n      inverse_scale = utils.multiplicative_inverse(scale, self.vocab_size)\n      shifted_inputs = utils.one_hot_minus(inputs, loc)\n      masked_outputs = (1. - mask) * utils.one_hot_multiply(shifted_inputs,\n                                                            inverse_scale)\n    elif net.shape[-1] == self.vocab_size:\n      loc = net\n      loc = tf.cast(utils.one_hot_argmax(loc, self.temperature), inputs.dtype)\n      masked_outputs = (1. - mask) * utils.one_hot_minus(inputs, loc)\n    else:\n      raise ValueError(\'Output of layer does not have compatible dimensions.\')\n    outputs = masked_inputs + masked_outputs\n    return outputs\n\n  def reverse(self, inputs, **kwargs):\n    """"""Reverse pass for the inverse bipartite transformation.""""""\n    if not self.built:\n      self._maybe_build(inputs)\n\n    inputs = tf.convert_to_tensor(inputs)\n    batch_ndims = inputs.shape.ndims - 2\n    mask = tf.reshape(tf.cast(self.mask, inputs.dtype),\n                      [1] * batch_ndims + [-1, 1])\n    masked_inputs = mask * inputs\n    net = self.layer(masked_inputs, **kwargs)\n    if net.shape[-1] == 2 * self.vocab_size:\n      loc, scale = tf.split(net, 2, axis=-1)\n      scale = tf.cast(utils.one_hot_argmax(scale, self.temperature),\n                      inputs.dtype)\n      scaled_inputs = utils.one_hot_multiply(inputs, scale)\n    elif net.shape[-1] == self.vocab_size:\n      loc = net\n      scaled_inputs = inputs\n    else:\n      raise ValueError(\'Output of layer does not have compatible dimensions.\')\n    loc = tf.cast(utils.one_hot_argmax(loc, self.temperature), inputs.dtype)\n    masked_outputs = (1. - mask) * utils.one_hot_add(loc, scaled_inputs)\n    outputs = masked_inputs + masked_outputs\n    return outputs\n\n  def log_det_jacobian(self, inputs):\n    return tf.cast(0, inputs.dtype)\n\n\nclass SinkhornAutoregressiveFlow(tf.keras.layers.Layer):\n  """"""A discrete reversible layer using Sinkhorn normalization for permutations.\n\n  The flow takes as input a one-hot Tensor of shape `[..., length, vocab_size]`.\n  The flow returns a Tensor of same shape and dtype. (To enable gradients, the\n  input must have float dtype.)\n  """"""\n\n  def __init__(self, layer, temperature, **kwargs):\n    """"""Constructs flow.\n\n    Args:\n      layer: Masked network taking inputs with shape `[..., length, vocab_size]`\n        and returning a real-valued Tensor of shape\n        `[..., length, vocab_size ** 2]`. Sinkhorn iterations are applied to\n        each `layer` output to produce permutation matrices.\n      temperature: Positive value determining bias of gradient estimator.\n      **kwargs: kwargs of parent class.\n    """"""\n    super(SinkhornAutoregressiveFlow, self).__init__(**kwargs)\n    self.layer = layer\n    self.temperature = temperature\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    self.vocab_size = input_shape[-1]\n    if self.vocab_size is None:\n      raise ValueError(\'The last dimension of the inputs to \'\n                       \'`DiscreteAutoregressiveFlow` should be defined. Found \'\n                       \'`None`.\')\n    self.built = True\n\n  def __call__(self, inputs, *args, **kwargs):\n    if not isinstance(inputs, random_variable.RandomVariable):\n      return super(SinkhornAutoregressiveFlow, self).__call__(\n          inputs, *args, **kwargs)\n    return transformed_random_variable.TransformedRandomVariable(inputs, self)\n\n  def call(self, inputs, **kwargs):\n    """"""Forward pass for left-to-right autoregressive generation.""""""\n    inputs = tf.convert_to_tensor(inputs)\n    length = inputs.shape[-2]\n    if length is None:\n      raise NotImplementedError(\'length dimension must be known.\')\n    # Form initial sequence tensor of shape [..., 1, vocab_size]. In a loop, we\n    # incrementally build a Tensor of shape [..., t, vocab_size] as t grows.\n    outputs = self._initial_call(inputs[..., 0, :], length, **kwargs)\n    for t in range(1, length):\n      outputs = self._per_timestep_call(outputs,\n                                        inputs[..., t, :],\n                                        length,\n                                        t,\n                                        **kwargs)\n    return outputs\n\n  def _initial_call(self, new_inputs, length, **kwargs):\n    """"""Returns Tensor of shape [..., 1, vocab_size].\n\n    Args:\n      new_inputs: Tensor of shape [..., vocab_size], the new input to generate\n        its output.\n      length: Length of final desired sequence.\n      **kwargs: Optional keyword arguments to layer.\n    """"""\n    inputs = new_inputs[..., tf.newaxis, :]\n    # TODO(trandustin): To handle variable lengths, extend MADE to subset its\n    # input and output layer weights rather than pad inputs.\n    batch_ndims = inputs.shape.ndims - 2\n    padded_inputs = tf.pad(\n        inputs, paddings=[[0, 0]] * batch_ndims + [[0, length - 1], [0, 0]])\n    temperature = 1.\n    logits = self.layer(padded_inputs / temperature, **kwargs)\n    logits = logits[..., 0:1, :]\n    logits = tf.reshape(\n        logits,\n        logits.shape[:-1].concatenate([self.vocab_size, self.vocab_size]))\n    soft = utils.sinkhorn(logits)\n    hard = tf.cast(utils.soft_to_hard_permutation(soft), inputs.dtype)\n    hard = tf.reshape(hard, logits.shape)\n    # Inverse of permutation matrix is its transpose.\n    # inputs is [batch_size, timestep + 1, vocab_size].\n    # hard is [batch_size, timestep + 1, vocab_size, vocab_size].\n    outputs = tf.matmul(inputs[..., tf.newaxis, :],\n                        hard,\n                        transpose_b=True)[..., 0, :]\n    return outputs\n\n  def _per_timestep_call(self,\n                         current_outputs,\n                         new_inputs,\n                         length,\n                         timestep,\n                         **kwargs):\n    """"""Returns Tensor of shape [..., timestep+1, vocab_size].\n\n    Args:\n      current_outputs: Tensor of shape [..., timestep, vocab_size], the so-far\n        generated sequence Tensor.\n      new_inputs: Tensor of shape [..., vocab_size], the new input to generate\n        its output given current_outputs.\n      length: Length of final desired sequence.\n      timestep: Current timestep.\n      **kwargs: Optional keyword arguments to layer.\n    """"""\n    inputs = tf.concat([current_outputs,\n                        new_inputs[..., tf.newaxis, :]], axis=-2)\n    # TODO(trandustin): To handle variable lengths, extend MADE to subset its\n    # input and output layer weights rather than pad inputs.\n    batch_ndims = inputs.shape.ndims - 2\n    padded_inputs = tf.pad(\n        inputs,\n        paddings=[[0, 0]] * batch_ndims + [[0, length - timestep - 1], [0, 0]])\n    logits = self.layer(padded_inputs, **kwargs)\n    logits = logits[..., :(timestep+1), :]\n    logits = tf.reshape(\n        logits,\n        logits.shape[:-1].concatenate([self.vocab_size, self.vocab_size]))\n    soft = utils.sinkhorn(logits / self.temperature)\n    hard = tf.cast(utils.soft_to_hard_permutation(soft), inputs.dtype)\n    hard = tf.reshape(hard, logits.shape)\n    # Inverse of permutation matrix is its transpose.\n    # inputs is [batch_size, timestep + 1, vocab_size].\n    # hard is [batch_size, timestep + 1, vocab_size, vocab_size].\n    new_outputs = tf.matmul(inputs[..., tf.newaxis, :],\n                            hard,\n                            transpose_b=True)[..., 0, :]\n    outputs = tf.concat([current_outputs, new_outputs[..., -1:, :]], axis=-2)\n    if not tf.executing_eagerly():\n      outputs.set_shape([None] * batch_ndims + [timestep+1, self.vocab_size])\n    return outputs\n\n  def reverse(self, inputs, **kwargs):\n    """"""Reverse pass returning the inverse autoregressive transformation.""""""\n    if not self.built:\n      self._maybe_build(inputs)\n\n    logits = self.layer(inputs, **kwargs)\n    logits = tf.reshape(\n        logits,\n        logits.shape[:-1].concatenate([self.vocab_size, self.vocab_size]))\n    soft = utils.sinkhorn(logits / self.temperature, n_iters=20)\n    hard = utils.soft_to_hard_permutation(soft)\n    hard = tf.reshape(hard, logits.shape)\n    # Recover the permutation by right-multiplying by the permutation matrix.\n    outputs = tf.matmul(inputs[..., tf.newaxis, :], hard)[..., 0, :]\n    return outputs\n\n  def log_det_jacobian(self, inputs):\n    return tf.cast(0, inputs.dtype)\n'"
edward2/tensorflow/layers/discrete_flows_test.py,31,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for discrete flows.""""""\n\nfrom absl.testing import parameterized\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass DiscreteFlowsTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.parameters(\n      (False,),\n      (True,),\n  )\n  def testDiscreteAutoregressiveFlowCall(self, loc_only):\n    batch_size = 3\n    vocab_size = 79\n    length = 5\n    if loc_only:\n      units = vocab_size\n      network = ed.layers.MADE(units, [])\n    else:\n      units = 2 * vocab_size\n      mask = tf.reshape([0] * vocab_size + [-1e10] + [0] * (vocab_size - 1),\n                        [1, 1, 2 * vocab_size])\n      network_ = ed.layers.MADE(units, [])\n      network = lambda inputs, **kwargs: mask + network_(inputs, **kwargs)\n    inputs = np.random.randint(0, vocab_size - 1, size=(batch_size, length))\n    inputs = tf.one_hot(inputs, depth=vocab_size, dtype=tf.float32)\n    layer = ed.layers.DiscreteAutoregressiveFlow(network, 1.)\n    outputs = layer(inputs)\n    self.assertEqual(outputs.shape, (batch_size, length, vocab_size))\n    self.assertAllGreaterEqual(outputs, 0)\n    self.assertAllLessEqual(outputs, vocab_size - 1)\n\n  @parameterized.parameters(\n      (False,),\n      (True,),\n  )\n  def testDiscreteAutoregressiveFlowSample(self, loc_only):\n    batch_size = 5\n    length = 2\n    vocab_size = 2\n    if loc_only:\n      units = vocab_size\n      network = ed.layers.MADE(units, [])\n    else:\n      units = 2 * vocab_size\n      mask = tf.reshape([0] * vocab_size + [-1e10] + [0] * (vocab_size - 1),\n                        [1, 1, 2 * vocab_size])\n      network_ = ed.layers.MADE(units, [])\n      network = lambda inputs, **kwargs: mask + network_(inputs, **kwargs)\n    layer = ed.layers.DiscreteAutoregressiveFlow(network, 1.)\n    logits = tf.tile(tf.random.normal([length, vocab_size])[tf.newaxis],\n                     [batch_size, 1, 1])\n    base = ed.OneHotCategorical(logits=logits, dtype=tf.float32)\n    outputs = layer(base)\n    self.assertEqual(outputs.shape, (batch_size, length, vocab_size))\n    self.assertAllGreaterEqual(tf.convert_to_tensor(outputs), 0)\n    self.assertAllLessEqual(tf.convert_to_tensor(outputs), vocab_size - 1)\n\n  @parameterized.parameters(\n      (False,),\n      (True,),\n  )\n  def testDiscreteAutoregressiveFlowInverse(self, loc_only):\n    batch_size = 2\n    vocab_size = 79\n    length = 5\n    if loc_only:\n      units = vocab_size\n      network = ed.layers.MADE(units, [])\n    else:\n      units = 2 * vocab_size\n      mask = tf.reshape([0] * vocab_size + [-1e10] + [0] * (vocab_size - 1),\n                        [1, 1, 2 * vocab_size])\n      network_ = ed.layers.MADE(units, [])\n      network = lambda inputs, **kwargs: mask + network_(inputs, **kwargs)\n    inputs = np.random.randint(0, vocab_size - 1, size=(batch_size, length))\n    inputs = tf.one_hot(inputs, depth=vocab_size, dtype=tf.float32)\n    layer = ed.layers.DiscreteAutoregressiveFlow(network, 1.)\n    rev_fwd_inputs = layer.reverse(layer(inputs))\n    fwd_rev_inputs = layer(layer.reverse(inputs))\n    self.assertAllClose(inputs, rev_fwd_inputs, rtol=1e-4, atol=1e-4)\n    self.assertAllClose(inputs, fwd_rev_inputs, rtol=1e-4, atol=1e-4)\n\n  @parameterized.parameters(\n      (False,),\n      (True,),\n  )\n  def testDiscreteAutoregressiveFlowRandomVariable(self, loc_only):\n    batch_size = 2\n    length = 4\n    vocab_size = 5\n    if loc_only:\n      units = vocab_size\n      network = ed.layers.MADE(units, [])\n    else:\n      units = 2 * vocab_size\n      mask = tf.reshape([0] * vocab_size + [-1e10] + [0] * (vocab_size - 1),\n                        [1, 1, 2 * vocab_size])\n      network_ = ed.layers.MADE(units, [])\n      network = lambda inputs, **kwargs: mask + network_(inputs, **kwargs)\n    base = ed.OneHotCategorical(logits=tf.random.normal([batch_size,\n                                                         length,\n                                                         vocab_size]),\n                                dtype=tf.float32)\n    flow = ed.layers.DiscreteAutoregressiveFlow(network, 1.)\n    flow_rv = flow(base)\n    self.assertEqual(flow_rv.dtype, tf.float32)\n\n    self.assertEqual(flow_rv.shape, (batch_size, length, vocab_size))\n    self.assertAllGreaterEqual(tf.convert_to_tensor(flow_rv), 0)\n    self.assertAllLessEqual(tf.convert_to_tensor(flow_rv), vocab_size - 1)\n\n    inputs = np.random.randint(0, vocab_size - 1, size=(batch_size, length))\n    inputs = tf.one_hot(inputs, depth=vocab_size, dtype=tf.float32)\n    outputs = flow(inputs)\n    rev_outputs = flow.reverse(outputs)\n    self.assertAllClose(inputs, rev_outputs)\n\n    inputs_log_prob = base.distribution.log_prob(inputs)\n    outputs_log_prob = flow_rv.distribution.log_prob(outputs)\n    self.assertEqual(inputs_log_prob.shape, (batch_size, length))\n    self.assertAllClose(inputs_log_prob, outputs_log_prob)\n\n  @parameterized.parameters(\n      (False,),\n      (True,),\n  )\n  def testDiscreteAutoregressiveFlowReverseGradients(self, loc_only):\n    batch_size = 2\n    length = 4\n    vocab_size = 2\n    if loc_only:\n      units = vocab_size\n      network_ = ed.layers.MADE(units, [16, 16])\n      network = network_\n    else:\n      units = 2 * vocab_size\n      network_ = ed.layers.MADE(units, [16, 16])\n      mask = tf.reshape([0] * vocab_size + [-1e10] + [0] * (vocab_size - 1),\n                        [1, 1, 2 * vocab_size])\n      network = lambda inputs, **kwargs: mask + network_(inputs, **kwargs)\n    with tf.GradientTape() as tape:\n      base = ed.OneHotCategorical(\n          logits=tf.random.normal([batch_size, length, vocab_size]))\n      flow = ed.layers.DiscreteAutoregressiveFlow(network, 1.)\n      flow_rv = flow(base)\n      features = np.random.randint(0, vocab_size - 1, size=(batch_size, length))\n      features = tf.one_hot(features, depth=vocab_size, dtype=tf.float32)\n      loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(\n          labels=flow.reverse(features),\n          logits=flow_rv.distribution.base.logits))\n    grads = tape.gradient(loss, network_.weights)\n    for grad in grads:\n      self.assertIsNotNone(grad)\n\n  def testDiscreteBipartiteFlowCall(self):\n    batch_size = 3\n    vocab_size = 79\n    length = 5\n    inputs = np.random.randint(0, vocab_size - 1, size=(batch_size, length))\n    inputs = tf.one_hot(inputs, depth=vocab_size, dtype=tf.float32)\n    layer = ed.layers.DiscreteBipartiteFlow(\n        lambda inputs, **kwargs: tf.identity(inputs),\n        mask=tf.random.uniform([length], minval=0, maxval=2, dtype=tf.int32),\n        temperature=1.)\n    outputs = layer(inputs)\n    self.assertEqual(outputs.shape, (batch_size, length, vocab_size))\n    self.assertAllGreaterEqual(outputs, 0)\n    self.assertAllLessEqual(outputs, vocab_size - 1)\n\n  def testDiscreteBipartiteFlowInverse(self):\n    batch_size = 2\n    vocab_size = 79\n    length = 5\n    inputs = np.random.randint(0, vocab_size - 1, size=(batch_size, length))\n    inputs = tf.one_hot(inputs, depth=vocab_size, dtype=tf.float32)\n    layer = ed.layers.DiscreteBipartiteFlow(\n        lambda inputs, **kwargs: tf.identity(inputs),\n        mask=tf.random.uniform([length], minval=0, maxval=2, dtype=tf.int32),\n        temperature=1.)\n    rev_fwd_inputs = layer.reverse(layer(inputs))\n    fwd_rev_inputs = layer(layer.reverse(inputs))\n    self.assertAllClose(inputs, rev_fwd_inputs)\n    self.assertAllClose(inputs, fwd_rev_inputs)\n\n  def testSinkhornAutoregressiveFlowCall(self):\n    batch_size = 3\n    vocab_size = 79\n    length = 5\n    units = vocab_size ** 2\n    inputs = np.random.randint(0, vocab_size - 1, size=(batch_size, length))\n    inputs = tf.one_hot(inputs, depth=vocab_size, dtype=tf.float32)\n    layer = ed.layers.SinkhornAutoregressiveFlow(\n        ed.layers.MADE(units, []), 1.)\n    outputs = layer(inputs)\n    self.assertEqual(outputs.shape, (batch_size, length, vocab_size))\n    self.assertAllGreaterEqual(outputs, 0)\n    self.assertAllLessEqual(outputs, vocab_size - 1)\n\n  def testDiscreteSinkhornFlowInverse(self):\n    batch_size = 2\n    vocab_size = 79\n    length = 5\n    units = vocab_size ** 2\n    inputs = np.random.randint(0, vocab_size - 1, size=(batch_size, length))\n    inputs = tf.one_hot(inputs, depth=vocab_size, dtype=tf.float32)\n    layer = ed.layers.SinkhornAutoregressiveFlow(\n        ed.layers.MADE(units, []), 1.)\n    rev_fwd_inputs = layer.reverse(layer(inputs))\n    fwd_rev_inputs = layer(layer.reverse(inputs))\n    self.assertAllEqual(inputs, rev_fwd_inputs)\n    self.assertAllEqual(inputs, fwd_rev_inputs)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
edward2/tensorflow/layers/embeddings.py,2,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Bayesian embedding layers.""""""\n\nfrom edward2.tensorflow import constraints\nfrom edward2.tensorflow import initializers\nfrom edward2.tensorflow import regularizers\nfrom edward2.tensorflow.layers import utils\n\nimport tensorflow as tf\n\n\n@utils.add_weight\nclass EmbeddingReparameterization(tf.keras.layers.Embedding):\n  """"""Bayesian embedding layer estimated via reparameterization.\n\n  The layer computes a variational Bayesian approximation to the distribution\n  over embedding layer functions,\n\n  ```\n  p(outputs | inputs) = int embedding(inputs; weights) p(weights) dweights.\n  ```\n\n  It does this with a stochastic forward pass, sampling from learnable\n  distributions on the embedding weights. Gradients with respect to the\n  distributions\' learnable parameters backpropagate via reparameterization.\n  Minimizing cross-entropy plus the layer\'s losses performs variational minimum\n  description length, i.e., it minimizes an upper bound to the negative marginal\n  likelihood.\n  """"""\n\n  def __init__(self,\n               input_dim,\n               output_dim,\n               embeddings_initializer=\'trainable_normal\',\n               embeddings_regularizer=\'normal_kl_divergence\',\n               activity_regularizer=None,\n               embeddings_constraint=None,\n               mask_zero=False,\n               input_length=None,\n               **kwargs):\n    """"""Initializes the reparameterized Bayesian embeddings layer.\n\n    Args:\n      input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index +\n      1.\n      output_dim: int >= 0. Dimension of the dense embedding.\n      embeddings_initializer: Initializer for the `embeddings` matrix.\n      embeddings_regularizer: Regularizer function applied to the `embeddings`\n        matrix.\n      activity_regularizer: Regularizer function applied to the output of the\n        layer (its ""activation"").\n      embeddings_constraint: Constraint function applied to the `embeddings`\n        matrix.\n      mask_zero: Whether or not the input value 0 is a special ""padding"" value\n        that should be masked out.  This is useful when using recurrent layers\n        which may take variable length input.  If this is `True` then all\n        subsequent layers in the model need to support masking or an exception\n        will be raised.  If mask_zero is set to True, as a consequence, index 0\n        cannot be used in the vocabulary (input_dim should equal size of\n        vocabulary + 1).\n      input_length: Length of input sequences, when it is constant.  This\n        argument is required if you are going to connect `Flatten` then `Dense`\n        layers upstream (without it, the shape of the dense outputs cannot be\n        computed).\n      **kwargs: Additional keyword arguments to pass to the super class.\n    """"""\n    super(EmbeddingReparameterization, self).__init__(\n        input_dim=input_dim,\n        output_dim=output_dim,\n        embeddings_initializer=initializers.get(embeddings_initializer),\n        embeddings_regularizer=regularizers.get(embeddings_regularizer),\n        activity_regularizer=regularizers.get(activity_regularizer),\n        embeddings_constraint=constraints.get(embeddings_constraint),\n        mask_zero=mask_zero,\n        input_length=input_length,\n        **kwargs)\n\n  def call_weights(self):\n    """"""Calls any weights if the initializer is itself a layer.""""""\n    if isinstance(self.embeddings_initializer, tf.keras.layers.Layer):\n      self.embeddings = self.embeddings_initializer(self.embeddings.shape,\n                                                    self.dtype)\n\n  def call(self, *args, **kwargs):\n    """"""Computes the forward pass of this function.""""""\n    self.call_weights()\n    kwargs.pop(\'training\', None)\n    return super(EmbeddingReparameterization, self).call(*args, **kwargs)\n'"
edward2/tensorflow/layers/embeddings_test.py,8,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for Bayesian embedding layers.""""""\n\nfrom absl.testing import parameterized\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass EmbeddingTest(parameterized.TestCase, tf.test.TestCase):\n  """"""Tests for the Bayesian Embedding layers.""""""\n\n  def setUp(self):\n    self.batch_size = 5\n    self.timesteps = 3\n    self.input_dim = 12\n    self.output_dim = 15\n    self.inputs = np.random.randint(\n        self.input_dim - 1, size=(self.batch_size, self.timesteps))\n    super(EmbeddingTest, self).setUp()\n\n  @parameterized.parameters(\n      {""embeddings_initializer"": ""uniform"", ""all_close"": True},\n      {""embeddings_initializer"": ""trainable_normal"", ""all_close"": False},\n  )\n  def testEmbedding(self, embeddings_initializer, all_close):\n    layer = ed.layers.EmbeddingReparameterization(\n        self.input_dim,\n        output_dim=self.output_dim,\n        embeddings_initializer=embeddings_initializer)\n    outputs1 = tf.convert_to_tensor(layer(self.inputs))\n    outputs2 = tf.convert_to_tensor(layer(self.inputs))\n    self.assertEqual(outputs1.shape,\n                     (self.batch_size, self.timesteps, self.output_dim))\n    if all_close:\n      self.assertAllClose(outputs1, outputs2)\n    else:\n      self.assertNotAllClose(outputs1, outputs2)\n\n  @parameterized.parameters(\n      {""embeddings_initializer"": ""uniform"",\n       ""embeddings_regularizer"": None,\n       ""all_close"": True,\n       ""num_losses"": 0},\n      {""embeddings_initializer"": ""trainable_normal"",\n       ""embeddings_regularizer"": ""normal_kl_divergence"",\n       ""all_close"": False,\n       ""num_losses"": 1},\n  )\n  def testEmbeddingModel(self, embeddings_initializer, embeddings_regularizer,\n                         all_close, num_losses):\n    model_output_dim = 2\n    model = tf.keras.Sequential([\n        ed.layers.EmbeddingReparameterization(\n            self.input_dim,\n            output_dim=self.output_dim,\n            embeddings_initializer=embeddings_initializer,\n            embeddings_regularizer=embeddings_regularizer),\n        tf.keras.layers.RNN(tf.keras.layers.LSTMCell(5)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(model_output_dim),\n    ])\n    outputs1 = model(self.inputs, training=True)\n    outputs2 = model(self.inputs, training=True)\n    self.assertEqual(outputs1.shape, (self.batch_size, model_output_dim))\n    if all_close:\n      self.assertAllClose(outputs1, outputs2)\n    else:\n      self.assertNotAllClose(outputs1, outputs2)\n    self.assertLen(model.losses, num_losses)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
edward2/tensorflow/layers/gaussian_process.py,36,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Gaussian process layers.""""""\n\nfrom edward2.tensorflow import constraints\nfrom edward2.tensorflow import generated_random_variables\nfrom edward2.tensorflow import initializers\nfrom edward2.tensorflow import regularizers\nfrom edward2.tensorflow.layers import utils\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n\nclass Zeros(object):\n  """"""Function returning zeros tensor of same shape excluding the last dim.""""""\n\n  def __call__(self, inputs):\n    return tf.zeros(tf.shape(inputs)[:-1], inputs.dtype)\n\n  def get_config(self):\n    return {}\n\n\nclass ExponentiatedQuadratic(object):\n  """"""Exponentiated quadratic kernel.""""""\n\n  def __init__(self, variance, lengthscale):\n    self.variance = variance\n    self.lengthscale = lengthscale\n\n  def __call__(self, x1, x2):\n    """"""Computes exponentiated quadratic over all pairs of inputs.\n\n    Args:\n      x1: Tensor of shape [batch_x1, ...]. Slices along the batch axis denote an\n        individual input to be passed to the kernel. It is computed pairwise\n        with each input sliced from x2.\n      x2: Tensor of shape [batch_x2, ...]. Slices along the batch axis denote an\n        individual input passed to the kernel function. It is computed pairwise\n        with each input sliced from x1.\n\n    Returns:\n      Tensor of shape [batch_x1, batch_x2].\n    """"""\n    size = tf.convert_to_tensor(x1).shape.ndims\n    if size > 2:\n      raise NotImplementedError(\'Multiple feature dimensions is not yet \'\n                                \'supported.\')\n    x1 = x1 / self.lengthscale\n    x2 = x2 / self.lengthscale\n    x1_squared = tf.reduce_sum(tf.square(x1), list(range(1, len(x1.shape))))\n    x2_squared = tf.reduce_sum(tf.square(x2), list(range(1, len(x2.shape))))\n    square = (x1_squared[:, tf.newaxis] +\n              x2_squared[tf.newaxis, :] -\n              2 * tf.matmul(x1, x2, transpose_b=True))\n    return self.variance * tf.exp(-square / 2)\n\n  def get_config(self):\n    return {\'variance\': self.variance, \'lengthscale\': self.lengthscale}\n\n\nclass LinearKernel(object):\n  """"""Linear kernel, optionally on top of a feature extractor (e.g., encoder).""""""\n\n  def __init__(self, variance, bias, encoder=tf.identity):\n    self.variance = variance\n    self.bias = bias\n    self.encoder = encoder\n\n  def __call__(self, x1, x2):\n    """"""Computes scaled dot product of over all pairs of encoded inputs.\n\n    Args:\n      x1: Tensor of shape [batch_x1] + encoder domain. Slices along the batch\n        axis denote an individual input to be passed to the kernel. It is\n        computed pairwise with each input sliced from x2.\n      x2: Tensor of shape [batch_x2] + encoder domain. Slices along the batch\n        axis denote an individual input to be passed to the kernel. It is\n        computed pairwise with each input sliced from x1.\n\n    Returns:\n      Tensor of shape [batch_x1, batch_x2].\n    """"""\n    encoded_x1 = self.encoder(x1)\n    encoded_x2 = self.encoder(x2)\n    dot_product = tf.matmul(encoded_x1, encoded_x2, transpose_b=True)\n    return self.variance * dot_product + self.bias\n\n  def get_config(self):\n    return {\n        \'variance\': self.variance,\n        \'bias\': self.bias,\n        \'encoder\': tf.keras.utils.serialize_keras_object(self.encoder),\n    }\n\n\nclass GaussianProcess(tf.keras.layers.Layer):\n  r""""""Gaussian process layer.\n\n  The layer represents a distribution over functions, where a\n  stochastic forward pass appears as\n\n  ```none\n  f ~ GP(f | conditional_inputs, conditional_outputs; mean_fn, covariance_fn)\n  outputs = f(inputs)\n  ```\n\n  The optional arguments `conditional_inputs` and `conditional_outputs`\n  capture data that the GP ""memorizes"", i.e., it forms a posterior predictive\n  distribution. If left unspecified, the GP posits a prior predictive.\n\n  Given a call to `inputs`, an equivalent formulation in terms of function\n  outputs is\n\n  ```none\n  outputs ~ \\prod_{unit=1}^{units} MultivariateNormal(output[:, unit] |\n      mean = mean_fn(inputs) + Knm Kmm^{-1} (conditional_outputs[:, unit]-mean),\n      covariance = Knn - Knm Kmm^{-1} Kmn)\n  ```\n\n  where Knm is the covariance function evaluated between all `inputs` and\n  `conditional_inputs`; Knn is between all `inputs`; Kmm is between all\n  `conditional_inputs`; and mean is the mean function evaluated on\n  `conditional_inputs`. The multivariate normal is correlated across input\n  dimensions and is independent across output dimensions.\n  """"""\n\n  def __init__(\n      self,\n      units,\n      mean_fn=Zeros(),\n      covariance_fn=ExponentiatedQuadratic(variance=1., lengthscale=1.),\n      conditional_inputs=None,\n      conditional_outputs=None,\n      **kwargs):\n    """"""Constructs layer.\n\n    Args:\n      units: integer, dimensionality of layer.\n      mean_fn: Mean function, a callable taking an inputs Tensor of shape\n        [batch, ...] and returning a Tensor of shape [batch].\n      covariance_fn: Covariance function, a callable taking two input Tensors\n        of shape [batch_x1, ...] and [batch_x2, ...] respectively, and returning\n        a positive semi-definite matrix of shape [batch_x1, batch_x2].\n      conditional_inputs: Tensor of shape [batch, ...], where batch must be the\n        same as conditional_outputs\', and ellipses must match layer inputs.\n      conditional_outputs: Tensor of shape [batch, units], where batch must be\n        the same as conditional_inputs\' and units is the layer\'s units size.\n      **kwargs: kwargs passed to parent class.\n    """"""\n    super(GaussianProcess, self).__init__(**kwargs)\n    self.units = int(units)\n    self.mean_fn = mean_fn\n    self.covariance_fn = covariance_fn\n    self.conditional_inputs = conditional_inputs\n    self.conditional_outputs = conditional_outputs\n\n    self.supports_masking = True\n    self.input_spec = tf.keras.layers.InputSpec(min_ndim=2)\n\n  def build(self, input_shape=None):\n    # Don\'t track trainable variables such as in the kernel. The user should\n    # refer to any via, e.g., self.covariance_fn or the user environment.\n    self.built = True\n\n  def call(self, inputs):\n    if self.conditional_inputs is None and self.conditional_outputs is None:\n      covariance_matrix = self.covariance_fn(inputs, inputs)\n      # Tile locations so output has shape [units, batch_size]. Covariance will\n      # broadcast to [units, batch_size, batch_size], and we perform\n      # shape manipulations to get a random variable over [batch_size, units].\n      loc = self.mean_fn(inputs)\n      loc = tf.tile(loc[tf.newaxis], [self.units] + [1] * len(loc.shape))\n    else:\n      knn = self.covariance_fn(inputs, inputs)\n      knm = self.covariance_fn(inputs, self.conditional_inputs)\n      kmm = self.covariance_fn(self.conditional_inputs, self.conditional_inputs)\n      kmm = tf.linalg.set_diag(\n          kmm, tf.linalg.diag_part(kmm) + tf.keras.backend.epsilon())\n      kmm_tril = tf.linalg.cholesky(kmm)\n      kmm_tril_operator = tf.linalg.LinearOperatorLowerTriangular(kmm_tril)\n      knm_operator = tf.linalg.LinearOperatorFullMatrix(knm)\n\n      # TODO(trandustin): Vectorize linear algebra for multiple outputs. For\n      # now, we do each separately and stack to obtain a locations Tensor of\n      # shape [units, batch_size].\n      loc = []\n      for conditional_outputs_unit in tf.unstack(self.conditional_outputs,\n                                                 axis=-1):\n        center = conditional_outputs_unit - self.mean_fn(\n            self.conditional_inputs)\n        loc_unit = knm_operator.matvec(\n            kmm_tril_operator.solvevec(kmm_tril_operator.solvevec(center),\n                                       adjoint=True))\n        loc.append(loc_unit)\n      loc = tf.stack(loc) + self.mean_fn(inputs)[tf.newaxis]\n\n      covariance_matrix = knn\n      covariance_matrix -= knm_operator.matmul(\n          kmm_tril_operator.solve(\n              kmm_tril_operator.solve(knm, adjoint_arg=True), adjoint=True))\n\n    covariance_matrix = tf.linalg.set_diag(\n        covariance_matrix,\n        tf.linalg.diag_part(covariance_matrix) + tf.keras.backend.epsilon())\n\n    # Form a multivariate normal random variable with batch_shape units and\n    # event_shape batch_size. Then make it be independent across the units\n    # dimension. Then transpose its dimensions so it is [batch_size, units].\n    random_variable = (\n        generated_random_variables.MultivariateNormalFullCovariance(\n            loc=loc, covariance_matrix=covariance_matrix))\n    random_variable = generated_random_variables.Independent(\n        random_variable.distribution, reinterpreted_batch_ndims=1)\n    bijector = tfp.bijectors.Inline(\n        forward_fn=lambda x: tf.transpose(x, perm=[1, 0]),\n        inverse_fn=lambda y: tf.transpose(y, perm=[1, 0]),\n        forward_event_shape_fn=lambda input_shape: input_shape[::-1],\n        forward_event_shape_tensor_fn=lambda input_shape: input_shape[::-1],\n        inverse_log_det_jacobian_fn=lambda y: tf.cast(0, y.dtype),\n        forward_min_event_ndims=2)\n    random_variable = generated_random_variables.TransformedDistribution(\n        random_variable.distribution, bijector=bijector)\n    return random_variable\n\n  def compute_output_shape(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    input_shape = input_shape.with_rank_at_least(2)\n    input_dim = input_shape[-1]\n    if input_dim is None:\n      raise ValueError(\n          \'The innermost dimension of input_shape must be defined, but saw: %s\'\n          % input_shape)\n    return input_shape[:-1].concatenate(self.units)\n\n  def get_config(self):\n    config = {\n        \'units\': self.units,\n        \'mean_fn\': tf.keras.utils.serialize_keras_object(self.mean_fn),\n        \'covariance_fn\': tf.keras.utils.serialize_keras_object(\n            self.covariance_fn),\n        \'conditional_inputs\': None,  # don\'t serialize as it can be large\n        \'conditional_outputs\': None,  # don\'t serialize as it can be large\n    }\n    base_config = super(GaussianProcess, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n\n\n@utils.add_weight\nclass SparseGaussianProcess(GaussianProcess):\n  r""""""Gaussian process layer with inducing input and output variables.\n\n  The layer represents a distribution over functions, where a\n  stochastic forward pass appears as\n\n  ```none\n  f ~ GP(f | inducing_inputs, inducing_outputs; mean_fn, covariance_fn)\n  outputs = f(inputs)\n  ```\n\n  The arguments `inducing_inputs` and `inducing_outputs`\n  capture data that the GP ""memorizes"", i.e., it forms a posterior predictive\n  distribution. Typically in a variational inference scheme (and by default),\n  the inducing outputs are normally distributed with learnable location and\n  scale parameters, and the inducing inputs are learnable parameters.\n\n  Given a call to `inputs` with these defaults, an equivalent formulation in\n  terms of function outputs is\n\n  ```none\n  inducing_outputs ~ Normal(inducing_outputs | mean, stddev)\n  outputs ~ \\prod_{unit=1}^{units} MultivariateNormal(output[:, unit] |\n      mean = mean_fn(inputs) + Knm Kmm^{-1} (inducing_outputs[:, unit]-mean),\n      covariance = Knn - Knm Kmm^{-1} Kmn)\n  ```\n\n  where Knm is the covariance function evaluated between all `inputs` and\n  `inducing_inputs`; Knn is between all `inputs`; Kmm is between all\n  `inducing_inputs`; and mean is the mean function evaluated on\n  `inducing_inputs`. The multivariate normal is correlated across input\n  dimensions and is independent across output dimensions.\n\n  #### Examples\n\n  We demonstrate a three-layer deep GP with variational inference (Salimbeni and\n  Deisenroth, 2017; Damianou and Lawrence, 2013). The code snippet mirrors\n  Figure 5 of Bayesian Layers. We apply it for regression given batches of\n  spatial inputs and vector-valued outputs. We flatten inputs to use the\n  default squared exponential kernel; this naturally extends to pass in a\n  more sophisticated kernel function.\n\n  ```python\n  from tensor2tensor.layers import bayes\n\n  batch_size = 256\n  dataset_size = 10000\n  features, labels = load_spatial_data(batch_size)\n\n  model = tf.keras.Sequential([\n    tf.keras.layers.Flatten(),\n    layers.SparseGaussianProcess(256, num_inducing=512),\n    layers.SparseGaussianProcess(256, num_inducing=512),\n    layers.SparseGaussianProcess(10, num_inducing=512),\n  ])\n\n  # Run training loop.\n  num_steps = 1000\n  for _ in range(num_steps):\n    with tf.GradientTape() as tape:\n      predictions = model(features)\n      nll = tf.losses.mean_squared_error(labels=labels, predictions=predictions)\n      kl = sum(model.losses) / dataset_size\n      loss = nll + kl\n    gradients = tape.gradient(loss, model.variables)  # use any optimizer here\n  ```\n  """"""\n\n  def __init__(\n      self,\n      units,\n      num_inducing,\n      mean_fn=Zeros(),\n      covariance_fn=ExponentiatedQuadratic(variance=1., lengthscale=1.),\n      inducing_inputs_initializer=\'random_normal\',\n      inducing_outputs_initializer=\'trainable_normal\',\n      inducing_inputs_regularizer=None,\n      inducing_outputs_regularizer=\'normal_kl_divergence\',\n      inducing_inputs_constraint=None,\n      inducing_outputs_constraint=None,\n      **kwargs):\n    """"""Constructs layer.\n\n    Args:\n      units: integer, dimensionality of layer.\n      num_inducing: integer, number of inducing points for the approximation.\n      mean_fn: Mean function, a callable taking an inputs Tensor of shape\n        [batch, ...] and returning a Tensor of shape [batch].\n      covariance_fn: Covariance function, a callable taking two input Tensors\n        of shape [batch_x1, ...] and [batch_x2, ...] respectively, and returning\n        a positive semi-definite matrix of shape [batch_x1, batch_x2].\n      inducing_inputs_initializer: Initializer for the inducing inputs.\n      inducing_outputs_initializer: Initializer for the inducing outputs.\n      inducing_inputs_regularizer: Regularizer function applied to the inducing\n        inputs.\n      inducing_outputs_regularizer: Regularizer function applied to the inducing\n        outputs.\n      inducing_inputs_constraint: Constraint function applied to the inducing\n        inputs.\n      inducing_outputs_constraint: Constraint function applied to the inducing\n        outputs.\n      **kwargs: kwargs passed to parent class.\n    """"""\n    super(SparseGaussianProcess, self).__init__(\n        units=units,\n        mean_fn=mean_fn,\n        covariance_fn=covariance_fn,\n        conditional_inputs=None,\n        conditional_outputs=None,\n        **kwargs)\n    self.num_inducing = num_inducing\n    self.inducing_inputs_initializer = initializers.get(\n        inducing_inputs_initializer)\n    self.inducing_outputs_initializer = initializers.get(\n        inducing_outputs_initializer)\n    self.inducing_inputs_regularizer = regularizers.get(\n        inducing_inputs_regularizer)\n    self.inducing_outputs_regularizer = regularizers.get(\n        inducing_outputs_regularizer)\n    self.inducing_inputs_constraint = constraints.get(\n        inducing_inputs_constraint)\n    self.inducing_outputs_constraint = constraints.get(\n        inducing_outputs_constraint)\n\n  def build(self, input_shape=None):\n    input_shape = tf.TensorShape(input_shape)\n    input_dim = input_shape[-1]\n    self.conditional_inputs = self.add_weight(\n        shape=(self.num_inducing, input_dim),\n        name=\'inducing_inputs\',\n        initializer=self.inducing_inputs_initializer,\n        regularizer=self.inducing_inputs_regularizer,\n        constraint=self.inducing_inputs_constraint)\n    self.conditional_outputs = self.add_weight(\n        shape=(self.num_inducing, self.units),\n        name=\'inducing_outputs\',\n        initializer=self.inducing_outputs_initializer,\n        regularizer=self.inducing_outputs_regularizer,\n        constraint=self.inducing_outputs_constraint)\n    super(SparseGaussianProcess, self).build(input_shape)\n\n  def call_weights(self):\n    """"""Calls any weights if the initializer is itself a layer.""""""\n    if isinstance(self.inducing_inputs_initializer, tf.keras.layers.Layer):\n      self.conditional_inputs = self.inducing_inputs_initializer(\n          self.conditional_inputs.shape, self.dtype)\n    if isinstance(self.inducing_outputs_initializer, tf.keras.layers.Layer):\n      self.conditional_outputs = self.inducing_outputs_initializer(\n          self.conditional_outputs.shape, self.dtype)\n\n  def call(self, inputs):\n    self.call_weights()\n    return super(SparseGaussianProcess, self).call(inputs)\n'"
edward2/tensorflow/layers/gaussian_process_test.py,8,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for Gaussian process layers.""""""\n\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass GaussianProcessTest(tf.test.TestCase):\n\n  def testGaussianProcessPosterior(self):\n    train_batch_size = 3\n    test_batch_size = 2\n    input_dim = 4\n    output_dim = 5\n    features = np.random.rand(train_batch_size, input_dim).astype(np.float32)\n    labels = np.random.rand(train_batch_size, output_dim).astype(np.float32)\n    layer = ed.layers.GaussianProcess(output_dim,\n                                      conditional_inputs=features,\n                                      conditional_outputs=labels)\n    test_features = np.random.rand(test_batch_size, input_dim).astype(\n        np.float32)\n    test_labels = np.random.rand(test_batch_size, output_dim).astype(\n        np.float32)\n    test_outputs = layer(test_features)\n    test_nats = -test_outputs.distribution.log_prob(test_labels)\n    self.assertEqual(test_nats.shape, ())\n    self.assertGreaterEqual(test_nats, 0.)\n    self.assertEqual(test_outputs.shape, (test_batch_size, output_dim))\n\n  def testGaussianProcessPrior(self):\n    batch_size = 3\n    input_dim = 4\n    output_dim = 5\n    features = np.random.rand(batch_size, input_dim).astype(np.float32)\n    labels = np.random.rand(batch_size, output_dim).astype(np.float32)\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(2, activation=None),\n        ed.layers.GaussianProcess(output_dim),\n    ])\n    outputs = model(features)\n    log_prob = outputs.distribution.log_prob(labels)\n    self.assertEqual(log_prob.shape, ())\n    self.assertLessEqual(log_prob, 0.)\n    self.assertEqual(outputs.shape, (batch_size, output_dim))\n\n  def testSparseGaussianProcess(self):\n    dataset_size = 10\n    batch_size = 3\n    input_dim = 4\n    output_dim = 5\n    features = np.random.rand(batch_size, input_dim).astype(np.float32)\n    labels = np.random.rand(batch_size, output_dim).astype(np.float32)\n    model = ed.layers.SparseGaussianProcess(output_dim, num_inducing=2)\n    with tf.GradientTape() as tape:\n      predictions = model(features)\n      nll = -tf.reduce_mean(predictions.distribution.log_prob(labels))\n      kl = sum(model.losses) / dataset_size\n      loss = nll + kl\n\n    grads = tape.gradient(loss, model.variables)\n    for grad in grads:\n      self.assertIsNotNone(grad)\n\n    self.assertEqual(loss.shape, ())\n    self.assertGreaterEqual(loss, 0.)\n    self.assertEqual(predictions.shape, (batch_size, output_dim))\n\n    # Check that gradients work on a second iteration. This can fail if\n    # trainable initializers do not recall their weights.\n    with tf.GradientTape() as tape:\n      predictions = model(features)\n      nll = -tf.reduce_mean(predictions.distribution.log_prob(labels))\n      kl = sum(model.losses) / dataset_size\n      loss = nll + kl\n\n    grads = tape.gradient(loss, model.variables)\n    for grad in grads:\n      self.assertIsNotNone(grad)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
edward2/tensorflow/layers/made.py,17,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Masked autoencoder for distribution estimation.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass MADE(tf.keras.Model):\n  """"""Masked autoencoder for distribution estimation (Germain et al., 2015).\n\n  MADE takes as input a real Tensor of shape [..., length, channels] and returns\n  a Tensor of shape [..., length, units] and same dtype. It masks layer weights\n  to satisfy autoregressive constraints with respect to the length dimension. In\n  particular, for a given ordering, each input dimension of length can be\n  reconstructed from previous dimensions.\n\n  The output\'s units dimension captures per-time-step representations. For\n  example, setting units to 2 can parameterize the location and log-scale of an\n  autoregressive Gaussian distribution.\n  """"""\n\n  def __init__(self,\n               units,\n               hidden_dims,\n               input_order=\'left-to-right\',\n               hidden_order=\'left-to-right\',\n               activation=None,\n               use_bias=True,\n               **kwargs):\n    """"""Constructs network.\n\n    Args:\n      units: Positive integer, dimensionality of the output space.\n      hidden_dims: list with the number of hidden units per layer. It does not\n        include the output layer; those number of units will always be set to\n        the input dimension multiplied by `num_heads`. Each hidden unit size\n        must be at least the size of length (otherwise autoregressivity is not\n        possible).\n      input_order: Order of degrees to the input units: \'random\',\n        \'left-to-right\', \'right-to-left\', or an array of an explicit order.\n        For example, \'left-to-right\' builds an autoregressive model\n        p(x) = p(x1) p(x2 | x1) ... p(xD | x<D).\n      hidden_order: Order of degrees to the hidden units: \'random\',\n        \'left-to-right\'. If \'left-to-right\', hidden units are allocated equally\n        (up to a remainder term) to each degree.\n      activation: Activation function.\n      use_bias: Whether to use a bias.\n      **kwargs: Keyword arguments of parent class.\n    """"""\n    super(MADE, self).__init__(**kwargs)\n    self.units = int(units)\n    self.hidden_dims = hidden_dims\n    self.input_order = input_order\n    self.hidden_order = hidden_order\n    self.activation = tf.keras.activations.get(activation)\n    self.use_bias = use_bias\n    self.network = tf.keras.Sequential([])\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    length = input_shape[-2]\n    channels = input_shape[-1]\n    if length is None or channels is None:\n      raise ValueError(\'The two last dimensions of the inputs to \'\n                       \'`MADE` should be defined. Found `None`.\')\n    masks = create_masks(input_dim=length,\n                         hidden_dims=self.hidden_dims,\n                         input_order=self.input_order,\n                         hidden_order=self.hidden_order)\n\n    # Input-to-hidden layer: [..., length, channels] -> [..., hidden_dims[0]].\n    self.network.add(tf.keras.layers.Reshape([length * channels]))\n    # Tile the mask so each element repeats contiguously; this is compatible\n    # with the autoregressive contraints unlike naive tiling.\n    mask = masks[0]\n    mask = tf.tile(mask[:, tf.newaxis, :], [1, channels, 1])\n    mask = tf.reshape(mask, [mask.shape[0] * channels, mask.shape[-1]])\n    if self.hidden_dims:\n      layer = tf.keras.layers.Dense(\n          self.hidden_dims[0],\n          kernel_initializer=make_masked_initializer(mask),\n          kernel_constraint=make_masked_constraint(mask),\n          activation=self.activation,\n          use_bias=self.use_bias)\n      self.network.add(layer)\n\n    # Hidden-to-hidden layers: [..., hidden_dims[l-1]] -> [..., hidden_dims[l]].\n    for l in range(1, len(self.hidden_dims)):\n      layer = tf.keras.layers.Dense(\n          self.hidden_dims[l],\n          kernel_initializer=make_masked_initializer(masks[l]),\n          kernel_constraint=make_masked_constraint(masks[l]),\n          activation=self.activation,\n          use_bias=self.use_bias)\n      self.network.add(layer)\n\n    # Hidden-to-output layer: [..., hidden_dims[-1]] -> [..., length, units].\n    # Tile the mask so each element repeats contiguously; this is compatible\n    # with the autoregressive contraints unlike naive tiling.\n    if self.hidden_dims:\n      mask = masks[-1]\n    mask = tf.tile(mask[..., tf.newaxis], [1, 1, self.units])\n    mask = tf.reshape(mask, [mask.shape[0], mask.shape[1] * self.units])\n    layer = tf.keras.layers.Dense(\n        length * self.units,\n        kernel_initializer=make_masked_initializer(mask),\n        kernel_constraint=make_masked_constraint(mask),\n        activation=None,\n        use_bias=self.use_bias)\n    self.network.add(layer)\n    self.network.add(tf.keras.layers.Reshape([length, self.units]))\n    self.built = True\n\n  def call(self, inputs):\n    return self.network(inputs)\n\n\ndef create_degrees(input_dim,\n                   hidden_dims,\n                   input_order=\'left-to-right\',\n                   hidden_order=\'left-to-right\'):\n  """"""Returns a list of degree vectors, one for each input and hidden layer.\n\n  A unit with degree d can only receive input from units with degree < d. Output\n  units always have the same degree as their associated input unit.\n\n  Args:\n    input_dim: Number of inputs.\n    hidden_dims: list with the number of hidden units per layer. It does not\n      include the output layer. Each hidden unit size must be at least the size\n      of length (otherwise autoregressivity is not possible).\n    input_order: Order of degrees to the input units: \'random\', \'left-to-right\',\n      \'right-to-left\', or an array of an explicit order. For example,\n      \'left-to-right\' builds an autoregressive model\n      p(x) = p(x1) p(x2 | x1) ... p(xD | x<D).\n    hidden_order: Order of degrees to the hidden units: \'random\',\n      \'left-to-right\'. If \'left-to-right\', hidden units are allocated equally\n      (up to a remainder term) to each degree.\n  """"""\n  if (isinstance(input_order, str) and\n      input_order not in (\'random\', \'left-to-right\', \'right-to-left\')):\n    raise ValueError(\'Input order is not valid.\')\n  if hidden_order not in (\'random\', \'left-to-right\'):\n    raise ValueError(\'Hidden order is not valid.\')\n\n  degrees = []\n  if isinstance(input_order, str):\n    input_degrees = np.arange(1, input_dim + 1)\n    if input_order == \'right-to-left\':\n      input_degrees = np.flip(input_degrees, 0)\n    elif input_order == \'random\':\n      np.random.shuffle(input_degrees)\n  else:\n    input_order = np.array(input_order)\n    if np.all(np.sort(input_order) != np.arange(1, input_dim + 1)):\n      raise ValueError(\'invalid input order\')\n    input_degrees = input_order\n  degrees.append(input_degrees)\n\n  for units in hidden_dims:\n    if hidden_order == \'random\':\n      min_prev_degree = min(np.min(degrees[-1]), input_dim - 1)\n      hidden_degrees = np.random.randint(\n          low=min_prev_degree, high=input_dim, size=units)\n    elif hidden_order == \'left-to-right\':\n      hidden_degrees = (np.arange(units) % max(1, input_dim - 1) +\n                        min(1, input_dim - 1))\n    degrees.append(hidden_degrees)\n  return degrees\n\n\ndef create_masks(input_dim,\n                 hidden_dims,\n                 input_order=\'left-to-right\',\n                 hidden_order=\'left-to-right\'):\n  """"""Returns a list of binary mask matrices respecting autoregressive ordering.\n\n  Args:\n    input_dim: Number of inputs.\n    hidden_dims: list with the number of hidden units per layer. It does not\n      include the output layer; those number of units will always be set to\n      input_dim downstream. Each hidden unit size must be at least the size of\n      length (otherwise autoregressivity is not possible).\n    input_order: Order of degrees to the input units: \'random\', \'left-to-right\',\n      \'right-to-left\', or an array of an explicit order. For example,\n      \'left-to-right\' builds an autoregressive model\n      p(x) = p(x1) p(x2 | x1) ... p(xD | x<D).\n    hidden_order: Order of degrees to the hidden units: \'random\',\n      \'left-to-right\'. If \'left-to-right\', hidden units are allocated equally\n      (up to a remainder term) to each degree.\n  """"""\n  degrees = create_degrees(input_dim, hidden_dims, input_order, hidden_order)\n  masks = []\n  # Create input-to-hidden and hidden-to-hidden masks.\n  for input_degrees, output_degrees in zip(degrees[:-1], degrees[1:]):\n    mask = tf.cast(input_degrees[:, np.newaxis] <= output_degrees, tf.float32)\n    masks.append(mask)\n\n  # Create hidden-to-output mask.\n  mask = tf.cast(degrees[-1][:, np.newaxis] < degrees[0], tf.float32)\n  masks.append(mask)\n  return masks\n\n\ndef make_masked_initializer(mask):\n  initializer = tf.keras.initializers.GlorotUniform()\n  def masked_initializer(shape, dtype=None):\n    return mask * initializer(shape, dtype)\n  return masked_initializer\n\n\ndef make_masked_constraint(mask):\n  constraint = tf.identity\n  def masked_constraint(x):\n    return mask * constraint(x)\n  return masked_constraint\n'"
edward2/tensorflow/layers/made_test.py,7,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for MADE.""""""\n\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass MADETest(tf.test.TestCase):\n\n  def testMADELeftToRight(self):\n    np.random.seed(83243)\n    batch_size = 2\n    length = 3\n    channels = 1\n    units = 5\n    network = ed.layers.MADE(units, [4], activation=tf.nn.relu)\n    inputs = tf.zeros([batch_size, length, channels])\n    outputs = network(inputs)\n\n    num_weights = sum([np.prod(weight.shape) for weight in network.weights])\n    # Disable lint error for open-source. pylint: disable=g-generic-assert\n    self.assertEqual(len(network.weights), 4)\n    # pylint: enable=g-generic-assert\n    self.assertEqual(num_weights, (3*1*4 + 4) + (4*3*5 + 3*5))\n\n    self.assertAllEqual(outputs[:, 0, :], np.zeros((batch_size, units)))\n    self.assertEqual(outputs.shape, (batch_size, length, units))\n\n  def testMADERightToLeft(self):\n    np.random.seed(1328)\n    batch_size = 2\n    length = 3\n    channels = 5\n    units = 1\n    network = ed.layers.MADE(units, [4, 3],\n                             input_order=\'right-to-left\',\n                             activation=tf.nn.relu,\n                             use_bias=False)\n    inputs = tf.zeros([batch_size, length, channels])\n    outputs = network(inputs)\n\n    num_weights = sum([np.prod(weight.shape) for weight in network.weights])\n    # Disable lint error for open-source. pylint: disable=g-generic-assert\n    self.assertEqual(len(network.weights), 3)\n    # pylint: enable=g-generic-assert\n    self.assertEqual(num_weights, 3*5*4 + 4*3 + 3*3*1)\n\n    self.assertAllEqual(outputs[:, -1, :], np.zeros((batch_size, units)))\n    self.assertEqual(outputs.shape, (batch_size, length, units))\n\n  def testMADENoHidden(self):\n    np.random.seed(532)\n    batch_size = 2\n    length = 3\n    channels = 5\n    units = 4\n    network = ed.layers.MADE(units, [], input_order=\'left-to-right\')\n    inputs = tf.zeros([batch_size, length, channels])\n    outputs = network(inputs)\n\n    num_weights = sum([np.prod(weight.shape) for weight in network.weights])\n    # Disable lint error for open-source. pylint: disable=g-generic-assert\n    self.assertEqual(len(network.weights), 2)\n    # pylint: enable=g-generic-assert\n    self.assertEqual(num_weights, 3*5*3*4 + 3*4)\n\n    self.assertAllEqual(outputs[:, 0, :], np.zeros((batch_size, units)))\n    self.assertEqual(outputs.shape, (batch_size, length, units))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
edward2/tensorflow/layers/neural_process.py,44,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Neural process.""""""\n\nfrom edward2.tensorflow import generated_random_variables\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n\ndef batch_mlp(inputs, hidden_sizes):\n  """"""Apply MLP to the final axis of a 3D tensor.\n\n  Args:\n    inputs: input Tensor of shape [batch_size, n, d_in].\n    hidden_sizes: An iterable containing the hidden layer sizes of the MLP.\n\n  Returns:\n    Tensor of shape [batch_size, n, d_out] where d_out = output_sizes[-1].\n  """"""\n  inputs = tf.convert_to_tensor(inputs)\n  batch_size, _, filter_size = inputs.shape.as_list()\n  hidden = tf.reshape(inputs, (-1, filter_size))\n\n  for size in hidden_sizes[:-1]:\n    hidden = tf.keras.layers.Dense(size, activation=tf.nn.relu)(hidden)\n\n  output = tf.keras.layers.Dense(hidden_sizes[-1], activation=None)(hidden)\n  output = tf.reshape(output, (batch_size, -1, hidden_sizes[-1]))\n  return output\n\n\n# TODO(adityagrover): Reimplement using preexisting attention routines in T2T\ndef uniform_attention(q, v):\n  """"""Computes uniform attention. Equivalent to neural process.\n\n  Args:\n    q: queries. Tensor of shape [batch_size, m, d_k].\n    v: values. Tensor of shape [batch_size, n, d_v].\n\n  Returns:\n    Tensor of shape [batch_size, m, d_v].\n  """"""\n  total_points = tf.shape(q)[1]\n  rep = tf.reduce_mean(v, axis=1, keepdims=True)  # [batch_size, 1, d_v]\n  rep = tf.tile(rep, [1, total_points, 1])\n  return rep\n\n\ndef laplace_attention(q, k, v, scale, normalise):\n  """"""Computes laplace exponential attention.\n\n  Args:\n    q: queries. Tensor of shape [batch_size, m, d_k].\n    k: keys. Tensor of shape [batch_size, n, d_k].\n    v: values. Tensor of shape [batch_size, n, d_v].\n    scale: float that scales the L1 distance.\n    normalise: Boolean that determines whether weights sum to 1.\n\n  Returns:\n    Tensor of shape [batch_size, m, d_v].\n  """"""\n  k = tf.expand_dims(k, axis=1)  # [batch_size, 1, n, d_k]\n  q = tf.expand_dims(q, axis=2)  # [batch_size, m, 1, d_k]\n  unnorm_weights = - tf.abs((k - q) / scale)  # [batch_size, m, n, d_k]\n  unnorm_weights = tf.reduce_sum(unnorm_weights, axis=-1)  # [batch_size, m, n]\n  if normalise:\n    weight_fn = tf.nn.softmax\n  else:\n    weight_fn = lambda x: 1 + tf.tanh(x)\n  weights = weight_fn(unnorm_weights)  # [batch_size, m, n]\n  rep = tf.einsum(\'bik,bkj->bij\', weights, v)  # [batch_size, m, d_v]\n  return rep\n\n\ndef dot_product_attention(q, k, v, normalise):\n  """"""Computes dot product attention.\n\n  Args:\n    q: queries. Tensor of  shape [batch_size, m, d_k].\n    k: keys. Tensor of shape [batch_size, n, d_k].\n    v: values. Tensor of shape [batch_size, n, d_v].\n    normalise: Boolean that determines whether weights sum to 1.\n\n  Returns:\n    Tensor of shape [batch_size, m, d_v].\n  """"""\n  d_k = tf.shape(q)[-1]\n  scale = tf.sqrt(tf.cast(d_k, tf.float32))\n  unnorm_weights = tf.einsum(\'bjk,bik->bij\', k, q) / scale  # [batch_size,m,n]\n  if normalise:\n    weight_fn = tf.nn.softmax\n  else:\n    weight_fn = tf.sigmoid\n  weights = weight_fn(unnorm_weights)  # [batch_size,m,n]\n  rep = tf.einsum(\'bik,bkj->bij\', weights, v)  # [batch_size,m,d_v]\n  return rep\n\n\ndef multihead_attention(q, k, v, num_heads=8):\n  """"""Computes multi-head attention.\n\n  Args:\n    q: queries. Tensor of  shape [batch_size, m, d_k].\n    k: keys. Tensor of shape [batch_size, n, d_k].\n    v: values. Tensor of shape [batch_size, n, d_v].\n    num_heads: number of heads. Should divide d_v.\n\n  Returns:\n    Tensor of shape [batch_size, m, d_v].\n  """"""\n  d_k = q.shape.as_list()[-1]\n  d_v = v.shape.as_list()[-1]\n  head_size = int(d_v / num_heads)\n  key_initializer = tf.keras.initializers.RandomNormal(stddev=d_k**-0.5)\n  value_initializer = tf.keras.initializers.RandomNormal(stddev=d_v**-0.5)\n  rep = tf.constant(0.0)\n  for h in range(num_heads):\n    o = dot_product_attention(\n        tf.keras.layers.Conv1D(\n            head_size, 1, kernel_initializer=key_initializer,\n            name=\'wq%d\' % h, use_bias=False, padding=\'VALID\')(q),\n        tf.keras.layers.Conv1D(\n            head_size, 1, kernel_initializer=key_initializer,\n            name=\'wk%d\' % h, use_bias=False, padding=\'VALID\')(k),\n        tf.keras.layers.Conv1D(\n            head_size, 1, kernel_initializer=key_initializer,\n            name=\'wv%d\' % h, use_bias=False, padding=\'VALID\')(v),\n        normalise=True)\n    rep += tf.keras.layers.Conv1D(d_v, 1, kernel_initializer=value_initializer,\n                                  name=\'wo%d\' % h, use_bias=False,\n                                  padding=\'VALID\')(o)\n  return rep\n\n\n# TODO(adityagrover): Implement via T2T.\nclass Attention(object):\n  """"""The Attention module.""""""\n\n  def __init__(self, rep, output_sizes, att_type, scale=1., normalise=True,\n               num_heads=8):\n    """"""Creates a attention module.\n\n    Takes in context inputs, target inputs and\n    representations of each context input/output pair\n    to output an aggregated representation of the context data.\n\n    Args:\n      rep: transformation to apply to contexts before computing attention.\n          One of: [\'identity\', \'mlp\'].\n      output_sizes: list of number of hidden units per layer of mlp.\n          Used only if rep == \'mlp\'.\n      att_type: type of attention. One of the following:\n          [\'uniform\', \'laplace\', \'dot_product\', \'multihead\']\n      scale: scale of attention.\n      normalise: Boolean determining whether to:\n          1. apply softmax to weights so they sum to 1 across context pts or\n          2. apply custom transformation to have weights in [0, 1].\n      num_heads: number of heads for multihead.\n    """"""\n    self._rep = rep\n    self._output_sizes = output_sizes\n    self._type = att_type\n    self._scale = scale\n    self._normalise = normalise\n    if self._type == \'multihead\':\n      self._num_heads = num_heads\n\n  def __call__(self, x1, x2, r):\n    """"""Applies attention to create aggregated representation of r.\n\n    Args:\n      x1: Tensor of shape [B ,n1, d_x].\n      x2: Tensor of shape [batch_size, n2, d_x].\n      r: Tensor of shape [batch_size, n1, d].\n\n    Returns:\n      Tensor of shape [batch_size, n2, d]\n\n    Raises:\n      NameError: The argument for rep/type was invalid.\n    """"""\n    if self._rep == \'identity\':\n      k, q = (x1, x2)\n    elif self._rep == \'mlp\':\n      k = batch_mlp(x1, self._output_sizes)\n      q = batch_mlp(x2, self._output_sizes)\n    else:\n      raise NameError(""\'rep\' not among [\'identity\', \'mlp\']"")\n\n    if self._type == \'uniform\':\n      rep = uniform_attention(q, r)\n    elif self._type == \'laplace\':\n      rep = laplace_attention(q, k, r, self._scale, self._normalise)\n    elif self._type == \'dot_product\':\n      rep = dot_product_attention(q, k, r, self._normalise)\n    elif self._type == \'multihead\':\n      rep = multihead_attention(q, k, r, self._num_heads)\n    else:\n      raise NameError((""\'att_type\' not among [\'uniform\', \'laplace\', ""\n                       ""\'dot_product\', \'multihead\']""))\n\n    return rep\n\n\n# TODO(adityagrover): Make the encoder and decoder configurable.\nclass NeuralProcess(tf.keras.Model):\n  """"""Attentive Neural Process (Kim et al., 2019; Garnelo et al., 2018).""""""\n\n  def __init__(self,\n               latent_encoder_sizes,\n               num_latents,\n               decoder_sizes,\n               use_deterministic_path=True,\n               deterministic_encoder_sizes=None,\n               attention_wrapper=None):\n    """"""Initializes the Neural Process model.\n\n    Args:\n      latent_encoder_sizes: (list of ints) Hidden layer sizes for latent\n          encoder.\n      num_latents: (int) Dimensionality of global latent variable.\n      decoder_sizes: (list of ints) Hidden layer sizes for decoder\n      use_deterministic_path: (bool) Uses deterministic encoder as well if True.\n      deterministic_encoder_sizes: (list of ints) Hidden layer sizes for\n          deterministic encoder.\n      attention_wrapper: Instance of Attention class to apply for\n          determinitic encoder embedding.\n    """"""\n    super(NeuralProcess, self).__init__()\n    self._num_latents = num_latents\n    self._latent_encoder_sizes = latent_encoder_sizes\n    self._deterministic_encoder_sizes = deterministic_encoder_sizes\n    self._decoder_sizes = decoder_sizes\n    self._use_deterministic_path = use_deterministic_path\n    self._attention = attention_wrapper\n\n  def latent_encoder(self, x, y):\n    """"""Encodes the inputs into one representation.\n\n    Args:\n      x: Tensor of shape [batch_size, observations, d_x]. For the prior, these\n         are context x-values. For the posterior, these are target x-values.\n      y: Tensor of shape [batch_size, observations, d_y]. For the prior, these\n         are context y-values. For the posterior, these are target y-values.\n\n    Returns:\n      A normal distribution over tensors of shape [batch_size, num_latents].\n    """"""\n    encoder_input = tf.concat([x, y], axis=-1)\n    per_example_embedding = batch_mlp(\n        encoder_input, self._latent_encoder_sizes)\n    dataset_embedding = tf.reduce_mean(per_example_embedding, axis=1)\n    hidden = tf.keras.layers.Dense(\n        (self._latent_encoder_sizes[-1] + self._num_latents)//2,\n        activation=tf.nn.relu)(dataset_embedding)\n    loc = tf.keras.layers.Dense(self._num_latents, activation=None)(hidden)\n    untransformed_scale = tf.keras.layers.Dense(self._num_latents,\n                                                activation=None)(hidden)\n    # Constraint scale following Garnelo et al. (2018).\n    scale_diag = 0.1 + 0.9 * tf.sigmoid(untransformed_scale)\n    return generated_random_variables.MultivariateNormalDiag(\n        loc=loc, scale_diag=scale_diag)\n\n  def deterministic_encoder(self, context_x, context_y, target_x):\n    """"""Encodes the inputs into one representation.\n\n    Args:\n      context_x: Tensor of shape [batch_size, observations, d_x].\n        Observed x-values.\n      context_y: Tensor of shape [batch_size, observations, d_y].\n        Observed y-values.\n      target_x: Tensor of shape [batch_size, target_observations, d_x].\n        Target x-values.\n\n    Returns:\n      Encodings. Tensor of shape [batch_size, target_observations, d].\n    """"""\n    encoder_input = tf.concat([context_x, context_y], axis=-1)\n    per_example_embedding = batch_mlp(encoder_input,\n                                      self._deterministic_encoder_sizes)\n    per_target_embedding = self._attention(context_x,\n                                           target_x,\n                                           per_example_embedding)\n    return per_target_embedding\n\n  def decoder(self, representation, target_x):\n    """"""Decodes the individual targets.\n\n    Args:\n      representation: The representation of the context for target predictions.\n          Tensor of shape [batch_size, target_observations, ?].\n      target_x: The x locations for the target query.\n          Tensor of shape [batch_size, target_observations, d_x].\n\n    Returns:\n      dist: A multivariate Gaussian over the target points. A distribution over\n          tensors of shape [batch_size, target_observations, d_y].\n    """"""\n    decoder_input = tf.concat([representation, target_x], axis=-1)\n    hidden = batch_mlp(decoder_input, self._decoder_sizes)\n    loc, untransformed_scale = tf.split(hidden, 2, axis=-1)\n    scale_diag = 0.1 + 0.9 * tf.nn.softplus(untransformed_scale)\n    return tfp.distributions.MultivariateNormalDiag(loc=loc,\n                                                    scale_diag=scale_diag)\n\n  def __call__(self, query, target_y=None):\n    """"""Returns the predicted mean and variance at the target points.\n\n    Args:\n      query: Nested tuple containing ((context_x, context_y), target_x) where:\n              context_x is Tensor of shape [batch_size, num_contexts, d_x].\n                  Contains the x values of the context points.\n              context_y is Tensor of shape [batch_size, num_contexts, d_y].\n                  Contains the y values of the context points.\n              target_x is Tensor of shape [batch_size, num_targets, d_x].\n                  Contains the x values of the target points.\n      target_y: The ground truth y values of the target y.\n          Tensor of shape [batch_size, num_targets, d_y].\n\n    Returns:\n      predictive_dist: Predictive posterior distribution over the predicted y.\n    """"""\n\n    (context_x, context_y), target_x = query\n    num_targets = tf.shape(target_x)[1]\n    prior = self.latent_encoder(context_x, context_y)\n\n    # For training, when target_y is available, use targets for latent encoder.\n    # Note that targets contain contexts by design.\n    # For testing, when target_y unavailable, use contexts for latent encoder.\n    if target_y is None:\n      latent_rep = prior\n    else:\n      posterior = self.latent_encoder(target_x, target_y)\n      latent_rep = posterior\n    latent_rep = tf.tile(tf.expand_dims(latent_rep, axis=1),\n                         [1, num_targets, 1])\n    if self._use_deterministic_path:\n      deterministic_rep = self.deterministic_encoder(context_x,\n                                                     context_y,\n                                                     target_x)\n      representation = tf.concat([deterministic_rep, latent_rep], axis=-1)\n    else:\n      representation = latent_rep\n\n    predictive_dist = self.decoder(representation, target_x)\n\n    if target_y is not None:\n      kl = tf.expand_dims(\n          posterior.distribution.kl_divergence(prior.distribution),\n          -1)\n      self.add_loss(lambda: kl)\n\n    return predictive_dist\n\n  call = __call__\n'"
edward2/tensorflow/layers/neural_process_test.py,9,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for Neural processes.""""""\n\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\ndef train_neural_process(model,\n                         train_data,\n                         valid_data,\n                         num_epochs,\n                         batch_size,\n                         learning_rate=1e-4):\n  """"""Trains the NeuralProcess model.\n\n  Validation data is used for early stopping,\n\n  Args:\n    model: A NeuralProcess Model subclassing Keras model.\n    train_data: (4-tuple of tensors) Values of x and y for contexts and targets.\n    valid_data: 4-tuple of tensors) Values of x and y for contexts and targets.\n    num_epochs: (int) Number of epochs to train the model for.\n    batch_size: (int) Size of batch.\n    learning_rate: (float) Learning rate for Adam optimizer.\n\n  Returns:\n    best_loss: (float) Average validation loss of best early-stopped model.\n  """"""\n  optimizer = tf.keras.optimizers.Adam(learning_rate)\n  context_x, context_y, target_x, target_y = train_data\n  valid_context_x, valid_context_y, valid_target_x, valid_target_y = valid_data\n  train_data_size = target_x.shape[0]\n  num_updates_per_epoch = train_data_size//batch_size\n  best_loss = np.inf\n  valid_query = (valid_context_x, valid_context_y), valid_target_x\n\n  for _ in range(num_epochs):\n    for i in range(num_updates_per_epoch):\n      start_idx, end_idx = batch_size*i, batch_size*(i+1)\n      batch_query = ((context_x[start_idx:end_idx],\n                      context_y[start_idx:end_idx]),\n                     target_x[start_idx:end_idx])\n      batch_target_y = target_y[start_idx:end_idx]\n      num_targets = tf.shape(batch_target_y)[1]\n      with tf.GradientTape() as tape:\n        predictive_dist = model(batch_query, batch_target_y)\n        log_p = predictive_dist.log_prob(batch_target_y)\n        kl = tf.tile(model.losses[-1], [1, num_targets])\n        loss = -tf.reduce_mean(log_p - kl/tf.cast(num_targets, tf.float32))\n      gradients = tape.gradient(loss, model.trainable_variables)\n      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    predictive_dist = model(valid_query, valid_target_y)\n    log_p = predictive_dist.log_prob(valid_target_y)\n    kl = tf.tile(model.losses[-1], [1, tf.shape(valid_target_y)[1]])\n    valid_loss = -tf.reduce_mean(log_p - kl/tf.cast(num_targets, tf.float32))\n    if valid_loss < best_loss:\n      best_loss = valid_loss\n\n  return best_loss\n\n\nclass NeuralProcessTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create a dummy multi-task fake dataset\n    num_train_problems = 32\n    num_valid_problems = 32\n    num_targets = 50\n    num_contexts = 10\n    input_dim = 5\n\n    def _create_fake_dataset(num_problems):\n      target_x = np.random.rand(num_problems,\n                                num_targets,\n                                input_dim).astype(np.float32)\n      target_y = np.random.rand(num_problems,\n                                num_targets,\n                                1).astype(np.float32)\n      context_x, context_y = (target_x[:, :num_contexts, :],\n                              target_y[:, :num_contexts, :])\n      return (context_x, context_y, target_x, target_y)\n\n    self.train_data = _create_fake_dataset(num_train_problems)\n    self.valid_data = _create_fake_dataset(num_valid_problems)\n\n    hidden_size = 128\n    num_latents = 16\n\n    np_attention_wrapper = ed.layers.Attention(\n        rep=\'identity\', output_sizes=None, att_type=\'uniform\')\n    self.np_model = ed.layers.NeuralProcess(\n        latent_encoder_sizes=[hidden_size]*4,\n        num_latents=num_latents,\n        decoder_sizes=[hidden_size]*2 + [2],\n        use_deterministic_path=True,\n        deterministic_encoder_sizes=[hidden_size]*4,\n        attention_wrapper=np_attention_wrapper)\n\n    anp_attention_wrapper = ed.layers.Attention(\n        rep=\'mlp\', output_sizes=[hidden_size]*2, att_type=\'multihead\')\n    self.anp_model = ed.layers.NeuralProcess(\n        latent_encoder_sizes=[hidden_size]*4,\n        num_latents=num_latents,\n        decoder_sizes=[hidden_size]*2 + [2],\n        use_deterministic_path=True,\n        deterministic_encoder_sizes=[hidden_size]*4,\n        attention_wrapper=anp_attention_wrapper)\n\n    self.models = [self.np_model, self.anp_model]\n    self.num_latents, self.hidden_size, self.num_targets = (num_latents,\n                                                            hidden_size,\n                                                            num_targets)\n    super(NeuralProcessTest, self).setUp()\n\n  def testTermination(self):\n    for model in self.models:\n      validation_loss = train_neural_process(\n          model,\n          self.train_data,\n          self.valid_data,\n          num_epochs=2,\n          batch_size=16,\n          learning_rate=1e-4)\n\n      self.assertGreaterEqual(validation_loss, 0.)\n\n  def testLatentEncoder(self):\n    valid_context_x, valid_context_y, _, _ = self.valid_data\n    batch_size = valid_context_x.shape[0]\n\n    for model in self.models:\n      dist = model.latent_encoder(valid_context_x, valid_context_y).distribution\n      self.assertEqual(dist.loc.shape, (batch_size, self.num_latents))\n      self.assertEqual(dist.scale.shape,\n                       (batch_size, self.num_latents, self.num_latents))\n\n  def testDeterministicEncoder(self):\n    valid_context_x, valid_context_y, valid_target_x, _ = self.valid_data\n    batch_size = valid_context_x.shape[0]\n\n    for model in self.models:\n      embedding = model.deterministic_encoder(\n          valid_context_x, valid_context_y, valid_target_x)\n      self.assertEqual(embedding.shape, (batch_size, self.num_targets,\n                                         self.hidden_size))\n\n  def testCall(self):\n    (valid_context_x,\n     valid_context_y,\n     valid_target_x,\n     valid_target_y) = self.valid_data\n    batch_size = valid_context_x.shape[0]\n\n    for model in self.models:\n      query = (valid_context_x, valid_context_y), valid_target_x\n      # test \'training\' when target_y is available\n      predictive_dist = model(query, valid_target_y)\n      self.assertEqual(predictive_dist.loc.shape, (batch_size, self.num_targets,\n                                                   1))\n      self.assertEqual(predictive_dist.scale.shape,\n                       (batch_size, self.num_targets, 1, 1))\n      self.assertAllGreaterEqual(model.losses, 0.)\n\n      # test \'testing\' when target_y is unavailable\n      predictive_dist = model(query)\n      self.assertEqual(predictive_dist.loc.shape, (batch_size, self.num_targets,\n                                                   1))\n      self.assertEqual(predictive_dist.scale.shape,\n                       (batch_size, self.num_targets, 1, 1))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
edward2/tensorflow/layers/noise.py,33,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Bayesian noise layers.""""""\n\nfrom edward2.tensorflow import generated_random_variables\nfrom edward2.tensorflow import random_variable\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n\nclass NCPNormalPerturb(tf.keras.layers.Layer):\n  """"""Noise contrastive prior for continuous inputs (Hafner et al., 2018).\n\n  The layer doubles the inputs\' batch size and adds a random normal perturbation\n  to the concatenated second batch. This acts an input prior to be used in\n  combination with an output prior. The output prior reduces the second batch\n  (reverting to the inputs\' original shape) and computes a regularizer that\n  matches the second batch towards some output (e.g., uniform distribution).\n  This layer implementation is inspired by the Aboleth library.\n\n  #### Examples\n\n  Below implements neural network regression with heteroskedastic noise,\n  noise contrastive priors, and being Bayesian only at the mean\'s output layer.\n\n  ```python\n  batch_size, dataset_size = 128, 1000\n  features, labels = get_some_dataset()\n\n  inputs = keras.Input(shape=(25,))\n  x = ed.layers.NCPNormalPerturb()(inputs)  # double input batch\n  x = tf.keras.layers.Dense(64, activation=\'relu\')(x)\n  x = tf.keras.layers.Dense(64, activation=\'relu\')(x)\n  means = ed.layers.DenseVariationalDropout(1, activation=None)(x)  # get mean\n  means = ed.layers.NCPNormalOutput(labels)(means)  # halve input batch\n  stddevs = tf.keras.layers.Dense(1, activation=\'softplus\')(x[:batch_size])\n  outputs = tf.keras.layers.Lambda(lambda x: ed.Normal(x[0], x[1]))([means,\n                                                                     stddevs])\n  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n  # Run training loop.\n  num_steps = 1000\n  for _ in range(num_steps):\n    with tf.GradientTape() as tape:\n      predictions = model(features)\n      loss = tf.reduce_mean(predictions.distribution.log_prob(labels))\n      loss += model.losses[0] / dataset_size  # KL regularizer for output layer\n      loss += model.losses[-1]\n    gradients = tape.gradient(loss, model.variables)  # use any optimizer here\n  ```\n\n  The network applies `ed.layers.NCPNormalPerturb()` to double the input batch\n  size and add Gaussian noise to the second half; then feedforward layers; then\n  `ed.layers.DenseVariational` to be Bayesian about the output density\'s mean;\n  then `ed.layers.NCPNormalOutput` centered at the labels to revert to the batch\n  size and compute a loss on the second half; then parameterize the output\n  density\'s standard deviations; then compute the total loss function as the sum\n  of the model\'s negative log-likelihood, KL divergence for the Bayesian mean\n  layer, and NCP loss.\n  """"""\n\n  def __init__(self, mean=0., stddev=1., seed=None, **kwargs):\n    self.mean = mean\n    self.stddev = stddev\n    self.seed = seed\n    super(NCPNormalPerturb, self).__init__(**kwargs)\n\n  def call(self, inputs):\n    noise = tf.random.normal(tf.shape(inputs),\n                             mean=self.mean,\n                             stddev=self.stddev,\n                             dtype=inputs.dtype,\n                             seed=self.seed)\n    perturbed_inputs = inputs + noise\n    return tf.concat([inputs, perturbed_inputs], 0)\n\n\nclass NCPCategoricalPerturb(tf.keras.layers.Layer):\n  """"""Noise contrastive prior for discrete inputs (Hafner et al., 2018).\n\n  The layer doubles the inputs\' batch size and randomly flips categories\n  for the concatenated second batch (all features must be integer-valued). This\n  acts an input prior to be used in combination with an output prior. The output\n  prior reduces the second batch (reverting to the inputs\' original shape) and\n  computes a regularizer that matches the second batch towards some output\n  (e.g., uniform distribution). This layer implementation is inspired by the\n  Aboleth library.\n\n  #### Examples\n\n  Below implements neural network regression with heteroskedastic noise,\n  noise contrastive priors, and being Bayesian only at the mean\'s output layer.\n\n  ```python\n  batch_size, dataset_size = 128, 1000\n  features, labels = get_some_dataset()\n\n  inputs = keras.Input(shape=(25,))\n  x = ed.layers.NCPCategoricalPerturb(10)(inputs)  # double input batch\n  x = tf.keras.layers.Dense(64, activation=\'relu\')(x)\n  x = tf.keras.layers.Dense(64, activation=\'relu\')(x)\n  means = ed.layers.DenseVariationalDropout(1, activation=None)(x)  # get mean\n  means = ed.layers.NCPNormalOutput(labels)(means)  # halve input batch\n  stddevs = tf.keras.layers.Dense(1, activation=\'softplus\')(x[:batch_size])\n  outputs = tf.keras.layers.Lambda(lambda x: ed.Normal(x[0], x[1]))([means,\n                                                                     stddevs])\n  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n  # Run training loop.\n  num_steps = 1000\n  for _ in range(num_steps):\n    with tf.GradientTape() as tape:\n      predictions = model(features)\n      loss = tf.reduce_mean(predictions.distribution.log_prob(labels))\n      loss += model.losses[0] / dataset_size  # KL regularizer for output layer\n      loss += model.losses[-1]\n    gradients = tape.gradient(loss, model.variables)  # use any optimizer here\n  ```\n\n  The network applies `ed.layers.NCPCategoricalPerturb()` to double the input\n  batch size and flip categories for the second half; then feedforward layers;\n  then `ed.layers.DenseVariational` to be Bayesian about the output density\'s\n  mean; then `ed.layers.NCPNormalOutput` centered at the labels to revert to the\n  batch size and compute a loss on the second half; then parameterize the output\n  density\'s standard deviations; then compute the total loss function as the sum\n  of the model\'s negative log-likelihood, KL divergence for the Bayesian mean\n  layer, and NCP loss.\n  """"""\n\n  def __init__(self, input_dim, probs=0.1, **kwargs):\n    """"""Creates layer.\n\n    Args:\n      input_dim: int > 0. Size of the category, i.e. maximum integer index + 1.\n      probs: Probability that a category is randomly flipped.\n      **kwargs: kwargs to parent class.\n    """"""\n    self.input_dim = input_dim\n    self.probs = probs\n    super(NCPCategoricalPerturb, self).__init__(**kwargs)\n\n  def call(self, inputs):\n    mask = tf.cast(tf.random.uniform(tf.shape(inputs)) <= self.probs,\n                   inputs.dtype)\n    flips = tf.random.uniform(\n        tf.shape(inputs), minval=0, maxval=self.input_dim, dtype=inputs.dtype)\n    flipped_inputs = mask * flips + (1 - mask) * inputs\n    return tf.concat([inputs, flipped_inputs], 0)\n\n\nclass NCPNormalOutput(tf.keras.layers.Layer):\n  """"""Noise contrastive prior for continuous outputs (Hafner et al., 2018).\n\n  The layer returns the first half of the inputs\' batch. It computes a KL\n  regularizer as a side-effect, which matches the inputs\' second half towards a\n  normal distribution (the output prior), and averaged over the number of inputs\n  in the second half. This layer is typically in combination with an input prior\n  which doubles the batch. This layer implementation is inspired by the Aboleth\n  library.\n\n  The layer computes the exact KL divergence from a normal distribution to\n  the input RandomVariable. It is an unbiased estimate if the input\n  RandomVariable has random parameters. If the input is a Tensor, then it\n  assumes its density is `ed.Normal(input, 1.)`, i.e., mean squared error loss.\n\n  #### Examples\n\n  Below implements neural network regression with heteroskedastic noise,\n  noise contrastive priors, and being Bayesian only at the mean\'s output layer.\n\n  ```python\n  batch_size, dataset_size = 128, 1000\n  features, labels = get_some_dataset()\n\n  inputs = keras.Input(shape=(25,))\n  x = ed.layers.NCPNormalPerturb()(inputs)  # double input batch\n  x = tf.keras.layers.Dense(64, activation=\'relu\')(x)\n  x = tf.keras.layers.Dense(64, activation=\'relu\')(x)\n  means = ed.layers.DenseVariationalDropout(1, activation=None)(x)  # get mean\n  means = ed.layers.NCPNormalOutput(labels)(means)  # halve input batch\n  stddevs = tf.keras.layers.Dense(1, activation=\'softplus\')(x[:batch_size])\n  outputs = tf.keras.layers.Lambda(lambda x: ed.Normal(x[0], x[1]))([means,\n                                                                     stddevs])\n  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n  # Run training loop.\n  num_steps = 1000\n  for _ in range(num_steps):\n    with tf.GradientTape() as tape:\n      predictions = model(features)\n      loss = tf.reduce_mean(predictions.distribution.log_prob(labels))\n      loss += model.losses[0] / dataset_size  # KL regularizer for output layer\n      loss += model.losses[-1]\n    gradients = tape.gradient(loss, model.variables)  # use any optimizer here\n  ```\n\n  The network applies `ed.layers.NCPNormalPerturb()` to double the input batch\n  size and add Gaussian noise to the second half; then feedforward layers; then\n  `ed.layers.DenseVariational` to be Bayesian about the output density\'s mean;\n  then `ed.layers.NCPNormalOutput` centered at the labels to revert to the batch\n  size and compute a loss on the second half; then parameterize the output\n  density\'s standard deviations; then compute the total loss function as the sum\n  of the model\'s negative log-likelihood, KL divergence for the Bayesian mean\n  layer, and NCP loss.\n  """"""\n\n  def __init__(self, mean=0., stddev=1., **kwargs):\n    self.mean = mean\n    self.stddev = stddev\n    super(NCPNormalOutput, self).__init__(**kwargs)\n\n  def call(self, inputs):\n    if not isinstance(inputs, random_variable.RandomVariable):\n      # Default to a unit normal, i.e., derived from mean squared error loss.\n      inputs = generated_random_variables.Normal(loc=inputs, scale=1.)\n    batch_size = tf.shape(inputs)[0] // 2\n    # TODO(trandustin): Depend on github\'s ed2 for indexing RVs. This is a hack.\n    # _, _ = inputs[:batch_size], inputs[batch_size:]\n    original_inputs = random_variable.RandomVariable(\n        inputs.distribution[:batch_size],\n        value=inputs.value[:batch_size])\n    perturbed_inputs = random_variable.RandomVariable(\n        inputs.distribution[batch_size:],\n        value=inputs.value[batch_size:])\n    loss = tf.reduce_sum(\n        tfp.distributions.Normal(self.mean, self.stddev).kl_divergence(\n            perturbed_inputs.distribution))\n    loss /= tf.cast(batch_size, dtype=tf.float32)\n    self.add_loss(loss)\n    return original_inputs\n'"
edward2/tensorflow/layers/noise_test.py,8,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for Bayesian noise layers.""""""\n\nfrom absl.testing import parameterized\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass NoiseTest(parameterized.TestCase, tf.test.TestCase):\n\n  def testNCPNormalPerturb(self):\n    batch_size = 3\n    inputs = tf.cast(np.random.rand(batch_size, 4), dtype=tf.float32)\n    model = ed.layers.NCPNormalPerturb()\n    outputs = model(inputs)\n    self.assertEqual(outputs.shape, (2 * batch_size, 4))\n    self.assertAllEqual(inputs, outputs[:batch_size])\n\n  def testNCPCategoricalPerturb(self):\n    input_dim = 5\n    batch_size = 3\n    inputs = tf.cast(np.random.choice(input_dim, size=(batch_size, 4)),\n                     dtype=tf.float32)\n    model = ed.layers.NCPCategoricalPerturb(input_dim)\n    outputs = model(inputs)\n    self.assertEqual(outputs.shape, (2 * batch_size, 4))\n    self.assertAllEqual(inputs, outputs[:batch_size])\n\n  def testNCPNormalOutput(self):\n    batch_size = 3\n    features = ed.Normal(loc=tf.random.normal([2 * batch_size, 1]), scale=1.)\n    labels = np.random.rand(batch_size).astype(np.float32)\n    model = ed.layers.NCPNormalOutput(mean=labels)\n    predictions = model(features)\n    self.assertLen(model.losses, 1)\n    self.assertAllEqual(tf.convert_to_tensor(features[:batch_size]),\n                        tf.convert_to_tensor(predictions))\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
edward2/tensorflow/layers/normalization.py,59,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Normalization layers.""""""\n\nfrom edward2.tensorflow import random_variable\nfrom edward2.tensorflow import transformed_random_variable\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tf1\n\n\nclass ActNorm(tf.keras.layers.Layer):\n  """"""Actnorm, an affine reversible layer (Prafulla and Kingma, 2018).\n\n  Weights use data-dependent initialization in which outputs have zero mean\n  and unit variance per channel (last dimension). The mean/variance statistics\n  are computed from the first batch of inputs.\n  """"""\n\n  def __init__(self, epsilon=tf.keras.backend.epsilon(), **kwargs):\n    super(ActNorm, self).__init__(**kwargs)\n    self.epsilon = epsilon\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    last_dim = input_shape[-1]\n    if last_dim is None:\n      raise ValueError(\'The last dimension of the inputs to `ActNorm` \'\n                       \'should be defined. Found `None`.\')\n    bias = self.add_weight(\'bias\', [last_dim], dtype=self.dtype)\n    log_scale = self.add_weight(\'log_scale\', [last_dim], dtype=self.dtype)\n    # Set data-dependent initializers.\n    bias = bias.assign(self.bias_initial_value)\n    with tf.control_dependencies([bias]):\n      self.bias = bias\n    log_scale = log_scale.assign(self.log_scale_initial_value)\n    with tf.control_dependencies([log_scale]):\n      self.log_scale = log_scale\n    self.built = True\n\n  def __call__(self, inputs, *args, **kwargs):\n    if not self.built:\n      mean, variance = tf.nn.moments(\n          inputs, axes=list(range(inputs.shape.ndims - 1)))\n      self.bias_initial_value = -mean\n      # TODO(trandustin): Optionally, actnorm multiplies log_scale by a fixed\n      # log_scale factor (e.g., 3.) and initializes by\n      # initial_value / log_scale_factor.\n      self.log_scale_initial_value = tf.math.log(\n          1. / (tf.sqrt(variance) + self.epsilon))\n\n    if not isinstance(inputs, random_variable.RandomVariable):\n      return super(ActNorm, self).__call__(inputs, *args, **kwargs)\n    return transformed_random_variable.TransformedRandomVariable(inputs, self)\n\n  def call(self, inputs):\n    return (inputs + self.bias) * tf.exp(self.log_scale)\n\n  def reverse(self, inputs):\n    return inputs * tf.exp(-self.log_scale) - self.bias\n\n  def log_det_jacobian(self, inputs):\n    """"""Returns log det | dx / dy | = num_events * sum log | scale |.""""""\n    # Number of events is number of all elements excluding the batch and\n    # channel dimensions.\n    num_events = tf.reduce_prod(tf.shape(inputs)[1:-1])\n    log_det_jacobian = num_events * tf.reduce_sum(self.log_scale)\n    return log_det_jacobian\n\n\ndef ensemble_batchnorm(x, ensemble_size=1, use_tpu=True, **kwargs):\n  """"""A modified batch norm layer for Batch Ensemble model.\n\n  Args:\n    x: input tensor.\n    ensemble_size: number of ensemble members.\n    use_tpu: whether the model is running on TPU.\n    **kwargs: Keyword arguments to batch normalization layers.\n\n  Returns:\n    Output tensor for the block.\n  """"""\n  # In BatchEnsemble inference stage, the input to the model is tiled which\n  # leads to dynamic shape because of the tf.split function. Such operation\n  # is not supported in tf2.0 on TPU. For current workaround, we use single\n  # BatchNormalization layer for all ensemble member. This is not correct in\n  # math but works in practice.\n  if ensemble_size == 1 or use_tpu:\n    return tf.keras.layers.BatchNormalization(**kwargs)(x)\n  name = kwargs.get(\'name\')\n  split_inputs = tf.split(x, ensemble_size, axis=0)\n  for i in range(ensemble_size):\n    if name is not None:\n      kwargs[\'name\'] = name + \'_{}\'.format(i)\n    split_inputs[i] = tf.keras.layers.BatchNormalization(**kwargs)(\n        split_inputs[i])\n  return tf.concat(split_inputs, axis=0)\n\n\nclass EnsembleSyncBatchNorm(tf.keras.layers.Layer):\n  """"""BatchNorm that averages over ALL replicas. Only works for `NHWC` inputs.""""""\n\n  def __init__(self, axis=3, ensemble_size=1, momentum=0.99, epsilon=0.001,\n               trainable=True, name=None, **kwargs):\n    super(EnsembleSyncBatchNorm, self).__init__(\n        trainable=trainable, name=name, **kwargs)\n    self.axis = axis\n    self.momentum = momentum\n    self.epsilon = epsilon\n    self.ensemble_size = ensemble_size\n\n  def build(self, input_shape):\n    """"""Build function.""""""\n    dim = input_shape[-1]\n    if self.ensemble_size > 1:\n      shape = [self.ensemble_size, dim]\n    else:\n      shape = [dim]\n\n    self.gamma = self.add_weight(\n        name=\'gamma\',\n        shape=shape,\n        dtype=self.dtype,\n        initializer=\'ones\',\n        trainable=True)\n\n    self.beta = self.add_weight(\n        name=\'beta\',\n        shape=shape,\n        dtype=self.dtype,\n        initializer=\'zeros\',\n        trainable=True)\n\n    self.moving_mean = self.add_weight(\n        name=\'moving_mean\',\n        shape=shape,\n        dtype=self.dtype,\n        initializer=\'zeros\',\n        synchronization=tf.VariableSynchronization.ON_READ,\n        trainable=False,\n        aggregation=tf.VariableAggregation.MEAN)\n\n    self.moving_variance = self.add_weight(\n        name=\'moving_variance\',\n        shape=shape,\n        dtype=self.dtype,\n        initializer=\'ones\',\n        synchronization=tf.VariableSynchronization.ON_READ,\n        trainable=False,\n        aggregation=tf.VariableAggregation.MEAN)\n\n  def _get_mean_and_variance(self, x):\n    """"""Cross-replica mean and variance.""""""\n    replica_context = tf.distribute.get_replica_context()\n\n    if replica_context is not None:\n      num_replicas_in_sync = replica_context.num_replicas_in_sync\n      if num_replicas_in_sync <= 8:\n        group_assignment = None\n        num_replicas_per_group = tf.cast(num_replicas_in_sync, tf.float32)\n      else:\n        num_replicas_per_group = max(8, num_replicas_in_sync // 8)\n        group_assignment = np.arange(num_replicas_in_sync, dtype=np.int32)\n        group_assignment = group_assignment.reshape(\n            [-1, num_replicas_per_group])\n        group_assignment = group_assignment.tolist()\n        num_replicas_per_group = tf.cast(num_replicas_per_group, tf.float32)\n\n    # This only supports NHWC format.\n    if self.ensemble_size > 1:\n      height = tf.shape(x)[1]\n      width = tf.shape(x)[2]\n      input_channels = tf.shape(x)[3]\n      x = tf.reshape(x, [self.ensemble_size, -1, height, width, input_channels])\n      mean = tf.reduce_mean(x, axis=[1, 2, 3])  # [ensemble_size, channels]\n      mean = tf.cast(mean, tf.float32)\n\n      # Var[x] = E[x^2] - E[x]^2\n      mean_sq = tf.reduce_mean(tf.square(x), axis=[1, 2, 3])\n      mean_sq = tf.cast(mean_sq, tf.float32)\n      if replica_context is not None:\n        mean = tf1.tpu.cross_replica_sum(mean, group_assignment)\n        mean = mean / num_replicas_per_group\n        mean_sq = tf1.tpu.cross_replica_sum(mean_sq, group_assignment)\n        mean_sq = mean_sq / num_replicas_per_group\n      variance = mean_sq - tf.square(mean)\n    else:\n      mean = tf.reduce_mean(x, axis=[0, 1, 2])\n      mean = tf.cast(mean, tf.float32)\n\n      mean_sq = tf.reduce_mean(tf.square(x), axis=[0, 1, 2])\n      mean_sq = tf.cast(mean_sq, tf.float32)\n      if replica_context is not None:\n        mean = tf1.tpu.cross_replica_sum(mean, group_assignment)\n        mean = mean / num_replicas_per_group\n        mean_sq = tf1.tpu.cross_replica_sum(mean_sq, group_assignment)\n        mean_sq = mean_sq / num_replicas_per_group\n      variance = mean_sq - tf.square(mean)\n\n    def _assign(moving, normal):\n      decay = tf.cast(1. - self.momentum, tf.float32)\n      diff = tf.cast(moving, tf.float32) - tf.cast(normal, tf.float32)\n      return moving.assign_sub(decay * diff)\n\n    self.add_update(_assign(self.moving_mean, mean))\n    self.add_update(_assign(self.moving_variance, variance))\n\n    mean = tf.cast(mean, x.dtype)\n    variance = tf.cast(variance, x.dtype)\n\n    return mean, variance\n\n  def call(self, inputs, training=None):\n    """"""Call function.""""""\n    if training:\n      mean, variance = self._get_mean_and_variance(inputs)\n    else:\n      mean, variance = self.moving_mean, self.moving_variance\n    if self.ensemble_size > 1:\n      batch_size = tf.shape(inputs)[0]\n      input_dim = tf.shape(mean)[-1]\n      examples_per_model = batch_size // self.ensemble_size\n      mean = tf.reshape(tf.tile(mean, [1, examples_per_model]),\n                        [batch_size, input_dim])\n      variance_epsilon = tf.cast(self.epsilon, variance.dtype)\n      inv = tf.math.rsqrt(variance + variance_epsilon)\n      if self.gamma is not None:\n        inv *= self.gamma\n      inv = tf.reshape(tf.tile(inv, [1, examples_per_model]),\n                       [batch_size, input_dim])\n      # Assuming channel last.\n      inv = tf.expand_dims(inv, axis=1)\n      inv = tf.expand_dims(inv, axis=1)\n      mean = tf.expand_dims(mean, axis=1)\n      mean = tf.expand_dims(mean, axis=1)\n      if self.beta is not None:\n        beta = tf.reshape(tf.tile(self.beta, [1, examples_per_model]),\n                          [batch_size, input_dim])\n        beta = tf.expand_dims(beta, axis=1)\n        beta = tf.expand_dims(beta, axis=1)\n      x = inputs * tf.cast(inv, inputs.dtype) + tf.cast(\n          beta - mean * inv if self.beta is not None else (\n              -mean * inv), inputs.dtype)\n    else:\n      x = tf.nn.batch_normalization(\n          inputs,\n          mean=mean,\n          variance=variance,\n          offset=self.beta,\n          scale=self.gamma,\n          variance_epsilon=tf.cast(self.epsilon, variance.dtype),\n      )\n    return x\n'"
edward2/tensorflow/layers/normalization_test.py,6,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for normalization layers.""""""\n\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass NormalizationTest(tf.test.TestCase):\n\n  def testActNorm(self):\n    np.random.seed(83243)\n    batch_size = 25\n    length = 15\n    channels = 4\n    inputs = 3. + 0.8 * np.random.randn(batch_size, length, channels)\n    inputs = tf.cast(inputs, tf.float32)\n    layer = ed.layers.ActNorm()\n    outputs = layer(inputs)\n    mean, variance = tf.nn.moments(outputs, axes=[0, 1])\n    self.assertAllClose(mean, np.zeros(channels), atol=1e-3)\n    self.assertAllClose(variance, np.ones(channels), atol=1e-3)\n\n    inputs = 3. + 0.8 * np.random.randn(batch_size, length, channels)\n    inputs = tf.cast(inputs, tf.float32)\n    outputs = layer(inputs)\n    mean, variance = tf.nn.moments(outputs, axes=[0, 1])\n    self.assertAllClose(mean, np.zeros(channels), atol=0.25)\n    self.assertAllClose(variance, np.ones(channels), atol=0.25)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
edward2/tensorflow/layers/recurrent.py,60,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Bayesian recurrent cells and layers.""""""\n\nfrom typing import List, Tuple, Union  # pylint:disable=g-bad-import-order\n\nfrom edward2.tensorflow import constraints\nfrom edward2.tensorflow import initializers\nfrom edward2.tensorflow import random_variable\nfrom edward2.tensorflow import regularizers\nfrom edward2.tensorflow.layers import utils\n\nimport tensorflow as tf\n\n\n@utils.add_weight\nclass LSTMCellReparameterization(tf.keras.layers.LSTMCell):\n  """"""Bayesian LSTM cell class estimated via reparameterization.\n\n  The layer computes a variational Bayesian approximation to the distribution\n  over LSTM cell functions,\n\n  ```\n  p(outputs | inputs) = int lstm_cell(inputs; weights, bias) p(weights, bias)\n    dweights dbias,\n  ```\n\n  where the weights consist of both input and recurrent weights.\n\n  It does this with a stochastic forward pass, sampling from learnable\n  distributions on the kernel, recurrent kernel, and bias. Gradients with\n  respect to the distributions\' learnable parameters backpropagate via\n  reparameterization.  Minimizing cross-entropy plus the layer\'s losses performs\n  variational minimum description length, i.e., it minimizes an upper bound to\n  the negative marginal likelihood.\n  """"""\n\n  def __init__(self,\n               units,\n               activation=\'tanh\',\n               recurrent_activation=\'sigmoid\',\n               use_bias=True,\n               kernel_initializer=\'trainable_normal\',\n               recurrent_initializer=\'trainable_normal\',\n               bias_initializer=\'zeros\',\n               unit_forget_bias=True,\n               kernel_regularizer=\'normal_kl_divergence\',\n               recurrent_regularizer=\'normal_kl_divergence\',\n               bias_regularizer=None,\n               kernel_constraint=None,\n               recurrent_constraint=None,\n               bias_constraint=None,\n               dropout=0.,\n               recurrent_dropout=0.,\n               implementation=2,\n               **kwargs):\n    self.called_weights = False\n    super(LSTMCellReparameterization, self).__init__(\n        units=units,\n        activation=activation,\n        recurrent_activation=recurrent_activation,\n        use_bias=use_bias,\n        kernel_initializer=initializers.get(kernel_initializer),\n        recurrent_initializer=initializers.get(recurrent_initializer),\n        bias_initializer=initializers.get(bias_initializer),\n        unit_forget_bias=unit_forget_bias,\n        kernel_regularizer=regularizers.get(kernel_regularizer),\n        recurrent_regularizer=regularizers.get(recurrent_regularizer),\n        bias_regularizer=regularizers.get(bias_regularizer),\n        kernel_constraint=constraints.get(kernel_constraint),\n        recurrent_constraint=constraints.get(recurrent_constraint),\n        bias_constraint=constraints.get(bias_constraint),\n        dropout=dropout,\n        recurrent_dropout=recurrent_dropout,\n        implementation=implementation,\n        **kwargs)\n\n  # TODO(dusenberrymw): TensorFlow has an open RFC\n  # (https://github.com/tensorflow/community/pull/208) for adding core TF types\n  # to the library such that the library and end users can upgrade to the use of\n  # type annotations. Once that RFC is accepted and implemented, we should\n  # update these types below.\n  def build(\n      self, input_shape: Union[tf.TensorShape, List[int], Tuple[int, ...]]\n  ) -> None:\n    input_shape = tf.TensorShape(input_shape)\n    input_dim = input_shape[-1]\n    self.kernel = self.add_weight(\n        shape=(input_dim, self.units * 4),\n        name=\'kernel\',\n        initializer=self.kernel_initializer,\n        regularizer=self.kernel_regularizer,\n        constraint=self.kernel_constraint)\n    self.recurrent_kernel = self.add_weight(\n        shape=(self.units, self.units * 4),\n        name=\'recurrent_kernel\',\n        initializer=self.recurrent_initializer,\n        regularizer=self.recurrent_regularizer,\n        constraint=self.recurrent_constraint)\n\n    if self.use_bias:\n      if (self.unit_forget_bias and not isinstance(self.bias_initializer,\n                                                   tf.keras.layers.Layer)):\n        def bias_initializer(_, *args, **kwargs):\n          return tf.keras.backend.concatenate([\n              self.bias_initializer((self.units,), *args, **kwargs),\n              tf.keras.initializers.Ones()((self.units,), *args, **kwargs),\n              self.bias_initializer((self.units * 2,), *args, **kwargs),\n          ])\n      else:\n        bias_initializer = self.bias_initializer\n      self.bias = self.add_weight(\n          shape=(self.units * 4,),\n          name=\'bias\',\n          initializer=bias_initializer,\n          regularizer=self.bias_regularizer,\n          constraint=self.bias_constraint)\n    else:\n      self.bias = None\n    self.built = True\n\n  def call(self, *args, **kwargs):\n    if not self.called_weights:\n      # Call weights if never called before. This ensures TF ops executed during\n      # the cell\'s first ever call (e.g., constraints applied to free parameters\n      # in the variational distribution) are recorded properly on any gradient\n      # tape. Unlike variational dense or convolutional layers, LSTM cell weight\n      # noise is reused across calls (i.e., timesteps). Call get_initial_state()\n      # or call_weights() explicitly to get a new sample of the weights.\n      self.call_weights()\n    return super(LSTMCellReparameterization, self).call(*args, **kwargs)\n\n  def call_weights(self):\n    """"""Calls any weights if the initializer is itself a layer.""""""\n    if isinstance(self.kernel_initializer, tf.keras.layers.Layer):\n      self.kernel = self.kernel_initializer(self.kernel.shape, self.dtype)\n    if isinstance(self.recurrent_initializer, tf.keras.layers.Layer):\n      self.recurrent_kernel = self.recurrent_initializer(\n          self.recurrent_kernel.shape, self.dtype)\n    if isinstance(self.bias_initializer, tf.keras.layers.Layer):\n      self.bias = self.bias_initializer(self.bias.shape, self.dtype)\n    self.called_weights = True\n\n  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n    """"""Get the initial state and side-effect sampling of stochastic weights.""""""\n    if self.built:\n      self.call_weights()\n    return super(LSTMCellReparameterization, self).get_initial_state(\n        inputs=inputs, batch_size=batch_size, dtype=dtype)\n\n  def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n    """"""Computes carry and output using split kernels.""""""\n    x_i, x_f, x_c, x_o = x\n    h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\n    # Index the associated tensor as indexing does not work over the event shape\n    # of distributions.\n    recurrent_kernel = tf.convert_to_tensor(self.recurrent_kernel)\n    i = self.recurrent_activation(\n        x_i + tf.keras.backend.dot(h_tm1_i, recurrent_kernel[:, :self.units]))\n    f = self.recurrent_activation(x_f + tf.keras.backend.dot(\n        h_tm1_f, recurrent_kernel[:, self.units:self.units * 2]))\n    c = f * c_tm1 + i * self.activation(x_c + tf.keras.backend.dot(\n        h_tm1_c, recurrent_kernel[:, self.units * 2:self.units * 3]))\n    o = self.recurrent_activation(\n        x_o + tf.keras.backend.dot(\n            h_tm1_o, recurrent_kernel[:, self.units * 3:]))\n    return c, o\n\n\nclass LSTMCellFlipout(LSTMCellReparameterization):\n  """"""Bayesian LSTM cell class estimated via Flipout (Wen et al., 2018).\n\n  The layer computes a variational Bayesian approximation to the distribution\n  over LSTM cell functions,\n\n  ```\n  p(outputs | inputs) = int lstm_cell(inputs; weights, bias) p(weights, bias)\n    dweights dbias,\n  ```\n\n  where the weights consist of both input and recurrent weights.\n\n  It does this with a stochastic forward pass, sampling from learnable\n  distributions on the kernel, recurrent kernel, and bias. Gradients with\n  respect to the distributions\' learnable parameters backpropagate via\n  reparameterization.  Minimizing cross-entropy plus the layer\'s losses performs\n  variational minimum description length, i.e., it minimizes an upper bound to\n  the negative marginal likelihood.\n\n  This layer uses the Flipout estimator (Wen et al., 2018) for integrating with\n  respect to the `kernel` and `recurrent_kernel`. Namely, it applies\n  pseudo-independent weight perturbations via independent sign flips for each\n  example, enabling variance reduction over independent weight perturbations.\n  For this estimator to work, the `kernel` and `recurrent_kernel` random\n  variable must be able to decompose as a sum of its mean and a perturbation\n  distribution; the perturbation distribution must be independent across weight\n  elements and symmetric around zero (for example, a fully factorized Gaussian).\n  """"""\n\n  def _call_sign_flips(self, inputs=None, batch_size=None, dtype=None):\n    """"""Builds per-example sign flips for pseudo-independent perturbations.""""""\n    # We add and call this method separately from build(). build() operates on a\n    # static input_shape, and we need the batch size which is often dynamic.\n    if inputs is not None:\n      batch_size = tf.shape(inputs)[0]\n      dtype = inputs.dtype\n    input_dim = self.kernel.shape[0]\n    self.sign_input = 2 * tf.random.uniform(\n        [batch_size, input_dim], minval=0, maxval=2, dtype=tf.int32) - 1\n    self.sign_output = 2 * tf.random.uniform(\n        [batch_size, 4 * self.units], minval=0, maxval=2, dtype=tf.int32) - 1\n    self.recurrent_sign_input = 2 * tf.random.uniform(\n        [batch_size, self.units], minval=0, maxval=2, dtype=tf.int32) - 1\n    self.recurrent_sign_output = 2 * tf.random.uniform(\n        [batch_size, 4 * self.units], minval=0, maxval=2, dtype=tf.int32) - 1\n    self.sign_input = tf.cast(self.sign_input, dtype)\n    self.sign_output = tf.cast(self.sign_output, dtype)\n    self.recurrent_sign_input = tf.cast(self.recurrent_sign_input, dtype)\n    self.recurrent_sign_output = tf.cast(self.recurrent_sign_output, dtype)\n\n  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n    """"""Get the initial state and side-effect sampling of stochastic weights.""""""\n    if self.built:\n      self._call_sign_flips(inputs, batch_size, dtype)\n    return super(LSTMCellFlipout, self).get_initial_state(\n        inputs=inputs, batch_size=batch_size, dtype=dtype)\n\n  def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n    """"""Computes carry and output using split kernels.""""""\n    if not isinstance(self.recurrent_kernel, random_variable.RandomVariable):\n      return super(LSTMCellFlipout, self)._compute_carry_and_output(x,\n                                                                    h_tm1,\n                                                                    c_tm1)\n    x_i, x_f, x_c, x_o = x\n    h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\n    kernel_mean = self.recurrent_kernel.distribution.mean()\n    perturbation = self.recurrent_kernel - kernel_mean\n    k_i, k_f, k_c, k_o = tf.split(kernel_mean, num_or_size_splits=4, axis=1)\n    p_i, p_f, p_c, p_o = tf.split(perturbation, num_or_size_splits=4, axis=1)\n    so_i, so_f, so_c, so_o = tf.split(self.recurrent_sign_output,\n                                      num_or_size_splits=4, axis=1)\n    z0 = (x_i + tf.keras.backend.dot(h_tm1_i, k_i) +\n          tf.keras.backend.dot(h_tm1_i * self.recurrent_sign_input, p_i) * so_i)\n    z1 = (x_f + tf.keras.backend.dot(h_tm1_f, k_f) +\n          tf.keras.backend.dot(h_tm1_f * self.recurrent_sign_input, p_f) * so_f)\n    z2 = (x_c + tf.keras.backend.dot(h_tm1_c, k_c) +\n          tf.keras.backend.dot(h_tm1_c * self.recurrent_sign_input, p_c) * so_c)\n    z3 = (x_o + tf.keras.backend.dot(h_tm1_o, k_o) +\n          tf.keras.backend.dot(h_tm1_o * self.recurrent_sign_input, p_o) * so_o)\n    i = self.recurrent_activation(z0)\n    f = self.recurrent_activation(z1)\n    c = f * c_tm1 + i * self.activation(z2)\n    o = self.recurrent_activation(z3)\n    return c, o\n\n  def call(self, inputs, states, training=None):\n    # TODO(trandustin): Enable option for Flipout on only the kernel or\n    # recurrent_kernel. If only one is a random variable, we currently default\n    # to weight reparameterization.\n    if (not isinstance(self.kernel, random_variable.RandomVariable) or\n        not isinstance(self.recurrent_kernel, random_variable.RandomVariable)):\n      return super(LSTMCellFlipout, self).call(inputs, states, training)\n    if not self.called_weights:\n      self.call_weights()\n      self._call_sign_flips(inputs)\n    h_tm1 = states[0]  # previous memory state\n    c_tm1 = states[1]  # previous carry state\n\n    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n        h_tm1, training, count=4)\n\n    if self.implementation == 1:\n      if 0 < self.dropout < 1.:\n        inputs_i = inputs * dp_mask[0]\n        inputs_f = inputs * dp_mask[1]\n        inputs_c = inputs * dp_mask[2]\n        inputs_o = inputs * dp_mask[3]\n      else:\n        inputs_i = inputs\n        inputs_f = inputs\n        inputs_c = inputs\n        inputs_o = inputs\n      kernel_mean = self.kernel.distribution.mean()\n      perturbation = self.kernel - kernel_mean\n      k_i, k_f, k_c, k_o = tf.split(kernel_mean, num_or_size_splits=4, axis=1)\n      p_i, p_f, p_c, p_o = tf.split(perturbation, num_or_size_splits=4, axis=1)\n      so_i, so_f, so_c, so_o = tf.split(self.sign_output,\n                                        num_or_size_splits=4, axis=1)\n      x_i = (tf.keras.backend.dot(inputs_i, k_i) +\n             tf.keras.backend.dot(inputs_i * self.sign_input, p_i) * so_i)\n      x_f = (tf.keras.backend.dot(inputs_f, k_f) +\n             tf.keras.backend.dot(inputs_f * self.sign_input, p_f) * so_f)\n      x_c = (tf.keras.backend.dot(inputs_c, k_c) +\n             tf.keras.backend.dot(inputs_c * self.sign_input, p_c) * so_c)\n      x_o = (tf.keras.backend.dot(inputs_o, k_o) +\n             tf.keras.backend.dot(inputs_o * self.sign_input, p_o) * so_o)\n      if self.use_bias:\n        b_i, b_f, b_c, b_o = tf.split(\n            self.bias, num_or_size_splits=4, axis=0)\n        x_i = tf.keras.backend.bias_add(x_i, b_i)\n        x_f = tf.keras.backend.bias_add(x_f, b_f)\n        x_c = tf.keras.backend.bias_add(x_c, b_c)\n        x_o = tf.keras.backend.bias_add(x_o, b_o)\n\n      if 0 < self.recurrent_dropout < 1.:\n        h_tm1_i = h_tm1 * rec_dp_mask[0]\n        h_tm1_f = h_tm1 * rec_dp_mask[1]\n        h_tm1_c = h_tm1 * rec_dp_mask[2]\n        h_tm1_o = h_tm1 * rec_dp_mask[3]\n      else:\n        h_tm1_i = h_tm1\n        h_tm1_f = h_tm1\n        h_tm1_c = h_tm1\n        h_tm1_o = h_tm1\n      x = (x_i, x_f, x_c, x_o)\n      h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)\n      c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)\n    else:\n      if 0. < self.dropout < 1.:\n        inputs = inputs * dp_mask[0]\n      kernel_mean = self.kernel.distribution.mean()\n      perturbation = self.kernel - kernel_mean\n      z = tf.keras.backend.dot(inputs, kernel_mean)\n      z += tf.keras.backend.dot(inputs * self.sign_input,\n                                perturbation) * self.sign_output\n      if 0. < self.recurrent_dropout < 1.:\n        h_tm1 = h_tm1 * rec_dp_mask[0]\n      recurrent_kernel_mean = self.recurrent_kernel.distribution.mean()\n      perturbation = self.recurrent_kernel - recurrent_kernel_mean\n      z += tf.keras.backend.dot(h_tm1, recurrent_kernel_mean)\n      z += tf.keras.backend.dot(h_tm1 * self.recurrent_sign_input,\n                                perturbation) * self.recurrent_sign_output\n      if self.use_bias:\n        z = tf.keras.backend.bias_add(z, self.bias)\n\n      z = tf.split(z, num_or_size_splits=4, axis=1)\n      c, o = self._compute_carry_and_output_fused(z, c_tm1)\n\n    h = o * self.activation(c)\n    return h, [h, c]\n'"
edward2/tensorflow/layers/recurrent_test.py,11,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for Bayesian recurrent cells and layers.""""""\n\nfrom absl.testing import parameterized\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass RecurrentTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.parameters(\n      {""lstm_cell"": ed.layers.LSTMCellFlipout,\n       ""kernel_initializer"": ""zeros"",\n       ""recurrent_initializer"": ""orthogonal"",\n       ""bias_initializer"": ""zeros"",\n       ""implementation"": 1,\n       ""all_close"": True},\n      {""lstm_cell"": ed.layers.LSTMCellFlipout,\n       ""kernel_initializer"": ""trainable_normal"",\n       ""recurrent_initializer"": ""orthogonal"",\n       ""bias_initializer"": ""zeros"",\n       ""implementation"": 1,\n       ""all_close"": False},\n      {""lstm_cell"": ed.layers.LSTMCellFlipout,\n       ""kernel_initializer"": ""zeros"",\n       ""recurrent_initializer"": ""orthogonal"",\n       ""bias_initializer"": ""trainable_normal"",\n       ""implementation"": 1,\n       ""all_close"": False},\n      {""lstm_cell"": ed.layers.LSTMCellFlipout,\n       ""kernel_initializer"": ""zeros"",\n       ""recurrent_initializer"": ""orthogonal"",\n       ""bias_initializer"": ""zeros"",\n       ""implementation"": 2,\n       ""all_close"": True},\n      {""lstm_cell"": ed.layers.LSTMCellFlipout,\n       ""kernel_initializer"": ""trainable_normal"",\n       ""recurrent_initializer"": ""orthogonal"",\n       ""bias_initializer"": ""zeros"",\n       ""implementation"": 2,\n       ""all_close"": False},\n      {""lstm_cell"": ed.layers.LSTMCellFlipout,\n       ""kernel_initializer"": ""zeros"",\n       ""recurrent_initializer"": ""orthogonal"",\n       ""bias_initializer"": ""trainable_normal"",\n       ""implementation"": 2,\n       ""all_close"": False},\n      {""lstm_cell"": ed.layers.LSTMCellReparameterization,\n       ""kernel_initializer"": ""zeros"",\n       ""recurrent_initializer"": ""orthogonal"",\n       ""bias_initializer"": ""zeros"",\n       ""implementation"": 1,\n       ""all_close"": True},\n      {""lstm_cell"": ed.layers.LSTMCellReparameterization,\n       ""kernel_initializer"": ""trainable_normal"",\n       ""recurrent_initializer"": ""orthogonal"",\n       ""bias_initializer"": ""zeros"",\n       ""implementation"": 1,\n       ""all_close"": False},\n      {""lstm_cell"": ed.layers.LSTMCellReparameterization,\n       ""kernel_initializer"": ""zeros"",\n       ""recurrent_initializer"": ""trainable_normal"",\n       ""bias_initializer"": ""zeros"",\n       ""implementation"": 1,\n       ""all_close"": False},\n      {""lstm_cell"": ed.layers.LSTMCellReparameterization,\n       ""kernel_initializer"": ""zeros"",\n       ""recurrent_initializer"": ""orthogonal"",\n       ""bias_initializer"": ""trainable_normal"",\n       ""implementation"": 1,\n       ""all_close"": False},\n      {""lstm_cell"": ed.layers.LSTMCellReparameterization,\n       ""kernel_initializer"": ""trainable_normal"",\n       ""recurrent_initializer"": ""orthogonal"",\n       ""bias_initializer"": ""zeros"",\n       ""implementation"": 2,\n       ""all_close"": False},\n      {""lstm_cell"": ed.layers.LSTMCellReparameterization,\n       ""kernel_initializer"": ""trainable_deterministic"",\n       ""recurrent_initializer"": ""trainable_deterministic"",\n       ""bias_initializer"": ""trainable_deterministic"",\n       ""implementation"": 1,\n       ""all_close"": True},\n      {""lstm_cell"": ed.layers.LSTMCellReparameterization,\n       ""kernel_initializer"": ""trainable_deterministic"",\n       ""recurrent_initializer"": ""trainable_deterministic"",\n       ""bias_initializer"": ""trainable_deterministic"",\n       ""implementation"": 2,\n       ""all_close"": True},\n  )\n  def testLSTMCell(self,\n                   lstm_cell,\n                   kernel_initializer,\n                   recurrent_initializer,\n                   bias_initializer,\n                   implementation,\n                   all_close):\n    batch_size, dim = 5, 12\n    hidden_size = 10\n    inputs = np.random.rand(batch_size, dim).astype(np.float32)\n    cell = lstm_cell(hidden_size,\n                     kernel_initializer=kernel_initializer,\n                     recurrent_initializer=recurrent_initializer,\n                     bias_initializer=bias_initializer,\n                     implementation=implementation)\n    noise = np.random.rand(1, hidden_size).astype(np.float32)\n    h0, c0 = cell.get_initial_state(inputs)\n    state = (h0 + noise, c0)\n    outputs1, _ = cell(inputs, state)\n    outputs2, _ = cell(inputs, state)\n    cell.call_weights()\n    outputs3, _ = cell(inputs, state)\n    self.assertEqual(outputs1.shape, (batch_size, hidden_size))\n    self.assertAllClose(outputs1, outputs2)\n    if all_close:\n      self.assertAllClose(outputs1, outputs3)\n    else:\n      self.assertNotAllClose(outputs1, outputs3)\n    cell.get_config()\n\n  @parameterized.parameters(\n      {""lstm_cell"": ed.layers.LSTMCellFlipout},\n      {""lstm_cell"": ed.layers.LSTMCellReparameterization},\n  )\n  def testLSTMCellLoss(self, lstm_cell):\n    features = np.random.rand(5, 1, 12).astype(np.float32)\n    labels = np.random.rand(5, 10).astype(np.float32)\n    cell = lstm_cell(10)\n    state = (tf.zeros([1, 10]), tf.zeros([1, 10]))\n\n    # Imagine this is the 1st epoch.\n    with tf.GradientTape(persistent=True) as tape:\n      predictions, _ = cell(features[:, 0, :], state)  # first call forces build\n      cell(features[:, 0, :], state)  # ensure robustness after multiple calls\n      cell.get_initial_state(features[:, 0, :])\n      cell(features[:, 0, :], state)  # ensure robustness after multiple calls\n      nll = tf.keras.losses.mean_squared_error(labels, predictions)\n      kl = sum(cell.losses)\n\n    variables = [\n        cell.kernel_initializer.mean, cell.kernel_initializer.stddev,\n        cell.recurrent_initializer.mean, cell.recurrent_initializer.stddev,\n    ]\n    for v in variables:\n      # Note in TF 2.0, checking membership (v in cell.weights) raises an error\n      # for lists of differently shaped Tensors.\n      self.assertTrue(any(v is weight for weight in cell.weights))\n\n    # This will be fine, since the layer was built inside this tape, and thus\n    # the distribution init ops were inside this tape.\n    grads = tape.gradient(nll, variables)\n    for grad in grads:\n      self.assertIsNotNone(grad)\n    grads = tape.gradient(kl, variables)\n    for grad in grads:\n      self.assertIsNotNone(grad)\n\n    # Imagine this is the 2nd epoch.\n    with tf.GradientTape(persistent=True) as tape:\n      cell.get_initial_state(features[:, 0, :])\n      predictions, _ = cell(features[:, 0, :], state)  # build is not called\n      nll = tf.keras.losses.mean_squared_error(labels, predictions)\n      kl = sum(cell.losses)\n\n    variables = [\n        cell.kernel_initializer.mean, cell.kernel_initializer.stddev,\n        cell.recurrent_initializer.mean, cell.recurrent_initializer.stddev,\n    ]\n    for v in variables:\n      # Note in TF 2.0, checking membership (v in cell.weights) raises an error\n      # for lists of differently shaped Tensors.\n      self.assertTrue(any(v is weight for weight in cell.weights))\n\n    # This would fail, since the layer was built inside the tape from the 1st\n    # epoch, and thus the distribution init ops were inside that tape instead of\n    # this tape. By using a callable for the variable, this will no longer fail.\n    grads = tape.gradient(nll, variables)\n    for grad in grads:\n      self.assertIsNotNone(grad)\n    grads = tape.gradient(kl, variables)\n    for grad in grads:\n      self.assertIsNotNone(grad)\n\n  @parameterized.parameters(\n      {""lstm_cell"": ed.layers.LSTMCellFlipout},\n      {""lstm_cell"": ed.layers.LSTMCellReparameterization},\n  )\n  def testLSTMCellModel(self, lstm_cell):\n    batch_size, timesteps, dim = 5, 3, 12\n    hidden_size = 10\n    inputs = np.random.rand(batch_size, timesteps, dim).astype(np.float32)\n    cell = lstm_cell(hidden_size)\n    model = tf.keras.Sequential([\n        tf.keras.layers.RNN(cell, return_sequences=True)\n    ])\n    outputs1 = model(inputs)\n    outputs2 = model(inputs)\n    state = (tf.zeros([1, hidden_size]), tf.zeros([1, hidden_size]))\n    outputs3 = []\n    for t in range(timesteps):\n      out, state = cell(inputs[:, t, :], state)\n      outputs3.append(out)\n    outputs3 = tf.stack(outputs3, axis=1)\n    self.assertEqual(outputs1.shape, (batch_size, timesteps, hidden_size))\n    self.assertEqual(outputs3.shape, (batch_size, timesteps, hidden_size))\n    # NOTE: `cell.call_weights` should have been called at the beginning of\n    # each call, so these should be different.\n    self.assertNotAllClose(outputs1, outputs2)\n    # NOTE: We didn\'t call `cell.call_weights` again before computing\n    # `outputs3`, so the cell should have had the same weights as it did\n    # during computation of `outputs2`, and thus yielded the same output\n    # tensor.\n    self.assertAllClose(outputs2, outputs3)\n    self.assertLen(model.losses, 2)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
edward2/tensorflow/layers/stochastic_output.py,9,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Stochastic output layers.\n\nStochastic output layers apply a linear layer to project from the input\ndimension to the proper number of dimensions for parameterizing a distribution.\nThey also apply a set of constraints from the real-valued dimensions to the\ndomain for each parameter of the outputted random variable.\n\nTo avoid a bloated namespace where there is one stochastic output layer for\nevery available ed.RandomVariable, this module only implements stochastic\noutput layers that involve a lot of tensor manipulation (e.g., mixture\ndistributions and multi-parameter distributions with constraints).\n\nFor non-built-in stochastic output layers, we recommend you create your own:\n\n```python\ndataset_size = 10\nbatch_size = 2\nnum_classes = 5\n\nnumpy_features = np.random.normal(size=[dataset_size, 3]).astype(\'float32\')\nnumpy_labels = np.random.randint(num_classes, size=dataset_size).astype(\'int32\')\ndataset = tf.data.Dataset.from_tensor_slices((numpy_features, numpy_labels))\ndataset = dataset.repeat().batch(batch_size)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(num_classes),\n    tf.keras.layers.Lambda(lambda inputs: ed.Categorical(logits=inputs)),\n])\n\nmodel.compile(tf.keras.optimizers.Adam(0.1),\n              loss=lambda y_true, y_pred: -y_pred.distribution.log_prob(y_true))\nmodel.fit(dataset,\n          steps_per_epoch=dataset_size // batch_size,\n          epochs=10)\n```\n""""""\n\nfrom edward2.tensorflow import constraints\nfrom edward2.tensorflow import generated_random_variables\n\nimport tensorflow as tf\n\n\nclass MixtureLogistic(tf.keras.layers.Layer):\n  """"""Stochastic output layer, distributed as a mixture of logistics.\n\n  Given an input tensor of shape [..., input_dim], the output layer returns\n  an ed.Mixture of Logistic random variables of shape [...].\n  """"""\n\n  def __init__(self,\n               num_components,\n               logits_constraint=None,\n               loc_constraint=None,\n               scale_constraint=\'softplus\',\n               **kwargs):\n    super(MixtureLogistic, self).__init__(**kwargs)\n    self.num_components = num_components\n    self.logits_constraint = constraints.get(logits_constraint)\n    self.loc_constraint = constraints.get(loc_constraint)\n    self.scale_constraint = constraints.get(scale_constraint)\n    self.layer = tf.keras.layers.Dense(num_components * 3)\n\n  def build(self, input_shape=None):\n    self.layer.build(input_shape)\n    self.built = True\n\n  def call(self, inputs):\n    net = self.layer(inputs)\n    logits, loc, scale = tf.split(net, 3, axis=-1)\n    if self.logits_constraint:\n      logits = self.logits_constraint(logits)\n    if self.loc_constraint:\n      loc = self.loc_constraint(loc)\n    if self.scale_constraint:\n      scale = self.scale_constraint(scale)\n    return generated_random_variables.MixtureSameFamily(\n        mixture_distribution=generated_random_variables.Categorical(\n            logits=logits).distribution,\n        components_distribution=generated_random_variables.Logistic(\n            loc=loc, scale=scale).distribution)\n\n  def compute_output_shape(self, input_shape):\n    return tf.TensorShape(input_shape)[:-1]\n\n  def get_config(self):\n    config = {\'num_components\': self.num_components}\n    base_config = super(MixtureLogistic, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n'"
edward2/tensorflow/layers/stochastic_output_test.py,5,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for stochastic output layers.""""""\n\nfrom absl.testing import parameterized\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass StochasticOutputTest(parameterized.TestCase, tf.test.TestCase):\n\n  def testMixtureLogistic(self):\n    batch_size = 3\n    features = np.random.rand(batch_size, 4).astype(np.float32)\n    labels = np.random.rand(batch_size).astype(np.float32)\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(2, activation=None),\n        ed.layers.MixtureLogistic(5),\n    ])\n    outputs = model(features)\n    log_likelihood = tf.reduce_sum(outputs.distribution.log_prob(labels))\n    self.assertEqual(log_likelihood.shape, ())\n    self.assertLessEqual(log_likelihood, 0.)\n    self.assertEqual(outputs.shape, (batch_size,))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
edward2/tensorflow/layers/utils.py,43,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Layer utilities.""""""\n\nimport functools\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tf1\n\n# SciPy is not a mandatory dependency when using the TF backend.\ntry:\n  from scipy.optimize import linear_sum_assignment  # pylint: disable=g-import-not-at-top\nexcept ImportError:\n  pass\n\n\ndef add_weight(cls):\n  """"""Decorator for Layers, overriding add_weight for trainable initializers.""""""\n  @functools.wraps(cls.add_weight)\n  def _add_weight(self,\n                  name=None,\n                  shape=None,\n                  dtype=None,\n                  initializer=None,\n                  regularizer=None,\n                  **kwargs):\n    """"""Adds weight.""""""\n    # Rely on the keras trackable machinery to pick up weights where applicable.\n    # The name for the field is arbitrary.\n    if getattr(self, \'tracked_add_weight_dependencies\', None) is None:\n      self.tracked_add_weight_dependencies = []\n    self.tracked_add_weight_dependencies.append((regularizer, initializer))\n\n    if isinstance(regularizer, tf.keras.layers.Layer):\n      if not regularizer.built:\n        regularizer.build(shape)\n    if isinstance(initializer, tf.keras.layers.Layer):\n      with tf.name_scope(name):\n        weight = initializer(shape, dtype)\n      if regularizer is not None:\n        def loss_fn():\n          """"""Creates a regularization loss `Tensor`.""""""\n          with tf.name_scope(name + \'/Regularizer\'):\n            return regularizer(initializer(shape, dtype))\n        self.add_loss(loss_fn)\n      return weight\n    return super(cls, self).add_weight(name=name,\n                                       shape=shape,\n                                       dtype=dtype,\n                                       initializer=initializer,\n                                       regularizer=regularizer,\n                                       **kwargs)\n  cls.add_weight = _add_weight\n  return cls\n\n\ndef one_hot_argmax(inputs, temperature, axis=-1):\n  """"""Returns one-hot of argmax with backward pass set to softmax-temperature.""""""\n  vocab_size = inputs.shape[-1]\n  hard = tf.one_hot(tf.argmax(inputs, axis=axis),\n                    depth=vocab_size,\n                    axis=axis,\n                    dtype=inputs.dtype)\n  soft = tf.nn.softmax(inputs / temperature, axis=axis)\n  outputs = soft + tf.stop_gradient(hard - soft)\n  return outputs\n\n\ndef one_hot_add(inputs, shift):\n  """"""Performs (inputs + shift) % vocab_size in the one-hot space.\n\n  Args:\n    inputs: Tensor of shape `[..., vocab_size]`. Typically a soft/hard one-hot\n      Tensor.\n    shift: Tensor of shape `[..., vocab_size]`. Typically a soft/hard one-hot\n      Tensor specifying how much to shift the corresponding one-hot vector in\n      inputs. Soft values perform a ""weighted shift"": for example,\n      shift=[0.2, 0.3, 0.5] performs a linear combination of 0.2 * shifting by\n      zero; 0.3 * shifting by one; and 0.5 * shifting by two.\n\n  Returns:\n    Tensor of same shape and dtype as inputs.\n  """"""\n  # Compute circular 1-D convolution with shift as the kernel.\n  inputs = tf.cast(inputs, tf.complex64)\n  shift = tf.cast(shift, tf.complex64)\n  return tf.math.real(\n      tf.signal.ifft(tf.signal.fft(inputs) * tf.signal.fft(shift)))\n\n\ndef one_hot_minus(inputs, shift):\n  """"""Performs (inputs - shift) % vocab_size in the one-hot space.\n\n  Args:\n    inputs: Tensor of shape `[..., vocab_size]`. Typically a soft/hard one-hot\n      Tensor.\n    shift: Tensor of shape `[..., vocab_size]`. Typically a soft/hard one-hot\n      Tensor specifying how much to shift the corresponding one-hot vector in\n      inputs. Soft values perform a ""weighted shift"": for example,\n      shift=[0.2, 0.3, 0.5] performs a linear combination of 0.2 * shifting by\n      zero; 0.3 * shifting by one; and 0.5 * shifting by two.\n\n  Returns:\n    Tensor of same shape and dtype as inputs.\n  """"""\n  # TODO(trandustin): Implement with circular conv1d.\n  inputs = tf.convert_to_tensor(inputs)\n  shift = tf.cast(shift, inputs.dtype)\n  vocab_size = inputs.shape[-1]\n  # Form a [..., vocab_size, vocab_size] matrix. Each batch element of\n  # inputs will vector-matrix multiply the vocab_size x vocab_size matrix. This\n  # ""shifts"" the inputs batch element by the corresponding shift batch element.\n  shift_matrix = tf.stack([tf.roll(shift, i, axis=-1)\n                           for i in range(vocab_size)], axis=-2)\n  outputs = tf.einsum(\'...v,...uv->...u\', inputs, shift_matrix)\n  return outputs\n\n\ndef one_hot_multiply(inputs, scale):\n  """"""Performs (inputs * scale) % vocab_size in the one-hot space.\n\n  Args:\n    inputs: Tensor of shape `[..., vocab_size]`. Typically a soft/hard one-hot\n      Tensor.\n    scale: Tensor of shape `[..., vocab_size]`. Typically a soft/hard one-hot\n      Tensor specifying how much to scale the corresponding one-hot vector in\n      inputs. Soft values perform a ""weighted scale"": for example,\n      scale=[0.2, 0.3, 0.5] performs a linear combination of\n      0.2 * scaling by zero; 0.3 * scaling by one; and 0.5 * scaling by two.\n\n  Returns:\n    Tensor of same shape and dtype as inputs.\n  """"""\n  # TODO(trandustin): Implement with circular conv1d.\n  inputs = tf.convert_to_tensor(inputs)\n  scale = tf.cast(scale, inputs.dtype)\n  batch_shape = inputs.shape[:-1].as_list()\n  vocab_size = inputs.shape[-1]\n  # Form a [..., vocab_size, vocab_size] tensor. The ith row of the\n  # batched vocab_size x vocab_size matrix represents scaling inputs by i.\n  permutation_matrix = tf.math.floormod(\n      tf.tile(tf.range(vocab_size)[:, tf.newaxis], [1, vocab_size]) *\n      tf.range(vocab_size)[tf.newaxis], vocab_size)\n  permutation_matrix = tf.one_hot(permutation_matrix, depth=vocab_size, axis=-1)\n  # Scale the inputs according to the permutation matrix of all possible scales.\n  scaled_inputs = tf.einsum(\'...v,avu->...au\', inputs, permutation_matrix)\n  scaled_inputs = tf.concat([tf.zeros(batch_shape + [1, vocab_size]),\n                             scaled_inputs[..., 1:, :]], axis=-2)\n  # Reduce rows of the scaled inputs by the scale values. This forms a\n  # weighted linear combination of scaling by zero, scaling by one, and so on.\n  outputs = tf.einsum(\'...v,...vu->...u\', scale, scaled_inputs)\n  return outputs\n\n\ndef py_multiplicative_inverse(a, n):\n  """"""Multiplicative inverse of a modulo n (in Python).\n\n  Implements extended Euclidean algorithm.\n\n  Args:\n    a: int-like np.ndarray.\n    n: int.\n\n  Returns:\n    Multiplicative inverse as an int32 np.ndarray with same shape as a.\n  """"""\n  batched_a = np.asarray(a, dtype=np.int32)\n  batched_inverse = []\n  for a in np.nditer(batched_a):\n    inverse = 0\n    new_inverse = 1\n    remainder = n\n    new_remainder = a\n    while new_remainder != 0:\n      quotient = remainder // new_remainder\n      (inverse, new_inverse) = (new_inverse, inverse - quotient * new_inverse)\n      (remainder, new_remainder) = (new_remainder,\n                                    remainder - quotient * new_remainder)\n    if remainder > 1:\n      return ValueError(\n          \'Inverse for {} modulo {} does not exist.\'.format(a, n))\n    if inverse < 0:\n      inverse += n\n    batched_inverse.append(inverse)\n  return np.asarray(batched_inverse, dtype=np.int32).reshape(batched_a.shape)\n\n\ndef multiplicative_inverse(a, n):\n  """"""Multiplicative inverse of a modulo n.\n\n  Args:\n    a: Tensor of shape [..., vocab_size]. It denotes an integer in the one-hot\n      space.\n    n: int Tensor of shape [...].\n\n  Returns:\n    Tensor of same shape and dtype as a.\n  """"""\n  a = tf.convert_to_tensor(a)\n  n = tf.convert_to_tensor(n)\n  vocab_size = a.shape[-1]\n  a_dtype = a.dtype\n  sparse_a = tf.argmax(a, axis=-1)\n  # TODO(trandustin): Change to tf.py_function.\n  sparse_outputs = tf1.py_func(\n      py_multiplicative_inverse, [sparse_a, n], tf.int32)\n  sparse_outputs.set_shape(sparse_a.shape)\n  outputs = tf.one_hot(sparse_outputs, depth=vocab_size, dtype=a_dtype)\n  return outputs\n\n\ndef soft_to_hard_permutation(inputs):\n  """"""Returns permutation matrices by solving a matching problem.\n\n  Solves linear sum assignment to convert doubly-stochastic matrices to\n  permutation matrices. It uses scipy.optimize.linear_sum_assignment to solve\n  the optimization problem max_P sum_i,j M_i,j P_i,j with P a permutation\n  matrix. Notice the negative sign; the reason, the original function solves a\n  minimization problem.\n\n  Code is adapted from Mena et al. [1].\n\n  [1] Gonzalo Mena, David Belanger, Scott Linderman, Jasper Snoek.\n  Learning latent permutations with Gumbel-Sinkhorn networks. International\n  Conference on Learning Representations, 2018.\n\n  Args:\n    inputs: A `Tensor` with shape `[:, vocab_size, vocab_size]` that is\n      doubly-stochastic in its last two dimensions.\n\n  Returns:\n    outputs: A hard permutation `Tensor` with the same shape as `inputs` (in\n      other words the last two dimensions are doubly-stochastic and each element\n      is 0 or 1).\n  """"""\n\n  def hungarian(x):\n    """"""Hungarian algorithm.""""""\n    x = x.numpy()\n    if x.ndim == 2:\n      x = np.reshape(x, [1, x.shape[0], x.shape[1]])\n    sol = np.zeros((x.shape[0], x.shape[1]), dtype=np.int32)\n    for i in range(x.shape[0]):\n      try:\n        sol[i, :] = linear_sum_assignment(-x[i, :])[1].astype(np.int32)\n      except NameError:\n        raise NameError(\'linear_sum_assignment requires SciPy to be installed.\')\n    return tf.convert_to_tensor(sol)\n\n  vocab_size = inputs.shape[-1]\n  # Note: tf.py_function isn\'t currently supported on headless GPUs.\n  # TODO(vafa): Fix tf.py_function headless GPU bug.\n  permutation_lists = tf.py_function(hungarian, [inputs], tf.int32)\n  hard = tf.one_hot(permutation_lists, depth=vocab_size)\n  outputs = tf.stop_gradient(hard - inputs) + inputs\n  return outputs\n\n\ndef sinkhorn(inputs, n_iters=20):\n  """"""Performs incomplete Sinkhorn normalization to inputs.\n\n  By a theorem by Sinkhorn and Knopp [1], a sufficiently well-behaved  matrix\n  with positive entries can be turned into a doubly-stochastic matrix\n  (i.e. its rows and columns add up to one) via the succesive row and column\n  normalization.\n  -To ensure positivity, the effective input to sinkhorn has to be\n  exp(inputs) (elementwise).\n  -However, for stability, sinkhorn works in the log-space. It is only at\n   return time that entries are exponentiated.\n\n  Code is adapted from Mena et al. [2].\n\n  [1] Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and\n  doubly stochastic matrices. Pacific Journal of Mathematics, 1967.\n\n  [2] Gonzalo Mena, David Belanger, Scott Linderman, Jasper Snoek.\n  Learning latent permutations with Gumbel-Sinkhorn networks. International\n  Conference on Learning Representations, 2018.\n\n  Args:\n    inputs: A `Tensor` with shape `[..., vocab_size, vocab_size]`.\n    n_iters: Number of sinkhorn iterations (in practice, as little as 20\n      iterations are needed to achieve decent convergence for `vocab_size` ~100)\n\n  Returns:\n    outputs: A `Tensor` of close-to-doubly-stochastic matrices with shape\n      `[:, vocab_size, vocab_size]`.\n  """"""\n  vocab_size = tf.shape(inputs)[-1]\n  log_alpha = tf.reshape(inputs, [-1, vocab_size, vocab_size])\n\n  for _ in range(n_iters):\n    log_alpha -= tf.reshape(tf.reduce_logsumexp(log_alpha, axis=2),\n                            [-1, vocab_size, 1])\n    log_alpha -= tf.reshape(tf.reduce_logsumexp(log_alpha, axis=1),\n                            [-1, 1, vocab_size])\n  outputs = tf.exp(log_alpha)\n  return outputs\n\n\n# From `tensorflow/python/framework/smart_cond.py`\ndef smart_constant_value(pred):\n  """"""Return the bool value for `pred`, or None if `pred` had a dynamic value.\n\n  Arguments:\n    pred: A scalar, either a Python bool or tensor.\n\n  Returns:\n    True or False if `pred` has a constant boolean value, None otherwise.\n\n  Raises:\n    TypeError: If `pred` is not a Tensor or bool.\n  """"""\n  if pred in {0, 1}:  # Accept 1/0 as valid boolean values\n    pred_value = bool(pred)\n  elif isinstance(pred, bool):\n    pred_value = pred\n  elif isinstance(pred, tf.Tensor):\n    pred_value = tf.get_static_value(pred)\n  else:\n    raise TypeError(\'`pred` must be a Tensor, or a Python bool, or 1 or 0. \'\n                    \'Found instead: %s\' % pred)\n  return pred_value\n'"
edward2/tensorflow/layers/utils_test.py,40,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for utilities.""""""\n\nfrom absl.testing import parameterized\nimport edward2 as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass UtilsTest(parameterized.TestCase, tf.test.TestCase):\n\n  def testAddWeightWithTrainableInitializer(self):\n    dense_wrapped = ed.layers.utils.add_weight(tf.keras.layers.Dense)\n    initializer = ed.initializers.get(\'trainable_normal\')\n    layer = dense_wrapped(2, kernel_initializer=initializer, name=\'dense\')\n    inputs = tf.random.normal([1, 3])\n    _ = layer(inputs)\n    self.assertTrue(initializer.built, True)\n    self.assertNotEmpty(initializer.weights)\n    for weight in initializer.weights:\n      self.assertTrue(np.any([weight is lweight for lweight in layer.weights]))\n    layer_weights_names = [weight.name for weight in layer.weights]\n    self.assertEqual(layer_weights_names[0], \'dense/bias:0\')\n    self.assertEqual(layer_weights_names[1], \'dense/kernel/mean:0\')\n    self.assertEqual(layer_weights_names[2], \'dense/kernel/stddev:0\')\n\n  def testAddWeightWithTrainableRegularizer(self):\n    dense_wrapped = ed.layers.utils.add_weight(tf.keras.layers.Dense)\n    regularizer = ed.regularizers.get(\'trainable_normal_kl_divergence_stddev\')\n    layer = dense_wrapped(2, kernel_regularizer=regularizer)\n    inputs = tf.random.normal([1, 3])\n    _ = layer(inputs)\n    self.assertTrue(regularizer.built, True)\n    self.assertNotEmpty(regularizer.weights)\n    for weight in regularizer.weights:\n      self.assertTrue(np.any([weight is lweight for lweight in layer.weights]))\n\n  def testOneHotAddExactHard(self):\n    inputs = tf.constant([[0., 1., 0.],\n                          [0., 0., 1.]])\n    shift = tf.constant([[0., 1., 0.],\n                         [1., 0., 0.]])\n\n    outputs = ed.layers.utils.one_hot_add(inputs, shift)\n    self.assertAllClose(outputs,\n                        np.array([[0., 0., 1.],\n                                  [0., 0., 1.]], dtype=np.float32),\n                        rtol=1e-4, atol=1e-4)\n\n  def testOneHotMinusExactHard(self):\n    inputs = tf.constant([[0., 1., 0.],\n                          [0., 0., 1.]])\n    shift = tf.constant([[0., 1., 0.],\n                         [1., 0., 0.]])\n\n    outputs = ed.layers.utils.one_hot_minus(inputs, shift)\n    self.assertAllEqual(outputs, np.array([[1., 0., 0.],\n                                           [0., 0., 1.]], dtype=np.float32))\n\n  def testOneHotMultiplyExactHard(self):\n    inputs = tf.constant([[0., 1., 0.],\n                          [0., 0., 1.]])\n    scale = tf.constant([[0., 1., 0.],\n                         [0., 0., 1.]])\n\n    outputs = ed.layers.utils.one_hot_multiply(inputs, scale)\n    self.assertAllEqual(outputs, np.array([[0., 1., 0.],\n                                           [0., 1., 0.]], dtype=np.float32))\n\n  def testOneHotAddExactSoft(self):\n    inputs = tf.constant([[0., 1., 0.],\n                          [0., 0., 1.]])\n    shift = tf.constant([[0.1, 0.6, 0.3],\n                         [0.2, 0.4, 0.4]])\n    outputs = ed.layers.utils.one_hot_add(inputs, shift)\n\n    shift_zero = inputs\n    shift_one = np.array([[0., 0., 1.],\n                          [1., 0., 0.]])\n    shift_two = np.array([[1., 0., 0.],\n                          [0., 1., 0.]])\n    expected_outputs = (shift[..., 0][..., tf.newaxis] * shift_zero +\n                        shift[..., 1][..., tf.newaxis] * shift_one +\n                        shift[..., 2][..., tf.newaxis] * shift_two)\n    self.assertAllClose(outputs, expected_outputs)\n\n  def testOneHotMinusExactSoft(self):\n    inputs = tf.constant([[0., 1., 0.],\n                          [0., 0., 1.]])\n    shift = tf.constant([[0.1, 0.6, 0.3],\n                         [0.2, 0.4, 0.4]])\n    outputs = ed.layers.utils.one_hot_minus(inputs, shift)\n\n    shift_zero = inputs\n    shift_one = np.array([[1., 0., 0.],\n                          [0., 1., 0.]])\n    shift_two = np.array([[0., 0., 1.],\n                          [1., 0., 0.]])\n    expected_outputs = (shift[..., 0][..., tf.newaxis] * shift_zero +\n                        shift[..., 1][..., tf.newaxis] * shift_one +\n                        shift[..., 2][..., tf.newaxis] * shift_two)\n    self.assertAllEqual(outputs, expected_outputs)\n\n  def testOneHotMultiplyExactSoft(self):\n    inputs = tf.constant([[0., 1., 0.],\n                          [0., 0., 1.]])\n    scale = tf.constant([[0.1, 0.6, 0.3],\n                         [0.2, 0.4, 0.4]])\n    outputs = ed.layers.utils.one_hot_multiply(inputs, scale)\n\n    scale_zero = np.array([[0., 0., 0.],\n                           [0., 0., 0.]])\n    scale_one = inputs\n    scale_two = np.array([[0., 0., 1.],\n                          [0., 1., 0.]])\n    expected_outputs = (scale[..., 0][..., tf.newaxis] * scale_zero +\n                        scale[..., 1][..., tf.newaxis] * scale_one +\n                        scale[..., 2][..., tf.newaxis] * scale_two)\n    self.assertAllEqual(outputs, expected_outputs)\n\n  @parameterized.parameters(\n      (ed.layers.utils.one_hot_add,),\n      (ed.layers.utils.one_hot_minus,),\n  )\n  def testOneHotAddShapeHard(self, one_hot_add_fn):\n    batch_size = 2\n    length = 4\n    vocab_size = 5\n    inputs = tf.random.uniform(\n        [batch_size, length], minval=0, maxval=vocab_size, dtype=tf.int32)\n    inputs = tf.one_hot(inputs, depth=vocab_size, dtype=tf.float32)\n    shift = tf.random.uniform(\n        [batch_size, length], minval=0, maxval=vocab_size, dtype=tf.int32)\n    shift = tf.one_hot(shift, depth=vocab_size)\n\n    outputs = one_hot_add_fn(inputs, shift)\n    self.assertEqual(outputs.shape, (batch_size, length, vocab_size))\n\n  @parameterized.parameters(\n      (ed.layers.utils.one_hot_add,),\n      (ed.layers.utils.one_hot_minus,),\n  )\n  def testOneHotAddShapeSoft(self, one_hot_add_fn):\n    batch_size = 2\n    length = 4\n    vocab_size = 5\n    inputs = tf.random.uniform([batch_size, length, vocab_size])\n    shift = tf.random.uniform([batch_size, length, vocab_size])\n\n    outputs = one_hot_add_fn(inputs, shift)\n    self.assertEqual(outputs.shape, (batch_size, length, vocab_size))\n\n  def testMultiplicativeInverse(self):\n    batch_size = 3\n    vocab_size = 79\n    length = 5\n    inputs = np.random.randint(0, vocab_size - 1, size=(batch_size, length))\n    one_hot_inputs = tf.one_hot(inputs, depth=vocab_size)\n\n    one_hot_inv = ed.layers.utils.multiplicative_inverse(one_hot_inputs,\n                                                         vocab_size)\n    inv_inputs = tf.argmax(one_hot_inv, axis=-1)\n    inputs_inv_inputs = tf.math.floormod(inputs * inv_inputs, vocab_size)\n    self.assertAllEqual(inputs_inv_inputs, np.ones((batch_size, length)))\n\n  def testApproximatelyStochastic(self):\n    rng = np.random.RandomState(0)\n    tf.random.set_seed(1)\n    for dims in [2, 5, 10]:\n      for batch_size in [1, 2, 10]:\n        log_alpha = rng.randn(batch_size, dims, dims)\n        result = ed.layers.utils.sinkhorn(log_alpha)\n        self.assertAllClose(np.sum(result, 1),\n                            np.tile([1.0], (batch_size, dims)),\n                            atol=1e-3)\n        self.assertAllClose(np.sum(result, 2),\n                            np.tile([1.0], (batch_size, dims)),\n                            atol=1e-3)\n\n  def testSoftToHardPermutation(self):\n    """"""The solution of the matching for the identity matrix is range(N).""""""\n    dims = 10\n    identity = tf.eye(dims)\n    result_matching = ed.layers.utils.soft_to_hard_permutation(identity)\n    self.assertAllEqual(result_matching[0], np.eye(dims))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
experimental/attentive_uncertainty/contextual_bandits/benchmark.py,6,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n# pytype: disable=attribute-error\n""""""Benchmark script for the wheel bandit task.\n""""""\n\nimport os\nimport pickle\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom experimental.attentive_uncertainty import attention  # local file import\nfrom experimental.attentive_uncertainty.contextual_bandits import offline_contextual_bandits  # local file import\nfrom experimental.attentive_uncertainty.contextual_bandits import online_contextual_bandits  # local file import\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nfrom deep_contextual_bandits import contextual_bandit  # local file import\nfrom deep_contextual_bandits import neural_linear_sampling  # local file import\nfrom deep_contextual_bandits import posterior_bnn_sampling  # local file import\nfrom deep_contextual_bandits import uniform_sampling  # local file import\n\nfrom tensorflow.contrib import training as contrib_training\n\ngfile = tf.compat.v1.gfile\n\ntf.compat.v1.enable_eager_execution()\n\nFLAGS = flags.FLAGS\nFLAGS.set_default(\'alsologtostderr\', True)\nflags.DEFINE_string(\n    \'logdir\',\n    \'/tmp/bandits/\',\n    \'Base directory to save output.\')\nflags.DEFINE_integer(\n    \'trial_idx\',\n    0,\n    \'Rerun idx of problem instance.\')\nflags.DEFINE_float(\n    \'delta\',\n    0.5,\n    \'delta parameter for wheel bandit instance.\')\nflags.DEFINE_string(\n    \'modeldir\',\n    \'/tmp/wheel_bandit/models/multitask\',\n    \'Directory with pretrained models.\')\nflags.DEFINE_string(\n    \'savedir\',\n    \'/tmp/wheel_bandit/results/\',\n    \'Directory with saved pkl files for full results.\')\nflags.DEFINE_string(\n    \'ckptdir\',\n    \'/tmp/wheel_bandit/ckpts/\',\n    \'Directory with saved pkl files for full ckpts.\')\nflags.DEFINE_string(\n    \'datasetdir\',\n    \'/tmp/wheel_bandit/data/\',\n    \'Directory with saved data instances.\')\nflags.DEFINE_integer(\n    \'exp_idx\',\n    0,\n    \'Experiment idx of full run.\')\nflags.DEFINE_string(\n    \'prefix\',\n    \'best_\',\n    \'Prefix of best model ckpts.\')\nflags.DEFINE_string(\n    \'suffix\',\n    \'_mse.ckpt\',\n    \'Suffix of best model ckpts.\')\nflags.DEFINE_list(\n    \'algo_names\',\n    [\'uniform\', \'snp_posterior_gp_offline\'],\n    \'List of algorithms to benchmark.\')\n\ncontext_dim = 2\nnum_actions = 5\n\n\ndef run_contextual_bandit(dataset, algos, save_once=False, pkl_file=None):\n  """"""Run a contextual bandit problem on a set of algorithms.\n\n  Args:\n    dataset: Matrix where every row is a context + num_actions rewards.\n    algos: List of algorithms to use in the contextual bandit instance.\n    save_once: True if state has been saved once before\n    pkl_file: pickle file for saving state.\n\n  Returns:\n    h_actions: Matrix with actions: size (num_context, num_algorithms).\n    h_rewards: Matrix with rewards: size (num_context, num_algorithms).\n  """"""\n\n  num_contexts = dataset.shape[0]\n\n  # Create contextual bandit\n  cmab = contextual_bandit.ContextualBandit(context_dim, num_actions)\n  cmab.feed_data(dataset)\n  if not save_once or pkl_file is None:\n    h_actions = np.empty((0, len(algos)), float)\n    h_rewards = np.empty((0, len(algos)), float)\n\n    start_context = 0\n  else:\n    with gfile.Open(pkl_file, \'rb\') as infile:\n      saved_state = pickle.load(infile)\n      start_context = saved_state[\'start_context\']\n      algos[0].data_h.replace_data(\n          saved_state[\'contexts\'],\n          saved_state[\'actions\'],\n          saved_state[\'rewards\'])\n      h_actions = saved_state[\'h_actions\']\n      h_rewards = saved_state[\'h_rewards\']\n\n  # Run the contextual bandit process\n  for i in range(start_context, num_contexts):\n    context = cmab.context(i)\n    actions = [a.action(context) for a in algos]\n    rewards = [cmab.reward(i, action) for action in actions]\n\n    for j, a in enumerate(algos):\n      a.update(context, actions[j], rewards[j])\n\n    h_actions = np.vstack((h_actions, np.array(actions)))\n    h_rewards = np.vstack((h_rewards, np.array(rewards)))\n\n    if (i+1) % 500 == 0 and pkl_file is not None:\n      savedict = {\'h_rewards\': h_rewards,\n                  \'h_actions\': h_actions,\n                  \'contexts\': algos[0].data_h.contexts,\n                  \'actions\': algos[0].data_h.actions,\n                  \'rewards\': algos[0].data_h.rewards,\n                  \'start_context\': i+1}\n      with gfile.Open(pkl_file, \'wb\') as outfile:\n        pickle.dump(savedict, outfile)\n\n  return h_actions, h_rewards\n\n\ndef run_trial(trial_idx, delta, algo_names):\n  """"""Runs a trial of wheel bandit problem instance for a set of algorithms.""""""\n\n  all_algo_names = \'_\'.join(algo_names)\n  runfile = str(delta) + \'_\' + str(trial_idx) + \'_\' + all_algo_names + \'.pkl\'\n  savefile = os.path.join(FLAGS.savedir, runfile)\n  if gfile.Exists(savefile):\n    print(\'File exists...terminating\')\n    with gfile.Open(savefile, \'rb\') as infile:\n      saved_state = pickle.load(infile, encoding=\'latin-1\')\n    return saved_state[\'h_rewards\'], saved_state[\'time\']\n\n  filename = os.path.join(\n      FLAGS.datasetdir,\n      str(delta) + \'_\' + str(trial_idx) + \'.npz\')\n  with gfile.GFile(filename, \'r\') as f:\n    sampled_vals = np.load(f)\n    dataset = sampled_vals[\'dataset\']\n\n  x_hidden_size = 100\n  x_encoder_sizes = [x_hidden_size]*2\n\n  algos = []\n  ckptfile = None\n  save_once = False\n  for algo_name in algo_names:\n    if algo_name == \'uniform\':\n      hparams = contrib_training.HParams(num_actions=num_actions)\n      algos.append(uniform_sampling.UniformSampling(algo_name, hparams))\n    elif algo_name == \'neurolinear\':\n      hparams = contrib_training.HParams(\n          num_actions=num_actions,\n          context_dim=context_dim,\n          init_scale=0.3,\n          activation=tf.nn.relu,\n          output_activation=tf.nn.relu,\n          layer_sizes=x_encoder_sizes,\n          batch_size=512,\n          activate_decay=True,\n          initial_lr=0.1,\n          max_grad_norm=5.0,\n          show_training=False,\n          freq_summary=1000,\n          buffer_s=-1,\n          initial_pulls=2,\n          reset_lr=True,\n          lr_decay_rate=0.5,\n          training_freq=1,\n          training_freq_network=20,\n          training_epochs=50,\n          a0=12,\n          b0=30,\n          lambda_prior=23)\n      algos.append(neural_linear_sampling.NeuralLinearPosteriorSampling(\n          algo_name, hparams))\n    elif algo_name == \'multitaskgp\':\n      hparams_gp = contrib_training.HParams(\n          num_actions=num_actions,\n          num_outputs=num_actions,\n          context_dim=context_dim,\n          reset_lr=False,\n          learn_embeddings=True,\n          max_num_points=1000,\n          show_training=False,\n          freq_summary=1000,\n          batch_size=512,\n          keep_fixed_after_max_obs=True,\n          training_freq=20,\n          initial_pulls=2,\n          training_epochs=50,\n          lr=0.01,\n          buffer_s=-1,\n          initial_lr=0.001,\n          lr_decay_rate=0.0,\n          optimizer=\'RMS\',\n          task_latent_dim=5,\n          activate_decay=False)\n      algos.append(posterior_bnn_sampling.PosteriorBNNSampling(\n          algo_name, hparams_gp, \'GP\'))\n    elif algo_name[:3] == \'snp\' or algo_name[:3] == \'anp\':\n      hidden_size = 64\n      latent_units = 32\n      global_latent_net_sizes = [hidden_size]*2 + [2*latent_units]\n      if algo_name[:3] == \'snp\':\n        local_latent_net_sizes = [hidden_size]*3 + [2]\n      else:\n        local_latent_net_sizes = [hidden_size]*3 + [2*5]\n      x_y_encoder_sizes = [hidden_size]*3\n      heteroskedastic_net_sizes = None\n      mean_att_type = attention.laplace_attention\n      scale_att_type_1 = attention.laplace_attention\n      scale_att_type_2 = attention.laplace_attention\n      att_type = \'multihead\'\n      att_heads = 8\n      data_uncertainty = False\n      is_anp = True if algo_name[:3] == \'anp\' else False\n\n      hparams = contrib_training.HParams(\n          num_actions=num_actions,\n          context_dim=context_dim,\n          init_scale=0.3,\n          activation=tf.nn.relu,\n          output_activation=tf.nn.relu,\n          x_encoder_sizes=x_encoder_sizes,\n          x_y_encoder_sizes=x_y_encoder_sizes,\n          global_latent_net_sizes=global_latent_net_sizes,\n          local_latent_net_sizes=local_latent_net_sizes,\n          heteroskedastic_net_sizes=heteroskedastic_net_sizes,\n          att_type=att_type,\n          att_heads=att_heads,\n          mean_att_type=mean_att_type,\n          scale_att_type_1=scale_att_type_1,\n          scale_att_type_2=scale_att_type_2,\n          data_uncertainty=data_uncertainty,\n          batch_size=512,\n          activate_decay=True,\n          initial_lr=0.1,\n          max_grad_norm=5.0,\n          show_training=False,\n          freq_summary=1000,\n          buffer_s=-1,\n          initial_pulls=2,\n          reset_lr=True,\n          lr_decay_rate=0.5,\n          training_freq=10,\n          training_freq_network=20,\n          training_epochs=50,\n          uncertainty_type=\'attentive_freeform\',\n          local_variational=True,\n          model_path=None,\n          is_anp=is_anp)\n\n      config = algo_name.split(\'_\')\n      if config[1] == \'prior\':\n        hparams.set_hparam(\'local_variational\', False)\n\n      if config[2] == \'gp\':\n        hparams.set_hparam(\'uncertainty_type\', \'attentive_gp\')\n\n      if config[3] == \'warmstart\' or config[3] == \'offline\':\n        mfile = FLAGS.prefix + config[1] + \'_\' + config[2] + FLAGS.suffix\n        if algo_name[:3] == \'anp\':\n          mfile = \'anp_\' + mfile\n        mpath = os.path.join(FLAGS.modeldir, mfile)\n        hparams.set_hparam(\'model_path\', mpath)\n\n      if config[3] == \'online\' or config[3] == \'warmstart\':\n        algos.append(online_contextual_bandits.OnlineContextualBandits(\n            algo_name, hparams))\n      else:\n        algos.append(offline_contextual_bandits.OfflineContextualBandits(\n            algo_name, hparams))\n        ckptfile = os.path.join(FLAGS.ckptdir, runfile)\n        if gfile.Exists(ckptfile):\n          save_once = True\n\n  t_init = time.time()\n  print(\'started\')\n  _, h_rewards = run_contextual_bandit(\n      dataset,\n      algos,\n      save_once=save_once,\n      pkl_file=ckptfile)\n  t_final = time.time()\n\n  savedict = {\'h_rewards\': h_rewards, \'time\': t_final-t_init}\n  with gfile.Open(savefile, \'wb\') as outfile:\n    pickle.dump(savedict, outfile)\n  return h_rewards, t_final - t_init\n\n\ndef main(_):\n  run_trial(FLAGS.trial_idx, FLAGS.delta, FLAGS.algo_names)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
experimental/attentive_uncertainty/contextual_bandits/benchmark_gnp.py,6,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Benchmark script for the wheel bandit task.\n""""""\n\nimport os\nimport pickle\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom experimental.attentive_uncertainty.contextual_bandits import offline_contextual_bandits_gnp  # local file import\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nfrom deep_contextual_bandits import contextual_bandit  # local file import\nfrom deep_contextual_bandits import neural_linear_sampling  # local file import\nfrom deep_contextual_bandits import posterior_bnn_sampling  # local file import\nfrom deep_contextual_bandits import uniform_sampling  # local file import\n\nfrom tensorflow.contrib import training as contrib_training\n\ngfile = tf.compat.v1.gfile\n\ntf.compat.v1.enable_eager_execution()\n\nFLAGS = flags.FLAGS\nFLAGS.set_default(\'alsologtostderr\', True)\nflags.DEFINE_string(\n    \'logdir\',\n    \'/tmp/bandits/\',\n    \'Base directory to save output.\')\nflags.DEFINE_integer(\n    \'trial_idx\',\n    0,\n    \'Rerun idx of problem instance.\')\nflags.DEFINE_float(\n    \'delta\',\n    0.5,\n    \'delta parameter for wheel bandit instance.\')\nflags.DEFINE_string(\n    \'modeldir\',\n    \'/tmp/wheel_bandit/models/multitask\',\n    \'Directory with pretrained models.\')\nflags.DEFINE_string(\n    \'savedir\',\n    \'/tmp/wheel_bandit/results/gnp\',\n    \'Directory with saved pkl files for full results.\')\nflags.DEFINE_string(\n    \'ckptdir\',\n    \'/tmp/wheel_bandit/ckpts/gnp\',\n    \'Directory with saved pkl files for full ckpts.\')\nflags.DEFINE_string(\n    \'datasetdir\',\n    \'/tmp/wheel_bandit/data/\',\n    \'Directory with saved data instances.\')\nflags.DEFINE_integer(\n    \'exp_idx\',\n    0,\n    \'Experiment idx of full run.\')\nflags.DEFINE_string(\n    \'prefix\',\n    \'gnp_\',\n    \'Prefix of best model ckpts.\')\nflags.DEFINE_string(\n    \'suffix\',\n    \'.ckpt\',\n    \'Suffix of best model ckpts.\')\nflags.DEFINE_list(\n    \'algo_names\',\n    [\'uniform\', \'gnp_acns\'],\n    \'List of algorithms to benchmark.\')\n\ncontext_dim = 2\nnum_actions = 5\n\n\ndef run_trial(trial_idx, delta, algo_names):\n  """"""Runs a trial of wheel bandit problem instance for a set of algorithms.""""""\n\n  all_algo_names = \'_\'.join(algo_names)\n  runfile = str(delta) + \'_\' + str(trial_idx) + \'_\' + all_algo_names + \'.pkl\'\n  savefile = os.path.join(FLAGS.savedir, runfile)\n  if gfile.Exists(savefile):\n    print(\'File exists...terminating\')\n    print(savefile)\n    with gfile.Open(savefile, \'rb\') as infile:\n      saved_state = pickle.load(infile, encoding=\'latin-1\')\n    return saved_state[\'h_rewards\'], saved_state[\'time\']\n\n  filename = os.path.join(\n      FLAGS.datasetdir,\n      str(delta) + \'_\' + str(trial_idx) + \'.npz\')\n  with gfile.GFile(filename, \'r\') as f:\n    sampled_vals = np.load(f)\n    dataset = sampled_vals[\'dataset\']\n\n  x_hidden_size = 100\n  x_encoder_sizes = [x_hidden_size]*2\n\n  algos = []\n  ckptfile = None\n  save_once = False\n  for algo_name in algo_names:\n    if algo_name == \'uniform\':\n      hparams = contrib_training.HParams(num_actions=num_actions)\n      algos.append(uniform_sampling.UniformSampling(algo_name, hparams))\n    elif algo_name == \'neurolinear\':\n      hparams = contrib_training.HParams(\n          num_actions=num_actions,\n          context_dim=context_dim,\n          init_scale=0.3,\n          activation=tf.nn.relu,\n          output_activation=tf.nn.relu,\n          layer_sizes=x_encoder_sizes,\n          batch_size=512,\n          activate_decay=True,\n          initial_lr=0.1,\n          max_grad_norm=5.0,\n          show_training=False,\n          freq_summary=1000,\n          buffer_s=-1,\n          initial_pulls=2,\n          reset_lr=True,\n          lr_decay_rate=0.5,\n          training_freq=1,\n          training_freq_network=20,\n          training_epochs=50,\n          a0=12,\n          b0=30,\n          lambda_prior=23)\n      algos.append(neural_linear_sampling.NeuralLinearPosteriorSampling(\n          algo_name, hparams))\n    elif algo_name == \'multitaskgp\':\n      hparams_gp = contrib_training.HParams(\n          num_actions=num_actions,\n          num_outputs=num_actions,\n          context_dim=context_dim,\n          reset_lr=False,\n          learn_embeddings=True,\n          max_num_points=1000,\n          show_training=False,\n          freq_summary=1000,\n          batch_size=512,\n          keep_fixed_after_max_obs=True,\n          training_freq=20,\n          initial_pulls=2,\n          training_epochs=50,\n          lr=0.01,\n          buffer_s=-1,\n          initial_lr=0.001,\n          lr_decay_rate=0.0,\n          optimizer=\'RMS\',\n          task_latent_dim=5,\n          activate_decay=False)\n      algos.append(posterior_bnn_sampling.PosteriorBNNSampling(\n          algo_name, hparams_gp, \'GP\'))\n    elif algo_name[:3] == \'gnp\':\n      hidden_size = 64\n      x_encoder_net_sizes = None\n      decoder_net_sizes = [hidden_size]*3 + [2*num_actions]\n      heteroskedastic_net_sizes = None\n      att_type = \'multihead\'\n      att_heads = 8\n      data_uncertainty = False\n\n      config = algo_name.split(\'_\')\n      model_type = config[1]\n\n      if algo_name[:len(\'gnp_anp_beta_\')] == \'gnp_anp_beta_\':\n        mfile = algo_name + FLAGS.suffix\n        x_y_encoder_net_sizes = [hidden_size]*3\n        global_latent_net_sizes = [hidden_size]*2\n        local_latent_net_sizes = None\n        beta = float(config[3])\n        temperature = float(config[5])\n      else:\n        mfile = FLAGS.prefix + config[1] + FLAGS.suffix\n\n        if model_type == \'cnp\':\n          x_y_encoder_net_sizes = [hidden_size]*4\n          global_latent_net_sizes = None\n          local_latent_net_sizes = None\n        elif model_type == \'np\':\n          x_y_encoder_net_sizes = [hidden_size]*2\n          global_latent_net_sizes = [hidden_size]*2\n          local_latent_net_sizes = None\n        elif model_type == \'anp\':\n          x_y_encoder_net_sizes = [hidden_size]*2\n          global_latent_net_sizes = [hidden_size]*2\n          local_latent_net_sizes = None\n        elif model_type == \'acnp\':\n          x_y_encoder_net_sizes = [hidden_size]*4\n          global_latent_net_sizes = None\n          local_latent_net_sizes = None\n        elif model_type == \'acns\':\n          x_y_encoder_net_sizes = [hidden_size]*2\n          global_latent_net_sizes = [hidden_size]*2\n          local_latent_net_sizes = [hidden_size]*2\n\n        beta = 1.\n        temperature = 1.\n\n      mpath = os.path.join(FLAGS.modeldir, mfile)\n\n      hparams = contrib_training.HParams(\n          num_actions=num_actions,\n          context_dim=context_dim,\n          init_scale=0.3,\n          activation=tf.nn.relu,\n          output_activation=tf.nn.relu,\n          x_encoder_net_sizes=x_encoder_net_sizes,\n          x_y_encoder_net_sizes=x_y_encoder_net_sizes,\n          global_latent_net_sizes=global_latent_net_sizes,\n          local_latent_net_sizes=local_latent_net_sizes,\n          decoder_net_sizes=decoder_net_sizes,\n          heteroskedastic_net_sizes=heteroskedastic_net_sizes,\n          att_type=att_type,\n          att_heads=att_heads,\n          model_type=model_type,\n          data_uncertainty=data_uncertainty,\n          beta=beta,\n          temperature=temperature,\n          model_path=mpath,\n          batch_size=512,\n          activate_decay=True,\n          initial_lr=0.1,\n          max_grad_norm=5.0,\n          show_training=False,\n          freq_summary=1000,\n          buffer_s=-1,\n          initial_pulls=2,\n          reset_lr=True,\n          lr_decay_rate=0.5,\n          training_freq=10,\n          training_freq_network=20,\n          training_epochs=50)\n      algos.append(offline_contextual_bandits_gnp.OfflineContextualBandits(\n          algo_name, hparams))\n      ckptfile = os.path.join(FLAGS.ckptdir, runfile)\n      if gfile.Exists(ckptfile):\n        save_once = True\n\n  t_init = time.time()\n  print(\'started\')\n  print([algo.name for algo in algos])\n  _, h_rewards = contextual_bandit.run_contextual_bandit_new(  # pytype: disable=module-attr\n      context_dim,\n      num_actions,\n      dataset,\n      algos,\n      save_once=save_once,\n      pkl_file=ckptfile)\n  t_final = time.time()\n\n  savedict = {\'h_rewards\': h_rewards, \'time\': t_final-t_init}\n  with gfile.Open(savefile, \'wb\') as outfile:\n    pickle.dump(savedict, outfile)\n  return h_rewards, t_final - t_init\n\n\ndef main(_):\n  run_trial(FLAGS.trial_idx, FLAGS.delta, FLAGS.algo_names)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
experimental/attentive_uncertainty/contextual_bandits/datasets.py,1,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Generates and saves instances of wheel bandit problems.\n""""""\n\nimport os\nfrom absl import app\nfrom absl import flags\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nfrom deep_contextual_bandits import synthetic_data_sampler  # local file import\ngfile = tf.compat.v1.gfile\n\nFLAGS = flags.FLAGS\nflags.DEFINE_integer(\n    \'num_instances\',\n    100,\n    \'Number of contexts.\')\nflags.DEFINE_integer(\n    \'num_contexts\',\n    80000,\n    \'Number of contexts.\')\nflags.DEFINE_string(\n    \'datadir\',\n    \'/tmp/wheel_bandit/data/\',\n    \'Directory for saving npz files.\')\nflags.DEFINE_list(\'deltas\', [\'0.5\', \'0.7\', \'0.9\', \'0.95\', \'0.99\'],\n                  \'List of deltas for wheel bandit.\')\n\n\ndef get_wheel_data(num_contexts, delta, seed):\n  """"""Samples wheel bandit data according to the benchmark configuration.""""""\n  mean_v = [1.0, 1.0, 1.0, 1.0, 1.2]\n  std_v = [0.01, 0.01, 0.01, 0.01, 0.01]\n  mu_large = 50\n  std_large = 0.01\n\n  # For reproducible generation.\n  np.random.seed(int(100*delta) + seed)\n  dataset, opt_wheel = synthetic_data_sampler.sample_wheel_bandit_data(\n      num_contexts,\n      delta,\n      mean_v,\n      std_v,\n      mu_large,\n      std_large)\n  opt_rewards, opt_actions = opt_wheel\n  return dataset, opt_rewards, opt_actions\n\n\ndef main(argv):\n  del argv  # unused arg\n  num_instances = FLAGS.num_instances\n  num_contexts = FLAGS.num_contexts\n  datadir = FLAGS.datadir\n  deltas = FLAGS.deltas\n\n  for delta_str in deltas:\n    delta = float(delta_str)\n    print(\'Delta\', delta)\n    for i in range(num_instances):\n      print(\'Instance\', i)\n      dataset, opt_rewards, opt_actions = get_wheel_data(num_contexts, delta, i)\n      filename = os.path.join(datadir, str(delta) + \'_\' + str(i) + \'.npz\')\n      with gfile.GFile(filename, \'w\') as f:\n        np.savez(\n            f,\n            dataset=dataset,\n            opt_rewards=opt_rewards,\n            opt_actions=opt_actions)\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
experimental/attentive_uncertainty/contextual_bandits/offline_contextual_bandits.py,13,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Thompson sampling for contextual bandit problems via offline SNPs.\n""""""\n\nfrom experimental.attentive_uncertainty import regressor  # local file import\nfrom experimental.attentive_uncertainty.contextual_bandits import utils  # local file import\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nfrom deep_contextual_bandits import bandit_algorithm  # local file import\nfrom deep_contextual_bandits import contextual_dataset  # local file import\n\n\nclass OfflineContextualBandits(bandit_algorithm.BanditAlgorithm):\n  """"""Thompson sampling via offline strutured neural processes.""""""\n\n  def __init__(self,\n               name,\n               hparams):\n    self.name = name\n    self.hparams = hparams\n    self.verbose = getattr(hparams, \'verbose\', True)\n    self._is_anp = getattr(hparams, \'is_anp\', False)\n    if self._is_anp:\n      input_dim = hparams.context_dim\n      output_dim = hparams.num_actions\n    else:\n      input_dim = hparams.context_dim + hparams.num_actions\n      output_dim = 1\n\n    self.t = 0\n    self.data_h = contextual_dataset.ContextualDataset(\n        hparams.context_dim, hparams.num_actions, intercept=False)\n\n    if self.verbose:\n      print(\'Initializing model {}.\'.format(self.name))\n    self.snp = regressor.Regressor(\n        input_dim=input_dim,\n        output_dim=output_dim,\n        x_encoder_sizes=hparams.x_encoder_sizes,\n        x_y_encoder_sizes=hparams.x_y_encoder_sizes,\n        global_latent_net_sizes=hparams.global_latent_net_sizes,\n        local_latent_net_sizes=hparams.local_latent_net_sizes,\n        heteroskedastic_net_sizes=hparams.heteroskedastic_net_sizes,\n        att_type=hparams.att_type,\n        att_heads=hparams.att_heads,\n        uncertainty_type=hparams.uncertainty_type,\n        mean_att_type=hparams.mean_att_type,\n        scale_att_type_1=hparams.scale_att_type_1,\n        scale_att_type_2=hparams.scale_att_type_2,\n        activation=hparams.activation,\n        output_activation=hparams.output_activation,\n        data_uncertainty=hparams.data_uncertainty,\n        local_variational=hparams.local_variational,\n        model_path=hparams.model_path)\n\n    self._one_hot_vectors = tf.one_hot(\n        indices=np.arange(hparams.num_actions),\n        depth=hparams.num_actions)\n\n  def action(self, context):\n    """"""Samples rewards from posterior, and chooses best action accordingly.\n\n    Args:\n      context: A d-dimensional np.ndarray with the context.\n\n    Returns:\n      Greedy action based on Thompson sampling.\n    """"""\n    # Round robin until each action has been selected ""initial_pulls"" times\n    if self.t < self.hparams.num_actions * self.hparams.initial_pulls:\n      return self.t % self.hparams.num_actions\n\n    vals = []\n\n    context = tf.to_float(context)\n    if self._is_anp:\n      contexts, rewards, actions = self.data_h.get_data_with_weights()\n      historical_x = tf.to_float(tf.expand_dims(contexts, axis=0))\n      historical_y = tf.to_float(tf.expand_dims(rewards*actions, axis=0))\n      target_x = tf.expand_dims(tf.reshape(context, [1, -1]), axis=0)\n    else:\n      contexts, rewards, actions = utils.get_data_with_masked_rewards(\n          self.data_h)\n      context_action_pairs = tf.concat([contexts, actions], axis=-1)\n\n      historical_x = tf.to_float(tf.expand_dims(context_action_pairs, axis=0))\n      historical_y = tf.to_float(rewards.reshape(1, -1, 1))\n      tiled_context = tf.concat(\n          [tf.tile(tf.reshape(context, [1, -1]), [self.hparams.num_actions, 1]),\n           self._one_hot_vectors], axis=-1\n      )\n      target_x = tf.expand_dims(tiled_context, axis=0)\n    target_y = None\n\n    predictions = self.snp(historical_x, historical_y, target_x, target_y)\n    vals = tf.squeeze(predictions.distribution.mean())\n\n    return tf.argmax(vals).numpy()\n\n  def update(self, context, action, reward):\n    """"""Updates the posterior of the SNP model.\n\n    For an offline SNP model, the posterior gets directly updated by\n    updating the observed dataset. No parameter updates are needed.\n\n    Args:\n      context: A d-dimensional np.ndarray with the context.\n      action: Integer between 0 and k-1 representing the chosen arm.\n      reward: Real number representing the reward for the (context, action).\n\n    Returns:\n      None.\n    """"""\n    self.t += 1\n    self.data_h.add(context, action, reward)\n    if self.verbose and self.t % 100 == 0:\n      print(\'Number of contexts=\', self.t)\n\n'"
experimental/attentive_uncertainty/contextual_bandits/offline_contextual_bandits_gnp.py,6,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Thompson sampling for contextual bandit problems via offline GNPs.\n""""""\n\nfrom experimental.attentive_uncertainty import generalized_neural_process  # local file import\nimport tensorflow.compat.v1 as tf\n\nfrom deep_contextual_bandits import bandit_algorithm  # local file import\nfrom deep_contextual_bandits import contextual_dataset  # local file import\n\n\nclass OfflineContextualBandits(bandit_algorithm.BanditAlgorithm):\n  """"""Thompson sampling via offline neural processes.""""""\n\n  def __init__(self,\n               name,\n               hparams):\n    self.name = name\n    self.hparams = hparams\n    self.verbose = getattr(hparams, \'verbose\', True)\n\n    self.t = 0\n    self.data_h = contextual_dataset.ContextualDataset(\n        hparams.context_dim, hparams.num_actions, intercept=False)\n\n    if self.verbose:\n      print(\'Initializing model {}.\'.format(self.name))\n    self.gnp = generalized_neural_process.Regressor(\n        input_dim=hparams.context_dim,\n        output_dim=hparams.num_actions,\n        x_encoder_net_sizes=hparams.x_encoder_net_sizes,\n        x_y_encoder_net_sizes=hparams.x_y_encoder_net_sizes,\n        global_latent_net_sizes=hparams.global_latent_net_sizes,\n        local_latent_net_sizes=hparams.local_latent_net_sizes,\n        decoder_net_sizes=hparams.decoder_net_sizes,\n        heteroskedastic_net_sizes=hparams.heteroskedastic_net_sizes,\n        att_type=hparams.att_type,\n        att_heads=hparams.att_heads,\n        model_type=hparams.model_type,\n        activation=hparams.activation,\n        output_activation=hparams.output_activation,\n        data_uncertainty=hparams.data_uncertainty,\n        beta=hparams.beta,\n        temperature=hparams.temperature,\n        model_path=hparams.model_path)\n\n  def action(self, context):\n    """"""Samples rewards from posterior, and chooses best action accordingly.\n\n    Args:\n      context: A d-dimensional np.ndarray with the context.\n\n    Returns:\n      Greedy action based on Thompson sampling.\n    """"""\n    # Round robin until each action has been selected ""initial_pulls"" times\n    if self.t < self.hparams.num_actions * self.hparams.initial_pulls:\n      return self.t % self.hparams.num_actions\n\n    vals = []\n\n    context = tf.to_float(context)\n    contexts, rewards, actions = self.data_h.get_data_with_weights()\n    historical_x = tf.to_float(tf.expand_dims(contexts, axis=0))\n    historical_y = tf.to_float(tf.expand_dims(rewards*actions, axis=0))\n    target_x = tf.expand_dims(tf.reshape(context, [1, -1]), axis=0)\n    target_y = None\n\n    predictions = self.gnp(historical_x, historical_y, target_x, target_y)\n    vals = tf.squeeze(predictions.distribution.mean())\n\n    return tf.argmax(vals).numpy()\n\n  def update(self, context, action, reward):\n    """"""Updates the posterior of the SNP model.\n\n    For an offline SNP model, the posterior gets directly updated by\n    updating the observed dataset. No parameter updates are needed.\n\n    Args:\n      context: A d-dimensional np.ndarray with the context.\n      action: Integer between 0 and k-1 representing the chosen arm.\n      reward: Real number representing the reward for the (context, action).\n\n    Returns:\n      None.\n    """"""\n    self.t += 1\n    self.data_h.add(context, action, reward)\n    if self.verbose and self.t % 100 == 0:\n      print(\'Number of contexts=\', self.t)\n\n\n'"
experimental/attentive_uncertainty/contextual_bandits/online_contextual_bandits.py,20,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Thompson sampling for contextual bandit problems via online learning of SNPs.\n""""""\n\nfrom experimental.attentive_uncertainty import regressor  # local file import\nfrom experimental.attentive_uncertainty import utils  # local file import\nfrom experimental.attentive_uncertainty.contextual_bandits import utils as bandit_utils  # local file import\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nfrom deep_contextual_bandits import bandit_algorithm  # local file import\nfrom deep_contextual_bandits import contextual_dataset  # local file import\n\n\nclass OnlineContextualBandits(bandit_algorithm.BanditAlgorithm):\n  """"""Thompson sampling via online structured neural processes.""""""\n\n  def __init__(self,\n               name,\n               hparams,\n               optimizer=\'RMS\'):\n    self.name = name\n    self.hparams = hparams\n    self.verbose = getattr(hparams, \'verbose\', True)\n\n    self.update_freq_lr = hparams.training_freq\n    self.update_freq_nn = hparams.training_freq_network\n\n    self.t = 0\n    self.num_epochs = hparams.training_epochs\n    self.data_h = contextual_dataset.ContextualDataset(\n        hparams.context_dim, hparams.num_actions, intercept=False)\n\n    self.gradient_updates = tf.Variable(0, trainable=False)\n    if self.hparams.activate_decay:\n      self.lr = tf.train.inverse_time_decay(\n          self.hparams.initial_lr, self.gradient_updates,\n          1, self.hparams.lr_decay_rate)\n    else:\n      self.lr = tf.Variable(self.hparams.initial_lr, trainable=False)\n    optimizer = tf.train.RMSPropOptimizer(self.lr)\n    self._optimizer_config = {\'optimizer\': optimizer,\n                              \'max_grad_norm\': hparams.max_grad_norm}\n\n    if self.verbose:\n      print(\'Initializing model {}.\'.format(self.name))\n    self.snp = regressor.Regressor(\n        input_dim=hparams.context_dim + hparams.num_actions,\n        output_dim=1,\n        x_encoder_sizes=hparams.x_encoder_sizes,\n        x_y_encoder_sizes=hparams.x_y_encoder_sizes,\n        global_latent_net_sizes=hparams.global_latent_net_sizes,\n        local_latent_net_sizes=hparams.local_latent_net_sizes,\n        heteroskedastic_net_sizes=hparams.heteroskedastic_net_sizes,\n        att_type=hparams.att_type,\n        att_heads=hparams.att_heads,\n        uncertainty_type=hparams.uncertainty_type,\n        mean_att_type=hparams.mean_att_type,\n        scale_att_type_1=hparams.scale_att_type_1,\n        scale_att_type_2=hparams.scale_att_type_2,\n        activation=hparams.activation,\n        output_activation=hparams.output_activation,\n        data_uncertainty=hparams.data_uncertainty,\n        local_variational=hparams.local_variational,\n        model_path=hparams.model_path)\n\n    self._step = tf.function(utils.mse_step.python_function)  # pytype: disable=module-attr\n\n    self._one_hot_vectors = tf.one_hot(\n        indices=np.arange(hparams.num_actions),\n        depth=hparams.num_actions)\n\n  def action(self, context):\n    """"""Samples rewards from posterior, and chooses best action accordingly.""""""\n\n    # Round robin until each action has been selected ""initial_pulls"" times\n    if self.t < self.hparams.num_actions * self.hparams.initial_pulls:\n      return self.t % self.hparams.num_actions\n\n    vals = []\n    states, rewards, actions = bandit_utils.get_data_with_masked_rewards(\n        self.data_h)\n    state_action_pairs = tf.concat([states, actions], axis=-1)\n\n    historical_x = tf.to_float(tf.expand_dims(state_action_pairs, axis=0))\n    historical_y = tf.to_float(rewards.reshape(1, -1, 1))\n\n    context = tf.to_float(context)\n    tiled_context = tf.concat(\n        [tf.tile(tf.reshape(context, [1, -1]), [self.hparams.num_actions, 1]),\n         self._one_hot_vectors], axis=-1\n    )\n    target_x = tf.expand_dims(tiled_context, axis=0)\n    target_y = None\n\n    prediction = self.snp(historical_x, historical_y, target_x, target_y)\n    vals = tf.squeeze(prediction.distribution.mean())\n\n    return tf.argmax(vals).numpy()\n\n  def update(self, context, action, reward):\n    """"""Updates the posterior.""""""\n\n    self.t += 1\n    self.data_h.add(context, action, reward)\n\n    # Retrain the network on the original data (data_h)\n    if self.t % self.update_freq_nn == 0:\n      print(\'Number of contexts observed=\', self.t)\n      if self.hparams.reset_lr:\n        self.lr = self.hparams.initial_lr\n      self.train(self.data_h, self.num_epochs)\n\n  def train(self, data, num_steps):\n    """"""Trains the network for num_steps, using the provided data.\n\n    Args:\n      data: ContextualDataset object that provides the data.\n      num_steps: Number of minibatches to train the network for.\n    """"""\n\n    if self.verbose:\n      print(\'Training {} for {} steps...\'.format(self.name, num_steps))\n\n    avg_nll, avg_mse, avg_local_z_kl, avg_global_z_kl = 0., 0., 0., 0.\n    for _ in range(num_steps):\n      states, rewards, actions = bandit_utils.get_batch_with_masked_rewards(\n          self.data_h,\n          self.hparams.batch_size)\n      state_action_pairs = tf.concat([states, actions], axis=-1)\n      target_x = tf.to_float(tf.expand_dims(state_action_pairs, axis=0))\n      target_y = tf.to_float(rewards.reshape(1, -1, 1))\n      num_historical = tf.random.uniform(\n          (), 1, max(2, target_x.shape[1]), dtype=tf.dtypes.int32)\n      historical_x = target_x[:, :num_historical]\n      historical_y = target_y[:, :num_historical]\n      unseen_targets = target_y[:, num_historical:]\n      data = (historical_x, historical_y, target_x, target_y, unseen_targets)\n\n      nll, mse, local_z_kl, global_z_kl = self._step(self.snp,\n                                                     data,\n                                                     self._optimizer_config)\n      avg_nll += nll\n      avg_mse += mse\n      avg_local_z_kl += local_z_kl\n      avg_global_z_kl += global_z_kl\n      self.gradient_updates.assign_add(1)\n\n    if self.verbose:\n      avg_nll /= num_steps\n      avg_mse /= num_steps\n      avg_local_z_kl /= num_steps\n      avg_global_z_kl /= num_steps\n      print(\'Average nll: {}, mse: {}, local kl: {} global kl: {}\'\n            .format(avg_nll, avg_mse, avg_local_z_kl, avg_global_z_kl))\n'"
experimental/attentive_uncertainty/contextual_bandits/pretrain.py,7,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Pretrains structured neural processes for the wheel bandit task.\n""""""\n\nfrom experimental.attentive_uncertainty import regressor  # local file import\nfrom experimental.attentive_uncertainty import utils  # local file import\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\ntf.compat.v1.enable_eager_execution()\n\n\ndef sample_single_wheel_bandit_data(num_datapoints,\n                                    num_actions,\n                                    context_dim,\n                                    delta,\n                                    mean_v,\n                                    std_v,\n                                    mu_large,\n                                    std_large):\n  """"""Samples from Wheel bandit game (see Riquelme et al. (2018)).\n\n  Args:\n    num_datapoints: Number (n) of (context, action, reward) triplets to sample.\n    num_actions: (a) Number of actions.\n    context_dim: (c) Number of dimensions in the context.\n    delta: Exploration parameter: high reward in one region if norm above delta.\n    mean_v: Mean reward for each action if context norm is below delta.\n    std_v: Gaussian reward std for each action if context norm is below delta.\n    mu_large: Mean reward for optimal action if context norm is above delta.\n    std_large: Reward std for optimal action if context norm is above delta.\n\n  Returns:\n    context_action_pairs: Sampled (context, action) matrix of size (n, c + a).\n    rewards: Sampled reward matrix of size (n, 1).\n  """"""\n  data = []\n  actions = []\n  rewards = []\n\n  # Sample uniform contexts in unit ball.\n  while len(data) < num_datapoints:\n    raw_data = np.random.uniform(-1, 1, (int(num_datapoints / 3), context_dim))\n\n    for i in range(raw_data.shape[0]):\n      if np.linalg.norm(raw_data[i, :]) <= 1:\n        data.append(raw_data[i, :])\n\n  contexts = np.stack(data)[:num_datapoints, :]\n\n  # Sample rewards and random actions.\n  for i in range(num_datapoints):\n    r = [np.random.normal(mean_v[j], std_v[j]) for j in range(num_actions)]\n    if np.linalg.norm(contexts[i, :]) >= delta:\n      # Large reward in the right region for the context.\n      r_big = np.random.normal(mu_large, std_large)\n      if contexts[i, 0] > 0:\n        if contexts[i, 1] > 0:\n          r[0] = r_big\n        else:\n          r[1] = r_big\n      else:\n        if contexts[i, 1] > 0:\n          r[2] = r_big\n        else:\n          r[3] = r_big\n    one_hot_vector = np.zeros((5))\n    random_action = np.random.randint(num_actions)\n    one_hot_vector[random_action] = 1\n    actions.append(one_hot_vector)\n    rewards.append(r[random_action])\n\n  rewards = np.expand_dims(np.array(rewards), -1)\n  context_action_pairs = np.hstack([contexts, actions])\n  perm = np.random.permutation(len(rewards))\n  return context_action_pairs[perm, :], rewards[perm, :]\n\n\ndef get_single_wheel_data(num_datapoints, num_actions, context_dim, delta):\n  """"""Samples data for a single wheel with default benchmark configuration.\n\n  Args:\n    num_datapoints: Number (n) of (context, action, reward) triplets to sample.\n    num_actions: (a) Number of actions.\n    context_dim: (c) Number of dimensions in the context.\n    delta: Exploration parameter: high reward in one region if norm above delta.\n\n  Returns:\n    context_action_pairs: Sampled (context, action) matrix of size (n, c + a).\n    rewards: Sampled reward matrix of size (n, 1).\n  """"""\n  mean_v = [1.0, 1.0, 1.0, 1.0, 1.2]\n  std_v = [0.01, 0.01, 0.01, 0.01, 0.01]\n  mu_large = 50\n  std_large = 0.01\n  context_action_pairs, rewards = sample_single_wheel_bandit_data(\n      num_datapoints,\n      num_actions,\n      context_dim,\n      delta,\n      mean_v,\n      std_v,\n      mu_large,\n      std_large)\n  return context_action_pairs, rewards\n\n\ndef procure_dataset(hparams, num_wheels, seed=0):\n  """"""Samples the full dataset for pretraining GNPs.""""""\n  np.random.seed(seed)\n\n  all_context_action_pairs, all_rewards = [], []\n  for _ in range(num_wheels):\n    delta = np.random.uniform()\n    context_action_pairs, rewards = get_single_wheel_data(\n        hparams.num_target + hparams.num_context,\n        hparams.num_actions,\n        hparams.context_dim,\n        delta)\n    all_context_action_pairs.append(context_action_pairs)\n    all_rewards.append(rewards)\n\n  all_context_action_pairs = np.stack(all_context_action_pairs)\n  all_rewards = np.stack(all_rewards)\n  return all_context_action_pairs, all_rewards\n\n\ndef get_splits(dataset, n_context, batch_size, points_perm=True):\n  """"""Splits the dataset into target and context sets.""""""\n  full_x, full_y = dataset\n  dataset_perm = np.random.permutation(len(full_x))[:batch_size]\n  if points_perm:\n    datapoints_perm = np.random.permutation(full_x.shape[1])\n  else:\n    datapoints_perm = np.arange(full_x.shape[1])\n\n  target_x = tf.to_float(full_x[dataset_perm[:, None], datapoints_perm])\n  target_y = tf.to_float(full_y[dataset_perm[:, None], datapoints_perm])\n  context_x = target_x[:, :n_context, :]\n  context_y = target_y[:, :n_context, :]\n  unseen_targets = target_y[:, n_context:]\n  return context_x, context_y, target_x, target_y, unseen_targets\n\n\ndef training_loop(train_dataset,\n                  valid_dataset,\n                  model,\n                  hparams):\n  """"""Trains an SNP for a fixed number of iterations.""""""\n  optimizer_config = {\'optimizer\': hparams.optimizer(hparams.lr),\n                      \'max_grad_norm\': hparams.max_grad_norm}\n  num_context = hparams.num_context\n  best_mse = np.inf\n  step = tf.function(utils.mse_step.python_function)  # pytype: disable=module-attr\n\n  for it in range(hparams.num_iterations):\n    batch_train_data = get_splits(\n        train_dataset,\n        num_context,\n        hparams.batch_size,\n        points_perm=True)\n    nll, mse, local_z_kl, global_z_kl = step(\n        model,\n        batch_train_data,\n        optimizer_config)\n\n    if it % hparams.print_every == 0:\n      (batch_context_x,\n       batch_context_y,\n       batch_target_x,\n       batch_target_y,\n       batch_unseen_targets) = get_splits(valid_dataset,\n                                          num_context,\n                                          hparams.batch_size,\n                                          points_perm=False)\n      prediction = model(batch_context_x,\n                         batch_context_y,\n                         batch_target_x,\n                         batch_target_y)\n\n      batch_unseen_predictions = prediction[:, num_context:]\n      valid_nll = utils.nll(batch_unseen_targets, batch_unseen_predictions)\n      valid_mse = utils.mse(batch_unseen_targets, batch_unseen_predictions)\n      if model.local_variational:\n        valid_local_kl = tf.reduce_mean(\n            tf.reduce_sum(model.losses[-1][:, num_context:], axis=[1, 2]))\n      else:\n        valid_local_kl = 0.\n      valid_global_kl = tf.reduce_mean(tf.reduce_sum(model.losses[-2], axis=-1))\n\n      print(\'it: {}, train nll: {}, mse: {}, local kl: {} global kl: {} \'\n            \'valid nll: {}, mse: {}, local kl: {} global kl: {}\'\n            .format(it, nll, mse, local_z_kl, global_z_kl,\n                    valid_nll, valid_mse, valid_local_kl, valid_global_kl))\n      if valid_mse.numpy() < best_mse:\n        best_mse = valid_mse.numpy()\n        print(\'Saving best model with MSE\', best_mse)\n        model.save_weights(hparams.save_path)\n\n\ndef train(data_hparams,\n          model_hparams,\n          training_hparams):\n  """"""Executes the training pipeline for SNPs.""""""\n  all_context_action_pairs, all_rewards = procure_dataset(\n      data_hparams,\n      num_wheels=100,\n      seed=0)\n  train_dataset = (all_context_action_pairs, all_rewards)\n\n  all_context_action_pairs, all_rewards = procure_dataset(\n      data_hparams,\n      num_wheels=10,\n      seed=42)\n  valid_dataset = (all_context_action_pairs, all_rewards)\n\n  model = regressor.Regressor(\n      input_dim=data_hparams.context_dim + data_hparams.num_actions,\n      output_dim=1,\n      x_encoder_sizes=model_hparams.x_encoder_sizes,\n      x_y_encoder_sizes=model_hparams.x_y_encoder_sizes,\n      global_latent_net_sizes=model_hparams.global_latent_net_sizes,\n      local_latent_net_sizes=model_hparams.local_latent_net_sizes,\n      heteroskedastic_net_sizes=model_hparams.heteroskedastic_net_sizes,\n      att_type=model_hparams.att_type,\n      att_heads=model_hparams.att_heads,\n      uncertainty_type=model_hparams.uncertainty_type,\n      mean_att_type=model_hparams.mean_att_type,\n      scale_att_type_1=model_hparams.scale_att_type_1,\n      scale_att_type_2=model_hparams.scale_att_type_2,\n      activation=model_hparams.activation,\n      output_activation=model_hparams.output_activation,\n      data_uncertainty=model_hparams.data_uncertainty,\n      local_variational=model_hparams.local_variational)\n\n  training_loop(train_dataset,\n                valid_dataset,\n                model,\n                training_hparams)\n\n'"
experimental/attentive_uncertainty/contextual_bandits/pretrain_gnp.py,6,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Pretrains generalized neural processes for the wheel bandit task.\n""""""\n\nfrom experimental.attentive_uncertainty import generalized_neural_process  # local file import\nfrom experimental.attentive_uncertainty.contextual_bandits import utils  # local file import\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\ntf.compat.v1.enable_eager_execution()\n\n\ndef sample_single_wheel_bandit_data(num_datapoints,\n                                    num_actions,\n                                    context_dim,\n                                    delta,\n                                    mean_v,\n                                    std_v,\n                                    mu_large,\n                                    std_large):\n  """"""Samples from Wheel bandit game (see Riquelme et al. (2018)).\n\n  Args:\n    num_datapoints: Number (n) of (context, action, reward) triplets to sample.\n    num_actions: (a) Number of actions.\n    context_dim: (c) Number of dimensions in the context.\n    delta: Exploration parameter: high reward in one region if norm above delta.\n    mean_v: Mean reward for each action if context norm is below delta.\n    std_v: Gaussian reward std for each action if context norm is below delta.\n    mu_large: Mean reward for optimal action if context norm is above delta.\n    std_large: Reward std for optimal action if context norm is above delta.\n\n  Returns:\n    contexts: Sampled context matrix of size (n, c).\n    actions: Sampled action matrix of size (n, a).\n    rewards: Sampled reward matrix of size (n, 1).\n  """"""\n  data = []\n  actions = []\n  rewards = []\n\n  # Sample uniform contexts in unit ball.\n  while len(data) < num_datapoints:\n    raw_data = np.random.uniform(-1, 1, (int(num_datapoints / 3), context_dim))\n\n    for i in range(raw_data.shape[0]):\n      if np.linalg.norm(raw_data[i, :]) <= 1:\n        data.append(raw_data[i, :])\n\n  contexts = np.stack(data)[:num_datapoints, :]\n\n  # Sample rewards and random actions.\n  for i in range(num_datapoints):\n    r = [np.random.normal(mean_v[j], std_v[j]) for j in range(num_actions)]\n    if np.linalg.norm(contexts[i, :]) >= delta:\n      # Large reward in the right region for the context.\n      r_big = np.random.normal(mu_large, std_large)\n      if contexts[i, 0] > 0:\n        if contexts[i, 1] > 0:\n          r[0] = r_big\n        else:\n          r[1] = r_big\n      else:\n        if contexts[i, 1] > 0:\n          r[2] = r_big\n        else:\n          r[3] = r_big\n    one_hot_vector = np.zeros((5))\n    random_action = np.random.randint(num_actions)\n    one_hot_vector[random_action] = 1\n    actions.append(one_hot_vector)\n    rewards.append(r[random_action])\n\n  actions = np.stack(actions)\n  rewards = np.expand_dims(np.array(rewards), -1)\n  perm = np.random.permutation(len(rewards))\n  return contexts[perm, :], actions[perm, :], rewards[perm, :]\n\n\ndef get_single_wheel_data(num_datapoints, num_actions, context_dim, delta):\n  """"""Samples data for a single wheel with default benchmark configuration.\n\n  Args:\n    num_datapoints: Number (n) of (context, action, reward) triplets to sample.\n    num_actions: (a) Number of actions.\n    context_dim: (c) Number of dimensions in the context.\n    delta: Exploration parameter: high reward in one region if norm above delta.\n\n  Returns:\n    contexts: Sampled context matrix of size (n, c).\n    actions: Sampled context matrix of size (n, a).\n    rewards: Sampled reward matrix of size (n, 1).\n  """"""\n  mean_v = [1.0, 1.0, 1.0, 1.0, 1.2]\n  std_v = [0.01, 0.01, 0.01, 0.01, 0.01]\n  mu_large = 50\n  std_large = 0.01\n  contexts, actions, rewards = sample_single_wheel_bandit_data(\n      num_datapoints,\n      num_actions,\n      context_dim,\n      delta,\n      mean_v,\n      std_v,\n      mu_large,\n      std_large)\n  return contexts, actions, rewards\n\n\ndef procure_dataset(hparams, num_wheels, seed=0):\n  """"""Samples the full dataset for pretraining GNPs.""""""\n  np.random.seed(seed)\n\n  all_contexts, all_actions, all_rewards = [], [], []\n  for _ in range(num_wheels):\n    delta = np.random.uniform()\n    contexts, actions, rewards = get_single_wheel_data(\n        hparams.num_target + hparams.num_context,\n        hparams.num_actions,\n        hparams.context_dim,\n        delta)\n    all_contexts.append(contexts)\n    all_actions.append(actions)\n    all_rewards.append(rewards)\n\n  all_contexts = np.stack(all_contexts)\n  all_actions = np.stack(all_actions)\n  all_rewards = np.stack(all_rewards)\n  return all_contexts, all_actions, all_rewards\n\n\ndef get_splits(dataset, n_context, batch_size, points_perm=True):\n  """"""Splits the dataset into target and context sets.""""""\n  full_x, full_a, rewards = dataset\n  full_y = rewards * full_a\n  dataset_perm = np.random.permutation(len(full_x))[:batch_size]\n  if points_perm:\n    datapoints_perm = np.random.permutation(full_x.shape[1])\n  else:\n    datapoints_perm = np.arange(full_x.shape[1])\n\n  target_x = tf.to_float(full_x[dataset_perm[:, None], datapoints_perm])\n  target_y = tf.to_float(full_y[dataset_perm[:, None], datapoints_perm])\n  target_a = tf.to_float(full_a[dataset_perm[:, None], datapoints_perm])\n  context_x = target_x[:, :n_context, :]\n  context_y = target_y[:, :n_context, :]\n  unseen_target_y = target_y[:, n_context:]\n  unseen_target_a = target_a[:, n_context:]\n  return (context_x,\n          context_y,\n          target_x,\n          target_y,\n          unseen_target_y,\n          unseen_target_a)\n\n\ndef training_loop(train_dataset,\n                  valid_dataset,\n                  model,\n                  hparams):\n  """"""Trains a GNP for a fixed number of iterations.""""""\n  optimizer_config = {\'optimizer\': hparams.optimizer(hparams.lr),\n                      \'max_grad_norm\': hparams.max_grad_norm}\n  num_context = hparams.num_context\n  best_recon_loss = np.inf\n  if hparams.is_nll:\n    step = tf.function(utils.nll_gnp_step_bandits.python_function)\n    valid_metric = utils.nll\n  else:\n    step = tf.function(utils.mse_gnp_step_bandits.python_function)\n    valid_metric = utils.mse\n\n  for it in range(hparams.num_iterations):\n    batch_train_data = get_splits(\n        train_dataset,\n        num_context,\n        hparams.batch_size,\n        points_perm=True)\n    recon_loss, local_z_kl, global_z_kl = step(\n        model,\n        batch_train_data,\n        optimizer_config)\n\n    if it % hparams.print_every == 0:\n      batch_valid_data = get_splits(\n          valid_dataset,\n          num_context,\n          hparams.batch_size,\n          points_perm=False)\n      (batch_context_x,\n       batch_context_y,\n       batch_target_x,\n       batch_target_y,\n       batch_unseen_target_y,\n       batch_unseen_target_a) = batch_valid_data\n      prediction = model(batch_context_x,\n                         batch_context_y,\n                         batch_target_x,\n                         batch_target_y)\n      batch_unseen_predictions = prediction[:, num_context:]\n      valid_recon_loss = valid_metric(batch_unseen_target_y,\n                                      batch_unseen_predictions,\n                                      batch_unseen_target_a)\n\n      print(\'it: {}, train recon loss: {}, local kl: {} global kl: {} \'\n            \'valid reconstr loss: {}\'.format(it, recon_loss, local_z_kl,\n                                             global_z_kl, valid_recon_loss))\n      if valid_recon_loss.numpy() < best_recon_loss:\n        best_recon_loss = valid_recon_loss.numpy()\n        print(\'Saving best model with reconstruction loss\',\n              best_recon_loss, flush=True)\n        model.save_weights(hparams.save_path)\n\n\ndef train(data_hparams,\n          model_hparams,\n          training_hparams):\n  """"""Executes the training pipeline for GNPs.""""""\n  all_contexts, all_actions, all_rewards = procure_dataset(\n      data_hparams,\n      num_wheels=100,\n      seed=0)\n  train_dataset = (all_contexts, all_actions, all_rewards)\n\n  all_contexts, all_actions, all_rewards = procure_dataset(\n      data_hparams,\n      num_wheels=10,\n      seed=42)\n  valid_dataset = (all_contexts, all_actions, all_rewards)\n\n  model = generalized_neural_process.Regressor(\n      input_dim=data_hparams.context_dim,\n      output_dim=data_hparams.num_actions,\n      x_encoder_net_sizes=model_hparams.x_encoder_net_sizes,\n      x_y_encoder_net_sizes=model_hparams.x_y_encoder_net_sizes,\n      global_latent_net_sizes=model_hparams.global_latent_net_sizes,\n      local_latent_net_sizes=model_hparams.local_latent_net_sizes,\n      decoder_net_sizes=model_hparams.decoder_net_sizes,\n      heteroskedastic_net_sizes=model_hparams.heteroskedastic_net_sizes,\n      model_type=model_hparams.model_type,\n      activation=model_hparams.activation,\n      output_activation=model_hparams.output_activation,\n      data_uncertainty=model_hparams.data_uncertainty,\n      beta=model_hparams.beta,\n      temperature=model_hparams.temperature)\n\n  training_loop(train_dataset,\n                valid_dataset,\n                model,\n                training_hparams)\n\n'"
experimental/attentive_uncertainty/contextual_bandits/run_offline_contextual_bandits.py,6,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Benchmark script for the wheel bandit task.\n""""""\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom experimental.attentive_uncertainty import attention  # local file import\nfrom experimental.attentive_uncertainty.contextual_bandits import offline_contextual_bandits  # local file import\nfrom experimental.attentive_uncertainty.contextual_bandits import utils  # local file import\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nfrom deep_contextual_bandits import contextual_bandit  # local file import\nfrom deep_contextual_bandits import neural_linear_sampling  # local file import\nfrom deep_contextual_bandits import posterior_bnn_sampling  # local file import\nfrom deep_contextual_bandits import uniform_sampling  # local file import\nfrom tensorflow.contrib import training as contrib_training\n\ngfile = tf.compat.v1.gfile\n\ntf.compat.v1.enable_eager_execution()\n\nFLAGS = flags.FLAGS\nFLAGS.set_default(\'alsologtostderr\', True)\nflags.DEFINE_string(\n    \'logdir\',\n    \'/tmp/bandits/\',\n    \'Base directory to save output.\')\nflags.DEFINE_integer(\n    \'num_trials\',\n    5,\n    \'Number of trials\')\nflags.DEFINE_integer(\n    \'num_contexts\',\n    2000,\n    \'Number of contexts\')\nflags.DEFINE_list(\'deltas\', [\'0.5\', \'0.7\', \'0.9\', \'0.95\', \'0.99\'],\n                  \'delta parameters for wheel bandit instance.\')\nflags.DEFINE_string(\n    \'modeldir\',\n    \'/tmp/wheel_bandit/models/multitask\',\n    \'Directory with pretrained models.\')\nflags.DEFINE_string(\n    \'datasetdir\',\n    \'/tmp/wheel_bandit/data/\',\n    \'Directory with saved data instances.\')\nflags.DEFINE_integer(\n    \'exp_idx\',\n    0,\n    \'Experiment idx of full run.\')\nflags.DEFINE_string(\n    \'prefix\',\n    \'best_\',\n    \'Prefix of best model ckpts.\')\nflags.DEFINE_string(\n    \'suffix\',\n    \'_mse.ckpt\',\n    \'Suffix of best model ckpts.\')\nflags.DEFINE_list(\n    \'algo_names\',\n    [\'uniform\', \'snp_prior_freeform_offline\'],\n    \'List of algorithms to benchmark.\')\n\ncontext_dim = 2\nnum_actions = 5\n\n\ndef run_trial(trial_idx, delta, algo_names):\n  """"""Runs a trial of wheel bandit problem instance for a set of algorithms.""""""\n\n  filename = os.path.join(\n      FLAGS.datasetdir,\n      str(delta) + \'_\' + str(trial_idx) + \'.npz\')\n  with gfile.GFile(filename, \'r\') as f:\n    sampled_vals = np.load(f)\n    dataset = sampled_vals[\'dataset\']\n    opt_rewards = sampled_vals[\'opt_rewards\']\n\n  x_hidden_size = 100\n  x_encoder_sizes = [x_hidden_size]*2\n\n  algos = []\n  for algo_name in algo_names:\n    if algo_name == \'uniform\':\n      hparams = contrib_training.HParams(num_actions=num_actions)\n      algos.append(uniform_sampling.UniformSampling(algo_name, hparams))\n    elif algo_name == \'neurolinear\':\n      hparams = contrib_training.HParams(\n          num_actions=num_actions,\n          context_dim=context_dim,\n          init_scale=0.3,\n          activation=tf.nn.relu,\n          output_activation=tf.nn.relu,\n          layer_sizes=x_encoder_sizes,\n          batch_size=512,\n          activate_decay=True,\n          initial_lr=0.1,\n          max_grad_norm=5.0,\n          show_training=False,\n          freq_summary=1000,\n          buffer_s=-1,\n          initial_pulls=2,\n          reset_lr=True,\n          lr_decay_rate=0.5,\n          training_freq=1,\n          training_freq_network=20,\n          training_epochs=50,\n          a0=12,\n          b0=30,\n          lambda_prior=23)\n      algos.append(neural_linear_sampling.NeuralLinearPosteriorSampling(\n          algo_name, hparams))\n    elif algo_name == \'multitaskgp\':\n      hparams_gp = contrib_training.HParams(\n          num_actions=num_actions,\n          num_outputs=num_actions,\n          context_dim=context_dim,\n          reset_lr=False,\n          learn_embeddings=True,\n          max_num_points=1000,\n          show_training=False,\n          freq_summary=1000,\n          batch_size=512,\n          keep_fixed_after_max_obs=True,\n          training_freq=20,\n          initial_pulls=2,\n          training_epochs=50,\n          lr=0.01,\n          buffer_s=-1,\n          initial_lr=0.001,\n          lr_decay_rate=0.0,\n          optimizer=\'RMS\',\n          task_latent_dim=5,\n          activate_decay=False)\n      algos.append(posterior_bnn_sampling.PosteriorBNNSampling(\n          algo_name, hparams_gp, \'GP\'))\n    elif algo_name[:3] == \'snp\' or algo_name[:3] == \'anp\':\n      hidden_size = 64\n      latent_units = 32\n      global_latent_net_sizes = [hidden_size]*2 + [2*latent_units]\n      local_latent_net_sizes = [hidden_size]*3 + [2]\n      x_y_encoder_sizes = [hidden_size]*3\n      heteroskedastic_net_sizes = None\n      mean_att_type = attention.laplace_attention\n      scale_att_type_1 = attention.laplace_attention\n      scale_att_type_2 = attention.laplace_attention\n      att_type = \'multihead\'\n      att_heads = 8\n      data_uncertainty = False\n      is_anp = False\n\n      config = algo_name.split(\'_\')\n      mfile = FLAGS.prefix + config[1] + \'_\' + config[2] + FLAGS.suffix\n      if algo_name[:3] == \'anp\':\n        mfile = \'anp_\' + mfile\n        local_latent_net_sizes = [hidden_size]*3 + [2*5]\n        is_anp = True\n      mpath = os.path.join(FLAGS.modeldir, mfile)\n\n      hparams = contrib_training.HParams(\n          num_actions=num_actions,\n          context_dim=context_dim,\n          init_scale=0.3,\n          activation=tf.nn.relu,\n          output_activation=tf.nn.relu,\n          x_encoder_sizes=x_encoder_sizes,\n          x_y_encoder_sizes=x_y_encoder_sizes,\n          global_latent_net_sizes=global_latent_net_sizes,\n          local_latent_net_sizes=local_latent_net_sizes,\n          heteroskedastic_net_sizes=heteroskedastic_net_sizes,\n          att_type=att_type,\n          att_heads=att_heads,\n          mean_att_type=mean_att_type,\n          scale_att_type_1=scale_att_type_1,\n          scale_att_type_2=scale_att_type_2,\n          data_uncertainty=data_uncertainty,\n          batch_size=512,\n          activate_decay=True,\n          initial_lr=0.1,\n          max_grad_norm=5.0,\n          show_training=False,\n          freq_summary=1000,\n          buffer_s=-1,\n          initial_pulls=2,\n          reset_lr=True,\n          lr_decay_rate=0.5,\n          training_freq=10,\n          training_freq_network=20,\n          training_epochs=50,\n          uncertainty_type=\'attentive_freeform\',\n          local_variational=True,\n          model_path=mpath,\n          is_anp=is_anp)\n\n      if config[1] == \'prior\':\n        hparams.set_hparam(\'local_variational\', False)\n\n      if config[2] == \'gp\':\n        hparams.set_hparam(\'uncertainty_type\', \'attentive_gp\')\n\n      algos.append(offline_contextual_bandits.OfflineContextualBandits(\n          algo_name, hparams))\n\n  t_init = time.time()\n  _, h_rewards = contextual_bandit.run_contextual_bandit(\n      context_dim,\n      num_actions,\n      dataset,\n      algos,\n      num_contexts=FLAGS.num_contexts)  # pytype: disable=wrong-keyword-args\n  t_final = time.time()\n\n  return h_rewards, t_final - t_init, opt_rewards[:FLAGS.num_contexts]\n\n\ndef benchmark():\n  """"""Benchmark performance on wheel-bandit.""""""\n  for delta_str in FLAGS.deltas:\n    delta = float(delta_str)\n    all_regrets, all_times = [], []\n    for idx in range(FLAGS.num_trials):\n      summary_results = run_trial(idx, delta, FLAGS.algo_names)\n      h_rewards, t, opt_rewards = summary_results\n      regrets = np.expand_dims(opt_rewards, axis=-1) - h_rewards\n      utils.display_results(FLAGS.algo_names,\n                            regrets,\n                            t,\n                            str(delta) + \'_\' + str(idx))\n      all_regrets.append(regrets)\n      all_times.append(t)\n    all_regrets = np.mean(np.stack(all_regrets), axis=0)\n    all_times = np.sum(all_times)\n    print(\'Overall Summary for delta = \', delta)\n    utils.display_results(FLAGS.algo_names,\n                          all_regrets,\n                          all_times,\n                          str(delta))\n\n\ndef main(argv):\n  del argv\n  benchmark()\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
experimental/attentive_uncertainty/contextual_bandits/run_offline_contextual_bandits_gnp.py,6,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Benchmark script for the wheel bandit task.\n""""""\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom experimental.attentive_uncertainty.contextual_bandits import offline_contextual_bandits_gnp  # local file import\nfrom experimental.attentive_uncertainty.contextual_bandits import utils  # local file import\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nfrom deep_contextual_bandits import contextual_bandit  # local file import\nfrom deep_contextual_bandits import neural_linear_sampling  # local file import\nfrom deep_contextual_bandits import posterior_bnn_sampling  # local file import\nfrom deep_contextual_bandits import uniform_sampling  # local file import\nfrom tensorflow.contrib import training as contrib_training\ngfile = tf.compat.v1.gfile\n\ntf.compat.v1.enable_eager_execution()\n\nFLAGS = flags.FLAGS\nFLAGS.set_default(\'alsologtostderr\', True)\nflags.DEFINE_string(\n    \'logdir\',\n    \'/tmp/bandits/\',\n    \'Base directory to save output.\')\nflags.DEFINE_integer(\n    \'num_trials\',\n    5,\n    \'Number of trials\')\nflags.DEFINE_integer(\n    \'num_contexts\',\n    2000,\n    \'Number of contexts\')\nflags.DEFINE_list(\'deltas\', [\'0.5\', \'0.7\', \'0.9\', \'0.95\', \'0.99\'],\n                  \'delta parameters for wheel bandit instance.\')\nflags.DEFINE_string(\n    \'modeldir\',\n    \'/tmp/wheel_bandit/models/multitask\',\n    \'Directory with pretrained models.\')\nflags.DEFINE_string(\n    \'savedir\',\n    \'/tmp/wheel_bandit/results/\',\n    \'Directory with saved pkl files for full results.\')\nflags.DEFINE_string(\n    \'ckptdir\',\n    \'/tmp/wheel_bandit/ckpts/\',\n    \'Directory with saved pkl files for full ckpts.\')\nflags.DEFINE_string(\n    \'datasetdir\',\n    \'/tmp/wheel_bandit/data/\',\n    \'Directory with saved data instances.\')\nflags.DEFINE_string(\n    \'prefix\',\n    \'gnp_\',\n    \'Prefix of best model ckpts.\')\nflags.DEFINE_string(\n    \'suffix\',\n    \'.ckpt\',\n    \'Suffix of best model ckpts.\')\nflags.DEFINE_list(\n    \'algo_names\',\n    [\'uniform\', \'gnp_anp_beta_5.0_temp_0.001\'],\n    \'List of algorithms to benchmark.\')\n\ncontext_dim = 2\nnum_actions = 5\n\n\ndef run_trial(trial_idx, delta, algo_names):\n  """"""Runs a trial of wheel bandit problem instance for a set of algorithms.""""""\n\n  filename = os.path.join(\n      FLAGS.datasetdir,\n      str(delta) + \'_\' + str(trial_idx) + \'.npz\')\n  with gfile.GFile(filename, \'r\') as f:\n    sampled_vals = np.load(f)\n    dataset = sampled_vals[\'dataset\']\n    opt_rewards = sampled_vals[\'opt_rewards\']\n\n  x_hidden_size = 100\n  x_encoder_sizes = [x_hidden_size]*2\n\n  algos = []\n  for algo_name in algo_names:\n    if algo_name == \'uniform\':\n      hparams = contrib_training.HParams(num_actions=num_actions)\n      algos.append(uniform_sampling.UniformSampling(algo_name, hparams))\n    elif algo_name == \'neurolinear\':\n      hparams = contrib_training.HParams(\n          num_actions=num_actions,\n          context_dim=context_dim,\n          init_scale=0.3,\n          activation=tf.nn.relu,\n          output_activation=tf.nn.relu,\n          layer_sizes=x_encoder_sizes,\n          batch_size=512,\n          activate_decay=True,\n          initial_lr=0.1,\n          max_grad_norm=5.0,\n          show_training=False,\n          freq_summary=1000,\n          buffer_s=-1,\n          initial_pulls=2,\n          reset_lr=True,\n          lr_decay_rate=0.5,\n          training_freq=1,\n          training_freq_network=20,\n          training_epochs=50,\n          a0=12,\n          b0=30,\n          lambda_prior=23)\n      algos.append(neural_linear_sampling.NeuralLinearPosteriorSampling(\n          algo_name, hparams))\n    elif algo_name == \'multitaskgp\':\n      hparams_gp = contrib_training.HParams(\n          num_actions=num_actions,\n          num_outputs=num_actions,\n          context_dim=context_dim,\n          reset_lr=False,\n          learn_embeddings=True,\n          max_num_points=1000,\n          show_training=False,\n          freq_summary=1000,\n          batch_size=512,\n          keep_fixed_after_max_obs=True,\n          training_freq=20,\n          initial_pulls=2,\n          training_epochs=50,\n          lr=0.01,\n          buffer_s=-1,\n          initial_lr=0.001,\n          lr_decay_rate=0.0,\n          optimizer=\'RMS\',\n          task_latent_dim=5,\n          activate_decay=False)\n      algos.append(posterior_bnn_sampling.PosteriorBNNSampling(\n          algo_name, hparams_gp, \'GP\'))\n    elif algo_name[:3] == \'gnp\':\n      hidden_size = 64\n      x_encoder_net_sizes = None\n      decoder_net_sizes = [hidden_size]*3 + [2*num_actions]\n      heteroskedastic_net_sizes = None\n      att_type = \'multihead\'\n      att_heads = 8\n      data_uncertainty = False\n      config = algo_name.split(\'_\')\n      model_type = config[1]\n      if algo_name[:len(\'gnp_anp_beta_\')] == \'gnp_anp_beta_\':\n        mfile = algo_name + FLAGS.suffix\n        x_y_encoder_net_sizes = [hidden_size]*3\n        global_latent_net_sizes = [hidden_size]*2\n        local_latent_net_sizes = None\n        beta = float(config[3])\n        temperature = float(config[5])\n      else:\n        mfile = FLAGS.prefix + config[1] + FLAGS.suffix\n        if model_type == \'cnp\':\n          x_y_encoder_net_sizes = [hidden_size]*4\n          global_latent_net_sizes = None\n          local_latent_net_sizes = None\n        elif model_type == \'np\':\n          x_y_encoder_net_sizes = [hidden_size]*2\n          global_latent_net_sizes = [hidden_size]*2\n          local_latent_net_sizes = None\n        elif model_type == \'anp\':\n          x_y_encoder_net_sizes = [hidden_size]*2\n          global_latent_net_sizes = [hidden_size]*2\n          local_latent_net_sizes = None\n        elif model_type == \'acnp\':\n          x_y_encoder_net_sizes = [hidden_size]*4\n          global_latent_net_sizes = None\n          local_latent_net_sizes = None\n        elif model_type == \'acns\':\n          x_y_encoder_net_sizes = [hidden_size]*2\n          global_latent_net_sizes = [hidden_size]*2\n          local_latent_net_sizes = [hidden_size]*2\n\n        beta = 1.\n        temperature = 1.\n\n      mpath = os.path.join(FLAGS.modeldir, mfile)\n\n      hparams = contrib_training.HParams(\n          num_actions=num_actions,\n          context_dim=context_dim,\n          init_scale=0.3,\n          activation=tf.nn.relu,\n          output_activation=tf.nn.relu,\n          x_encoder_net_sizes=x_encoder_net_sizes,\n          x_y_encoder_net_sizes=x_y_encoder_net_sizes,\n          global_latent_net_sizes=global_latent_net_sizes,\n          local_latent_net_sizes=local_latent_net_sizes,\n          decoder_net_sizes=decoder_net_sizes,\n          heteroskedastic_net_sizes=heteroskedastic_net_sizes,\n          att_type=att_type,\n          att_heads=att_heads,\n          model_type=model_type,\n          data_uncertainty=data_uncertainty,\n          beta=beta,\n          temperature=temperature,\n          model_path=mpath,\n          batch_size=512,\n          activate_decay=True,\n          initial_lr=0.1,\n          max_grad_norm=5.0,\n          show_training=False,\n          freq_summary=1000,\n          buffer_s=-1,\n          initial_pulls=2,\n          reset_lr=True,\n          lr_decay_rate=0.5,\n          training_freq=10,\n          training_freq_network=20,\n          training_epochs=50)\n\n      algos.append(offline_contextual_bandits_gnp.OfflineContextualBandits(\n          algo_name, hparams))\n\n  t_init = time.time()\n  _, h_rewards = contextual_bandit.run_contextual_bandit(\n      context_dim,\n      num_actions,\n      dataset,\n      algos,\n      num_contexts=FLAGS.num_contexts)  # pytype: disable=wrong-keyword-args\n  t_final = time.time()\n\n  return h_rewards, t_final - t_init, opt_rewards[:FLAGS.num_contexts]\n\n\ndef benchmark():\n  """"""Benchmark performance on wheel-bandit.""""""\n  for delta_str in FLAGS.deltas:\n    delta = float(delta_str)\n    all_regrets, all_times = [], []\n    for idx in range(FLAGS.num_trials):\n      summary_results = run_trial(idx, delta, FLAGS.algo_names)\n      h_rewards, t, opt_rewards = summary_results\n      regrets = np.expand_dims(opt_rewards, axis=-1) - h_rewards\n      utils.display_results(FLAGS.algo_names,\n                            regrets,\n                            t,\n                            str(delta) + \'_\' + str(idx))\n      all_regrets.append(regrets)\n      all_times.append(t)\n    all_regrets = np.mean(np.stack(all_regrets), axis=0)\n    all_times = np.sum(all_times)\n    print(\'Overall Summary for delta = \', delta)\n    utils.display_results(FLAGS.algo_names,\n                          all_regrets,\n                          all_times,\n                          str(delta))\n\n\ndef main(argv):\n  del argv\n  benchmark()\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
experimental/attentive_uncertainty/contextual_bandits/utils.py,23,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Defined utility functions for contextual bandits via SNPs.\n""""""\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom deep_contextual_bandits import contextual_bandit  # local file import\n\n\ndef get_data_with_masked_rewards(dataset):\n  """"""Returns all observations with one-hot weights for actions.""""""\n  weights = np.zeros((dataset.contexts.shape[0], dataset.num_actions))\n  a_ind = np.array(list(enumerate(dataset.actions)))\n  weights[a_ind[:, 0], a_ind[:, 1]] = 1.0\n  masked_rewards = dataset.rewards[np.arange(len(dataset.actions)),\n                                   dataset.actions]\n  return dataset.contexts, masked_rewards, weights\n\n\ndef get_batch_with_masked_rewards(dataset, batch_size, with_replacement=True):\n  """"""Returns a random mini-batch with one-hot weights for actions.""""""\n  n, _ = dataset.contexts.shape\n  if dataset.buffer_s == -1:\n    # Use all the data.\n    ind = np.random.choice(range(n), batch_size, replace=with_replacement)\n  else:\n    # Use only buffer (last buffer_s obs).\n    ind = np.random.choice(range(max(0, n - dataset.buffer_s), n), batch_size,\n                           replace=with_replacement)\n\n  weights = np.zeros((batch_size, dataset.num_actions))\n  sampled_actions = np.array(dataset.actions)[ind]\n  a_ind = np.array(list(enumerate(sampled_actions)))\n  weights[a_ind[:, 0], a_ind[:, 1]] = 1.0\n  masked_rewards = dataset.rewards[ind, sampled_actions]\n  return dataset.contexts[ind, :], masked_rewards, weights\n\n\ndef run_contextual_bandit(context_dim,\n                          num_actions,\n                          dataset,\n                          algos,\n                          num_contexts=None):\n  """"""Run a contextual bandit problem on a set of algorithms.\n\n  Args:\n    context_dim: Dimension of the context.\n    num_actions: Number of available actions.\n    dataset: Matrix where every row is a context + num_actions rewards.\n    algos: List of algorithms to use in the contextual bandit instance.\n    num_contexts: Number of contexts.\n\n  Returns:\n    h_actions: Matrix with actions: size (num_context, num_algorithms).\n    h_rewards: Matrix with rewards: size (num_context, num_algorithms).\n  """"""\n  if num_contexts is None:\n    num_contexts = dataset.shape[0]\n\n  # Create contextual bandit\n  cmab = contextual_bandit.ContextualBandit(context_dim, num_actions)\n  cmab.feed_data(dataset)\n\n  h_actions = np.empty((0, len(algos)), float)\n  h_rewards = np.empty((0, len(algos)), float)\n\n  # Run the contextual bandit process\n  for i in range(num_contexts):\n    context = cmab.context(i)\n    actions = [a.action(context) for a in algos]\n    rewards = [cmab.reward(i, action) for action in actions]\n\n    for j, a in enumerate(algos):\n      a.update(context, actions[j], rewards[j])\n\n    h_actions = np.vstack((h_actions, np.array(actions)))\n    h_rewards = np.vstack((h_rewards, np.array(rewards)))\n\n  return h_actions, h_rewards\n\n\ndef display_results(algo_names, per_timestep_regrets, time, name):\n  """"""Displays summary statistics of the performance of each algorithm.""""""\n  print(\'---------------------------------------------------\')\n  print(\'---------------------------------------------------\')\n  print(\'{} bandit completed after {} seconds.\'.format(name, time))\n  print(\'---------------------------------------------------\')\n\n  performance_pairs = []\n  for j, a in enumerate(algo_names):\n    performance_pairs.append((a, np.sum(per_timestep_regrets[:, j])))\n  performance_pairs = sorted(performance_pairs, key=lambda elt: elt[1])\n  for i, (name, reward) in enumerate(performance_pairs):\n    print(\'{:3}){:20}|\\t\\t cummulative regret = {:10}.\'.format(i, name, reward))\n  print(\'---------------------------------------------------\')\n  print(\'---------------------------------------------------\')\n\n  sim_performance_pairs = []\n  for j, a in enumerate(algo_names):\n    sim_performance_pairs.append((a, np.sum(per_timestep_regrets[-500:, j])))\n  sim_performance_pairs = sorted(sim_performance_pairs, key=lambda elt: elt[1])\n  for i, (name, reward) in enumerate(sim_performance_pairs):\n    print(\'{:3}) {:20}|\\t\\t simple regret = {:10}.\'.format(i, name, reward))\n  print(\'---------------------------------------------------\')\n  print(\'---------------------------------------------------\')\n\n\ndef mse(y_true, y_pred_dist, actions):\n  """"""Returns the mean squared error for a predictive distribution.\n\n  Args:\n    y_true: (float) Tensor of target labels.\n    y_pred_dist: A tfp distribution object.\n    actions: One-hot masking actions.\n  """"""\n  return tf.losses.mean_squared_error(y_true,\n                                      actions*y_pred_dist.distribution.mean())\n\n\ndef nll(y_true, y_pred_dist, actions):\n  """"""Returns the negative log-likelihood of a model w.r.t. true targets.\n\n  Args:\n    y_true: (float) Tensor of target labels.\n    y_pred_dist: An edward2 distribution object.\n    actions: One-hot masking actions.\n  """"""\n  log_p = actions * y_pred_dist.distribution.log_prob(y_true)\n  return -tf.reduce_mean(tf.reduce_sum(log_p, axis=-1))\n\n\n@tf.function\ndef mse_anp_step(model, data, optimizer_config):\n  """"""Applies gradient updates and returns appropriate metrics.\n\n  Args:\n    model: An instance of SNP Regressor.\n    data: A 5-tuple consisting of context_x, context_y, target_x, target_y,\n      unseen_targets (i.e., target_x-context_x).\n    optimizer_config: A dictionary with two keys: an \'optimizer\' object and\n      a \'max_grad_norm\' for clipping gradients.\n\n  Returns:\n    mse_term: Mean squared error of model for unseen targets.\n    local_kl: KL loss for latent variables of unseen targets.\n    global_kl: KL loss for global latent variable.\n  """"""\n  (context_x,\n   context_y,\n   target_x,\n   target_y,\n   unseen_target_y,\n   unseen_target_a) = data\n  num_context = tf.shape(context_x)[1]\n  with tf.GradientTape() as tape:\n    prediction = model(context_x,\n                       context_y,\n                       target_x,\n                       target_y)\n    unseen_predictions = prediction[:, num_context:]\n    mse_term = mse(unseen_target_y, unseen_predictions, unseen_target_a)\n    if model.local_variational:\n      local_kl = tf.reduce_mean(\n          tf.reduce_sum(model.losses[-1][:, num_context:], axis=[1, 2]))\n    else:\n      local_kl = 0.\n    global_kl = tf.reduce_mean(tf.reduce_sum(model.losses[-2], axis=-1))\n    loss = mse_term + local_kl + global_kl\n  gradients = tape.gradient(loss, model.trainable_variables)\n  max_grad_norm = optimizer_config[\'max_grad_norm\']\n  optimizer = optimizer_config[\'optimizer\']\n  clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm)\n  optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n  return mse_term, local_kl, global_kl\n\n\n@tf.function\ndef mse_gnp_step_bandits(model, data, optimizer_config):\n  """"""Applies gradient updates and returns appropriate metrics.\n\n  Args:\n    model: An instance of SNP Regressor.\n    data: A 5-tuple consisting of context_x, context_y, target_x, target_y,\n      unseen_targets (i.e., target_x-context_x).\n    optimizer_config: A dictionary with two keys: an \'optimizer\' object and\n      a \'max_grad_norm\' for clipping gradients.\n\n  Returns:\n    mse_term: Mean squared error of model for unseen targets.\n    local_kl: KL loss for latent variables of unseen targets.\n    global_kl: KL loss for global latent variable.\n  """"""\n  (context_x,\n   context_y,\n   target_x,\n   target_y,\n   unseen_target_y,\n   unseen_target_a) = data\n  num_context = tf.shape(context_x)[1]\n  with tf.GradientTape() as tape:\n    prediction = model(context_x,\n                       context_y,\n                       target_x,\n                       target_y)\n    unseen_predictions = prediction[:, num_context:]\n    mse_term = mse(unseen_target_y, unseen_predictions, unseen_target_a)\n    local_kl = tf.reduce_mean(\n        tf.reduce_sum(model.losses[-1][:, num_context:], axis=[1, 2]))\n    global_kl = tf.reduce_mean(tf.reduce_sum(model.losses[-2], axis=-1))\n    loss = mse_term + local_kl + global_kl\n  gradients = tape.gradient(loss, model.trainable_variables)\n  max_grad_norm = optimizer_config[\'max_grad_norm\']\n  optimizer = optimizer_config[\'optimizer\']\n  clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm)\n  optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n  return mse_term, local_kl, global_kl\n\n\n@tf.function\ndef nll_gnp_step_bandits(model, data, optimizer_config):\n  """"""Applies gradient updates and returns appropriate metrics.\n\n  Args:\n    model: An instance of SNP Regressor.\n    data: A 5-tuple consisting of context_x, context_y, target_x, target_y,\n      unseen_targets (i.e., target_x-context_x).\n    optimizer_config: A dictionary with two keys: an \'optimizer\' object and\n      a \'max_grad_norm\' for clipping gradients.\n\n  Returns:\n    nll_term: Negative log-likelihood of model for unseen targets.\n    local_kl: KL loss for latent variables of unseen targets.\n    global_kl: KL loss for global latent variable.\n  """"""\n  (context_x,\n   context_y,\n   target_x,\n   target_y,\n   unseen_target_y,\n   unseen_target_a) = data\n  num_context = tf.shape(context_x)[1]\n  with tf.GradientTape() as tape:\n    prediction = model(context_x,\n                       context_y,\n                       target_x,\n                       target_y)\n    unseen_predictions = prediction[:, num_context:]\n    nll_term = nll(unseen_target_y, unseen_predictions, unseen_target_a)\n    local_kl = tf.reduce_mean(\n        tf.reduce_sum(model.losses[-1][:, num_context:], axis=[1, 2]))\n    global_kl = tf.reduce_mean(tf.reduce_sum(model.losses[-2], axis=-1))\n    loss = nll_term + local_kl + global_kl\n  gradients = tape.gradient(loss, model.trainable_variables)\n  max_grad_norm = optimizer_config[\'max_grad_norm\']\n  optimizer = optimizer_config[\'optimizer\']\n  clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm)\n  optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n  return nll_term, local_kl, global_kl\n'"
experimental/attentive_uncertainty/toy_regression/datasets.py,26,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Parses real and synthetic datasets.\n""""""\n\nimport collections\nimport tensorflow.compat.v1 as tf\n\nNPRegressionDescription = collections.namedtuple(\n    ""NPRegressionDescription"",\n    (""context_x"", ""context_y"", ""target_x"", ""target_y""))\n\n\nclass GPCurvesReader(object):\n  """"""Generates curves using a Gaussian Process (GP).\n\n  Supports vector inputs (x) and vector outputs (y). Kernel is\n  mean-squared exponential, using the x-value l2 coordinate distance scaled by\n  some factor chosen randomly in a range. Outputs are independent gaussian\n  processes.\n  """"""\n\n  def __init__(self,\n               batch_size,\n               max_num_context,\n               x_size=1,\n               y_size=1,\n               l1_scale=0.6,\n               sigma_scale=1.0,\n               random_kernel_parameters=False,\n               testing=False):\n    """"""Creates a regression dataset of functions sampled from a GP.\n\n    Args:\n      batch_size: An integer.\n      max_num_context: The max number of observations in the context.\n      x_size: Integer >= 1 for length of ""x values"" vector.\n      y_size: Integer >= 1 for length of ""y values"" vector.\n      l1_scale: Float; typical scale for kernel distance function.\n      sigma_scale: Float; typical scale for variance.\n      random_kernel_parameters: If `True`, the kernel parameters (l1 and sigma)\n          are sampled uniformly within [0.1, l1_scale] and [0.1, sigma_scale].\n      testing: Boolean that indicates whether we are testing. If so there are\n          more targets for visualization.\n    """"""\n    self._batch_size = batch_size\n    self._max_num_context = max_num_context\n    self._x_size = x_size\n    self._y_size = y_size\n    self._l1_scale = l1_scale\n    self._sigma_scale = sigma_scale\n    self._random_kernel_parameters = random_kernel_parameters\n    self._testing = testing\n\n  def _gaussian_kernel(self, xdata, l1, sigma_f, sigma_noise=2e-2):\n    """"""Applies the Gaussian kernel to generate curve data.\n\n    Args:\n      xdata: Tensor of shape [B, num_total_points, x_size] with\n          the values of the x-axis data.\n      l1: Tensor of shape [B, y_size, x_size], the scale\n          parameter of the Gaussian kernel.\n      sigma_f: Tensor of shape [B, y_size], the magnitude\n          of the std.\n      sigma_noise: Float, std of the noise that we add for stability.\n\n    Returns:\n      The kernel, a float tensor of shape\n      [B, y_size, num_total_points, num_total_points].\n    """"""\n    num_total_points = tf.shape(xdata)[1]\n\n    # Expand and take the difference\n    xdata1 = tf.expand_dims(xdata, axis=1)  # [B, 1, num_total_points, x_size]\n    xdata2 = tf.expand_dims(xdata, axis=2)  # [B, num_total_points, 1, x_size]\n    diff = xdata1 - xdata2  # [B, num_total_points, num_total_points, x_size]\n\n    # [B, y_size, num_total_points, num_total_points, x_size]\n    norm = tf.square(diff[:, None, :, :, :] / l1[:, :, None, None, :])\n\n    norm = tf.reduce_sum(\n        norm, -1)  # [B, data_size, num_total_points, num_total_points]\n\n    # [B, y_size, num_total_points, num_total_points]\n    kernel = tf.square(sigma_f)[:, :, None, None] * tf.exp(-0.5 * norm)\n\n    # Add some noise to the diagonal to make the cholesky work.\n    kernel += (sigma_noise**2) * tf.eye(num_total_points)\n\n    return kernel\n\n  def generate_curves(self, num_context=None):\n    """"""Builds the op delivering the data.\n\n    Generated functions are `float32` with x values between -2 and 2.\n\n    Args:\n      num_context: Number of context points. If None, chosen randomly.\n\n    Returns:\n      A `CNPRegressionDescription` namedtuple.\n    """"""\n    if num_context is None:\n      num_context = tf.random_uniform(\n          shape=[], minval=3, maxval=self._max_num_context, dtype=tf.int32)\n\n    # If we are testing we want to have more targets and have them evenly\n    # distributed in order to plot the function.\n    if self._testing:\n      num_target = 400\n      num_total_points = num_target\n      x_values = tf.tile(\n          tf.expand_dims(tf.range(-2., 2., 1. / 100, dtype=tf.float32), axis=0),\n          [self._batch_size, 1])\n      x_values = tf.expand_dims(x_values, axis=-1)\n    # During training the number of target points and their x-positions are\n    # selected at random\n    else:\n      num_target = tf.random_uniform(shape=(), minval=0,\n                                     maxval=self._max_num_context - num_context,\n                                     dtype=tf.int32)\n      num_total_points = num_context + num_target\n      x_values = tf.random_uniform(\n          [self._batch_size, num_total_points, self._x_size], -2, 2)\n\n    # Set kernel parameters\n    # Either choose a set of random parameters for the mini-batch\n    if self._random_kernel_parameters:\n      l1 = tf.random_uniform([self._batch_size, self._y_size,\n                              self._x_size], 0.1, self._l1_scale)\n      sigma_f = tf.random_uniform([self._batch_size, self._y_size],\n                                  0.1, self._sigma_scale)\n    # Or use the same fixed parameters for all mini-batches\n    else:\n      l1 = tf.ones(shape=[self._batch_size, self._y_size,\n                          self._x_size]) * self._l1_scale\n      sigma_f = tf.ones(shape=[self._batch_size,\n                               self._y_size]) * self._sigma_scale\n\n    # Pass the x_values through the Gaussian kernel\n    # [batch_size, y_size, num_total_points, num_total_points]\n    kernel = self._gaussian_kernel(x_values, l1, sigma_f)\n\n    # Calculate Cholesky, using double precision for better stability:\n    cholesky = tf.cast(tf.cholesky(tf.cast(kernel, tf.float64)), tf.float32)\n\n    # Sample a curve\n    # [batch_size, y_size, num_total_points, 1]\n    y_values = tf.matmul(\n        cholesky,\n        tf.random_normal([self._batch_size, self._y_size, num_total_points, 1]))\n\n    # [batch_size, num_total_points, y_size]\n    y_values = tf.transpose(tf.squeeze(y_values, 3), [0, 2, 1])\n\n    if self._testing:\n      # Select the targets\n      target_x = x_values\n      target_y = y_values\n\n      # Select the observations\n      idx = tf.random_shuffle(tf.range(num_target))\n      context_x = tf.gather(x_values, idx[:num_context], axis=1)\n      context_y = tf.gather(y_values, idx[:num_context], axis=1)\n\n    else:\n      # Select the targets which will consist of the context points as well as\n      # some new target points\n      target_x = x_values[:, :num_target + num_context, :]\n      target_y = y_values[:, :num_target + num_context, :]\n\n      # Select the observations\n      context_x = x_values[:, :num_context, :]\n      context_y = y_values[:, :num_context, :]\n\n    return NPRegressionDescription(\n        context_x=context_x,\n        context_y=context_y,\n        target_x=target_x,\n        target_y=target_y)\n'"
experimental/attentive_uncertainty/toy_regression/utils.py,0,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Defined utility functions for toy regression problem.\n""""""\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_functions(target_x, target_y, context_x, context_y, pred_y, std):\n  """"""Plots the predicted mean and variance and the context points.\n\n  Args:\n    target_x: An array of shape [B, num_targets, 1] that contains the\n        x values of the target points.\n    target_y: An array of shape [B, num_targets, 1] that contains the\n        y values of the target points.\n    context_x: An array of shape [B, num_contexts, 1] that contains\n        the x values of the context points.\n    context_y: An array of shape [B, num_contexts, 1] that contains\n        the y values of the context points.\n    pred_y: An array of shape [B, num_targets, 1] that contains the\n        predicted means of the y values at the target points in target_x.\n    std: An array of shape [B, num_targets, 1] that contains the\n        predicted std dev of the y values at the target points in target_x.\n  """"""\n  # Plot everything\n  plt.plot(target_x[0], pred_y[0], \'b\', linewidth=2)\n  plt.plot(target_x[0], target_y[0], \'k:\', linewidth=2)\n  plt.plot(context_x[0], context_y[0], \'ko\', markersize=10)\n  plt.fill_between(\n      target_x[0, :, 0],\n      pred_y[0, :, 0] - std[0, :, 0],\n      pred_y[0, :, 0] + std[0, :, 0],\n      alpha=0.2,\n      facecolor=\'#65c9f7\',\n      interpolate=True)\n\n  # Make the plot pretty\n  plt.yticks([-2, 0, 2], fontsize=16)\n  plt.xticks([-2, 0, 2], fontsize=16)\n  plt.ylim([-2, 2])\n  plt.grid(False)\n  plt.show()\n'"
experimental/auxiliary_sampling/deterministic_baseline/lenet5.py,12,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Build a convolutional neural network.""""""\n\nimport edward2 as ed\nimport tensorflow as tf\n\n\ndef lenet5(input_shape, num_classes):\n  """"""Builds LeNet5.""""""\n  inputs = tf.keras.layers.Input(shape=input_shape)\n  conv1 = tf.keras.layers.Conv2D(6,\n                                 kernel_size=5,\n                                 padding=\'SAME\',\n                                 activation=\'relu\')(inputs)\n  pool1 = tf.keras.layers.MaxPooling2D(pool_size=[2, 2],\n                                       strides=[2, 2],\n                                       padding=\'SAME\')(conv1)\n  conv2 = tf.keras.layers.Conv2D(16,\n                                 kernel_size=5,\n                                 padding=\'SAME\',\n                                 activation=\'relu\')(pool1)\n  pool2 = tf.keras.layers.MaxPooling2D(pool_size=[2, 2],\n                                       strides=[2, 2],\n                                       padding=\'SAME\')(conv2)\n  conv3 = tf.keras.layers.Conv2D(120,\n                                 kernel_size=5,\n                                 padding=\'SAME\',\n                                 activation=tf.nn.relu)(pool2)\n  flatten = tf.keras.layers.Flatten()(conv3)\n  dense1 = tf.keras.layers.Dense(84, activation=tf.nn.relu)(flatten)\n  logits = tf.keras.layers.Dense(num_classes)(dense1)\n  outputs = tf.keras.layers.Lambda(lambda x: ed.Categorical(logits=x))(logits)\n  return tf.keras.Model(inputs=inputs, outputs=outputs)\n'"
experimental/auxiliary_sampling/deterministic_baseline/run_det_training.py,12,"b'# coding=utf-8\n# Copyright 2020 The Edward2 Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""DNN on CIFAR-10 trained with maximum likelihood and gradient descent.""""""\n\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom experimental.auxiliary_sampling import datasets  # local file import\nfrom experimental.auxiliary_sampling.compute_metrics import ensemble_metrics  # local file import\nfrom experimental.auxiliary_sampling.deterministic_baseline.lenet5 import lenet5  # local file import\nfrom experimental.auxiliary_sampling.res_net import res_net  # local file import\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tf1\n\nflags.DEFINE_integer(\'ensemble_size\', 1, \'Number of ensemble members.\')\nflags.DEFINE_boolean(\'bootstrap\', False,\n                     \'Sample the training set for bootstrapping.\')\nflags.DEFINE_integer(\'training_steps\', 40000, \'Training steps.\')\nflags.DEFINE_integer(\'batch_size\', 256, \'Batch size.\')\nflags.DEFINE_float(\'learning_rate\', 0.001, \'Learning rate.\')\nflags.DEFINE_integer(\'validation_freq\', 5, \'Validation frequency in steps.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/det_training\',\n                    \'The directory where the model weights and \'\n                    \'training/evaluation summaries are stored.\')\nflags.DEFINE_integer(\'seed\', 0, \'Random seed.\')\nflags.DEFINE_boolean(\n    \'resnet\', False, \'Use a ResNet for image classification.\' +\n    \'The default is to use the LeNet5 arhitecture.\' +\n    \'Currently only supported on cifar10.\')\nflags.DEFINE_boolean(\'batchnorm\', False,\n                     \'Use batchnorm. Only applies when resnet is True.\')\nFLAGS = flags.FLAGS\n\n\ndef main(argv):\n  del argv  # unused arg\n  np.random.seed(FLAGS.seed)\n  tf.random.set_seed(FLAGS.seed)\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  tf1.disable_v2_behavior()\n\n  session = tf1.Session()\n  x_train, y_train, x_test, y_test = datasets.load(session)\n  n_train = x_train.shape[0]\n  num_classes = int(np.amax(y_train)) + 1\n\n  ensemble_filenames = []\n  for i in range(FLAGS.ensemble_size):\n    # TODO(trandustin): We re-build the graph for each ensemble member. This\n    # is due to an unknown bug where the variables are otherwise not\n    # re-initialized to be random. While this is inefficient in graph mode, I\'m\n    # keeping this for now as we\'d like to move to eager mode anyways.\n    if not FLAGS.resnet:\n      model = lenet5(x_train.shape[1:], num_classes)\n    else:\n      model = res_net(\n          n_train,\n          x_train.shape[1:],\n          num_classes,\n          batchnorm=FLAGS.batchnorm,\n          variational=False)\n\n      def schedule_fn(epoch):\n        """"""Learning rate schedule function.""""""\n        rate = FLAGS.learning_rate\n        if epoch > 180:\n          rate *= 0.5e-3\n        elif epoch > 160:\n          rate *= 1e-3\n        elif epoch > 120:\n          rate *= 1e-2\n        elif epoch > 80:\n          rate *= 1e-1\n        return rate\n\n      lr_callback = tf.keras.callbacks.LearningRateScheduler(schedule_fn)\n\n    def negative_log_likelihood(y, rv_y):\n      del rv_y  # unused arg\n      return -model.output.distribution.log_prob(tf.squeeze(y))  # pylint: disable=cell-var-from-loop\n\n    def accuracy(y_true, y_sample):\n      del y_sample  # unused arg\n      return tf.equal(\n          tf.argmax(input=model.output.distribution.logits, axis=1),  # pylint: disable=cell-var-from-loop\n          tf.cast(tf.squeeze(y_true), tf.int64))\n\n    def log_likelihood(y_true, y_sample):\n      del y_sample  # unused arg\n      return model.output.distribution.log_prob(tf.squeeze(y_true))  # pylint: disable=cell-var-from-loop\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(lr=FLAGS.learning_rate),\n        loss=negative_log_likelihood,\n        metrics=[log_likelihood, accuracy])\n    member_dir = os.path.join(FLAGS.output_dir, \'member_\' + str(i))\n    tensorboard = tf1.keras.callbacks.TensorBoard(\n        log_dir=member_dir,\n        update_freq=FLAGS.batch_size * FLAGS.validation_freq)\n\n    if FLAGS.bootstrap:\n      inds = np.random.choice(n_train, n_train, replace=True)\n      x_sampled = x_train[inds]\n      y_sampled = y_train[inds]\n\n    model.fit(\n        x=x_train if not FLAGS.bootstrap else x_sampled,\n        y=y_train if not FLAGS.bootstrap else y_sampled,\n        batch_size=FLAGS.batch_size,\n        epochs=(FLAGS.batch_size * FLAGS.training_steps) // n_train,\n        validation_data=(x_test, y_test),\n        validation_freq=max(\n            (FLAGS.validation_freq * FLAGS.batch_size) // n_train, 1),\n        verbose=1,\n        callbacks=[tensorboard]\n        if not FLAGS.resnet else [tensorboard, lr_callback])\n\n    member_filename = os.path.join(member_dir, \'model.weights\')\n    ensemble_filenames.append(member_filename)\n    model.save_weights(member_filename)\n\n  labels = tf.keras.layers.Input(shape=y_train.shape[1:])\n  ll = tf.keras.backend.function([model.input, labels], [\n      model.output.distribution.log_prob(tf.squeeze(labels)),\n      model.output.distribution.logits,\n  ])\n\n  ensemble_metrics_vals = {\n      \'train\': ensemble_metrics(\n          x_train, y_train, model, ll, weight_files=ensemble_filenames),\n      \'test\': ensemble_metrics(\n          x_test, y_test, model, ll, weight_files=ensemble_filenames),\n  }\n\n  for split, metrics in ensemble_metrics_vals.items():\n    logging.info(split)\n    for metric_name in metrics:\n      logging.info(\'%s: %s\', metric_name, metrics[metric_name])\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
