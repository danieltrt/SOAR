file_path,api_count,code
nucleus/__init__.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\n\n# We want any imports of ""google.protobuf"" to use Nucleus\'s version, so\n# we have to delete any pre-existing ""google"" module.\nif \'google\' in sys.modules:\n  del sys.modules[\'google\']\n'"
nucleus/examples/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
nucleus/examples/add_ad_to_vcf.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""This example program adds the AD info field to a VCF file.\n\nIt assumes that the AD field of the individual variant calls is already\npopulated.\n\nSample usage:\n  $ add_ad_to_vcf input.vcf.gz output.vcf.gz\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\n\nfrom absl import app\nimport six\nfrom nucleus.io import vcf\nfrom nucleus.util import variant_utils\nfrom nucleus.util import variantcall_utils\nfrom nucleus.util import vcf_constants\n\n\ndef get_variant_ad(variant):\n  """"""Returns the allele depth for the Variant, calculated across its calls.""""""\n  num_alleles = len(variant.alternate_bases) + 1\n  call_ads = [variantcall_utils.get_format(vc, \'AD\') for vc in\n              variant.calls]\n  assert(len(call_ad) == num_alleles for call_ad in call_ads)\n  return [sum(call_ad[i] for call_ad in call_ads)\n          for i in six.moves.xrange(num_alleles)]\n\n\ndef main(argv):\n  if len(argv) != 3:\n    print(\'Usage: %s <input_vcf> <output_vcf>\' % argv[0])\n    sys.exit(-1)\n  in_vcf = argv[1]\n  out_vcf = argv[2]\n\n  with vcf.VcfReader(in_vcf) as reader:\n    if \'AD\' in [info.id for info in reader.header.infos]:\n      print(\'%s already contains AD field.\' % in_vcf)\n      sys.exit(-1)\n    out_header = reader.header\n    out_header.infos.extend([vcf_constants.reserved_info_field(\'AD\')])\n\n    with vcf.VcfWriter(out_vcf, header=out_header) as writer:\n      for variant in reader:\n        variant_utils.set_info(variant, \'AD\', get_variant_ad(variant),\n                               writer)\n        writer.write(variant)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
nucleus/examples/add_ad_to_vcf_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.examples.add_ad_to_vcf.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\n\nfrom nucleus.examples import add_ad_to_vcf\nfrom nucleus.io import vcf\nfrom nucleus.testing import test_utils\nfrom nucleus.util import variant_utils\n\n\nclass AddAdToVcfTest(absltest.TestCase):\n\n  def test_main(self):\n    in_fname = test_utils.genomics_core_testdata(\'test_allele_depth.vcf\')\n    out_fname = test_utils.test_tmpfile(\'output.vcf\')\n    add_ad_to_vcf.main([\'add_ad_to_vcf\', in_fname, out_fname])\n\n    with vcf.VcfReader(out_fname) as reader:\n      info_ids = [info.id for info in reader.header.infos]\n      self.assertTrue(\'AD\' in info_ids)\n      variant1 = next(reader)\n      self.assertEqual([3, 3], variant_utils.get_info(variant1, \'AD\', reader))\n      variant2 = next(reader)\n      self.assertEqual([30, 44], variant_utils.get_info(variant2, \'AD\', reader))\n      variant3 = next(reader)\n      self.assertEqual([15, 4], variant_utils.get_info(variant3, \'AD\', reader))\n      variant4 = next(reader)\n      self.assertEqual([2, 4], variant_utils.get_info(variant4, \'AD\', reader))\n      variant5 = next(reader)\n      self.assertEqual([24, 2], variant_utils.get_info(variant5, \'AD\', reader))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/examples/apply_genotyping_prior.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Given new genotype priors, update the variant calls in a VCF or gVCF file.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nfrom absl import app\nfrom absl import flags\n\nfrom nucleus.io import vcf\nfrom nucleus.util import variant_utils\nfrom nucleus.util import variantcall_utils\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    \'input_vcf\', None,\n    \'Filename of the VCF file to read.\')\nflags.DEFINE_string(\n    \'output_vcf\', None,\n    \'Filename of the VCF file to write to.\')\nflags.DEFINE_list(\n    \'genotype_priors\', [],\n    \'List of genotype priors.  The length of this list should be the same \'\n    \'as the length of the PL or GL field in the input_vcf.\')\nflags.DEFINE_float(\n    \'posterior_margin\', 0.0,\n    \'If the difference between the top two posteriors is less than this, \'\n    \'make no call (i.e., call it as ""./."").\')\n\n\ndef recall_variant(log_priors, variant):\n  """"""Update the genotype calls in variant given the new genotype priors.""""""\n  for call in variant.calls:\n    if len(log_priors) != len(call.genotype_likelihood):\n      continue\n\n    posteriors = [x + y for x, y in zip(log_priors, call.genotype_likelihood)]\n    sorted_posts = sorted(posteriors)\n    highest_post = sorted_posts[-1]\n\n    margin = 0\n    if FLAGS.posterior_margin > 0.0 and len(log_priors) > 1:\n      margin = math.pow(10.0, highest_post) - math.pow(10.0, sorted_posts[-2])\n\n    ploidy = variantcall_utils.ploidy(call)\n    if margin < FLAGS.posterior_margin:\n      call.genotype[:] = [-1] * ploidy\n    else:\n      best_genotype = posteriors.index(highest_post)\n      call.genotype[:] = (\n          variant_utils.allele_indices_for_genotype_likelihood_index(\n              best_genotype, ploidy=ploidy))\n\n\ndef main(argv):\n  del argv\n\n  priors = map(float, FLAGS.genotype_priors)\n  sump = sum(priors)\n  log_priors = [math.log10(x / sump) for x in priors]\n\n  with vcf.VcfReader(FLAGS.input_vcf) as reader:\n    with vcf.VcfWriter(FLAGS.output_vcf, header=reader.header) as writer:\n      for variant in reader:\n        recall_variant(log_priors, variant)\n        # TODO(thomaswc): Also update the variant\'s quality.\n        writer.write(variant)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flags_as_required([\'input_vcf\', \'output_vcf\'])\n  app.run(main)\n'"
nucleus/examples/apply_genotyping_prior_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.examples.apply_genotyping_prior.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nfrom absl.testing import absltest\n\nfrom nucleus.examples import apply_genotyping_prior\nfrom nucleus.protos import variants_pb2\n\n\nclass RecallVariantsTest(absltest.TestCase):\n\n  def test_recall_variant(self):\n    variant = variants_pb2.Variant(reference_bases=\'A\',\n                                   alternate_bases=[\'T\'])\n    call = variant.calls.add()\n    call.genotype[:] = [0, 1]\n    call.genotype_likelihood[:] = [-2.5, -1.0, -2.0]\n\n    log_third = math.log10(1.0 / 3.0)\n    flat_priors = [log_third, log_third, log_third]\n    apply_genotyping_prior.recall_variant(flat_priors, variant)\n    self.assertEqual([0, 1], variant.calls[0].genotype)\n\n    realistic_priors = [math.log10(0.99), -2.0, -2.0]\n    apply_genotyping_prior.recall_variant(realistic_priors, variant)\n    self.assertEqual([0, 0], variant.calls[0].genotype)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n\n'"
nucleus/examples/ascii_pileup.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Print an ASCII art pileup image.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\n\nfrom nucleus.io import sam\nfrom nucleus.util import ranges\n\nANSI_BOLD = \'\\033[1m\'\nANSI_RED = \'\\033[91m\'\nANSI_OFF = \'\\033[0m\'\n\n# TODO(thomaswc): Also have this print out the reference sequence at the\n# top if a reference fasta file is supplied.\n\n\ndef read_str(left_pos, start, highlight_position, seq):\n  """"""Returns an aligned and highlighted ASCII representation of sequence.""""""\n  s = \' \' * (start - left_pos)\n  i = highlight_position - start\n  j = i + 1\n  s += seq[:i] + ANSI_BOLD + ANSI_RED + seq[i:j] + ANSI_OFF + seq[j:]\n  return s\n\n\ndef ascii_pileup(sam_filename, query):\n  """"""Returns an ASCII pileup image for the query as a list of strings.\n\n  Args:\n    sam_filename: The filename of the BAM/SAM file.\n    query: String version of range.\n  """"""\n  r = ranges.parse_literal(query)\n  position = r.start\n\n  with sam.SamReader(sam_filename) as sam_reader:\n    reads = sam_reader.query(r)\n    pos_seq_pairs = sorted(\n        (read.alignment.position.position, read.aligned_sequence)\n        for read in reads)\n    if not pos_seq_pairs:\n      print(\'No overlapping reads found for\', query)\n      return []\n\n    left_position = pos_seq_pairs[0][0]\n    return [read_str(left_position, start, position, seq)\n            for start, seq in pos_seq_pairs]\n\n\ndef main(argv):\n  if len(argv) != 3:\n    print(\'Usage: {} <input_sam> <chromosome>:<position>\'.format(argv[0]))\n    return -1\n  in_sam = argv[1]\n  query = argv[2]\n  print(\'\\n\'.join(ascii_pileup(in_sam, query)))\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
nucleus/examples/ascii_pileup_test.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.examples.ascii_pileup.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\n\nfrom nucleus.examples import ascii_pileup\nfrom nucleus.testing import test_utils\n\n\nclass AsciiPileupTest(absltest.TestCase):\n\n  def test_read_str(self):\n    s = ascii_pileup.read_str(10, 15, 20, \'AACCGGTTAACCGGTT\')\n    self.assertTrue(s.startswith(\'     AACCG\'))\n    self.assertTrue(s.endswith(\'TTAACCGGTT\'))\n\n  def test_ascii_pileup(self):\n    in_sam = test_utils.genomics_core_testdata(\'test.bam\')\n\n    self.assertEqual([], ascii_pileup.ascii_pileup(in_sam, \'chr1:10050\'))\n    ap = ascii_pileup.ascii_pileup(in_sam, \'chr20:9999999\')\n    self.assertLen(ap, 45)\n    self.assertEqual(\n        \'CAACTGACCATAGGTGTATTGGTTTATTTCTGTACTCTTAGTAGATTCCATTGACCTATATCTCT\'\n        \'ATCCTTATGCCAGTACCACACT\' + ascii_pileup.ANSI_BOLD +\n        ascii_pileup.ANSI_RED + \'G\' + ascii_pileup.ANSI_OFF + \'TTTTGTTTACTAC\',\n        ap[0])\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/examples/count_variants.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Counts variants in a VCF, both by type and per chromosome.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport sys\n\nfrom absl import app\nfrom nucleus.io import vcf\nfrom nucleus.util import variant_utils\n\n\ndef main(argv):\n  if len(argv) != 2:\n    print(\'Usage: {} <input_vcf>\'.format(argv[0]))\n    sys.exit(-1)\n  in_vcf = argv[1]\n\n  total = 0\n  by_type = collections.defaultdict(int)\n  by_ref = collections.defaultdict(int)\n\n  with vcf.VcfReader(in_vcf) as reader:\n    for variant in reader:\n      total += 1\n      by_type[variant_utils.variant_type(variant)] += 1\n      by_ref[variant.reference_name] += 1\n\n  print(\'# variants: {}\'.format(total))\n  print(\'# ref variants: {}\'.format(by_type[variant_utils.VariantType.ref]))\n  print(\'# SNP variants: {}\'.format(by_type[variant_utils.VariantType.snp]))\n  print(\'# indel variants: {}\'.format(by_type[variant_utils.VariantType.indel]))\n  for k, v in sorted(by_ref.items()):\n    print(\'# variants in {}: {}\'.format(k, v))\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
nucleus/examples/count_variants_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.examples.count_variants.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nimport mock\nimport six\n\nfrom nucleus.examples import count_variants\nfrom nucleus.testing import test_utils\n\n\nclass CountVariantsTest(absltest.TestCase):\n\n  def test_main(self):\n    in_fname = test_utils.genomics_core_testdata(\'test_allele_depth.vcf\')\n    with mock.patch.object(six.moves.builtins, \'print\') as mock_print:\n      count_variants.main([\'count_variants\', in_fname])\n      self.assertEqual([\n          mock.call(\'# variants: 5\'),\n          mock.call(\'# ref variants: 0\'),\n          mock.call(\'# SNP variants: 5\'),\n          mock.call(\'# indel variants: 0\'),\n          mock.call(\'# variants in Chr1: 5\'),\n      ], mock_print.mock_calls)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/examples/filter_vcf.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Writes all the variants in a VCF file with a quality greater than 3.01.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\n\nfrom absl import app\n\nfrom nucleus.io import vcf\n\n\ndef main(argv):\n  if len(argv) != 3:\n    print(\'Usage: {} <input_vcf> <output_vcf>\'.format(argv[0]))\n    sys.exit(-1)\n  in_vcf = argv[1]\n  out_vcf = argv[2]\n\n  # Please try to keep the following part in sync with the documenation in\n  # g3doc/overview.md.\n  with vcf.VcfReader(in_vcf) as reader:\n    print(\'Sample names in VCF: \', \' \'.join(reader.header.sample_names))\n    with vcf.VcfWriter(out_vcf, header=reader.header) as writer:\n      for variant in reader:\n        if variant.quality > 3.01:\n          writer.write(variant)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
nucleus/examples/filter_vcf_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.examples.filter_vcf.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\n\nfrom nucleus.examples import filter_vcf\nfrom nucleus.io import vcf\nfrom nucleus.testing import test_utils\n\n\nclass FilterVcfTest(absltest.TestCase):\n\n  def test_main(self):\n    in_fname = test_utils.genomics_core_testdata(\'test_vaf.vcf\')\n    out_fname = test_utils.test_tmpfile(\'output.vcf\')\n    filter_vcf.main([\'filter_vcf\', in_fname, out_fname])\n\n    with vcf.VcfReader(out_fname) as reader:\n      variants = list(reader)\n      self.assertEqual(3, len(variants))\n      self.assertEqual([\'DogSNP4\', \'DogSNP5\', \'DogSNP6\'],\n                       [v.names[0] for v in variants])\n      for v in variants:\n        self.assertTrue(v.quality > 3.01)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/examples/print_tfrecord.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Prints a TFRecord file created by Nucleus.\n\nUsage:\n  print_tfrecord <filename> <proto_name>\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\n\nfrom absl import app\n\nfrom google.protobuf import text_format\nfrom nucleus.io import genomics_reader\nfrom nucleus.protos import bed_pb2\nfrom nucleus.protos import bedgraph_pb2\nfrom nucleus.protos import fasta_pb2\nfrom nucleus.protos import fastq_pb2\nfrom nucleus.protos import gff_pb2\nfrom nucleus.protos import reads_pb2\nfrom nucleus.protos import variants_pb2\n# pylint: disable=g-direct-tensorflow-import\nfrom nucleus.protos import example_pb2\n\nPROTO_DB = {\n    \'BedGraphRecord\': bedgraph_pb2.BedGraphRecord,\n    \'BedRecord\': bed_pb2.BedRecord,\n    \'FastaRecord\': fasta_pb2.FastaRecord,\n    \'FastqRecord\': fastq_pb2.FastqRecord,\n    \'GffRecord\': gff_pb2.GffRecord,\n    \'Read\': reads_pb2.Read,\n    \'Variant\': variants_pb2.Variant,\n    \'Example\': example_pb2.Example\n}\n\n\ndef main(argv):\n  if len(argv) != 3:\n    print(\'Usage: {} <filename> <proto_name>\\n\'.format(argv[0]))\n    sys.exit(-1)\n\n  filename = argv[1]\n  proto_name = argv[2]\n\n  if proto_name not in PROTO_DB:\n    print(\'Unknown protocol buffer name {}\\n\'.format(proto_name))\n    print(\'Known names are: {}\\n\'.format(\' \'.join(PROTO_DB.keys())))\n    sys.exit(-1)\n\n  proto = PROTO_DB[proto_name]\n\n  with genomics_reader.TFRecordReader(filename, proto=proto) as reader:\n    for record in reader:\n      print(text_format.MessageToString(record))\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
nucleus/examples/validate_vcf.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Validates that a VCF file and a FASTA reference file correspond.\n\nThey correspond if:\na) they cover the same contigs,\nb) the reference covers every variant in the vcf file, and\nc) they agree on the reference bases covered by the variants.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\n\nfrom absl import app\n\nfrom nucleus.io import fasta\nfrom nucleus.io import vcf\nfrom nucleus.util import variant_utils\n\n\ndef validate_contigs(ref_contigs, vcf_contigs):\n  """"""Validate that the two lists of ContigInfos have the same set of names.""""""\n  ref_names = {ci.name for ci in ref_contigs}\n  vcf_names = {ci.name for ci in vcf_contigs}\n  if ref_names != vcf_names:\n    print(\'Contig names differ!\')\n    print(\'The following contigs are in one but not both: \')\n    print(ref_names ^ vcf_names)\n    sys.exit(-1)\n\n\ndef validate_variant(ref_reader, variant):\n  """"""Validate that variant is covered by the reference and agrees with it.""""""\n  var_range = variant_utils.variant_range(variant)\n  ref_bases = \'\'\n  try:\n    ref_bases = ref_reader.query(var_range)\n  except ValueError:\n    print(\'Reference does not cover range {}: {}-{}\'.format(\n        var_range.reference_name, var_range.start, var_range.end))\n    sys.exit(-1)\n\n  if ref_bases != variant.reference_bases:\n    print(\'In range {}:{}-{} \'.format(\n        var_range.reference_name, var_range.start, var_range.end))\n    print(\'Reference says \', ref_bases)\n    print(\'But variant says \', variant.reference_bases)\n    sys.exit(-1)\n\n\ndef main(argv):\n  if len(argv) != 3:\n    print(\'Usage: {} <input_ref> <input_vcf>\'.format(argv[0]))\n    sys.exit(-1)\n  in_ref = argv[1]\n  in_vcf = argv[2]\n\n  with fasta.IndexedFastaReader(in_ref) as ref_reader:\n    with vcf.VcfReader(in_vcf) as vcf_reader:\n      validate_contigs(ref_reader.header.contigs, vcf_reader.header.contigs)\n      for variant in vcf_reader:\n        validate_variant(ref_reader, variant)\n\n  # VCF is valid!\n  print(\'Reference and VCF are compatible.\')\n  sys.exit(0)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
nucleus/examples/validate_vcf_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.examples.validate_vcf.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\n\nfrom absl.testing import absltest\nimport mock\nfrom nucleus.examples import validate_vcf\nfrom nucleus.protos import reference_pb2\nfrom nucleus.protos import variants_pb2\nfrom nucleus.testing import test_utils\n\n\nclass ValidateVcfTest(absltest.TestCase):\n\n  def test_validate_contigs(self):\n    contigs1 = [\n        reference_pb2.ContigInfo(name=\'chr1\'),\n        reference_pb2.ContigInfo(name=\'chr2\')]\n    contigs2 = [\n        reference_pb2.ContigInfo(name=\'ChrX\'),\n        reference_pb2.ContigInfo(name=\'ChrY\')]\n\n    with mock.patch.object(sys, \'exit\') as mock_exit:\n      validate_vcf.validate_contigs(contigs1, contigs1)\n      mock_exit.assert_not_called()\n\n    with mock.patch.object(sys, \'exit\') as mock_exit:\n      validate_vcf.validate_contigs(contigs2, contigs2)\n      mock_exit.assert_not_called()\n\n    with mock.patch.object(sys, \'exit\') as mock_exit:\n      validate_vcf.validate_contigs(contigs1, contigs2)\n      mock_exit.assert_called_once_with(-1)\n\n  def test_validate_variant(self):\n    variant = variants_pb2.Variant(\n        reference_name=\'chr1\',\n        start=5,\n        end=10,\n        reference_bases=\'AATTG\')\n\n    with mock.patch.object(sys, \'exit\') as mock_exit:\n      mock_ref = mock.MagicMock()\n      mock_ref.query.return_value = \'AATTG\'\n      validate_vcf.validate_variant(mock_ref, variant)\n      mock_exit.assert_not_called()\n\n    with mock.patch.object(sys, \'exit\') as mock_exit:\n      mock_ref2 = mock.MagicMock()\n      mock_ref2.query.return_value = \'CATAT\'\n      validate_vcf.validate_variant(mock_ref2, variant)\n      mock_exit.assert_called_once_with(-1)\n\n  def test_main(self):\n    in_ref = test_utils.genomics_core_testdata(\'test.fasta\')\n    in_vcf = test_utils.genomics_core_testdata(\'test_phaseset.vcf\')\n\n    with mock.patch.object(sys, \'exit\') as mock_exit:\n      validate_vcf.main([\'validate_vcf\', in_ref, in_vcf])\n      # Only the first call to sys.exit() matters.\n      self.assertEqual(mock.call(-1), mock_exit.mock_calls[0])\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
nucleus/io/bed.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Classes for reading and writing BED files.\n\nThe BED format is described at\nhttps://genome.ucsc.edu/FAQ/FAQformat.html#format1\n\nAPI for reading:\n\n```python\nfrom nucleus.io import bed\n\n# Iterate through all records.\nwith bed.BedReader(input_path) as reader:\n  for record in reader:\n    print(record)\n```\n\nwhere `record` is a `nucleus.genomics.v1.BedRecord` protocol buffer.\n\nAPI for writing:\n\n```python\nfrom nucleus.io import bed\nfrom nucleus.protos import bed_pb2\n\n# records is an iterable of nucleus.genomics.v1.BedRecord protocol buffers.\nrecords = ...\n\n# header defines how many fields to write out.\nheader = bed_pb2.BedHeader(num_fields=5)\n\n# Write all records to the desired output path.\nwith bed.BedWriter(output_path, header) as writer:\n  for record in records:\n    writer.write(record)\n```\n\nFor both reading and writing, if the path provided to the constructor contains\n\'.tfrecord\' as an extension, a `TFRecord` file is assumed and attempted to be\nread or written. Otherwise, the filename is treated as a true BED file.\n\nFiles that end in a \'.gz\' suffix cause the file to be treated as compressed\n(with BGZF if it is a true BED file, and with gzip if it is a TFRecord file).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom nucleus.io import genomics_reader\nfrom nucleus.io import genomics_writer\nfrom nucleus.io.python import bed_reader\nfrom nucleus.io.python import bed_writer\nfrom nucleus.protos import bed_pb2\n\n\nclass NativeBedReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from native BED files.\n\n  Most users will want to use BedReader instead, because it dynamically\n  dispatches between reading native BED files and TFRecord files based on the\n  filename\'s extension.\n  """"""\n\n  def __init__(self, input_path, num_fields=0):\n    """"""Initializes a NativeBedReader.\n\n    Args:\n      input_path: string. A path to a resource containing BED records.\n      num_fields: int. The number of fields to read in the BED. If unset or set\n        to zero, all fields in the input are read.\n    """"""\n    super(NativeBedReader, self).__init__()\n\n    bed_path = input_path.encode(\'utf8\')\n    options = bed_pb2.BedReaderOptions(num_fields=num_fields)\n    self._reader = bed_reader.BedReader.from_file(bed_path, options)\n    self.header = self._reader.header\n\n  def query(self):\n    """"""Returns an iterator for going through the records in the region.\n\n    NOTE: This function is not currently implemented by NativeBedReader though\n    it could be implemented for sorted, tabix-indexed BED files.\n    """"""\n    raise NotImplementedError(\'Can not currently query a BED file\')\n\n  def iterate(self):\n    """"""Returns an iterable of BedRecord protos in the file.""""""\n    return self._reader.iterate()\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass BedReader(genomics_reader.DispatchingGenomicsReader):\n  """"""Class for reading BedRecord protos from BED or TFRecord files.""""""\n\n  def _native_reader(self, input_path, **kwargs):\n    return NativeBedReader(input_path, **kwargs)\n\n  def _record_proto(self):\n    return bed_pb2.BedRecord\n\n\nclass NativeBedWriter(genomics_writer.GenomicsWriter):\n  """"""Class for writing to native BED files.\n\n  Most users will want BedWriter, which will write to either native BED\n  files or TFRecord files, based on the output filename\'s extension.\n  """"""\n\n  def __init__(self, output_path, header=None):\n    """"""Initializer for NativeBedWriter.\n\n    Args:\n      output_path: str. The path to which to write the BED file.\n      header: nucleus.genomics.v1.BedHeader. The header that defines all\n        information germane to the constituent BED records.\n    """"""\n    super(NativeBedWriter, self).__init__()\n    if header is None:\n      header = bed_pb2.BedHeader(num_fields=3)\n    writer_options = bed_pb2.BedWriterOptions()\n    self._writer = bed_writer.BedWriter.to_file(output_path, header,\n                                                writer_options)\n\n  def write(self, proto):\n    self._writer.write(proto)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass BedWriter(genomics_writer.DispatchingGenomicsWriter):\n  """"""Class for writing BedRecord protos to BED or TFRecord files.""""""\n\n  def _native_writer(self, output_path, header):\n    return NativeBedWriter(output_path, header=header)\n'"
nucleus/io/bed_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.io.bed.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import bed\nfrom nucleus.protos import bed_pb2\nfrom nucleus.testing import test_utils\n\n_VALID_NUM_BED_FIELDS = [3, 4, 5, 6, 8, 9, 12]\n\n\nclass BedReaderTests(parameterized.TestCase):\n\n  @parameterized.parameters(\'test_regions.bed\', \'test_regions.bed.gz\',\n                            \'test_regions.bed.tfrecord\',\n                            \'test_regions.bed.tfrecord.gz\')\n  def test_iterate_bed_reader(self, bed_filename):\n    bed_path = test_utils.genomics_core_testdata(bed_filename)\n    expected = [(\'chr1\', 10, 20), (\'chr1\', 100, 200)]\n    with bed.BedReader(bed_path) as reader:\n      records = list(reader.iterate())\n    self.assertLen(records, 2)\n    self.assertEqual([(r.reference_name, r.start, r.end) for r in records],\n                     expected)\n\n  @parameterized.parameters(\'test_regions.bed\', \'test_regions.bed.gz\')\n  def test_native_bed_header(self, bed_filename):\n    bed_path = test_utils.genomics_core_testdata(bed_filename)\n    with bed.BedReader(bed_path) as reader:\n      self.assertEqual(reader.header.num_fields, 12)\n    with bed.NativeBedReader(bed_path) as native_reader:\n      self.assertEqual(native_reader.header.num_fields, 12)\n\n  @parameterized.parameters(1, 2, 7, 10, 11, 13)\n  def test_invalid_num_fields(self, invalid_num_fields):\n    bed_path = test_utils.genomics_core_testdata(\'test_regions.bed\')\n    with self.assertRaisesRegexp(ValueError, \'Invalid requested number of fie\'):\n      _ = bed.BedReader(bed_path, num_fields=invalid_num_fields)\n\n\nclass BedWriterTests(parameterized.TestCase):\n  """"""Tests for BedWriter.""""""\n\n  def setUp(self):\n    self.records = [\n        bed_pb2.BedRecord(\n            reference_name=\'chr1\', start=30, end=40, name=\'first\', score=55.5),\n        bed_pb2.BedRecord(\n            reference_name=\'chr2\', start=32, end=38, name=\'second\', score=0),\n        bed_pb2.BedRecord(\n            reference_name=\'chr3\', start=40, end=50, name=\'third\', score=99),\n    ]\n    self.tokens = [\n        [\n            \'chr1\', \'30\', \'40\', \'first\', \'55.5\', \'+\', \'35\', \'38\', \'128,242,16\',\n            \'2\', \'5,3\', \'30,37\'\n        ],\n        [\n            \'chr2\', \'32\', \'38\', \'second\', \'0\', \'.\', \'32\', \'38\', \'128,128,128\',\n            \'1\', \'6\', \'32\'\n        ],\n        [\n            \'chr3\', \'40\', \'50\', \'third\', \'99\', \'-\', \'40\', \'44\', \'0,0,0\', \'3\',\n            \'40,43,48\', \'3,2,2\'\n        ],\n    ]\n\n  @parameterized.parameters(\'test_raw.bed\', \'test_zipped.bed.gz\',\n                            \'test_raw.tfrecord\', \'test_zipped.tfrecord.gz\')\n  def test_roundtrip_writer(self, filename):\n    output_path = test_utils.test_tmpfile(filename)\n    with bed.BedWriter(\n        output_path, header=bed_pb2.BedHeader(num_fields=5)) as writer:\n      for record in self.records:\n        writer.write(record)\n\n    with bed.BedReader(output_path) as reader:\n      v2_records = list(reader.iterate())\n\n    self.assertEqual(self.records, v2_records)\n\n  @parameterized.parameters(3, 4, 5, 6, 8, 9, 12)\n  def test_roundtrip_num_fields(self, num_fields):\n    all_num_fields_in_file = [\n        n for n in _VALID_NUM_BED_FIELDS if n >= num_fields\n    ]\n    for num_fields_in_file in all_num_fields_in_file:\n      lines = [\'\\t\'.join(line[:num_fields_in_file]) for line in self.tokens]\n      contents = \'{}\\n\'.format(\'\\n\'.join(lines))\n      input_path = test_utils.test_tmpfile(\'test_field.bed\', contents=contents)\n\n      with bed.BedReader(input_path, num_fields=num_fields) as reader:\n        records = list(reader.iterate())\n      output_path = test_utils.test_tmpfile(\'test_field2.bed\')\n      with bed.BedWriter(output_path, header=reader.header) as writer:\n        for record in records:\n          writer.write(record)\n\n      with bed.BedReader(output_path) as reader2:\n        v2_records = list(reader2.iterate())\n\n      self.assertLen(records, 3)\n      self.assertEqual(records, v2_records)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/bedgraph.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Classes for reading and writing BedGraph files.\n\nThe BedGraph format is described at\nhttps://genome.ucsc.edu/goldenpath/help/bedgraph.html\n\nAPI for reading:\n\n```python\nfrom nucleus.io import bedgraph\n\n# Iterate through all records.\nwith bed.BedGraphReader(input_path) as reader:\n  for record in reader:\n    print(record)\n```\n\nwhere `record` is a `nucleus.genomics.v1.BedGraphRecord` protocol buffer.\n\nAPI for writing:\n\n```python\nfrom nucleus.io import bedgraph\nfrom nucleus.protos import bedgraph_pb2\n\n# records is an iterable of nucleus.genomics.v1.BedGraphRecord protocol buffers.\nrecords = ...\n\n# Write all records to the desired output path.\nwith bed.BedGraphWriter(output_path) as writer:\n  for record in records:\n    writer.write(record)\n```\n\nFor both reading and writing, if the path provided to the constructor contains\n\'.tfrecord\' as an extension, a `TFRecord` file is assumed and attempted to be\nread or written. Otherwise, the filename is treated as a true BedGraph file.\n\nFiles that end in a \'.gz\' suffix cause the file to be treated as compressed\n(with BGZF if it is a BedGraph file, and with gzip if it is a TFRecord file).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom nucleus.io import genomics_reader\nfrom nucleus.io import genomics_writer\nfrom nucleus.io.python import bedgraph_reader\nfrom nucleus.io.python import bedgraph_writer\nfrom nucleus.protos import bedgraph_pb2\n\n\nclass NativeBedGraphReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from native BedGraph files.\n\n  Most users will want to use BedGraphReader instead, because it dynamically\n  dispatches between reading native BedGraph files and TFRecord files based on\n  the filename\'s extension.\n  """"""\n\n  def __init__(self, input_path, num_fields=0):\n    """"""Initializes a NativeBedGraphReader.\n\n    Args:\n      input_path: string. A path to a resource containing BedGraph records.\n      num_fields: int. The number of fields to read in the BedGraph. If unset or\n        set to zero, all fields in the input are read.\n    """"""\n    super(NativeBedGraphReader, self).__init__()\n\n    bedgraph_path = input_path.encode(\'utf8\')\n    self._reader = bedgraph_reader.BedGraphReader.from_file(bedgraph_path)\n\n  def query(self):\n    """"""Returns an iterator for going through the records in the region.\n\n    NOTE: This function is not currently implemented by NativeBedGraphReader\n    though it could be implemented for sorted, tabix-indexed BedGraph files.\n    """"""\n    raise NotImplementedError(\'Can not currently query a BedGraph file\')\n\n  def iterate(self):\n    """"""Returns an iterable of BedGraphRecord protos in the file.""""""\n    return self._reader.iterate()\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass BedGraphReader(genomics_reader.DispatchingGenomicsReader):\n  """"""Class for reading BedGraphRecord protos from BedGraph or TFRecord files.""""""\n\n  def _native_reader(self, input_path, **kwargs):\n    return NativeBedGraphReader(input_path, **kwargs)\n\n  def _record_proto(self):\n    return bedgraph_pb2.BedGraphRecord\n\n\nclass NativeBedGraphWriter(genomics_writer.GenomicsWriter):\n  """"""Class for writing to native BedGraph files.\n\n  Most users will want BedGraphWriter, which will write to either native\n  BedGraph files or TFRecord files, based on the output filename\'s extension.\n  """"""\n\n  def __init__(self, output_path, header=None):\n    """"""Initializer for NativeBedGraphWriter.\n\n    Args:\n      output_path: str. The path to which to write the BedGraph file.\n    """"""\n    super(NativeBedGraphWriter, self).__init__()\n    self._writer = bedgraph_writer.BedGraphWriter.to_file(output_path)\n\n  def write(self, proto):\n    self._writer.write(proto)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass BedGraphWriter(genomics_writer.DispatchingGenomicsWriter):\n  """"""Class for writing BedGraphRecord protos to BedGraph or TFRecord files.""""""\n\n  def _native_writer(self, output_path):\n    return NativeBedGraphWriter(output_path)\n'"
nucleus/io/bedgraph_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.io.bedgraph.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import bedgraph\nfrom nucleus.protos import bedgraph_pb2\nfrom nucleus.testing import test_utils\n\n\nclass BedGraphTests(parameterized.TestCase):\n\n  @parameterized.parameters(\'test_regions.bedgraph\', \'test_regions.bedgraph.gz\')\n  def test_iterate_bedgraph_reader(self, bedgraph_path):\n    bedgraph_path = test_utils.genomics_core_testdata(bedgraph_path)\n    expected = [(\'chr1\', 10, 20, 100), (\'chr1\', 100, 200, 250),\n                (\'chr1\', 300, 400, 150.1), (\'chr1\', 500, 501, 20.13)]\n    with bedgraph.BedGraphReader(bedgraph_path) as reader:\n      records = list(reader.iterate())\n    self.assertLen(records, 4)\n    self.assertEqual(\n        [(r.reference_name, r.start, r.end, r.data_value) for r in records],\n        expected)\n\n  @parameterized.parameters(\'test_regions.bedgraph\', \'test_regions.bedgraph.gz\')\n  def test_roundtrip_writer(self, bedgraph_path):\n    output_path = test_utils.test_tmpfile(bedgraph_path)\n    input_path = test_utils.genomics_core_testdata(bedgraph_path)\n    records = []\n    with bedgraph.BedGraphReader(input_path) as reader:\n      records = list(reader.iterate())\n\n    with bedgraph.BedGraphWriter(output_path) as writer:\n      for record in records:\n        writer.write(record)\n\n    with bedgraph.BedGraphReader(output_path) as reader:\n      v2_records = list(reader.iterate())\n\n    self.assertLen(records, 4)\n    self.assertEqual(records, v2_records)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/clif_postproc.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""CLIF postprocessors.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport six\n\nfrom nucleus.protos import bed_pb2\nfrom nucleus.protos import bedgraph_pb2\nfrom nucleus.protos import fastq_pb2\nfrom nucleus.protos import gff_pb2\nfrom nucleus.protos import reads_pb2\nfrom nucleus.protos import variants_pb2\n\n\ndef ValueErrorOnFalse(ok, *args):\n  """"""Returns None / arg / (args,...) if ok.""""""\n  if not isinstance(ok, bool):\n    raise TypeError(\'Use ValueErrorOnFalse only on bool return value\')\n  if not ok:\n    raise ValueError(\'CLIF wrapped call returned False\')\n  # Plain return args will turn 1 into (1,)  and None into () which is unwanted.\n  if args:\n    return args if len(args) > 1 else args[0]\n  return None\n\n\nclass WrappedCppIterable(six.Iterator):\n  """"""This class gives Python iteration semantics on top of a C++ \'Iterable\'.""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, cc_iterable):\n    self._cc_iterable = cc_iterable\n\n  def __enter__(self):\n    self._cc_iterable.__enter__()\n    return self\n\n  def __exit__(self, type_, value, traceback):\n    self._cc_iterable.__exit__(type_, value, traceback)\n\n  def __iter__(self):\n    return self\n\n  @abc.abstractmethod\n  def _raw_next(self):\n    """"""Sub-classes should implement __next__ in this method.""""""\n\n  def __next__(self):\n    try:\n      record, not_done = self._raw_next()\n    except AttributeError:\n      if self._cc_iterable is None:\n        raise ValueError(\'No underlying iterable. This may occur if trying to \'\n                         \'create multiple concurrent iterators from the same \'\n                         \'reader. Try wrapping your call to the iterator in a \'\n                         \'`with` block or materializing the entire iterable \'\n                         \'explicitly.\')\n      else:\n        raise\n    if not_done:\n      return record\n    else:\n      raise StopIteration\n\n\nclass WrappedBedIterable(WrappedCppIterable):\n\n  def _raw_next(self):\n    record = bed_pb2.BedRecord()\n    not_done = self._cc_iterable.PythonNext(record)\n    return record, not_done\n\n\nclass WrappedBedGraphIterable(WrappedCppIterable):\n\n  def _raw_next(self):\n    record = bedgraph_pb2.BedGraphRecord()\n    not_done = self._cc_iterable.PythonNext(record)\n    return record, not_done\n\n\nclass WrappedFastqIterable(WrappedCppIterable):\n\n  def _raw_next(self):\n    record = fastq_pb2.FastqRecord()\n    not_done = self._cc_iterable.PythonNext(record)\n    return record, not_done\n\n\nclass WrappedGffIterable(WrappedCppIterable):\n\n  def _raw_next(self):\n    record = gff_pb2.GffRecord()\n    not_done = self._cc_iterable.PythonNext(record)\n    return record, not_done\n\n\nclass WrappedReferenceIterable(WrappedCppIterable):\n\n  def _raw_next(self):\n    not_done, record = self._cc_iterable.Next()\n    return record, not_done\n\n\nclass WrappedSamIterable(WrappedCppIterable):\n\n  def _raw_next(self):\n    record = reads_pb2.Read()\n    not_done = self._cc_iterable.PythonNext(record)\n    return record, not_done\n\n\nclass WrappedVariantIterable(WrappedCppIterable):\n\n  def _raw_next(self):\n    record = variants_pb2.Variant()\n    not_done = self._cc_iterable.PythonNext(record)\n    return record, not_done\n'"
nucleus/io/converter.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A universal converter program for nucleus-supported genomics file formats.\n\nInvoked with a single argument, this program will open a genomics data file and\niterate over its contents, doing no writing.  This is a good benchmark for I/O\nand reader processing speed.\n\nInvoked with two arguments, the program will open the first file, read its\nrecords, and write them, one at a time, to the second file.  The filetypes for\nthe first and second filename must be compatible ways of encoding the same\nnucleus genomics record type (for example, `infile.gff` and\n`outfile.gff.tfrecord.gz` are compatible, but `infile.gff` and `outfile.bam` are\nnot.\n\nNote: at present we have no convention for encoding a file *header* in\ntfrecords, so conversion is not possible from tfrecord to any native file format\nfor which a header is compulsory.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport sys\nimport time\n\nfrom absl import app\nfrom absl import logging\n\nfrom nucleus.io import bed\nfrom nucleus.io import fastq\nfrom nucleus.io import gff\nfrom nucleus.io import sam\nfrom nucleus.io import vcf\n\n\ndef _is_native_file(filename):\n  """"""Returns true if filename is a native (non-tfrecord) genomics data file.""""""\n  return not re.match(r"".*\\.tfrecord(\\.gz)?"", filename)\n\n\ndef _filename_pattern(ext):\n  """"""Returns an re matching native or tfrecord files of format `ext`.""""""\n  return r"".*\\.{}(\\.tfrecord)?(\\.gz)?"".format(ext)\n\n\n_FileType = collections.namedtuple(\n    ""_FileType"", (""reader_class"", ""writer_class"", ""has_header""))\n\n_FILETYPE_LOOKUP = {\n    _filename_pattern(""bed""):\n        _FileType(bed.BedReader, bed.BedWriter, False),\n    _filename_pattern(""(fastq|fq)""):\n        _FileType(fastq.FastqReader, fastq.FastqWriter, False),\n    _filename_pattern(""gff""):\n        _FileType(gff.GffReader, gff.GffWriter, True),\n    _filename_pattern(""(bam|sam)""):\n        _FileType(sam.SamReader, sam.SamWriter, True),\n    _filename_pattern(""vcf""):\n        _FileType(vcf.VcfReader, vcf.VcfWriter, True),\n}\n\n\ndef _lookup_filetype(filename):\n  for pattern in _FILETYPE_LOOKUP:\n    if re.match(pattern, filename):\n      return _FILETYPE_LOOKUP[pattern]\n  raise ConversionError(""Unrecognized extension!"")\n\nLOG_EVERY = 100000\n\n\nclass ConversionError(Exception):\n  """"""An exception used to signal file conversion error.""""""\n  pass\n\n\nclass NullWriter(object):\n  """"""A writer class whose .write() method is a no-op.\n\n  This allows us to create and use a writer object where one is required by\n  context but we do not wish to write to any file.\n  """"""\n\n  def __init__(self, unused_filename, header=None):\n    pass\n\n  def write(self, unused_record):\n    pass\n\n  def __enter__(self):\n    return self\n\n  def __exit__(self, exc_type, exc_value, traceback):\n    pass\n\n\ndef _reader_writer_classes(in_filename, out_filename):\n  """"""Returns reader, writer classes for filenames, if conversion is possible.\n\n  Args:\n    in_filename: filename of a genomics data file to use as input.\n    out_filename: filename of a genomics data file to use as output, or None,\n      if no output should be written.\n\n  Raises:\n    ConversionError: if in_filename is not convertible to out_filename.\n  """"""\n  in_filetype = _lookup_filetype(in_filename)\n  out_filetype = _lookup_filetype(out_filename) if out_filename else None\n\n  if out_filetype:\n    if in_filetype != out_filetype:\n      raise ConversionError(\n          ""Input and output filetypes specified are incompatible."")\n    input_has_header = in_filetype.has_header and _is_native_file(in_filename)\n    output_requires_header = (\n        out_filetype.has_header and _is_native_file(out_filename))\n    if output_requires_header and not input_has_header:\n      raise ConversionError(\n          ""Input file does not have a header, which is needed to construct ""\n          ""output file"")\n    writer_class = out_filetype.writer_class\n\n  else:\n    writer_class = NullWriter\n\n  return in_filetype.reader_class, writer_class\n\n\ndef convert(in_filename, out_filename):\n  """"""Converts a recognized genomics file `in_filename` to `out_filename`.\n\n  Args:\n    in_filename: str; filename of a genomics data file to use as input.\n    out_filename: str; filename of a genomics data file to use as output, or\n      None, if no output should be written.\n\n  Raises:\n    ConversionError, if the conversion could not be executed.\n  """"""\n  reader_class, writer_class = _reader_writer_classes(in_filename, out_filename)\n  reader = reader_class(in_filename)\n\n  with reader_class(in_filename) as reader:\n    with writer_class(out_filename, header=reader.header) as writer:\n      start = time.time()\n      i = 0\n      for record in reader:\n        i += 1\n        writer.write(record)\n        logging.log_every_n(logging.INFO, ""Progress: %d records"", LOG_EVERY, i)\n      elapsed = time.time() - start\n      logging.info(""Done, processed %d records in %0.2f seconds."", i, elapsed)\n\n\ndef main(argv):\n  if len(argv) not in (2, 3):\n    print(""Usage: %s <input_filename> [<output_filename>]"" % argv[0])\n    sys.exit(1)\n\n  input_filename = argv[1]\n  output_filename = None if len(argv) == 2 else argv[2]\n\n  try:\n    convert(input_filename, output_filename)\n  except ConversionError as e:\n    print(""Could not execute conversion:"", e)\n    sys.exit(1)\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
nucleus/io/converter_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.examples.convert_genomics_file.\n\nThese tests do NOT establish the correctness of conversions---tests of the\nfidelity of the Reader and Writer classes exist elsewhere in Nucleus.  Rather,\nthese tests simply exercise that the conversion *runs* for each input/output\nfile type.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport unittest\n\nfrom absl import logging\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom nucleus.io import converter\nfrom nucleus.testing import test_utils\n\nbasename = os.path.basename\n\n# Initial (native) input files we will use to begin conversions.\nORIGINAL_TEST_FILES = [\n    ""test.bed"", ""test_reads.fastq"", ""test_features.gff"", ""test.sam"",\n    ""test_sites.vcf""\n]\n\n# These formats require a header, so conversion from tfrecord to a native file\n# format cannot be done faithfully.\nFORMATS_REQUIRING_HEADER = ["".bam"", "".gff"", "".sam"", "".vcf""]\n\n\nclass ConvertGenomicsFileTest(parameterized.TestCase):\n\n  def _convert(self, src, dest):\n    logging.info(""#### Converting: %s --> %s ... "", basename(src),\n                 basename(dest))\n    converter.convert(src, dest)\n\n  @parameterized.parameters(*ORIGINAL_TEST_FILES)\n  def test_conversion_to_tfrecord_and_back(self, original_input_file):\n    """"""Test conversion from a native file format to tfrecord.gz, then back.""""""\n    input_path = test_utils.genomics_core_testdata(original_input_file)\n    tfrecord_output_path = test_utils.test_tmpfile(original_input_file +\n                                                   "".tfrecord.gz"")\n    native_output_path = test_utils.test_tmpfile(original_input_file)\n\n    # Test conversion from native format to tfrecord.\n    self._convert(input_path, tfrecord_output_path)\n\n    # TODO(b/63133103): remove this when SAM writer is implemented.\n    if native_output_path.endswith("".sam""):\n      raise unittest.SkipTest(""SAM writing not yet supported"")\n\n    # Test conversion from tfrecord format back to native format.  Ensure that\n    # conversions where we would need a header, but don\'t have one from the\n    # input, trigger an error message.\n    if any(\n        native_output_path.endswith(ext) for ext in FORMATS_REQUIRING_HEADER):\n      with self.assertRaisesRegexp(\n          converter.ConversionError,\n          ""Input file does not have a header, which is needed to construct ""\n          ""output file""):\n        self._convert(tfrecord_output_path, native_output_path)\n\n    else:\n      self._convert(tfrecord_output_path, native_output_path)\n\n\nif __name__ == ""__main__"":\n  absltest.main()\n \n'"
nucleus/io/fasta.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Classes for reading FASTA files.\n\nThe FASTA format is described at\nhttps://en.wikipedia.org/wiki/FASTA_format\n\nAPI for reading:\n\n```python\nfrom nucleus.io import fasta\nfrom nucleus.protos import range_pb2\n\nwith fasta.IndexedFastaReader(input_path) as reader:\n  region = range_pb2.Range(reference_name=\'chrM\', start=1, end=6)\n  basepair_string = reader.query(region)\n  print(basepair_string)\n```\n\nIf `input_path` ends with \'.gz\', it is assumed to be compressed.  All FASTA\nfiles are assumed to be indexed with the index file located at\n`input_path + \'.fai\'`.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport six\n\nfrom nucleus.io import gfile\nfrom nucleus.io import genomics_reader\nfrom nucleus.io.python import reference\nfrom nucleus.protos import fasta_pb2\nfrom nucleus.protos import reference_pb2\nfrom nucleus.util import ranges\n\n# TODO(thomaswc): Replace this with a real protocol buffer definition.\nRefFastaHeader = collections.namedtuple(\n    \'RefFastaHeader\', [\'contigs\'])\n\n\nclass FastaReader(genomics_reader.DispatchingGenomicsReader):\n  """"""Class for reading (name, bases) tuples from FASTA files.""""""\n\n  def _native_reader(self, input_path, **kwargs):\n    fai_path = input_path + \'.fai\'\n    if gfile.Exists(fai_path):\n      return IndexedFastaReader(input_path, **kwargs)\n    return UnindexedFastaReader(input_path, **kwargs)\n\n  def _record_proto(self):\n    return fasta_pb2.FastaRecord\n\n\nclass IndexedFastaReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from FASTA files containing a reference genome.""""""\n\n  def __init__(self, input_path, keep_true_case=False, cache_size=None):\n    """"""Initializes an IndexedFastaReader.\n\n    Args:\n      input_path: string. A path to a resource containing FASTA records.\n      keep_true_case: bool. If False, casts all bases to uppercase before\n        returning them.\n      cache_size: integer. Number of bases to cache from previous queries.\n        Defaults to 64K.  The cache can be disabled using cache_size=0.\n    """"""\n    super(IndexedFastaReader, self).__init__()\n\n    options = fasta_pb2.FastaReaderOptions(keep_true_case=keep_true_case)\n\n    fasta_path = input_path\n    fai_path = fasta_path + \'.fai\'\n    if cache_size is None:\n      # Use the C++-defined default cache size.\n      self._reader = reference.IndexedFastaReader.from_file(\n          fasta_path, fai_path, options)\n    else:\n      self._reader = reference.IndexedFastaReader.from_file(\n          fasta_path, fai_path, options, cache_size)\n\n    # TODO(thomaswc): Define a RefFastaHeader proto, and use it instead of this.\n    self.header = RefFastaHeader(contigs=self._reader.contigs)\n\n  def iterate(self):\n    """"""Returns an iterable of (name, bases) tuples contained in this file.""""""\n    return self._reader.iterate()\n\n  def query(self, region):\n    """"""Returns the base pairs (as a string) in the given region.""""""\n    return self._reader.bases(region)\n\n  def is_valid(self, region):\n    """"""Returns whether the region is contained in this FASTA file.""""""\n    return self._reader.is_valid_interval(region)\n\n  def contig(self, contig_name):\n    """"""Returns a ContigInfo proto for contig_name.""""""\n    return self._reader.contig(contig_name)\n\n  @property\n  def c_reader(self):\n    """"""Returns the underlying C++ reader.""""""\n    return self._reader\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass UnindexedFastaReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from unindexed FASTA files.""""""\n\n  def __init__(self, input_path):\n    """"""Initializes an UnindexedFastaReader.\n\n    Args:\n      input_path: string. A path to a resource containing FASTA records.\n    """"""\n    super(UnindexedFastaReader, self).__init__()\n\n    self._reader = reference.UnindexedFastaReader.from_file(input_path)\n\n  def iterate(self):\n    """"""Returns an iterable of (name, bases) tuples contained in this file.""""""\n    return self._reader.iterate()\n\n  def query(self, region):\n    """"""Returns the base pairs (as a string) in the given region.""""""\n    raise NotImplementedError(\'Can not query an unindexed FASTA file\')\n\n  def is_valid(self, region):\n    """"""Returns whether the region is contained in this FASTA file.""""""\n    return self._reader.is_valid_interval(region)\n\n  def contig(self, contig_name):\n    """"""Returns a ContigInfo proto for contig_name.""""""\n    raise NotImplementedError(\'Contigs unknown for an unindexed FASTA file\')\n\n  @property\n  def c_reader(self):\n    """"""Returns the underlying C++ reader.""""""\n    return self._reader\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass InMemoryFastaReader(genomics_reader.GenomicsReader):\n  """"""An `IndexedFastaReader` getting its bases from an in-memory data structure.\n\n  An `InMemoryFastaReader` provides the same API as `IndexedFastaReader` but\n  doesn\'t fetch its data from an on-disk FASTA file but rather fetches the bases\n  from an in-memory cache containing (chromosome, start, bases) tuples.\n\n  In particular, the `query(Range(chrom, start, end))` operation fetches bases\n  from the tuple where `chrom` == chromosome, and then from the bases where the\n  first base of bases starts at start. If start > 0, then the bases string is\n  assumed to contain bases starting from that position in the region. For\n  example, the record (\'1\', 10, \'ACGT\') implies that\n  `query(ranges.make_range(\'1\', 11, 12))` will return the base \'C\', as the \'A\'\n  base is at position 10. This makes it straightforward to cache a small region\n  of a full chromosome without having to store the entire chromosome sequence in\n  memory (potentially big!).\n  """"""\n\n  def __init__(self, chromosomes):\n    """"""Initializes an InMemoryFastaReader using data from chromosomes.\n\n    Args:\n      chromosomes: list[tuple]. The chromosomes we are caching in memory as a\n        list of tuples. Each tuple must be exactly three elements in length,\n        containing (chromosome name [str], start [int], bases [str]).\n\n    Raises:\n      ValueError: If any of the chromosomes tuples are invalid.\n    """"""\n    super(InMemoryFastaReader, self).__init__()\n\n    ref_seqs = []\n    contigs = []\n    for i, (contig_name, start, bases) in enumerate(chromosomes):\n      if start < 0:\n        raise ValueError(\'start={} must be >= for chromosome={}\'.format(\n            start, contig_name))\n      if not bases:\n        raise ValueError(\n            \'Bases must contain at least one base, but got ""{}""\'.format(bases))\n\n      end = start + len(bases)\n      ref_seqs.append(reference_pb2.ReferenceSequence(\n          region=ranges.make_range(contig_name, start, end), bases=bases))\n      contigs.append(\n          reference_pb2.ContigInfo(\n              name=contig_name, n_bases=end, pos_in_fasta=i))\n\n    self._reader = reference.InMemoryFastaReader.create(contigs, ref_seqs)\n    self.header = RefFastaHeader(contigs=self._reader.contigs)\n\n  def iterate(self):\n    """"""Returns an iterable of (name, bases) tuples contained in this file.""""""\n    return self._reader.iterate()\n\n  def query(self, region):\n    """"""Returns the base pairs (as a string) in the given region.""""""\n    return self._reader.bases(region)\n\n  def is_valid(self, region):\n    """"""Returns whether the region is contained in this FASTA file.""""""\n    return self._reader.is_valid_interval(region)\n\n  def contig(self, contig_name):\n    """"""Returns a ContigInfo proto for contig_name.""""""\n    return self._reader.contig(contig_name)\n\n  @property\n  def c_reader(self):\n    """"""Returns the underlying C++ reader.""""""\n    return self._reader\n\n  def __str__(self):\n\n    def _format_refseq(refseq):\n      bases = refseq.bases\n      if len(bases) >= 50:\n        bases = bases[0:50] + \'...\'\n      return \'Contig(chrom={} start={}, end={}, bases={})\'.format(\n          refseq.region.reference_name, refseq.region.start, refseq.region.end,\n          bases)\n\n    contigs_strs = [\n        _format_refseq(refseq)\n        for refseq in six.itervalues(self._reader.reference_sequences)\n    ]\n    return \'InMemoryFastaReader(contigs={})\'.format(\'\'.join(contigs_strs))\n\n  __repr__ = __str__\n'"
nucleus/io/fasta_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for nucleus.io.fasta.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nimport six\nfrom nucleus.io import fasta\nfrom nucleus.io.python import reference\nfrom nucleus.testing import test_utils\nfrom nucleus.util import ranges\n\n\nclass FastaReaderTests(parameterized.TestCase):\n\n  def test_dispatching_reader(self):\n    with fasta.FastaReader(\n        test_utils.genomics_core_testdata(\'test.fasta\')) as reader:\n      # The reader is an instance of IndexedFastaReader which supports query().\n      self.assertEqual(reader.query(ranges.make_range(\'chrM\', 1, 6)), \'ATCAC\')\n    with fasta.FastaReader(\n        test_utils.genomics_core_testdata(\'unindexed.fasta\')) as reader:\n      # The reader is an instance of UnindexedFastaReader which doesn\'t support\n      # query().\n      with self.assertRaises(NotImplementedError):\n        reader.query(ranges.make_range(\'chrM\', 1, 5))\n\n\nclass IndexedFastaReaderTests(parameterized.TestCase):\n\n  @parameterized.parameters(\'test.fasta\', \'test.fasta.gz\')\n  def test_make_ref_reader_default(self, fasta_filename):\n    fasta_path = test_utils.genomics_core_testdata(fasta_filename)\n    with fasta.IndexedFastaReader(fasta_path) as reader:\n      self.assertEqual(reader.query(ranges.make_range(\'chrM\', 1, 6)), \'ATCAC\')\n\n  @parameterized.parameters(\'test.fasta\', \'test.fasta.gz\')\n  def test_make_ref_reader_with_true_case(self, fasta_filename):\n    fasta_path = test_utils.genomics_core_testdata(fasta_filename)\n    with fasta.IndexedFastaReader(fasta_path, keep_true_case=True) as reader:\n      self.assertEqual(reader.query(ranges.make_range(\'chrM\', 22, 27)), \'TaaCC\')\n\n  @parameterized.parameters(\'test.fasta\', \'test.fasta.gz\')\n  def test_make_ref_reader_cache_specified(self, fasta_filename):\n    fasta_path = test_utils.genomics_core_testdata(fasta_filename)\n    with fasta.IndexedFastaReader(fasta_path, cache_size=10) as reader:\n      self.assertEqual(reader.query(ranges.make_range(\'chrM\', 1, 5)), \'ATCA\')\n\n  def test_c_reader(self):\n    with fasta.IndexedFastaReader(\n        test_utils.genomics_core_testdata(\'test.fasta\')) as reader:\n      self.assertIsInstance(reader.c_reader,\n                            reference.IndexedFastaReader)\n\n\nclass UnindexedFastaReaderTests(parameterized.TestCase):\n\n  def test_query(self):\n    unindexed_fasta_reader = fasta.UnindexedFastaReader(\n        test_utils.genomics_core_testdata(\'unindexed.fasta\'))\n    with self.assertRaises(NotImplementedError):\n      unindexed_fasta_reader.query(ranges.make_range(\'chrM\', 1, 5))\n\n  @parameterized.parameters(\'test.fasta\', \'test.fasta.gz\')\n  def test_iterate(self, fasta_filename):\n    # Check the indexed fasta file\'s iterable matches that of the unindexed\n    # fasta file.\n    indexed_fasta_reader = fasta.IndexedFastaReader(\n        test_utils.genomics_core_testdata(fasta_filename))\n    unindexed_fasta_reader = fasta.UnindexedFastaReader(\n        test_utils.genomics_core_testdata(fasta_filename))\n    self.assertEqual(\n        list(indexed_fasta_reader.iterate()),\n        list(unindexed_fasta_reader.iterate()))\n\n\nclass InMemoryFastaReaderTests(parameterized.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    cls.fasta_reader = fasta.IndexedFastaReader(\n        test_utils.genomics_core_testdata(\'test.fasta\'))\n\n    cls.in_mem = fasta.InMemoryFastaReader(\n        [(contig.name, 0,\n          cls.fasta_reader.query(\n              ranges.make_range(contig.name, 0, contig.n_bases)))\n         for contig in cls.fasta_reader.header.contigs])\n\n  def test_non_zero_start_query(self):\n    bases = \'ACGTAACCGGTT\'\n    for start in range(len(bases)):\n      reader = fasta.InMemoryFastaReader([(\'1\', start, bases[start:])])\n      self.assertEqual(reader.header.contigs[0].name, \'1\')\n      self.assertEqual(reader.header.contigs[0].n_bases, len(bases))\n\n      # Check that our query operation works as expected with a start position.\n      for end in range(start, len(bases)):\n        self.assertEqual(reader.query(ranges.make_range(\'1\', start, end)),\n                         bases[start:end])\n\n  @parameterized.parameters(\n      # Start is 10, so this raises because it\'s before the bases starts.\n      dict(start=0, end=1),\n      # Spans into the start of the bases; make sure it detects it\'s bad.\n      dict(start=8, end=12),\n      # Spans off the end of the bases.\n      dict(start=12, end=15),\n  )\n  def test_bad_query_with_start(self, start, end):\n    reader = fasta.InMemoryFastaReader([(\'1\', 10, \'ACGT\')])\n    with self.assertRaises(ValueError):\n      reader.query(ranges.make_range(\'1\', start, end))\n\n  def test_query_edge_cases(self):\n    reader = fasta.InMemoryFastaReader([(\'1\', 0, \'ACGT\')])\n    # Check that we can query the first base correctly.\n    self.assertEqual(reader.query(ranges.make_range(\'1\', 0, 1)), \'A\')\n    # Check that we can query the last base correctly.\n    self.assertEqual(reader.query(ranges.make_range(\'1\', 3, 4)), \'T\')\n    # Check that we can query the entire sequence correctly.\n    self.assertEqual(reader.query(ranges.make_range(\'1\', 0, 4)), \'ACGT\')\n\n  def test_contigs(self):\n    # Our contigs can have a different order, descriptions are dropped, etc so\n    # we need to check specific fields by hand.\n    fasta_contigs = {\n        contig.name: contig for contig in self.fasta_reader.header.contigs\n    }\n    mem_contigs = {contig.name: contig for contig in self.in_mem.header.contigs}\n\n    self.assertEqual(fasta_contigs.keys(), mem_contigs.keys())\n    for name, fasta_contig in fasta_contigs.items():\n      self.assertContigsAreEqual(mem_contigs[name], fasta_contig)\n\n  def assertContigsAreEqual(self, actual, expected):\n    self.assertEqual(actual.name, expected.name)\n    self.assertEqual(actual.n_bases, expected.n_bases)\n    self.assertEqual(actual.pos_in_fasta, expected.pos_in_fasta)\n\n  def test_iterate(self):\n    # Check the in-memory fasta file\'s iterable matches the info in the header.\n    expected_names = [\n        contig.name for contig in self.fasta_reader.header.contigs]\n    expected_lengths = [\n        contig.n_bases for contig in self.fasta_reader.header.contigs]\n    in_mem_records = list(self.in_mem.iterate())\n    self.assertLen(in_mem_records, len(self.fasta_reader.header.contigs))\n    self.assertEqual([r[0] for r in in_mem_records], expected_names)\n    self.assertEqual([len(r[1]) for r in in_mem_records], expected_lengths)\n\n    # Check the in-memory fasta file\'s iterable matches that of the indexed\n    # fasta file.\n    fasta_records = list(self.fasta_reader.iterate())\n    self.assertEqual(in_mem_records, fasta_records)\n\n  @parameterized.parameters(\n      dict(region=ranges.make_range(\'chr1\', 0, 10), expected=True),\n      dict(region=ranges.make_range(\'chr1\', 10, 50), expected=True),\n      dict(region=ranges.make_range(\'chr1\', 10, 500), expected=False),\n      dict(region=ranges.make_range(\'chr3\', 10, 20), expected=False),\n  )\n  def test_is_valid(self, region, expected):\n    self.assertEqual(self.in_mem.is_valid(region), expected)\n    self.assertEqual(self.fasta_reader.is_valid(region), expected)\n\n  def test_known_contig(self):\n    for contig in self.fasta_reader.header.contigs:\n      self.assertContigsAreEqual(self.in_mem.contig(contig.name), contig)\n\n  def test_str_and_repr(self):\n    self.assertIsInstance(str(self.in_mem), six.string_types)\n    self.assertIsInstance(repr(self.in_mem), six.string_types)\n\n  def test_unknown_contig(self):\n    for reader in [self.fasta_reader, self.in_mem]:\n      with self.assertRaises(ValueError):\n        reader.contig(\'unknown\')\n\n  def test_good_query(self):\n    for contig in self.fasta_reader.header.contigs:\n      for start in range(contig.n_bases):\n        for end in range(start, contig.n_bases):\n          region = ranges.make_range(contig.name, start, end)\n          self.assertEqual(\n              self.in_mem.query(region), self.fasta_reader.query(region))\n\n  @parameterized.parameters(\n      ranges.make_range(\'chr1\', -1, 10),     # bad start.\n      ranges.make_range(\'chr1\', 10, 1),      # end < start.\n      ranges.make_range(\'chr1\', 0, 1000),    # off end of chromosome.\n      ranges.make_range(\'unknown\', 0, 10),   # unknown chromosome.\n  )\n  def test_bad_query(self, region):\n    for reader in [self.fasta_reader, self.in_mem]:\n      with self.assertRaises(ValueError):\n        reader.query(region)\n\n  def test_bad_create_args(self):\n    with self.assertRaisesRegexp(ValueError, \'multiple ones were found on 1\'):\n      fasta.InMemoryFastaReader([\n          (\'1\', 10, \'AC\'),\n          (\'1\', 20, \'AC\'),\n      ])\n\n  def test_c_reader(self):\n    self.assertIsInstance(self.in_mem.c_reader,\n                          reference.InMemoryFastaReader)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/fastq.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Classes for reading and writing FASTQ files.\n\nThe FASTQ format is described at\nhttps://en.wikipedia.org/wiki/FASTQ_format\n\nAPI for reading:\n\n```python\nfrom nucleus.io import fastq\n\nwith fastq.FastqReader(input_path) as reader:\n  for record in reader:\n    print(record)\n```\n\nwhere `record` is a `nucleus.genomics.v1.FastqRecord` protocol buffer.\n\nAPI for writing:\n\n```python\nfrom nucleus.io import fastq\n\n# records is an iterable of nucleus.genomics.v1.FastqRecord protocol buffers.\nrecords = ...\n\nwith fastq.FastqWriter(output_path) as writer:\n  for record in records:\n    writer.write(record)\n```\n\nFor both reading and writing, if the path provided to the constructor contains\n\'.tfrecord\' as an extension, a `TFRecord` file is assumed and attempted to be\nread or written. Otherwise, the filename is treated as a true FASTQ file.\n\nFiles that end in a \'.gz\' suffix cause the file to be treated as compressed\n(with BGZF if it is a true FASTQ file, and with gzip if it is a TFRecord file).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom nucleus.io import genomics_reader\nfrom nucleus.io import genomics_writer\nfrom nucleus.io.python import fastq_reader\nfrom nucleus.io.python import fastq_writer\nfrom nucleus.protos import fastq_pb2\n\n\nclass NativeFastqReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from native FASTQ files.\n\n  Most users will want to use FastqReader instead, because it dynamically\n  dispatches between reading native FASTQ files and TFRecord files based on the\n  filename\'s extension.\n  """"""\n\n  def __init__(self, input_path):\n    """"""Initializes a NativeFastqReader.\n\n    Args:\n      input_path: str. A path to a resource containing FASTQ records.\n    """"""\n    super(NativeFastqReader, self).__init__()\n\n    fastq_path = input_path.encode(\'utf8\')\n    options = fastq_pb2.FastqReaderOptions()\n    self._reader = fastq_reader.FastqReader.from_file(fastq_path, options)\n    self.header = None\n\n  def query(self, region):\n    """"""Returns an iterator for going through the records in the region.\n\n    NOTE: This function is not implemented by NativeFastqReader as there is no\n    concept of genome ordering in the FASTQ format.\n    """"""\n    raise NotImplementedError(\'Can not query a FASTQ file\')\n\n  def iterate(self):\n    """"""Returns an iterable of FastqRecord protos in the file.""""""\n    return self._reader.iterate()\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass FastqReader(genomics_reader.DispatchingGenomicsReader):\n  """"""Class for reading FastqRecord protos from FASTQ or TFRecord files.""""""\n\n  def _native_reader(self, input_path, **kwargs):\n    return NativeFastqReader(input_path, **kwargs)\n\n  def _record_proto(self):\n    return fastq_pb2.FastqRecord\n\n\nclass NativeFastqWriter(genomics_writer.GenomicsWriter):\n  """"""Class for writing to native FASTQ files.\n\n  Most users will want FastqWriter, which will write to either native FASTQ\n  files or TFRecord files, based on the output filename\'s extension.\n  """"""\n\n  def __init__(self, output_path, **kwargs):\n    """"""Initializer for NativeFastqWriter.\n\n    Args:\n      output_path: str. The path to which to write the FASTQ file.\n      **kwargs: optional arguments; presently ignored.\n    """"""\n    super(NativeFastqWriter, self).__init__()\n\n    writer_options = fastq_pb2.FastqWriterOptions()\n    self._writer = fastq_writer.FastqWriter.to_file(output_path, writer_options)\n\n  def write(self, proto):\n    self._writer.write(proto)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass FastqWriter(genomics_writer.DispatchingGenomicsWriter):\n  """"""Class for writing FastqRecord protos to FASTQ or TFRecord files.""""""\n\n  def _native_writer(self, output_path, **kwargs):\n    return NativeFastqWriter(output_path, **kwargs)\n'"
nucleus/io/fastq_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.io.fastq.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import fastq\nfrom nucleus.protos import fastq_pb2\nfrom nucleus.testing import test_utils\n\n\nclass FastqReaderTests(parameterized.TestCase):\n\n  @parameterized.parameters(\n      \'test_reads.fastq\', \'test_reads.fastq.gz\', \'test_reads.bgzip.fastq.gz\',\n      \'test_reads.fastq.tfrecord\', \'test_reads.fastq.tfrecord.gz\')\n  def test_iterate_fastq_reader(self, fastq_filename):\n    fastq_path = test_utils.genomics_core_testdata(fastq_filename)\n    expected_ids = [\n        \'NODESC:header\', \'M01321:49:000000000-A6HWP:1:1101:17009:2216\', \'FASTQ\',\n        \'FASTQ_with_trailing_space\'\n    ]\n    with fastq.FastqReader(fastq_path) as reader:\n      records = list(reader.iterate())\n    self.assertLen(records, 4)\n    self.assertEqual([r.id for r in records], expected_ids)\n\n\nclass FastqWriterTests(parameterized.TestCase):\n  """"""Tests for FastqWriter.""""""\n\n  def setUp(self):\n    self.records = [\n        fastq_pb2.FastqRecord(id=\'id1\', sequence=\'ACGTG\', quality=\'ABCDE\'),\n        fastq_pb2.FastqRecord(id=\'id2\', sequence=\'ATTT\', quality=\'ABC@\'),\n        fastq_pb2.FastqRecord(\n            id=\'ID3\',\n            description=\'multi desc\',\n            sequence=\'GATAC\',\n            quality=\'ABC@!\'),\n    ]\n\n  @parameterized.parameters(\'test_raw.fastq\', \'test_zipped.fastq.gz\',\n                            \'test_raw.tfrecord\', \'test_zipped.tfrecord.gz\')\n  def test_roundtrip_writer(self, filename):\n    output_path = test_utils.test_tmpfile(filename)\n    with fastq.FastqWriter(output_path) as writer:\n      for record in self.records:\n        writer.write(record)\n\n    with fastq.FastqReader(output_path) as reader:\n      v2_records = list(reader.iterate())\n\n    self.assertEqual(self.records, v2_records)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/genomics_io_noplugin_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for genomics_io\'s plugin system.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\n\nfrom nucleus.io import sam\n\n\nclass PluginAbsenceTest(absltest.TestCase):\n  """"""Test that we get the right error when the plugin cannot load.""""""\n\n  def test_tfbam_plugin_does_not_load(self):\n    with self.assertRaisesRegexp(\n        ImportError, \'tfbam_lib module not found, cannot read .tfbam files.\'):\n      _ = sam.SamReader(\'mouse@25.tfbam\')\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/genomics_reader.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Classes that provide the interface for reading genomics data.\n\n`GenomicsReader` defines the core API supported by readers, and is subclassed\ndirectly or indirectly (via `DispatchingGenomicsReader`) for all concrete\nimplementations.\n\n`TFRecordReader` is an implementation of the `GenomicsReader` API for reading\n`TFRecord` files. This is usable for all data types when encoding data in\nprotocol buffers.\n\n`DispatchingGenomicsReader` is an abstract class defined for convenience on top\nof `GenomicsReader` that supports reading from either the native file format or\nfrom `TFRecord` files of the corresponding protocol buffer used to encode data\nof that file type. The input format assumed is dependent upon the filename of\nthe input data.\n\nConcrete implementations for individual file types (e.g. BED, SAM, VCF, etc.)\nreside in type-specific modules in this package. The instantiation of readers\nmay have reader-specific requirements documented there. General examples of the\n`iterate()` and `query()` functionality are shown below.\n\n```python\n# Equivalent ways to iterate through all elements in a reader.\n# 1. Using the reader itself as an iterable object.\nkwargs = ...  # Reader-specific keyword arguments.\nwith GenomicsReaderSubClass(output_path, **kwargs) as reader:\n  for proto in reader:\n    do_something(reader.header, proto)\n\n# 2. Calling the iterate() method of the reader explicitly.\nwith GenomicsReaderSubClass(output_path, **kwargs) as reader:\n  for proto in reader.iterate():\n    do_something(reader.header, proto)\n\n# Querying for all elements within a specific region of the genome.\nfrom nucleus.protos import range_pb2\nregion = range_pb2.Range(reference_name=\'chr1\', start=10, end=20)\n\nwith GenomicsReaderSubClass(output_path, **kwargs) as reader:\n  for proto in reader.query(region):\n    do_something(reader.header, proto)\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport errno\n\nfrom absl import logging\nimport six\n\nfrom nucleus.io.python import tfrecord_reader\n\n\nclass GenomicsReader(six.Iterator):\n  """"""Abstract base class for reading genomics data.\n\n  In addition to the abstractmethods defined below, subclasses should\n  also set a `header` member variable in their objects.\n  """"""\n\n  __metaclass__ = abc.ABCMeta\n\n  @abc.abstractmethod\n  def iterate(self):\n    """"""Returns an iterator for going through all the file\'s records.""""""\n\n  @abc.abstractmethod\n  def query(self, region):\n    """"""Returns an iterator for going through the records in the region.\n\n    Args:\n      region:  A nucleus.genomics.v1.Range.\n\n    Returns:\n      An iterator containing all and only records within the specified region.\n    """"""\n\n  def __enter__(self):\n    """"""Enter a `with` block.""""""\n    return self\n\n  def __exit__(self, unused_type, unused_value, unused_traceback):\n    """"""Exit a `with` block.  Typically, this will close the file.""""""\n\n  def __init__(self):\n    """"""Initializer.""""""\n    # Some readers can only support one iterator at a time, so don\'t\n    # create one now.  Rather, create it when needed in next().\n    self.iterator = None\n\n  def __iter__(self):\n    """"""Allows users to use the object itself as an iterator.""""""\n    return self.iterate()\n\n  def __next__(self):\n    """"""Allows users to use the object itself as an iterator.""""""\n    if self.iterator is None:\n      self.iterator = self.iterate()\n    return six.next(self.iterator)\n\n\nclass TFRecordReader(GenomicsReader):\n  """"""A GenomicsReader that reads protocol buffers from a TFRecord file.\n\n  Example usage:\n    reader = TFRecordReader(\'/tmp/my_file.tfrecords.gz\',\n                            proto=tensorflow.Example)\n    for example in reader:\n      process(example)\n\n  Note that TFRecord files do not have headers, and do not need\n  to be wrapped in a ""with"" block.\n  """"""\n\n  def __init__(self, input_path, proto, compression_type=None):\n    """"""Initializes the TFRecordReader.\n\n    Args:\n      input_path:  The filename of the file to read.\n      proto:  The protocol buffer type the TFRecord file is expected to\n        contain.  For example, variants_pb2.Variant or reads_pb2.Read.\n      compression_type:  Either \'ZLIB\', \'GZIP\', \'\' (uncompressed), or\n        None.  If None, __init__ will guess the compression type based on\n        the input_path\'s suffix.\n\n    Raises:\n      IOError: if there was any problem opening input_path for reading.\n    """"""\n    super(TFRecordReader, self).__init__()\n\n    self.input_path = input_path\n    self.proto = proto\n    self.header = None\n\n    if compression_type is None:\n      compression_type = \'GZIP\' if input_path.endswith(\'.gz\') else \'\'\n\n    self.reader = tfrecord_reader.TFRecordReader.from_file(\n        input_path, compression_type)\n    if self.reader is None:\n      raise IOError(errno.EIO,\n                    \'Error trying to open %s for reading\' % input_path)\n\n  def iterate(self):\n    """"""Returns an iterator for going through all the file\'s records.""""""\n    while self.reader.get_next():\n      yield self.proto.FromString(self.reader.get_record())\n\n  def query(self, region):\n    """"""Returns an iterator for going through the records in the region.\n\n    NOTE: This function is not currently implemented by TFRecordReader as the\n    TFRecord format does not provide a general mechanism for fast random access\n    to elements in genome order.\n    """"""\n    raise NotImplementedError(\'Can not query TFRecord file\')\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self.reader.close()\n\n  @property\n  def c_reader(self):\n    """"""Returns the underlying C++ reader.""""""\n    return self.reader\n\n\nclass DispatchingGenomicsReader(GenomicsReader):\n  """"""A GenomicsReader that dispatches based on the file extension.\n\n  If \'.tfrecord\' is present in the filename, a TFRecordReader is used.\n  Otherwise, a native reader is.\n\n  Subclasses of DispatchingGenomicsReader must define the following methods:\n    * _native_reader()\n    * _record_proto()\n  """"""\n\n  def __init__(self, input_path, **kwargs):\n    super(DispatchingGenomicsReader, self).__init__()\n\n    if \'.tfrecord\' in input_path:\n      self._reader = TFRecordReader(\n          input_path, proto=self._record_proto(),\n          compression_type=kwargs.get(\'compression_type\', None))\n    else:\n      # Remove compression_type, if present, from the arguments we pass to the\n      # native reader.\n      kwargs.pop(\'compression_type\', None)\n      self._reader = self._native_reader(input_path, **kwargs)\n    logging.info(\'Reading %s with %s\',\n                 input_path, self._reader.__class__.__name__)\n    self.header = getattr(self._reader, \'header\', None)\n    self._post_init_hook()\n\n  @abc.abstractmethod\n  def _native_reader(self, input_path, **kwargs):\n    """"""Returns a GenomicsReader for reading the records `natively`.\n\n    Args:\n      input_path: The path to the native file to read.\n      **kwargs:  Zero or more keyword arguments.\n\n    Returns:\n      A GenomicsReader.\n    """"""\n\n  @abc.abstractmethod\n  def _record_proto(self):\n    """"""Returns the protocol buffer type used by this reader.""""""\n\n  def iterate(self):\n    return self._reader.iterate()\n\n  def query(self, region):\n    return self._reader.query(region)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n  def _post_init_hook(self):\n    """"""Hook for subclasses to run code at the end of __init__.""""""\n'"
nucleus/io/genomics_reader_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.io.vcf.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nimport six\n\nfrom nucleus.io import genomics_reader\nfrom nucleus.protos import gff_pb2\nfrom nucleus.testing import test_utils\n\n\nclass DummyReader(genomics_reader.GenomicsReader):\n  """"""A GenomicsReader that produces consecutive integers.""""""\n\n  def __init__(self, input_path):\n    self.limit = int(input_path)\n    super(DummyReader, self).__init__()\n\n  def iterate(self):\n    for i in range(self.limit):\n      yield i\n\n  def query(self, region):\n    raise NotImplementedError(\'Can not query DummyReader\')\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    pass\n\n\nclass GenomicsReaderTests(absltest.TestCase):\n  """"""Tests for GenomicsReader.""""""\n\n  def testIteration(self):\n    dreader = DummyReader(\'10\')\n    self.assertEqual(list(range(10)), list(dreader))\n\n  def testTwoIteratorsAtTheSameTime(self):\n    dreader = DummyReader(\'100\')\n    iter2 = iter(dreader)\n    for i in range(100):\n      self.assertEqual(i, six.next(dreader))\n      self.assertEqual(i, six.next(iter2))\n\n\nclass TFRecordReaderTests(absltest.TestCase):\n  """"""Tests for TFRecordReader.""""""\n\n  def testUncompressed(self):\n    reader = genomics_reader.TFRecordReader(\n        test_utils.genomics_core_testdata(\'test_features.gff.tfrecord\'),\n        gff_pb2.GffRecord(),\n    )\n    records = list(reader.iterate())\n    self.assertEqual(\'GenBank\', records[0].source)\n    self.assertEqual(\'ctg123\', records[1].range.reference_name)\n    self.assertNotEqual(reader.c_reader, 0)\n\n  def testUncompressedExplicit(self):\n    reader = genomics_reader.TFRecordReader(\n        test_utils.genomics_core_testdata(\'test_features.gff.tfrecord\'),\n        gff_pb2.GffRecord(),\n        compression_type=\'\'\n    )\n    records = list(reader.iterate())\n    self.assertEqual(\'GenBank\', records[0].source)\n    self.assertEqual(\'ctg123\', records[1].range.reference_name)\n\n  def testCompressed(self):\n    reader = genomics_reader.TFRecordReader(\n        test_utils.genomics_core_testdata(\'test_features.gff.tfrecord.gz\'),\n        gff_pb2.GffRecord(),\n    )\n    records = list(reader.iterate())\n    self.assertEqual(\'GenBank\', records[0].source)\n    self.assertEqual(\'ctg123\', records[1].range.reference_name)\n\n  def testCompressedExplicit(self):\n    reader = genomics_reader.TFRecordReader(\n        test_utils.genomics_core_testdata(\'test_features.gff.tfrecord.gz\'),\n        gff_pb2.GffRecord(),\n        compression_type=\'GZIP\'\n    )\n    records = list(reader.iterate())\n    self.assertEqual(\'GenBank\', records[0].source)\n    self.assertEqual(\'ctg123\', records[1].range.reference_name)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/genomics_writer.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Classes that provide the interface for writing genomics data.\n\n`GenomicsWriter` defines the core API supported by writers, and is subclassed\ndirectly or indirectly (via `DispatchingGenomicsWriter`) for all concrete\nimplementations.\n\n`TFRecordWriter` is an implementation of the `GenomicsWriter` API for reading\n`TFRecord` files. This is usable for all data types when writing data as\nserialized protocol buffers.\n\n`DispatchingGenomicsWriter` is an abstract class defined for convenience on top\nof `GenomicsWriter` that supports writing to either the native file format or to\n`TFRecord` files of the corresponding protocol buffer used to encode data of\nthat file type. The output format chosen is dependent upon the filename to which\nthe data are being written.\n\nConcrete implementations for individual file types (e.g. BED, SAM, VCF, etc.)\nreside in type-specific modules in this package. A general example of the write\nfunctionality is shown below.\n\n```python\n# options is a writer-specific set of options.\noptions = ...\n\n# records is an iterable of protocol buffers of the specific data type.\nrecords = ...\n\nwith GenomicsWriterSubClass(output_path, options) as writer:\n  for proto in records:\n    writer.write(proto)\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport errno\n\nfrom absl import logging\n\nfrom nucleus.io.python import tfrecord_writer\n\n\nclass GenomicsWriter(object):\n  """"""Abstract base class for writing genomics data.\n\n  A GenomicsWriter only has one method, write, which writes a single\n  protocol buffer to a file.\n  """"""\n\n  __metaclass__ = abc.ABCMeta\n\n  @abc.abstractmethod\n  def write(self, proto):\n    """"""Writes proto to the file.\n\n    Args:\n      proto:  A protocol buffer.\n    """"""\n\n  def __enter__(self):\n    """"""Enter a `with` block.""""""\n    return self\n\n  @abc.abstractmethod\n  def __exit__(self, unused_type, unused_value, unused_traceback):\n    """"""Exit a `with` block.  Typically, this will close the file.""""""\n\n\nclass TFRecordWriter(GenomicsWriter):\n  """"""A GenomicsWriter that writes to a TFRecord file.\n\n  Example usage:\n    writer = TFRecordWriter(\'/tmp/my_output.tfrecord.gz\')\n    for record in records:\n      writer.write(record)\n\n  Note that TFRecord files do not need to be wrapped in a ""with"" block.\n  """"""\n\n  def __init__(self, output_path, header=None, compression_type=None):\n    """"""Initializer.\n\n    Args:\n      output_path: str. The output path to which the records are written.\n      header: An optional header for the particular data type. This can be\n        useful for file types that have logical headers where some operations\n        depend on that header information (e.g. VCF using its headers to\n        determine type information of annotation fields).\n      compression_type:  Either \'ZLIB\', \'GZIP\', \'\' (uncompressed), or\n        None.  If None, __init__ will guess the compression type based on\n        the input_path\'s suffix.\n\n    Raises:\n      IOError:  if there was any problem opening output_path for writing.\n    """"""\n    super(TFRecordWriter, self).__init__()\n    self.header = header\n\n    if compression_type is None:\n      compression_type = \'GZIP\' if output_path.endswith(\'.gz\') else \'\'\n\n    self._writer = tfrecord_writer.TFRecordWriter.from_file(\n        output_path, compression_type)\n    if self._writer is None:\n      raise IOError(errno.EIO, \'Error opening %s for writing\' % output_path)\n\n  def write(self, proto):\n    """"""Writes the proto to the TFRecord file.""""""\n    self._writer.write(proto.SerializeToString())\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.close()\n\n\nclass DispatchingGenomicsWriter(GenomicsWriter):\n  """"""A GenomicsWriter that dispatches based on the file extension.\n\n  If \'.tfrecord\' is present in the filename, a TFRecordWriter is used.\n  Otherwise, a native writer is.\n\n  Sub-classes of DispatchingGenomicsWriter must define a _native_writer()\n  method.\n  """"""\n\n  def __init__(self, output_path, **kwargs):\n    """"""Initializer.\n\n    Args:\n      output_path: str. The output path to which the records are written.\n      **kwargs: k=v named args. Keyword arguments used to instantiate the native\n        writer, if applicable.\n    """"""\n    super(DispatchingGenomicsWriter, self).__init__()\n    self.header = kwargs.get(\'header\', None)\n\n    if \'.tfrecord\' in output_path:\n      self._writer = TFRecordWriter(output_path, header=self.header)\n    else:\n      self._writer = self._native_writer(output_path, **kwargs)\n    logging.info(\'Writing %s with %s\',\n                 output_path, self._writer.__class__.__name__)\n    self._post_init_hook()\n\n  @abc.abstractmethod\n  def _native_writer(self, output_path, **kwargs):\n    """"""Returns a GenomicsWriter for writing the records `natively`.\n\n    Args:\n      output_path: The path to write the records to.\n      **kwargs:  Zero or more keyword arguments.\n\n    Returns:\n      A GenomicsWriter.\n    """"""\n\n  def write(self, proto):\n    self._writer.write(proto)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.__exit__(exit_type, exit_value, exit_traceback)\n\n  def _post_init_hook(self):\n    """"""Hook for subclasses to run code at the end of __init__.""""""\n'"
nucleus/io/gff.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Classes for reading and writing GFF files.\n\nThe GFF format is described at\nhttps://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md.\n\nAPI for reading:\n\n```python\nfrom nucleus.io import gff\n\n# Iterate through all records.\nwith gff.GffReader(input_path) as reader:\n  for record in reader:\n    print(record)\n```\n\nwhere `record` is a `nucleus.genomics.v1.GffRecord` protocol buffer.\n\nAPI for writing:\n\n```python\nfrom nucleus.io import gff\nfrom nucleus.protos import gff_pb2\n\n# records is an iterable of nucleus.genomics.v1.GffRecord protocol buffers.\nrecords = ...\n\nheader = gff_pb2.GffHeader()\n\n# Write all records to the desired output path.\nwith gff.GffWriter(output_path, header) as writer:\n  for record in records:\n    writer.write(record)\n```\n\nFor both reading and writing, if the path provided to the constructor contains\n\'.tfrecord\' as an extension, a `TFRecord` file is assumed and attempted to be\nread or written. Otherwise, the filename is treated as a true GFF file.\n\nFiles that end in a \'.gz\' suffix cause the file to be treated as compressed\n(with BGZF if it is a true GFF file, and with gzip if it is a TFRecord file).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom nucleus.io import genomics_reader\nfrom nucleus.io import genomics_writer\nfrom nucleus.io.python import gff_reader\nfrom nucleus.io.python import gff_writer\nfrom nucleus.protos import gff_pb2\n\n\nclass NativeGffReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from native GFF files.\n\n  Most users will want to use GffReader instead, because it dynamically\n  dispatches between reading native GFF files and TFRecord files based on the\n  filename\'s extension.\n  """"""\n\n  def __init__(self, input_path):\n    """"""Initializes a NativeGffReader.\n\n    Args:\n      input_path: string. A path to a resource containing GFF records.\n    """"""\n    super(NativeGffReader, self).__init__()\n    gff_path = input_path.encode(\'utf8\')\n    reader_options = gff_pb2.GffReaderOptions()\n    self._reader = gff_reader.GffReader.from_file(gff_path, reader_options)\n    self.header = self._reader.header\n\n  def query(self):\n    """"""Returns an iterator for going through the records in the region.\n\n    NOTE: This function is not currently implemented by NativeGffReader though\n    it could be implemented for sorted, tabix-indexed GFF files.\n    """"""\n    raise NotImplementedError(\'Can not currently query a GFF file\')\n\n  def iterate(self):\n    """"""Returns an iterable of GffRecord protos in the file.""""""\n    return self._reader.iterate()\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass GffReader(genomics_reader.DispatchingGenomicsReader):\n  """"""Class for reading GffRecord protos from GFF or TFRecord files.""""""\n\n  def _native_reader(self, input_path, **kwargs):\n    return NativeGffReader(input_path, **kwargs)\n\n  def _record_proto(self):\n    return gff_pb2.GffRecord\n\n\nclass NativeGffWriter(genomics_writer.GenomicsWriter):\n  """"""Class for writing to native GFF files.\n\n  Most users will want GffWriter, which will write to either native GFF\n  files or TFRecord files, based on the output filename\'s extension.\n  """"""\n\n  def __init__(self, output_path, header):\n    """"""Initializer for NativeGffWriter.\n\n    Args:\n      output_path: str. The path to which to write the GFF file.\n      header: nucleus.genomics.v1.GffHeader. The header that defines all\n        information germane to the constituent GFF records.\n    """"""\n    super(NativeGffWriter, self).__init__()\n    writer_options = gff_pb2.GffWriterOptions()\n    self._writer = gff_writer.GffWriter.to_file(output_path, header,\n                                                writer_options)\n\n  def write(self, proto):\n    self._writer.write(proto)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass GffWriter(genomics_writer.DispatchingGenomicsWriter):\n  """"""Class for writing GffRecord protos to GFF or TFRecord files.""""""\n\n  def _native_writer(self, output_path, header):\n    return NativeGffWriter(output_path, header=header)\n'"
nucleus/io/gff_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.io.gff.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import gff\nfrom nucleus.io import tfrecord\nfrom nucleus.protos import gff_pb2\nfrom nucleus.testing import test_utils\nfrom nucleus.util import ranges\n\n# Names of testdata GFF files; we also reuse these basenames for output files\n# in the tmp directory.\nTEXT_GFF_FILES = (\'test_features.gff\', \'test_features.gff.gz\')\nTFRECORD_GFF_FILES = (\'test_features.gff.tfrecord\',\n                      \'test_features.gff.tfrecord.gz\')\nALL_GFF_FILES = TEXT_GFF_FILES + TFRECORD_GFF_FILES\n\nEXPECTED_GFF_VERSION = \'gff-version 3.2.1\'\n\n\nclass GffReaderTests(parameterized.TestCase):\n\n  @parameterized.parameters(*ALL_GFF_FILES)\n  def test_iterate_gff_reader(self, gff_filename):\n    gff_path = test_utils.genomics_core_testdata(gff_filename)\n    expected = [(\'ctg123\', 999, 9000), (\'ctg123\', 999, 1012)]\n\n    with gff.GffReader(gff_path) as reader:\n      records = list(reader.iterate())\n    self.assertLen(records, 2)\n    self.assertEqual(\n        [(r.range.reference_name, r.range.start, r.range.end) for r in records],\n        expected)\n\n  @parameterized.parameters(*TEXT_GFF_FILES)\n  def test_native_gff_header(self, gff_filename):\n    gff_path = test_utils.genomics_core_testdata(gff_filename)\n    with gff.GffReader(gff_path) as reader:\n      self.assertEqual(EXPECTED_GFF_VERSION, reader.header.gff_version)\n    with gff.NativeGffReader(gff_path) as native_reader:\n      self.assertEqual(EXPECTED_GFF_VERSION, native_reader.header.gff_version)\n\n\nclass GffWriterTests(parameterized.TestCase):\n  """"""Tests for GffWriter.""""""\n\n  def setUp(self):\n    tfrecord_file = test_utils.genomics_core_testdata(\n        \'test_features.gff.tfrecord\')\n    self.records = list(\n        tfrecord.read_tfrecords(tfrecord_file, proto=gff_pb2.GffRecord))\n    self.header = gff_pb2.GffHeader(\n        sequence_regions=[ranges.make_range(\'ctg123\', 0, 1497228)])\n\n  @parameterized.parameters(*ALL_GFF_FILES)\n  def test_roundtrip_writer(self, filename):\n    output_path = test_utils.test_tmpfile(filename)\n    with gff.GffWriter(output_path, header=self.header) as writer:\n      for record in self.records:\n        writer.write(record)\n\n    with gff.GffReader(output_path) as reader:\n      v2_records = list(reader.iterate())\n\n    self.assertEqual(self.records, v2_records)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/gfile.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A Python interface for files.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\n\nfrom nucleus.io.python import gfile\n\n\ndef Exists(filename):\n  return gfile.Exists(filename)\n\n\ndef Glob(pattern):\n  return gfile.Glob(pattern)\n\n\nclass ReadableFile(six.Iterator):\n  """"""Wraps gfile.ReadableFile to add iteration, enter/exit and readlines.""""""\n\n  def __init__(self, filename):\n    self._file = gfile.ReadableFile.New(filename)\n\n  def __enter__(self):\n    return self\n\n  def __exit__(self, type_, value, traceback):\n    self._file.__exit__()\n\n  def __iter__(self):\n    return self\n\n  def __next__(self):\n    ok, line = self._file.Readline()\n    if ok:\n      return line\n    else:\n      raise StopIteration\n\n  def readlines(self):\n    lines = []\n    while True:\n      ok, line = self._file.Readline()\n      if ok:\n        lines.append(line)\n      else:\n        break\n    return lines\n\n\ndef Open(filename, mode=""r""):\n  if mode.startswith(""r""):\n    return ReadableFile(filename)\n  elif mode.startswith(""w""):\n    return gfile.WritableFile.New(filename)\n  else:\n    raise ValueError(""Unsupported mode \'{}\' for Open"".format(mode))\n'"
nucleus/io/gfile_test.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for nucleus.io.gfile.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\n\nfrom nucleus.io import gfile\nfrom nucleus.testing import test_utils\n\n\nclass GfileTest(absltest.TestCase):\n\n  def test_exists(self):\n    self.assertTrue(gfile.Exists(\n        test_utils.genomics_core_testdata(\'test_regions.bedgraph\')))\n    self.assertFalse(gfile.Exists(\n        test_utils.genomics_core_testdata(\'does_not_exist\')))\n\n  def test_glob(self):\n    g1 = gfile.Glob(test_utils.genomics_core_testdata(\'test*\'))\n    self.assertGreater(len(g1), 1)\n    self.assertIn(\n        test_utils.genomics_core_testdata(\'test.bam\'), g1)\n    g2 = gfile.Glob(test_utils.genomics_core_testdata(\'does_not_exist*\'))\n    self.assertEqual([], g2)\n\n  def test_reading(self):\n    with gfile.Open(test_utils.genomics_core_testdata(\'headerless.sam\')) as f:\n      for line in f:\n        self.assertTrue(line.startswith(\'SRR3656745\'))\n\n  def test_writing(self):\n    path = test_utils.test_tmpfile(\'test_gfile\')\n    with gfile.Open(path, \'w\') as f:\n      f.write(\'test\\n\')\n      f.write(\'end\\n\')\n\n    with gfile.Open(path, \'r\') as f2:\n      lines = f2.readlines()\n\n    self.assertEqual([\'test\\n\', \'end\\n\'], lines)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/sam.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# pylint: disable=line-too-long\n""""""Classes for reading and writing SAM and BAM files.\n\nThe SAM/BAM/CRAM formats are described at\nhttps://samtools.github.io/hts-specs/SAMv1.pdf\nhttps://samtools.github.io/hts-specs/CRAMv3.pdf\n\nAPI for reading:\n\n```python\nfrom nucleus.io import sam\n\nwith sam.SamReader(input_path) as reader:\n  for read in reader:\n    print(read)\n```\n\nwhere `read` is a `nucleus.genomics.v1.Read` protocol buffer. input_path will\ndynamically decode the underlying records depending the file extension, with\n`.sam` for SAM files, `.bam` for BAM files, and `.cram` for CRAM files. It will\nalso search for an appropriate index file to use to enable calls to the\n`query()` method.\n\nAPI for writing SAM/BAM:\n\n```python\nfrom nucleus.io import sam\n\n# reads is an iterable of nucleus.genomics.v1.Read protocol buffers.\nreads = ...\n\nwith sam.SamWriter(output_path, header=header) as writer:\n  for read in reads:\n    writer.write(read)\n```\n\nAPI for writing CRAM:\n\n```python\n# ref_path is required for writing CRAM files. If embed_ref, the output CRAM\n# file will embed reference sequences.\nwith sam.SamWriter(output_path, header=header, ref_path=ref_path,\n                   embed_ref=embed_ref) as writer:\n  for read in reads:\n    writer.write(read)\n```\n\nFor both reading and writing, if the path provided to the constructor contains\n\'.tfrecord\' as an extension, a `TFRecord` file is assumed and attempted to be\nread or written. Otherwise, the filename is treated as a true SAM/BAM/CRAM file.\n\nFor `TFRecord` files, ending in a \'.gz\' suffix causes the file to be treated as\ncompressed with gzip.\n\nNotes on using CRAM with SamReader\n--------------------------------\n\nNucleus supports reading from CRAM files using the same API as for SAM/BAM:\n\n```python\nfrom nucleus.io import sam\n\nwith sam.SamReader(""/path/to/sample.cram"") as reader:\n  for read in reader:\n    print(read)\n```\n\nThere is one type of CRAM file, though, that has a slightly more complicated\nAPI. If the CRAM file uses read sequence compression with an external reference\nfile, and this reference file is no longer accessible in the location specified\nby the CRAM file\'s ""UR"" tag and cannot be found in the local genome cache, its\nlocation must be passed to SamReader via the ref_path parameter:\n\n```python\nfrom nucleus.io import sam\n\ncram_path = ""/path/to/sample.cram""\nref_path = ""/path/to/genome.fasta""\nwith sam.SamReader(cram_path, ref_path=ref_path) as reader:\n  for read in reader:\n    print(read)\n```\n\nUnfortunately, htslib is unable to load the ref_path from anything other than a\nPOSIX filesystem. (htslib plugin filesystems like S3 or GCS buckets won\'t work).\nFor that reason, we don\'t recommend the use of CRAM files with external\nreference files, but instead suggest using read sequence compression with\nembedded reference data. (This has a minor impact on file size, but\nsignificantly improves file access simplicity and safety.)\n\nFor more information about CRAM, see:\n* The `samtools` documentation at http://www.htslib.org/doc/samtools.html\n* The ""Global Options"" section of the samtools docs at http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS\n* How reference sequences are encoded in CRAM at http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES\n* Finally, benchmarking of different CRAM options http://www.htslib.org/benchmarks/CRAM.html\n""""""\n# pylint: enable=line-too-long\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom nucleus.io import genomics_reader\nfrom nucleus.io import genomics_writer\nfrom nucleus.io.python import sam_reader\nfrom nucleus.io.python import sam_writer\nfrom nucleus.protos import reads_pb2\nfrom nucleus.util import ranges\nfrom nucleus.util import utils\n\n\nclass NativeSamReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from native SAM/BAM/CRAM files.\n\n  Most users will want to use SamReader instead, because it dynamically\n  dispatches between reading native SAM/BAM/CRAM files and TFRecord files based\n  on the filename\'s extensions.\n  """"""\n\n  def __init__(self,\n               input_path,\n               ref_path=None,\n               read_requirements=None,\n               parse_aux_fields=False,\n               hts_block_size=None,\n               downsample_fraction=None,\n               random_seed=None,\n               use_original_base_quality_scores=False):\n    """"""Initializes a NativeSamReader.\n\n    Args:\n      input_path: str. A path to a resource containing SAM/BAM/CRAM records.\n        Currently supports SAM text format, BAM binary format, and CRAM.\n      ref_path: optional str or None. Only used for CRAM decoding, and only\n        necessary if the UR encoded path in the CRAM itself needs to be\n        overridden. If provided, we will tell the CRAM decoder to use this FASTA\n        for the reference sequence.\n      read_requirements: optional ReadRequirement proto. If not None, this proto\n        is used to control which reads are filtered out by the reader before\n        they are passed to the client.\n      parse_aux_fields: optional bool, defaulting to False. If False, we do not\n        parse the auxiliary fields of the SAM/BAM/CRAM records (see SAM spec for\n        details). Parsing the aux fields is unnecessary for many applications,\n        and adds a significant parsing cost to access. If you need these aux\n        fields, set parse_aux_fields to True and these fields will be parsed and\n        populate the appropriate Read proto fields (e.g., read.info).\n      hts_block_size: int or None. If specified, this configures the block size\n        of the underlying htslib file object. Larger values (e.g. 1M) may be\n        beneficial for reading remote files. If None, the reader uses the\n        default htslib block size.\n      downsample_fraction: float in the interval [0.0, 1.0] or None. If\n        specified as a positive float, the reader will only keep each read with\n        probability downsample_fraction, randomly. If None or zero, all reads\n        are kept.\n      random_seed: None or int. The random seed to use with this sam reader, if\n        needed. If None, a fixed random value will be assigned.\n      use_original_base_quality_scores: optional bool, defaulting to False. If\n        True, quality scores are read from OQ tag.\n\n    Raises:\n      ValueError: If downsample_fraction is not None and not in the interval\n        (0.0, 1.0].\n      ImportError: If someone tries to load a tfbam file.\n    """"""\n    if input_path.endswith(\'.tfbam\'):\n      # Delayed loading of tfbam_lib.\n      try:\n        from tfbam_lib import tfbam_reader  # pylint: disable=g-import-not-at-top\n        self._reader = tfbam_reader.make_sam_reader(\n            input_path,\n            read_requirements=read_requirements,\n            unused_block_size=hts_block_size,\n            downsample_fraction=downsample_fraction,\n            random_seed=random_seed)\n      except ImportError:\n        raise ImportError(\n            \'tfbam_lib module not found, cannot read .tfbam files.\')\n    else:\n      aux_field_handling = reads_pb2.SamReaderOptions.SKIP_AUX_FIELDS\n      if parse_aux_fields:\n        aux_field_handling = reads_pb2.SamReaderOptions.PARSE_ALL_AUX_FIELDS\n\n      # We make 0 be a valid value that means ""keep all reads"" so that proto\n      # defaults (=0) do not omit all reads.\n      if downsample_fraction is not None and downsample_fraction != 0:\n        if not 0.0 < downsample_fraction <= 1.0:\n          raise ValueError(\n              \'downsample_fraction must be in the interval (0.0, 1.0]\',\n              downsample_fraction)\n\n      if random_seed is None:\n        # Fixed random seed produced with \'od -vAn -N4 -tu4 < /dev/urandom\'.\n        random_seed = 2928130004\n\n      self._reader = sam_reader.SamReader.from_file(\n          input_path.encode(\'utf8\'),\n          ref_path.encode(\'utf8\') if ref_path is not None else \'\',\n          reads_pb2.SamReaderOptions(\n              read_requirements=read_requirements,\n              aux_field_handling=aux_field_handling,\n              hts_block_size=(hts_block_size or 0),\n              downsample_fraction=downsample_fraction,\n              random_seed=random_seed,\n              use_original_base_quality_scores=use_original_base_quality_scores)\n      )\n\n      self.header = self._reader.header\n\n    super(NativeSamReader, self).__init__()\n\n  def iterate(self):\n    """"""Returns an iterable of Read protos in the file.""""""\n    return self._reader.iterate()\n\n  def query(self, region):\n    """"""Returns an iterator for going through the reads in the region.""""""\n    return self._reader.query(region)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass SamReader(genomics_reader.DispatchingGenomicsReader):\n  """"""Class for reading Read protos from SAM/BAM/CRAM or TFRecord files.""""""\n\n  def _native_reader(self, input_path, **kwargs):\n    return NativeSamReader(input_path, **kwargs)\n\n  def _record_proto(self):\n    return reads_pb2.Read\n\n\nclass NativeSamWriter(genomics_writer.GenomicsWriter):\n  """"""Class for writing to native SAM/BAM/CRAM files.\n\n  Most users will want SamWriter, which will write to either native SAM/BAM/CRAM\n  files or TFRecords files, based on the output filename\'s extensions.\n  """"""\n\n  def __init__(self, output_path, header, ref_path=None, embed_ref=False):\n    """"""Initializer for NativeSamWriter.\n\n    Args:\n      output_path: str. A path where we\'ll write our SAM/BAM/CRAM file.\n      ref_path: str. Path to the reference file. Required for CRAM file.\n      embed_ref: bool. Whether to embed the reference sequences in CRAM file.\n        Default is False.\n      header: A nucleus.SamHeader proto.  The header is used both for writing\n        the header, and to control the sorting applied to the rest of the file.\n    """"""\n    super(NativeSamWriter, self).__init__()\n    self._writer = sam_writer.SamWriter.to_file(\n        output_path,\n        ref_path.encode(\'utf8\') if ref_path is not None else \'\', embed_ref,\n        header)\n\n  def write(self, proto):\n    self._writer.write(proto)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass SamWriter(genomics_writer.DispatchingGenomicsWriter):\n  """"""Class for writing Read protos to SAM or TFRecord files.""""""\n\n  def _native_writer(self, output_path, **kwargs):\n    return NativeSamWriter(output_path, **kwargs)\n\n\nclass InMemorySamReader(object):\n  """"""Python interface class for in-memory SAM/BAM/CRAM reader.\n\n  Attributes:\n    reads: list[nucleus.genomics.v1.Read]. The list of in-memory reads.\n    is_sorted: bool, True if reads are sorted.\n  """"""\n\n  def __init__(self, reads, is_sorted=False):\n    self.replace_reads(reads, is_sorted=is_sorted)\n\n  def replace_reads(self, reads, is_sorted=False):\n    """"""Replace the reads stored by this reader.""""""\n    self.reads = reads\n    self.is_sorted = is_sorted\n\n  def iterate(self):\n    """"""Iterate over all records in the reads.\n\n    Returns:\n      An iterator over nucleus.genomics.v1.Read\'s.\n    """"""\n    return self.reads\n\n  def query(self, region):\n    """"""Returns an iterator for going through the reads in the region.\n\n    Args:\n      region: nucleus.genomics.v1.Range. The query region.\n\n    Returns:\n      An iterator over nucleus.genomics.v1.Read protos.\n    """"""\n    # TODO(b/37353140): Add a faster query version for sorted reads.\n    return (\n        read for read in self.reads if utils.read_overlaps_region(read, region))\n'"
nucleus/io/sam_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.util.io.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nimport six\n\nfrom nucleus.io import gfile\nfrom nucleus.io import sam\nfrom nucleus.io import tfrecord\nfrom nucleus.protos import reads_pb2\nfrom nucleus.protos import reference_pb2\nfrom nucleus.testing import test_utils\nfrom nucleus.util import ranges\n\n\nclass SamReaderTests(parameterized.TestCase):\n  """"""Test the iteration functionality provided by io.SamReader.""""""\n\n  def test_sam_iterate(self):\n    reader = sam.SamReader(test_utils.genomics_core_testdata(\'test.sam\'))\n    with reader:\n      self.assertEqual(test_utils.iterable_len(reader.iterate()), 6)\n\n  def test_bam_iterate(self):\n    reader = sam.SamReader(test_utils.genomics_core_testdata(\'test.bam\'))\n    with reader:\n      self.assertEqual(test_utils.iterable_len(reader.iterate()), 106)\n\n  def test_bam_iterate_partially(self):\n    """"""Verify that iteration provides results incrementally, not all at once.""""""\n    reader = sam.SamReader(test_utils.genomics_core_testdata(\'test.bam\'))\n    with reader:\n      iterable = reader.iterate()\n      # We expect 106 records in total.\n      for _ in range(10):\n        results = list(itertools.islice(iterable, 10))\n        self.assertEqual(len(results), 10)\n      results = list(itertools.islice(iterable, 10))\n      self.assertEqual(len(results), 6)\n\n  def test_sam_query(self):\n    reader = sam.SamReader(test_utils.genomics_core_testdata(\'test.bam\'))\n    expected = [(ranges.parse_literal(\'chr20:10,000,000-10,000,100\'), 106),\n                (ranges.parse_literal(\'chr20:10,000,000-10,000,000\'), 45)]\n    with reader:\n      for interval, n_expected in expected:\n        with reader.query(interval) as iterable:\n          self.assertEqual(test_utils.iterable_len(iterable), n_expected)\n\n  def test_sam_query_alternate_index_name(self):\n    reader = sam.SamReader(\n        test_utils.genomics_core_testdata(\'test_alternate_index.bam\'))\n    expected = [(ranges.parse_literal(\'chr20:10,000,000-10,000,100\'), 106),\n                (ranges.parse_literal(\'chr20:10,000,000-10,000,000\'), 45)]\n    with reader:\n      for interval, n_expected in expected:\n        with reader.query(interval) as iterable:\n          self.assertEqual(test_utils.iterable_len(iterable), n_expected)\n\n  @parameterized.parameters((\'\\t\'.join(x[0] for x in items), {\n      k: v for t in items for k, v in t[1].items()\n  }) for r in [1, 2] for items in itertools.permutations(\n      [\n          (\'X1:i:0\', {\n              \'X1\': 0\n          }),\n          (\'X2:i:127\', {\n              \'X2\': 127\n          }),\n          (\'X3:i:255\', {\n              \'X3\': 255\n          }),\n          (\'X4:A:a\', {\n              \'X4\': \'a\'\n          }),\n          (\'X5:f:1.234\', {\n              \'X5\': 1.234\n          }),\n          (\'X6:Z:string\', {\n              \'X6\': \'string\'\n          }),\n          (\'X7:Z:with spaces\', {\n              \'X7\': \'with spaces\'\n          }),\n          (\'ZJ:Z:\', {\n              \'ZJ\': \'\'\n          }),  # Empty string.\n          # We skip H hex byte-array tags as they appear deprecated.\n          (\'X8:H:1AE301\', {}),\n          (\'X9:B:i,1\', {\n              \'X9\': [1]\n          }),\n          (\'XA:B:i,1,2\', {\n              \'XA\': [1, 2]\n          }),\n          (\'XB:B:i,1,2,3\', {\n              \'XB\': [1, 2, 3]\n          }),\n          (\'XC:B:i,1,2,3,4,5,6,7,8,9,10\', {\n              \'XC\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n          }),\n          (\'XD:B:I,1,2,3\', {\n              \'XD\': [1, 2, 3]\n          }),\n          (\'XE:B:c,1,2,3\', {\n              \'XE\': [1, 2, 3]\n          }),\n          (\'XF:B:C,1,2,3\', {\n              \'XF\': [1, 2, 3]\n          }),\n          (\'XG:B:f,0.12,0.34\', {\n              \'XG\': [0.12, 0.34]\n          }),\n          (\'XH:B:s,1,2,3\', {\n              \'XH\': [1, 2, 3]\n          }),\n          (\'XI:B:S,1,2,3\', {\n              \'XI\': [1, 2, 3]\n          }),\n      ],\n      r=r))\n  def test_parsing_aux_tags(self, tag_string, expected_info):\n    # Minimal header line to create a valid SAM file.\n    reads = self._parse_read_with_aux_tags(tag_string)\n    self.assertLen(reads, 1)\n    self.assertInfoMapEqual(reads[0].info, expected_info)\n\n  @parameterized.parameters(\n      \'\\t\'.join(tags) for r in [1, 2, 3] for tags in itertools.permutations(\n          [\n              \'X2:i:x\',  # Integer with character value.\n              \'X3:f:string\',  # A string instead of the expected float.\n              \'X4:A:\',  # Supposed to be single char, but we none here.\n              \'X5:A:ab\',  # Supposed to be single char, but we have two here.\n              \'X6:B:i\',  # Empty byte array.\n              \'X7:B:i,1.23\',  # Integer byte array with a float.\n              \'X8:B:i,string\',  # Integer byte array with a string.\n              \'X8:B:f,string\',  # Float byte array with a string.\n              \'X8:B:z,1,2,3\',  # z is not a valid subtype.\n          ],\n          r=r))\n  def test_survives_malformed_lines(self, tag_string):\n    try:\n      reads = self._parse_read_with_aux_tags(tag_string)\n      # If we didn\'t detect the error, make sure we actually still parsed the\n      # read itself.\n      self.assertLen(reads, 1)\n      self.assertEqual(reads[0].fragment_name, \'read_name\')\n      self.assertEqual(reads[0].aligned_sequence, \'CCC\')\n      self.assertEqual(reads[0].alignment.position.reference_name, \'chr1\')\n      self.assertEqual(reads[0].alignment.position.position, 0)\n    except ValueError as e:\n      if \'Failed to parse SAM record\' not in str(e):\n        self.fail(\'Parsing failed but unexpected exception was seen: \' + str(e))\n\n  def _parse_read_with_aux_tags(self, tag_string):\n    # Minimal header line to create a valid SAM file.\n    header_lines = \'@HD\\tVN:1.3\\tSO:coordinate\\n@SQ\\tSN:chr1\\tLN:248956422\\n\'\n    # A single stock read we\'ll add our AUX fields to.\n    read = \'read_name\\t0\\tchr1\\t1\\t0\\t3M\\t*\\t0\\t0\\tCCC\\tAAA\\t\' + tag_string\n    path = test_utils.test_tmpfile(\'aux_tags.bam\')\n    with gfile.Open(path, \'w\') as fout:\n      fout.write(header_lines)\n      fout.write(read + \'\\n\')\n    with sam.SamReader(path, parse_aux_fields=True) as reader:\n      return list(reader.iterate())\n\n  def assertInfoMapEqual(self, info_map, expected_info):\n    self.assertCountEqual(\n        info_map.keys(), expected_info.keys(),\n        \'info has {} keys but we expected {}\'.format(info_map.keys(),\n                                                     expected_info.keys()))\n    for key, expected_values in expected_info.items():\n      if not isinstance(expected_values, list):\n        expected_values = [expected_values]\n      for actual_value, expected_value in zip(info_map[key].values,\n                                              expected_values):\n        if isinstance(expected_value, float):\n          self.assertAlmostEqual(actual_value.number_value, expected_value)\n        elif isinstance(expected_value, six.integer_types):\n          self.assertEqual(actual_value.int_value, expected_value)\n        elif isinstance(expected_value, str):\n          self.assertEqual(actual_value.string_value, expected_value)\n        else:\n          self.fail(\'Unsupported expected_value type {}\'.format(expected_value))\n\n  @parameterized.parameters(\n      # These expected counts are deterministic because we always set the random\n      # seed in each test.\n      # There are 106 total reads if we iterate.\n      (\'iterate\', None, 1.0, 106),\n      (\'iterate\', None, 0.5, 59),\n      (\'iterate\', None, 0.25, 31),\n      # There are 45 total reads if we don\'t downsample.\n      (\'query\', \'chr20:10,000,000-10,000,000\', 1.0, 45),\n      (\'query\', \'chr20:10,000,000-10,000,000\', 0.5, 25),\n      (\'query\', \'chr20:10,000,000-10,000,000\', 0.25, 13),\n  )\n  def test_downsampling(self, method, maybe_range, fraction, expected_n_reads):\n    reader = sam.SamReader(\n        test_utils.genomics_core_testdata(\'test.bam\'),\n        downsample_fraction=fraction,\n        random_seed=12345)\n    with reader:\n      if method == \'iterate\':\n        reads_iter = reader.iterate()\n      elif method == \'query\':\n        reads_iter = reader.query(ranges.parse_literal(maybe_range))\n      else:\n        self.fail(\'Unexpected method \' + str(method))\n      self.assertEqual(test_utils.iterable_len(reads_iter), expected_n_reads)\n\n\n# Note that CRAM version 2.1 files work with Nucleus but they cannot be used in\n# our test here because CRAM 2.1 embeds an exact path to the reference file\n# which LEAKR flags as leaking internal google paths.\n@parameterized.parameters(\n    dict(\n        filename=\'test_cram.embed_ref_0_version_3.0.cram\',\n        has_embedded_ref=False),\n    dict(\n        filename=\'test_cram.embed_ref_1_version_3.0.cram\',\n        has_embedded_ref=True),\n)\nclass CramReaderTests(parameterized.TestCase):\n  """"""Test io.SamReader on CRAM formatted files.""""""\n\n  def _make_reader(self, filename, has_embedded_ref):\n    if has_embedded_ref:\n      # If we have an embedded reference, force the reader to use it by not\n      # providing an argument for ref_path.\n      return sam.SamReader(test_utils.genomics_core_testdata(filename))\n    else:\n      # Otherwise we need to explicitly override the reference encoded in the UR\n      # of the CRAM file to use the path provided to our test.fasta.\n      return sam.SamReader(\n          test_utils.genomics_core_testdata(filename),\n          ref_path=test_utils.genomics_core_testdata(\'test.fasta\'))\n\n  def test_header(self, filename, has_embedded_ref):\n    with self._make_reader(filename, has_embedded_ref) as reader:\n      self.assertEqual(reader.header.format_version, \'1.3\')\n      self.assertEqual([contig.name for contig in reader.header.contigs],\n                       [\'chrM\', \'chr1\', \'chr2\'])\n\n  def test_iterate(self, filename, has_embedded_ref):\n    with self._make_reader(filename, has_embedded_ref) as reader:\n      reads = list(reader.iterate())\n      self.assertEqual(len(reads), 3)\n      self.assertEqual([read.fragment_name for read in reads],\n                       [\'cram1\', \'cram2\', \'cram3\'])\n      self.assertEqual([read.aligned_sequence for read in reads], [\n          \'CCCTAACCCTAACCCTAACCCTAACCCTANNNNNN\',\n          (\'TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCAAAACGAATCAAAAAAGAAAAACGAA\'\n           \'AAAAAAA\'),\n          \'CACAGACGCTT\'\n      ])\n\n  def test_query(self, filename, has_embedded_ref):\n    with self._make_reader(filename, has_embedded_ref) as reader:\n      for interval, n_expected in [(\'chr1:1-100\', 3), (\'chr2:1-121\', 0)]:\n        with reader.query(ranges.parse_literal(interval)) as iterable:\n          self.assertEqual(test_utils.iterable_len(iterable), n_expected)\n\n\nclass ReadWriterTests(parameterized.TestCase):\n  """"""Tests for sam.SamWriter.""""""\n\n  def setUp(self):\n    self.read1 = test_utils.make_read(\n        bases=\'ACCGT\',\n        chrom=\'chr1\',\n        start=10,\n        cigar=\'5M\',\n        mapq=50,\n        quals=range(30, 35),\n        name=\'read1\')\n    self.read2 = test_utils.make_read(\n        bases=\'AACCTT\',\n        chrom=\'chr2\',\n        start=15,\n        cigar=\'7M\',\n        mapq=40,\n        quals=range(20, 26),\n        name=\'read2\')\n    self.contigs = [\n        reference_pb2.ContigInfo(name=\'chr1\'),\n        reference_pb2.ContigInfo(name=\'chr2\'),\n    ]\n    self.header = reads_pb2.SamHeader()\n\n  def test_make_read_writer_tfrecords(self):\n    outfile = test_utils.test_tmpfile(\'test.tfrecord\')\n    writer = sam.SamWriter(outfile, header=self.header)\n\n    # Test that the writer is a context manager and that we can write a read to\n    # it.\n    with writer:\n      writer.write(self.read1)\n      writer.write(self.read2)\n\n    # Our output should have exactly one read in it.\n    self.assertEqual([self.read1, self.read2],\n                     list(\n                         tfrecord.read_tfrecords(outfile,\n                                                 proto=reads_pb2.Read)))\n\n  @parameterized.parameters(\'test.bam\', \'test.sam\')\n  def test_roundtrip_writer(self, filename):\n    output_path = test_utils.test_tmpfile(filename)\n    original_reader = sam.SamReader(test_utils.genomics_core_testdata(filename))\n    original_records = list(original_reader.iterate())\n    with sam.SamWriter(output_path, header=original_reader.header) as writer:\n      for record in original_records:\n        writer.write(record)\n    with sam.SamReader(output_path) as new_reader:\n      self.assertEqual(original_records, list(new_reader.iterate()))\n\n  @parameterized.parameters(\n      dict(\n          filename=\'test_cram.embed_ref_0_version_3.0.cram\',\n          has_embedded_ref=False),\n      dict(\n          filename=\'test_cram.embed_ref_1_version_3.0.cram\',\n          has_embedded_ref=True))\n  def test_roundtrip_cram_writer(self, filename, has_embedded_ref):\n    output_path = test_utils.test_tmpfile(filename)\n    writer_ref_path = test_utils.genomics_core_testdata(\'test.fasta\')\n    reader_ref_path = \'\'\n    if not has_embedded_ref:\n      reader_ref_path = writer_ref_path\n    original_reader = sam.SamReader(\n        test_utils.genomics_core_testdata(filename), ref_path=reader_ref_path)\n    original_records = list(original_reader.iterate())\n    with sam.SamWriter(\n        output_path,\n        header=original_reader.header,\n        ref_path=writer_ref_path,\n        embed_ref=has_embedded_ref) as writer:\n      for record in original_records:\n        writer.write(record)\n    with sam.SamReader(output_path, ref_path=reader_ref_path) as new_reader:\n      self.assertEqual(original_records, list(new_reader.iterate()))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/sharded_file_utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utility functions for working with sharded files.\n\nA sharded file is a single conceptual file that is broken into a collection\nof files to make parallelization easier.  A sharded file spec is like a\nfilename for a sharded file; the file spec ""/some/path/prefix@200.txt""\nsays that the sharded file consists of 200 actual files that have names like\n""/some/path/prefix-00000-of-00200.txt"", ""/some/path/prefix-00001-of-00200.txt"",\netc.  This module contains functions for parsing, generating, detecting and\nresolving sharded file specs.\n""""""\n\n# Important: Please keep this module free of TensorFlow c++ extensions.\n# This makes it easy to build pure python packages for training that work with\n# CMLE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport re\nimport six\n\nfrom nucleus.io import gfile\n\nSHARD_SPEC_PATTERN = re.compile(R\'((.*)\\@(\\d*[1-9]\\d*)(?:\\.(.+))?)\')\n\n\nclass ShardError(Exception):\n  """"""An I/O error.""""""\n\n\ndef parse_sharded_file_spec(spec):\n  """"""Parse a sharded file specification.\n\n  Args:\n    spec: str. The sharded file specification. A sharded file spec is one like\n      \'gs://some/file@200.txt\'. Here, \'@200\' specifies the number of shards.\n\n  Returns:\n    basename: str. The basename for the files.\n    num_shards: int >= 0. The number of shards.\n    suffix: str. The suffix if there is one, or \'\' if not.\n  Raises:\n    ShardError: If the spec is not a valid sharded specification.\n  """"""\n  m = SHARD_SPEC_PATTERN.match(spec)\n  if not m:\n    raise ShardError((\'The file specification {0} is not a sharded file \'\n                      \'specification because it did not match the regex \'\n                      \'{1}\').format(spec, SHARD_SPEC_PATTERN.pattern))\n\n  # If there\'s a non-empty suffix, we need to prepend \'.\' so we get files like\n  # foo@20.ext instead of foo@ext. The original C++ parser version has:\n  # string ext = StrCat(suff.empty() ? """" : ""."", suff);\n  suffix = \'.\' + m.group(4) if m.group(4) else \'\'\n\n  return m.group(2), int(m.group(3)), suffix\n\n\ndef _shard_width(num_shards):\n  """"""Return the width of the shard matcher based on the number of shards.""""""\n  return max(5, int(math.floor(math.log10(num_shards)) + 1))\n\n\ndef generate_sharded_filenames(spec):\n  """"""Generate the list of filenames corresponding to the sharding path.\n\n  Args:\n    spec: str. Represents a filename with a sharding specification.\n      e.g., \'gs://some/file@200.txt\' represents a file sharded 200 ways.\n\n  Returns:\n    List of filenames.\n\n  Raises:\n    ShardError: If spec is not a valid sharded file specification.\n  """"""\n  basename, num_shards, suffix = parse_sharded_file_spec(spec)\n  files = []\n  width = _shard_width(num_shards)\n  format_str = \'{{0}}-{{1:0{0}}}-of-{{2:0{0}}}{{3}}\'.format(width)\n  for i in range(num_shards):\n    files.append(format_str.format(basename, i, num_shards, suffix))\n\n  return files\n\n\ndef glob_list_sharded_file_patterns(comma_separated_patterns, sep=\',\'):\n  """"""Generate list of filenames corresponding to `comma_separated_patterns`.\n\n  Args:\n    comma_separated_patterns: str. A pattern or a comma-separated list of\n      patterns that represent file names.\n    sep: char. Separator character.\n\n  Returns:\n    List of filenames, sorted and dedupped.\n  """"""\n  return sorted(set([\n      f\n      for pattern in comma_separated_patterns.split(sep)\n      for f in gfile.Glob(normalize_to_sharded_file_pattern(pattern))\n  ]))\n\n\ndef generate_sharded_file_pattern(basename, num_shards, suffix):\n  """"""Generate a sharded file pattern.\n\n  Args:\n    basename: str. The basename for the files.\n    num_shards: int. The number of shards.\n    suffix: str. The suffix if there is one or \'\'.\n  Returns:\n    pattern:\n  """"""\n  width = _shard_width(num_shards)\n  specifier = \'?\' * width\n  format_str = \'{{0}}-{{1}}-of-{{2:0{0}}}{{3}}\'.format(width)\n  return format_str.format(basename, specifier, num_shards, suffix)\n\n\ndef normalize_to_sharded_file_pattern(spec_or_pattern):\n  """"""Take a sharding spec or sharding file pattern and return a sharded pattern.\n\n  The input can be a sharding spec(e.g \'/some/file@10\') or a sharded file\n  pattern (e.g. \'/some/file-?????-of-00010)\n\n  Args:\n    spec_or_pattern: str. A sharded file specification or sharded file pattern.\n\n  Returns:\n    A sharded file pattern.\n  """"""\n  try:\n    basename, num_shards, suffix = parse_sharded_file_spec(spec_or_pattern)\n  except ShardError:\n    return spec_or_pattern\n  return generate_sharded_file_pattern(basename, num_shards, suffix)\n\n\ndef is_sharded_file_spec(spec):\n  """"""Returns True if spec is a sharded file specification.""""""\n  m = SHARD_SPEC_PATTERN.match(spec)\n  return m is not None\n\n\n# TODO(mdepristo): retire when GenerateShardedFilename is added to library.\ndef sharded_filename(spec, i):\n  """"""Gets a path appropriate for writing the ith file of a sharded spec.""""""\n  return generate_sharded_filenames(spec)[i]\n\n\n# TODO(b/64046543): Improve the return value (instead of using tuple). It hurts\n# readability when there are multiple input filespecs.\ndef resolve_filespecs(shard, *filespecs):\n  """"""Transforms potentially sharded filespecs into their paths for single shard.\n\n  This function takes a shard number and a varargs of potentially-sharded\n  filespecs, and returns a list where the filespecs have been resolved into\n  concrete file paths for a single shard.\n\n  This function has a concept of a master filespec, which is used to constrain\n  and check the validity of other filespecs. The first filespec is considered\n  the master, and it cannot be None. For example, if master is not sharded, none\n  of the other specs can be sharded, and vice versa. They must all also have a\n  consistent sharding (e.g., master is @10, then all others must be @10).\n\n  Note that filespecs (except the master) may be None or any other False value,\n  which are returned as-is in the output list.\n\n  Args:\n    shard: int >= 0. Our shard number.\n    *filespecs: list[str]. Contains all of the filespecs we want to resolve into\n      shard-specific file paths.\n\n  Returns:\n    A list. The first element is the number of shards, which is an int >= 1 when\n    filespecs contains sharded paths and 0 if none do. All subsequent\n    returned values follow the shard-specific paths for each filespec, in order.\n\n  Raises:\n    ValueError: if any filespecs are inconsistent.\n  """"""\n  if not filespecs:\n    raise ValueError(\'filespecs must have at least one element.\')\n\n  master = filespecs[0]\n  master_is_sharded = is_sharded_file_spec(master)\n\n  master_num_shards = 0\n  if master_is_sharded:\n    _, master_num_shards, _ = parse_sharded_file_spec(master)\n    if shard >= master_num_shards or shard < 0:\n      raise ValueError(\'Invalid shard={} value with master={} sharding\'.format(\n          shard, master))\n  elif shard > 0:\n    raise ValueError(\'Output is not sharded but shard > 0: {}\'.format(shard))\n\n  def resolve_one(filespec):\n    """"""Resolves a single filespec into a concrete filepath.""""""\n    if not filespec:\n      return filespec\n\n    is_sharded = is_sharded_file_spec(filespec)\n    if master_is_sharded != is_sharded:\n      raise ValueError(\'Master={} and {} have inconsistent sharding\'.format(\n          master, filespec))\n\n    if not is_sharded:  # Not sharded => filespec is the actual filename.\n      return filespec\n\n    _, filespec_num_shards, _ = parse_sharded_file_spec(filespec)\n    if filespec_num_shards != master_num_shards:\n      raise ValueError(\'Master={} and {} have inconsistent sharding\'.format(\n          master, filespec))\n    return sharded_filename(filespec, shard)\n\n  return [master_num_shards] + [resolve_one(spec) for spec in filespecs]\n\n\ndef maybe_generate_sharded_filenames(filespec):\n  """"""Potentially expands sharded filespec into a list of paths.\n\n  This function takes in a potentially sharded filespec and expands it into a\n  list containing the full set of corresponding concrete sharded file paths. If\n  the input filespec is not sharded then a list containing just that file path\n  is returned. This function is useful, for example, when the input to a binary\n  can either be sharded or not.\n\n  Args:\n    filespec: String. A potentially sharded filespec to expand.\n\n  Returns:\n    A list of file paths.\n\n  Raises:\n    TypeError: if filespec is not in valid string_types.\n  """"""\n  if not isinstance(filespec, six.string_types):\n    raise TypeError(\'Invalid filespec: %s\' % filespec)\n  if is_sharded_file_spec(filespec):\n    return generate_sharded_filenames(filespec)\n  else:\n    return [filespec]\n'"
nucleus/io/sharded_file_utils_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for nucleus.io.sharded_file_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import sharded_file_utils as io\nfrom nucleus.testing import test_utils\n\n\nclass IOTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      # Unsharded outputs pass through as expected.\n      dict(task_id=0, filespecs=[\'foo.txt\'], expected=[0, \'foo.txt\']),\n      dict(\n          task_id=0,\n          filespecs=[\'foo.txt\', \'bar.txt\'],\n          expected=[0, \'foo.txt\', \'bar.txt\']),\n      dict(\n          task_id=0,\n          filespecs=[\'bar.txt\', \'foo.txt\'],\n          expected=[0, \'bar.txt\', \'foo.txt\']),\n      # It\'s ok to have False values for other bindings.\n      dict(\n          task_id=0, filespecs=[\'foo.txt\', None], expected=[0, \'foo.txt\',\n                                                            None]),\n      dict(task_id=0, filespecs=[\'foo.txt\', \'\'], expected=[0, \'foo.txt\', \'\']),\n      dict(\n          task_id=0,\n          filespecs=[\'foo@10.txt\', None],\n          expected=[10, \'foo-00000-of-00010.txt\', None]),\n      dict(\n          task_id=0,\n          filespecs=[\'foo@10.txt\', \'\'],\n          expected=[10, \'foo-00000-of-00010.txt\', \'\']),\n      # Simple check that master behaves as expected.\n      dict(\n          task_id=0,\n          filespecs=[\'foo@10.txt\', None],\n          expected=[10, \'foo-00000-of-00010.txt\', None]),\n      dict(\n          task_id=0,\n          filespecs=[\'foo@10\', None],\n          expected=[10, \'foo-00000-of-00010\', None]),\n      dict(\n          task_id=1,\n          filespecs=[\'foo@10\', None],\n          expected=[10, \'foo-00001-of-00010\', None]),\n      dict(\n          task_id=9,\n          filespecs=[\'foo@10\', None],\n          expected=[10, \'foo-00009-of-00010\', None]),\n      # Make sure we handle sharding of multiple filespecs.\n      dict(\n          task_id=0,\n          filespecs=[\'foo@10\', \'bar@10\', \'baz@10\'],\n          expected=[\n              10, \'foo-00000-of-00010\', \'bar-00000-of-00010\',\n              \'baz-00000-of-00010\'\n          ]),\n      dict(\n          task_id=9,\n          filespecs=[\'foo@10\', \'bar@10\', \'baz@10\'],\n          expected=[\n              10, \'foo-00009-of-00010\', \'bar-00009-of-00010\',\n              \'baz-00009-of-00010\'\n          ]),\n  )\n  def test_resolve_filespecs(self, task_id, filespecs, expected):\n    self.assertEqual(io.resolve_filespecs(task_id, *filespecs), expected)\n\n  @parameterized.parameters(\n      # shard >= num_shards.\n      (10, [\'foo@10\']),\n      # shard > 0 but master isn\'t sharded.\n      (1, [\'foo\']),\n      # Inconsistent sharding.\n      (0, [\'foo@10\', \'bad@11\']),\n      # master isn\'t sharded but bad is.\n      (0, [\'foo\', \'bad@11\']),\n  )\n  def test_resolve_filespecs_raises_with_bad_inputs(self, task_id, outputs):\n    with self.assertRaises(ValueError):\n      io.resolve_filespecs(task_id, *outputs)\n\n  @parameterized.parameters(\n      # Unsharded files work.\n      (\'foo.txt\', [\'foo.txt\']),\n      (\'foo-00000-of-00010.txt\', [\'foo-00000-of-00010.txt\']),\n      # Sharded file patterns work.\n      (\'foo@3.txt\', [\n          \'foo-00000-of-00003.txt\', \'foo-00001-of-00003.txt\',\n          \'foo-00002-of-00003.txt\'\n      ]),\n      (\'foo@3\',\n       [\'foo-00000-of-00003\', \'foo-00001-of-00003\', \'foo-00002-of-00003\']),\n  )\n  def test_maybe_generate_sharded_filenames(self, filespec, expected):\n    self.assertEqual(io.maybe_generate_sharded_filenames(filespec), expected)\n\n\nclass ShardsTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (\'no_suffix\', \'/dir/foo/bar@3\', \'/dir/foo/bar\', 3, \'\'),\n      (\'suffix-dot\', \'/dir/foo/bar@3.txt\', \'/dir/foo/bar\', 3, \'.txt\'),\n  )\n  def testParseShardedFileSpec(self, spec, expected_basename,\n                               expected_num_shards, expected_suffix):\n\n    basename, num_shards, suffix = io.parse_sharded_file_spec(spec)\n    self.assertEqual(basename, expected_basename)\n    self.assertEqual(num_shards, expected_num_shards)\n    self.assertEqual(suffix, expected_suffix)\n\n  def testParseShardedFileSpecInvalid(self):\n    self.assertRaises(io.ShardError,\n                      io.parse_sharded_file_spec, \'/dir/foo/bar@0\')\n\n  @parameterized.named_parameters(\n      (\'no_suffix\', \'/dir/foo/bar@3\', [\n          \'/dir/foo/bar-00000-of-00003\', \'/dir/foo/bar-00001-of-00003\',\n          \'/dir/foo/bar-00002-of-00003\'\n      ]),\n      (\'suffix\', \'/dir/foo/bar@3.txt\', [\n          \'/dir/foo/bar-00000-of-00003.txt\', \'/dir/foo/bar-00001-of-00003.txt\',\n          \'/dir/foo/bar-00002-of-00003.txt\'\n      ]),\n  )\n  def testGenerateShardedFilenames(self, spec, expected):\n    names = io.generate_sharded_filenames(spec)\n    self.assertEqual(names, expected)\n\n  def testGenerateShardedFilenamesManyShards(self):\n    names = io.generate_sharded_filenames(\'/dir/foo/bar@100000\')\n    self.assertEqual(len(names), 100000)\n    self.assertEqual(names[99999], \'/dir/foo/bar-099999-of-100000\')\n\n  @parameterized.named_parameters(\n      (\'no_spec\', \'/dir/foo/bar\'),\n      (\'zero_shards\', \'/dir/foo/bar@0\'),\n  )\n  def testGenerateShardedFilenamesError(self, spec):\n    self.assertRaises(io.ShardError, io.generate_sharded_filenames, spec)\n\n  @parameterized.named_parameters(\n      (\'basic\', \'/dir/foo/bar@3\', True),\n      (\'suffix\', \'/dir/foo/bar@3,txt\', True),\n      (\'many_shards\', \'/dir/foo/bar@123456\', True),\n      (\'invalid_spec\', \'/dir/foo/bar@0\', False),\n      (\'not_spec\', \'/dir/foo/bar\', False),\n  )\n  def testIsShardedFileSpec(self, spec, expected):\n    actual = io.is_sharded_file_spec(spec)\n    self.assertEqual(actual, expected,\n                      \'io.IshShardedFileSpec({0}) is {1} expected {2}\'.format(\n                          spec, actual, expected))\n\n  @parameterized.named_parameters(\n      (\'no_suffix\', \'/dir/foo/bar\', 3, \'\', \'/dir/foo/bar-?????-of-00003\'),\n      (\'suffix\', \'/dir/foo/bar\', 3, \'.txt\', \'/dir/foo/bar-?????-of-00003.txt\'),\n      (\'many\', \'/dir/foo/bar\', 1234567, \'.txt\',\n       \'/dir/foo/bar-???????-of-1234567.txt\'),\n  )\n  def testGenerateShardedFilePattern(self, basename, num_shards, suffix,\n                                     expected):\n\n    self.assertEqual(io.generate_sharded_file_pattern(\n        basename, num_shards, suffix), expected)\n\n  @parameterized.named_parameters(\n      (\'no_spec\', \'/dir/foo/bar\', \'/dir/foo/bar\'),\n      (\'suffix\', \'/dir/foo/bar@3.txt\', \'/dir/foo/bar-?????-of-00003.txt\'),\n      (\'no_suffix\', \'/dir/foo/bar@3\', \'/dir/foo/bar-?????-of-00003\'),\n      (\'1000\', \'/dir/foo/bar@1000\', \'/dir/foo/bar-?????-of-01000\'),\n      (\'many\', \'/dir/foo/bar@12345678\', \'/dir/foo/bar-????????-of-12345678\'),\n  )\n  def testNormalizeToShardedFilePattern(self, spec, expected):\n    self.assertEqual(expected, io.normalize_to_sharded_file_pattern(spec))\n\n  @parameterized.named_parameters(\n      (\'no_spec\', \'no_spec\', [\'no_spec\']),\n      (\'sharded\', \'sharded@3\', [\'sharded-00000-of-00003\',\n                                \'sharded-00001-of-00003\',\n                                \'sharded-00002-of-00003\']),\n      (\'wildcard1\', \'*.ext\', [\'cat.ext\', \'dog.ext\']),\n      (\'wildcard2\', \'fo?bar\', [\'foobar\']),\n      (\'comma_list\', \'file1,file2,file3\', [\'file1\', \'file2\', \'file3\']),\n      (\'mixed_list\', \'mixed.*txt,mixed@1,mixed_file\',\n       [\'mixed.1txt\', \'mixed.2txt\', \'mixed-00000-of-00001\', \'mixed_file\']),\n      (\'with_dups\', \'with_dups*\',\n       [\'with_dups.1txt\', \'with_dups.2txt\', \'with_dups-00000-of-00001\',\n        \'with_dups\']),\n  )\n  def testGlobListShardedFilePatterns(self, specs, expected_files):\n    # First, create all expected_files so Glob will work later.\n    expected_full_files = [test_utils.test_tmpfile(f, \'\')\n                           for f in expected_files]\n    # Create the full spec names. This one doesn\'t create the files.\n    full_specs = \',\'.join(\n        [test_utils.test_tmpfile(spec) for spec in specs.split(\',\')])\n    self.assertEqual(sorted(set(expected_full_files)),\n                     io.glob_list_sharded_file_patterns(full_specs))\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/tabix.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Creates tabix indices for VCFs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom nucleus.io.python import tabix_indexer\n\n\ndef build_index(path):\n  """"""Builds a tabix index for VCF at the specified path.""""""\n  tabix_indexer.tbx_index_build(path)\n\n\ndef build_csi_index(path, min_shift):\n  """"""Builds a csi index for VCF at the specified path.""""""\n  tabix_indexer.csi_index_build(path, min_shift)\n'"
nucleus/io/tabix_test.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.io.tabix.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\n\nfrom absl.testing import absltest\n\nfrom nucleus.io import gfile\nfrom nucleus.io import tabix\nfrom nucleus.io import vcf\nfrom nucleus.testing import test_utils\nfrom nucleus.util import ranges\n\n\nclass TabixTest(absltest.TestCase):\n  """"""Test the functionality of tabix.build_index.""""""\n\n  def setUp(self):\n    super(TabixTest, self).setUp()\n    self.input_file = test_utils.genomics_core_testdata(\'test_samples.vcf.gz\')\n    self.output_file = test_utils.test_tmpfile(\'test_samples.vcf.gz\')\n    shutil.copyfile(self.input_file, self.output_file)\n    self.tbx_index_file = self.output_file + \'.tbi\'\n    self.csi_index_file = self.output_file + \'.csi\'\n\n  def tearDown(self):\n    super(TabixTest, self).tearDown()\n    os.remove(self.output_file)\n    try:\n      os.remove(self.tbx_index_file)\n    except OSError:\n      pass\n    try:\n      os.remove(self.csi_index_file)\n    except OSError:\n      pass\n\n  def test_build_index_tbx(self):\n    self.assertFalse(gfile.Exists(self.tbx_index_file))\n    tabix.build_index(self.output_file)\n    self.assertTrue(gfile.Exists(self.tbx_index_file))\n\n  def test_build_index_csi(self):\n    min_shift = 14\n    self.assertFalse(gfile.Exists(self.csi_index_file))\n    tabix.build_csi_index(self.output_file, min_shift)\n    self.assertTrue(gfile.Exists(self.csi_index_file))\n\n  def test_vcf_query_tbx(self):\n    tabix.build_index(self.output_file)\n    self.input_reader = vcf.VcfReader(self.input_file)\n    self.output_reader = vcf.VcfReader(self.output_file)\n\n    range1 = ranges.parse_literal(\'chr3:100,000-500,000\')\n    self.assertEqual(\n        list(self.input_reader.query(range1)),\n        list(self.output_reader.query(range1)))\n\n  def test_vcf_query_csi(self):\n    min_shift = 14\n    tabix.build_csi_index(self.output_file, min_shift)\n    self.input_reader = vcf.VcfReader(self.input_file)\n    self.output_reader = vcf.VcfReader(self.output_file)\n\n    range1 = ranges.parse_literal(\'chr3:100,000-500,000\')\n    self.assertEqual(\n        list(self.input_reader.query(range1)),\n        list(self.output_reader.query(range1)))\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/tfrecord.py,1,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""I/O for TFRecord files.\n\nUtilities for reading and writing TFRecord files, especially those containing\nserialized TensorFlow Example protocol buffers.\n""""""\n\n# Important: Please keep this module free of TensorFlow C++ extensions.\n# This makes it easy to build pure python packages for training that work with\n# CMLE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport heapq\n\nimport contextlib2\n\nfrom nucleus.io import genomics_reader\nfrom nucleus.io import genomics_writer\nfrom nucleus.io import sharded_file_utils\nfrom nucleus.protos import example_pb2\n\n\n# pylint: disable=invalid-name\ndef Reader(path, proto=None, compression_type=None):\n  """"""A TFRecordReader that defaults to tf.Example protos.""""""\n  if not proto:\n    proto = example_pb2.Example\n\n  return genomics_reader.TFRecordReader(\n      path, proto, compression_type=compression_type)\n\n\ndef Writer(path, compression_type=None):\n  """"""A convenience wrapper around genomics_writer.TFRecordWriter.""""""\n  return genomics_writer.TFRecordWriter(\n      path, compression_type=compression_type)\n# pylint: enable=invalid-name\n\n\n# TODO(thomaswc): Refactor all of the following (b/128406743).\ndef read_tfrecords(path, proto=None, max_records=None, compression_type=None):\n  """"""Yields the parsed records in a TFRecord file path.\n\n  Note that path can be sharded filespec (path@N) in which case this function\n  will read each shard in order; i.e. shard 0 will read each entry in order,\n  then shard 1, ...\n\n  Args:\n    path: String. A path to a TFRecord file containing protos.\n    proto: A proto class. proto.FromString() will be called on each serialized\n      record in path to parse it.\n    max_records: int >= 0 or None. Maximum number of records to read from path.\n      If None, the default, all records will be read.\n    compression_type: \'GZIP\', \'ZLIB\', \'\' (uncompressed), or None to autodetect\n      based on file extension.\n\n  Yields:\n    proto.FromString() values on each record in path in order.\n  """"""\n  if sharded_file_utils.is_sharded_file_spec(path):\n    paths = sharded_file_utils.generate_sharded_filenames(path)\n  else:\n    paths = [path]\n\n  i = 0\n  for path in paths:\n    with Reader(path, proto, compression_type=compression_type) as reader:\n      for record in reader.iterate():\n        i += 1\n        if max_records is not None and i > max_records:\n          return\n        yield record\n\n\ndef read_shard_sorted_tfrecords(path,\n                                key,\n                                proto=None,\n                                max_records=None,\n                                compression_type=None):\n  """"""Yields the parsed records in a TFRecord file path in sorted order.\n\n  The input TFRecord file must have each shard already in sorted order when\n  using the key function for comparison (but elements can be interleaved across\n  shards). Under those constraints, the elements will be yielded in a global\n  sorted order.\n\n  Args:\n    path: String. A path to a TFRecord-formatted file containing protos.\n    key: Callable. A function that takes as input a single instance of the proto\n      class and returns a value on which the comparison for sorted ordering is\n      performed.\n    proto: A proto class. proto.FromString() will be called on each serialized\n      record in path to parse it.\n    max_records: int >= 0 or None. Maximum number of records to read from path.\n      If None, the default, all records will be read.\n    compression_type: \'GZIP\', \'ZLIB\', \'\' (uncompressed), or None to autodetect\n      based on file extension.\n\n  Yields:\n    proto.FromString() values on each record in path in sorted order.\n  """"""\n  if sharded_file_utils.is_sharded_file_spec(path):\n    paths = sharded_file_utils.generate_sharded_filenames(path)\n  else:\n    paths = [path]\n\n  keyed_iterables = []\n  for path in paths:\n    protos = Reader(path, proto, compression_type=compression_type).iterate()\n    keyed_iterables.append(((key(elem), elem) for elem in protos))\n\n  for i, (_, value) in enumerate(heapq.merge(*keyed_iterables)):\n    if max_records is not None and i >= max_records:\n      return\n    yield value\n\n\ndef write_tfrecords(protos, output_path, compression_type=None):\n  """"""Writes protos to output_path.\n\n  This function writes serialized strings of each proto in protos to output_path\n  in their original order. If output_path is a sharded file (e.g., foo@2), this\n  function will write the protos spread out as evenly as possible among the\n  individual components of the sharded spec (e.g., foo-00000-of-00002 and\n  foo-00001-of-00002). Note that the order of records in the sharded files may\n  differ from the order in protos due to the striping.\n\n  Args:\n    protos: An iterable of protobufs. The objects we want to write out.\n    output_path: str. The filepath where we want to write protos.\n    compression_type: \'GZIP\', \'ZLIB\', \'\' (uncompressed), or None to autodetect\n      based on file extension.\n  """"""\n  if sharded_file_utils.is_sharded_file_spec(output_path):\n    with contextlib2.ExitStack() as stack:\n      _, n_shards, _ = sharded_file_utils.parse_sharded_file_spec(output_path)\n      writers = [\n          stack.enter_context(\n              Writer(sharded_file_utils.sharded_filename(\n                  output_path, i), compression_type=compression_type))\n          for i in range(n_shards)\n      ]\n      for i, proto in enumerate(protos):\n        writers[i % n_shards].write(proto)\n  else:\n    with Writer(output_path, compression_type=compression_type) as writer:\n      for proto in protos:\n        writer.write(proto)\n'"
nucleus/io/tfrecord_test.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for nucleus.io.tfrecord.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport types\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import tfrecord\n\nfrom nucleus.protos import reference_pb2\nfrom nucleus.testing import test_utils\n\n\nclass IOTest(parameterized.TestCase):\n\n  def write_test_protos(self, filename):\n    protos = [reference_pb2.ContigInfo(name=str(i)) for i in range(10)]\n    path = test_utils.test_tmpfile(filename)\n    tfrecord.write_tfrecords(protos, path)\n    return protos, path\n\n  @parameterized.parameters(\'foo.tfrecord\', \'foo@2.tfrecord\', \'foo@3.tfrecord\')\n  def test_read_write_tfrecords(self, filename):\n    protos, path = self.write_test_protos(filename)\n\n    # Create our generator of records from read_tfrecords.\n    reader = tfrecord.read_tfrecords(path, reference_pb2.ContigInfo)\n\n    # Make sure it\'s actually a generator.\n    self.assertEqual(type(reader), types.GeneratorType)\n\n    # Check the round-trip contents.\n    if \'@\' in filename:\n      # Sharded outputs are striped across shards, so order isn\'t preserved.\n      self.assertCountEqual(protos, reader)\n    else:\n      self.assertEqual(protos, list(reader))\n\n  @parameterized.parameters((filename, max_records)\n                            for max_records in [None, 0, 1, 3, 100]\n                            for filename in [\'foo.tfrecord\', \'foo@2.tfrecord\'])\n  def test_read_tfrecords_max_records(self, filename, max_records):\n    protos, path = self.write_test_protos(filename)\n\n    # Create our generator of records from read_tfrecords.\n    if max_records is None:\n      expected_n = len(protos)\n    else:\n      expected_n = min(max_records, len(protos))\n    actual = tfrecord.read_tfrecords(\n        path, reference_pb2.ContigInfo, max_records=max_records)\n    self.assertLen(list(actual), expected_n)\n\n  @parameterized.parameters(\'foo.tfrecord\', \'foo@2.tfrecord\', \'foo@3.tfrecord\')\n  def test_shard_sorted_tfrecords(self, filename):\n    protos, path = self.write_test_protos(filename)\n\n    # Create our generator of records.\n    key = lambda x: int(x.name)\n    reader = tfrecord.read_shard_sorted_tfrecords(\n        path, key=key, proto=reference_pb2.ContigInfo)\n\n    # Make sure it\'s actually a generator.\n    self.assertEqual(type(reader), types.GeneratorType)\n\n    # Check the round-trip contents.\n    contents = list(reader)\n    self.assertEqual(protos, contents)\n    self.assertEqual(contents, sorted(contents, key=key))\n\n  @parameterized.parameters((filename, max_records)\n                            for max_records in [None, 0, 1, 3, 100]\n                            for filename in [\'foo.tfrecord\', \'foo@2.tfrecord\'])\n  def test_shard_sorted_tfrecords_max_records(self, filename, max_records):\n    protos, path = self.write_test_protos(filename)\n\n    if max_records is None:\n      expected_n = len(protos)\n    else:\n      expected_n = min(max_records, len(protos))\n    # Create our generator of records from read_tfrecords.\n    actual = tfrecord.read_shard_sorted_tfrecords(\n        path,\n        key=lambda x: int(x.name),\n        proto=reference_pb2.ContigInfo,\n        max_records=max_records)\n    self.assertLen(list(actual), expected_n)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/vcf.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Classes for reading and writing VCF files.\n\nThe VCF format is described at\nhttps://samtools.github.io/hts-specs/VCFv4.3.pdf\n\nAPI for reading:\n\n```python\nfrom nucleus.io import vcf\n\nwith vcf.VcfReader(input_path) as reader:\n  for variant in reader:\n    print(variant)\n```\n\nAPI for writing:\n\n```python\nfrom nucleus.io import vcf\n\n# variants is an iterable of nucleus.genomics.v1.Variant protocol buffers.\nvariants = ...\n\nwith vcf.VcfWriter(output_path, header=header) as writer:\n  for variant in variants:\n    writer.write(variant)\n```\n\nThe class attempts to infer the file format (`TFRecord` vs VCF) from the file\npath provided to the constructor.\n\n1. For files that end with \'.tfrecord\' and \'.tfrecord.gz\' (a gzipped version),\n  a `TFRecord` file is assumed and is attempted to be read or written.\n\n2. For all other files, the VCF format will be used.\n\n  VCF format used in writing is inferred from file paths:\n    - ending in \'.bcf.gz\': BGZF compressed BCF format will be written;\n    - ending in \'.bcf\': uncompressed BCF format will be written;\n    - ending in \'.gz\' and not in \'.bcf.gz\': BGZP compressed VCF format will be\n        written;\n    - all other suffixes: uncompressed VCF format will be written.\n\n  VCF format used in reading is inferred from the contents of the file.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom nucleus.io import genomics_reader\nfrom nucleus.io import genomics_writer\nfrom nucleus.io.python import vcf_reader\nfrom nucleus.io.python import vcf_writer\nfrom nucleus.protos import variants_pb2\nfrom nucleus.util import ranges\nfrom nucleus.util import variant_utils\nfrom nucleus.util import vcf_constants\n\n\ndef _create_get_fn_cache(fields):\n  """"""Returns a dictionary from field to a callable that extracts its value.""""""\n  return {\n      field.id: vcf_constants.create_get_fn(field.type, field.number)\n      for field in fields\n  }\n\n\ndef _create_set_fn_cache(fields):\n  """"""Returns a dictionary from field to a callable that sets its value.""""""\n  return {field.id: vcf_constants.SET_FN_LOOKUP[field.type] for field in fields}\n\n\nclass VcfHeaderCache(object):\n  """"""This class creates a cache of accessors to structured fields in Variants.\n\n  The INFO and FORMAT fields within Variant protos are structured and typed,\n  with types defined by the corresponding VCF header. This cache object provides\n  provides {info,format}_field_{get,set}_fn functions that can be used to\n  extract information from the structured Variant protos based on the types\n  defined therein.\n\n  NOTE: Users should not need to interact with this class at all. It is used\n  by the variant_utils.{get,set}_info and variantcall_utils.{get,set}_format\n  functions for interacting with the INFO and FORMAT fields in a Variant proto.\n  """"""\n\n  def __init__(self, header):\n    """"""Initializer.\n\n    Args:\n      header: nucleus.genomics.v1.VcfHeader proto. Used to define the accessor\n        functions needed.\n    """"""\n    if header is None:\n      header = variants_pb2.VcfHeader()\n    self._info_get_cache = _create_get_fn_cache(header.infos)\n    self._info_set_cache = _create_set_fn_cache(header.infos)\n    self._format_get_cache = _create_get_fn_cache(header.formats)\n    self._format_set_cache = _create_set_fn_cache(header.formats)\n\n  def info_field_get_fn(self, field_name):\n    """"""Returns a callable that extracts the given INFO field based on its type.\n\n    Args:\n      field_name: str. The INFO field name of interest, e.g. \'AA\', \'DB\', \'AF\'.\n\n    Returns:\n      A callable used to extract the given INFO field from a Variant proto.\n    """"""\n    return self._info_get_cache[field_name]\n\n  def info_field_set_fn(self, field_name):\n    """"""Returns a callable that sets the given INFO field based on its type.""""""\n    return self._info_set_cache[field_name]\n\n  def format_field_get_fn(self, field_name):\n    """"""Returns a callable that gets the given FORMAT field based on its type.""""""\n    return self._format_get_cache[field_name]\n\n  def format_field_set_fn(self, field_name):\n    """"""Returns a callable that sets the given FORMAT field based on its type.""""""\n    return self._format_set_cache[field_name]\n\n\nclass NativeVcfReader(genomics_reader.GenomicsReader):\n  """"""Class for reading from native VCF files.\n\n  Most users will want to use VcfReader instead, because it dynamically\n  dispatches between reading native VCF files and TFRecord files based\n  on the filename\'s extensions.\n  """"""\n\n  def __init__(self,\n               input_path,\n               excluded_info_fields=None,\n               excluded_format_fields=None,\n               store_gl_and_pl_in_info_map=False,\n               header=None):\n    """"""Initializer for NativeVcfReader.\n\n    Args:\n      input_path: str. The path to the VCF file to read.\n      excluded_info_fields: list(str). A list of INFO field IDs that should not\n        be parsed into the Variants. If None, all INFO fields are included.\n      excluded_format_fields: list(str). A list of FORMAT field IDs that should\n        not be parsed into the Variants. If None, all FORMAT fields are\n        included.\n      store_gl_and_pl_in_info_map: bool. If True, the ""GL"" and ""PL"" FORMAT\n        fields are stored in the VariantCall.info map rather than as top-level\n        values in the VariantCall.genotype_likelihood field.\n      header: If not None, specifies the variants_pb2.VcfHeader. The file at\n        input_path must not contain any header information.\n    """"""\n    super(NativeVcfReader, self).__init__()\n\n    options = variants_pb2.VcfReaderOptions(\n        excluded_info_fields=excluded_info_fields,\n        excluded_format_fields=excluded_format_fields,\n        store_gl_and_pl_in_info_map=store_gl_and_pl_in_info_map)\n    if header is not None:\n      self._reader = vcf_reader.VcfReader.from_file_with_header(\n          input_path.encode(\'utf8\'), options, header)\n    else:\n      self._reader = vcf_reader.VcfReader.from_file(\n          input_path.encode(\'utf8\'), options)\n\n    self.header = self._reader.header\n    self.field_access_cache = VcfHeaderCache(self.header)\n\n  def iterate(self):\n    """"""Returns an iterable of Variant protos in the file.""""""\n    return self._reader.iterate()\n\n  def query(self, region):\n    """"""Returns an iterator for going through variants in the region.""""""\n    return self._reader.query(region)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._reader.__exit__(exit_type, exit_value, exit_traceback)\n\n  @property\n  def c_reader(self):\n    """"""Returns the underlying C++ reader.""""""\n    return self._reader\n\n\nclass VcfReader(genomics_reader.DispatchingGenomicsReader):\n  """"""Class for reading Variant protos from VCF or TFRecord files.""""""\n\n  def _native_reader(self, input_path, **kwargs):\n    return NativeVcfReader(input_path, **kwargs)\n\n  def _record_proto(self):\n    return variants_pb2.Variant\n\n  def _post_init_hook(self):\n    # Initialize field_access_cache.  If we are dispatching to a\n    # NativeVcfReader, we use its field_access_cache. Otherwise, we need to\n    # create a new one.\n    self.field_access_cache = getattr(\n        self._reader, \'field_access_cache\', VcfHeaderCache(self.header))\n\n  @property\n  def c_reader(self):\n    """"""Returns the underlying C++ reader.\n\n    Note that the C++ reader might be a VcfReader or it might be a\n    TFRecordReader, depending on the input_path\'s extension.\n    """"""\n    return self._reader.c_reader\n\n\nclass NativeVcfWriter(genomics_writer.GenomicsWriter):\n  """"""Class for writing to native VCF files.\n\n  Most users will want VcfWriter, which will write to either native VCF\n  files or TFRecords files, based on the output filename\'s extensions.\n  """"""\n\n  def __init__(self,\n               output_path,\n               header=None,\n               round_qualities=False,\n               excluded_info_fields=None,\n               excluded_format_fields=None,\n               retrieve_gl_and_pl_from_info_map=False,\n               exclude_header=False):\n    """"""Initializer for NativeVcfWriter.\n\n    Args:\n      output_path: str. The path to which to write the VCF file.\n      header: nucleus.genomics.v1.VcfHeader. The header that defines all\n        information germane to the constituent variants. This includes contigs,\n        FILTER fields, INFO fields, FORMAT fields, samples, and all other\n        structured and unstructured header lines.\n      round_qualities: bool. If True, the QUAL field is rounded to one point\n        past the decimal.\n      excluded_info_fields: list(str). A list of INFO field IDs that should not\n        be written to the output. If None, all INFO fields are included.\n      excluded_format_fields: list(str). A list of FORMAT field IDs that should\n        not be written to the output. If None, all FORMAT fields are included.\n      retrieve_gl_and_pl_from_info_map: bool. If True, the ""GL"" and ""PL"" FORMAT\n        fields are retrieved from the VariantCall.info map rather than from the\n        top-level value in the VariantCall.genotype_likelihood field.\n      exclude_header: bool. If True, write a headerless VCF.\n    """"""\n    super(NativeVcfWriter, self).__init__()\n\n    if header is None:\n      header = variants_pb2.VcfHeader()\n    writer_options = variants_pb2.VcfWriterOptions(\n        round_qual_values=round_qualities,\n        excluded_info_fields=excluded_info_fields,\n        excluded_format_fields=excluded_format_fields,\n        retrieve_gl_and_pl_from_info_map=retrieve_gl_and_pl_from_info_map,\n        exclude_header=exclude_header,\n    )\n    self._writer = vcf_writer.VcfWriter.to_file(output_path, header,\n                                                writer_options)\n    self.field_access_cache = VcfHeaderCache(header)\n\n  def write(self, proto):\n    self._writer.write(proto)\n\n  def __exit__(self, exit_type, exit_value, exit_traceback):\n    self._writer.__exit__(exit_type, exit_value, exit_traceback)\n\n\nclass VcfWriter(genomics_writer.DispatchingGenomicsWriter):\n  """"""Class for writing Variant protos to VCF or TFRecord files.""""""\n\n  def _native_writer(self,\n                     output_path,\n                     header,\n                     round_qualities=False,\n                     excluded_info_fields=None,\n                     excluded_format_fields=None,\n                     retrieve_gl_and_pl_from_info_map=False,\n                     exclude_header=False):\n    return NativeVcfWriter(\n        output_path,\n        header=header,\n        round_qualities=round_qualities,\n        excluded_info_fields=excluded_info_fields,\n        excluded_format_fields=excluded_format_fields,\n        retrieve_gl_and_pl_from_info_map=retrieve_gl_and_pl_from_info_map,\n        exclude_header=exclude_header)\n\n  def _post_init_hook(self):\n    # Initialize field_access_cache.  If we are dispatching to a\n    # NativeVcfWriter, we use its field_access_cache.  Otherwise, we\n    # need to create a new one.\n    self.field_access_cache = getattr(\n        self._writer, \'field_access_cache\', VcfHeaderCache(self.header))\n\n\nclass InMemoryVcfReader(genomics_reader.GenomicsReader):\n  """"""Class for ""reading"" Variant protos from an in-memory cache of variants.\n\n  ```python\n  from nucleus.io import vcf\n  from nucleus.protos import variants_pb2\n\n  variants = [... Variant protos ...]\n  header = variants_pb2.VcfHeader()\n\n  with vcf.InMemoryVcfReader(variants, header) as reader:\n    for variant in reader:\n      print(variant)\n  ```\n\n  This class accepts a collection of variants and optionally a header and\n  provides all of the standard API functions of VcfReader but instead of\n  fetching variants from a file the variants are queried from an in-memory cache\n  of variant protos.\n\n  Note that the input variants provided to this class aren\'t checked in any way,\n  and their ordering determines the order of variants emitted by this class for\n  the iterate() and query() operations. This is intentional, to make this class\n  easy to use for testing where you often want to use less-than-perfectly formed\n  inputs. In order to fully meet the contract of a standard VcfReader, variants\n  should be sorted by their contig ordering and then by their start and finally\n  by their ends.\n\n  Implementation note:\n    The current implementation will be very slow for query() if the provided\n    cache of variants is large, as we do a O(n) search to collect all of the\n    overlapping variants for each query. There are several straightforward\n    optimizations to do if we need/want to scale this up. (a) sort the variants\n    and use a binary search to find overlapping variants (b) partition the\n    variants by contig, so we have dict[contig] => [variants on contig], which\n    allows us to completely avoid considering any variants on any other contigs.\n    Neither of these optimizations are worth it if len(variants) is small, but\n    it may be worth considering if we want to use this functionality with a\n    large number of variants.\n  """"""\n\n  def __init__(self, variants, header=None):\n    """"""Creates a VCFReader backed by a collection of variants.\n\n    Args:\n      variants: list of nucleus.genomics.v1.Variant protos we will ""read""\n        from.\n      header: a VCFHeader object to provide as a result to calls to self.header,\n        or None, indicating that we don\'t have a header associated with this\n        reader.\n    """"""\n    super(InMemoryVcfReader, self).__init__()\n    self.variants = list(variants)\n    self.header = header\n\n  def iterate(self):\n    return iter(self.variants)\n\n  def query(self, region):\n    return iter(\n        variant for variant in self.variants\n        if ranges.ranges_overlap(variant_utils.variant_range(variant), region)\n    )\n'"
nucleus/io/vcf_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus.io.vcf.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import gfile\nfrom nucleus.io import vcf\nfrom nucleus.protos import reference_pb2\nfrom nucleus.protos import struct_pb2\nfrom nucleus.protos import variants_pb2\nfrom nucleus.testing import test_utils\nfrom nucleus.util import ranges\n\n\nclass VcfHeaderCacheTests(parameterized.TestCase):\n  """"""Test the functionality of the VcfHeaderCache class.""""""\n\n  def setUp(self):\n    self.vcf_reader = vcf.VcfReader(\n        test_utils.genomics_core_testdata(\'test_sites.vcf\'))\n    self.cache = self.vcf_reader.field_access_cache\n\n  @parameterized.parameters(\n      \'DP\',\n      \'AF\',\n      \'END\',\n      \'ExcessHet\',\n      \'culprit\',\n  )\n  def test_valid_info_get_funcs(self, field_name):\n    fn = self.cache.info_field_get_fn(field_name)\n    self.assertTrue(callable(fn))\n\n  @parameterized.parameters(\n      \'DP\',\n      \'AF\',\n      \'END\',\n      \'ExcessHet\',\n      \'culprit\',\n      \'HaplotypeScore\',\n      \'InbreedingCoeff\',\n  )\n  def test_valid_info_set_funcs(self, field_name):\n    fn = self.cache.info_field_set_fn(field_name)\n    self.assertTrue(callable(fn))\n\n  def test_invalid_info_funcs(self):\n    with self.assertRaises(KeyError):\n      self.cache.info_field_get_fn(\'RGQ\')\n    with self.assertRaises(KeyError):\n      self.cache.info_field_set_fn(\'PID\')\n\n  @parameterized.parameters(\n      \'AD\',\n      \'DP\',\n      \'PID\',\n      \'RGQ\',\n  )\n  def test_valid_format_get_funcs(self, field_name):\n    fn = self.cache.format_field_get_fn(field_name)\n    self.assertTrue(callable(fn))\n\n  @parameterized.parameters(\n      \'AD\',\n      \'DP\',\n      \'PID\',\n      \'RGQ\',\n  )\n  def test_valid_format_set_funcs(self, field_name):\n    fn = self.cache.format_field_set_fn(field_name)\n    self.assertTrue(callable(fn))\n\n  def test_invalid_format_funcs(self):\n    with self.assertRaises(KeyError):\n      self.cache.format_field_get_fn(\'culprit\')\n    with self.assertRaises(KeyError):\n      self.cache.format_field_set_fn(\'ExcessHet\')\n\n\nclass VcfReaderTests(absltest.TestCase):\n  """"""Test the iteration functionality provided by vcf.VcfReader.""""""\n\n  def setUp(self):\n    self.sites_reader = vcf.VcfReader(\n        test_utils.genomics_core_testdata(\'test_sites.vcf\'))\n\n    self.samples_reader = vcf.VcfReader(\n        test_utils.genomics_core_testdata(\'test_samples.vcf.gz\'))\n\n  def test_vcf_iterate(self):\n    self.assertEqual(test_utils.iterable_len(self.sites_reader.iterate()), 5)\n\n  def test_vcf_query(self):\n    range1 = ranges.parse_literal(\'chr3:100,000-500,000\')\n    self.assertEqual(\n        test_utils.iterable_len(self.samples_reader.query(range1)), 4)\n\n  def test_vcf_iter(self):\n    n = 0\n    for _ in self.sites_reader:\n      n += 1\n    self.assertEqual(n, 5)\n\n  def test_fail_multiple_concurrent_iterations(self):\n    range1 = ranges.parse_literal(\'chr3:100,000-500,000\')\n    reads = self.samples_reader.query(range1)\n    for read in reads:\n      pass\n\n    r2 = self.samples_reader.query(range1)\n    with self.assertRaisesRegexp(ValueError, \'No underlying iterable. This \'):\n      next(r2)\n\n  def test_c_reader(self):\n    self.assertNotEqual(self.sites_reader.c_reader, 0)\n    self.assertNotEqual(self.samples_reader.c_reader, 0)\n\n    tfrecord_reader = vcf.VcfReader(\n        test_utils.genomics_core_testdata(\'test_samples.vcf.golden.tfrecord\'))\n    self.assertNotEqual(tfrecord_reader.c_reader, 0)\n\n\nclass VcfReaderInputTests(absltest.TestCase):\n  """"""Tests VcfReader behavior on specific inputs.""""""\n\n  def test_header_format_mixed_order(self):\n    """"""Tests reading a VCF with unconventional FORMAT field definition.\n\n    Tests reading a VCF in which the properties of the format\n    fields are defined in mixed order in the header. For example,\n\n    ##FORMAT=<ID=GT,Type=String,Number=1,Description=""GT description"">\n\n    (In normal VCFs ""Number"" should come before ""Type"".)\n    """"""\n    with vcf.VcfReader(\n        test_utils.genomics_core_testdata(\n            \'header_format_mixed_order.vcf\')) as vreader:\n      formats = vreader.header.formats\n      variants = list(vreader)\n    self.assertLen(formats, 1)\n    self.assertEqual(formats[0].id, \'GT\')\n    self.assertEqual(formats[0].number, \'1\')\n    self.assertEqual(formats[0].type, \'String\')\n    self.assertEqual(formats[0].description, \'GT description\')\n    self.assertLen(variants, 2)\n    self.assertEqual(variants[0].calls[0].genotype, [0, 1])\n    self.assertEqual(variants[1].calls[0].genotype, [1, 1])\n\n\ndef _format_expected_variant(ref, alts, format_spec, *samples):\n  base = [\'20\', 1, \'.\', ref, alts, 0, \'.\', \'.\', format_spec]\n  return base + list(samples)\n\n\ndef _format_test_variant(alleles, call_infos):\n  variant = test_utils.make_variant(chrom=\'20\', start=0, alleles=alleles)\n  for i, call_info in enumerate(call_infos):\n    call = variant.calls.add(call_set_name=\'sample\' + str(i))\n    for key, value in call_info.items():\n      if not isinstance(value, (list, tuple)):\n        value = [value]\n      call.info[key].values.extend(\n          [struct_pb2.Value(int_value=v) for v in value])\n  return variant\n\n\nclass VcfWriterTests(parameterized.TestCase):\n  """"""Tests for VcfWriter.""""""\n\n  def assertWrittenVCFRecordsEqual(self, path, expected_lines):\n\n    def cleanup_line(line):\n      if isinstance(line, (list, tuple)):\n        return \'\\t\'.join(str(x) for x in line)\n      else:\n        return line\n\n    expected_lines = [cleanup_line(line) for line in expected_lines]\n    with gfile.Open(path, \'r\') as fin:\n      self.assertEqual([\n          line.strip() for line in fin.readlines() if not line.startswith(\'#\')\n      ], expected_lines)\n\n  def write_variant_to_tempfile(self, variant):\n    output_path = test_utils.test_tmpfile(\'test.vcf\')\n    header = variants_pb2.VcfHeader(\n        contigs=[reference_pb2.ContigInfo(name=\'20\')],\n        sample_names=[call.call_set_name for call in variant.calls],\n        formats=[\n            variants_pb2.VcfFormatInfo(\n                id=\'DP\', number=\'1\', type=\'Integer\', description=\'Read depth\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'AD\',\n                number=\'R\',\n                type=\'Integer\',\n                description=\'Read depth for each allele\')\n        ])\n    writer = vcf.VcfWriter(output_path, header=header)\n    with writer:\n      writer.write(variant)\n    return output_path\n\n  @parameterized.parameters(\n      # Check that our DP field is getting written out properly.\n      (_format_test_variant([\'A\', \'T\'], [{\n          \'DP\': 1\n      }, {\n          \'DP\': 2\n      }]), _format_expected_variant(\'A\', \'T\', \'DP\', \'1\', \'2\')),\n      # Checks that we get the missing value when DP is missing in some samples.\n      (_format_test_variant([\'A\', \'T\'], [{\n          \'DP\': 1\n      }, {}]), _format_expected_variant(\'A\', \'T\', \'DP\', \'1\', \'.\')),\n      (_format_test_variant([\'A\', \'T\'], [{}, {\n          \'DP\': 2\n      }]), _format_expected_variant(\'A\', \'T\', \'DP\', \'.\', \'2\')),\n  )\n  def test_single_value_format_field(self, variant, expected_vcf_line):\n    self.assertWrittenVCFRecordsEqual(\n        self.write_variant_to_tempfile(variant), [expected_vcf_line])\n\n  @parameterized.parameters(\n      # Check that our AD field is getting written correctly.\n      (_format_test_variant([\'A\', \'T\'], [{\n          \'AD\': [0, 1]\n      }, {\n          \'AD\': [2, 3]\n      }]), _format_expected_variant(\'A\', \'T\', \'AD\', \'0,1\', \'2,3\')),\n      (_format_test_variant([\'A\', \'T\'], [{}, {\n          \'AD\': [2, 3]\n      }]), _format_expected_variant(\'A\', \'T\', \'AD\', \'.\', \'2,3\')),\n      (_format_test_variant([\'A\', \'T\'], [{\n          \'AD\': [0, 1]\n      }, {}]), _format_expected_variant(\'A\', \'T\', \'AD\', \'0,1\', \'.\')),\n      # Let\'s try a tri-allelic site where we have 3 AD values / sample.\n      (_format_test_variant([\'A\', \'T\', \'C\'], [{\n          \'AD\': [0, 1, 2]\n      }, {\n          \'AD\': [4, 5, 6]\n      }]), _format_expected_variant(\'A\', \'T,C\', \'AD\', \'0,1,2\', \'4,5,6\')),\n      # Check that we handle missing values properly.\n      (_format_test_variant([\'A\', \'T\', \'C\'], [{\n          \'AD\': [0, 1, 2]\n      }, {}]), _format_expected_variant(\'A\', \'T,C\', \'AD\', \'0,1,2\', \'.\')),\n      (_format_test_variant([\'A\', \'T\', \'C\'], [{}, {\n          \'AD\': [4, 5, 6]\n      }]), _format_expected_variant(\'A\', \'T,C\', \'AD\', \'.\', \'4,5,6\')),\n  )\n  def test_multi_value_format_field(self, variant, expected_vcf_line):\n    self.assertWrittenVCFRecordsEqual(\n        self.write_variant_to_tempfile(variant), [expected_vcf_line])\n\n  @parameterized.parameters(\n      # Now let\'s combine some AD and DP fields.\n      (_format_test_variant([\'A\', \'T\', \'C\'], [{\n          \'DP\': 3,\n          \'AD\': [0, 1, 2]\n      }, {\n          \'DP\': 12,\n          \'AD\': [3, 4, 5]\n      }]), _format_expected_variant(\'A\', \'T,C\', \'DP:AD\', \'3:0,1,2\', \'12:3,4,5\')\n      ),\n      (_format_test_variant([\'A\', \'T\', \'C\'], [{\n          \'DP\': 3\n      }, {\n          \'AD\': [3, 4, 5]\n      }]), _format_expected_variant(\'A\', \'T,C\', \'DP:AD\', \'3:.\', \'.:3,4,5\')),\n  )\n  def test_multiple_format_fields(self, variant, expected_vcf_line):\n    self.assertWrittenVCFRecordsEqual(\n        self.write_variant_to_tempfile(variant), [expected_vcf_line])\n\n\nclass VcfWriterHeaderlessTests(absltest.TestCase):\n  """"""Tests for VcfWriter with exclude_header=True.""""""\n\n  def test_headerless_vcf(self):\n    """"""Writes a headerless vcf and reads it back out.""""""\n    test_vcf = test_utils.genomics_core_testdata(\'test_sites.vcf\')\n    output_vcf = test_utils.test_tmpfile(\'output.vcf\')\n    expected_variants = []\n    with vcf.VcfReader(test_vcf) as reader:\n      with vcf.VcfWriter(\n          output_vcf, header=reader.header, exclude_header=True) as writer:\n        for record in reader:\n          expected_variants.append(record)\n          writer.write(record)\n\n      with vcf.VcfReader(output_vcf, header=reader.header) as actual_reader:\n        self.assertEqual(expected_variants, list(actual_reader))\n\n\nclass VcfRoundtripTests(parameterized.TestCase):\n  """"""Test the ability to round-trip VCF files.""""""\n\n  def setUp(self):\n    self.header = (\n        \'##fileformat=VCFv4.2\\n##FILTER=<ID=PASS,Description=""All filters \'\n        \'passed"">\\n##INFO=<ID=DB,Number=0,Type=Flag,Description=""In \'\n        \'dbSNP"">\\n##INFO=<ID=MIN_DP,Number=1,Type=Integer,Description=""Min \'\n        \'DP"">\\n##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">\\n##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic\'\n        \' depths"">\\n##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read \'\n        \'depth"">\\n##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Genotype \'\n        \'likelihood,Phred-encoded"">\\n##contig=<ID=chr1,length=248956422>\\n#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\tFORMAT\\tS1\\tS2\\n\')\n    self.record_format_strings = [\n        \'chr1\\t13613\\t.\\tT\\tA\\t39.88\\tPASS\\t{info}\\t{fmt}\\t0/1{efmts1}\\t1/1{efmts2}\\n\',\n        \'chr1\\t13813\\trs1\\tT\\tG\\t90.28\\tPASS\\t{info}\\t{fmt}\\t1/1{efmts1}\\t0|1{efmts2}\\n\',\n        \'chr1\\t13838\\t.\\tC\\tT\\t62.74\\tPASS\\t{info}\\t{fmt}\\t0/1{efmts1}\\t0/1{efmts2}\\n\',\n    ]\n\n  @parameterized.parameters(\n      dict(\n          expected_infos=[\'DB;MIN_DP=4\', \'MIN_DP=15\', \'DB;MIN_DP=10\'],\n          expected_fmt=\'GT:AD:DP:PL\',\n          expected_fmt1=[\n              \':1,3:4:10,5,0\', \':11,13:24:55,0,50\', \':5,5:10:20,0,20\'\n          ],\n          expected_fmt2=[\n              \':1,19:20:100,90,0\', \':7,8:15:15,0,12\', \':.:10:0,0,50\'\n          ],\n      ),\n      dict(\n          expected_infos=[\'DB\', \'.\', \'DB\'],\n          expected_fmt=\'GT:AD:DP:PL\',\n          expected_fmt1=[\n              \':1,3:4:10,5,0\', \':11,13:24:55,0,50\', \':5,5:10:20,0,20\'\n          ],\n          expected_fmt2=[\n              \':1,19:20:100,90,0\', \':7,8:15:15,0,12\', \':.:10:0,0,50\'\n          ],\n          reader_excluded_info=[\'MIN_DP\'],\n      ),\n      dict(\n          expected_infos=[\'DB\', \'.\', \'DB\'],\n          expected_fmt=\'GT\',\n          expected_fmt1=[\'\', \'\', \'\'],\n          expected_fmt2=[\'\', \'\', \'\'],\n          reader_excluded_info=[\'MIN_DP\'],\n          reader_excluded_format=[\'AD\', \'DP\', \'PL\'],\n      ),\n      dict(\n          expected_infos=[\'DB\', \'.\', \'DB\'],\n          expected_fmt=\'GT\',\n          expected_fmt1=[\'\', \'\', \'\'],\n          expected_fmt2=[\'\', \'\', \'\'],\n          writer_excluded_info=[\'MIN_DP\'],\n          writer_excluded_format=[\'AD\', \'DP\', \'PL\'],\n      ),\n      dict(\n          expected_infos=[\'DB\', \'.\', \'DB\'],\n          expected_fmt=\'GT\',\n          expected_fmt1=[\'\', \'\', \'\'],\n          expected_fmt2=[\'\', \'\', \'\'],\n          reader_excluded_info=[\'MIN_DP\'],\n          reader_excluded_format=[\'AD\'],\n          writer_excluded_info=[\'MIN_DP\'],\n          writer_excluded_format=[\'DP\', \'PL\'],\n      ),\n  )\n  def test_roundtrip(self,\n                     expected_infos,\n                     expected_fmt,\n                     expected_fmt1,\n                     expected_fmt2,\n                     reader_excluded_info=None,\n                     reader_excluded_format=None,\n                     writer_excluded_info=None,\n                     writer_excluded_format=None):\n    expected_records = [\n        record.format(info=info, fmt=expected_fmt, efmts1=e1,\n                      efmts2=e2) for record, info, e1, e2 in zip(\n                          self.record_format_strings, expected_infos,\n                          expected_fmt1, expected_fmt2)\n    ]\n    expected = self.header + \'\'.join(expected_records)\n    for info_map_pl in [False, True]:\n      with vcf.VcfReader(\n          test_utils.genomics_core_testdata(\'test_py_roundtrip.vcf\'),\n          excluded_info_fields=reader_excluded_info,\n          excluded_format_fields=reader_excluded_format,\n          store_gl_and_pl_in_info_map=info_map_pl) as reader:\n        records = list(reader.iterate())\n        output_path = test_utils.test_tmpfile(\n            \'test_roundtrip_tmpfile_{}.vcf\'.format(info_map_pl))\n        with vcf.VcfWriter(\n            output_path,\n            header=reader.header,\n            excluded_info_fields=writer_excluded_info,\n            excluded_format_fields=writer_excluded_format,\n            retrieve_gl_and_pl_from_info_map=info_map_pl) as writer:\n          for record in records:\n            writer.write(record)\n\n      with open(output_path) as f:\n        actual = f.read()\n      self.assertEqual(actual, expected)\n\n\nclass InMemoryVcfReaderTests(parameterized.TestCase):\n  """"""Test the functionality provided by vcf.InMemoryVcfReader.""""""\n\n  def setUp(self):\n    self.variants = [\n        test_utils.make_variant(chrom=\'1\', start=10),\n        test_utils.make_variant(chrom=\'1\', start=20),\n        test_utils.make_variant(chrom=\'1\', start=30),\n        test_utils.make_variant(chrom=\'2\', start=25),\n        test_utils.make_variant(chrom=\'2\', start=55),\n        test_utils.make_variant(chrom=\'3\', start=10),\n    ]\n    self.header = variants_pb2.VcfHeader(\n        contigs=[\n            reference_pb2.ContigInfo(name=\'1\', n_bases=100),\n            reference_pb2.ContigInfo(name=\'2\', n_bases=100),\n            reference_pb2.ContigInfo(name=\'3\', n_bases=100),\n            reference_pb2.ContigInfo(name=\'4\', n_bases=100),\n        ],\n        filters=[],\n        sample_names=[\'NA12878\'])\n    self.reader = vcf.InMemoryVcfReader(\n        self.variants, self.header)\n\n  def test_iterate(self):\n    """"""Tests that iterate returns an iterable containing our variants.""""""\n    self.assertEqual(list(self.reader.iterate()), self.variants)\n\n  def test_header(self):\n    """"""Tests that the reader provides us back the header we gave it.""""""\n    self.assertEqual(self.reader.header, self.header)\n\n  @parameterized.parameters(\n      dict(query=\'1\', expected_variant_indices=[0, 1, 2]),\n      dict(query=\'2\', expected_variant_indices=[3, 4]),\n      dict(query=\'3\', expected_variant_indices=[5]),\n      dict(query=\'4\', expected_variant_indices=[]),\n      dict(query=\'1:1-15\', expected_variant_indices=[0]),\n      dict(query=\'1:1-25\', expected_variant_indices=[0, 1]),\n      dict(query=\'1:1-35\', expected_variant_indices=[0, 1, 2]),\n      dict(query=\'1:15-35\', expected_variant_indices=[1, 2]),\n      dict(query=\'1:25-35\', expected_variant_indices=[2]),\n  )\n  def test_query(self, query, expected_variant_indices):\n    range1 = ranges.parse_literal(query, ranges.contigs_dict(\n        self.header.contigs))\n    self.assertEqual(\n        list(self.reader.query(range1)),\n        [self.variants[i] for i in expected_variant_indices])\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/pip_package/setup.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Fake setup.py module for installing Nucleus.\n\nUsually, setup.py is invoked twice:  first, to build the pip package\nand second to install it.\n\nThis setup.py is only used for installation; build_pip_package.sh is\nused to create the package.  We do it this way because we need our\npackage to include symbolic links, which normal setup.py doesn\'t\nsupport.\n\nFor the same reason, this setup.py is not implemented using setuptools.\nInstead, we directly implement the four commands run by pip install\n(https://pip.pypa.io/en/stable/reference/pip_install/#id46):\n  * setup.py egg_info [--egg-base XXX]\n  * setup.py install --record XXX [--single-version-externally-managed]\n          [--root XXX] [--compile|--no-compile] [--install-headers XXX]\n  * setup.py bdist_wheel -d XXX\n  * setup.py clean\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom distutils import dist\nimport distutils.command.install as dist_install\nimport glob\nimport os\nimport shutil\nimport sys\n\n\ndef touch(fname):\n  open(fname, \'w+\').close()\n\n\ndef find_destination(is_user):\n  """"""Returns the directory we are supposed to install into.""""""\n  install_cmd = dist_install.install(dist.Distribution())\n  install_cmd.finalize_options()\n  if is_user:\n    return install_cmd.install_usersite\n  else:\n    return install_cmd.install_platlib\n\n\ndef main():\n  if len(sys.argv) < 2:\n    print(\'Missing command\')\n    sys.exit(1)\n\n  cmd = sys.argv[1]\n  args = sys.argv[2:]\n\n  if cmd == \'egg_info\':\n    egg_srcs = glob.glob(\'google_nucleus-*-py*.egg-info\')\n    if not egg_srcs:\n      print(\'Could not find source .egg-info directory\')\n      sys.exit(1)\n    egg_src = egg_srcs[0]\n\n    egg_dir = \'google_nucleus.egg-info\'\n    if len(args) > 1 and args[0] == \'--egg-base\':\n      egg_dir = os.path.join(args[1], egg_dir)\n\n    print(\'Copying egg-info from \', egg_src, \' to \', egg_dir)\n    shutil.copytree(egg_src, egg_dir)\n    sys.exit(0)\n\n  if cmd == \'install\':\n    destination = find_destination(\'--user\' in args)\n\n    record_file = \'install-record.txt\'\n    if \'--record\' in args:\n      i = args.index(\'--record\')\n      record_file = args[i+1]\n\n    print(\'Removing old protobuf files\')\n    os.system(\'rm -fR \' + destination + \'/google/protobuf\')\n\n    print(\'Installing Nucleus to \' + destination\n          + \' with record file at \' + record_file)\n    os.system(\'cp -R -v google nucleus \' + destination\n              + "" | awk \'{print substr($3,2,length($3)-2)}\' > "" + record_file)\n\n    sys.exit(0)\n\n  if cmd == \'bdist_wheel\':\n    print(\'This package does not support wheel creation.\')\n    sys.exit(1)\n\n  if cmd == \'clean\':\n    # Nothing to do\n    sys.exit(0)\n\n  print(\'Unknown command: \', cmd)\n  sys.exit(1)\n\n\nif __name__ == \'__main__\':\n  main()\n\n'"
nucleus/protos/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
nucleus/testing/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
nucleus/testing/protobuf_implementation_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test that our protobuf implementation behaves as we\'d expect.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\n\nfrom google.protobuf.internal import api_implementation\n# This next import is unused, but we are testing that any program\n# which includes a Nucleus library uses the cpp protobuf\n# implementation.\n# pylint: disable=unused-import\nfrom nucleus.io import sam\n\n\nclass ProtobufImplementationTest(absltest.TestCase):\n  """"""Checks that our protobufs have the properties we expect.""""""\n\n  def test_protobuf_uses_fast_cpp(self):\n    """"""Checks that we are using the fast cpp version of python protobufs.""""""\n    self.assertEqual(api_implementation.Type(), \'cpp\')\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/testing/test_utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities to help with testing code.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import flags\nfrom absl.testing import absltest\nimport six\n\nfrom nucleus.io import gfile\nfrom nucleus.protos import position_pb2\nfrom nucleus.protos import reads_pb2\nfrom nucleus.protos import struct_pb2\nfrom nucleus.protos import variants_pb2\nfrom nucleus.util import cigar as _cigar\n\nFLAGS = flags.FLAGS\n\n# In the OSS version these will be \'\'.\nDATADIR = \'\'\nDEFAULT_WORKSPACE = \'\'\n\n# In the OSS version this becomes \'nucleus/testdata\'\nRELATIVE_TESTDATA_PATH = \'nucleus/testdata\'\n\n\ndef genomics_testdata(path, datadir=DATADIR):\n  """"""Gets the path to a testdata file in genomics at relative path.\n\n  Args:\n    path: A path to a testdata file *relative* to the genomics root\n      directory. For example, if you have a test file in\n      ""datadir/nucleus/testdata/foo.txt"", path should be\n      ""nucleus/testdata/foo.txt"" to get a path to it.\n    datadir: The path of the genomics root directory *relative* to\n      the testing source directory.\n\n  Returns:\n    The absolute path to a testdata file.\n  """"""\n  if hasattr(FLAGS, \'test_srcdir\'):\n    # Google code uses FLAG.test_srcdir\n    # TensorFlow uses a routine googletest.test_src_dir_path.\n    test_workspace = os.environ.get(\'TEST_WORKSPACE\', DEFAULT_WORKSPACE)\n    test_srcdir = os.path.join(FLAGS.test_srcdir, test_workspace)\n  else:\n    # In bazel TEST_SRCDIR points at the runfiles directory, and\n    # TEST_WORKSPACE names the workspace.  We need to append to the\n    # path the name of the workspace in order to get to the root of our\n    # source tree.\n    test_workspace = os.environ[\'TEST_WORKSPACE\']\n    test_srcdir = os.path.join(os.environ[\'TEST_SRCDIR\'], test_workspace)\n  return os.path.join(test_srcdir, datadir, path)\n\n\n# TODO(mdepristo): is this necessary?\ndef genomics_core_testdata(filename):\n  """"""Gets the path to a testdata named filename in util/testdata.\n\n  Args:\n    filename: The name of a testdata file in the core genomics testdata\n      directory. For example, if you have a test file in\n      ""third_party/nucleus/util/testdata/foo.txt"", filename should be\n      ""foo.txt"" to get a path to it.\n\n  Returns:\n    The absolute path to a testdata file.\n  """"""\n  return genomics_testdata(os.path.join(RELATIVE_TESTDATA_PATH, filename))\n\n\ndef test_tmpfile(name, contents=None):\n  """"""Returns a path to a tempfile named name in the test_tmpdir.\n\n  Args:\n    name: str; the name of the file, should not contain any slashes.\n    contents: bytes, or None. If not None, tmpfile\'s contents will be set to\n      contents before returning the path.\n\n  Returns:\n    str path to a tmpfile with filename name in our test tmpfile directory.\n  """"""\n  path = os.path.join(absltest.get_default_test_tmpdir(), name)\n  if contents is not None:\n    with gfile.Open(path, \'wb\') as fout:\n      fout.write(contents)\n  return path\n\n\ndef set_list_values(list_value, values):\n  """"""Sets a ListValue to have the values in values.""""""\n\n  def format_one(value):\n    if isinstance(value, str):\n      return struct_pb2.Value(string_value=value)\n    elif isinstance(value, float):\n      return struct_pb2.Value(number_value=value)\n    elif isinstance(value, six.integer_types):\n      return struct_pb2.Value(int_value=value)\n    else:\n      raise ValueError(\'Unsupported type \', value)\n\n  del list_value.values[:]\n  list_value.values.extend([format_one(value) for value in values])\n  # list_value.values.extend(vals)\n\n\ndef make_variant(chrom=\'chr1\',\n                 start=10,\n                 alleles=None,\n                 end=None,\n                 filters=None,\n                 qual=None,\n                 gt=None,\n                 gq=None,\n                 sample_name=None,\n                 gls=None,\n                 is_phased=None):\n  """"""Creates a new Variant proto from args.\n\n  Args:\n    chrom: str. The reference_name for this variant.\n    start: int. The starting position of this variant.\n    alleles: list of str with at least one element. alleles[0] is the reference\n      bases and alleles[1:] will be set to alternate_bases of variant. If None,\n      defaults to [\'A\', \'C\'].\n    end: int or None. If not None, the variant\'s end will be set to this value.\n      If None, will be set to the start + len(reference_bases).\n    filters: str, list of str, or None. Sets the filters field of the variant to\n      this value if not None. If filters is a string `value`, this is equivalent\n      to an argument [`value`]. If None, no value will be assigned to the\n      filters field.\n    qual: int or None. The quality score for this variant. If None, no quality\n      score will be written in the Variant.\n    gt: A list of ints, or None. If present, creates a VariantCall in Variant\n      with genotype field set to this value. The special \'DEFAULT\' value, if\n      provided, will set the genotype to [0, 1]. This is the default behavior.\n    gq: int or None. If not None and gt is not None, we will add an this GQ\n      value to our VariantCall.\n    sample_name: str or None. If not None and gt is not None, sets the\n      call_set_name of our VariantCall to this value.\n    gls: array-list of float, or None. If not None and gt is not None, sets the\n      genotype_likelihoods of our VariantCall to this value.\n    is_phased: bool. Indicates whether a VariantCall should be phased.\n\n  Returns:\n    nucleus.genomics.v1.Variant proto.\n  """"""\n  return make_variant_multiple_calls(\n      chrom=chrom,\n      start=start,\n      alleles=alleles,\n      end=end,\n      filters=filters,\n      qual=qual,\n      gts=None if gt is None else [gt],\n      gqs=None if gq is None else [gq],\n      sample_names=None if sample_name is None else [sample_name],\n      glss=None if gls is None else [gls],\n      is_phased=None if is_phased is None else [is_phased])\n\n\ndef make_variant_multiple_calls(chrom=\'chr1\',\n                                start=10,\n                                alleles=None,\n                                end=None,\n                                filters=None,\n                                qual=None,\n                                gts=None,\n                                gqs=None,\n                                sample_names=None,\n                                glss=None,\n                                is_phased=None):\n  """"""Creates a new Variant proto from args that contains multi-sample calls.\n\n  Args:\n    chrom: str. The reference_name for this variant.\n    start: int. The starting position of this variant.\n    alleles: list of str with at least one element. alleles[0] is the reference\n      bases and alleles[1:] will be set to alternate_bases of variant. If None,\n        defaults to [\'A\', \'C\'].\n    end: int or None. If not None, the variant\'s end will be set to this value.\n      If None, will be set to the start + len(reference_bases).\n    filters: str, list of str, or None. Sets the filters field of the variant to\n      this value if not None. If filters is a string `value`, this is equivalent\n      to an argument [`value`]. If None, no value will be assigned to the\n      filters field.\n    qual: int or None. The quality score for this variant. If None, no quality\n      score will be written in the Variant.\n    gts: A list of lists of ints. For each list in this list, creates a\n      VariantCall in Variant with genotype field set to this value.\n    gqs: A list of ints or None. Must match the gts arg if specified. Sets the\n      GQ value of corresponding VariantCall.\n    sample_names: A list of strs or None. Must match the gts arg if specified.\n      Sets the call_set_name of the corresponding VariantCall.\n    glss: A list of array-lists of float, or None. Must match the gts arg if\n      specified. Sets the genotype_likelihoods of the corresponding VariantCall.\n    is_phased: list of bools. Must match the gts arg if specified. Indicates\n      whether the corresponding VariantCall should be phased.\n\n  Returns:\n    nucleus.genomics.v1.Variant proto.\n  """"""\n  if alleles is None:\n    alleles = [\'A\', \'C\']\n\n  if not end:\n    end = start + len(alleles[0])\n\n  variant = variants_pb2.Variant(\n      reference_name=chrom,\n      start=start,\n      end=end,\n      reference_bases=alleles[0],\n      alternate_bases=alleles[1:],\n      quality=qual,\n  )\n\n  if filters is not None:\n    if not isinstance(filters, (list, tuple)):\n      filters = [filters]\n    variant.filter[:] = filters\n\n  if gts:\n    for i in range(len(gts)):\n      call = variant.calls.add(genotype=gts[i])\n\n      if sample_names and sample_names[i] is not None:\n        call.call_set_name = sample_names[i]\n\n      if gqs and gqs[i] is not None:\n        set_list_values(call.info[\'GQ\'], [gqs[i]])\n\n      if glss and glss[i] is not None:\n        call.genotype_likelihood.extend(glss[i])\n\n      if is_phased and is_phased[i] is not None:\n        call.is_phased = is_phased[i]\n\n  return variant\n\n\ndef make_read(bases,\n              start,\n              quals=None,\n              cigar=None,\n              mapq=50,\n              chrom=\'chr1\',\n              name=None):\n  """"""Makes a nucleus.genomics.v1.Read for testing.""""""\n  if quals and len(bases) != len(quals):\n    raise ValueError(\'Incompatable bases and quals\', bases, quals)\n  read = reads_pb2.Read(\n      fragment_name=name if name else \'read_\' + str(make_read.counter),\n      proper_placement=True,\n      read_number=1,\n      number_reads=2,\n      aligned_sequence=bases,\n      aligned_quality=quals,\n      alignment=reads_pb2.LinearAlignment(\n          position=position_pb2.Position(reference_name=chrom, position=start),\n          mapping_quality=mapq,\n          cigar=_cigar.to_cigar_units(cigar) if cigar else []))\n  make_read.counter += 1\n  return read\nmake_read.counter = 0\n\n\ndef cc_iterable_len(cc_iterable):\n  """"""Count the number of elements in an Iterable object.\n\n  Args:\n    cc_iterable: a CLIF-wrap of a subclass of the C++ Iterable class.\n\n  Returns:\n    integer count\n  """"""\n  count = 0\n  while True:\n    not_done, _ = cc_iterable.Next()\n    if not not_done:\n      break\n    count += 1\n  return count\n\n\ndef iterable_len(iterable):\n  """"""Returns the length of a Python iterable, by advancing it.""""""\n  return sum(1 for _ in iterable)\n\n\n# TODO(b/63955799): remove and replace uses when bug is fixed in mock.\ndef assert_not_called_workaround(mock):\n  """"""Asserts that a mock has not been called.\n\n  There\'s a bug in mock where some of the assert functions on a mock are being\n  dropped when that mock is created with an autospec:\n\n    https://bugs.python.org/issue28380\n\n  The mock 2.0.0 backport doesn\'t have the fix yet. The required patch is:\n\n    https://bugs.python.org/file44991/fix_autospecced_mock_functions.patch\n\n  but the current mock (checked 07/22/17) backport code is missing the fix:\n\n    https://github.com/testing-cabal/mock/blob/master/mock/mock.py#L315\n\n  This is an open issue on the mock github repo:\n\n    https://github.com/testing-cabal/mock/issues/398\n\n  And they claim that it\'ll be a few months (as of April 2017) before it is\n  incorporated into the backport.\n\n  Args:\n    mock: The mock to assert hasn\'t been called.\n\n  Raises:\n    AssertionError: mock has been called.\n  """"""\n  if mock.call_count != 0:\n    raise AssertionError(""Expected no calls to \'{}\' but was called {} times""\n                         .format(mock.name, mock.call_count))\n\n\n# TODO(b/63955799): remove and replace uses when bug is fixed in mock.\ndef assert_called_once_workaround(mock):\n  """"""Asserts that a mock has been called exactly once.\n\n  See assert_not_called_workaround for the backstory on why this function\n  exists.\n\n  Args:\n    mock: The mock that should have been called exactly once.\n\n  Raises:\n    AssertionError: mock wasn\'t called exactly once.\n  """"""\n  if mock.call_count != 1:\n    raise AssertionError(\n        ""Expected exactly one call to \'{}\' but was called {} times"".format(\n            mock.name, mock.call_count))\n'"
nucleus/testing/test_utils_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for nucleus\'s testing.test_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\n\nfrom nucleus.protos import cigar_pb2\nfrom nucleus.testing import test_utils\n\n\nclass TestUtilsTests(absltest.TestCase):\n\n  def test_make_read(self):\n    bases = \'ACG\'\n    quals = [30, 40, 50]\n    cigar = \'3M\'\n    mapq = 42\n    chrom = \'chr10\'\n    start = 123\n    name = \'myname\'\n    read = test_utils.make_read(\n        bases,\n        quals=quals,\n        cigar=cigar,\n        mapq=mapq,\n        chrom=chrom,\n        start=start,\n        name=name)\n\n    self.assertEqual(read.aligned_sequence, bases)\n    self.assertEqual(read.aligned_quality, quals)\n    self.assertEqual(list(read.alignment.cigar), [\n        cigar_pb2.CigarUnit(\n            operation_length=3, operation=cigar_pb2.CigarUnit.ALIGNMENT_MATCH)\n    ])\n    self.assertEqual(read.alignment.mapping_quality, mapq)\n    self.assertEqual(read.alignment.position.reference_name, chrom)\n    self.assertEqual(read.alignment.position.position, start)\n    self.assertEqual(read.fragment_name, name)\n\n  def test_make_read_produces_unique_read_names(self):\n    start = 0\n    read1 = test_utils.make_read(\'A\', start=start)\n    read2 = test_utils.make_read(\'A\', start=start)\n    self.assertGreater(len(read1.fragment_name), 0)\n    self.assertGreater(len(read2.fragment_name), 0)\n    self.assertNotEqual(read1.fragment_name, read2.fragment_name)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/tools/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
nucleus/util/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
nucleus/util/cigar.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utility functions for working with alignment CIGAR operations.\n\nThe CIGAR format is defined within the SAM spec, available at\nhttps://samtools.github.io/hts-specs/SAMv1.pdf\n\nThis module provides utility functions for interacting with the parsed\nrepresentations of CIGAR strings.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport six\nfrom nucleus.protos import cigar_pb2\n\n# A frozenset of all CigarUnit.Operation enum values that advance the alignment\n# with respect to the reference genome and read, respectively.\nREF_ADVANCING_OPS = frozenset([\n    cigar_pb2.CigarUnit.ALIGNMENT_MATCH,\n    cigar_pb2.CigarUnit.SEQUENCE_MATCH,\n    cigar_pb2.CigarUnit.DELETE,\n    cigar_pb2.CigarUnit.SKIP,\n    cigar_pb2.CigarUnit.SEQUENCE_MISMATCH\n])\nREAD_ADVANCING_OPS = frozenset([\n    cigar_pb2.CigarUnit.ALIGNMENT_MATCH,\n    cigar_pb2.CigarUnit.SEQUENCE_MATCH,\n    cigar_pb2.CigarUnit.INSERT,\n    cigar_pb2.CigarUnit.CLIP_SOFT,\n    cigar_pb2.CigarUnit.SEQUENCE_MISMATCH\n])\n\n# A map from CigarUnit.Operation (e.g., CigarUnit.ALIGNMENT_MATCH) enum values\n# to their corresponding single character cigar codes (e.g., \'M\').\nCIGAR_OPS_TO_CHAR = {\n    cigar_pb2.CigarUnit.ALIGNMENT_MATCH: \'M\',\n    cigar_pb2.CigarUnit.INSERT: \'I\',\n    cigar_pb2.CigarUnit.DELETE: \'D\',\n    cigar_pb2.CigarUnit.SKIP: \'N\',\n    cigar_pb2.CigarUnit.CLIP_SOFT: \'S\',\n    cigar_pb2.CigarUnit.CLIP_HARD: \'H\',\n    cigar_pb2.CigarUnit.PAD: \'P\',\n    cigar_pb2.CigarUnit.SEQUENCE_MATCH: \'=\',\n    cigar_pb2.CigarUnit.SEQUENCE_MISMATCH: \'X\',\n}\n\n# A map from single character cigar codes (e.g., \'M\') to their corresponding\n# CigarUnit.Operation (e.g., CigarUnit.ALIGNMENT_MATCH) enum values.\nCHAR_TO_CIGAR_OPS = {v: k for k, v in CIGAR_OPS_TO_CHAR.items()}\n\n# All of the CigarUnit.Operation values in a frozen set.\nALL_CIGAR_OPS = frozenset(CIGAR_OPS_TO_CHAR.keys())\n\n# Regular expression that matches only valid full cigar strings.\nVALID_CIGAR_RE = re.compile(\n    r\'^(\\d+[\' + \'\'.join(CHAR_TO_CIGAR_OPS.keys()) + \'])+$\')\n\n# Regular expression that matches a single len/op cigar element. The match is\n# grouped, so CIGAR_STR_SPLITTER_RE.finditer(cigar_str) returns grouped units\n# of the cigar string in order.\nCIGAR_STR_SPLITTER_RE = re.compile(\n    r\'(\\d+[\' + \'\'.join(CHAR_TO_CIGAR_OPS.keys()) + \'])\')\n\n\ndef format_cigar_units(cigar_units):\n  """"""Returns the string version of an iterable of CigarUnit protos.\n\n  Args:\n    cigar_units: iterable[CigarUnit] protos.\n\n  Returns:\n    A string representation of the CigarUnit protos that conforms to the\n    CIGAR string specification.\n  """"""\n  return \'\'.join(\n      str(unit.operation_length) + CIGAR_OPS_TO_CHAR[unit.operation]\n      for unit in cigar_units)\n\n\ndef parse_cigar_string(cigar_str):\n  """"""Parse a cigar string into a list of cigar units.\n\n  For example, if cigar_str is 150M2S, this function will return:\n\n  [\n    CigarUnit(operation=ALIGNMENT_MATCH, operation_length=150),\n    CigarUnit(operation=SOFT_CLIP, operation_length=2)\n  ]\n\n  Args:\n    cigar_str: str containing a valid cigar.\n\n  Returns:\n    list[cigar_pb2.CigarUnit].\n\n  Raises:\n    ValueError: If cigar_str isn\'t a well-formed CIGAR.\n  """"""\n  if not cigar_str:\n    raise ValueError(\'cigar_str cannot be empty\')\n  if not VALID_CIGAR_RE.match(cigar_str):\n    raise ValueError(\'Malformed CIGAR string {}\'.format(cigar_str))\n  parts = CIGAR_STR_SPLITTER_RE.finditer(cigar_str)\n  return [to_cigar_unit(part.group(1)) for part in parts]\n\n\ndef alignment_length(cigar_units):\n  """"""Computes the span in basepairs of the cigar units.\n\n  Args:\n    cigar_units: iterable[CigarUnit] whose alignment length we want to compute.\n\n  Returns:\n    The number of basepairs spanned by the cigar_units.\n  """"""\n  return sum(unit.operation_length\n             for unit in cigar_units\n             if unit.operation in REF_ADVANCING_OPS)\n\n\ndef to_cigar_unit(source):\n  """"""Creates a cigar_pb2 CigarUnit from source.\n\n  This function attempts to convert source into a CigarUnit protobuf. If\n  source is a string, it must be a single CIGAR string specification like\n  \'12M\'. If source is a tuple or a list, must have exactly two elements\n  (operation_length, opstr). operation_length can be a string or int, and must\n  be >= 1. opstr should be a single character CIGAR specification (e.g., \'M\').\n  If source is already a CigarUnit, it is just passed through unmodified.\n\n  Args:\n    source: many types allowed. The object we want to convert to a CigarUnit\n      proto.\n\n  Returns:\n    CigarUnit proto with operation_length and operation set to values from\n      source.\n\n  Raises:\n    ValueError: if source cannot be converted or is malformed.\n  """"""\n  try:\n    if isinstance(source, cigar_pb2.CigarUnit):\n      return source\n    elif isinstance(source, six.string_types):\n      l, op = source[:-1], source[-1]\n    elif isinstance(source, (tuple, list)):\n      l, op = source\n    else:\n      raise ValueError(\'Unexpected source\', source)\n\n    if isinstance(op, six.string_types):\n      op = CHAR_TO_CIGAR_OPS[op]\n    l = int(l)\n    if l < 1:\n      raise ValueError(\'Length must be >= 1\', l)\n    return cigar_pb2.CigarUnit(operation=op, operation_length=int(l))\n  except (KeyError, IndexError):\n    raise ValueError(\'Failed to convert {} into a CigarUnit\'.format(source))\n\n\ndef to_cigar_units(source):\n  """"""Converts object to a list of CigarUnit.\n\n  This function attempts to convert source into a list of CigarUnit protos.\n  If source is a string, we assume it is a CIGAR string and call\n  parse_cigar_string on it, returning the result. It not, we assume it\'s an\n  iterable containing element to be converted with to_cigar_unit(). The\n  resulting list of converted elements is returned.\n\n  Args:\n    source: str or iterable to convert to CigarUnit protos.\n\n  Returns:\n    list[CigarUnit].\n  """"""\n  if isinstance(source, six.string_types):\n    return parse_cigar_string(source)\n  else:\n    return [to_cigar_unit(singleton) for singleton in source]\n'"
nucleus/util/cigar_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for cigar.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.protos import cigar_pb2\nfrom nucleus.util import cigar\n\n_CIGAR_TUPLES_AND_CIGAR_UNITS = [\n    ((1, \'M\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.ALIGNMENT_MATCH, operation_length=1)),\n    ((2, \'I\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.INSERT, operation_length=2)),\n    ((3, \'D\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.DELETE, operation_length=3)),\n    ((4, \'N\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.SKIP, operation_length=4)),\n    ((5, \'S\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.CLIP_SOFT, operation_length=5)),\n    ((6, \'H\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.CLIP_HARD, operation_length=6)),\n    ((7, \'P\'),\n     cigar_pb2.CigarUnit(operation=cigar_pb2.CigarUnit.PAD,\n                         operation_length=7)),\n    ((8, \'=\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.SEQUENCE_MATCH, operation_length=8)),\n    ((9, \'X\'),\n     cigar_pb2.CigarUnit(\n         operation=cigar_pb2.CigarUnit.SEQUENCE_MISMATCH, operation_length=9)),\n]\n\n\ndef _example_cigar_string_and_units(repeat=3):\n  examples = {}\n  for x in itertools.product(_CIGAR_TUPLES_AND_CIGAR_UNITS, repeat=repeat):\n    lengths_and_opstrs, cigar_units = zip(*x)\n    cigar_str = \'\'.join(str(l) + opstr for l, opstr in lengths_and_opstrs)\n    examples[cigar_str] = list(cigar_units)\n  return examples\n\n\nclass CigarTests(parameterized.TestCase):\n\n  @parameterized.parameters((cigar_units, cigar_str)\n                            for cigar_str, cigar_units in\n                            _example_cigar_string_and_units(3).items())\n  def test_format_cigar_units(self, cigar_units, expected):\n    self.assertEqual(cigar.format_cigar_units(cigar_units), expected)\n\n  @parameterized.parameters(\n      (\'10M\', 10),\n      (\'10=\', 10),\n      (\'10X\', 10),\n      (\'10M2I3M\', 13),\n      (\'10M2D3M\', 15),\n      (\'10M2N3M\', 15),\n      (\'1S10M2D3M\', 15),\n      (\'1S10M2D3M1S\', 15),\n      (\'1S10M2D3M1S5H\', 15),\n      (\'8H1S10M2D3M1S5H\', 15),\n      (\'8H1S10M2N3M1S5H\', 15),\n  )\n  def test_alignment_length(self, cigar_str, expected):\n    cigar_units = cigar.parse_cigar_string(cigar_str)\n    self.assertEqual(cigar.alignment_length(cigar_units), expected)\n\n  @parameterized.parameters(\n      (length, opstr, expected)\n      for (length, opstr), expected in _CIGAR_TUPLES_AND_CIGAR_UNITS)\n  def test_to_cigar_unit(self, length, opstr, expected):\n    # Check we can convert a tuple and list of length and opstr.\n    self.assertEqual(cigar.to_cigar_unit((length, opstr)), expected)\n    self.assertEqual(cigar.to_cigar_unit([length, opstr]), expected)\n\n    # Check that we can convert a string version len+opstr.\n    self.assertEqual(cigar.to_cigar_unit(str(length) + opstr), expected)\n\n    # Check that we can pass a CigarUnit through without modification.\n    self.assertEqual(cigar.to_cigar_unit(expected), expected)\n\n    # Check that we can pass length as a string.\n    self.assertEqual(cigar.to_cigar_unit((str(length), opstr)), expected)\n\n  @parameterized.parameters(\n      \'-1M\',\n      \'0M\',\n      \'\',\n      \'M\',\n      \'M12\',\n      \'4m\',\n      # Have to be wrapped in a list to stop parameterized from treating the\n      # tuple as the positional arguments to the test function.\n      [()],\n      [(4)],\n      [(4, \'M\', \'X\')],\n      [(4, \'M\', 10)],\n      [{4, \'M\'}],\n      # This integer is too large to fit in an int64 cigar, make sure an\n      # exception is thrown. Max int64 is 9,223,372,036,854,775,807, so we try\n      # one more.\n      \'9223372036854775808M\',\n  )\n  def test_to_cigar_unit_detects_bad_args(self, bad):\n    with self.assertRaises(ValueError):\n      cigar.to_cigar_unit(bad)\n\n  @parameterized.parameters(\n      list(zip(*to_convert))\n      for to_convert in itertools.product(\n          _CIGAR_TUPLES_AND_CIGAR_UNITS, repeat=3))\n  def test_to_cigar_units(self, to_convert, expected):\n    # We can convert the raw form.\n    to_convert = list(to_convert)\n    expected = list(expected)\n\n    actual = cigar.to_cigar_units(to_convert)\n    self.assertEqual(actual, expected)\n\n    # We can also convert the string form by formatting actual.\n    self.assertEqual(\n        cigar.to_cigar_units(cigar.format_cigar_units(actual)), expected)\n\n  @parameterized.parameters(\n      (str(length) + opstr, [expected])\n      for (length, opstr), expected in _CIGAR_TUPLES_AND_CIGAR_UNITS)\n  def test_parse_cigar_string_single(self, cigar_str, expected):\n    self.assertEqual(cigar.parse_cigar_string(cigar_str), expected)\n\n  @parameterized.parameters(_example_cigar_string_and_units(3).items())\n  def test_parse_cigar_string_three_pieces(self, cigar_str, expected):\n    self.assertEqual(cigar.parse_cigar_string(cigar_str), expected)\n\n  @parameterized.parameters(\n      \'\',\n      \'12\',\n      \'12m\',\n      \'12?\',\n      \'M12\',\n      \'12M1\',\n      \'12MI\',\n      \'12M-1I\',\n      \'12.0M\',\n  )\n  def test_parse_cigar_string_detects_bad_inputs(self, bad_cigar_str):\n    with self.assertRaises(ValueError):\n      cigar.parse_cigar_string(bad_cigar_str)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/util/errors.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Library of application-specific errors.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport errno\nimport sys\n\nfrom absl import logging\n\n\nclass Error(Exception):\n  """"""Base class for core error types.""""""\n\n\nclass CommandLineError(Error):\n  """"""Exception class related to invalid command-line flags.""""""\n\n\ndef log_and_raise(msg, exception_class=Error):\n  """"""Logs the given message at ERROR level and raises exception.\n\n  Args:\n    msg: [`string`]. The message to log and use in the raised exception.\n    exception_class: [`Exception`]. The class of exception to raise.\n\n  Raises:\n    Error: An exception of the type specified by the input exception_class.\n  """"""\n  logging.error(msg)\n  raise exception_class(msg)\n\n\n@contextlib.contextmanager\ndef clean_commandline_error_exit(\n    allowed_exceptions=(Error, CommandLineError), exit_value=errno.ENOENT):\n  """"""Wraps commands to capture certain exceptions and exit without stacktraces.\n\n  This function is intended to wrap all code within main() of Python binaries\n  to provide a mechanism for user errors to exit abnormally without causing\n  exceptions to be thrown. Any exceptions that are subclasses of those listed\n  in `allowed_exceptions` will be caught and the program will quietly exit with\n  `exit_value`. Other exceptions are propagated normally.\n\n  NOTE: This function should only be used as a context manager and its usage\n  should be limited to main().\n\n  Args:\n    allowed_exceptions: [`tuple of Exception`]. A tuple of Exception classes\n      that should not be raised, but instead quietly caused to exit the program.\n    exit_value: [`int`]. The value to return upon program exit.\n\n  Yields:\n    The yield in this function is used to allow the block nested in the ""with""\n    statement to be executed.\n  """"""\n  try:\n    yield\n  except allowed_exceptions:\n    sys.exit(exit_value)\n'"
nucleus/util/errors_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for nucleus.util.errors.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport errno\nimport sys\n\nfrom absl import logging\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\nfrom nucleus.util import errors\n\n\nclass ErrorsTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      (\'empty flag\', errors.CommandLineError),\n      (\'bad value\', ValueError),\n      (\'base error\', errors.Error),\n  )\n  def test_log_and_raise(self, msg, cls):\n    with mock.patch.object(logging, \'error\') as mock_logging:\n      with self.assertRaisesRegexp(cls, msg):\n        errors.log_and_raise(msg, cls)\n      mock_logging.assert_called_once_with(msg)\n\n  @parameterized.parameters(\n      (ValueError, \'ValueError exception\'),\n      (IOError, \'IOError exception\'),\n  )\n  def test_clean_commandline_error_exit_raise_non_allowed(self, exc_type, msg):\n    with self.assertRaisesRegexp(exc_type, msg):\n      with errors.clean_commandline_error_exit():\n        raise exc_type(msg)\n\n  @parameterized.parameters(\n      (errors.CommandLineError, errno.ENOENT),\n      (errors.Error, errno.EINVAL),\n  )\n  def test_clean_commandline_error_exit_clean_exit(self, exc_type, exit_value):\n    with mock.patch.object(sys, \'exit\') as mock_exit:\n      with errors.clean_commandline_error_exit(exit_value=exit_value):\n        raise exc_type()\n    mock_exit.assert_called_once_with(exit_value)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/util/genomics_math.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Mathematics functions for working with genomics data.\n\nA quick note on terminology here.\n\nThere are a bunch kinds of probabilities used commonly in genomics:\n\n-- pError: the probability of being wrong.\n-- pTrue: the probability of being correct.\n\nNormalized probabilities vs. unnormalized likelihoods:\n\n-- Normalized probabilities: p_1, ..., p_n such that sum(p_i) == 1 are said\n   said to be normalized because they represent a valid probability\n   distribution over the states 1 ... n.\n-- Unnormalized likelihoods: p_1, ..., p_n where sum(p_i) != 1. These are not\n   normalized and so aren\'t a valid probabilities distribution.\n\nTo add even more complexity, probabilities are often represented in three\nsemi-equivalent spaces:\n\n-- Real-space: the classic space, with values ranging from [0.0, 1.0]\n   inclusive.\n-- log10-space: If p is the real-space value, in log10-space this would be\n   represented as log10(p). How the p == 0 case is handled is often function\n   dependent, which may accept/return -Inf or not handle the case entirely.\n-- Phred-scaled: See https://en.wikipedia.org/wiki/Phred_quality_score for\n   more information. Briefly, the Phred-scale maintains resolution in the lower\n   parts of the probability space using integer quality scores (though using\n   ints is optional, really). The phred-scale is defined as\n\n     `phred(p) = -10 * log10(p)`\n\n   where p is a real-space probability.\n\nThe functions in math.h dealing with probabilities are very explicit about what\nkinds of probability and representation they expect and return, as unfortunately\nthese are all commonly represented as doubles in C++. Though it is tempting to\naddress this issue with classic software engineering practices like creating\na Probability class, in practice this is extremely difficult to do as this\ncode is often performance critical and the low-level mathematical operations\nused in this code (e.g., log10) don\'t distiguish themselves among the types\nof probabilities.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport numpy as np\n\nfrom nucleus.util.python import math as math_\n\n# C++ CLIF functions:\n#\n# We are enumerating the C++ functions exported by python/math.clif here, so\n# it\'s clear to people what functions are available in python without digging\n# into the raw python/C++ CLIF code.\nlog10_ptrue_to_phred = math_.log10_ptrue_to_phred\nphred_to_perror = math_.phred_to_perror\nphred_to_log10_perror = math_.phred_to_log10_perror\nperror_to_log10_perror = math_.perror_to_log10_perror\nperror_to_phred = math_.perror_to_phred\nlog10_perror_to_phred = math_.log10_perror_to_phred\nperror_to_rounded_phred = math_.perror_to_rounded_phred\nlog10_perror_to_rounded_phred = math_.log10_perror_to_rounded_phred\nlog10_perror_to_perror = math_.log10_perror_to_perror\nzero_shift_log10_probs = math_.zero_shift_log10_probs\n\n# Maximum confidence in a variant call. Used to prevent overflow with log10.\n# Note: -10 * log_10(1.25e-10) ~= 99.\n_MAX_CONFIDENCE = 1.0 - 1.25e-10\n\nLOG_10_OF_E = np.log10(np.e)\nLOG_E_OF_10 = np.log(10.0)\n\n\ndef perror_to_bounded_log10_perror(perror, min_prob=1.0 - _MAX_CONFIDENCE):\n  """"""Computes log10(p) for the given probability.\n\n  The log10 probability is capped by -_MAX_CONFIDENCE.\n\n  Args:\n    perror: float. The probability to log10.\n    min_prob: float. The minimum allowed probability.\n\n  Returns:\n    log10(p).\n\n  Raises:\n    ValueError: If probability is outside of [0.0, 1.0].\n  """"""\n  if not 0 <= perror <= 1:\n    raise ValueError(\'perror must be between zero and one: {}\'.format(perror))\n  return perror_to_log10_perror(max(perror, min_prob))\n\n\ndef ptrue_to_bounded_phred(ptrue, max_prob=_MAX_CONFIDENCE):\n  """"""Computes the Phred-scaled confidence from the given ptrue probability.\n\n  See https://en.wikipedia.org/wiki/Phred_quality_score for more information.\n  The quality score is capped by _MAX_CONFIDENCE.\n\n  Args:\n    ptrue: float. The ptrue probability to Phred scale.\n    max_prob: float. The maximum allowed probability.\n\n  Returns:\n    Phred-scaled version of 1 - ptrue.\n\n  Raises:\n    ValueError: If ptrue is outside of [0.0, 1.0].\n  """"""\n  if not 0 <= ptrue <= 1:\n    raise ValueError(\'ptrue must be between zero and one: {}\'.format(ptrue))\n  return perror_to_phred(1.0 - min(ptrue, max_prob))\n\n\ndef log10_binomial(k, n, p):\n  """"""Calculates numerically-stable value of log10(binomial(k, n, p)).\n\n  Returns the log10 of the binomial density for k successes in n trials where\n  each success has a probability of occurring of p.\n\n  In real-space, we would calculate:\n\n     result = (n choose k) * (1-p)^(n-k) * p^k\n\n  This function computes the log10 of result, which is:\n\n     log10(result) = log10(n choose k) + (n-k) * log10(1-p) + k * log10(p)\n\n  This is equivalent to invoking the R function:\n    dbinom(x=k, size=n, prob=p, log=TRUE)\n\n  See https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Binomial.html\n  for more details on the binomial.\n\n  Args:\n    k: int >= 0. Number of successes.\n    n: int >= k. Number of trials.\n    p: 0.0 <= float <= 1.0. Probability of success.\n\n  Returns:\n    log10 probability of seeing k successes in n trials with p.\n  """"""\n  r = math.lgamma(n + 1) - (math.lgamma(k + 1) + math.lgamma(n - k + 1))\n  if k > 0:\n    r += k * math.log(p)\n  if n > k:\n    r += (n-k) * math.log1p(-p)\n  return r / LOG_E_OF_10\n\n\ndef log10sumexp(log10_probs):\n  """"""Returns log10(sum(10^log10_probs)) computed in a numerically-stable way.\n\n  Args:\n    log10_probs: array-like of floats. An array of log10 probabilties.\n\n  Returns:\n    Float.\n  """"""\n  m = max(log10_probs)\n  return m + math.log10(sum(pow(10.0, x - m) for x in log10_probs))\n\n\ndef normalize_log10_probs(log10_probs):\n  """"""Approximately normalizes log10 probabilities.\n\n  This function normalizes log10 probabilities. What this means is that we\n  return an equivalent array of probabilities but whereas sum(10^log10_probs) is\n  not necessarily 1.0, the resulting array is sum(10^result) ~= 1.0. The ~=\n  indicates that the result is not necessarily == 1.0 but very close.\n\n  This function is a fast and robust approximation of the true normalization of\n  a log10 transformed probability vector. To understand the approximation,\n  let\'s start with the exact calculation. Suppose I have three models, each\n  emitting a probability that some data was generated by that model:\n\n    data = {0.1, 0.01, 0.001} => probabilities from models A, B, and C\n\n  These probabilities are unnormalized, in the sense that the total probability\n  over the vector doesn\'t sum to 1 (sum(data) = 0.111). In many applications we\n  want to normalize this vector so that sum(normalized(data)) = 1 and the\n  relative magnitudes of the original probabilities are preserved (i.e,:\n\n    data[i] / data[j] = normalized(data)[i] / normalized(data)[j]\n\n  for all pairs of values indexed by i and j. For much of the work we do in\n  genomics, we have so much data that representing these raw probability\n  vectors in real-space risks numeric underflow/overflow, so we instead\n  represent our probability vectors in log10 space:\n\n    log10_data = log10(data) = {-1, -2, -3}\n\n  Given that we expect numeric problems in real-space, normalizing this log10\n  vector is hard, because the standard way you\'d do the normalization is via:\n\n    data[i] = data[i] / sum(data)\n    log10_data[i] = log10_data[i] - log10(sum(10^data))\n\n  But computing the sum of log10 values this way is dangerous because the naive\n  implementation converts back to real-space to do the sum, the very operation\n  we\'re trying to avoid due to numeric instability.\n\n  This function implements an approximate normalization, which relaxes the need\n  for an exact calculation of the sum. This function ensures that the\n  normalization is numerically safe at the expense of the sum not being exactly\n  equal to 1 but rather just close.\n\n  Args:\n    log10_probs: array-like of floats. An array of log10 probabilties.\n\n  Returns:\n    np.array with the same shape as log10_probs but where sum(10^result) ~= 1.0.\n\n  Raises:\n    ValueError: if any log10_probs > 0.0\n  """"""\n  log10_probs = np.array(log10_probs)\n  if np.max(log10_probs) > 0.0:\n    raise ValueError(\'log10_probs all must be <= 0\', log10_probs)\n  lse = log10sumexp(log10_probs)\n  # np.minimum protects us from producing values slightly > 0.0 (e.g., 1e-16).\n  return np.minimum(log10_probs - lse, 0.0)\n'"
nucleus/util/genomics_math_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for nucleus.util.genomics_math.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport numpy as np\nimport numpy.testing as npt\n\nfrom nucleus.util import genomics_math\n\n\nclass MathTests(parameterized.TestCase):\n\n  @parameterized.parameters(\n      (0.9000000, None, 10.0),\n      (0.9900000, None, 20.0),\n      (0.9990000, None, 30.0),\n      (0.9999000, None, 40.0),\n      (0.9999900, None, 50.0),\n      (0.9999990, None, 60.0),\n      (0.9999999, None, 70.0),\n      # Check that bounding works.\n      (0.9999999, 1 - 1e-1, 10.0),\n      (0.9999999, 1 - 1e-2, 20.0),\n      (0.9999999, 1 - 1e-3, 30.0),\n      (0.9999999, 1 - 1e-9, 70.0),\n  )\n  def test_phred_scale(self, prob, bound, expected):\n    if bound:\n      actual = genomics_math.ptrue_to_bounded_phred(prob, bound)\n    else:\n      actual = genomics_math.ptrue_to_bounded_phred(prob)\n    self.assertAlmostEqual(actual, expected, places=6)\n\n  @parameterized.parameters(\n      (1.000000, None, 0.0),\n      (0.100000, None, -1.0),\n      (0.010000, None, -2.0),\n      (0.001000, None, -3.0),\n      (0.000100, None, -4.0),\n      (0.000010, None, -5.0),\n      (0.000001, None, -6.0),\n      # Check that bounding works.\n      (0.000100, 1e-1, -1.0),\n      (0.000100, 1e-2, -2.0),\n      (0.000100, 1e-3, -3.0),\n      (0.000100, 1e-4, -4.0),\n      (0.000100, 1e-5, -4.0),\n      (0.000100, 1e-6, -4.0),\n  )\n  def test_log10_prob(self, prob, bound, expected):\n    if bound:\n      actual = genomics_math.perror_to_bounded_log10_perror(prob, bound)\n    else:\n      actual = genomics_math.perror_to_bounded_log10_perror(prob)\n    self.assertAlmostEqual(actual, expected, places=6)\n\n  @parameterized.parameters(\n      (np.log10(0.900000), -1.0, 10.0),\n      (np.log10(0.990000), -1.0, 20.0),\n      (np.log10(0.999000), -1.0, 30.0),\n      # A huge negative value is handled correctly.\n      (-10000000.0, -1.0, 0.0),\n      # Check that we can pass in a 0.0 probability and get a good value.\n      (0.0, -1.0, -1.0),\n      # This probability is still calculated correctly, included for safety.\n      (0 - 1e-16, -1.0, 156.53559774527022),\n      # Pass in a prob close to one, making sure we get bound value back.\n      (0 - 1e-32, -1.0, -1.0),\n  )\n  def test_log10_ptrue_to_phred(self, prob, value_if_not_finite, expected):\n    actual = genomics_math.log10_ptrue_to_phred(prob, value_if_not_finite)\n    self.assertAlmostEqual(actual, expected, places=6)\n\n  # R code to produce the expectation table.\n  # expected <- function(k, n, p) {\n  #   pbin <- dbinom(k, n, p, log=T) * log10(exp(1))\n  #   likelihoods = paste(sprintf(""%.6f"", pbin), collapse="", "")\n  #   result = paste(k, n, p, pbin, sep="", "")\n  #   cat(paste(""("", result, ""),\\n"", sep=""""))\n  # }\n  #\n  # for (n in c(0, 5, 10)) {\n  #   for (k in seq(0, n)) {\n  #     for (p in c(0.01, 0.5)) {\n  #       expected(k, n, p)\n  #     }\n  #   }\n  # }\n  # expected(0, 1000, 0.5)\n  # expected(0, 10000, 0.5)\n  # expected(100, 10000, 0.5)\n  @parameterized.parameters(\n      (0, 0, 0.01, 0),\n      (0, 0, 0.5, 0),\n      (0, 5, 0.01, -0.0218240270122504),\n      (0, 5, 0.5, -1.50514997831991),\n      (1, 5, 0.01, -1.31848921727378),\n      (1, 5, 0.5, -0.806179973983887),\n      (2, 5, 0.01, -3.01309441620735),\n      (2, 5, 0.5, -0.505149978319906),\n      (3, 5, 0.01, -5.0087296108049),\n      (3, 5, 0.5, -0.505149978319906),\n      (4, 5, 0.01, -7.30539480106643),\n      (4, 5, 0.5, -0.806179973983887),\n      (5, 5, 0.01, -10),\n      (5, 5, 0.5, -1.50514997831991),\n      (0, 10, 0.01, -0.0436480540245008),\n      (0, 10, 0.5, -3.01029995663981),\n      (1, 10, 0.01, -1.03928324862205),\n      (1, 10, 0.5, -2.01029995663981),\n      (2, 10, 0.01, -2.38170592944426),\n      (2, 10, 0.5, -1.35708744286447),\n      (3, 10, 0.01, -3.95137239176953),\n      (3, 10, 0.5, -0.931118710592187),\n      (4, 10, 0.01, -5.70396953768078),\n      (4, 10, 0.5, -0.688080661905893),\n      (5, 10, 0.01, -7.62042348623071),\n      (5, 10, 0.5, -0.608899415858268),\n      (6, 10, 0.01, -9.69523992687588),\n      (6, 10, 0.5, -0.688080661905893),\n      (7, 10, 0.01, -11.9339131701597),\n      (7, 10, 0.5, -0.931118710592187),\n      (8, 10, 0.01, -14.3555170970296),\n      (8, 10, 0.5, -1.35708744286447),\n      (9, 10, 0.01, -17.0043648054024),\n      (9, 10, 0.5, -2.01029995663981),\n      (10, 10, 0.01, -20),\n      (10, 10, 0.5, -3.01029995663981),\n      (0, 1000, 0.5, -301.029995663981),\n      (0, 10000, 0.5, -3010.29995663981),\n      (100, 10000, 0.5, -2768.48565263445),\n  )\n  def test_log10_binomial(self, k, n, p, expected):\n    self.assertAlmostEqual(genomics_math.log10_binomial(k, n, p), expected)\n\n  @parameterized.parameters(\n      ([0], 0.0),\n      ([0.0], 0.0),\n      ([0.0, -10000.0], 0.0),\n      ([-1000.0, -10000.0], -1000.0),\n      ([-1, -10, -100], -1.0),\n      ([-1, -10, -1], -0.69897),\n      ([-1, -1, -1], -0.5228787),\n      ([-1, -1, -1, -100], -0.5228787),\n      ([-1, -1, -1, -100, -1000], -0.5228787),\n      ([-1, -1, -1, -100, -1000, -10000], -0.5228787),\n      ([-1, -1, -1, -100, -1000, -10000, -100000], -0.5228787),\n  )\n  def test_log10sumexp(self, log10_probs, expected):\n    self.assertAlmostEqual(genomics_math.log10sumexp(log10_probs), expected)\n\n  # R code to compute expected results.\n  # expected <- function(lprobs) {\n  #   result = lprobs - log10(sum(10^lprobs))\n  #   lprob_str = paste(""["", paste(sprintf(""%.6f"", lprobs), collapse="", ""),\n  #                     ""]"", sep="""")\n  #   result_str = paste(""["", paste(sprintf(""%.6f"", result), collapse="", ""),\n  #                     ""]"", sep="""")\n  #   cat(paste(""("", lprob_str, "", "", result_str, ""),\\n"", sep=""""))\n  # }\n  #\n  # expected(c(0))\n  # expected(c(-1, -10))\n  # expected(c(-1, -100))\n  # expected(c(-1, -1000))\n  # expected(c(-1, -2))\n  # expected(c(-1, -2, -3))\n  # expected(c(-1, -2, -3, -100))\n  # expected(c(-1, -2, -100))\n  # expected(c(-1, -2, -100, -100))\n  @parameterized.parameters(\n      dict(\n          log10_probs=[0.000000],\n          expected=[0.000000]),\n      dict(\n          log10_probs=[-1.000000, -10.000000],\n          expected=[-0.000000, -9.000000]),\n      dict(\n          log10_probs=[-1.000000, -100.000000],\n          expected=[0.000000, -99.000000]),\n      dict(\n          log10_probs=[-1.000000, -1000.000000],\n          expected=[0.000000, -999.000000]),\n      dict(\n          log10_probs=[-1.000000, -2.000000],\n          expected=[-0.041393, -1.041393]),\n      dict(\n          log10_probs=[-1.000000, -2.000000, -3.000000],\n          expected=[-0.045323, -1.045323, -2.045323]),\n      dict(\n          log10_probs=[-1.000000, -2.000000, -3.000000, -100.000000],\n          expected=[-0.045323, -1.045323, -2.045323, -99.045323]),\n      dict(\n          log10_probs=[-1.000000, -2.000000, -100.000000],\n          expected=[-0.041393, -1.041393, -99.041393]),\n      dict(\n          log10_probs=[-1.000000, -2.000000, -100.000000, -100.000000],\n          expected=[-0.041393, -1.041393, -99.041393, -99.041393]),\n  )\n  def test_normalize_log10_probs(self, log10_probs, expected):\n    npt.assert_allclose(\n        genomics_math.normalize_log10_probs(log10_probs), expected, atol=1e-6)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/util/proto_utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utility library for working with protobufs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom google.protobuf.internal import api_implementation\n\n\ndef uses_fast_cpp_protos_or_die():\n  """"""Raises an error if a slow protobuf implementation is being used.""""""\n  if api_implementation.Type() != \'cpp\':\n    raise ValueError(\'Expected to be using C++ protobuf implementation \'\n                     \'(api_implementation.Type() == ""cpp"") but it is {}\'.format(\n                         api_implementation.Type()))\n'"
nucleus/util/ranges.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utilities for Range overlap detection.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\n\nfrom absl import logging\nimport intervaltree\nimport six\n\nfrom nucleus.io import gfile\nfrom nucleus.io import bed\nfrom nucleus.protos import position_pb2\nfrom nucleus.protos import range_pb2\n\n# Regular expressions for matching literal chr:start-stop strings.\n_REGION_LITERAL_REGEXP = re.compile(r\'^(\\S+):([0-9,]+)-([0-9,]+)$\')\n\n# Regular expressions for matching literal chr:start strings.\n_POSITION_LITERAL_REGEXP = re.compile(r\'^(\\S+):([0-9,]+)$\')\n\n# Logging frequency when building our rangeset objects, which can take some time\n# to complete. Rather than just pausing for a few minutes, we provide an update\n# logging message every _LOG_EVERY_N_RANGES_IN_RANGESET_INIT records added. See\n# b/110987941 for more information.\n_LOG_EVERY_N_RANGES_IN_RANGESET_INIT = 250000\n\n\nclass RangeSet(object):\n  """"""Fast overlap detection of a genomic position against a database of Ranges.\n\n  Enables O(log n) computation of whether a point chr:pos falls within one of a\n  large number of genomic ranges.\n\n  This class does not supports overlapping or adjacent intervals. Any such\n  intervals will be automatically merged together in the constructor.\n\n  This class is immutable. No methods should be added that directly modify the\n  ranges held by the class.\n  """"""\n\n  def __init__(self, ranges=None, contigs=None, quiet=False):\n    """"""Creates a RangeSet backed by ranges.\n\n    Note that the Range objects in ranges are *not* stored directly here, so\n    they can safely be modified after they are passed to this RangeSet.\n\n    Args:\n      ranges: list(nucleus.genomics.v1.Range) protos (or anything with\n        reference_name, start, and end properties following the Range\n        convention). If None, no ranges will be used, and overlaps() will always\n        return False.\n      contigs: list(nucleus.genomics.v1.ContigInfo) protos. Used to define the\n        iteration order over contigs (i.e., by contig.pos_in_fasta).  If this\n        list is not provided, the iteration order will be determined by the\n        alphabetical order of the contig names.\n      quiet: bool; defaults to False: If False, we will emit a logging message\n        every _LOG_EVERY_N_RANGES_IN_RANGESET_INIT records processed while\n        building this intervaltree. Set to True to stop all of the logging.\n\n    Raises:\n      ValueError: if any range\'s reference_name does not correspond to any\n        contig in `contigs`.\n    """"""\n    if contigs is not None:\n      self._contigs = contigs\n      self._contig_map = contigs_dict(contigs)\n      self._contig_sort_key_fn = (\n          lambda name: self._contig_map[name].pos_in_fasta)\n      self._is_valid_contig = lambda name: name in self._contig_map\n    else:\n      self._contigs = None\n      self._contig_map = None\n      self._contig_sort_key_fn = lambda name: name\n      self._is_valid_contig = lambda name: True\n\n    if ranges is None:\n      ranges = []\n\n    # Add each range to our contig-specific intervaltrees.\n    self._by_chr = collections.defaultdict(intervaltree.IntervalTree)\n    for i, range_ in enumerate(ranges):\n      if not self._is_valid_contig(range_.reference_name):\n        raise ValueError(\n            \'Range {} is on an unrecognized contig.\'.format(range_))\n      self._by_chr[range_.reference_name].addi(range_.start, range_.end, None)\n      if not quiet and i > 0 and i % _LOG_EVERY_N_RANGES_IN_RANGESET_INIT == 0:\n        # We do our test directly here on i > 0 so we only see the log messages\n        # if we add at least _LOG_EVERY_N_RANGES_IN_RANGESET_INIT records.\n        logging.info(\'Adding interval %s to intervaltree\', to_literal(range_))\n\n    # Merge overlapping / adjacent intervals in each tree.\n    for tree in six.itervalues(self._by_chr):\n      tree.merge_overlaps(strict=False)\n\n  def __iter__(self):\n    """"""Iterate over the ranges in this RangeSet.\n\n    Yields:\n      Each range of this RangeSet, in sorted order (by chromosome, then start\n      end positions). Relative ordering of chromosomes is defined by the\n      contig.pos_in_fasta integer key for the associated contig. These objects\n      are new range protos so can be freely modified.\n    """"""\n    for refname in sorted(\n        six.iterkeys(self._by_chr), key=self._contig_sort_key_fn):\n      for start, end, _ in sorted(self._by_chr[refname]):\n        yield make_range(refname, start, end)\n\n  @classmethod\n  def from_regions(cls, regions, contig_map=None):\n    """"""Parses a command-line style literal regions flag into a RangeSet.\n\n    Args:\n      regions: An iterable or None. If not None, regions will be parsed with\n        ranges.from_regions.\n      contig_map: An optional dictionary mapping from contig names to ContigInfo\n        protobufs. If provided, allows literals of the format ""contig_name"",\n        which will be parsed into a Range with reference_name=contig_name,\n        start=0, end=n_bases where n_bases comes from the ContigInfo;\n        additionally the sort order of the RangeSet will be determined by\n        contig.pos_in_fasta.\n\n    Returns:\n      A RangeSet object.\n    """"""\n    if regions is None:\n      return cls(ranges=[])\n    else:\n      return cls(ranges=from_regions(regions, contig_map=contig_map))\n\n  @classmethod\n  def from_contigs(cls, contigs):\n    """"""Creates a RangeSet with an interval covering each base of each contig.""""""\n    return cls(\n        (make_range(contig.name, 0, contig.n_bases) for contig in contigs),\n        contigs)\n\n  @classmethod\n  def from_bed(cls, source, contigs=None):\n    """"""Creates a RangeSet containing the intervals from source.\n\n    Args:\n      source: A path to a BED (or equivalent) file of intervals.\n      contigs: An optional list of ContigInfo proto, used by RangeSet\n        constructor.\n\n    Returns:\n      A RangeSet.\n    """"""\n    return cls(bed_parser(source), contigs)\n\n  def intersection(self, *others):\n    """"""Computes the intersection among this RangeSet and *others RangeSets.\n\n    This function computes the intersection of all of the intervals in self and\n    *others, returning a RangeSet containing only intervals common to all. The\n    intersection here is an ranged intersection, not an identity intersection,\n    so the resulting set of intervals may not contain any of the original\n    intervals in any of the sets.\n\n    To be concrete, suppose we have three sets to intersect, each having two\n    intervals:\n\n      self   : chr1:1-10, chr2:20-30\n      other1 : chr1:5-8, chr3:10-40\n      other2 : chr1:3-7, chr3:10-30\n\n    self.intersection(other1, other2) produces a RangeSet with one interval\n    chr1:5-7, the common bases on chr1 in self, other1, and other2. No intervals\n    on chr2 or chr3 are included since the chr2 only occurs in self and the two\n    intervals on chr3, despite having some shared bases, don\'t have an\n    overlapping interval in self.\n\n    Args:\n      *others: A list of RangeSet objects to intersect with the intervals in\n        this RangeSet.\n\n    Returns:\n      A RangeSet. If *others is empty, this function returns self rather than\n      making an unnecessary copy. In all other cases, the returned value will be\n      a freshly allocated RangeSet.\n    """"""\n\n    def _intersect2(refname, tree1, tree2):\n      """"""Intersects the intervals of two IntervalTrees.""""""\n      # Yields all of the overlapping intervals from each interval of tree1\n      # found in tree2. Since each tree has only non-adjacent, non-overlapping,\n      # intervals this calculation is straightforward and safe and produces only\n      # non-adjacent, non-overlapping intervals.\n      if len(tree1) > len(tree2):\n        (bigtree, smalltree) = (tree1, tree2)\n      else:\n        (bigtree, smalltree) = (tree2, tree1)\n      return (make_range(refname, max(interval1.begin, overlapping.begin),\n                         min(interval1.end, overlapping.end))\n              for interval1 in bigtree\n              for overlapping in smalltree.overlap(interval1))\n\n    # Iteratively intersect each of our *other RangeSets with this RangeSet.\n    # Sort by size so we do the smallest number of element merge first.\n    # TODO(mdepristo): Note we could optimize this code by computing the set of\n    # common contigs upfront across all others and only looping over those.\n    intersected = self\n    for other in sorted(others, key=len):\n      intersected_intervals = []\n      # pylint: disable=protected-access\n      # So we can intersect intervals within each contig separately.\n      for refname, intervals in six.iteritems(intersected._by_chr):\n        # If refname is present in other, intersect those two IntervalTrees\n        # directly and add those contigs to our growing list of intersected\n        # intervals. If refname isn\'t present, all of the intervals on refname\n        # should be dropped as there are no intervals to overlap.\n        other_chr = other._by_chr.get(refname, None)\n        if other_chr:\n          intersected_intervals.extend(\n              _intersect2(refname, intervals, other_chr))\n\n      # Update our intersected RangeSet with the new intervals.\n      intersected = RangeSet(intersected_intervals, self._contigs)\n\n    return intersected\n\n  def exclude_regions(self, other):\n    """"""Chops out all of the intervals in other from this this RangeSet.\n\n    NOTE: This is a *MUTATING* operation for performance reasons. Make a copy\n    of self if you want to avoid modifying the RangeSet.\n\n    Args:\n      other: A RangeSet object whose intervals will be removed from this\n        RangeSet.\n    """"""\n    # pylint: disable=protected-access\n    for chrname, chr_intervals in six.iteritems(other._by_chr):\n      # If refname is present in self, difference those two IntervalTrees.\n      self_intervals = self._by_chr.get(chrname, None)\n      if self_intervals:\n        for begin, end, _ in chr_intervals:\n          self_intervals.chop(begin, end)\n        if self_intervals.is_empty():\n          # Cleanup after ourselves by removing empty trees from our map.\n          del self._by_chr[chrname]\n\n  def __len__(self):\n    """"""Gets the number of ranges used by this RangeSet.""""""\n    return sum(len(for_chr) for for_chr in six.itervalues(self._by_chr))\n\n  def __nonzero__(self):\n    """"""Returns True if this RangeSet is not empty.""""""\n    return bool(self._by_chr)\n\n  __bool__ = __nonzero__  # Python 3 compatibility.\n\n  def variant_overlaps(self, variant, empty_set_return_value=True):\n    """"""Returns True if the variant\'s range overlaps with any in this set.""""""\n    if not self:\n      return empty_set_return_value\n    else:\n      return self.overlaps(variant.reference_name, variant.start)\n\n  def overlaps(self, chrom, pos):\n    """"""Returns True if chr:pos overlaps with any range in this RangeSet.\n\n    Uses a fast bisection algorithm to determine the overlap in O(log n) time.\n\n    Args:\n      chrom: str. The chromosome name.\n      pos: int. The position (0-based).\n\n    Returns:\n      True if chr:pos overlaps with a range.\n    """"""\n    chr_ranges = self._by_chr.get(chrom, None)\n    if chr_ranges is None:\n      return False\n    return chr_ranges.overlaps(pos)\n\n  def partition(self, max_size):\n    """"""Splits our intervals so that none are larger than max_size.\n\n    Slices up the intervals in this RangeSet into a equivalent set of intervals\n    (i.e., spanning the same set of bases), each of which is at most max_size in\n    length.\n\n    This function does not modify this RangeSet.\n\n    Because RangeSet merges adjacent intervals, this function cannot use a\n    RangeSet to represent the partitioned intervals and so instead generates\n    these intervals via a yield statement.\n\n    Args:\n      max_size: int > 0. The maximum size of any interval.\n\n    Yields:\n      nucleus.genomics.v1.Range protos, in sorted order (see comment about order\n      in __iter__).\n\n    Raises:\n      ValueError: if max_size <= 0.\n    """"""\n    if max_size <= 0:\n      raise ValueError(\'max_size must be > 0: {}\'.format(max_size))\n\n    for interval in self:\n      refname = interval.reference_name\n      for pos in range(interval.start, interval.end, max_size):\n        yield make_range(refname, pos, min(interval.end, pos + max_size))\n\n  def envelops(self, chrom, start, end):\n    """"""Returns True iff some range in this RangeSet envelops the range.\n\n    Args:\n      chrom: str. The chromosome of interest.\n      start: int. Zero-based inclusive index of the query range.\n      end: int: Zero-based exclusive index of the query range.\n\n    Returns:\n      True if and only if some range in `self` completely spans the query\n      range.\n    """"""\n    chr_ranges = self._by_chr.get(chrom, None)\n    if chr_ranges is None:\n      return False\n    # The intervaltree package does the inverse check, i.e. whether ranges\n    # contained in it overlap with the query region. So it returns nothing\n    # when start == end. We by convention want anything overlapping the start\n    # position to still indicate enveloping in this case.\n    if start == end:\n      return chr_ranges.overlaps(start)\n    else:\n      overlap_set = chr_ranges.overlap(begin=start, end=end)\n      return any(ov.begin <= start and ov.end >= end for ov in overlap_set)\n\n\ndef make_position(chrom, position, reverse_strand=False):\n  """"""Returns a nucleus.genomics.v1.Position.\n\n  Args:\n    chrom: str. The chromosome name.\n    position: int. The start position (0-based, inclusive).\n    reverse_strand: bool. If True, indicates the position is on the negative\n      strand.\n  """"""\n  return position_pb2.Position(\n      reference_name=chrom, position=position, reverse_strand=reverse_strand)\n\n\ndef make_range(chrom, start, end):\n  """"""Returns a nucleus.genomics.v1.Range.\n\n  Args:\n    chrom: str. The chromosome name.\n    start: int. The start position (0-based, inclusive) of this range.\n    end: int. The end position (0-based, exclusive) of this range.\n\n  Returns:\n    A nucleus.genomics.v1.Range.\n  """"""\n  return range_pb2.Range(reference_name=chrom, start=start, end=end)\n\n\ndef position_overlaps(chrom, pos, interval):\n  """"""Returns True iff the position chr:pos overlaps the interval.\n\n  Args:\n    chrom: str. The chromosome name.\n    pos: int. The position (0-based, inclusive).\n    interval: nucleus.genomics.v1.Range object.\n\n  Returns:\n    True if interval overlaps chr:pos.\n  """"""\n  return (chrom == interval.reference_name and\n          interval.start <= pos < interval.end)\n\n\ndef ranges_overlap(i1, i2):\n  """"""Returns True iff ranges i1 and i2 overlap.\n\n  Args:\n    i1: nucleus.genomics.v1.Range object.\n    i2: nucleus.genomics.v1.Range object.\n\n  Returns:\n    True if and only if i1 and i2 overlap.\n  """"""\n  return (i1.reference_name == i2.reference_name and i1.end > i2.start and\n          i1.start < i2.end)\n\n\ndef bedpe_parser(filename):\n  """"""Parses Range objects from a BEDPE-formatted file object.\n\n  See http://bedtools.readthedocs.org/en/latest/content/general-usage.html\n  for more information on the BEDPE format.\n\n  Skips events that span across chromosomes. For example, if the starting\n  location is on chr1 and the ending location is on chr2, that record will\n  not appear in the output.\n\n  Args:\n    filename: file name of a BEDPE-formatted file.\n\n  Yields:\n    nucleus.genomics.v1.Range protobuf objects.\n  """"""\n  for line in gfile.Open(filename):\n    parts = line.split(\'\\t\')\n    if parts[0] == parts[3]:\n      # only keep events on the same chromosome\n      yield make_range(parts[0], int(parts[1]), int(parts[5]))\n\n\ndef bed_parser(filename):\n  """"""Parses Range objects from a BED-formatted file object.\n\n  See http://bedtools.readthedocs.org/en/latest/content/general-usage.html\n  for more information on the BED format.\n\n  Args:\n    filename: file name of a BED-formatted file.\n\n  Yields:\n    nucleus.genomics.v1.Range protobuf objects.\n  """"""\n  with bed.BedReader(filename) as fin:\n    for r in fin.iterate():\n      yield make_range(r.reference_name, r.start, r.end)\n\n\ndef from_regions(regions, contig_map=None):\n  """"""Parses each region of `regions` into a Range proto.\n\n  This function provides a super high-level interface for\n  reading/parsing/converting objects into Range protos. Each `region` of\n  `regions` is processed in turn, yielding one or more Range protos. This\n  function inspects the contents of `region` to determine how to convert it to\n  Range(s) protos. The following types of `region` strings are supported:\n\n    * If region ends with an extension known in _get_parser_for_file, we treat\n      region as a file and read the Range protos from it with the corresponding\n      reader from _get_parser_for_file, yielding each Range from the file in\n      order.\n    * Otherwise we parse region as a region literal (`chr20:1-10`) and return\n      the Range proto.\n\n  Args:\n    regions: iterable[str]. Converts each element of this iterable into\n      region(s).\n    contig_map: An optional dictionary mapping from contig names to ContigInfo\n      protobufs. If provided, allows literals of the format ""contig_name"",\n      which will be parsed into a Range with reference_name=contig_name,\n      start=0, end=n_bases where n_bases comes from the ContigInfo.\n\n  Yields:\n    A Range proto.\n  """"""\n  for region in regions:\n    reader = _get_parser_for_file(region)\n    if reader:\n      for elt in reader(region):\n        yield elt\n    else:\n      yield parse_literal(region, contig_map)\n\n\n# Cannot be at the top of the file because these parser functions need to be\n# defined before adding them to the dictionary.\n_REGION_FILE_READERS = {\n    bed_parser: frozenset([\'.bed\']),\n    bedpe_parser: frozenset([\'.bedpe\']),\n}\n\n\ndef _get_parser_for_file(filename):\n  for reader, exts in six.iteritems(_REGION_FILE_READERS):\n    if any(filename.lower().endswith(ext) for ext in exts):\n      return reader\n  return None\n\n\ndef to_literal(range_pb):\n  """"""Converts Range protobuf into string literal form.\n\n  The string literal form looks like:\n\n    reference_name:start+1-end\n\n  since start and end are zero-based inclusive (start) and exclusive (end),\n  while the literal form is one-based inclusive on both ends.\n\n  Args:\n    range_pb: A nucleus.genomics.v1.Range object.\n\n  Returns:\n    A string representation of the Range.\n  """"""\n  return \'{}:{}-{}\'.format(range_pb.reference_name, range_pb.start + 1,\n                           range_pb.end)\n\n\ndef parse_literal(region_literal, contig_map=None):\n  """"""Parses a Range from a string representation like chr:start-end.\n\n  The region literal must conform to the following pattern:\n\n    chromosome:start-end\n    chromosome:position\n    chromosome  [if contig_map is provided]\n\n  chromosome can be any non-empty string without whitespace. start and end must\n  both be positive integers. They can contain commas for readability. start and\n  end are positions not offsets, so start == 1 means an offset of zero. If only\n  a single position is provided, this creates a 1 bp interval starting at\n  position - 1 and ending at position.\n\n  Inspired by the samtools region specification:\n  http://www.htslib.org/doc/samtools.html\n\n  Args:\n    region_literal: str. The literal to parse.\n    contig_map: An optional dictionary mapping from contig names to ContigInfo\n      protobufs. If provided, allows literals of the format ""contig_name"", which\n      will be parsed into a Range with reference_name=contig_name, start=0,\n      end=n_bases where n_bases comes from the ContigInfo.\n\n  Returns:\n    nucleus.genomics.v1.Range.\n\n  Raises:\n    ValueError: if region_literal cannot be parsed.\n  """"""\n\n  def parse_position(pos_str):\n    return int(pos_str.replace(\',\', \'\'))\n\n  matched = _REGION_LITERAL_REGEXP.match(region_literal)\n  if matched:\n    chrom, start, end = matched.groups()\n    return make_range(chrom, parse_position(start) - 1, parse_position(end))\n\n  matched = _POSITION_LITERAL_REGEXP.match(region_literal)\n  if matched:\n    chrom, pos = matched.groups()\n    pos = parse_position(pos)\n    return make_range(chrom, pos - 1, pos)\n\n  if contig_map and region_literal in contig_map:\n    # If the region_literals is an exact contig name like chr1 or MT return a\n    # range over the entire contig.\n    return make_range(region_literal, 0, contig_map[region_literal].n_bases)\n  raise ValueError(\n      \'Could not parse ""{}"" as a region literal.  Region literals \'\n      \'should have the form ""chr:start-stop"" or ""chr:start"" or \'\n      \'just ""chr"".  A common error is to use the ""chr"" prefix on \'\n      \'inputs that don\\\'t have it, or vice-versa.\'.format(region_literal))\n\n\ndef parse_literals(region_literals, contig_map=None):\n  """"""Parses each literal of region_literals in order.""""""\n  return [parse_literal(literal, contig_map) for literal in region_literals]\n\n\ndef contigs_n_bases(contigs):\n  """"""Returns the sum of all n_bases of contigs.""""""\n  return sum(c.n_bases for c in contigs)\n\n\ndef contigs_dict(contigs):\n  """"""Creates a dictionary for contigs.\n\n  Args:\n    contigs: Iterable of ContigInfo protos.\n\n  Returns:\n    A dictionary mapping contig.name: contig for each contig in contigs.\n  """"""\n  return {contig.name: contig for contig in contigs}\n\n\ndef sorted_ranges(ranges, contigs=None):\n  """"""Sorts ranges by reference_name, start, and end.\n\n  Args:\n    ranges: Iterable of nucleus.genomics.v1.Range protos that we want to sort.\n    contigs: None or an iterable of ContigInfo protos. If not None, we will use\n      the order of the contigs (as defined by their pos_in_fasta field values)\n      to sort the Ranges on different contigs with respect to each other.\n\n  Returns:\n    A newly allocated list of nucleus.genomics.v1.Range protos.\n  """"""\n  if contigs:\n    contig_map = contigs_dict(contigs)\n\n    def to_key(range_):\n      pos = contig_map[range_.reference_name].pos_in_fasta\n      return pos, range_.start, range_.end\n  else:\n    to_key = as_tuple\n\n  return sorted(ranges, key=to_key)\n\n\ndef as_tuple(range_):\n  """"""Returns a Python tuple (reference_name, start, end).""""""\n  return range_.reference_name, range_.start, range_.end\n\n\ndef overlap_len(range1, range2):\n  """"""Computes the number of overlapping bases of range1 and range2.\n\n  Args:\n    range1: nucleus.genomics.v1.Range.\n    range2: nucleus.genomics.v1.Range.\n\n  Returns:\n    int. The number of basepairs in common. 0 if the ranges are not on the same\n    contig.\n  """"""\n  if range1.reference_name != range2.reference_name:\n    return 0\n  return max(0, (min(range1.end, range2.end) - max(range1.start, range2.start)))\n\n\ndef find_max_overlapping(query_range, search_ranges):\n  """"""Gets the index of the element in search_ranges with max overlap with query.\n\n  In case of ties, selects the lowest index range in search_ranges.\n\n  Args:\n    query_range: nucleus.genomics.v1.Range, read genomic range.\n    search_ranges: list[nucleus.genomics.v1.Read]. The list of regions we want\n      to search for the maximal overlap with query_range. NOTE: this must be a\n      list (not a generator) as we loop over the search_ranges multiple times.\n\n  Returns:\n    int, the search_ranges index with the maximum read overlap. Returns None\n    when read has no overlap with any of the search_ranges or search_ranges is\n    empty.\n  """"""\n  if not search_ranges:\n    return None\n  overlaps = [overlap_len(query_range, srange) for srange in search_ranges]\n  argmax = max(range(len(search_ranges)), key=lambda i: overlaps[i])\n  # We return None if the read doesn\'t overlap at all.\n  return None if overlaps[argmax] == 0 else argmax\n\n\ndef expand(region, n_bp, contig_map=None):\n  """"""Expands region by n_bp in both directions.\n\n  Takes a Range(chrom, start, stop) and returns a new\n  Range(chrom, new_start, new_stop), where:\n\n  -- new_start is max(start - n_bp, 0)\n  -- new_stop is stop + n_bp if contig_map is None, or min(stop + n_bp, max_bp)\n     where max_bp is contig_map[chrom].n_bp.\n\n  Args:\n    region: A nucleus.genomics.v1.Range proto.\n    n_bp: int >= 0. how many basepairs to increase region by.\n    contig_map: dict[string, ContigInfo] or None. If not None, used to get the\n      maximum extent to increase stop by. Must have region.reference_name as a\n      key.\n\n  Returns:\n    nucleus.genomics.v1.Range proto.\n\n  Raises:\n    ValueError: if n_bp is invalid.\n    KeyError: contig_map is not None and region.reference_name isn\'t a key.\n  """"""\n  if n_bp < 0:\n    raise ValueError(\'n_bp must be >= 0 but got {}\'.format(n_bp))\n\n  new_start = max(region.start - n_bp, 0)\n  new_end = region.end + n_bp\n  if contig_map is not None:\n    new_end = min(new_end, contig_map[region.reference_name].n_bases)\n  return make_range(region.reference_name, new_start, new_end)\n\n\ndef span(regions):\n  """"""Returns a region that spans all of the bases in regions.\n\n  This function returns a Range(chrom, start, stop), where start is the min\n  of the starts in regions, and stop is the max end in regions. It may not be\n  freshly allocated.\n\n  Args:\n    regions: list[Range]: a list of Range protos.\n\n  Returns:\n    A single Range proto.\n\n  Raises:\n    ValueError: if not all regions have the same reference_name.\n    ValueError: if regions is empty.\n  """"""\n  if not regions:\n    raise ValueError(\'regions is empty but must have at least one region\')\n  elif len(regions) == 1:\n    return regions[0]\n  elif any(r.reference_name != regions[0].reference_name for r in regions):\n    raise ValueError(\'regions must be all on the same contig\')\n  else:\n    start = min(r.start for r in regions)\n    end = max(r.end for r in regions)\n    return make_range(regions[0].reference_name, start, end)\n\n\ndef length(region):\n  """"""Returns the length in basepairs of region.""""""\n  return region.end - region.start\n'"
nucleus/util/ranges_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for ranges.py.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\n\nfrom nucleus.protos import position_pb2\nfrom nucleus.protos import reference_pb2\nfrom nucleus.protos import variants_pb2\nfrom nucleus.testing import test_utils\nfrom nucleus.util import ranges\n\n_TEST_BED_REGIONS = [\n    ranges.make_range(\'chr1\', 1, 10),\n    ranges.make_range(\'chr2\', 20, 30),\n    ranges.make_range(\'chr2\', 40, 60),\n    ranges.make_range(\'chr3\', 80, 90),\n]\n\n_TEST_CONTIGS = [\n    reference_pb2.ContigInfo(name=\'chr1\', n_bases=10, pos_in_fasta=0),\n    reference_pb2.ContigInfo(name=\'chr2\', n_bases=100, pos_in_fasta=1),\n    reference_pb2.ContigInfo(name=\'chr3\', n_bases=500, pos_in_fasta=2),\n]\n\n\nclass RangesTests(parameterized.TestCase):\n\n  def test_ranges_overlaps(self):\n\n    def check_overlaps(chr1, start1, end1, chr2, start2, end2, expected):\n      i1 = ranges.make_range(chr1, start1, end1)\n      i2 = ranges.make_range(chr2, start2, end2)\n      self.assertEqual(ranges.ranges_overlap(i1, i2), expected)\n      self.assertEqual(ranges.ranges_overlap(i2, i1), expected)\n\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 4, 10, False)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 3, 10, False)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 2, 10, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 1, 10, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 0, 10, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 0, 1, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 0, 2, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 0, 3, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 1, 2, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 1, 3, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 2, 3, True)\n    check_overlaps(\'chr1\', 0, 3, \'chr1\', 3, 3, False)\n    check_overlaps(\'chr1\', 1, 3, \'chr1\', 0, 4, True)\n    check_overlaps(\'chr1\', 1, 3, \'chr1\', 1, 4, True)\n\n  def test_detector_no_ranges(self):\n    range_set = ranges.RangeSet()\n    # don\'t have any ranges by default\n    self.assertEqual(bool(range_set), False)\n    # make sure we can call overlaps without any ranges\n    self.assertFalse(range_set.overlaps(\'chr1\', 10))\n\n  def test_from_regions_not_empty(self):\n    literals = [\'chr1\', \'chr2:10-20\']\n    self.assertItemsEqual(\n        [ranges.make_range(\'chr1\', 0, 10),\n         ranges.make_range(\'chr2\', 9, 20)],\n        ranges.RangeSet.from_regions(\n            literals, ranges.contigs_dict(_TEST_CONTIGS)))\n\n  def test_from_regions_empty_literals(self):\n    range_set = ranges.RangeSet.from_regions([])\n    # The set is empty.\n    self.assertItemsEqual([], range_set)\n    self.assertFalse(range_set)\n\n  def test_unrecognized_contig_triggers_exception(self):\n    with self.assertRaises(ValueError):\n      _ = ranges.RangeSet([ranges.make_range(\'bogus_chromosome\', 1, 10)],\n                          _TEST_CONTIGS)\n\n  @parameterized.parameters(\n      # Overlapping intervals get merged.\n      ([\'1:1-5\', \'1:3-8\'], [\'1:1-8\']),\n      ([\'1:1-5\', \'1:3-8\', \'1:6-9\'], [\'1:1-9\']),\n      # Adjacent intervals are merged.\n      ([\'1:1-5\', \'1:5-8\'], [\'1:1-8\']),\n      ([\'1:1-5\', \'1:5-8\', \'1:8-10\'], [\'1:1-10\']),\n      # Sanity check that non-overlapping aren\'t merged.\n      ([\'1:1-5\', \'1:6-8\'], [\'1:1-5\', \'1:6-8\']),\n  )\n  def test_overlapping_and_adjacent_ranges_are_merged(self, regions, expected):\n    self.assertCountEqual(\n        ranges.RangeSet.from_regions(expected),\n        ranges.RangeSet.from_regions(regions))\n\n  def test_detector_ranges(self):\n    test_ranges = [\n        ranges.make_range(\'chr1\', 0, 5),\n        ranges.make_range(\'chr1\', 8, 10),\n        ranges.make_range(\'chr1\', 12, 13),\n        ranges.make_range(\'chr2\', 2, 5),\n    ]\n    range_set = ranges.RangeSet(test_ranges)\n    self.assertEqual(bool(range_set), True)\n    self.assertEqual(len(range_set), 4)\n\n    self.assertEqual(range_set.overlaps(\'chr1\', 0), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 1), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 2), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 3), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 4), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 5), False)\n    self.assertEqual(range_set.overlaps(\'chr1\', 6), False)\n    self.assertEqual(range_set.overlaps(\'chr1\', 7), False)\n    self.assertEqual(range_set.overlaps(\'chr1\', 8), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 9), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 10), False)\n    self.assertEqual(range_set.overlaps(\'chr1\', 11), False)\n    self.assertEqual(range_set.overlaps(\'chr1\', 12), True)\n    self.assertEqual(range_set.overlaps(\'chr1\', 13), False)\n    self.assertEqual(range_set.overlaps(\'chr1\', 100), False)\n    self.assertEqual(range_set.overlaps(\'chr1\', 1000), False)\n    self.assertEqual(range_set.overlaps(\'chr2\', 0), False)\n    self.assertEqual(range_set.overlaps(\'chr2\', 1), False)\n    self.assertEqual(range_set.overlaps(\'chr2\', 2), True)\n    self.assertEqual(range_set.overlaps(\'chr2\', 3), True)\n    self.assertEqual(range_set.overlaps(\'chr2\', 4), True)\n    self.assertEqual(range_set.overlaps(\'chr2\', 5), False)\n    self.assertEqual(range_set.overlaps(\'chr2\', 6), False)\n    self.assertEqual(range_set.overlaps(\'chr3\', 3), False)\n\n  def test_overlaps_variant_with_ranges(self):\n    variant = variants_pb2.Variant(reference_name=\'chr2\', start=10, end=11)\n    range_set = ranges.RangeSet([ranges.make_range(\'chr1\', 0, 5)])\n    with mock.patch.object(range_set, \'overlaps\') as mock_overlaps:\n      mock_overlaps.return_value = True\n      self.assertEqual(range_set.variant_overlaps(variant), True)\n      mock_overlaps.assert_called_once_with(\'chr2\', 10)\n\n  def test_overlaps_variant_empty_range(self):\n    variant = variants_pb2.Variant(reference_name=\'chr2\', start=10, end=11)\n    empty_set = ranges.RangeSet()\n    self.assertEqual(\n        empty_set.variant_overlaps(variant, empty_set_return_value=\'foo\'),\n        \'foo\')\n\n  def test_envelops(self):\n    start_ix = 5\n    end_ix = 10\n    start_ix2 = end_ix + 1\n    end_ix2 = end_ix + 5\n    range_set = ranges.RangeSet([\n        ranges.make_range(\'chr1\', start_ix, end_ix),\n        ranges.make_range(\'chr1\', start_ix2, end_ix2)\n    ])\n\n    # No start position before the first start range is enveloped.\n    for i in range(start_ix):\n      self.assertFalse(range_set.envelops(\'chr1\', i, start_ix + 1))\n\n    # All regions within a single record are enveloped.\n    for six in range(start_ix, end_ix):\n      for eix in range(six, end_ix + 1):\n        self.assertTrue(\n            range_set.envelops(\'chr1\', six, eix),\n            \'chr1 {} {} not enveloped\'.format(six, eix))\n\n    # Bridging across two ranges is not enveloped.\n    for six in range(start_ix, end_ix):\n      for eix in range(start_ix2, end_ix2 + 1):\n        self.assertFalse(range_set.envelops(\'chr1\', six, eix))\n\n    # Other chromosome is not spanned.\n    self.assertFalse(range_set.envelops(\'chr2\', start_ix, start_ix + 1))\n\n  @parameterized.parameters(\n      (ranges.make_range(\'1\', 10, 50), \'1\', 9, False),\n      (ranges.make_range(\'1\', 10, 50), \'1\', 10, True),\n      (ranges.make_range(\'1\', 10, 50), \'2\', 10, False),\n      (ranges.make_range(\'1\', 10, 50), \'1\', 30, True),\n      (ranges.make_range(\'1\', 10, 50), \'2\', 30, False),\n      (ranges.make_range(\'1\', 10, 50), \'1\', 49, True),\n      (ranges.make_range(\'1\', 10, 50), \'1\', 50, False),\n      (ranges.make_range(\'1\', 10, 50), \'1\', 51, False),\n  )\n  def test_position_overlaps(self, interval, chrom, pos, expected):\n    self.assertEqual(ranges.position_overlaps(chrom, pos, interval), expected)\n\n  def test_make_position(self):\n    self.assertEqual(\n        ranges.make_position(\'chr1\', 10),\n        position_pb2.Position(\n            reference_name=\'chr1\', position=10, reverse_strand=False))\n    self.assertEqual(\n        ranges.make_position(\'chr2\', 100, reverse_strand=True),\n        position_pb2.Position(\n            reference_name=\'chr2\', position=100, reverse_strand=True))\n\n  def test_make_range(self):\n    interval = ranges.make_range(\'chr1\', 1, 10)\n    self.assertEqual(interval.reference_name, \'chr1\')\n    self.assertEqual(interval.start, 1)\n    self.assertEqual(interval.end, 10)\n\n  def test_to_literal(self):\n    self.assertEqual(\n        ranges.to_literal(ranges.make_range(\'chr1\', 0, 20)), \'chr1:1-20\')\n\n  @parameterized.parameters([\'chr1\', \'1\', \'MT\', \'chrM\', \'chrX\', \'X\', \'Y\'])\n  def test_parse_literal_chromosomes(self, chrom):\n    self.assertEqual(\n        ranges.parse_literal(chrom + \':1-20\'), ranges.make_range(chrom, 0, 20))\n\n  @parameterized.parameters(\n      (\'chr1:{}-{}\'.format(start_str, end_str), start_val, end_val)\n      for start_str, start_val in [(\'12\', 11), (\'1,234\', 1233)]\n      for end_str, end_val in [(\'56789\', 56789), (\'56,789\', 56789)])\n  def test_parse_literal_numerics(self, literal, start_val, end_val):\n    self.assertEqual(\n        ranges.parse_literal(literal),\n        ranges.make_range(\'chr1\', start_val, end_val))\n\n  def test_parse_literal_one_bp(self):\n    self.assertEqual(\n        ranges.parse_literal(\'1:10\'), ranges.make_range(\'1\', 9, 10))\n    self.assertEqual(\n        ranges.parse_literal(\'1:100\'), ranges.make_range(\'1\', 99, 100))\n    self.assertEqual(\n        ranges.parse_literal(\'1:1,000\'), ranges.make_range(\'1\', 999, 1000))\n\n  @parameterized.parameters([\'x\', \'chr1\', \'chr1:\', \'chr1:10-\', \'chr1:-1-10\'])\n  def test_parse_literal_bad(self, bad_literal):\n    with self.assertRaisesRegexp(ValueError, bad_literal):\n      ranges.parse_literal(bad_literal)\n\n  @parameterized.parameters(\'test.bed\', \'test.bed.gz\')\n  def test_from_bed(self, bed_filename):\n    source = test_utils.genomics_core_testdata(bed_filename)\n    self.assertCountEqual([\n        ranges.make_range(\'chr1\', 1, 10),\n        ranges.make_range(\'chr2\', 20, 30),\n        ranges.make_range(\'chr2\', 40, 60),\n        ranges.make_range(\'chr3\', 80, 90),\n    ], ranges.RangeSet.from_bed(source))\n\n  @parameterized.parameters(\n      dict(regions=[], expected=[]),\n      dict(regions=[\'chr1:10-20\'], expected=[ranges.make_range(\'chr1\', 9, 20)]),\n      dict(regions=[\'test.bed\'], expected=_TEST_BED_REGIONS),\n      dict(\n          regions=[\'test.bed\', \'test.bed\'],\n          expected=_TEST_BED_REGIONS + _TEST_BED_REGIONS),\n      dict(\n          regions=[\'chr1:10-20\', \'test.bed\'],\n          expected=[ranges.make_range(\'chr1\', 9, 20)] + _TEST_BED_REGIONS),\n      dict(\n          regions=[\'test.bed\', \'chr1:10-20\'],\n          expected=_TEST_BED_REGIONS + [ranges.make_range(\'chr1\', 9, 20)]),\n      dict(\n          regions=[\'chr1:9-19\', \'test.bed\', \'chr1:10-20\'],\n          expected=([ranges.make_range(\'chr1\', 8, 19)] + _TEST_BED_REGIONS +\n                    [ranges.make_range(\'chr1\', 9, 20)])),\n  )\n  def test_from_regions(self, regions, expected):\n    # For convenience we allow \'test.bed\' in our regions but the actual file\n    # path is in our testdata directory.\n    for i in range(len(regions)):\n      if regions[i] == \'test.bed\':\n        regions[i] = test_utils.genomics_core_testdata(\'test.bed\')\n\n    self.assertEqual(list(ranges.from_regions(regions)), expected)\n\n  @parameterized.parameters(\n      # Intersection with 1, 2, 3 identical RangeSets produces the original set.\n      ([[\'1:1-10\']], [\'1:1-10\']),\n      ([[\'1:1-10\'], [\'1:1-10\']], [\'1:1-10\']),\n      ([[\'1:1-10\'], [\'1:1-10\'], [\'1:1-10\']], [\'1:1-10\']),\n      # Test some simple overlap configurations.\n      ([[\'1:1-10\'], [\'1:11-15\']], []),\n      ([[\'1:1-10\'], [\'1:10-15\']], [\'1:10\']),\n      ([[\'1:1-10\'], [\'1:9-15\']], [\'1:9-10\']),\n      ([[\'1:5-10\'], [\'1:1-15\']], [\'1:5-10\']),\n      ([[\'1:5-10\'], [\'1:1-4\']], []),\n      ([[\'1:5-10\'], [\'1:1-5\']], [\'1:5\']),\n      # Check cutting a single interval into multiple pieces.\n      ([[\'1:5-15\'], [\'1:6-8\', \'1:10-12\']], [\'1:6-8\', \'1:10-12\']),\n      ([[\'1:5-15\'], [\'1:3-8\', \'1:10-12\']], [\'1:5-8\', \'1:10-12\']),\n      ([[\'1:5-15\'], [\'1:3-8\', \'1:10-20\']], [\'1:5-8\', \'1:10-15\']),\n      # We have multiple overlapping intervals; make sure we merge intervals.\n      ([[\'1:5-15\'], [\'1:3-8\', \'1:6-10\']], [\'1:5-10\']),\n      ([[\'1:5-15\'], [\'1:3-8\', \'1:6-10\', \'1:13\']], [\'1:5-10\', \'1:13\']),\n      # Check that multiple intervals work.\n      ([[\'1:5-15\', \'1:20-25\'], [\'1:3-8\', \'1:16-23\']], [\'1:5-8\', \'1:20-23\']),\n      ([[\'1:5-15\', \'1:20-25\'], [\'1:3-8\', \'1:50-60\']], [\'1:5-8\']),\n      ([[\'1:5-15\', \'1:20-25\'], [\'1:3-4\', \'1:16-23\']], [\'1:20-23\']),\n      # Check that multiple sets can be intersected.\n      ([[\'1:10-20\'], [\'1:5-15\']], [\'1:10-15\']),\n      ([[\'1:10-20\'], [\'1:5-15\'], [\'1:13-30\']], [\'1:13-15\']),\n      ([[\'1:10-20\'], [\'1:5-15\'], [\'1:25-30\']], []),\n      # Check that different chromosomes are kept separate.\n      ([[\'1:10-20\'], [\'2:10-20\']], []),\n      ([[\'1:10-20\', \'2:11-14\'], [\'1:11-14\']], [\'1:11-14\']),\n      ([[\'1:10-20\', \'2:11-14\'], [\'2:10-20\']], [\'2:11-14\']),\n  )\n  def test_intersection(self, regions, expected):\n    regions_list = [ranges.RangeSet.from_regions(r) for r in regions]\n    copies = [ranges.RangeSet(rs) for rs in regions_list]\n\n    # Check that the intersection is as expected.\n    self.assertCountEqual(\n        ranges.RangeSet.from_regions(expected),\n        regions_list[0].intersection(*regions_list[1:]))\n\n    # Check that the intersection is as expected even if we do it in a different\n    # direction.\n    self.assertCountEqual(\n        ranges.RangeSet.from_regions(expected),\n        regions_list[-1].intersection(*regions_list[:-1]))\n\n    # Check that no one was modified.\n    for pre, post in zip(copies, regions_list):\n      self.assertCountEqual(pre, post)\n\n  @parameterized.parameters(\n      dict(lhs=[\'1:1-100\'], rhs=[\'1:10-20\'], expected=[\'1:1-9\', \'1:21-100\']),\n      dict(lhs=[\'1:1-100\'], rhs=[], expected=[\'1:1-100\']),\n      dict(lhs=[\'1:1-100\', \'2:1-10\'], rhs=[\'2:1-100\'], expected=[\'1:1-100\']),\n      dict(\n          lhs=[\'1:1-100\'],\n          rhs=[\'1:10-20\', \'1:15-30\'],\n          expected=[\'1:1-9\', \'1:31-100\']),\n      dict(\n          lhs=[\'1:1-100\'],\n          rhs=[\'1:10-20\', \'1:30-40\'],\n          expected=[\'1:1-9\', \'1:21-29\', \'1:41-100\']),\n      # Excluding regions not in lhs has no impact.\n      dict(lhs=[\'1:1-100\'], rhs=[\'2:1-100\'], expected=[\'1:1-100\']),\n      # Check that excluding the whole region results in an empty RangeSet.\n      dict(lhs=[\'1:1-100\'], rhs=[\'1:1-100\'], expected=[]),\n      # An empty tree remains empty.\n      dict(lhs=[], rhs=[\'1:1-100\'], expected=[]),\n  )\n  def test_exclude_regions(self, lhs, rhs, expected):\n    lhs = ranges.RangeSet.from_regions(lhs)\n    rhs = ranges.RangeSet.from_regions(rhs)\n    # Mutating operation returns None.\n    self.assertIsNone(lhs.exclude_regions(rhs))\n    self.assertCountEqual(ranges.RangeSet.from_regions(expected), lhs)\n\n  @parameterized.parameters((\'chr1\', ranges.make_range(\'chr1\', 0, 10)),\n                            (\'chr2\', ranges.make_range(\'chr2\', 0, 5)))\n  def test_parse_literal_with_contig_map(self, contig_name, expected):\n    contig_map = {\n        \'chr1\': reference_pb2.ContigInfo(name=\'chr1\', n_bases=10),\n        \'chr2\': reference_pb2.ContigInfo(name=\'chr2\', n_bases=5),\n    }\n    self.assertEqual(\n        ranges.parse_literal(contig_name, contig_map=contig_map), expected)\n\n  @parameterized.parameters([\'x\', \'chr1:\', \'chr1:10-\', \'chr1:-1-10\'])\n  def test_parse_literal_with_contig_map_and_bad_input_raises_exception(\n      self, bad_literal):\n    with self.assertRaises(ValueError):\n      ranges.parse_literal(\n          bad_literal,\n          contig_map={\n              \'chr1\': reference_pb2.ContigInfo(name=\'chr1\', n_bases=10)\n          })\n\n  def test_from_contigs(self):\n    contigs = [\n        reference_pb2.ContigInfo(name=\'chr1\', n_bases=10),\n        reference_pb2.ContigInfo(name=\'chr2\', n_bases=5),\n    ]\n    self.assertCountEqual([\n        ranges.make_range(\'chr1\', 0, 10),\n        ranges.make_range(\'chr2\', 0, 5),\n    ], ranges.RangeSet.from_contigs(contigs))\n\n  @parameterized.parameters(\n      # Chop our contigs into 50 bp pieces.\n      (50, [(\'chr1\', 0, 50), (\'chr1\', 50, 76), (\'chr2\', 0, 50),\n            (\'chr2\', 50, 100), (\'chr2\', 100, 121), (\'chrM\', 0, 50),\n            (\'chrM\', 50, 100)]),\n      # Chop our contigs in 120 bp pieces, leaving a 1 bp fragment in chr2.\n      (120, [(\'chr1\', 0, 76), (\'chr2\', 0, 120), (\'chr2\', 120, 121),\n             (\'chrM\', 0, 100)]),\n      # A 500 max size spans each of our contigs fully.\n      (500, [(\'chr1\', 0, 76), (\'chr2\', 0, 121), (\'chrM\', 0, 100)]),\n  )\n  def test_partitions(self, interval_size, expected):\n    rangeset = ranges.RangeSet([\n        ranges.make_range(\'chrM\', 0, 100),\n        ranges.make_range(\'chr1\', 0, 76),\n        ranges.make_range(\'chr2\', 0, 121),\n    ])\n    self.assertEqual([ranges.make_range(*args) for args in expected],\n                     list(rangeset.partition(interval_size)))\n\n  def test_partitions_bad_interval_size_raises(self):\n    # list() is necessary to force the generator to execute.\n    with self.assertRaisesRegexp(ValueError, \'max_size\'):\n      list(ranges.RangeSet([ranges.make_range(\'chrM\', 0, 100)]).partition(-10))\n    with self.assertRaisesRegexp(ValueError, \'max_size\'):\n      list(ranges.RangeSet([ranges.make_range(\'chrM\', 0, 100)]).partition(0))\n\n  @parameterized.parameters(\n      (10, [(\'1\', 0, 10), (\'1\', 20, 30), (\'1\', 30, 40), (\'1\', 45, 50)]),\n      (7, [(\'1\', 0, 7), (\'1\', 7, 10), (\'1\', 20, 27), (\'1\', 27, 34),\n           (\'1\', 34, 40), (\'1\', 45, 50)]),\n      (50, [(\'1\', 0, 10), (\'1\', 20, 40), (\'1\', 45, 50)]),\n  )\n  def test_partition_of_multiple_intervals(self, interval_size, expected):\n    rangeset = ranges.RangeSet([\n        ranges.make_range(\'1\', 0, 10),\n        ranges.make_range(\'1\', 20, 40),\n        ranges.make_range(\'1\', 45, 50),\n    ])\n    self.assertCountEqual([ranges.make_range(*args) for args in expected],\n                          rangeset.partition(interval_size))\n\n  def test_bed_parser(self):\n    test_bed_path = test_utils.test_tmpfile(\n        \'test_bed_parser.bed\', \'\\n\'.join([\n            \'chr20\\t61724611\\t61725646\', \'chr20\\t61304163\\t61305182\',\n            \'chr20\\t61286467\\t61286789\'\n        ]))\n    self.assertEqual(\n        list(ranges.bed_parser(test_bed_path)), [\n            ranges.make_range(\'chr20\', 61724611, 61725646),\n            ranges.make_range(\'chr20\', 61304163, 61305182),\n            ranges.make_range(\'chr20\', 61286467, 61286789),\n        ])\n\n  def test_bedpe_parser(self):\n    # pylint: disable=line-too-long\n    data = \'\\n\'.join([\n        \'chr20\\t25763416\\t25765517\\tchr20\\t25825181\\t25826882\\tP2_PM_20_1549\\t63266\\t+\\tTYPE:DELETION\',\n        \'chr20\\t25972820\\t25972991\\tchr20\\t26045347\\t26045538\\tP2_PM_20_696\\t72548\\t+\\tTYPE:DELETION\',\n        \'chr20\\t23719873\\t23721974\\tchr20\\t23794822\\t23796523\\tP2_PM_20_1548\\t76450\\t+\\tTYPE:DELETION\',\n    ])\n    test_bedpe_path = test_utils.test_tmpfile(\'test_bedpe_parser.bedpe\', data)\n    self.assertEqual(\n        list(ranges.bedpe_parser(test_bedpe_path)), [\n            ranges.make_range(\'chr20\', 25763416, 25826882),\n            ranges.make_range(\'chr20\', 25972820, 26045538),\n            ranges.make_range(\'chr20\', 23719873, 23796523),\n        ])\n\n  def test_bedpe_parser_skips_cross_chr_events(self):\n    # pylint: disable=line-too-long\n    data = \'\\n\'.join([\n        \'chr20\\t25763416\\t25765517\\tchr21\\t25825181\\t25826882\\tP2_PM_20_1549\\t63266\\t+\\tTYPE:DELETION\',\n        \'chr20\\t25972820\\t25972991\\tchr20\\t26045347\\t26045538\\tP2_PM_20_696\\t72548\\t+\\tTYPE:DELETION\',\n        \'chr20\\t23719873\\t23721974\\tchr20\\t23794822\\t23796523\\tP2_PM_20_1548\\t76450\\t+\\tTYPE:DELETION\',\n    ])\n    test_bedpe_path = test_utils.test_tmpfile(\'test_bedpe_parser2.bedpe\', data)\n    self.assertEqual(\n        list(ranges.bedpe_parser(test_bedpe_path)), [\n            ranges.make_range(\'chr20\', 25972820, 26045538),\n            ranges.make_range(\'chr20\', 23719873, 23796523),\n        ])\n\n  def test_contigs_n_bases(self):\n    c1 = reference_pb2.ContigInfo(name=\'c\', n_bases=100, pos_in_fasta=0)\n    c2 = reference_pb2.ContigInfo(name=\'a\', n_bases=50, pos_in_fasta=1)\n    c3 = reference_pb2.ContigInfo(name=\'b\', n_bases=25, pos_in_fasta=2)\n    self.assertEqual(100, ranges.contigs_n_bases([c1]))\n    self.assertEqual(50, ranges.contigs_n_bases([c2]))\n    self.assertEqual(25, ranges.contigs_n_bases([c3]))\n    self.assertEqual(150, ranges.contigs_n_bases([c1, c2]))\n    self.assertEqual(125, ranges.contigs_n_bases([c1, c3]))\n    self.assertEqual(175, ranges.contigs_n_bases([c1, c2, c3]))\n\n  def test_rangeset_iteration_order(self):\n    contigs = [\n        reference_pb2.ContigInfo(name=\'c\', n_bases=100, pos_in_fasta=0),\n        reference_pb2.ContigInfo(name=\'b\', n_bases=121, pos_in_fasta=2),\n        reference_pb2.ContigInfo(name=\'a\', n_bases=76, pos_in_fasta=1),\n    ]\n    unsorted = ranges.parse_literals(\n        [\'a:10\', \'c:20\', \'b:30\', \'b:10-15\', \'a:5\'])\n\n    # Iteration order over a RangeSet instantiated with a contigs list is\n    # determined by pos_in_fasta, start, end.\n    range_set_with_contigs = ranges.RangeSet(unsorted, contigs)\n    self.assertEqual(\n        ranges.parse_literals(\n            [\'c:20\', \'a:5\', \'a:10\', \'b:10-15\', \'b:30\']),\n        [range_ for range_ in range_set_with_contigs])\n\n    # For a RangeSet instantiated *without* a contig map, the iteration order\n    # is determined by reference_name, start, end.\n    range_set_no_contigs = ranges.RangeSet(unsorted)\n    self.assertEqual(\n        ranges.parse_literals(\n            [\'a:5\', \'a:10\', \'b:10-15\', \'b:30\', \'c:20\']),\n        [range_ for range_ in range_set_no_contigs])\n\n  def test_sort_ranges(self):\n    contigs = [\n        reference_pb2.ContigInfo(name=\'c\', n_bases=100, pos_in_fasta=0),\n        reference_pb2.ContigInfo(name=\'a\', n_bases=76, pos_in_fasta=1),\n        reference_pb2.ContigInfo(name=\'b\', n_bases=121, pos_in_fasta=2),\n    ]\n    unsorted = ranges.parse_literals(\n        [\'a:10\', \'c:20\', \'b:30\', \'b:10-15\', \'b:10\', \'a:5\'])\n\n    # Without contigs we sort the contigs by name lexicographically.\n    self.assertEqual(\n        ranges.parse_literals(\n            [\'a:5\', \'a:10\', \'b:10\', \'b:10-15\', \'b:30\', \'c:20\']),\n        ranges.sorted_ranges(unsorted))\n\n    # With contigs we sort by the position of the contigs themselves.\n    self.assertEqual(\n        ranges.parse_literals(\n            [\'c:20\', \'a:5\', \'a:10\', \'b:10\', \'b:10-15\', \'b:30\']),\n        ranges.sorted_ranges(unsorted, contigs))\n\n  @parameterized.parameters(\n      (ranges.make_range(\'1\', 0, 10), ranges.make_range(\'2\', 0, 10), 0),\n      (ranges.make_range(\'1\', 0, 10), ranges.make_range(\'1\', 10, 20), 0),\n      (ranges.make_range(\'1\', 0, 10), ranges.make_range(\'1\', 100, 200), 0),\n      (ranges.make_range(\'1\', 10, 10), ranges.make_range(\'1\', 0, 20), 0),\n      (ranges.make_range(\'1\', 0, 100), ranges.make_range(\'1\', 50, 99), 49),\n      # Check that the overlap handles a few key edge cases.\n      (ranges.make_range(\'1\', 0, 10), ranges.make_range(\'1\', 0, 1), 1),\n      (ranges.make_range(\'1\', 0, 10), ranges.make_range(\'1\', 0, 2), 2),\n      (ranges.make_range(\'1\', 1, 10), ranges.make_range(\'1\', 0, 1), 0),\n  )\n  def test_overlap_len(self, region_1, region_2, expected_overlap):\n    """"""Test ReadAssigner.overlap_len().""""""\n    self.assertEqual(expected_overlap, ranges.overlap_len(region_1, region_2))\n    self.assertEqual(expected_overlap, ranges.overlap_len(region_2, region_1))\n\n  @parameterized.parameters(\n      # No search_regions produces None.\n      dict(\n          query_range=ranges.make_range(\'1\', 20, 30),\n          search_ranges=[],\n          expected=None),\n\n      # Read overlaps with none of the ranges returns None.\n      dict(\n          query_range=ranges.make_range(\'1\', 20, 30),\n          search_ranges=[\n              ranges.make_range(\'1\', 0, 10),\n              ranges.make_range(\'1\', 5, 10)\n          ],\n          expected=None),\n\n      # Read has longer overlap with the first range.\n      dict(\n          query_range=ranges.make_range(\'1\', 4, 10),\n          search_ranges=[\n              ranges.make_range(\'1\', 0, 10),\n              ranges.make_range(\'1\', 5, 10)\n          ],\n          expected=0),\n\n      # Read has longer overlap with the second range.\n      dict(\n          query_range=ranges.make_range(\'1\', 9, 20),\n          search_ranges=[\n              ranges.make_range(\'1\', 0, 10),\n              ranges.make_range(\'1\', 5, 15)\n          ],\n          expected=1),\n\n      # Read has the maximum overlap with the third range.\n      dict(\n          query_range=ranges.make_range(\'1\', 9, 20),\n          search_ranges=[\n              ranges.make_range(\'1\', 0, 10),\n              ranges.make_range(\'1\', 0, 15),\n              ranges.make_range(\'1\', 5, 20)\n          ],\n          expected=2),\n\n      # Read has the maximum overlap with the middle range.\n      dict(\n          query_range=ranges.make_range(\'1\', 5, 13),\n          search_ranges=[\n              ranges.make_range(\'1\', 0, 10),\n              ranges.make_range(\'1\', 0, 15),\n              ranges.make_range(\'1\', 10, 20)\n          ],\n          expected=1),\n\n      # Read has a different reference_name with other ranges.\n      dict(\n          query_range=ranges.make_range(\'2\', 0, 10),\n          search_ranges=[\n              ranges.make_range(\'1\', 0, 10),\n              ranges.make_range(\'2\', 5, 15),\n              ranges.make_range(\'3\', 0, 10)\n          ],\n          expected=1),\n\n      # Read has equal overlap in two ranges.\n      dict(\n          query_range=ranges.make_range(\'1\', 5, 15),\n          search_ranges=[\n              ranges.make_range(\'1\', 0, 10),\n              ranges.make_range(\'1\', 10, 20),\n              ranges.make_range(\'1\', 12, 20)\n          ],\n          expected=0),\n  )\n  def test_find_max_overlapping(self, query_range, search_ranges, expected):\n    actual = ranges.find_max_overlapping(query_range, search_ranges)\n    self.assertEqual(expected, actual)\n\n  def test_find_max_overlapping_allows_unordered_search_ranges(self):\n    query_range = ranges.make_range(\'1\', 4, 12)\n    search_ranges = [\n        ranges.make_range(\'1\', 0, 10),\n        ranges.make_range(\'1\', 10, 20),\n        ranges.make_range(\'1\', 12, 20)\n    ]\n    max_overlapping_range = search_ranges[0]\n\n    for permutated_ranges in itertools.permutations(search_ranges):\n      self.assertEqual(\n          permutated_ranges.index(max_overlapping_range),\n          ranges.find_max_overlapping(query_range, permutated_ranges))\n\n  def test_find_max_overlapping_returns_least_index(self):\n    query_range = ranges.make_range(\'1\', 0, 10)\n    search_ranges = [\n        ranges.make_range(\'1\', 0, 5),\n        ranges.make_range(\'1\', 5, 10)\n    ]\n\n    for to_search in [search_ranges, list(reversed(search_ranges))]:\n      self.assertEqual(0, ranges.find_max_overlapping(query_range, to_search))\n\n  @parameterized.parameters(\n      dict(\n          regions=[\n              ranges.make_range(\'1\', 1, 10),\n          ],\n          expected_span=ranges.make_range(\'1\', 1, 10),\n      ),\n      dict(\n          regions=[\n              ranges.make_range(\'1\', 1, 10),\n              ranges.make_range(\'1\', 10, 100),\n          ],\n          expected_span=ranges.make_range(\'1\', 1, 100),\n      ),\n      dict(\n          regions=[\n              ranges.make_range(\'1\', 1, 10),\n              ranges.make_range(\'1\', 10, 100),\n              ranges.make_range(\'1\', 2, 20),\n          ],\n          expected_span=ranges.make_range(\'1\', 1, 100),\n      ),\n      # potential edge cases:\n      # same start, different ends.\n      dict(\n          regions=[\n              ranges.make_range(\'1\', 1, 10),\n              ranges.make_range(\'1\', 1, 100),\n          ],\n          expected_span=ranges.make_range(\'1\', 1, 100),\n      ),\n      # same end, different starts.\n      dict(\n          regions=[\n              ranges.make_range(\'1\', 1, 10),\n              ranges.make_range(\'1\', 2, 10),\n          ],\n          expected_span=ranges.make_range(\'1\', 1, 10),\n      ),\n  )\n  def test_span_computes_span_correctly(self, regions, expected_span):\n    for permutation in itertools.permutations(regions, len(regions)):\n      self.assertEqual(expected_span, ranges.span(permutation))\n\n  @parameterized.parameters(\n      dict(regions=[], regexp=\'empty\'),\n      dict(\n          regions=[\n              ranges.make_range(\'1\', 0, 2),\n              ranges.make_range(\'2\', 0, 2),\n          ],\n          regexp=\'regions must be all on the same contig\'),\n      dict(\n          regions=[\n              ranges.make_range(\'1\', 0, 2),\n              ranges.make_range(\'1\', 0, 3),\n              ranges.make_range(\'2\', 0, 2),\n          ],\n          regexp=\'regions must be all on the same contig\'),\n  )\n  def test_span_raises_on_bad_input(self, regions, regexp):\n    with self.assertRaisesRegexp(ValueError, regexp):\n      ranges.span(regions)\n\n  @parameterized.parameters(\n      dict(\n          region=ranges.make_range(\'1\', 10, 20),\n          n_bp=n_bp,\n          contig_map=None,\n          expected=ranges.make_range(\'1\', 10 - n_bp, 20 + n_bp),\n      ) for n_bp in range(10))\n  def test_expand_is_correct(self, region, n_bp, contig_map, expected):\n    self.assertEqual(expected, ranges.expand(region, n_bp, contig_map))\n\n  @parameterized.parameters(\n      # Check that we don\'t create Ranges with negative starts.\n      dict(\n          region=ranges.make_range(\'1\', 10, 20),\n          n_bp=20,\n          contig_map=None,\n          expected=ranges.make_range(\'1\', 0, 40),\n      ),\n      # Check that we respect n_bp if contig_map is provided.\n      dict(\n          region=ranges.make_range(\'1\', 10, 20),\n          n_bp=40,\n          contig_map={\n              \'1\': reference_pb2.ContigInfo(name=\'1\', n_bases=50),\n          },\n          expected=ranges.make_range(\'1\', 0, 50),\n      ),\n  )\n  def test_expand_handles_boundaries(self, region, n_bp, contig_map, expected):\n    self.assertEqual(expected, ranges.expand(region, n_bp, contig_map))\n\n  def test_expand_raises_on_negative_n_bp(self):\n    with self.assertRaisesRegexp(ValueError, \'n_bp must be >= 0 but got -10\'):\n      ranges.expand(ranges.make_range(\'1\', 10, 20), -10)\n\n  def test_expand_raises_with_missing_contig_in_map(self):\n    # Empty contig_map should raise.\n    with self.assertRaises(KeyError):\n      ranges.expand(ranges.make_range(\'1\', 10, 20), 1, contig_map={})\n\n    # Missing \'1\' from the contig map should raise.\n    with self.assertRaises(KeyError):\n      ranges.expand(\n          ranges.make_range(\'1\', 10, 20),\n          1,\n          contig_map={\n              \'2\': reference_pb2.ContigInfo(name=\'2\', n_bases=50),\n          })\n\n  @parameterized.parameters(\n      dict(\n          region=ranges.make_range(chrom, start, start + length),\n          expected_length=length,\n      )\n      for length in range(10)\n      for start in [10, 20, 1000]\n      for chrom in [\'1\', \'20\']\n  )\n  def test_length_is_correct(self, region, expected_length):\n    self.assertEqual(expected_length, ranges.length(region))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/util/sequence_utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utility functions for manipulating DNA sequences.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass Error(Exception):\n  """"""Base error class.""""""\n\n\ndef _add_lowercase(d):\n  """"""Returns a dictionary with the lowercase keys and values entered.""""""\n  retval = d.copy()\n  retval.update({k.lower(): v.lower() for k, v in d.items()})\n  return retval\n\n\nSTRICT_DNA_COMPLEMENT_UPPER = {\'A\': \'T\', \'T\': \'A\', \'C\': \'G\', \'G\': \'C\'}\nDNA_COMPLEMENT_UPPER = {\'A\': \'T\', \'T\': \'A\', \'C\': \'G\', \'G\': \'C\', \'N\': \'N\'}\nIUPAC_DNA_COMPLEMENT_UPPER = {\n    \'A\': \'T\',\n    \'T\': \'A\',\n    \'C\': \'G\',\n    \'G\': \'C\',\n    \'R\': \'Y\',  # R is A/G\n    \'Y\': \'R\',  # Y is C/T\n    \'S\': \'S\',  # S is C/G\n    \'W\': \'W\',  # W is A/T\n    \'K\': \'M\',  # K is G/T\n    \'M\': \'K\',  # M is A/C\n    \'B\': \'V\',  # B is C/G/T\n    \'V\': \'B\',  # V is A/C/G\n    \'D\': \'H\',  # D is A/G/T\n    \'H\': \'D\',  # H is A/C/T\n    \'N\': \'N\',  # N is any base\n}\n\nIUPAC_TO_CANONICAL_BASES_UPPER = {\n    \'A\': [\'A\'],\n    \'T\': [\'T\'],\n    \'C\': [\'C\'],\n    \'G\': [\'G\'],\n    \'R\': [\'A\', \'G\'],\n    \'Y\': [\'C\', \'T\'],\n    \'S\': [\'C\', \'G\'],\n    \'W\': [\'A\', \'T\'],\n    \'K\': [\'G\', \'T\'],\n    \'M\': [\'A\', \'C\'],\n    \'B\': [\'C\', \'G\', \'T\'],\n    \'V\': [\'A\', \'C\', \'G\'],\n    \'D\': [\'A\', \'G\', \'T\'],\n    \'H\': [\'A\', \'C\', \'T\'],\n    \'N\': [\'A\', \'C\', \'G\', \'T\'],\n}\n\nSTRICT_DNA_COMPLEMENT = _add_lowercase(STRICT_DNA_COMPLEMENT_UPPER)\nDNA_COMPLEMENT = _add_lowercase(DNA_COMPLEMENT_UPPER)\nIUPAC_DNA_COMPLEMENT = _add_lowercase(IUPAC_DNA_COMPLEMENT_UPPER)\n\n\nSTRICT_DNA_BASES_UPPER = frozenset([\'A\', \'C\', \'G\', \'T\'])\nSTRICT_DNA_BASES = frozenset([\'a\', \'c\', \'g\', \'t\', \'A\', \'C\', \'G\', \'T\'])\nDNA_BASES_UPPER = frozenset([\'A\', \'C\', \'G\', \'T\', \'N\'])\nDNA_BASES = frozenset([\'a\', \'c\', \'g\', \'t\', \'n\', \'A\', \'C\', \'G\', \'T\', \'N\'])\n\n\ndef reverse_complement(sequence, complement_dict=None):\n  """"""Returns the reverse complement of a DNA sequence.\n\n  By default this will successfully reverse complement sequences comprised\n  solely of A, C, G, and T letters. Other complement dictionaries can be\n  passed in for more permissive matching.\n\n  Args:\n    sequence: str. The input sequence to reverse complement.\n    complement_dict: dict[str, str]. The lookup dictionary holding the\n      complement base pairs.\n\n  Returns:\n    The reverse complement DNA sequence.\n\n  Raises:\n    Error: The sequence contains letters not present in complement_dict.\n  """"""\n  if complement_dict is None:\n    complement_dict = STRICT_DNA_COMPLEMENT_UPPER\n\n  try:\n    return \'\'.join(complement_dict[nt] for nt in reversed(sequence))\n  except KeyError:\n    raise Error(\'Unknown base in {}, cannot reverse complement using {}\'.format(\n        sequence, str(complement_dict)))\n'"
nucleus/util/sequence_utils_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for nucleus.util.sequence_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\n\nfrom absl.testing import parameterized\n\nfrom nucleus.util import sequence_utils\n\n\nclass SequenceUtilsTests(parameterized.TestCase):\n\n  @parameterized.parameters(\n      dict(seq=\'\', expected=\'\'),\n      dict(seq=\'A\', expected=\'T\'),\n      dict(seq=\'T\', expected=\'A\'),\n      dict(seq=\'C\', expected=\'G\'),\n      dict(seq=\'G\', expected=\'C\'),\n      dict(seq=\'GGGCAGATT\', expected=\'AATCTGCCC\'),\n      dict(\n          seq=\'GGGCAGANN\',\n          expected=\'NNTCTGCCC\',\n          complement_dict=sequence_utils.DNA_COMPLEMENT_UPPER),\n      dict(\n          seq=\'accgt\',\n          expected=\'acggt\',\n          complement_dict=sequence_utils.DNA_COMPLEMENT),\n      dict(\n          seq=\'ATCGRYSWKMBVDHN\',\n          expected=\'NDHBVKMWSRYCGAT\',\n          complement_dict=sequence_utils.IUPAC_DNA_COMPLEMENT_UPPER),\n      dict(\n          seq=\'ATCGRYSWKMBVDHNatcgryswkmbvdhn\',\n          expected=\'ndhbvkmwsrycgatNDHBVKMWSRYCGAT\',\n          complement_dict=sequence_utils.IUPAC_DNA_COMPLEMENT),\n  )\n  def test_reverse_complement(self, seq, expected, complement_dict=None):\n    """"""Tests canonical DNA sequences are reverse complemented correctly.""""""\n    self.assertEqual(\n        sequence_utils.reverse_complement(seq, complement_dict), expected)\n\n  @parameterized.parameters(\n      dict(seq=\'GGGCAGANN\'),\n      dict(seq=\'accgt\'),\n      dict(\n          seq=\'ATCGRYSWKMBVDHNatcgryswkmbvdhn\',\n          complement_dict=sequence_utils.IUPAC_DNA_COMPLEMENT_UPPER),\n      dict(seq=\'X\', complement_dict=sequence_utils.IUPAC_DNA_COMPLEMENT),\n  )\n  def test_bad_reverse_complement(self, seq, complement_dict=None):\n    """"""Tests error is raised when complement_dict does not cover given seq.""""""\n    with self.assertRaisesRegexp(sequence_utils.Error, \'Unknown base in\'):\n      sequence_utils.reverse_complement(seq, complement_dict)\n\n  @parameterized.parameters(\n      dict(\n          bases_set=sequence_utils.STRICT_DNA_BASES_UPPER,\n          complement_dict=sequence_utils.STRICT_DNA_COMPLEMENT_UPPER),\n      dict(\n          bases_set=sequence_utils.STRICT_DNA_BASES,\n          complement_dict=sequence_utils.STRICT_DNA_COMPLEMENT),\n      dict(\n          bases_set=sequence_utils.DNA_BASES_UPPER,\n          complement_dict=sequence_utils.DNA_COMPLEMENT_UPPER),\n      dict(\n          bases_set=sequence_utils.DNA_BASES,\n          complement_dict=sequence_utils.DNA_COMPLEMENT),\n  )\n  def test_base_set_definitions(self, bases_set, complement_dict):\n    """"""Tests that base set and complement dict definitions are consistent.""""""\n    self.assertEqual(bases_set, frozenset(complement_dict.keys()))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/util/struct_utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Struct proto utilities.\n\nThis class provides wrappers for conveniently interacting with protos defined\nin struct.proto, mostly ListValue and Value objects. It should primarily be used\nby variant_utils and variantcallutils rather than being used directly.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport types\n\nfrom nucleus.protos import struct_pb2\n\n# Field names of values defined in struct_pb2.Value.\n_BOOL_TYPE = \'bool_value\'\n_INT_TYPE = \'int_value\'\n_NUMBER_TYPE = \'number_value\'\n_STRING_TYPE = \'string_value\'\n\n\ndef _add_field_with_type(field_map, field_name, value, value_type):\n  """"""Adds values to a particular map field containing a ListValue.""""""\n  if not isinstance(value, (list, types.GeneratorType, tuple)):\n    value = [value]\n  struct_values = [struct_pb2.Value(**{value_type: v}) for v in value]\n  field_map[field_name].values.extend(struct_values)\n\n\ndef _set_field_with_type(field_map, field_name, value, value_type):\n  """"""Sets values to a particular map field containing a ListValue.""""""\n  if field_name in field_map:\n    del field_map[field_name]\n  _add_field_with_type(field_map, field_name, value, value_type)\n\n\ndef _get_field_with_type(field_map, field_name, is_single_field, value_type):\n  fields = [getattr(v, value_type) for v in field_map[field_name].values]\n  return fields[0] if is_single_field and fields else fields\n\n\ndef add_number_field(field_map, field_name, value):\n  """"""Appends the given number value(s) to field_map[field_name].\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to append value to.\n    value: The number value(s) to append to the field. This can be a single\n      number or a list of numbers.\n  """"""\n  _add_field_with_type(field_map, field_name, value, _NUMBER_TYPE)\n\n\ndef set_number_field(field_map, field_name, value):\n  """"""Sets field_map[field_name] with the given number value(s).\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to set.\n    value: The number value(s) to set the field to. This can be a single number\n      or a list of numbers.\n  """"""\n  _set_field_with_type(field_map, field_name, value, _NUMBER_TYPE)\n\n\ndef get_number_field(field_map, field_name, is_single_field=False):\n  """"""Returns the number value(s) stored in `field_map[field_name]`.\n\n  If the field_name is not present in field_map, the empty list is returned.\n\n  Args:\n    field_map: Map(str --> ListValue) of interest.\n    field_name: str. The name of the field to extract number values from.\n    is_single_field: bool. If True, return the first number value stored (it\n      should be the only one in the field). Otherwise, return the list of\n      numbers.\n\n  Returns:\n    The number value(s) stored in the field_map under this field_name.\n  """"""\n  return _get_field_with_type(field_map, field_name, is_single_field,\n                              _NUMBER_TYPE)\n\n\ndef add_int_field(field_map, field_name, value):\n  """"""Appends the given int value(s) to field_map[field_name].\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to append value to.\n    value: The int value(s) to append to the field. This can be a single\n      int or a list of ints.\n  """"""\n  _add_field_with_type(field_map, field_name, value, _INT_TYPE)\n\n\ndef set_int_field(field_map, field_name, value):\n  """"""Sets field_map[field_name] with the given int value(s).\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to set.\n    value: The int value(s) to set the field to. This can be a single int\n      or a list of ints.\n  """"""\n  _set_field_with_type(field_map, field_name, value, _INT_TYPE)\n\n\ndef get_int_field(field_map, field_name, is_single_field=False):\n  """"""Returns the int value(s) stored in `field_map[field_name]`.\n\n  If the field_name is not present in field_map, the empty list is returned.\n\n  Args:\n    field_map: Map(str --> ListValue) of interest.\n    field_name: str. The name of the field to extract int values from.\n    is_single_field: bool. If True, return the first int value stored (it\n      should be the only one in the field). Otherwise, return the list of\n      ints.\n\n  Returns:\n    The int value(s) stored in the field_map under this field_name.\n  """"""\n  return _get_field_with_type(field_map, field_name, is_single_field, _INT_TYPE)\n\n\ndef add_string_field(field_map, field_name, value):\n  """"""Appends the given string value(s) to field_map[field_name].\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to append value to.\n    value: The string value(s) to append to the field. This can be a single\n      string or a list of strings.\n  """"""\n  _add_field_with_type(field_map, field_name, value, _STRING_TYPE)\n\n\ndef set_string_field(field_map, field_name, value):\n  """"""Sets field_map[field_name] with the given string value(s).\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to set.\n    value: The int value(s) to set the field to. This can be a single string or\n      a list of strings.\n  """"""\n  _set_field_with_type(field_map, field_name, value, _STRING_TYPE)\n\n\ndef get_string_field(field_map, field_name, is_single_field=False):\n  """"""Returns the string value(s) stored in `field_map[field_name]`.\n\n  If the field_name is not present in field_map, the empty list is returned.\n\n  Args:\n    field_map: Map(str --> ListValue) of interest.\n    field_name: str. The name of the field to extract string values from.\n    is_single_field: bool. If True, return the first string value stored (it\n      should be the only one in the field). Otherwise, return the list of\n      strings.\n\n  Returns:\n    The string value(s) stored in the field_map under this field_name.\n  """"""\n  return _get_field_with_type(field_map, field_name, is_single_field,\n                              _STRING_TYPE)\n\n\ndef add_bool_field(field_map, field_name, value):\n  """"""Appends the given boolean value(s) to field_map[field_name].\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to append value to.\n    value: The boolean value(s) to append to the field. This can be a single\n      boolean or a list of booleans.\n  """"""\n  _add_field_with_type(field_map, field_name, value, _BOOL_TYPE)\n\n\ndef set_bool_field(field_map, field_name, value):\n  """"""Sets field_map[field_name] with the given boolean value(s).\n\n  Args:\n    field_map: Map(str --> ListValue) to modify.\n    field_name: str. The name of the field to set.\n    value: The boolean value(s) to set the field to. This can be a single\n      boolean or a list of booleans.\n  """"""\n  _set_field_with_type(field_map, field_name, value, _BOOL_TYPE)\n\n\ndef get_bool_field(field_map, field_name, is_single_field=False):\n  """"""Returns the bool value(s) stored in `field_map[field_name]`.\n\n  If the field_name is not present in field_map, the empty list is returned.\n\n  Args:\n    field_map: Map(str --> ListValue) of interest.\n    field_name: str. The name of the field to extract bool values from.\n    is_single_field: bool. If True, return the first bool value stored (it\n      should be the only one in the field). Otherwise, return the list of\n      bools.\n\n  Returns:\n    The bool value(s) stored in the field_map under this field_name.\n  """"""\n  return _get_field_with_type(field_map, field_name, is_single_field,\n                              _BOOL_TYPE)\n'"
nucleus/util/struct_utils_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for nucleus.util.struct_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom nucleus.protos import struct_pb2\nfrom nucleus.protos import variants_pb2\nfrom nucleus.util import struct_utils\n\n\ndef _set_protomap_from_dict(d):\n  """"""Returns a proto Map(str --> ListValue) with the given fields set.\n\n  Args:\n    d: dict(str --> list(Value)). The data to populate.\n\n  Returns:\n    The protocol buffer-defined Map(str --> ListValue).\n  """"""\n  # We use a Variant as an intermediate data structure since it contains the\n  # desired output map types.\n  v = variants_pb2.Variant()\n  for key, values in d.items():\n    v.info[key].values.extend(values)\n  return v.info\n\n\ndef _wrapped_value_and_num(value):\n  """"""Returns a list containing value plus the list\'s length.""""""\n  if isinstance(value, (list, tuple)):\n    return value, len(value)\n  else:\n    return [value], 1\n\n\nclass StructUtilsTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=1, expected=[1]),\n      dict(initial_fields={}, value=[1], expected=[1]),\n      dict(initial_fields={}, value=[1, 2.5], expected=[1, 2.5]),\n      dict(initial_fields={\'field\': []}, value=[1, 2.5], expected=[1, 2.5]),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(number_value=5)]},\n          value=[1, 2.5],\n          expected=[5, 1, 2.5]),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(number_value=5),\n                  struct_pb2.Value(number_value=-3.3),\n              ]\n          },\n          value=[1, 2.5],\n          expected=[5, -3.3, 1, 2.5]),\n  )\n  def test_add_number_field(self, initial_fields, value, expected):\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.add_number_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertEqual([v.number_value for v in field_map[\'field\'].values],\n                     expected)\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=1),\n      dict(initial_fields={}, value=[1]),\n      dict(initial_fields={}, value=[1, 2.5]),\n      dict(initial_fields={\'field\': []}, value=[1, 2.5]),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(number_value=5)]},\n          value=[1, 2.5]),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(number_value=5),\n                  struct_pb2.Value(number_value=-3.3),\n              ]\n          },\n          value=[1, 2.5]),\n  )\n  def test_set_number_field(self, initial_fields, value):\n    list_value, num_values = _wrapped_value_and_num(value)\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.set_number_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertLen(field_map[\'field\'].values, num_values)\n    self.assertEqual([v.number_value for v in field_map[\'field\'].values],\n                     list_value)\n\n  @parameterized.parameters(\n      dict(value=[], is_single_field=False, expected=[]),\n      dict(value=[], is_single_field=True, expected=[]),\n      dict(value=[1.5], is_single_field=False, expected=[1.5]),\n      dict(value=[1.5], is_single_field=True, expected=1.5),\n      dict(value=[1.5, 2], is_single_field=False, expected=[1.5, 2]),\n      dict(value=[1.5, 2], is_single_field=True, expected=1.5),\n  )\n  def test_get_number_field(self, value, is_single_field, expected):\n    key = \'field\'\n    field_map = _set_protomap_from_dict({})\n    struct_utils.set_number_field(field_map, key, value)\n    actual = struct_utils.get_number_field(field_map, key, is_single_field)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=1, expected=[1]),\n      dict(initial_fields={}, value=[1], expected=[1]),\n      dict(initial_fields={}, value=[1, 2], expected=[1, 2]),\n      dict(initial_fields={\'field\': []}, value=[1, 2], expected=[1, 2]),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(int_value=5)]},\n          value=[1, 2],\n          expected=[5, 1, 2]),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(int_value=5),\n                  struct_pb2.Value(int_value=-3),\n              ]\n          },\n          value=[1, 2],\n          expected=[5, -3, 1, 2]),\n  )\n  def test_add_int_field(self, initial_fields, value, expected):\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.add_int_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertEqual([v.int_value for v in field_map[\'field\'].values], expected)\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=1),\n      dict(initial_fields={}, value=[1]),\n      dict(initial_fields={}, value=[1, 2]),\n      dict(initial_fields={\'field\': []}, value=[1, 2]),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(int_value=5)]},\n          value=[1, 2]),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(int_value=5),\n                  struct_pb2.Value(int_value=-3),\n              ]\n          },\n          value=[1, 2]),\n  )\n  def test_set_int_field(self, initial_fields, value):\n    list_value, num_values = _wrapped_value_and_num(value)\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.set_int_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertLen(field_map[\'field\'].values, num_values)\n    self.assertEqual([v.int_value for v in field_map[\'field\'].values],\n                     list_value)\n\n  @parameterized.parameters(\n      dict(value=[], is_single_field=False, expected=[]),\n      dict(value=[], is_single_field=True, expected=[]),\n      dict(value=[1], is_single_field=False, expected=[1]),\n      dict(value=[1], is_single_field=True, expected=1),\n      dict(value=[1, 2], is_single_field=False, expected=[1, 2]),\n      dict(value=[1, 2], is_single_field=True, expected=1),\n  )\n  def test_get_int_field(self, value, is_single_field, expected):\n    key = \'field\'\n    field_map = _set_protomap_from_dict({})\n    struct_utils.set_int_field(field_map, key, value)\n    actual = struct_utils.get_int_field(field_map, key, is_single_field)\n    self.assertEqual(actual, expected)\n    # Test long handling in Python 2\n    if sys.version_info.major < 3:\n      field_map = _set_protomap_from_dict({})\n      struct_utils.set_int_field(field_map, key, [long(v) for v in value])\n      actual = struct_utils.get_int_field(field_map, key, is_single_field)\n      self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=\'hello\', expected=[\'hello\']),\n      dict(initial_fields={}, value=[\'hello\'], expected=[\'hello\']),\n      dict(initial_fields={}, value=[\'a\', \'aah\'], expected=[\'a\', \'aah\']),\n      dict(\n          initial_fields={\'field\': []},\n          value=[\'bc\', \'de\'],\n          expected=[\'bc\', \'de\']),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(string_value=\'hi\')]},\n          value=[\'a\', \'z\'],\n          expected=[\'hi\', \'a\', \'z\']),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(string_value=\'abc\'),\n                  struct_pb2.Value(string_value=u\'def\'),\n              ]\n          },\n          value=[\'ug\', u\'h\'],\n          expected=[\'abc\', \'def\', \'ug\', \'h\']),\n  )\n  def test_add_string_field(self, initial_fields, value, expected):\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.add_string_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertEqual([v.string_value for v in field_map[\'field\'].values],\n                     expected)\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=\'hello\'),\n      dict(initial_fields={}, value=[\'hello\']),\n      dict(initial_fields={}, value=[\'a\', \'aah\']),\n      dict(initial_fields={\'field\': []}, value=[\'bc\', \'de\']),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(string_value=\'hi\')]},\n          value=[\'a\', \'z\']),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(string_value=\'abc\'),\n                  struct_pb2.Value(string_value=u\'def\'),\n              ]\n          },\n          value=[\'ug\', u\'h\']),\n  )\n  def test_set_string_field(self, initial_fields, value):\n    list_value, num_values = _wrapped_value_and_num(value)\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.set_string_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertLen(field_map[\'field\'].values, num_values)\n    self.assertEqual([v.string_value for v in field_map[\'field\'].values],\n                     list_value)\n\n  @parameterized.parameters(\n      dict(value=[], is_single_field=False, expected=[]),\n      dict(value=[], is_single_field=True, expected=[]),\n      dict(value=[\'hi\'], is_single_field=False, expected=[\'hi\']),\n      dict(value=[\'single\'], is_single_field=True, expected=\'single\'),\n      dict(value=[\'2\', \'f\'], is_single_field=False, expected=[\'2\', \'f\']),\n      dict(value=[\'two\', \'fields\'], is_single_field=True, expected=\'two\'),\n  )\n  def test_get_string_field(self, value, is_single_field, expected):\n    key = \'field\'\n    field_map = _set_protomap_from_dict({})\n    struct_utils.set_string_field(field_map, key, value)\n    actual = struct_utils.get_string_field(field_map, key, is_single_field)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=True, expected=[True]),\n      dict(initial_fields={}, value=[True], expected=[True]),\n      dict(initial_fields={}, value=[True, False], expected=[True, False]),\n      dict(\n          initial_fields={\'field\': []},\n          value=[False, True],\n          expected=[False, True]),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(bool_value=True)]},\n          value=[False, True],\n          expected=[True, False, True]),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(bool_value=False),\n                  struct_pb2.Value(bool_value=True),\n              ]\n          },\n          value=[True, True],\n          expected=[False, True, True, True]),\n  )\n  def test_add_bool_field(self, initial_fields, value, expected):\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.add_bool_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertEqual([v.bool_value for v in field_map[\'field\'].values],\n                     expected)\n\n  @parameterized.parameters(\n      dict(initial_fields={}, value=True),\n      dict(initial_fields={}, value=[True]),\n      dict(initial_fields={}, value=[False, True]),\n      dict(initial_fields={\'field\': []}, value=[True, False]),\n      dict(\n          initial_fields={\'field\': [struct_pb2.Value(bool_value=True)]},\n          value=[True, False]),\n      dict(\n          initial_fields={\n              \'field\': [\n                  struct_pb2.Value(bool_value=False),\n                  struct_pb2.Value(bool_value=True),\n              ]\n          },\n          value=[True, False]),\n  )\n  def test_set_bool_field(self, initial_fields, value):\n    list_value, num_values = _wrapped_value_and_num(value)\n    field_map = _set_protomap_from_dict(initial_fields)\n    struct_utils.set_bool_field(field_map, \'field\', value)\n    self.assertIn(\'field\', field_map)\n    self.assertLen(field_map[\'field\'].values, num_values)\n    self.assertEqual([v.bool_value for v in field_map[\'field\'].values],\n                     list_value)\n\n  @parameterized.parameters(\n      dict(value=[], is_single_field=False, expected=[]),\n      dict(value=[], is_single_field=True, expected=[]),\n      dict(value=[True], is_single_field=False, expected=[True]),\n      dict(value=[True], is_single_field=True, expected=True),\n      dict(value=[True, False], is_single_field=False, expected=[True, False]),\n      dict(value=[False, True], is_single_field=True, expected=False),\n  )\n  def test_get_bool_field(self, value, is_single_field, expected):\n    key = \'field\'\n    field_map = _set_protomap_from_dict({})\n    struct_utils.set_bool_field(field_map, key, value)\n    actual = struct_utils.get_bool_field(field_map, key, is_single_field)\n    self.assertEqual(actual, expected)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/util/utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utility functions for working with reads.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom nucleus.protos import range_pb2\nfrom nucleus.util.python import utils as utils_cpp\n\n\ndef read_overlaps_region(read, region):\n  """"""Returns True if read overlaps read.\n\n  This function is equivalent to calling:\n\n    `ranges.ranges_overlap(region, read_range(read))`\n\n  But is optimized for speed and memory performance in C++.\n\n  Args:\n    read: nucleus.genomics.v1.Read.\n    region: nucleus.genomics.v1.Range.\n\n  Returns:\n    True if read and region overlap (i.e, have the same reference_name and their\n    start/ends overlap at least one basepair).\n  """"""\n  return utils_cpp.read_overlaps_region(read, region)\n\n\ndef read_range(read):\n  """"""Creates a Range proto from the alignment of Read.\n\n  Args:\n    read: nucleus.genomics.v1.Read. The read to calculate the range for.\n\n  Returns:\n    A nucleus.genomics.v1.Range for read.\n  """"""\n  range_pb = range_pb2.Range()\n  utils_cpp.read_range(read, range_pb)\n  return range_pb\n\n\ndef read_end(read):\n  """"""Returns the read start + alignment length for Read read.""""""\n  return read_range(read).end\n\n\ndef reservoir_sample(iterable, k, random=None):\n  """"""Samples k elements with uniform probability from an iterable.\n\n  Selects a subset of k elements from n input elements with uniform probability\n  without needing to hold all n elements in memory at the same time. This\n  implementation has max space complexity O(min(k, n)), i.e., we allocate up to\n  min(k, n) elements to store the samples. This means that we only use ~n\n  elements when n is smaller than k, which can be important when k is large. If\n  n elements are added to this sampler, and n <= k, all n elements will be\n  retained. If n > k, each added element will be retained with a uniform\n  probability of k / n.\n\n  The order of the k retained samples from our n elements is undefined. In\n  particular that means that the elements in the returned list can occur in a\n  different order than they appeared in the iterable.\n\n  More details about reservoir sampling (and the specific algorithm used here\n  called Algorithm R) can be found on wikipedia:\n\n  https://en.wikipedia.org/wiki/Reservoir_sampling#Algorithm_R\n\n  Args:\n    iterable: Python iterable. The iterable to sample from.\n    k: int. The number of elements to sample.\n    random: A random number generator or None.\n\n  Returns:\n    A list containing the k sampled elements.\n\n  Raises:\n    ValueError: If k is negative.\n  """"""\n  if k < 0:\n    raise ValueError(\'k must be nonnegative, but got {}\'.format(k))\n  if random is None:\n    random = np.random\n  sample = []\n  for i, item in enumerate(iterable):\n    if len(sample) < k:\n      sample.append(item)\n    else:\n      j = random.randint(0, i + 1)\n      if j < k:\n        sample[j] = item\n  return sample\n'"
nucleus/util/utils_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for nucleus.util.utils.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom absl.testing import absltest\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport numpy.testing as npt\n\nfrom nucleus.testing import test_utils\nfrom nucleus.util import ranges\nfrom nucleus.util import utils\n\n\nclass UtilsTest(parameterized.TestCase):\n\n  def test_read_range(self):\n    """"""Tests reads have their ranges calculated correctly.""""""\n    start = 10000001\n    read = test_utils.make_read(\n        \'AAACAG\',\n        chrom=\'chrX\',\n        start=start,\n        cigar=\'2M1I3M\',\n        quals=range(10, 16),\n        name=\'read1\')\n    self.assertEqual(\n        ranges.make_range(\'chrX\', start, start + 5), utils.read_range(read))\n    read = test_utils.make_read(\n        \'AAACAG\',\n        chrom=\'chrX\',\n        start=start,\n        cigar=\'2M16D3M\',\n        quals=range(10, 16),\n        name=\'read1\')\n    self.assertEqual(\n        ranges.make_range(\'chrX\', start, start + 5 + 16),\n        utils.read_range(read))\n\n  def test_read_end(self):\n    """"""Tests reads have their ends calculated correctly.""""""\n    start = 10000001\n    read = test_utils.make_read(\n        \'AAACAG\',\n        chrom=\'chrX\',\n        start=start,\n        cigar=\'2M1I3M\',\n        quals=range(10, 16),\n        name=\'read1\')\n    self.assertEqual(\n        start + 5, utils.read_end(read))\n    read = test_utils.make_read(\n        \'AAACAG\',\n        chrom=\'chrX\',\n        start=start,\n        cigar=\'2M16D3M\',\n        quals=range(10, 16),\n        name=\'read1\')\n    self.assertEqual(\n        start + 5 + 16,\n        utils.read_end(read))\n\n  def test_reservoir_sample_length(self):\n    """"""Tests samples have expected length.""""""\n    first_ten_ints = range(10)\n    # Test sampling with k > len(iterable).\n    self.assertEqual(len(utils.reservoir_sample(first_ten_ints, 11)), 10)\n    # Test sampling with k == len(iterable).\n    self.assertEqual(len(utils.reservoir_sample(first_ten_ints, 10)), 10)\n    # Test sampling with k < len(iterable).\n    self.assertEqual(len(utils.reservoir_sample(first_ten_ints, 9)), 9)\n    # Test sampling with k == 0.\n    self.assertEqual(len(utils.reservoir_sample(first_ten_ints, 0)), 0)\n    # Test sampling with k < 0 (bad args).\n    with self.assertRaises(ValueError):\n      utils.reservoir_sample(first_ten_ints, -1)\n\n  @parameterized.parameters(\n      (10, 0),\n      (1, 1),\n      (10, 1),\n      (1, 3),\n      (3, 3),\n      (6, 3),\n      (10, 3),\n  )\n  def test_reservoir_sample_frequency(self, iterable_size, k):\n    """"""Tests observed frequency is close to expected frequency.""""""\n    # Use a fixed random number so our test is deterministic.\n    random = np.random.RandomState(123456789)\n    n_replicates = 100000\n    counts = collections.Counter(\n        item\n        for _ in range(n_replicates)\n        for item in utils.reservoir_sample(range(iterable_size), k, random))\n    expected_frequency = min(k / float(iterable_size), 1.0)\n    for c in counts.values():\n      observed_frequency = c / float(n_replicates)\n      npt.assert_allclose(observed_frequency, expected_frequency, atol=0.01)\n\n  @parameterized.parameters(\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=4, e2=10, expected=False),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=3, e2=10, expected=False),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=2, e2=10, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=1, e2=10, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=0, e2=10, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=0, e2=1, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=0, e2=2, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=0, e2=3, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=1, e2=2, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=1, e2=3, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=2, e2=3, expected=True),\n      # dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=3, e2=3, expected=False),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=0, e2=4, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr1\', s2=1, e2=4, expected=True),\n      dict(ref1=\'chr1\', s1=0, e1=3, ref2=\'chr2\', s2=1, e2=4, expected=False),\n  )\n  def test_read_overlaps_region(self, ref1, s1, e1, ref2, s2, e2, expected):\n\n    def check_overlaps(chr1, start1, end1, chr2, start2, end2, expected):\n      nbp = end1 - start1\n      read = test_utils.make_read(\n          \'A\' * nbp, chrom=chr1, start=start1, cigar=\'{}M\'.format(nbp))\n      region = ranges.make_range(chr2, start2, end2)\n      self.assertEqual(utils.read_overlaps_region(read, region), expected)\n      # This check ensures we get the same result calling ranges.ranges_overlap.\n      self.assertEqual(\n          ranges.ranges_overlap(region, utils.read_range(read)), expected)\n\n    check_overlaps(ref1, s1, e1, ref2, s2, e2, expected)\n    check_overlaps(ref2, s2, e2, ref1, s1, e1, expected)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/util/variant_utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Variant utilities.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport itertools\n\nimport enum\nimport six\n\nfrom nucleus.protos import variants_pb2\nfrom nucleus.util import ranges\nfrom nucleus.util import variantcall_utils\nfrom nucleus.util import vcf_constants\n\n\ndef only_call(variant):\n  """"""Ensures the Variant has exactly one VariantCall, and returns it.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant. The variant of interest.\n\n  Returns:\n    The single nucleus.genomics.v1.VariantCall in the variant.\n\n  Raises:\n    ValueError: Not exactly one VariantCall is in the variant.\n  """"""\n  if len(variant.calls) != 1:\n    raise ValueError(\'Expected exactly one VariantCall in {}\'.format(variant))\n  return variant.calls[0]\n\n\ndef decode_variants(encoded_iter):\n  """"""Yields a genomics.Variant from encoded_iter.\n\n  Args:\n    encoded_iter: An iterable that produces binary encoded\n      nucleus.genomics.v1.Variant strings.\n\n  Yields:\n    A parsed nucleus.genomics.v1.Variant for each encoded element of\n    encoded_iter in order.\n  """"""\n  for encoded in encoded_iter:\n    yield variants_pb2.Variant.FromString(encoded)\n\n\ndef variant_position(variant):\n  """"""Returns a new Range at the start position of variant.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    A new Range with the same reference_name as variant and start but an end\n    that is start + 1. This produces a range that is the single basepair of the\n    start of variant, hence the name position.\n  """"""\n  return ranges.make_range(variant.reference_name, variant.start,\n                           variant.start + 1)\n\n\ndef variant_range(variant):\n  """"""Returns a new Range covering variant.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    A new Range with the same reference_name, start, and end as variant.\n  """"""\n  return ranges.make_range(variant.reference_name, variant.start, variant.end)\n\n\ndef variant_range_tuple(variant):\n  """"""Returns a new tuple of (reference_name, start, end) for the variant.\n\n  A common use case for this function is to sort variants by chromosomal\n  location, with usage like `sorted(variants, key=variant_range_tuple)`.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    A three-tuple with the same reference_name, start, and end as variant.\n  """"""\n  return (variant.reference_name, variant.start, variant.end)\n\n\n@enum.unique\nclass GenotypeType(enum.Enum):\n  """"""An enumeration of the types of genotypes.""""""\n  hom_ref = (\'homozygous reference\', [0, 0], 0)\n  het = (\'heterozygous\', [0, 1], 1)\n  hom_var = (\'homozygous non-reference\', [1, 1], 2)\n  no_call = (\'no call\', [-1, -1], -1)\n\n  def __init__(self, full_name, example_gt, class_id):\n    """"""Create a GenotypeType with the given name, GT and class_id.""""""\n    self.full_name = full_name\n    self.example_gt = example_gt\n    self.class_id = class_id\n\n\n@enum.unique\nclass VariantType(enum.Enum):\n  """"""An enumeration of the types of variants.""""""\n  # A variant.proto where there is no alt allele.\n  ref = 0\n  # A non-reference variant.proto where all ref and alt alleles are single\n  # basepairs.\n  snp = 1\n  # A non-reference variant.proto where at least one of ref or alt alleles are\n  # longer than 1 bp.\n  indel = 2\n\n\ndef format_filters(variant):\n  """"""Returns a human-readable string showing the filters applied to variant.\n\n  Returns a string with the filter field values of variant separated by commas.\n  If the filter field isn\'t set, returns vcf_constants.MISSING_FIELD (\'.\').\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    A string.\n  """"""\n  if variant.filter:\n    return \',\'.join(variant.filter)\n  else:\n    return vcf_constants.MISSING_FIELD\n\n\ndef format_alleles(variant):\n  """"""Gets a string representation of the variant\'s alleles.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    A string ref_bases/alt1,alt2 etc.\n  """"""\n  return \'{}/{}\'.format(variant.reference_bases, \',\'.join(\n      variant.alternate_bases))\n\n\ndef format_position(variant):\n  """"""Gets a string representation of the variant\'s position.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    A string chr:start + 1 (as start is zero-based).\n  """"""\n  return \'{}:{}\'.format(variant.reference_name, variant.start + 1)\n\n\ndef _non_excluded_alts(alts, exclude_alleles=None):\n  """"""Exclude any alts listed, by default: \'<*>\', \'.\', and \'<NON_REF>\'.\n\n  These alleles are sometimes listed in ALT column but they shouldn\'t be\n  analyzed and usually indicate reference blocks in formats like gVCF.\n\n  E.g. \'A\'->\'<*>\' is NOT an insertion, and \'A\'->\'.\' is NOT a SNP.\n\n  Args:\n    alts: a list of strings representing the alternate alleles.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    alts alleles except those in exclude_alleles, by default excluding the GVCF\n    \'<*>\' allele, the \'<NON_REF>\' symbolic allele, and \'.\' missing field by\n    default.\n  """"""\n  if exclude_alleles is None:\n    exclude_alleles = [\n        vcf_constants.GVCF_ALT_ALLELE, vcf_constants.SYMBOLIC_ALT_ALLELE,\n        vcf_constants.MISSING_FIELD\n    ]\n  return [a for a in alts if a not in exclude_alleles]\n\n\ndef is_snp(variant, exclude_alleles=None):\n  """"""Is variant a SNP?\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    True if all alleles of variant are 1 bp in length.\n  """"""\n  relevant_alts = _non_excluded_alts(variant.alternate_bases, exclude_alleles)\n  return (len(variant.reference_bases) == 1 and len(relevant_alts) >= 1 and\n          all(len(x) == 1 for x in relevant_alts))\n\n\ndef is_indel(variant, exclude_alleles=None):\n  """"""Is variant an indel?\n\n  An indel event is simply one where the size of at least one of the alleles\n  is > 1.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    True if the alleles in variant indicate an insertion/deletion event\n    occurs at this site.\n  """"""\n  relevant_alts = _non_excluded_alts(variant.alternate_bases, exclude_alleles)\n  if not relevant_alts:\n    return False\n  return (len(variant.reference_bases) > 1 or\n          any(len(alt) > 1 for alt in relevant_alts))\n\n\ndef is_biallelic(variant, exclude_alleles=None):\n  """"""Returns True if variant has exactly one alternate allele.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    True if the variant has exactly one alternate allele.\n  """"""\n  relevant_alts = _non_excluded_alts(variant.alternate_bases, exclude_alleles)\n  return len(relevant_alts) == 1\n\n\ndef is_multiallelic(variant, exclude_alleles=None):\n  """"""Does variant have multiple alt alleles?\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    True if variant has more than one alt allele.\n  """"""\n  relevant_alts = _non_excluded_alts(variant.alternate_bases, exclude_alleles)\n  return len(relevant_alts) > 1\n\n\ndef variant_is_insertion(variant, exclude_alleles=None):\n  """"""Are all the variant\'s alt alleles insertions?\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    True if variant has at least one alt allele and all alts are insertions.\n  """"""\n  relevant_alts = _non_excluded_alts(variant.alternate_bases, exclude_alleles)\n  if not relevant_alts:\n    return False\n  return all(\n      is_insertion(variant.reference_bases, alt) for alt in relevant_alts)\n\n\ndef variant_is_deletion(variant, exclude_alleles=None):\n  """"""Are all the variant\'s alt alleles deletions?\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    True if variant has at least one alt allele and all alts are deletions.\n  """"""\n  relevant_alts = _non_excluded_alts(variant.alternate_bases, exclude_alleles)\n  if not relevant_alts:\n    return False\n  return all(is_deletion(variant.reference_bases, alt) for alt in relevant_alts)\n\n\ndef is_ref(variant, exclude_alleles=None):\n  """"""Returns true if variant is a reference record.\n\n  Variant protos can encode sites that aren\'t actually mutations in the\n  sample. For example, the record ref=\'A\', alt=\'.\' indicates that there is\n  no mutation present (i.e., alt is the missing value).\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    exclude_alleles: list(str). The alleles in this list will be ignored.\n\n  Returns:\n    True if there are no actual alternate alleles.\n  """"""\n  relevant_alts = _non_excluded_alts(variant.alternate_bases, exclude_alleles)\n  return not relevant_alts\n\n\ndef variant_type(variant):\n  """"""Gets the VariantType of variant.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    VariantType indicating the type of this variant.\n  """"""\n  if is_ref(variant):\n    return VariantType.ref\n  elif is_snp(variant):\n    return VariantType.snp\n  else:\n    return VariantType.indel\n\n\ndef is_transition(allele1, allele2):\n  """"""Is the pair of single bp alleles a transition?\n\n  Args:\n    allele1: A string of the first allele, must be 1 bp in length.\n    allele2: A string of the second allele, must be 1 bp in length.\n\n  Returns:\n    True if allele1/allele2 are a transition SNP.\n\n  Raises:\n    ValueError: if allele1 and allele2 are equal or aren\'t 1 bp in length.\n  """"""\n  if allele1 == allele2:\n    raise ValueError(\'Alleles must be unique:\', allele1, allele2)\n  if len(allele1) != 1:\n    raise ValueError(\'Alleles must be 1 bp in length.\', allele1)\n  if len(allele2) != 1:\n    raise ValueError(\'Alleles must be 1 bp in length.\', allele2)\n\n  alleles_set = {allele1, allele2}\n  return any(alleles_set == x for x in [{\'A\', \'G\'}, {\'C\', \'T\'}])\n\n\ndef is_insertion(ref, alt):\n  """"""Is alt an insertion w.r.t. ref?\n\n  Args:\n    ref: A string of the reference allele.\n    alt: A string of the alternative allele.\n\n  Returns:\n    True if alt is an insertion w.r.t. ref.\n  """"""\n  return len(ref) < len(alt)\n\n\ndef is_deletion(ref, alt):\n  """"""Is alt a deletion w.r.t. ref?\n\n  Args:\n    ref: A string of the reference allele.\n    alt: A string of the alternative allele.\n\n  Returns:\n    True if alt is a deletion w.r.t. ref.\n  """"""\n  return len(ref) > len(alt)\n\n\ndef has_insertion(variant):\n  """"""Does variant have an insertion?\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    True if the alleles in variant indicate an insertion event\n    occurs at this site.\n  """"""\n  ref = variant.reference_bases\n  return (is_indel(variant) and\n          any(is_insertion(ref, alt) for alt in variant.alternate_bases))\n\n\ndef has_deletion(variant):\n  """"""Does variant have a deletion?\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    True if the alleles in variant indicate an deletion event\n    occurs at this site.\n  """"""\n  ref = variant.reference_bases\n  return (is_indel(variant) and\n          any(is_deletion(ref, alt) for alt in variant.alternate_bases))\n\n\n@enum.unique\nclass AlleleMismatchType(enum.Enum):\n  """"""An enumeration of the types of allele mismatches we detect.""""""\n  # Duplicate alleles.\n  duplicate_eval_alleles = 1\n  duplicate_true_alleles = 2\n  # Truth has an allele that doesn\'t match any allele in eval.\n  unmatched_true_alleles = 3\n  # Eval has an allele that doesn\'t match any allele in truth.\n  unmatched_eval_alleles = 4\n\n\ndef allele_mismatches(evalv, truev):\n  """"""Determines the set of allele mismatch discordances between evalv and truev.\n\n  Compares the alleles present in evalv and truev to determine if there are any\n  disagreements between the set of called alleles in the two Variant protos. The\n  type of differences basically boil down to:\n\n  -- Are there duplicate alt alleles?\n  -- Can we find a matching allele in the truev for each allele in evalv, and\n    vice versa?\n\n  Two alleles A and B match when they would produce the same sequence of bases\n  in ref and alt haplotypes starting at the same position. So CA=>TA is the same\n  as C=>T (position is the same, replacing A by A is a noop) but AC=>AT isn\'t\n  the same as C=>T because the former event changes bases 1 bp further along in\n  the reference genome than the C=>T allele.\n\n  Args:\n    evalv: A nucleus.genomics.v1.Variant.\n    truev: A nucleus.genomics.v1.Variant.\n\n  Returns:\n    A set of AlleleMismatchType values.\n  """"""\n  unmatched_eval_alleles = []\n  # Use set removes duplicate alleles in truth and eval variants.\n  allele_matches = {alt: [] for alt in set(truev.alternate_bases)}\n  for eval_alt in set(evalv.alternate_bases):\n    # Loop over each possible alt allele, adding eval_alt to each matching alt\n    # allele.\n    found_match = False\n    for true_alt in allele_matches:\n      if (simplify_alleles(evalv.reference_bases, eval_alt) == simplify_alleles(\n          truev.reference_bases, true_alt)):\n        # We are a match to true_alt, so record that fact in allele_matches\n        allele_matches[true_alt].append(eval_alt)\n        found_match = True\n    if not found_match:\n      # We never found a match for eval_alt.\n      unmatched_eval_alleles.append(eval_alt)\n\n  # At this point we\'ve checked every alt against every eval allele, and are\n  # ready to summarize the differences using our AlleleMismatchType enum.\n  types = set()\n  if len(set(evalv.alternate_bases)) != len(evalv.alternate_bases):\n    types.add(AlleleMismatchType.duplicate_eval_alleles)\n  if len(set(truev.alternate_bases)) != len(truev.alternate_bases):\n    types.add(AlleleMismatchType.duplicate_true_alleles)\n  if unmatched_eval_alleles:\n    types.add(AlleleMismatchType.unmatched_eval_alleles)\n  if any(len(match) != 1 for match in six.itervalues(allele_matches)):\n    types.add(AlleleMismatchType.unmatched_true_alleles)\n  return types\n\n\ndef simplify_alleles(*alleles):\n  """"""Simplifies alleles by stripping off common postfix bases.\n\n  For example, simplify(""AC"", ""GC"") would produce the tuple ""A"", ""G"" as the ""C""\n  base is a common postfix of both alleles. But simplify(""AC"", ""GT"") would\n  produce ""AC"", ""GT"" as there is no common postfix.\n\n  Note this function will never simplify any allele down to the empty string. So\n  if alleles = [\'CACA\', \'CA\'], the longest common postfix is \'CA\' but we will\n  not produce [\'CA\', \'\'] as this is an invalid Variant allele encoding. Instead\n  we produce [\'CAC\', \'C\'].\n\n  Args:\n    *alleles: A tuple of bases, each as a string, to simplify.\n\n  Returns:\n    A tuple, one for each allele in alleles in order, with any common postfix\n    bases stripped off.\n  """"""\n\n  def all_the_same(items):\n    first = next(items)\n    return all(item == first for item in items)\n\n  # Loop over the alleles to determine the length of the shared postfix. Start\n  # at 1 so every allele, even after trimming the postfix, has at least len 1.\n  # For example, alleles = [\'ATT\', \'TT\'] reduces to [\'AT\', \'T\'] not [\'A\', \'\'].\n  shortest_allele_len = min(len(a) for a in alleles)\n  common_postfix_len = 0\n  for i in range(1, shortest_allele_len):\n    if not all_the_same(a[-i] for a in alleles):\n      break\n    common_postfix_len = i\n\n  if common_postfix_len:\n    return tuple(a[0:-common_postfix_len] for a in alleles)\n  else:\n    # Fast path for the case where there\'s no shared postfix.\n    return alleles\n\n\ndef simplify_variant_alleles(variant):\n  """"""Replaces the alleles in variants with their simplified versions.\n\n  This function takes a variant and replaces its ref and alt alleles with those\n  produced by a call to variant_utils.simplify_alleles() to remove common\n  postfix bases in the alleles that may be present due to pruning away alleles.\n\n  Args:\n    variant: learning.genomics.genomics.Variant proto we want to simplify.\n\n  Returns:\n    variant with its ref and alt alleles replaced with their simplified\n      equivalents.\n  """"""\n  simplified_alleles = simplify_alleles(variant.reference_bases,\n                                        *variant.alternate_bases)\n  variant.reference_bases = simplified_alleles[0]\n  variant.alternate_bases[:] = simplified_alleles[1:]\n  variant.end = variant.start + len(variant.reference_bases)\n  return variant\n\n\ndef is_filtered(variant):\n  """"""Returns True if variant has a non-PASS filter field, or False otherwise.""""""\n  return bool(variant.filter) and any(\n      f not in {\'PASS\', vcf_constants.MISSING_FIELD} for f in variant.filter)\n\n\ndef is_variant_call(variant,\n                    require_non_ref_genotype=True,\n                    no_calls_are_variant=False,\n                    call_indices=None,\n                    apply_filter=True):\n  """"""Is variant a non-reference call?\n\n  A Variant proto doesn\'t always imply that there\'s a variant present in the\n  genome. The call may not have alternate bases, may be filtered, may a have\n  hom-ref genotype, etc. This function looks for all of those configurations\n  and returns true iff the variant is asserting that a mutation is present\n  in the same.\n\n  Note that this code allows a variant without a calls field to be variant,\n  but one with a genotype call must have a non-reference genotype to be\n  considered variant (if require_non_ref_genotype is True, the default). If\n  False, a variant that passes all of the site-level requirements for being\n  a variant_call will return a True value, regardless of the genotypes, which\n  means that we\'ll consider a site with a sample with a hom-ref or no-call site\n  a variant call.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    require_non_ref_genotype: Should we require a site with a genotype call to\n      have a non-reference (het, hom-var) genotype for the site to be considered\n      a variant call?\n    no_calls_are_variant: If a site has genotypes, should we consider no_call\n      genotypes as being variant or not? e.g. -1/1 listed as ./. in VCF\n    call_indices: A list of 0-based indices. If specified, only the calls\n      at the given indices will be considered. The function will return\n      True if any of those calls are variant.\n    apply_filter: If set to True, will never treat this site as variant when\n      any filter other than PASS or . is set.\n\n  Returns:\n    True if variant is really a mutation call.\n  """"""\n  if is_ref(variant):\n    # No actual alt allele listed in ALT column\n    return False\n  elif apply_filter and is_filtered(variant):\n    # Anything other than PASS or . in FILTER column\n    return False\n  elif not variant.calls or not require_non_ref_genotype:\n    return True\n  # All tests after this point should only look at genotype-based fields, as\n  # we may have aborted out in the prev. line due to require_non_ref_genotype.\n  else:\n    # Check for non-ref genotypes and optionally no-call (-1) genotypes\n    if call_indices is None:\n      call_indices = range(len(variant.calls))\n    for i in call_indices:\n      for g in variant.calls[i].genotype:\n        if g > 0 or (no_calls_are_variant and g < 0):\n          return True\n    return False\n\n\ndef has_calls(variant):\n  """"""Does variant have any genotype calls?\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    True if variant has one or more VariantCalls.\n  """"""\n  return bool(variant.calls)\n\n\ndef genotype_type(variant):\n  """"""Gets the GenotypeType for variant.\n\n  If variant doesn\'t have genotypes, returns no_call. Otherwise\n  returns one of no_call, hom_ref, het, or hom_var depending on the\n  status of the genotypes in the call field of variant.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    A GenotypeType.\n\n  Raises:\n    ValueError: If variant has more than one call (i.e., is multi-sample).\n  """"""\n  if not has_calls(variant):\n    return GenotypeType.no_call\n  elif len(variant.calls) > 1:\n    raise ValueError(\'Unsupported: multiple genotypes found at\', variant)\n  else:\n    gt = set(only_call(variant).genotype)\n    if gt == {-1}:\n      return GenotypeType.no_call\n    elif gt == {0}:\n      return GenotypeType.hom_ref\n    elif len(gt) > 1:\n      return GenotypeType.het\n    else:\n      return GenotypeType.hom_var\n\n\ndef genotype_as_alleles(variant, call_ix=0):\n  """"""Gets genotype of the sample in variant as a list of actual alleles.\n\n  Returns the alleles specified by the genotype indices of variant.calls[0].\n  For example, if variant.reference_bases = \'A\' and variant.alternative_bases\n  = [\'C\'] and the genotypes are [0, 1], this function will return\n  [\'A\', \'C\'].\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n    call_ix: int. The index into the calls attribute indicating which\n      VariantCall to return alleles for.\n\n  Returns:\n    A list of allele (string) from variant, one for each genotype in\n    variant.calls[call_ix], in order.\n\n  Raises:\n    ValueError: If variant doesn\'t have a call at the specified index.\n  """"""\n  if not 0 <= call_ix < len(variant.calls):\n    raise ValueError(\n        \'Unsupported: requesting call {} in variant with {} calls: {}\'.format(\n            call_ix, len(variant.calls), variant))\n  else:\n    # Genotypes are encoded as integers, where 0 is the reference allele,\n    # indices > 0 refer to alt alleles, and the no-call genotypes is encoded\n    # as -1 in the genotypes. This code relies on this encoding to quickly\n    # reference into the alleles by adding 1 to the genotype index.\n    alleles = ([vcf_constants.MISSING_FIELD, variant.reference_bases] +\n               list(variant.alternate_bases))\n    return [alleles[i + 1] for i in variant.calls[call_ix].genotype]\n\n\ndef unphase_all_genotypes(variant):\n  """"""Sorts genotype and removes phasing bit of all calls in variant.\n\n  This mutation is done in place rather than returning a different copy.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    The variant with unphased calls.\n  """"""\n  for c in variant.calls:\n    c.is_phased = False\n    c.genotype.sort()\n  return variant\n\n\ndef is_gvcf(variant):\n  """"""Returns true if variant encodes a standard gVCF reference block.\n\n  This means in practice that variant has a single alternate allele that is the\n  canonical gVCF allele vcf_constants.GVCF_ALT_ALLELE.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Returns:\n    Boolean. True if variant is a gVCF record, False otherwise.\n  """"""\n  return variant.alternate_bases == [vcf_constants.GVCF_ALT_ALLELE]\n\n\ndef _genotype_order_in_likelihoods(num_alts, ploidy=2):\n  """"""Yields tuples of `ploidy` ints for the given number of alt alleles.\n\n  https://samtools.github.io/hts-specs/VCFv4.1.pdf\n  ""If A is the allele in REF and B,C,... are the alleles as ordered in ALT,\n  the ordering of genotypes for the likelihoods is given by:\n  F(j/k) = (k*(k+1)/2)+j. In other words, for biallelic sites the ordering is:\n  AA,AB,BB; for triallelic sites the ordering is: AA,AB,BB,AC,BC,CC, etc.""\n  The biallelic sites in our case are 0/0, 0/1, 1/1.\n  The triallelic sites are 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.\n  This wiki page has more information that generalizes to different ploidy.\n  http://genome.sph.umich.edu/wiki/Relationship_between_Ploidy,_Alleles_and_Genotypes\n\n  Args:\n    num_alts: int. The number of alternate alleles at the site.\n    ploidy: int. The ploidy for which to return genotypes.\n\n  Yields:\n    Tuples of `ploidy` ints representing allele indices in the order they appear\n    in the corresponding genotype likelihood array.\n  """"""\n  if ploidy == 1:\n    for i in range(num_alts + 1):\n      yield (i,)\n  elif ploidy == 2:\n    for j in range(num_alts + 1):\n      for i in range(j + 1):\n        yield (i, j)\n  else:\n    raise NotImplementedError(\'Only haploid and diploid supported.\')\n\n\ndef genotype_ordering_in_likelihoods(variant):\n  """"""Yields (i, j, allele_i, allele_j) for the genotypes ordering in GLs.\n\n  https://samtools.github.io/hts-specs/VCFv4.1.pdf\n  ""If A is the allele in REF and B,C,... are the alleles as ordered in ALT,\n  the ordering of genotypes for the likelihoods is given by:\n  F(j/k) = (k*(k+1)/2)+j. In other words, for biallelic sites the ordering is:\n  AA,AB,BB; for triallelic sites the ordering is: AA,AB,BB,AC,BC,CC, etc.""\n  The biallelic sites in our case are 0/0, 0/1, 1/1.\n  The triallelic sites are 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.\n  This wiki page has more information that generalizes ot different ploidy.\n  http://genome.sph.umich.edu/wiki/Relationship_between_Ploidy,_Alleles_and_Genotypes\n\n  Currently this function only implements for diploid cases.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant.\n\n  Yields:\n    allele indices and strings (i, j, allele_i, allele_j) in the correct order.\n  """"""\n  alleles = [variant.reference_bases] + list(variant.alternate_bases)\n  for i, j in _genotype_order_in_likelihoods(\n      len(variant.alternate_bases), ploidy=2):\n    yield i, j, alleles[i], alleles[j]\n\n\ndef genotype_likelihood(variant_call, allele_indices):\n  """"""Returns the genotype likelihood for the given allele indices.\n\n  Args:\n    variant_call: nucleus.genomics.v1.VariantCall. The VariantCall from\n      which to extract the genotype likelihood of the allele indices.\n    allele_indices: list(int). The list of allele indices for a given genotype.\n      E.g. diploid heterozygous alternate can be represented as [0, 1].\n\n  Returns:\n    The float value of the genotype likelihood of this set of alleles.\n  """"""\n  return variant_call.genotype_likelihood[genotype_likelihood_index(\n      allele_indices)]\n\n\ndef genotype_likelihood_index(allele_indices):\n  """"""Returns the genotype likelihood index for the given allele indices.\n\n  Args:\n    allele_indices: list(int). The list of allele indices for a given genotype.\n      E.g. diploid homozygous reference is represented as [0, 0].\n\n  Returns:\n    The index into the associated genotype likelihood array corresponding to\n    the likelihood of this list of alleles.\n\n  Raises:\n    NotImplementedError: The allele_indices are more than diploid.\n  """"""\n  if len(allele_indices) == 1:\n    # Haploid case.\n    return allele_indices[0]\n  elif len(allele_indices) == 2:\n    # Diploid case.\n    g1, g2 = sorted(allele_indices)\n    return g1 + (g2 * (g2 + 1) // 2)\n  else:\n    raise NotImplementedError(\n        \'Genotype likelihood index only supports haploid and diploid: {}\'.\n        format(allele_indices))\n\n\ndef allele_indices_for_genotype_likelihood_index(gl_index, ploidy=2):\n  """"""Returns a tuple of allele_indices corresponding to the given GL index.\n\n  This is the inverse function to `genotype_likelihood_index`.\n\n  Args:\n    gl_index: int. The index within a genotype likelihood array for which to\n      determine the associated alleles.\n    ploidy: int. The ploidy of the result.\n\n  Returns:\n    A tuple of `ploidy` ints representing the allele indices at this GL index.\n\n  Raises:\n    NotImplementedError: The requested allele indices are more than diploid.\n  """"""\n  if ploidy == 1:\n    return gl_index\n  elif ploidy == 2:\n    # TODO(cym): Implement using algorithm described at\n    # https://genome.sph.umich.edu/wiki/Relationship_between_Ploidy,_Alleles_and_Genotypes\n    # rather than creating all genotypes explicitly.\n    num_alts = 1\n    while genotype_likelihood_index([num_alts, num_alts]) < gl_index:\n      num_alts += 1\n    genotypes = list(_genotype_order_in_likelihoods(num_alts, ploidy=ploidy))\n    return genotypes[gl_index]\n  else:\n    raise NotImplementedError(\n        \'Allele calculations only supported for haploid and diploid.\')\n\n\ndef allele_indices_with_num_alts(variant, num_alts, ploidy=2):\n  """"""Returns a list of allele indices configurations with `num_alts` alternates.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant. The variant of interest, which\n      defines the candidate alternate alleles that can be used to generate\n      allele indices configurations.\n    num_alts: int in [0, `ploidy`]. The number of non-reference alleles for\n      which to create the allele indices configurations.\n    ploidy: int. The ploidy for which to return allele indices configurations.\n\n  Returns: A list of tuples. Each tuple is of length `ploidy` and represents the\n    allele indices of all `ploidy` genotypes that contain `num_alts`\n    non-reference alleles.\n\n  Raises:\n    ValueError: The domain of `num_alts` is invalid.\n    NotImplementedError: `ploidy` is not diploid.\n  """"""\n  if ploidy != 2:\n    raise NotImplementedError(\n        \'allele_indices_with_num_alts only supports diploid.\')\n  if not 0 <= num_alts <= ploidy:\n    raise ValueError(\n        \'Invalid number of alternate alleles requested: {} for ploidy {}\'.\n        format(num_alts, ploidy))\n\n  max_candidate_alt_ix = len(variant.alternate_bases)\n  if num_alts == 0:\n    return [(0, 0)]\n  elif num_alts == 1:\n    return [(0, i) for i in range(1, max_candidate_alt_ix + 1)]\n  else:\n    return [(i, j)\n            for i in range(1, max_candidate_alt_ix + 1)\n            for j in range(i, max_candidate_alt_ix + 1)]\n\n\ndef variants_overlap(variant1, variant2):\n  """"""Returns True if the range of variant1 and variant2 overlap.\n\n  This is equivalent to:\n\n    ranges_overlap(variant_range(variant1), variant_range(variant2))\n\n  Args:\n    variant1: nucleus.genomics.v1.Variant we want to compare for overlap.\n    variant2: nucleus.genomics.v1.Variant we want to compare for overlap.\n\n  Returns:\n    True if the variants overlap, False otherwise.\n  """"""\n  return ranges.ranges_overlap(variant_range(variant1), variant_range(variant2))\n\n\ndef variant_key(variant, sort_alleles=True):\n  """"""Gets a human-readable string key that is almost unique for Variant.\n\n  Gets a string key that contains key information about the variant, formatted\n  as:\n\n    reference_name:start+1:reference_bases->alternative_bases\n\n  where alternative bases is joined with a \'/\' for each entry in\n  alternative_bases. The start+1 is so we display the position, which starts at\n  1, and not the offset, which starts at 0.\n\n  For example, a Variant(reference_name=\'20\', start=10, reference_bases=\'AC\',\n  alternative_bases=[\'A\', \'ACC\']) would have a key of:\n\n    20:11:AC->A/ACC\n\n  The key is \'almost unique\' in that the reference_name + start + alleles should\n  generally occur once within a single VCF file, given the way the VCF\n  specification works.\n\n  Args:\n    variant: nucleus.genomics.v1.Variant to make into a key.\n    sort_alleles: bool. If True, the alternative_bases of variant will be sorted\n      according to their lexicographic order. If False, the alternative_bases\n      will be displayed in their order in the Variant.\n\n  Returns:\n    A str.\n  """"""\n  alts = variant.alternate_bases\n  if sort_alleles:\n    alts = sorted(alts)\n  return \'{}:{}:{}->{}\'.format(variant.reference_name, variant.start + 1,\n                               variant.reference_bases, \'/\'.join(alts))\n\n\ndef sorted_variants(variants):\n  """"""Returns sorted(variants, key=variant_range_tuple).""""""\n  return sorted(variants, key=variant_range_tuple)\n\n\ndef variants_are_sorted(variants):\n  """"""Returns True if variants are sorted w.r.t. variant_range.\n\n  Args:\n    variants: list[nucleus.genomics.v1.Variant]. A list of Variant\n      protos that may or may not be sorted.\n\n  Returns:\n    True if variants are sorted, False otherwise.\n  """"""\n  def _pairwise(iterable):\n    a, b = itertools.tee(iterable)\n    next(b, None)\n    return six.moves.zip(a, b)\n\n  for r1, r2 in _pairwise(variant_range_tuple(v) for v in variants):\n    if r2 < r1:\n      return False\n  return True\n\n\ndef set_info(variant, field_name, value, vcf_object=None):\n  """"""Sets a field of the info map of the `Variant` to the given value(s).\n\n  `variant.info` is analogous to the INFO field of a VCF record.\n\n  Args:\n    variant: Variant proto. The Variant to modify.\n    field_name: str. The name of the field to set.\n    value: A single value or list of values to update the Variant with. The type\n      of the value is determined by the `vcf_object` if one is given, otherwise\n      is looked up based on the reserved INFO fields in the VCF specification.\n    vcf_object: (Optional) A VcfReader or VcfWriter object. If not None, the\n      type of the field is inferred from the associated VcfReader or VcfWriter\n      based on its name. Otherwise, the type is inferred if it is a reserved\n      field.\n  """"""\n  if vcf_object is None:\n    set_field_fn = vcf_constants.reserved_info_field_set_fn(field_name)\n  else:\n    set_field_fn = vcf_object.field_access_cache.info_field_set_fn(field_name)\n  set_field_fn(variant.info, field_name, value)\n\n\ndef get_info(variant, field_name, vcf_object=None):\n  """"""Returns the value of the `field_name` INFO field.\n\n  The `vcf_object` is used to determine the type of the resulting value. If it\n  is a single value or a Flag, that single value will be returned. Otherwise,\n  the list of values is returned.\n\n  Args:\n    variant: Variant proto. The Variant of interest.\n    field_name: str. The name of the field to retrieve values from.\n    vcf_object: (Optional) A VcfReader or VcfWriter object. If not None, the\n      type of the field is inferred from the associated VcfReader or VcfWriter\n      based on its name. Otherwise, the type is inferred if it is a reserved\n      field.\n  """"""\n  if vcf_object is None:\n    get_field_fn = vcf_constants.reserved_info_field_get_fn(field_name)\n  else:\n    get_field_fn = vcf_object.field_access_cache.info_field_get_fn(field_name)\n  return get_field_fn(variant.info, field_name)\n\n\ndef calc_ac(variant):\n  """"""Returns a list of alt counts based on variant.calls.""""""\n  counts = [0] * len(variant.alternate_bases)\n  for call in variant.calls:\n    for gt in call.genotype:\n      if gt > 0:\n        counts[gt - 1] += 1\n  return counts\n\n\ndef calc_an(variant):\n  """"""Returns the total number of alleles in called genotypes in variant.""""""\n  return sum(\n      len([1 for gt in call.genotype if gt > -1]) for call in variant.calls)\n\n\ndef is_singleton(variant):\n  """"""Returns True iff the variant has exactly one non-ref VariantCall.""""""\n  non_ref_count = 0\n  for c in variant.calls:\n    if variantcall_utils.has_variation(c):\n      non_ref_count += 1\n      if non_ref_count > 1:\n        return False\n  return non_ref_count == 1\n\n\ndef major_allele_frequency(variant):\n  """"""Returns the frequency of the most common allele in the variant.""""""\n  count_dict = collections.defaultdict(int)\n  for call in variant.calls:\n    for geno in call.genotype:\n      count_dict[geno] += 1\n  denom = sum(cnt for geno, cnt in count_dict.items() if geno >= 0)\n  if denom > 0:\n    numer = max(cnt for geno, cnt in count_dict.items() if geno >= 0)\n    return float(numer) / denom\n  else:\n    return 0\n'"
nucleus/util/variant_utils_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for variant_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport itertools\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\nimport six\n\nfrom nucleus.protos import struct_pb2\nfrom nucleus.protos import variants_pb2\nfrom nucleus.testing import test_utils\nfrom nucleus.util import ranges\nfrom nucleus.util import struct_utils\nfrom nucleus.util import variant_utils\n\nNO_MISMATCH = set()\nEVAL_DUP = variant_utils.AlleleMismatchType.duplicate_eval_alleles\nTRUE_DUP = variant_utils.AlleleMismatchType.duplicate_true_alleles\nTRUE_MISS = variant_utils.AlleleMismatchType.unmatched_true_alleles\nEVAL_MISS = variant_utils.AlleleMismatchType.unmatched_eval_alleles\n\n_DEFAULT_SAMPLE_NAME = \'NA12878\'\n\n\ndef _create_variant_with_alleles(ref=None, alts=None, start=0):\n  """"""Creates a Variant record with specified alternate_bases.""""""\n  return variants_pb2.Variant(\n      reference_bases=ref,\n      alternate_bases=alts,\n      start=start,\n      calls=[variants_pb2.VariantCall(call_set_name=_DEFAULT_SAMPLE_NAME)])\n\n\nclass VariantUtilsTests(parameterized.TestCase):\n\n  def test_only_call(self):\n    expected = variants_pb2.VariantCall(call_set_name=\'name\', genotype=[0, 1])\n    variant = variants_pb2.Variant(calls=[expected])\n    actual = variant_utils.only_call(variant)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      0,\n      2,\n      3,\n  )\n  def test_invalid_only_call(self, num_calls):\n    calls = [\n        variants_pb2.VariantCall(call_set_name=str(x)) for x in range(num_calls)\n    ]\n    variant = variants_pb2.Variant(calls=calls)\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Expected exactly one VariantCall\'):\n      variant_utils.only_call(variant)\n\n  def test_modify_only_call(self):\n    variant = variants_pb2.Variant(calls=[variants_pb2.VariantCall()])\n    call = variant_utils.only_call(variant)\n    call.call_set_name = \'name\'\n    call.genotype[:] = [0, 1]\n    self.assertLen(variant.calls, 1)\n    self.assertEqual(variant.calls[0].call_set_name, \'name\')\n    self.assertEqual(variant.calls[0].genotype, [0, 1])\n\n  def test_decode_variants(self):\n    variants = [\n        test_utils.make_variant(start=1),\n        test_utils.make_variant(start=2)\n    ]\n    encoded = [variant.SerializeToString() for variant in variants]\n    actual = variant_utils.decode_variants(encoded)\n    # We have an iterable, so actual isn\'t equal to variants.\n    self.assertNotEqual(actual, variants)\n    # Making actual a list now makes it equal.\n    self.assertEqual(list(actual), variants)\n\n  def test_variant_position_and_range(self):\n    v1 = test_utils.make_variant(chrom=\'1\', alleles=[\'A\', \'C\'], start=10)\n    v2 = test_utils.make_variant(chrom=\'1\', alleles=[\'AGCT\', \'C\'], start=10)\n    pos = ranges.make_range(\'1\', 10, 11)\n    range_ = ranges.make_range(\'1\', 10, 14)\n    v1_range_tuple = (\'1\', 10, 11)\n    v2_range_tuple = (\'1\', 10, 14)\n    self.assertEqual(pos, variant_utils.variant_position(v1))\n    self.assertEqual(pos, variant_utils.variant_position(v2))\n    self.assertEqual(pos, variant_utils.variant_range(v1))\n    self.assertEqual(range_, variant_utils.variant_range(v2))\n    self.assertEqual(v1_range_tuple, variant_utils.variant_range_tuple(v1))\n    self.assertEqual(v2_range_tuple, variant_utils.variant_range_tuple(v2))\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), \'A/C\'),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\']), \'A/C,T\'),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\']), \'A/AT\'),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\']), \'AT/A\'),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'CT\']), \'AT/A,CT\'),\n  )\n  def test_format_alleles(self, variant, expected):\n    self.assertEqual(variant_utils.format_alleles(variant), expected)\n\n  @parameterized.parameters(\n      (None, \'.\'),\n      ([\'.\'], \'.\'),\n      ([\'PASS\'], \'PASS\'),\n      ([\'FILTER1\', \'FILTER2\'], \'FILTER1,FILTER2\'),\n      ([\'FILTER1\', \'FILTER2\', \'FILTER3\'], \'FILTER1,FILTER2,FILTER3\'),\n  )\n  def test_format_filters(self, filters, expected):\n    variant = test_utils.make_variant(filters=filters)\n    if filters is None:\n      variant.ClearField(\'filter\')\n    self.assertEqual(variant_utils.format_filters(variant), expected)\n\n  @parameterized.parameters(\n      # variant => status if we require non_ref genotype / status if we don\'t.\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), True, True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=None), True, True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'AT\']), True, True),\n      (test_utils.make_variant(alleles=[\'A\']), False, False),\n      (test_utils.make_variant(alleles=[\'A\', \'.\']), False, False),\n      (test_utils.make_variant(filters=[\'FAIL\']), False, False),\n      (test_utils.make_variant(gt=[-1, -1]), False, True),\n      (test_utils.make_variant(gt=[0, 0]), False, True),\n      (test_utils.make_variant(gt=[0, 1]), True, True),\n      (test_utils.make_variant(gt=[1, 1]), True, True),\n      (test_utils.make_variant_multiple_calls(gts=[[0, 0], [0, 0]]), False,\n       True),\n      (test_utils.make_variant_multiple_calls(gts=[[0, 1], [0, 0]]), True,\n       True),\n      (test_utils.make_variant_multiple_calls(gts=[[-1, -1], [0, 0]]), False,\n       True),\n  )\n  def test_is_variant_call(self, variant, expected_req_non_ref,\n                           expected_any_genotype):\n    # Check that default call checks for genotypes.\n    self.assertEqual(\n        variant_utils.is_variant_call(variant), expected_req_non_ref)\n    # Ask explicitly for genotypes to be included.\n    self.assertEqual(\n        variant_utils.is_variant_call(variant, require_non_ref_genotype=True),\n        expected_req_non_ref)\n    # Don\'t require non_ref genotypes.\n    self.assertEqual(\n        variant_utils.is_variant_call(variant, require_non_ref_genotype=False),\n        expected_any_genotype)\n\n    with self.assertRaises(Exception):\n      variant_utils.is_variant_call(None)\n\n  @parameterized.parameters(\n      ([-1, 0], [\'PASS\'], False, False),\n      ([-1, 0], [], False, False),\n      ([-1, 1], [\'FAIL\'], False, True),\n      ([0, 0], [\'PASS\'], False, False),\n      ([0, 1], [\'VQSRTrancheSNP99.80to100.00\'], False, True),\n      ([0, 1], [\'PASS\'], True, True),\n      ([0, 1], [], True, True),\n      ([1, 1], [\'FAIL\'], False, True),\n      ([1, 1], [\'PASS\'], True, True),\n      ([1, 2], [], True, True),\n  )\n  def test_is_variant_call_apply_filter(self, genotype, filters,\n                                        expected_when_applied,\n                                        expected_when_not_applied):\n    variant = test_utils.make_variant(gt=genotype, filters=filters)\n    # The default is apply_filter=True.\n    self.assertEqual(\n        variant_utils.is_variant_call(variant), expected_when_applied)\n    self.assertEqual(\n        variant_utils.is_variant_call(variant, apply_filter=True),\n        expected_when_applied)\n    self.assertEqual(\n        variant_utils.is_variant_call(variant, apply_filter=False),\n        expected_when_not_applied)\n\n  def test_is_variant_call_no_calls_are_variant(self):\n\n    def check_is_variant(variant, expected, **kwargs):\n      self.assertEqual(\n          variant_utils.is_variant_call(variant, **kwargs), expected)\n\n    no_genotypes = test_utils.make_variant(gt=[])\n    no_call = test_utils.make_variant(gt=[-1, -1])\n    hom_ref = test_utils.make_variant(gt=[0, 0])\n    het = test_utils.make_variant(gt=[0, 1])\n    hom_var = test_utils.make_variant(gt=[1, 1])\n    mult1 = test_utils.make_variant_multiple_calls(gts=[[-1, -1], [0, 0]])\n    mult2 = test_utils.make_variant_multiple_calls(\n        gts=[[0, 0], [0, 1], [-1, -1]])\n\n    check_is_variant(no_genotypes, False, no_calls_are_variant=False)\n    check_is_variant(no_genotypes, False, no_calls_are_variant=False)\n    check_is_variant(no_call, False, no_calls_are_variant=False)\n    check_is_variant(no_call, True, no_calls_are_variant=True)\n    check_is_variant(hom_ref, False, no_calls_are_variant=False)\n    check_is_variant(hom_ref, False, no_calls_are_variant=True)\n    check_is_variant(het, True, no_calls_are_variant=False)\n    check_is_variant(het, True, no_calls_are_variant=True)\n    check_is_variant(hom_var, True, no_calls_are_variant=False)\n    check_is_variant(hom_var, True, no_calls_are_variant=True)\n    check_is_variant(mult1, False, no_calls_are_variant=False)\n    check_is_variant(mult1, True, no_calls_are_variant=True)\n    check_is_variant(mult2, False, call_indices=[0])\n    check_is_variant(mult2, True, call_indices=[1])\n    check_is_variant(mult2, True, call_indices=[0, 1])\n    check_is_variant(mult2, False, call_indices=[2])\n\n  @parameterized.parameters(\n      (test_utils.make_variant(filters=None), False),\n      (test_utils.make_variant(filters=[\'.\']), False),\n      (test_utils.make_variant(filters=[\'PASS\']), False),\n      (test_utils.make_variant(filters=[\'FAIL\']), True),\n      (test_utils.make_variant(filters=[\'FAIL1\', \'FAIL2\']), True),\n      # These two are not allowed in VCF, but worth testing our\n      # code\'s behavior\n      (test_utils.make_variant(filters=[\'FAIL1\', \'PASS\']), True),\n      (test_utils.make_variant(filters=[\'FAIL1\', \'.\']), True),\n  )\n  def test_is_filtered(self, variant, expected):\n    self.assertEqual(variant_utils.is_filtered(variant), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']),\n       variant_utils.VariantType.snp),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\']),\n       variant_utils.VariantType.snp),\n      (test_utils.make_variant(alleles=[\'A\']), variant_utils.VariantType.ref),\n      (test_utils.make_variant(alleles=[\'A\', \'.\']),\n       variant_utils.VariantType.ref),\n      (test_utils.make_variant(alleles=[\'A\', \'AC\']),\n       variant_utils.VariantType.indel),\n      (test_utils.make_variant(alleles=[\'AC\', \'A\']),\n       variant_utils.VariantType.indel),\n      (test_utils.make_variant(alleles=[\'A\', \'AC\', \'ACC\']),\n       variant_utils.VariantType.indel),\n      (test_utils.make_variant(alleles=[\'ACC\', \'AC\', \'A\']),\n       variant_utils.VariantType.indel),\n  )\n  def test_variant_type(self, variant, expected):\n    self.assertEqual(variant_utils.variant_type(variant), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(\'chr1\', 10), \'chr1:11\'),\n      (test_utils.make_variant(\'chr2\', 100), \'chr2:101\'),\n  )\n  def test_format_position(self, variant, expected):\n    self.assertEqual(variant_utils.format_position(variant), expected)\n\n  @parameterized.parameters(\n      ([\'C\', \'<*>\'], None, [\'C\']),\n      ([\'C\', \'.\'], None, [\'C\']),\n      ([\'C\', \'<NON_REF>\'], None, [\'C\']),\n      ([\'C\', \'<*>\', \'A\', \'T\'], None, [\'C\', \'A\', \'T\']),\n      ([\'C\', \'<*>\', \'A\', \'T\'], [], [\'C\', \'<*>\', \'A\', \'T\']),\n      ([\'C\'], None, [\'C\']),\n      ([\'TEST\'], [\'TEST\'], []),\n  )\n  def test_non_excluded_alts(self, alts, excluded, expected):\n    self.assertEqual(\n        variant_utils._non_excluded_alts(alts, exclude_alleles=excluded),\n        expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\']), False),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\']), False),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'CT\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'AT\']), False),\n      (test_utils.make_variant(alleles=[\'A\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'.\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'G\', \'C\', \'AT\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'G\', \'T\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'T\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'T\', \'G\', \'<*>\']), True),\n  )\n  def test_is_snp(self, variant, expected):\n    self.assertEqual(variant_utils.is_snp(variant), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<NON_REF>\']), [\'<NON_REF>\'\n                                                                 ], True),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'<NON_REF>\']), [\'<NON_REF>\'\n                                                                  ], False),\n      # <NON_REF> is excluded by default\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<NON_REF>\']), None, True),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'<NON_REF>\']), None, False),\n  )\n  def test_is_snp_symbolic_allele(self, variant, exclude_alleles, expected):\n    self.assertEqual(\n        variant_utils.is_snp(variant, exclude_alleles=exclude_alleles),\n        expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\']), True),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\']), True),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'CT\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'AT\']), True),\n      (test_utils.make_variant(alleles=[\'A\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'.\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'AT\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'AT\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<NON_REF>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'<NON_REF>\']), True),\n  )\n  def test_is_indel(self, variant, expected):\n    self.assertEqual(variant_utils.is_indel(variant), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<NON_REF>\']), [\'<NON_REF>\'],\n       False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'<NON_REF>\']), [\'<NON_REF>\'],\n       True),\n  )\n  def test_is_indel_symbolic_allele(self, variant, exclude_alleles, expected):\n    self.assertEqual(\n        variant_utils.is_indel(variant, exclude_alleles=exclude_alleles),\n        expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\']), False),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\']), False),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'CT\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'AT\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\', \'<*>\']), True),\n  )\n  def test_is_multiallelic(self, variant, expected):\n    self.assertEqual(variant_utils.is_multiallelic(variant), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\']), True),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\']), True),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'CT\']), False),\n      (test_utils.make_variant(alleles=[\'AT\']), False),\n      (test_utils.make_variant(alleles=[\'AT\', \'.\']), False),\n      (test_utils.make_variant(alleles=[\'AT\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\', \'<*>\']), False),\n  )\n  def test_is_biallelic(self, variant, expected):\n    self.assertEqual(variant_utils.is_biallelic(variant), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'AA\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'G\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'AG\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'A\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\', \'AGG\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'.\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'<NON_REF>\']), False),\n      (test_utils.make_variant(alleles=[\'A\']), False),\n  )\n  def test_variant_is_insertion(self, variant, expected):\n    self.assertEqual(variant_utils.variant_is_insertion(variant), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'AG\', \'A\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'G\']), False),\n      (test_utils.make_variant(alleles=[\'AAG\', \'AC\', \'AG\']), True),\n      (test_utils.make_variant(alleles=[\'AAC\', \'ATG\', \'A\']), False),\n      (test_utils.make_variant(alleles=[\'AAT\', \'A\', \'AA\']), True),\n      (test_utils.make_variant(alleles=[\'AGA\', \'.\']), False),\n      (test_utils.make_variant(alleles=[\'AGTT\', \'<*>\']), False),\n      (test_utils.make_variant(alleles=[\'AGAGTCGACTGAT\', \'<NON_REF>\']), False),\n      (test_utils.make_variant(alleles=[\'AT\']), False),\n  )\n  def test_variant_is_deletion(self, variant, expected):\n    self.assertEqual(variant_utils.variant_is_deletion(variant), expected)\n\n  @parameterized.parameters(\n      ([\'A\', \'C\'], [\'A\', \'C\']),\n      ([\'AA\', \'CA\'], [\'A\', \'C\']),\n      ([\'AAG\', \'CAG\'], [\'A\', \'C\']),\n      ([\'AAGAG\', \'CAGAG\'], [\'A\', \'C\']),\n      ([\'AACAG\', \'CAGAG\'], [\'AAC\', \'CAG\']),\n      ([\'AACAC\', \'CAGAG\'], [\'AACAC\', \'CAGAG\']),\n      ([\'ACT\', \'A\'], [\'ACT\', \'A\']),\n      ([\'ACTCT\', \'ACT\'], [\'ACT\', \'A\']),\n      ([\'ACTCT\', \'A\'], [\'ACTCT\', \'A\']),\n      ([\'CAG\', \'GAG\'], [\'C\', \'G\']),\n      # Make sure we don\'t reduce an allele to nothing.\n      ([\'AT\', \'ATAT\'], [\'A\', \'ATA\']),\n      # Tests for multi-allelics.\n      # There\'s one extra T here.\n      ([\'ATT\', \'AT\', \'ATTT\'], [\'AT\', \'A\', \'ATT\']),\n      # Another single base postfix where we can remove a \'G\'.\n      ([\'CAG\', \'GAG\', \'TCG\'], [\'CA\', \'GA\', \'TC\']),\n      # There are two extra Ts to remove.\n      ([\'ATTT\', \'ATT\', \'ATTTT\'], [\'AT\', \'A\', \'ATT\']),\n      # One pair can simplify, but not the other, so nothing can reduce.\n      ([\'CAG\', \'GAG\', \'TCA\'], [\'CAG\', \'GAG\', \'TCA\']),\n      # Example from b/64022627.\n      ([\'CGGCGG\', \'CGG\', \'CAACGG\'], [\'CGGC\', \'C\', \'CAAC\']),\n  )\n  def test_simplify_alleles(self, alleles, expected):\n    self.assertEqual(variant_utils.simplify_alleles(*alleles), tuple(expected))\n    self.assertEqual(\n        variant_utils.simplify_alleles(*reversed(alleles)),\n        tuple(reversed(expected)))\n\n  @parameterized.parameters(\n      dict(\n          alleles=[\'CAA\', \'CA\', \'C\'],\n          start=5,\n          expected_alleles=[\'CAA\', \'CA\', \'C\'],\n          expected_end=8),\n      dict(\n          alleles=[\'CAA\', \'CA\'],\n          start=4,\n          expected_alleles=[\'CA\', \'C\'],\n          expected_end=6),\n      dict(\n          alleles=[\'CAA\', \'C\'],\n          start=3,\n          expected_alleles=[\'CAA\', \'C\'],\n          expected_end=6),\n      dict(\n          alleles=[\'CCA\', \'CA\'],\n          start=2,\n          expected_alleles=[\'CC\', \'C\'],\n          expected_end=4),\n  )\n  def test_simplify_variant_alleles(self, alleles, start, expected_alleles,\n                                    expected_end):\n    """"""Test that simplify_variant_alleles works as expected.""""""\n    variant = _create_variant_with_alleles(\n        ref=alleles[0], alts=alleles[1:], start=start)\n    simplified = variant_utils.simplify_variant_alleles(variant)\n    self.assertEqual(simplified.reference_bases, expected_alleles[0])\n    self.assertEqual(simplified.alternate_bases, expected_alleles[1:])\n    self.assertEqual(simplified.start, start)\n    self.assertEqual(simplified.end, expected_end)\n\n  @parameterized.parameters(\n      ([\'A\', \'C\'], [\'A\', \'C\'], NO_MISMATCH),\n      ([\'A\', \'AC\'], [\'A\', \'AC\'], NO_MISMATCH),\n      ([\'AC\', \'A\'], [\'AC\', \'A\'], NO_MISMATCH),\n      ([\'AC\', \'A\', \'ACT\'], [\'AC\', \'A\', \'ACT\'], NO_MISMATCH),\n      ([\'AC\', \'A\', \'ACT\'], [\'AC\', \'ACT\', \'A\'], NO_MISMATCH),\n      # Alleles are incompatible, so we have mismatches in both directions.\n      ([\'A\', \'C\'], [\'A\', \'T\'], {TRUE_MISS, EVAL_MISS}),\n      ([\'A\', \'C\'], [\'G\', \'C\'], {TRUE_MISS, EVAL_MISS}),\n      # Missing alts specific to eval and truth.\n      ([\'A\', \'C\', \'G\'], [\'A\', \'C\'], {EVAL_MISS}),\n      ([\'A\', \'C\'], [\'A\', \'C\', \'G\'], {TRUE_MISS}),\n      # Duplicate alleles.\n      ([\'A\', \'C\', \'C\'], [\'A\', \'C\'], {EVAL_DUP}),\n      ([\'A\', \'C\'], [\'A\', \'C\', \'C\'], {TRUE_DUP}),\n      ([\'A\', \'C\', \'C\'], [\'A\', \'C\', \'C\'], {EVAL_DUP, TRUE_DUP}),\n      # Dups in truth, discordant alleles.\n      ([\'A\', \'C\'], [\'A\', \'G\', \'G\'], {TRUE_DUP, EVAL_MISS, TRUE_MISS}),\n      # Simplification of alleles does the right matching.\n      ([\'A\', \'C\'], [\'AA\', \'CA\'], NO_MISMATCH),  # trailing A.\n      # preceding A, doesn\'t simplify so it\'s a mismatch.\n      ([\'A\', \'C\'], [\'AA\', \'AC\'], {EVAL_MISS, TRUE_MISS}),\n      # both training preceding A, doesn\'t simplify, so mismatches\n      ([\'A\', \'C\'], [\'AAA\', \'ACA\'], {EVAL_MISS, TRUE_MISS}),\n      # # Eval has 1 of the two alt alleles, so no eval mismatch.\n      ([\'ACT\', \'A\'], [\'ACTCT\', \'ACT\', \'A\'], {TRUE_MISS}),\n      # Eval has extra unmatched alleles, so it\'s got a mismatch.\n      ([\'ACTCT\', \'ACT\', \'A\'], [\'ACT\', \'A\'], {EVAL_MISS}),\n  )\n  def test_allele_mismatch(self, a1, a2, expected):\n    v1 = test_utils.make_variant(alleles=a1)\n    v2 = test_utils.make_variant(alleles=a2)\n    self.assertEqual(variant_utils.allele_mismatches(v1, v2), expected)\n\n  @parameterized.parameters(\n      ([\'A\', \'C\'], False),\n      ([\'A\', \'G\'], True),\n      ([\'A\', \'T\'], False),\n      ([\'C\', \'G\'], False),\n      ([\'C\', \'T\'], True),\n      ([\'G\', \'T\'], False),\n  )\n  def test_is_transition(self, ordered_alleles, expected):\n    for alleles in [ordered_alleles, reversed(ordered_alleles)]:\n      self.assertEqual(variant_utils.is_transition(*alleles), expected)\n\n  def test_is_transition_raises_with_bad_args(self):\n    with self.assertRaises(ValueError):\n      variant_utils.is_transition(\'A\', \'A\')\n    with self.assertRaises(ValueError):\n      variant_utils.is_transition(\'A\', \'AA\')\n    with self.assertRaises(ValueError):\n      variant_utils.is_transition(\'AA\', \'A\')\n\n  @parameterized.parameters(\n      # alleles followed by is_insertion and is_deletion expectation\n      ([\'A\', \'C\'], False, False),\n      ([\'A\', \'AT\'], True, False),\n      ([\'A\', \'ATT\'], True, False),\n      ([\'AT\', \'A\'], False, True),\n      ([\'ATT\', \'A\'], False, True),\n      ([\'CAT\', \'TCA\'], False, False),\n\n      # These are examples where ref is not simplified, such as could occur\n      # a multi-allelic record, such as the following:\n      # alleles = AT, A, ATT, CT (1 deletion, 1 insertion, 1 SNP)\n      ([\'AT\', \'A\'], False, True),\n      ([\'AT\', \'ATT\'], True, False),\n      ([\'AT\', \'CT\'], False, False),\n  )\n  def test_is_insertion_deletion(self, alleles, is_insertion, is_deletion):\n    self.assertEqual(variant_utils.is_insertion(*alleles), is_insertion)\n    self.assertEqual(variant_utils.is_deletion(*alleles), is_deletion)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), False, False),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\']), False, False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\']), True, False),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\']), False, True),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'ATT\']), True, True),\n      (test_utils.make_variant(alleles=[\'AT\', \'A\', \'CT\']), False, True),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'AT\']), True, False),\n      (test_utils.make_variant(alleles=[\'A\']), False, False),\n      (test_utils.make_variant(alleles=[\'AGA\', \'.\']), False, False),\n  )\n  def test_has_insertion_deletion(self, variant, has_insertion, has_deletion):\n    self.assertEqual(variant_utils.has_insertion(variant), has_insertion)\n    self.assertEqual(variant_utils.has_deletion(variant), has_deletion)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\']), None, True),\n      (test_utils.make_variant(alleles=[\'A\', \'.\']), None, True),\n      # a gVCF reference block record is counted as ref\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\']), None, True),\n      # symbolic allele <NON_REF> practically counts as ref\n      (test_utils.make_variant(alleles=[\'A\', \'<NON_REF>\']), None, True),\n      (test_utils.make_variant(alleles=[\'A\', \'G\']), None, False),\n      (test_utils.make_variant(alleles=[\'A\', \'<NON_REF>\']), [\'.\'], False),\n  )\n  def test_is_ref(self, variant, excluded, expected):\n    self.assertEqual(variant_utils.is_ref(variant, excluded), expected)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(gt=None), False),\n      (test_utils.make_variant(gt=[0, 0]), True),\n      (test_utils.make_variant(gt=[0, 1]), True),\n      (test_utils.make_variant(gt=[1, 1]), True),\n      (test_utils.make_variant(gt=[-1, -1]), True),\n      (variants_pb2.Variant(calls=[]), False),\n      (variants_pb2.Variant(\n          calls=[variants_pb2.VariantCall(call_set_name=\'no_geno\')]), True),\n      (variants_pb2.Variant(calls=[\n          variants_pb2.VariantCall(call_set_name=\'no_geno\'),\n          variants_pb2.VariantCall(call_set_name=\'no_geno2\'),\n      ]), True),\n  )\n  def test_has_calls(self, variant, expected):\n    self.assertEqual(variant_utils.has_calls(variant), expected)\n\n  def test_has_calls_raises_with_bad_inputs(self):\n    with self.assertRaises(Exception):\n      variant_utils.has_calls(None)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(gt=None), variant_utils.GenotypeType.no_call),\n      (test_utils.make_variant(gt=[-1, -1]),\n       variant_utils.GenotypeType.no_call),\n      (test_utils.make_variant(gt=[0, 0]), variant_utils.GenotypeType.hom_ref),\n      (test_utils.make_variant(gt=[0, 1]), variant_utils.GenotypeType.het),\n      (test_utils.make_variant(gt=[1, 0]), variant_utils.GenotypeType.het),\n      (test_utils.make_variant(gt=[0, 2]), variant_utils.GenotypeType.het),\n      (test_utils.make_variant(gt=[2, 0]), variant_utils.GenotypeType.het),\n      (test_utils.make_variant(gt=[1, 1]), variant_utils.GenotypeType.hom_var),\n      (test_utils.make_variant(gt=[1, 2]), variant_utils.GenotypeType.het),\n  )\n  def test_genotype_type(self, variant, expected):\n    self.assertEqual(variant_utils.genotype_type(variant), expected)\n\n  def test_genotype_type_raises_with_bad_args(self):\n    with self.assertRaises(Exception):\n      variant_utils.genotype_type(None)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[0, 0]), [\'A\', \'A\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[0, 1]), [\'A\', \'C\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[1, 0]), [\'C\', \'A\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[1, 1]), [\'C\', \'C\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\'], gt=[0, 0]), [\'A\', \'A\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\'], gt=[0, 1]), [\'A\', \'C\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\'], gt=[0, 2]), [\'A\', \'T\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\'], gt=[1, 2]), [\'C\', \'T\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\'], gt=[2, 1]), [\'T\', \'C\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\'], gt=[1, 1]), [\'C\', \'C\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'T\'], gt=[2, 2]), [\'T\', \'T\']),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[-1, -1]), [\'.\', \'.\']),\n  )\n  def test_genotype_as_alleles(self, variant, expected):\n    self.assertEqual(variant_utils.genotype_as_alleles(variant), expected)\n\n  def test_genotype_as_alleles_raises_with_bad_inputs(self):\n    with self.assertRaises(Exception):\n      variant_utils.genotype_as_alleles(None)\n    with self.assertRaises(Exception):\n      variant_utils.genotype_as_alleles(test_utils.make_variant(gt=None))\n    with self.assertRaises(Exception):\n      variant_utils.genotype_as_alleles(\n          test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[0, 0]), call_ix=1)\n    with self.assertRaises(Exception):\n      variant_utils.genotype_type(None)\n\n  @parameterized.parameters(\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[0, 1], is_phased=False),\n       test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[0, 1], is_phased=False)),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[1, 0], is_phased=True),\n       test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[0, 1], is_phased=False)),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[1, 1], is_phased=True),\n       test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[1, 1], is_phased=False)),\n      (test_utils.make_variant(\n          alleles=[\'A\', \'C\', \'T\'], gt=[2, 1], is_phased=True),\n       test_utils.make_variant(\n           alleles=[\'A\', \'C\', \'T\'], gt=[1, 2], is_phased=False)),\n      (test_utils.make_variant(alleles=[\'A\', \'C\'], gt=[-1, -1], is_phased=True),\n       test_utils.make_variant(\n           alleles=[\'A\', \'C\'], gt=[-1, -1], is_phased=False)),\n  )\n  def test_unphase_all_genotypes(self, variant, expected):\n    self.assertEqual(variant_utils.unphase_all_genotypes(variant), expected)\n\n  @parameterized.parameters(\n      # Ref without an alt isn\'t gVCF.\n      (test_utils.make_variant(alleles=[\'A\']), False),\n      # SNPs and indels aren\'t gVCF records.\n      (test_utils.make_variant(alleles=[\'A\', \'T\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'AT\']), False),\n      (test_utils.make_variant(alleles=[\'AT\', \'T\']), False),\n      # These are gVCF records.\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\']), True),\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\'], filters=\'PASS\'), True),\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\'], filters=\'FAIL\'), True),\n      # These are close but not exactly gVCFs.\n      (test_utils.make_variant(alleles=[\'A\', \'<*>\', \'C\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'<*F>\']), False),\n      (test_utils.make_variant(alleles=[\'A\', \'<CNV>\']), False),\n  )\n  def test_is_gvcf(self, variant, expected):\n    self.assertEqual(variant_utils.is_gvcf(variant), expected)\n\n  @parameterized.parameters(\n      # Variants with one ref and one alt allele.\n      (test_utils.make_variant(alleles=[\'A\', \'C\']), [(0, 0, \'A\', \'A\'),\n                                                     (0, 1, \'A\', \'C\'),\n                                                     (1, 1, \'C\', \'C\')]),\n      # Variants with one ref and two alt alleles.\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'G\']), [(0, 0, \'A\', \'A\'),\n                                                          (0, 1, \'A\', \'C\'),\n                                                          (1, 1, \'C\', \'C\'),\n                                                          (0, 2, \'A\', \'G\'),\n                                                          (1, 2, \'C\', \'G\'),\n                                                          (2, 2, \'G\', \'G\')]),\n      # Variants with one ref and three alt alleles.\n      (test_utils.make_variant(alleles=[\'A\', \'C\', \'G\', \'T\']),\n       [(0, 0, \'A\', \'A\'), (0, 1, \'A\', \'C\'), (1, 1, \'C\', \'C\'), (0, 2, \'A\', \'G\'),\n        (1, 2, \'C\', \'G\'), (2, 2, \'G\', \'G\'), (0, 3, \'A\', \'T\'), (1, 3, \'C\', \'T\'),\n        (2, 3, \'G\', \'T\'), (3, 3, \'T\', \'T\')]),\n  )\n  def test_genotype_ordering_in_likelihoods(self, variant, expected):\n    self.assertEqual(\n        list(variant_utils.genotype_ordering_in_likelihoods(variant)), expected)\n\n  @parameterized.parameters(\n      # Haploid.\n      dict(gls=[0.], allele_indices=[0], expected=0.),\n      dict(gls=[-1, -2], allele_indices=[1], expected=-2),\n      dict(gls=[-1, -2, -3], allele_indices=[2], expected=-3),\n      # Diploid.\n      dict(gls=[0.], allele_indices=[0, 0], expected=0.),\n      dict(gls=[-1, -2, -3], allele_indices=[0, 0], expected=-1),\n      dict(gls=[-1, -2, -3], allele_indices=[0, 1], expected=-2),\n      dict(gls=[-1, -2, -3], allele_indices=[1, 0], expected=-2),\n      dict(gls=[-1, -2, -3], allele_indices=[1, 1], expected=-3),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[0, 0], expected=-1),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[0, 1], expected=-2),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[1, 0], expected=-2),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[1, 1], expected=-3),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[0, 2], expected=-4),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[2, 0], expected=-4),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[1, 2], expected=-5),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[2, 1], expected=-5),\n      dict(gls=[-1, -2, -3, -4, -5, -6], allele_indices=[2, 2], expected=-6),\n      dict(gls=range(10), allele_indices=[0, 3], expected=6),\n      dict(gls=range(10), allele_indices=[1, 3], expected=7),\n      dict(gls=range(10), allele_indices=[2, 3], expected=8),\n      dict(gls=range(10), allele_indices=[3, 3], expected=9),\n  )\n  def test_genotype_likelihood(self, gls, allele_indices, expected):\n    variantcall = variants_pb2.VariantCall(genotype_likelihood=gls)\n    actual = variant_utils.genotype_likelihood(variantcall, allele_indices)\n    self.assertEqual(actual, expected)\n\n  def test_unsupported_genotype_likelihood(self):\n    variantcall = variants_pb2.VariantCall(genotype_likelihood=[-1, -2, -3])\n    with self.assertRaisesRegexp(NotImplementedError,\n                                 \'only supports haploid and diploid\'):\n      variant_utils.genotype_likelihood(variantcall, [0, 1, 1])\n\n  def test_haploid_allele_indices_for_genotype_likelihood_index(self):\n    for aix in six.moves.xrange(20):\n      allele_indices = (aix,)\n      ix = variant_utils.genotype_likelihood_index(allele_indices)\n      actual = variant_utils.allele_indices_for_genotype_likelihood_index(\n          ix, ploidy=1)\n      self.assertEqual(actual, aix)\n\n  def test_diploid_allele_indices_for_genotype_likelihood_index(self):\n    for aix in range(20):\n      for bix in range(20):\n        allele_indices = (aix, bix)\n        expected = tuple(sorted(allele_indices))\n        ix = variant_utils.genotype_likelihood_index(allele_indices)\n        actual = variant_utils.allele_indices_for_genotype_likelihood_index(\n            ix, ploidy=2)\n        self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(ploidy=-1),\n      dict(ploidy=0),\n      dict(ploidy=3),\n  )\n  def test_unsupported_allele_indices_for_genotype_likelihood_index(\n      self, ploidy):\n    with self.assertRaisesRegexp(NotImplementedError,\n                                 \'only supported for haploid and diploid\'):\n      variant_utils.allele_indices_for_genotype_likelihood_index(0, ploidy)\n\n  @parameterized.parameters(\n      dict(alt_bases=[], num_alts=0, expected=[(0, 0)]),\n      dict(alt_bases=[\'A\'], num_alts=0, expected=[(0, 0)]),\n      dict(alt_bases=[\'A\'], num_alts=1, expected=[(0, 1)]),\n      dict(alt_bases=[\'A\'], num_alts=2, expected=[(1, 1)]),\n      dict(alt_bases=[\'A\', \'C\'], num_alts=0, expected=[(0, 0)]),\n      dict(alt_bases=[\'A\', \'C\'], num_alts=1, expected=[(0, 1), (0, 2)]),\n      dict(alt_bases=[\'A\', \'C\'], num_alts=2, expected=[(1, 1), (1, 2), (2, 2)]),\n  )\n  def test_allele_indices_with_num_alts(self, alt_bases, num_alts, expected):\n    variant = variants_pb2.Variant(alternate_bases=alt_bases)\n    actual = variant_utils.allele_indices_with_num_alts(\n        variant, num_alts, ploidy=2)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(alt_bases=[\'A\'], num_alts=0, ploidy=1),\n      dict(alt_bases=[\'A\'], num_alts=0, ploidy=3),\n      dict(alt_bases=[\'A\'], num_alts=-1, ploidy=2),\n      dict(alt_bases=[\'A\'], num_alts=3, ploidy=2),\n  )\n  def test_invalid_allele_indices_with_num_alts(self, alt_bases, num_alts,\n                                                ploidy):\n    variant = variants_pb2.Variant(alternate_bases=alt_bases)\n    with self.assertRaises((NotImplementedError, ValueError)):\n      variant_utils.allele_indices_with_num_alts(variant, num_alts, ploidy)\n\n  def test_variants_overlap(self):\n    v1 = test_utils.make_variant(chrom=\'1\', alleles=[\'A\', \'C\'], start=10)\n    v2 = test_utils.make_variant(chrom=\'1\', alleles=[\'A\', \'C\'], start=20)\n    with mock.patch.object(ranges, \'ranges_overlap\') as mock_overlap:\n      mock_overlap.return_value = \'SENTINEL\'\n      self.assertEqual(variant_utils.variants_overlap(v1, v2), \'SENTINEL\')\n      mock_overlap.assert_called_once_with(\n          variant_utils.variant_range(v1), variant_utils.variant_range(v2))\n\n  @parameterized.parameters(\n      # Degenerate cases - no and one variant.\n      dict(sorted_variants=[],),\n      dict(sorted_variants=[\n          test_utils.make_variant(chrom=\'1\', start=10),\n      ],),\n      # Two variants on the same chromosome.\n      dict(\n          sorted_variants=[\n              test_utils.make_variant(chrom=\'1\', start=10),\n              test_utils.make_variant(chrom=\'1\', start=15),\n          ],),\n      # The first variant has start > the second, but it\'s on a later chrom.\n      dict(\n          sorted_variants=[\n              test_utils.make_variant(chrom=\'1\', start=15),\n              test_utils.make_variant(chrom=\'2\', start=10),\n          ],),\n      # Make sure the end is respected.\n      dict(\n          sorted_variants=[\n              test_utils.make_variant(chrom=\'1\', start=10),\n              test_utils.make_variant(chrom=\'1\', start=15),\n              test_utils.make_variant(chrom=\'1\', alleles=[\'AA\', \'A\'], start=15),\n          ],),\n      # Complex example with multiple chromosomes, ends, etc.\n      dict(\n          sorted_variants=[\n              test_utils.make_variant(chrom=\'1\', start=10),\n              test_utils.make_variant(chrom=\'2\', start=5),\n              test_utils.make_variant(chrom=\'2\', alleles=[\'AA\', \'A\'], start=5),\n              test_utils.make_variant(chrom=\'2\', start=6),\n              test_utils.make_variant(chrom=\'2\', start=10),\n              test_utils.make_variant(chrom=\'3\', start=2),\n          ],),\n  )\n  def test_sorted_variants(self, sorted_variants):\n    for permutation in itertools.permutations(\n        sorted_variants, r=len(sorted_variants)):\n\n      # Check that sorting the permutations produced sorted.\n      self.assertEqual(\n          variant_utils.sorted_variants(permutation), sorted_variants)\n\n      # Check that variants_are_sorted() is correct, which we detect if\n      # the range_tuples of permutation == the range_tuples of sorted_variants.\n      def _range_tuples(variants):\n        return [variant_utils.variant_range_tuple(v) for v in variants]\n\n      self.assertEqual(\n          variant_utils.variants_are_sorted(permutation),\n          _range_tuples(permutation) == _range_tuples(sorted_variants))\n\n  @parameterized.parameters(\n      dict(\n          variant=test_utils.make_variant(\n              chrom=\'1\', start=10, alleles=[\'A\', \'C\']),\n          expected_key=\'1:11:A->C\'),\n      dict(\n          variant=test_utils.make_variant(\n              chrom=\'1\', start=10, alleles=[\'A\', \'G\', \'C\']),\n          sort_alleles=True,\n          expected_key=\'1:11:A->C/G\'),\n      dict(\n          variant=test_utils.make_variant(\n              chrom=\'1\', start=10, alleles=[\'A\', \'G\', \'C\']),\n          sort_alleles=False,\n          expected_key=\'1:11:A->G/C\'),\n  )\n  def test_variant_key(self, variant, expected_key, sort_alleles=True):\n    self.assertEqual(\n        variant_utils.variant_key(variant, sort_alleles=sort_alleles),\n        expected_key)\n\n  @parameterized.parameters(\n      dict(\n          field_name=\'AD\',\n          value=[23, 25],\n          reader=None,\n          expected=[\n              struct_pb2.Value(int_value=23),\n              struct_pb2.Value(int_value=25)\n          ],\n      ),\n      dict(\n          field_name=\'AA\',\n          value=\'C\',\n          reader=True,\n          expected=[struct_pb2.Value(string_value=\'C\')],\n      ),\n  )\n  def test_set_info(self, field_name, value, reader, expected):\n    if reader is not None:\n      reader = mock.Mock()\n      reader.field_access_cache.info_field_set_fn.return_value = (\n          struct_utils.set_string_field)\n    variant = variants_pb2.Variant()\n    variant_utils.set_info(variant, field_name, value, reader)\n    actual = variant.info[field_name].values\n    self.assertEqual(len(actual), len(expected))\n    for actual_elem, expected_elem in zip(actual, expected):\n      self.assertEqual(actual_elem, expected_elem)\n\n  @parameterized.parameters(\n      dict(field_name=\'AD\', reader=None, expected=[23, 25]),\n      dict(field_name=\'AA\', reader=True, expected=\'C\'),\n      dict(field_name=\'1000G\', reader=None, expected=True),\n  )\n  def test_get_info(self, field_name, reader, expected):\n    if reader is not None:\n      reader = mock.Mock()\n      reader.field_access_cache.info_field_get_fn.return_value = (\n          functools.partial(\n              struct_utils.get_string_field, is_single_field=True))\n    variant = variants_pb2.Variant()\n    variant_utils.set_info(variant, \'AD\', [23, 25])\n    variant_utils.set_info(variant, \'AA\', \'C\')\n    variant_utils.set_info(variant, \'1000G\', True)\n    variant_utils.set_info(variant, \'DB\', False)\n    actual = variant_utils.get_info(variant, field_name, vcf_object=reader)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(alt_bases=[\'A\', \'T\'], calls=[[0, 0], [0, 1], [1, 2]],\n           expected=[2, 1]),\n      dict(alt_bases=[\'C\'], calls=[[0, 0], [0, 0]], expected=[0]),\n      dict(alt_bases=[], calls=[[0, 0], [0, 0], [0, 0]], expected=[]),\n  )\n  def test_calc_ac(self, alt_bases, calls, expected):\n    variant = variants_pb2.Variant()\n    variant.alternate_bases[:] = alt_bases\n    for gt in calls:\n      variant.calls.add().genotype[:] = gt\n    self.assertEqual(variant_utils.calc_ac(variant), expected)\n\n  @parameterized.parameters(\n      dict(calls=[[0, 0], [0, 1], [1, 2]], expected=6),\n      dict(calls=[[0, 0], [0, 0]], expected=4),\n      dict(calls=[[0, 0], [-1, -1], [0, -1]], expected=3),\n  )\n  def test_calc_an(self, calls, expected):\n    variant = variants_pb2.Variant()\n    for gt in calls:\n      variant.calls.add().genotype[:] = gt\n    self.assertEqual(variant_utils.calc_an(variant), expected)\n\n  @parameterized.parameters(\n      dict(calls=[], expected=False),\n      dict(calls=[[0, 0]], expected=False),\n      dict(calls=[[0, 1]], expected=True),\n      dict(calls=[[1, 1]], expected=True),\n      dict(calls=[[1, 2]], expected=True),\n      dict(calls=[[0, 0], [0, 1]], expected=True),\n      dict(calls=[[0, 0], [1, 1]], expected=True),\n      dict(calls=[[0, 1], [0, 1]], expected=False),\n      dict(calls=[[0, 2], [0, -1]], expected=True),\n      dict(calls=[[0, 0], [0, 1], [-1, -1]], expected=True),\n      dict(calls=[[0, 0], [0, 1], [0, 0]], expected=True),\n  )\n  def test_is_singleton(self, calls, expected):\n    variant = variants_pb2.Variant()\n    for gt in calls:\n      variant.calls.add().genotype[:] = gt\n    actual = variant_utils.is_singleton(variant)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(calls=[], expected=0.),\n      dict(calls=[[0, 0]], expected=1.),\n      dict(calls=[[0, 1]], expected=0.5),\n      dict(calls=[[1, 1]], expected=1.),\n      dict(calls=[[1, 2]], expected=0.5),\n      dict(calls=[[0, 0], [0, 1]], expected=0.75),\n      dict(calls=[[0, 0], [1, 1]], expected=0.5),\n      dict(calls=[[0, 1], [0, 1]], expected=0.5),\n      dict(calls=[[0, 2], [0, -1]], expected=2. / 3),\n      dict(calls=[[0, 0], [0, 1], [-1, -1]], expected=0.75),\n      dict(calls=[[0, 0], [0, 1], [0, 0]], expected=5. / 6),\n  )\n  def test_major_allele_frequency(self, calls, expected):\n    variant = variants_pb2.Variant()\n    for gt in calls:\n      variant.calls.add().genotype[:] = gt\n    actual = variant_utils.major_allele_frequency(variant)\n    self.assertAlmostEqual(actual, expected)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/util/variantcall_utils.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""VariantCall utilities.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom nucleus.protos import struct_pb2\nfrom nucleus.protos import variants_pb2\nfrom nucleus.util import struct_utils\nfrom nucleus.util import vcf_constants\n\n# Special-cased FORMAT fields that are first-class fields in the VariantCall.\n_GL = \'GL\'\n_GT = \'GT\'\n\n\ndef set_format(variant_call, field_name, value, vcf_object=None):\n  """"""Sets a field of the info map of the `VariantCall` to the given value(s).\n\n  `variant_call.info` is analogous to the FORMAT field of a VCF call.\n\n  Example usage:\n  with vcf.VcfReader(\'/path/to/my.vcf\') as vcf_reader:\n    for variant in vcf_reader:\n      first_call = variant.calls[0]\n      # Type can be inferred for reserved VCF fields.\n      set_format(first_call, \'AD\', 25)\n      # Specify the reader explicitly for unknown fields.\n      set_format(first_call, \'MYFIELD\', 30, vcf_reader)\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to modify.\n    field_name: str. The name of the field to set.\n    value: A single value or list of values to update the VariantCall with.\n      The type of the value is determined by the `vcf_object` if one is given,\n      otherwise is looked up based on the reserved FORMAT fields in the VCF\n      specification.\n    vcf_object: (Optional) A VcfReader or VcfWriter object. If not None, the\n      type of the field is inferred from the associated VcfReader or VcfWriter\n      based on its name. Otherwise, the type is inferred if it is a reserved\n      field.\n  """"""\n  if field_name == _GL:\n    set_gl(variant_call, value)\n    return\n  if field_name == _GT:\n    set_gt(variant_call, value)\n    return\n\n  if vcf_object is None:\n    set_field_fn = vcf_constants.reserved_format_field_set_fn(field_name)\n  else:\n    set_field_fn = vcf_object.field_access_cache.format_field_set_fn(field_name)\n  set_field_fn(variant_call.info, field_name, value)\n\n\ndef get_format(variant_call, field_name, vcf_object=None):\n  """"""Returns the value of the `field_name` FORMAT field.\n\n  The `vcf_object` is used to determine the type of the resulting value. If it\n  is a single value or a Flag, that single value will be returned. Otherwise,\n  the list of values is returned.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall of interest.\n    field_name: str. The name of the field to retrieve values from.\n    vcf_object: (Optional) A VcfReader or VcfWriter object. If not None, the\n      type of the field is inferred from the associated VcfReader or VcfWriter\n      based on its name. Otherwise, the type is inferred if it is a reserved\n      field.\n  """"""\n  if field_name == _GL:\n    return get_gl(variant_call)\n  if field_name == _GT:\n    return get_gt(variant_call)\n\n  if vcf_object is None:\n    get_field_fn = vcf_constants.reserved_format_field_get_fn(field_name)\n  else:\n    get_field_fn = vcf_object.field_access_cache.format_field_get_fn(field_name)\n  return get_field_fn(variant_call.info, field_name)\n\n\n# The following functions are convenience methods for getting/setting some\n# reserved FORMAT fields of a VariantCall as well as some non-reserved FORMAT\n# fields used by DeepVariant. Note that these functions will use the types of\n# each field as defined by the VCF 4.3 specification, mirrored in\n# vcf_constants.py, so if you have redefined any of these fields to have\n# different types these functions will not do what you want.\ndef set_ad(variant_call, ad):\n  """"""Sets the allele depth of the VariantCall.""""""\n  set_format(variant_call, \'AD\', ad)\n\n\ndef get_ad(variant_call):\n  """"""Gets the allele depth of the VariantCall.""""""\n  return get_format(variant_call, \'AD\')\n\n\ndef set_gl(variant_call, gl):\n  """"""Sets the genotype likelihoods of the VariantCall.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to modify.\n    gl: list(float). The list of genotype likelihoods for the VariantCall.\n  """"""\n  # Note: genotype_likelihood is extracted to a first-class field within\n  # VariantCall. Consequently, we just set its value directly here.\n  variant_call.genotype_likelihood[:] = gl\n\n\ndef get_gl(variant_call):\n  """"""Returns the genotype likelihoods of the VariantCall.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall for which to return GLs.\n\n  Returns:\n    A list of floats representing the genotype likelihoods of this call.\n  """"""\n  return variant_call.genotype_likelihood\n\n\ndef set_gt(variant_call, gt):\n  """"""Sets the genotypes of the VariantCall.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to modify.\n    gt: list(int). The list of genotypes for the VariantCall.\n  """"""\n  # Note: genotype is extracted to a first-class field within\n  # VariantCall. Consequently, we just set its value directly here.\n  variant_call.genotype[:] = gt\n\n\ndef get_gt(variant_call):\n  """"""Returns the genotypes of the VariantCall.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall for which to return GTs.\n\n  Returns:\n    A list of ints representing the genotype indices of this call.\n  """"""\n  return variant_call.genotype\n\n\ndef set_gq(variant_call, gq):\n  """"""Sets the genotype quality of the VariantCall.""""""\n  set_format(variant_call, \'GQ\', gq)\n\n\ndef get_gq(variant_call):\n  """"""Gets the genotype quality of the VariantCall.""""""\n  return get_format(variant_call, \'GQ\')\n\n\ndef set_min_dp(variant_call, min_dp):\n  """"""Sets the \'MIN_DP\' field of the VariantCall.""""""\n  struct_utils.set_int_field(variant_call.info, \'MIN_DP\', min_dp)\n\n\ndef get_min_dp(variant_call):\n  """"""Gets the \'MIN_DP\' field of the VariantCall.""""""\n  return struct_utils.get_int_field(\n      variant_call.info, \'MIN_DP\', is_single_field=True)\n\n\ndef has_genotypes(variant_call):\n  """"""Returns True iff the VariantCall has one or more called genotypes.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to evaluate.\n\n  Returns:\n    True if the VariantCall has one or more called genotypes, False otherwise.\n  """"""\n  return any(gt >= 0 for gt in variant_call.genotype)\n\n\ndef has_full_genotypes(variant_call):\n  """"""Returns True iff the VariantCall has only known genotypes.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to evaluate.\n\n  Returns:\n    True if all `genotype` fields are known genotypes.\n  """"""\n  return all(gt >= 0 for gt in variant_call.genotype)\n\n\ndef ploidy(variant_call):\n  """"""Returns the ploidy of the VariantCall.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to evaluate.\n\n  Returns:\n    The ploidy of the call (a non-negative integer).\n  """"""\n  # Unknown genotypes are represented as -1 in VariantCall protos. When\n  # a VCF is parsed that contains multiple ploidies in different samples,\n  # a separate padding value of -2**30 - 1 is inserted into the calls.\n  return sum(gt >= -1 for gt in variant_call.genotype)\n\n\ndef has_variation(variant_call):\n  """"""Returns True if and only if the call has a non-reference genotype.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to evaluate.\n\n  Returns:\n    True if and only if the call has a non-reference genotype.\n  """"""\n  return any(gt > 0 for gt in variant_call.genotype)\n\n\ndef is_heterozygous(variant_call):\n  """"""Returns True if and only if the call is heterozygous.\n\n  Args:\n    variant_call: VariantCall proto. The VariantCall to evaluate.\n\n  Returns:\n    True if and only if the call is heterozygous.\n  """"""\n  return len({gt for gt in variant_call.genotype if gt >= 0}) >= 2\n'"
nucleus/util/variantcall_utils_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for variantcall_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport mock\n\nfrom nucleus.protos import struct_pb2\nfrom nucleus.protos import variants_pb2\nfrom nucleus.util import struct_utils\nfrom nucleus.util import variantcall_utils\n\n\n\n\nclass VariantcallUtilsTests(parameterized.TestCase):\n\n  def _assert_struct_lists_equal(self, actual, expected):\n    self.assertEqual(len(actual), len(expected))\n    for actual_elem, expected_elem in zip(actual, expected):\n      self.assertEqual(actual_elem, expected_elem)\n\n  @parameterized.parameters(\n      dict(\n          field_name=\'GP\',\n          value=[.1, .2, .7],\n          reader=None,\n          expected=[struct_pb2.Value(number_value=v) for v in [.1, .2, .7]]),\n      dict(\n          field_name=\'AD\',\n          value=[23],\n          reader=None,\n          expected=[struct_pb2.Value(int_value=23)]),\n      dict(\n          field_name=\'FT\',\n          value=[\'PASS\'],\n          reader=None,\n          expected=[struct_pb2.Value(string_value=\'PASS\')]),\n      dict(\n          field_name=\'FT\',\n          value=[\'PASS\'],\n          reader=True,\n          expected=[struct_pb2.Value(string_value=\'PASS\')]),\n  )\n  def test_set_format(self, field_name, value, reader, expected):\n    if reader is not None:\n      reader = mock.Mock()\n      reader.field_access_cache.format_field_set_fn.return_value = (\n          struct_utils.set_string_field)\n    call = variants_pb2.VariantCall()\n    variantcall_utils.set_format(call, field_name, value, reader)\n    actual = call.info[field_name].values\n    self._assert_struct_lists_equal(actual, expected)\n\n  @parameterized.parameters(\n      dict(field_name=\'GP\', reader=None, expected=[.1, .2, .7]),\n      dict(field_name=\'AD\', reader=None, expected=[55, 3]),\n      dict(field_name=\'DP\', reader=None, expected=58),\n      dict(field_name=\'GL\', reader=None, expected=[-1, -3, -5.5]),\n      dict(field_name=\'GT\', reader=None, expected=[0, 1]),\n      dict(field_name=\'FT\', reader=None, expected=\'LowQual\'),\n      dict(field_name=\'FT\', reader=True, expected=\'LowQual\'),\n  )\n  def test_get_format(self, field_name, reader, expected):\n    if reader is not None:\n      reader = mock.Mock()\n      reader.field_access_cache.format_field_get_fn.return_value = (\n          functools.partial(\n              struct_utils.get_string_field, is_single_field=True))\n\n    call = variants_pb2.VariantCall()\n    variantcall_utils.set_format(call, \'GP\', [.1, .2, .7])\n    variantcall_utils.set_format(call, \'AD\', [55, 3])\n    variantcall_utils.set_format(call, \'DP\', 58)\n    variantcall_utils.set_format(call, \'GL\', [-1, -3, -5.5])\n    variantcall_utils.set_format(call, \'GT\', [0, 1])\n    variantcall_utils.set_format(call, \'FT\', [\'LowQual\'])\n    actual = variantcall_utils.get_format(call, field_name, vcf_object=reader)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(\n          field_name=\'AD\',\n          setter=variantcall_utils.set_ad,\n          getter=variantcall_utils.get_ad,\n          values=[[1, 5], [30, 29]]),\n      dict(\n          field_name=\'GL\',\n          setter=variantcall_utils.set_gl,\n          getter=variantcall_utils.get_gl,\n          values=[[-1, -2, -3.3], [-0.001, -3, -10]]),\n      dict(\n          field_name=\'GQ\',\n          setter=variantcall_utils.set_gq,\n          getter=variantcall_utils.get_gq,\n          values=range(10)),\n      dict(\n          field_name=\'GT\',\n          setter=variantcall_utils.set_gt,\n          getter=variantcall_utils.get_gt,\n          values=[[0, 1], [1, 1], [1, 2]]),\n      dict(\n          field_name=\'MIN_DP\',\n          setter=variantcall_utils.set_min_dp,\n          getter=variantcall_utils.get_min_dp,\n          values=range(10)),\n  )\n  def test_variantcall_format_roundtrip(self, field_name, setter, getter,\n                                        values):\n    vc = variants_pb2.VariantCall()\n    self.assertNotIn(field_name, vc.info)\n    for value in values:\n      setter(vc, value)\n      if field_name not in [\'GT\', \'GL\']:\n        self.assertIn(field_name, vc.info)\n      actual = getter(vc)\n      self.assertEqual(actual, value)\n\n  @parameterized.parameters(\n      dict(genotype=[], expected=False),\n      dict(genotype=[-1], expected=False),\n      dict(genotype=[-1, -1], expected=False),\n      dict(genotype=[-1, -1073741825], expected=False),\n      dict(genotype=[-1, 0], expected=True),\n      dict(genotype=[0, 0], expected=True),\n      dict(genotype=[0, 1], expected=True),\n      dict(genotype=[0, 1, -1], expected=True),\n      dict(genotype=[-1, 0, -1073741825], expected=True),\n  )\n  def test_has_genotypes(self, genotype, expected):\n    call = variants_pb2.VariantCall(genotype=genotype)\n    actual = variantcall_utils.has_genotypes(call)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(genotype=[], expected=True),\n      dict(genotype=[-1], expected=False),\n      dict(genotype=[-1, -1073741825], expected=False),\n      dict(genotype=[-1, 0], expected=False),\n      dict(genotype=[0, 0], expected=True),\n      dict(genotype=[0, 1], expected=True),\n      dict(genotype=[0, 1, -1], expected=False),\n      dict(genotype=[1, 0, -1073741825], expected=False),\n  )\n  def test_has_full_genotypes(self, genotype, expected):\n    call = variants_pb2.VariantCall(genotype=genotype)\n    actual = variantcall_utils.has_full_genotypes(call)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(genotype=[], expected=0),\n      dict(genotype=[-1], expected=1),\n      dict(genotype=[-1, -1], expected=2),\n      dict(genotype=[-1, -1073741825], expected=1),\n      dict(genotype=[-1, 0], expected=2),\n      dict(genotype=[0, 0], expected=2),\n      dict(genotype=[0, 1], expected=2),\n      dict(genotype=[0, 1, -1], expected=3),\n      dict(genotype=[-1, 0, -1073741825], expected=2),\n  )\n  def test_ploidy(self, genotype, expected):\n    call = variants_pb2.VariantCall(genotype=genotype)\n    actual = variantcall_utils.ploidy(call)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(genotype=[], expected=False),\n      dict(genotype=[-1], expected=False),\n      dict(genotype=[-1, -1], expected=False),\n      dict(genotype=[-1, -1073741825], expected=False),\n      dict(genotype=[-1, 0], expected=False),\n      dict(genotype=[0, 0], expected=False),\n      dict(genotype=[0, 1], expected=True),\n      dict(genotype=[0, 1, -1], expected=True),\n      dict(genotype=[-1, 0, -1073741825], expected=False),\n  )\n  def test_has_variation(self, genotype, expected):\n    call = variants_pb2.VariantCall(genotype=genotype)\n    actual = variantcall_utils.has_variation(call)\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(genotype=[], expected=False),\n      dict(genotype=[-1], expected=False),\n      dict(genotype=[-1, -1], expected=False),\n      dict(genotype=[-1, -1073741825], expected=False),\n      dict(genotype=[-1, 0], expected=False),\n      dict(genotype=[0, 0], expected=False),\n      dict(genotype=[0, 1], expected=True),\n      dict(genotype=[0, 2], expected=True),\n      dict(genotype=[1, 1], expected=False),\n      dict(genotype=[2, 2], expected=False),\n      dict(genotype=[1, 2], expected=True),\n      dict(genotype=[0, 1, -1], expected=True),\n      dict(genotype=[0, 1, 2], expected=True),\n      dict(genotype=[-1, 0, -1073741825], expected=False),\n  )\n  def test_is_heterozygous(self, genotype, expected):\n    call = variants_pb2.VariantCall(genotype=genotype)\n    actual = variantcall_utils.is_heterozygous(call)\n    self.assertEqual(actual, expected)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/util/vcf_constants.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Constants related to the VCF variant specification.\n\nSee the full specification at https://samtools.github.io/hts-specs/VCFv4.3.pdf\nfor details.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nfrom nucleus.protos import variants_pb2\nfrom nucleus.util import struct_utils\n\n# The alternate allele string for reference (no alt).\nNO_ALT_ALLELE = \'.\'\n\n# The alternate allele string for the gVCF ""any"" alternate allele.\nGVCF_ALT_ALLELE = \'<*>\'\n\n# Older symbolic alt allele, similar in meaning to gVCF alt allele\nSYMBOLIC_ALT_ALLELE = \'<NON_REF>\'\n\n# The replacement field used for missing data.\nMISSING_FIELD = \'.\'\n\n# Valid types for INFO and FORMAT fields, as per the VCF 4.3 spec.\nCHARACTER_TYPE = \'Character\'\nFLAG_TYPE = \'Flag\'\nFLOAT_TYPE = \'Float\'\nINTEGER_TYPE = \'Integer\'\nSTRING_TYPE = \'String\'\n\n# Reserved FILTER field definitions.\nRESERVED_FILTER_FIELDS = [\n    variants_pb2.VcfFilterInfo(id=\'PASS\', description=\'All filters passed\'),\n]\n\n# Reserved INFO field definitions, as per the VCF 4.3 spec.\nRESERVED_INFO_FIELDS = [\n    variants_pb2.VcfInfo(\n        id=\'AA\', number=\'1\', type=STRING_TYPE, description=\'Ancestral allele\'),\n    variants_pb2.VcfInfo(\n        id=\'AC\',\n        number=\'A\',\n        type=INTEGER_TYPE,\n        description=\'Allele count in genotypes, for each ALT \'\n        \'allele, in the same order as listed\'),\n    variants_pb2.VcfInfo(\n        id=\'AD\',\n        number=\'R\',\n        type=INTEGER_TYPE,\n        description=\'Total read depth for each allele\'),\n    variants_pb2.VcfInfo(\n        id=\'ADF\',\n        number=\'R\',\n        type=INTEGER_TYPE,\n        description=\'Read depth for each allele on the forward \'\n        \'strand\'),\n    variants_pb2.VcfInfo(\n        id=\'ADR\',\n        number=\'R\',\n        type=INTEGER_TYPE,\n        description=\'Read depth for each allele on the reverse strand\'),\n    variants_pb2.VcfInfo(\n        id=\'AF\',\n        number=\'A\',\n        type=FLOAT_TYPE,\n        description=\'Allele frequency for each ALT allele in \'\n        \'the same order as listed (estimated from \'\n        \'primary data, not called genotypes)\'),\n    variants_pb2.VcfInfo(\n        id=\'AN\',\n        number=\'1\',\n        type=INTEGER_TYPE,\n        description=\'Total number of alleles in called genotypes\'),\n    variants_pb2.VcfInfo(\n        id=\'BQ\', number=\'1\', type=FLOAT_TYPE, description=\'RMS base quality\'),\n    variants_pb2.VcfInfo(\n        id=\'CIGAR\',\n        number=\'A\',\n        type=STRING_TYPE,\n        description=\'Cigar string describing how to align an \'\n        \'alternate allele to the reference allele\'),\n    variants_pb2.VcfInfo(\n        id=\'DB\', number=\'0\', type=FLAG_TYPE, description=\'dbSNP membership\'),\n    variants_pb2.VcfInfo(\n        id=\'DP\',\n        number=\'1\',\n        type=INTEGER_TYPE,\n        description=\'Combined depth across samples\'),\n    variants_pb2.VcfInfo(\n        id=\'END\',\n        number=\'1\',\n        type=INTEGER_TYPE,\n        description=\'End position (for use with symbolic alleles)\'),\n    variants_pb2.VcfInfo(\n        id=\'H2\', number=\'0\', type=FLAG_TYPE, description=\'HapMap2 membership\'),\n    variants_pb2.VcfInfo(\n        id=\'H3\', number=\'0\', type=FLAG_TYPE, description=\'HapMap3 membership\'),\n    # NOTE: In the VCF 4.3 spec, the type of \'MQ\' is listed as \'.\', even though\n    # that is not specified as a valid type. Because root mean square is\n    # typically a float value, we specify its type as FLOAT_TYPE.\n    variants_pb2.VcfInfo(\n        id=\'MQ\', number=\'1\', type=FLOAT_TYPE,\n        description=\'RMS mapping quality\'),\n    variants_pb2.VcfInfo(\n        id=\'MQ0\',\n        number=\'1\',\n        type=INTEGER_TYPE,\n        description=\'Number of MAPQ == 0 reads\'),\n    variants_pb2.VcfInfo(\n        id=\'NS\',\n        number=\'1\',\n        type=INTEGER_TYPE,\n        description=\'Number of samples with data\'),\n    # NOTE: In the VCF 4.3 spec, the type of \'SB\' is listed as \'.\', even though\n    # that is not specified as a valid type. Because strand bias is usually a\n    # numerical measurement (e.g. p-value of contingency table), we specify its\n    # type as FLOAT_TYPE.\n    variants_pb2.VcfInfo(\n        id=\'SB\', number=\'.\', type=FLOAT_TYPE, description=\'Strand bias\'),\n    variants_pb2.VcfInfo(\n        id=\'SOMATIC\',\n        number=\'0\',\n        type=FLAG_TYPE,\n        description=\'Somatic mutation (for cancer genomics)\'),\n    variants_pb2.VcfInfo(\n        id=\'VALIDATED\',\n        number=\'0\',\n        type=FLAG_TYPE,\n        description=\'Validated by follow-up experiment\'),\n    variants_pb2.VcfInfo(\n        id=\'1000G\',\n        number=\'0\',\n        type=FLAG_TYPE,\n        description=\'1000 Genomes membership\'),\n]\n\n# Reserved FORMAT field definitions, as per the VCF 4.3 spec.\nRESERVED_FORMAT_FIELDS = [\n    variants_pb2.VcfFormatInfo(\n        id=\'AD\',\n        number=\'R\',\n        type=INTEGER_TYPE,\n        description=\'Read depth for each allele\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'ADF\',\n        number=\'R\',\n        type=INTEGER_TYPE,\n        description=\'Read depth for each allele on the \'\n        \'forward strand\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'ADR\',\n        number=\'R\',\n        type=INTEGER_TYPE,\n        description=\'Read depth for each allele on the \'\n        \'reverse strand\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'DP\', number=\'1\', type=INTEGER_TYPE, description=\'Read depth\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'EC\',\n        number=\'A\',\n        type=INTEGER_TYPE,\n        description=\'Expected alternate allele counts\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'FT\',\n        number=\'1\',\n        type=STRING_TYPE,\n        description=\'Filter indicating if this genotype \'\n        \'was ""called""\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'GL\',\n        number=\'G\',\n        type=FLOAT_TYPE,\n        description=\'Genotype likelihoods\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'GP\',\n        number=\'G\',\n        type=FLOAT_TYPE,\n        description=\'Genotype posterior probabilities\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'GQ\',\n        number=\'1\',\n        type=INTEGER_TYPE,\n        description=\'Conditional genotype quality\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'GT\', number=\'1\', type=STRING_TYPE, description=\'Genotype\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'HQ\', number=\'2\', type=INTEGER_TYPE,\n        description=\'Haplotype quality\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'MQ\',\n        number=\'1\',\n        type=INTEGER_TYPE,\n        description=\'RMS mapping quality\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'PL\',\n        number=\'G\',\n        type=INTEGER_TYPE,\n        description=\'Phred-scaled genotype likelihoods \'\n        \'rounded to the closest integer\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'PQ\', number=\'1\', type=INTEGER_TYPE, description=\'Phasing quality\'),\n    variants_pb2.VcfFormatInfo(\n        id=\'PS\', number=\'1\', type=INTEGER_TYPE, description=\'Phase set\'),\n]\n\n# Map from field type to the function used to set struct_pb2.Value elements\n# of that type.\nSET_FN_LOOKUP = {\n    INTEGER_TYPE: struct_utils.set_int_field,\n    FLOAT_TYPE: struct_utils.set_number_field,\n    STRING_TYPE: struct_utils.set_string_field,\n    CHARACTER_TYPE: struct_utils.set_string_field,\n    FLAG_TYPE: struct_utils.set_bool_field,\n}\n\n\ndef _get_reserved_field(field_id, reserved_fields):\n  """"""Returns the desired reserved field.\n\n  Args:\n    field_id: str. The id of the field to retrieve.\n    reserved_fields: list(fields). The reserved fields to search.\n\n  Returns:\n    The reserved field with the given `field_id`.\n\n  Raises:\n    ValueError: `field_id` is not a known reserved field.\n  """"""\n  matching_fields = [field for field in reserved_fields if field.id == field_id]\n  if not matching_fields:\n    raise ValueError(\'No reserved field with id `{}`\'.format(field_id))\n  return matching_fields[0]\n\n\ndef reserved_filter_field(field_id):\n  """"""Returns the reserved FILTER field with the given ID.""""""\n  return _get_reserved_field(field_id, RESERVED_FILTER_FIELDS)\n\n\ndef reserved_info_field(field_id):\n  """"""Returns the reserved INFO field with the given ID.""""""\n  return _get_reserved_field(field_id, RESERVED_INFO_FIELDS)\n\n\ndef reserved_format_field(field_id):\n  """"""Returns the reserved FORMAT field with the given ID.""""""\n  return _get_reserved_field(field_id, RESERVED_FORMAT_FIELDS)\n\n\ndef create_get_fn(value_type, number):\n  """"""Returns a callable that extracts the typed information from a ListValue.\n\n  Args:\n    value_type: str. The value type stored as defined in the VCF 4.3 spec.\n    number: str. The number of entries of this value as defined in the VCF spec.\n\n  Returns:\n    A callable that takes two inputs: A Map(str --> ListValue) and a string\n    field name and returns the associated typed value(s). The return value is\n    a list of typed values or a single typed value, depending on the expected\n    number of values returned.\n  """"""\n  is_single_field = (number == \'0\' or number == \'1\')\n  if value_type == CHARACTER_TYPE or value_type == STRING_TYPE:\n    return functools.partial(\n        struct_utils.get_string_field, is_single_field=is_single_field)\n  elif value_type == INTEGER_TYPE:\n    return functools.partial(\n        struct_utils.get_int_field, is_single_field=is_single_field)\n  elif value_type == FLOAT_TYPE:\n    return functools.partial(\n        struct_utils.get_number_field, is_single_field=is_single_field)\n  elif value_type == FLAG_TYPE:\n    return functools.partial(\n        struct_utils.get_bool_field, is_single_field=is_single_field)\n  else:\n    raise ValueError(\'Invalid value_type: {}\'.format(value_type))\n\n\n# Map from INFO field name to the function used to set struct_pb2.Value elements\n# of that field.\nRESERVED_INFO_FIELD_SET_FNS = {\n    info.id: SET_FN_LOOKUP[info.type]\n    for info in RESERVED_INFO_FIELDS\n}\n\n# Map from INFO field name to the function used to get struct_pb2.Value elements\n# of that field.\nRESERVED_INFO_FIELD_GET_FNS = {\n    info.id: create_get_fn(info.type, info.number)\n    for info in RESERVED_INFO_FIELDS\n}\n\n# Map from FORMAT field name to the function used to set struct_pb2.Value\n# elements of that field.\nRESERVED_FORMAT_FIELD_SET_FNS = {\n    fmt.id: SET_FN_LOOKUP[fmt.type]\n    for fmt in RESERVED_FORMAT_FIELDS\n}\n\n# Map from FORMAT field name to the function used to get struct_pb2.Value\n# elements of that field.\nRESERVED_FORMAT_FIELD_GET_FNS = {\n    fmt.id: create_get_fn(fmt.type, fmt.number)\n    for fmt in RESERVED_FORMAT_FIELDS\n}\n\n\ndef reserved_info_field_set_fn(field_name):\n  """"""Returns the callable that sets the proper field for the given field_name.\n\n  Args:\n    field_name: str. The field name of the reserved INFO field (e.g. \'MQ\').\n\n  Returns:\n    The callable that takes in a Map(str --> ListValue), field name, and value\n    and modifies the map to populate the field_name entry with the given value.\n\n  Raises:\n    ValueError: The field_name is not a known reserved INFO field.\n  """"""\n  try:\n    return RESERVED_INFO_FIELD_SET_FNS[field_name]\n  except KeyError:\n    raise ValueError(\'Unknown reserved INFO field: {}\'.format(field_name))\n\n\ndef reserved_info_field_get_fn(field_name):\n  """"""Returns the callable that gets the proper field for the given field_name.\n\n  Args:\n    field_name: str. The field name of the reserved INFO field (e.g. \'MQ\').\n\n  Returns:\n    The callable that takes in a Map(str --> ListValue), and field name and\n    returns the associated typed value(s).\n\n  Raises:\n    ValueError: The field_name is not a known reserved INFO field.\n  """"""\n  try:\n    return RESERVED_INFO_FIELD_GET_FNS[field_name]\n  except KeyError:\n    raise ValueError(\n        \'Unknown reserved INFO field to get: {}\'.format(field_name))\n\n\ndef reserved_format_field_set_fn(field_name):\n  """"""Returns the callable that sets the proper field for the given field_name.\n\n  Args:\n    field_name: str. The field name of the reserved FORMAT field (e.g. \'AD\').\n\n  Returns:\n    The callable that takes in a Map(str --> ListValue), field name, and value\n    and modifies the map to populate the field_name entry with the given value.\n\n  Raises:\n    ValueError: The field_name is not a known reserved FORMAT field.\n  """"""\n  try:\n    return RESERVED_FORMAT_FIELD_SET_FNS[field_name]\n  except KeyError:\n    raise ValueError(\'Unknown reserved FORMAT field: {}\'.format(field_name))\n\n\ndef reserved_format_field_get_fn(field_name):\n  """"""Returns the callable that gets the proper field for the given field_name.\n\n  Args:\n    field_name: str. The field name of the reserved FORMAT field (e.g. \'AD\').\n\n  Returns:\n    The callable that takes in a Map(str --> ListValue), and field name and\n    returns the associated typed value(s).\n\n  Raises:\n    ValueError: The field_name is not a known reserved FORMAT field.\n  """"""\n  try:\n    return RESERVED_FORMAT_FIELD_GET_FNS[field_name]\n  except KeyError:\n    raise ValueError(\n        \'Unknown reserved FORMAT field to get: {}\'.format(field_name))\n'"
nucleus/util/vcf_constants_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for nucleus.util.vcf_constants.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom nucleus.protos import variants_pb2\nfrom nucleus.util import struct_utils\nfrom nucleus.util import vcf_constants\n\n\nclass VcfConstantsTest(parameterized.TestCase):\n\n  def test_unique_reserved_filter(self):\n    num_reserved_filter = len(vcf_constants.RESERVED_FILTER_FIELDS)\n    unique_filt_ids = {filt.id for filt in vcf_constants.RESERVED_FILTER_FIELDS}\n    self.assertLen(unique_filt_ids, num_reserved_filter)\n\n  def test_unique_reserved_info(self):\n    num_reserved_info = len(vcf_constants.RESERVED_INFO_FIELDS)\n    unique_info_ids = {info.id for info in vcf_constants.RESERVED_INFO_FIELDS}\n    self.assertLen(unique_info_ids, num_reserved_info)\n\n  def test_unique_reserved_format(self):\n    num_reserved_format = len(vcf_constants.RESERVED_FORMAT_FIELDS)\n    unique_format_ids = {f.id for f in vcf_constants.RESERVED_FORMAT_FIELDS}\n    self.assertLen(unique_format_ids, num_reserved_format)\n\n  def test_get_reserved_filter(self):\n    filt = vcf_constants.reserved_filter_field(\'PASS\')\n    self.assertIsInstance(filt, variants_pb2.VcfFilterInfo)\n    self.assertEqual(filt.id, \'PASS\')\n    self.assertEqual(filt.description, \'All filters passed\')\n\n  @parameterized.parameters(\n      \'RefCall\',\n      \'LowQual\',\n      \'AD\',\n      \'DP\',\n      \'GT\',\n      \'GQ\',\n  )\n  def test_invalid_get_reserved_filter(self, field_id):\n    with self.assertRaisesRegexp(ValueError, \'No reserved field with id\'):\n      vcf_constants.reserved_filter_field(field_id)\n\n  @parameterized.parameters(\n      \'AA\',\n      \'AC\',\n      \'AD\',\n      \'ADF\',\n      \'END\',\n      \'H2\',\n  )\n  def test_get_reserved_info(self, field_id):\n    info = vcf_constants.reserved_info_field(field_id)\n    self.assertIsInstance(info, variants_pb2.VcfInfo)\n    self.assertEqual(info.id, field_id)\n\n  @parameterized.parameters(\n      \'PASS\',\n      \'GT\',\n      \'GQ\',\n      \'GL\',\n      \'FT\',\n  )\n  def test_invalid_get_reserved_info(self, field_id):\n    with self.assertRaisesRegexp(ValueError, \'No reserved field with id\'):\n      vcf_constants.reserved_info_field(field_id)\n\n  @parameterized.parameters(\n      \'AD\',\n      \'ADF\',\n      \'DP\',\n      \'GT\',\n      \'GQ\',\n      \'GL\',\n      \'FT\',\n      \'PL\',\n  )\n  def test_get_reserved_format(self, field_id):\n    fmt = vcf_constants.reserved_format_field(field_id)\n    self.assertIsInstance(fmt, variants_pb2.VcfFormatInfo)\n    self.assertEqual(fmt.id, field_id)\n\n  @parameterized.parameters(\n      \'PASS\',\n      \'AN\',\n      \'1000G\',\n      \'END\',\n      \'H2\',\n  )\n  def test_invalid_get_reserved_format(self, field_id):\n    with self.assertRaisesRegexp(ValueError, \'No reserved field with id\'):\n      vcf_constants.reserved_format_field(field_id)\n\n  @parameterized.parameters(\n      dict(\n          value_type=vcf_constants.CHARACTER_TYPE,\n          values=[\'a\'],\n          number=\'1\',\n          expected=\'a\'),\n      dict(\n          value_type=vcf_constants.CHARACTER_TYPE,\n          values=[\'b\'],\n          number=\'.\',\n          expected=[\'b\']),\n      dict(\n          value_type=vcf_constants.CHARACTER_TYPE,\n          values=[\'c\', \'d\'],\n          number=\'R\',\n          expected=[\'c\', \'d\']),\n      dict(\n          value_type=vcf_constants.FLAG_TYPE,\n          values=[True],\n          number=\'0\',\n          expected=True),\n      dict(\n          value_type=vcf_constants.FLOAT_TYPE,\n          values=[2.5],\n          number=\'1\',\n          expected=2.5),\n      dict(\n          value_type=vcf_constants.FLOAT_TYPE,\n          values=[2.5],\n          number=\'.\',\n          expected=[2.5]),\n      dict(\n          value_type=vcf_constants.FLOAT_TYPE,\n          values=[2.5, 3.5],\n          number=\'A\',\n          expected=[2.5, 3.5]),\n      dict(\n          value_type=vcf_constants.INTEGER_TYPE,\n          values=[2, 3, 4],\n          number=\'G\',\n          expected=[2, 3, 4]),\n      dict(\n          value_type=vcf_constants.STRING_TYPE,\n          values=[\'a\', \'bc\'],\n          number=\'.\',\n          expected=[\'a\', \'bc\']),\n  )\n  def test_create_get_fn(self, value_type, values, number, expected):\n    info = variants_pb2.Variant().info\n    set_fn = vcf_constants.SET_FN_LOOKUP[value_type]\n    set_fn(info, \'field\', values)\n    get_fn = vcf_constants.create_get_fn(value_type, number)\n    actual = get_fn(info, \'field\')\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(field=\'CIGAR\', expected=struct_utils.set_string_field),\n      dict(field=\'DP\', expected=struct_utils.set_int_field),\n      dict(field=\'MQ\', expected=struct_utils.set_number_field),\n      dict(field=\'SOMATIC\', expected=struct_utils.set_bool_field),\n  )\n  def test_reserved_info_field_set_fn(self, field, expected):\n    actual = vcf_constants.reserved_info_field_set_fn(field)\n    self.assertIs(actual, expected)\n\n  @parameterized.parameters(\n      dict(field=\'INVALID\'),\n      dict(field=\'EC\'),\n      dict(field=\'HQ\'),\n  )\n  def test_invalid_reserved_info_field_set_fn(self, field):\n    with self.assertRaisesRegexp(ValueError, \'Unknown reserved INFO field:\'):\n      vcf_constants.reserved_info_field_set_fn(field)\n\n  def test_reserved_info_field_get_fn(self):\n    info = variants_pb2.Variant().info\n    values = [\'C\']\n    struct_utils.set_string_field(info, \'AA\', values)\n    get_fn = vcf_constants.reserved_info_field_get_fn(\'AA\')\n    actual = get_fn(info, \'AA\')\n    self.assertEqual(actual, values[0])\n\n  @parameterized.parameters(\n      dict(field=\'INVALID\'),\n      dict(field=\'EC\'),\n      dict(field=\'HQ\'),\n  )\n  def test_invalid_reserved_info_field_get_fn(self, field):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Unknown reserved INFO field to get:\'):\n      vcf_constants.reserved_info_field_get_fn(field)\n\n  @parameterized.parameters(\n      dict(field=\'AD\', expected=struct_utils.set_int_field),\n      dict(field=\'GL\', expected=struct_utils.set_number_field),\n      dict(field=\'FT\', expected=struct_utils.set_string_field),\n  )\n  def test_reserved_format_field_set_fn(self, field, expected):\n    actual = vcf_constants.reserved_format_field_set_fn(field)\n    self.assertIs(actual, expected)\n\n  @parameterized.parameters(\n      dict(field=\'INVALID\'),\n      dict(field=\'CIGAR\'),\n      dict(field=\'H2\'),\n  )\n  def test_invalid_reserved_format_field_set_fn(self, field):\n    with self.assertRaisesRegexp(ValueError, \'Unknown reserved FORMAT field:\'):\n      vcf_constants.reserved_format_field_set_fn(field)\n\n  def test_reserved_format_field_get_fn(self):\n    info = variants_pb2.VariantCall().info\n    expected = [0.2, 0.5, 0.3]\n    struct_utils.set_number_field(info, \'GP\', expected[:])\n    get_fn = vcf_constants.reserved_format_field_get_fn(\'GP\')\n    actual = get_fn(info, \'GP\')\n    self.assertEqual(actual, expected)\n\n  @parameterized.parameters(\n      dict(field=\'INVALID\'),\n      dict(field=\'CIGAR\'),\n      dict(field=\'H2\'),\n  )\n  def test_invalid_reserved_format_field_get_fn(self, field):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Unknown reserved FORMAT field to get:\'):\n      vcf_constants.reserved_format_field_get_fn(field)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/util/vis.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Utility functions for visualization and inspection of pileup examples.\n\nVisualization and inspection utility functions enable showing image-like array\ndata including those used in DeepVariant.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom IPython import display\nimport numpy as np\nfrom PIL import Image\nfrom PIL import ImageDraw\n\nfrom nucleus.protos import variants_pb2\n\nDEEPVARIANT_CHANNEL_NAMES = [\n    \'read base\', \'base quality\', \'mapping quality\', \'strand\',\n    \'read supports variant\', \'base differs from ref\', \'alternate allele 1\',\n    \'alternate allele 2\'\n]\n\n\ndef get_image_array_from_example(example):\n  """"""Decode image/encoded and image/shape of an Example into a numpy array.\n\n  Parse image/encoded and image/shape features from a tensorflow Example and\n  decode the image into that shape.\n\n  Args:\n    example: a tensorflow Example containing features that include\n      ""image/encoded"" and ""image/shape""\n\n  Returns:\n    numpy array of dtype np.uint8.\n  """"""\n  features = example.features.feature\n  img = features[\'image/encoded\'].bytes_list.value[0]\n  shape = features[\'image/shape\'].int64_list.value[0:3]\n  return np.frombuffer(img, np.uint8).reshape(shape)\n\n\ndef split_3d_array_into_channels(arr):\n  """"""Split 3D array into a list of 2D arrays.\n\n  e.g. given a numpy array of shape (100, 200, 6), return a list of 6 channels,\n  each with shape (100, 200).\n\n  Args:\n    arr: a 3D numpy array.\n\n  Returns:\n    list of 2D numpy arrays.\n  """"""\n  return [arr[:, :, i] for i in range(arr.shape[-1])]\n\n\ndef channels_from_example(example):\n  """"""Extract image from an Example and return the list of channels.\n\n  Args:\n    example: a tensorflow Example containing features that include\n      ""image/encoded"" and ""image/shape""\n\n  Returns:\n    list of 2D numpy arrays, one for each channel.\n  """"""\n  image = get_image_array_from_example(example)\n  return split_3d_array_into_channels(image)\n\n\ndef convert_6_channels_to_rgb(channels):\n  """"""Convert 6-channel image from DeepVariant to RGB for quick visualization.\n\n  The 6 channels are: ""read base"", ""base quality"", ""mapping quality"", ""strand"",\n  ""supports variant"", ""base != reference"".\n\n  Args:\n    channels: a list of 6 numpy arrays.\n\n  Returns:\n    3D numpy array of 3 colors (Red, green, blue).\n  """"""\n  base = channels[0]\n  # qual is the minimum of base quality and mapping quality at each position\n  # 254 is the max value for quality scores because the SAM specification has\n  # 255 reserved for unavailable values.\n  qual = np.minimum(channels[1], channels[2])\n  strand = channels[3]\n  # alpha is <supports variant> * <base != reference>\n  alpha = np.multiply(channels[4] / 254.0, channels[5] / 254.0)\n  return np.multiply(np.stack([base, qual, strand]),\n                     alpha).astype(np.uint8).transpose([1, 2, 0])\n\n\ndef scale_colors_for_png(arr, vmin=0, vmax=255):\n  """"""Scale an array to integers between 0 and 255 to prep it for a PNG image.\n\n  Args:\n    arr: numpy array. Input array made up of integers or floats.\n    vmin: number. Minimum data value to map to 0. Values below this will be\n      clamped to this value and therefore become 0.\n    vmax: number. Maximum data value to map to 255. Values above this will be\n      clamped to this value and therefore become 255.\n\n  Returns:\n    numpy array of dtype np.uint8 (integers between 0 and 255).\n  """"""\n  if vmax == 0 or vmax <= vmin:\n    raise ValueError(\'vmin must be non-zero and higher than vmin.\')\n\n  # Careful not to modify the original array\n  scaled = np.copy(arr)\n\n  # Snap numbers in the array falling outside the range into the range,\n  # otherwise they will produce artifacts due to byte overflow\n  scaled[scaled > vmax] = vmax\n  scaled[scaled < vmin] = vmin\n\n  # Scale the input into the range of vmin to vmax\n  if vmin != 0 or vmax != 255:\n    scaled = ((scaled - vmin) / (vmax - vmin)) * 255\n  return scaled.astype(np.uint8)\n\n\ndef _get_image_type_from_array(arr):\n  """"""Find image type based on array dimensions.\n\n  Raises error on invalid image dimensions.\n  Args:\n    arr: numpy array. Input array.\n\n  Returns:\n    str. ""RGB"" or ""L"", meant for PIL.Image.fromarray.\n  """"""\n  if len(arr.shape) == 3 and arr.shape[2] == 3:\n    # 8-bit x 3 colors\n    return \'RGB\'\n  elif len(arr.shape) == 2:\n    # 8-bit, gray-scale\n    return \'L\'\n  else:\n    raise ValueError(\n        \'Input array must have either 2 dimensions or 3 dimensions where the \'\n        \'third dimension has 3 channels. i.e. arr.shape is (x,y) or (x,y,3). \'\n        \'Found shape {}.\'.format(arr.shape))\n\n\ndef autoscale_colors_for_png(arr, vmin=None, vmax=None):\n  """"""Adjust an array to prepare it for saving to an image.\n\n  Re-scale numbers in the input array to go from 0 to 255 to adapt them for a\n  PNG image.\n\n  Args:\n    arr: numpy array. Should be 2-dimensional or 3-dimensional where the third\n      dimension has 3 channels.\n    vmin: number (float or int). Minimum data value, which will correspond to\n      black in greyscale or lack of each color in RGB images. Default None takes\n      the minimum of the data from arr.\n    vmax: number (float or int). Maximum data value, which will correspond to\n      white in greyscale or full presence of each color in RGB images. Default\n      None takes the max of the data from arr.\n\n  Returns:\n    (modified numpy array, image_mode)\n  """"""\n  image_mode = _get_image_type_from_array(arr)\n\n  if vmin is None:\n    vmin = np.min(arr)\n  if vmax is None:\n    vmax = np.max(arr)\n\n  # In cases where all elements are the same, fix the vmax so that even though\n  # the whole image will be black, the user can at least see the shape\n  if vmin == vmax:\n    vmax = vmin + 1\n\n  scaled = scale_colors_for_png(arr, vmin=vmin, vmax=vmax)\n  return scaled, image_mode\n\n\ndef add_header(img, labels, mark_midpoints=True, header_height=20):\n  """"""Adds labels to the image, evenly distributed across the top.\n\n  This is primarily useful for showing the names of channels.\n\n  Args:\n    img: A PIL Image.\n    labels: list of strs. Labels for segments to write across the top.\n    mark_midpoints: bool. Whether to add a small vertical line marking the\n      center of each segment of the image.\n    header_height: int. Height of the header in pixels.\n\n  Returns:\n    A new PIL Image, taller than the original img and annotated.\n  """"""\n\n  # Create a taller image to make space for a header at the top.\n  new_height = header_height + img.size[1]\n  new_width = img.size[0]\n\n  if img.mode == \'RGB\':\n    placeholder_size = (new_height, new_width, 3)\n  else:\n    placeholder_size = (new_height, new_width)\n  placeholder = np.ones(placeholder_size, dtype=np.uint8) * 255\n\n  # Divide the image width into segments.\n  segment_width = img.size[0] / len(labels)\n\n  # Calculate midpoints for all segments.\n  midpoints = [int(segment_width * (i + 0.5)) for i in range(len(labels))]\n\n  if mark_midpoints:\n    # For each label, add a small line to mark the middle.\n    for x_position in midpoints:\n      placeholder[header_height - 5:header_height, x_position] = 0\n      # If image has an even width, it will need 2 pixels marked as the middle.\n      if segment_width % 2 == 0:\n        placeholder[header_height - 5:header_height, x_position + 1] = 0\n\n  bigger_img = Image.fromarray(placeholder, mode=img.mode)\n  # Place the original image inside the taller placeholder image.\n  bigger_img.paste(img, (0, header_height))\n\n  # Add a label for each segment.\n  draw = ImageDraw.Draw(bigger_img)\n  for i in range(len(labels)):\n    text = labels[i]\n    text_width = draw.textsize(text)[0]\n    # xy refers to the left top corner of the text, so to center the text on\n    # the midpoint, subtract half the text width from the midpoint position.\n    x_position = int(midpoints[i] - text_width / 2)\n    draw.text(xy=(x_position, 0), text=text, fill=\'black\')\n  return bigger_img\n\n\ndef save_to_png(arr,\n                path=None,\n                image_mode=None,\n                show=True,\n                labels=None,\n                scale=None):\n  """"""Make a PNG and show it from a numpy array of dtype=np.uint8.\n\n  Args:\n    arr: numpy array. Input array to save.\n    path: str. file path at which to save the image.\n    image_mode: ""RGB"" or ""L"". Leave as default=None to choose based on image\n      dimensions.\n    show: bool. Whether to display the image using IPython (for notebooks).\n    labels: list of str. Labels to show across the top of the image.\n    scale: integer. Number of pixels wide and tall to show each cell in the\n      array. This sizes up the image while keeping exactly the same number of\n      pixels for every cell in the array, preserving resolution and preventing\n      any interpolation or overlapping of pixels. Default None adapts to the\n      size of the image to multiply it up until a limit of 500 pixels, a\n      convenient size for use in notebooks. If saving to a file for automated\n      processing, scale=1 is recommended to keep output files small and simple\n      while still retaining all the information content.\n\n  Returns:\n    None. Saves an image at path and optionally shows it with IPython.display.\n  """"""\n  if image_mode is None:\n    image_mode = _get_image_type_from_array(arr)\n\n  img = Image.fromarray(arr, mode=image_mode)\n\n  if labels is not None:\n    img = add_header(img, labels)\n\n  if scale is None:\n    scale = max(1, int(500 / max(arr.shape)))\n\n  if scale != 1:\n    img = img.resize((img.size[0] * scale, img.size[1] * scale))\n\n  # Saving to a temporary file is needed even when showing in a notebook\n  if path is None:\n    path = \'/tmp/tmp.png\'\n  img.save(path)\n\n  # Show image (great for notebooks)\n  if show:\n    display.display(display.Image(path))\n\n\ndef array_to_png(arr,\n                 path=None,\n                 show=True,\n                 vmin=None,\n                 vmax=None,\n                 scale=None,\n                 labels=None):\n  """"""Save an array as a PNG image with PIL and show it.\n\n  Args:\n    arr: numpy array. Should be 2-dimensional or 3-dimensional where the third\n      dimension has 3 channels.\n    path: str. Path for the image output. Default is /tmp/tmp.png for quickly\n      showing the image in a notebook.\n    show: bool. Whether to show the image using IPython utilities, only works in\n      notebooks.\n    vmin: number. Minimum data value, which will correspond to black in\n      greyscale or lack of each color in RGB images. Default None takes the\n      minimum of the data from arr.\n    vmax: number. Maximum data value, which will correspond to white in\n      greyscale or full presence of each color in RGB images. Default None takes\n      the max of the data from arr.\n    scale: integer. Number of pixels wide and tall to show each cell in the\n      array. This sizes up the image while keeping exactly the same number of\n      pixels for every cell in the array, preserving resolution and preventing\n      any interpolation or overlapping of pixels. Default None adapts to the\n      size of the image to multiply it up until a limit of 500 pixels, a\n      convenient size for use in notebooks. If saving to a file for automated\n      processing, scale=1 is recommended to keep output files small and simple\n      while still retaining all the information content.\n    labels: list of str. Labels to show across the top of the image.\n\n  Returns:\n    None. Saves an image at path and optionally shows it with IPython.display.\n  """"""\n  scaled, image_mode = autoscale_colors_for_png(arr, vmin=vmin, vmax=vmax)\n  save_to_png(\n      scaled,\n      path=path,\n      show=show,\n      image_mode=image_mode,\n      labels=labels,\n      scale=scale)\n\n\ndef _deepvariant_channel_names(num_channels):\n  """"""Get DeepVariant channel names for the given number of channels.""""""\n  # Add additional empty labels if there are more channels than expected.\n  filler_labels = [\n      \'channel {}\'.format(i + 1)\n      for i in range(len(DEEPVARIANT_CHANNEL_NAMES), num_channels)\n  ]\n  labels = DEEPVARIANT_CHANNEL_NAMES + filler_labels\n  # Trim off any extra labels.\n  return labels[0:num_channels]\n\n\ndef draw_deepvariant_pileup(example=None,\n                            channels=None,\n                            composite_type=None,\n                            annotated=True,\n                            labels=None,\n                            path=None,\n                            show=True,\n                            scale=None):\n  """"""Quick utility for showing a pileup example as channels or RGB.\n\n  Args:\n    example: A tensorflow Example containing image/encoded and image/shape\n      features. Will be parsed through channels_from_example. Ignored if\n      channels are provided directly. Either example OR channels is required.\n    channels: list of 2D arrays containing the data to draw. Either example OR\n      channels is required.\n    composite_type: str or None. Method for combining channels. One of\n      [None,""RGB""].\n    annotated: bool. Whether to add channel labels and mark midpoints.\n    labels: list of str. Which labels to add to the image. If annotated=True,\n      use default channels labels for DeepVariant.\n    path: str. Output file path for saving as an image. If None, just show plot.\n    show: bool. Whether to display the image for ipython notebooks. Set to False\n      to prevent extra output when running in bulk.\n    scale: integer. Multiplier to enlarge the image. Default: None, which will\n      set it automatically for a human-readable size. Set to 1 for no scaling.\n\n  Returns:\n    None. Saves an image at path and optionally shows it with IPython.display.\n  """"""\n  if example and not channels:\n    channels = channels_from_example(example)\n  elif not channels:\n    raise ValueError(\'Either example OR channels must be specified.\')\n\n  if composite_type is None:\n    img_array = np.concatenate(channels, axis=1)\n    if annotated and labels is None:\n      labels = _deepvariant_channel_names(len(channels))\n  elif composite_type == \'RGB\':\n    img_array = convert_6_channels_to_rgb(channels)\n    if annotated and labels is None:\n      labels = [\'\']  # Creates one midpoint with no label.\n  else:\n    raise ValueError(\n        ""Unrecognized composite_type: {}. Must be None or \'RGB\'"".format(\n            composite_type))\n\n  array_to_png(\n      img_array,\n      path=path,\n      show=show,\n      scale=scale,\n      labels=labels,\n      vmin=0,\n      vmax=254)\n\n\ndef variant_from_example(example):\n  """"""Extract Variant object from the \'variant/encoded\' feature of an Example.\n\n  Args:\n    example: a DeepVariant-style make_examples output example.\n\n  Returns:\n    A Nucleus Variant.\n  """"""\n  features = example.features.feature\n  var_string = features[\'variant/encoded\'].bytes_list.value[0]\n  return variants_pb2.Variant.FromString(var_string)\n\n\ndef locus_id_from_variant(variant):\n  """"""Create a locus ID of form ""chr:pos_ref"" from a Variant object.\n\n  Args:\n    variant: a nucleus variant.\n\n  Returns:\n    str.\n  """"""\n  return \'{}:{}_{}\'.format(variant.reference_name, variant.start,\n                           variant.reference_bases)\n\n\ndef alt_allele_indices_from_example(example):\n  """"""Extract indices of the particular alt allele(s) the example represents.\n\n  Args:\n    example: a DeepVariant make_examples output example.\n\n  Returns:\n    list of indices.\n  """"""\n  features = example.features.feature\n  val = features[\'alt_allele_indices/encoded\'].bytes_list.value[0]\n  # Extract the encoded proto into unsigned integers and convert to regular ints\n  mapped = [int(x) for x in np.frombuffer(val, dtype=np.uint8)]\n  # Format is [<field id + type>, <number of elements in array>, ...<array>].\n  # Extract the array only, leaving out the metadata.\n  return mapped[2:]\n\n\ndef alt_bases_from_indices(alt_allele_indices, alternate_bases):\n  """"""Get alt allele bases based on their indices.\n\n  e.g. one alt allele: [0], [""C""] => ""C""\n  or with two alt alleles: [0,2], [""C"", ""TT"", ""A""] => ""C-A""\n\n  Args:\n    alt_allele_indices: list of integers. Indices of the alt alleles for a\n      particular example.\n    alternate_bases: list of strings. All alternate alleles for the variant.\n\n  Returns:\n    str. Alt allele(s) at the indices, joined by \'-\' if more than 1.\n  """"""\n  alleles = [alternate_bases[i] for i in alt_allele_indices]\n  # Avoiding \'/\' to support use in file paths.\n  return \'-\'.join(alleles)\n\n\ndef alt_from_example(example):\n  """"""Get alt allele(s) from a DeepVariant example.\n\n  Args:\n    example: a DeepVariant make_examples output example.\n\n  Returns:\n    str. The bases of the alt alleles, joined by a -.\n  """"""\n  variant = variant_from_example(example)\n  indices = alt_allele_indices_from_example(example)\n  return alt_bases_from_indices(indices, variant.alternate_bases)\n\n\ndef locus_id_with_alt(example):\n  """"""Get complete locus ID from a DeepVariant example.\n\n  Args:\n    example: a DeepVariant make_examples output example.\n\n  Returns:\n    str in the form ""chr:pos_ref_alt.\n  """"""\n  variant = variant_from_example(example)\n  locus_id = locus_id_from_variant(variant)\n  alt = alt_from_example(example)\n  return \'{}_{}\'.format(locus_id, alt)\n\n\ndef label_from_example(example):\n  """"""Get the ""label"" from an example.\n\n  Args:\n    example: a DeepVariant make_examples output example.\n\n  Returns:\n    integer (0, 1, or 2 for regular DeepVariant examples) or None if the\n        example has no label.\n  """"""\n  val = example.features.feature[\'label\'].int64_list.value\n  if val:\n    return int(val[0])\n  else:\n    return None\n'"
nucleus/util/vis_test.py,0,"b'# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Tests for nucleus.util.vis.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport os\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom nucleus.protos import variants_pb2\nfrom nucleus.testing import test_utils\nfrom nucleus.util import vis\n# pylint: disable=g-direct-tensorflow-import\nfrom nucleus.protos import example_pb2\nfrom nucleus.protos import feature_pb2\n\n\ndef _bytes_feature(list_of_strings):\n  """"""Returns a bytes_list from a list of string / byte.""""""\n  return feature_pb2.Feature(\n      bytes_list=feature_pb2.BytesList(value=list_of_strings))\n\n\ndef _int_feature(list_of_ints):\n  """"""Returns a int64_list from a list of int / bool.""""""\n  return feature_pb2.Feature(\n      int64_list=feature_pb2.Int64List(value=list_of_ints))\n\n\ndef _image_array(shape):\n  return np.random.randint(255, size=shape, dtype=np.uint8)\n\n\ndef _mock_example_with_image(shape):\n  arr = _image_array(shape)\n  feature = {\n      \'image/encoded\': _bytes_feature([arr.tobytes()]),\n      \'image/shape\': _int_feature(shape)\n  }\n  return arr, example_pb2.Example(\n      features=feature_pb2.Features(feature=feature))\n\n\ndef _mock_example_with_variant_and_alt_allele_indices(\n    encoded_indices=b\'\\n\\x01\\x00\', alleles=(\'A\', \'C\')):\n  variant = test_utils.make_variant(chrom=\'X\', alleles=alleles, start=10)\n  feature = {\n      \'variant/encoded\': _bytes_feature([variant.SerializeToString()]),\n      \'alt_allele_indices/encoded\': _bytes_feature([encoded_indices])\n  }\n  return example_pb2.Example(features=feature_pb2.Features(feature=feature))\n\n\nclass VisTest(parameterized.TestCase):\n\n  def test_get_image_array_from_example(self):\n    shape = (3, 2, 4)\n    arr, example = _mock_example_with_image(shape)\n    decoded_image_array = vis.get_image_array_from_example(example)\n    self.assertTrue((arr == decoded_image_array).all())\n\n  @parameterized.parameters(((5, 4, 3),), ((10, 7, 5),))\n  def test_split_3d_array_into_channels(self, input_shape):\n    arr = np.random.random(input_shape)\n    output = vis.split_3d_array_into_channels(arr)\n    self.assertLen(output, input_shape[2])\n    for i in range(input_shape[2]):\n      self.assertEqual(output[i].shape, arr.shape[0:2])\n      self.assertTrue((output[i] == arr[:, :, i]).all())\n\n  def test_channels_from_example(self):\n    shape = (3, 2, 4)\n    arr, example = _mock_example_with_image(shape)\n    channels = vis.channels_from_example(example)\n    self.assertLen(channels, shape[2])\n    self.assertTrue((channels[0] == arr[:, :, 0]).all())\n\n  @parameterized.parameters(((4, 8), (4, 8, 3)), ((100, 20), (100, 20, 3)))\n  def test_convert_6_channels_to_rgb(self, input_shape, expected_output_shape):\n    channels = [np.random.random(input_shape) for _ in range(6)]\n    rgb = vis.convert_6_channels_to_rgb(channels)\n    self.assertEqual(rgb.shape, expected_output_shape)\n\n  @parameterized.parameters((None,), (\'RGB\',))\n  def test_draw_deepvariant_pileup_with_example_input(self, composite_type):\n    _, example = _mock_example_with_image((100, 10, 7))\n    # Testing that it runs without error\n    vis.draw_deepvariant_pileup(example=example, composite_type=composite_type)\n\n  @parameterized.parameters((None,), (\'RGB\',))\n  def test_draw_deepvariant_pileup_with_channels_input(self, composite_type):\n    channels = [_image_array((100, 221)) for _ in range(6)]\n    # Testing that it runs without error\n    vis.draw_deepvariant_pileup(\n        channels=channels, composite_type=composite_type)\n\n  @parameterized.parameters(\n      ([[0.0, 1], [5, 10]], 0, 10, [[0, 25], [127, 255]]),\n      ([[0.0, 0.1], [0.5, 1]], 0, 1, [[0, 25], [127, 255]]),\n      ([[0.0, 0.1], [0.5, 1]], 0, 0.5, [[0, 51], [255, 255]]),\n      ([[0.0, 0.1], [0.5, 1]], 0.5, 1, [[0, 0], [0, 255]]),\n      ([[0.0, 0.1], [0.5, 1]], -1, 1, [[127, 140], [191, 255]]),\n      ([[0.0, 0.1], [0.5, 1]], -1, 2, [[85, 93], [127, 170]]))\n  def test_scale_colors_for_png(self, arr, vmin, vmax, expected):\n    arr = np.array(arr)\n    scaled = vis.scale_colors_for_png(arr, vmin=vmin, vmax=vmax)\n    self.assertTrue((scaled == expected).all())\n\n  @parameterized.parameters(\n      ((100, 200), \'L\'),\n      ((100, 200, 3), \'RGB\'),\n  )\n  def test_autoscale_colors_for_png(self, shape, expected_image_mode):\n    arr = np.random.random(shape)\n    scaled, image_mode = vis.autoscale_colors_for_png(arr)\n    # Original array should be unchanged.\n    self.assertLess(np.max(arr), 1)\n    self.assertNotEqual(arr.dtype, np.uint8)\n    # Output values have been scaled up and the array\'s data type changed.\n    self.assertGreater(np.max(scaled), 1)\n    self.assertEqual(scaled.dtype, np.uint8)\n    self.assertEqual(image_mode, expected_image_mode)\n\n  @parameterized.parameters(\n      ((100, 200), \'L\'),\n      ((10, 1), \'L\'),\n      ((100, 200, 3), \'RGB\'),\n      ((10, 1, 3), \'RGB\'),\n      ((100, 200, 6), None),\n      ((100, 200, 3, 1), None),\n      ((100), None),\n  )\n  def test_get_image_type_from_array(self, shape, expected):\n    arr = _image_array(shape)\n    if expected is not None:\n      self.assertEqual(vis._get_image_type_from_array(arr), expected)\n    else:\n      self.assertRaisesWithPredicateMatch(\n          ValueError, lambda x: str(x).index(\'dimensions\') != -1,\n          vis.save_to_png, arr)\n\n  @parameterized.parameters(\n      ((100, 200, 3), True),\n      ((100, 200), True),\n      ((100, 200, 6), False),\n      ((100, 200, 3, 1), False),\n      ((100), False),\n  )\n  def test_save_to_png(self, shape, should_succeed):\n    arr = _image_array(shape)\n\n    if should_succeed:\n      temp_dir = self.create_tempdir().full_path\n      output_path = os.path.join(temp_dir, \'test.png\')\n      # check the file doesn\'t already exist before function runs\n      self.assertEmpty(glob.glob(output_path))\n      vis.save_to_png(arr, path=output_path)\n      self.assertLen(glob.glob(output_path), 1)\n    else:\n      self.assertRaisesWithPredicateMatch(\n          ValueError, lambda x: str(x).index(\'dimensions\') != -1,\n          vis.save_to_png, arr)\n\n  @parameterized.parameters(\n      ((100, 200, 3), True),\n      ((100, 200), True),\n      ((100, 200, 6), False),\n      ((100, 200, 3, 1), False),\n      ((100), False),\n  )\n  def test_array_to_png_works_with_floats(self, shape, should_succeed):\n    arr = np.random.random(shape)\n\n    if should_succeed:\n      temp_dir = self.create_tempdir().full_path\n      output_path = os.path.join(temp_dir, \'test.png\')\n      # Check the file doesn\'t already exist before function runs.\n      self.assertEmpty(glob.glob(output_path))\n      vis.array_to_png(arr, path=output_path)\n      self.assertLen(glob.glob(output_path), 1)\n    else:\n      self.assertRaisesWithPredicateMatch(\n          ValueError, lambda x: str(x).index(\'dimensions\') != -1,\n          vis.array_to_png, arr)\n\n  def test_variant_from_example(self):\n    example = _mock_example_with_variant_and_alt_allele_indices()\n    variant = vis.variant_from_example(example)\n    self.assertIsInstance(variant, variants_pb2.Variant)\n\n  @parameterized.parameters(\n      (b\'\\n\\x01\\x00\', [0]),\n      (b\'\\n\\x02\\x00\\x01\', [0, 1]),\n  )\n  def test_alt_allele_indices_from_example(self, encoded_indices, expected):\n    example = _mock_example_with_variant_and_alt_allele_indices(encoded_indices)\n    indices = vis.alt_allele_indices_from_example(example)\n    self.assertEqual(indices, expected)\n\n  @parameterized.parameters(\n      (\'chr1\', 100, \'G\', \'chr1:100_G\'),\n      (\'X\', 0, \'GACGT\', \'X:0_GACGT\'),\n  )\n  def test_locus_id_from_variant(self, chrom, pos, ref, expected):\n    variant = test_utils.make_variant(\n        chrom=chrom, alleles=[ref, \'A\'], start=pos)\n    locus_id = vis.locus_id_from_variant(variant)\n    self.assertEqual(locus_id, expected)\n\n  @parameterized.parameters(\n      (b\'\\n\\x01\\x00\', [\'A\', \'G\', \'GA\', \'AG\'], \'G\'),\n      (b\'\\n\\x02\\x00\\x01\', [\'C\', \'CA\', \'T\', \'TA\'], \'CA-T\'),\n      (b\'\\n\\x02\\x01\\x02\', [\'C\', \'CA\', \'T\', \'TA\'], \'T-TA\'),\n  )\n  def test_alt_from_example(self, encoded_indices, alleles, expected):\n    example = _mock_example_with_variant_and_alt_allele_indices(\n        encoded_indices=encoded_indices, alleles=alleles)\n    alt = vis.alt_from_example(example)\n    self.assertEqual(alt, expected)\n\n  @parameterized.parameters(\n      (b\'\\n\\x01\\x00\', [\'A\', \'G\', \'GA\', \'AG\'], \'X:10_A_G\'),\n      (b\'\\n\\x02\\x00\\x01\', [\'C\', \'CA\', \'T\', \'TA\'], \'X:10_C_CA-T\'),\n      (b\'\\n\\x02\\x01\\x02\', [\'C\', \'CA\', \'T\', \'TA\'], \'X:10_C_T-TA\'),\n  )\n  def test_locus_id_with_alt(self, encoded_indices, alleles, expected):\n    example = _mock_example_with_variant_and_alt_allele_indices(\n        encoded_indices=encoded_indices, alleles=alleles)\n    locus_id_with_alt = vis.locus_id_with_alt(example)\n    self.assertEqual(locus_id_with_alt, expected)\n\n  @parameterized.parameters(\n      ([0], [\'C\'], \'C\'),\n      ([0, 1], [\'C\', \'TT\'], \'C-TT\'),\n      ([3, 4], [\'C\', \'TT\', \'T\', \'G\', \'A\'], \'G-A\'),\n  )\n  def test_alt_bases_from_indices(self, indices, alternate_bases, expected):\n    alt = vis.alt_bases_from_indices(indices, alternate_bases)\n    self.assertEqual(alt, expected)\n\n  @parameterized.parameters([(0), (1), (2)])\n  def test_label_from_example(self, truth_label):\n    feature = {\'label\': _int_feature([truth_label])}\n    example = example_pb2.Example(\n        features=feature_pb2.Features(feature=feature))\n    output = vis.label_from_example(example)\n    self.assertEqual(truth_label, output)\n\n  @parameterized.parameters([(0), (1), (2), (8), (9), (20)])\n  def test_deepvariant_channel_names(self, num_channels):\n    output = vis._deepvariant_channel_names(num_channels)\n    self.assertLen(output, num_channels)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/vendor/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
nucleus/io/python/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
nucleus/io/python/bed_reader_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for bed_reader CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import clif_postproc\nfrom nucleus.io.python import bed_reader\nfrom nucleus.protos import bed_pb2\nfrom nucleus.testing import test_utils\n\n\nclass BedReaderTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.bed = test_utils.genomics_core_testdata(\'test_regions.bed\')\n    self.zipped_bed = test_utils.genomics_core_testdata(\'test_regions.bed.gz\')\n    self.options = bed_pb2.BedReaderOptions()\n    self.first = bed_pb2.BedRecord(\n        reference_name=\'chr1\',\n        start=10,\n        end=20,\n        name=\'first\',\n        score=100,\n        strand=bed_pb2.BedRecord.FORWARD_STRAND,\n        thick_start=12,\n        thick_end=18,\n        item_rgb=\'255,124,1\',\n        block_count=3,\n        block_sizes=\'2,6,2\',\n        block_starts=\'10,12,18\')\n\n  def test_bed_iterate(self):\n    with bed_reader.BedReader.from_file(self.bed, self.options) as reader:\n      self.assertEqual(reader.header.num_fields, 12)\n      iterable = reader.iterate()\n      self.assertIsInstance(iterable, clif_postproc.WrappedCppIterable)\n      actual = list(iterable)\n      self.assertLen(actual, 2)\n      self.assertEqual(actual[0], self.first)\n\n    zreader = bed_reader.BedReader.from_file(\n        self.zipped_bed,\n        bed_pb2.BedReaderOptions())\n    self.assertEqual(zreader.header.num_fields, 12)\n    with zreader:\n      ziterable = zreader.iterate()\n      self.assertIsInstance(ziterable, clif_postproc.WrappedCppIterable)\n      zactual = list(ziterable)\n      self.assertLen(zactual, 2)\n      self.assertEqual(zactual[0], self.first)\n\n  def test_from_file_raises_with_missing_bed(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Not found: Could not open missing.bed\'):\n      bed_reader.BedReader.from_file(\'missing.bed\', self.options)\n\n  def test_ops_on_closed_reader_raise(self):\n    reader = bed_reader.BedReader.from_file(self.bed, self.options)\n    with reader:\n      pass\n    # At this point the reader is closed.\n    with self.assertRaisesRegexp(ValueError, \'Cannot Iterate a closed\'):\n      reader.iterate()\n\n  @parameterized.parameters(\'malformed.bed\', \'malformed2.bed\')\n  def test_bed_iterate_raises_on_malformed_record(self, filename):\n    malformed = test_utils.genomics_core_testdata(filename)\n    reader = bed_reader.BedReader.from_file(malformed, self.options)\n    iterable = iter(reader.iterate())\n    self.assertIsNotNone(next(iterable))\n    with self.assertRaises(ValueError):\n      list(iterable)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/python/bed_writer_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for BedWriter CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import gfile\nfrom nucleus.io import tfrecord\nfrom nucleus.io.python import bed_writer\nfrom nucleus.protos import bed_pb2\nfrom nucleus.testing import test_utils\n\n\n_DOUBLE_CLOSE_ERROR = \'Cannot close an already closed BedWriter\'\n_WRITE_TO_CLOSED_ERROR = \'Cannot write to closed BED stream\'\n\n\nclass WrapBedWriterTest(parameterized.TestCase):\n\n  def setUp(self):\n    out_fname = test_utils.test_tmpfile(\'output.bed\')\n    self.writer = bed_writer.BedWriter.to_file(\n        out_fname, bed_pb2.BedHeader(num_fields=12), bed_pb2.BedWriterOptions())\n    self.expected_bed_content = [\n        \'chr1\\t10\\t20\\tfirst\\t100\\t+\\t12\\t18\\t255,124,1\\t3\\t2,6,2\\t10,12,18\\n\',\n        \'chr1\\t100\\t200\\tsecond\\t250\\t.\\t120\\t180\\t252,122,12\\t2\\t35,40\\t\'\n        \'100,160\\n\'\n    ]\n    self.record = bed_pb2.BedRecord(\n        reference_name=\'chr1\', start=20, end=30, name=\'r\')\n\n  def test_writing_canned_records(self):\n    """"""Tests writing all the records that are \'canned\' in our tfrecord file.""""""\n    # This file is in TFRecord format.\n    tfrecord_file = test_utils.genomics_core_testdata(\n        \'test_regions.bed.tfrecord\')\n\n    header = bed_pb2.BedHeader(num_fields=12)\n    writer_options = bed_pb2.BedWriterOptions()\n    bed_records = list(\n        tfrecord.read_tfrecords(tfrecord_file, proto=bed_pb2.BedRecord))\n    out_fname = test_utils.test_tmpfile(\'output.bed\')\n    with bed_writer.BedWriter.to_file(out_fname, header,\n                                      writer_options) as writer:\n      for record in bed_records:\n        writer.write(record)\n\n    with gfile.Open(out_fname, \'r\') as f:\n      self.assertEqual(f.readlines(), self.expected_bed_content)\n\n  def test_context_manager(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.record))\n\n    # self.writer should be closed, so writing again will fail.\n    with self.assertRaisesRegexp(ValueError, _WRITE_TO_CLOSED_ERROR):\n      self.writer.write(self.record)\n\n  def test_double_context_manager(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.record))\n\n    with self.assertRaisesRegexp(ValueError, _DOUBLE_CLOSE_ERROR):\n      # Entering the closed writer should be fine.\n      with self.writer:\n        pass  # We want to raise an error on exit, so nothing to do in context.\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/python/fastq_reader_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for fastq_reader CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import clif_postproc\nfrom nucleus.io.python import fastq_reader\nfrom nucleus.protos import fastq_pb2\nfrom nucleus.testing import test_utils\n\n\nclass FastqReaderTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.fastq = test_utils.genomics_core_testdata(\'test_reads.fastq\')\n    self.options = fastq_pb2.FastqReaderOptions()\n\n  @parameterized.parameters(\'test_reads.fastq\', \'test_reads.fastq.gz\',\n                            \'test_reads.bgzip.fastq.gz\')\n  def test_fastq_iterate(self, filename):\n    path = test_utils.genomics_core_testdata(filename)\n    with fastq_reader.FastqReader.from_file(path, self.options) as reader:\n      iterable = reader.iterate()\n      self.assertIsInstance(iterable, clif_postproc.WrappedCppIterable)\n      self.assertEqual(test_utils.iterable_len(iterable), 4)\n\n  def test_from_file_raises_with_missing_fastq(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Not found: Could not open missing.fastq\'):\n      fastq_reader.FastqReader.from_file(\'missing.fastq\', self.options)\n\n  def test_ops_on_closed_reader_raise(self):\n    reader = fastq_reader.FastqReader.from_file(self.fastq, self.options)\n    with reader:\n      pass\n    # At this point the reader is closed.\n    with self.assertRaisesRegexp(ValueError, \'Cannot Iterate a closed\'):\n      reader.iterate()\n\n  @parameterized.parameters(\'malformed.fastq\', \'malformed2.fastq\')\n  def test_fastq_iterate_raises_on_malformed_record(self, filename):\n    malformed = test_utils.genomics_core_testdata(filename)\n    reader = fastq_reader.FastqReader.from_file(malformed, self.options)\n    iterable = iter(reader.iterate())\n    self.assertIsNotNone(next(iterable))\n    with self.assertRaises(ValueError):\n      list(iterable)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/python/fastq_writer_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for FastqWriter CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import gfile\nfrom nucleus.io import fastq\nfrom nucleus.io import tfrecord\nfrom nucleus.io.python import fastq_writer\nfrom nucleus.protos import fastq_pb2\nfrom nucleus.testing import test_utils\n\n_DOUBLE_CLOSE_ERROR = \'Cannot close an already closed FastqWriter\'\n_WRITE_TO_CLOSED_ERROR = \'Cannot write to closed FASTQ stream\'\n\n\nclass WrapFastqWriterTest(parameterized.TestCase):\n\n  def setUp(self):\n    writer_options = fastq_pb2.FastqWriterOptions()\n    out_fname = test_utils.test_tmpfile(\'output.fastq\')\n    self.writer = fastq_writer.FastqWriter.to_file(out_fname, writer_options)\n    self.expected_fastq_content = [\n        \'@NODESC:header\\n\',\n        \'GATTACA\\n\',\n        \'+\\n\',\n        \'BB>B@FA\\n\',\n        \'@M01321:49:000000000-A6HWP:1:1101:17009:2216 1:N:0:1\\n\',\n        \'CGTTAGCGCAGGGGGCATCTTCACACTGGTGACAGGTAACCGCCGTAGTAAAGGTTCCGCCTTTCACT\\n\',\n        \'+\\n\',\n        \'AAAAABF@BBBDGGGG?FFGFGHBFBFBFABBBHGGGFHHCEFGGGGG?FGFFHEDG3EFGGGHEGHG\\n\',\n        \'@FASTQ contains multiple spaces in description\\n\',\n        \'CGGCTGGTCAGGCTGACATCGCCGCCGGCCTGCAGCGAGCCGCTGC\\n\',\n        \'+\\n\',\n        \'FAFAF;F/9;.:/;999B/9A.DFFF;-->.AAB/FC;9-@-=;=.\\n\',\n        \'@FASTQ_with_trailing_space\\n\',\n        \'CGG\\n\',\n        \'+\\n\',\n        \'FAD\\n\',\n    ]\n    self.record = fastq_pb2.FastqRecord(\n        id=\'ID\', description=\'desc\', sequence=\'ACGTAC\', quality=\'ABCDEF\')\n\n  def test_writing_canned_records(self):\n    """"""Tests writing all the variants that are \'canned\' in our tfrecord file.""""""\n    # This file is in TFRecord format.\n    tfrecord_file = test_utils.genomics_core_testdata(\n        \'test_reads.fastq.tfrecord\')\n\n    writer_options = fastq_pb2.FastqWriterOptions()\n    fastq_records = list(\n        tfrecord.read_tfrecords(tfrecord_file, proto=fastq_pb2.FastqRecord))\n    out_fname = test_utils.test_tmpfile(\'output.fastq\')\n    with fastq_writer.FastqWriter.to_file(out_fname, writer_options) as writer:\n      for record in fastq_records:\n        writer.write(record)\n\n    with gfile.Open(out_fname, \'r\') as f:\n      self.assertEqual(f.readlines(), self.expected_fastq_content)\n\n  def test_context_manager(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.record))\n\n    # self.writer should be closed, so writing again will fail.\n    with self.assertRaisesRegexp(ValueError, _WRITE_TO_CLOSED_ERROR):\n      self.writer.write(self.record)\n\n  def test_double_context_manager(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.record))\n\n    with self.assertRaisesRegexp(ValueError, _DOUBLE_CLOSE_ERROR):\n      # Entering the closed writer should be fine.\n      with self.writer:\n        pass  # We want to raise an error on exit, so nothing to do in context.\n\n\nclass WrapFastqWriterRoundTripTests(parameterized.TestCase):\n\n  @parameterized.parameters(\'test_reads.fastq\', \'test_reads.fastq.gz\')\n  def test_round_trip_fastq(self, test_datum_name):\n    # Round-trip FASTQ records through writing and reading:\n    # 1. Read records v1 from FastqReader;\n    # 2. Write v1 to fastq using our FastqWriter;\n    # 3. Read back in using FastqReader -- v2;\n    # 4. compare v1 and v2.\n    in_file = test_utils.genomics_core_testdata(test_datum_name)\n    out_file = test_utils.test_tmpfile(\'output_\' + test_datum_name)\n\n    v1_reader = fastq.FastqReader(in_file)\n    v1_records = list(v1_reader.iterate())\n    self.assertTrue(v1_records, \'Reader failed to find records\')\n\n    writer_options = fastq_pb2.FastqWriterOptions()\n\n    with fastq_writer.FastqWriter.to_file(out_file, writer_options) as writer:\n      for record in v1_records:\n        writer.write(record)\n\n    v2_reader = fastq.FastqReader(out_file)\n    v2_records = list(v2_reader.iterate())\n    self.assertEqual(v1_records, v2_records,\n                     \'Round-tripped FASTQ files not as expected\')\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/python/gff_reader_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for gff_reader CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import clif_postproc\nfrom nucleus.io.python import gff_reader\nfrom nucleus.protos import gff_pb2\nfrom nucleus.testing import test_utils\n\n\nclass GffReaderTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.options = gff_pb2.GffReaderOptions()\n    self.first = gff_pb2.GffRecord()\n    self.first.range.reference_name = \'ctg123\'\n    self.first.range.start = 999\n    self.first.range.end = 9000\n    self.first.source = \'GenBank\'\n    self.first.type = \'gene\'\n    self.first.score = 2.5\n    self.first.strand = gff_pb2.GffRecord.FORWARD_STRAND\n    self.first.phase = 0\n    self.first.attributes[\'ID\'] = \'gene00001\'\n    self.first.attributes[\'Name\'] = \'EDEN\'\n\n    self.second = gff_pb2.GffRecord()\n    self.second.range.reference_name = \'ctg123\'\n    self.second.range.start = 999\n    self.second.range.end = 1012\n    self.second.phase = -1\n    self.second.score = -float(\'inf\')\n\n  @parameterized.parameters(\'test_features.gff\', \'test_features.gff.gz\')\n  def test_gff_iterate(self, test_features_gff_filename):\n    file_path = test_utils.genomics_core_testdata(test_features_gff_filename)\n    with gff_reader.GffReader.from_file(file_path, self.options) as reader:\n      iterable = reader.iterate()\n      self.assertIsInstance(iterable, clif_postproc.WrappedCppIterable)\n      actual = list(iterable)\n      self.assertLen(actual, 2)\n      self.assertEqual(actual[0], self.first)\n      self.assertEqual(actual[1], self.second)\n\n  def test_from_file_raises_with_missing_gff(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Not found: Could not open missing.gff\'):\n      gff_reader.GffReader.from_file(\'missing.gff\', self.options)\n\n  def test_ops_on_closed_reader_raise(self):\n    file_path = test_utils.genomics_core_testdata(\'test_features.gff\')\n    reader = gff_reader.GffReader.from_file(file_path, self.options)\n    with reader:\n      pass\n    # At this point the reader is closed.\n    with self.assertRaisesRegexp(ValueError, \'Cannot Iterate a closed\'):\n      reader.iterate()\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/python/gff_writer_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for GffWriter CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import tfrecord\nfrom nucleus.io.python import gff_writer\nfrom nucleus.protos import gff_pb2\nfrom nucleus.testing import test_utils\nfrom nucleus.util import ranges\n\n_DOUBLE_CLOSE_ERROR = \'Cannot close an already closed GffWriter\'\n_WRITE_TO_CLOSED_ERROR = \'Cannot write to closed GFF stream\'\n\n\nclass WrapGffWriterTest(parameterized.TestCase):\n\n  def setUp(self):\n    out_fname = test_utils.test_tmpfile(\'output.gff\')\n    self.writer = gff_writer.GffWriter.to_file(out_fname, gff_pb2.GffHeader(),\n                                               gff_pb2.GffWriterOptions())\n    self.expected_gff_content = open(\n        test_utils.genomics_core_testdata(\'test_features.gff\')).readlines()\n    self.header = gff_pb2.GffHeader(\n        sequence_regions=[ranges.make_range(\'ctg123\', 0, 1497228)])\n    self.record = gff_pb2.GffRecord(\n        range=ranges.make_range(\'ctg123\', 1000, 1100))\n\n  def test_writing_canned_records(self):\n    """"""Tests writing all the records that are \'canned\' in our tfrecord file.""""""\n    # This file is in TFRecord format.\n    tfrecord_file = test_utils.genomics_core_testdata(\n        \'test_features.gff.tfrecord\')\n    writer_options = gff_pb2.GffWriterOptions()\n    gff_records = list(\n        tfrecord.read_tfrecords(tfrecord_file, proto=gff_pb2.GffRecord))\n    out_fname = test_utils.test_tmpfile(\'output.gff\')\n    with gff_writer.GffWriter.to_file(out_fname, self.header,\n                                      writer_options) as writer:\n      for record in gff_records:\n        writer.write(record)\n\n    with open(out_fname) as f:\n      self.assertEqual(f.readlines(), self.expected_gff_content)\n\n  def test_context_manager(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.record))\n\n    # self.writer should be closed, so writing again will fail.\n    with self.assertRaisesRegexp(ValueError, _WRITE_TO_CLOSED_ERROR):\n      self.writer.write(self.record)\n\n  def test_double_close(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.record))\n\n    with self.assertRaisesRegexp(ValueError, _DOUBLE_CLOSE_ERROR):\n      # Entering the closed writer should be fine.\n      with self.writer:\n        pass  # We want to raise an error on exit, so nothing to do in context.\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/python/hts_verbose_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for hts_verbose.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\n\nfrom nucleus.io.python import hts_verbose\n\n\nclass HtsVerbose(absltest.TestCase):\n\n  def test_set(self):\n    hts_verbose.set(hts_verbose.htsLogLevel.HTS_LOG_TRACE)\n    level = hts_verbose.get()\n    self.assertEqual(level, hts_verbose.htsLogLevel.HTS_LOG_TRACE)\n\n    hts_verbose.set(hts_verbose.htsLogLevel.HTS_LOG_INFO)\n    level = hts_verbose.get()\n    self.assertEqual(level, hts_verbose.htsLogLevel.HTS_LOG_INFO)\n\n    hts_verbose.set(hts_verbose.htsLogLevel[\'HTS_LOG_DEBUG\'])\n    level = hts_verbose.get()\n    self.assertEqual(level, hts_verbose.htsLogLevel.HTS_LOG_DEBUG)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/python/reference_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for GenomeReference CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io.python import reference\nfrom nucleus.protos import fasta_pb2\nfrom nucleus.testing import test_utils\nfrom nucleus.util import ranges\n\n\nclass WrapReferenceTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      (\'test.fasta\', False, \'TAACC\'),\n      (\'test.fasta\', True, \'TaaCC\'),\n      (\'test.fasta.gz\', False, \'TAACC\'),\n      (\'test.fasta.gz\', True, \'TaaCC\'))\n  def test_wrap(self, fasta_filename, keep_true_case, expected_bases):\n    chr_names = [\'chrM\', \'chr1\', \'chr2\']\n    chr_lengths = [100, 76, 121]\n    fasta = test_utils.genomics_core_testdata(fasta_filename)\n    fai = test_utils.genomics_core_testdata(fasta_filename + \'.fai\')\n    options = fasta_pb2.FastaReaderOptions(keep_true_case=keep_true_case)\n    with reference.IndexedFastaReader.from_file(fasta, fai, options) as ref:\n      self.assertEqual(ref.contig_names, chr_names)\n      self.assertEqual(ref.bases(ranges.make_range(\'chrM\', 22, 27)),\n                       expected_bases)\n\n      self.assertTrue(ref.is_valid_interval(ranges.make_range(\'chrM\', 1, 10)))\n      self.assertFalse(\n          ref.is_valid_interval(ranges.make_range(\'chrM\', 1, 100000)))\n\n      self.assertLen(ref.contigs, 3)\n      self.assertEqual([c.name for c in ref.contigs], chr_names)\n      self.assertEqual([c.n_bases for c in ref.contigs], chr_lengths)\n      for contig in ref.contigs:\n        self.assertEqual(ref.contig(contig.name), contig)\n        self.assertTrue(ref.has_contig(contig.name))\n        self.assertFalse(ref.has_contig(contig.name + \'.unknown\'))\n\n  @parameterized.parameters(\n      # The fasta and the FAI are both missing.\n      (\'missing.fasta\', \'missing.fasta.fai\'),\n      # The fasta is present but the FAI is missing.\n      (\'test.fasta\', \'missing.fasta.fai\'),\n      # The fasta is missing but the FAI is present.\n      (\'missing.fasta\', \'test.fasta.fai\'),\n  )\n  def test_from_file_raises_with_missing_inputs(self, fasta_filename,\n                                                fai_filename):\n    fasta = test_utils.genomics_core_testdata(fasta_filename)\n    fai = test_utils.genomics_core_testdata(fai_filename)\n    with self.assertRaisesRegexp(\n        ValueError,\n        \'Not found: could not load fasta and/or fai for fasta \' + fasta):\n      reference.IndexedFastaReader.from_file(fasta, fai,\n                                             fasta_pb2.FastaReaderOptions())\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/python/sam_reader_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for sam_reader CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import clif_postproc\nfrom nucleus.io.python import sam_reader\nfrom nucleus.protos import reads_pb2\nfrom nucleus.protos import reference_pb2\nfrom nucleus.testing import test_utils\nfrom nucleus.util import ranges\n\n\nclass SamReaderTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.bam = test_utils.genomics_core_testdata(\'test.bam\')\n    self.options = reads_pb2.SamReaderOptions()\n\n  def test_bam_iterate(self):\n    reader = sam_reader.SamReader.from_file(\n        reads_path=self.bam, ref_path=\'\', options=self.options)\n    with reader:\n      iterable = reader.iterate()\n      self.assertIsInstance(iterable, clif_postproc.WrappedCppIterable)\n      self.assertEqual(test_utils.iterable_len(iterable), 106)\n\n  def test_bam_query(self):\n    reader = sam_reader.SamReader.from_file(\n        reads_path=self.bam, ref_path=\'\', options=self.options)\n    expected = [(ranges.parse_literal(\'chr20:10,000,000-10,000,100\'), 106),\n                (ranges.parse_literal(\'chr20:10,000,000-10,000,000\'), 45)]\n    with reader:\n      for interval, n_expected in expected:\n        with reader.query(interval) as iterable:\n          self.assertIsInstance(iterable, clif_postproc.WrappedCppIterable)\n          self.assertEqual(test_utils.iterable_len(iterable), n_expected)\n\n  def test_bam_samples(self):\n    reader = sam_reader.SamReader.from_file(\n        reads_path=self.bam, ref_path=\'\', options=self.options)\n    with reader:\n      self.assertLen(reader.header.read_groups, 1)\n      self.assertEqual(reader.header.read_groups[0].sample_id, \'NA12878\')\n\n  def test_sam_contigs(self):\n    reader = sam_reader.SamReader.from_file(\n        reads_path=self.bam, ref_path=\'\', options=self.options)\n    with reader:\n      self.assertEqual([\n          reference_pb2.ContigInfo(name=\'chrM\', pos_in_fasta=0, n_bases=16571),\n          reference_pb2.ContigInfo(\n              name=\'chr1\', pos_in_fasta=1, n_bases=249250621),\n          reference_pb2.ContigInfo(\n              name=\'chr2\', pos_in_fasta=2, n_bases=243199373),\n          reference_pb2.ContigInfo(\n              name=\'chr3\', pos_in_fasta=3, n_bases=198022430),\n          reference_pb2.ContigInfo(\n              name=\'chr4\', pos_in_fasta=4, n_bases=191154276),\n          reference_pb2.ContigInfo(\n              name=\'chr5\', pos_in_fasta=5, n_bases=180915260),\n          reference_pb2.ContigInfo(\n              name=\'chr6\', pos_in_fasta=6, n_bases=171115067),\n          reference_pb2.ContigInfo(\n              name=\'chr7\', pos_in_fasta=7, n_bases=159138663),\n          reference_pb2.ContigInfo(\n              name=\'chr8\', pos_in_fasta=8, n_bases=146364022),\n          reference_pb2.ContigInfo(\n              name=\'chr9\', pos_in_fasta=9, n_bases=141213431),\n          reference_pb2.ContigInfo(\n              name=\'chr10\', pos_in_fasta=10, n_bases=135534747),\n          reference_pb2.ContigInfo(\n              name=\'chr11\', pos_in_fasta=11, n_bases=135006516),\n          reference_pb2.ContigInfo(\n              name=\'chr12\', pos_in_fasta=12, n_bases=133851895),\n          reference_pb2.ContigInfo(\n              name=\'chr13\', pos_in_fasta=13, n_bases=115169878),\n          reference_pb2.ContigInfo(\n              name=\'chr14\', pos_in_fasta=14, n_bases=107349540),\n          reference_pb2.ContigInfo(\n              name=\'chr15\', pos_in_fasta=15, n_bases=102531392),\n          reference_pb2.ContigInfo(\n              name=\'chr16\', pos_in_fasta=16, n_bases=90354753),\n          reference_pb2.ContigInfo(\n              name=\'chr17\', pos_in_fasta=17, n_bases=81195210),\n          reference_pb2.ContigInfo(\n              name=\'chr18\', pos_in_fasta=18, n_bases=78077248),\n          reference_pb2.ContigInfo(\n              name=\'chr19\', pos_in_fasta=19, n_bases=59128983),\n          reference_pb2.ContigInfo(\n              name=\'chr20\', pos_in_fasta=20, n_bases=63025520),\n          reference_pb2.ContigInfo(\n              name=\'chr21\', pos_in_fasta=21, n_bases=48129895),\n          reference_pb2.ContigInfo(\n              name=\'chr22\', pos_in_fasta=22, n_bases=51304566),\n          reference_pb2.ContigInfo(\n              name=\'chrX\', pos_in_fasta=23, n_bases=155270560),\n          reference_pb2.ContigInfo(\n              name=\'chrY\', pos_in_fasta=24, n_bases=59373566),\n      ], list(reader.header.contigs))\n\n  def test_context_manager(self):\n    """"""Test that we can use context manager to do two queries in sequence.""""""\n    reader = sam_reader.SamReader.from_file(\n        reads_path=self.bam, ref_path=\'\', options=self.options)\n    region = ranges.parse_literal(\'chr20:10,000,000-10,000,100\')\n    with reader:\n      with reader.query(region) as query_iterable1:\n        self.assertIsNotNone(query_iterable1)\n        self.assertIsInstance(query_iterable1, clif_postproc.WrappedCppIterable)\n      with reader.query(region) as query_iterable2:\n        self.assertIsNotNone(query_iterable2)\n        self.assertIsInstance(query_iterable2, clif_postproc.WrappedCppIterable)\n\n  def test_from_file_raises_with_missing_bam(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Not found: Could not open missing.bam\'):\n      sam_reader.SamReader.from_file(\n          reads_path=\'missing.bam\', ref_path=\'\', options=self.options)\n\n  def test_ops_on_closed_reader_raise(self):\n    reader = sam_reader.SamReader.from_file(\n        reads_path=self.bam, ref_path=\'\', options=self.options)\n    with reader:\n      pass\n    # At this point the reader is closed.\n    with self.assertRaisesRegexp(ValueError, \'Cannot Iterate a closed\'):\n      reader.iterate()\n    with self.assertRaisesRegexp(ValueError, \'Cannot Query a closed\'):\n      reader.query(ranges.parse_literal(\'chr20:10,000,000-10,000,100\'))\n\n  @parameterized.parameters(\'test.sam\', \'unindexed.bam\')\n  def test_query_without_index_raises(self, unindexed_file_name):\n    path = test_utils.genomics_core_testdata(unindexed_file_name)\n    window = ranges.parse_literal(\'chr20:10,000,000-10,000,100\')\n    with sam_reader.SamReader.from_file(\n        reads_path=path, ref_path=\'\', options=self.options) as reader:\n      with self.assertRaisesRegexp(ValueError, \'Cannot query without an index\'):\n        reader.query(window)\n\n  def test_query_raises_with_bad_range(self):\n    with sam_reader.SamReader.from_file(\n        reads_path=self.bam, ref_path=\'\', options=self.options) as reader:\n      with self.assertRaisesRegexp(ValueError, \'Unknown reference_name\'):\n        reader.query(ranges.parse_literal(\'XXX:1-10\'))\n      with self.assertRaisesRegexp(ValueError, \'unknown reference interval\'):\n        reader.query(ranges.parse_literal(\'chr20:10-5\'))\n\n  def test_sam_iterate_raises_on_malformed_record(self):\n    malformed = test_utils.genomics_core_testdata(\'malformed.sam\')\n    reader = sam_reader.SamReader.from_file(\n        reads_path=malformed, ref_path=\'\', options=self.options)\n    iterable = iter(reader.iterate())\n    self.assertIsNotNone(next(iterable))\n    with self.assertRaises(ValueError):\n      list(iterable)\n\n  def test_headless_sam_raises(self):\n    headerless = test_utils.genomics_core_testdata(\'headerless.sam\')\n    reader = sam_reader.SamReader.from_file(\n        reads_path=headerless, ref_path=\'\', options=self.options)\n    iterable = iter(reader.iterate())\n    with self.assertRaises(ValueError):\n      next(iterable)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/python/vcf_reader_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for vcf_reader CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\n\nfrom nucleus.io.python import vcf_reader\nfrom nucleus.protos import reference_pb2\nfrom nucleus.protos import variants_pb2\nfrom nucleus.testing import test_utils\nfrom nucleus.util import ranges\n\nexpected_sites_contigs = [\n    reference_pb2.ContigInfo(name=\'chr1\', pos_in_fasta=0, n_bases=248956422),\n    reference_pb2.ContigInfo(name=\'chr2\', pos_in_fasta=1, n_bases=242193529),\n    reference_pb2.ContigInfo(name=\'chr3\', pos_in_fasta=2, n_bases=198295559),\n    reference_pb2.ContigInfo(name=\'chr4\', pos_in_fasta=3, n_bases=190214555),\n    reference_pb2.ContigInfo(name=\'chr5\', pos_in_fasta=4, n_bases=181538259),\n    reference_pb2.ContigInfo(name=\'chr6\', pos_in_fasta=5, n_bases=170805979),\n    reference_pb2.ContigInfo(name=\'chr7\', pos_in_fasta=6, n_bases=159345973),\n    reference_pb2.ContigInfo(name=\'chr8\', pos_in_fasta=7, n_bases=145138636),\n    reference_pb2.ContigInfo(name=\'chr9\', pos_in_fasta=8, n_bases=138394717),\n    reference_pb2.ContigInfo(name=\'chr10\', pos_in_fasta=9, n_bases=133797422),\n    reference_pb2.ContigInfo(name=\'chr11\', pos_in_fasta=10, n_bases=135086622),\n    reference_pb2.ContigInfo(name=\'chr12\', pos_in_fasta=11, n_bases=133275309),\n    reference_pb2.ContigInfo(name=\'chr13\', pos_in_fasta=12, n_bases=114364328),\n    reference_pb2.ContigInfo(name=\'chr14\', pos_in_fasta=13, n_bases=107043718),\n    reference_pb2.ContigInfo(name=\'chr15\', pos_in_fasta=14, n_bases=101991189),\n    reference_pb2.ContigInfo(name=\'chr16\', pos_in_fasta=15, n_bases=90338345),\n    reference_pb2.ContigInfo(name=\'chr17\', pos_in_fasta=16, n_bases=83257441),\n    reference_pb2.ContigInfo(name=\'chr18\', pos_in_fasta=17, n_bases=80373285),\n    reference_pb2.ContigInfo(name=\'chr19\', pos_in_fasta=18, n_bases=58617616),\n    reference_pb2.ContigInfo(name=\'chr20\', pos_in_fasta=19, n_bases=64444167),\n    reference_pb2.ContigInfo(name=\'chr21\', pos_in_fasta=20, n_bases=46709983),\n    reference_pb2.ContigInfo(name=\'chr22\', pos_in_fasta=21, n_bases=50818468),\n    reference_pb2.ContigInfo(name=\'chrX\', pos_in_fasta=22, n_bases=156040895),\n    reference_pb2.ContigInfo(name=\'chrY\', pos_in_fasta=23, n_bases=57227415),\n    reference_pb2.ContigInfo(name=\'chrM\', pos_in_fasta=24, n_bases=16569),\n]\n\n# pylint: disable=line-too-long\nexpected_samples_filters = [\n    variants_pb2.VcfFilterInfo(id=\'PASS\', description=\'All filters passed\'),\n    variants_pb2.VcfFilterInfo(id=\'LowQual\', description=\'Low\tquality\'),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL95.00to96.00\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod:\t0.9364\t<=\tx\t<\t1.0415\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL96.00to97.00\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod:\t0.8135\t<=\tx\t<\t0.9364\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL97.00to99.00\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod:\t0.323\t<=\tx\t<\t0.8135\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL99.00to99.50\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod:\t-0.1071\t<=\tx\t<\t0.323\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL99.50to99.90\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod:\t-1.845\t<=\tx\t<\t-0.1071\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL99.90to99.95\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod:\t-3.2441\t<=\tx\t<\t-1.845\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL99.95to100.00+\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod\t<\t-57172.0693\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheINDEL99.95to100.00\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tINDEL\tmodel\tat\tVQS\tLod:\t-57172.0693\t<=\tx\t<\t-3.2441\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheSNP99.50to99.60\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tSNP\tmodel\tat\tVQS\tLod:\t-0.751\t<=\tx\t<\t-0.6681\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheSNP99.60to99.80\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tSNP\tmodel\tat\tVQS\tLod:\t-1.0839\t<=\tx\t<\t-0.751\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheSNP99.80to99.90\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tSNP\tmodel\tat\tVQS\tLod:\t-1.7082\t<=\tx\t<\t-1.0839\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheSNP99.90to99.95\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tSNP\tmodel\tat\tVQS\tLod:\t-3.0342\t<=\tx\t<\t-1.7082\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheSNP99.95to100.00+\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tSNP\tmodel\tat\tVQS\tLod\t<\t-40235.9641\'\n    ),\n    variants_pb2.VcfFilterInfo(\n        id=\'VQSRTrancheSNP99.95to100.00\',\n        description=\n        \'Truth\tsensitivity\ttranche\tlevel\tfor\tSNP\tmodel\tat\tVQS\tLod:\t-40235.9641\t<=\tx\t<\t-3.0342\'\n    )\n]\n\n# pylint: enable=line-too-long\n\n\nclass WrapVcfReaderTests(absltest.TestCase):\n\n  def setUp(self):\n    self.sites_vcf = test_utils.genomics_core_testdata(\'test_sites.vcf\')\n    self.samples_vcf = test_utils.genomics_core_testdata(\'test_samples.vcf.gz\')\n    self.options = variants_pb2.VcfReaderOptions()\n    self.sites_reader = vcf_reader.VcfReader.from_file(self.sites_vcf,\n                                                       self.options)\n    self.samples_reader = vcf_reader.VcfReader.from_file(\n        self.samples_vcf, self.options)\n\n  def test_vcf_iterate(self):\n    iterable = self.sites_reader.iterate()\n    self.assertEqual(test_utils.iterable_len(iterable), 5)\n\n  def test_vcf_header(self):\n    header = self.sites_reader.header\n    expected1 = variants_pb2.VcfStructuredExtra(\n        key=\'ALT\',\n        fields=[\n            variants_pb2.VcfExtra(key=\'ID\', value=\'NON_REF\'),\n            variants_pb2.VcfExtra(\n                key=\'Description\',\n                value=\'Represents\tany\tpossible\talternative\tallele\tat\tth\'\n                \'is\tlocation\')\n        ])\n    expected2 = variants_pb2.VcfStructuredExtra(\n        key=\'META\',\n        fields=[\n            variants_pb2.VcfExtra(key=\'ID\', value=\'TESTMETA\'),\n            variants_pb2.VcfExtra(key=\'Description\', value=\'blah\')\n        ])\n    self.assertLen(header.structured_extras, 2)\n    self.assertEqual(header.structured_extras[1], expected2)\n    self.assertEqual(header.structured_extras[0], expected1)\n\n  def test_vcf_contigs(self):\n    self.assertEqual(expected_sites_contigs,\n                     list(self.sites_reader.header.contigs))\n\n  def test_vcf_filters(self):\n    self.assertEqual(expected_samples_filters,\n                     list(self.samples_reader.header.filters))\n\n  def test_vcf_samples(self):\n    self.assertEqual(list(self.sites_reader.header.sample_names), [])\n    self.assertEqual(\n        list(self.samples_reader.header.sample_names), [\'NA12878_18_99\'])\n\n  def test_vcf_query(self):\n    range1 = ranges.parse_literal(\'chr3:100,000-500,000\')\n    iterable = self.samples_reader.query(range1)\n    self.assertEqual(test_utils.iterable_len(iterable), 4)\n\n  def test_vcf_from_string(self):\n    v = self.samples_reader.from_string(\n        \'chr3\\t370537\\trs142286746\\tC\\tCA,CAA\\t350.73\\tPASS\\t\'\n        \'AC=1,1;AF=0.500,0.500;AN=2;DB;DP=16;ExcessHet=3.0103;\'\n        \'FS=0.000;MLEAC=1,1;MLEAF=0.500,0.500;MQ=60.00;QD=26.98;\'\n        \'SOR=1.179;VQSLOD=2.88;culprit=FS\\tGT:AD:DP:GQ:PL\\t\'\n        \'1/2:0,6,7:13:99:388,188,149,140,0,116\')\n    self.assertEqual(v.reference_name, \'chr3\')\n    self.assertEqual(v.start, 370536)\n    self.assertEqual(list(v.names), [\'rs142286746\'])\n    self.assertEqual(v.reference_bases, \'C\')\n    self.assertEqual(list(v.alternate_bases), [\'CA\', \'CAA\'])\n    self.assertEqual(len(v.calls), 1)\n\n  def test_vcf_from_string_raises_on_bad_input(self):\n    with self.assertRaises(ValueError):\n      self.samples_reader.from_string(\'BAD NOT A VCF RECORD\\n;;\')\n\n  def test_from_file_raises_with_missing_source(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Not found: Could not open missing.vcf\'):\n      vcf_reader.VcfReader.from_file(\'missing.vcf\', self.options)\n\n  def test_ops_on_closed_reader_raise(self):\n    with self.samples_reader:\n      pass\n    # At this point the reader is closed.\n    with self.assertRaisesRegexp(ValueError, \'Cannot Iterate a closed\'):\n      self.samples_reader.iterate()\n    with self.assertRaisesRegexp(ValueError, \'Cannot Query a closed\'):\n      self.samples_reader.query(\n          ranges.parse_literal(\'chr1:10,000,000-10,000,100\'))\n\n  def test_query_on_unindexed_reader_raises(self):\n    window = ranges.parse_literal(\'chr1:10,000,000-10,000,100\')\n    unindexed_file = test_utils.genomics_core_testdata(\'test_samples.vcf\')\n    with vcf_reader.VcfReader.from_file(unindexed_file, self.options) as reader:\n      with self.assertRaisesRegexp(ValueError, \'Cannot query without an index\'):\n        reader.query(window)\n\n  def test_query_raises_with_bad_range(self):\n    with self.assertRaisesRegexp(ValueError, \'Unknown reference_name\'):\n      self.samples_reader.query(ranges.parse_literal(\'XXX:1-10\'))\n    with self.assertRaisesRegexp(ValueError, \'Malformed region\'):\n      self.samples_reader.query(ranges.parse_literal(\'chr1:0-5\'))\n    with self.assertRaisesRegexp(ValueError, \'Malformed region\'):\n      self.samples_reader.query(ranges.parse_literal(\'chr1:6-5\'))\n    with self.assertRaisesRegexp(ValueError, \'Malformed region\'):\n      self.samples_reader.query(ranges.parse_literal(\'chr1:10-5\'))\n\n  def test_context_manager(self):\n    with vcf_reader.VcfReader.from_file(self.sites_vcf, self.options) as f:\n      self.assertEqual(expected_sites_contigs, list(f.header.contigs))\n\n  # Commented out because we in fact don\'t detect the malformed VCF yet. It is\n  # unclear if it\'s even possible to detect the issue with the API provided by\n  # htslib.\n  # def test_vcf_iterate_raises_on_malformed_record(self):\n  #   malformed = test_utils.genomics_core_testdata(\'malformed.vcf\')\n  #   reader = vcf_reader.VcfReader.from_file(malformed, self.unindexed_options)\n  #   iterable = iter(reader.iterate())\n  #   self.assertIsNotNone(next(iterable))\n  #   with self.assertRaises(ValueError):\n  #     print(list(iterable))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/io/python/vcf_writer_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for VcfWriter CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\nfrom nucleus.io import gfile\nfrom nucleus.io import tfrecord\nfrom nucleus.io import vcf\nfrom nucleus.io.python import vcf_writer\nfrom nucleus.protos import reference_pb2\nfrom nucleus.protos import variants_pb2\nfrom nucleus.testing import test_utils\n\n_DOUBLE_CLOSE_ERROR = \'Cannot close an already closed VcfWriter\'\n_WRITE_TO_CLOSED_ERROR = \'Cannot write to closed VCF stream\'\n_WRONG_NUMBER_OF_SAMPLES = (\n    \'Variant call count \\d+ must match number of samples \\d+\')\n_DISCORDANT_SAMPLE_NAMES_ERROR = (\n    \'Out-of-order call set names, or unrecognized call set name, with respect \'\n    \'to samples declared in VCF header.\')\n_UNKNOWN_CONTIG_ERROR = ""Record\'s reference name is not available in VCF header""\n_FILTER_NOT_FOUND_ERROR = \'Filter must be found in header\'\n\n\nclass WrapVcfWriterTest(parameterized.TestCase):\n\n  def setUp(self):\n    self.out_fname = test_utils.test_tmpfile(\'output.vcf\')\n    self.header = variants_pb2.VcfHeader(\n        contigs=[\n            reference_pb2.ContigInfo(name=\'Chr1\', n_bases=50, pos_in_fasta=0),\n            reference_pb2.ContigInfo(name=\'Chr2\', n_bases=25, pos_in_fasta=1),\n        ],\n        sample_names=[\'Fido\', \'Spot\'],\n        formats=[\n            variants_pb2.VcfFormatInfo(\n                id=\'GT\', number=\'1\', type=\'String\', description=\'Genotype\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'GQ\',\n                number=\'1\',\n                type=\'Float\',\n                description=\'Genotype Quality\')\n        ],\n    )\n    self.options = variants_pb2.VcfWriterOptions()\n    self.writer = vcf_writer.VcfWriter.to_file(self.out_fname, self.header,\n                                               self.options)\n    self.variant = test_utils.make_variant(\n        chrom=\'Chr1\',\n        start=10,\n        alleles=[\'A\', \'C\'],\n    )\n    self.variant.calls.extend([\n        variants_pb2.VariantCall(genotype=[0, 0], call_set_name=\'Fido\'),\n        variants_pb2.VariantCall(genotype=[0, 1], call_set_name=\'Spot\'),\n    ])\n\n  def test_writing_canned_variants(self):\n    """"""Tests writing all the variants that are \'canned\' in our tfrecord file.""""""\n    # This file is in the TF record format\n    tfrecord_file = test_utils.genomics_core_testdata(\n        \'test_samples.vcf.golden.tfrecord\')\n\n    writer_options = variants_pb2.VcfWriterOptions()\n    header = variants_pb2.VcfHeader(\n        contigs=[\n            reference_pb2.ContigInfo(name=\'chr1\', n_bases=248956422),\n            reference_pb2.ContigInfo(name=\'chr2\', n_bases=242193529),\n            reference_pb2.ContigInfo(name=\'chr3\', n_bases=198295559),\n            reference_pb2.ContigInfo(name=\'chrX\', n_bases=156040895)\n        ],\n        sample_names=[\'NA12878_18_99\'],\n        filters=[\n            variants_pb2.VcfFilterInfo(\n                id=\'PASS\', description=\'All filters passed\'),\n            variants_pb2.VcfFilterInfo(id=\'LowQual\', description=\'\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL95.00to96.00\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL96.00to97.00\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL97.00to99.00\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL99.00to99.50\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL99.50to99.90\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL99.90to99.95\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL99.95to100.00+\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheINDEL99.95to100.00\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheSNP99.50to99.60\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheSNP99.60to99.80\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheSNP99.80to99.90\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheSNP99.90to99.95\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheSNP99.95to100.00+\'),\n            variants_pb2.VcfFilterInfo(id=\'VQSRTrancheSNP99.95to100.00\'),\n        ],\n        infos=[\n            variants_pb2.VcfInfo(\n                id=\'END\',\n                number=\'1\',\n                type=\'Integer\',\n                description=\'Stop position of the interval\')\n        ],\n        formats=[\n            variants_pb2.VcfFormatInfo(\n                id=\'GT\', number=\'1\', type=\'String\', description=\'Genotype\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'GQ\',\n                number=\'1\',\n                type=\'Integer\',\n                description=\'Genotype Quality\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'DP\',\n                number=\'1\',\n                type=\'Integer\',\n                description=\'Read depth of all passing filters reads.\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'MIN_DP\',\n                number=\'1\',\n                type=\'Integer\',\n                description=\'Minimum DP observed within the GVCF block.\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'AD\',\n                number=\'R\',\n                type=\'Integer\',\n                description=\n                \'Read depth of all passing filters reads for each allele.\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'VAF\',\n                number=\'A\',\n                type=\'Float\',\n                description=\'Variant allele fractions.\'),\n            variants_pb2.VcfFormatInfo(\n                id=\'PL\',\n                number=\'G\',\n                type=\'Integer\',\n                description=\'Genotype likelihoods, Phred encoded\'),\n        ],\n    )\n    variant_records = list(\n        tfrecord.read_tfrecords(tfrecord_file, proto=variants_pb2.Variant))\n    out_fname = test_utils.test_tmpfile(\'output.vcf\')\n    with vcf_writer.VcfWriter.to_file(out_fname, header,\n                                      writer_options) as writer:\n      for record in variant_records[:5]:\n        writer.write(record)\n\n    # Check: are the variants written as expected?\n    # pylint: disable=line-too-long\n    expected_vcf_content = [\n        \'##fileformat=VCFv4.2\\n\',\n        \'##FILTER=<ID=PASS,Description=""All filters passed"">\\n\',\n        \'##FILTER=<ID=LowQual,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL95.00to96.00,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL96.00to97.00,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL97.00to99.00,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL99.00to99.50,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL99.50to99.90,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL99.90to99.95,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL99.95to100.00+,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheINDEL99.95to100.00,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheSNP99.50to99.60,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheSNP99.60to99.80,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheSNP99.80to99.90,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheSNP99.90to99.95,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheSNP99.95to100.00+,Description="""">\\n\',\n        \'##FILTER=<ID=VQSRTrancheSNP99.95to100.00,Description="""">\\n\',\n        \'##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of \'\n        \'the interval"">\\n\',\n        \'##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">\\n\',\n        \'##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">\\n\',\n        \'##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth of all \'\n        \'passing filters reads."">\\n\',\n        \'##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP \'\n        \'observed within the GVCF block."">\\n\',\n        \'##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth of all \'\n        \'passing filters reads for each allele."">\\n\',\n        \'##FORMAT=<ID=VAF,Number=A,Type=Float,Description=\\""Variant allele \'\n        \'fractions."">\\n\',\n        \'##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Genotype \'\n        \'likelihoods, Phred encoded"">\\n\',\n        \'##contig=<ID=chr1,length=248956422>\\n\',\n        \'##contig=<ID=chr2,length=242193529>\\n\',\n        \'##contig=<ID=chr3,length=198295559>\\n\',\n        \'##contig=<ID=chrX,length=156040895>\\n\',\n        \'#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\tFORMAT\\tNA12878_18_99\\n\',\n        \'chr1\\t13613\\t.\\tT\\tA\\t39.88\\tVQSRTrancheSNP99.90to99.95\\t.\\tGT:GQ:DP:AD:PL\\t0/1:16:4:1,3:68,0,16\\n\',\n        \'chr1\\t13813\\t.\\tT\\tG\\t90.28\\tPASS\\t.\\tGT:GQ:DP:AD:PL\\t1/1:9:3:0,3:118,9,0\\n\',\n        \'chr1\\t13838\\trs28428499\\tC\\tT\\t62.74\\tPASS\\t.\\tGT:GQ:DP:AD:PL\\t1/1:6:2:0,2:90,6,0\\n\',\n        \'chr1\\t14397\\trs756427959\\tCTGT\\tC\\t37.73\\tPASS\\t.\\tGT:GQ:DP:AD:PL\\t0/1:75:5:3,2:75,0,152\\n\',\n        \'chr1\\t14522\\t.\\tG\\tA\\t49.77\\tVQSRTrancheSNP99.60to99.80\\t.\\tGT:GQ:DP:AD:PL\\t0/1:78:10:6,4:78,0,118\\n\'\n    ]\n    # pylint: enable=line-too-long\n\n    with gfile.Open(out_fname, \'r\') as f:\n      self.assertEqual(f.readlines(), expected_vcf_content)\n\n  def test_write_variant_is_ok(self):\n    self.assertIsNone(self.writer.write(self.variant))\n\n  def test_write_raises_with_unknown_contig(self):\n    with self.assertRaisesRegexp(ValueError, _UNKNOWN_CONTIG_ERROR):\n      self.variant.reference_name = \'BadChrom\'\n      self.writer.write(self.variant)\n\n  def test_write_raises_with_unknown_filter(self):\n    with self.assertRaisesRegexp(ValueError, _FILTER_NOT_FOUND_ERROR):\n      self.variant.filter[:] = [\'BadFilter\']\n      self.writer.write(self.variant)\n\n  @parameterized.parameters(\n      ([], _WRONG_NUMBER_OF_SAMPLES),\n      ([\'Spot\'], _WRONG_NUMBER_OF_SAMPLES),\n      ([\'Fido\'], _WRONG_NUMBER_OF_SAMPLES),\n      ([\'Unknown\', \'Fido\'], _DISCORDANT_SAMPLE_NAMES_ERROR),\n      ([\'Spot\', \'Unknown\'], _DISCORDANT_SAMPLE_NAMES_ERROR),\n      ([\'Spot\', \'Fido\'], _DISCORDANT_SAMPLE_NAMES_ERROR),  # Out of order.\n      ([\'Fido\', \'Spot\', \'Extra\'], _WRONG_NUMBER_OF_SAMPLES),\n  )\n  def test_write_raises_with_unknown_sample(self, sample_names, message):\n    with self.assertRaisesRegexp(ValueError, message):\n      del self.variant.calls[:]\n      for sample_name in sample_names:\n        self.variant.calls.add(genotype=[0, 0], call_set_name=sample_name)\n      self.writer.write(self.variant)\n\n  def test_context_manager(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.variant))\n\n    # self.writer should be closed, so writing again will fail.\n    with self.assertRaisesRegexp(ValueError, _WRITE_TO_CLOSED_ERROR):\n      self.writer.write(self.variant)\n\n  def test_double_context_manager(self):\n    with self.writer:\n      # Writing within the context manager succeeds.\n      self.assertIsNone(self.writer.write(self.variant))\n\n    with self.assertRaisesRegexp(ValueError, _DOUBLE_CLOSE_ERROR):\n      # Entering the closed writer should be fine.\n      with self.writer:\n        pass  # We want to raise an error on exit, so nothing to do in context.\n\n\nclass WrapVcfWriterRoundTripTests(parameterized.TestCase):\n\n  @parameterized.parameters((\'test_samples.vcf\',), (\'test_samples.vcf.gz\',),\n                            (\'test_sites.vcf\',))\n  def test_round_trip_vcf(self, test_datum_name):\n    # Round-trip variants through writing and reading:\n    # 1. Read variants v1 from VcfReader;\n    # 2. Write v1 to vcf using our VcfWriter;\n    # 3. Read back in using VcfReader -- v2;\n    # 4. compare v1 and v2.\n    in_file = test_utils.genomics_core_testdata(test_datum_name)\n    out_file = test_utils.test_tmpfile(\'output_\' + test_datum_name)\n\n    v1_reader = vcf.VcfReader(in_file)\n    v1_records = list(v1_reader.iterate())\n    self.assertTrue(v1_records, \'Reader failed to find records\')\n\n    header = copy.deepcopy(v1_reader.header)\n    writer_options = variants_pb2.VcfWriterOptions()\n\n    with vcf_writer.VcfWriter.to_file(out_file, header,\n                                      writer_options) as writer:\n      for record in v1_records:\n        writer.write(record)\n\n    v2_reader = vcf.VcfReader(out_file)\n    v2_records = list(v2_reader.iterate())\n\n    self.assertEqual(v1_records, v2_records,\n                     \'Round-tripped variants not as expected\')\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/util/python/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
nucleus/util/python/math_wrap_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for Math CLIF python wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\n\nfrom nucleus.util.python import math\n\n\nclass MathWrapTest(absltest.TestCase):\n\n  def test_one_minus_log10_prob_to_phred(self):\n    self.assertAlmostEqual(16.4277471723837,\n                           math.log10_ptrue_to_phred(-0.01, 1000))\n\n  def test_phred_to_prob(self):\n    self.assertEqual(0.1, math.phred_to_perror(10))\n\n  def test_phred_to_log10_prob(self):\n    self.assertEqual(-1, math.phred_to_log10_perror(10))\n\n  def test_prob_to_phred(self):\n    self.assertEqual(10.0, math.perror_to_phred(0.1))\n\n  def test_prob_to_rounded_phred(self):\n    self.assertEqual(10, math.perror_to_rounded_phred(0.1))\n\n  def test_prob_to_log10_prob(self):\n    self.assertEqual(-1, math.perror_to_log10_perror(0.1))\n\n  def test_log10_prob_to_phred(self):\n    self.assertEqual(10, math.log10_perror_to_phred(-1))\n\n  def test_log10_prob_to_rounded_phred(self):\n    self.assertEqual(10, math.log10_perror_to_rounded_phred(-1))\n\n  def test_log10_prob_to_prob(self):\n    self.assertEqual(0.1, math.log10_perror_to_perror(-1))\n\n  def test_zero_shift_log10_probs(self):\n    self.assertSequenceEqual([0, -1, -2],\n                             math.zero_shift_log10_probs([-1, -2, -3]))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
nucleus/vendor/python/__init__.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
nucleus/vendor/python/statusor_examples_test.py,0,"b'# Copyright 2018 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom nucleus.vendor.python import statusor_examples\n\n\nclass StatusorClifWrapTest(absltest.TestCase):\n\n  def test_make_int_ok(self):\n    self.assertEqual(statusor_examples.MakeIntOK(), 42)\n\n  def test_make_int_fail(self):\n    with self.assertRaisesRegexp(ValueError, \'Invalid argument: MakeIntFail\'):\n      statusor_examples.MakeIntFail()\n\n  def test_make_str_ok(self):\n    self.assertEqual(statusor_examples.MakeStrOK(), \'hello\')\n\n  # See CLIF wrapper for a discussion of why this is commented out.\n  # def test_make_str_ok_stripped_type(self):\n  #   self.assertEqual(statusor_examples.MakeStrOKStrippedType(), \'hello\')\n\n  def test_make_str_fail(self):\n    with self.assertRaisesRegexp(ValueError, \'Invalid argument: MakeStrFail\'):\n      statusor_examples.MakeStrFail()\n\n  def test_make_int_unique_ptr_ok(self):\n    self.assertEqual(statusor_examples.MakeIntUniquePtrOK(), 421)\n\n  def test_make_int_unique_ptr_fail(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Invalid argument: MakeIntUniquePtrFail\'):\n      statusor_examples.MakeIntUniquePtrFail()\n\n  def test_make_int_vector_ok(self):\n    self.assertEqual(statusor_examples.MakeIntVectorOK(), [1, 2, 42])\n\n  def test_make_int_vector_fail(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Invalid argument: MakeIntVectorFail\'):\n      statusor_examples.MakeIntVectorFail()\n\n  def test_returning_status_ok_returns_none(self):\n    self.assertEqual(statusor_examples.FuncReturningStatusOK(), None)\n\n  def test_returning_status_fail_raises(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Invalid argument: FuncReturningStatusFail\'):\n      statusor_examples.FuncReturningStatusFail()\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
