file_path,api_count,code
setup.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Setup for pip package.""""""\n\nimport datetime\nfrom os import path\nimport sys\n\nfrom setuptools import find_packages\nfrom setuptools import setup\nfrom setuptools.dist import Distribution\n\nif sys.version_info[0] < 3:\n  # Need to load open from io to support encoding arg when using Python 2.\n  from io import open  # pylint: disable=redefined-builtin, g-importing-member, g-import-not-at-top\n\n# Read the contents of the README file and set that as the long package\n# description.\ncwd = path.abspath(path.dirname(__file__))\nwith open(path.join(cwd, \'README.md\'), encoding=\'utf-8\') as f:\n  long_description = f.read()\n\ndescription = \'High-performance TensorFlow library for quantitative finance.\'\n\nmajor_version = \'0\'\nminor_version = \'0\'\npatch_version = \'1\'\n\nif \'--nightly\' in sys.argv:\n  # Run `python3 setup.py --nightly ...` to create a nightly build.\n  sys.argv.remove(\'--nightly\')\n  project_name = \'tff-nightly\'\n  release_suffix = datetime.datetime.utcnow().strftime(\'.dev%Y%m%d\')\n  tfp_package = \'tfp-nightly\'\nelse:\n  project_name = \'tf-quant-finance\'\n  # The suffix should be replaced with \'aN\', \'bN\', or \'rcN\' (note: no dots) for\n  # respective alpha releases, beta releases, and release candidates. And it\n  # should be cleared, i.e. set to \'\', for stable releases (c.f. PEP 440).\n  release_suffix = \'.dev19\'\n  tfp_package = \'tensorflow-probability >= 0.8.0\'\n\n__version__ = \'.\'.join([major_version, minor_version, patch_version])\nif release_suffix:\n  __version__ += release_suffix\n\nREQUIRED_PACKAGES = [\n    \'attrs >= 18.2.0\', tfp_package, \'numpy >= 1.16.0\'\n]\n\n\nclass BinaryDistribution(Distribution):\n  """"""This class is needed in order to create OS specific wheels.""""""\n\n  def has_ext_modules(self):\n    return False\n\n\nsetup(\n    name=project_name,\n    version=__version__,\n    description=description,\n    author=\'Google Inc.\',\n    author_email=\'tf-quant-finance@google.com\',\n    url=\'https://github.com/google/tf-quant-finance\',\n    # Contained modules and scripts.\n    packages=find_packages(),\n    install_requires=REQUIRED_PACKAGES,\n    # Add in any packaged data.\n    include_package_data=True,\n    zip_safe=False,\n    distclass=BinaryDistribution,\n    # PyPI package information.\n    classifiers=[\n        \'Development Status :: 2 - Pre-Alpha\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Financial and Insurance Industry\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Natural Language :: English\',\n        \'Topic :: Scientific/Engineering :: Mathematics\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Operating System :: OS Independent\',\n    ],\n    license=\'Apache 2.0\',\n    keywords=\'tensorflow quantitative finance hpc gpu option pricing\',\n    package_data={\n        \'tf_quant_finance\': [\n            \'third_party/sobol_data/new-joe-kuo.6.21201\',\n            \'third_party/sobol_data/LICENSE\'\n        ]\n    },\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\'\n)\n'"
tf_quant_finance/__init__.py,2,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""TensorFlow Quantitative Finance.""""""\n\nimport sys\n\n# We need to put some imports inside a function call below, and the function\n# call needs to come before the *actual* imports that populate the\n# tf_quant_finance namespace. Hence, we disable this lint check throughout\n# the file.\n#\n# pylint: disable=g-import-not-at-top\n\n# Update this whenever we need to depend on a newer TensorFlow release.\n_REQUIRED_TENSORFLOW_VERSION = ""2.1""  # pylint: disable=g-statement-before-imports\n\n\n# Ensure Python 3 is used.\ndef _check_py_version():\n  if sys.version_info[0] < 3:\n    raise Exception(""Please use Python 3. Python 2 is not supported."")\n\n\n# Ensure TensorFlow is importable and its version is sufficiently recent. This\n# needs to happen before anything else, since the imports below will try to\n# import tensorflow, too.\ndef _ensure_tf_install():  # pylint: disable=g-statement-before-imports\n  """"""Attempt to import tensorflow, and ensure its version is sufficient.\n\n  Raises:\n    ImportError: if either tensorflow is not importable or its version is\n    inadequate.\n  """"""\n  try:\n    import tensorflow.compat.v2 as tf\n  except ImportError:\n    # Print more informative error message, then reraise.\n    print(""\\n\\nFailed to import TensorFlow. Please note that TensorFlow is not ""\n          ""installed by default when you install TF Quant Finance library. ""\n          ""This is so that users can decide whether to install the GPU-enabled ""\n          ""TensorFlow package. To use TF Quant Finance library, please install ""\n          ""the most recent version of TensorFlow, by following instructions at ""\n          ""https://tensorflow.org/install.\\n\\n"")\n    raise\n\n  import distutils.version\n\n  if (distutils.version.LooseVersion(tf.__version__) <\n      distutils.version.LooseVersion(_REQUIRED_TENSORFLOW_VERSION)):\n    raise ImportError(\n        ""This version of TF Quant Finance library requires TensorFlow ""\n        ""version >= {required}; Detected an installation of version {present}. ""\n        ""Please upgrade TensorFlow to proceed."".format(\n            required=_REQUIRED_TENSORFLOW_VERSION, present=tf.__version__))\n\n\n_check_py_version()\n_ensure_tf_install()\n\nfrom tf_quant_finance import black_scholes\nfrom tf_quant_finance import datetime\nfrom tf_quant_finance import experimental\nfrom tf_quant_finance import math\nfrom tf_quant_finance import models\nfrom tf_quant_finance import rates\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    ""black_scholes"",\n    ""datetime"",\n    ""experimental"",\n    ""math"",\n    ""models"",\n    ""rates"",\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/black_scholes/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""TensorFlow Quantitative Finance volatility surfaces and vanilla options.""""""\n\nfrom tf_quant_finance.black_scholes import approximations\nfrom tf_quant_finance.black_scholes import brownian_bridge\nfrom tf_quant_finance.black_scholes import crr_binomial_tree\nfrom tf_quant_finance.black_scholes import vanilla_prices\nfrom tf_quant_finance.black_scholes.implied_vol_approximation import implied_vol as implied_vol_approx\nfrom tf_quant_finance.black_scholes.implied_vol_lib import implied_vol\nfrom tf_quant_finance.black_scholes.implied_vol_lib import ImpliedVolMethod\nfrom tf_quant_finance.black_scholes.implied_vol_newton_root import implied_vol as implied_vol_newton\n\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\nbinary_price = vanilla_prices.binary_price\nbarrier_price = vanilla_prices.barrier_price\nbrownian_bridge_single = brownian_bridge.brownian_bridge_single\nbrownian_bridge_double = brownian_bridge.brownian_bridge_double\noption_price = vanilla_prices.option_price\noption_price_binomial = crr_binomial_tree.option_price_binomial\n\n_allowed_symbols = [\n    \'approximations\',\n    \'binary_price\',\n    \'brownian_bridge_single\',\n    \'brownian_bridge_double\',\n    \'implied_vol\',\n    \'implied_vol_approx\',\n    \'implied_vol_newton\',\n    \'option_price\',\n    \'option_price_binomial\',\n    \'ImpliedVolMethod\',\n    \'barrier_price\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/black_scholes/brownian_bridge.py,17,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Methods for brownian bridges.\n\nThese can be used in Monte-Carlo simulation for payoff with continuous barrier.\nIndeed, the Monte-Carlo simulation is inherently discrete in time, and to\nimprove convergence (w.r.t. the number of time steps) for payoff with continuous\nbarrier, adjustment with brownian bridge can be made.\n\n## References\n\n[1] Emmanuel Gobet. Advanced Monte Carlo methods for barrier and related\nexotic options.\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=1265669\n""""""\n\nimport tensorflow.compat.v2 as tf\n\n\ndef brownian_bridge_double(*,\n                           x_start,\n                           x_end,\n                           variance,\n                           upper_barrier,\n                           lower_barrier,\n                           n_cutoff=3,\n                           dtype=None,\n                           name=None):\n  """"""Computes probability of not touching the barriers for a 1D Brownian Bridge.\n\n  The Brownian bridge starts at `x_start`, ends at `x_end` and has a variance\n  `variance`. The no-touch probabilities are calculated assuming that `x_start`\n  and `x_end` are within the barriers \'lower_barrier\' and \'upper_barrier\'.\n  This can be used in Monte Carlo pricing for adjusting probability of\n  touching the barriers from discrete case to continuous case.\n  Typically in practice, the tensors `x_start`, `x_end` and `variance` should be\n  of rank 2 (with time steps and paths being the 2 dimensions).\n\n  #### Example\n\n  ```python\n  x_start = np.asarray([[4.5, 4.5, 4.5], [4.5, 4.6, 4.7]])\n  x_end = np.asarray([[5.0, 4.9, 4.8], [4.8, 4.9, 5.0]])\n  variance = np.asarray([[0.1, 0.2, 0.1], [0.3, 0.1, 0.2]])\n  upper_barrier = 5.1\n  lower_barrier = 4.4\n\n  no_touch_proba = brownian_bridge_double(\n    x_start=x_start,\n    x_end=x_end,\n    variance=variance,\n    upper_barrier=upper_barrier,\n    lower_barrier=lower_barrier,\n    n_cutoff=3,\n    )\n  # Expected print output of no_touch_proba:\n  #[[0.45842169 0.21510919 0.52704599]\n  #[0.09394963 0.73302813 0.22595022]]\n  ```\n\n  #### References\n\n  [1] Emmanuel Gobet. Advanced Monte Carlo methods for barrier and related\n  exotic options.\n  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1265669\n\n  Args:\n    x_start: A real `Tensor` of any shape and dtype.\n    x_end: A real `Tensor` of the same dtype and compatible shape as\n      `x_start`.\n    variance: A real `Tensor` of the same dtype and compatible shape as\n      `x_start`.\n    upper_barrier: A scalar `Tensor` of the same dtype as `x_start`. Stands for\n      the upper boundary for the Brownian Bridge.\n    lower_barrier: A scalar `Tensor` of the same dtype as `x_start`. Stands for\n      lower the boundary for the Brownian Bridge.\n    n_cutoff: A positive scalar int32 `Tensor`. This controls when to cutoff\n      the sum which would otherwise have an infinite number of terms.\n      Default value: 3.\n    dtype: Optional `tf.DType`. If supplied, the dtype to be used for conversion\n      of any supplied non-`Tensor` arguments to `Tensor`.\n      Default value: None which maps to the default dtype inferred by\n      TensorFlow.\n    name: str. The name for the ops created by this function.\n      Default value: None which is mapped to the default name\n      `brownian_bridge_double`.\n\n  Returns:\n      A `Tensor` of the same shape as the input data which is the probability\n      of not touching the upper and lower barrier.\n  """"""\n  with tf.name_scope(name or \'brownian_bridge_double\'):\n    x_start = tf.convert_to_tensor(x_start, dtype=dtype, name=\'x_start\')\n    dtype = x_start.dtype\n    variance = tf.convert_to_tensor(variance, dtype=dtype, name=\'variance\')\n    x_end = tf.convert_to_tensor(x_end, dtype=dtype, name=\'x_end\')\n    barrier_diff = upper_barrier - lower_barrier\n\n    x_start = tf.expand_dims(x_start, -1)\n    x_end = tf.expand_dims(x_end, -1)\n    variance = tf.expand_dims(variance, -1)\n\n    k = tf.expand_dims(tf.range(-n_cutoff, n_cutoff + 1, dtype=dtype), 0)\n\n    a = k * barrier_diff * (k * barrier_diff + (x_end - x_start))\n    b = (k * barrier_diff + x_start - upper_barrier)\n    b *= k * barrier_diff + (x_end - upper_barrier)\n\n    # TODO(b/152731702): replace with a numericall stable procedure.\n    output = tf.math.exp(- 2 * a / variance) - tf.math.exp(-2 * b / variance)\n\n    return tf.reduce_sum(output, axis=-1)\n\n\ndef brownian_bridge_single(*,\n                           x_start,\n                           x_end,\n                           variance,\n                           barrier,\n                           dtype=None,\n                           name=None):\n  """"""Computes proba of not touching the barrier for a 1D Brownian Bridge.\n\n  The Brownian bridge starts at `x_start`, ends at `x_end` and has a variance\n  `variance`. The no-touch probabilities are calculated assuming that `x_start`\n  and `x_end` are the same side of the barrier (either both above or both\n  below).\n  This can be used in Monte Carlo pricing for adjusting probability of\n  touching the barrier from discrete case to continuous case.\n  Typically in practise, the tensors `x_start`, `x_end` and `variance` should be\n  bi-dimensional (with time steps and paths being the 2 dimensions).\n\n  #### Example\n\n  ```python\n  x_start = np.asarray([[4.5, 4.5, 4.5], [4.5, 4.6, 4.7]])\n  x_end = np.asarray([[5.0, 4.9, 4.8], [4.8, 4.9, 5.0]])\n  variance = np.asarray([[0.1, 0.2, 0.1], [0.3, 0.1, 0.2]])\n  barrier = 5.1\n\n  no_touch_proba = brownian_bridge_single(\n    x_start=x_start,\n    x_end=x_end,\n    variance=variance,\n    barrier=barrier)\n  # Expected print output of no_touch_proba:\n  # [[0.69880579 0.69880579 0.97267628]\n  #  [0.69880579 0.86466472 0.32967995]]\n  ```\n\n  #### References\n\n  [1] Emmanuel Gobet. Advanced Monte Carlo methods for barrier and related\n  exotic options.\n  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1265669\n\n  Args:\n    x_start: A real `Tensor` of any shape and dtype.\n    x_end: A real `Tensor` of the same dtype and compatible shape as\n      `x_start`.\n    variance: A real `Tensor` of the same dtype and compatible shape as\n      `x_start`.\n    barrier: A scalar `Tensor` of the same dtype as `x_start`. Stands for the\n      boundary for the Brownian Bridge.\n    dtype: Optional `tf.DType`. If supplied, the dtype to be used for conversion\n      of any supplied non-`Tensor` arguments to `Tensor`.\n      Default value: None which maps to the default dtype inferred by\n      TensorFlow.\n    name: str. The name for the ops created by this function.\n      Default value: None which is mapped to the default name\n      `brownian_bridge_single`.\n\n  Returns:\n      A `Tensor` of the same shape as the input data which is the probability\n      of not touching the barrier.\n  """"""\n  with tf.name_scope(name or \'brownian_bridge_single\'):\n    x_start = tf.convert_to_tensor(x_start, dtype=dtype, name=\'x_start\')\n    dtype = x_start.dtype\n    variance = tf.convert_to_tensor(variance, dtype=dtype, name=\'variance\')\n    x_end = tf.convert_to_tensor(x_end, dtype=dtype, name=\'x_end\')\n\n    a = (x_start - barrier) * (x_end - barrier)\n    return 1 - tf.math.exp(-2 * a / variance)\n\n'"
tf_quant_finance/black_scholes/brownian_bridge_test.py,2,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for Brownian Bridge method.""""""\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass BrownianBridgeTest(parameterized.TestCase, tf.test.TestCase):\n  """"""Tests for Brownian Bridge method.""""""\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32\n      }, {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64\n      })\n  def test_brownian_bridge_double(self, dtype):\n    def brownian_bridge_numpy(x_start, x_end, upper_b, lower_b, variance,\n                              n_cutoff):\n      def f(k):\n        a = np.exp(-2 * k * (upper_b - lower_b) * (\n            k * (upper_b - lower_b) + (x_end - x_start)) / variance)\n        b = np.exp(-2 * (k * (upper_b - lower_b) + x_start - upper_b) * (\n            k * (upper_b - lower_b) + (x_end - upper_b)) / variance)\n        return a - b\n\n      return np.sum([f(k) for k in range(-n_cutoff, n_cutoff + 1)], axis=0)\n\n    x_start = np.asarray([[1.0, 1.1, 1.1], [1.05, 1.11, 1.11]], dtype=dtype)\n    x_end = np.asarray([[2.0, 2.1, 2.8], [2.05, 2.11, 2.11]], dtype=dtype)\n    variance = np.asarray([1.0, 1.0, 1.1], dtype=dtype)\n    n_cutoff = 3\n\n    upper_barrier = 3.0\n    lower_barrier = 0.5\n\n    np_values = brownian_bridge_numpy(\n        x_start,\n        x_end,\n        upper_barrier,\n        lower_barrier,\n        variance,\n        n_cutoff=n_cutoff)\n\n    tff_values = self.evaluate(\n        tff.black_scholes.brownian_bridge_double(\n            x_start=x_start,\n            x_end=x_end,\n            variance=variance,\n            dtype=dtype,\n            upper_barrier=upper_barrier,\n            lower_barrier=lower_barrier,\n            n_cutoff=n_cutoff))\n\n    self.assertEqual(tff_values.shape, np_values.shape)\n    self.assertArrayNear(tff_values.flatten(), np_values.flatten(), 1e-7)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32\n      }, {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64\n      })\n  def test_brownian_bridge_single(self, dtype):\n    def brownian_bridge_numpy(x_start, x_end, barrier, variance):\n      return 1 - np.exp(-2 * (x_start - barrier) * (x_end - barrier) / variance)\n\n    x_start = np.asarray([[1.0, 1.1, 1.1], [1.05, 1.11, 1.11]], dtype=dtype)\n    x_end = np.asarray([[2.0, 2.1, 2.8], [2.05, 2.11, 2.11]], dtype=dtype)\n    variance = np.asarray([1.0, 1.0, 1.1], dtype=dtype)\n\n    barrier = 3.0\n\n    np_values = brownian_bridge_numpy(\n        x_start,\n        x_end,\n        barrier,\n        variance)\n\n    tff_values = self.evaluate(\n        tff.black_scholes.brownian_bridge_single(\n            x_start=x_start,\n            x_end=x_end,\n            variance=variance,\n            dtype=dtype,\n            barrier=barrier))\n\n    self.assertEqual(tff_values.shape, np_values.shape)\n    self.assertArrayNear(tff_values.flatten(), np_values.flatten(), 1e-7)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/black_scholes/crr_binomial_tree.py,43,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Black Scholes prices of options using CRR binomial trees.""""""\n\nimport tensorflow.compat.v2 as tf\n\n\n# TODO(b/150447187): Generalize to time dependent parameters.\ndef option_price_binomial(*,\n                          volatilities,\n                          strikes,\n                          expiries,\n                          spots,\n                          discount_rates=None,\n                          dividend_rates=None,\n                          is_call_options=None,\n                          is_american=None,\n                          num_steps=100,\n                          dtype=None,\n                          name=None):\n  """"""Computes the BS price for a batch of European or American options.\n\n  Uses the Cox-Ross-Rubinstein version of the binomial tree method to compute\n  the price of American or European options. Supports batching of the options\n  and allows mixing of European and American style exercises in a batch.\n  For more information about the binomial tree method and the\n  Cox-Ross-Rubinstein method in particular see the references below.\n\n  #### Example\n\n  ```python\n  # Prices 5 options with a mix of Call/Put, American/European features\n  # in a single batch.\n  dtype = np.float64\n  spots = np.array([1.0, 2.0, 3.0, 4.0, 5.0], dtype=dtype)\n  strikes = np.array([3.0, 3.0, 3.0, 3.0, 3.0], dtype=dtype)\n  volatilities = np.array([0.1, 0.22, 0.32, 0.01, 0.4], dtype=dtype)\n  is_call_options = np.array([True, True, False, False, False])\n  is_american = np.array([False, True, True, False, True])\n  discount_rates = np.array(0.035, dtype=dtype)\n  dividend_rates = np.array([0.02, 0.0, 0.07, 0.01, 0.0], dtype=dtype)\n  expiries = np.array(1.0, dtype=dtype)\n\n  prices = option_price_binomial(\n      volatilities=volatilities,\n      strikes=strikes,\n      expiries=expiries,\n      spots=spots,\n      discount_rates=discount_rates,\n      dividend_rates=dividend_rates,\n      is_call_options=is_call_options,\n      is_american=is_american,\n      dtype=dtype)\n  # Prints [0., 0.0098847, 0.41299509, 0., 0.06046989]\n  ```\n\n  #### References\n\n  [1] Hull, John C., Options, Futures and Other Derivatives. Pearson, 2018.\n  [2] Wikipedia contributors. Binomial Options Pricing Model. Available at:\n    https://en.wikipedia.org/wiki/Binomial_options_pricing_model\n\n  Args:\n    volatilities: Real `Tensor` of any shape and dtype. The volatilities to\n      expiry of the options to price.\n    strikes: A real `Tensor` of the same dtype and compatible shape as\n      `volatilities`. The strikes of the options to be priced.\n    expiries: A real `Tensor` of same dtype and compatible shape as\n      `volatilities`. The expiry of each option. The units should be such that\n      `expiry * volatility**2` is dimensionless.\n    spots: A real `Tensor` of any shape that broadcasts to the shape of the\n      `volatilities`. The current spot price of the underlying.\n    discount_rates: An optional real `Tensor` of same dtype as the\n      `volatilities`. The risk free discount rate. If None the rate is assumed\n      to be 0.\n      Default value: None, equivalent to discount rates = 0..\n    dividend_rates: An optional real `Tensor` of same dtype as the\n      `volatilities`. If None the rate is assumed to be 0.\n      Default value: None, equivalent to discount rates = 1.\n    is_call_options: A boolean `Tensor` of a shape compatible with\n      `volatilities`. Indicates whether the option is a call (if True) or a put\n      (if False). If not supplied, call options are assumed.\n      Default value: None, equivalent to is_call_options = True.\n    is_american: A boolean `Tensor` of a shape compatible with `volatilities`.\n      Indicates whether the option exercise style is American (if True) or\n      European (if False). If not supplied, European style exercise is assumed.\n      Default value: None, equivalent to is_american = False.\n    num_steps: A positive scalar int32 `Tensor`. The size of the time\n      discretization to use.\n      Default value: 100.\n    dtype: Optional `tf.DType`. If supplied, the dtype to be used for conversion\n      of any supplied non-`Tensor` arguments to `Tensor`.\n      Default value: None which maps to the default dtype inferred by TensorFlow\n        (float32).\n    name: str. The name for the ops created by this function.\n      Default value: None which is mapped to the default name `option_price`.\n\n  Returns:\n    A `Tensor` of the same shape as the inferred batch shape of the input data.\n    The Black Scholes price of the options computed on a binomial tree.\n  """"""\n  with tf.name_scope(name or \'crr_option_price\'):\n    strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\'strikes\')\n    dtype = strikes.dtype\n    volatilities = tf.convert_to_tensor(\n        volatilities, dtype=dtype, name=\'volatilities\')\n    expiries = tf.convert_to_tensor(expiries, dtype=dtype, name=\'expiries\')\n    spots = tf.convert_to_tensor(spots, dtype=dtype, name=\'spots\')\n\n    if discount_rates is None:\n      discount_rates = tf.zeros_like(volatilities)\n    else:\n      discount_rates = tf.convert_to_tensor(\n          discount_rates, dtype=dtype, name=\'discount_rates\')\n    if dividend_rates is None:\n      dividend_rates = tf.zeros_like(volatilities)\n    else:\n      dividend_rates = tf.convert_to_tensor(\n          dividend_rates, dtype=dtype, name=\'dividend_rates\')\n    if is_call_options is None:\n      is_call_options = tf.ones_like(\n          volatilities, dtype=tf.bool, name=\'is_call_options\')\n    else:\n      is_call_options = tf.convert_to_tensor(\n          is_call_options, dtype=tf.bool, name=\'is_call_options\')\n    if is_american is None:\n      is_american = tf.zeros_like(\n          volatilities, dtype=tf.bool, name=\'is_american\')\n    else:\n      is_american = tf.convert_to_tensor(\n          is_american, dtype=tf.bool, name=\'is_american\')\n\n    num_steps = tf.cast(num_steps, dtype=dtype)\n    dt = expiries / num_steps\n\n    # CRR choices for the up and down move multipliers\n    ln_up = volatilities * tf.math.sqrt(dt)\n    ln_dn = -ln_up\n\n    # Prepares the spot grid.\n    grid_idx = tf.range(num_steps + 1)\n    # Stores the grid as shape [input_batch, N + 1] where N = num_steps.\n    log_spot_grid_1 = tf.expand_dims(\n        tf.math.log(spots) + ln_up * num_steps, axis=-1)\n    log_spot_grid_2 = tf.expand_dims(ln_dn - ln_up, axis=-1) * grid_idx\n    log_spot_grid = log_spot_grid_1 + log_spot_grid_2\n\n    # Reshaping needed to ensure that batch axis is at the right place in\n    # both the spots (coming from log_spot_grid above) and the strikes etc.\n    payoff_fn = _get_payoff_fn(\n        tf.reshape(strikes, [-1, 1]), tf.reshape(is_call_options, [-1, 1]))\n    value_mod_fn = _get_value_modifier(\n        tf.reshape(is_american, [-1, 1]), payoff_fn)\n\n    # Shape [batch shape, num time steps + 1]\n    values_grid = payoff_fn(tf.math.exp(log_spot_grid))\n\n    p_up = tf.math.exp((discount_rates - dividend_rates) * dt + ln_up) - 1\n    p_up /= tf.math.exp(2 * ln_up) - 1\n    p_up = tf.expand_dims(p_up, axis=-1)\n    p_dn = 1 - p_up\n    discount_factors = tf.expand_dims(\n        tf.math.exp(-discount_rates * dt), axis=-1)\n    ln_up = tf.expand_dims(ln_up, axis=-1)\n\n    def one_step_back(current_values, current_log_spot_grid):\n      next_values = current_values[:, 1:] * p_dn + current_values[:, :-1] * p_up\n      next_log_spot_grid = current_log_spot_grid[:, :-1] - ln_up\n      next_values = value_mod_fn(next_values, tf.math.exp(next_log_spot_grid))\n      return discount_factors * next_values, next_log_spot_grid\n\n    def should_continue(current_values, current_log_spot_grid):\n      del current_values, current_log_spot_grid\n      return True\n\n    batch_size = values_grid.shape[0]\n    pv, _ = tf.while_loop(\n        should_continue,\n        one_step_back, (values_grid, log_spot_grid),\n        maximum_iterations=tf.cast(num_steps, dtype=tf.int32),\n        shape_invariants=(tf.TensorShape([batch_size, None]),\n                          tf.TensorShape([batch_size, None])))\n    return tf.squeeze(pv, axis=-1)\n\n\ndef _get_payoff_fn(strikes, is_call_options):\n  """"""Constructs the payoff functions.""""""\n  option_signs = tf.cast(is_call_options, dtype=strikes.dtype) * 2 - 1\n\n  def payoff(spots):\n    """"""Computes payff for the specified options given the spot grid.\n\n    Args:\n      spots: Tensor of shape [batch_size, grid_size, 1]. The spot values at some\n        time.\n\n    Returns:\n      Payoffs for exercise at the specified strikes.\n    """"""\n    return tf.nn.relu((spots - strikes) * option_signs)\n\n  return payoff\n\n\ndef _get_value_modifier(is_american, payoff_fn):\n  """"""Constructs the value modifier for american style exercise.""""""\n\n  def modifier(values, spots):\n    immediate_exercise_value = payoff_fn(spots)\n    return tf.where(is_american,\n                    tf.math.maximum(immediate_exercise_value, values), values)\n\n  return modifier\n'"
tf_quant_finance/black_scholes/crr_binomial_tree_test.py,2,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for Cox Ross Rubinstein Binomial tree method.""""""\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n# TODO(b/150448863): Enhance test coverage.\n@test_util.run_all_in_graph_and_eager_modes\nclass BinomialModelPrice(parameterized.TestCase, tf.test.TestCase):\n  """"""Tests for Binomial tree method prices.""""""\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32\n      }, {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64\n      })\n  def test_option_prices(self, dtype):\n    """"""Tests that the BS prices are correct.""""""\n    spots = np.array([1.0, 2.0, 3.0, 4.0, 5.0], dtype=dtype)\n    strikes = np.array([3.0, 3.0, 3.0, 3.0, 3.0], dtype=dtype)\n    volatilities = np.array([0.1, 0.22, 0.32, 0.01, 0.4], dtype=dtype)\n    is_call_options = np.array([True, True, False, False, False])\n    is_american = np.array([False, True, True, False, True])\n    discount_rates = np.array(0.035, dtype=dtype)\n    dividend_rates = np.array([0.02, 0.0, 0.07, 0.01, 0.0], dtype=dtype)\n    expiries = np.array(1.0, dtype=dtype)\n\n    prices = self.evaluate(\n        tff.black_scholes.option_price_binomial(\n            volatilities=volatilities,\n            strikes=strikes,\n            expiries=expiries,\n            spots=spots,\n            discount_rates=discount_rates,\n            dividend_rates=dividend_rates,\n            is_call_options=is_call_options,\n            is_american=is_american,\n            dtype=dtype))\n    expected_prices = np.array(\n        [0., 0.0098847, 0.41299509, 0., 0.06046989])\n    self.assertArrayNear(expected_prices, prices, 1e-5)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/black_scholes/implied_vol_approximation.py,51,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Methods to approximate the implied vol of options from market prices.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\n\ndef implied_vol(*,\n                prices,\n                strikes,\n                expiries,\n                spots=None,\n                forwards=None,\n                discount_factors=None,\n                is_call_options=None,\n                validate_args=False,\n                polya_factor=(2 / np.pi),\n                dtype=None,\n                name=None):\n  """"""Approximates the implied vol using the Stefanica-Radiocic algorithm.\n\n  Finds an approximation to the implied vol using the Polya approximation for\n  the Normal CDF. This algorithm was described by Stefanica and Radiocic in\n  ref [1]. They show that if the Normal CDFs appearing in the Black Scholes\n  formula for the option price are replaced with Polya\'s approximation, the\n  implied vol can be solved for analytically. The Polya approximation produces\n  absolute errors of less than 0.003 and the resulting implied vol is fairly\n  close to the true value. For practical purposes, this may not be accurate\n  enough so this result should be used as a starting point for some method with\n  controllable tolerance (e.g. a root finder).\n\n  #### References:\n  [1]: Dan Stefanica and Rados Radoicic. An explicit implied volatility formula.\n    International Journal of Theoretical and Applied Finance,\n    Vol. 20, no. 7, 2017.\n    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2908494\n  [2]: Omar Eidous, Samar Al-Salman. One-term approximation for Normal\n    distribution function. Mathematics and Statistics 4(1), 2016.\n    http://www.hrpub.org/download/20160229/MS2-13405192.pdf\n\n  Args:\n    prices: A real `Tensor` of any shape. The prices of the options whose\n      implied vol is to be calculated.\n    strikes: A real `Tensor` of the same dtype as `prices` and a shape that\n      broadcasts with `prices`. The strikes of the options.\n    expiries: A real `Tensor` of the same dtype as `prices` and a shape that\n      broadcasts with `prices`. The expiry for each option. The units should\n      be such that `expiry * volatility**2` is dimensionless.\n    spots: A real `Tensor` of any shape that broadcasts to the shape\n      of the `prices`. The current spot price of the underlying. Either this\n      argument or the `forwards` (but not both) must be supplied.\n    forwards: A real `Tensor` of any shape that broadcasts to the shape of\n      `prices`. The forwards to maturity. Either this argument or the `spots`\n      must be supplied but both must not be supplied.\n    discount_factors: An optional real `Tensor` of same dtype as the `prices`.\n      If not None, these are the discount factors to expiry (i.e. e^(-rT)).\n      If None, no discounting is applied (i.e. it is assumed that the\n      undiscounted option prices are provided ). If `spots` is supplied and\n      `discount_factors` is not None then this is also used to compute the\n      forwards to expiry.\n      Default value: None, equivalent to discount factors = 1.\n    is_call_options: A boolean `Tensor` of a shape compatible with `prices`.\n      Indicates whether the option is a call (if True) or a put (if False).\n      If not supplied, call options are assumed.\n    validate_args: A Python bool. If True, indicates that arguments should be\n      checked for correctness before performing the computation. The checks\n      performed are: (1) Forwards/spots and strikes are positive. (2) The prices\n        satisfy the arbitrage bounds (i.e. for call options, checks the\n        inequality `max(F-K, 0) <= Price <= F` and for put options, checks that\n        `max(K-F, 0) <= Price <= K`.). (3) Checks that the prices are not too\n        close to the bounds. It is numerically unstable to compute the implied\n        vols from options too far in the money or out of the money.\n      Default value: False\n    polya_factor: A real scalar. The coefficient to use in the\n      approximation for the Normal CDF. The approximation is: `N(x) ~ 0.5 + 0.5\n        * sign(x) * sqrt[ 1 - exp(-k * x**2) ]` where `k` is the coefficient\n        supplied with `polya_factor`. The original Polya approximation has the\n        value `2 / pi` and this is approximation used in Ref [1]. However, as\n        described in Ref [2], a slightly more accurate approximation is achieved\n        if we use the value of `k=5/8`).\n    dtype: `tf.Dtype` to use when converting arguments to `Tensor`s. If not\n      supplied, the default TensorFlow conversion will take place. Note that\n      this argument does not do any casting for `Tensor`s or numpy arrays.\n      Default value: None.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name \'implied_vol\' is\n      used.\n      Default value: None\n\n  Returns:\n    implied_vols: A `Tensor` of the same dtype as `prices` and shape as the\n      common broadcasted shape of `(prices, spots/forwards, strikes, expiries)`.\n      The approximate implied total volatilities computed using the Polya\n      approximation method.\n\n  Raises:\n    ValueError: If both `forwards` and `spots` are supplied or if neither is\n      supplied.\n  """"""\n  if (spots is None) == (forwards is None):\n    raise ValueError(\'Either spots or forwards must be supplied but not both.\')\n\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'implied_vol\',\n      values=[\n          prices, spots, forwards, strikes, expiries, discount_factors,\n          is_call_options\n      ]):\n    prices = tf.convert_to_tensor(prices, dtype=dtype, name=\'prices\')\n    dtype = prices.dtype\n    strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\'strikes\')\n    expiries = tf.convert_to_tensor(expiries, dtype=dtype, name=\'expiries\')\n    if discount_factors is None:\n      discount_factors = tf.convert_to_tensor(\n          1.0, dtype=dtype, name=\'discount_factors\')\n    else:\n      discount_factors = tf.convert_to_tensor(\n          discount_factors, dtype=dtype, name=\'discount_factors\')\n\n    if forwards is not None:\n      forwards = tf.convert_to_tensor(forwards, dtype=dtype, name=\'forwards\')\n    else:\n      spots = tf.convert_to_tensor(spots, dtype=dtype, name=\'spots\')\n      forwards = spots / discount_factors\n\n    control_inputs = None\n    if validate_args:\n      control_inputs = _validate_args_control_deps(prices, forwards, strikes,\n                                                   expiries, discount_factors,\n                                                   is_call_options)\n    with tf.compat.v1.control_dependencies(control_inputs):\n      adjusted_strikes = strikes * discount_factors\n      normalized_prices = prices / adjusted_strikes\n      normalized_forwards = forwards / strikes\n      return _approx_implied_vol_polya(normalized_prices, normalized_forwards,\n                                       expiries, is_call_options, polya_factor)\n\n\ndef _validate_args_control_deps(prices, forwards, strikes, expiries,\n                                discount_factors, is_call_options):\n  """"""Returns assertions for no-arbitrage conditions on the prices.""""""\n  epsilon = tf.convert_to_tensor(1e-8, dtype=prices.dtype)\n  forwards_positive = tf.compat.v1.debugging.assert_positive(forwards)\n  strikes_positive = tf.compat.v1.debugging.assert_positive(strikes)\n  expiries_positive = tf.compat.v1.debugging.assert_non_negative(expiries)\n  put_lower_bounds = tf.nn.relu(strikes - forwards)\n  call_lower_bounds = tf.nn.relu(forwards - strikes)\n  if is_call_options is not None:\n    is_call_options = tf.convert_to_tensor(is_call_options,\n                                           dtype=tf.bool,\n                                           name=\'is_call_options\')\n    lower_bounds = tf.where(\n        is_call_options, x=call_lower_bounds, y=put_lower_bounds)\n    upper_bounds = tf.where(is_call_options, x=forwards, y=strikes)\n  else:\n    lower_bounds = call_lower_bounds\n    upper_bounds = forwards\n\n  undiscounted_prices = prices / discount_factors\n  bounds_satisfied = [\n      tf.compat.v1.debugging.assert_less_equal(lower_bounds,\n                                               undiscounted_prices),\n      tf.compat.v1.debugging.assert_greater_equal(upper_bounds,\n                                                  undiscounted_prices)\n  ]\n  not_too_close_to_bounds = [\n      tf.compat.v1.debugging.assert_greater(\n          tf.math.abs(undiscounted_prices - lower_bounds), epsilon),\n      tf.compat.v1.debugging.assert_greater(\n          tf.math.abs(undiscounted_prices - upper_bounds), epsilon)\n  ]\n  return [expiries_positive, forwards_positive, strikes_positive\n         ] + bounds_satisfied + not_too_close_to_bounds\n\n\ndef _approx_implied_vol_polya(normalized_prices, normalized_forwards, expiries,\n                              is_call_options, polya_factor):\n  """"""Computes approximate implied vol using the Stefanica-Radoicic algorithm.\n\n  ## Implementation Notes\n  The mapping between the notation used in the reference paper and the code\n  below is as follows:\n    y -> log_normalized_forwards\n    alpha_c -> normalized_prices\n  This notation is used in the in-line comments.\n\n  Args:\n    normalized_prices: `Tensor` of real dtype and any shape. The prices of the\n      options to be inverted. Normalization means that the raw price is divided\n      by the strike discounted to the present.\n    normalized_forwards: `Tensor` or same dtype and shape as `normalized_prices`\n      The forwards divided by the strike of the options.\n    expiries: A real `Tensor` of same shape and dtype as `normalized_forwards`.\n      The expiry for each option.\n    is_call_options: Boolean `Tensor` of same shape as `normalized_prices` or\n      None. Indicates whether a price is for a call option (if True) or a put\n      option (if False). If None is specified, it is assumed that all the\n      options are call options.\n    polya_factor: Scalar `Tensor` of same dtype as `normalized_prices`. This is\n      the factor to use for approximating the normal CDF in a Polya-like\n      expression. Polya approximation is: N(x) ~ 0.5 + sign(x) sqrt(1-e^(-k\n        x^2)) with k = 2 / pi. However, it has been found that other values for\n        `k` may be more accurate. The value that minimizes the absolute error\n        over the range [-10, 10] is 0.62305051 (approximately 5/8).\n\n  Returns:\n    implied_vols: A `Tensor` of same shape and dtype as\n      `undiscounted_prices`. The approximate implied volatilities\n      computed using the Polya approximation for the normal CDF.\n  """"""\n  if polya_factor is None:\n    polya_factor = tf.convert_to_tensor(\n        2.0 / np.pi, dtype=normalized_prices.dtype)\n  floored_forwards = tf.math.maximum(normalized_forwards, 1)\n  capped_forwards = tf.math.minimum(normalized_forwards, 1)\n\n  log_normalized_forwards = tf.math.log(normalized_forwards)\n  sign_log_forward = tf.math.sign(log_normalized_forwards)\n\n  if is_call_options is not None:\n    is_call_options = tf.convert_to_tensor(is_call_options,\n                                           dtype=tf.bool,\n                                           name=\'is_call_options\')\n    ones = tf.ones_like(is_call_options, dtype=normalized_forwards.dtype)\n    option_signs = tf.where(is_call_options, ones, -ones)\n  else:\n    option_signs = 1\n  signs = option_signs * sign_log_forward\n\n  cdfs = 0.5 + 0.5 * signs * tf.math.sqrt(\n      -tf.math.expm1(-2 * polya_factor * tf.math.abs(log_normalized_forwards)))\n\n  # This corresponds to the expressions C_0 or P_0 in the table 2 of Ref [1].\n  threshold = signs * (floored_forwards * cdfs - capped_forwards / 2)\n\n  a, b, lnc = _get_quadratic_coeffs(normalized_prices, normalized_forwards,\n                                    log_normalized_forwards, option_signs,\n                                    polya_factor)\n  c = tf.math.exp(lnc)\n  lntwo = tf.convert_to_tensor(np.log(2.0), dtype=normalized_forwards.dtype)\n  lnbeta = lntwo + lnc - tf.math.log(b + tf.math.sqrt(b * b + 4 * a * c))\n  gamma = -lnbeta / polya_factor\n\n  term1 = tf.math.sqrt(gamma + log_normalized_forwards)\n  term2 = tf.math.sqrt(gamma - log_normalized_forwards)\n  sqrt_var = tf.where(normalized_prices <= threshold,\n                      sign_log_forward * (term1 - term2), term1 + term2)\n  return sqrt_var / tf.math.sqrt(expiries)\n\n\ndef _get_quadratic_coeffs(normalized_prices, normalized_forwards,\n                          log_normalized_forwards, option_signs, polya_factor):\n  """"""Computes the coefficients of the quadratic in Stefanica-Radiocic method.\n\n  Computes the coefficients described in Table 3 in Ref [1].\n\n  Args:\n    normalized_prices: `Tensor` of real dtype and any shape. The prices of the\n      options to be inverted. Normalization means that the raw price is divided\n      by the strike discounted to the present.\n    normalized_forwards: `Tensor` or same dtype and shape as `normalized_prices`\n      The forwards divided by the strike of the options.\n    log_normalized_forwards: `Tensor` or same dtype and shape as\n      `normalized_prices`. Log of the normalized forwards.\n    option_signs: Real `Tensor` of same shape and dtype as `normalized_prices`.\n      Should be +1 for a Call option and -1 for a put option.\n    polya_factor: Scalar `Tensor` of same dtype as `normalized_prices`. This is\n      the factor to use for approximating the normal CDF in a Polya-like\n      expression. Polya approximation is (here `k` is the `polya_factor`) N(x) ~\n      0.5 + sign(x) sqrt(1-e^(-k x^2)) with k = 2 / pi. However, it has been\n      found that other values for `k` may be more accurate. The value that\n      minimizes the absolute error over the range [-10, 10] is 0.62305051\n      (approximately 5/8).\n\n  Returns:\n    (A, B, ln(C)): A 3-tuple of coefficients in terms of which the approximate\n      implied vol is calculated.\n  """"""\n  # Corresponds to expressions in Table 3 in Ref [1].\n  # corresponds to (e^y - 1) = F/K - 1\n  q1 = normalized_forwards - 1\n  # corresponds to (e^y + 1) = F/K + 1\n  q2 = normalized_forwards + 1\n  # r here corresponds to R in the paper and not to the interest rate.\n  r = 2 * normalized_prices - option_signs * q1\n\n  # f1 is e^{-polya_factor * y}, f2 is e^{polya_factor * y}\n  f1 = tf.math.pow(normalized_forwards, -polya_factor)\n  f2 = 1 / f1\n\n  # g1 is e^{(1-polya_factor) y}, g2 is e^{-(1-polya_factor) y}\n  g1 = f1 * normalized_forwards\n  g2 = 1 / g1\n\n  # a corresponds to the coefficient A in the paper.\n  a = tf.math.square(g1 - g2)\n\n  # b corresponds to the coefficient B in the paper.\n  h = tf.math.square(normalized_forwards)\n  r2 = tf.math.square(r)\n  b = (4 * (f1 + f2) - 2 * (g1 + g2) * (1 + h - r2) / normalized_forwards)\n\n  # lnc corresponds to the logarithm of C in the paper,\n  # handled here on the log scale to minimize numerical instability.\n  lnc1 = tf.math.log(4.0 * normalized_prices) + \\\n      tf.math.log(normalized_prices - option_signs * q1)\n  lnc2 = tf.math.log(q2 - r) + tf.math.log(q2 + r)\n  lnc = lnc1 + lnc2 - 2.0 * log_normalized_forwards\n\n  return a, b, lnc\n'"
tf_quant_finance/black_scholes/implied_vol_approximation_test.py,3,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for implied_vol_approx.""""""\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\nbs = tff.black_scholes\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass ApproxImpliedVolTest(parameterized.TestCase, tf.test.TestCase):\n  """"""Tests for methods in implied_vol module.""""""\n\n  def test_approx_implied_vol(self):\n    """"""Basic test of the implied vol calculation.""""""\n    np.random.seed(6589)\n    n = 100\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      volatilities = np.exp(np.random.randn(n) / 2)\n      forwards = np.exp(np.random.randn(n))\n      strikes = forwards * (1 + (np.random.rand(n) - 0.5) * 0.2)\n      expiries = np.exp(np.random.randn(n))\n      prices = self.evaluate(\n          bs.option_price(\n              volatilities=volatilities,\n              strikes=strikes,\n              expiries=expiries,\n              forwards=forwards,\n              dtype=dtype))\n\n      implied_vols = self.evaluate(\n          bs.implied_vol_approx(\n              prices=prices,\n              strikes=strikes,\n              expiries=expiries,\n              forwards=forwards,\n              dtype=dtype))\n      self.assertArrayNear(volatilities, implied_vols, 0.6)\n\n  def test_approx_implied_vol_validate(self):\n    """"""Test the Radiocic-Polya approx doesn\'t raise where it shouldn\'t.""""""\n    np.random.seed(6589)\n    n = 100\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      volatilities = np.exp(np.random.randn(n) / 2)\n      forwards = np.exp(np.random.randn(n))\n      strikes = forwards * (1 + (np.random.rand(n) - 0.5) * 0.2)\n      expiries = np.exp(np.random.randn(n))\n      prices = self.evaluate(\n          bs.option_price(\n              volatilities=volatilities,\n              strikes=strikes,\n              expiries=expiries,\n              forwards=forwards,\n              dtype=dtype))\n\n      implied_vols = self.evaluate(\n          bs.implied_vol_approx(\n              prices=prices,\n              strikes=strikes,\n              expiries=expiries,\n              forwards=forwards,\n              validate_args=True,\n              dtype=dtype))\n      self.assertArrayNear(volatilities, implied_vols, 0.6)\n\n  @parameterized.named_parameters(\n      # This case should hit the call lower bound since C = F - K.\n      (\'call_lower\', 0.0, 1.0, 1.0, 1.0, True),\n      # This case should hit the call upper bound since C = F\n      (\'call_upper\', 1.0, 1.0, 1.0, 1.0, True),\n      # This case should hit the put upper bound since C = K\n      (\'put_lower\', 1.0, 1.0, 1.0, 1.0, False),\n      # This case should hit the call lower bound since C = F - K.\n      (\'put_upper\', 0.0, 1.0, 1.0, 1.0, False))\n  def test_approx_implied_vol_validate_raises(self, price, forward, strike,\n                                              expiry, is_call_option):\n    """"""Test the Radiocic-Polya approximation raises appropriately.""""""\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      prices = np.array([price]).astype(dtype)\n      forwards = np.array([forward]).astype(dtype)\n      strikes = np.array([strike]).astype(dtype)\n      expiries = np.array([expiry]).astype(dtype)\n      is_call_options = np.array([is_call_option])\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        implied_vols = bs.implied_vol_approx(\n            prices=prices,\n            strikes=strikes,\n            expiries=expiries,\n            forwards=forwards,\n            is_call_options=is_call_options,\n            validate_args=True,\n            dtype=dtype)\n        self.evaluate(implied_vols)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/black_scholes/implied_vol_lib.py,2,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Calculation of the Black-Scholes implied volatility via Newton\'s method.""""""\n\nimport enum\n\nfrom tf_quant_finance.black_scholes import implied_vol_approximation as approx\nfrom tf_quant_finance.black_scholes import implied_vol_newton_root as newton\n\n\n@enum.unique\nclass ImpliedVolMethod(enum.Enum):\n  """"""Implied volatility methods.\n\n  * `FAST_APPROX`: A faster but approximate method.\n  * `NEWTON`: Uses Newton root search to find an accurate value.\n  """"""\n  FAST_APPROX = 1\n  NEWTON = 2\n\n\ndef implied_vol(*,\n                prices,\n                strikes,\n                expiries,\n                spots=None,\n                forwards=None,\n                discount_factors=None,\n                is_call_options=None,\n                method=ImpliedVolMethod.NEWTON,\n                validate_args=False,\n                dtype=None,\n                name=None,\n                **kwargs):\n  """"""Finds the implied volatilities of options under the Black Scholes model.\n\n  #### Examples\n  ```python\n  forwards = np.array([1.0, 1.0, 1.0, 1.0])\n  strikes = np.array([1.0, 2.0, 1.0, 0.5])\n  expiries = np.array([1.0, 1.0, 1.0, 1.0])\n  discount_factors = np.array([1.0, 1.0, 1.0, 1.0])\n  option_signs = np.array([1.0, 1.0, -1.0, -1.0])\n  volatilities = np.array([1.0, 1.0, 1.0, 1.0])\n  prices = black_scholes.option_price(\n      forwards,\n      strikes,\n      volatilities,\n      expiries,\n      discount_factors=discount_factors,\n      is_call_options=is_call_options)\n  implied_vols = newton_vol.implied_vol(forwards,\n                                        strikes,\n                                        expiries,\n                                        discount_factors,\n                                        prices,\n                                        option_signs)\n  with tf.Session() as session:\n    print(session.run(implied_vols[0]))\n  # Expected output:\n  # [ 1.  1.  1.  1.]\n\n  Args:\n    prices: A real `Tensor` of any shape. The prices of the options whose\n      implied vol is to be calculated.\n    strikes: A real `Tensor` of the same dtype as `prices` and a shape that\n      broadcasts with `prices`. The strikes of the options.\n    expiries: A real `Tensor` of the same dtype as `prices` and a shape that\n      broadcasts with `prices`. The expiry for each option. The units should be\n      such that `expiry * volatility**2` is dimensionless.\n    spots: A real `Tensor` of any shape that broadcasts to the shape of the\n      `prices`. The current spot price of the underlying. Either this argument\n      or the `forwards` (but not both) must be supplied.\n    forwards: A real `Tensor` of any shape that broadcasts to the shape of\n      `prices`. The forwards to maturity. Either this argument or the `spots`\n      must be supplied but both must not be supplied.\n    discount_factors: An optional real `Tensor` of same dtype as the `prices`.\n      If not None, these are the discount factors to expiry (i.e. e^(-rT)). If\n      None, no discounting is applied (i.e. it is assumed that the undiscounted\n      option prices are provided ). If `spots` is supplied and\n      `discount_factors` is not None then this is also used to compute the\n      forwards to expiry.\n      Default value: None, equivalent to discount factors = 1.\n    is_call_options: A boolean `Tensor` of a shape compatible with `prices`.\n      Indicates whether the option is a call (if True) or a put (if False). If\n      not supplied, call options are assumed.\n    method: Enum value of ImpliedVolMethod to select the algorithm to use to\n      infer the implied volatility.\n    validate_args: A Python bool. If True, indicates that arguments should be\n      checked for correctness before performing the computation. The checks\n      performed are: (1) Forwards and strikes are positive. (2) The prices\n        satisfy the arbitrage bounds (i.e. for call options, checks the\n        inequality `max(F-K, 0) <= Price <= F` and for put options, checks that\n        `max(K-F, 0) <= Price <= K`.). (3) Checks that the prices are not too\n        close to the bounds. It is numerically unstable to compute the implied\n        vols from options too far in the money or out of the money.\n    dtype: `tf.Dtype` to use when converting arguments to `Tensor`s. If not\n      supplied, the default TensorFlow conversion will take place. Note that\n      this argument does not do any casting for `Tensor`s or numpy arrays.\n      Default value: None.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name \'implied_vol\' is used.\n      Default value: None\n    **kwargs: Any other keyword arguments to be passed to the specific\n      implementation. (See black_scholes.implied_vol_approx and\n      black_scholes.implied_vol_newton for details).\n\n  Returns:\n    implied_vols: A `Tensor` of the same dtype as `prices` and shape as the\n      common broadcasted shape of `(prices, spots/forwards, strikes, expiries)`.\n      The implied volatilities as inferred by the chosen method.\n\n  Raises:\n    ValueError: If both `forwards` and `spots` are supplied or if neither is\n      supplied.\n  """"""\n  if method == ImpliedVolMethod.FAST_APPROX:\n    return approx.implied_vol(\n        prices=prices,\n        strikes=strikes,\n        expiries=expiries,\n        spots=spots,\n        forwards=forwards,\n        discount_factors=discount_factors,\n        is_call_options=is_call_options,\n        validate_args=validate_args,\n        dtype=dtype,\n        name=name,\n        **kwargs)\n  if method == ImpliedVolMethod.NEWTON:\n    return newton.implied_vol(\n        prices=prices,\n        strikes=strikes,\n        expiries=expiries,\n        spots=spots,\n        forwards=forwards,\n        discount_factors=discount_factors,\n        is_call_options=is_call_options,\n        validate_args=validate_args,\n        dtype=dtype,\n        name=name,\n        **kwargs)[0]\n  raise ValueError(\'Unknown implied vol method {}\'.format(method))\n'"
tf_quant_finance/black_scholes/implied_vol_lib_test.py,3,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for implied_vol_approx.""""""\n\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\nbs = tff.black_scholes\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass ImpliedVolTest(parameterized.TestCase, tf.test.TestCase):\n  """"""Tests for methods in implied_vol module.""""""\n\n  def test_implied_vol(self):\n    """"""Basic test of the implied vol calculation.""""""\n    np.random.seed(6589)\n    n = 100\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      volatilities = np.exp(np.random.randn(n) / 2)\n      forwards = np.exp(np.random.randn(n))\n      strikes = forwards * (1 + (np.random.rand(n) - 0.5) * 0.2)\n      expiries = np.exp(np.random.randn(n))\n      prices = self.evaluate(\n          bs.option_price(\n              volatilities=volatilities,\n              strikes=strikes,\n              expiries=expiries,\n              forwards=forwards,\n              dtype=dtype))\n      # Using the default method\n      implied_vols_default = self.evaluate(\n          bs.implied_vol(\n              prices=prices,\n              strikes=strikes,\n              expiries=expiries,\n              forwards=forwards,\n              dtype=dtype))\n      self.assertArrayNear(volatilities, implied_vols_default, 0.2)\n\n      # Using Newton explicitly\n      implied_vols_newton = self.evaluate(\n          bs.implied_vol(\n              prices=prices,\n              strikes=strikes,\n              expiries=expiries,\n              forwards=forwards,\n              dtype=dtype,\n              method=bs.ImpliedVolMethod.NEWTON))\n      self.assertArrayNear(implied_vols_default, implied_vols_newton, 1e-5)\n\n      # Using approximation.\n      implied_vols_approx = self.evaluate(\n          bs.implied_vol(\n              prices=prices,\n              strikes=strikes,\n              expiries=expiries,\n              forwards=forwards,\n              dtype=dtype,\n              method=bs.ImpliedVolMethod.FAST_APPROX))\n      self.assertArrayNear(volatilities, implied_vols_approx, 0.6)\n\n  def test_validate(self):\n    """"""Test the algorithm doesn\'t raise where it shouldn\'t.""""""\n    np.random.seed(6589)\n    n = 100\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      volatilities = np.exp(np.random.randn(n) / 2)\n      forwards = np.exp(np.random.randn(n))\n      strikes = forwards * (1 + (np.random.rand(n) - 0.5) * 0.2)\n      expiries = np.exp(np.random.randn(n))\n      prices = self.evaluate(\n          bs.option_price(\n              volatilities=volatilities,\n              strikes=strikes,\n              expiries=expiries,\n              forwards=forwards,\n              dtype=dtype))\n\n      implied_vols = self.evaluate(\n          bs.implied_vol(\n              prices=prices,\n              strikes=strikes,\n              expiries=expiries,\n              forwards=forwards,\n              validate_args=True,\n              dtype=dtype))\n      self.assertArrayNear(volatilities, implied_vols, 0.6)\n\n  @parameterized.named_parameters(\n      # This case should hit the call lower bound since C = F - K.\n      (\'call_lower\', 0.0, 1.0, 1.0, 1.0, True),\n      # This case should hit the call upper bound since C = F\n      (\'call_upper\', 1.0, 1.0, 1.0, 1.0, True),\n      # This case should hit the put upper bound since C = K\n      (\'put_lower\', 1.0, 1.0, 1.0, 1.0, False),\n      # This case should hit the call lower bound since C = F - K.\n      (\'put_upper\', 0.0, 1.0, 1.0, 1.0, False))\n  def test_validate_raises(self, price, forward, strike, expiry,\n                           is_call_option):\n    """"""Test algorithm raises appropriately.""""""\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      prices = np.array([price]).astype(dtype)\n      forwards = np.array([forward]).astype(dtype)\n      strikes = np.array([strike]).astype(dtype)\n      expiries = np.array([expiry]).astype(dtype)\n      is_call_options = np.array([is_call_option])\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        implied_vols = bs.implied_vol(\n            prices=prices,\n            strikes=strikes,\n            expiries=expiries,\n            forwards=forwards,\n            is_call_options=is_call_options,\n            validate_args=True,\n            dtype=dtype)\n        self.evaluate(implied_vols)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/black_scholes/implied_vol_newton_root.py,33,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Calculation of the Black-Scholes implied volatility via Newton\'s method.""""""\n\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\n\nfrom tf_quant_finance.black_scholes import implied_vol_approximation as approx\n\n\ndef implied_vol(*,\n                prices,\n                strikes,\n                expiries,\n                spots=None,\n                forwards=None,\n                discount_factors=None,\n                is_call_options=None,\n                initial_volatilities=None,\n                tolerance=1e-8,\n                max_iterations=20,\n                validate_args=False,\n                dtype=None,\n                name=None):\n  """"""Computes implied volatilities from given call or put option prices.\n\n  This method applies a Newton root search algorithm to back out the implied\n  volatility given the price of either a put or a call option.\n\n  The implementation assumes that each cell in the supplied tensors corresponds\n  to an independent volatility to find.\n\n  Args:\n    prices: A real `Tensor` of any shape. The prices of the options whose\n      implied vol is to be calculated.\n    strikes: A real `Tensor` of the same dtype as `prices` and a shape that\n      broadcasts with `prices`. The strikes of the options.\n    expiries: A real `Tensor` of the same dtype as `prices` and a shape that\n      broadcasts with `prices`. The expiry for each option. The units should be\n      such that `expiry * volatility**2` is dimensionless.\n    spots: A real `Tensor` of any shape that broadcasts to the shape of the\n      `prices`. The current spot price of the underlying. Either this argument\n      or the `forwards` (but not both) must be supplied.\n    forwards: A real `Tensor` of any shape that broadcasts to the shape of\n      `prices`. The forwards to maturity. Either this argument or the `spots`\n      must be supplied but both must not be supplied.\n    discount_factors: An optional real `Tensor` of same dtype as the `prices`.\n      If not None, these are the discount factors to expiry (i.e. e^(-rT)). If\n      None, no discounting is applied (i.e. it is assumed that the undiscounted\n      option prices are provided ). If `spots` is supplied and\n      `discount_factors` is not None then this is also used to compute the\n      forwards to expiry.\n      Default value: None, equivalent to discount factors = 1.\n    is_call_options: A boolean `Tensor` of a shape compatible with `prices`.\n      Indicates whether the option is a call (if True) or a put (if False). If\n      not supplied, call options are assumed.\n    initial_volatilities: A real `Tensor` of the same shape and dtype as\n      `forwards`. The starting postions for Newton\'s method.\n      Default value: None. If not supplied, the starting point is chosen using\n        the Stefanica-Radoicic scheme. See `polya_approx.implied_vol` for\n        details.\n    tolerance: `float`. The root finder will stop where this tolerance is\n      crossed.\n    max_iterations: `int`. The maximum number of iterations of Newton\'s method.\n    validate_args: A Python bool. If True, indicates that arguments should be\n      checked for correctness before performing the computation. The checks\n      performed are: (1) Forwards and strikes are positive. (2) The prices\n        satisfy the arbitrage bounds (i.e. for call options, checks the\n        inequality `max(F-K, 0) <= Price <= F` and for put options, checks that\n        `max(K-F, 0) <= Price <= K`.). (3) Checks that the prices are not too\n        close to the bounds. It is numerically unstable to compute the implied\n        vols from options too far in the money or out of the money.\n    dtype: `tf.Dtype` to use when converting arguments to `Tensor`s. If not\n      supplied, the default TensorFlow conversion will take place. Note that\n      this argument does not do any casting for `Tensor`s or numpy arrays.\n      Default value: None.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name \'implied_vol\' is used.\n      Default value: None\n\n  Returns:\n    A 3-tuple containing the following items in order:\n       (a) implied_vols: A `Tensor` of the same dtype as `prices` and shape as\n         the common broadcasted shape of\n         `(prices, spots/forwards, strikes, expiries)`. The implied vols as\n         inferred by the algorithm. It is possible that the search may not have\n         converged or may have produced NaNs. This can be checked for using the\n         following return values.\n       (b) converged: A boolean `Tensor` of the same shape as `implied_vols`\n         above. Indicates whether the corresponding vol has converged to within\n         tolerance.\n       (c) failed: A boolean `Tensor` of the same shape as `implied_vols` above.\n         Indicates whether the corresponding vol is NaN or not a finite number.\n         Note that converged being True implies that failed will be false.\n         However, it may happen that converged is False but failed is not True.\n         This indicates the search did not converge in the permitted number of\n         iterations but may converge if the iterations are increased.\n\n  Raises:\n    ValueError: If both `forwards` and `spots` are supplied or if neither is\n      supplied.\n  """"""\n  if (spots is None) == (forwards is None):\n    raise ValueError(\'Either spots or forwards must be supplied but not both.\')\n\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'implied_vol\',\n      values=[\n          prices, spots, forwards, strikes, expiries, discount_factors,\n          is_call_options, initial_volatilities\n      ]):\n    prices = tf.convert_to_tensor(prices, dtype=dtype, name=\'prices\')\n    dtype = prices.dtype\n    strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\'strikes\')\n    expiries = tf.convert_to_tensor(expiries, dtype=dtype, name=\'expiries\')\n    if discount_factors is None:\n      discount_factors = tf.convert_to_tensor(\n          1.0, dtype=dtype, name=\'discount_factors\')\n    else:\n      discount_factors = tf.convert_to_tensor(\n          discount_factors, dtype=dtype, name=\'discount_factors\')\n\n    if forwards is not None:\n      forwards = tf.convert_to_tensor(forwards, dtype=dtype, name=\'forwards\')\n    else:\n      spots = tf.convert_to_tensor(spots, dtype=dtype, name=\'spots\')\n      forwards = spots / discount_factors\n\n    if initial_volatilities is None:\n      initial_volatilities = approx.implied_vol(\n          prices=prices,\n          strikes=strikes,\n          expiries=expiries,\n          forwards=forwards,\n          discount_factors=discount_factors,\n          is_call_options=is_call_options,\n          validate_args=validate_args)\n    else:\n      initial_volatilities = tf.convert_to_tensor(\n          initial_volatilities, dtype=dtype, name=\'initial_volatilities\')\n\n    implied_vols, converged, failed = _newton_implied_vol(\n        prices, strikes, expiries, forwards, discount_factors, is_call_options,\n        initial_volatilities, tolerance, max_iterations)\n    return implied_vols, converged, failed\n\n\n# TODO(b/139566435): Extract the Newton root finder to a separate module.\ndef newton_root_finder(value_and_grad_func,\n                       initial_values,\n                       max_iterations=20,\n                       tolerance=1e-8,\n                       dtype=None,\n                       name=None):\n  """"""Uses Newton\'s method to find roots of scalar functions of scalar variables.\n\n  This method uses Newton\'s algorithm to find values `x` such that `f(x)=0` for\n  some real-valued differentiable function `f`. Given an initial value `x_0` the\n  values are iteratively updated as:\n\n    `x_{n+1} = x_n - f(x_n) / f\'(x_n),`\n\n  for further details on Newton\'s method, see [1]. The implementation accepts\n  array-like arguments and assumes that each cell corresponds to an independent\n  scalar model.\n\n  #### Examples\n  ```python\n  # Set up the problem of finding the square roots of three numbers.\n  constants = np.array([4.0, 9.0, 16.0])\n  initial_values = np.ones(len(constants))\n  def objective_and_gradient(values):\n    objective = values**2 - constants\n    gradient = 2.0 * values\n    return objective, gradient\n\n  # Obtain and evaluate a tensor containing the roots.\n  roots = newton_vol.newton_newton_newton_root_finder(objective_and_gradient,\n                                        initial_values)\n  with tf.Session() as sess:\n    root_values, converged, failed = sess.run(roots)\n    print(root_values)  # Expected output: [ 2.  3.  4.]\n    print(converged)  # Expected output: [ True  True  True]\n    print(failed)  # Expected output: [False False False]\n  ```\n\n  #### References\n  [1] Luenberger, D.G., 1984. \'Linear and Nonlinear Programming\'. Reading, MA:\n  Addison-Wesley.\n\n  Args:\n    value_and_grad_func: A python callable that takes a `Tensor` of the same\n      shape and dtype as the `initial_values` and which returns a two-`tuple` of\n      `Tensors`, namely the objective function and the gradient evaluated at the\n      passed parameters.\n    initial_values: A real `Tensor` of any shape. The initial values of the\n      parameters to use (`x_0` in the notation above).\n    max_iterations: positive `int`, default 100. The maximum number of\n      iterations of Newton\'s method.\n    tolerance: positive scalar `Tensor`, default 1e-8. The root finder will\n      judge an element to have converged if `|f(x_n) - a|` is less than\n      `tolerance` (using the notation above), or if `x_n` becomes `nan`. When an\n      element is judged to have converged it will no longer be updated. If all\n      elements converge before `max_iterations` is reached then the root finder\n      will return early.\n    dtype: optional `tf.DType`. If supplied the `initial_values` will be coerced\n      to this data type.\n    name: `str`, default ""newton_root_finder"", to be prefixed to the name of\n      TensorFlow ops created by this function.\n\n  Returns:\n    A three tuple of `Tensor`s, each the same shape as `initial_values`. It\n    contains the found roots (same dtype as `initial_values`), a boolean\n    `Tensor` indicating whether the corresponding root results in an objective\n    function value less than the tolerance, and a boolean `Tensor` which is true\n    where the corresponding \'root\' is not finite.\n  """"""\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'newton_root_finder\',\n      values=[initial_values, tolerance]):\n\n    initial_values = tf.convert_to_tensor(\n        initial_values, dtype=dtype, name=\'initial_values\')\n\n    starting_position = (tf.constant(0, dtype=tf.int32), initial_values,\n                         tf.zeros_like(initial_values, dtype=tf.bool),\n                         tf.math.is_nan(initial_values))\n\n    def _condition(counter, parameters, converged, failed):\n      del parameters\n      early_stop = tf.reduce_all(converged | failed)\n      return ~((counter >= max_iterations) | early_stop)\n\n    def _updater(counter, parameters, converged, failed):\n      """"""Updates each parameter via Newton\'s method.""""""\n      values, gradients = value_and_grad_func(parameters)\n      # values, _ = value_and_grad_func(parameters)\n      # values_bump, _ = value_and_grad_func(parameters + 1e-6)\n      # gradients = (values_bump - values) / 1e-6\n      converged = tf.abs(values) < tolerance\n      # Used to zero out updates to cells that have converged.\n      update_mask = tf.cast(~converged, dtype=parameters.dtype)\n      increment = -update_mask * values / gradients\n      updated_parameters = parameters + increment\n      failed = ~tf.math.is_finite(updated_parameters)\n\n      return counter + 1, updated_parameters, converged, failed\n\n    return tf.while_loop(_condition, _updater, starting_position)[1:]\n\n\ndef _newton_implied_vol(prices, strikes, expiries, forwards, discount_factors,\n                        is_call_options, initial_volatilities, tolerance,\n                        max_iterations):\n  """"""Uses Newton\'s method to find Black Scholes implied volatilities of options.\n\n  Finds the volatility implied under the Black Scholes option pricing scheme for\n  a set of European options given observed market prices. The implied volatility\n  is found via application of Newton\'s algorithm for locating the root of a\n  differentiable function.\n\n  The implmentation assumes that each cell in the supplied tensors corresponds\n  to an independent volatility to find.\n\n  Args:\n    prices: A real `Tensor` of any shape. The prices of the options whose\n      implied vol is to be calculated.\n    strikes: A real `Tensor` of the same dtype as `prices` and a shape that\n      broadcasts with `prices`. The strikes of the options.\n    expiries: A real `Tensor` of the same dtype as `prices` and a shape that\n      broadcasts with `prices`. The expiry for each option. The units should be\n      such that `expiry * volatility**2` is dimensionless.\n    forwards: A real `Tensor` of any shape that broadcasts to the shape of\n      `prices`. The forwards to maturity.\n    discount_factors: An optional real `Tensor` of same dtype as the `prices`.\n      If not None, these are the discount factors to expiry (i.e. e^(-rT)). If\n      None, no discounting is applied (i.e. it is assumed that the undiscounted\n      option prices are provided ).\n    is_call_options: A boolean `Tensor` of a shape compatible with `prices`.\n      Indicates whether the option is a call (if True) or a put (if False). If\n      not supplied, call options are assumed.\n    initial_volatilities: A real `Tensor` of the same shape and dtype as\n      `forwards`. The starting postions for Newton\'s method.\n    tolerance: `float`. The root finder will stop where this tolerance is\n      crossed.\n    max_iterations: `int`. The maximum number of iterations of Newton\'s method.\n\n  Returns:\n    A three tuple of `Tensor`s, each the same shape as `forwards`. It\n    contains the implied volatilities (same dtype as `forwards`), a boolean\n    `Tensor` indicating whether the corresponding implied volatility converged,\n    and a boolean `Tensor` which is true where the corresponding implied\n    volatility is not a finite real number.\n  """"""\n  pricer = _make_black_objective_and_vega_func(prices, forwards, strikes,\n                                               expiries, is_call_options,\n                                               discount_factors)\n  results = newton_root_finder(\n      pricer,\n      initial_volatilities,\n      max_iterations=max_iterations,\n      tolerance=tolerance)\n  return results\n\n\ndef _make_black_objective_and_vega_func(prices, forwards, strikes, expiries,\n                                        is_call_options, discount_factors):\n  """"""Produces an objective and vega function for the Black Scholes model.\n\n  The returned function maps volatilities to a tuple of objective function\n  values and their gradients with respect to the volatilities. The objective\n  function is the difference between Black Scholes prices and observed market\n  prices, whereas the gradient is called vega of the option. That is:\n\n  ```\n  g(s) = (f(s) - a, f\'(s))\n  ```\n\n  Where `g` is the returned function taking volatility parameter `s`, `f` the\n  Black Scholes price with all other variables curried and `f\'` its derivative,\n  and `a` the observed market prices of the options. Hence `g` calculates the\n  information necessary for finding the volatility implied by observed market\n  prices for options with given terms using first order methods.\n\n  #### References\n  [1] Hull, J., 2018. Options, Futures, and Other Derivatives. Harlow, England.\n  Pearson. (p.358 - 361)\n\n  Args:\n    prices: A real `Tensor` of any shape. The observed market prices of the\n      assets.\n    forwards: A real `Tensor` of the same shape and dtype as `prices`. The\n      current forward prices to expiry.\n    strikes: A real `Tensor` of the same shape and dtype as `prices`. The strike\n      prices of the options.\n    expiries: A real `Tensor` of same shape and dtype as `forwards`. The expiry\n      for each option. The units should be such that `expiry * volatility**2` is\n      dimensionless.\n    is_call_options: A boolean `Tensor` of same shape and dtype as `forwards`.\n      Positive one where option is a call, negative one where option is a put.\n    discount_factors: A real `Tensor` of the same shape and dtype as `forwards`.\n      The total discount factors to apply.\n\n  Returns:\n    A function from volatilities to a Black Scholes objective and its\n    derivative (which is coincident with Vega).\n  """"""\n  dtype = prices.dtype\n  phi = tfp.distributions.Normal(\n      loc=tf.zeros(1, dtype=dtype), scale=tf.ones(1, dtype=dtype))\n  # orientations will decide the normalization strategy.\n  orientations = strikes >= forwards\n  # normalization is the greater of strikes or forwards\n  normalization = tf.where(orientations, strikes, forwards)\n  normalized_prices = prices / normalization\n  if discount_factors is not None:\n    normalized_prices /= discount_factors\n\n  units = tf.ones_like(forwards)\n  # y is 1 when strikes >= forwards and strikes/forwards otherwise\n  y = tf.where(orientations, units, strikes / forwards)\n  # x is forwards/strikes when strikes >= forwards and 1 otherwise\n  x = tf.where(orientations, forwards / strikes, units)\n  lnz = tf.math.log(forwards) - tf.math.log(strikes)\n  sqrt_t = tf.sqrt(expiries)\n  if is_call_options is not None:\n    is_call_options = tf.convert_to_tensor(is_call_options,\n                                           dtype=tf.bool,\n                                           name=\'is_call_options\')\n  def _black_objective_and_vega(volatilities):\n    """"""Calculate the Black Scholes price and vega for a given volatility.\n\n    This method returns normalized results.\n\n    Args:\n      volatilities: A real `Tensor` of same shape and dtype as `forwards`. The\n        volatility to expiry.\n\n    Returns:\n      A tuple containing (value, gradient) of the black scholes price, both of\n        which are `Tensor`s of the same shape and dtype as `volatilities`.\n    """"""\n    v = volatilities * sqrt_t\n    d1 = (lnz / v + v / 2)\n    d2 = d1 - v\n    implied_prices = x * phi.cdf(d1) - y * phi.cdf(d2)\n    if is_call_options is not None:\n      put_prices = implied_prices - x + y\n      implied_prices = tf.where(\n          tf.broadcast_to(is_call_options, tf.shape(put_prices)),\n          implied_prices, put_prices)\n    vega = x * phi.prob(d1) * sqrt_t\n    return implied_prices - normalized_prices, vega\n\n  return _black_objective_and_vega\n'"
tf_quant_finance/black_scholes/implied_vol_newton_root_test.py,5,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for implied_volatility.newton_vol.""""""\n\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass ImpliedVolNewtonTest(parameterized.TestCase, tf.test.TestCase):\n  """"""Tests for methods in newton_vol module.""""""\n\n  def test_basic_newton_finder(self):\n    """"""Tests the Newton root finder recovers the volatility on a few cases.""""""\n    forwards = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n    strikes = np.array([1.0, 2.0, 1.0, 0.5, 1.0, 1.0])\n    expiries = np.array([1.0, 1.0, 1.0, 1.0, 0.5, 2.0])\n    discounts = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n    init_vols = np.array([2.0, 0.5, 2.0, 0.5, 1.5, 1.5])\n    is_call_options = np.array([True, True, False, False, True, True])\n    volatilities = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n    prices = np.array([\n        0.38292492, 0.19061012, 0.38292492, 0.09530506, 0.27632639, 0.52049988\n    ])\n    implied_vols, converged, failed = self.evaluate(\n        tff.black_scholes.implied_vol_newton(\n            prices=prices,\n            strikes=strikes,\n            expiries=expiries,\n            forwards=forwards,\n            discount_factors=discounts,\n            is_call_options=is_call_options,\n            initial_volatilities=init_vols,\n            max_iterations=100))\n    self.assertTrue(np.all(converged))\n    self.assertFalse(np.any(failed))\n    self.assertArrayNear(volatilities, implied_vols, 1e-7)\n\n  def test_basic_radiocic_newton_combination_finder(self):\n    """"""Tests the Newton root finder recovers the volatility on a few cases.""""""\n    forwards = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n    strikes = np.array([1.0, 2.0, 1.0, 0.5, 1.0, 1.0])\n    expiries = np.array([1.0, 1.0, 1.0, 1.0, 0.5, 2.0])\n    discounts = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n    is_call_options = np.array([True, True, False, False, True, True])\n    volatilities = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n    prices = np.array([\n        0.38292492, 0.19061012, 0.38292492, 0.09530506, 0.27632639, 0.52049988\n    ])\n    implied_vols, converged, failed = self.evaluate(\n        tff.black_scholes.implied_vol_newton(\n            prices=prices,\n            strikes=strikes,\n            expiries=expiries,\n            forwards=forwards,\n            discount_factors=discounts,\n            is_call_options=is_call_options))\n    self.assertTrue(np.all(converged))\n    self.assertFalse(np.any(failed))\n    self.assertArrayNear(volatilities, implied_vols, 1e-7)\n\n  @parameterized.named_parameters(\n      # This case should hit the call lower bound since C = F - K.\n      (\'call_lower\', 0.0, 1.0, 1.0, 1.0, 1.0),\n      # This case should hit the call upper bound since C = F\n      (\'call_upper\', 1.0, 1.0, 1.0, 1.0, 1.0),\n      # This case should hit the put upper bound since C = K\n      (\'put_lower\', 1.0, 1.0, 1.0, 1.0, -1.0),\n      # This case should hit the call lower bound since C = F - K.\n      (\'put_upper\', 0.0, 1.0, 1.0, 1.0, -1.0))\n  def test_implied_vol_validate_raises(self, price, forward, strike, expiry,\n                                       option_sign):\n    """"""Tests validation errors raised where BS model assumptions violated.""""""\n    prices = np.array([price])\n    forwards = np.array([forward])\n    strikes = np.array([strike])\n    expiries = np.array([expiry])\n    is_call_options = np.array([option_sign > 0])\n    discounts = np.array([1.0])\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      self.evaluate(\n          tff.black_scholes.implied_vol_newton(\n              prices=prices,\n              strikes=strikes,\n              expiries=expiries,\n              forwards=forwards,\n              discount_factors=discounts,\n              validate_args=True,\n              is_call_options=is_call_options))\n\n  def test_implied_vol_extensive(self):\n\n    np.random.seed(135)\n    num_examples = 1000\n\n    expiries = np.linspace(0.8, 1.2, num_examples)\n    rates = np.linspace(0.03, 0.08, num_examples)\n    discount_factors = np.exp(-rates * expiries)\n    spots = np.ones(num_examples)\n    forwards = spots / discount_factors\n    strikes = np.linspace(0.8, 1.2, num_examples)\n    volatilities = np.ones_like(forwards)\n    call_options = np.random.binomial(n=1, p=0.5, size=num_examples)\n    is_call_options = np.array(call_options, dtype=np.bool)\n\n    prices = self.evaluate(\n        tff.black_scholes.option_price(\n            volatilities=volatilities,\n            strikes=strikes,\n            expiries=expiries,\n            forwards=forwards,\n            is_call_options=is_call_options,\n            discount_factors=discount_factors,\n            dtype=tf.float64))\n\n    implied_vols = self.evaluate(\n        tff.black_scholes.implied_vol_newton(\n            prices=prices,\n            strikes=strikes,\n            expiries=expiries,\n            forwards=forwards,\n            discount_factors=discount_factors,\n            is_call_options=is_call_options,\n            dtype=tf.float64,\n            max_iterations=1000,\n            tolerance=1e-8))[0]\n\n    self.assertArrayNear(volatilities, implied_vols, 1e-7)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/black_scholes/newton_root_test.py,2,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n""""""Tests for the Newton root finder in implied_vol_newton.py.""""""\n\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.black_scholes import implied_vol_newton_root as ivn\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass NewtonRootTest(parameterized.TestCase, tf.test.TestCase):\n  """"""Tests for the Newton root finder.""""""\n\n  def test_newton_root_finder(self):\n    """"""Tests that the newton root finder works on a square root example.""""""\n\n    # Set up the problem of finding the square roots of three numbers.\n    constants = np.array([4.0, 9.0, 16.0])\n    initial_values = np.ones(len(constants))\n\n    def objective_and_gradient(values):\n      objective = values**2 - constants\n      gradient = 2.0 * values\n      return objective, gradient\n\n    # Obtain and evaluate a tensor containing the roots.\n    roots = ivn.newton_root_finder(objective_and_gradient, initial_values)\n    root_values, converged, failed = self.evaluate(roots)\n\n    # Reference values.\n    roots_bench = np.array([2.0, 3.0, 4.0])\n    converged_bench = np.array([True, True, True])\n    failed_bench = np.array([False, False, False])\n\n    # Assert that the values we obtained are close to the true values.\n    np.testing.assert_array_equal(converged, converged_bench)\n    np.testing.assert_array_equal(failed, failed_bench)\n    np.testing.assert_almost_equal(root_values, roots_bench, decimal=7)\n\n  def test_failure_and_non_convergence(self):\n    """"""Tests that we can determine when the root finder has failed.""""""\n\n    # Set up the problem of finding the square roots of three numbers.\n    constants = np.array([4.0, 9.0, 16.0])\n    # Choose a bad initial position.\n    initial_values = np.zeros(len(constants))\n\n    def objective_and_gradient(values):\n      objective = values**2 - constants\n      gradient = 2.0 * values\n      return objective, gradient\n\n    # Obtain and evaluate a tensor containing the roots.\n    roots = ivn.newton_root_finder(objective_and_gradient, initial_values)\n    _, converged, failed = self.evaluate(roots)\n\n    # Reference values - we should not have converged and should have failed.\n    converged_bench = np.array([False, False, False])\n    failed_bench = np.array([True, True, True])\n\n    # Assert that the values we obtained are close to the true values.\n    np.testing.assert_array_equal(converged, converged_bench)\n    np.testing.assert_array_equal(failed, failed_bench)\n\n  def test_too_low_max_iterations(self):\n    """"""Tests that we can determine when max_iterations was too small.""""""\n\n    # Set up the problem of finding the square roots of three numbers.\n    constants = np.array([4.0, 9.0, 16.0])\n    initial_values = np.ones(len(constants))\n\n    def objective_and_gradient(values):\n      objective = values**2 - constants\n      gradient = 2.0 * values\n      return objective, gradient\n\n    # Obtain and evaluate a tensor containing the roots.\n    roots = ivn.newton_root_finder(\n        objective_and_gradient, initial_values, max_iterations=1)\n    _, converged, failed = self.evaluate(roots)\n\n    # Reference values - we should neither have converged nor failed.\n    converged_bench = np.array([False, False, False])\n    failed_bench = np.array([False, False, False])\n\n    # Assert that the values we obtained are close to the true values.\n    np.testing.assert_array_equal(converged, converged_bench)\n    np.testing.assert_array_equal(failed, failed_bench)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/black_scholes/vanilla_prices.py,80,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Black Scholes prices of a batch of European options.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\n\ndef option_price(*,\n                 volatilities,\n                 strikes,\n                 expiries,\n                 spots=None,\n                 forwards=None,\n                 discount_rates=None,\n                 continuous_dividends=None,\n                 cost_of_carries=None,\n                 discount_factors=None,\n                 is_call_options=None,\n                 dtype=None,\n                 name=None):\n  """"""Computes the Black Scholes price for a batch of call or put options.\n\n  #### Example\n\n  ```python\n    # Price a batch of 5 vanilla call options.\n    volatilities = np.array([0.0001, 102.0, 2.0, 0.1, 0.4])\n    forwards = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n    # Strikes will automatically be broadcasted to shape [5].\n    strikes = np.array([3.0])\n    # Expiries will be broadcast to shape [5], i.e. each option has strike=3\n    # and expiry = 1.\n    expiries = 1.0\n    computed_prices = tff.black_scholes.option_price(\n        volatilities=volatilities,\n        strikes=strikes,\n        expiries=expiries,\n        forwards=forwards)\n  # Expected print output of computed prices:\n  # [ 0.          2.          2.04806848  1.00020297  2.07303131]\n  ```\n\n  #### References:\n  [1] Hull, John C., Options, Futures and Other Derivatives. Pearson, 2018.\n  [2] Wikipedia contributors. Black-Scholes model. Available at:\n    https://en.wikipedia.org/w/index.php?title=Black%E2%80%93Scholes_model\n\n  Args:\n    volatilities: Real `Tensor` of any shape and dtype. The volatilities to\n      expiry of the options to price.\n    strikes: A real `Tensor` of the same dtype and compatible shape as\n      `volatilities`. The strikes of the options to be priced.\n    expiries: A real `Tensor` of same dtype and compatible shape as\n      `volatilities`. The expiry of each option. The units should be such that\n      `expiry * volatility**2` is dimensionless.\n    spots: A real `Tensor` of any shape that broadcasts to the shape of the\n      `volatilities`. The current spot price of the underlying. Either this\n      argument or the `forwards` (but not both) must be supplied.\n    forwards: A real `Tensor` of any shape that broadcasts to the shape of\n      `volatilities`. The forwards to maturity. Either this argument or the\n      `spots` must be supplied but both must not be supplied.\n    discount_rates: An optional real `Tensor` of same dtype as the\n      `volatilities` and of the shape that broadcasts with `volatilities`.\n      If not `None`, discount factors are calculated as e^(-rT),\n      where r are the discount rates, or risk free rates. At most one of\n      discount_rates and discount_factors can be supplied.\n      Default value: `None`, equivalent to r = 0 and discount factors = 1 when\n      discount_factors also not given.\n    continuous_dividends: An optional real `Tensor` of same dtype as the\n      `volatilities` and of the shape that broadcasts with `volatilities`.\n      If not `None`, `cost_of_carries` is calculated as r - q,\n      where r are the `discount_rates` and q is `continuous_dividends`. Either\n      this or `cost_of_carries` can be given.\n      Default value: `None`, equivalent to q = 0.\n    cost_of_carries: An optional real `Tensor` of same dtype as the\n      `volatilities` and of the shape that broadcasts with `volatilities`.\n      Cost of storing a physical commodity, the cost of interest paid when\n      long, or the opportunity cost, or the cost of paying dividends when short.\n      If not `None`, and `spots` is supplied, used to calculate forwards from\n      `spots`: F = e^(bT) * S, where F is the forwards price, b is the cost of\n      carries, T is expiries and S is the spot price. If `None`, value assumed\n      to be equal to the `discount_rate` - `continuous_dividends`\n      Default value: `None`, equivalent to b = r.\n    discount_factors: An optional real `Tensor` of same dtype as the\n      `volatilities`. If not `None`, these are the discount factors to expiry\n      (i.e. e^(-rT)). Mutually exclusive with discount_rate and cost_of_carry.\n      If neither is given, no discounting is applied (i.e. the undiscounted\n      option price is returned). If `spots` is supplied and `discount_factors`\n      is not `None` then this is also used to compute the forwards to expiry.\n      At most one of discount_rates and discount_factors can be supplied.\n      Default value: `None`, which maps to -log(discount_factors) / expiries\n    is_call_options: A boolean `Tensor` of a shape compatible with\n      `volatilities`. Indicates whether the option is a call (if True) or a put\n      (if False). If not supplied, call options are assumed.\n    dtype: Optional `tf.DType`. If supplied, the dtype to be used for conversion\n      of any supplied non-`Tensor` arguments to `Tensor`.\n      Default value: `None` which maps to the default dtype inferred by\n        TensorFlow.\n    name: str. The name for the ops created by this function.\n      Default value: `None` which is mapped to the default name `option_price`.\n\n  Returns:\n    option_prices: A `Tensor` of the same shape as `forwards`. The Black\n    Scholes price of the options.\n\n  Raises:\n    ValueError: If both `forwards` and `spots` are supplied or if neither is\n      supplied.\n    ValueError: If both `discount_rates` and `discount_factors` is supplied.\n    ValueError: If both `continuous_dividends` and `cost_of_carries` is\n      supplied.\n  """"""\n  if (spots is None) == (forwards is None):\n    raise ValueError(\'Either spots or forwards must be supplied but not both.\')\n  if (discount_rates is not None) and (discount_factors is not None):\n    raise ValueError(\'At most one of discount_rates and discount_factors may \'\n                     \'be supplied\')\n  if (continuous_dividends is not None) and (cost_of_carries is not None):\n    raise ValueError(\'At most one of continuous_dividends and cost_of_carries \'\n                     \'may be supplied\')\n\n  with tf.name_scope(name or \'option_price\'):\n    strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\'strikes\')\n    dtype = strikes.dtype\n    volatilities = tf.convert_to_tensor(\n        volatilities, dtype=dtype, name=\'volatilities\')\n    expiries = tf.convert_to_tensor(expiries, dtype=dtype, name=\'expiries\')\n\n    if discount_rates is not None:\n      discount_rates = tf.convert_to_tensor(\n          discount_rates, dtype=dtype, name=\'discount_rates\')\n    elif discount_factors is not None:\n      discount_rates = -tf.math.log(discount_factors) / expiries\n    else:\n      discount_rates = tf.convert_to_tensor(\n          0.0, dtype=dtype, name=\'discount_rates\')\n\n    if continuous_dividends is None:\n      continuous_dividends = tf.convert_to_tensor(\n          0.0, dtype=dtype, name=\'continuous_dividends\')\n\n    if cost_of_carries is not None:\n      cost_of_carries = tf.convert_to_tensor(\n          cost_of_carries, dtype=dtype, name=\'cost_of_carries\')\n    else:\n      cost_of_carries = discount_rates - continuous_dividends\n\n    if discount_factors is not None:\n      discount_factors = tf.convert_to_tensor(\n          discount_factors, dtype=dtype, name=\'discount_factors\')\n    else:\n      discount_factors = tf.exp(-discount_rates * expiries)\n\n    if forwards is not None:\n      forwards = tf.convert_to_tensor(forwards, dtype=dtype, name=\'forwards\')\n    else:\n      spots = tf.convert_to_tensor(spots, dtype=dtype, name=\'spots\')\n      forwards = spots * tf.exp(cost_of_carries * expiries)\n\n    sqrt_var = volatilities * tf.math.sqrt(expiries)\n    d1 = (tf.math.log(forwards / strikes) + sqrt_var * sqrt_var / 2) / sqrt_var\n    d2 = d1 - sqrt_var\n    undiscounted_calls = forwards * _ncdf(d1) - strikes * _ncdf(d2)\n    if is_call_options is None:\n      return discount_factors * undiscounted_calls\n    undiscounted_forward = forwards - strikes\n    undiscounted_puts = undiscounted_calls - undiscounted_forward\n    predicate = tf.broadcast_to(is_call_options, tf.shape(undiscounted_calls))\n    return discount_factors * tf.where(predicate, undiscounted_calls,\n                                       undiscounted_puts)\n\n\ndef barrier_price(*,\n                  volatilities,\n                  strikes,\n                  expiries,\n                  spots,\n                  barriers,\n                  rebates,\n                  discount_rates=None,\n                  continuous_dividends=None,\n                  cost_of_carries=None,\n                  is_barrier_down=None,\n                  is_knock_out=None,\n                  is_call_options=None,\n                  dtype=None,\n                  name=None):\n  """"""Prices barrier options in a Black-Scholes Model.\n\n  Function determines the analytical price for the barriers option. The\n  computation is done as in [2] where each integral is split into two matrices.\n  The first matrix contains the algebraic terms and the second matrix contains\n  the probability distribution terms. Masks are used to filter appropriate terms\n  for calculating the integral. Then a dot product of each row in the matricies\n  coupled with the masks work to calculate the prices of the barriers option.\n\n  #### Example [2], Page 154\n\n  ```python\n  dtype = np.float32\n  discount_rates = np.array([.08, .08])\n  continuous_dividends = np.array([.04, .04])\n  spots = np.array([100., 100.])\n  strikes = np.array([90., 90.])\n  barriers = np.array([95. 95.])\n  rebates = np.array([3. 3.])\n  volatilities = np.array([.25, .25])\n  expiries = np.array([.5, .5])\n  barriers_type = np.array([5, 1])\n  is_barrier_down = np.array([True, False])\n  is_knock_out = np.array([False, False])\n  is_call_option = np.array([True, True])\n\n  price = price_barriers_option(\n    discount_rates, continuous_dividends, spots, strikes,\n    barriers, rebates, volatilities,\n    expiries, is_barrier_down, is_knock_out, is_call_options)\n\n  # Expected output\n  #  `Tensor` with values [9.024, 7.7627]\n  ```\n\n  #### References\n\n  [1]: Lee Clewlow, Javier Llanos, Chris Strickland, Caracas Venezuela\n  Pricing Exotic Options in a Black-Scholes World, 1994\n  https://warwick.ac.uk/fac/soc/wbs/subjects/finance/research/wpaperseries/1994/94-54.pdf\n\n  [2]: Espen Gaarder Haug, The Complete Guide to Option Pricing Formulas,\n  2nd Edition, 1997\n\n  Args:\n    volatilities: Real `Tensor` of any shape and dtype. The volatilities to\n      expiry of the options to price.\n    strikes: A real `Tensor` of the same dtype and compatible shape as\n      `volatilities`. The strikes of the options to be priced.\n    expiries: A real `Tensor` of same dtype and compatible shape as\n      `volatilities`. The expiry of each option. The units should be such that\n      `expiry * volatility**2` is dimensionless.\n    spots: A real `Tensor` of any shape that broadcasts to the shape of the\n      `volatilities`. The current spot price of the underlying.\n    barriers: A real `Tensor` of same dtype as the `volatilities` and of the\n      shape that broadcasts with `volatilities`. The barriers of each option.\n    rebates: A real `Tensor` of same dtype as the `volatilities` and of the\n      shape that broadcasts with `volatilities`. A rebates contingent upon\n      reaching the barriers price.\n    discount_rates: A real `Tensor` of same dtype as the\n      `volatilities` and of the shape that broadcasts with `volatilities`.\n      Discount rates, or risk free rates.\n      Default value: `None`, equivalent to discount_rate = 0.\n    continuous_dividends: A real `Tensor` of same dtype as the\n      `volatilities` and of the shape that broadcasts with `volatilities`.\n      Either this or `cost_of_carries` can be given. If `None`,\n      `cost_of_carries` must be supplied.\n      Default value: `None`, calculated from `cost_of_carries`.\n    cost_of_carries: A optional real `Tensor` of same dtype as the\n      `volatilities` and of the shape that broadcasts with `volatilities`.\n      Cost of storing a physical commodity, the cost of interest paid when\n      long, or the opportunity cost, o the cost of paying dividends when short.\n      If not `None`, `continuous_dividends` is calculated as r - c,\n      where r are the `discount_rates` and c is `cost_of_carries`.\n    is_barrier_down: A real `Tensor` of `boolean` values and of the shape\n      that broadcasts with `volatilities`. True if barrier is below asset\n      price at expiration.\n      Default value: `True`.\n    is_knock_out: A real `Tensor` of `boolean` values and of the shape\n      that broadcasts with `volatilities`. True if option is knock out\n      else false.\n      Default value: `True`.\n    is_call_options: A real `Tensor` of `boolean` values and of the shape\n      that broadcasts with `volatilities`. True if option is call else\n      false.\n      Default value: `True`.\n    dtype: Optional `tf.DType`. If supplied, the dtype to be used for conversion\n      of any supplied non-`Tensor` arguments to `Tensor`.\n      Default value: `None` which maps to the default dtype inferred by\n      TensorFlow.\n    name: str. The name for the ops created by this function.\n      Default value: `None` which is mapped to the default name `barrier_price`.\n  Returns:\n    option_prices: A `Tensor` of same shape as `spots`. The approximate price of\n    the barriers option under black scholes.\n\n  """"""\n  if (continuous_dividends is None) == (cost_of_carries is None):\n    raise ValueError(\'At most one of continuous_dividends and cost of carries \'\n                     \'may be supplied\')\n  with tf.name_scope(name or \'barrier_price\'):\n    spots = tf.convert_to_tensor(spots, dtype=dtype, name=\'spots\')\n    dtype = spots.dtype\n    strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\'strikes\')\n    volatilities = tf.convert_to_tensor(\n        volatilities, dtype=dtype, name=\'volatilities\')\n    expiries = tf.convert_to_tensor(expiries, dtype=dtype, name=\'expiries\')\n    barriers = tf.convert_to_tensor(barriers, dtype=dtype, name=\'barriers\')\n    rebates = tf.convert_to_tensor(rebates, dtype=dtype, name=\'rebates\')\n\n    # Convert all to tensor and enforce float dtype where required\n    if discount_rates is not None:\n      discount_rates = tf.convert_to_tensor(\n          discount_rates, dtype=dtype, name=\'discount_rates\')\n    else:\n      discount_rates = tf.convert_to_tensor(\n          1, dtype=dtype, name=\'discount_rates\')\n\n    if continuous_dividends is not None:\n      continuous_dividends = tf.convert_to_tensor(\n          continuous_dividends, dtype=dtype, name=\'continuous_dividends\')\n\n    if cost_of_carries is not None:\n      continuous_dividends = tf.convert_to_tensor(\n          discount_rates - cost_of_carries, dtype=dtype,\n          name=\'continuous_dividends\')\n\n    if is_barrier_down is None:\n      is_barrier_down = tf.constant(1, name=\'is_barrier_down\')\n    else:\n      is_barrier_down = tf.convert_to_tensor(is_barrier_down, dtype=tf.bool,\n                                             name=\'is_barrier_down\')\n      is_barrier_down = tf.where(is_barrier_down, 1, 0)\n    if is_knock_out is None:\n      is_knock_out = tf.constant(1, name=\'is_knock_out\')\n    else:\n      is_knock_out = tf.convert_to_tensor(is_knock_out, dtype=tf.bool,\n                                          name=\'is_knock_out\')\n      is_knock_out = tf.where(is_knock_out, 1, 0)\n    if is_call_options is None:\n      is_call_options = tf.constant(1, name=\'is_call_options\')\n    else:\n      is_call_options = tf.convert_to_tensor(is_call_options, dtype=tf.bool,\n                                             name=\'is_call_options\')\n      is_call_options = tf.where(is_call_options, 1, 0)\n    # Indices which range from 0-7 are used to select the appropriate\n    # mask for each barrier\n    indices = tf.bitwise.left_shift(\n        is_barrier_down, 2) + tf.bitwise.left_shift(\n            is_knock_out, 1) + is_call_options\n\n    # Masks select the appropriate terms for integral approximations\n    # Integrals are seperated by algebraic terms and probability\n    # distribution terms. This give 12 different terms per matrix\n    # (6 integrals, 2 terms each)\n    # shape = [8, 12]\n    mask_matrix_greater_strike = tf.constant([\n        [1, 1, -1, -1, 0, 0, 1, 1, 1, 1, 0, 0],  # up and in put\n        [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],  # up and in call\n        [0, 0, 1, 1, 0, 0, -1, -1, 0, 0, 1, 1],  # up and out put\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],  # up and out call\n        [0, 0, 1, 1, -1, -1, 1, 1, 0, 0, 1, 1],  # down and in put\n        [0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0],  # down and in call\n        [1, 1, -1, -1, 1, 1, -1, -1, 0, 0, 1, 1],  # down and out put\n        [1, 1, 0, 0, -1, -1, 0, 0, 0, 0, 1, 1]])  # down and out call\n\n    mask_matrix_lower_strike = tf.constant([\n        [0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0],  # up and in put\n        [0, 0, 1, 1, -1, -1, 1, 1, 1, 1, 0, 0],  # up and in call\n        [1, 1, 0, 0, -1, -1, 0, 0, 0, 0, 1, 1],  # up and out put\n        [1, 1, -1, -1, 1, 1, -1, -1, 0, 0, 1, 1],  # up and out call\n        [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],  # down and in put\n        [1, 1, -1, -1, 0, 0, 1, 1, 1, 1, 0, 0],  # down and in call\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],  # down and out put\n        [0, 0, 1, 1, 0, 0, -1, -1, 0, 0, 1, 1]])  # down and out call\n\n    # Create masks\n    # Masks are shape [strikes.shape, 12]\n    masks_lower = tf.gather(mask_matrix_lower_strike, indices, axis=0)\n    masks_greater = tf.gather(mask_matrix_greater_strike, indices, axis=0)\n    strikes_greater = tf.expand_dims(strikes > barriers, axis=-1)\n    masks = tf.where(strikes_greater, masks_greater, masks_lower)\n    masks = tf.cast(masks, dtype=dtype)\n    one = tf.constant(1, dtype=dtype)\n    call_or_put = tf.cast(tf.where(tf.equal(is_call_options, 0), -one, one),\n                          dtype=dtype)\n    below_or_above = tf.cast(tf.where(tf.equal(is_barrier_down, 0), -one, one),\n                             dtype=dtype)\n\n    # Calculate params for integrals\n    sqrt_var = volatilities * tf.math.sqrt(expiries)\n    mu = (discount_rates - continuous_dividends) - ((volatilities**2) / 2)\n    lamda = 1 + (mu / (volatilities**2))\n    x = (tf.math.log(spots / strikes) / (sqrt_var)) + (lamda * sqrt_var)\n    x1 = (tf.math.log(spots / barriers) / (sqrt_var)) + (lamda * sqrt_var)\n    y = (tf.math.log((barriers**2) / (spots * strikes)) / (\n        sqrt_var)) + (lamda * sqrt_var)\n    y1 = (tf.math.log(barriers / spots) / (sqrt_var)) + (lamda * sqrt_var)\n    b = ((mu**2) + (2 * (volatilities**2) * discount_rates)) / (volatilities**2)\n    z = (tf.math.log(barriers / spots) / (sqrt_var)) + (b * sqrt_var)\n    a = mu / (volatilities**2)\n\n    # Other params used for integrals\n    discount_rates_exponent = tf.math.exp(-discount_rates * expiries,\n                                          name=\'discount_rates_exponent\')\n    continuous_dividends_exponent = tf.math.exp(\n        -continuous_dividends * expiries,\n        name=\'continuous_dividends_exponent\')\n    barriers_ratio = tf.math.divide(barriers, spots, name=\'barriers_ratio\')\n    spots_term = call_or_put * spots * continuous_dividends_exponent\n    strikes_term = call_or_put * strikes * discount_rates_exponent\n\n    # rank is used to stack elements and reduce_sum\n    strike_rank = len(strikes.shape)\n\n    # Constructing Matrix with first and second algebraic terms for each\n    # integral [strike.shape, 12]\n    terms_mat = tf.stack(\n        (spots_term, -strikes_term,\n         spots_term, -strikes_term,\n         spots_term * (barriers_ratio**(2 * lamda)),\n         -strikes_term * (barriers_ratio**((2 * lamda) - 2)),\n         spots_term * (barriers_ratio**(2 * lamda)),\n         -strikes_term * (barriers_ratio**((2 * lamda) - 2)),\n         rebates * discount_rates_exponent,\n         -rebates * discount_rates_exponent * (\n             barriers_ratio**((2 * lamda) - 2)),\n         rebates * (barriers_ratio**(a + b)),\n         rebates * (barriers_ratio**(a - b))),\n        name=\'term_matrix\', axis=strike_rank)\n\n    # Constructing Matrix with first and second norm for each integral\n    # [strikes.shape, 12]\n    cdf_mat = tf.stack(\n        (call_or_put * x,\n         call_or_put * (x - sqrt_var),\n         call_or_put * x1,\n         call_or_put * (x1 - sqrt_var),\n         below_or_above * y,\n         below_or_above * (y - sqrt_var),\n         below_or_above * y1,\n         below_or_above * (y1 - sqrt_var),\n         below_or_above * (x1 - sqrt_var),\n         below_or_above * (y1 - sqrt_var),\n         below_or_above * z,\n         below_or_above * (z - (2 * b * sqrt_var))),\n        name=\'cdf_matrix\', axis=strike_rank)\n    cdf_mat = _ncdf(cdf_mat)\n    # Calculating and returning price for each option\n    return tf.reduce_sum(masks * terms_mat * cdf_mat, axis=strike_rank)\n\n\n# TODO(b/154806390): Binary price signature should be the same as that of the\n# vanilla price.\ndef binary_price(*,\n                 volatilities,\n                 strikes,\n                 expiries,\n                 spots=None,\n                 forwards=None,\n                 discount_factors=None,\n                 is_call_options=None,\n                 dtype=None,\n                 name=None):\n  """"""Computes the Black Scholes price for a batch of binary call or put options.\n\n  The binary call (resp. put) option priced here is that which pays off a unit\n  of cash if the underlying asset has a value greater (resp. smaller) than the\n  strike price at expiry. Hence the binary option price is the discounted\n  probability that the asset will end up higher (resp. lower) than the\n  strike price at expiry.\n\n  #### Example\n\n  ```python\n    # Price a batch of 5 binary call options.\n    volatilities = np.array([0.0001, 102.0, 2.0, 0.1, 0.4])\n    forwards = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n    # Strikes will automatically be broadcasted to shape [5].\n    strikes = np.array([3.0])\n    # Expiries will be broadcast to shape [5], i.e. each option has strike=3\n    # and expiry = 1.\n    expiries = 1.0\n    computed_prices = tff.black_scholes.binary_price(\n        volatilities=volatilities,\n        strikes=strikes,\n        expiries=expiries,\n        forwards=forwards)\n  # Expected print output of prices:\n  # [0.         0.         0.15865525 0.99764937 0.85927418]\n  ```\n\n  #### References:\n\n  [1] Hull, John C., Options, Futures and Other Derivatives. Pearson, 2018.\n  [2] Wikipedia contributors. Binary option. Available at:\n  https://en.wikipedia.org/w/index.php?title=Binary_option\n\n  Args:\n    volatilities: Real `Tensor` of any shape and dtype. The volatilities to\n      expiry of the options to price.\n    strikes: A real `Tensor` of the same dtype and compatible shape as\n      `volatilities`. The strikes of the options to be priced.\n    expiries: A real `Tensor` of same dtype and compatible shape as\n      `volatilities`. The expiry of each option. The units should be such that\n      `expiry * volatility**2` is dimensionless.\n    spots: A real `Tensor` of any shape that broadcasts to the shape of the\n      `volatilities`. The current spot price of the underlying. Either this\n      argument or the `forwards` (but not both) must be supplied.\n    forwards: A real `Tensor` of any shape that broadcasts to the shape of\n      `volatilities`. The forwards to maturity. Either this argument or the\n      `spots` must be supplied but both must not be supplied.\n    discount_factors: An optional real `Tensor` of same dtype as the\n      `volatilities`. If not None, these are the discount factors to expiry\n      (i.e. e^(-rT)). If None, no discounting is applied (i.e. the undiscounted\n      option price is returned). If `spots` is supplied and `discount_factors`\n      is not None then this is also used to compute the forwards to expiry.\n      Default value: None, equivalent to discount factors = 1.\n    is_call_options: A boolean `Tensor` of a shape compatible with\n      `volatilities`. Indicates whether the option is a call (if True) or a put\n      (if False). If not supplied, call options are assumed.\n    dtype: Optional `tf.DType`. If supplied, the dtype to be used for conversion\n      of any supplied non-`Tensor` arguments to `Tensor`.\n      Default value: None which maps to the default dtype inferred by TensorFlow\n        (float32).\n    name: str. The name for the ops created by this function.\n      Default value: None which is mapped to the default name `binary_price`.\n\n  Returns:\n    binary_prices: A `Tensor` of the same shape as `forwards`. The Black\n    Scholes price of the binary options.\n\n  Raises:\n    ValueError: If both `forwards` and `spots` are supplied or if neither is\n      supplied.\n  """"""\n  if (spots is None) == (forwards is None):\n    raise ValueError(\'Either spots or forwards must be supplied but not both.\')\n\n  with tf.name_scope(name or \'binary_price\'):\n    strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\'strikes\')\n    dtype = strikes.dtype\n    volatilities = tf.convert_to_tensor(\n        volatilities, dtype=dtype, name=\'volatilities\')\n    expiries = tf.convert_to_tensor(expiries, dtype=dtype, name=\'expiries\')\n\n    if discount_factors is None:\n      discount_factors = tf.convert_to_tensor(\n          1.0, dtype=dtype, name=\'discount_factors\')\n    else:\n      discount_factors = tf.convert_to_tensor(\n          discount_factors, dtype=dtype, name=\'discount_factors\')\n\n    if forwards is not None:\n      forwards = tf.convert_to_tensor(forwards, dtype=dtype, name=\'forwards\')\n    else:\n      spots = tf.convert_to_tensor(spots, dtype=dtype, name=\'spots\')\n      forwards = spots / discount_factors\n\n    sqrt_var = volatilities * tf.math.sqrt(expiries)\n    d1 = (tf.math.log(forwards / strikes) + sqrt_var * sqrt_var / 2) / sqrt_var\n    d2 = d1 - sqrt_var\n    undiscounted_calls = _ncdf(d2)\n    if is_call_options is None:\n      return discount_factors * undiscounted_calls\n    is_call_options = tf.convert_to_tensor(is_call_options,\n                                           dtype=tf.bool,\n                                           name=\'is_call_options\')\n    undiscounted_puts = 1 - undiscounted_calls\n    predicate = tf.broadcast_to(is_call_options, tf.shape(undiscounted_calls))\n    return discount_factors * tf.where(predicate, undiscounted_calls,\n                                       undiscounted_puts)\n\n\ndef _ncdf(x):\n  return (tf.math.erf(x / _SQRT_2) + 1) / 2\n\n\n_SQRT_2 = np.sqrt(2.0, dtype=np.float64)\n'"
tf_quant_finance/black_scholes/vanilla_prices_test.py,10,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n""""""Tests for vanilla_price.""""""\n\nimport functools\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass VanillaPrice(parameterized.TestCase, tf.test.TestCase):\n  """"""Tests for methods for the vanilla pricing module.""""""\n\n  def test_option_prices(self):\n    """"""Tests that the BS prices are correct.""""""\n    forwards = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n    strikes = np.array([3.0, 3.0, 3.0, 3.0, 3.0])\n    volatilities = np.array([0.0001, 102.0, 2.0, 0.1, 0.4])\n    expiries = 1.0\n    computed_prices = self.evaluate(\n        tff.black_scholes.option_price(\n            volatilities=volatilities,\n            strikes=strikes,\n            expiries=expiries,\n            forwards=forwards))\n    expected_prices = np.array(\n        [0.0, 2.0, 2.0480684764112578, 1.0002029716043364, 2.0730313058959933])\n    self.assertArrayNear(expected_prices, computed_prices, 1e-10)\n\n  def test_price_zero_vol(self):\n    """"""Tests that zero volatility is handled correctly.""""""\n    # If the volatility is zero, the option\'s value should be correct.\n    forwards = np.array([1.0, 1.0, 1.0, 1.0])\n    strikes = np.array([1.1, 0.9, 1.1, 0.9])\n    volatilities = np.array([0.0, 0.0, 0.0, 0.0])\n    expiries = 1.0\n    is_call_options = np.array([True, True, False, False])\n    expected_prices = np.array([0.0, 0.1, 0.1, 0.0])\n    computed_prices = self.evaluate(\n        tff.black_scholes.option_price(\n            volatilities=volatilities,\n            strikes=strikes,\n            expiries=expiries,\n            forwards=forwards,\n            is_call_options=is_call_options))\n    self.assertArrayNear(expected_prices, computed_prices, 1e-10)\n\n  def test_price_zero_expiry(self):\n    """"""Tests that zero expiry is correctly handled.""""""\n    # If the expiry is zero, the option\'s value should be correct.\n    forwards = np.array([1.0, 1.0, 1.0, 1.0])\n    strikes = np.array([1.1, 0.9, 1.1, 0.9])\n    volatilities = np.array([0.1, 0.2, 0.5, 0.9])\n    expiries = 0.0\n    is_call_options = np.array([True, True, False, False])\n    expected_prices = np.array([0.0, 0.1, 0.1, 0.0])\n    computed_prices = self.evaluate(\n        tff.black_scholes.option_price(\n            volatilities=volatilities,\n            strikes=strikes,\n            expiries=expiries,\n            forwards=forwards,\n            is_call_options=is_call_options))\n    self.assertArrayNear(expected_prices, computed_prices, 1e-10)\n\n  def test_price_long_expiry_calls(self):\n    """"""Tests that very long expiry call option behaves like the asset.""""""\n    forwards = np.array([1.0, 1.0, 1.0, 1.0])\n    strikes = np.array([1.1, 0.9, 1.1, 0.9])\n    volatilities = np.array([0.1, 0.2, 0.5, 0.9])\n    expiries = 1e10\n    expected_prices = forwards\n    computed_prices = self.evaluate(\n        tff.black_scholes.option_price(\n            volatilities=volatilities,\n            strikes=strikes,\n            expiries=expiries,\n            forwards=forwards))\n    self.assertArrayNear(expected_prices, computed_prices, 1e-10)\n\n  def test_price_long_expiry_puts(self):\n    """"""Tests that very long expiry put option is worth the strike.""""""\n    forwards = np.array([1.0, 1.0, 1.0, 1.0])\n    strikes = np.array([0.1, 10.0, 3.0, 0.0001])\n    volatilities = np.array([0.1, 0.2, 0.5, 0.9])\n    expiries = 1e10\n    expected_prices = strikes\n    computed_prices = self.evaluate(\n        tff.black_scholes.option_price(\n            volatilities=volatilities,\n            strikes=strikes,\n            expiries=expiries,\n            forwards=forwards,\n            is_call_options=False))\n    self.assertArrayNear(expected_prices, computed_prices, 1e-10)\n\n  def test_price_vol_and_expiry_scaling(self):\n    """"""Tests that the price is invariant under vol->k vol, T->T/k**2.""""""\n    np.random.seed(1234)\n    n = 20\n    forwards = np.exp(np.random.randn(n))\n    volatilities = np.exp(np.random.randn(n) / 2)\n    strikes = np.exp(np.random.randn(n))\n    expiries = np.exp(np.random.randn(n))\n    scaling = 5.0\n    base_prices = self.evaluate(\n        tff.black_scholes.option_price(\n            volatilities=volatilities,\n            strikes=strikes,\n            expiries=expiries,\n            forwards=forwards))\n    scaled_prices = self.evaluate(\n        tff.black_scholes.option_price(\n            volatilities=volatilities * scaling,\n            strikes=strikes,\n            expiries=expiries / scaling / scaling,\n            forwards=forwards))\n    self.assertArrayNear(base_prices, scaled_prices, 1e-10)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32\n      }, {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64},\n  )\n  def test_option_prices_detailed_discount(self, dtype):\n    """"""Tests the prices with discount_rates and cost_of_carries are.""""""\n    spots = np.array([80.0, 90.0, 100.0, 110.0, 120.0] * 2)\n    strikes = np.array([100.0] * 10)\n    discount_rates = 0.08\n    volatilities = 0.2\n    expiries = 0.25\n\n    is_call_options = np.array([True] * 5 + [False] * 5)\n    cost_of_carries = -0.04\n    computed_prices = self.evaluate(\n        tff.black_scholes.option_price(\n            volatilities=volatilities,\n            strikes=strikes,\n            expiries=expiries,\n            spots=spots,\n            discount_rates=discount_rates,\n            cost_of_carries=cost_of_carries,\n            is_call_options=is_call_options,\n            dtype=dtype))\n    expected_prices = np.array([0.03, 0.57, 3.42, 9.85, 18.62, 20.41, 11.25,\n                                4.40, 1.12, 0.18])\n    self.assertArrayNear(expected_prices, computed_prices, 5e-3)\n\n  def test_binary_prices(self):\n    """"""Tests that the BS binary option prices are correct.""""""\n    forwards = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n    strikes = np.array([3.0, 3.0, 3.0, 3.0, 3.0])\n    volatilities = np.array([0.0001, 102.0, 2.0, 0.1, 0.4])\n    expiries = 1.0\n    computed_prices = self.evaluate(\n        tff.black_scholes.binary_price(\n            volatilities=volatilities,\n            strikes=strikes,\n            expiries=expiries,\n            forwards=forwards))\n    expected_prices = np.array([0.0, 0.0, 0.15865525, 0.99764937, 0.85927418])\n    self.assertArrayNear(expected_prices, computed_prices, 1e-8)\n\n  def test_binary_prices_bulk(self):\n    """"""Tests unit of cash binary option pricing over a wide range of settings.\n\n    Uses the fact that if the underlying follows a geometric brownian motion\n    then, given the mean on the exponential scale and the variance on the log\n    scale, the mean on the log scale is known. In particular for underlying S\n    with forward price F, strike K, volatility sig, and expiry T:\n\n    log(S) ~ N(log(F) - sig^2 T, sig^2 T)\n\n    The price of the binary call option is the discounted probability that S\n    will be greater than K at expiry (and for a put option, less than K). Since\n    quantiles are preserved under monotonic transformations we can find this\n    probability on the log scale. This provides an alternate calculation for the\n    same price which we can use to corroborate the standard method.\n    """"""\n    np.random.seed(321)\n    num_examples = 1000\n    forwards = np.exp(np.random.normal(size=num_examples))\n    strikes = np.exp(np.random.normal(size=num_examples))\n    volatilities = np.exp(np.random.normal(size=num_examples))\n    expiries = np.random.gamma(shape=1.0, scale=1.0, size=num_examples)\n    log_scale = np.sqrt(expiries) * volatilities\n    log_loc = np.log(forwards) - 0.5 * log_scale**2\n    call_options = np.random.binomial(n=1, p=0.5, size=num_examples)\n    discount_factors = np.random.beta(a=1.0, b=1.0, size=num_examples)\n\n    cdf_values = self.evaluate(\n        tfp.distributions.Normal(loc=log_loc,\n                                 scale=log_scale).cdf(np.log(strikes)))\n\n    expected_prices = discount_factors * (\n        call_options + ((-1.0)**call_options) * cdf_values)\n\n    is_call_options = np.array(call_options, dtype=np.bool)\n    computed_prices = self.evaluate(\n        tff.black_scholes.binary_price(\n            volatilities=volatilities,\n            strikes=strikes,\n            expiries=expiries,\n            forwards=forwards,\n            is_call_options=is_call_options,\n            discount_factors=discount_factors))\n    self.assertArrayNear(expected_prices, computed_prices, 1e-10)\n\n  def test_binary_vanilla_call_consistency(self):\n    r""""""Tests code consistency through relationship of binary and vanilla prices.\n\n    With forward F, strike K, discount rate r, and expiry T, a vanilla call\n    option should have price CV:\n\n    $$ VC(K) = e^{-rT}( N(d_1)F - N(d_2)K ) $$\n\n    A unit of cash paying binary call option should have price BC:\n\n    $$ BC(K) = e^{-rT} N(d_2) $$\n\n    Where d_1 and d_2 are standard Black-Scholes quanitities and depend on K\n    through the ratio F/K. Hence for a small increment e:\n\n    $$ (VC(K + e) - Vc(K))/e \\approx -N(d_2)e^{-rT} = -BC(K + e) $$\n\n    Similarly, for a vanilla put:\n\n    $$ (VP(K + e) - VP(K))/e \\approx N(-d_2)e^{-rT} = BP(K + e) $$\n\n    This enables a test for consistency of pricing between vanilla and binary\n    options prices.\n    """"""\n    np.random.seed(135)\n    num_examples = 1000\n    forwards = np.exp(np.random.normal(size=num_examples))\n    strikes_0 = np.exp(np.random.normal(size=num_examples))\n    epsilon = 1e-8\n    strikes_1 = strikes_0 + epsilon\n    volatilities = np.exp(np.random.normal(size=num_examples))\n    expiries = np.random.gamma(shape=1.0, scale=1.0, size=num_examples)\n    call_options = np.random.binomial(n=1, p=0.5, size=num_examples)\n    is_call_options = np.array(call_options, dtype=np.bool)\n    discount_factors = np.ones_like(forwards)\n\n    option_prices_0 = self.evaluate(\n        tff.black_scholes.option_price(\n            volatilities=volatilities,\n            strikes=strikes_0,\n            expiries=expiries,\n            forwards=forwards,\n            is_call_options=is_call_options))\n\n    option_prices_1 = self.evaluate(\n        tff.black_scholes.option_price(\n            volatilities=volatilities,\n            strikes=strikes_1,\n            expiries=expiries,\n            forwards=forwards,\n            is_call_options=is_call_options))\n\n    binary_approximation = (-1.0)**call_options * (option_prices_1 -\n                                                   option_prices_0) / epsilon\n\n    binary_prices = self.evaluate(\n        tff.black_scholes.binary_price(\n            volatilities=volatilities,\n            strikes=strikes_1,\n            expiries=expiries,\n            forwards=forwards,\n            is_call_options=is_call_options,\n            discount_factors=discount_factors))\n\n    self.assertArrayNear(binary_approximation, binary_prices, 1e-6)\n\n  def test_binary_vanilla_consistency_exact(self):\n    """"""Tests that the binary price is the negative gradient of vanilla price.""""""\n\n    # The binary call option payoff is 1 when spot > strike and 0 otherwise.\n    # This payoff is the proportional to the gradient of the payoff of a vanilla\n    # call option (max(S-K, 0)) with respect to K. This test verifies that this\n    # relationship is satisfied. A similar relation holds true between vanilla\n    # puts and binary puts.\n    dtype = np.float64\n    strikes = tf.constant([1.0, 2.0], dtype=dtype)\n    spots = tf.constant([1.5, 1.5], dtype=dtype)\n    expiries = tf.constant([2.1, 1.3], dtype=dtype)\n    discount_rates = tf.constant([0.03, 0.04], dtype=dtype)\n    discount_factors = tf.exp(-discount_rates * expiries)\n    is_call_options = tf.constant([True, False])\n    volatilities = tf.constant([0.3, 0.4], dtype=dtype)\n    actual_binary_price = self.evaluate(\n        tff.black_scholes.binary_price(\n            volatilities=volatilities,\n            strikes=strikes,\n            expiries=expiries,\n            spots=spots,\n            discount_factors=discount_factors,\n            is_call_options=is_call_options))\n    price_fn = functools.partial(\n        tff.black_scholes.option_price,\n        volatilities=volatilities,\n        spots=spots,\n        expiries=expiries,\n        discount_rates=discount_rates,\n        is_call_options=is_call_options)\n    implied_binary_price = tff.math.fwd_gradient(lambda x: price_fn(strikes=x),\n                                                 strikes)\n    implied_binary_price = self.evaluate(\n        tf.where(is_call_options, -implied_binary_price, implied_binary_price))\n    self.assertArrayNear(implied_binary_price, actual_binary_price, 1e-10)\n    return (90.0, 105, 1.4653, False, False, False, 0)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'ScalarInputsUIP\',\n          \'volatilities\': 0.25,\n          \'strikes\': 90.0,\n          \'expiries\': 0.5,\n          \'spots\': 100.0,\n          \'discount_rates\': 0.08,\n          \'continuous_dividends\': 0.04,\n          \'barriers\': 105.0,\n          \'rebates\': 3.0,\n          \'is_barrier_down\': False,\n          \'is_knock_out\': False,\n          \'is_call_options\': False,\n          \'expected_price\': 1.4653,\n      },\n      {\n          \'testcase_name\': \'VectorInputs\',\n          \'volatilities\': [.25, .25, .25, .25, .25, .25, .25, .25],\n          \'strikes\': [90., 90., 90., 90., 90., 90., 90., 90.],\n          \'expiries\': [.5, .5, .5, .5, .5, .5, .5, .5],\n          \'spots\': [100., 100., 100., 100., 100., 100., 100., 100.],\n          \'discount_rates\': [.08, .08, .08, .08, .08, .08, .08, .08],\n          \'continuous_dividends\': [.04, .04, .04, .04, .04, .04, .04, .04],\n          \'barriers\': [95., 95., 105., 105., 95., 105., 95., 105.],\n          \'rebates\': [3., 3., 3., 3., 3., 3., 3., 3.],\n          \'is_barrier_down\': [\n              True, True, False, False, True, False, True, False],\n          \'is_knock_out\': [\n              True, False, True, False, True, True, False, False],\n          \'is_call_options\': [\n              True, True, True, True, False, False, False, False],\n          \'expected_price\': [\n              9.024, 7.7627, 2.6789, 14.1112, 2.2798, 3.7760, 2.95586, 1.4653],\n      },\n      {\n          \'testcase_name\': \'MatrixInputs\',\n          \'volatilities\': [[.25, .25], [.25, .25], [.25, .25], [.25, .25]],\n          \'strikes\': [[90., 90.], [90., 90.], [90., 90.], [90., 90.]],\n          \'expiries\': [[.5, .5], [.5, .5], [.5, .5], [.5, .5]],\n          \'spots\': [[100., 100.], [100., 100.], [100., 100.], [100., 100.]],\n          \'discount_rates\': [[.08, .08], [.08, .08], [.08, .08], [.08, .08]],\n          \'continuous_dividends\': [\n              [.04, .04], [.04, .04], [.04, .04], [.04, .04]\n          ],\n          \'barriers\': [[95., 95.], [105., 105.], [95., 105.], [95., 105.]],\n          \'rebates\': [[3., 3.], [3., 3.], [3., 3.], [3., 3.]],\n          \'is_barrier_down\': [\n              [True, True],\n              [False, False],\n              [True, False],\n              [True, False]\n          ],\n          \'is_knock_out\': [\n              [True, False],\n              [True, False],\n              [True, True],\n              [False, False]\n          ],\n          \'is_call_options\': [\n              [True, True],\n              [True, True],\n              [False, False],\n              [False, False]\n          ],\n          \'expected_price\': [\n              [9.024, 7.7627],\n              [2.6789, 14.1112],\n              [2.2798, 3.7760],\n              [2.95586, 1.4653]\n          ],\n      },\n      {\n          \'testcase_name\': \'cost_of_carries\',\n          \'volatilities\': 0.25,\n          \'strikes\': 90.0,\n          \'expiries\': 0.5,\n          \'spots\': 100.0,\n          \'discount_rates\': 0.08,\n          \'cost_of_carries\': 0.04,\n          \'barriers\': 105.0,\n          \'rebates\': 3.0,\n          \'is_barrier_down\': False,\n          \'is_knock_out\': False,\n          \'is_call_options\': False,\n          \'expected_price\': 1.4653,\n      })\n  def test_barrier_option(self, *,\n                          volatilities,\n                          strikes,\n                          expiries,\n                          spots,\n                          discount_rates,\n                          barriers,\n                          rebates,\n                          is_barrier_down,\n                          is_knock_out,\n                          is_call_options,\n                          expected_price,\n                          continuous_dividends=None,\n                          cost_of_carries=None):\n    """"""Computes test barrier option prices for the parameterized inputs.""""""\n    # The input values are from examples in the following textbook:\n    # The Complete guide to Option Pricing Formulas, 2nd Edition, Page 154\n    if cost_of_carries is not None:\n      price = tff.black_scholes.barrier_price(\n          volatilities=volatilities, strikes=strikes,\n          expiries=expiries, spots=spots,\n          discount_rates=discount_rates,\n          cost_of_carries=cost_of_carries,\n          continuous_dividends=continuous_dividends,\n          barriers=barriers, rebates=rebates,\n          is_barrier_down=is_barrier_down,\n          is_knock_out=is_knock_out,\n          is_call_options=is_call_options)\n    else:\n      price = tff.black_scholes.barrier_price(\n          volatilities=volatilities, strikes=strikes,\n          expiries=expiries, spots=spots,\n          discount_rates=discount_rates,\n          continuous_dividends=continuous_dividends,\n          barriers=barriers, rebates=rebates,\n          is_barrier_down=is_barrier_down,\n          is_knock_out=is_knock_out,\n          is_call_options=is_call_options)\n    self.assertAllClose(price, expected_price, 10e-3)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32\n      }, {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64\n      })\n  def test_barrier_option_dtype(self, dtype):\n    """"""Function tests barrier option pricing for with given data type.""""""\n    spots = 100.0\n    rebates = 3.0\n    expiries = 0.5\n    discount_rates = 0.08\n    cost_of_carries = 0.04\n    strikes = 90.0\n    barriers = 95.0\n    expected_price = 9.0246\n    is_call_options = True\n    is_barrier_down = True\n    is_knock_out = True\n    volatilities = 0.25\n    price = tff.black_scholes.barrier_price(\n        volatilities=volatilities, strikes=strikes,\n        expiries=expiries, spots=spots,\n        discount_rates=discount_rates,\n        cost_of_carries=cost_of_carries,\n        barriers=barriers, rebates=rebates,\n        is_barrier_down=is_barrier_down,\n        is_knock_out=is_knock_out,\n        is_call_options=is_call_options,\n        dtype=dtype)\n    self.assertAllClose(price, expected_price, 10e-3)\n    self.assertEqual(price.dtype, dtype)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/datetime/__init__.py,0,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Date-related utilities.""""""\n\nfrom tf_quant_finance.datetime import date_utils as utils\n\nfrom tf_quant_finance.datetime.constants import BusinessDayConvention\nfrom tf_quant_finance.datetime.constants import Month\nfrom tf_quant_finance.datetime.constants import PeriodType\nfrom tf_quant_finance.datetime.constants import WeekDay\nfrom tf_quant_finance.datetime.constants import WeekendMask\n\nfrom tf_quant_finance.datetime.date_tensor import convert_to_date_tensor\nfrom tf_quant_finance.datetime.date_tensor import DateTensor\nfrom tf_quant_finance.datetime.date_tensor import from_datetimes as dates_from_datetimes\nfrom tf_quant_finance.datetime.date_tensor import from_np_datetimes as dates_from_np_datetimes\nfrom tf_quant_finance.datetime.date_tensor import from_ordinals as dates_from_ordinals\nfrom tf_quant_finance.datetime.date_tensor import from_tensor as dates_from_tensor\nfrom tf_quant_finance.datetime.date_tensor import from_tuples as dates_from_tuples\nfrom tf_quant_finance.datetime.date_tensor import from_year_month_day as dates_from_year_month_day\nfrom tf_quant_finance.datetime.date_tensor import random_dates\nfrom tf_quant_finance.datetime.daycounts import actual_360 as daycount_actual_360\nfrom tf_quant_finance.datetime.daycounts import actual_365_actual as daycount_actual_365_actual\nfrom tf_quant_finance.datetime.daycounts import actual_365_fixed as daycount_actual_365_fixed\nfrom tf_quant_finance.datetime.daycounts import thirty_360_isda as daycount_thirty_360_isda\nfrom tf_quant_finance.datetime.holiday_calendar import HolidayCalendar\nfrom tf_quant_finance.datetime.holiday_calendar_factory import create_holiday_calendar\nfrom tf_quant_finance.datetime.periods import day\nfrom tf_quant_finance.datetime.periods import days\nfrom tf_quant_finance.datetime.periods import month\nfrom tf_quant_finance.datetime.periods import months\nfrom tf_quant_finance.datetime.periods import PeriodTensor\nfrom tf_quant_finance.datetime.periods import week\nfrom tf_quant_finance.datetime.periods import weeks\nfrom tf_quant_finance.datetime.periods import year\nfrom tf_quant_finance.datetime.periods import years\nfrom tf_quant_finance.datetime.schedules import BusinessDaySchedule\nfrom tf_quant_finance.datetime.schedules import PeriodicSchedule\n\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n\n_allowed_symbols = [\n    \'BusinessDayConvention\',\n    \'BusinessDaySchedule\',\n    \'DateTensor\',\n    \'HolidayCalendar\',\n    \'create_holiday_calendar\',\n    \'Month\',\n    \'PeriodType\',\n    \'WeekDay\',\n    \'WeekendMask\',\n    \'convert_to_date_tensor\',\n    \'dates_from_datetimes\',\n    \'dates_from_np_datetimes\',\n    \'dates_from_ordinals\',\n    \'dates_from_tensor\',\n    \'dates_from_tuples\',\n    \'dates_from_year_month_day\',\n    \'day\',\n    \'days\',\n    \'week\',\n    \'weeks\',\n    \'month\',\n    \'months\',\n    \'year\',\n    \'years\',\n    \'utils\',\n    \'PeriodTensor\',\n    \'PeriodicSchedule\',\n    \'random_dates\',\n    \'daycount_actual_360\',\n    \'daycount_actual_365_actual\',\n    \'daycount_actual_365_fixed\',\n    \'daycount_thirty_360_isda\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/datetime/bounded_holiday_calendar.py,36,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""HolidayCalendar definition.""""""\n\nimport attr\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.datetime import constants\nfrom tf_quant_finance.datetime import date_tensor as dt\nfrom tf_quant_finance.datetime import holiday_calendar\nfrom tf_quant_finance.datetime import periods\n\n\n_ORDINAL_OF_1_1_1970 = 719163\n_OUT_OF_BOUNDS_MSG = ""Went out of calendar boundaries!""\n\n\nclass BoundedHolidayCalendar(holiday_calendar.HolidayCalendar):\n  """"""HolidayCalendar implementation.\n\n  Requires the calendar to be bounded. Constructs tables of precomputed values\n  of interest for each day in the calendar, which enables better performance\n  compared to a more flexible UnboundedHolidayCalendar.\n  """"""\n\n  def __init__(\n      self,\n      weekend_mask=None,\n      holidays=None,\n      start_year=None,\n      end_year=None):\n    """"""Initializer.\n\n    Args:\n      weekend_mask: Tensor of 7 elements, where ""0"" means work day and ""1"" -\n        day off. The first element is Monday. By default, no weekends are\n        applied. Some of the common weekend patterns are defined in\n        `dates.WeekendMask`.\n        Default value: None which maps to no weekend days.\n      holidays: Defines the holidays that are added to the weekends defined by\n      `weekend_mask`. An instance of `dates.DateTensor` or an object\n       convertible to `DateTensor`.\n       Default value: None which means no holidays other than those implied by\n       the weekends (if any).\n      start_year: Integer giving the earliest year this calendar includes. If\n        `holidays` is specified, then `start_year` and `end_year` are ignored,\n        and the boundaries are derived from `holidays`. If `holidays` is `None`,\n        both `start_year` and `end_year` must be specified.\n      end_year: Integer giving the latest year this calendar includes. If\n        `holidays` is specified, then `start_year` and `end_year` are ignored,\n        and the boundaries are derived from `holidays`. If `holidays` is `None`,\n        both `start_year` and `end_year` must be specified.\n    """"""\n    self._weekend_mask = tf.convert_to_tensor(weekend_mask or\n                                              constants.WeekendMask.NONE)\n    if holidays is None:\n      self._holidays = None\n    else:\n      self._holidays = dt.convert_to_date_tensor(holidays)\n    start_year, end_year = _resolve_calendar_boundaries(self._holidays,\n                                                        start_year, end_year)\n    self._ordinal_offset = dt.from_year_month_day(start_year, 1, 1).ordinal()\n    self._calendar_size = (\n        dt.from_year_month_day(end_year + 1, 1, 1).ordinal() -\n        self._ordinal_offset)\n\n    # Precomputed tables. These are constant 1D Tensors, mapping each day in the\n    # [start_year, end_year] period to some quantity of interest, e.g. next\n    # business day. The tables should be indexed with\n    # `date.ordinal - self._offset`. All tables are computed lazily.\n    # All tables have an extra element at the beginning and the end, see comment\n    # in _compute_is_bus_day_table().\n    self._table_cache = _TableCache()\n\n  def is_business_day(self, date_tensor):\n    """"""Returns a tensor of bools for whether given dates are business days.""""""\n    is_bus_day_table = self._compute_is_bus_day_table()\n    is_bus_day_int32 = self._gather(\n        is_bus_day_table,\n        date_tensor.ordinal() - self._ordinal_offset + 1)\n    with tf.control_dependencies(\n        self._assert_ordinals_in_bounds(date_tensor.ordinal())):\n      return tf.cast(is_bus_day_int32, dtype=tf.bool)\n\n  def roll_to_business_day(self, date_tensor, roll_convention):\n    """"""Rolls the given dates to business dates according to given convention.\n\n    Args:\n      date_tensor: DateTensor of dates to roll from.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting DateTensor.\n    """"""\n    if roll_convention == constants.BusinessDayConvention.NONE:\n      return date_tensor\n    rolled_ordinals_table = self._compute_rolled_dates_table(roll_convention)\n    ordinals_with_offset = date_tensor.ordinal() - self._ordinal_offset + 1\n    rolled_ordinals = self._gather(rolled_ordinals_table, ordinals_with_offset)\n    with tf.control_dependencies(\n        self._assert_ordinals_in_bounds(rolled_ordinals)):\n      return dt.from_ordinals(rolled_ordinals, validate=False)\n\n  def add_period_and_roll(self,\n                          date_tensor,\n                          period_tensor,\n                          roll_convention=constants.BusinessDayConvention.NONE):\n    """"""Adds given periods to given dates and rolls to business days.\n\n    The original dates are not rolled prior to addition.\n\n    Args:\n      date_tensor: DateTensor of dates to add to.\n      period_tensor: PeriodTensor broadcastable to `date_tensor`.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting DateTensor.\n    """"""\n    return self.roll_to_business_day(date_tensor + period_tensor,\n                                     roll_convention)\n\n  def add_business_days(self,\n                        date_tensor,\n                        num_days,\n                        roll_convention=constants.BusinessDayConvention.NONE):\n    """"""Adds given number of business days to given dates.\n\n    Note that this is different from calling `add_period_and_roll` with\n    PeriodType.DAY. For example, adding 5 business days to Monday gives the next\n    Monday (unless there are holidays on this week or next Monday). Adding 5\n    days and rolling means landing on Saturday and then rolling either to next\n    Monday or to Friday of the same week, depending on the roll convention.\n\n    If any of the dates in `date_tensor` are not business days, they will be\n    rolled to business days before doing the addition. If `roll_convention` is\n    `NONE`, and any dates are not business days, an exception is raised.\n\n    Args:\n      date_tensor: DateTensor of dates to advance from.\n      num_days: Tensor of int32 type broadcastable to `date_tensor`.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting DateTensor.\n    """"""\n    control_deps = []\n    if roll_convention == constants.BusinessDayConvention.NONE:\n      message = (""Some dates in date_tensor are not business days. ""\n                 ""Please specify the roll_convention argument."")\n      is_bus_day = self.is_business_day(date_tensor)\n      control_deps.append(\n          tf.debugging.assert_equal(is_bus_day, True, message=message))\n    else:\n      date_tensor = self.roll_to_business_day(date_tensor, roll_convention)\n\n    with tf.control_dependencies(control_deps):\n      cumul_bus_days_table = self._compute_cumul_bus_days_table()\n      cumul_bus_days = self._gather(\n          cumul_bus_days_table,\n          date_tensor.ordinal() - self._ordinal_offset + 1)\n      target_cumul_bus_days = cumul_bus_days + num_days\n\n      bus_day_ordinals_table = self._compute_bus_day_ordinals_table()\n      ordinals = self._gather(bus_day_ordinals_table, target_cumul_bus_days)\n      with tf.control_dependencies(self._assert_ordinals_in_bounds(ordinals)):\n        return dt.from_ordinals(ordinals, validate=False)\n\n  def subtract_period_and_roll(\n      self,\n      date_tensor,\n      period_tensor,\n      roll_convention=constants.BusinessDayConvention.NONE):\n    """"""Subtracts given periods from given dates and rolls to business days.\n\n    The original dates are not rolled prior to subtraction.\n\n    Args:\n      date_tensor: DateTensor of dates to subtract from.\n      period_tensor: PeriodTensor broadcastable to `date_tensor`.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting DateTensor.\n    """"""\n    minus_period_tensor = periods.PeriodTensor(-period_tensor.quantity(),\n                                               period_tensor.period_type())\n    return self.add_period_and_roll(date_tensor, minus_period_tensor,\n                                    roll_convention)\n\n  def subtract_business_days(\n      self,\n      date_tensor,\n      num_days,\n      roll_convention=constants.BusinessDayConvention.NONE):\n    """"""Adds given number of business days to given dates.\n\n    Note that this is different from calling `subtract_period_and_roll` with\n    PeriodType.DAY. For example, subtracting 5 business days from Friday gives\n    the previous Friday (unless there are holidays on this week or previous\n    Friday). Subtracting 5 days and rolling means landing on Sunday and then\n    rolling either to Monday or to Friday, depending on the roll convention.\n\n    If any of the dates in `date_tensor` are not business days, they will be\n    rolled to business days before doing the subtraction. If `roll_convention`\n    is `NONE`, and any dates are not business days, an exception is raised.\n\n    Args:\n      date_tensor: DateTensor of dates to advance from.\n      num_days: Tensor of int32 type broadcastable to `date_tensor`.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting DateTensor.\n    """"""\n    return self.add_business_days(date_tensor, -num_days, roll_convention)\n\n  def business_days_in_period(self, date_tensor, period_tensor):\n    """"""Calculates number of business days in a period.\n\n    Includes the dates in `date_tensor`, but excludes final dates resulting from\n    addition of `period_tensor`.\n\n    Args:\n      date_tensor: DateTensor of starting dates.\n      period_tensor: PeriodTensor, should be broadcastable to `date_tensor`.\n\n    Returns:\n       An int32 Tensor with the number of business days in given periods that\n       start at given dates.\n\n    """"""\n    return self.business_days_between(date_tensor, date_tensor + period_tensor)\n\n  def business_days_between(self, from_dates, to_dates):\n    """"""Calculates number of business between pairs of dates.\n\n    For each pair, the initial date is included in the difference, and the final\n    date is excluded. If the final date is the same or earlier than the initial\n    date, zero is returned.\n\n    Args:\n      from_dates: DateTensor of initial dates.\n      to_dates: DateTensor of final dates, should be broadcastable to\n        `from_dates`.\n\n    Returns:\n       An int32 Tensor with the number of business days between the\n       corresponding pairs of dates.\n    """"""\n    cumul_bus_days_table = self._compute_cumul_bus_days_table()\n    ordinals_1, ordinals_2 = from_dates.ordinal(), to_dates.ordinal()\n    with tf.control_dependencies(\n        self._assert_ordinals_in_bounds(ordinals_1) +\n        self._assert_ordinals_in_bounds(ordinals_2)):\n      ordinals_2 = tf.broadcast_to(ordinals_2, ordinals_1.shape)\n      cumul_bus_days_1 = self._gather(cumul_bus_days_table,\n                                      ordinals_1 - self._ordinal_offset + 1)\n      cumul_bus_days_2 = self._gather(cumul_bus_days_table,\n                                      ordinals_2 - self._ordinal_offset + 1)\n      return tf.math.maximum(cumul_bus_days_2 - cumul_bus_days_1, 0)\n\n  def _compute_rolled_dates_table(self, convention):\n    """"""Computes and caches rolled dates table.""""""\n    already_computed = self._table_cache.rolled_dates.get(convention, None)\n    if already_computed is not None:\n      return already_computed\n\n    # To make tensor caching safe, lift the ops out of the current scope using\n    # tf.init_scope(). This allows e.g. to cache these tensors in one\n    # tf.function and reuse them in another tf.function.\n    with tf.init_scope():\n      rolled_date_table = (\n          self._compute_rolled_dates_table_without_cache(convention))\n      self._table_cache.rolled_dates[convention] = rolled_date_table\n    return rolled_date_table\n\n  def _compute_rolled_dates_table_without_cache(self, convention):\n    is_bus_day = self._compute_is_bus_day_table()\n    cumul_bus_days = self._compute_cumul_bus_days_table()\n    bus_day_ordinals = self._compute_bus_day_ordinals_table()\n\n    if convention == constants.BusinessDayConvention.FOLLOWING:\n      return tf.gather(bus_day_ordinals, cumul_bus_days)\n\n    if convention == constants.BusinessDayConvention.PRECEDING:\n      return tf.gather(bus_day_ordinals,\n                       cumul_bus_days - (1 - is_bus_day))\n\n    following = self._compute_rolled_dates_table(\n        constants.BusinessDayConvention.FOLLOWING)\n    preceding = self._compute_rolled_dates_table(\n        constants.BusinessDayConvention.PRECEDING)\n    dates_following = dt.from_ordinals(following)\n    dates_preceding = dt.from_ordinals(preceding)\n    original_dates = dt.from_ordinals(\n        tf.range(self._ordinal_offset - 1,\n                 self._ordinal_offset + self._calendar_size + 1))\n\n    if convention == constants.BusinessDayConvention.MODIFIED_FOLLOWING:\n      return tf.where(tf.equal(dates_following.month(), original_dates.month()),\n                      following, preceding)\n\n    if convention == constants. BusinessDayConvention.MODIFIED_PRECEDING:\n      return tf.where(tf.equal(dates_preceding.month(), original_dates.month()),\n                      preceding, following)\n\n    raise ValueError(""Unrecognized convention: {}"".format(convention))\n\n  def _compute_is_bus_day_table(self):\n    """"""Computes and caches ""is business day"" table.""""""\n    if self._table_cache.is_bus_day is not None:\n      return self._table_cache.is_bus_day\n\n    with tf.init_scope():\n      ordinals = tf.range(self._ordinal_offset,\n                          self._ordinal_offset + self._calendar_size)\n      # Apply weekend mask\n      week_days = (ordinals - 1) % 7\n      is_holiday = tf.gather(self._weekend_mask, week_days)\n\n      # Apply holidays\n      if self._holidays is not None:\n        indices = self._holidays.ordinal() - self._ordinal_offset\n        ones_at_indices = tf.scatter_nd(\n            tf.expand_dims(indices, axis=-1), tf.ones_like(indices),\n            is_holiday.shape)\n        is_holiday = tf.bitwise.bitwise_or(is_holiday, ones_at_indices)\n\n      # Add a business day at the beginning and at the end, i.e. at 31 Dec of\n      # start_year-1 and at 1 Jan of end_year+1. This trick is to avoid dealing\n      # with special cases on boundaries.\n      # For example, for Following and Preceding conventions we\'d need a special\n      # value that means ""unknown"" in the tables. More complicated conventions\n      # then combine the Following and Preceding tables, and would need special\n      # treatment of the ""unknown"" values.\n      # With these ""fake"" business days, all computations are automatically\n      # correct, unless we land on those extra days - for this reason we add\n      # assertions in all API calls before returning.\n      is_bus_day_table = tf.concat([[1], 1 - is_holiday, [1]], axis=0)\n      self._table_cache.is_bus_day = is_bus_day_table\n    return is_bus_day_table\n\n  def _compute_cumul_bus_days_table(self):\n    """"""Computes and caches cumulative business days table.""""""\n    if self._table_cache.cumul_bus_days is not None:\n      return self._table_cache.cumul_bus_days\n\n    is_bus_day_table = self._compute_is_bus_day_table()\n    with tf.init_scope():\n      cumul_bus_days_table = tf.math.cumsum(is_bus_day_table, exclusive=True,\n                                            name=""cumul_bus_days_table"")\n      self._table_cache.cumul_bus_days = cumul_bus_days_table\n    return cumul_bus_days_table\n\n  def _compute_bus_day_ordinals_table(self):\n    """"""Computes and caches rolled business day ordinals table.""""""\n    if self._table_cache.bus_day_ordinals is not None:\n      return self._table_cache.bus_day_ordinals\n\n    is_bus_day_table = self._compute_is_bus_day_table()\n    with tf.init_scope():\n      bus_day_ordinals_table = (\n          tf.cast(tf.where(is_bus_day_table)[:, 0], tf.int32) +\n          self._ordinal_offset - 1)\n      self._table_cache.bus_day_ordinals = bus_day_ordinals_table\n    return bus_day_ordinals_table\n\n  def _gather(self, table, indices):\n    table_size = self._calendar_size + 2\n    assert1 = tf.debugging.assert_greater_equal(\n        indices, 0, message=_OUT_OF_BOUNDS_MSG)\n    assert2 = tf.debugging.assert_less(\n        indices, table_size, message=_OUT_OF_BOUNDS_MSG)\n    with tf.control_dependencies([assert1, assert2]):\n      return tf.gather(table, indices)\n\n  def _assert_ordinals_in_bounds(self, ordinals):\n    assert1 = tf.debugging.assert_greater_equal(\n        ordinals, self._ordinal_offset, message=_OUT_OF_BOUNDS_MSG)\n    assert2 = tf.debugging.assert_less(\n        ordinals,\n        self._ordinal_offset + self._calendar_size,\n        message=_OUT_OF_BOUNDS_MSG)\n    return [assert1, assert2]\n\n\ndef _resolve_calendar_boundaries(holidays, start_year, end_year):\n  if holidays is None or holidays.shape.num_elements() in [None, 0]:\n    if start_year is None or end_year is None:\n      raise ValueError(""Please specify either holidays or both start_year and ""\n                       ""end_year arguments"")\n    return start_year, end_year\n\n  return tf.math.reduce_min(holidays.year()), tf.math.reduce_max(\n      holidays.year())\n\n\n@attr.s\nclass _TableCache(object):\n  """"""Cache of pre-computed tables.""""""\n\n  # Tables of rolled date ordinals keyed by BusinessDayConvention.\n  rolled_dates = attr.ib(factory=dict)\n\n  # Table with ""1"" on business days and ""0"" otherwise.\n  is_bus_day = attr.ib(default=None)\n\n  # Table with number of business days before each date. Starts with 0.\n  cumul_bus_days = attr.ib(default=None)\n\n  # Table with ordinals of each business day in the [start_year, end_year],\n  # in order.\n  bus_day_ordinals = attr.ib(default=None)\n'"
tf_quant_finance/datetime/constants.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Date-related constants and enums.""""""\n\nimport enum\n\n\nclass Month(enum.Enum):\n  """"""Months. Values are one-based.""""""\n  JANUARY = 1\n  FEBUARY = 2\n  MARCH = 3\n  APRIL = 4\n  MAY = 5\n  JUNE = 6\n  JULY = 7\n  AUGUST = 8\n  SEPTEMBER = 9\n  OCTOBER = 10\n  NOVEMBER = 11\n  DECEMBER = 12\n\n\nclass WeekDay(enum.Enum):\n  """"""Named days of the week. Values are zero-based with Monday = 0.""""""\n  # We follow Python datetime convention of starting from 0.\n  MONDAY = 0\n  TUESDAY = 1\n  WEDNESDAY = 2\n  THURSDAY = 3\n  FRIDAY = 4\n  SATURDAY = 5\n  SUNDAY = 6\n\n\nclass PeriodType(enum.Enum):\n  """"""Periods that can be added or subtracted from DateTensors.""""""\n  DAY = 0\n  WEEK = 1\n  MONTH = 2\n  YEAR = 3\n\n\nclass BusinessDayConvention(enum.Enum):\n  """"""Conventions that determine how to roll dates that fall on holidays.\n\n  * `NONE`: No adjustment\n  * `FOLLOWING`: Choose the first business day after the given holiday.\n  * `MODIFIED_FOLLOWING`: Choose the first business day after the given holiday\n  unless that day falls in the next calendar month, in which case choose the\n  first business day before the holiday.\n  * `PRECEDING`: Choose the first business day before the given holiday.\n  * `MODIFIED_PRECEDING`: Choose the first business day before the given holiday\n  unless that day falls in the previous calendar month, in which case choose the\n  first business day after the holiday.\n  """"""\n  NONE = 0\n  FOLLOWING = 1\n  MODIFIED_FOLLOWING = 2\n  PRECEDING = 3\n  MODIFIED_PRECEDING = 4\n\n# TODO(b/148011715): add NEAREST convention.\n\n\nclass WeekendMask(object):\n  """"""Provides weekend masks for some of the common weekend patterns.""""""\n\n  # E.g. US/UK/Europe etc.\n  SATURDAY_SUNDAY = (0, 0, 0, 0, 0, 1, 1)\n\n  # E.g. Most countries in the Middle East.\n  FRIDAY_SATURDAY = (0, 0, 0, 0, 1, 1, 0)\n\n  # E.g. India, Nepal.\n  SUNDAY_ONLY = (0, 0, 0, 0, 0, 0, 1)\n\n  # Default value.\n  NONE = (0, 0, 0, 0, 0, 0, 0)\n'"
tf_quant_finance/datetime/date_tensor.py,73,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""DateTensor definition.""""""\nimport collections\nimport datetime\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.datetime import constants\nfrom tf_quant_finance.datetime import date_utils\nfrom tf_quant_finance.datetime import periods\nfrom tf_quant_finance.datetime import tensor_wrapper\n\n# Days in each month of a non-leap year.\n_DAYS_IN_MONTHS_NON_LEAP = [\n    31,  # January.\n    28,  # February.\n    31,  # March.\n    30,  # April.\n    31,  # May.\n    30,  # June.\n    31,  # July.\n    31,  # August.\n    30,  # September.\n    31,  # October.\n    30,  # November.\n    31,  # December.\n]\n\n# Days in each month of a leap year.\n_DAYS_IN_MONTHS_LEAP = [\n    31,  # January.\n    29,  # February.\n    31,  # March.\n    30,  # April.\n    31,  # May.\n    30,  # June.\n    31,  # July.\n    31,  # August.\n    30,  # September.\n    31,  # October.\n    30,  # November.\n    31,  # December.\n]\n\n# Combined list of days per month. A sentinel value of 0 is added to the top of\n# the array so indexing is easier.\n_DAYS_IN_MONTHS_COMBINED = [0] + _DAYS_IN_MONTHS_NON_LEAP + _DAYS_IN_MONTHS_LEAP\n\n_ORDINAL_OF_1_1_1970 = 719163\n\n\nclass DateTensor(tensor_wrapper.TensorWrapper):\n  """"""Represents a tensor of dates.""""""\n\n  def __init__(self, ordinals, years, months, days):\n    """"""Initializer.\n\n    This initializer is primarily for internal use. More convenient construction\n    methods are available via \'dates_from_*\' functions.\n\n    Args:\n      ordinals: Tensor of type int32. Each value is number of days since 1 Jan\n        0001. 1 Jan 0001 has `ordinal=1`. `years`, `months` and `days` must\n        represent the same dates as `ordinals`.\n      years: Tensor of type int32, of same shape as `ordinals`.\n      months: Tensor of type int32, of same shape as `ordinals`\n      days: Tensor of type int32, of same shape as `ordinals`.\n    """"""\n    # The internal representation of a DateTensor is all four int32 Tensors\n    # (ordinals, years, months, days). Why do we need such redundancy?\n    #\n    # Imagine we kept only ordinals, and consider the following example:\n    # User creates a DateTensor, adds a certain number of months, and then\n    # calls .day() on resulting DateTensor. The transformations would be as\n    # follows: o -> y, m, d -> y\', m\', d\' -> o\' -> y\', m\', d\'.\n    # The first transformation is required for adding months.\n    # The second is actually adding months. Third - for creating a new\n    # DateTensor object that is backed by o\'. Last - to yield a result from\n    # new_date_tensor.day(). The last transformation is clearly unnecessary and\n    # it\'s expensive.\n    #\n    # With a ""redundant"" representation we have:\n    # o -> y, m, d -> y\', m\', d\' -> o\' or o <- y, m, d -> y\', m\', d\' -> o\',\n    # depending on how the first DateTensor is created. new_date_tensor.day()\n    # yields m\', which we didn\'t discard, and if o and o\' are never needed,\n    # they\'ll be eliminated (in graph mode).\n    #\n    # A similar argument shows why (y, m, d) is not an optimal representation\n    # either - for e.g. adding days instead of months.\n\n    self._ordinals = tf.convert_to_tensor(\n        ordinals, dtype=tf.int32, name=""dt_ordinals"")\n    self._years = tf.convert_to_tensor(years, dtype=tf.int32, name=""dt_years"")\n    self._months = tf.convert_to_tensor(\n        months, dtype=tf.int32, name=""dt_months"")\n    self._days = tf.convert_to_tensor(days, dtype=tf.int32, name=""dt_days"")\n    self._day_of_year = None  # Computed lazily.\n\n  def day(self):\n    """"""Returns an int32 tensor of days since the beginning the month.\n\n    The result is one-based, i.e. yields 1 for first day of the month.\n\n    #### Example\n\n    ```python\n    dates = tff.datetime.dates_from_tuples([(2019, 1, 25), (2020, 3, 2)])\n    dates.day()  # [25, 2]\n    ```\n    """"""\n    return self._days\n\n  def day_of_week(self):\n    """"""Returns an int32 tensor of weekdays.\n\n    The result is zero-based according to Python datetime convention, i.e.\n    Monday is ""0"".\n\n    #### Example\n\n    ```python\n    dates = tff.datetime.dates_from_tuples([(2019, 1, 25), (2020, 3, 2)])\n    dates.day_of_week()  # [5, 1]\n    ```\n    """"""\n    # 1 Jan 0001 was Monday according to the proleptic Gregorian calendar.\n    # So, 1 Jan 0001 has ordinal 1, and the weekday is 0.\n    return (self._ordinals - 1) % 7\n\n  def month(self):\n    """"""Returns an int32 tensor of months.\n\n    #### Example\n\n    ```python\n    dates = tff.datetime.dates_from_tuples([(2019, 1, 25), (2020, 3, 2)])\n    dates.month()  # [1, 3]\n    ```\n    """"""\n    return self._months\n\n  def year(self):\n    """"""Returns an int32 tensor of years.\n\n    #### Example\n\n    ```python\n    dates = tff.datetime.dates_from_tuples([(2019, 1, 25), (2020, 3, 2)])\n    dates.year()  # [2019, 2020]\n    ```\n    """"""\n    return self._years\n\n  def ordinal(self):\n    """"""Returns an int32 tensor of ordinals.\n\n    Ordinal is the number of days since 1st Jan 0001.\n\n    #### Example\n\n    ```python\n    dates = tff.datetime.dates_from_tuples([(2019, 3, 25), (1, 1, 1)])\n    dates.ordinal()  # [737143, 1]\n    ```\n    """"""\n    return self._ordinals\n\n  def to_tensor(self):\n    """"""Packs the dates into a single Tensor.\n\n    The Tensor has shape `date_tensor.shape() + (3,)`, where the last dimension\n    represents years, months and days, in this order.\n\n    This can be convenient when the dates are the final result of a computation\n    in the graph mode: a `tf.function` can return `date_tensor.to_tensor()`, or,\n    if one uses `tf.compat.v1.Session`, they can call\n    `session.run(date_tensor.to_tensor())`.\n\n    Returns:\n      A Tensor of shape `date_tensor.shape() + (3,)`.\n\n    #### Example\n\n    ```python\n    dates = tff.datetime.dates_from_tuples([(2019, 1, 25), (2020, 3, 2)])\n    dates.to_tensor()  # tf.Tensor with contents [[2019, 1, 25], [2020, 3, 2]].\n    ```\n    """"""\n    return tf.stack((self.year(), self.month(), self.day()), axis=-1)\n\n  def day_of_year(self):\n    """"""Calculates the number of days since the beginning of the year.\n\n    Returns:\n      Tensor of int32 type with elements in range [1, 366]. January 1st yields\n      ""1"".\n\n    #### Example\n\n    ```python\n    dt = tff.datetime.dates_from_tuples([(2019, 1, 25), (2020, 3, 2)])\n    dt.day_of_year()  # [25, 62]\n    ```\n    """"""\n    if self._day_of_year is None:\n      cumul_days_in_month_nonleap = tf.math.cumsum(\n          _DAYS_IN_MONTHS_NON_LEAP, exclusive=True)\n      cumul_days_in_month_leap = tf.math.cumsum(\n          _DAYS_IN_MONTHS_LEAP, exclusive=True)\n      days_before_month_non_leap = tf.gather(cumul_days_in_month_nonleap,\n                                             self.month() - 1)\n      days_before_month_leap = tf.gather(cumul_days_in_month_leap,\n                                         self.month() - 1)\n      days_before_month = tf.where(\n          date_utils.is_leap_year(self.year()), days_before_month_leap,\n          days_before_month_non_leap)\n      self._day_of_year = days_before_month + self.day()\n    return self._day_of_year\n\n  def days_until(self, target_date_tensor):\n    """"""Computes the number of days until the target dates.\n\n    Args:\n      target_date_tensor: A DateTensor object broadcastable to the shape of\n        ""self"".\n\n    Returns:\n      An int32 tensor with numbers of days until the target dates.\n\n     #### Example\n\n     ```python\n    dates = tff.datetime.dates_from_tuples([(2020, 1, 25), (2020, 3, 2)])\n    target = tff.datetime.dates_from_tuples([(2020, 3, 5)])\n    dates.days_until(target) # [40, 3]\n\n    targets = tff.datetime.dates_from_tuples([(2020, 2, 5), (2020, 3, 5)])\n    dates.days_until(targets)  # [11, 3]\n    ```\n    """"""\n    return target_date_tensor.ordinal() - self._ordinals\n\n  def period_length_in_days(self, period_tensor):\n    """"""Computes the number of days in each period.\n\n    Args:\n      period_tensor: A PeriodTensor object broadcastable to the shape of ""self"".\n\n    Returns:\n      An int32 tensor with numbers of days each period takes.\n\n    #### Example\n\n    ```python\n    dates = tff.datetime.dates_from_tuples([(2020, 2, 25), (2020, 3, 2)])\n    dates.period_length_in_days(month())  # [29, 31]\n\n    periods = tff.datetime.months([1, 2])\n    dates.period_length_in_days(periods)  # [29, 61]\n    ```\n    """"""\n    return (self + period_tensor).ordinal() - self._ordinals\n\n  def is_end_of_month(self):\n    """"""Returns a bool Tensor indicating whether dates are at ends of months.""""""\n    return tf.math.equal(self._days,\n                         _num_days_in_month(self._months, self._years))\n\n  def to_end_of_month(self):\n    """"""Returns a new DateTensor with each date shifted to the end of month.""""""\n    days = _num_days_in_month(self._months, self._years)\n    return from_year_month_day(self._years, self._months, days, validate=False)\n\n  @property\n  def shape(self):\n    return self._ordinals.shape\n\n  @property\n  def rank(self):\n    return tf.rank(self._ordinals)\n\n  def __add__(self, period_tensor):\n    """"""Adds a tensor of periods.\n\n    When adding months or years, the resulting day of the month is decreased\n    to the largest valid value if necessary. E.g. 31.03.2020 + 1 month =\n    30.04.2020, 29.02.2020 + 1 year = 28.02.2021.\n\n    Args:\n      period_tensor: A `PeriodTensor` object broadcastable to the shape of\n      ""self"".\n\n    Returns:\n      The new instance of DateTensor.\n\n    #### Example\n    ```python\n    dates = tff.datetime.dates_from_tuples([(2020, 2, 25), (2020, 3, 31)])\n    new_dates = dates + tff.datetime.month()\n    # DateTensor([(2020, 3, 25), (2020, 4, 30)])\n\n    new_dates = dates + tff.datetime.month([1, 2])\n    # DateTensor([(2020, 3, 25), (2020, 5, 31)])\n    ```\n    """"""\n    period_type = period_tensor.period_type()\n\n    if period_type == constants.PeriodType.DAY:\n      ordinals = self._ordinals + period_tensor.quantity()\n      return from_ordinals(ordinals)\n\n    if period_type == constants.PeriodType.WEEK:\n      return self + periods.PeriodTensor(period_tensor.quantity() * 7,\n                                         constants.PeriodType.DAY)\n\n    def adjust_day(year, month, day):\n      return tf.math.minimum(day, _num_days_in_month(month, year))\n\n    if period_type == constants.PeriodType.MONTH:\n      m = self._months - 1 + period_tensor.quantity()\n      y = self._years + m // 12\n      m = m % 12 + 1\n      d = adjust_day(y, m, self._days)\n      return from_year_month_day(y, m, d, validate=False)\n\n    if period_type == constants.PeriodType.YEAR:\n      y = self._years + period_tensor.quantity()\n      m = tf.broadcast_to(self._months, y.shape)\n      d = adjust_day(y, m, self._days)\n      return from_year_month_day(y, m, d, validate=False)\n\n    raise ValueError(""Unrecognized period type: {}"".format(period_type))\n\n  def __sub__(self, period_tensor):\n    """"""Subtracts a tensor of periods.\n\n    When subtracting months or years, the resulting day of the month is\n    decreased to the largest valid value if necessary. E.g. 31.03.2020 - 1 month\n    = 29.02.2020, 29.02.2020 - 1 year = 28.02.2019.\n\n    Args:\n      period_tensor: a PeriodTensor object broadcastable to the shape of ""self"".\n\n    Returns:\n      The new instance of DateTensor.\n    """"""\n    return self + periods.PeriodTensor(-period_tensor.quantity(),\n                                       period_tensor.period_type())\n\n  def __eq__(self, other):\n    """"""Compares two DateTensors by ""=="", returning a Tensor of bools.""""""\n    # Note that tf doesn\'t override ""=="" and  ""!="", unlike numpy.\n    return tf.math.equal(self._ordinals, other.ordinal())\n\n  def __ne__(self, other):\n    """"""Compares two DateTensors by ""!="", returning a Tensor of bools.""""""\n    return tf.math.not_equal(self._ordinals, other.ordinal())\n\n  def __gt__(self, other):\n    """"""Compares two DateTensors by "">"", returning a Tensor of bools.""""""\n    return self._ordinals > other.ordinal()\n\n  def __ge__(self, other):\n    """"""Compares two DateTensors by "">="", returning a Tensor of bools.""""""\n    return self._ordinals >= other.ordinal()\n\n  def __lt__(self, other):\n    """"""Compares two DateTensors by ""<"", returning a Tensor of bools.""""""\n\n    return self._ordinals < other.ordinal()\n\n  def __le__(self, other):\n    """"""Compares two DateTensors by ""<="", returning a Tensor of bools.""""""\n    return self._ordinals <= other.ordinal()\n\n  def __repr__(self):\n    output = ""DateTensor: shape={}"".format(self.shape)\n    if tf.executing_eagerly():\n      contents_np = np.stack(\n          (self._years.numpy(), self._months.numpy(), self._days.numpy()),\n          axis=-1)\n      return output + "", contents={}"".format(repr(contents_np))\n    return output\n\n  @classmethod\n  def _apply_sequence_to_tensor_op(cls, op_fn, tensor_wrappers):\n    o = op_fn([t.ordinal() for t in tensor_wrappers])\n    y = op_fn([t.year() for t in tensor_wrappers])\n    m = op_fn([t.month() for t in tensor_wrappers])\n    d = op_fn([t.day() for t in tensor_wrappers])\n    return DateTensor(o, y, m, d)\n\n  def _apply_op(self, op_fn):\n    o, y, m, d = (\n        op_fn(t)\n        for t in (self._ordinals, self._years, self._months, self._days))\n    return DateTensor(o, y, m, d)\n\n\ndef _num_days_in_month(month, year):\n  """"""Returns number of days in a given month of a given year.""""""\n  days_in_months = tf.constant(_DAYS_IN_MONTHS_COMBINED, tf.int32)\n  is_leap = date_utils.is_leap_year(year)\n  return tf.gather(days_in_months,\n                   month + 12 * tf.dtypes.cast(is_leap, tf.int32))\n\n\ndef convert_to_date_tensor(date_inputs):\n  """"""Converts supplied data to a `DateTensor` if possible.\n\n  Args:\n    date_inputs: One of the supported types that can be converted to a\n      DateTensor. The following input formats are supported. 1. Sequence of\n      `datetime.datetime`, `datetime.date`, or any other structure with data\n      attributes called \'year\', \'month\' and \'day\'. 2. A numpy array of\n      `datetime64` type. 3. Sequence of (year, month, day) Tuples. Months are\n      1-based (with January as 1) and constants.Months enum may be used instead\n      of ints. Days are also 1-based. 4. A tuple of three int32 `Tensor`s\n      containing year, month and date as positive integers in that order. 5. A\n      single int32 `Tensor` containing ordinals (i.e. number of days since 31\n      Dec 0 with 1 being 1 Jan 1.)\n\n  Returns:\n    A `DateTensor` object representing the supplied dates.\n\n  Raises:\n    ValueError: If conversion fails for any reason.\n  """"""\n  if isinstance(date_inputs, DateTensor):\n    return date_inputs\n\n  if isinstance(date_inputs, np.ndarray):  # Case 2.\n    date_inputs = date_inputs.astype(""datetime64[D]"")\n    return from_np_datetimes(date_inputs)\n\n  if tf.is_tensor(date_inputs):  # Case 5\n    return from_ordinals(date_inputs)\n\n  if isinstance(date_inputs, collections.Sequence):\n    if not date_inputs:\n      return from_ordinals([])\n    test_element = date_inputs[0]\n    if hasattr(test_element, ""year""):  # Case 1.\n      return from_datetimes(date_inputs)\n    # Case 3\n    if isinstance(test_element, collections.Sequence):\n      return from_tuples(date_inputs)\n    if len(date_inputs) == 3:  # Case 4.\n      return from_year_month_day(date_inputs[0], date_inputs[1], date_inputs[2])\n  # As a last ditch effort, try to convert the sequence to a Tensor to see if\n  # that can work\n  try:\n    as_ordinals = tf.convert_to_tensor(date_inputs, dtype=tf.int32)\n    return from_ordinals(as_ordinals)\n  except ValueError as e:\n    raise ValueError(""Failed to convert inputs to DateTensor. ""\n                     ""Unrecognized format. Error: "" + e)\n\n\ndef from_datetimes(datetimes):\n  """"""Creates DateTensor from a sequence of Python datetime objects.\n\n  Args:\n    datetimes: Sequence of Python datetime objects.\n\n  Returns:\n    DateTensor object.\n\n  #### Example\n\n  ```python\n  import datetime\n\n  dates = [datetime.date(2015, 4, 15), datetime.date(2017, 12, 30)]\n  date_tensor = tff.datetime.dates_from_datetimes(dates)\n  ```\n  """"""\n  if isinstance(datetimes, (datetime.date, datetime.datetime)):\n    return from_year_month_day(datetimes.year, datetimes.month, datetimes.day,\n                               validate=False)\n  years = tf.constant([dt.year for dt in datetimes], dtype=tf.int32)\n  months = tf.constant([dt.month for dt in datetimes], dtype=tf.int32)\n  days = tf.constant([dt.day for dt in datetimes], dtype=tf.int32)\n\n  # datetime stores year, month and day internally, and datetime.toordinal()\n  # performs calculations. We use a tf routine to perform these calculations\n  # instead.\n  return from_year_month_day(years, months, days, validate=False)\n\n\ndef from_np_datetimes(np_datetimes):\n  """"""Creates DateTensor from a Numpy array of dtype datetime64.\n\n  Args:\n    np_datetimes: Numpy array of dtype datetime64.\n\n  Returns:\n    DateTensor object.\n\n  #### Example\n\n  ```python\n  import datetime\n  import numpy as np\n\n  date_tensor_np = np.array(\n    [[datetime.date(2019, 3, 25), datetime.date(2020, 6, 2)],\n     [datetime.date(2020, 9, 15), datetime.date(2020, 12, 27)]],\n     dtype=np.datetime64)\n\n  date_tensor = tff.datetime.dates_from_np_datetimes(date_tensor_np)\n  ```\n  """"""\n\n  # There\'s no easy way to extract year, month, day from numpy datetime, so\n  # we start with ordinals.\n  ordinals = tf.constant(np_datetimes, dtype=tf.int32) + _ORDINAL_OF_1_1_1970\n  return from_ordinals(ordinals, validate=False)\n\n\ndef from_tuples(year_month_day_tuples, validate=True):\n  """"""Creates DateTensor from a sequence of year-month-day Tuples.\n\n  Args:\n    year_month_day_tuples: Sequence of (year, month, day) Tuples. Months are\n      1-based; constants from Months enum can be used instead of ints. Days are\n      also 1-based.\n    validate: Whether to validate the dates.\n\n  Returns:\n    DateTensor object.\n\n  #### Example\n\n  ```python\n  date_tensor = tff.datetime.dates_from_tuples([(2015, 4, 15), (2017, 12, 30)])\n  ```\n  """"""\n  years, months, days = [], [], []\n  for t in year_month_day_tuples:\n    years.append(t[0])\n    months.append(t[1])\n    days.append(t[2])\n  years = tf.constant(years, dtype=tf.int32)\n  months = tf.constant(months, dtype=tf.int32)\n  days = tf.constant(days, dtype=tf.int32)\n  return from_year_month_day(years, months, days, validate)\n\n\ndef from_year_month_day(year, month, day, validate=True):\n  """"""Creates DateTensor from tensors of years, months and days.\n\n  Args:\n    year: Tensor of int32 type. Elements should be positive.\n    month: Tensor of int32 type of same shape as `year`. Elements should be in\n      range `[1, 12]`.\n    day: Tensor of int32 type of same shape as `year`. Elements should be in\n      range `[1, 31]` and represent valid dates together with corresponding\n      elements of `month` and `year` Tensors.\n    validate: Whether to validate the dates.\n\n  Returns:\n    DateTensor object.\n\n  #### Example\n\n  ```python\n  year = tf.constant([2015, 2017], dtype=tf.int32)\n  month = tf.constant([4, 12], dtype=tf.int32)\n  day = tf.constant([15, 30], dtype=tf.int32)\n  date_tensor = tff.datetime.dates_from_year_month_day(year, month, day)\n  ```\n  """"""\n  year = tf.convert_to_tensor(year, tf.int32)\n  month = tf.convert_to_tensor(month, tf.int32)\n  day = tf.convert_to_tensor(day, tf.int32)\n\n  control_deps = []\n  if validate:\n    control_deps.append(\n        tf.debugging.assert_positive(year, message=""Year must be positive.""))\n    control_deps.append(\n        tf.debugging.assert_greater_equal(\n            month,\n            constants.Month.JANUARY.value,\n            message=f""Month must be >= {constants.Month.JANUARY.value}""))\n    control_deps.append(\n        tf.debugging.assert_less_equal(\n            month,\n            constants.Month.DECEMBER.value,\n            message=""Month must be <= {constants.Month.JANUARY.value}""))\n    control_deps.append(\n        tf.debugging.assert_positive(day, message=""Day must be positive.""))\n    is_leap = date_utils.is_leap_year(year)\n    days_in_months = tf.constant(_DAYS_IN_MONTHS_COMBINED, tf.int32)\n    max_days = tf.gather(days_in_months,\n                         month + 12 * tf.dtypes.cast(is_leap, np.int32))\n    control_deps.append(\n        tf.debugging.assert_less_equal(\n            day, max_days, message=""Invalid day-month pairing.""))\n    with tf.compat.v1.control_dependencies(control_deps):\n      # Ensure years, months, days themselves are under control_deps.\n      year = tf.identity(year)\n      month = tf.identity(month)\n      day = tf.identity(day)\n\n  with tf.compat.v1.control_dependencies(control_deps):\n    ordinal = date_utils.year_month_day_to_ordinal(year, month, day)\n    return DateTensor(ordinal, year, month, day)\n\n\ndef from_ordinals(ordinals, validate=True):\n  """"""Creates DateTensor from tensors of ordinals.\n\n  Args:\n    ordinals: Tensor of type int32. Each value is number of days since 1 Jan\n      0001. 1 Jan 0001 has `ordinal=1`.\n    validate: Whether to validate the dates.\n\n  Returns:\n    DateTensor object.\n\n  #### Example\n\n  ```python\n  ordinals = tf.constant([\n    735703,  # 2015-4-12\n    736693   # 2017-12-30\n  ], dtype=tf.int32)\n\n  date_tensor = tff.datetime.dates_from_ordinals(ordinals)\n  ```\n  """"""\n  ordinals = tf.convert_to_tensor(ordinals, dtype=tf.int32)\n\n  control_deps = []\n  if validate:\n    control_deps.append(\n        tf.debugging.assert_positive(\n            ordinals, message=""Ordinals must be positive.""))\n    with tf.compat.v1.control_dependencies(control_deps):\n      ordinals = tf.identity(ordinals)\n\n  with tf.compat.v1.control_dependencies(control_deps):\n    years, months, days = date_utils.ordinal_to_year_month_day(ordinals)\n    return DateTensor(ordinals, years, months, days)\n\n\ndef from_tensor(tensor, validate=True):\n  """"""Creates DateTensor from a single tensor containing years, months and days.\n\n  This function is complementary to DateTensor.to_tensor: given an int32 Tensor\n  of shape (..., 3), creates a DateTensor. The three elements of the last\n  dimension are years, months and days, in this order.\n\n  Args:\n    tensor: Tensor of type int32 and shape (..., 3).\n    validate: Whether to validate the dates.\n\n  Returns:\n    DateTensor object.\n\n  #### Example\n\n  ```python\n  tensor = tf.constant([[2015, 4, 15], [2017, 12, 30]], dtype=tf.int32)\n  date_tensor = tff.datetime.dates_from_tensor(tensor)\n  ```\n  """"""\n  tensor = tf.convert_to_tensor(tensor, dtype=tf.int32)\n  return from_year_month_day(\n      year=tensor[..., 0],\n      month=tensor[..., 1],\n      day=tensor[..., 2],\n      validate=validate)\n\n\ndef random_dates(*, start_date, end_date, size=1, seed=None, name=None):\n  """"""Generates random dates between the supplied start and end dates.\n\n  Generates specified number of random dates between the given start and end\n  dates. The start and end dates are supplied as `DateTensor` objects. The dates\n  uniformly distributed between the start date (inclusive) and end date\n  (exclusive). Note that the dates are uniformly distributed over the calendar\n  range, i.e. no holiday calendar is taken into account.\n\n  Args:\n    start_date: DateTensor of arbitrary shape. The start dates of the range from\n      which to sample. The start dates are themselves included in the range.\n    end_date: DateTensor of shape compatible with the `start_date`. The end date\n      of the range from which to sample. The end dates are excluded from the\n      range.\n    size: Positive scalar int32 Tensor. The number of dates to draw between the\n      start and end date.\n      Default value: 1.\n    seed: Optional seed for the random generation.\n    name: Optional str. The name to give to the ops created by this function.\n      Default value: \'random_dates\'.\n\n  Returns:\n    A DateTensor of shape [size] + dates_shape where dates_shape is the common\n    broadcast shape for (start_date, end_date).\n\n  #### Example\n\n  ```python\n  # Note that the start and end dates need to be of broadcastable shape (though\n  # not necessarily the same shape).\n  # In this example, the start dates are of shape [2] and the end dates are\n  # of a compatible but non-identical shape [1].\n  start_dates = tff.datetime.dates_from_tuples([\n    (2020, 5, 16),\n    (2020, 6, 13)\n  ])\n  end_dates = tff.datetime.dates_from_tuples([(2021, 5, 21)])\n  size = 3  # Generate 3 dates for each pair of (start, end date).\n  sample = tff.datetime.random_dates(start_date=start_dates, end_date=end_dates,\n                              size=size)\n  # sample is a DateTensor of shape [3, 2]. The [3] is from the size and [2] is\n  # the common broadcast shape of start and end date.\n  ```\n  """"""\n  with tf.name_scope(name or ""random_dates""):\n    size = tf.reshape(\n        tf.convert_to_tensor(size, dtype=tf.int32, name=""size""), [-1])\n    start_date = convert_to_date_tensor(start_date)\n    end_date = convert_to_date_tensor(end_date)\n    # Note that tf.random.uniform cannot deal with non scalar max value with\n    # int dtypes. So we do this in float64 space and then floor. This incurs\n    # some non-uniformity of the distribution but for practical purposes this\n    # will be negligible.\n    ordinal_range = tf.cast(\n        end_date.ordinal() - start_date.ordinal(), dtype=tf.float64)\n    sample_shape = tf.concat((size, tf.shape(ordinal_range)), axis=0)\n    ordinal_sample = tf.cast(\n        tf.random.uniform(\n            sample_shape,\n            maxval=ordinal_range,\n            seed=seed,\n            name=""ordinal_sample"",\n            dtype=tf.float64),\n        dtype=tf.int32)\n    return from_ordinals(start_date.ordinal() + ordinal_sample, validate=False)\n'"
tf_quant_finance/datetime/date_tensor_test.py,11,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for date_tensor.py.""""""\n\nimport datetime\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tf_quant_finance.datetime import test_data\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\ndateslib = tff.datetime\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass DateTensorTest(tf.test.TestCase):\n\n  def test_convert_to_date_tensor_tuples(self):\n    inputs = [(2018, 5, 4), (2042, 11, 22), (1947, 8, 15)]\n    date_tensor = dateslib.convert_to_date_tensor(inputs)\n    y, m, d = zip(*inputs)\n    self.assert_date_tensor_components(date_tensor, y, m, d, None)\n\n  def test_convert_to_date_tensor_datetimes(self):\n    inputs = [\n        datetime.date(2018, 5, 4),\n        datetime.date(2042, 11, 22),\n        datetime.date(1947, 8, 15)\n    ]\n    date_tensor = dateslib.convert_to_date_tensor(inputs)\n    y, m, d = [2018, 2042, 1947], [5, 11, 8], [4, 22, 15]\n    self.assert_date_tensor_components(date_tensor, y, m, d, None)\n\n  def test_convert_to_date_tensor_ordinals(self):\n    inputs = [1, 2, 3, 4, 5]\n    inputs2 = tf.constant(inputs)\n    date_tensor = dateslib.convert_to_date_tensor(inputs)\n    date_tensor2 = dateslib.convert_to_date_tensor(inputs2)\n    self.assert_date_tensor_components(date_tensor, [1, 1, 1, 1, 1],\n                                       [1, 1, 1, 1, 1], [1, 2, 3, 4, 5], inputs)\n\n    self.assert_date_tensor_components(date_tensor2, [1, 1, 1, 1, 1],\n                                       [1, 1, 1, 1, 1], [1, 2, 3, 4, 5], inputs)\n\n  def test_convert_to_date_tensor_tensor_tuples(self):\n    inputs = [\n        tf.constant([2018, 2042, 1947]),\n        tf.constant([5, 11, 8]),\n        tf.constant([4, 22, 15])\n    ]\n    date_tensor = dateslib.convert_to_date_tensor(inputs)\n    y, m, d = [2018, 2042, 1947], [5, 11, 8], [4, 22, 15]\n    self.assert_date_tensor_components(date_tensor, y, m, d, None)\n\n  def test_convert_to_date_tensor_npdatetime(self):\n    inputs = np.array([\n        datetime.date(2018, 5, 4),\n        datetime.date(2042, 11, 22),\n        datetime.date(1947, 8, 15)\n    ],\n                      dtype=\'datetime64\')\n    date_tensor = dateslib.convert_to_date_tensor(inputs)\n    y, m, d = [2018, 2042, 1947], [5, 11, 8], [4, 22, 15]\n    self.assert_date_tensor_components(date_tensor, y, m, d, None)\n\n  def test_create_from_date_time_list(self):\n    dates = test_data.test_dates\n    y, m, d, o, datetimes = unpack_test_dates(dates)\n    date_tensor = dateslib.dates_from_datetimes(datetimes)\n    self.assert_date_tensor_components(date_tensor, y, m, d, o)\n\n  def test_create_from_date_time_scalar(self):\n    test_date = datetime.date(2018, 5, 4)\n    date_tensor = dateslib.dates_from_datetimes(test_date)\n    self.assertEqual(self.evaluate(date_tensor.year()), 2018)\n    self.assertEqual(self.evaluate(date_tensor.month()), 5)\n    self.assertEqual(self.evaluate(date_tensor.day()), 4)\n\n  def test_create_from_np_datetimes(self):\n    dates = test_data.test_dates\n    y, m, d, o, datetimes = unpack_test_dates(dates)\n    np_datetimes = np.array(datetimes, dtype=np.datetime64)\n    date_tensor = dateslib.dates_from_np_datetimes(np_datetimes)\n    self.assert_date_tensor_components(date_tensor, y, m, d, o)\n\n  def test_create_from_tuples(self):\n    dates = test_data.test_dates\n    y, m, d, o, _ = unpack_test_dates(dates)\n    date_tensor = dateslib.dates_from_tuples(dates)\n    self.assert_date_tensor_components(date_tensor, y, m, d, o)\n\n  def test_create_from_year_month_day(self):\n    dates = test_data.test_dates\n    y, m, d, o, _ = unpack_test_dates(dates)\n    date_tensor = dateslib.dates_from_year_month_day(y, m, d)\n    self.assert_date_tensor_components(date_tensor, y, m, d, o)\n\n  def test_create_from_ordinals(self):\n    dates = test_data.test_dates\n    y, m, d, o, _ = unpack_test_dates(dates)\n    date_tensor = dateslib.dates_from_ordinals(o)\n    self.assert_date_tensor_components(date_tensor, y, m, d, o)\n\n  def test_to_and_from_tensor(self):\n    dates = [[[2020, 1, 21], [2021, 2, 22], [2022, 3, 23]],\n             [[2023, 4, 24], [2024, 5, 25], [2025, 6, 26]]]\n    date_tensor = dateslib.dates_from_tensor(dates)\n\n    with self.subTest(\'from_tensor\'):\n      self.assert_date_tensor_components(\n          date_tensor,\n          [[2020, 2021, 2022], [2023, 2024, 2025]],\n          [[1, 2, 3], [4, 5, 6]],\n          [[21, 22, 23], [24, 25, 26]])\n\n    with self.subTest(\'to_tensor\'):\n      self.assertAllEqual(dates, date_tensor.to_tensor())\n\n  def test_validation(self):\n    not_raised = []\n    for y, m, d in test_data.invalid_dates:\n      try:\n        self.evaluate(dateslib.dates_from_tuples([(y, m, d)]).month())\n        not_raised.append((y, m, d))\n      except tf.errors.InvalidArgumentError:\n        pass\n    self.assertEmpty(not_raised)\n\n    for invalid_ordinal in [-5, 0]:\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        self.evaluate(dateslib.dates_from_ordinals([invalid_ordinal]).month())\n\n  def test_day_of_week(self):\n    dates = test_data.test_dates\n    datetimes = unpack_test_dates(dates)[-1]\n    date_tensor = dateslib.dates_from_datetimes(datetimes)\n    expected_day_of_week = np.array([dt.weekday() for dt in datetimes])\n    self.assertAllEqual(expected_day_of_week, date_tensor.day_of_week())\n\n  def test_days_until(self):\n    dates = test_data.test_dates\n    diffs = np.arange(0, len(dates))\n    _, _, _, o, datetimes = unpack_test_dates(dates)\n    date_tensor = dateslib.dates_from_datetimes(datetimes)\n\n    target_ordinals = o + diffs\n    target_datetimes = [datetime.date.fromordinal(o) for o in target_ordinals]\n    target_date_tensor = dateslib.dates_from_datetimes(target_datetimes)\n    self.assertAllEqual(diffs, date_tensor.days_until(target_date_tensor))\n\n  def test_days_addition(self):\n    self.perform_addition_test(test_data.day_addition_data,\n                               dateslib.PeriodType.DAY)\n\n  def test_week_addition(self):\n    self.perform_addition_test(test_data.week_addition_data,\n                               dateslib.PeriodType.WEEK)\n\n  def test_month_addition(self):\n    self.perform_addition_test(test_data.month_addition_data,\n                               dateslib.PeriodType.MONTH)\n\n  def test_year_addition(self):\n    self.perform_addition_test(test_data.year_addition_data,\n                               dateslib.PeriodType.YEAR)\n\n  def perform_addition_test(self, data, period_type):\n    dates_from, quantities, expected_dates = [], [], []\n    for date_from, quantity, expected_date in data:\n      dates_from.append(date_from)\n      quantities.append(quantity)\n      expected_dates.append(expected_date)\n\n    datetimes = unpack_test_dates(dates_from)[-1]\n    date_tensor = dateslib.dates_from_datetimes(datetimes)\n    period_tensor = dateslib.PeriodTensor(quantities, period_type)\n    result_date_tensor = date_tensor + period_tensor\n\n    y, m, d, o, _ = unpack_test_dates(expected_dates)\n    self.assert_date_tensor_components(result_date_tensor, y, m, d, o)\n\n  def test_date_subtraction(self):\n    # Subtraction trivially transforms to addition, so we don\'t test\n    # extensively.\n    dates_from = dateslib.dates_from_tuples([(2020, 3, 15), (2020, 3, 31)])\n    period = dateslib.PeriodTensor([2, 1], dateslib.PeriodType.MONTH)\n    expected_ordinals = np.array([datetime.date(2020, 1, 15).toordinal(),\n                                  datetime.date(2020, 2, 29).toordinal()])\n    self.assertAllEqual(expected_ordinals, (dates_from - period).ordinal())\n\n  def test_comparisons(self):\n    dates1 = dateslib.dates_from_tuples(\n        [(2020, 3, 15), (2020, 3, 31), (2021, 2, 28)])\n    dates2 = dateslib.dates_from_tuples(\n        [(2020, 3, 18), (2020, 3, 31), (2019, 2, 28)])\n    self.assertAllEqual(np.array([False, True, False]), dates1 == dates2)\n    self.assertAllEqual(np.array([True, False, True]), dates1 != dates2)\n    self.assertAllEqual(np.array([False, False, True]), dates1 > dates2)\n    self.assertAllEqual(np.array([False, True, True]), dates1 >= dates2)\n    self.assertAllEqual(np.array([True, False, False]), dates1 < dates2)\n    self.assertAllEqual(np.array([True, True, False]), dates1 <= dates2)\n\n  def test_tensor_wrapper_ops(self):\n    dates1 = dateslib.dates_from_tuples(\n        [(2019, 3, 25), (2020, 1, 2), (2019, 1, 2)])\n    dates2 = dateslib.dates_from_tuples(\n        [(2019, 4, 25), (2020, 5, 2), (2018, 1, 2)])\n    dates = dateslib.DateTensor.stack((dates1, dates2), axis=-1)\n    self.assertEqual((3, 2), dates.shape)\n    self.assertEqual((2,), dates[0].shape)\n    self.assertEqual((2, 2), dates[1:].shape)\n    self.assertEqual((2, 1), dates[1:, :-1].shape)\n    self.assertEqual((3, 1, 2), dates.expand_dims(axis=1).shape)\n    self.assertEqual((3, 3, 2), dates.broadcast_to((3, 3, 2)).shape)\n\n  def test_boolean_mask(self):\n    dates = dateslib.dates_from_tuples(\n        [(2019, 3, 25), (2020, 1, 2), (2019, 1, 2)])\n    mask = [True, False, True]\n    expected = dateslib.DateTensor.stack((dates[0], dates[2]))\n    self.assert_date_tensor_equals(expected, dates.boolean_mask(mask))\n\n  def test_day_of_year(self):\n    data = test_data.day_of_year_data\n    date_tuples, expected_days_of_year = zip(*data)\n    dates = dateslib.dates_from_tuples(date_tuples)\n    self.assertAllEqual(expected_days_of_year, dates.day_of_year())\n\n  def test_random_dates(self):\n    start_dates = dateslib.dates_from_tuples([(2020, 5, 16), (2020, 6, 13)])\n    end_dates = dateslib.dates_from_tuples([(2021, 5, 21)])\n    size = 3  # Generate 3 dates for each pair of (start, end date).\n    sample = dateslib.random_dates(\n        start_date=start_dates, end_date=end_dates, size=size, seed=42)\n    self.assertEqual(sample.shape, (3, 2))\n    self.assertTrue(self.evaluate(tf.reduce_all(sample < end_dates)))\n    self.assertTrue(self.evaluate(tf.reduce_all(sample >= start_dates)))\n\n  def test_is_end_of_month(self):\n    cases = test_data.end_of_month_test_cases\n    dates = dateslib.dates_from_tuples([case[0] for case in cases])\n    expected = tf.constant([case[1] for case in cases])\n    self.assertAllEqual(expected, dates.is_end_of_month())\n\n  def test_to_end_of_month(self):\n    cases = test_data.end_of_month_test_cases\n    dates = dateslib.dates_from_tuples([case[0] for case in cases])\n    expected = dateslib.dates_from_tuples([case[2] for case in cases])\n    self.assert_date_tensor_equals(expected, dates.to_end_of_month())\n\n  def assert_date_tensor_equals(self, expected_date_tensor, actual_date_tensor):\n    """"""Asserts given two DateTensors are equal.""""""\n    self.assertAllEqual(expected_date_tensor.ordinal(),\n                        actual_date_tensor.ordinal())\n\n  def assert_date_tensor_components(self, date_tensor, expected_years_np,\n                                    expected_months_np, expected_days_np,\n                                    expected_ordinals_np=None):\n    """"""Asserts given DateTensor has expected components.""""""\n    self.assertAllEqual(expected_years_np, date_tensor.year())\n    self.assertAllEqual(expected_months_np, date_tensor.month())\n    self.assertAllEqual(expected_days_np, date_tensor.day())\n    if expected_ordinals_np is not None:\n      self.assertAllEqual(expected_ordinals_np, date_tensor.ordinal())\n\n\ndef unpack_test_dates(dates):\n  y, m, d = (np.array([d[i] for d in dates], dtype=np.int32) for i in range(3))\n  datetimes = [datetime.date(y, m, d) for y, m, d in dates]\n  o = np.array([datetime.date(y, m, d).toordinal() for y, m, d in dates],\n               dtype=np.int32)\n  return y, m, d, o, datetimes\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/datetime/date_utils.py,16,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utilities for working with dates.""""""\n\nimport tensorflow.compat.v2 as tf\n\n_DAYS_IN_ERA = 146097  # Era is 400 years.\n_YEARS_IN_ERA = 400\n_DAYS_IN_YEAR = 365\n_DAYS_IN_4_YEARS = 4 * _DAYS_IN_YEAR + 1\n_DAYS_IN_100_YEARS = 100 * _DAYS_IN_YEAR + 24\n_ORDINAL_OF_1_3_0000 = -305\n\n\ndef _day_of_year_to_month(day_of_year):\n  # Converts day of year to month, when the year is counted from 1st of March,\n  # and both days and months are zero-based (i.e. for 1st of March,\n  # day_of_year = 0, and month = 0).\n  # The numbers 5, 2 and 153 are ""magic"" numbers: this linear function just\n  # happens to always give correct results, both for leap and non-leap years.\n  return (5 * day_of_year + 2) // 153\n\n\ndef _days_in_year_before_month(month):\n  # Calculates the number of days that there are between given month and 1st of\n  # March. Again, 153, 2, 5 are ""magic"" numbers.\n  return (153 * month + 2) // 5\n\n\ndef ordinal_to_year_month_day(ordinals):\n  """"""Calculates years, months and dates Tensor given ordinals Tensor.\n\n  Args:\n    ordinals: Tensor of int32 type. Each element is number of days since 1 Jan\n     0001. 1 Jan 0001 has `ordinal = 1`.\n\n  Returns:\n    Tuple (years, months, days), each element is an int32 Tensor of the same\n    shape as `ordinals`. `months` and `days` are one-based.\n  """"""\n  with tf.compat.v1.name_scope(None, ""o2ymd"", [ordinals]):\n    ordinals = tf.convert_to_tensor(ordinals, dtype=tf.int32, name=""ordinals"")\n\n    # The algorithm is adapted from\n    # http://howardhinnant.github.io/date_algorithms.html\n\n    # Take the fictional date of 1 March 0000 as reference and consider 1 March\n    # as start of the year. This simplifies computations.\n    ordinals -= _ORDINAL_OF_1_3_0000\n    era = ordinals // _DAYS_IN_ERA\n    day_of_era = ordinals % _DAYS_IN_ERA\n    year_of_era = (day_of_era - day_of_era // (_DAYS_IN_4_YEARS - 1)\n                   + day_of_era // _DAYS_IN_100_YEARS\n                   - day_of_era // (_DAYS_IN_ERA - 1)) // _DAYS_IN_YEAR\n    year = year_of_era + era * _YEARS_IN_ERA\n    day_of_year = day_of_era - (_DAYS_IN_YEAR * year_of_era + year_of_era // 4 -\n                                year_of_era // 100)\n    months = _day_of_year_to_month(day_of_year)\n    days = day_of_year - _days_in_year_before_month(months) + 1\n\n    # Go back from 1 March to 1 January as start of year.\n    months = months + tf.compat.v2.where(months < 10, 3, -9)\n    year += tf.compat.v2.where(months <= 2, 1, 0)\n    return year, months, days\n\n\ndef year_month_day_to_ordinal(year, month, day):\n  """"""Calculates ordinals Tensor given years, months and dates.\n\n  Args:\n    year: Tensor of int32 type. Elements should be positive.\n    month: Tensor of int32 type of same shape as `year`. Elements should be in\n      range `[1, 12]`.\n    day: Tensor of int32 type of same shape as `year`. Elements should be in\n      range `[1, 31]` and represent valid dates together with corresponding\n      elements of `month` and `year` Tensors.\n\n  Returns:\n    Tensor of int32 type. Each element is number of days since 1 Jan 0001. 1 Jan\n    0001 has `ordinal = 1`.\n  """"""\n  with tf.compat.v1.name_scope(None, ""ymd2o"", [year, month, day]):\n    year = tf.convert_to_tensor(year, tf.int32, name=""year"")\n    month = tf.convert_to_tensor(month, tf.int32, name=""month"")\n    day = tf.convert_to_tensor(day, tf.int32, name=""day"")\n\n    # The algorithm is adapted from\n    # http://howardhinnant.github.io/date_algorithms.html\n\n    # Take the fictional date of 1 March 0000 as reference and consider 1 March\n    # as start of the year. This simplifies computations.\n    year -= tf.compat.v2.where(month <= 2, 1, 0)\n    month += tf.compat.v2.where(month > 2, -3, 9)\n\n    era = year // _YEARS_IN_ERA\n    year_of_era = year % _YEARS_IN_ERA\n    day_of_year = _days_in_year_before_month(month) + day - 1\n    day_of_era = (year_of_era * _DAYS_IN_YEAR + year_of_era // 4\n                  - year_of_era // 100 + day_of_year)\n    return era * _DAYS_IN_ERA + day_of_era + _ORDINAL_OF_1_3_0000\n\n\ndef is_leap_year(years):\n  """"""Calculates whether years are leap years.\n\n  Args:\n    years: Tensor of int32 type. Elements should be positive.\n\n  Returns:\n    Tensor of bool type.\n  """"""\n  years = tf.convert_to_tensor(years, tf.int32)\n  def divides_by(n):\n    return tf.math.equal(years % n, 0)\n  return tf.math.logical_and(\n      divides_by(4), tf.math.logical_or(~divides_by(100), divides_by(400)))\n\n\ndef days_in_leap_years_between(start_date, end_date):\n  """"""Calculates number of days between two dates that fall on leap years.\n\n  \'start_date\' is included and \'end_date\' is excluded from the period.\n\n  For example, for dates `2019-12-24` and `2024-2-10` the result is\n  406: 366 days in 2020, 31 in Jan 2024 and 9 in Feb 2024.\n\n  If `end_date` is earlier than `start_date`, the result will be negative or\n  zero.\n\n  Args:\n    start_date: DateTensor.\n    end_date: DateTensor compatible with `start_date`.\n\n  Returns:\n    Tensor of type \'int32\'.\n  """"""\n  def days_in_leap_years_since_1jan0001(date):\n    prev_year = date.year() - 1\n    leap_years_before = prev_year // 4 - prev_year // 100 + prev_year // 400\n    n_leap_days = leap_years_before * 366\n\n    days_in_cur_year = date.day_of_year() - 1  # exclude current day.\n    n_leap_days += tf.where(is_leap_year(date.year()), days_in_cur_year, 0)\n    return n_leap_days\n\n  return (days_in_leap_years_since_1jan0001(end_date) -\n          days_in_leap_years_since_1jan0001(start_date))\n\n\ndef days_in_leap_and_nonleap_years_between(start_date, end_date):\n  """"""Calculates number of days that fall on leap and non-leap years.\n\n  Calculates a tuple \'(days_in_leap_years, days_in_nonleap_years)\'.\n  \'start_date\' is included and \'end_date\' is excluded from the period.\n\n  For example, for dates `2019-12-24` and `2024-2-10` the result is\n  (406, 1103):\n  406 = 366 days in 2020 + 31 in Jan 2024 + 9 in Feb 2024,\n  1103 = 8 in 2019 + 365 in 2021 + 365 in 2022 + 365 in 2023.\n\n  If `end_date` is earlier than `start_date`, the result will be negative or\n  zero.\n\n  Args:\n    start_date: DateTensor.\n    end_date: DateTensor compatible with `start_date`.\n\n  Returns:\n    Tuple of two Tensors of type \'int32\'.\n  """"""\n  days_between = end_date.ordinal() - start_date.ordinal()\n  days_in_leap_years = days_in_leap_years_between(start_date, end_date)\n  return days_in_leap_years, days_between - days_in_leap_years\n\n\ndef leap_days_between(start_date, end_date):\n  """"""Calculates number of leap days (29 Feb) between two dates.\n\n  \'start_date\' is included and \'end_date\' is excluded from the period.\n\n  For example, for dates `2019-12-24` and `2024-3-10` the result is\n  2: there is 29 Feb 2020 and 29 Feb 2024 between 24 Dec 2019 (inclusive) and\n  10 Mar 2024 (exclusive).\n\n  If `end_date` is earlier than `start_date`, the result will be negative or\n  zero.\n\n  Args:\n    start_date: DateTensor.\n    end_date: DateTensor compatible with `start_date`.\n\n  Returns:\n    Tensor of type \'int32\'.\n  """"""\n  def leap_days_since_year_0(date_tensor):\n    year = date_tensor.year()\n    month = date_tensor.month()\n    leap_years_since_0 = year // 4 - year // 100 + year // 400\n    needs_adjustment = (is_leap_year(year) & (month <= 2))\n    return leap_years_since_0 - tf.where(needs_adjustment, 1, 0)\n  return leap_days_since_year_0(end_date) - leap_days_since_year_0(start_date)\n\n\n__all__ = [\n    ""ordinal_to_year_month_day"",\n    ""year_month_day_to_ordinal"",\n    ""is_leap_year"",\n    ""days_in_leap_years_between"",\n    ""days_in_leap_and_nonleap_years_between"",\n    ""leap_days_between"",\n]\n'"
tf_quant_finance/datetime/date_utils_test.py,3,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for date_utils.py.""""""\n\nimport datetime\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tf_quant_finance.datetime import test_data\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\ndates = tff.datetime\ndate_utils = tff.datetime.utils\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass DateUtilsTest(tf.test.TestCase):\n\n  def test_ordinal_to_year_month_day(self):\n    date_tuples = test_data.test_dates\n    ordinals = np.array(\n        [datetime.date(y, m, d).toordinal() for y, m, d in date_tuples],\n        dtype=np.int32)\n    y, m, d = date_utils.ordinal_to_year_month_day(ordinals)\n    result = tf.stack((y, m, d), axis=1)\n    self.assertAllEqual(date_tuples, result)\n\n  def test_year_month_day_to_ordinal(self):\n    date_tuples = test_data.test_dates\n    expected = np.array(\n        [datetime.date(y, m, d).toordinal() for y, m, d in date_tuples],\n        dtype=np.int32)\n    dates_np = np.array(date_tuples)\n    years, months, days = dates_np[:, 0], dates_np[:, 1], dates_np[:, 2]\n    actual = date_utils.year_month_day_to_ordinal(years, months, days)\n    self.assertAllEqual(expected, actual)\n\n  def test_is_leap_year(self):\n    years = np.array([\n        1600, 1700, 1800, 1900, 1901, 1903, 1904, 1953, 2000, 2020, 2025, 2100\n    ])\n    expected = np.array([\n        True, False, False, False, False, False, True, False, True, True, False,\n        False\n    ])\n    self.assertAllEqual(\n        expected, date_utils.is_leap_year(years))\n\n  def test_days_in_leap_years_between(self):\n    test_cases = test_data.days_in_leap_years_test_cases\n    date_tuples1, date_tuples2, expected_num_days = [], [], []\n    for case in test_cases:\n      date_tuples1.append(case[""date1""])\n      date_tuples2.append(case[""date2""])\n      expected_num_days.append(case[""expected""])\n    dates1 = dates.dates_from_tuples(date_tuples1)\n    dates2 = dates.dates_from_tuples(date_tuples2)\n    actual_num_days = date_utils.days_in_leap_years_between(dates1, dates2)\n    self.assertAllEqual(expected_num_days, actual_num_days)\n\n  def test_leap_days_between(self):\n    test_cases = test_data.leap_days_between_dates_test_cases\n    date_tuples1, date_tuples2, expected_num_days = [], [], []\n    for case in test_cases:\n      date_tuples1.append(case[""date1""])\n      date_tuples2.append(case[""date2""])\n      expected_num_days.append(case[""expected""])\n    dates1 = dates.dates_from_tuples(date_tuples1)\n    dates2 = dates.dates_from_tuples(date_tuples2)\n    actual_num_days = date_utils.leap_days_between(dates1, dates2)\n    self.assertAllEqual(expected_num_days, actual_num_days)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/datetime/daycounts.py,24,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Day Count Conventions.\n\nDay count conventions are a system for determining how a coupon accumulates over\na coupon period. They can also be seen as a method for converting date\ndifferences to elapsed time. For example, suppose we need to calculate the total\ninterest accrued over a period of 5 months starting from 6 Jan, 2020 to\n8 June, 2020 given that the interest rate is quoted at 4% annually on a\nprincipal of $100. Without the day count convention, we do not know how to\ndivide the total annual interest of $4 for the five month period. As an example\nof the ambiguity, should the pro-rating be done by the total number of days\nor by total number of months (or by some other metric)? The answer to this is\nprovided by assigning a specific day count convention to the quoted rate. For\nexample, one could use the Money market basis (Actual/360) which states that the\nelapsed period for interest accrual between two dates D1 and D2 is the ratio\nof the actual number of days between D1 and D2 and 360. For our example, it\nleads to `154 / 360 = 0.4278`. Hence the accumulated interest is\n`100 * 0.04 * 0.4278 = $1.71. For more details on the many conventions used, see\nRef. [1] and [2].\n\nThe functions in this module provide implementations of the commonly used day\ncount conventions. Some of the conventions also require a knowledge of the\npayment schedule to be specified (e.g. Actual/Actual ISMA as in Ref [3] below.).\n\n## References\n\n[1] Wikipedia Contributors. Day Count Conventions. Available at:\n  https://en.wikipedia.org/wiki/Day_count_convention\n[2] ISDA, ISDA Definitions 2006.\n  https://www.isda.org/book/2006-isda-definitions/\n[3] ISDA, EMU and Market Conventions: Recent Developments,\n  https://www.isda.org/a/AIJEE/1998-ISDA-memo-%E2%80%9CEMU-and-Market-Conventions-Recent-Developments%E2%80%9D.pdf\n""""""\n\n# TODO(b/149382857): Move these implementations to use an interface.\n\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance.datetime import date_tensor as dt\nfrom tf_quant_finance.datetime import date_utils as du\nfrom tf_quant_finance.datetime import periods\n\n\ndef actual_360(*,\n               start_date,\n               end_date,\n               schedule_info=None,\n               dtype=None,\n               name=None):\n  """"""Computes the year fraction between the specified dates.\n\n  The actual/360 convention specifies the year fraction between the start and\n  end date as the actual number of days between the two dates divided by 360.\n\n  Note that the schedule info is not needed for this convention and is ignored\n  if supplied.\n\n  For more details see:\n  https://en.wikipedia.org/wiki/Day_count_convention#Actual/360\n\n  Args:\n    start_date: A `DateTensor` object of any shape.\n    end_date: A `DateTensor` object of compatible shape with `start_date`.\n    schedule_info: The schedule info. Ignored for this convention.\n    dtype: The dtype of the result. Either `tf.float32` or `tf.float64`. If not\n      supplied, `tf.float32` is returned.\n    name: Python `str` name prefixed to ops created by this function. If not\n      supplied, `actual_360` is used.\n\n  Returns:\n    A real `Tensor` of supplied `dtype` and shape of `start_date`. The year\n    fraction between the start and end date as computed by Actual/360\n    convention.\n  """"""\n  del schedule_info\n  with tf.name_scope(name or \'actual_360\'):\n    end_date = dt.convert_to_date_tensor(end_date)\n    start_date = dt.convert_to_date_tensor(start_date)\n    dtype = dtype or tf.constant(0.).dtype\n    actual_days = tf.cast(start_date.days_until(end_date), dtype=dtype)\n    return actual_days / 360\n\n\ndef actual_365_fixed(*,\n                     start_date,\n                     end_date,\n                     schedule_info=None,\n                     dtype=None,\n                     name=None):\n  """"""Computes the year fraction between the specified dates.\n\n  The actual/365 convention specifies the year fraction between the start and\n  end date as the actual number of days between the two dates divided by 365.\n\n  Note that the schedule info is not needed for this convention and is ignored\n  if supplied.\n\n  For more details see:\n  https://en.wikipedia.org/wiki/Day_count_convention#Actual/365_Fixed\n\n  Args:\n    start_date: A `DateTensor` object of any shape.\n    end_date: A `DateTensor` object of compatible shape with `start_date`.\n    schedule_info: The schedule info. Ignored for this convention.\n    dtype: The dtype of the result. Either `tf.float32` or `tf.float64`. If not\n      supplied, `tf.float32` is returned.\n    name: Python `str` name prefixed to ops created by this function. If not\n      supplied, `actual_365_fixed` is used.\n\n  Returns:\n    A real `Tensor` of supplied `dtype` and shape of `start_date`. The year\n    fraction between the start and end date as computed by Actual/365 fixed\n    convention.\n  """"""\n  del schedule_info\n  with tf.name_scope(name or \'actual_365_fixed\'):\n    end_date = dt.convert_to_date_tensor(end_date)\n    start_date = dt.convert_to_date_tensor(start_date)\n    dtype = dtype or tf.constant(0.).dtype\n    actual_days = tf.cast(start_date.days_until(end_date), dtype=dtype)\n    return actual_days / 365\n\n\ndef actual_365_actual(*,\n                      start_date,\n                      end_date,\n                      schedule_info=None,\n                      dtype=None,\n                      name=None):\n  """"""Computes the year fraction between the specified dates.\n\n  The actual/365 actual convention specifies the year fraction between the\n  start and end date as the actual number of days between the two dates divided\n  365 if no leap day is contained in the date range and 366 otherwise.\n\n  When determining whether a leap day is contained in the date range,\n  \'start_date\' is excluded and \'end_date\' is included.\n\n  Note that the schedule info is not needed for this convention and is ignored\n  if supplied.\n\n  Args:\n    start_date: A `DateTensor` object of any shape.\n    end_date: A `DateTensor` object of compatible shape with `start_date`.\n    schedule_info: The schedule info. Ignored for this convention.\n    dtype: The dtype of the result. Either `tf.float32` or `tf.float64`. If not\n      supplied, `tf.float32` is returned.\n    name: Python `str` name prefixed to ops created by this function. If not\n      supplied, `actual_365_actual` is used.\n\n  Returns:\n    A real `Tensor` of supplied `dtype` and shape of `start_date`. The year\n    fraction between the start and end date as computed by Actual/365 Actual\n    convention.\n  """"""\n  del schedule_info\n  with tf.name_scope(name or \'actual_365_actual\'):\n    end_date = dt.convert_to_date_tensor(end_date)\n    start_date = dt.convert_to_date_tensor(start_date)\n    dtype = dtype or tf.constant(0.).dtype\n    actual_days = tf.cast(start_date.days_until(end_date), dtype=dtype)\n    # Add a day to start_date and end_date so that start_date is excluded and\n    # end_date is included.\n    day = periods.day()\n    leap_days_between = du.leap_days_between(\n        start_date=start_date + day, end_date=end_date + day)\n    denominator = tf.cast(tf.where(leap_days_between > 0, 366, 365),\n                          dtype=dtype)\n    return actual_days / denominator\n\n\ndef thirty_360_isda(*,\n                    start_date,\n                    end_date,\n                    schedule_info=None,\n                    dtype=None,\n                    name=None):\n  """"""Computes the year fraction between the specified dates.\n\n  The 30/360 (ISDA / Bond Basis) convention specifies the year fraction\n  between the start and end date as the number of days by the following\n  formula between the two dates divided by 360.\n\n    day difference = (Y2 - Y1) * 360 + (M2 - M1) * 30 + (D2 - D1)\n\n  where\n\n    Y1 is the year, expressed as a number, of the start date;\n\n    Y2 is the year, expressed as a number, of the end date;\n\n    M1 is the calendar month, expressed as a number, of the start date;\n\n    M2 is the calendar month, expressed as a number of the last date;\n\n    D1 is the start date calendar day, unless such number would be 31, in\n    which case D1 will be 30;\n\n    D2 is the last date calendar day, unless such number would be 31 and D1\n    is either 30 or 31, in which case D2 will be 30\n\n  Note that the schedule info is not needed for this convention and is ignored\n  if supplied.\n\n  For more details see:\n  https://en.wikipedia.org/wiki/Day_count_convention#30/360_Bond_Basis\n  https://www.isda.org/2008/12/22/30-360-day-count-conventions\n\n  Args:\n    start_date: A `DateTensor` object of any shape.\n    end_date: A `DateTensor` object of compatible shape with `start_date`.\n    schedule_info: The schedule info. Ignored for this convention.\n    dtype: The dtype of the result. Either `tf.float32` or `tf.float64`. If not\n      supplied, `tf.float32` is returned.\n    name: Python `str` name prefixed to ops created by this function. If not\n      supplied, `thirty_360_isda` is used.\n\n  Returns:\n    A real `Tensor` of supplied `dtype` and shape of `start_date`. The year\n    fraction between the start and end date as computed by 30/360 convention.\n  """"""\n  del schedule_info\n  with tf.name_scope(name or \'thirty_360_isda\'):\n    d1_days = tf.minimum(start_date.day(), 30)\n    d2_days = tf.where(\n        tf.equal(d1_days, 30) & tf.equal(end_date.day(), 31),\n        30,\n        end_date.day()\n    )\n\n    day_difference = (d2_days - d1_days)\n    month_difference = (end_date.month() - start_date.month()) * 30\n    year_difference = (end_date.year() - start_date.year()) * 360\n    dtype = dtype or tf.constant(0.).dtype\n    total_day_difference = tf.cast(\n        day_difference + month_difference + year_difference,\n        dtype=dtype\n    )\n\n    return total_day_difference / 360\n\n\n__all__ = [\n    \'actual_360\',\n    \'actual_365_actual\',\n    \'actual_365_fixed\',\n    \'thirty_360_isda\',\n]\n'"
tf_quant_finance/datetime/daycounts_test.py,2,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for daycounts.py.""""""\n\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\ndateslib = tff.datetime\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass DayCountsTest(tf.test.TestCase):\n\n  def test_actual_360(self):\n    start_date = dateslib.dates_from_tuples([(2019, 12, 17)])\n    end_date = dateslib.dates_from_tuples([(2021, 2, 11)])\n    yf = self.evaluate(\n        dateslib.daycount_actual_360(start_date=start_date, end_date=end_date))\n    self.assertAllClose(yf, [1.17222], atol=1e-5)\n\n  def test_actual_365_fixed(self):\n    start_date = dateslib.dates_from_tuples([(2019, 12, 17)])\n    end_date = dateslib.dates_from_tuples([(2021, 2, 11)])\n    yf = self.evaluate(\n        dateslib.daycount_actual_365_fixed(\n            start_date=start_date, end_date=end_date))\n    self.assertAllClose(yf, [1.156164], atol=1e-5)\n\n  def test_actual_365_actual_with_leap_day(self):\n    start_date = dateslib.dates_from_tuples([(2019, 12, 17)])\n    end_date = dateslib.dates_from_tuples([(2021, 2, 11)])\n    yf = self.evaluate(\n        dateslib.daycount_actual_365_actual(\n            start_date=start_date, end_date=end_date))\n    self.assertAllClose(yf, [1.153005], atol=1e-5)\n\n  def test_actual_365_actual_no_leap_day(self):\n    start_date = dateslib.dates_from_tuples([(2020, 3, 17)])\n    end_date = dateslib.dates_from_tuples([(2021, 2, 17)])\n    yf = self.evaluate(\n        dateslib.daycount_actual_365_actual(\n            start_date=start_date, end_date=end_date))\n    self.assertAllClose(yf, [0.923288], atol=1e-5)\n\n  def test_actual_365_actual_with_leap_day_on_start(self):\n    start_date = dateslib.dates_from_tuples([(2020, 2, 29)])\n    end_date = dateslib.dates_from_tuples([(2021, 2, 11)])\n    yf = self.evaluate(\n        dateslib.daycount_actual_365_actual(\n            start_date=start_date, end_date=end_date))\n    self.assertAllClose(yf, [0.953424], atol=1e-5)\n\n  def test_actual_365_actual_with_leap_day_on_end(self):\n    start_date = dateslib.dates_from_tuples([(2019, 2, 11)])\n    end_date = dateslib.dates_from_tuples([(2020, 2, 29)])\n    yf = self.evaluate(\n        dateslib.daycount_actual_365_actual(\n            start_date=start_date, end_date=end_date))\n    self.assertAllClose(yf, [1.046448], atol=1e-5)\n\n  def test_thirty_360_isda_no_leap_year(self):\n    """"""Test 30/360 ISDA on start / last dates without leap year.\n\n    The test cases and results are from\n    https://www.isda.org/2008/12/22/30-360-day-count-conventions\n    """"""\n    start_date = dateslib.dates_from_tuples([\n        (2007, 1, 15),\n        (2007, 1, 15),\n        (2007, 1, 15),\n        (2007, 9, 30),\n        (2007, 9, 30),\n        (2007, 9, 30),\n    ])\n    end_date = dateslib.dates_from_tuples([\n        (2007, 1, 30),\n        (2007, 2, 15),\n        (2007, 7, 15),\n        (2008, 3, 31),\n        (2007, 10, 31),\n        (2008, 9, 30),\n    ])\n    yf = self.evaluate(\n        dateslib.daycount_thirty_360_isda(\n            start_date=start_date, end_date=end_date))\n    self.assertAllClose(yf, [\n        0.041666667,\n        0.083333333,\n        0.5,\n        0.5,\n        0.083333333,\n        1.0], atol=1e-5)\n\n  def test_thirty_360_isda_with_leap_year_on_start(self):\n    """"""Test 30/360 ISDA on start dates in leap year.\n\n    The test cases and results are from\n    https://www.isda.org/2008/12/22/30-360-day-count-conventions\n    """"""\n    start_date = dateslib.dates_from_tuples([\n        (2008, 2, 29),\n        (2008, 2, 29),\n        (2008, 2, 29),\n    ])\n    end_date = dateslib.dates_from_tuples([\n        (2009, 2, 28),\n        (2008, 3, 30),\n        (2008, 3, 31),\n    ])\n    yf = self.evaluate(\n        dateslib.daycount_thirty_360_isda(\n            start_date=start_date, end_date=end_date))\n    self.assertAllClose(yf, [\n        0.997222222,\n        0.086111111,\n        0.088888889], atol=1e-5)\n\n  def test_thirty_360_isda_with_leap_year_on_end(self):\n    """"""Test 30/360 ISDA on last dates in leap year.\n\n    The test cases and results are from\n    https://www.isda.org/2008/12/22/30-360-day-count-conventions\n    """"""\n    start_date = dateslib.dates_from_tuples([\n        (2007, 2, 26),\n        (2007, 8, 31),\n    ])\n    end_date = dateslib.dates_from_tuples([\n        (2008, 2, 29),\n        (2008, 2, 29),\n    ])\n    yf = self.evaluate(\n        dateslib.daycount_thirty_360_isda(\n            start_date=start_date, end_date=end_date))\n    self.assertAllClose(yf, [\n        1.008333333,\n        0.497222222], atol=1e-5)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/datetime/holiday_calendar.py,0,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""HolidayCalendar definition.""""""\n\nimport abc\n\nfrom tf_quant_finance.datetime import constants\n\n\nclass HolidayCalendar(abc.ABC):\n  """"""Represents a holiday calendar.\n\n  Provides methods for manipulating the dates taking into account the holidays,\n  and the business day roll conventions. Weekends are treated as holidays.\n  """"""\n\n  @abc.abstractmethod\n  def is_business_day(self, date_tensor):\n    """"""Returns a tensor of bools for whether given dates are business days.""""""\n    pass\n\n  @abc.abstractmethod\n  def roll_to_business_day(self, date_tensor, roll_convention):\n    """"""Rolls the given dates to business dates according to given convention.\n\n    Args:\n      date_tensor: DateTensor of dates to roll from.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting DateTensor.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def add_period_and_roll(self,\n                          date_tensor,\n                          period_tensor,\n                          roll_convention=constants.BusinessDayConvention.NONE):\n    """"""Adds given periods to given dates and rolls to business days.\n\n    The original dates are not rolled prior to addition.\n\n    Args:\n      date_tensor: DateTensor of dates to add to.\n      period_tensor: PeriodTensor broadcastable to `date_tensor`.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting DateTensor.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def add_business_days(self,\n                        date_tensor,\n                        num_days,\n                        roll_convention=constants.BusinessDayConvention.NONE):\n    """"""Adds given number of business days to given dates.\n\n    Note that this is different from calling `add_period_and_roll` with\n    PeriodType.DAY. For example, adding 5 business days to Monday gives the next\n    Monday (unless there are holidays on this week or next Monday). Adding 5\n    days and rolling means landing on Saturday and then rolling either to next\n    Monday or to Friday of the same week, depending on the roll convention.\n\n    If any of the dates in `date_tensor` are not business days, they will be\n    rolled to business days before doing the addition. If `roll_convention` is\n    `NONE`, and any dates are not business days, an exception is raised.\n\n    Args:\n      date_tensor: DateTensor of dates to advance from.\n      num_days: Tensor of int32 type broadcastable to `date_tensor`.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting DateTensor.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def subtract_period_and_roll(\n      self,\n      date_tensor,\n      period_tensor,\n      roll_convention=constants.BusinessDayConvention.NONE):\n    """"""Subtracts given periods from given dates and rolls to business days.\n\n    The original dates are not rolled prior to subtraction.\n\n    Args:\n      date_tensor: DateTensor of dates to subtract from.\n      period_tensor: PeriodTensor broadcastable to `date_tensor`.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting DateTensor.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def subtract_business_days(\n      self,\n      date_tensor,\n      num_days,\n      roll_convention=constants.BusinessDayConvention.NONE):\n    """"""Adds given number of business days to given dates.\n\n    Note that this is different from calling `subtract_period_and_roll` with\n    PeriodType.DAY. For example, subtracting 5 business days from Friday gives\n    the previous Friday (unless there are holidays on this week or previous\n    Friday). Subtracting 5 days and rolling means landing on Sunday and then\n    rolling either to Monday or to Friday, depending on the roll convention.\n\n    If any of the dates in `date_tensor` are not business days, they will be\n    rolled to business days before doing the subtraction. If `roll_convention`\n    is `NONE`, and any dates are not business days, an exception is raised.\n\n    Args:\n      date_tensor: DateTensor of dates to advance from.\n      num_days: Tensor of int32 type broadcastable to `date_tensor`.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting DateTensor.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def business_days_in_period(self, date_tensor, period_tensor):\n    """"""Calculates number of business days in a period.\n\n    Includes the dates in `date_tensor`, but excludes final dates resulting from\n    addition of `period_tensor`.\n\n    Args:\n      date_tensor: DateTensor of starting dates.\n      period_tensor: PeriodTensor, should be broadcastable to `date_tensor`.\n\n    Returns:\n       An int32 Tensor with the number of business days in given periods that\n       start at given dates.\n\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def business_days_between(self, from_dates, to_dates):\n    """"""Calculates number of business between pairs of dates.\n\n    For each pair, the initial date is included in the difference, and the final\n    date is excluded. If the final date is the same or earlier than the initial\n    date, zero is returned.\n\n    Args:\n      from_dates: DateTensor of initial dates.\n      to_dates: DateTensor of final dates, should be broadcastable to\n        `from_dates`.\n\n    Returns:\n       An int32 Tensor with the number of business days between the\n       corresponding pairs of dates.\n    """"""\n    pass\n'"
tf_quant_finance/datetime/holiday_calendar_factory.py,1,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Factory for HolidayCalendar implementations.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.datetime import bounded_holiday_calendar\nfrom tf_quant_finance.datetime import unbounded_holiday_calendar\n\n\ndef create_holiday_calendar(\n    weekend_mask=None,\n    holidays=None,\n    start_year=None,\n    end_year=None):\n  """"""Creates a holiday calendar.\n\n  Note: providing bounds for the calendar, i.e. `holidays` and/or `start_year`,\n  `end_year` yields a better-performing calendar.\n\n  Args:\n    weekend_mask: Boolean `Tensor` of 7 elements one for each day of the week\n      starting with Monday at index 0. A `True` value indicates the day is\n      considered a weekend day and a `False` value implies a week day.\n      Default value: None which means no weekends are applied.\n    holidays: Defines the holidays that are added to the weekends defined by\n      `weekend_mask`. An instance of `dates.DateTensor` or an object\n      convertible to `DateTensor`.\n      Default value: None which means no holidays other than those implied by\n      the weekends (if any).\n      Note that it is necessary to provide holidays for each year, and also\n      adjust the holidays that fall on the weekends if required, e.g.\n      2021-12-25 to 2021-12-24. To avoid doing this manually one can use\n      AbstractHolidayCalendar from Pandas:\n\n      ```python\n      from pandas.tseries.holiday import AbstractHolidayCalendar\n      from pandas.tseries.holiday import Holiday\n      from pandas.tseries.holiday import nearest_workday\n\n      class MyCalendar(AbstractHolidayCalendar):\n          rules = [\n              Holiday(\'NewYear\', month=1, day=1, observance=nearest_workday),\n              Holiday(\'Christmas\', month=12, day=25,\n                       observance=nearest_workday)\n          ]\n\n      calendar = MyCalendar()\n      holidays_index = calendar.holidays(\n          start=datetime.date(2020, 1, 1),\n          end=datetime.date(2030, 12, 31))\n      holidays = np.array(holidays_index.to_pydatetime(), dtype=""<M8[D]"")\n      ```\n\n    start_year: Integer giving the earliest year this calendar includes. If\n      `holidays` is specified, then `start_year` and `end_year` are ignored,\n      and the boundaries are derived from `holidays`.\n      Default value: None which means start year is inferred from `holidays`, if\n      present.\n    end_year: Integer giving the latest year this calendar includes. If\n      `holidays` is specified, then `start_year` and `end_year` are ignored,\n      and the boundaries are derived from `holidays`.\n      Default value: None which means start year is inferred from `holidays`, if\n      present.\n\n  Returns:\n    A HolidayCalendar instance.\n  """"""\n  # Choose BoundedHolidayCalendar if possible, for better performance, otherwise\n  # choose UnboundedHolidayCalendar.\n  is_bounded = (_tensor_is_not_empty(holidays) or\n                (start_year is not None and end_year is not None))\n  if is_bounded:\n    return bounded_holiday_calendar.BoundedHolidayCalendar(\n        weekend_mask, holidays, start_year, end_year)\n  return unbounded_holiday_calendar.UnboundedHolidayCalendar(\n      weekend_mask, holidays)\n\n\ndef _tensor_is_not_empty(t):\n  """"""Returns whether t is definitely not empty.""""""\n  # False means either empty or unknown.\n  if t is None:\n    return False\n  if isinstance(t, np.ndarray):\n    return t.size > 0\n  if isinstance(t, tf.Tensor):\n    num_elem = t.shape.num_elements\n    return num_elem is not None and num_elem > 0  # None means shape is unknown.\n  return bool(t)\n'"
tf_quant_finance/datetime/holiday_calendar_test.py,9,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for HolidayCalendar implementations.""""""\n\nimport datetime\nimport functools\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tf_quant_finance.datetime import bounded_holiday_calendar\nfrom tf_quant_finance.datetime import test_data\nfrom tf_quant_finance.datetime import unbounded_holiday_calendar\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\ndates = tff.datetime\n\n\ndef test_both_impls(test_fn):\n  # Decorator to run the test with both BoundedHolidayCalendar and\n  # UnboundedHolidayCalendar.\n  # Create the calendar as `self.impl(args)`.\n  def create_unbounded_calendar(**kwargs):\n    kwargs.pop(""start_year"", None)\n    kwargs.pop(""end_year"", None)\n    return unbounded_holiday_calendar.UnboundedHolidayCalendar(**kwargs)\n\n  @functools.wraps(test_fn)\n  def wrapped(*args, **kwargs):\n    self = args[0]\n    with self.subTest(""Bounded""):\n      self.impl = bounded_holiday_calendar.BoundedHolidayCalendar\n      test_fn(*args, **kwargs)\n    with self.subTest(""Unbounded""):\n      self.impl = create_unbounded_calendar\n      test_fn(*args, **kwargs)\n  return wrapped\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass HolidayCalendarTest(tf.test.TestCase, parameterized.TestCase):\n\n  rolling_test_parameters = {\n      ""testcase_name"": ""unadjusted"",\n      ""rolling_enum_value"": dates.BusinessDayConvention.NONE,\n      ""data_key"": ""unadjusted""\n  }, {\n      ""testcase_name"": ""following"",\n      ""rolling_enum_value"": dates.BusinessDayConvention.FOLLOWING,\n      ""data_key"": ""following""  # See test_data.adjusted_dates_data\n  }, {\n      ""testcase_name"": ""modified_following"",\n      ""rolling_enum_value"": dates.BusinessDayConvention.MODIFIED_FOLLOWING,\n      ""data_key"": ""modified_following""\n  }, {\n      ""testcase_name"": ""preceding"",\n      ""rolling_enum_value"": dates.BusinessDayConvention.PRECEDING,\n      ""data_key"": ""preceding""\n  }, {\n      ""testcase_name"": ""modified_preceding"",\n      ""rolling_enum_value"": dates.BusinessDayConvention.MODIFIED_PRECEDING,\n      ""data_key"": ""modified_preceding""\n  }\n\n  @parameterized.named_parameters({\n      ""testcase_name"": ""as_tuples"",\n      ""holidays"": [(2020, 1, 1), (2020, 12, 25), (2021, 1, 1)],\n  }, {\n      ""testcase_name"": ""as_datetimes"",\n      ""holidays"": [datetime.date(2020, 1, 1),\n                   datetime.date(2020, 12, 25),\n                   datetime.date(2021, 1, 1)],\n  }, {\n      ""testcase_name"": ""as_numpy_array"",\n      ""holidays"": np.array([""2020-01-01"", ""2020-12-25"", ""2021-01-01""],\n                           dtype=np.datetime64),\n  }, {\n      ""testcase_name"": ""as_date_tensors"",\n      ""holidays"": [(2020, 1, 1), (2020, 12, 25), (2021, 1, 1)],\n      ""convert_to_date_tensor"": True,  # Can\'t do this in parameter definition.\n  })\n  @test_both_impls\n  def test_providing_holidays(self, holidays, convert_to_date_tensor=False):\n    if convert_to_date_tensor:\n      holidays = dates.convert_to_date_tensor(holidays)\n    cal = self.impl(\n        weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY, holidays=holidays)\n    date_tensor = dates.dates_from_tuples([(2020, 1, 1), (2020, 5, 1),\n                                           (2020, 12, 25), (2021, 3, 8),\n                                           (2021, 1, 1)])\n    self.assertAllEqual([False, True, False, True, False],\n                        cal.is_business_day(date_tensor))\n\n  @test_both_impls\n  def test_custom_weekend_mask(self):\n    weekend_mask = [0, 0, 0, 0, 1, 0, 1]  # Work Saturdays instead of Fridays.\n    cal = self.impl(start_year=2020, end_year=2021, weekend_mask=weekend_mask)\n    date_tensor = dates.dates_from_tuples([(2020, 1, 2), (2020, 1, 3),\n                                           (2020, 1, 4), (2020, 1, 5),\n                                           (2020, 1, 6), (2020, 5, 1),\n                                           (2020, 5, 2)])\n    self.assertAllEqual([True, False, True, False, True, False, True],\n                        cal.is_business_day(date_tensor))\n\n  @test_both_impls\n  def test_holidays_intersect_with_weekends(self):\n    holidays = [(2020, 1, 4)]  # Saturday.\n    cal = self.impl(\n        weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY, holidays=holidays)\n    date_tensor = dates.dates_from_tuples([(2020, 1, 3), (2020, 1, 4),\n                                           (2020, 1, 5), (2020, 1, 6)])\n    self.assertAllEqual([True, False, False, True],\n                        cal.is_business_day(date_tensor))\n\n  @test_both_impls\n  def test_no_holidays_specified(self):\n    cal = self.impl(\n        weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY,\n        start_year=2020,\n        end_year=2021)\n    date_tensor = dates.dates_from_tuples([(2020, 1, 3), (2020, 1, 4),\n                                           (2021, 12, 24), (2021, 12, 25)])\n    self.assertAllEqual([True, False, True, False],\n                        cal.is_business_day(date_tensor))\n\n  def test_tensor_caching_in_bounded_calendar(self):\n    cal = bounded_holiday_calendar.BoundedHolidayCalendar(\n        weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY,\n        holidays=test_data.holidays)\n    conv = dates.BusinessDayConvention.FOLLOWING\n\n    @tf.function\n    def foo():\n      # Trigger caching of all tables.\n      date_tensor = dates.dates_from_tuples([])\n      return (cal.is_business_day(date_tensor),\n              cal.roll_to_business_day(date_tensor, conv).ordinal(),\n              cal.business_days_between(date_tensor, date_tensor),\n              cal.add_business_days(date_tensor, 2, conv).ordinal())\n\n    @tf.function\n    def bar():\n      date_tensor = dates.dates_from_tuples([(2020, 1, 3), (2020, 1, 4),\n                                             (2021, 12, 24), (2021, 12, 25)])\n      return (cal.is_business_day(date_tensor),\n              cal.roll_to_business_day(date_tensor, conv).ordinal(),\n              cal.business_days_between(date_tensor, date_tensor),\n              cal.add_business_days(date_tensor, 2, conv).ordinal())\n\n    foo()\n    self.assertAllEqual([True, False, False, False], bar()[0])\n\n  @parameterized.named_parameters(*rolling_test_parameters)\n  @test_both_impls\n  def test_roll_to_business_days(self, rolling_enum_value, data_key):\n    data = test_data.adjusted_dates_data\n    date_tensor = dates.dates_from_tuples([item[""date""] for item in data])\n    expected_dates = dates.dates_from_tuples([item[data_key] for item in data])\n\n    cal = self.impl(\n        weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY,\n        holidays=test_data.holidays)\n    actual_dates = cal.roll_to_business_day(date_tensor,\n                                            roll_convention=rolling_enum_value)\n    self.assertAllEqual(expected_dates.ordinal(), actual_dates.ordinal())\n\n  @parameterized.named_parameters(*rolling_test_parameters)\n  @test_both_impls\n  def test_add_months_and_roll(self, rolling_enum_value, data_key):\n    data = test_data.add_months_data\n    date_tensor = dates.dates_from_tuples([item[""date""] for item in data])\n    periods = dates.months([item[""months""] for item in data])\n    expected_dates = dates.dates_from_tuples([item[data_key] for item in data])\n    cal = self.impl(\n        weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY,\n        holidays=test_data.holidays)\n    actual_dates = cal.add_period_and_roll(date_tensor, periods,\n                                           roll_convention=rolling_enum_value)\n    self.assertAllEqual(expected_dates.ordinal(), actual_dates.ordinal())\n\n  @test_both_impls\n  def test_add_business_days(self):\n    data = test_data.add_days_data\n    date_tensor = dates.dates_from_tuples([item[""date""] for item in data])\n    days = tf.constant([item[""days""] for item in data])\n    expected_dates = dates.dates_from_tuples([item[""shifted_date""]\n                                              for item in data])\n    cal = self.impl(\n        weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY,\n        holidays=test_data.holidays)\n    actual_dates = cal.add_business_days(\n        date_tensor, days,\n        roll_convention=dates.BusinessDayConvention.MODIFIED_FOLLOWING)\n    self.assertAllEqual(expected_dates.ordinal(), actual_dates.ordinal())\n\n  @test_both_impls\n  def test_add_business_days_raises_on_invalid_input(self):\n    data = test_data.add_days_data  # Contains some holidays.\n    date_tensor = dates.dates_from_tuples([item[""date""] for item in data])\n    days = tf.constant([item[""days""] for item in data])\n    cal = self.impl(\n        weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY,\n        holidays=test_data.holidays)\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      new_dates = cal.add_business_days(\n          date_tensor, days,\n          roll_convention=dates.BusinessDayConvention.NONE)\n      self.evaluate(new_dates.ordinal())\n\n  @test_both_impls\n  def test_business_days_between(self):\n    data = test_data.days_between_data\n    date_tensor1 = dates.dates_from_tuples([item[""date1""] for item in data])\n    date_tensor2 = dates.dates_from_tuples([item[""date2""] for item in data])\n    expected_days_between = [item[""days""] for item in data]\n    cal = self.impl(\n        weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY,\n        holidays=test_data.holidays)\n    actual_days_between = cal.business_days_between(date_tensor1, date_tensor2)\n    self.assertAllEqual(expected_days_between, actual_days_between)\n\n  @test_both_impls\n  def test_is_business_day(self):\n    data = test_data.is_business_day_data\n    date_tensor = dates.dates_from_tuples([item[0] for item in data])\n    expected = [item[1] for item in data]\n    cal = self.impl(\n        weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY,\n        holidays=test_data.holidays)\n    actual = cal.is_business_day(date_tensor)\n    self.assertEqual(tf.bool, actual.dtype)\n    self.assertAllEqual(expected, actual)\n\n  def test_bounded_impl_near_boundaries(self):\n    cal = bounded_holiday_calendar.BoundedHolidayCalendar(\n        weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY,\n        start_year=2017,\n        end_year=2022)\n\n    def assert_roll_raises(roll_convention, date):\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        self.evaluate(cal.roll_to_business_day(date, roll_convention).year())\n\n    def assert_rolls_to(roll_convention, date, expected_date):\n      rolled = cal.roll_to_business_day(date, roll_convention)\n      self.assertAllEqual(rolled.ordinal(), expected_date.ordinal())\n\n    date = dates.dates_from_tuples([(2022, 12, 31)])  # Saturday\n    preceding = dates.dates_from_tuples([(2022, 12, 30)])\n    with self.subTest(""following_upper_bound""):\n      assert_roll_raises(dates.BusinessDayConvention.FOLLOWING, date)\n    with self.subTest(""preceding_upper_bound""):\n      assert_rolls_to(dates.BusinessDayConvention.PRECEDING, date, preceding)\n    with self.subTest(""modified_following_upper_bound""):\n      assert_rolls_to(dates.BusinessDayConvention.MODIFIED_FOLLOWING,\n                      date, preceding)\n    with self.subTest(""modified_preceding_upper_bound""):\n      assert_rolls_to(dates.BusinessDayConvention.MODIFIED_PRECEDING,\n                      date, preceding)\n\n    date = dates.dates_from_tuples([(2017, 1, 1)])  # Sunday\n    following = dates.dates_from_tuples([(2017, 1, 2)])\n\n    with self.subTest(""following_lower_bound""):\n      assert_rolls_to(dates.BusinessDayConvention.FOLLOWING, date, following)\n    with self.subTest(""preceding_lower_bound""):\n      assert_roll_raises(dates.BusinessDayConvention.PRECEDING, date)\n    with self.subTest(""modified_following_lower_bound""):\n      assert_rolls_to(dates.BusinessDayConvention.MODIFIED_FOLLOWING,\n                      date, following)\n    with self.subTest(""modified_preceding_lower_bound""):\n      assert_rolls_to(dates.BusinessDayConvention.MODIFIED_PRECEDING,\n                      date, following)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/datetime/holiday_utils.py,20,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utils to manipulate holidays.""""""\n\nimport tensorflow.compat.v2 as tf\n\n# In Gregorian Calendar, 1-Jan-1 was a Monday, hence ordinal 0 corresponds\n# to a Sunday.\n_DAYOFWEEK_0 = 6\n\n\ndef business_day_mappers(weekend_mask=None, holidays=None):\n  """"""Returns functions to map from ordinal to biz day and back.""""""\n  if weekend_mask is None and holidays is None:\n    return (lambda x: (x, tf.ones_like(x, dtype=tf.bool))), (lambda x: x)\n\n  weekday_fwd, weekday_back = _week_day_mappers(weekend_mask)\n\n  if holidays is None:\n    return weekday_fwd, weekday_back\n\n  # Apply the weekend adjustment to the holidays as well\n  holidays_raw = tf.convert_to_tensor(holidays, dtype=tf.int32)\n  holidays, is_weekday = weekday_fwd(holidays_raw)\n\n  # Keep only the holidays that are not on weekends\n  holidays = holidays[is_weekday]\n\n  # The above step can lead to an empty holidays set which causes problems.\n  # To mitigate this, we add a safe fake holiday.\n  holidays = tf.concat([[0], holidays], axis=0)\n  reverse_holidays = tf.reverse(-holidays, axis=[0])\n  num_holidays = tf.size(holidays) - 1\n\n  def bizday_fwd(x):\n    """"""Calculates business day ordinal and whether it is a business day.""""""\n    left = tf.searchsorted(holidays, x, side=\'left\')\n    right = num_holidays - tf.searchsorted(reverse_holidays, -x, side=\'left\')\n    is_bizday = tf.not_equal(left, right)\n    bizday_ordinal = x - right\n    return bizday_ordinal, is_bizday\n\n  cum_holidays = tf.range(num_holidays + 1, dtype=holidays.dtype)\n  bizday_at_holidays = holidays - cum_holidays\n\n  def bizday_back(x):\n    left = tf.searchsorted(bizday_at_holidays, x, side=\'left\')\n    ordinal = x + left - 1\n    return ordinal\n\n  def from_ordinal(ordinals):\n    """"""Maps ordinals to business day and whether it is a work day.""""""\n    ordinals = tf.convert_to_tensor(ordinals, dtype=tf.int32)\n    weekday_values, is_weekday = weekday_fwd(ordinals)\n    biz_ordinal, is_bizday = bizday_fwd(weekday_values)\n    return biz_ordinal, (is_weekday & is_bizday)\n\n  def to_ordinal(biz_values):\n    """"""Maps from business day count to ordinals.""""""\n    return weekday_back(bizday_back(biz_values))\n\n  return from_ordinal, to_ordinal\n\n\ndef _week_day_mappers(weekend_mask):\n  """"""Creates functions to map from ordinals to week days and inverse.\n\n  Creates functions to map from ordinal space (i.e. days since 31 Dec 0) to\n  week days. The function assigns the value of 0 to the first non weekend\n  day in the week starting on Sunday, 31 Dec 1 through to Saturday, 6 Jan 1 and\n  the value assigned to each successive work day is incremented by 1. For a day\n  that is not a week day, this count is not incremented from the previous week\n  day (hence, multiple ordinal days may have the same week day value).\n\n  Args:\n    weekend_mask: A bool `Tensor` of length 7 or None. The weekend mask.\n\n  Returns:\n    A tuple of callables.\n      `forward`: Takes one `Tensor` argument containing ordinals and returns a\n        tuple of two `Tensor`s of the same shape as the input. The first\n        `Tensor` is of type `int32` and contains the week day value. The second\n        is a bool `Tensor` indicating whether the supplied ordinal was a weekend\n        day (i.e. True where the day is a weekend day and False otherwise).\n      `backward`: Takes one int32 `Tensor` argument containing week day values\n        and returns an int32 `Tensor` containing ordinals for those week days.\n  """"""\n  if weekend_mask is None:\n    default_forward = lambda x: (x, tf.zeros_like(x, dtype=tf.bool))\n    identity = lambda x: x\n    return default_forward, identity\n  weekend_mask = tf.convert_to_tensor(weekend_mask, dtype=tf.bool)\n  weekend_mask = tf.roll(weekend_mask, -_DAYOFWEEK_0, axis=0)\n  weekday_mask = tf.logical_not(weekend_mask)\n  weekday_offsets = tf.cumsum(tf.cast(weekday_mask, dtype=tf.int32))\n  num_workdays = weekday_offsets[-1]\n  weekday_offsets -= 1  # Adjust the first workday to index 0.\n  ordinal_offsets = tf.convert_to_tensor([0, 1, 2, 3, 4, 5, 6], dtype=tf.int32)\n  ordinal_offsets = ordinal_offsets[weekday_mask]\n\n  def forward(ordinals):\n    """"""Adjusts the ordinals by removing the number of weekend days so far.""""""\n    mod, remainder = ordinals // 7, ordinals % 7\n    weekday_values = mod * num_workdays + tf.gather(weekday_offsets, remainder)\n    is_weekday = tf.gather(weekday_mask, remainder)\n    return weekday_values, is_weekday\n\n  def backward(weekday_values):\n    """"""Converts from weekend adjusted values to ordinals.""""""\n    return ((weekday_values // num_workdays) * 7 +\n            tf.gather(ordinal_offsets, weekday_values % num_workdays))\n\n  return forward, backward\n'"
tf_quant_finance/datetime/periods.py,5,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PeriodTensor definition.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.datetime import constants\nfrom tf_quant_finance.datetime import tensor_wrapper\n\n\ndef day():\n  return days(1)\n\n\ndef days(n):\n  return PeriodTensor(n, constants.PeriodType.DAY)\n\n\ndef week():\n  return weeks(1)\n\n\ndef weeks(n):\n  return PeriodTensor(n, constants.PeriodType.WEEK)\n\n\ndef month():\n  return months(1)\n\n\ndef months(n):\n  return PeriodTensor(n, constants.PeriodType.MONTH)\n\n\ndef year():\n  return years(1)\n\n\ndef years(n):\n  return PeriodTensor(n, constants.PeriodType.YEAR)\n\n\nclass PeriodTensor(tensor_wrapper.TensorWrapper):\n  """"""Represents a tensor of time periods.""""""\n\n  def __init__(self, quantity, period_type):\n    """"""Initializer.\n\n    Args:\n      quantity: A Tensor of type tf.int32, representing the quantities\n        of period types (e.g. how many months). Can be both positive and\n        negative.\n      period_type: A PeriodType (a day, a month, etc). Currently only one\n        PeriodType per instance of PeriodTensor is supported.\n\n    Example:\n    ```python\n    two_weeks = PeriodTensor(2, PeriodType.WEEK)\n\n    months = [3, 6, 9, 12]\n    periods = PeriodTensor(months, PeriodType.MONTH)\n    ```\n    """"""\n    self._quantity = tf.convert_to_tensor(quantity, dtype=tf.int32,\n                                          name=""pt_quantity"")\n    self._period_type = period_type\n\n  def period_type(self):\n    """"""Returns the PeriodType of this PeriodTensor.""""""\n    return self._period_type\n\n  def quantity(self):\n    """"""Returns the quantities tensor of this PeriodTensor.""""""\n    return self._quantity\n\n  def __mul__(self, multiplier):\n    """"""Multiplies PeriodTensor by a Tensor of ints.""""""\n    multiplier = tf.convert_to_tensor(multiplier, tf.int32)\n    return PeriodTensor(self._quantity * multiplier, self._period_type)\n\n  def __add__(self, other):\n    """"""Adds another PeriodTensor of the same type.""""""\n    if other.period_type() != self._period_type:\n      raise ValueError(""Mixing different period types is not supported"")\n\n    return PeriodTensor(self._quantity + other.quantity(), self._period_type)\n\n  def __sub__(self, other):\n    """"""Subtracts another PeriodTensor of the same type.""""""\n    if other.period_type() != self._period_type:\n      raise ValueError(""Mixing different period types is not supported"")\n\n    return PeriodTensor(self._quantity - other.quantity(), self._period_type)\n\n  @property\n  def shape(self):\n    return self._quantity.shape\n\n  @property\n  def rank(self):\n    return tf.rank(self._quantity)\n\n  @classmethod\n  def _apply_sequence_to_tensor_op(cls, op_fn, tensor_wrappers):\n    q = op_fn([t.quantity() for t in tensor_wrappers])\n    period_type = tensor_wrappers[0].period_type()\n    if not all(t.period_type() == period_type for t in tensor_wrappers[1:]):\n      raise ValueError(""Combined PeriodTensors must have the same PeriodType"")\n    return PeriodTensor(q, period_type)\n\n  def _apply_op(self, op_fn):\n    q = op_fn(self._quantity)\n    return PeriodTensor(q, self._period_type)\n\n  def __repr__(self):\n    output = ""PeriodTensor: shape={}"".format(self.shape)\n    if tf.executing_eagerly():\n      return output + "", quantities={}"".format(repr(self._quantity.numpy()))\n    return output\n\n\n__all__ = [\n    ""day"",\n    ""days"",\n    ""month"",\n    ""months"",\n    ""week"",\n    ""weeks"",\n    ""year"",\n    ""years"",\n    ""PeriodTensor"",\n]\n'"
tf_quant_finance/datetime/schedules.py,19,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Functions for creating schedules.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.datetime import constants\nfrom tf_quant_finance.datetime import date_tensor\n\n\n_MIN_DAYS_IN_PERIOD = {\n    constants.PeriodType.DAY: 1,\n    constants.PeriodType.WEEK: 7,\n    constants.PeriodType.MONTH: 28,\n    constants.PeriodType.YEAR: 365\n}\n\n\nclass PeriodicSchedule:\n  """"""Defines an array of dates specified by a regular schedule.""""""\n\n  def __init__(self,\n               *,\n               start_date,\n               end_date,\n               tenor,\n               holiday_calendar=None,\n               roll_convention=constants.BusinessDayConvention.NONE,\n               backward=False,\n               end_of_month=False):\n    """"""Initializes the schedule.\n\n    Initializes a schedule with a given tenor, date range and holiday calendar.\n\n    A schedule is an increasing sequence of dates at a regular interval subject\n    to holiday adjustments.\n\n    The rules for schedule generation (accessed via the `dates()` method)\n    are as follows.\n\n    (a) If `backward=False`, take `start_date` and add `tenor` multiplied by\n      0, 1, 2, etc. until the resulting date is greater than `end_date`.\n    (b) If `backward=True`, take `end_date` and subtract `tenor` multiplied by\n      0, 1, 2, etc. until the resulting date is smaller than `start_date`.\n      Ensure that the result is in ascending order.\n    (c) Both `start_date` and `end_date` are included, even if the distance\n      between then is not an integer number of tenor periods.\n    (d) If `holiday_calendar` is specified, roll all the dates according to\n      `roll_convention`. The rolling includes `start_date` and `end_date` when\n      they are part of resulting schedule. Thus if `start_date` or `end_date`\n      fall on holidays, they will change and may go out of the\n      [`start_date`, `end_date`] interval.\n\n    Note that `tenor = PeriodType.DAY` is treated as an actual day, not as\n    a business day. So a schedule with `tenor = days(7)` is the same\n    as one with `tenor = week()`.\n\n    The `dates()` can create multiple schedules simultaneously.\n    The start and end dates may have any (compatible) shape.\n    The `DateTensor` returned by `dates()` has the shape\n    `start_date.shape + (n,)`, where `n` is the maximum length of schedules in\n    the batch. If schedules have different lengths, the extra elements will be\n    padded with extra `end_date` elements at the end, if `backward=False`\n    and with extra `start_date` elements in the beginning if\n    `backward=True`. In all cases each schedule in the batch is monotonic.\n\n    The following examples demonstrate the batch and non-batch usage.\n\n    #### Example Usage (Non-batch)\n\n    ```python\n      start_date = datetime.dates_from_tuples([(2020, 1, 18)])\n      end_date = datetime.dates_from_tuples([(2021, 3, 25)])\n      tenor = datetime.months(3)\n      backward = False\n      holiday_calendar = dates.HolidayCalendar(start_year=2020, end_year=2021)\n      roll_convention = dates.BusinessDayConvention.FOLLOWING\n      schedule = datetime.PeriodicSchedule(\n          start_date=start_date,\n          end_date=end_date,\n          tenor=tenor,\n          holiday_calendar=holiday_calendar,\n          roll_convention=datetime.BusinessDayConvention.FOLLOWING,\n          backward=backward).dates()\n      # schedule is a DateTensor of\n      # [[(2020, 1, 18), (2020, 4, 20), (2020, 7, 20), (2020, 10, 19),\n      #   (2021, 1, 18), (2021, 3, 25)]] for backward = False and\n      # [[(2020, 1, 18), (2020, 3, 25), (2020, 6, 25), (2020, 9, 25),\n      #   (2020, 12, 25), (2021, 3, 25)]] for backward = True.\n    ```\n\n    The following example demonstrates this batching property.\n\n    #### Example Usage (Batch)\n\n    ```python\n      start_date = datetime.dates_from_tuples([(2020, 1, 15), (2020, 4, 15)])\n      end_date = datetime.dates_from_tuples([(2021, 3, 31), (2021, 1, 1)])\n      tenor = datetime.months([4, 3])\n      schedule = datetime.PeriodicSchedule(\n          start_dates,\n          end_dates,\n          tenors,\n          dates.HolidayCalendar(start_year=2020, end_year=2021),\n          roll_convention=datetime.BusinessDayConvention.FOLLOWING,\n          backward=False).dates()\n      # Returns DateTensor of\n      # [[(2020, 1, 15), (2020, 5, 15), (2020, 9, 15), (2021, 1, 15),\n      #   (2021, 3, 31)],\n      # [(2020, 4, 15), (2020, 7, 15), (2020, 10, 15), (2021, 1, 1),\n      #  (2021, 1, 1)]].\n    ```\n\n    Args:\n      start_date: `DateTensor`. Defines the lower boundary of schedule. If\n        `backward=True` must be broadcastable to `end_date`, otherwise has\n        arbitrary shape.\n      end_date: `DateTensor`. Defines the upper boundary of the schedule.\n        If `backward=False` must be broadcastable to `start_date`, otherwise has\n        arbitrary shape.\n      tenor: `PeriodTensor`. Defines the frequency of the schedule. Must\n        be broadcastable to `start_date` if `backward=False`, and to `end_date`\n        if `backward=True`.\n      holiday_calendar: `dates.HolidayCalendar`. If `None`, the dates in the\n        schedule will not be rolled to business days.\n      roll_convention: BusinessDayConvention. Defines how dates in the schedule\n        should be rolled to business days if they fall on holidays. Ignored if\n        `holiday_calendar = None`.\n        Default value: BusinessDayConvention.NONE (i.e. no rolling).\n      backward: Python `bool`. Whether to build the schedule from the\n        `start_date` moving forwards or from the `end_date` and moving\n        backwards.\n      end_of_month: Python `bool`. If `True`, shifts all dates in schedule to\n        the ends of corresponding months, if `start_date` or `end_date` (\n        depending on `backward`) is at the end of a month. The shift is applied\n        before applying `roll_convention`. In the batched case, only those\n        schedules in a batch, whose corresponding `start_date` (or `end_date`)\n        are at ends of months, will be shifted.\n    """"""\n    if end_of_month and tenor.period_type() not in [constants.PeriodType.MONTH,\n                                                    constants.PeriodType.YEAR]:\n      raise ValueError(\n          ""end_of_month may only be used with tenors of PeriodType.MONTH or ""\n          ""PeriodType.YEAR""\n      )\n\n    self._start_date = start_date\n    self._end_date = end_date\n    self._tenor = tenor\n    self._holiday_calendar = holiday_calendar\n    self._roll_convention = roll_convention\n    self._backward = backward\n    self._end_of_month = end_of_month\n\n  def dates(self):\n    """"""Returns the dates as computed from the schedule as a DateTensor.\n\n    Constructs the date schedule from the supplied data. For more details see\n    the initializer docstring.\n\n    Returns:\n      `DateTensor` of rank one more than `start_date` or `end_date`\n      (depending on `backwards`), representing schedules for each element\n      of the input.\n    """"""\n    return _gen_periodic_schedule(\n        self._start_date,\n        self._end_date,\n        self._tenor,\n        holiday_calendar=self._holiday_calendar,\n        roll_convention=self._roll_convention,\n        backward=self._backward,\n        end_of_month=self._end_of_month)\n\n  @property\n  def start_date(self):\n    return self._start_date\n\n  @property\n  def end_date(self):\n    return self._end_date\n\n  @property\n  def tenor(self):\n    return self._tenor\n\n  @property\n  def holiday_calendar(self):\n    return self._holiday_calendar\n\n  @property\n  def roll_convention(self):\n    return self._roll_convention\n\n  @property\n  def generate_backwards(self):\n    """"""Returns whether the schedule is generated from the end date.""""""\n    return self._backward\n\n  @property\n  def end_of_month(self):\n    return self._end_of_month\n\n\nclass BusinessDaySchedule:\n  """"""Generates schedules containing every business day in a period.""""""\n\n  def __init__(self,\n               *,\n               start_date,\n               end_date,\n               holiday_calendar,\n               backward=False):\n    """"""Initializes the schedule.\n\n    Initializes a schedule with a given date range and holiday calendar.\n\n    The schedule includes all business days between and including `start_date`\n    and `end_date`.\n\n    Can create multiple schedules simultaneously. The start and end dates may\n    have any (compatible) shape. The `DateTensor` returned by `dates()` has the\n    shape `start_date.shape + (n,)`, where `n` is the maximum length of\n    schedules in the batch. If schedules have different lengths, the extra\n    elements will be padded with extra `end_date` elements at the end, if\n    `backward=False` and with extra `start_date` elements in the beginning if\n    `backward=True`. In all cases each schedule in the batch is monotonic.\n\n    #### Example Usage (Non-batch)\n\n    ```python\n      start_date = datetime.dates_from_tuples([(2020, 3, 19)])\n      end_date = datetime.dates_from_tuples([(2021, 3, 25)])\n      holiday_calendar = datetime.HolidayCalendar(start_year=2020,\n                                                  end_year=2021)\n      schedule = datetime.BusinessDaysSchedule(\n          start_date=start_date,\n          end_date=end_date,\n          holiday_calendar=holiday_calendar,\n          roll_convention=datetime.BusinessDayConvention.FOLLOWING,\n          backward=False).dates()\n      # schedule is a DateTensor of\n      # [[(2020, 3, 19), (2020, 3, 20), (2020, 3, 23), (2020, 3, 24),\n      #   (2021, 3, 25)]] regardless of `backward`.\n    ```\n\n    #### Example Usage (Batch)\n\n    ```python\n      start_date = datetime.dates_from_tuples([(2020, 3, 19), (2020, 4, 15)])\n      end_date = datetime.dates_from_tuples([(2021, 3, 13), (2021, 3, 17)])\n      schedule = datetime.BusinessDaysSchedule(\n          start_dates,\n          end_dates,\n          datetime.HolidayCalendar(start_year=2020, end_year=2021),\n          backward=False).dates()\n      # Returns DateTensor of\n      # [[(2020, 3, 19), (2020, 3, 20), (2020, 3, 23), (2020, 3, 24),\n      #   (2021, 3, 25)],\n      # [(2020, 3, 13), (2020, 3, 16), (2020, 3, 17), (2020, 3, 17),\n      #  (2021, 3, 17)]], if `backward` is True.\n      # [[(2020, 3, 19), (2020, 3, 20), (2020, 3, 23), (2020, 3, 24),\n      #   (2021, 3, 25)],\n      # [(2020, 3, 13), (2020, 3, 13), (2020, 3, 13), (2020, 3, 16),\n      #  (2021, 3, 17)]], if `backward` is True.\n    ```\n    Args:\n      start_date: `dates.DateTensor`. Defines the lower boundary of schedule. If\n        `backward=True` must be broadcastable to `end_date`, otherwise has\n        arbitrary shape.\n      end_date: `dates.DateTensor`. Defines the upper boundary of the schedule.\n        If `backward=False` must be broadcastable to `start_date`, otherwise has\n        arbitrary shape.\n      holiday_calendar: `dates.HolidayCalendar` that defines which days will be\n        included.\n      backward: Python `bool`. Defines the way padding is applied in case of\n        batching. If schedules in a batch have different lengths, the extra\n        elements will be padded with extra `end_date` elements at the end, if\n        `backward=False` and with extra `start_date` elements in the beginning\n        if `backward=True`.\n    """"""\n    self._start_date = start_date\n    self._end_date = end_date\n    self._holiday_calendar = holiday_calendar\n    self._backward = backward\n\n  def dates(self):\n    """"""Returns the dates as computed from the schedule as a DateTensor.\n\n    Constructs the date schedule from the supplied data. For more details see\n    the initializer docstring.\n\n    Returns:\n      `DateTensor` of rank one more than `start_date` or `end_date`\n      (depending on `backwards`), representing schedules for each element\n      of the input.\n    """"""\n    return _gen_business_days(self._start_date,\n                              self._end_date,\n                              self._holiday_calendar,\n                              self._backward)\n\n  @property\n  def holiday_calendar(self):\n    return self._holiday_calendar\n\n  @property\n  def start_date(self):\n    return self._start_date\n\n  @property\n  def end_date(self):\n    return self._end_date\n\n  @property\n  def generate_backwards(self):\n    return self._backward\n\n\ndef _gen_periodic_schedule(start_date,\n                           end_date,\n                           tenor,\n                           holiday_calendar=None,\n                           roll_convention=constants.BusinessDayConvention.NONE,\n                           backward=False,\n                           end_of_month=False):\n  """"""Generates a periodic schedule, see PeriodicSchedule.""""""\n\n  # Validate inputs.\n  control_deps = [\n      tf.debugging.assert_greater_equal(\n          end_date.ordinal(),\n          start_date.ordinal(),\n          message=""End date must be >= to start date.""),\n      tf.debugging.assert_positive(\n          tenor.quantity(), message=""Tenor quantity must be positive."")\n  ]\n\n  with tf.compat.v1.control_dependencies(control_deps):\n    # Reshape the input Tensors.\n    if backward:\n      start_date = start_date.broadcast_to(end_date.shape)\n      tenor = tenor.broadcast_to(end_date.shape)\n    else:\n      end_date = end_date.broadcast_to(start_date.shape)\n      tenor = tenor.broadcast_to(start_date.shape)\n    start_date = start_date.expand_dims(axis=-1)\n    end_date = end_date.expand_dims(axis=-1)\n    tenor = tenor.expand_dims(axis=-1)\n\n    # Figure out the upper bound of the schedule length.\n    min_days_in_period = _MIN_DAYS_IN_PERIOD[tenor.period_type()]\n    days_between = end_date.ordinal() - start_date.ordinal() + 1\n    schedule_len_upper_bound = tf.cast(\n        tf.math.ceil(tf.math.reduce_max(\n            days_between / (tenor.quantity() * min_days_in_period))),\n        dtype=tf.int32)\n\n    # Add the periods.\n    if backward:\n      # Subtract tenor * n, where n = n_max, ..., 2, 1, 0.\n      tenors_expanded = tenor * tf.range(schedule_len_upper_bound - 1, -1, -1,\n                                         dtype=tf.int32)\n      schedules = end_date - tenors_expanded\n      # Prepend start_date to ensure we always include it.\n      schedules = date_tensor.DateTensor.concat((start_date, schedules),\n                                                axis=-1)\n      in_bounds = schedules.ordinal() >= start_date.ordinal()\n\n      # Pad with start_date.\n      schedules = date_tensor.DateTensor.where(in_bounds, schedules, start_date)\n\n      # Find how much we overestimated max schedule length and trim the extras.\n      not_start_date = tf.math.not_equal(schedules.ordinal(),\n                                         start_date.ordinal())\n      max_schedule_len_error = (\n          tf.math.reduce_min(tf.where(not_start_date)[..., -1]) - 1)\n      schedules = schedules[..., max_schedule_len_error:]\n    else:\n      # Add tenor * n, where n = 0, 1, 2, ..., n_max.\n      tenors_expanded = tenor * tf.range(schedule_len_upper_bound,\n                                         dtype=tf.int32)\n      schedules = start_date + tenors_expanded\n      # Append end_date to ensure we always include it.\n      schedules = date_tensor.DateTensor.concat((schedules, end_date), axis=-1)\n\n      in_bounds = schedules.ordinal() <= end_date.ordinal()\n\n      # Pad with end_date.\n      schedules = date_tensor.DateTensor.where(in_bounds, schedules, end_date)\n\n      # Find the actual schedule length and trim the extras.\n      not_end_date = tf.math.not_equal(schedules.ordinal(), end_date.ordinal())\n      max_schedule_len = tf.math.reduce_max(tf.where(not_end_date)[..., -1]) + 2\n      schedules = schedules[..., :max_schedule_len]\n\n    # Move to the end of month where necessary.\n    if end_of_month:\n      where_cond = (end_date if backward else start_date).is_end_of_month()\n      schedules = date_tensor.DateTensor.where(where_cond,\n                                               schedules.to_end_of_month(),\n                                               schedules)\n\n    # Roll to business days.\n    if holiday_calendar is not None:\n      schedules = holiday_calendar.roll_to_business_day(schedules,\n                                                        roll_convention)\n\n    return schedules\n\n\ndef _gen_business_days(start_date, end_date, holiday_calendar, backward=False):\n  """"""Generates business days between given dates, see BusinessDaySchedule.""""""\n  # Handle the case when start_date or end_date fall on holidays.\n  start_date = holiday_calendar.roll_to_business_day(\n      start_date, roll_convention=constants.BusinessDayConvention.FOLLOWING)\n  end_date = holiday_calendar.roll_to_business_day(\n      end_date, roll_convention=constants.BusinessDayConvention.PRECEDING)\n\n  # Validate inputs.\n  control_deps = [\n      tf.debugging.assert_greater_equal(\n          end_date.ordinal(),\n          start_date.ordinal(),\n          message=""End date must be >= Start date.""),\n  ]\n  with tf.compat.v1.control_dependencies(control_deps):\n    # Reshape the input Tensors.\n    if backward:\n      start_date = start_date.broadcast_to(end_date.shape)\n    else:\n      end_date = end_date.broadcast_to(start_date.shape)\n    start_date = start_date.expand_dims(axis=-1)\n    end_date = end_date.expand_dims(axis=-1)\n\n    # Find the longest schedule in the batch.\n    max_len = tf.math.abs(tf.math.reduce_max(\n        holiday_calendar.business_days_between(start_date, end_date))) + 1\n\n    if backward:\n      # Subtract n days, where n = max_len-1, ..., 2, 1, 0.\n      days = tf.range(-max_len + 1, 1, dtype=tf.int32)\n      schedules = holiday_calendar.add_business_days(end_date, days)\n      in_bounds = schedules.ordinal() >= start_date.ordinal()\n      # Pad with start_date.\n      schedules = date_tensor.DateTensor.where(in_bounds, schedules, start_date)\n    else:\n      # Add n days, where n = 0, 1, 2, ..., max_len-1.\n      days = tf.range(max_len, dtype=tf.int32)\n      schedules = holiday_calendar.add_business_days(start_date, days)\n      in_bounds = schedules.ordinal() <= end_date.ordinal()\n      # Pad with end_date.\n      schedules = date_tensor.DateTensor.where(in_bounds, schedules, end_date)\n\n    return schedules\n'"
tf_quant_finance/datetime/schedules_test.py,2,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for schedules.py.""""""\n\nimport datetime\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tf_quant_finance.datetime import test_data\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\ndates = tff.datetime\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass SchedulesTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      *test_data.periodic_schedule_test_cases)\n  def test_periodic_schedule(self, start_dates, end_dates, period_quantities,\n                             period_type, backward, expected_schedule,\n                             end_of_month=False):\n    start_dates = dates.dates_from_np_datetimes(_to_np_datetimes(start_dates))\n    end_dates = dates.dates_from_np_datetimes(_to_np_datetimes(end_dates))\n    tenors = dates.PeriodTensor(period_quantities, period_type)\n    expected_schedule = dates.dates_from_np_datetimes(\n        _to_np_datetimes(expected_schedule))\n    actual_schedule = dates.PeriodicSchedule(\n        start_date=start_dates,\n        end_date=end_dates,\n        tenor=tenors,\n        holiday_calendar=dates.create_holiday_calendar(\n            weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY,\n            start_year=2020,\n            end_year=2028),\n        roll_convention=dates.BusinessDayConvention.MODIFIED_FOLLOWING,\n        backward=backward,\n        end_of_month=end_of_month).dates()\n    self.assertAllEqual(expected_schedule.ordinal(), actual_schedule.ordinal())\n\n  @parameterized.named_parameters(*test_data.business_day_schedule_test_cases)\n  def test_business_day_schedule(self, start_dates, end_dates, holidays,\n                                 backward, expected_schedule):\n    start_dates = dates.dates_from_np_datetimes(_to_np_datetimes(start_dates))\n    end_dates = dates.dates_from_np_datetimes(_to_np_datetimes(end_dates))\n    holiday_calendar = dates.create_holiday_calendar(\n        weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY,\n        holidays=holidays,\n        start_year=2020,\n        end_year=2020)\n    expected_schedule = dates.dates_from_np_datetimes(\n        _to_np_datetimes(expected_schedule))\n    actual_schedule = dates.BusinessDaySchedule(\n        start_date=start_dates,\n        end_date=end_dates,\n        holiday_calendar=holiday_calendar,\n        backward=backward).dates()\n    self.assertAllEqual(expected_schedule.ordinal(), actual_schedule.ordinal())\n\n\ndef _to_np_datetimes(nested_date_tuples):\n\n  def recursive_convert_to_datetimes(sequence):\n    result = []\n    for item in sequence:\n      if isinstance(item, list):\n        result.append(recursive_convert_to_datetimes(item))\n      else:\n        result.append(datetime.date(*item))\n    return result\n\n  return np.array(\n      recursive_convert_to_datetimes(nested_date_tuples), dtype=np.datetime64)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/datetime/tensor_wrapper.py,26,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Base class for Tensor wrappers.""""""\n\nimport abc\nimport tensorflow.compat.v2 as tf\n\n\nclass TensorWrapper(metaclass=abc.ABCMeta):\n  """"""Base class for Tensor wrappers.\n\n  Implements ops that manipulate the backing tensors of Tensor wrappers\n  (e.g. DateTensor, PeriodTensor). These ops are mostly about reshaping the\n  backing tensors, such as tf.reshape, tf.expand_dims, tf.stack, etc. Also\n  includes indexing and slicing.\n\n  Inheritors must implement _apply_op(self, op_fn) and provide a static method\n  _apply_sequence_to_tensor_op(op_fn, tensors). For example:\n\n  ```python\n  class MyWrapper(TensorWrapper):\n    def __init__(self, backing_tensor):\n       self._backing_tensor = backing_tensor\n\n    def _apply_op(self, op_fn):\n      new_backing_tensor = op_fn(self._backing_tensor)\n      return MyWrapper(new_backing_tensor)\n\n    @staticmethod\n    def _apply_sequence_to_tensor_op(op_fn, tensors):\n      new_backing_tensor = op_fn([t._backing_tensor for t in tensors])\n      return MyWrapper(new_backing_tensor)\n  ```\n\n  Then \'MyWrapper` can be used as follows:\n\n  ```python\n  m1 = MyWrapper(tf.constant([[1, 2, 3], [4, 5, 6]]))\n  m2 = MyWrapper(...)\n  m3 = m1[0, 1:-1]\n  m4 = m1.expand_dims(axis=-1)\n  m5 = MyWrapper.concat((m1, m2), axis=-1)\n  # etc.\n  ```\n  """"""\n\n  @classmethod\n  def concat(cls, tensor_wrappers, axis):\n    """"""See tf.concat.""""""\n    cls._validate_tensor_types(tensor_wrappers, ""concat"")\n    return cls._apply_sequence_to_tensor_op(\n        lambda ts: tf.concat(ts, axis), tensor_wrappers)\n\n  @classmethod\n  def stack(cls, tensor_wrappers, axis=0):\n    """"""See tf.stack.""""""\n    cls._validate_tensor_types(tensor_wrappers, ""stack"")\n    return cls._apply_sequence_to_tensor_op(\n        lambda ts: tf.stack(ts, axis), tensor_wrappers)\n\n  @classmethod\n  def where(cls, condition, tensor_wrapper_1, tensor_wrapper_2):\n    """"""See tf.where. Only three-argument version is supported here.""""""\n    tensor_wrappers = [tensor_wrapper_1, tensor_wrapper_2]\n    cls._validate_tensor_types(tensor_wrappers, ""where"")\n    return cls._apply_sequence_to_tensor_op(\n        lambda ts: tf.compat.v2.where(condition, ts[0], ts[1]), tensor_wrappers)\n\n  @classmethod\n  def _validate_tensor_types(cls, tensor_wrappers, function_name):\n    for tensor in tensor_wrappers:\n      if not isinstance(tensor, cls):\n        raise ValueError(""{}.{} cannot be applied to {}"".format(\n            cls.__name__, function_name,\n            type(tensor).__name__))\n\n  def expand_dims(self, axis):\n    """"""See tf.expand_dims.""""""\n    return self._apply_op(lambda t: tf.expand_dims(t, axis))\n\n  def reshape(self, shape):\n    """"""See tf.reshape.""""""\n    return self._apply_op(lambda t: tf.reshape(t, shape))\n\n  def identity(self):\n    """"""See tf.identity.""""""\n    return self._apply_op(tf.identity)\n\n  def broadcast_to(self, shape):\n    """"""See tf.broadcast_to.""""""\n    return self._apply_op(lambda t: tf.broadcast_to(t, shape))\n\n  def transpose(self, perm=None):\n    """"""See tf.transpose.""""""\n    return self._apply_op(lambda t: tf.transpose(t, perm))\n\n  def squeeze(self, axis=None):\n    """"""See tf.squeeze.""""""\n    return self._apply_op(lambda t: tf.squeeze(t, axis))\n\n  def boolean_mask(self, mask, axis=None):\n    """"""See tf.boolean_mask.""""""\n    return self._apply_op(lambda t: tf.boolean_mask(t, mask, axis=axis))\n\n  def __getitem__(self, key):\n    """"""Implements indexing.""""""\n    return self._apply_op(lambda t: t.__getitem__(key))\n\n  def __getslice__(self, *args):\n    """"""Implements slicing.""""""\n    return self._apply_op(lambda t: t.__getslice__(*args))\n\n  @classmethod\n  @abc.abstractmethod\n  def _apply_sequence_to_tensor_op(cls, op_fn, tensor_wrappers):\n    """"""Applies given sequence-to-tensor op.\n\n    This method is used for implementing ops that take a sequence of tensors and\n    return a new tensor, such as tf.concat and tf.stack. Implementing wrappers\n    should apply `op_fn` to the backing tensor(s) and return an new wrapper\n    instance with the combined backing tensor.\n\n    Args:\n     op_fn: Callable that applies sequence-to-tensor op to the given sequence\n       of Tensors. E.g. applies tf.concat.\n     tensor_wrappers: a sequence of tensor wrappers to be transformed. All\n       elements have the type of the implementing TensorWrapper class.\n\n    Returns:\n      A TensorWrapper instance with combined backing tensor(s).\n    """"""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def _apply_op(self, op_fn):\n    """"""Applies given tensor-to-tensor op.\n\n    This method is used for implementing ops that take a tensor and return a new\n    tensor, such as tf.expand_dims or tf.transpose. Implementing wrappers\n    should apply `op_fn` to the backing tensor(s) and return an new wrapper\n    instance with the updated backing tensor.\n\n    Args:\n       op_fn: Callable that applies tensor-to-tensor op to the given Tensor.\n        E.g. applies tf.expand_dims.\n\n    Returns:\n      A TensorWrapper instance with updated backing tensor(s).\n    """"""\n    raise NotImplementedError()\n'"
tf_quant_finance/datetime/test_data.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Test data for tests in this package.""""""\n\nimport tf_quant_finance as tff\n\ndates = tff.datetime\n\ntest_dates = [\n    (1901, 1, 1),\n    (1901, 2, 15),\n    (1901, 2, 28),\n    (1901, 3, 28),\n    (2019, 1, 1),\n    (2019, 1, 15),\n    (2019, 1, 31),\n    (2019, 2, 1),\n    (2019, 2, 15),\n    (2019, 2, 28),\n    (2019, 3, 1),\n    (2019, 3, 15),\n    (2019, 3, 31),\n    (2019, 4, 1),\n    (2019, 4, 15),\n    (2019, 4, 30),\n    (2019, 5, 1),\n    (2019, 5, 15),\n    (2019, 5, 31),\n    (2019, 6, 1),\n    (2019, 6, 15),\n    (2019, 6, 30),\n    (2019, 7, 1),\n    (2019, 7, 15),\n    (2019, 7, 31),\n    (2019, 8, 1),\n    (2019, 8, 15),\n    (2019, 8, 31),\n    (2019, 9, 1),\n    (2019, 9, 15),\n    (2019, 9, 30),\n    (2019, 10, 1),\n    (2019, 10, 15),\n    (2019, 10, 31),\n    (2019, 11, 1),\n    (2019, 11, 15),\n    (2019, 11, 30),\n    (2019, 12, 1),\n    (2019, 12, 15),\n    (2019, 12, 31),\n    (2020, 1, 1),\n    (2020, 1, 15),\n    (2020, 1, 31),\n    (2020, 2, 1),\n    (2020, 2, 15),\n    (2020, 2, 28),\n    (2020, 2, 29),\n    (2020, 3, 1),\n    (2020, 3, 15),\n    (2020, 3, 31),\n    (2020, 4, 1),\n    (2020, 4, 15),\n    (2020, 4, 30),\n    (2020, 5, 1),\n    (2020, 5, 15),\n    (2020, 5, 31),\n    (2020, 6, 1),\n    (2020, 6, 15),\n    (2020, 6, 30),\n    (2020, 7, 1),\n    (2020, 7, 15),\n    (2020, 7, 31),\n    (2020, 8, 1),\n    (2020, 8, 15),\n    (2020, 8, 31),\n    (2020, 9, 1),\n    (2020, 9, 15),\n    (2020, 9, 30),\n    (2020, 10, 1),\n    (2020, 10, 15),\n    (2020, 10, 31),\n    (2020, 11, 1),\n    (2020, 11, 15),\n    (2020, 11, 30),\n    (2020, 12, 1),\n    (2020, 12, 15),\n    (2020, 12, 31),\n    (2000, 2, 15),\n    (2000, 2, 28),\n    (2000, 2, 29),\n    (2000, 3, 1),\n    (2099, 3, 15),\n    (2099, 2, 15),\n    (2099, 2, 28),\n    (2099, 12, 31),\n    (1900, 2, 15),\n    (1900, 2, 28),\n    (1900, 3, 1),\n    (2100, 2, 15),\n    (2100, 2, 28),\n    (2100, 3, 1),\n    (2100, 3, 15),\n    (2200, 2, 15),\n    (2200, 2, 28),\n    (2200, 3, 1),\n    (2200, 3, 15),\n    (2300, 2, 15),\n    (2300, 2, 28),\n    (2300, 3, 1),\n    (2300, 3, 15),\n]\n\nday_addition_data = [\n    ((2019, 2, 15), 5, (2019, 2, 20)),\n    ((2019, 2, 15), 15, (2019, 3, 2)),\n    ((2019, 2, 15), 365, (2020, 2, 15)),\n    ((2019, 2, 15), 365 * 2, (2021, 2, 14)),\n    ((2019, 2, 15), -5, (2019, 2, 10)),\n    ((2019, 2, 15), -15, (2019, 1, 31)),\n]\n\nweek_addition_data = [\n    ((2019, 2, 15), 1, (2019, 2, 22)),\n    ((2019, 2, 15), 3, (2019, 3, 8)),\n    ((2019, 2, 15), 60, (2020, 4, 10)),\n    ((2019, 2, 15), -2, (2019, 2, 1)),\n]\n\nmonth_addition_data = [\n    ((2019, 2, 15), 1, (2019, 3, 15)),\n    ((2019, 2, 15), 25, (2021, 3, 15)),\n    ((2019, 1, 31), 3, (2019, 4, 30)),\n    ((2019, 1, 31), 1, (2019, 2, 28)),\n    ((2018, 11, 15), 1, (2018, 12, 15)),\n    ((2018, 11, 15), 2, (2019, 1, 15)),\n    ((2018, 11, 15), 13, (2019, 12, 15)),\n    ((2018, 11, 15), 14, (2020, 1, 15)),\n    ((2018, 11, 15), 16, (2020, 3, 15)),\n    ((2018, 12, 15), 1, (2019, 1, 15)),\n    ((2018, 12, 30), 2, (2019, 2, 28)),\n    ((2018, 11, 29), 3, (2019, 2, 28)),\n    ((2018, 11, 29), 15, (2020, 2, 29)),\n    ((2020, 2, 29), 12, (2021, 2, 28)),\n    ((2019, 2, 15), -1, (2019, 1, 15)),\n    ((2019, 5, 31), -3, (2019, 2, 28)),\n]\n\nyear_addition_data = [\n    ((2019, 5, 15), 1, (2020, 5, 15)),\n    ((2019, 7, 15), 25, (2044, 7, 15)),\n    ((2020, 2, 29), 1, (2021, 2, 28)),\n    ((2021, 2, 28), 1, (2022, 2, 28)),\n    ((2020, 2, 29), 4, (2024, 2, 29)),\n    ((2024, 2, 29), -5, (2019, 2, 28)),\n]\n\nday_of_year_data = [\n    ((2019, 1, 1), 1),\n    ((2019, 2, 15), 46),\n    ((2019, 3, 10), 69),\n    ((2019, 4, 8), 98),\n    ((2019, 5, 20), 140),\n    ((2019, 6, 24), 175),\n    ((2019, 7, 9), 190),\n    ((2019, 8, 12), 224),\n    ((2019, 9, 30), 273),\n    ((2019, 10, 11), 284),\n    ((2019, 11, 28), 332),\n    ((2019, 12, 31), 365),\n    ((2020, 1, 1), 1),\n    ((2020, 2, 15), 46),\n    ((2020, 3, 10), 70),\n    ((2020, 4, 8), 99),\n    ((2020, 5, 20), 141),\n    ((2020, 6, 24), 176),\n    ((2020, 7, 9), 191),\n    ((2020, 8, 12), 225),\n    ((2020, 9, 30), 274),\n    ((2020, 10, 11), 285),\n    ((2020, 11, 28), 333),\n    ((2020, 12, 31), 366),\n]\n\ninvalid_dates = [\n    (-5, 3, 15),\n    (2015, -3, 15),\n    (2015, 0, 15),\n    (2015, 3, 0),\n    (2015, 3, 32),\n    (2015, 4, 31),\n    (2015, 2, 29),\n    (2016, 2, 30),\n]\n\nend_of_month_test_cases = [\n    # (date, expected_is_end_of_month, expected_to_end_of_month)\n    ((2019, 1, 30), False, (2019, 1, 31)),\n    ((2019, 1, 31), True, (2019, 1, 31)),\n    ((2019, 2, 15), False, (2019, 2, 28)),\n    ((2019, 2, 28), True, (2019, 2, 28)),\n    ((2020, 2, 28), False, (2020, 2, 29)),\n    ((2020, 2, 29), True, (2020, 2, 29)),\n    ((2018, 11, 5), False, (2018, 11, 30)),\n    ((2018, 11, 30), True, (2018, 11, 30)),\n]\n\nholidays = [\n    (2020, 1, 1),  # Wed\n    (2020, 7, 3),  # Fri\n    (2020, 12, 25),  # Fri\n    (2021, 1, 1),  # Fri\n    (2021, 7, 5),  # Mon\n    (2021, 12, 24),  # Fri\n]\n\nis_business_day_data = [\n    ((2020, 1, 1), False),\n    ((2020, 1, 2), True),\n    ((2020, 1, 3), True),\n    ((2020, 1, 4), False),\n    ((2020, 1, 5), False),\n    ((2020, 3, 20), True),\n    ((2020, 3, 21), False),\n    ((2020, 3, 22), False),\n    ((2020, 3, 23), True),\n    ((2020, 7, 3), False),\n    ((2020, 12, 25), False),\n    ((2021, 1, 1), False),\n    ((2021, 7, 5), False),\n    ((2021, 12, 24), False),\n    ((2021, 12, 25), False),\n    ((2021, 12, 26), False),\n    ((2021, 12, 31), True)\n]\n\nadjusted_dates_data = [\n    {\n        ""date"": (2020, 1, 4),  # Sat\n        ""unadjusted"": (2020, 1, 4),\n        ""following"": (2020, 1, 6),\n        ""preceding"": (2020, 1, 3),\n        ""modified_following"": (2020, 1, 6),\n        ""modified_preceding"": (2020, 1, 3),\n    },\n    {\n        ""date"": (2020, 1, 5),  # Sun\n        ""unadjusted"": (2020, 1, 5),\n        ""following"": (2020, 1, 6),\n        ""preceding"": (2020, 1, 3),\n        ""modified_following"": (2020, 1, 6),\n        ""modified_preceding"": (2020, 1, 3),\n    },\n    {\n        ""date"": (2020, 1, 3),  # Fri, business day\n        ""unadjusted"": (2020, 1, 3),\n        ""following"": (2020, 1, 3),\n        ""preceding"": (2020, 1, 3),\n        ""modified_following"": (2020, 1, 3),\n        ""modified_preceding"": (2020, 1, 3),\n    },\n    {\n        ""date"": (2020, 7, 3),  # Fri, holiday\n        ""unadjusted"": (2020, 7, 3),\n        ""following"": (2020, 7, 6),\n        ""preceding"": (2020, 7, 2),\n        ""modified_following"": (2020, 7, 6),\n        ""modified_preceding"": (2020, 7, 2),\n    },\n    {\n        ""date"": (2020, 7, 4),  # Sat, after holiday\n        ""unadjusted"": (2020, 7, 4),\n        ""following"": (2020, 7, 6),\n        ""preceding"": (2020, 7, 2),\n        ""modified_following"": (2020, 7, 6),\n        ""modified_preceding"": (2020, 7, 2),\n    },\n    {\n        ""date"": (2021, 2, 27),  # Sat\n        ""unadjusted"": (2021, 2, 27),\n        ""following"": (2021, 3, 1),\n        ""preceding"": (2021, 2, 26),\n        ""modified_following"": (2021, 2, 26),\n        ""modified_preceding"": (2021, 2, 26),\n    },\n    {\n        ""date"": (2021, 2, 28),  # Sun\n        ""unadjusted"": (2021, 2, 28),\n        ""following"": (2021, 3, 1),\n        ""preceding"": (2021, 2, 26),\n        ""modified_following"": (2021, 2, 26),\n        ""modified_preceding"": (2021, 2, 26),\n    },\n    {\n        ""date"": (2020, 2, 1),  # Sat\n        ""unadjusted"": (2020, 2, 1),\n        ""following"": (2020, 2, 3),\n        ""preceding"": (2020, 1, 31),\n        ""modified_following"": (2020, 2, 3),\n        ""modified_preceding"": (2020, 2, 3),\n    },\n]\n\nadd_months_data = [\n    {\n        ""date"": (2019, 12, 4),\n        ""months"": 1,\n        ""unadjusted"": (2020, 1, 4),\n        ""following"": (2020, 1, 6),\n        ""preceding"": (2020, 1, 3),\n        ""modified_following"": (2020, 1, 6),\n        ""modified_preceding"": (2020, 1, 3),\n    },\n    {\n        ""date"": (2019, 11, 5),\n        ""months"": 2,\n        ""unadjusted"": (2020, 1, 5),\n        ""following"": (2020, 1, 6),\n        ""preceding"": (2020, 1, 3),\n        ""modified_following"": (2020, 1, 6),\n        ""modified_preceding"": (2020, 1, 3),\n    },\n    {\n        ""date"": (2019, 10, 3),\n        ""months"": 3,\n        ""unadjusted"": (2020, 1, 3),\n        ""following"": (2020, 1, 3),\n        ""preceding"": (2020, 1, 3),\n        ""modified_following"": (2020, 1, 3),\n        ""modified_preceding"": (2020, 1, 3),\n    },\n    {\n        ""date"": (2020, 2, 3),\n        ""months"": 5,\n        ""unadjusted"": (2020, 7, 3),\n        ""following"": (2020, 7, 6),\n        ""preceding"": (2020, 7, 2),\n        ""modified_following"": (2020, 7, 6),\n        ""modified_preceding"": (2020, 7, 2),\n    },\n    {\n        ""date"": (2020, 2, 29),\n        ""months"": 12,\n        ""unadjusted"": (2021, 2, 28),\n        ""following"": (2021, 3, 1),\n        ""preceding"": (2021, 2, 26),\n        ""modified_following"": (2021, 2, 26),\n        ""modified_preceding"": (2021, 2, 26),\n    },\n    {\n        ""date"": (2020, 2, 1),\n        ""months"": 6,\n        ""unadjusted"": (2020, 8, 1),\n        ""following"": (2020, 8, 3),\n        ""preceding"": (2020, 7, 31),\n        ""modified_following"": (2020, 8, 3),\n        ""modified_preceding"": (2020, 8, 3),\n    },\n]\n\nadd_days_data = [  # Assumes ""modified following"" convention.\n    {\n        ""date"": (2020, 1, 2),\n        ""days"": 1,\n        ""shifted_date"": (2020, 1, 3),\n    },\n    {\n        ""date"": (2020, 1, 2),\n        ""days"": 2,\n        ""shifted_date"": (2020, 1, 6),\n    },\n    {\n        ""date"": (2020, 1, 2),\n        ""days"": 10,\n        ""shifted_date"": (2020, 1, 16),\n    },\n    {\n        ""date"": (2020, 1, 4),   # Sat\n        ""days"": 0,\n        ""shifted_date"": (2020, 1, 6),\n    },\n    {\n        ""date"": (2020, 1, 4),\n        ""days"": 1,\n        ""shifted_date"": (2020, 1, 7),\n    },\n    {\n        ""date"": (2020, 1, 4),\n        ""days"": -1,\n        ""shifted_date"": (2020, 1, 3),\n    },\n    {\n        ""date"": (2020, 2, 27),\n        ""days"": 3,\n        ""shifted_date"": (2020, 3, 3),\n    },\n    {\n        ""date"": (2021, 2, 26),\n        ""days"": 3,\n        ""shifted_date"": (2021, 3, 3),\n    },\n    {\n        ""date"": (2020, 2, 29),   # Sat\n        ""days"": 0,\n        ""shifted_date"": (2020, 2, 28),\n    },\n    {\n        ""date"": (2020, 2, 29),\n        ""days"": 1,\n        ""shifted_date"": (2020, 3, 2),\n    },\n    {\n        ""date"": (2020, 2, 29),\n        ""days"": -1,\n        ""shifted_date"": (2020, 2, 27),\n    },\n    {\n        ""date"": (2020, 6, 29),\n        ""days"": 5,\n        ""shifted_date"": (2020, 7, 7),\n    },\n    {\n        ""date"": (2020, 6, 29),\n        ""days"": 10,\n        ""shifted_date"": (2020, 7, 14),\n    },\n    {\n        ""date"": (2020, 12, 23),\n        ""days"": 10,\n        ""shifted_date"": (2021, 1, 8),\n    },\n    {\n        ""date"": (2021, 12, 29),\n        ""days"": 2,\n        ""shifted_date"": (2021, 12, 31),\n    }\n]\n\ndays_between_data = [\n    {\n        ""date1"": (2020, 1, 2),\n        ""date2"": (2020, 1, 3),\n        ""days"": 1,\n    },\n    {\n        ""date1"": (2020, 1, 2),\n        ""date2"": (2020, 1, 6),\n        ""days"": 2,\n    },\n    {\n        ""date1"": (2020, 1, 6),\n        ""date2"": (2020, 1, 2),\n        ""days"": 0,\n    },\n    {\n        ""date1"": (2020, 1, 6),\n        ""date2"": (2020, 1, 6),\n        ""days"": 0,\n    },\n    {\n        ""date1"": (2020, 1, 2),\n        ""date2"": (2020, 1, 16),\n        ""days"": 10,\n    },\n    {\n        ""date1"": (2020, 1, 3),\n        ""date2"": (2020, 1, 4),\n        ""days"": 1,\n    },\n    {\n        ""date1"": (2020, 1, 4),\n        ""date2"": (2020, 1, 6),\n        ""days"": 0,\n    },\n    {\n        ""date1"": (2020, 1, 4),\n        ""date2"": (2020, 1, 5),\n        ""days"": 0,\n    },\n    {\n        ""date1"": (2020, 1, 4),\n        ""date2"": (2020, 1, 7),\n        ""days"": 1,\n    },\n    {\n        ""date1"": (2020, 1, 4),\n        ""date2"": (2020, 1, 3),\n        ""days"": 0,\n    },\n    {\n        ""date1"": (2020, 2, 27),\n        ""days"": 3,\n        ""date2"": (2020, 3, 3),\n    },\n    {\n        ""date1"": (2021, 2, 26),\n        ""days"": 3,\n        ""date2"": (2021, 3, 3),\n    },\n    {\n        ""date1"": (2020, 2, 29),   # Sat\n        ""date2"": (2020, 2, 28),\n        ""days"": 0,\n    },\n    {\n        ""date1"": (2020, 2, 29),\n        ""date2"": (2020, 3, 1),\n        ""days"": 0,\n    },\n    {\n        ""date1"": (2020, 2, 29),\n        ""date2"": (2020, 3, 2),\n        ""days"": 0,\n    },\n    {\n        ""date1"": (2020, 2, 29),\n        ""date2"": (2020, 3, 3),\n        ""days"": 1,\n    },\n    {\n        ""date1"": (2020, 6, 29),\n        ""date2"": (2020, 7, 7),\n        ""days"": 5,\n    },\n    {\n        ""date1"": (2020, 6, 29),\n        ""date2"": (2020, 7, 14),\n        ""days"": 10,\n    },\n    {\n        ""date1"": (2020, 12, 23),\n        ""date2"": (2021, 1, 8),\n        ""days"": 10,\n    },\n    {\n        ""date1"": (2021, 12, 29),\n        ""date2"": (2021, 12, 31),\n        ""days"": 2,\n    }\n]\n\n# Assumes calendar with only weekends and modified-following convention.\n# end_of_month is False by default.\nperiodic_schedule_test_cases = [\n    {\n        ""testcase_name"": ""monthly_forward"",\n        ""start_dates"": [(2020, 1, 1)],\n        ""end_dates"": [(2021, 3, 31)],\n        ""period_type"": dates.PeriodType.MONTH,\n        ""period_quantities"": 1,\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 1, 1), (2020, 2, 3), (2020, 3, 2),\n                               (2020, 4, 1), (2020, 5, 1), (2020, 6, 1),\n                               (2020, 7, 1), (2020, 8, 3), (2020, 9, 1),\n                               (2020, 10, 1), (2020, 11, 2), (2020, 12, 1),\n                               (2021, 1, 1), (2021, 2, 1), (2021, 3, 1),\n                               (2021, 3, 31)]]\n    },\n    {\n        ""testcase_name"": ""monthly_backward"",\n        ""start_dates"": [(2020, 1, 1)],\n        ""end_dates"": [(2021, 3, 31)],\n        ""period_type"": dates.PeriodType.MONTH,\n        ""period_quantities"": 1,\n        ""backward"": True,\n        ""expected_schedule"": [[(2020, 1, 1), (2020, 1, 31), (2020, 2, 28),\n                               (2020, 3, 31), (2020, 4, 30), (2020, 5, 29),\n                               (2020, 6, 30), (2020, 7, 31), (2020, 8, 31),\n                               (2020, 9, 30), (2020, 10, 30), (2020, 11, 30),\n                               (2020, 12, 31), (2021, 1, 29), (2021, 2, 26),\n                               (2021, 3, 31)]]\n    },\n    {\n        ""testcase_name"": ""quarterly_forward"",\n        ""start_dates"": [(2020, 1, 15)],\n        ""end_dates"": [(2021, 3, 31)],\n        ""period_type"": dates.PeriodType.MONTH,\n        ""period_quantities"": 3,\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 1, 15), (2020, 4, 15), (2020, 7, 15),\n                               (2020, 10, 15), (2021, 1, 15), (2021, 3, 31)]]\n    },\n    {\n        ""testcase_name"": ""quarterly_backward"",\n        ""start_dates"": [(2020, 1, 15)],\n        ""end_dates"": [(2021, 3, 25)],\n        ""period_type"": dates.PeriodType.MONTH,\n        ""period_quantities"": 3,\n        ""backward"": True,\n        ""expected_schedule"": [[(2020, 1, 15), (2020, 3, 25), (2020, 6, 25),\n                               (2020, 9, 25), (2020, 12, 25), (2021, 3, 25)]]\n    },\n    {\n        ""testcase_name"": ""yearly"",\n        ""start_dates"": [(2024, 2, 29)],\n        ""end_dates"": [(2028, 12, 31)],\n        ""period_type"": dates.PeriodType.YEAR,\n        ""period_quantities"": 1,\n        ""backward"": False,\n        ""expected_schedule"": [[(2024, 2, 29), (2025, 2, 28), (2026, 2, 27),\n                               (2027, 2, 26), (2028, 2, 29), (2028, 12, 29)]]\n    },\n    {\n        ""testcase_name"": ""biweekly"",\n        ""start_dates"": [(2020, 11, 20)],\n        ""end_dates"": [(2021, 1, 31)],\n        ""period_type"": dates.PeriodType.WEEK,\n        ""period_quantities"": 2,\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 11, 20), (2020, 12, 4), (2020, 12, 18),\n                               (2021, 1, 1), (2021, 1, 15), (2021, 1, 29),\n                               (2021, 1, 29)]]\n    },\n    {\n        ""testcase_name"": ""every_10_days"",\n        ""start_dates"": [(2020, 5, 1)],\n        ""end_dates"": [(2020, 7, 1)],\n        ""period_type"": dates.PeriodType.DAY,\n        ""period_quantities"": 10,\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 5, 1), (2020, 5, 11), (2020, 5, 21),\n                               (2020, 5, 29), (2020, 6, 10), (2020, 6, 22),\n                               (2020, 6, 30), (2020, 7, 1)]]\n    },\n    {\n        ""testcase_name"": ""includes_end_date_if_on_schedule"",\n        ""start_dates"": [(2020, 11, 20)],\n        ""end_dates"": [(2021, 1, 29)],\n        ""period_type"": dates.PeriodType.WEEK,\n        ""period_quantities"": 2,\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 11, 20), (2020, 12, 4), (2020, 12, 18),\n                               (2021, 1, 1), (2021, 1, 15), (2021, 1, 29)]]\n    },\n    {\n        ""testcase_name"": ""includes_start_date_if_on_schedule"",\n        ""start_dates"": [(2020, 3, 25)],\n        ""end_dates"": [(2021, 3, 25)],\n        ""period_type"": dates.PeriodType.MONTH,\n        ""period_quantities"": 3,\n        ""backward"": True,\n        ""expected_schedule"": [[(2020, 3, 25), (2020, 6, 25), (2020, 9, 25),\n                               (2020, 12, 25), (2021, 3, 25)]]\n    },\n    {\n        ""testcase_name"": ""rolls_start_date_out_of_bounds"",\n        ""start_dates"": [(2020, 2, 29)],\n        ""end_dates"": [(2024, 12, 31)],\n        ""period_type"": dates.PeriodType.YEAR,\n        ""period_quantities"": 1,\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 2, 28), (2021, 2, 26), (2022, 2, 28),\n                               (2023, 2, 28), (2024, 2, 29), (2024, 12, 31)]]\n    },\n    {\n        ""testcase_name"": ""rolls_end_date_out_of_bounds"",\n        ""start_dates"": [(2020, 1, 1)],\n        ""end_dates"": [(2020, 2, 2)],\n        ""period_type"": dates.PeriodType.MONTH,\n        ""period_quantities"": 1,\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 1, 1), (2020, 2, 3), (2020, 2, 3)]]\n    },\n    {\n        ""testcase_name"": ""batch_with_same_period"",\n        ""start_dates"": [(2020, 1, 15), (2020, 4, 15)],\n        ""end_dates"": [(2021, 3, 31), (2021, 1, 1)],\n        ""period_type"": dates.PeriodType.MONTH,\n        ""period_quantities"": 3,\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 1, 15), (2020, 4, 15), (2020, 7, 15),\n                               (2020, 10, 15), (2021, 1, 15), (2021, 3, 31)],\n                              [(2020, 4, 15), (2020, 7, 15), (2020, 10, 15),\n                               (2021, 1, 1), (2021, 1, 1), (2021, 1, 1)]]\n    },\n    {\n        ""testcase_name"": ""batch_with_different_periods"",\n        ""start_dates"": [(2020, 1, 15), (2020, 4, 15)],\n        ""end_dates"": [(2021, 3, 31), (2021, 1, 1)],\n        ""period_type"": dates.PeriodType.MONTH,\n        ""period_quantities"": [4, 6],\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 1, 15), (2020, 5, 15), (2020, 9, 15),\n                               (2021, 1, 15), (2021, 3, 31)],\n                              [(2020, 4, 15), (2020, 10, 15), (2021, 1, 1),\n                               (2021, 1, 1), (2021, 1, 1)]]\n    },\n    {\n        ""testcase_name"": ""batch_backward"",\n        ""start_dates"": [(2020, 1, 15), (2020, 4, 15)],\n        ""end_dates"": [(2021, 3, 31), (2021, 1, 1)],\n        ""period_type"": dates.PeriodType.MONTH,\n        ""period_quantities"": [4, 6],\n        ""backward"": True,\n        ""expected_schedule"": [[(2020, 1, 15), (2020, 3, 31), (2020, 7, 31),\n                               (2020, 11, 30), (2021, 3, 31)],\n                              [(2020, 4, 15), (2020, 4, 15), (2020, 4, 15),\n                               (2020, 7, 1), (2021, 1, 1)]]\n    },\n    {\n        ""testcase_name"": ""rank_2_batch"",\n        ""start_dates"": [[(2020, 1, 15), (2020, 4, 15)],\n                        [(2020, 1, 17), (2020, 5, 12)]],\n        ""end_dates"": [[(2020, 12, 31), (2020, 11, 30)],\n                      [(2020, 10, 31), (2020, 9, 30)]],\n        ""period_type"": dates.PeriodType.MONTH,\n        ""period_quantities"": [[4, 3], [3, 2]],\n        ""backward"": False,\n        ""expected_schedule"": [[[(2020, 1, 15), (2020, 5, 15), (2020, 9, 15),\n                                (2020, 12, 31), (2020, 12, 31)],\n                               [(2020, 4, 15), (2020, 7, 15), (2020, 10, 15),\n                                (2020, 11, 30), (2020, 11, 30)]],\n                              [[(2020, 1, 17), (2020, 4, 17), (2020, 7, 17),\n                                (2020, 10, 19), (2020, 10, 30)],\n                               [(2020, 5, 12), (2020, 7, 13), (2020, 9, 14),\n                                (2020, 9, 30), (2020, 9, 30)]]]\n    },\n    {\n        ""testcase_name"": ""end_of_month_forward"",\n        ""start_dates"": [(2020, 1, 15), (2020, 4, 30)],\n        ""end_dates"": [(2021, 3, 31), (2021, 1, 1)],\n        ""period_type"": dates.PeriodType.MONTH,\n        ""period_quantities"": [4, 3],\n        ""backward"": False,\n        ""end_of_month"": True,\n        ""expected_schedule"": [[(2020, 1, 15), (2020, 5, 15), (2020, 9, 15),\n                               (2021, 1, 15), (2021, 3, 31)],\n                              [(2020, 4, 30), (2020, 7, 31), (2020, 10, 30),\n                               (2021, 1, 29), (2021, 1, 29)]]\n    },\n    {\n        ""testcase_name"": ""end_of_month_backward"",\n        ""start_dates"": [(2020, 1, 15), (2020, 4, 15)],\n        # Note that (2021, 2, 28) is Sunday, so it\'s rolled, but we still apply\n        # end_of_month.\n        ""end_dates"": [(2021, 2, 28), (2021, 1, 1)],\n        ""period_type"": dates.PeriodType.MONTH,\n        ""period_quantities"": [4, 6],\n        ""backward"": True,\n        ""end_of_month"": True,\n        ""expected_schedule"": [[(2020, 1, 31), (2020, 2, 28), (2020, 6, 30),\n                               (2020, 10, 30), (2021, 2, 26)],\n                              [(2020, 4, 15), (2020, 4, 15), (2020, 4, 15),\n                               (2020, 7, 1), (2021, 1, 1)]]\n    },\n]\n\ndays_in_leap_years_test_cases = [\n    {\n        ""date1"": (2020, 2, 17),\n        ""date2"": (2020, 2, 19),\n        ""expected"": 2\n    },\n    {\n        ""date1"": (2019, 2, 17),\n        ""date2"": (2019, 2, 19),\n        ""expected"": 0\n    },\n    {\n        ""date1"": (2100, 2, 17),\n        ""date2"": (2100, 2, 19),\n        ""expected"": 0\n    },\n    {\n        ""date1"": (2020, 2, 17),\n        ""date2"": (2020, 4, 5),\n        ""expected"": 13 + 31 + 4\n    },\n    {\n        ""date1"": (2020, 2, 17),\n        ""date2"": (2020, 12, 31),\n        ""expected"": 318  # 366 - 31 - 17\n    },\n    {\n        ""date1"": (2020, 2, 17),\n        ""date2"": (2021, 1, 1),\n        ""expected"": 319\n    },\n    {\n        ""date1"": (2020, 2, 17),\n        ""date2"": (2021, 5, 12),\n        ""expected"": 319\n    },\n    {\n        ""date1"": (2020, 2, 17),\n        ""date2"": (2023, 12, 31),\n        ""expected"": 319\n    },\n    {\n        ""date1"": (2020, 2, 17),\n        ""date2"": (2024, 1, 1),\n        ""expected"": 319\n    },\n    {\n        ""date1"": (2020, 2, 17),\n        ""date2"": (2024, 1, 5),\n        ""expected"": 323\n    },\n    {\n        ""date1"": (2020, 2, 17),\n        ""date2"": (2024, 12, 31),\n        ""expected"": 684\n    },\n    {\n        ""date1"": (2020, 2, 17),\n        ""date2"": (2025, 1, 1),\n        ""expected"": 685\n    },\n    {\n        ""date1"": (2020, 1, 1),\n        ""date2"": (2025, 1, 1),\n        ""expected"": 732\n    },\n    {\n        ""date1"": (2019, 12, 31),\n        ""date2"": (2025, 1, 1),\n        ""expected"": 732\n    },\n    {\n        ""date1"": (2019, 8, 24),\n        ""date2"": (2025, 1, 1),\n        ""expected"": 732\n    },\n    {\n        ""date1"": (2019, 8, 24),\n        ""date2"": (2024, 2, 10),\n        ""expected"": 406  # 366 + 31 + 9\n    },\n    {\n        ""date1"": (2019, 8, 24),\n        ""date2"": (2024, 1, 1),\n        ""expected"": 366\n    },\n    {\n        ""date1"": (2019, 8, 24),\n        ""date2"": (2022, 4, 11),\n        ""expected"": 366\n    },\n    {\n        ""date1"": (2019, 8, 24),\n        ""date2"": (2020, 12, 31),\n        ""expected"": 365\n    },\n    {\n        ""date1"": (2019, 8, 24),\n        ""date2"": (2020, 2, 10),\n        ""expected"": 40\n    },\n    {\n        ""date1"": (2019, 8, 24),\n        ""date2"": (2020, 1, 1),\n        ""expected"": 0\n    },\n    {\n        ""date1"": (2019, 8, 24),\n        ""date2"": (2019, 10, 17),\n        ""expected"": 0\n    },\n    {\n        ""date1"": (2020, 12, 31),\n        ""date2"": (2021, 10, 17),\n        ""expected"": 1\n    },\n    {\n        ""date1"": (2100, 12, 31),\n        ""date2"": (2101, 10, 17),\n        ""expected"": 0\n    },\n    {\n        ""date1"": (2100, 5, 3),\n        ""date2"": (2100, 12, 31),\n        ""expected"": 0\n    },\n    {\n        ""date1"": (2096, 12, 15),\n        ""date2"": (2100, 12, 31),\n        ""expected"": 17\n    },\n    {\n        ""date1"": (2096, 12, 15),\n        ""date2"": (2104, 1, 4),\n        ""expected"": 20\n    },\n    {\n        ""date1"": (2020, 2, 19),\n        ""date2"": (2020, 2, 17),\n        ""expected"": -2\n    },\n    {\n        ""date1"": (2024, 2, 10),\n        ""date2"": (2019, 8, 24),\n        ""expected"": -406\n    },\n]\n\nleap_days_between_dates_test_cases = [\n    {\n        ""date1"": (2020, 2, 17),\n        ""date2"": (2020, 2, 19),\n        ""expected"": 0\n    },\n    {\n        ""date1"": (2020, 2, 17),\n        ""date2"": (2020, 3, 19),\n        ""expected"": 1\n    },\n    {\n        ""date1"": (2020, 2, 17),\n        ""date2"": (2020, 3, 1),\n        ""expected"": 1\n    },\n    {\n        ""date1"": (2020, 2, 17),\n        ""date2"": (2020, 2, 29),\n        ""expected"": 0\n    },\n    {\n        ""date1"": (2019, 2, 17),\n        ""date2"": (2019, 3, 19),\n        ""expected"": 0\n    },\n    {\n        ""date1"": (2100, 2, 17),\n        ""date2"": (2100, 3, 19),\n        ""expected"": 0\n    },\n    {\n        ""date1"": (2000, 2, 17),\n        ""date2"": (2000, 3, 19),\n        ""expected"": 1\n    },\n    {\n        ""date1"": (2019, 2, 17),\n        ""date2"": (2024, 3, 1),\n        ""expected"": 2\n    },\n    {\n        ""date1"": (2020, 2, 29),\n        ""date2"": (2024, 3, 1),\n        ""expected"": 2\n    },\n    {\n        ""date1"": (2020, 3, 1),\n        ""date2"": (2024, 3, 1),\n        ""expected"": 1\n    },\n    {\n        ""date1"": (2003, 3, 1),\n        ""date2"": (2024, 3, 1),\n        ""expected"": 6\n    },\n    {\n        ""date1"": (2003, 3, 1),\n        ""date2"": (2103, 3, 1),\n        ""expected"": 24\n    },\n]\n\nbusiness_day_schedule_test_cases = [\n    {\n        ""testcase_name"": ""within_business_week"",\n        ""start_dates"": [(2020, 3, 17)],\n        ""end_dates"": [(2020, 3, 20)],\n        ""holidays"": [],\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 3, 17), (2020, 3, 18), (2020, 3, 19),\n                               (2020, 3, 20)]]\n    },\n    {\n        ""testcase_name"": ""across_weekend"",\n        ""start_dates"": [(2020, 3, 17)],\n        ""end_dates"": [(2020, 3, 24)],\n        ""holidays"": [],\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 3, 17), (2020, 3, 18), (2020, 3, 19),\n                               (2020, 3, 20), (2020, 3, 23), (2020, 3, 24)]]\n    },\n    {\n        ""testcase_name"": ""across_holidays"",\n        ""start_dates"": [(2020, 3, 17)],\n        ""end_dates"": [(2020, 3, 24)],\n        ""holidays"": [(2020, 3, 18), (2020, 3, 20)],\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 3, 17), (2020, 3, 19),\n                               (2020, 3, 23), (2020, 3, 24)]]\n    },\n    {\n        ""testcase_name"": ""ending_on_weekend"",\n        ""start_dates"": [(2020, 3, 17)],\n        ""end_dates"": [(2020, 3, 22)],\n        ""holidays"": [],\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 3, 17), (2020, 3, 18), (2020, 3, 19),\n                               (2020, 3, 20)]]\n    },\n    {\n        ""testcase_name"": ""starting_on_weekend"",\n        ""start_dates"": [(2020, 3, 14)],\n        ""end_dates"": [(2020, 3, 19)],\n        ""holidays"": [],\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 3, 16), (2020, 3, 17), (2020, 3, 18),\n                               (2020, 3, 19)]]\n    },\n    {\n        ""testcase_name"": ""batch_forward"",\n        ""start_dates"": [(2020, 3, 17), (2020, 3, 14)],\n        ""end_dates"": [(2020, 3, 24), (2020, 3, 19)],\n        ""holidays"": [],\n        ""backward"": False,\n        ""expected_schedule"": [[(2020, 3, 17), (2020, 3, 18), (2020, 3, 19),\n                               (2020, 3, 20), (2020, 3, 23), (2020, 3, 24)],\n                              [(2020, 3, 16), (2020, 3, 17), (2020, 3, 18),\n                               (2020, 3, 19), (2020, 3, 19), (2020, 3, 19)]]\n    },\n    {\n        ""testcase_name"": ""batch_backward"",\n        ""start_dates"": [(2020, 3, 17), (2020, 3, 14)],\n        ""end_dates"": [(2020, 3, 24), (2020, 3, 19)],\n        ""holidays"": [],\n        ""backward"": True,\n        ""expected_schedule"": [[(2020, 3, 17), (2020, 3, 18), (2020, 3, 19),\n                               (2020, 3, 20), (2020, 3, 23), (2020, 3, 24)],\n                              [(2020, 3, 16), (2020, 3, 16), (2020, 3, 16),\n                               (2020, 3, 17), (2020, 3, 18), (2020, 3, 19)]]\n    },\n]\n\n'"
tf_quant_finance/datetime/unbounded_holiday_calendar.py,13,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""HolidayCalendar definition.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.datetime import constants\nfrom tf_quant_finance.datetime import date_tensor as dt\nfrom tf_quant_finance.datetime import date_utils as du\nfrom tf_quant_finance.datetime import holiday_calendar\nfrom tf_quant_finance.datetime import holiday_utils as hol\nfrom tf_quant_finance.datetime import periods\n\n\nclass UnboundedHolidayCalendar(holiday_calendar.HolidayCalendar):\n  """"""HolidayCalendar implementation.\n\n  Unlike BoundedHolidayCalendar, doesn\'t require specifying calendar bounds, and\n  supports weekends and holiday supplied as `Tensor`s. However, it is\n  (potentially significantly) slower than the dates.HolidayCalendar\n  implementation.\n  """"""\n\n  def __init__(self, weekend_mask=None, holidays=None):\n    """"""Initializer.\n\n    Args:\n      weekend_mask: Boolean `Tensor` of 7 elements one for each day of the week\n        starting with Monday at index 0. A `True` value indicates the day is\n        considered a weekend day and a `False` value implies a week day.\n        Default value: None which means no weekends are applied.\n      holidays: Defines the holidays that are added to the weekends defined by\n        `weekend_mask`. An instance of `dates.DateTensor` or an object\n        convertible to `DateTensor`.\n        Default value: None which means no holidays other than those implied by\n          the weekends (if any).\n    """"""\n    if weekend_mask is not None:\n      weekend_mask = tf.cast(weekend_mask, dtype=tf.bool)\n    if holidays is not None:\n      holidays = dt.convert_to_date_tensor(holidays).ordinal()\n    self._to_biz_space, self._from_biz_space = hol.business_day_mappers(\n        weekend_mask=weekend_mask, holidays=holidays)\n\n  def is_business_day(self, date_tensor):\n    """"""Returns a tensor of bools for whether given dates are business days.""""""\n    ordinals = dt.convert_to_date_tensor(date_tensor).ordinal()\n    return self._to_biz_space(ordinals)[1]\n\n  def roll_to_business_day(self, date_tensor, roll_convention):\n    """"""Rolls the given dates to business dates according to given convention.\n\n    Args:\n      date_tensor: `DateTensor` of dates to roll from.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting `DateTensor`.\n    """"""\n    if roll_convention == constants.BusinessDayConvention.NONE:\n      return date_tensor\n    ordinals = dt.convert_to_date_tensor(date_tensor).ordinal()\n    biz_days, is_bizday = self._to_biz_space(ordinals)\n    biz_days_rolled = self._apply_roll_biz_space(date_tensor, biz_days,\n                                                 is_bizday, roll_convention)\n    return dt.from_ordinals(self._from_biz_space(biz_days_rolled))\n\n  def _apply_roll_biz_space(self, date_tensor, biz_days, is_bizday,\n                            roll_convention):\n    """"""Applies roll in business day space.""""""\n    if roll_convention == constants.BusinessDayConvention.NONE:\n      # If no business convention is specified, return the current business\n      # day.\n      return biz_days\n\n    if roll_convention == constants.BusinessDayConvention.FOLLOWING:\n      return tf.where(is_bizday, biz_days, biz_days + 1)\n\n    if roll_convention == constants.BusinessDayConvention.PRECEDING:\n      return biz_days\n\n    if roll_convention == constants.BusinessDayConvention.MODIFIED_FOLLOWING:\n      maybe_prev_biz_day = biz_days\n      maybe_next_biz_day = tf.where(is_bizday, biz_days, biz_days + 1)\n      maybe_next_biz_ordinal = self._from_biz_space(maybe_next_biz_day)\n      take_previous = tf.not_equal(\n          _get_month(maybe_next_biz_ordinal), date_tensor.month())\n      return tf.where(take_previous, maybe_prev_biz_day, maybe_next_biz_day)\n\n    if roll_convention == constants.BusinessDayConvention.MODIFIED_PRECEDING:\n      maybe_prev_biz_day = biz_days\n      maybe_next_biz_day = tf.where(is_bizday, biz_days, biz_days + 1)\n      maybe_prev_biz_ordinal = self._from_biz_space(maybe_prev_biz_day)\n      take_next = tf.not_equal(\n          _get_month(maybe_prev_biz_ordinal), date_tensor.month())\n      return tf.where(take_next, maybe_next_biz_day, maybe_prev_biz_day)\n\n    raise ValueError(\'Unsupported roll convention: {}\'.format(roll_convention))\n\n  def add_period_and_roll(self,\n                          date_tensor,\n                          period_tensor,\n                          roll_convention=constants.BusinessDayConvention.NONE):\n    """"""Adds given periods to given dates and rolls to business days.\n\n    The original dates are not rolled prior to addition.\n\n    Args:\n      date_tensor: `DateTensor` of dates to add to.\n      period_tensor: PeriodTensor broadcastable to `date_tensor`.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting `DateTensor`.\n    """"""\n    return self.roll_to_business_day(date_tensor + period_tensor,\n                                     roll_convention)\n\n  def add_business_days(self,\n                        date_tensor,\n                        num_days,\n                        roll_convention=constants.BusinessDayConvention.NONE):\n    """"""Adds given number of business days to given dates.\n\n    Note that this is different from calling `add_period_and_roll` with\n    PeriodType.DAY. For example, adding 5 business days to Monday gives the next\n    Monday (unless there are holidays on this week or next Monday). Adding 5\n    days and rolling means landing on Saturday and then rolling either to next\n    Monday or to Friday of the same week, depending on the roll convention.\n\n    If any of the dates in `date_tensor` are not business days, they will be\n    rolled to business days before doing the addition. If `roll_convention` is\n    `NONE`, and any dates are not business days, an exception is raised.\n\n    Args:\n      date_tensor: `DateTensor` of dates to advance from.\n      num_days: Tensor of int32 type broadcastable to `date_tensor`.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting `DateTensor`.\n    """"""\n    control_deps = []\n    biz_days, is_bizday = self._to_biz_space(\n        dt.convert_to_date_tensor(date_tensor).ordinal())\n    if roll_convention == constants.BusinessDayConvention.NONE:\n      control_deps.append(\n          tf.debugging.assert_equal(\n              is_bizday,\n              True,\n              message=\'Non business starting day with no roll convention.\'))\n\n    with tf.compat.v1.control_dependencies(control_deps):\n      biz_days_rolled = self._apply_roll_biz_space(date_tensor, biz_days,\n                                                   is_bizday, roll_convention)\n      return dt.from_ordinals(\n          self._from_biz_space(biz_days_rolled + num_days))\n\n  def subtract_period_and_roll(\n      self,\n      date_tensor,\n      period_tensor,\n      roll_convention=constants.BusinessDayConvention.NONE):\n    """"""Subtracts given periods from given dates and rolls to business days.\n\n    The original dates are not rolled prior to subtraction.\n\n    Args:\n      date_tensor: `DateTensor` of dates to subtract from.\n      period_tensor: PeriodTensor broadcastable to `date_tensor`.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting `DateTensor`.\n    """"""\n    minus_period_tensor = periods.PeriodTensor(-period_tensor.quantity(),\n                                               period_tensor.period_type())\n    return self.add_period_and_roll(date_tensor, minus_period_tensor,\n                                    roll_convention)\n\n  def subtract_business_days(\n      self,\n      date_tensor,\n      num_days,\n      roll_convention=constants.BusinessDayConvention.NONE):\n    """"""Adds given number of business days to given dates.\n\n    Note that this is different from calling `subtract_period_and_roll` with\n    PeriodType.DAY. For example, subtracting 5 business days from Friday gives\n    the previous Friday (unless there are holidays on this week or previous\n    Friday). Subtracting 5 days and rolling means landing on Sunday and then\n    rolling either to Monday or to Friday, depending on the roll convention.\n\n    If any of the dates in `date_tensor` are not business days, they will be\n    rolled to business days before doing the subtraction. If `roll_convention`\n    is `NONE`, and any dates are not business days, an exception is raised.\n\n    Args:\n      date_tensor: `DateTensor` of dates to advance from.\n      num_days: Tensor of int32 type broadcastable to `date_tensor`.\n      roll_convention: BusinessDayConvention. Determines how to roll a date that\n        falls on a holiday.\n\n    Returns:\n      The resulting `DateTensor`.\n    """"""\n    return self.add_business_days(date_tensor, -num_days, roll_convention)\n\n  def business_days_in_period(self, date_tensor, period_tensor):\n    """"""Calculates number of business days in a period.\n\n    Includes the dates in `date_tensor`, but excludes final dates resulting from\n    addition of `period_tensor`.\n\n    Args:\n      date_tensor: `DateTensor` of starting dates.\n      period_tensor: PeriodTensor, should be broadcastable to `date_tensor`.\n\n    Returns:\n       An int32 Tensor with the number of business days in given periods that\n       start at given dates.\n\n    """"""\n    return self.business_days_between(date_tensor, date_tensor + period_tensor)\n\n  def business_days_between(self, from_dates, to_dates):\n    """"""Calculates number of business between pairs of dates.\n\n    For each pair, the initial date is included in the difference, and the final\n    date is excluded. If the final date is the same or earlier than the initial\n    date, zero is returned.\n\n    Args:\n      from_dates: `DateTensor` of initial dates.\n      to_dates: `DateTensor` of final dates, should be broadcastable to\n        `from_dates`.\n\n    Returns:\n       An int32 Tensor with the number of business days between the\n       corresponding pairs of dates.\n    """"""\n    from_biz, from_is_bizday = self._to_biz_space(\n        dt.convert_to_date_tensor(from_dates).ordinal())\n    to_biz, to_is_bizday = self._to_biz_space(\n        dt.convert_to_date_tensor(to_dates).ordinal())\n    from_biz = tf.where(from_is_bizday, from_biz, from_biz + 1)\n    to_biz = tf.where(to_is_bizday, to_biz, to_biz + 1)\n    return tf.math.maximum(to_biz - from_biz, 0)\n\n\ndef _get_month(ordinals):\n  return du.ordinal_to_year_month_day(ordinals)[1]\n'"
tf_quant_finance/experimental/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Experimental modules.""""""\n\nfrom tf_quant_finance.experimental import instruments\nfrom tf_quant_finance.experimental import lsm_algorithm\nimport tf_quant_finance.experimental.io\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n\n_allowed_symbols = [\n    \'instruments\',\n    \'io\',\n    \'lsm_algorithm\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/experimental/io.py,13,"b'# Lint as: python3\n""""""Utilities to serialize and deserialize dictionaries of numpy arrays.\n\nThis module defines a data reader and writer to export collections of numpy\narrays to files. It is based on `TFRecords` format. The main difference is\nthat instead of directly storing elements in `FloatList` (or `IntList` etc)\nprotos, it first serializes them to bytes and then stores them as a single\nelement `BytesList`. This is necessary to improve deserialization performance\nwhen we have large arrays.\n""""""\n\nfrom typing import Dict, Callable, Optional\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\n\n__all__ = [\n    \'encode_array\',\n    \'decode_array\',\n    \'ArrayDictReader\',\n    \'ArrayDictWriter\'\n]\n\n\n# Needed for decoding serialized arrays.\n_CLS = type(tf.make_tensor_proto([0]))\n\nArrayEncoderFn = Callable[[np.ndarray], bytes]\nArrayDecoderFn = Callable[[bytes], np.ndarray]\n\n\ndef encode_array(x: np.ndarray) -> bytes:\n  """"""Encodes a numpy array using `TensorProto` protocol buffer.""""""\n  return tf.make_tensor_proto(x).SerializeToString()\n\n\ndef decode_array(bytestring: bytes) -> np.ndarray:\n  """"""Decodes a bytestring into a numpy array.\n\n  The bytestring should be a serialized `TensorProto` instance. For more details\n  see `tf.make_tensor_proto`.\n\n  Args:\n    bytestring: The serialized `TensorProto`.\n\n  Returns:\n    A numpy array.\n  """"""\n  return tf.make_ndarray(_CLS.FromString(bytestring))\n\n\nclass ArrayDictWriter:\n  """"""Writer to write dictionaries of numpy arrays in binary format.\n\n  Writes dictionaries with string keys and numpy array values as records into\n  a [tfrecords](https://www.tensorflow.org/tutorials/load_data/tfrecord) file.\n  The usage of tfrecords should be treated as an implementation detail. To\n  read the file, use the `ArrayDictReader` class.\n\n  Notes and Limitations:\n\n  1. Any values which are not numpy arrays will be first converted to\n    an array before serializing.\n  2. Serializing strings or arrays of strings is complicated because numpy\n    doesn\'t support variable length strings. By default, a python string\n    converted to a numpy array will be converted to a fixed size dtype (of the\n    form \'Un\' where n is the size of the largest string). During serialization\n    this information is lost and the deserialization produces an object array\n    with bytes elements. These need to be manually converted back\n    to a unicode string using `object.astype(\'U?\') where ? is the length of the\n    largest string in the array.\n\n  #### Example\n  ```python\n    options_data = {\n        \'instrument_type\': \'EuropeanOption\',\n        \'strikes\': np.array([1.0, 2.0, 3.0], dtype=np.float64),\n        \'is_call\': np.array([True, True, False]),\n        \'expiries\': np.array([0.4, 1.3, 2.3], dtype=np.float64)\n    }\n    barriers_data = {\n        \'instrument_type\': \'BarrierOption\',\n        \'strikes\': np.array([1.0, 2.0, 3.0], dtype=np.float64),\n        \'is_call\': np.array([True, True, False]),\n        \'expiries\': np.array([0.4, 1.3, 2.3], dtype=np.float64),\n        \'barrier\': np.array([1.4, 2.5, 2.5], dtype=np.float64),\n        \'is_knockout\': np.array([True, True, False])\n    }\n    with ArrayDictWriter(\'datafile.bin\') as writer:\n      writer.write(options_data)\n      writer.write(barriers_data)\n  ```\n  """"""\n\n  def __init__(self, path):\n    self._writer = tf.io.TFRecordWriter(path)\n\n  def __enter__(self) -> \'ArrayDictWriter\':\n    return self\n\n  def __exit__(self, unused_type, unused_value, unused_traceback):\n    """"""Exits a `with` block and closes the file.""""""\n    del unused_type, unused_value, unused_traceback\n    self.close()\n\n  def write(self, array_dict: Dict[str, np.ndarray]) -> None:\n    """"""Writes a dictionary of arrays to the file.\n\n    Args:\n      array_dict: A record to write. Should be a dictionary with string keys and\n        numpy array values.\n    """"""\n    self._writer.write(_make_example(array_dict))\n\n  def flush(self):\n    """"""Flushes the file.""""""\n    self._writer.flush()\n\n  def close(self):\n    """"""Close the file.""""""\n    self._writer.close()\n\n\nclass ArrayDictReader():\n  """"""Iterator over the data serialized by `ArrayDictWriter`.\n\n  The reader counterpart of the `ArrayDictWriter`. It deserializes the binary\n  `tfrecords` files written by the writer.\n\n  Provides an iterable interface.\n\n  #### Example\n  ```python\n    filename = \'...\'\n    reader = ArrayDictReader(filename)\n    # Read one record.\n    first_record = next(reader)\n    for record in reader:\n      print(record)  # Print the rest of the records.\n  ```\n  """"""\n\n  def __init__(self, path: str):\n    self._iter = tf.data.TFRecordDataset([path]).as_numpy_iterator()\n\n  def __iter__(self):\n    return self\n\n  def __next__(self) -> Dict[str, np.ndarray]:\n    # Pull an element out of the dataset and parse it.\n    raw_data = self._iter.next()\n    example = tf.train.Example()\n    example.ParseFromString(raw_data)\n    feature = example.features.feature\n    output = {\n        key: decode_array(value.bytes_list.value[0])\n        for key, value in feature.items()\n    }\n    return output\n\n  def next(self):\n    """"""Returns the next record if there is one or raises `StopIteration`.""""""\n    return next(self)\n\n\ndef _make_feature(\n    arr: np.ndarray,\n    array_encoder: Optional[ArrayEncoderFn] = None) -> tf.train.Feature:\n  """"""Encodes the array and wraps it into a `tf.train.Feature`.""""""\n  if array_encoder is None:\n    array_encoder = encode_array\n  # This is important to do to ensure that values not actually\n  # wrapped already in a numpy array are encoded and decoded predictably.\n  if not isinstance(arr, np.ndarray):\n    arr = np.array(arr)\n  return tf.train.Feature(\n      bytes_list=tf.train.BytesList(value=[encode_array(arr)]))\n\n\ndef _make_example(d: Dict[str, np.ndarray],\n                  array_encoder: Optional[ArrayEncoderFn] = None) -> bytes:\n  """"""Serializes a dictionary of arrays using an `tf.train.Example` proto.""""""\n  features_dict = {\n      key: _make_feature(value, array_encoder) for key, value in d.items()\n  }\n  return tf.train.Example(features=tf.train.Features(\n      feature=features_dict)).SerializeToString()\n\n'"
tf_quant_finance/experimental/io_test.py,2,"b'# Lint as: python3\n""""""Tests for io module.""""""\n\nfrom os import path\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_quant_finance.experimental import io\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\nclass IoTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""float32"",\n          ""input_array"": np.array([1, 2, 3], dtype=np.float32),\n      }, {\n          ""testcase_name"": ""float64"",\n          ""input_array"": np.array([1, 2, 3], dtype=np.float64),\n      }, {\n          ""testcase_name"": ""int32"",\n          ""input_array"": np.array([1, 2, 3], dtype=np.int32),\n      }, {\n          ""testcase_name"": ""int64"",\n          ""input_array"": np.array([1, 2, 3], dtype=np.int64),\n      }, {\n          ""testcase_name"": ""bool"",\n          ""input_array"": np.array([True, False, True], dtype=np.int32),\n      })\n  def test_array_encoder_decoder(self, input_array):\n    as_bytes = io.encode_array(input_array)\n    recovered = io.decode_array(as_bytes)\n    np.testing.assert_array_equal(input_array, recovered)\n\n  def test_array_encoder_decoder_string(self):\n    input_array = np.array([""centaur"", ""satyr"", ""harpy""])\n    as_bytes = io.encode_array(input_array)\n    recovered = io.decode_array(as_bytes).astype(""U"")\n    np.testing.assert_array_equal(input_array, recovered)\n\n  def test_read_write(self):\n    options_data = {\n        ""instrument_type"": ""EuropeanOption"",\n        ""strikes"": np.array([1.0, 2.0, 3.0], dtype=np.float64),\n        ""is_call"": np.array([True, True, False]),\n        ""expiries"": np.array([0.4, 1.3, 2.3], dtype=np.float64)\n    }\n    barriers_data = {\n        ""instrument_type"": ""BarrierOption"",\n        ""strikes"": np.array([1.0, 2.0, 3.0], dtype=np.float64),\n        ""is_call"": np.array([True, True, False]),\n        ""expiries"": np.array([0.4, 1.3, 2.3], dtype=np.float64),\n        ""barrier"": np.array([1.4, 2.5, 2.5], dtype=np.float64),\n        ""is_knockout"": np.array([True, True, False])\n    }\n    temp_dir = self.create_tempdir()\n    temp_file = path.join(temp_dir.full_path, ""datafile.bin"")\n    with io.ArrayDictWriter(temp_file) as writer:\n      writer.write(options_data)\n      writer.write(barriers_data)\n\n    self.assertTrue(path.exists(temp_file))\n\n    # Check that we can read the file.\n    reader = io.ArrayDictReader(temp_file)\n    first_record = reader.next()\n    self.assertEqual(options_data.keys(), first_record.keys())\n    for key, value in options_data.items():\n      extracted = first_record[key]\n      if key == ""instrument_type"":\n        # Strings need to be explicitly coerced to unicode.\n        extracted = extracted.astype(""U"")\n      np.testing.assert_array_equal(value, extracted)\n\n    remaining = []\n    for record in reader:\n      remaining.append(record)\n\n    self.assertLen(remaining, 1)\n    self.assertEqual(barriers_data.keys(), remaining[0].keys())\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/math/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""TensorFlow Quantitative Finance general math functions.""""""\n\n\nfrom tf_quant_finance.math import integration\nfrom tf_quant_finance.math import interpolation\nfrom tf_quant_finance.math import optimizer\nfrom tf_quant_finance.math import pde\nfrom tf_quant_finance.math import piecewise\nfrom tf_quant_finance.math import random_ops as random\nfrom tf_quant_finance.math import root_search\nfrom tf_quant_finance.math import segment_ops\nfrom tf_quant_finance.math.diff_ops import diff\nfrom tf_quant_finance.math.gradient import fwd_gradient\nfrom tf_quant_finance.math.gradient import gradients\nfrom tf_quant_finance.math.gradient import make_val_and_grad_fn\nfrom tf_quant_finance.math.gradient import value_and_gradient\n\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'fwd_gradient\',\n    \'gradients\',\n    \'integration\',\n    \'interpolation\',\n    \'optimizer\',\n    \'pde\',\n    \'piecewise\',\n    \'random\',\n    \'root_search\',\n    \'diff\',\n    \'segment_ops\',\n    \'value_and_gradient\',\n    \'make_val_and_grad_fn\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/math/custom_loops.py,63,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Custom implementations of loops for improved performance.""""""\n\nimport tensorflow.compat.v2 as tf\n\n\ndef for_loop(body_fn, initial_state, params, num_iterations, name=None):\n  """"""A for loop with a custom batched gradient.\n\n  A for loop with a custom gradient that in certain cases outperforms the\n  tf.while_loop gradient implementation.\n\n  This is not a general replacement for tf.while_loop as imposes a number of\n  restrictions on the inputs:\n  - All tensors in loop state must have the same shape except for the last\n  dimension.\n  - All dimensions except for the last one are treated as batch dimensions, i.e.\n  it is assumed that batch elements don\'t interact with each other inside the\n  loop body.\n  - The last dimensions and the number of parameters must be statically known\n  and be reasonably small (so that the full Jacobian matrix with respect to them\n  can be calculated efficiently, see below).\n  - It requires an explicit list of parameters used in the loop body, with\n  respect to which one wishes to calculate derivatives. This is different from\n  tf.while_loop which automatically deals with tensors captured in the closure.\n  - Parameters must be a sequence of zero-dimensional tensors.\n  - Arbitrary nested structure of state is not supported, the state must be a\n  flat sequence of tensors.\n\n  The issue this implementation addresses is the additional while loops created\n  by the gradient of `tf.while_loop`. To compute the backward gradient\n  (more precisely, the vector-Jacobian product) of a while loop, one needs to\n  run the loop ""backwards"". This implementation avoids creating a second loop by\n  calculating the full (batched) Jacobian matrix in the forward pass. It is\n  efficient when the non-batched part of the shape of the Jacobian is small.\n  This part has size `nd * (nd + p)` where `nd` is the sum of last dimensions of\n  tensors in the state and `p` is the number of parameters.\n\n  This implementation is suitable for e.g. Monte-Carlo sampling, where the state\n  represents a batch of independent paths.\n\n  #### Example:\n\n  ```python\n  x = tf.constant([[3.0, 4.0], [30.0, 40.0]])\n  y = tf.constant([[7.0, 8.0], [70.0, 80.0]])\n  alpha = tf.constant(2.0)\n  beta = tf.constant(1.0)\n\n  with tf.GradientTape(persistent=True) as tape:\n    tape.watch([alpha, beta])\n    def body(i, state):\n      x, y = state\n      return [x * alpha - beta, y * beta + x]\n    x_out, y_out = for_loop(body, [x, y], [alpha, beta], 3)\n\n  grad = tape.gradient(y_out, beta)  # Returns tf.Tensor(783.0)\n  ```\n\n  Args:\n    body_fn: A Callable. Accepts an iteration index as a 0-dimensional int32\n      tensor and state - a tuple of Tensors of same shape as `initial_state`.\n      Should return the output state with the same structure as the input state.\n    initial_state: A sequence of Tensors with common batch shape. All dimensions\n      except the last are treated as batch shape (i.e. not mixed in loop body).\n    params: A list of zero-dimensional Tensors - tensors that `body_fn` uses,\n      and with respect to which the differentiation is going to happen.\n    num_iterations: A rank 0 or rank 1 integer tensor. If the rank is 1, the\n      entries are expected to be unique and ordered and  the output will contain\n      results obtained at each iteration number specified in `num_iterations`,\n      stacked along the first dimension. E.g. if `initial_state` has shapes\n      `(10, 20, 2)` and `(10, 20, 3)`, and `num_iterations = [2, 5, 7, 10]` the\n      output is a list of tensors with shapes `(4, 10, 20, 2)` and\n      `(4, 10, 20, 3)`.\n\n    name: Python str. The name to give to the ops created by this function,\n      \'for_loop\' by default.\n\n  Returns:\n   A list of Tensors of the same shape as `initial_state`, if `num_iterations`\n   is a single integer, or with extra first dimension of size\n   `len(num_iterations)` otherwise.\n   The outputs are differentiable with respect to `initial_state` and `params`,\n   but not any other tensors that are captured by `body_fn`. Differentiating\n   with respect to an element of `initial_state` yields a tensor with the same\n   shape as that element. Differentiating with respect to one of `params` yields\n   a tensor of zero shape. If the output state doesn\'t depend on the given\n   parameter, the tensor will be filled with zeros.\n  """"""\n\n  # Implementation explanation.\n  #\n  # Notation:\n  # n - number of state tensors,\n  # d - last dim of state (it can be different among the n state tensors, but\n  # for illustration we\'ll assume it\'s the same),\n  # p - number of parameters.\n  #\n  # The common batch dimensions are omitted below.\n  #\n  # The Jacobian has the form\n  # | Js Jp |\n  # | 0  I  |,\n  # where\n  # Js = d state / d state, shape = (nd, nd),\n  # Js = d state / d params, shape = (nd, p),\n  # 0 - zero matrix (p, nd),\n  # I - unit matrix (p, p).\n  #\n  # Multiplying two Jacobians yields\n  # | Js1 Js2    Js1 Jp2 + Jp1 |\n  # |   0              I       |\n  #\n  # The custom gradient function receives output weights ws with shape = (nd,).\n  # We turn them into row vectors ws\' with shape(1, nd), multiply by Jacobians,\n  # (ws\' Js, ws\' Jp), yielding shapes (1, nd) and (1, p), and finally squeeze\n  # the dimensions to get the desired shapes (nd,), (p,).\n  #\n  # Js and Jp have block structure themselves:\n  #\n  # Js =  | Js_11 ... Js_1n|\n  #       | ...   ... ...  |\n  #       | Js_n1 ... Js_nn|\n  #\n  # Js =  | Jp_11 ... Jp_1p|\n  #       | ...   ... ...  |\n  #       | Jp_n1 ... Jp_np|\n  #\n  # where Js_ij have shape (d, d), Jp_ij have shape (d, 1).\n  #\n  # Js_ij and Jp_ij are Tensors, and the rest are nested lists. We multiply and\n  # add the tensors with TF, and the nested lists - manually with Python loops.\n  #\n  # Note that we can\'t concatenate the parameters into a single Tensor to avoid\n  # some Python loops, even though it\'s cheap. There will be no path from\n  # tf.concat node to the body_fn output, because the user uses the original\n  # (not concatenated) parameters in body_fn.\n\n  num_iterations = tf.convert_to_tensor(num_iterations, dtype=tf.int32,\n                                        name=""num_iterations"")\n  num_iterations_shape = num_iterations.shape.as_list()\n  if num_iterations_shape is None:\n    raise ValueError(""Rank of num_iterations must be statically known."")\n  if len(num_iterations_shape) > 1:\n    raise ValueError(""Rank of num_iterations must be 0 or 1"")\n  if len(num_iterations_shape) == 1:\n    return _accumulating_for_loop(body_fn, initial_state, params,\n                                  num_iterations, name)\n\n  with tf.name_scope(name or ""for_loop""):\n    initial_jac = _make_unit_jacobian(initial_state, params)\n    n = len(initial_state)\n\n    @tf.custom_gradient\n    def inner(*args):\n      initial_state, params = args[:n], args[n:]\n      def while_cond(i, state, jac):\n        del state, jac\n        return i < num_iterations\n\n      def while_body(i, state, jac):\n        with tf.GradientTape(persistent=True) as tape:\n          tape.watch(state)\n          tape.watch(params)\n          next_state = tuple(body_fn(i, state))\n        step_jac = _compute_step_jacobian(state, next_state, params, tape)\n        next_jac = _multiply_jacobians(step_jac, jac)\n        return i + 1, next_state, next_jac\n\n      loop_vars = (0, initial_state, initial_jac)\n\n      _, state, jac = tf.compat.v2.while_loop(\n          while_cond, while_body, loop_vars=loop_vars,\n          maximum_iterations=num_iterations)\n\n      def gradient(*ws):\n        # tf.custom_gradient converts any structure of function outputs into a\n        # flat tuple when calling the custom gradient.\n\n        # Expand into (..., 1, d), so that we can matmul it with Jacobian.\n        ws = [tf.expand_dims(w, axis=-2) for w in ws]\n        ws = [ws]  # expand dims on block level as well.\n\n        js, jp = jac\n        ws_js, ws_jp = _block_matmul(ws, js), _block_matmul(ws, jp)\n\n        # Now undo the expansions\n        ws_js, ws_jp = ws_js[0], ws_jp[0]\n        ws_js = [tf.squeeze(t, axis=-2) for t in ws_js]\n        # These should be 0-dimensional\n        ws_jp = [tf.reduce_sum(t) for t in ws_jp]\n\n        # Flatten into a single tuple, so that it has the same structure as args\n        # in inner().\n        return ws_js + ws_jp\n\n      return state, gradient\n\n    # tf.custom_gradient can only handle a flat sequence of args.\n    args = tuple(initial_state + params)\n    return inner(*args)\n\n\ndef _make_unit_jacobian(initial_state, params):\n  """"""Creates a unit Jacobian matrix.""""""\n  n = len(initial_state)\n  d = [initial_state[i].shape.as_list()[-1] for i in range(n)]\n  if None in d:\n    raise ValueError(""Last dimensions of initial_state Tensors must be known."")\n  p = len(params)\n  dtype = initial_state[0].dtype\n\n  def make_js_block(i, j):\n    shape = initial_state[i].shape.concatenate((d[j],))\n    if i != j:\n      return tf.zeros(shape, dtype=dtype)\n    eye = tf.eye(d[i], dtype=dtype)\n    return tf.broadcast_to(eye, shape)\n\n  def make_jp_block(i, j):\n    del j\n    shape = initial_state[i].shape.concatenate((1,))\n    return tf.zeros(shape, dtype=dtype)\n\n  js = [[make_js_block(i, j) for j in range(n)] for i in range(n)]\n  jp = [[make_jp_block(i, j) for j in range(p)] for i in range(n)]\n  return js, jp\n\n\ndef _compute_step_jacobian(state, next_state, params, tape):\n  """"""Computes a Jacobian of a transformation next_state = f(state, params).""""""\n  n = len(state)\n  p = len(params)\n  js = [[_batch_jacobian(next_state[i], state[j], tape)\n         for j in range(n)]\n        for i in range(n)]\n  jp = [[_jacobian_wrt_parameter(next_state[i], params[j], tape)\n         for j in range(p)]\n        for i in range(n)]\n  return js, jp\n\n\ndef _batch_jacobian(y, x, tape):\n  """"""Computes a Jacobian w.r.t. last dimensions of y and x.""""""\n  # y and x must have the same batch dimensions.\n  # For input shapes (b, dy), (b, dx) yields shape (b, dy, dx).\n  d = y.shape.as_list()[-1]\n  if d is None:\n    raise ValueError(""Last dimension of state Tensors must be known."")\n  grads = []\n  for i in range(d):\n    w = tf.broadcast_to(tf.one_hot(i, d, dtype=y.dtype), y.shape)\n    # We must use tf.UnconnectedGradients.ZERO here and below, because some\n    # state components may legitimately not depend on each other or some of the\n    # params.\n    grad = tape.gradient(y, x, output_gradients=w,\n                         unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    grads.append(grad)\n  return tf.stack(grads, axis=-2)\n\n\ndef _jacobian_wrt_parameter(y, param, tape):\n  """"""Computes a Jacobian w.r.t. a parameter.""""""\n  # For input shapes (b, dy), yields shape (b, dy, 1) (1 is added for\n  # convenience elsewhere).\n  # To avoid having to broadcast param to y\'s shape, we need to take a forward\n  # gradient.\n  with tf.GradientTape() as w_tape:\n    w = tf.zeros_like(y)\n    w_tape.watch(w)\n    vjp = tape.gradient(y, param, output_gradients=w)\n  if vjp is None:  # Unconnected.\n    return tf.expand_dims(tf.zeros_like(y), axis=-1)\n  return tf.expand_dims(w_tape.gradient(vjp, w), axis=-1)\n\n\ndef _multiply_jacobians(jac1, jac2):\n  """"""Multiplies two Jacobians.""""""\n  js1, jp1 = jac1\n  js2, jp2 = jac2\n  return _block_matmul(js1, js2), _block_add(_block_matmul(js1, jp2), jp1)\n\n\ndef _block_matmul(m1, m2):\n  """"""Multiplies block matrices represented as nested lists.""""""\n  # Calls itself recursively to multiply blocks, until reaches the level of\n  # tf.Tensors.\n  if isinstance(m1, tf.Tensor):\n    assert isinstance(m2, tf.Tensor)\n    return tf.matmul(m1, m2)\n  assert _is_nested_list(m1) and _is_nested_list(m2)\n\n  i_max = len(m1)\n  k_max = len(m2)\n  j_max = 0 if k_max == 0 else len(m2[0])\n  if i_max > 0:\n    assert len(m1[0]) == k_max\n\n  def row_by_column(i, j):\n    return _block_add(*[_block_matmul(m1[i][k], m2[k][j])\n                        for k in range(k_max)])\n  return [[row_by_column(i, j) for j in range(j_max)] for i in range(i_max)]\n\n\ndef _block_add(*ms):\n  """"""Adds block matrices represented as nested lists.""""""\n  # Calls itself recursively to add blocks, until reaches the level of\n  # tf.Tensors.\n  if len(ms) == 1:\n    return ms[0]\n  if isinstance(ms[0], tf.Tensor):\n    assert all(isinstance(m, tf.Tensor) for m in ms[1:])\n    return tf.math.add_n(ms)\n  assert all(_is_nested_list(m) for m in ms)\n  for i in range(1, len(ms)):\n    tf.nest.assert_same_structure(ms[0], ms[i])\n\n  i_max = len(ms[0])\n  j_max = 0 if i_max == 0 else len(ms[0][0])\n  return [[_block_add(*[ms[k][i][j] for k in range(len(ms))])\n           for j in range(j_max)]\n          for i in range(i_max)]\n\n\ndef _is_nested_list(m):\n  return isinstance(m, list) and (not m or isinstance(m[0], list))\n\n\ndef _accumulating_for_loop(body_fn, initial_state, params, num_iterations,\n                           name=None):\n  """"""Version of for_loop with multiple values of num_iterations.""""""\n  # Every tensor in nested tensors (state and Jacobian) gets an extra\n  # ""accumulating"" dimension in front. Functions _create_accumulators etc. below\n  # help to work with this dimension.\n\n  with tf.name_scope(name or ""accumulating_for_loop""):\n    max_iterations = tf.math.reduce_max(num_iterations)\n    acc_size = num_iterations.shape[0]\n\n    # num_iteration = [2, 5] -> mask = [0, 0, 1, 0, 0, 1]. Tells when we should\n    # increment acc index before writing. Last element won\'t be used (i = 0..4).\n    mask = tf.scatter_nd(indices=tf.expand_dims(num_iterations, axis=-1),\n                         updates=tf.ones_like(num_iterations),\n                         shape=(max_iterations + 1,))\n\n    n = len(initial_state)\n\n    @tf.custom_gradient\n    def inner(*args):\n      initial_state, params = args[:n], args[n:]\n      def while_cond(i, acc_index, acc_state, acc_jac):\n        del acc_index, acc_state, acc_jac\n        return i < max_iterations\n\n      def while_body(i, acc_index, acc_state, acc_jac):\n        state = _read_from_accumulators(acc_state, acc_index)\n        jac = _read_from_accumulators(acc_jac, acc_index)\n        with tf.GradientTape(persistent=True) as tape:\n          tape.watch(state)\n          tape.watch(params)\n          next_state = tuple(body_fn(i, state))\n        step_jac = _compute_step_jacobian(state, next_state, params, tape)\n        next_jac = _multiply_jacobians(step_jac, jac)\n        acc_index += mask[i]\n        acc_state = _write_to_accumulators(acc_state, next_state, acc_index)\n        acc_jac = _write_to_accumulators(acc_jac, next_jac, acc_index)\n\n        return i + 1, acc_index, acc_state, acc_jac\n\n      initial_acc_state = _create_accumulators(initial_state, acc_size)\n      initial_acc_state = _write_to_accumulators(initial_acc_state,\n                                                 initial_state, 0)\n\n      initial_jac = _make_unit_jacobian(initial_state, params)\n      initial_acc_jac = _create_accumulators(initial_jac, acc_size)\n      initial_acc_jac = _write_to_accumulators(initial_acc_jac, initial_jac, 0)\n\n      loop_vars = (0, 0, initial_acc_state, initial_acc_jac)\n\n      _, _, final_acc_state, final_acc_jac = tf.compat.v2.while_loop(\n          while_cond, while_body, loop_vars=loop_vars,\n          maximum_iterations=max_iterations)\n\n      def gradient(*ws):\n        # Same as in for_loop, except we need to sum over the accumulating\n        # dimension. E.g. if x = for_loop(... num_iterations=[2, 5]) and\n        # y = 2*x[0] + 3*x[1], then taking gradient of y will lead to ws having\n        # coeffs 2 and 3 in the acc dimension, and we should sum over it.\n        ws = [tf.expand_dims(w, axis=-2) for w in ws]\n        ws = [ws]  # expand dims on block level as well.\n\n        js, jp = final_acc_jac\n        ws_js, ws_jp = _block_matmul(ws, js), _block_matmul(ws, jp)\n\n        ws_js, ws_jp = ws_js[0], ws_jp[0]\n        ws_js = [tf.squeeze(t, axis=-2) for t in ws_js]\n        ws_jp = [tf.squeeze(t, axis=[-2, -1]) for t in ws_jp]\n\n        # Sum over acc axis.\n        ws_js = [tf.math.reduce_sum(t, axis=0) for t in ws_js]\n        # ws_jp should be 0-dimensional\n        ws_jp = [tf.math.reduce_sum(t) for t in ws_jp]\n\n        return ws_js + ws_jp\n\n      return final_acc_state, gradient\n\n    # tf.custom_gradient can only handle a flat sequence of args.\n    args = tuple(initial_state + params)\n    return inner(*args)\n\n\ndef _create_accumulators(nested_tensor, size):\n  if isinstance(nested_tensor, tf.Tensor):\n    # Single tensor.\n    return tf.zeros(shape=[size] + nested_tensor.shape.as_list(),\n                    dtype=nested_tensor.dtype)\n  return [_create_accumulators(t, size) for t in nested_tensor]\n\n\ndef _write_to_accumulators(nested_acc, nested_tensor, index):\n  if isinstance(nested_tensor, tf.Tensor):\n    assert isinstance(nested_acc, tf.Tensor)\n    acc_size = nested_acc.shape.as_list()[0]\n    one_hot = tf.one_hot(index, depth=acc_size)\n    one_hot = tf.reshape(one_hot, [acc_size] + [1] * len(nested_tensor.shape))\n    return tf.where(one_hot > 0, nested_tensor, nested_acc)\n\n  return [_write_to_accumulators(acc, t, index)\n          for acc, t in zip(nested_acc, nested_tensor)]\n\n\ndef _read_from_accumulators(nested_acc, index):\n  if isinstance(nested_acc, tf.Tensor):\n    return nested_acc[index]\n  return [_read_from_accumulators(acc, index) for acc in nested_acc]\n\n\n# We don\'t currently expose this module as a library API, but may use it\n# internally, e.g. in Monte-Carlo sampling.\n__all__ = []\n'"
tf_quant_finance/math/custom_loops_test.py,46,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for custom_loops.py.""""""\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\nfrom tf_quant_finance.math import custom_loops\n\nfor_loop = custom_loops.for_loop\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass ForLoopWithCustomGradientTest(parameterized.TestCase, tf.test.TestCase):\n\n  def test_simple_grad_wrt_parameter(self):\n    x = tf.constant([3.0])\n    sigma = tf.constant(2.0)\n\n    with tf.GradientTape() as tape:\n      tape.watch(sigma)\n      def body(i, state):\n        del i\n        x = state[0]\n        return [x * sigma]\n      out = for_loop(body, [x], [sigma], 3)[0]\n\n    grad = tape.gradient(out, sigma)\n    self.assertAllEqual(36, grad)\n\n  def test_simple_grad_wrt_initial_state(self):\n    x = tf.constant([3.0])\n    sigma = tf.constant(2.0)\n\n    with tf.GradientTape() as tape:\n      tape.watch(x)\n      def body(i, state):\n        del i\n        x = state[0]\n        return [x * sigma]\n      out = for_loop(body, [x], [sigma], 3)[0]\n\n    grad = tape.gradient(out, x)\n    self.assertAllEqual([8], grad)\n\n  def test_multiple_state_vars(self):\n    x = tf.constant([3.0, 4.0])\n    y = tf.constant([5.0, 6.0])\n    z = tf.constant([7.0, 8.0])\n    alpha = tf.constant(2.0)\n    beta = tf.constant(1.0)\n\n    with tf.GradientTape(persistent=True) as tape:\n      tape.watch([alpha, beta])\n      def body(i, state):\n        x, y, z = state\n        k = tf.cast(i + 1, tf.float32)\n        return [x * alpha - beta, y * k * alpha * beta, z * beta + x]\n      out = for_loop(body, [x, y, z], [alpha, beta], 3)\n\n    with self.subTest(""independent_vars""):\n      grad = tape.gradient(out[1], alpha)\n      self.assertAllEqual(792, grad)\n    with self.subTest(""dependent_vars""):\n      grad = tape.gradient(out[2], beta)\n      self.assertAllEqual(63, grad)\n\n  def test_batching(self):\n    x = tf.constant([[3.0, 4.0], [30.0, 40.0]])\n    y = tf.constant([[5.0, 6.0], [50.0, 60.0]])\n    z = tf.constant([[7.0, 8.0], [70.0, 80.0]])\n    alpha = tf.constant(2.0)\n    beta = tf.constant(1.0)\n\n    with tf.GradientTape(persistent=True) as tape:\n      tape.watch([alpha, beta])\n      def body(i, state):\n        x, y, z = state\n        k = tf.cast(i + 1, tf.float32)\n        return [x * alpha - beta, y * k * alpha * beta, z * beta + x]\n      out = for_loop(body, [x, y, z], [alpha, beta], 3)\n    with self.subTest(""independent_vars""):\n      grad = tape.gradient(out[1], alpha)\n      self.assertAllEqual(8712, grad)\n    with self.subTest(""dependent_vars""):\n      grad = tape.gradient(out[2], beta)\n      self.assertAllEqual(783, grad)\n\n  def test_with_xla(self):\n    @tf.function\n    def fn():\n      x = tf.constant([[3.0, 4.0], [30.0, 40.0]])\n      y = tf.constant([[7.0, 8.0], [70.0, 80.0]])\n      alpha = tf.constant(2.0)\n      beta = tf.constant(1.0)\n      with tf.GradientTape(persistent=True) as tape:\n        tape.watch([alpha, beta])\n        def body(i, state):\n          del i\n          x, y = state\n          return [x * alpha - beta, y * beta + x]\n        out = for_loop(body, [x, y], [alpha, beta], 3)\n      return tape.gradient(out[1], beta)\n\n    grad = self.evaluate(tf.xla.experimental.compile(fn))[0]\n    self.assertAllEqual(783, grad)\n\n  def test_state_independent_of_param(self):\n    x = tf.constant([3.0])\n    sigma = tf.constant(2.0)\n\n    with tf.GradientTape() as tape:\n      tape.watch(sigma)\n      def body(i, state):\n        del i\n        return [state[0] * 2]\n      out = for_loop(body, [x], [sigma], 3)[0]\n\n    grad = tape.gradient(out, sigma)\n    self.assertAllEqual(0, grad)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""1_state_1_param"",\n          ""state_dims"": (1,),\n          ""num_params"": 1,\n          ""times"": 3,\n      },\n      {\n          ""testcase_name"": ""1_state_3_params"",\n          ""state_dims"": (1,),\n          ""num_params"": 3,\n          ""times"": 3,\n      },\n      {\n          ""testcase_name"": ""1_state_0_params"",\n          ""state_dims"": (1,),\n          ""num_params"": 0,\n          ""times"": 3,\n      },\n      {\n          ""testcase_name"": ""3_states_1_param"",\n          ""state_dims"": (1, 1, 1),\n          ""num_params"": 1,\n          ""times"": 3,\n      },\n      {\n          ""testcase_name"": ""3_states_3_param"",\n          ""state_dims"": (1, 1, 1),\n          ""num_params"": 3,\n          ""times"": 3,\n      },\n      {\n          ""testcase_name"": ""states_with_same_dims"",\n          ""state_dims"": (3, 3, 3),\n          ""num_params"": 2,\n          ""times"": 3,\n      },\n      {\n          ""testcase_name"": ""states_with_different_dims"",\n          ""state_dims"": (2, 3, 1),\n          ""num_params"": 3,\n          ""times"": [3],\n      },\n      {\n          ""testcase_name"": ""states_with_different_dims_multiple_times"",\n          ""state_dims"": (2, 3, 1),\n          ""num_params"": 3,\n          ""times"": [2, 3],\n      },\n  )\n  def test_shapes(self, state_dims, num_params, times):\n    # Checks that the loop can handle various shapes and outputs correct shapes.\n    def test_with_batch_shape(batch_shape):\n      initial_state = [tf.ones(shape=batch_shape + (d,)) for d in state_dims]\n      params = [tf.constant(1.0) for _ in range(num_params)]\n      with tf.GradientTape(persistent=True) as tape:\n        tape.watch(initial_state)\n        tape.watch(params)\n        def body(i, state):\n          del i\n          if not params:\n            return state\n          sum_params = tf.add_n(params)\n          state = [s * sum_params for s in state]\n          return state\n        final_state = for_loop(body, initial_state, params, times)\n\n      for s_in in initial_state:\n        for s_out in final_state:\n          grad = tape.gradient(s_out, s_in)\n          self.assertAllEqual(s_in.shape, grad.shape)\n\n      for p in params:\n        for s_out in final_state:\n          grad = tape.gradient(s_out, p)\n          self.assertAllEqual([], grad.shape)\n\n    with self.subTest(""no_batch""):\n      test_with_batch_shape(batch_shape=())\n    with self.subTest(""simple_batch""):\n      test_with_batch_shape(batch_shape=(5,))\n    with self.subTest(""complex_batch""):\n      test_with_batch_shape(batch_shape=(2, 8, 3))\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""params_test"",\n          ""params_test"": True\n      },\n      {\n          ""testcase_name"": ""initial_state_test"",\n          ""params_test"": False\n      },\n  )\n  def test_accumulating_for_loop_grap_param(self, params_test):\n    # Multiple  number of iterations produce correct gradients for params\n    x = np.arange(24)\n    x = np.reshape(x, [4, 3, 2])\n    sigma_np = 2.0\n    initial_state = tf.convert_to_tensor(x, dtype=tf.float64)\n    sigma = tf.convert_to_tensor(sigma_np, dtype=tf.float64)\n\n    def fn(initial_state, sigma):\n      def body(i, state):\n        del i\n        x = state[0]\n        return [x * sigma]\n      return for_loop(body, [initial_state], [sigma], [3, 5])[0]\n\n    # Tests for parameters gradient\n    if params_test:\n      def fwd_grad_fn(sigma):\n        g = lambda sigma: fn(initial_state, sigma)\n        return  tff.math.fwd_gradient(g, sigma, use_gradient_tape=True)\n\n      def value_and_gradient(sigma):\n        g = lambda sigma: fn(initial_state, sigma)\n        return  tff.math.value_and_gradient(g, sigma, use_gradient_tape=True)\n      expected_val = np.stack([sigma_np**3 * x, sigma_np**5 * x], axis=0)\n      expected_fwd_grad = np.stack([3 * sigma_np**2 * x,\n                                    5 * sigma_np**4 * x], axis=0)\n      expected_grad = np.sum(expected_fwd_grad)\n      with self.subTest(""ParamsForwardGradXLA""):\n        fwd_grad = tf.xla.experimental.compile(\n            tf.function(fwd_grad_fn), [sigma])[0]\n        self.assertAllClose(fwd_grad, expected_fwd_grad)\n      with self.subTest(""ParamsValueAndGradXLA""):\n        val, grad = tf.xla.experimental.compile(\n            tf.function(value_and_gradient), [sigma])\n        self.assertAllClose(expected_val, val)\n        self.assertAllClose(grad, expected_grad)\n      with self.subTest(""ParamsForwardGrad""):\n        fwd_grad = fwd_grad_fn(sigma)\n        self.assertAllClose(fwd_grad, expected_fwd_grad)\n      with self.subTest(""ParamsValueAndGrad""):\n        val, grad = value_and_gradient(sigma)\n        self.assertAllClose(expected_val, val)\n        self.assertAllClose(grad, expected_grad)\n    # Tests for initial state gradient\n    if not params_test:\n      def state_fwd_grad_fn(initial_state):\n        g = lambda initial_state: fn(initial_state, sigma)\n        return  tff.math.fwd_gradient(g, initial_state, use_gradient_tape=True)\n\n      def state_value_and_gradient(initial_state):\n        g = lambda initial_state: fn(initial_state, sigma)\n        return  tff.math.value_and_gradient(g, initial_state,\n                                            use_gradient_tape=True)\n      expected_val = np.stack([sigma_np**3 * x, sigma_np**5 * x], axis=0)\n      expected_fwd_grad = np.stack([sigma_np**3 * np.ones_like(x),\n                                    sigma_np**5 * np.ones_like(x)], axis=0)\n      expected_grad = np.sum(expected_fwd_grad, axis=0)\n      with self.subTest(""StateForwardGradXLA""):\n        fwd_grad = tf.xla.experimental.compile(\n            tf.function(state_fwd_grad_fn), [initial_state])[0]\n        self.assertAllClose(fwd_grad, expected_fwd_grad)\n      with self.subTest(""StateValueAndGradXLA""):\n        val, grad = tf.xla.experimental.compile(\n            tf.function(state_value_and_gradient), [initial_state])\n        self.assertAllClose(expected_val, val)\n        self.assertAllClose(grad, expected_grad)\n      with self.subTest(""StateForwardGrad""):\n        fwd_grad = state_fwd_grad_fn(initial_state)\n        self.assertAllClose(fwd_grad, expected_fwd_grad)\n      with self.subTest(""StateValueAndGrad""):\n        val, grad = state_value_and_gradient(initial_state)\n        self.assertAllClose(expected_val, val)\n        self.assertAllClose(grad, expected_grad)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/math/diff_ops.py,5,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Array difference ops.""""""\n\n\nimport tensorflow.compat.v2 as tf\n\n\n# TODO(b/136354274): Move this function to a math library and provide a more\n# efficient C++ kernel.\ndef diff(x, order=1, exclusive=False, dtype=None, name=None):\n  """"""Computes the difference between elements of an array at a regular interval.\n\n  If exclusive is True, then computes\n\n  ```\n    result[i] = x[i+order] - x[i] for i < size(x) - order\n\n  ```\n\n  This is the same as doing `x[order:] - x[:-order]`. Note that in this case\n  the result `Tensor` is smaller in size than the input `Tensor`.\n\n  If exclusive is False, then computes:\n\n  ```\n    result[i] = x[i] - x[i-order] for i >= order\n    result[i] = x[i]  for 0 <= i < order\n\n  ```\n\n  #### Example\n\n  ```python\n    x = tf.constant([1, 2, 3, 4, 5])\n    dx = diff(x, order=1, exclusive=False)  # Returns [1, 1, 1, 1, 1]\n    dx1 = diff(x, order=1, exclusive=True)  # Returns [1, 1, 1, 1]\n    dx2 = diff(x, order=2, exclusive=False)  # Returns [1, 2, 2, 2, 2]\n  ```\n\n  Args:\n    x: A rank 1 `Tensor` of any dtype for which arithmetic operations are\n      permitted.\n    order: Positive Python int. The order of the difference to compute. `order =\n      1` corresponds to the difference between successive elements.\n      Default value: 1\n    exclusive: Python bool. See description above.\n      Default value: False\n    dtype: Optional `tf.Dtype`. If supplied, the dtype for `x` to use when\n      converting to `Tensor`.\n      Default value: None which maps to the default dtype inferred by TF.\n    name: Python `str` name prefixed to Ops created by this class.\n      Default value: None which is mapped to the default name \'diff\'.\n\n  Returns:\n    diffs: A `Tensor` of the same dtype as `x`. If `exclusive` is True,\n      then the size is `n-min(order, size(x))` where `n` is the size of x. If\n      `exclusive` is False, then the size is `n`.\n  """"""\n  with tf.compat.v1.name_scope(name, default_name=\'diff\', values=[x]):\n    x = tf.convert_to_tensor(x, dtype=dtype)\n    exclusive_diff = x[order:] - x[:-order]\n    if exclusive:\n      return exclusive_diff\n\n    return tf.concat([x[:order], exclusive_diff], axis=0)\n'"
tf_quant_finance/math/diff_ops_test.py,6,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for diff.py.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance import math\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\nclass DiffOpsTest(tf.test.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_diffs(self):\n    x = tf.constant([1, 2, 3, 4, 5])\n    dx = self.evaluate(math.diff(x, order=1, exclusive=False))\n    np.testing.assert_array_equal(dx, [1, 1, 1, 1, 1])\n\n    dx1 = self.evaluate(math.diff(x, order=1, exclusive=True))\n    np.testing.assert_array_equal(dx1, [1, 1, 1, 1])\n\n    dx2 = self.evaluate(math.diff(x, order=2, exclusive=False))\n    np.testing.assert_array_equal(dx2, [1, 2, 2, 2, 2])\n\n  @test_util.deprecated_graph_mode_only\n  def test_diffs_differentiable(self):\n    """"""Tests that the diffs op is differentiable.""""""\n    x = tf.constant(2.0)\n    xv = tf.stack([x, x * x, x * x * x], axis=0)\n\n    # Produces [x, x^2 - x, x^3 - x^2]\n    dxv = self.evaluate(math.diff(xv))\n    np.testing.assert_array_equal(dxv, [2., 2., 4.])\n\n    grad = self.evaluate(tf.gradients(math.diff(xv), x)[0])\n    # Note that TF gradients adds up the components of the jacobian.\n    # The sum of [1, 2x-1, 3x^2-2x] at x = 2 is 12.\n    self.assertEqual(grad, 12.0)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/math/gradient.py,42,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Helper functions for computing gradients.""""""\n\n\nimport functools\n\nimport tensorflow.compat.v2 as tf\n\n\ndef fwd_gradient(func_or_y, x, input_gradients=None, use_gradient_tape=False,\n                 unconnected_gradients=None,\n                 name=None):\n  """"""Computes forward mode gradient.\n\n  Implementation based on suggestions in\n  [this thread](https://github.com/tensorflow/tensorflow/issues/19361).\n\n  TensorFlow computes gradients using the reverse mode automatic\n  differentiation which is suitable for typical machine learning situations\n  where one has a scalar loss function that one wants to differentiate with\n  respect to the parameters. In some cases, one needs to be able to compute\n  directional derivatives of non-scalar functions. Suppose F is a function from\n  R^n to R^m and let u be a fixed vector in R^n, w a fixed vector in R^m and\n  x a variable taking values in R^n. Let J(F) denote the jacobian matrix of\n  F of shape [m, n] (i.e. J(F)[i, j] = dF_i / dx_j). Then the default\n  gradients function in TF computes the expression\n  w^T.J(F) (i.e. Sum[w_i dF_i / dx_j, 1 <= i <= m]).\n\n  On the other hand, one also often needs to compute the directional derivative\n  J(F).u (i.e. Sum[u_j dF_i / dx_j, 1 <= j <= n]). Unfortunately, TensorFlow\n  has no native support for accumulating this. Providing first class support\n  for forward mode differentiation requires some significant changes in the core\n  architecture of TF (including writing a directional derivative for each\n  op).\n\n  The following function sidesteps this by using two passes of reverse mode\n  differentiation. Mathematically, the idea is simple. If F: R^n -> R^m, then\n  w^T.J(F) seen as a function of w is a function from R^m to R^n (because\n  w is in R^m, and w^T.J(F) is in R^n). Hence a reverse mode differentiation\n  with respect to w should produce J(F).u.\n\n  This function provides only a small subset of the flexibility of\n  the tf.gradients function. This may be extended in the future.\n\n  #### Example\n\n  Following example demonstrates the usage and the difference between this\n  op and the standard `tf.gradients`\n  ```python\n    t = tf.range(1, 3, dtype=tf.float32)  # Shape [2]\n    def fn(t):\n      return tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2]\n    # Produces shape [3, 2] with values [[1, 1], [2, 4], [3, 12]]\n    fwd_grad_y = fwd_gradient(fn, t)\n    # Produces shape [2] with values [6, 17].\n    bck_grad_y = tf.gradients(y, t)[0]\n  ```\n\n  Args:\n    func_or_y: Either a `Tensor` conencted to the input `x` or a Python callable\n      accepting one `Tensor` of shape of `x` and returning a `Tensor` of any\n      shape. The function whose gradient is to be computed. If eagerly\n      executing, can only be a callable, i.e., one should not supply a Tensor\n      in eager mode.\n    x: A `Tensor` with respect to which the gradient is to be computed.\n    input_gradients: A `Tensor` of the same shape as `x`. The direction along\n      which the directional derivative is to be computed.\n      Default value: `None` which maps to a ones-like `Tensor` of `x`.\n    use_gradient_tape: Optional Python bool. Whether to use gradient tape even\n      when eager mode is not turned on.\n      Defaule value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., \'gradients\').\n\n  Returns:\n    A `Tensor` of the same shape as `func(x)`.\n\n  Raises:\n    ValueError: If `func_or_y` is not a callable and the output is eagerly\n      executed or when the `tf.GradientTape` is used.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  with tf.name_scope(name or ""gradients""):\n    f = _prepare_func(func_or_y)\n    if not tf.executing_eagerly() and not use_gradient_tape:\n      y = f(x)\n      w = tf.ones_like(y)\n      g = tf.gradients(y, x, grad_ys=w,\n                       unconnected_gradients=unconnected_gradients)\n      return tf.gradients(g, w, grad_ys=input_gradients,\n                          unconnected_gradients=unconnected_gradients)[0]\n    if not callable(func_or_y):\n      raise ValueError(""`func_or_y` should be a callable in eager mode or when ""\n                       ""`tf.GradientTape` is used."")\n    with tf.GradientTape() as outer_tape:\n      with tf.GradientTape() as inner_tape:\n        inner_tape.watch(x)\n        y = f(x)\n      w = tf.ones_like(y)\n      outer_tape.watch(w)\n      g = inner_tape.gradient(y, x, output_gradients=w,\n                              unconnected_gradients=unconnected_gradients)\n    return outer_tape.gradient(g, w, output_gradients=input_gradients,\n                               unconnected_gradients=unconnected_gradients)\n\n\ndef gradients(func_or_y, xs, output_gradients=None, use_gradient_tape=False,\n              unconnected_gradients=None,\n              name=None):\n  """"""Computes the gradients of `func_or_y` wrt to `*xs`.\n\n  Args:\n   func_or_y: Either a `Tensor` conencted to the input `x` or a Python callable\n      accepting one `Tensor` of shape of `x` and returning a `Tensor` of any\n      shape. The function whose gradient is to be computed. If eagerly\n      executing, can only be a callable, i.e., one should not supply a Tensor\n      in eager mode.\n    xs: Python list of parameters of `f` for which to differentiate. (Can also\n      be single `Tensor`.)\n    output_gradients: A `Tensor` or list of `Tensor`s the same size as the\n      result `ys = f(*xs)` and holding the gradients computed for each `y` in\n      `ys`. This argument is forwarded to the underlying gradient implementation\n      (i.e., either the `grad_ys` argument of `tf.gradients` or the\n      `output_gradients` argument of `tf.GradientTape.gradient`).\n      Default value: `None` which maps to a ones-like `Tensor` of `ys`.\n    use_gradient_tape: Python `bool` indicating that `tf.GradientTape` should be\n      used regardless of `tf.executing_eagerly()` status.\n      Default value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., \'gradients\').\n\n  Returns:\n    A `Tensor` with the gradient of `y` wrt each of `xs` or a list of `Tensor`s\n    if `xs` is a list.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  f = _prepare_func(func_or_y)\n  with tf.name_scope(name or ""gradients""):\n    xs, is_xs_list_like = _prepare_args(xs)\n    if not tf.executing_eagerly() and not use_gradient_tape:\n      y = f(*xs)\n      grad = tf.gradients(y, xs, grad_ys=output_gradients,\n                          unconnected_gradients=unconnected_gradients)\n    else:\n      if not callable(func_or_y):\n        raise ValueError(""`func_or_y` should be a callable in eager mode or ""\n                         ""when `tf.GradientTape` is used."")\n      with tf.GradientTape() as tape:\n        for x in xs:\n          tape.watch(x)\n        y = f(*xs)\n      grad = tape.gradient(y, xs, output_gradients=output_gradients,\n                           unconnected_gradients=unconnected_gradients)\n    if is_xs_list_like:\n      return grad\n    else:\n      return grad[0]\n\n\ndef value_and_gradient(f,\n                       xs,\n                       output_gradients=None,\n                       use_gradient_tape=False,\n                       unconnected_gradients=None,\n                       name=None):\n  """"""Computes `f(*xs)` and its gradients wrt to `*xs`.\n\n  Args:\n    f: Python `callable` to be differentiated. If `f` returns a scalar, this\n      scalar will be differentiated. If `f` returns a tensor or list of tensors,\n      by default a scalar will be computed by adding all their values to produce\n      a single scalar. If desired, the tensors can be elementwise multiplied by\n      the tensors passed as the `dy` keyword argument to the returned gradient\n      function.\n    xs: Python list of parameters of `f` for which to differentiate. (Can also\n      be single `Tensor`.)\n    output_gradients: A `Tensor` or list of `Tensor`s the same size as the\n      result `ys = f(*xs)` and holding the gradients computed for each `y` in\n      `ys`. This argument is forwarded to the underlying gradient implementation\n      (i.e., either the `grad_ys` argument of `tf.gradients` or the\n      `output_gradients` argument of `tf.GradientTape.gradient`).\n    use_gradient_tape: Python `bool` indicating that `tf.GradientTape` should be\n      used regardless of `tf.executing_eagerly()` status.\n      Default value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., `\'value_and_gradient\'`).\n\n  Returns:\n    A tuple of two elements. The first one is a `Tensor` representing the value\n    of the function at `xs` and the second one is either a `Tensot` or a list of\n    `Tensor`s representing grafient of `f(*xs)` wrt `xs`.\n    y: `y = f(*xs)`.\n    dydx: Gradient of `y` wrt each of `xs`.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  xs, is_xs_list_like = _prepare_args(xs)\n  with tf.name_scope(name or ""value_and_gradient""):\n    if tf.executing_eagerly() or use_gradient_tape:\n      with tf.GradientTape() as tape:\n        for x in xs:\n          tape.watch(x)\n        y = f(*xs)\n      grad = tape.gradient(y, xs, output_gradients=output_gradients,\n                           unconnected_gradients=unconnected_gradients)\n    else:\n      y = f(*xs)\n      grad = tf.gradients(ys=y, xs=xs, grad_ys=output_gradients,\n                          unconnected_gradients=unconnected_gradients)\n    if is_xs_list_like:\n      return y, grad\n    else:\n      return y, grad[0]\n\n\ndef make_val_and_grad_fn(value_fn):\n  """"""Function decorator to compute both function value and gradient.\n\n  For example:\n\n  ```\n  @tff.math.make_val_and_grad_fn\n  def quadratic(x):\n    return tf.reduce_sum(scales * (x - minimum) ** 2, axis=-1)\n  ```\n\n  Turns `quadratic` into a function that accepts a point as a `Tensor` as input\n  and returns a tuple of two `Tensor`s with the value and the gradient of the\n  defined quadratic function evaluated at the input point.\n\n  This is useful for constucting functions to optimize with tff.math.optimizer\n  methods.\n\n  Args:\n    value_fn: A python function to decorate.\n\n  Returns:\n    The decorated function.\n  """"""\n  @functools.wraps(value_fn)\n  def val_and_grad(x):\n    return value_and_gradient(value_fn, x)\n\n  return val_and_grad\n\n\ndef _prepare_func(func_or_y):\n  """"""Creates a function out of the input callable or `Tensor`.""""""\n  if callable(func_or_y):\n    return func_or_y\n  else:\n    return lambda *args: func_or_y\n\n\ndef _prepare_args(xs):\n  """"""Converst `xs` to a list if necessary.""""""\n  if isinstance(xs, (list, tuple)):\n    return xs, True\n  else:\n    return [xs], False\n'"
tf_quant_finance/math/gradient_test.py,22,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for math.gradient.py.""""""\n\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass GradientTest(tf.test.TestCase):\n\n  def test_forward_gradient(self):\n    t = tf.range(1, 3, dtype=tf.float32)  # Shape [2]\n    func = lambda t: tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2]\n    with self.subTest(""EagerExecution""):\n      fwd_grad = self.evaluate(tff.math.fwd_gradient(func, t))\n      self.assertEqual(fwd_grad.shape, (3, 2))\n      np.testing.assert_allclose(fwd_grad, [[1., 1.], [2., 4.], [3., 12.]])\n    with self.subTest(""GraphExecution""):\n      @tf.function\n      def grad_computation():\n        y = func(t)\n        return tff.math.fwd_gradient(y, t)\n      fwd_grad = self.evaluate(grad_computation())\n      self.assertEqual(fwd_grad.shape, (3, 2))\n      np.testing.assert_allclose(fwd_grad, [[1., 1.], [2., 4.], [3., 12.]])\n\n  def test_forward_unconnected_gradient(self):\n    t = tf.range(1, 3, dtype=tf.float32)  # Shape [2]\n    zeros = tf.zeros([2], dtype=t.dtype)\n    func = lambda t: tf.stack([zeros, zeros, zeros], axis=0)  # Shape [3, 2]\n    expected_result = [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]\n    with self.subTest(""EagerExecution""):\n      fwd_grad = self.evaluate(tff.math.fwd_gradient(\n          func, t, unconnected_gradients=tf.UnconnectedGradients.ZERO))\n      self.assertEqual(fwd_grad.shape, (3, 2))\n      np.testing.assert_allclose(fwd_grad, expected_result)\n    with self.subTest(""GraphExecution""):\n      @tf.function\n      def grad_computation():\n        y = func(t)\n        return tff.math.fwd_gradient(\n            y, t, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n      fwd_grad = self.evaluate(grad_computation())\n      self.assertEqual(fwd_grad.shape, (3, 2))\n      np.testing.assert_allclose(fwd_grad, expected_result)\n\n  def test_backward_gradient(self):\n    t = tf.range(1, 3, dtype=tf.float32)  # Shape [2]\n    func = lambda t: tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2]\n    with self.subTest(""EagerExecution""):\n      backward_grad = self.evaluate(tff.math.gradients(func, t))\n      self.assertEqual(backward_grad.shape, (2,))\n      np.testing.assert_allclose(backward_grad, [6., 17.])\n    with self.subTest(""GraphExecution""):\n      @tf.function\n      def grad_computation():\n        y = func(t)\n        return tff.math.gradients(y, t)\n      backward_grad = self.evaluate(grad_computation())\n      self.assertEqual(backward_grad.shape, (2,))\n      np.testing.assert_allclose(backward_grad, [6., 17.])\n\n  def test_backward_unconnected_gradient(self):\n    t = tf.range(1, 3, dtype=tf.float32)  # Shape [2]\n    zeros = tf.zeros([2], dtype=t.dtype)\n    expected_result = [0.0, 0.0]\n    func = lambda t: tf.stack([zeros, zeros, zeros], axis=0)  # Shape [3, 2]\n    with self.subTest(""EagerExecution""):\n      backward_grad = self.evaluate(tff.math.gradients(\n          func, t, unconnected_gradients=tf.UnconnectedGradients.ZERO))\n      self.assertEqual(backward_grad.shape, (2,))\n      np.testing.assert_allclose(backward_grad, expected_result)\n    with self.subTest(""GraphExecution""):\n      @tf.function\n      def grad_computation():\n        y = func(t)\n        return tff.math.gradients(\n            y, t, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n      backward_grad = self.evaluate(grad_computation())\n      self.assertEqual(backward_grad.shape, (2,))\n      np.testing.assert_allclose(backward_grad, expected_result)\n\n  def test_make_val_and_grad_fn(self):\n    minimum = np.array([1.0, 1.0])\n    scales = np.array([2.0, 3.0])\n\n    @tff.math.make_val_and_grad_fn\n    def quadratic(x):\n      return tf.reduce_sum(input_tensor=scales * (x - minimum)**2)\n\n    point = tf.constant([2.0, 2.0], dtype=tf.float64)\n    val, grad = self.evaluate(quadratic(point))\n    self.assertNear(val, 5.0, 1e-5)\n    self.assertArrayNear(grad, [4.0, 6.0], 1e-5)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/math/piecewise.py,61,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Piecewise utility functions.""""""\n\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\n\nclass PiecewiseConstantFunc(object):\n  """"""Creates a piecewise constant function.""""""\n\n  def __init__(self, jump_locations, values, dtype=None, name=None):\n    r""""""Initializes jumps of the piecewise constant function.\n\n    Sets jump locations and values for a piecewise constant function.\n    `jump_locations` split real line into intervals\n    `[-inf, jump_locations[..., 0]], ..,\n    [jump_locations[..., i], jump_locations[..., i + 1]], ...,\n    [jump_locations[..., -1], inf]`\n    so that the piecewise constant function takes corresponding `values` on the\n    intervals, i.e.,\n    ```\n    f(x) = \\sum_i values[..., i]\n           * I_{x \\in [jump_locations[..., i -1], jump_locations[..., i])}\n    ```\n\n    #### Example. Simple scalar-valued piecewise constant function.\n    ```python\n    dtype = np.float64\n    jump_locations = [0.1, 10]\n    values = [3, 4, 5]\n    piecewise_func = piecewise.PiecewiseConstantFunc(jump_locations, values,\n                                                     dtype=dtype)\n    # Locations at which to evaluate the function assuming it is\n    # left-continuous.\n    x = np.array([0., 0.1, 2., 11.])\n    value = piecewise_func(x)\n    # Expected values: [3, 3, 4, 5]\n    integral = piecewise_func.integrate(x, x + 1)\n    # Expected integrals: [3.9, 4, 4, 5]\n    ```\n\n    #### Example. Matrix-valued piecewise constant function.\n    ```python\n    dtype = np.float64\n    jump_locations = [0.1, 10]\n    # The function takes values [[1, 2], [3, 4]] on (-inf, 0.1),\n    # [[5, 6], [7, 8]] on (0.1, 0.5), and [[9, 10], [11, 12]] on (0.5, +inf).\n    values = [[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]]\n    piecewise_func = piecewise.PiecewiseConstantFunc(\n        jump_locations, values, dtype=dtype)\n    # Locations at which to evaluate the function assuming it is\n    # left-continuous.\n    x = np.array([0., 0.1, 2, 11])\n    value = piecewise_func(x)\n    # Expected values: [[[1, 2], [3, 4]], [[1, 2], [3, 4]],\n    #                   [[5, 6], [7, 8]], [[9, 10], [11, 12]]]\n    integral = piecewise_func.integrate(x, x + 1)\n    # Expected integrals: [[[4.6, 5.6], [6.6, 7.6]],\n    #                      [[5, 6], [7, 8]],\n    #                      [[5, 6], [7, 8]],\n    #                      [[9, 10], [11, 12]]]\n    ```\n\n    Args:\n      jump_locations: A real `Tensor` of shape\n        `batch_shape + [num_jump_points]`. The locations where the function\n        changes its values. Note that the values are expected to be ordered\n        along the last dimension. Repeated values are allowed but it is\n        up to the user to ensure that the corresponding `values` are also\n        repeated.\n      values: A `Tensor` of the same `dtype` as `jump_locations` and shape\n        `batch_shape + [num_jump_points + 1] + event_shape`. Defines\n        `values[batch_rank * slice(None), i]` on intervals\n        `(jump_locations[..., i - 1], jump_locations[..., i])`. Here\n        `event_shape` allows for array-valued piecewise constant functions\n        and `batch_rank = len(batch_shape)`.\n      dtype:  Optional dtype for `jump_locations` and `values`.\n        Default value: `None` which maps to the default dtype inferred by\n        TensorFlow.\n      name: Python `str` name prefixed to ops created by this class.\n        Default value: `None` which is mapped to the default name\n        `PiecewiseConstantFunc`.\n\n    Raises:\n      ValueError:\n        If `jump_locations` and `values` have different batch shapes or,\n        in case of static shapes, if the event shape of `values` is different\n        from `num_jump_points + 1`.\n    """"""\n    self._name = name or \'PiecewiseConstantFunc\'\n    # Add a property that indicates that the class instance is a\n    # piecewise constant function\n    self.is_piecewise_constant = True\n    with tf.compat.v1.name_scope(self._name, values=[jump_locations, values]):\n      self._jump_locations = tf.convert_to_tensor(jump_locations, dtype=dtype,\n                                                  name=\'jump_locations\')\n      self._values = tf.convert_to_tensor(values, dtype=dtype,\n                                          name=\'values\')\n      shape_values = self._values.shape.as_list()\n      shape_jump_locations = self._jump_locations.shape.as_list()\n      batch_rank = len(shape_jump_locations[:-1])\n      self._batch_rank = batch_rank\n      if shape_values[:batch_rank] != shape_jump_locations[:-1]:\n        raise ValueError(\'Batch shapes of `values` and `jump_locations` should \'\n                         \'be the same but are {0} and {1}\'.format(\n                             shape_values[:-1], shape_jump_locations[:-1]))\n      if shape_values[batch_rank] - 1 != shape_jump_locations[-1]:\n        raise ValueError(\'Event shape of `values` should have one more \'\n                         \'element than the event shape of `jump_locations` \'\n                         \'but are {0} and {1}\'.format(\n                             shape_values[-1], shape_jump_locations[-1]))\n\n  def values(self):\n    """"""The value of the piecewise constant function between jump locations.""""""\n    return self._values\n\n  def jump_locations(self):\n    """"""The jump locations of the piecewise constant function.""""""\n    return self._jump_locations\n\n  def name(self):\n    """"""The name to give to the ops created by this class.""""""\n    return self._name\n\n  def __call__(self, x, left_continuous=True, name=None):\n    """"""Computes value of the piecewise constant function.\n\n    Returns a value of the piecewise function with jump locations and values\n    given by the initializer.\n\n    Args:\n      x: A real `Tensor` of shape `batch_shape + [num_points]`. Points at which\n        the function has to be evaluated.\n      left_continuous: Python `bool`. Whether the function is left- or right-\n        continuous, i.e., at the `jump_locations[..., i]` left-continuity means\n        that the function has the same value\n        `values[batch_rank * slice(None), i]`, wheraes for\n        right-continuity, the value is\n        `values[batch_rank * slice(None), i + 1]`.\n        Default value: `True` which means that the function is left-continuous.\n      name: Python `str` name prefixed to ops created by this method.\n        Default value: `None` which is mapped to the default name\n        `self.name() + _call`.\n\n    Returns:\n      A `Tensor` of the same `dtype` as `x` and shape\n      `batch_shape + [num_points] + event_shape` containing values of the\n      piecewise constant function.\n\n    Raises:\n      ValueError:\n        If `batch_shape` of `x` is incompatible with the batch shape of\n        `self.jump_locations()`.\n    """"""\n    with tf.compat.v1.name_scope(name, self._name + \'_call\', [x]):\n      x = tf.convert_to_tensor(x, dtype=self._jump_locations.dtype, name=\'x\')\n      batch_shape = self._jump_locations.shape.as_list()[:-1]\n      x = _try_broadcast_to(x, batch_shape, name=\'x\')\n      side = \'left\' if left_continuous else \'right\'\n      return _piecewise_constant_function(\n          x, self._jump_locations, self._values, self._batch_rank, side=side)\n\n  def integrate(self, x1, x2, name=None):\n    """"""Integrates the piecewise constant function between end points.\n\n    Returns a value of the integral on the interval `[x1, x2]` of a piecewise\n    constant function with jump locations and values given by the initializer.\n\n    Args:\n      x1: A real `Tensor` of shape `batch_shape + [num_points]`. Left end points\n        at which the function has to be integrated.\n      x2: A `Tensor` of the same shape and `dtype` as `x1`. Right end points at\n        which the function has to be integrated.\n      name: Python `str` name prefixed to ops created by this method.\n        Default value: `None` which is mapped to the default name\n        `self.name() + `_integrate``.\n\n    Returns:\n      A `Tensor` of the same `dtype` as `x` and shape\n      `batch_shape + [num_points] + event_shape` containing values of the\n      integral of the piecewise constant function between `[x1, x2]`.\n\n    Raises:\n      ValueError:\n        If `batch_shape` of `x1` and `x2` are incompatible with the batch shape\n        of `self.jump_locations()`.\n    """"""\n    with tf.compat.v1.name_scope(name, self._name + \'_integrate\', [x1, x2]):\n      x1 = tf.convert_to_tensor(x1, dtype=self._jump_locations.dtype,\n                                name=\'x1\')\n      x2 = tf.convert_to_tensor(x2, dtype=self._jump_locations.dtype,\n                                name=\'x2\')\n      batch_shape = self._jump_locations.shape.as_list()[:-1]\n      x1 = _try_broadcast_to(x1, batch_shape, name=\'x1\')\n      x2 = _try_broadcast_to(x2, batch_shape, name=\'x1\')\n      return _piecewise_constant_integrate(\n          x1, x2, self._jump_locations, self._values, self._batch_rank)\n\n\ndef find_interval_index(query_xs,\n                        interval_lower_xs,\n                        last_interval_is_closed=False,\n                        dtype=None,\n                        name=None):\n  """"""Function to find the index of the interval where query points lies.\n\n  Given a list of adjacent half-open intervals [x_0, x_1), [x_1, x_2), ...,\n  [x_{n-1}, x_n), [x_n, inf), described by a list [x_0, x_1, ..., x_{n-1}, x_n].\n  Return the index where the input query points lie. If x >= x_n, n is returned,\n  and if x < x_0, -1 is returned. If `last_interval_is_closed` is set to `True`,\n  the last interval [x_{n-1}, x_n] is interpreted as closed (including x_n).\n\n  #### Example\n\n  ```python\n  interval_lower_xs = [0.25, 0.5, 1.0, 2.0, 3.0]\n  query_xs = [0.25, 3.0, 5.0, 0.0, 0.5, 0.8]\n  result = find_interval_index(query_xs, interval_lower_xs)\n  # result == [0, 4, 4, -1, 1, 1]\n  ```\n\n  Args:\n    query_xs: Rank 1 real `Tensor` of any size, the list of x coordinates for\n      which the interval index is to be found. The values must be strictly\n      increasing.\n    interval_lower_xs: Rank 1 `Tensor` of the same shape and dtype as\n      `query_xs`. The values x_0, ..., x_n that define the interval starts.\n    last_interval_is_closed: If set to `True`, the last interval is interpreted\n      as closed.\n    dtype: Optional `tf.Dtype`. If supplied, the dtype for `query_xs` and\n      `interval_lower_xs`.\n      Default value: None which maps to the default dtype inferred by TensorFlow\n        (float32).\n    name: Optional name of the operation.\n\n  Returns:\n    A tensor that matches the shape of `query_xs` with dtype=int32 containing\n    the indices of the intervals containing query points. `-1` means the query\n    point lies before all intervals and `n-1` means that the point lies in the\n    last half-open interval (if `last_interval_is_closed` is `False`) or that\n    the point lies to the right of all intervals (if `last_interval_is_closed`\n    is `True`).\n  """"""\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'find_interval_index\',\n      values=[query_xs, interval_lower_xs, last_interval_is_closed]):\n    # TODO(b/138988951): add ability to validate that intervals are increasing.\n    # TODO(b/138988951): validate that if last_interval_is_closed, input size\n    # must be > 1.\n    query_xs = tf.convert_to_tensor(query_xs, dtype=dtype)\n    interval_lower_xs = tf.convert_to_tensor(interval_lower_xs, dtype=dtype)\n\n    # Result assuming that last interval is half-open.\n    indices = tf.searchsorted(interval_lower_xs, query_xs, side=\'right\') - 1\n\n    # Handling the branch if the last interval is closed.\n    last_index = tf.shape(interval_lower_xs)[-1] - 1\n    last_x = tf.gather(interval_lower_xs, [last_index], axis=-1)\n    # should_cap is a tensor true where a cell is true iff indices is the last\n    # index at that cell and the query x <= the right boundary of the last\n    # interval.\n    should_cap = tf.logical_and(\n        tf.equal(indices, last_index), tf.less_equal(query_xs, last_x))\n\n    # cap to last_index if the query x is not in the last interval, otherwise,\n    # cap to last_index - 1.\n    caps = last_index - tf.cast(should_cap, dtype=tf.dtypes.int32)\n\n    return tf.compat.v1.where(last_interval_is_closed,\n                              tf.minimum(indices, caps), indices)\n\n\ndef _piecewise_constant_function(x, jump_locations, values,\n                                 batch_rank, side=\'left\'):\n  """"""Computes value of the piecewise constant function.""""""\n  # Initializer already verified that `jump_locations` and `values` have the\n  # same shape\n  batch_shape = jump_locations.shape.as_list()[:-1]\n  # Check that the batch shape of `x` is the same as of `jump_locations` and\n  # `values`\n  batch_shape_x = x.shape.as_list()[:batch_rank]\n  if batch_shape_x != batch_shape:\n    raise ValueError(\'Batch shape of `x` is {1} but should be {0}\'.format(\n        batch_shape, batch_shape_x))\n  if x.shape.as_list()[:batch_rank]:\n    no_batch_shape = False\n  else:\n    no_batch_shape = True\n    x = tf.expand_dims(x, 0)\n  # Expand batch size to one if there is no batch shape\n  if not batch_shape:\n    jump_locations = tf.expand_dims(jump_locations, 0)\n    values = tf.expand_dims(values, 0)\n  indices = tf.searchsorted(jump_locations, x, side=side)\n  index_matrix = _prepare_index_matrix(\n      indices.shape.as_list()[:-1], indices.shape.as_list()[-1], indices.dtype)\n  indices_nd = tf.concat(\n      [index_matrix, tf.expand_dims(indices, -1)], -1)\n  res = tf.gather_nd(values, indices_nd)\n  if no_batch_shape:\n    return tf.squeeze(res, 0)\n  else:\n    return res\n\n\ndef _piecewise_constant_integrate(x1, x2, jump_locations, values, batch_rank):\n  """"""Integrates piecewise constant function between `x1` and `x2`.""""""\n  # Initializer already verified that `jump_locations` and `values` have the\n  # same shape.\n  # Expand batch size to one if there is no batch shape.\n  if x1.shape.as_list()[:batch_rank]:\n    no_batch_shape = False\n  else:\n    no_batch_shape = True\n    x1 = tf.expand_dims(x1, 0)\n    x2 = tf.expand_dims(x2, 0)\n  if not jump_locations.shape.as_list()[:-1]:\n    jump_locations = tf.expand_dims(jump_locations, 0)\n    values = tf.expand_dims(values, 0)\n    batch_rank += 1\n\n  # Compute the index matrix that is later used for `tf.gather_nd`.\n  index_matrix = _prepare_index_matrix(\n      x1.shape.as_list()[:-1], x1.shape.as_list()[-1], tf.int32)\n  # Compute integral values at the jump locations starting from the first jump\n  # location.\n  event_shape = values.shape[(batch_rank+1):]\n  num_data_points = values.shape.as_list()[batch_rank]\n  diff = jump_locations[..., 1:] - jump_locations[..., :-1]\n  # Broadcast `diff` to the shape of\n  # `batch_shape + [num_data_points - 2] + [1] * sample_rank`.\n  for _ in event_shape:\n    diff = tf.expand_dims(diff, -1)\n  slice_indices = batch_rank * [slice(None)]\n  slice_indices += [slice(1, num_data_points - 1)]\n  integrals = tf.cumsum(values[slice_indices] * diff, batch_rank)\n  # Pad integrals with zero values on left and right.\n  batch_shape = integrals.shape.as_list()[:batch_rank]\n  zeros = tf.zeros(batch_shape + [1] + event_shape, dtype=integrals.dtype)\n  integrals = tf.concat([zeros, integrals, zeros], axis=batch_rank)\n  # Get jump locations and values and the integration end points\n  value1, jump_location1, indices_nd1 = _get_indices_and_values(\n      x1, index_matrix, jump_locations, values, \'left\', batch_rank)\n  value2, jump_location2, indices_nd2 = _get_indices_and_values(\n      x2, index_matrix, jump_locations, values, \'right\', batch_rank)\n  integrals1 = tf.gather_nd(integrals, indices_nd1)\n  integrals2 = tf.gather_nd(integrals, indices_nd2)\n  # Broadcast `x1`, `x2`, `jump_location1`, `jump_location2` to the shape\n  # `batch_shape + [num_points] + [1] * sample_rank`.\n  for _ in event_shape:\n    x1 = tf.expand_dims(x1, -1)\n    x2 = tf.expand_dims(x2, -1)\n    jump_location1 = tf.expand_dims(jump_location1, -1)\n    jump_location2 = tf.expand_dims(jump_location2, -1)\n  # Compute the value of the integral.\n  res = ((jump_location1 - x1) * value1\n         + (x2 - jump_location2) * value2\n         + integrals2 - integrals1)\n  if no_batch_shape:\n    return tf.squeeze(res, 0)\n  else:\n    return res\n\n\ndef _get_indices_and_values(x, index_matrix, jump_locations, values, side,\n                            batch_rank):\n  """"""Computes values and jump locations of the piecewise constant function.\n\n  Given `jump_locations` and the `values` on the corresponding segments of the\n  piecewise constant function, the function identifies the nearest jump to `x`\n  from the right or left (which is determined by the `side` argument) and the\n  corresponding value of the piecewise constant function at `x`\n\n  Args:\n    x: A real `Tensor` of shape `batch_shape + [num_points]`. Points at which\n      the function has to be evaluated.\n    index_matrix: An `int32` `Tensor` of shape\n      `batch_shape + [num_points] + [len(batch_shape)]` such that if\n      `batch_shape = [i1, .., in]`, then for all `j1, ..., jn, l`,\n      `index_matrix[j1,..,jn, l] = [j1, ..., jn]`.\n    jump_locations: A `Tensor` of the same `dtype` as `x` and shape\n      `batch_shape + [num_jump_points]`. The locations where the function\n      changes its values. Note that the values are expected to be ordered\n      along the last dimension.\n    values: A `Tensor` of the same `dtype` as `x` and shape\n      `batch_shape + [num_jump_points + 1]`. Defines `values[..., i]` on\n      `jump_locations[..., i - 1], jump_locations[..., i]`.\n    side: A Python string. Whether the function is left- or right- continuous.\n      The corresponding values for side should be `left` and `right`.\n    batch_rank: A Python scalar stating the batch rank of `x`.\n\n  Returns:\n    A tuple of three `Tensor` of the same `dtype` as `x` and shapes\n    `batch_shape + [num_points] + event_shape`, `batch_shape + [num_points]`,\n    and `batch_shape + [num_points] + [2 * len(batch_shape)]`. The `Tensor`s\n    correspond to the values, jump locations at `x`, and the corresponding\n    indices used to obtain jump locations via `tf.gather_nd`.\n  """"""\n  indices = tf.searchsorted(jump_locations, x, side=side)\n  num_data_points = tf.shape(values)[batch_rank] - 2\n  if side == \'right\':\n    indices_jump = indices - 1\n    indices_jump = tf.maximum(indices_jump, 0)\n  else:\n    indices_jump = tf.minimum(indices, num_data_points)\n  indices_nd = tf.concat(\n      [index_matrix, tf.expand_dims(indices, -1)], -1)\n  indices_jump_nd = tf.concat(\n      [index_matrix, tf.expand_dims(indices_jump, -1)], -1)\n  value = tf.gather_nd(values, indices_nd)\n  jump_location = tf.gather_nd(jump_locations, indices_jump_nd)\n  return value, jump_location, indices_jump_nd\n\n\ndef _prepare_index_matrix(batch_shape, num_points, dtype):\n  """"""Prepares index matrix for index argument of `tf.gather_nd`.""""""\n  batch_shape_reverse = batch_shape.copy()\n  batch_shape_reverse.reverse()\n  index_matrix = tf.constant(\n      np.flip(np.transpose(np.indices(batch_shape_reverse)), -1),\n      dtype=dtype)\n  batch_rank = len(batch_shape)\n  # Broadcast index matrix to the shape of\n  # `batch_shape + [num_points] + [batch_rank]`.\n  broadcasted_shape = batch_shape + [num_points] + [batch_rank]\n  index_matrix = tf.expand_dims(index_matrix, -2) + tf.zeros(\n      tf.TensorShape(broadcasted_shape), dtype=dtype)\n  return index_matrix\n\n\ndef _try_broadcast_to(x, batch_shape, name):\n  """"""Broadcasts batch shape of `x` to a `batch_shape` if possible.""""""\n  batch_shape_x = x.shape.as_list()[:-1]\n  if batch_shape_x != batch_shape:\n    try:\n      np.broadcast_to(np.zeros(batch_shape_x), batch_shape)\n    except ValueError:\n      raise ValueError(\'Batch shapes of `{2}` should be broadcastable with {0} \'\n                       \'but it is {1} instead\'.format(\n                           batch_shape, batch_shape_x, name))\n    return tf.broadcast_to(x, batch_shape + x.shape[-1:])\n  return x\n'"
tf_quant_finance/math/piecewise_test.py,12,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for math.piecewise.""""""\n\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\nfrom tf_quant_finance.math import piecewise\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass Piecewise(tf.test.TestCase):\n  """"""Tests for methods in piecewise module.""""""\n\n  def test_find_interval_index_correct_dtype(self):\n    """"""Tests find_interval_index outputs the correct type.""""""\n    result = self.evaluate(piecewise.find_interval_index([1.0], [0.0, 1.0]))\n    self.assertIsInstance(result[0], np.int32)\n\n  def test_find_interval_index_one_interval(self):\n    """"""Tests find_interval_index is correct with one half-open interval.""""""\n    result = self.evaluate(piecewise.find_interval_index([1.0], [1.0]))\n    self.assertAllEqual(result, [0])\n\n    result = self.evaluate(piecewise.find_interval_index([0.0], [1.0]))\n    self.assertAllEqual(result, [-1])\n\n    result = self.evaluate(piecewise.find_interval_index([2.0], [1.0]))\n    self.assertAllEqual(result, [0])\n\n  def test_find_interval_index(self):\n    """"""Tests find_interval_index is correct in the general case.""""""\n    interval_lower_xs = [0.25, 0.5, 1.0, 2.0, 3.0]\n    query_xs = [0.25, 3.0, 5.0, 0.0, 0.5, 0.8]\n    result = piecewise.find_interval_index(query_xs, interval_lower_xs)\n    self.assertAllEqual(result, [0, 4, 4, -1, 1, 1])\n\n  def test_find_interval_index_last_interval_is_closed(self):\n    """"""Tests find_interval_index is correct in the general case.""""""\n    result = piecewise.find_interval_index([3.0, 4.0], [2.0, 3.0],\n                                           last_interval_is_closed=True)\n    self.assertAllEqual(result, [0, 1])\n\n  def test_piecewise_constant_value_no_batch(self):\n    """"""Tests PiecewiseConstantFunc with no batching.""""""\n    for dtype in [np.float32, np.float64]:\n      x = np.array([0., 0.1, 2., 11.])\n      jump_locations = np.array([0.1, 10], dtype=dtype)\n      values = tf.constant([3, 4, 5], dtype=dtype)\n      piecewise_func = piecewise.PiecewiseConstantFunc(jump_locations, values,\n                                                       dtype=dtype)\n      # Also verifies left-continuity\n      value = piecewise_func(x)\n      self.assertEqual(value.dtype.as_numpy_dtype, dtype)\n      expected_value = np.array([3., 3., 4., 5.])\n      self.assertAllEqual(value, expected_value)\n\n  def test_piecewise_constant_integral_no_batch(self):\n    """"""Tests PiecewiseConstantFunc with no batching.""""""\n    for dtype in [np.float32, np.float64]:\n      x = np.array([-4.1, 0., 1., 1.5, 2., 4.5, 5.5])\n      jump_locations = np.array([1, 2, 3, 4, 5], dtype=dtype)\n      values = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n      piecewise_func = piecewise.PiecewiseConstantFunc(jump_locations, values,\n                                                       dtype=dtype)\n      value = piecewise_func.integrate(x, x + 4.1)\n      self.assertEqual(value.dtype.as_numpy_dtype, dtype)\n      expected_value = np.array([0.41, 1.05, 1.46, 1.66, 1.86, 2.41, 2.46])\n      self.assertAllClose(value, expected_value, atol=1e-5, rtol=1e-5)\n\n  def test_piecewise_constant_value_with_batch(self):\n    """"""Tests PiecewiseConstantFunc with batching.""""""\n    for dtype in [np.float32, np.float64]:\n      x = np.array([[[0.0, 0.1, 2.0, 11.0], [0.0, 2.0, 3.0, 9.0]],\n                    [[0.0, 1.0, 2.0, 3.0], [4.0, 5.0, 6.0, 7.0]]])\n      jump_locations = np.array([[[0.1, 10.0], [1.5, 10.0]],\n                                 [[1.0, 2.0], [5.0, 6.0]]])\n      values = tf.constant([[[3, 4, 5], [3, 4, 5]],\n                            [[3, 4, 5], [3, 4, 5]]], dtype=dtype)\n      piecewise_func = piecewise.PiecewiseConstantFunc(jump_locations, values,\n                                                       dtype=dtype)\n      # Also verifies right-continuity\n      value = piecewise_func(x, left_continuous=False)\n      self.assertEqual(value.dtype.as_numpy_dtype, dtype)\n      expected_value = np.array([[[3.0, 4.0, 4.0, 5.0],\n                                  [3.0, 4.0, 4.0, 4.0]],\n                                 [[3.0, 4.0, 5.0, 5.0],\n                                  [3.0, 4.0, 5.0, 5.0]]])\n      self.assertAllEqual(value, expected_value)\n\n  def test_piecewise_constant_value_with_batch_and_repetitions(self):\n    """"""Tests PiecewiseConstantFunc with batching and repetitive values.""""""\n    for dtype in [np.float32, np.float64]:\n      x = tf.constant([[-4.1, 0.1, 1., 2., 10, 11.],\n                       [1., 2., 3., 2., 5., 9.]], dtype=dtype)\n      jump_locations = tf.constant([[0.1, 0.1, 1., 1., 10., 10.],\n                                    [-1., 1.2, 2.2, 2.2, 2.2, 8.]], dtype=dtype)\n      values = tf.constant([[3, 3, 4, 5, 5., 2, 6.],\n                            [-1, -5, 2, 5, 5., 5., 1.]], dtype=dtype)\n      piecewise_func = piecewise.PiecewiseConstantFunc(jump_locations, values,\n                                                       dtype=dtype)\n      # Also verifies left-continuity\n      value = piecewise_func(x, left_continuous=True)\n      self.assertEqual(value.dtype.as_numpy_dtype, dtype)\n      expected_value = np.array([[3., 3., 4., 5., 5., 6.],\n                                 [-5., 2., 5., 2., 5., 1.]])\n      self.assertAllEqual(value, expected_value)\n\n  def test_piecewise_constant_integral_with_batch(self):\n    """"""Tests PiecewiseConstantFunc with batching.""""""\n    for dtype in [np.float32, np.float64]:\n      x = np.array([[[0.0, 0.1, 2.0, 11.0], [0.0, 2.0, 3.0, 9.0]],\n                    [[0.0, 1.0, 2.0, 3.0], [4.0, 5.0, 6.0, 7.0]]])\n      jump_locations = np.array([[[0.1, 10.0], [1.5, 10.0]],\n                                 [[1.0, 2.0], [5.0, 6.0]]])\n      values = tf.constant([[[3, 4, 5], [3, 4, 5]],\n                            [[3, 4, 5], [3, 4, 5]]], dtype=dtype)\n      piecewise_func = piecewise.PiecewiseConstantFunc(jump_locations, values,\n                                                       dtype=dtype)\n      value = piecewise_func.integrate(x, x + 1.1)\n      self.assertEqual(value.dtype.as_numpy_dtype, dtype)\n      expected_value = np.array([[[4.3, 4.4, 4.4, 5.5],\n                                  [3.3, 4.4, 4.4, 4.5]],\n                                 [[3.4, 4.5, 5.5, 5.5],\n                                  [3.4, 4.5, 5.5, 5.5]]])\n      self.assertAllClose(value, expected_value, atol=1e-5, rtol=1e-5)\n\n  def test_invalid_jump_batch_shape(self):\n    """"""Tests that `jump_locations` and `values` should have the same batch.""""""\n    for dtype in [np.float32, np.float64]:\n      jump_locations = np.array([[0.1, 10], [2., 10]])\n      values = tf.constant([[[3, 4, 5], [3, 4, 5]]], dtype=dtype)\n      with self.assertRaises(ValueError):\n        piecewise.PiecewiseConstantFunc(jump_locations, values, dtype=dtype)\n\n  def test_invalid_value_event_shape(self):\n    """"""Tests that `values` event shape is `jump_locations` event shape + 1.""""""\n    for dtype in [np.float32, np.float64]:\n      jump_locations = np.array([[0.1, 10], [2., 10]])\n      values = tf.constant([[3, 4, 5, 6], [3, 4, 5, 7]], dtype=dtype)\n      with self.assertRaises(ValueError):\n        piecewise.PiecewiseConstantFunc(jump_locations, values, dtype=dtype)\n\n  def test_matrix_event_shape_no_batch_shape(self):\n    """"""Tests that `values` event shape is `jump_locations` event shape + 1.""""""\n    for dtype in [np.float32, np.float64]:\n      x = np.array([0., 0.1, 2., 11.])\n      jump_locations = [0.1, 10]\n      values = [[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]]\n      piecewise_func = piecewise.PiecewiseConstantFunc(\n          jump_locations, values, dtype=dtype)\n      value = piecewise_func(x)\n      integral = piecewise_func.integrate(x, x + 1)\n      expected_value = [[[1, 2], [3, 4]], [[1, 2], [3, 4]],\n                        [[5, 6], [7, 8]], [[9, 10], [11, 12]]]\n      expected_integral = [[[4.6, 5.6], [6.6, 7.6]],\n                           [[5, 6], [7, 8]],\n                           [[5, 6], [7, 8]],\n                           [[9, 10], [11, 12]]]\n      self.assertAllClose(value, expected_value, atol=1e-5, rtol=1e-5)\n      self.assertAllClose(integral, expected_integral, atol=1e-5, rtol=1e-5)\n\n  def test_3d_event_shape_with_batch_shape(self):\n    """"""Tests that `values` event shape is `jump_locations` event shape + 1.""""""\n    for dtype in [np.float32, np.float64]:\n      x = np.array([[0, 1, 2, 3], [0.5, 1.5, 2.5, 3.5]])\n      jump_locations = [[0.5, 2], [0.5, 1.5]]\n      values = [[[0, 1, 1.5], [2, 3, 0], [1, 0, 1]],\n                [[0, 0.5, 1], [1, 3, 2], [2, 3, 1]]]\n      piecewise_func = piecewise.PiecewiseConstantFunc(\n          jump_locations, values, dtype=dtype)\n      value = piecewise_func(x)\n      integral = piecewise_func.integrate(x, x + 1)\n      expected_value = [[[0, 1, 1.5],\n                         [2, 3, 0],\n                         [2, 3, 0],\n                         [1, 0, 1]],\n                        [[0, 0.5, 1],\n                         [1, 3, 2],\n                         [2, 3, 1],\n                         [2, 3, 1]]]\n      expected_integral = [[[1, 2, 0.75],\n                            [2, 3, 0],\n                            [1, 0, 1],\n                            [1, 0, 1]],\n                           [[1, 3, 2],\n                            [2, 3, 1],\n                            [2, 3, 1],\n                            [2, 3, 1]]]\n      self.assertAllClose(value, expected_value, atol=1e-5, rtol=1e-5)\n      self.assertAllClose(integral, expected_integral, atol=1e-5, rtol=1e-5)\n\n  def test_invalid_x_batch_shape(self):\n    """"""Tests that `x` should have compatible batch with `jump_locations`.""""""\n    for dtype in [np.float32, np.float64]:\n      x = np.array([[0., 0.1, 2., 11.],\n                    [0., 0.1, 2., 11.], [0., 0.1, 2., 11.]])\n      jump_locations = np.array([[0.1, 10], [2., 10]])\n      values = tf.constant([[3, 4, 5], [3, 4, 5]], dtype=dtype)\n      piecewise_func = piecewise.PiecewiseConstantFunc(jump_locations, values,\n                                                       dtype=dtype)\n      with self.assertRaises(ValueError):\n        piecewise_func(x, left_continuous=False)\n\n  def test_incompatible_x1_x2_batch_shape(self):\n    """"""Tests that `x1` and `x2` should have the same batch shape.""""""\n    for dtype in [np.float32, np.float64]:\n      x1 = np.array([[0., 0.1, 2., 11.],\n                     [0., 0.1, 2., 11.]])\n      x2 = np.array([[0., 0.1, 2., 11.],\n                     [0., 0.1, 2., 11.], [0., 0.1, 2., 11.]])\n      x3 = x2 + 1\n      jump_locations = np.array([[0.1, 10]])\n      values = tf.constant([[3, 4, 5]], dtype=dtype)\n      piecewise_func = piecewise.PiecewiseConstantFunc(jump_locations, values,\n                                                       dtype=dtype)\n      with self.assertRaises(ValueError):\n        piecewise_func.integrate(x1, x2)\n      # `x2` and `x3` have the same batch shape but different from\n      # the batch shape of `jump_locations`\n      with self.assertRaises(ValueError):\n        piecewise_func.integrate(x2, x3)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/math/root_search.py,101,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Root search functions.""""""\n\n\nimport collections\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nBrentResults = collections.namedtuple(\n    ""BrentResults"",\n    [\n        # A tensor containing the best estimate. If the search was successful,\n        # this estimate is a root of the objective function.\n        ""estimated_root"",\n        # A tensor containing the value of the objective function at the best\n        # estimate. If the search was successful, then this is close to 0.\n        ""objective_at_estimated_root"",\n        # A tensor containing number of iterations performed for each pair of\n        # starting points.\n        ""num_iterations"",\n        # Scalar boolean tensor indicating whether the best estimate is a root\n        # within the tolerance specified for the search.\n        ""converged"",\n    ])\n\n# Values which remain fixed across all root searches (except for tensor dtypes\n# and shapes).\n_BrentSearchConstants = collections.namedtuple(""_BrentSearchConstants"", [\n    ""false"",\n    ""zero"",\n    ""zero_value"",\n])\n\n# Values which are updated during the root search.\n_BrentSearchState = collections.namedtuple(""_BrentSearchState"", [\n    ""best_estimate"",\n    ""value_at_best_estimate"",\n    ""last_estimate"",\n    ""value_at_last_estimate"",\n    ""contrapoint"",\n    ""value_at_contrapoint"",\n    ""step_to_best_estimate"",\n    ""step_to_last_estimate"",\n    ""num_iterations"",\n    ""finished"",\n])\n\n# Values which remain fixed for a given root search.\n_BrentSearchParams = collections.namedtuple(""_BrentSearchParams"", [\n    ""objective_fn"",\n    ""max_iterations"",\n    ""absolute_root_tolerance"",\n    ""relative_root_tolerance"",\n    ""function_tolerance"",\n    ""stopping_policy_fn"",\n])\n\n\ndef _swap_where(condition, x, y):\n  """"""Swaps the elements of `x` and `y` based on `condition`.\n\n  Args:\n    condition: A `Tensor` of dtype bool.\n    x: A `Tensor` with the same shape as `condition`.\n    y: A `Tensor` with the same shape and dtype as `x`.\n\n  Returns:\n    Two `Tensors` with the same shape as `x` and `y`.\n  """"""\n  return tf.where(condition, y, x), tf.where(condition, x, y)\n\n\ndef _secant_step(x1, x2, y1, y2):\n  """"""Returns the step size at the current position if using the secant method.\n\n  This function is meant for exclusive use by the `_brent_loop_body` function:\n  - It does not guard against divisions by zero, and instead assumes that `y1`\n    is distinct from `y2`. The `_brent_loop_body` function guarantees this\n    property.\n  - It does not guard against overflows which may occur if the difference\n    between `y1` and `y2` is small while that between `x1` and `x2` is not.\n    In this case, the resulting step size will be larger than `bisection_step`\n    and thus ignored by the `_brent_loop_body` function.\n\n  Args:\n    x1: `Tensor` containing the current position.\n    x2: `Tensor` containing the previous position.\n    y1: `Tensor` containing the value of `objective_fn` at `x1`.\n    y2: `Tensor` containing the value of `objective_fn` at `x2`.\n\n  Returns:\n    A `Tensor` with the same shape and dtype as `current`.\n  """"""\n  x_difference = x1 - x2\n  y_difference = y1 - y2\n  return -y1 * x_difference / y_difference\n\n\ndef _quadratic_interpolation_step(x1, x2, x3, y1, y2, y3):\n  """"""Returns the step size to use when using quadratic interpolation.\n\n  This function is meant for exclusive use by the `_brent_loop_body` function.\n  It does not guard against divisions by zero, and instead assumes that `y1` is\n  distinct from `y2` and `y3`. The `_brent_loop_body` function guarantees this\n  property.\n\n  Args:\n    x1: `Tensor` of any shape and real dtype containing the first position used\n      for extrapolation.\n    x2: `Tensor` of the same shape and dtype as `x1` containing the second\n      position used for extrapolation.\n    x3: `Tensor` of the same shape and dtype as `x1` containing the third\n      position used for extrapolation.\n    y1: `Tensor` containing the value of the interpolated function at `x1`.\n    y2: `Tensor` containing the value of interpolated function at `x2`.\n    y3: `Tensor` containing the value of interpolated function at `x3`.\n\n  Returns:\n    A `Tensor` with the same shape and dtype as `x1`.\n  """"""\n  r2 = (x2 - x1) / (y2 - y1)\n  r3 = (x3 - x1) / (y3 - y1)\n  return -x1 * (x3 * r3 - x2 * r2) / (r3 * r2 * (x3 - x2))\n\n\ndef default_relative_root_tolerance(dtype):\n  """"""Returns the default relative root tolerance used for a TensorFlow dtype.""""""\n  return 4 * np.finfo(dtype.as_numpy_dtype(0)).eps\n\n\ndef _should_stop(state, stopping_policy_fn):\n  """"""Indicates whether the overall Brent search should continue.\n\n  Args:\n    state: A Python `_BrentSearchState` namedtuple.\n    stopping_policy_fn: Python `callable` controlling the algorithm termination.\n\n  Returns:\n    A boolean value indicating whether the overall search should continue.\n  """"""\n  return tf.convert_to_tensor(\n      stopping_policy_fn(state.finished), name=""should_stop"", dtype=tf.bool)\n\n\n# This is a direct translation of the Brent root-finding method.\n# Each operation is guarded by a call to `tf.where` to avoid performing\n# unnecessary calculations.\ndef _brent_loop_body(state, params, constants):\n  """"""Performs one iteration of the Brent root-finding algorithm.\n\n  Args:\n    state: A Python `_BrentSearchState` namedtuple.\n    params: A Python `_BrentSearchParams` namedtuple.\n    constants: A Python `_BrentSearchConstants` namedtuple.\n\n  Returns:\n    The `Tensor`s to use for the next iteration of the algorithm.\n  """"""\n\n  best_estimate = state.best_estimate\n  last_estimate = state.last_estimate\n  contrapoint = state.contrapoint\n  value_at_best_estimate = state.value_at_best_estimate\n  value_at_last_estimate = state.value_at_last_estimate\n  value_at_contrapoint = state.value_at_contrapoint\n  step_to_best_estimate = state.step_to_best_estimate\n  step_to_last_estimate = state.step_to_last_estimate\n  num_iterations = state.num_iterations\n  finished = state.finished\n\n  # If the root is between the last two estimates, use the worst of the two\n  # as new contrapoint. Adjust step sizes accordingly.\n  replace_contrapoint = ~finished & (\n      value_at_last_estimate * value_at_best_estimate < constants.zero_value)\n\n  contrapoint = tf.where(replace_contrapoint, last_estimate, contrapoint)\n  value_at_contrapoint = tf.where(replace_contrapoint, value_at_last_estimate,\n                                  value_at_contrapoint)\n\n  step_to_last_estimate = tf.where(replace_contrapoint,\n                                   best_estimate - last_estimate,\n                                   step_to_last_estimate)\n  step_to_best_estimate = tf.where(replace_contrapoint, step_to_last_estimate,\n                                   step_to_best_estimate)\n\n  # If the contrapoint is a better guess than the current root estimate, swap\n  # them. Also, replace the worst of the two with the current contrapoint.\n  replace_best_estimate = tf.where(\n      finished, constants.false,\n      tf.math.abs(value_at_contrapoint) < tf.math.abs(value_at_best_estimate))\n\n  last_estimate = tf.where(replace_best_estimate, best_estimate, last_estimate)\n  best_estimate = tf.where(replace_best_estimate, contrapoint, best_estimate)\n  contrapoint = tf.where(replace_best_estimate, last_estimate, contrapoint)\n\n  value_at_last_estimate = tf.where(replace_best_estimate,\n                                    value_at_best_estimate,\n                                    value_at_last_estimate)\n  value_at_best_estimate = tf.where(replace_best_estimate, value_at_contrapoint,\n                                    value_at_best_estimate)\n  value_at_contrapoint = tf.where(replace_best_estimate, value_at_last_estimate,\n                                  value_at_contrapoint)\n\n  # Compute the tolerance used to control root search at the current position\n  # and the step size corresponding to the bisection method.\n  root_tolerance = 0.5 * (\n      params.absolute_root_tolerance +\n      params.relative_root_tolerance * tf.math.abs(best_estimate))\n  bisection_step = 0.5 * (contrapoint - best_estimate)\n\n  # Mark the search as finished if either:\n  # 1. the maximum number of iterations has been reached;\n  # 2. the desired tolerance has been reached (even if no root was found);\n  # 3. the current root estimate is good enough.\n  # Using zero as `function_tolerance` will check for exact roots and match\n  # both Brent\'s original algorithm and the SciPy implementation.\n  finished |= (num_iterations >= params.max_iterations) | (\n      tf.math.abs(bisection_step) <\n      root_tolerance) | (~tf.math.is_finite(value_at_best_estimate)) | (\n          tf.math.abs(value_at_best_estimate) <= params.function_tolerance)\n\n  # Determine whether interpolation or extrapolation are worth performing at\n  # the current position.\n  compute_short_step = tf.where(\n      finished, constants.false,\n      (root_tolerance < tf.math.abs(step_to_last_estimate)) &\n      (tf.math.abs(value_at_best_estimate) <\n       tf.math.abs(value_at_last_estimate)))\n\n  short_step = tf.where(\n      compute_short_step,\n      tf.where(\n          # The contrapoint cannot be equal to the current root estimate since\n          # they have opposite signs. However, it may be equal to the previous\n          # estimate.\n          tf.equal(last_estimate, contrapoint),\n          # If so, use the secant method to avoid a division by zero which\n          # would occur if using extrapolation.\n          _secant_step(best_estimate, last_estimate, value_at_best_estimate,\n                       value_at_last_estimate),\n          # Pass values of the objective function as x values, and root\n          # estimates as y values in order to perform *inverse* extrapolation.\n          _quadratic_interpolation_step(value_at_best_estimate,\n                                        value_at_last_estimate,\n                                        value_at_contrapoint, best_estimate,\n                                        last_estimate, contrapoint)),\n      # Default to zero if using bisection.\n      constants.zero)\n\n  # Use the step calculated above if both:\n  # 1. step size < |previous step size|\n  # 2. step size < 3/4 * |contrapoint - current root estimate|\n  # Ensure that `short_step` was calculated by guarding the calculation with\n  # `compute_short_step`.\n  use_short_step = tf.where(\n      compute_short_step, 2 * tf.math.abs(short_step) < tf.minimum(\n          3 * tf.math.abs(bisection_step) - root_tolerance,\n          tf.math.abs(step_to_last_estimate)), constants.false)\n\n  # Revert to bisection when not using `short_step`.\n  step_to_last_estimate = tf.where(use_short_step, step_to_best_estimate,\n                                   bisection_step)\n  step_to_best_estimate = tf.where(\n      finished, constants.zero,\n      tf.where(use_short_step, short_step, bisection_step))\n\n  # Update the previous and current root estimates.\n  last_estimate = tf.where(finished, last_estimate, best_estimate)\n  best_estimate += tf.where(\n      finished, constants.zero,\n      tf.where(root_tolerance < tf.math.abs(step_to_best_estimate),\n               step_to_best_estimate,\n               tf.where(bisection_step > 0, root_tolerance, -root_tolerance)))\n\n  value_at_last_estimate = tf.where(finished, value_at_last_estimate,\n                                    value_at_best_estimate)\n  value_at_best_estimate = tf.where(finished, value_at_best_estimate,\n                                    params.objective_fn(best_estimate))\n\n  num_iterations = tf.where(finished, num_iterations, num_iterations + 1)\n\n  return [\n      _BrentSearchState(\n          best_estimate=best_estimate,\n          last_estimate=last_estimate,\n          contrapoint=contrapoint,\n          value_at_best_estimate=value_at_best_estimate,\n          value_at_last_estimate=value_at_last_estimate,\n          value_at_contrapoint=value_at_contrapoint,\n          step_to_best_estimate=step_to_best_estimate,\n          step_to_last_estimate=step_to_last_estimate,\n          num_iterations=num_iterations,\n          finished=finished)\n  ]\n\n\ndef _prepare_brent_args(objective_fn,\n                        left_bracket,\n                        right_bracket,\n                        value_at_left_bracket,\n                        value_at_right_bracket,\n                        absolute_root_tolerance=2e-7,\n                        relative_root_tolerance=None,\n                        function_tolerance=2e-7,\n                        max_iterations=100,\n                        stopping_policy_fn=None):\n  r""""""Prepares arguments for root search using Brent\'s method.\n\n  Args:\n    objective_fn: Python callable for which roots are searched. It must be a\n      callable of a single `Tensor` parameter and return a `Tensor` of the same\n      shape and dtype as `left_bracket`.\n    left_bracket: `Tensor` or Python float representing the first starting\n      points. The function will search for roots between each pair of points\n      defined by `left_bracket` and `right_bracket`. The shape of `left_bracket`\n      should match that of the input to `objective_fn`.\n    right_bracket: `Tensor` of the same shape and dtype as `left_bracket` or\n      Python float representing the second starting points. The function will\n      search for roots between each pair of points defined by `left_bracket` and\n      `right_bracket`. This argument must have the same shape as `left_bracket`.\n    value_at_left_bracket: Optional `Tensor` or Pyhon float representing the\n      value of `objective_fn` at `left_bracket`. If specified, this argument\n      must have the same shape as `left_bracket`. If not specified, the value\n      will be evaluated during the search.\n      Default value: None.\n    value_at_right_bracket: Optional `Tensor` or Pyhon float representing the\n      value of `objective_fn` at `right_bracket`. If specified, this argument\n      must have the same shape as `right_bracket`. If not specified, the value\n      will be evaluated during the search.\n      Default value: None.\n    absolute_root_tolerance: Optional `Tensor` representing the absolute\n      tolerance for estimated roots, with the total tolerance being calculated\n      as `(absolute_root_tolerance + relative_root_tolerance * |root|) / 2`. If\n      specified, this argument must be positive, broadcast with the shape of\n      `left_bracket` and have the same dtype.\n      Default value: `2e-7`.\n    relative_root_tolerance: Optional `Tensor` representing the relative\n      tolerance for estimated roots, with the total tolerance being calculated\n      as `(absolute_root_tolerance + relative_root_tolerance * |root|) / 2`. If\n      specified, this argument must be positive, broadcast with the shape of\n      `left_bracket` and have the same dtype.\n      Default value: `None` which translates to `4 *\n        numpy.finfo(left_bracket.dtype.as_numpy_dtype).eps`.\n    function_tolerance: Optional `Tensor` representing the tolerance used to\n      check for roots. If the absolute value of `objective_fn` is smaller than\n      or equal to `function_tolerance` at a given estimate, then that estimate\n      is considered a root for the function. If specified, this argument must\n      broadcast with the shape of `left_bracket` and have the same dtype. Set to\n      zero to match Brent\'s original algorithm and to continue the search until\n      an exact root is found.\n      Default value: `2e-7`.\n    max_iterations: Optional `Tensor` of an integral dtype or Python integer\n      specifying the maximum number of steps to perform for each initial point.\n      Must broadcast with the shape of `left_bracket`. If an element is set to\n      zero, the function will not search for any root for the corresponding\n      points in `left_bracket` and `right_bracket`. Instead, it will return the\n      best estimate from the inputs.\n      Default value: `100`.\n    stopping_policy_fn: Python `callable` controlling the algorithm termination.\n      It must be a callable accepting a `Tensor` of booleans with the shape of\n      `left_bracket` (each denoting whether the search is finished for each\n      starting point), and returning a scalar boolean `Tensor` (indicating\n      whether the overall search should stop). Typical values are\n      `tf.reduce_all` (which returns only when the search is finished for all\n      pairs of points), and `tf.reduce_any` (which returns as soon as the search\n      is finished for any pair of points).\n      Default value: `None` which translates to `tf.reduce_all`.\n\n  Returns:\n    A tuple of 3 Python objects containing the state, parameters, and constants\n    to use for the search.\n  """"""\n  stopping_policy_fn = stopping_policy_fn or tf.reduce_all\n  if not callable(stopping_policy_fn):\n    raise ValueError(""stopping_policy_fn must be callable"")\n\n  left_bracket = tf.convert_to_tensor(left_bracket, name=""left_bracket"")\n  right_bracket = tf.convert_to_tensor(\n      right_bracket, name=""right_bracket"", dtype=left_bracket.dtype)\n\n  if value_at_left_bracket is None:\n    value_at_left_bracket = objective_fn(left_bracket)\n  if value_at_right_bracket is None:\n    value_at_right_bracket = objective_fn(right_bracket)\n\n  value_at_left_bracket = tf.convert_to_tensor(\n      value_at_left_bracket,\n      name=""value_at_left_bracket"",\n      dtype=left_bracket.dtype.base_dtype)\n  value_at_right_bracket = tf.convert_to_tensor(\n      value_at_right_bracket,\n      name=""value_at_right_bracket"",\n      dtype=left_bracket.dtype.base_dtype)\n\n  if relative_root_tolerance is None:\n    relative_root_tolerance = default_relative_root_tolerance(\n        left_bracket.dtype.base_dtype)\n\n  absolute_root_tolerance = tf.convert_to_tensor(\n      absolute_root_tolerance,\n      name=""absolute_root_tolerance"",\n      dtype=left_bracket.dtype)\n  relative_root_tolerance = tf.convert_to_tensor(\n      relative_root_tolerance,\n      name=""relative_root_tolerance"",\n      dtype=left_bracket.dtype)\n  function_tolerance = tf.convert_to_tensor(\n      function_tolerance, name=""function_tolerance"", dtype=left_bracket.dtype)\n\n  max_iterations = tf.broadcast_to(\n      tf.convert_to_tensor(max_iterations),\n      name=""max_iterations"",\n      shape=left_bracket.shape)\n  num_iterations = tf.zeros_like(max_iterations)\n\n  false = tf.constant(False, shape=left_bracket.shape)\n\n  zero = tf.zeros_like(left_bracket)\n  contrapoint = zero\n  step_to_last_estimate = zero\n  step_to_best_estimate = zero\n\n  zero_value = tf.zeros_like(value_at_left_bracket)\n  value_at_contrapoint = zero_value\n\n  # Select the best root estimates from the inputs.\n  # If no search is performed (e.g. `max_iterations` is `zero`), the estimate\n  # computed this way will be returned. This differs slightly from the SciPy\n  # implementation which always returns the `right_bracket`.\n  swap_positions = tf.math.abs(value_at_left_bracket) < tf.math.abs(\n      value_at_right_bracket)\n  best_estimate, last_estimate = _swap_where(swap_positions, right_bracket,\n                                             left_bracket)\n  value_at_best_estimate, value_at_last_estimate = _swap_where(\n      swap_positions, value_at_right_bracket, value_at_left_bracket)\n\n  # Check if the current root estimate is good enough.\n  # Using zero as `function_tolerance` will check for exact roots and match both\n  # Brent\'s original algorithm and the SciPy implementation.\n  finished = (num_iterations >=\n              max_iterations) | (~tf.math.is_finite(value_at_last_estimate)) | (\n                  ~tf.math.is_finite(value_at_best_estimate)) | (\n                      tf.math.abs(value_at_best_estimate) <= function_tolerance)\n\n  return (_BrentSearchState(\n      best_estimate=best_estimate,\n      last_estimate=last_estimate,\n      contrapoint=contrapoint,\n      value_at_best_estimate=value_at_best_estimate,\n      value_at_last_estimate=value_at_last_estimate,\n      value_at_contrapoint=value_at_contrapoint,\n      step_to_best_estimate=step_to_best_estimate,\n      step_to_last_estimate=step_to_last_estimate,\n      num_iterations=num_iterations,\n      finished=finished),\n          _BrentSearchParams(\n              objective_fn=objective_fn,\n              max_iterations=max_iterations,\n              absolute_root_tolerance=absolute_root_tolerance,\n              relative_root_tolerance=relative_root_tolerance,\n              function_tolerance=function_tolerance,\n              stopping_policy_fn=stopping_policy_fn),\n          _BrentSearchConstants(false=false, zero=zero, zero_value=zero_value))\n\n\n# `_brent` currently only support inverse quadratic extrapolation.\n# This will be fixed when adding the `brenth` variant.\ndef _brent(objective_fn,\n           left_bracket,\n           right_bracket,\n           value_at_left_bracket=None,\n           value_at_right_bracket=None,\n           absolute_root_tolerance=2e-7,\n           relative_root_tolerance=None,\n           function_tolerance=2e-7,\n           max_iterations=100,\n           stopping_policy_fn=None,\n           validate_args=False,\n           name=None):\n  r""""""Finds root(s) of a function of a single variable using Brent\'s method.\n\n  [Brent\'s method](https://en.wikipedia.org/wiki/Brent%27s_method) is a\n  root-finding algorithm combining the bisection method, the secant method and\n  extrapolation. Like bisection it is guaranteed to converge towards a root if\n  one exists, but that convergence is superlinear and on par with less reliable\n  methods.\n\n  This implementation is a translation of the algorithm described in the\n  [original article](https://academic.oup.com/comjnl/article/14/4/422/325237).\n\n  Args:\n    objective_fn: Python callable for which roots are searched. It must be a\n      callable of a single `Tensor` parameter and return a `Tensor` of the same\n      shape and dtype as `left_bracket`.\n    left_bracket: `Tensor` or Python float representing the first starting\n      points. The function will search for roots between each pair of points\n      defined by `left_bracket` and `right_bracket`. The shape of `left_bracket`\n      should match that of the input to `objective_fn`.\n    right_bracket: `Tensor` of the same shape and dtype as `left_bracket` or\n      Python float representing the second starting points. The function will\n      search for roots between each pair of points defined by `left_bracket` and\n      `right_bracket`. This argument must have the same shape as `left_bracket`.\n    value_at_left_bracket: Optional `Tensor` or Pyhon float representing the\n      value of `objective_fn` at `left_bracket`. If specified, this argument\n      must have the same shape as `left_bracket`. If not specified, the value\n      will be evaluated during the search.\n      Default value: None.\n    value_at_right_bracket: Optional `Tensor` or Pyhon float representing the\n      value of `objective_fn` at `right_bracket`. If specified, this argument\n      must have the same shape as `right_bracket`. If not specified, the value\n      will be evaluated during the search.\n      Default value: None.\n    absolute_root_tolerance: Optional `Tensor` representing the absolute\n      tolerance for estimated roots, with the total tolerance being calculated\n      as `(absolute_root_tolerance + relative_root_tolerance * |root|) / 2`. If\n      specified, this argument must be positive, broadcast with the shape of\n      `left_bracket` and have the same dtype.\n      Default value: `2e-7`.\n    relative_root_tolerance: Optional `Tensor` representing the relative\n      tolerance for estimated roots, with the total tolerance being calculated\n      as `(absolute_root_tolerance + relative_root_tolerance * |root|) / 2`. If\n      specified, this argument must be positive, broadcast with the shape of\n      `left_bracket` and have the same dtype.\n      Default value: `None` which translates to `4 *\n        numpy.finfo(left_bracket.dtype.as_numpy_dtype).eps`.\n    function_tolerance: Optional `Tensor` representing the tolerance used to\n      check for roots. If the absolute value of `objective_fn` is smaller than\n      or equal to `function_tolerance` at a given estimate, then that estimate\n      is considered a root for the function. If specified, this argument must\n      broadcast with the shape of `left_bracket` and have the same dtype. Set to\n      zero to match Brent\'s original algorithm and to continue the search until\n      an exact root is found.\n      Default value: `2e-7`.\n    max_iterations: Optional `Tensor` of an integral dtype or Python integer\n      specifying the maximum number of steps to perform for each initial point.\n      Must broadcast with the shape of `left_bracket`. If an element is set to\n      zero, the function will not search for any root for the corresponding\n      points in `left_bracket` and `right_bracket`. Instead, it will return the\n      best estimate from the inputs.\n      Default value: `100`.\n    stopping_policy_fn: Python `callable` controlling the algorithm termination.\n      It must be a callable accepting a `Tensor` of booleans with the shape of\n      `left_bracket` (each denoting whether the search is finished for each\n      starting point), and returning a scalar boolean `Tensor` (indicating\n      whether the overall search should stop). Typical values are\n      `tf.reduce_all` (which returns only when the search is finished for all\n      pairs of points), and `tf.reduce_any` (which returns as soon as the search\n      is finished for any pair of points).\n      Default value: `None` which translates to `tf.reduce_all`.\n    validate_args: Python `bool` indicating whether to validate arguments such\n      as `left_bracket`, `right_bracket`, `absolute_root_tolerance`,\n      `relative_root_tolerance`, `function_tolerance`, and `max_iterations`.\n      Default value: `False`.\n    name: Python `str` name prefixed to ops created by this function.\n\n  Returns:\n    brent_results: A Python object containing the following attributes:\n      estimated_root: `Tensor` containing the best estimate explored. If the\n        search was successful within the specified tolerance, this estimate is\n        a root of the objective function.\n      objective_at_estimated_root: `Tensor` containing the value of the\n        objective function at `estimated_root`. If the search was successful\n        within the specified tolerance, then this is close to 0. It has the\n        same dtype and shape as `estimated_root`.\n      num_iterations: `Tensor` containing the number of iterations performed.\n        It has the same dtype as `max_iterations` and shape as `estimated_root`.\n      converged: Scalar boolean `Tensor` indicating whether `estimated_root` is\n        a root within the tolerance specified for the search. It has the same\n        shape as `estimated_root`.\n\n  Raises:\n    ValueError: if the `stopping_policy_fn` is not callable.\n  """"""\n\n  with tf.compat.v1.name_scope(name, default_name=""brent_root"", values=[\n      left_bracket, right_bracket, value_at_left_bracket,\n      value_at_right_bracket, max_iterations\n  ]):\n\n    state, params, constants = _prepare_brent_args(\n        objective_fn, left_bracket, right_bracket, value_at_left_bracket,\n        value_at_right_bracket, absolute_root_tolerance,\n        relative_root_tolerance, function_tolerance, max_iterations,\n        stopping_policy_fn)\n\n    assertions = []\n    if validate_args:\n      assertions += [\n          tf.Assert(\n              tf.reduce_all(\n                  state.value_at_last_estimate *\n                  state.value_at_best_estimate <= constants.zero_value),\n              [state.value_at_last_estimate, state.value_at_best_estimate]),\n          tf.Assert(\n              tf.reduce_all(params.absolute_root_tolerance > constants.zero),\n              [params.absolute_root_tolerance]),\n          tf.Assert(\n              tf.reduce_all(params.relative_root_tolerance > constants.zero),\n              [params.relative_root_tolerance]),\n          tf.Assert(\n              tf.reduce_all(params.function_tolerance >= constants.zero),\n              [params.function_tolerance]),\n          tf.Assert(\n              tf.reduce_all(params.max_iterations >= state.num_iterations),\n              [params.max_iterations]),\n      ]\n\n    with tf.compat.v1.control_dependencies(assertions):\n      result = tf.while_loop(\n          # Negate `_should_stop` to determine if the search should continue.\n          # This means, in particular, that tf.reduce_*all* will return only\n          # when the search is finished for *all* starting points.\n          lambda loop_vars: ~_should_stop(loop_vars, params.stopping_policy_fn),\n          lambda state: _brent_loop_body(state, params, constants),\n          loop_vars=[state])\n\n  state = result[0]\n  converged = tf.math.abs(state.value_at_best_estimate) <= function_tolerance\n\n  return BrentResults(\n      estimated_root=state.best_estimate,\n      objective_at_estimated_root=state.value_at_best_estimate,\n      num_iterations=state.num_iterations,\n      converged=converged)\n\n\ndef brentq(objective_fn,\n           left_bracket,\n           right_bracket,\n           value_at_left_bracket=None,\n           value_at_right_bracket=None,\n           absolute_root_tolerance=2e-7,\n           relative_root_tolerance=None,\n           function_tolerance=2e-7,\n           max_iterations=100,\n           stopping_policy_fn=None,\n           validate_args=False,\n           name=None):\n  r""""""Finds root(s) of a function of single variable using Brent\'s method.\n\n  [Brent\'s method](https://en.wikipedia.org/wiki/Brent%27s_method) is a\n  root-finding algorithm combining the bisection method, the secant method and\n  extrapolation. Like bisection it is guaranteed to converge towards a root if\n  one exists, but that convergence is superlinear and on par with less reliable\n  methods.\n\n  This implementation is a translation of the algorithm described in the\n  [original article](https://academic.oup.com/comjnl/article/14/4/422/325237).\n\n  Args:\n    objective_fn: Python callable for which roots are searched. It must be a\n      callable of a single `Tensor` parameter and return a `Tensor` of the same\n      shape and dtype as `left_bracket`.\n    left_bracket: `Tensor` or Python float representing the first starting\n      points. The function will search for roots between each pair of points\n      defined by `left_bracket` and `right_bracket`. The shape of `left_bracket`\n      should match that of the input to `objective_fn`.\n    right_bracket: `Tensor` of the same shape and dtype as `left_bracket` or\n      Python float representing the second starting points. The function will\n      search for roots between each pair of points defined by `left_bracket` and\n      `right_bracket`. This argument must have the same shape as `left_bracket`.\n    value_at_left_bracket: Optional `Tensor` or Pyhon float representing the\n      value of `objective_fn` at `left_bracket`. If specified, this argument\n      must have the same shape as `left_bracket`. If not specified, the value\n      will be evaluated during the search.\n      Default value: None.\n    value_at_right_bracket: Optional `Tensor` or Pyhon float representing the\n      value of `objective_fn` at `right_bracket`. If specified, this argument\n      must have the same shape as `right_bracket`. If not specified, the value\n      will be evaluated during the search.\n      Default value: None.\n    absolute_root_tolerance: Optional `Tensor` representing the absolute\n      tolerance for estimated roots, with the total tolerance being calculated\n      as `(absolute_root_tolerance + relative_root_tolerance * |root|) / 2`. If\n      specified, this argument must be positive, broadcast with the shape of\n      `left_bracket` and have the same dtype.\n      Default value: `2e-7`.\n    relative_root_tolerance: Optional `Tensor` representing the relative\n      tolerance for estimated roots, with the total tolerance being calculated\n      as `(absolute_root_tolerance + relative_root_tolerance * |root|) / 2`. If\n      specified, this argument must be positive, broadcast with the shape of\n      `left_bracket` and have the same dtype.\n      Default value: `None` which translates to `4 *\n        numpy.finfo(left_bracket.dtype.as_numpy_dtype).eps`.\n    function_tolerance: Optional `Tensor` representing the tolerance used to\n      check for roots. If the absolute value of `objective_fn` is smaller than\n      or equal to `function_tolerance` at a given estimate, then that estimate\n      is considered a root for the function. If specified, this argument must\n      broadcast with the shape of `left_bracket` and have the same dtype. Set to\n      zero to match Brent\'s original algorithm and to continue the search until\n      an exact root is found.\n      Default value: `2e-7`.\n    max_iterations: Optional `Tensor` of an integral dtype or Python integer\n      specifying the maximum number of steps to perform for each initial point.\n      Must broadcast with the shape of `left_bracket`. If an element is set to\n      zero, the function will not search for any root for the corresponding\n      points in `left_bracket` and `right_bracket`. Instead, it will return the\n      best estimate from the inputs.\n      Default value: `100`.\n    stopping_policy_fn: Python `callable` controlling the algorithm termination.\n      It must be a callable accepting a `Tensor` of booleans with the shape of\n      `left_bracket` (each denoting whether the search is finished for each\n      starting point), and returning a scalar boolean `Tensor` (indicating\n      whether the overall search should stop). Typical values are\n      `tf.reduce_all` (which returns only when the search is finished for all\n      pairs of points), and `tf.reduce_any` (which returns as soon as the search\n      is finished for any pair of points).\n      Default value: `None` which translates to `tf.reduce_all`.\n    validate_args: Python `bool` indicating whether to validate arguments such\n      as `left_bracket`, `right_bracket`, `absolute_root_tolerance`,\n      `relative_root_tolerance`, `function_tolerance`, and `max_iterations`.\n      Default value: `False`.\n    name: Python `str` name prefixed to ops created by this function.\n\n  Returns:\n    brent_results: A Python object containing the following attributes:\n      estimated_root: `Tensor` containing the best estimate explored. If the\n        search was successful within the specified tolerance, this estimate is\n        a root of the objective function.\n      objective_at_estimated_root: `Tensor` containing the value of the\n        objective function at `estimated_root`. If the search was successful\n        within the specified tolerance, then this is close to 0. It has the\n        same dtype and shape as `estimated_root`.\n      num_iterations: `Tensor` containing the number of iterations performed.\n        It has the same dtype as `max_iterations` and shape as `estimated_root`.\n      converged: Scalar boolean `Tensor` indicating whether `estimated_root` is\n        a root within the tolerance specified for the search. It has the same\n        shape as `estimated_root`.\n\n  Raises:\n    ValueError: if the `stopping_policy_fn` is not callable.\n\n  #### Examples\n\n  ```python\n  import tensorflow.compat.v2 as tf\n  tf.enable_eager_execution()\n\n  # Example 1: Roots of a single function for two pairs of starting points.\n\n  f = lambda x: 63 * x**5 - 70 * x**3 + 15 * x + 2\n  x1 = tf.constant([-10, 1], dtype=tf.float64)\n  x2 = tf.constant([10, -1], dtype=tf.float64)\n\n  tf.math.brentq(objective_fn=f, left_bracket=x1, right_bracket=x2)\n  # ==> BrentResults(\n  #    estimated_root=array([-0.14823253, -0.14823253]),\n  #    objective_at_estimated_root=array([3.27515792e-15, 0.]),\n  #    num_iterations=array([11, 6]),\n  #    converged=array([True, True]))\n\n  tf.math.brentq(objective_fn=f,\n                 left_bracket=x1,\n                 right_bracket=x2,\n                 stopping_policy_fn=tf.reduce_any)\n  # ==> BrentResults(\n  #    estimated_root=array([-2.60718234, -0.14823253]),\n  #    objective_at_estimated_root=array([-6.38579115e+03, 2.39763764e-11]),\n  #    num_iterations=array([7, 6]),\n  #    converged=array([False, True]))\n\n  # Example 2: Roots of a multiplex function for one pair of starting points.\n\n  def f(x):\n    return tf.constant([0., 63.], dtype=tf.float64) * x**5 \\\n        + tf.constant([5., -70.], dtype=tf.float64) * x**3 \\\n        + tf.constant([-3., 15.], dtype=tf.float64) * x \\\n        + 2\n\n  x1 = tf.constant([-5, -5], dtype=tf.float64)\n  x2 = tf.constant([5, 5], dtype=tf.float64)\n\n  tf.math.brentq(objective_fn=f, left_bracket=x1, right_bracket=x2)\n  # ==> BrentResults(\n  #    estimated_root=array([-1., -0.14823253]),\n  #    objective_at_estimated_root=array([0., 2.08721929e-14]),\n  #    num_iterations=array([13, 11]),\n  #    converged=array([True, True]))\n\n  # Example 3: Roots of a multiplex function for two pairs of starting points.\n\n  def f(x):\n    return tf.constant([0., 63.], dtype=tf.float64) * x**5 \\\n        + tf.constant([5., -70.], dtype=tf.float64) * x**3 \\\n        + tf.constant([-3., 15.], dtype=tf.float64) * x \\\n        + 2\n\n  x1 = tf.constant([[-5, -5], [10, 10]], dtype=tf.float64)\n  x2 = tf.constant([[5, 5], [-10, -10]], dtype=tf.float64)\n\n  tf.math.brentq(objective_fn=f, left_bracket=x1, right_bracket=x2)\n  # ==> BrentResults(\n  #    estimated_root=array([\n  #        [-1, -0.14823253],\n  #        [-1, -0.14823253]]),\n  #    objective_at_estimated_root=array([\n  #        [0., 2.08721929e-14],\n  #        [0., 2.08721929e-14]]),\n  #    num_iterations=array([\n  #        [13, 11],\n  #        [15, 11]]),\n  #    converged=array([\n  #        [True, True],\n  #        [True, True]]))\n  ```\n  """"""\n\n  return _brent(\n      objective_fn,\n      left_bracket,\n      right_bracket,\n      value_at_left_bracket=value_at_left_bracket,\n      value_at_right_bracket=value_at_right_bracket,\n      absolute_root_tolerance=absolute_root_tolerance,\n      relative_root_tolerance=relative_root_tolerance,\n      function_tolerance=function_tolerance,\n      max_iterations=max_iterations,\n      stopping_policy_fn=stopping_policy_fn,\n      validate_args=validate_args,\n      name=name)\n'"
tf_quant_finance/math/root_search_test.py,38,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for math.root_search.""""""\n\nimport math\n\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\nfrom tf_quant_finance.math import root_search\n\n\ndef polynomial5(x):\n  """"""Polynomial function with 5 roots in the [-1, 1] range.""""""\n  return 63 * x**5 - 70 * x**3 + 15 * x + 2\n\n\ndef exp(x):\n  """"""Exponential function which can operate on floats and `Tensor`s.""""""\n  return math.exp(x) if isinstance(x, float) else tf.exp(x)\n\n\ndef cos(x):\n  """"""Cosine function which can operate on floats and `Tensor`s.""""""\n  return math.cos(x) if isinstance(x, float) else tf.cos(x)\n\n\ndef sin(x):\n  """"""Sine function which can operate on floats and `Tensor`s.""""""\n  return math.sin(x) if isinstance(x, float) else tf.sin(x)\n\n\nclass BrentqTest(tf.test.TestCase):\n\n  def _testFindsAllRoots(self,\n                         objective_fn,\n                         left_bracket,\n                         right_bracket,\n                         expected_roots,\n                         expected_num_iterations,\n                         dtype=tf.float64,\n                         absolute_root_tolerance=2e-7,\n                         relative_root_tolerance=None,\n                         function_tolerance=2e-7):\n    # expected_roots are pre-calculated as follows:\n    # import scipy.optimize as optimize\n    # roots = optimize.brentq(objective_fn,\n    #                         left_bracket[i],\n    #                         right_bracket[i],\n    #                         xtol=absolute_root_tolerance,\n    #                         rtol=relative_root_tolerance)\n\n    assert len(left_bracket) == len(\n        right_bracket), ""Brackets have different sizes""\n\n    if relative_root_tolerance is None:\n      relative_root_tolerance = root_search.default_relative_root_tolerance(\n          dtype)\n\n    expected_num_iterations, result = self.evaluate([\n        tf.constant(expected_num_iterations, dtype=tf.int32),\n        root_search.brentq(\n            objective_fn,\n            tf.constant(left_bracket, dtype=dtype),\n            tf.constant(right_bracket, dtype=dtype),\n            absolute_root_tolerance=absolute_root_tolerance,\n            relative_root_tolerance=relative_root_tolerance,\n            function_tolerance=function_tolerance)\n    ])\n\n    roots, value_at_roots, num_iterations, converged = result\n    zeros = [0.] * len(left_bracket)\n\n    # The output of SciPy and Tensorflow implementation should match for\n    # well-behaved functions.\n    self.assertAllClose(\n        roots,\n        expected_roots,\n        atol=2 * absolute_root_tolerance,\n        rtol=2 * relative_root_tolerance)\n    self.assertAllClose(value_at_roots, zeros, atol=10 * function_tolerance)\n    self.assertAllEqual(num_iterations, expected_num_iterations)\n    self.assertAllEqual(\n        converged,\n        [abs(value) <= function_tolerance for value in value_at_roots])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testFindsOneRoot(self):\n    self._testFindsAllRoots(\n        objective_fn=lambda x: 4 * x**2 - 4,\n        left_bracket=[1],  # Root\n        right_bracket=[0],\n        expected_roots=[1],\n        expected_num_iterations=[0])\n\n    self._testFindsAllRoots(\n        objective_fn=lambda x: x**3 - 4 * x**2 + 3,\n        left_bracket=[-1],\n        right_bracket=[1],  # Root\n        expected_roots=[1],\n        expected_num_iterations=[0])\n\n    self._testFindsAllRoots(\n        objective_fn=lambda x: x**2 - 7,\n        left_bracket=[2],\n        right_bracket=[3],\n        expected_roots=[2.6457513093775136],\n        expected_num_iterations=[4])\n\n    self._testFindsAllRoots(\n        objective_fn=polynomial5,\n        left_bracket=[-1],\n        right_bracket=[1],\n        expected_roots=[-0.14823253013216148],\n        expected_num_iterations=[6])\n\n    self._testFindsAllRoots(\n        objective_fn=lambda x: exp(x) - 2 * x**2,\n        left_bracket=[1],\n        right_bracket=[2],\n        expected_roots=[1.487962064137658],\n        expected_num_iterations=[4])\n\n    self._testFindsAllRoots(\n        objective_fn=lambda x: exp(-x) - 2 * x**2,\n        left_bracket=[0],\n        right_bracket=[1],\n        expected_roots=[0.5398352897010781],\n        expected_num_iterations=[5])\n\n    self._testFindsAllRoots(\n        objective_fn=lambda x: x * (1 - cos(x)),\n        left_bracket=[-1],\n        right_bracket=[1],\n        expected_roots=[0.0],\n        expected_num_iterations=[1])\n\n    self._testFindsAllRoots(\n        objective_fn=lambda x: 1 - x + sin(x),\n        left_bracket=[1],\n        right_bracket=[2],\n        expected_roots=[1.934563210652628],\n        expected_num_iterations=[4])\n\n    self._testFindsAllRoots(\n        # Flat in the [-0.5, 0.5] range.\n        objective_fn=lambda x: 0 if x == 0 else x * exp(-1 / x**2),\n        left_bracket=[-10],\n        right_bracket=[1],\n        expected_roots=[-0.017029902449646958],\n        expected_num_iterations=[22],\n        # Set the tolerance to zero to match the SciPy implementation.\n        function_tolerance=0)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testFindsAllRoots(self):\n    self._testFindsAllRoots(\n        objective_fn=polynomial5,\n        left_bracket=[-10, 1],\n        right_bracket=[10, -1],\n        expected_roots=[-0.14823252898856332, -0.14823253013216148],\n        expected_num_iterations=[10, 6])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testFindsAllRootsUsingFloat32(self):\n    self._testFindsAllRoots(\n        objective_fn=polynomial5,\n        left_bracket=[-4, 1],\n        right_bracket=[3, -1],\n        dtype=tf.float32,\n        expected_roots=[-0.14823253010472962, -0.14823253013216148],\n        expected_num_iterations=[13, 7])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testFindsAllRootsUsingFloat16(self):\n    left_bracket = [-2, 1]\n    right_bracket = [2, -1]\n    expected_num_iterations = [9, 4]\n\n    expected_num_iterations, result = self.evaluate([\n        tf.constant(expected_num_iterations, dtype=tf.int32),\n        root_search.brentq(polynomial5,\n                           tf.constant(left_bracket, dtype=tf.float16),\n                           tf.constant(right_bracket, dtype=tf.float16))\n    ])\n\n    _, value_at_roots, num_iterations, _ = result\n\n    # Simply check that the objective function is close to the root for the\n    # returned estimates. Do not check the estimates themselves.\n    # Using float16 may yield root estimates which differ from those returned\n    # by the SciPy implementation.\n    self.assertAllClose(value_at_roots, [0., 0.], atol=1e-3)\n    self.assertAllEqual(num_iterations, expected_num_iterations)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testFindsAnyRoots(self):\n    objective_fn = lambda x: (63 * x**5 - 70 * x**3 + 15 * x + 2) / 8.\n\n    left_bracket = [-10, 1]\n    right_bracket = [10, -1]\n    expected_num_iterations = [7, 6]\n\n    expected_num_iterations, result = self.evaluate([\n        tf.constant(expected_num_iterations, dtype=tf.int32),\n        root_search.brentq(\n            objective_fn,\n            tf.constant(left_bracket, dtype=tf.float64),\n            tf.constant(right_bracket, dtype=tf.float64),\n            stopping_policy_fn=tf.reduce_any)\n    ])\n\n    roots, value_at_roots, num_iterations, _ = result\n\n    expected_roots = [-0.14823253013443427, -0.14823253013443677]\n\n    self.assertNotAllClose(roots[0], expected_roots[0])\n    self.assertAllClose(roots[1], expected_roots[1])\n\n    self.assertNotAllClose(value_at_roots[0], 0.)\n    self.assertAllClose(value_at_roots[0], objective_fn(roots[0]))\n    self.assertAllClose(value_at_roots[1], 0.)\n\n    self.assertAllEqual(num_iterations, expected_num_iterations)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testFindsRootForFlatFunction(self):\n    # Flat in the [-0.5, 0.5] range.\n    objective_fn = lambda x: 0 if x == 0 else x * exp(-1 / x**2)\n\n    left_bracket = [-10]\n    right_bracket = [1]\n    expected_num_iterations = [13]\n\n    expected_num_iterations, result = self.evaluate([\n        tf.constant(expected_num_iterations, dtype=tf.int32),\n        root_search.brentq(objective_fn,\n                           tf.constant(left_bracket, dtype=tf.float64),\n                           tf.constant(right_bracket, dtype=tf.float64))\n    ])\n\n    _, value_at_roots, num_iterations, _ = result\n\n    # Simply check that the objective function is close to the root for the\n    # returned estimate. Do not check the estimate itself.\n    # Unlike Brent\'s original algorithm (and the SciPy implementation), this\n    # implementation stops the search as soon as a good enough root estimate is\n    # found. As a result, the estimate may significantly differ from the one\n    # returned by SciPy for functions which are extremely flat around the root.\n    self.assertAllClose(value_at_roots, [0.])\n    self.assertAllEqual(num_iterations, expected_num_iterations)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWithNoIteration(self):\n    left_bracket = [-10, 1]\n    right_bracket = [10, -1]\n\n    first_guess = tf.constant(left_bracket, dtype=tf.float64)\n    second_guess = tf.constant(right_bracket, dtype=tf.float64)\n\n    # Skip iteration entirely.\n    # Should return a Tensor built from the best guesses in input positions.\n    guess, result = self.evaluate([\n        tf.constant([-10, -1], dtype=tf.float64),\n        root_search.brentq(\n            polynomial5, first_guess, second_guess, max_iterations=0)\n    ])\n\n    self.assertAllEqual(result.estimated_root, guess)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWithValueAtPositionssOfSameSign(self):\n    f = lambda x: x**2\n\n    # Should fail: The objective function has the same sign at both positions.\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      self.evaluate(\n          root_search.brentq(\n              f,\n              tf.constant(-1, dtype=tf.float64),\n              tf.constant(1, dtype=tf.float64),\n              validate_args=True))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWithInvalidAbsoluteRootTolerance(self):\n    f = lambda x: x**3\n\n    # Should fail: Absolute root tolerance is negative.\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      self.evaluate(\n          root_search.brentq(\n              f,\n              tf.constant(-2, dtype=tf.float64),\n              tf.constant(2, dtype=tf.float64),\n              absolute_root_tolerance=-2e-7,\n              validate_args=True))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWithInvalidRelativeRootTolerance(self):\n    f = lambda x: x**3\n\n    # Should fail: Relative root tolerance is negative.\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      self.evaluate(\n          root_search.brentq(\n              f,\n              tf.constant(-1, dtype=tf.float64),\n              tf.constant(1, dtype=tf.float64),\n              relative_root_tolerance=-2e-7,\n              validate_args=True))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWithInvalidValueTolerance(self):\n    f = lambda x: x**3\n\n    # Should fail: Value tolerance is negative.\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      self.evaluate(\n          root_search.brentq(\n              f,\n              tf.constant(-1, dtype=tf.float64),\n              tf.constant(1, dtype=tf.float64),\n              function_tolerance=-2e-7,\n              validate_args=True))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWithInvalidMaxIterations(self):\n    f = lambda x: x**3\n\n    # Should fail: Maximum number of iterations is negative.\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      self.evaluate(\n          root_search.brentq(\n              f,\n              tf.constant(-1, dtype=tf.float64),\n              tf.constant(1, dtype=tf.float64),\n              max_iterations=-1,\n              validate_args=True))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/math/segment_ops.py,36,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Element wise ops acting on segments of arrays.""""""\n\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math import diff_ops\n\n\ndef segment_diff(x,\n                 segment_ids,\n                 order=1,\n                 exclusive=False,\n                 dtype=None,\n                 name=None):\n  """"""Computes difference of successive elements in a segment.\n\n  For a complete description of segment_* ops see documentation of\n  `tf.segment_max`. This op extends the `diff` functionality to segmented\n  inputs.\n\n  The behaviour of this op is the same as that of the op `diff` within each\n  segment. The result is effectively a concatenation of the results of `diff`\n  applied to each segment.\n\n  #### Example\n\n  ```python\n    x = tf.constant([2, 5, 1, 7, 9] + [32, 10, 12, 3] + [4, 8, 5])\n    segments = tf.constant([0, 0, 0, 0, 0] + [1, 1, 1, 1] + [2, 2, 2])\n    # First order diff. Expected result: [3, -4, 6, 2, -22, 2, -9, 4, -3]\n    dx1 = segment_diff(\n        x, segment_ids=segments, order=1, exclusive=True)\n    # Non-exclusive, second order diff.\n    # Expected result: [2, 5, -1, 2, 8, 32, 10, -20, -7, 4, 8, 1]\n    dx2 = segment_diff(\n        x, segment_ids=segments, order=2, exclusive=False)\n  ```\n\n  Args:\n    x: A rank 1 `Tensor` of any dtype for which arithmetic operations are\n      permitted.\n    segment_ids: A `Tensor`. Must be one of the following types: int32, int64. A\n      1-D tensor whose size is equal to the size of `x`. Values should be sorted\n      and can be repeated.\n    order: Positive Python int. The order of the difference to compute. `order =\n      1` corresponds to the difference between successive elements.\n      Default value: 1\n    exclusive: Python bool. See description above.\n      Default value: False\n    dtype: Optional `tf.Dtype`. If supplied, the dtype for `x` to use when\n      converting to `Tensor`.\n      Default value: None which maps to the default dtype inferred by TF.\n    name: Python `str` name prefixed to Ops created by this class.\n      Default value: None which is mapped to the default name \'segment_diff\'.\n\n  Returns:\n    diffs: A `Tensor` of the same dtype as `x`. Assuming that each segment is\n      of length greater than or equal to order, if `exclusive` is True,\n      then the size is `n-order*k` where `n` is the size of x,\n      `k` is the number of different segment ids supplied if `segment_ids` is\n      not None or 1 if `segment_ids` is None. If any of the segments is of\n      length less than the order, then the size is:\n      `n-sum(min(order, length(segment_j)), j)` where the sum is over segments.\n      If `exclusive` is False, then the size is `n`.\n  """"""\n  with tf.compat.v1.name_scope(name, default_name=\'segment_diff\', values=[x]):\n    x = tf.convert_to_tensor(x, dtype=dtype)\n    raw_diffs = diff_ops.diff(x, order=order, exclusive=exclusive)\n    if segment_ids is None:\n      return raw_diffs\n    # If segment ids are supplied, raw_diffs are incorrect at locations:\n    # p, p+1, ... min(p+order-1, m_p-1) where p is the index of the first\n    # element of a segment other than the very first segment (which is\n    # already correct). m_p is the segment length.\n    # Find positions where the segments begin.\n    has_segment_changed = tf.concat(\n        [[False], tf.not_equal(segment_ids[1:] - segment_ids[:-1], 0)], axis=0)\n    # Shape [k, 1]\n    segment_start_index = tf.cast(tf.where(has_segment_changed), dtype=tf.int32)\n    segment_end_index = tf.concat(\n        [tf.reshape(segment_start_index, [-1])[1:], [tf.size(segment_ids)]],\n        axis=0)\n    segment_end_index = tf.reshape(segment_end_index, [-1, 1])\n    # The indices of locations that need to be adjusted. This needs to be\n    # constructed in steps. First we generate p, p+1, ... p+order-1.\n    # Shape [num_segments-1, order]\n    fix_indices = (\n        segment_start_index + tf.range(order, dtype=segment_start_index.dtype))\n    in_bounds = tf.where(fix_indices < segment_end_index)\n    # Keep only the ones in bounds.\n    fix_indices = tf.reshape(tf.gather_nd(fix_indices, in_bounds), [-1, 1])\n\n    needs_fix = tf.scatter_nd(\n        fix_indices,\n        # Unfortunately, scatter_nd doesn\'t support bool on GPUs so we need to\n        # do ints here and then convert to bool.\n        tf.reshape(tf.ones_like(fix_indices, dtype=tf.int32), [-1]),\n        shape=tf.shape(x))\n    # If exclusive is False, then needs_fix means we need to replace the values\n    # in raw_diffs at those locations with the values in x.\n    needs_fix = tf.cast(needs_fix, dtype=tf.bool)\n    if not exclusive:\n      return tf.where(needs_fix, x, raw_diffs)\n\n    # If exclusive is True, we have to be more careful. The raw_diffs\n    # computation has removed the first \'order\' elements. After removing the\n    # corresponding elements from needs_fix, we use it to remove the elements\n    # from raw_diffs.\n    return tf.boolean_mask(raw_diffs, tf.logical_not(needs_fix[order:]))\n\n\ndef segment_cumsum(x, segment_ids, exclusive=False, dtype=None, name=None):\n  """"""Computes cumulative sum of elements in a segment.\n\n  For a complete description of segment_* ops see documentation of\n  `tf.segment_sum`. This op extends the `tf.math.cumsum` functionality to\n  segmented inputs.\n\n  The behaviour of this op is the same as that of the op `tf.math.cumsum` within\n  each segment. The result is effectively a concatenation of the results of\n  `tf.math.cumsum` applied to each segment with the same interpretation for the\n  argument `exclusive`.\n\n  #### Example\n\n  ```python\n    x = tf.constant([2, 5, 1, 7, 9] + [32, 10, 12, 3] + [4, 8, 5])\n    segments = tf.constant([0, 0, 0, 0, 0] + [1, 1, 1, 1] + [2, 2, 2])\n    # Inclusive cumulative sum.\n    # Expected result: [2, 7, 8, 15, 24, 32, 42, 54, 57, 4, 12, 17]\n    cumsum1 = segment_cumsum(\n        x, segment_ids=segments, exclusive=False)\n    # Exclusive cumsum.\n    # Expected result: [0, 2, 7, 8, 15, 0, 32, 42, 54, 0, 4, 12]\n    cumsum2 = segment_cumsum(\n        x, segment_ids=segments, exclusive=True)\n  ```\n\n  Args:\n    x: A rank 1 `Tensor` of any dtype for which arithmetic operations are\n      permitted.\n    segment_ids: A `Tensor`. Must be one of the following types: int32, int64. A\n      1-D tensor whose size is equal to the size of `x`. Values should be sorted\n      and can be repeated. Values must range from `0` to `num segments - 1`.\n    exclusive: Python bool. See description above.\n      Default value: False\n    dtype: Optional `tf.Dtype`. If supplied, the dtype for `x` to use when\n      converting to `Tensor`.\n      Default value: None which maps to the default dtype inferred by TF.\n    name: Python `str` name prefixed to Ops created by this class.\n      Default value: None which is mapped to the default name \'segment_cumsum\'.\n\n  Returns:\n    cumsums: A `Tensor` of the same dtype as `x`. Assuming that each segment is\n      of length greater than or equal to order, if `exclusive` is True,\n      then the size is `n-order*k` where `n` is the size of x,\n      `k` is the number of different segment ids supplied if `segment_ids` is\n      not None or 1 if `segment_ids` is None. If any of the segments is of\n      length less than the order, then the size is:\n      `n-sum(min(order, length(segment_j)), j)` where the sum is over segments.\n      If `exclusive` is False, then the size is `n`.\n  """"""\n  with tf.compat.v1.name_scope(name, default_name=\'segment_cumsum\', values=[x]):\n    x = tf.convert_to_tensor(x, dtype=dtype)\n    raw_cumsum = tf.math.cumsum(x, exclusive=exclusive)\n    if segment_ids is None:\n      return raw_cumsum\n    # It is quite tedious to do a vectorized version without a while loop so\n    # we skip that for now.\n    # TODO(b/137940928): Replace these ops with more efficient C++ kernels.\n    def scanner(accumulators, args):\n      cumsum, prev_segment, prev_value = accumulators\n      value, segment = args\n      if exclusive:\n        initial_value, inc_value = tf.zeros_like(value), cumsum + prev_value\n      else:\n        initial_value, inc_value = value, cumsum + value\n      next_cumsum = tf.where(\n          tf.equal(prev_segment, segment), inc_value, initial_value)\n      return next_cumsum, segment, value\n\n    return tf.scan(\n        scanner, (x, segment_ids),\n        initializer=(tf.zeros_like(x[0]), tf.zeros_like(segment_ids[0]) - 1,\n                     tf.zeros_like(x[0])))[0]\n\n\n__all__ = [\'segment_cumsum\', \'segment_diff\']\n'"
tf_quant_finance/math/segment_ops_test.py,20,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for math.segment_ops.py.""""""\n\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\nfrom tf_quant_finance.math import segment_ops\n\n\nclass SegmentOpsTest(tf.test.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_segment_diffs_no_segment_exclusive(self):\n    x = tf.constant([11, 13, 17, 19, 23])\n    dx1 = self.evaluate(\n        segment_ops.segment_diff(x, segment_ids=None, order=1, exclusive=True))\n    np.testing.assert_array_equal(dx1, [2, 4, 2, 4])\n    dx2 = self.evaluate(\n        segment_ops.segment_diff(x, segment_ids=None, order=2, exclusive=True))\n    np.testing.assert_array_equal(dx2, [6, 6, 6])\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_segment_diffs_no_segment_inclusive(self):\n    x = tf.constant([11, 13, 17, 19, 23])\n    dx1 = self.evaluate(\n        segment_ops.segment_diff(x, segment_ids=None, order=1, exclusive=False))\n    np.testing.assert_array_equal(dx1, [11, 2, 4, 2, 4])\n\n    dx2 = self.evaluate(\n        segment_ops.segment_diff(x, segment_ids=None, order=2, exclusive=False))\n    np.testing.assert_array_equal(dx2, [11, 13, 6, 6, 6])\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_segment_diffs_segment_exclusive(self):\n    x = tf.constant([2, 5, 1, 7, 9] + [32, 10, 12, 3] + [4, 8, 5])\n    segments = tf.constant([0, 0, 0, 0, 0] + [1, 1, 1, 1] + [2, 2, 2])\n    dx1 = self.evaluate(\n        segment_ops.segment_diff(\n            x, segment_ids=segments, order=1, exclusive=True))\n    np.testing.assert_array_equal(dx1, ([3, -4, 6, 2] + [-22, 2, -9] + [4, -3]))\n\n    dx2 = self.evaluate(\n        segment_ops.segment_diff(\n            x, segment_ids=segments, order=2, exclusive=True))\n    np.testing.assert_array_equal(dx2, ([-1, 2, 8] + [-20, -7] + [1]))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_segment_diffs_segment_inclusive(self):\n    x = tf.constant([2, 5, 1, 7, 9] + [32, 10, 12, 3] + [4, 8, 5])\n    segments = tf.constant([0, 0, 0, 0, 0] + [1, 1, 1, 1] + [2, 2, 2])\n    dx1 = self.evaluate(\n        segment_ops.segment_diff(\n            x, segment_ids=segments, order=1, exclusive=False))\n    np.testing.assert_array_equal(\n        dx1, ([2, 3, -4, 6, 2] + [32, -22, 2, -9] + [4, 4, -3]))\n\n    dx2 = self.evaluate(\n        segment_ops.segment_diff(\n            x, segment_ids=segments, order=2, exclusive=False))\n    np.testing.assert_array_equal(\n        dx2, ([2, 5, -1, 2, 8] + [32, 10, -20, -7] + [4, 8, 1]))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_segment_diffs_large_order(self):\n    x = tf.constant([2, 5, 1, 7, 9] + [32, 10, 12, 3] + [4, 8, 5])\n    segments = tf.constant([0, 0, 0, 0, 0] + [1, 1, 1, 1] + [2, 2, 2])\n    dx1 = self.evaluate(\n        segment_ops.segment_diff(\n            x, segment_ids=segments, order=4, exclusive=False))\n    np.testing.assert_array_equal(\n        dx1, ([2, 5, 1, 7, 7] + [32, 10, 12, 3] + [4, 8, 5]))\n\n    dx2 = self.evaluate(\n        segment_ops.segment_diff(\n            x, segment_ids=segments, order=4, exclusive=True))\n    np.testing.assert_array_equal(\n        dx2, ([7] + [] + []))  # The empty arrays are for the segments 1 and 2.\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_segment_diffs_small_segment(self):\n    x = tf.constant([2, 5, 1, 7] + [9] + [32, 10, 12, 3] + [4, 8, 5])\n    segments = tf.constant([0, 0, 0, 0] + [1] + [2, 2, 2, 2] + [3, 3, 3])\n    dx1 = self.evaluate(\n        segment_ops.segment_diff(\n            x, segment_ids=segments, order=2, exclusive=False))\n    np.testing.assert_array_equal(\n        dx1, ([2, 5, -1, 2] + [9] + [32, 10, -20, -7] + [4, 8, 1]))\n\n    dx2 = self.evaluate(\n        segment_ops.segment_diff(\n            x, segment_ids=segments, order=2, exclusive=True))\n    np.testing.assert_array_equal(dx2, ([-1, 2] + [] + [-20, -7] + [1]))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_segment_cumsum_no_segment_exclusive(self):\n    x = tf.constant([-11, 13, 17, 19, 23])\n    cx = self.evaluate(\n        segment_ops.segment_cumsum(x, segment_ids=None, exclusive=True))\n    np.testing.assert_array_equal(cx, [0, -11, 2, 19, 38])\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_segment_cumsum_no_segment_inclusive(self):\n    x = tf.constant([-11, 13, 17, 19, 23])\n    cx = self.evaluate(\n        segment_ops.segment_cumsum(x, segment_ids=None, exclusive=False))\n    np.testing.assert_array_equal(cx, [-11, 2, 19, 38, 61])\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_segment_cumsum_segment_exclusive(self):\n    x = tf.constant([2, 5, 1, 7, 9] + [32, 10, 12, 3] + [4, 8, 5])\n    segments = tf.constant([0, 0, 0, 0, 0] + [1, 1, 1, 1] + [2, 2, 2])\n    cx = self.evaluate(\n        segment_ops.segment_cumsum(x, segment_ids=segments, exclusive=True))\n    np.testing.assert_array_equal(cx, [0, 2, 7, 8, 15, 0, 32, 42, 54, 0, 4, 12])\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_segment_cumsum_segment_inclusive(self):\n    x = tf.constant([2, 5, 1, 7, 9] + [32, 10, 12, 3] + [4, 8, 5])\n    segments = tf.constant([0, 0, 0, 0, 0] + [1, 1, 1, 1] + [2, 2, 2])\n    dx1 = self.evaluate(\n        segment_ops.segment_cumsum(x, segment_ids=segments, exclusive=False))\n    np.testing.assert_array_equal(dx1,\n                                  [2, 7, 8, 15, 24, 32, 42, 54, 57, 4, 12, 17])\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_segment_cumsum_small_segment(self):\n    x = tf.constant([2, 5, 1, 7] + [9] + [32, 10, 12, 3] + [4, 8, 5])\n    segments = tf.constant([0, 0, 0, 0] + [1] + [2, 2, 2, 2] + [3, 3, 3])\n    cx1 = self.evaluate(\n        segment_ops.segment_cumsum(x, segment_ids=segments, exclusive=False))\n    np.testing.assert_array_equal(\n        cx1, ([2, 7, 8, 15] + [9] + [32, 42, 54, 57] + [4, 12, 17]))\n\n    cx2 = self.evaluate(\n        segment_ops.segment_cumsum(x, segment_ids=segments, exclusive=True))\n    np.testing.assert_array_equal(\n        cx2, ([0, 2, 7, 8] + [0] + [0, 32, 42, 54] + [0, 4, 12]))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/models/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""TensorFlow Quantitative Finance tools to build Diffusion Models.""""""\n\nfrom tf_quant_finance.models import euler_sampling\nfrom tf_quant_finance.models import heston\nfrom tf_quant_finance.models import hull_white\nfrom tf_quant_finance.models.generic_ito_process import GenericItoProcess\nfrom tf_quant_finance.models.geometric_brownian_motion.multivariate_geometric_brownian_motion import MultivariateGeometricBrownianMotion\nfrom tf_quant_finance.models.geometric_brownian_motion.univariate_geometric_brownian_motion import GeometricBrownianMotion\nfrom tf_quant_finance.models.heston import HestonModel\nfrom tf_quant_finance.models.ito_process import ItoProcess\nfrom tf_quant_finance.models.joined_ito_process import JoinedItoProcess\n\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'euler_sampling\',\n    \'heston\',\n    \'HestonModel\',\n    \'hull_white\',\n    \'GenericItoProcess\',\n    \'MultivariateGeometricBrownianMotion\',\n    \'GeometricBrownianMotion\',\n    \'ItoProcess\',\n    \'JoinedItoProcess\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/models/euler_sampling.py,35,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""The Euler sampling method for ito processes.""""""\n\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance.math import custom_loops\nfrom tf_quant_finance.math import random_ops as random\nfrom tf_quant_finance.models import utils\n\n\ndef sample(dim,\n           drift_fn,\n           volatility_fn,\n           times,\n           time_step,\n           num_samples=1,\n           initial_state=None,\n           random_type=None,\n           seed=None,\n           swap_memory=True,\n           skip=0,\n           precompute_normal_draws=True,\n           watch_params=None,\n           dtype=None,\n           name=None):\n  """"""Returns a sample paths from the process using Euler method.\n\n  For an Ito process,\n\n  ```\n    dX = a(t, X_t) dt + b(t, X_t) dW_t\n  ```\n  with given drift `a` and volatility `b` functions Euler method generates a\n  sequence {X_n} as\n\n  ```\n  X_{n+1} = X_n + a(t_n, X_n) dt + b(t_n, X_n) (N(0, t_{n+1}) - N(0, t_n)),\n  ```\n  where `dt = t_{n+1} - t_n` and `N` is a sample from the Normal distribution.\n  See [1] for details.\n\n  #### References\n  [1]: Wikipedia. Euler-Maruyama method:\n  https://en.wikipedia.org/wiki/Euler-Maruyama_method\n\n  Args:\n    dim: Python int greater than or equal to 1. The dimension of the Ito\n      Process.\n    drift_fn: A Python callable to compute the drift of the process. The\n      callable should accept two real `Tensor` arguments of the same dtype.\n      The first argument is the scalar time t, the second argument is the\n      value of Ito process X - tensor of shape `batch_shape + [dim]`.\n      The result is value of drift a(t, X). The return value of the callable\n      is a real `Tensor` of the same dtype as the input arguments and of shape\n      `batch_shape + [dim]`.\n    volatility_fn: A Python callable to compute the volatility of the process.\n      The callable should accept two real `Tensor` arguments of the same dtype\n      and shape `times_shape`. The first argument is the scalar time t, the\n      second argument is the value of Ito process X - tensor of shape\n      `batch_shape + [dim]`. The result is value of drift b(t, X). The return\n      value of the callable is a real `Tensor` of the same dtype as the input\n      arguments and of shape `batch_shape + [dim, dim]`.\n    times: Rank 1 `Tensor` of increasing positive real values. The times at\n      which the path points are to be evaluated.\n    time_step: Scalar real `Tensor` - maximal distance between points\n        in grid in Euler schema.\n    num_samples: Positive scalar `int`. The number of paths to draw.\n      Default value: 1.\n    initial_state: `Tensor` of shape `[dim]`. The initial state of the\n      process.\n      Default value: None which maps to a zero initial state.\n    random_type: Enum value of `RandomType`. The type of (quasi)-random\n      number generator to use to generate the paths.\n      Default value: None which maps to the standard pseudo-random numbers.\n    seed: Seed for the random number generator. The seed is\n      only relevant if `random_type` is one of\n      `[STATELESS, PSEUDO, HALTON_RANDOMIZED, PSEUDO_ANTITHETIC,\n        STATELESS_ANTITHETIC]`. For `PSEUDO`, `PSEUDO_ANTITHETIC` and\n      `HALTON_RANDOMIZED` the seed should be a Python integer. For\n      `STATELESS` and  `STATELESS_ANTITHETIC `must be supplied as an integer\n      `Tensor` of shape `[2]`.\n      Default value: `None` which means no seed is set.\n    swap_memory: A Python bool. Whether GPU-CPU memory swap is enabled for this\n      op. See an equivalent flag in `tf.while_loop` documentation for more\n      details. Useful when computing a gradient of the op since `tf.while_loop`\n      is used to propagate stochastic process in time.\n      Default value: True.\n    skip: `int32` 0-d `Tensor`. The number of initial points of the Sobol or\n      Halton sequence to skip. Used only when `random_type` is \'SOBOL\',\n      \'HALTON\', or \'HALTON_RANDOMIZED\', otherwise ignored.\n      Default value: `0`.\n    precompute_normal_draws: Python bool. Indicates whether the noise increments\n      `N(0, t_{n+1}) - N(0, t_n)` are precomputed. For `HALTON` and `SOBOL`\n      random types the increments are always precomputed. While the resulting\n      graph consumes more memory, the performance gains might be significant.\n      Default value: `True`.\n    watch_params: An optional list of zero-dimensional `Tensor`s of the same\n      `dtype` as `initial_state`. If provided, specifies `Tensor`s with respect\n      to which the differentiation of the sampling function will happen.\n      A more efficient algorithm is used when `watch_params` are specified.\n      Note the the function becomes differentiable onlhy wrt to these `Tensor`s\n      and the `initial_state`. The gradient wrt any other `Tensor` is set to be\n      zero.\n    dtype: `tf.Dtype`. If supplied the dtype for the input and output `Tensor`s.\n      Default value: None which means that the dtype implied by `times` is\n      used.\n    name: Python string. The name to give this op.\n      Default value: `None` which maps to `euler_sample`.\n\n  Returns:\n   A real `Tensor` of shape [num_samples, k, n] where `k` is the size of the\n      `times`, `n` is the dimension of the process.\n\n  Raises:\n    ValueError: If `time_step` or `times` have a non-constant value (e.g.,\n      values are random), and `random_type` is `SOBOL`. This will be fixed with\n      the release of TensorFlow 2.2.\n  """"""\n  name = name or \'euler_sample\'\n  with tf.name_scope(name):\n    times = tf.convert_to_tensor(times, dtype=dtype)\n    if dtype is None:\n      dtype = times.dtype\n    if initial_state is None:\n      initial_state = tf.zeros(dim, dtype=dtype)\n    initial_state = tf.convert_to_tensor(initial_state, dtype=dtype,\n                                         name=\'initial_state\')\n    num_requested_times = times.shape.as_list()[0]\n    # Create a time grid for the Euler scheme.\n    times, keep_mask, time_indices = _prepare_grid(\n        times=times, time_step=time_step, dtype=dtype)\n    if watch_params is not None:\n      watch_params = [tf.convert_to_tensor(param, dtype=dtype)\n                      for param in watch_params]\n    return _sample(\n        dim=dim,\n        drift_fn=drift_fn,\n        volatility_fn=volatility_fn,\n        times=times,\n        time_step=time_step,\n        keep_mask=keep_mask,\n        num_requested_times=num_requested_times,\n        num_samples=num_samples,\n        initial_state=initial_state,\n        random_type=random_type,\n        seed=seed,\n        swap_memory=swap_memory,\n        skip=skip,\n        precompute_normal_draws=precompute_normal_draws,\n        watch_params=watch_params,\n        time_indices=time_indices,\n        dtype=dtype)\n\n\ndef _sample(*, dim, drift_fn, volatility_fn, times, time_step, keep_mask,\n            num_requested_times,\n            num_samples, initial_state, random_type,\n            seed, swap_memory, skip, precompute_normal_draws,\n            watch_params,\n            time_indices,\n            dtype):\n  """"""Returns a sample of paths from the process using Euler method.""""""\n  dt = times[1:] - times[:-1]\n  sqrt_dt = tf.sqrt(dt)\n  current_state = initial_state + tf.zeros([num_samples, dim],\n                                           dtype=initial_state.dtype)\n  if dt.shape.is_fully_defined():\n    steps_num = dt.shape.as_list()[-1]\n  else:\n    steps_num = tf.shape(dt)[-1]\n  # In order to use low-discrepancy random_type we need to generate the sequence\n  # of independent random normals upfront. We also precompute random numbers\n  # for stateless random type in order to ensure independent samples for\n  # multiple function calls whith different seeds.\n  if precompute_normal_draws or random_type in (\n      random.RandomType.SOBOL,\n      random.RandomType.HALTON,\n      random.RandomType.HALTON_RANDOMIZED,\n      random.RandomType.STATELESS,\n      random.RandomType.STATELESS_ANTITHETIC):\n    normal_draws = utils.generate_mc_normal_draws(\n        num_normal_draws=dim, num_time_steps=steps_num,\n        num_sample_paths=num_samples, random_type=random_type,\n        dtype=dtype, seed=seed, skip=skip)\n    wiener_mean = None\n  else:\n    # If pseudo or anthithetic sampling is used, proceed with random sampling\n    # at each step.\n    wiener_mean = tf.zeros((dim,), dtype=dtype, name=\'wiener_mean\')\n    normal_draws = None\n  if watch_params is None:\n    # Use while_loop if `watch_params` is not passed\n    return  _while_loop(\n        dim=dim, steps_num=steps_num,\n        current_state=current_state,\n        drift_fn=drift_fn, volatility_fn=volatility_fn, wiener_mean=wiener_mean,\n        num_samples=num_samples, times=times,\n        dt=dt, sqrt_dt=sqrt_dt, time_step=time_step, keep_mask=keep_mask,\n        num_requested_times=num_requested_times,\n        swap_memory=swap_memory,\n        random_type=random_type, seed=seed, normal_draws=normal_draws)\n  else:\n    # Use custom for_loop if `watch_params` is specified\n    return _for_loop(\n        steps_num=steps_num, current_state=current_state,\n        drift_fn=drift_fn, volatility_fn=volatility_fn, wiener_mean=wiener_mean,\n        num_samples=num_samples, times=times,\n        dt=dt, sqrt_dt=sqrt_dt, time_indices=time_indices,\n        keep_mask=keep_mask, watch_params=watch_params,\n        random_type=random_type, seed=seed, normal_draws=normal_draws)\n\n\ndef _while_loop(*, dim, steps_num, current_state,\n                drift_fn, volatility_fn, wiener_mean,\n                num_samples, times, dt, sqrt_dt, time_step, num_requested_times,\n                keep_mask, swap_memory, random_type, seed, normal_draws):\n  """"""Smaple paths using tf.while_loop.""""""\n  cond_fn = lambda i, *args: i < steps_num\n  def step_fn(i, written_count, current_state, result):\n    return _euler_step(\n        i=i,\n        written_count=written_count,\n        current_state=current_state,\n        result=result,\n        drift_fn=drift_fn,\n        volatility_fn=volatility_fn,\n        wiener_mean=wiener_mean,\n        num_samples=num_samples,\n        times=times,\n        dt=dt,\n        sqrt_dt=sqrt_dt,\n        keep_mask=keep_mask,\n        random_type=random_type,\n        seed=seed,\n        normal_draws=normal_draws)\n  maximum_iterations = (tf.cast(1. / time_step, dtype=tf.int32)\n                        + tf.size(times))\n  result = tf.zeros((num_samples, num_requested_times, dim),\n                    dtype=current_state.dtype)\n  _, _, _, result = tf.while_loop(\n      cond_fn, step_fn, (0, 0, current_state, result),\n      maximum_iterations=maximum_iterations,\n      swap_memory=swap_memory)\n  return result\n\n\ndef _for_loop(*, steps_num, current_state,\n              drift_fn, volatility_fn, wiener_mean, watch_params,\n              num_samples, times, dt, sqrt_dt, time_indices,\n              keep_mask, random_type, seed, normal_draws):\n  """"""Smaple paths using custom for_loop.""""""\n  num_time_points = time_indices.shape.as_list()[-1]\n  if num_time_points == 1:\n    iter_nums = steps_num\n  else:\n    iter_nums = time_indices\n  def step_fn(i, current_state):\n    # Unpack current_state\n    current_state = current_state[0]\n    _, _, next_state, _ = _euler_step(\n        i=i,\n        written_count=0,\n        current_state=current_state,\n        result=tf.expand_dims(current_state, axis=1),\n        drift_fn=drift_fn,\n        volatility_fn=volatility_fn,\n        wiener_mean=wiener_mean,\n        num_samples=num_samples,\n        times=times,\n        dt=dt,\n        sqrt_dt=sqrt_dt,\n        keep_mask=keep_mask,\n        random_type=random_type,\n        seed=seed,\n        normal_draws=normal_draws)\n    return [next_state]\n  result = custom_loops.for_loop(\n      body_fn=step_fn,\n      initial_state=[current_state],\n      params=watch_params,\n      num_iterations=iter_nums)[0]\n  if num_time_points == 1:\n    return tf.expand_dims(result, axis=1)\n  return tf.transpose(result, (1, 0, 2))\n\n\ndef _euler_step(*, i, written_count, current_state, result,\n                drift_fn, volatility_fn, wiener_mean,\n                num_samples, times, dt, sqrt_dt, keep_mask,\n                random_type, seed, normal_draws):\n  """"""Performs one step of Euler scheme.""""""\n  current_time = times[i + 1]\n  written_count = tf.cast(written_count, tf.int32)\n  if normal_draws is not None:\n    dw = normal_draws[i]\n  else:\n    dw = random.mv_normal_sample(\n        (num_samples,), mean=wiener_mean, random_type=random_type,\n        seed=seed)\n  dw = dw * sqrt_dt[i]\n  dt_inc = dt[i] * drift_fn(current_time, current_state)  # pylint: disable=not-callable\n  dw_inc = tf.linalg.matvec(volatility_fn(current_time, current_state), dw)  # pylint: disable=not-callable\n  next_state = current_state + dt_inc + dw_inc\n  result = utils.maybe_update_along_axis(\n      tensor=result,\n      do_update=keep_mask[i + 1],\n      ind=written_count,\n      axis=1,\n      new_tensor=tf.expand_dims(next_state, axis=1))\n  written_count += tf.cast(keep_mask[i + 1], dtype=tf.int32)\n  return i + 1, written_count, next_state, result\n\n\ndef _prepare_grid(*, times, time_step, dtype):\n  """"""Prepares grid of times for path generation.\n\n  Args:\n    times:  Rank 1 `Tensor` of increasing positive real values. The times at\n      which the path points are to be evaluated.\n    time_step: Rank 0 real `Tensor`. Maximal distance between points in\n      resulting grid.\n    dtype: `tf.Dtype` of the input and output `Tensor`s.\n\n  Returns:\n    Tuple `(all_times, mask, time_points)`.\n    `all_times` is a 1-D real `Tensor` containing all points from \'times` and\n    the uniform grid of points between `[0, times[-1]]` with grid size equal to\n    `time_step`. The `Tensor` is sorted in ascending order and may contain\n    duplicates.\n    `mask` is a boolean 1-D `Tensor` of the same shape as \'all_times\', showing\n    which elements of \'all_times\' correspond to THE values from `times`.\n    Guarantees that times[0]=0 and mask[0]=False.\n    `time_indices`. An integer `Tensor` of the same shape as `times` indicating\n    `times` indices in `all_times`.\n  """"""\n  grid = tf.range(0.0, times[-1], time_step, dtype=dtype)\n  all_times = tf.concat([grid, times], axis=0)\n  mask = tf.concat([\n      tf.zeros_like(grid, dtype=tf.bool),\n      tf.ones_like(times, dtype=tf.bool)\n  ],\n                   axis=0)\n  perm = tf.argsort(all_times, stable=True)\n  all_times = tf.gather(all_times, perm)\n  # Remove duplicate points\n  all_times = tf.unique(all_times).y\n  time_indices = tf.searchsorted(all_times, times)\n  mask = tf.gather(mask, perm)\n  return all_times, mask, time_indices\n\n\n__all__ = [\'sample\']\n\n'"
tf_quant_finance/models/euler_sampling_test.py,19,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for methods in `euler_sampling`.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\neuler_sampling = tff.models.euler_sampling\nrandom = tff.math.random\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass EulerSamplingTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'CustomForLoop\',\n          \'watch_params\': True,\n      }, {\n          \'testcase_name\': \'WhileLoop\',\n          \'watch_params\': False,\n      })\n  def test_sample_paths_wiener(self, watch_params):\n    """"""Tests paths properties for Wiener process (dX = dW).""""""\n\n    def drift_fn(_, x):\n      return tf.zeros_like(x)\n\n    def vol_fn(_, x):\n      return tf.expand_dims(tf.ones_like(x), -1)\n\n    times = np.array([0.1, 0.2, 0.3])\n    num_samples = 10000\n    if watch_params:\n      watch_params = []\n    else:\n      watch_params = None\n    paths = euler_sampling.sample(\n        dim=1, drift_fn=drift_fn, volatility_fn=vol_fn,\n        times=times, num_samples=num_samples, seed=42, time_step=0.005,\n        watch_params=watch_params)\n    self.assertAllEqual(paths.shape.as_list(), [num_samples, 3, 1])\n    paths = self.evaluate(paths)\n    means = np.mean(paths, axis=0).reshape([-1])\n    covars = np.cov(paths.reshape([num_samples, -1]), rowvar=False)\n    expected_means = np.zeros((3,))\n    expected_covars = np.minimum(times.reshape([-1, 1]), times.reshape([1, -1]))\n    self.assertAllClose(means, expected_means, rtol=1e-2, atol=1e-2)\n    self.assertAllClose(covars, expected_covars, rtol=1e-2, atol=1e-2)\n\n  def test_sample_paths_1d(self):\n    """"""Tests path properties for 1-dimentional Ito process.\n\n    We construct the following Ito process.\n\n    ````\n    dX = mu * sqrt(t) * dt + (a * t + b) dW\n    ````\n\n    For this process expected value at time t is x_0 + 2/3 * mu * t^1.5 .\n    """"""\n    mu = 0.2\n    a = 0.4\n    b = 0.33\n\n    def drift_fn(t, x):\n      return mu * tf.sqrt(t) * tf.ones_like(x, dtype=t.dtype)\n\n    def vol_fn(t, x):\n      del x\n      return (a * t + b) * tf.ones([1, 1], dtype=t.dtype)\n\n    times = np.array([0.1, 0.21, 0.32, 0.43, 0.55])\n    num_samples = 10000\n    x0 = np.array([0.1])\n    paths = self.evaluate(\n        euler_sampling.sample(\n            dim=1,\n            drift_fn=drift_fn, volatility_fn=vol_fn,\n            times=times, num_samples=num_samples, initial_state=x0,\n            time_step=0.01, seed=12134))\n\n    self.assertAllClose(paths.shape, (num_samples, 5, 1), atol=0)\n    means = np.mean(paths, axis=0).reshape(-1)\n    expected_means = x0 + (2.0 / 3.0) * mu * np.power(times, 1.5)\n    self.assertAllClose(means, expected_means, rtol=1e-2, atol=1e-2)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'PSEUDO\',\n          \'random_type\': tff.math.random.RandomType.PSEUDO,\n          \'seed\': 12134,\n      }, {\n          \'testcase_name\': \'STATELESS\',\n          \'random_type\': tff.math.random.RandomType.STATELESS,\n          \'seed\': [1, 2],\n      }, {\n          \'testcase_name\': \'SOBOL\',\n          \'random_type\': tff.math.random.RandomType.SOBOL,\n          \'seed\': None,\n      }, {\n          \'testcase_name\': \'HALTON_RANDOMIZED\',\n          \'random_type\': tff.math.random.RandomType.HALTON_RANDOMIZED,\n          \'seed\': 12134,\n      })\n  def test_sample_paths_2d(self, random_type, seed):\n    """"""Tests path properties for 2-dimentional Ito process.\n\n    We construct the following Ito processes.\n\n    dX_1 = mu_1 sqrt(t) dt + s11 dW_1 + s12 dW_2\n    dX_2 = mu_2 sqrt(t) dt + s21 dW_1 + s22 dW_2\n\n    mu_1, mu_2 are constants.\n    s_ij = a_ij t + b_ij\n\n    For this process expected value at time t is (x_0)_i + 2/3 * mu_i * t^1.5.\n\n    Args:\n      random_type: Random number type defined by tff.math.random.RandomType\n        enum.\n      seed: Random seed.\n    """"""\n    mu = np.array([0.2, 0.7])\n    a = np.array([[0.4, 0.1], [0.3, 0.2]])\n    b = np.array([[0.33, -0.03], [0.21, 0.5]])\n\n    def drift_fn(t, x):\n      return mu * tf.sqrt(t) * tf.ones_like(x, dtype=t.dtype)\n\n    def vol_fn(t, x):\n      del x\n      return (a * t + b) * tf.ones([2, 2], dtype=t.dtype)\n\n    num_samples = 10000\n    times = np.array([0.1, 0.21, 0.32, 0.43, 0.55])\n    x0 = np.array([0.1, -1.1])\n    paths = self.evaluate(\n        euler_sampling.sample(\n            dim=2,\n            drift_fn=drift_fn, volatility_fn=vol_fn,\n            times=times,\n            num_samples=num_samples,\n            initial_state=x0,\n            time_step=0.01,\n            random_type=random_type,\n            seed=seed))\n\n    self.assertAllClose(paths.shape, (num_samples, 5, 2), atol=0)\n    means = np.mean(paths, axis=0)\n    times = np.reshape(times, [-1, 1])\n    expected_means = x0 + (2.0 / 3.0) * mu * np.power(times, 1.5)\n    self.assertAllClose(means, expected_means, rtol=1e-2, atol=1e-2)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'CustomForLoop\',\n          \'watch_params\': True,\n      }, {\n          \'testcase_name\': \'WhileLoop\',\n          \'watch_params\': False,\n      })\n  def test_halton_sample_paths_2d(self, watch_params):\n    """"""Tests path properties for 2-dimentional Ito process.""""""\n    # We construct the following Ito processes.\n    # dX_1 = mu_1 sqrt(t) dt + s11 dW_1 + s12 dW_2\n    # dX_2 = mu_2 sqrt(t) dt + s21 dW_1 + s22 dW_2\n    # mu_1, mu_2 are constants.\n    # s_ij = a_ij t + b_ij\n    # For this process expected value at time t is (x_0)_i + 2/3 * mu_i * t^1.5.\n    dtype = tf.float64\n    num_samples = 50000\n    times = np.array([0.1, 0.21, 0.32])\n    x0 = np.array([0.1, -1.1])\n    mu = np.array([0.2, 0.7])\n    a = np.array([[0.4, 0.1], [0.3, 0.2]])\n    b = np.array([[0.33, -0.03], [0.21, 0.5]])\n    def sample_fn(mu, a, b):\n      mu = tf.convert_to_tensor(mu, dtype=dtype)\n      a = tf.convert_to_tensor(a, dtype=dtype)\n      b = tf.convert_to_tensor(b, dtype=dtype)\n      def drift_fn(t, x):\n        return mu * tf.sqrt(t) * tf.ones_like(x, dtype=t.dtype)\n\n      def vol_fn(t, x):\n        del x\n        return (a * t + b) * tf.ones([2, 2], dtype=t.dtype)\n      if watch_params:\n        watch_params_tf = [a, b]\n      else:\n        watch_params_tf = None\n      return euler_sampling.sample(\n          dim=2,\n          drift_fn=drift_fn, volatility_fn=vol_fn,\n          times=times,\n          num_samples=num_samples,\n          initial_state=x0,\n          random_type=tff.math.random.RandomType.HALTON,\n          time_step=0.01,\n          seed=12134,\n          skip=100,\n          watch_params=watch_params_tf,\n          dtype=dtype)\n\n    paths = self.evaluate(tf.function(sample_fn)(mu, a, b))\n\n    self.assertAllClose(paths.shape, (num_samples, 3, 2), atol=0)\n    means = np.mean(paths, axis=0)\n    times = np.reshape(times, [-1, 1])\n    expected_means = x0 + (2.0 / 3.0) * mu * np.power(times, 1.5)\n    self.assertAllClose(means, expected_means, rtol=1e-2, atol=1e-2)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'PSEUDO_ANTITHETIC\',\n          \'random_type\': tff.math.random.RandomType.PSEUDO,\n          \'seed\': 12134,\n      }, {\n          \'testcase_name\': \'STATELESS_ANTITHETIC\',\n          \'random_type\': tff.math.random.RandomType.STATELESS,\n          \'seed\': [0, 12134],\n      })\n  def test_antithetic_sample_paths_mean_2d(self, random_type, seed):\n    """"""Tests path properties for 2-dimentional anthithetic variates method.\n\n    The same test as above but with `PSEUDO_ANTITHETIC` random type.\n    We construct the following Ito processes.\n\n    dX_1 = mu_1 sqrt(t) dt + s11 dW_1 + s12 dW_2\n    dX_2 = mu_2 sqrt(t) dt + s21 dW_1 + s22 dW_2\n\n    mu_1, mu_2 are constants.\n    s_ij = a_ij t + b_ij\n\n    For this process expected value at time t is (x_0)_i + 2/3 * mu_i * t^1.5.\n\n    Args:\n      random_type: Random number type defined by tff.math.random.RandomType\n        enum.\n      seed: Random seed.\n    """"""\n    mu = np.array([0.2, 0.7])\n    a = np.array([[0.4, 0.1], [0.3, 0.2]])\n    b = np.array([[0.33, -0.03], [0.21, 0.5]])\n\n    def drift_fn(t, x):\n      del x\n      return mu * tf.sqrt(t)\n\n    def vol_fn(t, x):\n      del x\n      return (a * t + b) * tf.ones([2, 2], dtype=t.dtype)\n\n    times = np.array([0.1, 0.21, 0.32, 0.43, 0.55])\n    num_samples = 5000\n    x0 = np.array([0.1, -1.1])\n    paths = self.evaluate(\n        euler_sampling.sample(\n            dim=2,\n            drift_fn=drift_fn, volatility_fn=vol_fn,\n            times=times,\n            num_samples=num_samples,\n            initial_state=x0,\n            random_type=random_type,\n            time_step=0.01,\n            seed=seed))\n\n    self.assertAllClose(paths.shape, (num_samples, 5, 2), atol=0)\n    means = np.mean(paths, axis=0)\n    times = np.reshape(times, [-1, 1])\n    expected_means = x0 + (2.0 / 3.0) * mu * np.power(times, 1.5)\n    # Antithetic variates method produces better estimate than the\n    # estimate with the `PSEUDO` random type\n    self.assertAllClose(means, expected_means, rtol=5e-3, atol=5e-3)\n\n  def test_sample_paths_dtypes(self):\n    """"""Sampled paths have the expected dtypes.""""""\n    for dtype in [np.float32, np.float64]:\n      drift_fn = lambda t, x: tf.sqrt(t) * tf.ones_like(x, dtype=t.dtype)\n      vol_fn = lambda t, x: t * tf.ones([1, 1], dtype=t.dtype)\n\n      paths = self.evaluate(\n          euler_sampling.sample(\n              dim=1,\n              drift_fn=drift_fn, volatility_fn=vol_fn,\n              times=[0.1, 0.2],\n              num_samples=10,\n              initial_state=[0.1],\n              time_step=0.01,\n              seed=123,\n              dtype=dtype))\n\n      self.assertEqual(paths.dtype, dtype)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/models/generic_ito_process.py,11,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Defines class to describe any Ito processes.\n\nUses Euler scheme for sampling and ADI scheme for solving the associated\nFeynman-Kac equation.\n""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math.pde import fd_solvers\nfrom tf_quant_finance.models import euler_sampling\nfrom tf_quant_finance.models import ito_process\n\n\nclass GenericItoProcess(ito_process.ItoProcess):\n  """"""Generic Ito process defined from a drift and volatility function.""""""\n\n  def __init__(self, dim, drift_fn, volatility_fn, dtype=None, name=None):\n    """"""Initializes the Ito process with given drift and volatility functions.\n\n    Represents a general Ito process:\n\n    ```None\n      dX_i = a_i(t, X) dt + Sum(S_{ij}(t, X) dW_j for 1 <= j <= n), 1 <= i <= n\n    ```\n\n    The vector coefficient `a_i` is referred to as the drift of the process and\n    the matrix `b_{ij}` as the volatility of the process. For the process to be\n    well defined, these coefficients need to satisfy certain technical\n    conditions which may be found in Ref. [1]. The vector `dW_j` represents\n    independent Brownian increments.\n\n    #### Example. Sampling from 2-dimensional Ito process of the form:\n\n    ```none\n    dX_1 = mu_1 * sqrt(t) dt + s11 * dW_1 + s12 * dW_2\n    dX_2 = mu_2 * sqrt(t) dt + s21 * dW_1 + s22 * dW_2\n    ```\n\n    ```python\n    mu = np.array([0.2, 0.7])\n    s = np.array([[0.3, 0.1], [0.1, 0.3]])\n    num_samples = 10000\n    dim = 2\n    dtype=tf.float64\n\n    # Define drift and volatility functions\n    def drift_fn(t, x):\n      return mu * tf.sqrt(t) * tf.ones([num_samples, dim], dtype=dtype)\n\n    def vol_fn(t, x):\n      return s * tf.ones([num_samples, dim, dim], dtype=dtype)\n\n    # Initialize `GenericItoProcess`\n    process = GenericItoProcess(dim=2, drift_fn=drift_fn, volatility_fn=vol_fn,\n                                dtype=dtype)\n    # Set starting location\n    x0 = np.array([0.1, -1.1])\n    # Sample `num_samples` paths at specified `times` locations using built-in\n    # Euler scheme.\n    times = [0.1, 1.0, 2.0]\n    paths = process.sample_paths(\n              times,\n              num_samples=num_samples,\n              initial_state=x0,\n              time_step=0.01,\n              seed=42)\n    ```\n\n    #### References\n    [1]: Brent Oksendal. Stochastic Differential Equations: An Introduction with\n      Applications. Springer. 2010.\n\n    Args:\n      dim: Python int greater than or equal to 1. The dimension of the Ito\n        process.\n      drift_fn: A Python callable to compute the drift of the process. The\n        callable should accept two real `Tensor` arguments of the same dtype.\n        The first argument is the scalar time t, the second argument is the\n        value of Ito process X - `Tensor` of shape `batch_shape + [dim]`. The\n        result is value of drift a(t, X). The return value of the callable is a\n        real `Tensor` of the same dtype as the input arguments and of shape\n        `batch_shape + [dim]`.\n      volatility_fn: A Python callable to compute the volatility of the process.\n        The callable should accept two real `Tensor` arguments of the same dtype\n        and shape `times_shape`. The first argument is the scalar time t, the\n        second argument is the value of Ito process X - `Tensor` of shape\n        `batch_shape + [dim]`. The result is value of volatility S_{ij}(t, X).\n        The return value of the callable is a real `Tensor` of the same dtype as\n        the input arguments and of shape `batch_shape + [dim, dim]`.\n      dtype: The default dtype to use when converting values to `Tensor`s.\n        Default value: None which means that default dtypes inferred by\n          TensorFlow are used.\n      name: str. The name scope under which ops created by the methods of this\n        class are nested.\n        Default value: None which maps to the default name\n          `generic_ito_process`.\n\n    Raises:\n      ValueError if the dimension is less than 1, or if either `drift_fn`\n        or `volatility_fn` is not supplied.\n    """"""\n    if dim < 1:\n      raise ValueError(\'Dimension must be 1 or greater.\')\n    if drift_fn is None or volatility_fn is None:\n      raise ValueError(\'Both drift and volatility functions must be supplied.\')\n    self._dim = dim\n    self._drift_fn = drift_fn\n    self._volatility_fn = volatility_fn\n    self._dtype = dtype\n    self._name = name or \'generic_ito_process\'\n\n  def dim(self):\n    """"""The dimension of the process.""""""\n    return self._dim\n\n  def dtype(self):\n    """"""The data type of process realizations.""""""\n    return self._dtype\n\n  def name(self):\n    """"""The name to give to ops created by this class.""""""\n    return self._name\n\n  def drift_fn(self):\n    """"""Python callable calculating instantaneous drift.\n\n    The callable should accept two real `Tensor` arguments of the same dtype.\n    The first argument is the scalar time t, the second argument is the value of\n    Ito process X - `Tensor` of shape `batch_shape + [dim]`. The result is the\n    value of drift a(t, X). The return value of the callable is a real `Tensor`\n    of the same dtype as the input arguments and of shape `batch_shape + [dim]`.\n\n    Returns:\n      The instantaneous drift rate callable.\n    """"""\n    return self._drift_fn\n\n  def volatility_fn(self):\n    """"""Python callable calculating the instantaneous volatility.\n\n    The callable should accept two real `Tensor` arguments of the same dtype and\n    shape `times_shape`. The first argument is the scalar time t, the second\n    argument is the value of Ito process X - `Tensor` of shape `batch_shape +\n    [dim]`. The result is value of volatility `S_ij`(t, X). The return value of\n    the callable is a real `Tensor` of the same dtype as the input arguments and\n    of shape `batch_shape + [dim, dim]`.\n\n    Returns:\n      The instantaneous volatility callable.\n    """"""\n    return self._volatility_fn\n\n  def sample_paths(self,\n                   times,\n                   num_samples=1,\n                   initial_state=None,\n                   random_type=None,\n                   seed=None,\n                   swap_memory=True,\n                   name=None,\n                   time_step=None,\n                   skip=0,\n                   precompute_normal_draws=True,\n                   watch_params=None):\n    """"""Returns a sample of paths from the process using Euler sampling.\n\n    The default implementation uses the Euler scheme. However, for particular\n    types of Ito processes more efficient schemes can be used.\n\n    Args:\n      times: Rank 1 `Tensor` of increasing positive real values. The times at\n        which the path points are to be evaluated.\n      num_samples: Positive scalar `int`. The number of paths to draw.\n        Default value: 1.\n      initial_state: `Tensor` of shape `[dim]`. The initial state of the\n        process.\n        Default value: None which maps to a zero initial state.\n      random_type: Enum value of `RandomType`. The type of (quasi)-random number\n        generator to use to generate the paths.\n        Default value: None which maps to the standard pseudo-random numbers.\n      seed: Seed for the random number generator. The seed is\n        only relevant if `random_type` is one of\n        `[STATELESS, PSEUDO, HALTON_RANDOMIZED, PSEUDO_ANTITHETIC,\n          STATELESS_ANTITHETIC]`. For `PSEUDO`, `PSEUDO_ANTITHETIC` and\n        `HALTON_RANDOMIZED` the seed should be an Python integer. For\n        `STATELESS` and  `STATELESS_ANTITHETIC `must be supplied as an integer\n        `Tensor` of shape `[2]`.\n        Default value: `None` which means no seed is set.\n      swap_memory: A Python bool. Whether GPU-CPU memory swap is enabled for\n        this op. See an equivalent flag in `tf.while_loop` documentation for\n        more details. Useful when computing a gradient of the op since\n        `tf.while_loop` is used to propagate stochastic process in time.\n        Default value: True.\n      name: Python string. The name to give this op.\n        Default value: `None` which maps to `sample_paths` is used.\n      time_step: Real scalar `Tensor`. The maximal distance between time points\n        in grid in Euler scheme.\n      skip: `int32` 0-d `Tensor`. The number of initial points of the Sobol or\n        Halton sequence to skip. Used only when `random_type` is \'SOBOL\',\n        \'HALTON\', or \'HALTON_RANDOMIZED\', otherwise ignored.\n        Default value: `0`.\n      precompute_normal_draws: Python bool. Indicates whether the noise\n        increments in Euler scheme are precomputed upfront (see\n        `models.euler_sampling.sample`). For `HALTON` and `SOBOL` random types\n        the increments are always precomputed. While the resulting graph\n        consumes more memory, the performance gains might be significant.\n        Default value: `True`.\n      watch_params: An optional list of zero-dimensional `Tensor`s of the same\n        `dtype` as `initial_state`. If provided, specifies `Tensor`s with\n        respect to which the differentiation of the sampling function will\n        happen. A more efficient algorithm is used when `watch_params` are\n        specified. Note the the function becomes differentiable onlhy wrt to\n        these `Tensor`s and the `initial_state`. The gradient wrt any other\n        `Tensor` is set to be zero.\n\n    Returns:\n     A real `Tensor` of shape `[num_samples, k, n]` where `k` is the size of the\n     `times`, and `n` is the dimension of the process.\n\n    Raises:\n      ValueError: If `time_step` is not supplied.\n    """"""\n    if time_step is None:\n      raise ValueError(\'`time_step` can not be `None` when calling \'\n                       \'sample_paths of GenericItoProcess.\')\n    name = name or (self._name + \'_sample_path\')\n    with tf.name_scope(name):\n      return euler_sampling.sample(\n          self._dim,\n          self._drift_fn,\n          self._volatility_fn,\n          times,\n          num_samples=num_samples,\n          initial_state=initial_state,\n          random_type=random_type,\n          time_step=time_step,\n          seed=seed,\n          swap_memory=swap_memory,\n          skip=skip,\n          precompute_normal_draws=precompute_normal_draws,\n          watch_params=watch_params,\n          dtype=self._dtype,\n          name=name)\n\n  def fd_solver_backward(self,\n                         start_time,\n                         end_time,\n                         coord_grid,\n                         values_grid,\n                         discounting=None,\n                         one_step_fn=None,\n                         boundary_conditions=None,\n                         start_step_count=0,\n                         num_steps=None,\n                         time_step=None,\n                         values_transform_fn=None,\n                         dtype=None,\n                         name=None,\n                         **kwargs):\n    """"""Returns a solver for Feynman-Kac PDE associated to the process.\n\n    This method applies a finite difference method to solve the final value\n    problem as it appears in the Feynman-Kac formula associated to this Ito\n    process. The Feynman-Kac PDE is closely related to the backward Kolomogorov\n    equation associated to the stochastic process and allows for the inclusion\n    of a discounting function.\n\n    For more details of the Feynman-Kac theorem see [1]. The PDE solved by this\n    method is:\n\n    ```None\n      V_t + Sum[mu_i(t, x) V_i, 1<=i<=n] +\n        (1/2) Sum[ D_{ij} V_{ij}, 1 <= i,j <= n] - r(t, x) V = 0\n    ```\n\n    In the above, `V_t` is the derivative of `V` with respect to `t`,\n    `V_i` is the partial derivative with respect to `x_i` and `V_{ij}` the\n    (mixed) partial derivative with respect to `x_i` and `x_j`. `mu_i` is the\n    drift of this process and `D_{ij}` are the components of the diffusion\n    tensor:\n\n    ```None\n      D_{ij}(t,x) = (Sigma(t,x) . Transpose[Sigma(t,x)])_{ij}\n    ```\n\n    This method evolves a spatially discretized solution of the above PDE from\n    time `t0` to time `t1 < t0` (i.e. backwards in time).\n    The solution `V(t,x)` is assumed to be discretized on an `n`-dimensional\n    rectangular grid. A rectangular grid, G, in n-dimensions may be described\n    by specifying the coordinates of the points along each axis. For example,\n    a 2 x 4 grid in two dimensions can be specified by taking the cartesian\n    product of [1, 3] and [5, 6, 7, 8] to yield the grid points with\n    coordinates: `[(1, 5), (1, 6), (1, 7), (1, 8), (3, 5) ... (3, 8)]`.\n\n    This method allows batching of solutions. In this context, batching means\n    the ability to represent and evolve multiple independent functions `V`\n    (e.g. V1, V2 ...) simultaneously. A single discretized solution is specified\n    by stating its values at each grid point. This can be represented as a\n    `Tensor` of shape [d1, d2, ... dn] where di is the grid size along the `i`th\n    axis. A batch of such solutions is represented by a `Tensor` of shape:\n    [K, d1, d2, ... dn] where `K` is the batch size. This method only requires\n    that the input parameter `values_grid` be broadcastable with shape\n    [K, d1, ... dn].\n\n    The evolution of the solution from `t0` to `t1` is often done by\n    discretizing the differential equation to a difference equation along\n    the spatial and temporal axes. The temporal discretization is given by a\n    (sequence of) time steps [dt_1, dt_2, ... dt_k] such that the sum of the\n    time steps is equal to the total time step `t0 - t1`. If a uniform time\n    step is used, it may equivalently be specified by stating the number of\n    steps (n_steps) to take. This method provides both options via the\n    `time_step` and `num_steps` parameters. However, not all methods need\n    discretization along time direction (e.g. method of lines) so this argument\n    may not be applicable to some implementations.\n\n    The workhorse of this method is the `one_step_fn`. For the commonly used\n    methods, see functions in `math.pde.steppers` module.\n\n    The mapping between the arguments of this method and the above\n    equation are described in the Args section below.\n\n    For a simple instructive example of implementation of this method, see\n    `models.GenericItoProcess.fd_solver_backward`.\n\n    TODO(b/142309558): Complete documentation.\n\n    Args:\n      start_time: Real positive scalar `Tensor`. The start time of the grid.\n        Corresponds to time `t0` above.\n      end_time: Real scalar `Tensor` smaller than the `start_time` and greater\n        than zero. The time to step back to. Corresponds to time `t1` above.\n      coord_grid: List of `n` rank 1 real `Tensor`s. `n` is the dimension of the\n        domain. The i-th `Tensor` has shape, `[d_i]` where `d_i` is the size of\n        the grid along axis `i`. The coordinates of the grid points. Corresponds\n        to the spatial grid `G` above.\n      values_grid: Real `Tensor` containing the function values at time\n        `start_time` which have to be stepped back to time `end_time`. The shape\n        of the `Tensor` must broadcast with `[K, d_1, d_2, ..., d_n]`. The first\n        axis of size `K` is the values batch dimension and allows multiple\n        functions (with potentially different boundary/final conditions) to be\n        stepped back simultaneously.\n      discounting: Callable corresponding to `r(t,x)` above. If not supplied,\n        zero discounting is assumed.\n      one_step_fn: The transition kernel. A callable that consumes the following\n        arguments by keyword:\n          1. \'time\': Current time\n          2. \'next_time\': The next time to step to. For the backwards in time\n            evolution, this time will be smaller than the current time.\n          3. \'coord_grid\': The coordinate grid.\n          4. \'values_grid\': The values grid.\n          5. \'boundary_conditions\': The boundary conditions.\n          6. \'quadratic_coeff\': A callable returning the quadratic coefficients\n            of the PDE (i.e. `(1/2)D_{ij}(t, x)` above). The callable accepts\n            the time and  coordinate grid as keyword arguments and returns a\n            `Tensor` with shape that broadcasts with `[dim, dim]`.\n          7. \'linear_coeff\': A callable returning the linear coefficients of the\n            PDE (i.e. `mu_i(t, x)` above). Accepts time and coordinate grid as\n            keyword arguments and returns a `Tensor` with shape that broadcasts\n            with `[dim]`.\n          8. \'constant_coeff\': A callable returning the coefficient of the\n            linear homogenous term (i.e. `r(t,x)` above). Same spec as above.\n            The `one_step_fn` callable returns a 2-tuple containing the next\n            coordinate grid, next values grid.\n      boundary_conditions: A list of size `dim` containing boundary conditions.\n        The i\'th element of the list is a 2-tuple containing the lower and upper\n        boundary condition for the boundary along the i`th axis.\n      start_step_count: Scalar integer `Tensor`. Initial value for the number of\n        time steps performed.\n        Default value: 0 (i.e. no previous steps performed).\n      num_steps: Positive int scalar `Tensor`. The number of time steps to take\n        when moving from `start_time` to `end_time`. Either this argument or the\n        `time_step` argument must be supplied (but not both). If num steps is\n        `k>=1`, uniform time steps of size `(t0 - t1)/k` are taken to evolve the\n        solution from `t0` to `t1`. Corresponds to the `n_steps` parameter\n        above.\n      time_step: The time step to take. Either this argument or the `num_steps`\n        argument must be supplied (but not both). The type of this argument may\n        be one of the following (in order of generality): (a) None in which case\n          `num_steps` must be supplied. (b) A positive real scalar `Tensor`. The\n          maximum time step to take. If the value of this argument is `dt`, then\n          the total number of steps taken is N = (t0 - t1) / dt rounded up to\n          the nearest integer. The first N-1 steps are of size dt and the last\n          step is of size `t0 - t1 - (N-1) * dt`. (c) A callable accepting the\n          current time and returning the size of the step to take. The input and\n          the output are real scalar `Tensor`s.\n      values_transform_fn: An optional callable applied to transform the\n        solution values at each time step. The callable is invoked after the\n        time step has been performed. The callable should accept the time of the\n        grid, the coordinate grid and the values grid and should return the\n        values grid. All input arguments to be passed by keyword.\n      dtype: The dtype to use.\n      name: The name to give to the ops.\n        Default value: None which means `solve_backward` is used.\n      **kwargs: Additional keyword args:\n        (1) pde_solver_fn: Function to solve the PDE that accepts all the above\n          arguments by name and returns the same tuple object as required below.\n          Defaults to `tff.math.pde.fd_solvers.solve_backward`.\n\n    Returns:\n      A tuple object containing at least the following attributes:\n        final_values_grid: A `Tensor` of same shape and dtype as `values_grid`.\n          Contains the final state of the values grid at time `end_time`.\n        final_coord_grid: A list of `Tensor`s of the same specification as\n          the input `coord_grid`. Final state of the coordinate grid at time\n          `end_time`.\n        step_count: The total step count (i.e. the sum of the `start_step_count`\n          and the number of steps performed in this call.).\n        final_time: The final time at which the evolution stopped. This value\n          is given by `max(min(end_time, start_time), 0)`.\n    """"""\n    pde_solver_fn = kwargs.get(\'pde_solver_fn\', fd_solvers.solve_backward)\n\n    second_order_coeff_fn, first_order_coeff_fn, zeroth_order_coeff_fn = (\n        _backward_pde_coeffs(self._drift_fn, self._volatility_fn, discounting))\n\n    return pde_solver_fn(\n        start_time=start_time,\n        end_time=end_time,\n        coord_grid=coord_grid,\n        values_grid=values_grid,\n        num_steps=num_steps,\n        start_step_count=start_step_count,\n        time_step=time_step,\n        one_step_fn=one_step_fn,\n        boundary_conditions=boundary_conditions,\n        values_transform_fn=values_transform_fn,\n        second_order_coeff_fn=second_order_coeff_fn,\n        first_order_coeff_fn=first_order_coeff_fn,\n        zeroth_order_coeff_fn=zeroth_order_coeff_fn,\n        dtype=dtype,\n        name=name)\n\n  def fd_solver_forward(self,\n                        start_time,\n                        end_time,\n                        coord_grid,\n                        values_grid,\n                        one_step_fn=None,\n                        boundary_conditions=None,\n                        start_step_count=0,\n                        num_steps=None,\n                        time_step=None,\n                        values_transform_fn=None,\n                        dtype=None,\n                        name=None,\n                        **kwargs):\n    r""""""Returns a solver for the Fokker Plank equation of this process.\n\n    The Fokker Plank equation (also known as the Kolmogorov Forward equation)\n    associated to this Ito process is given by:\n\n    ```None\n      V_t + Sum[(mu_i(t, x) V)_i, 1<=i<=n]\n        - (1/2) Sum[ (D_{ij} V)_{ij}, 1 <= i,j <= n] = 0\n    ```\n\n    with the initial value condition $$V(0, x) = u(x)$$.\n\n    This method evolves a spatially discretized solution of the above PDE from\n    time `t0` to time `t1 > t0` (i.e. forwards in time).\n    The solution `V(t,x)` is assumed to be discretized on an `n`-dimensional\n    rectangular grid. A rectangular grid, G, in n-dimensions may be described\n    by specifying the coordinates of the points along each axis. For example,\n    a 2 x 4 grid in two dimensions can be specified by taking the cartesian\n    product of [1, 3] and [5, 6, 7, 8] to yield the grid points with\n    coordinates: `[(1, 5), (1, 6), (1, 7), (1, 8), (3, 5) ... (3, 8)]`.\n\n    Batching of solutions is supported. In this context, batching means\n    the ability to represent and evolve multiple independent functions `V`\n    (e.g. V1, V2 ...) simultaneously. A single discretized solution is specified\n    by stating its values at each grid point. This can be represented as a\n    `Tensor` of shape [d1, d2, ... dn] where di is the grid size along the `i`th\n    axis. A batch of such solutions is represented by a `Tensor` of shape:\n    [K, d1, d2, ... dn] where `K` is the batch size. This method only requires\n    that the input parameter `values_grid` be broadcastable with shape\n    [K, d1, ... dn].\n\n    The evolution of the solution from `t0` to `t1` is often done by\n    discretizing the differential equation to a difference equation along\n    the spatial and temporal axes. The temporal discretization is given by a\n    (sequence of) time steps [dt_1, dt_2, ... dt_k] such that the sum of the\n    time steps is equal to the total time step `t1 - t0`. If a uniform time\n    step is used, it may equivalently be specified by stating the number of\n    steps (n_steps) to take. This method provides both options via the\n    `time_step` and `num_steps` parameters. However, not all methods need\n    discretization along time direction (e.g. method of lines) so this argument\n    may not be applicable to some implementations.\n\n    The workhorse of this method is the `one_step_fn`. For the commonly used\n    methods, see functions in `math.pde.steppers` module.\n\n    The mapping between the arguments of this method and the above\n    equation are described in the Args section below.\n\n    For a simple instructive example of implementation of this method, see\n    `models.GenericItoProcess.fd_solver_forward`.\n\n    TODO(b/142309558): Complete documentation.\n\n    Args:\n      start_time: Real positive scalar `Tensor`. The start time of the grid.\n        Corresponds to time `t0` above.\n      end_time: Real scalar `Tensor` smaller than the `start_time` and greater\n        than zero. The time to step back to. Corresponds to time `t1` above.\n      coord_grid: List of `n` rank 1 real `Tensor`s. `n` is the dimension of the\n        domain. The i-th `Tensor` has shape, `[d_i]` where `d_i` is the size of\n        the grid along axis `i`. The coordinates of the grid points. Corresponds\n        to the spatial grid `G` above.\n      values_grid: Real `Tensor` containing the function values at time\n        `start_time` which have to be stepped back to time `end_time`. The shape\n        of the `Tensor` must broadcast with `[K, d_1, d_2, ..., d_n]`. The first\n        axis of size `K` is the values batch dimension and allows multiple\n        functions (with potentially different boundary/final conditions) to be\n        stepped back simultaneously.\n      one_step_fn: The transition kernel. A callable that consumes the following\n        arguments by keyword:\n          1. \'time\': Current time\n          2. \'next_time\': The next time to step to. For the backwards in time\n            evolution, this time will be smaller than the current time.\n          3. \'coord_grid\': The coordinate grid.\n          4. \'values_grid\': The values grid.\n          5. \'quadratic_coeff\': A callable returning the quadratic coefficients\n            of the PDE (i.e. `(1/2)D_{ij}(t, x)` above). The callable accepts\n            the time and  coordinate grid as keyword arguments and returns a\n            `Tensor` with shape that broadcasts with `[dim, dim]`.\n          6. \'linear_coeff\': A callable returning the linear coefficients of the\n            PDE (i.e. `mu_i(t, x)` above). Accepts time and coordinate grid as\n            keyword arguments and returns a `Tensor` with shape that broadcasts\n            with `[dim]`.\n          7. \'constant_coeff\': A callable returning the coefficient of the\n            linear homogenous term (i.e. `r(t,x)` above). Same spec as above.\n            The `one_step_fn` callable returns a 2-tuple containing the next\n            coordinate grid, next values grid.\n      boundary_conditions: A list of size `dim` containing boundary conditions.\n        The i\'th element of the list is a 2-tuple containing the lower and upper\n        boundary condition for the boundary along the i`th axis.\n      start_step_count: Scalar integer `Tensor`. Initial value for the number of\n        time steps performed.\n        Default value: 0 (i.e. no previous steps performed).\n      num_steps: Positive int scalar `Tensor`. The number of time steps to take\n        when moving from `start_time` to `end_time`. Either this argument or the\n        `time_step` argument must be supplied (but not both). If num steps is\n        `k>=1`, uniform time steps of size `(t0 - t1)/k` are taken to evolve the\n        solution from `t0` to `t1`. Corresponds to the `n_steps` parameter\n        above.\n      time_step: The time step to take. Either this argument or the `num_steps`\n        argument must be supplied (but not both). The type of this argument may\n        be one of the following (in order of generality): (a) None in which case\n          `num_steps` must be supplied. (b) A positive real scalar `Tensor`. The\n          maximum time step to take. If the value of this argument is `dt`, then\n          the total number of steps taken is N = (t1 - t0) / dt rounded up to\n          the nearest integer. The first N-1 steps are of size dt and the last\n          step is of size `t1 - t0 - (N-1) * dt`. (c) A callable accepting the\n          current time and returning the size of the step to take. The input and\n          the output are real scalar `Tensor`s.\n      values_transform_fn: An optional callable applied to transform the\n        solution values at each time step. The callable is invoked after the\n        time step has been performed. The callable should accept the time of the\n        grid, the coordinate grid and the values grid and should return the\n        values grid. All input arguments to be passed by keyword.\n      dtype: The dtype to use.\n      name: The name to give to the ops.\n        Default value: None which means `solve_forward` is used.\n      **kwargs: Additional keyword args:\n        (1) pde_solver_fn: Function to solve the PDE that accepts all the above\n          arguments by name and returns the same tuple object as required below.\n          Defaults to `tff.math.pde.fd_solvers.solve_forward`.\n\n    Returns:\n      A tuple object containing at least the following attributes:\n        final_values_grid: A `Tensor` of same shape and dtype as `values_grid`.\n          Contains the final state of the values grid at time `end_time`.\n        final_coord_grid: A list of `Tensor`s of the same specification as\n          the input `coord_grid`. Final state of the coordinate grid at time\n          `end_time`.\n        step_count: The total step count (i.e. the sum of the `start_step_count`\n          and the number of steps performed in this call.).\n        final_time: The final time at which the evolution stopped. This value\n          is given by `max(min(end_time, start_time), 0)`.\n    """"""\n    pde_solver_fn = kwargs.get(\'pde_solver_fn\', fd_solvers.solve_forward)\n\n    backward_second_order, backward_first_order, backward_zeroth_order = (\n        _backward_pde_coeffs(self._drift_fn, self._volatility_fn,\n                             discounting=None))\n\n    # Transform backward to forward equation.\n    inner_second_order_coeff_fn = lambda t, x: -backward_second_order(t, x)\n    inner_first_order_coeff_fn = backward_first_order\n    zeroth_order_coeff_fn = backward_zeroth_order\n\n    return pde_solver_fn(\n        start_time=start_time,\n        end_time=end_time,\n        coord_grid=coord_grid,\n        values_grid=values_grid,\n        num_steps=num_steps,\n        start_step_count=start_step_count,\n        time_step=time_step,\n        one_step_fn=one_step_fn,\n        boundary_conditions=boundary_conditions,\n        values_transform_fn=values_transform_fn,\n        inner_second_order_coeff_fn=inner_second_order_coeff_fn,\n        inner_first_order_coeff_fn=inner_first_order_coeff_fn,\n        zeroth_order_coeff_fn=zeroth_order_coeff_fn,\n        dtype=dtype,\n        name=name)\n\n\ndef _backward_pde_coeffs(drift_fn, volatility_fn, discounting):\n  """"""Returns coeffs of the backward PDE.""""""\n  def second_order_coeff_fn(t, coord_grid):\n    sigma = volatility_fn(t, _coord_grid_to_mesh_grid(coord_grid))\n    sigma_times_sigma_t = tf.linalg.matmul(sigma, sigma, transpose_b=True)\n\n    # We currently have [dim, dim] as innermost dimensions, but the returned\n    # tensor must have [dim, dim] as outermost dimensions.\n    rank = len(sigma.shape.as_list())\n    perm = [rank - 2, rank - 1] + list(range(rank - 2))\n    sigma_times_sigma_t = tf.transpose(sigma_times_sigma_t, perm)\n    return sigma_times_sigma_t / 2\n\n  def first_order_coeff_fn(t, coord_grid):\n    mu = drift_fn(t, _coord_grid_to_mesh_grid(coord_grid))\n\n    # We currently have [dim] as innermost dimension, but the returned\n    # tensor must have [dim] as outermost dimension.\n    rank = len(mu.shape.as_list())\n    perm = [rank - 1] + list(range(rank - 1))\n    mu = tf.transpose(mu, perm)\n    return mu\n\n  def zeroth_order_coeff_fn(t, coord_grid):\n    if not discounting:\n      return None\n    return -discounting(t, _coord_grid_to_mesh_grid(coord_grid))\n\n  return second_order_coeff_fn, first_order_coeff_fn, zeroth_order_coeff_fn\n\n\ndef _coord_grid_to_mesh_grid(coord_grid):\n  if len(coord_grid) == 1:\n    return tf.expand_dims(coord_grid[0], -1)\n  return tf.stack(values=tf.meshgrid(*coord_grid, indexing=\'ij\'), axis=-1)\n'"
tf_quant_finance/models/generic_ito_process_test.py,44,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for `sample_paths` of `ItoProcess`.""""""\n\nfrom unittest import mock  # pylint: disable=g-importing-member\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\nGenericItoProcess = tff.models.GenericItoProcess\ngrids = tff.math.pde.grids\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass GenericItoProcessTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters({\n      ""testcase_name"": ""no_xla"",\n      ""use_xla"": False,\n  }, {\n      ""testcase_name"": ""xla"",\n      ""use_xla"": True,\n  })\n  def test_sample_paths_wiener(self, use_xla):\n    """"""Tests paths properties for Wiener process (dX = dW).""""""\n\n    def drift_fn(_, x):\n      return tf.zeros_like(x)\n\n    def vol_fn(_, x):\n      return tf.expand_dims(tf.ones_like(x), -1)\n\n    process = GenericItoProcess(dim=1, drift_fn=drift_fn, volatility_fn=vol_fn)\n    times = np.array([0.1, 0.2, 0.3])\n    num_samples = 10000\n\n    @tf.function\n    def fn():\n      return process.sample_paths(\n          times=times, num_samples=num_samples, seed=42, time_step=0.01)\n\n    if use_xla:\n      paths = self.evaluate(tf.xla.experimental.compile(fn))[0]\n    else:\n      paths = self.evaluate(fn())\n\n    means = np.mean(paths, axis=0).reshape([-1])\n    covars = np.cov(paths.reshape([num_samples, -1]), rowvar=False)\n    expected_means = np.zeros((3,))\n    expected_covars = np.minimum(times.reshape([-1, 1]), times.reshape([1, -1]))\n    with self.subTest(name=""Means""):\n      self.assertAllClose(means, expected_means, rtol=1e-2, atol=1e-2)\n    with self.subTest(name=""Covar""):\n      self.assertAllClose(covars, expected_covars, rtol=1e-2, atol=1e-2)\n\n  def test_sample_paths_2d(self):\n    """"""Tests path properties for 2-dimentional Ito process.\n\n    We construct the following Ito processes.\n\n    dX_1 = mu_1 sqrt(t) dt + s11 dW_1 + s12 dW_2\n    dX_2 = mu_2 sqrt(t) dt + s21 dW_1 + s22 dW_2\n\n    mu_1, mu_2 are constants.\n    s_ij = a_ij t + b_ij\n\n    For this process expected value at time t is (x_0)_i + 2/3 * mu_i * t^1.5.\n    """"""\n    mu = np.array([0.2, 0.7])\n    a = np.array([[0.4, 0.1], [0.3, 0.2]])\n    b = np.array([[0.33, -0.03], [0.21, 0.5]])\n\n    def drift_fn(t, x):\n      return mu * tf.sqrt(t) * tf.ones_like(x, dtype=t.dtype)\n\n    def vol_fn(t, x):\n      del x\n      return (a * t + b) * tf.ones([2, 2], dtype=t.dtype)\n\n    num_samples = 10000\n    process = GenericItoProcess(dim=2, drift_fn=drift_fn, volatility_fn=vol_fn)\n    times = np.array([0.1, 0.21, 0.32, 0.43, 0.55])\n    x0 = np.array([0.1, -1.1])\n    paths = self.evaluate(\n        process.sample_paths(\n            times,\n            num_samples=num_samples,\n            initial_state=x0,\n            time_step=0.01,\n            seed=12134))\n\n    self.assertAllClose(paths.shape, (num_samples, 5, 2), atol=0)\n    means = np.mean(paths, axis=0)\n    times = np.reshape(times, [-1, 1])\n    expected_means = x0 + (2.0 / 3.0) * mu * np.power(times, 1.5)\n    self.assertAllClose(means, expected_means, rtol=1e-2, atol=1e-2)\n\n  def test_sample_paths_dtypes(self):\n    """"""Sampled paths have the expected dtypes.""""""\n    for dtype in [np.float32, np.float64]:\n      drift_fn = lambda t, x: tf.sqrt(t) * tf.ones_like(x, dtype=t.dtype)\n      vol_fn = lambda t, x: t * tf.ones([1, 1], dtype=t.dtype)\n      process = GenericItoProcess(\n          dim=1, drift_fn=drift_fn, volatility_fn=vol_fn, dtype=dtype)\n\n      paths = self.evaluate(\n          process.sample_paths(\n              times=[0.1, 0.2],\n              num_samples=10,\n              initial_state=[0.1],\n              time_step=0.01,\n              seed=123))\n      self.assertEqual(paths.dtype, dtype)\n\n  # Several tests below are unit tests for GenericItoProcess.fd_solver_backward:\n  # they mock out the pde solver and check only the conversion of SDE to PDE,\n  # but not PDE solving. There are also integration tests further below.\n  def test_backward_pde_coeffs_with_constant_params_1d(self):\n    vol = 2\n    drift = 1\n    discounting = 3\n\n    # batch_shape = (1, ), dim = 1\n    # vol_fn(...).shape = batch_shape + (dim, dim) = (1, 1, 1)\n    vol_fn = lambda t, x: tf.constant([[[vol]]], dtype=tf.float32)\n    # drift_fn(...).shape = batch_shape + (dim,) = (1, 1)\n    drift_fn = lambda t, x: tf.constant([[drift]], dtype=tf.float32)\n    # discounting_fn(...).shape = batch_shape = (1, )\n    discounting_fn = lambda t, x: tf.constant([discounting], dtype=tf.float32)\n\n    process = GenericItoProcess(\n        dim=1, drift_fn=drift_fn, volatility_fn=vol_fn, dtype=tf.float32)\n\n    pde_solver_fn = mock.Mock()\n    coord_grid = [tf.constant([0])]\n    process.fd_solver_backward(\n        start_time=0,\n        end_time=0,\n        coord_grid=coord_grid,\n        values_grid=tf.constant([0]),\n        discounting=discounting_fn,\n        pde_solver_fn=pde_solver_fn)\n    kwargs = pde_solver_fn.call_args[1]\n    second_order_coeff = self.evaluate(kwargs[""second_order_coeff_fn""](\n        0, coord_grid))\n    self.assertAllClose(np.array([[[vol**2 / 2]]]), second_order_coeff)\n    first_order_coeff = self.evaluate(kwargs[""first_order_coeff_fn""](\n        0, coord_grid))\n    self.assertAllClose(np.array([[drift]]), first_order_coeff)\n    zeroth_order_coeff = self.evaluate(kwargs[""zeroth_order_coeff_fn""](\n        0, coord_grid))\n    self.assertAllClose(np.array([-discounting]), zeroth_order_coeff)\n\n  def test_backward_pde_coeffs_with_nonconstant_params_1d(self):\n    # vol = 2 * x**2\n    # drift = 3 * x\n    # discounting = 6 / x\n    # x = [1, 2, 3]\n\n    # batch_shape = (3, ), dim = 1\n    # vol_fn(...).shape = batch_shape + (dim, dim) = (3, 1, 1)\n    def vol_fn(t, x):\n      del t\n      x = x[:, 0]  # x.shape was (3, 1), now it\'s (3, )\n      return tf.reshape(2 * x**2, (-1, 1, 1))\n\n    # drift_fn(...).shape = batch_shape + (dim,) = (3, 1)\n    def drift_fn(t, x):\n      del t\n      x = x[:, 0]\n      return tf.reshape(3 * x, (-1, 1))\n\n    # discounting_fn(...).shape = batch_shape = (3, )\n    def discounting_fn(t, x):\n      del t\n      x = x[:, 0]\n      return 6 / x\n\n    coord_grid = [tf.constant([1, 2, 3])]\n\n    process = GenericItoProcess(\n        dim=1, drift_fn=drift_fn, volatility_fn=vol_fn, dtype=tf.float32)\n\n    pde_solver_fn = mock.Mock()\n    process.fd_solver_backward(\n        start_time=0,\n        end_time=0,\n        coord_grid=coord_grid,\n        values_grid=tf.constant([0]),\n        discounting=discounting_fn,\n        pde_solver_fn=pde_solver_fn)\n    kwargs = pde_solver_fn.call_args[1]\n    second_order_coeff = self.evaluate(kwargs[""second_order_coeff_fn""](\n        0, coord_grid))\n    self.assertAllClose(\n        np.array([[[2, 8**2 / 2, 18**2 / 2]]]), second_order_coeff)\n    first_order_coeff = self.evaluate(kwargs[""first_order_coeff_fn""](\n        0, coord_grid))\n    self.assertAllClose(np.array([[3, 6, 9]]), first_order_coeff)\n    zeroth_order_coeff = self.evaluate(kwargs[""zeroth_order_coeff_fn""](\n        0, coord_grid))\n    self.assertAllClose(np.array([-6, -3, -2]), zeroth_order_coeff)\n\n  def test_backward_pde_coeffs_with_constant_params_2d(self):\n    vol = [[1, 2], [3, 4]]\n    drift = [1, 2]\n    discounting = 3\n\n    # batch_shape = (1, ), dim = 2\n    # vol_fn(...).shape = batch_shape + (dim, dim) = (1, 2, 2)\n    def vol_fn(t, x):\n      del t, x\n      return tf.expand_dims(tf.constant(vol, dtype=tf.float32), 0)\n\n    # drift_fn(...).shape = batch_shape + (dim,) = (1, 2)\n    def drift_fn(t, x):\n      del t, x\n      return tf.expand_dims(tf.constant(drift, dtype=tf.float32), 0)\n    # discounting_fn(...).shape = batch_shape = (1, )\n    discounting_fn = lambda t, x: tf.constant([discounting], dtype=tf.float32)\n\n    process = GenericItoProcess(\n        dim=2, drift_fn=drift_fn, volatility_fn=vol_fn, dtype=tf.float32)\n\n    pde_solver_fn = mock.Mock()\n    coord_grid = [tf.constant([0])]\n    process.fd_solver_backward(\n        start_time=0,\n        end_time=0,\n        coord_grid=coord_grid,\n        values_grid=tf.constant([0]),\n        discounting=discounting_fn,\n        pde_solver_fn=pde_solver_fn)\n    kwargs = pde_solver_fn.call_args[1]\n    second_order_coeff = self.evaluate(kwargs[""second_order_coeff_fn""](\n        0, coord_grid))\n\n    # second_order_coeff.shape = (dim, dim) + grid_shape = (2, 2, 1)\n    self.assertAllClose(\n        np.array([[[2.5], [5.5]], [[5.5], [12.5]]]), second_order_coeff)\n\n    # first_order_coeff.shape = (dim) + grid_shape = (2, 1)\n    first_order_coeff = self.evaluate(kwargs[""first_order_coeff_fn""](\n        0, coord_grid))\n    self.assertAllClose(np.array([[1], [2]]), first_order_coeff)\n\n    # first_order_coeff.shape = grid_shape = (1)\n    zeroth_order_coeff = self.evaluate(kwargs[""zeroth_order_coeff_fn""](\n        0, coord_grid))\n    self.assertAllClose(np.array([-3]), zeroth_order_coeff)\n\n  def test_backward_pde_coeffs_with_nonconstant_params_2d(self):\n    # vol = [[x * y, 2 * x], [x + y, 3]]\n    # drift = [x + y, x - y]\n    # discounting = 3 * x * y\n    # x = [0, 1, 2], y = [-1, 0, 1]\n\n    # batch_shape = (3, 3 ), dim = 2\n    # vol_fn(...).shape = batch_shape + (dim, dim) = (3, 3, 2, 2)\n    def vol_fn(t, grid):\n      del t\n      self.assertEqual((3, 3, 2), grid.shape)\n      ys = grid[..., 0]\n      xs = grid[..., 1]\n      vol_yy = xs * ys\n      vol_yx = 2 * xs\n      vol_xy = xs + ys\n      vol_xx = 3 * tf.ones_like(xs)\n      vol = tf.stack((tf.stack(\n          (vol_yy, vol_xy), axis=-1), tf.stack((vol_yx, vol_xx), axis=-1)),\n                     axis=-1)\n      self.assertEqual((3, 3, 2, 2), vol.shape)\n      return vol\n\n    # drift_fn(...).shape = batch_shape + (dim, ) = (3, 3, 2)\n    def drift_fn(t, grid):\n      del t\n      self.assertEqual((3, 3, 2), grid.shape)\n      ys = grid[..., 0]\n      xs = grid[..., 1]\n      drift_y = xs + ys\n      drift_x = xs - ys\n      drift = tf.stack((drift_y, drift_x), axis=-1)\n      self.assertEqual((3, 3, 2), drift.shape)\n      return drift\n\n    # drift_fn(...).shape = batch_shape = (3, 3)\n    def discounting_fn(t, grid):\n      del t\n      self.assertEqual((3, 3, 2), grid.shape)\n      ys = grid[..., 0]\n      xs = grid[..., 1]\n      return 3 * xs * ys\n\n    process = GenericItoProcess(\n        dim=2, drift_fn=drift_fn, volatility_fn=vol_fn, dtype=tf.float32)\n\n    pde_solver_fn = mock.Mock()\n    coord_grid = [tf.constant([-1, 0, 1]), tf.constant([0, 1, 2])]\n    process.fd_solver_backward(\n        start_time=0,\n        end_time=0,\n        coord_grid=coord_grid,\n        values_grid=tf.zeros((3, 3)),\n        discounting=discounting_fn,\n        pde_solver_fn=pde_solver_fn)\n    kwargs = pde_solver_fn.call_args[1]\n    second_order_coeff = self.evaluate(kwargs[""second_order_coeff_fn""](\n        0, coord_grid))\n\n    expected_coeff_yy = np.array([[0, 2.5, 10], [0, 2, 8], [0, 2.5, 10]])\n    self.assertAllClose(expected_coeff_yy, second_order_coeff[0][0])\n\n    expected_coeff_xy = np.array([[0, 3, 5], [0, 3, 6], [0, 4, 9]])\n    self.assertAllClose(expected_coeff_xy, second_order_coeff[0][1])\n    self.assertAllClose(expected_coeff_xy, second_order_coeff[1][0])\n\n    expected_coeff_xx = np.array([[5, 4.5, 5], [4.5, 5, 6.5], [5, 6.5, 9]])\n    self.assertAllClose(expected_coeff_xx, second_order_coeff[1][1])\n\n    # first_order_coeff.shape = (dim) + grid_shape = (2, 1)\n    first_order_coeff = self.evaluate(kwargs[""first_order_coeff_fn""](\n        0, coord_grid))\n\n    expected_coeff_y = np.array([[-1, 0, 1], [0, 1, 2], [1, 2, 3]])\n    self.assertAllClose(expected_coeff_y, first_order_coeff[0])\n    expected_coeff_x = np.array([[1, 2, 3], [0, 1, 2], [-1, 0, 1]])\n    self.assertAllClose(expected_coeff_x, first_order_coeff[1])\n\n    # first_order_coeff.shape = grid_shape = (1)\n    zeroth_order_coeff = self.evaluate(kwargs[""zeroth_order_coeff_fn""](\n        0, coord_grid))\n    self.assertAllClose(\n        np.array([[0, 3, 6], [0, 0, 0], [0, -3, -6]]), zeroth_order_coeff)\n\n  def test_solving_backward_pde_for_sde_with_const_coeffs(self):\n    # Integration test for converting 2d SDE with constant coeffs to a\n    # backward Kolmogorov PDE and solving it.\n    # The SDE is:\n    # dS_x = (dW_1 + dW_2) / sqrt(2)\n    # dS_y = (dW_1 + dW_2) / sqrt(2)\n    # It is of course trivial, but we\'ll solve it the hard way for the sake of\n    # testing.\n    # The Kolmogorov backwards PDE is:\n    # u_{t} + D u_{xx} / 2 +  D u_{yy} / 2 + D u_{xy} = 0\n    # The equation can be rewritten as `u_{t} + D u_{zz} = 0`, where\n    # z = (x + y) / sqrt(2).\n    #  If the final condition is a gaussian centered at (0, 0) with variance\n    #  sigma, then the solution is:\n    # `u(x, y, t) = gaussian((x + y)/sqrt(2), sigma + 2D(t_final - t)) *\n    # gaussian((x - y)/sqrt(2), sigma)`.\n\n    def vol_fn(t, grid):\n      del t\n      xs = grid[..., 1]\n      vol_elem = tf.ones_like(xs) / np.sqrt(2)  # all 4 elements are equal.\n      return tf.stack((tf.stack(\n          (vol_elem, vol_elem), axis=-1), tf.stack(\n              (vol_elem, vol_elem), axis=-1)),\n                      axis=-1)\n\n    drift_fn = lambda t, grid: tf.zeros(grid.shape)\n\n    process = GenericItoProcess(\n        dim=2, volatility_fn=vol_fn, drift_fn=drift_fn, dtype=tf.float32)\n\n    grid = grids.uniform_grid(\n        minimums=[-10, -20],\n        maximums=[10, 20],\n        sizes=[201, 301],\n        dtype=tf.float32)\n    ys = self.evaluate(grid[0])\n    xs = self.evaluate(grid[1])\n\n    diff_coeff = 1\n    time_step = 0.1\n    final_t = 3\n    final_variance = 1\n    variance_along_diagonal = final_variance + 2 * diff_coeff * final_t\n\n    def expected_fn(x, y):\n      return (_gaussian(\n          (x + y) / np.sqrt(2), variance_along_diagonal) * _gaussian(\n              (x - y) / np.sqrt(2), final_variance))\n\n    expected = np.array([[expected_fn(x, y) for x in xs] for y in ys])\n\n    final_values = tf.expand_dims(\n        tf.constant(\n            np.outer(\n                _gaussian(ys, final_variance), _gaussian(xs, final_variance)),\n            dtype=tf.float32),\n        axis=0)\n\n    result = self.evaluate(\n        process.fd_solver_backward(\n            start_time=final_t,\n            end_time=0,\n            coord_grid=grid,\n            values_grid=final_values,\n            time_step=time_step,\n            dtype=tf.float32)[0])\n\n    self.assertLess(np.max(np.abs(result - expected)) / np.max(expected), 0.01)\n\n\ndef _gaussian(xs, variance):\n  return np.exp(-np.square(xs) / (2 * variance)) / np.sqrt(2 * np.pi * variance)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/models/ito_process.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Defines an interface for Ito processes.\n\nIto processes underlie most quantitative finance models. This module defines\na framework for describing Ito processes. An Ito process is usually defined\nvia an Ito SDE:\n\n```\n  dX = a(t, X_t) dt + b(t, X_t) dW_t\n\n```\n\nwhere `a(t, x)` is a function taking values in `R^n`, `b(t, X_t)` is a function\ntaking values in `n x n` matrices. For a complete mathematical definition,\nincluding the regularity conditions that must be imposed on the coefficients\n`a(t, X)` and `b(t, X)`, see Ref [1].\n\n#### References:\n  [1]: Brent Oksendal. Stochastic Differential Equations: An Introduction with\n    Applications. Springer. 2010.\n""""""\n\nimport abc\nimport six\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass ItoProcess(object):\n  """"""Interface for specifying Ito processes.\n\n    Interface for defining stochastic process defined by the Ito SDE:\n\n    ```None\n      dX_i = a_i(t, X) dt + Sum(S_{ij}(t, X) dW_j for 1 <= j <= n), 1 <= i <= n\n    ```\n\n  The vector coefficient `a_i` is referred to as the drift of the process and\n  the matrix `S_{ij}` as the volatility of the process. For the process to be\n  well defined, these coefficients need to satisfy certain technical conditions\n  which may be found in Ref. [1]. The vector `dW_j` represents independent\n  Brownian increments.\n\n  For a simple and instructive example of the implementation of this interface,\n  see `models.GenericItoProcess`.\n\n  #### References\n  [1]: Brent Oksendal. Stochastic Differential Equations: An Introduction with\n    Applications. Springer. 2010.\n  """"""\n\n  @abc.abstractmethod\n  def name(self):\n    """"""The name to give to ops created by this class.""""""\n    pass\n\n  @abc.abstractmethod\n  def dim(self):\n    """"""The dimension of the process. A positive python integer.""""""\n    pass\n\n  @abc.abstractmethod\n  def dtype(self):\n    """"""The data type of process realizations.""""""\n    pass\n\n  @abc.abstractmethod\n  def drift_fn(self):\n    """"""Python callable calculating instantaneous drift.\n\n    The callable should accept two real `Tensor` arguments of the same dtype.\n    The first argument is the scalar time t, the second argument is the value of\n    Ito process X as a tensor of shape `batch_shape + [dim]`. The result is\n    value of drift a(t, X). The return value of the callable is a real `Tensor`\n    of the same dtype as the input arguments and of shape `batch_shape + [dim]`.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def volatility_fn(self):\n    """"""Python callable calculating the instantaneous volatility.\n\n    The callable should accept two real `Tensor` arguments of the same dtype and\n    shape `times_shape`. The first argument is the scalar time t, the second\n    argument is the value of Ito process X as a tensor of shape `batch_shape +\n    [dim]`. The result is volatility at time t and location X: S(t, X). The\n    return value of the callable is a real `Tensor` of the same dtype as the\n    input arguments and of shape `batch_shape + [dim, dim]`.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def sample_paths(self,\n                   times,\n                   num_samples=1,\n                   initial_state=None,\n                   random_type=None,\n                   seed=None,\n                   **kwargs):\n    """"""Returns a sample of paths from the process.\n\n    Args:\n      times: Rank 1 `Tensor` of increasing positive real values. The times at\n        which the path points are to be evaluated.\n      num_samples: Positive scalar `int`. The number of paths to draw.\n      initial_state: `Tensor` of shape `[dim]`. The initial state of the\n        process.\n        Default value: None which maps to a zero initial state.\n      random_type: Enum value of `RandomType`. The type of (quasi)-random number\n        generator to use to generate the paths.\n        Default value: None which maps to the standard pseudo-random numbers.\n      seed: Seed for the random number generator. The seed is\n        only relevant if `random_type` is one of\n        `[STATELESS, PSEUDO, HALTON_RANDOMIZED, PSEUDO_ANTITHETIC,\n          STATELESS_ANTITHETIC]`. For `PSEUDO`, `PSEUDO_ANTITHETIC` and\n        `HALTON_RANDOMIZED` the seed should be an Python integer. For\n        `STATELESS` and  `STATELESS_ANTITHETIC `must be supplied as an integer\n        `Tensor` of shape `[2]`.\n        Default value: `None` which means no seed is set.\n      **kwargs: Any other keyword args needed by an implementation.\n\n    Returns:\n     A real `Tensor` of shape [num_samples, k, n] where `k` is the size of the\n        `times`, `n` is the dimension of the process.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def fd_solver_backward(self,\n                         start_time,\n                         end_time,\n                         coord_grid,\n                         values_grid,\n                         discounting=None,\n                         one_step_fn=None,\n                         boundary_conditions=None,\n                         start_step_count=0,\n                         num_steps=None,\n                         time_step=None,\n                         values_transform_fn=None,\n                         dtype=None,\n                         **kwargs):\n    """"""Returns a solver for Feynman-Kac PDE associated to the process.\n\n    This method applies a finite difference method to solve the final value\n    problem as it appears in the Feynman-Kac formula associated to this Ito\n    process. The Feynman-Kac PDE is closely related to the backward Kolomogorov\n    equation associated to the stochastic process and allows for the inclusion\n    of a discounting function.\n\n    For more details of the Feynman-Kac theorem see [1]. The PDE solved by this\n    method is:\n\n    ```None\n      V_t + Sum[mu_i(t, x) V_i, 1<=i<=n] +\n        (1/2) Sum[ D_{ij} V_{ij}, 1 <= i,j <= n] - r(t, x) V = 0\n    ```\n\n    In the above, `V_t` is the derivative of `V` with respect to `t`,\n    `V_i` is the partial derivative with respect to `x_i` and `V_{ij}` the\n    (mixed) partial derivative with respect to `x_i` and `x_j`. `mu_i` is the\n    drift of this process and `D_{ij}` are the components of the diffusion\n    tensor:\n\n    ```None\n      D_{ij}(t,x) = (Sigma(t,x) . Transpose[Sigma(t,x)])_{ij}\n    ```\n\n    This method evolves a spatially discretized solution of the above PDE from\n    time `t0` to time `t1 < t0` (i.e. backwards in time).\n    The solution `V(t,x)` is assumed to be discretized on an `n`-dimensional\n    rectangular grid. A rectangular grid, G, in n-dimensions may be described\n    by specifying the coordinates of the points along each axis. For example,\n    a 2 x 4 grid in two dimensions can be specified by taking the cartesian\n    product of [1, 3] and [5, 6, 7, 8] to yield the grid points with\n    coordinates: `[(1, 5), (1, 6), (1, 7), (1, 8), (3, 5) ... (3, 8)]`.\n\n    This method allows batching of solutions. In this context, batching means\n    the ability to represent and evolve multiple independent functions `V`\n    (e.g. V1, V2 ...) simultaneously. A single discretized solution is specified\n    by stating its values at each grid point. This can be represented as a\n    `Tensor` of shape [d1, d2, ... dn] where di is the grid size along the `i`th\n    axis. A batch of such solutions is represented by a `Tensor` of shape:\n    [K, d1, d2, ... dn] where `K` is the batch size. This method only requires\n    that the input parameter `values_grid` be broadcastable with shape\n    [K, d1, ... dn].\n\n    The evolution of the solution from `t0` to `t1` is often done by\n    discretizing the differential equation to a difference equation along\n    the spatial and temporal axes. The temporal discretization is given by a\n    (sequence of) time steps [dt_1, dt_2, ... dt_k] such that the sum of the\n    time steps is equal to the total time step `t0 - t1`. If a uniform time\n    step is used, it may equivalently be specified by stating the number of\n    steps (n_steps) to take. This method provides both options via the\n    `time_step` and `num_steps` parameters. However, not all methods need\n    discretization along time direction (e.g. method of lines) so this argument\n    may not be applicable to some implementations.\n\n    The workhorse of this method is the `one_step_fn`. For the commonly used\n    methods, see functions in `math.pde.steppers` module.\n\n    The mapping between the arguments of this method and the above\n    equation are described in the Args section below.\n\n    For a simple instructive example of implementation of this method, see\n    `models.GenericItoProcess.fd_solver_backward`.\n\n    TODO(b/142309558): Complete documentation.\n\n    Args:\n      start_time: Real positive scalar `Tensor`. The start time of the grid.\n        Corresponds to time `t0` above.\n      end_time: Real scalar `Tensor` smaller than the `start_time` and greater\n        than zero. The time to step back to. Corresponds to time `t1` above.\n      coord_grid: List of `n` rank 1 real `Tensor`s. `n` is the dimension of the\n        domain. The i-th `Tensor` has shape, `[d_i]` where `d_i` is the size of\n        the grid along axis `i`. The coordinates of the grid points. Corresponds\n        to the spatial grid `G` above.\n      values_grid: Real `Tensor` containing the function values at time\n        `start_time` which have to be stepped back to time `end_time`. The shape\n        of the `Tensor` must broadcast with `[K, d_1, d_2, ..., d_n]`. The first\n        axis of size `K` is the values batch dimension and allows multiple\n        functions (with potentially different boundary/final conditions) to be\n        stepped back simultaneously.\n      discounting: Callable corresponding to `r(t,x)` above. If not supplied,\n        zero discounting is assumed.\n      one_step_fn: The transition kernel. A callable that consumes the following\n        arguments by keyword:\n          1. \'time\': Current time\n          2. \'next_time\': The next time to step to. For the backwards in time\n            evolution, this time will be smaller than the current time.\n          3. \'coord_grid\': The coordinate grid.\n          4. \'values_grid\': The values grid.\n          5. \'boundary_conditions\': The boundary conditions.\n          6. \'quadratic_coeff\': A callable returning the quadratic coefficients\n            of the PDE (i.e. `(1/2)D_{ij}(t, x)` above). The callable accepts\n            the time and  coordinate grid as keyword arguments and returns a\n            `Tensor` with shape that broadcasts with `[dim, dim]`.\n          7. \'linear_coeff\': A callable returning the linear coefficients of the\n            PDE (i.e. `mu_i(t, x)` above). Accepts time and coordinate grid as\n            keyword arguments and returns a `Tensor` with shape that broadcasts\n            with `[dim]`.\n          8. \'constant_coeff\': A callable returning the coefficient of the\n            linear homogenous term (i.e. `r(t,x)` above). Same spec as above.\n            The `one_step_fn` callable returns a 2-tuple containing the next\n            coordinate grid, next values grid.\n      boundary_conditions: A list of size `dim` containing boundary conditions.\n        The i\'th element of the list is a 2-tuple containing the lower and upper\n        boundary condition for the boundary along the i`th axis.\n      start_step_count: Scalar integer `Tensor`. Initial value for the number of\n        time steps performed.\n        Default value: 0 (i.e. no previous steps performed).\n      num_steps: Positive int scalar `Tensor`. The number of time steps to take\n        when moving from `start_time` to `end_time`. Either this argument or the\n        `time_step` argument must be supplied (but not both). If num steps is\n        `k>=1`, uniform time steps of size `(t0 - t1)/k` are taken to evolve the\n        solution from `t0` to `t1`. Corresponds to the `n_steps` parameter\n        above.\n      time_step: The time step to take. Either this argument or the `num_steps`\n        argument must be supplied (but not both). The type of this argument may\n        be one of the following (in order of generality): (a) None in which case\n          `num_steps` must be supplied. (b) A positive real scalar `Tensor`. The\n          maximum time step to take. If the value of this argument is `dt`, then\n          the total number of steps taken is N = (t0 - t1) / dt rounded up to\n          the nearest integer. The first N-1 steps are of size dt and the last\n          step is of size `t0 - t1 - (N-1) * dt`. (c) A callable accepting the\n          current time and returning the size of the step to take. The input and\n          the output are real scalar `Tensor`s.\n      values_transform_fn: An optional callable applied to transform the\n        solution values at each time step. The callable is invoked after the\n        time step has been performed. The callable should accept the time of the\n        grid, the coordinate grid and the values grid and should return the\n        values grid. All input arguments to be passed by keyword.\n      dtype: The dtype to use.\n      **kwargs: Any other keyword args needed by an implementation.\n\n    Returns:\n      A tuple object containing at least the following attributes:\n        final_values_grid: A `Tensor` of same shape and dtype as `values_grid`.\n          Contains the final state of the values grid at time `end_time`.\n        final_coord_grid: A list of `Tensor`s of the same specification as\n          the input `coord_grid`. Final state of the coordinate grid at time\n          `end_time`.\n        step_count: The total step count (i.e. the sum of the `start_step_count`\n          and the number of steps performed in this call.).\n        final_time: The final time at which the evolution stopped. This value\n          is given by `max(min(end_time, start_time), 0)`.\n    """"""\n    pass\n\n  def fd_solver_forward(self,\n                        start_time,\n                        end_time,\n                        coord_grid,\n                        values_grid,\n                        one_step_fn=None,\n                        boundary_conditions=None,\n                        start_step_count=0,\n                        num_steps=None,\n                        time_step=None,\n                        values_transform_fn=None,\n                        dtype=None,\n                        **kwargs):\n    r""""""Returns a solver for the Fokker Plank equation of this process.\n\n    The Fokker Plank equation (also known as the Kolmogorov Forward equation)\n    associated to this Ito process is given by:\n\n    ```None\n      V_t + Sum[(mu_i(t, x) V)_i, 1<=i<=n]\n        - (1/2) Sum[ (D_{ij} V)_{ij}, 1 <= i,j <= n] = 0\n    ```\n\n    with the initial value condition $$V(0, x) = u(x)$$.\n\n    This method evolves a spatially discretized solution of the above PDE from\n    time `t0` to time `t1 > t0` (i.e. forwards in time).\n    The solution `V(t,x)` is assumed to be discretized on an `n`-dimensional\n    rectangular grid. A rectangular grid, G, in n-dimensions may be described\n    by specifying the coordinates of the points along each axis. For example,\n    a 2 x 4 grid in two dimensions can be specified by taking the cartesian\n    product of [1, 3] and [5, 6, 7, 8] to yield the grid points with\n    coordinates: `[(1, 5), (1, 6), (1, 7), (1, 8), (3, 5) ... (3, 8)]`.\n\n    Batching of solutions is supported. In this context, batching means\n    the ability to represent and evolve multiple independent functions `V`\n    (e.g. V1, V2 ...) simultaneously. A single discretized solution is specified\n    by stating its values at each grid point. This can be represented as a\n    `Tensor` of shape [d1, d2, ... dn] where di is the grid size along the `i`th\n    axis. A batch of such solutions is represented by a `Tensor` of shape:\n    [K, d1, d2, ... dn] where `K` is the batch size. This method only requires\n    that the input parameter `values_grid` be broadcastable with shape\n    [K, d1, ... dn].\n\n    The evolution of the solution from `t0` to `t1` is often done by\n    discretizing the differential equation to a difference equation along\n    the spatial and temporal axes. The temporal discretization is given by a\n    (sequence of) time steps [dt_1, dt_2, ... dt_k] such that the sum of the\n    time steps is equal to the total time step `t1 - t0`. If a uniform time\n    step is used, it may equivalently be specified by stating the number of\n    steps (n_steps) to take. This method provides both options via the\n    `time_step` and `num_steps` parameters. However, not all methods need\n    discretization along time direction (e.g. method of lines) so this argument\n    may not be applicable to some implementations.\n\n    The workhorse of this method is the `one_step_fn`. For the commonly used\n    methods, see functions in `math.pde.steppers` module.\n\n    The mapping between the arguments of this method and the above\n    equation are described in the Args section below.\n\n    For a simple instructive example of implementation of this method, see\n    `models.GenericItoProcess.fd_solver_forward`.\n\n    TODO(b/142309558): Complete documentation.\n\n    Args:\n      start_time: Real positive scalar `Tensor`. The start time of the grid.\n        Corresponds to time `t0` above.\n      end_time: Real scalar `Tensor` smaller than the `start_time` and greater\n        than zero. The time to step back to. Corresponds to time `t1` above.\n      coord_grid: List of `n` rank 1 real `Tensor`s. `n` is the dimension of the\n        domain. The i-th `Tensor` has shape, `[d_i]` where `d_i` is the size of\n        the grid along axis `i`. The coordinates of the grid points. Corresponds\n        to the spatial grid `G` above.\n      values_grid: Real `Tensor` containing the function values at time\n        `start_time` which have to be stepped back to time `end_time`. The shape\n        of the `Tensor` must broadcast with `[K, d_1, d_2, ..., d_n]`. The first\n        axis of size `K` is the values batch dimension and allows multiple\n        functions (with potentially different boundary/final conditions) to be\n        stepped back simultaneously.\n      one_step_fn: The transition kernel. A callable that consumes the following\n        arguments by keyword:\n          1. \'time\': Current time\n          2. \'next_time\': The next time to step to. For the backwards in time\n            evolution, this time will be smaller than the current time.\n          3. \'coord_grid\': The coordinate grid.\n          4. \'values_grid\': The values grid.\n          5. \'quadratic_coeff\': A callable returning the quadratic coefficients\n            of the PDE (i.e. `(1/2)D_{ij}(t, x)` above). The callable accepts\n            the time and  coordinate grid as keyword arguments and returns a\n            `Tensor` with shape that broadcasts with `[dim, dim]`.\n          6. \'linear_coeff\': A callable returning the linear coefficients of the\n            PDE (i.e. `mu_i(t, x)` above). Accepts time and coordinate grid as\n            keyword arguments and returns a `Tensor` with shape that broadcasts\n            with `[dim]`.\n          7. \'constant_coeff\': A callable returning the coefficient of the\n            linear homogenous term (i.e. `r(t,x)` above). Same spec as above.\n            The `one_step_fn` callable returns a 2-tuple containing the next\n            coordinate grid, next values grid.\n      boundary_conditions: A list of size `dim` containing boundary conditions.\n        The i\'th element of the list is a 2-tuple containing the lower and upper\n        boundary condition for the boundary along the i`th axis.\n      start_step_count: Scalar integer `Tensor`. Initial value for the number of\n        time steps performed.\n        Default value: 0 (i.e. no previous steps performed).\n      num_steps: Positive int scalar `Tensor`. The number of time steps to take\n        when moving from `start_time` to `end_time`. Either this argument or the\n        `time_step` argument must be supplied (but not both). If num steps is\n        `k>=1`, uniform time steps of size `(t0 - t1)/k` are taken to evolve the\n        solution from `t0` to `t1`. Corresponds to the `n_steps` parameter\n        above.\n      time_step: The time step to take. Either this argument or the `num_steps`\n        argument must be supplied (but not both). The type of this argument may\n        be one of the following (in order of generality): (a) None in which case\n          `num_steps` must be supplied. (b) A positive real scalar `Tensor`. The\n          maximum time step to take. If the value of this argument is `dt`, then\n          the total number of steps taken is N = (t1 - t0) / dt rounded up to\n          the nearest integer. The first N-1 steps are of size dt and the last\n          step is of size `t1 - t0 - (N-1) * dt`. (c) A callable accepting the\n          current time and returning the size of the step to take. The input and\n          the output are real scalar `Tensor`s.\n      values_transform_fn: An optional callable applied to transform the\n        solution values at each time step. The callable is invoked after the\n        time step has been performed. The callable should accept the time of the\n        grid, the coordinate grid and the values grid and should return the\n        values grid. All input arguments to be passed by keyword.\n      dtype: The dtype to use.\n      **kwargs: Any other keyword args needed by an implementation.\n\n    Returns:\n      A tuple object containing at least the following attributes:\n        final_values_grid: A `Tensor` of same shape and dtype as `values_grid`.\n          Contains the final state of the values grid at time `end_time`.\n        final_coord_grid: A list of `Tensor`s of the same specification as\n          the input `coord_grid`. Final state of the coordinate grid at time\n          `end_time`.\n        step_count: The total step count (i.e. the sum of the `start_step_count`\n          and the number of steps performed in this call.).\n        final_time: The final time at which the evolution stopped. This value\n          is given by `max(min(end_time, start_time), 0)`.\n    """"""\n    pass\n'"
tf_quant_finance/models/joined_ito_process.py,25,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Join a sequence of Ito Processes with specified correlations.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.models import euler_sampling\nfrom tf_quant_finance.models import generic_ito_process\nfrom tf_quant_finance.models import ito_process\nfrom tf_quant_finance.models import utils\n\n\nclass JoinedItoProcess(generic_ito_process.GenericItoProcess):\n  """"""Join of Ito Processes with specified time dependent correlations.\n\n  For a sequence of Ito processes `I_1, .., I_n` of dimensions `d_1,.., d_n`,\n  the class initializes a process `I` of dimension `d_1 + .. + d_n` with\n  marginal proceses `I_i` and a correlation function `Corr(t)`. That is, let the\n  Ito Process `I_i` describe an SDE\n\n  ```None\n  dX^i = a_i(t, X^i_t) dt + b_i(t, X^i_t) dW^i_t\n  ```\n\n  where `a_i(t, x)` is a function taking values in `R^{d_i}`, `b_i(t, X_t)` is a\n  function taking values in `d_i x d_i` matrices, `W_i` is a `d_i`-dimensional\n  Brownian motion.\n\n  Then `I` describes an SDE for the joint process `(X_1,..., X_n)` of dimension\n  `d:= d_1 + ... + d_n`\n\n  ```None\n  dX^i = a_i(t, X^i_t) dt + b_i(t, X^i_t) dB^i_t,\n  ```\n\n  where `(B_1, ..., B_n) = chol(t) * (W_1, ..., W_n)` for a Cholesky\n  decomposition `chol(t)` of the correlation matrix `Corr(t)` at time `t`.\n  Here `(W_1, ..., W_n)` is `d`-dimensional vector and `Corr(t)` is a `d x d`\n  correlation matrix.\n\n  `Corr(t)` is represented as a block-diagonal formed of a list of matrices\n  `[m_1(t), m_2(t), ..., m_k(t)]` with `sum(rank(m_i)) = d`.\n\n  # Example. # Black-scholes and Heston model join.\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tf_quant_finance as tff\n\n  dtype = tf.float64\n  # Define Black scholes model with zero rate and volatility `0.1`\n  sigma = 0.1\n  def drift_fn(t , x):\n    return -sigma**2 / 2\n  def vol_fn(t , x):\n    return sigma * tf.ones([1, 1], dtype=x.dtype)\n  black_scholes_process = tff.models.GenericItoProcess(\n      dim=1, drift_fn=drift_fn, volatility_fn=vol_fn, dtype=dtype)\n  # Define Heston Model\n  epsilon = tff.math.piecewise.PiecewiseConstantFunc(\n      jump_locations=[0.5], values=[0.01, 0.02], dtype=np.float64)\n  heston_process = tff.models.heston_model.HestonModel(\n      kappa=1.0, theta=0.04, epsilon=epsilon, rho=0.2, dtype=dtype)\n  # Define join process where correlation between `black_scholes_process` and\n  # log-asset price of the `heston_process` is 0.5.\n  corr_structure = [[[1.0, 0.5], [0.5, 1.0]], [1.0]]\n  # `corr_structure` corresponds to a `3 x 3` correlation matrix. Here Brownian\n  # motion of `black_scholes_process` is correlated only with the 1st dimension\n  # of `heston_process` but not with the second one.\n  join_process = JoinedItoProcess(\n      processes=[black_scholes_process, heston_process],\n      corr_structure=corr_structure)\n  # Sample 100,000 sample paths at times [0.1, 1.0] from the join process using\n  # Sobol random sequence\n  times = [0.1, 1.0]\n  # Wrap sample_paths method with a tf.function\n  sample_paths_fn = tf.function(process.sample_paths)\n  samples = sample_paths_fn(\n      times=times, time_step=0.01, num_samples=100000,\n      initial_state=np.array([0.0, 1.0, 0.04]),\n      random_type=random.RandomType.SOBOL)\n  # Estimated correlations.\n  np.corrcoef(samples[:, -1, :], rowvar=False)\n  # Expected result:\n  # [[1.        , 0.49567078, 0.08128067],\n  #  [0.49567078, 1.        , 0.16580689],\n  #  [0.08128067, 0.16580689, 1.        ]]\n  ```\n  """"""\n\n  def __init__(self, processes, corr_structure, dtype=None, name=None):\n    """"""Initializes a JoinedItoProcess.\n\n    Takes a list of `processes` which are instances of `tff.models.ItoProcess`\n    and a list `corr_structure` of correlation matrices and creates an Ito\n    process that joins `processes` using the correlation structure.\n    `corr_structure` describes block-diagonal structure of correlations for\n    the Brownian motions in `processes`. For example, if the dimension of the\n    JoinedItoProcess is `3` and\n    `corr_structure = [[[1.0, 0.5], [0.5, 1.0]], [1.0]]`, then the introduced\n    correlation is\n    `Corr(t) = [[1.0, 0.5, 0.0], [0.5, 1.0, 0.0], [0.0, 0.0, 1.0]]`,\n    where `Corr(t)` is the same as in the `JoinedItoProcess` docstring.\n\n    Args:\n      processes: A sequence of instances of `tff.models.ItoProcess`. All\n        processes should have the same `dtype.`\n      corr_structure: A list of correlation matrices. Each correlation matrix\n        is either a `Tensor` of the same `dtype` as the `processes` and\n        square shape  (i.e., `[d_i, d_i]` for some `d_i`) or a callable. The\n        callables should accept a scalar (stands for time `t`) and return a\n        square `Tensor`. The total dimension\n        `sum([m.shape[-1] for m in corr_structure]` of correlation\n        structure should be the same as the dimension of the `JoinedItoProcess`\n        `sum([p.dim() for p in processes])`.\n      dtype: The default `dtype` of the `processes`.\n        Default value: None which means that default dtypes inferred by\n          TensorFlow are used.\n      name: Python string. The name scope under which ops created by the methods\n        of this class are nested.\n        Default value: `None` which maps to the default name\n          `join_ito_process`.\n\n    Raises:\n      ValueError:\n        (a) If any of the `processes` is not an `ItoProcess`.\n        (b) If `processes` do not have the same `dtype`.\n    """"""\n    self._name = name or ""join_ito_process""\n    with tf.name_scope(self._name):\n      self._processes = []  # Input processes\n      dim = 0  # Dimension of the process\n      for process in processes:\n        if not isinstance(process, ito_process.ItoProcess):\n          raise ValueError(\n              ""All input process of JoinedItoProcess must be instances ""\n              ""of the ItoProcess class."")\n        self._processes.append(process)\n        d = process.dim()\n        dim += d  # Total dimension of the process\n        if dtype is None:\n          dtype = process.dtype()\n        elif dtype != process.dtype():\n          raise ValueError(""All processes should have the same `dtype`"")\n      self._corr_structure = [\n          corr if callable(corr) else tf.convert_to_tensor(\n              corr, dtype=dtype, name=""corr"")\n          for corr in corr_structure]\n      self._dim = dim\n\n      def _drift_fn(t, x):\n        """"""Drift function of the JoinedItoProcess.""""""\n        drifts = []\n        i1 = 0\n        i2 = 0\n        for p in self._processes:\n          dim = p.dim()\n          i2 += dim\n          position = x[..., i1:i2]\n          drift = tf.convert_to_tensor(p.drift_fn()(t, position),\n                                       dtype=dtype,\n                                       name=""drift"")\n          drift = tf.broadcast_to(drift, position.shape)\n          drifts.append(drift)\n          i1 += dim\n        return tf.concat(drifts, -1)\n\n      def _vol_fn(t, x):\n        """"""Volatility function of the JoinedItoProcess.""""""\n        vols = []\n        i1 = 0\n        i2 = 0\n        for p in self._processes:\n          dim = p.dim()\n          i2 += dim\n          position = x[..., i1:i2]\n          vol = tf.convert_to_tensor(p.volatility_fn()(t, position),\n                                     dtype=dtype,\n                                     name=""volatility"")\n          vol = tf.broadcast_to(vol, position.shape + [dim])\n          vols.append(vol)\n          i1 += dim\n        # Convert block diagonal volatilities to a dense correlation matrix\n        vol = utils.block_diagonal_to_dense(*vols)\n        # Compute Cholesky decomposition of the correlation structure\n        corr_structure = _get_parameters(\n            tf.expand_dims(t, -1), *self._corr_structure)\n        cholesky_decomp = [tf.linalg.cholesky(m) for m in corr_structure]\n        cholesky_decomp = utils.block_diagonal_to_dense(*cholesky_decomp)\n        return tf.linalg.matmul(vol, cholesky_decomp)\n      # The following will initialize the Generic Ito Process that has\n      # a sampling and PDE solving methods\n      super().__init__(dim, _drift_fn, _vol_fn, dtype, name)\n\n  def sample_paths(self,\n                   times,\n                   num_samples=1,\n                   initial_state=None,\n                   random_type=None,\n                   seed=None,\n                   time_step=None,\n                   swap_memory=True,\n                   skip=0,\n                   name=None):\n    """"""Returns a sample of paths from the process using Euler sampling.\n\n    Args:\n      times: Rank 1 `Tensor` of increasing positive real values. The times at\n        which the path points are to be evaluated.\n      num_samples: Positive scalar `int`. The number of paths to draw.\n        Default value: 1.\n      initial_state: `Tensor` of shape `[self._dim]`. The initial state of the\n        process.\n        Default value: None which maps to a zero initial state.\n      random_type: Enum value of `RandomType`. The type of (quasi)-random number\n        generator to use to generate the paths.\n        Default value: None which maps to the standard pseudo-random numbers.\n      seed: Seed for the random number generator. The seed is\n        only relevant if `random_type` is one of\n        `[STATELESS, PSEUDO, HALTON_RANDOMIZED, PSEUDO_ANTITHETIC,\n          STATELESS_ANTITHETIC]`. For `PSEUDO`, `PSEUDO_ANTITHETIC` and\n        `HALTON_RANDOMIZED` the seed should be an integer scalar `Tensor`. For\n        `STATELESS` and  `STATELESS_ANTITHETIC `must be supplied as an integer\n        `Tensor` of shape `[2]`.\n        Default value: `None` which means no seed is set.\n      time_step: Real scalar `Tensor`. The maximal distance between time points\n        in grid in Euler scheme.\n      swap_memory: A Python bool. Whether GPU-CPU memory swap is enabled for\n        this op. See an equivalent flag in `tf.while_loop` documentation for\n        more details. Useful when computing a gradient of the op since\n        `tf.while_loop` is used to propagate stochastic process in time.\n        Default value: True.\n      skip: `int32` 0-d `Tensor`. The number of initial points of the Sobol or\n        Halton sequence to skip. Used only when `random_type` is \'SOBOL\',\n        \'HALTON\', or \'HALTON_RANDOMIZED\', otherwise ignored.\n        Default value: `0`.\n      name: Python string. The name to give this op.\n        Default value: `None` which maps to `sample_paths` is used.\n\n    Returns:\n     A real `Tensor` of shape `[num_samples, k, n]` where `k` is the size of the\n     `times`, and `n` is the dimension of the process.\n\n    Raises:\n      ValueError: If `time_step` is not supplied.\n    """"""\n    if time_step is None:\n      raise ValueError(""`time_step` has to be supplied for JoinedItoProcess ""\n                       ""`sample_paths` method."")\n    name = name or self._name + ""sample_paths""\n    with tf.name_scope(name):\n      if initial_state is None:\n        initial_state = tf.zeros(self._dim, dtype=self.dtype(),\n                                 name=""initial_state"")\n      else:\n        if isinstance(initial_state, (tuple, list)):\n          initial_state = [tf.convert_to_tensor(state, dtype=self.dtype(),\n                                                name=""initial_state"")\n                           for state in initial_state]\n          initial_state = tf.stack(initial_state)\n        else:\n          initial_state = tf.convert_to_tensor(initial_state,\n                                               dtype=self.dtype(),\n                                               name=""initial_state"")\n      samples = euler_sampling.sample(self.dim(),\n                                      drift_fn=self.drift_fn(),\n                                      volatility_fn=self.volatility_fn(),\n                                      times=times,\n                                      time_step=time_step,\n                                      num_samples=num_samples,\n                                      initial_state=initial_state,\n                                      random_type=random_type,\n                                      seed=seed,\n                                      swap_memory=swap_memory,\n                                      skip=skip,\n                                      dtype=self.dtype())\n      return samples\n\n\ndef _get_parameters(times, *params):\n  """"""Gets parameter values at at specified `times`.""""""\n  res = []\n  for param in params:\n    if callable(param):\n      # Used only in drift and volatility computation.\n      # Here `times` is of shape [1]\n      t = tf.squeeze(times)\n      # The result has to have shape [1] + param.shape\n      param_value = tf.convert_to_tensor(param(t), dtype=times.dtype,\n                                         name=""param_value"")\n      res.append(tf.expand_dims(param_value, 0))\n    else:\n      res.append(param + tf.zeros(times.shape + param.shape, dtype=times.dtype))\n  return res\n'"
tf_quant_finance/models/joined_ito_process_test.py,5,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for the join of Ito processes.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass JoinedItoProcessTest(tf.test.TestCase):\n\n  def test_join_hull_white(self):\n    """"""Tests that join of Hull White is the same as VectorHullWhite.""""""\n    tf.random.set_seed(42)  # Fix global random seed\n    dtype = np.float64\n    instant_forward_rate_fn_1 = lambda t: 2 * [0.2]\n    process_1 = tff.models.hull_white.VectorHullWhiteModel(\n        dim=2, mean_reversion=[0.1, 0.2], volatility=[0.1, 0.2],\n        initial_discount_rate_fn=instant_forward_rate_fn_1,\n        dtype=dtype)\n    instant_forward_rate_fn_2 = lambda t: 3 * [0.1]\n    process_2 = tff.models.hull_white.VectorHullWhiteModel(\n        dim=3, mean_reversion=[0.3, 0.4, 0.5], volatility=[0.1, 0.1, 0.1],\n        initial_discount_rate_fn=instant_forward_rate_fn_2,\n        dtype=dtype)\n    # Correlation structure\n    corr_1 = [[1.0, 0.3, 0.2],\n              [0.3, 1.0, 0.5],\n              [0.2, 0.5, 1.0]]\n    def corr_2(t):\n      del t\n      return [[1.0, 0.1], [0.1, 1.0]]\n    matrices = [corr_1, corr_2]\n    process_join = tff.models.JoinedItoProcess([process_1, process_2], matrices)\n    expected_corr_matrix = np.array([[1.0, 0.3, 0.2, 0.0, 0.0],\n                                     [0.3, 1.0, 0.5, 0.0, 0.0],\n                                     [0.2, 0.5, 1.0, 0.0, 0.0],\n                                     [0.0, 0.0, 0.0, 1.0, 0.1],\n                                     [0.0, 0.0, 0.0, 0.1, 1.0]])\n    expected_mean = [0.0109434, 0.02356047, 0.01500711, 0.01915375, 0.0230985]\n    expected_var = [0.00475813, 0.01812692, 0.0043197, 0.004121, 0.00393469]\n    num_samples = 110000\n    samples = process_join.sample_paths(\n        times=[0.1, 0.5], time_step=0.01, num_samples=num_samples,\n        random_type=tff.math.random.RandomType.PSEUDO_ANTITHETIC,\n        seed=42)\n    self.assertEqual(samples.dtype, dtype)\n    self.assertEqual(samples.shape, [num_samples, 2, 5])\n    samples = self.evaluate(samples)\n    self.assertAllClose(np.corrcoef(samples[:, -1, :], rowvar=False),\n                        expected_corr_matrix, rtol=1e-2, atol=1e-2)\n    self.assertAllClose(np.mean(samples[:, -1, :], axis=0),\n                        expected_mean, rtol=1e-3, atol=1e-3)\n    self.assertAllClose(np.var(samples[:, -1, :], axis=0),\n                        expected_var, rtol=1e-3, atol=1e-3)\n\n  def test_invalid_processes(self):\n    """"""Tests that all proceses should be `ItoProcess`es.""""""\n    def drift_fn(t, x):\n      del t, x\n      return -1. / 2\n    def vol_fn(t, x):\n      del t\n      return tf.ones([1, 1], dtype=x.dtype)\n    process = tff.models.GenericItoProcess(\n        dim=1, drift_fn=drift_fn, volatility_fn=vol_fn)\n    with self.assertRaises(ValueError):\n      tff.models.JoinedItoProcess([process, lambda x: x], [[1.0], [1.0]])\n\n  def test_inconsistent_dtype(self):\n    """"""Tests that all proceses should have the same dtype.""""""\n    def drift_fn(t, x):\n      del t, x\n      return -1. / 2\n    def vol_fn(t, x):\n      del t\n      return tf.ones([1, 1], dtype=x.dtype)\n    process_1 = tff.models.GenericItoProcess(\n        dim=1, drift_fn=drift_fn, volatility_fn=vol_fn, dtype=np.float32)\n    process_2 = tff.models.GenericItoProcess(\n        dim=1, drift_fn=drift_fn, volatility_fn=vol_fn, dtype=np.float64)\n    with self.assertRaises(ValueError):\n      tff.models.JoinedItoProcess([process_1, process_2], [[1.0], [1.0]])\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/models/utils.py,18,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Common methods for model building.""""""\n\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance.math import random_ops as random\n\n\ndef generate_mc_normal_draws(num_normal_draws,\n                             num_time_steps,\n                             num_sample_paths,\n                             random_type,\n                             skip=0,\n                             seed=None,\n                             dtype=None,\n                             name=None):\n  """"""Generates normal random samples to be consumed by a Monte Carlo algorithm.\n\n  Many of Monte Carlo (MC) algorithms can be re-written so that all necessary\n  random (or quasi-random) variables are drawn in advance as a `Tensor` of\n  shape `[num_time_steps, num_samples, num_normal_draws]`, where\n  `num_time_steps` is the number of time steps Monte Carlo algorithm performs,\n  `num_sample_paths` is a number of sample paths of the Monte Carlo algorithm\n  and `num_normal_draws` is a number of independent normal draws per sample\n  paths.\n  For example, in order to use quasi-random numbers in a Monte Carlo algorithm,\n  the samples have to be drawn in advance.\n  The function generates a `Tensor`, say, `x` in a format such that for a\n  quasi-`random_type` `x[i]` is correspond to different dimensions of the\n  quasi-random sequence, so that it can be used in a Monte Carlo algorithm\n\n  Args:\n    num_normal_draws: A scalar int32 `Tensor`. The number of independent normal\n      draws at each time step for each sample path. Should be a graph\n      compilation constant.\n    num_time_steps: A scalar int32 `Tensor`. The number of time steps at which\n      to draw the independent normal samples. Should be a graph compilation\n      constant.\n    num_sample_paths: A scalar int32 `Tensor`. The number of trajectories (e.g.,\n      Monte Carlo paths) for which to draw the independent normal samples.\n      Should be a graph compilation constant.\n    random_type: Enum value of `tff.math.random.RandomType`. The type of\n      (quasi)-random number generator to use to generate the paths.\n    skip: `int32` 0-d `Tensor`. The number of initial points of the Sobol or\n      Halton sequence to skip. Used only when `random_type` is \'SOBOL\',\n      \'HALTON\', or \'HALTON_RANDOMIZED\', otherwise ignored.\n      Default value: `0`.\n      seed: Seed for the random number generator. The seed is\n        only relevant if `random_type` is one of\n        `[STATELESS, PSEUDO, HALTON_RANDOMIZED, PSEUDO_ANTITHETIC,\n          STATELESS_ANTITHETIC]`. For `PSEUDO`, `PSEUDO_ANTITHETIC` and\n        `HALTON_RANDOMIZED` the seed should be an Python integer. For\n        `STATELESS` and  `STATELESS_ANTITHETIC `must be supplied as an integer\n        `Tensor` of shape `[2]`.\n        Default value: `None` which means no seed is set.\n    dtype: The `dtype` of the output `Tensor`.\n      Default value: `None` which maps to `float32`.\n    name: Python string. The name to give this op.\n      Default value: `None` which maps to `generate_mc_normal_draws`.\n\n  Returns:\n   A `Tensor` of shape `[num_time_steps, num_sample_paths, num_normal_draws]`.\n  """"""\n  if name is None:\n    name = \'generate_mc_normal_draws\'\n  if skip is None:\n    skip = 0\n  with tf.name_scope(name):\n    if dtype is None:\n      dtype = tf.float32\n    # In case of quasi-random draws, the total dimension of the draws should be\n    # `num_time_steps * dim`\n    total_dimension = tf.zeros([num_time_steps * num_normal_draws], dtype=dtype,\n                               name=\'total_dimension\')\n    normal_draws = random.mv_normal_sample(\n        [num_sample_paths], mean=total_dimension,\n        random_type=random_type,\n        seed=seed,\n        skip=skip)\n    # Reshape and transpose\n    normal_draws = tf.reshape(\n        normal_draws, [num_sample_paths, num_time_steps, num_normal_draws])\n    # Shape [steps_num, num_samples, dim]\n    normal_draws = tf.transpose(normal_draws, [1, 0, 2])\n    return normal_draws\n\n\ndef maybe_update_along_axis(*,\n                            tensor,\n                            new_tensor,\n                            axis,\n                            ind,\n                            do_update,\n                            dtype=None,\n                            name=None):\n  """"""Replace `tensor` entries with `new_tensor` along a given axis.\n\n  This updates elements of `tensor` that correspond to the elements returned by\n  `numpy.take(updated, ind, axis)` with the corresponding elements of\n  `new_tensor`.\n\n  # Example\n  ```python\n  tensor = tf.ones([5, 4, 3, 2])\n  new_tensor = tf.zeros([5, 4, 3, 2])\n  updated_tensor = maybe_update_along_axis(tensor=tensor,\n                                           new_tensor=new_tensor,\n                                           axis=1,\n                                           ind=2,\n                                           do_update=True)\n  # Returns a `Tensor` of ones where\n  # `updated_tensor[:, 2, :, :].numpy() == 0`\n  ```\n  If the `do_update` is set to `False`, then the update does not happen unless\n  the number of dimensions along the `axis` is equal to 1. This functionality\n  is useful when, for example, aggregating samples of an Ito process.\n\n  Args:\n    tensor: A `Tensor` of any shape and `dtype`.\n    new_tensor: A `Tensor` of the same `dtype` as `tensor` and of shape\n      broadcastable with `tensor`.\n    axis: A Python integer. The axis of `tensor` along which the elements have\n      to be updated.\n    ind: An int32 scalar `Tensor` that denotes an index on the `axis` which\n      defines the updated slice of `tensor` (see example above).\n    do_update: A bool scalar `Tensor`. If `False`, the output is the same as\n      `tensor`, unless  the dimension of the `tensor` along the `axis` is equal\n      to 1.\n    dtype: The `dtype` of the input `Tensor`s.\n      Default value: `None` which means that default dtypes inferred by\n        TensorFlow are used.\n    name: Python string. The name to give this op.\n      Default value: `None` which maps to `maybe_update_along_axis`.\n\n  Returns:\n    A `Tensor` of the same shape and `dtype` as `tensor`.\n  """"""\n  name = name or \'maybe_update_along_axis\'\n  with tf.name_scope(name):\n    tensor = tf.convert_to_tensor(tensor, dtype=dtype,\n                                  name=\'tensor\')\n    dtype = tensor.dtype\n    new_tensor = tf.convert_to_tensor(new_tensor, dtype=dtype,\n                                      name=\'new_tensor\')\n    ind = tf.convert_to_tensor(ind, name=\'ind\')\n    do_update = tf.convert_to_tensor(do_update, name=\'do_update\')\n    size_along_axis = tensor.shape.as_list()[axis]\n    def _write_update_to_result():\n      one_hot = tf.one_hot(ind, depth=size_along_axis)\n      mask_shape = len(tensor.shape) * [1]\n      mask_shape[axis] = size_along_axis\n      mask = tf.reshape(one_hot > 0, mask_shape)\n      return tf.where(mask, new_tensor, tensor)\n    # Update only if size_along_axis > 1.\n    if size_along_axis > 1:\n      return tf.cond(do_update,\n                     _write_update_to_result,\n                     lambda: tensor)\n    else:\n      return new_tensor\n\n\ndef block_diagonal_to_dense(*matrices):\n  """"""Given a sequence of matrices, creates a block-diagonal dense matrix.""""""\n  operators = [tf.linalg.LinearOperatorFullMatrix(m) for m in matrices]\n  return tf.linalg.LinearOperatorBlockDiag(operators).to_dense()\n'"
tf_quant_finance/models/utils_test.py,10,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for the `utils` module.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\nfrom tf_quant_finance.models import utils\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass UtilsTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (\'SinglePrecision\', np.float32),\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_sobol_numbers_generation(self, dtype):\n    """"""Sobol random dtype results in the correct draws.""""""\n    num_draws = tf.constant(2, dtype=tf.int32)\n    steps_num = tf.constant(3, dtype=tf.int32)\n    num_samples = tf.constant(4, dtype=tf.int32)\n    random_type = tff.math.random.RandomType.SOBOL\n    skip = 10\n    samples = utils.generate_mc_normal_draws(\n        num_normal_draws=num_draws, num_time_steps=steps_num,\n        num_sample_paths=num_samples, random_type=random_type,\n        dtype=dtype, skip=skip)\n    expected_samples = [[[0.8871465, 0.48877636],\n                         [-0.8871465, -0.48877636],\n                         [0.48877636, 0.8871465],\n                         [-0.15731068, 0.15731068]],\n                        [[0.8871465, -1.5341204],\n                         [1.5341204, -0.15731068],\n                         [-0.15731068, 1.5341204],\n                         [-0.8871465, 0.48877636]],\n                        [[-0.15731068, 1.5341204],\n                         [0.15731068, -0.48877636],\n                         [-1.5341204, 0.8871465],\n                         [0.8871465, -1.5341204]]]\n    self.assertAllClose(samples, expected_samples, rtol=1e-5, atol=1e-5)\n\n  @parameterized.named_parameters(\n      (\'SinglePrecision\', np.float32),\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_maybe_update_along_axis(self, dtype):\n    """"""Tests that the values are updated correctly.""""""\n    tensor = tf.ones([5, 4, 3, 2], dtype=dtype)\n    new_tensor = tf.zeros([5, 4, 1, 2], dtype=dtype)\n    @tf.function\n    def maybe_update_along_axis(do_update):\n      return utils.maybe_update_along_axis(\n          tensor=tensor, new_tensor=new_tensor, axis=1, ind=2,\n          do_update=do_update)\n    updated_tensor = maybe_update_along_axis(True)\n    with self.subTest(name=\'Shape\'):\n      self.assertEqual(updated_tensor.shape, tensor.shape)\n    with self.subTest(name=\'UpdatedVals\'):\n      self.assertAllEqual(updated_tensor[:, 2, :, :],\n                          tf.zeros_like(updated_tensor[:, 2, :, :]))\n    with self.subTest(name=\'NotUpdatedVals\'):\n      self.assertAllEqual(updated_tensor[:, 1, :, :],\n                          tf.ones_like(updated_tensor[:, 2, :, :]))\n    with self.subTest(name=\'DoNotUpdateVals\'):\n      not_updated_tensor = maybe_update_along_axis(False)\n      self.assertAllEqual(not_updated_tensor, tensor)\n\n  def test_block_diagonal_to_dense(self):\n    matrices = [[[1.0, 0.1], [0.1, 1.0]],\n                [[1.0, 0.3, 0.2],\n                 [0.3, 1.0, 0.5],\n                 [0.2, 0.5, 1.0]], [[1.0]]]\n    dense = utils.block_diagonal_to_dense(*matrices)\n    expected_result = [[1.0, 0.1, 0.0, 0.0, 0.0, 0.0],\n                       [0.1, 1.0, 0.0, 0.0, 0.0, 0.0],\n                       [0.0, 0.0, 1.0, 0.3, 0.2, 0.0],\n                       [0.0, 0.0, 0.3, 1.0, 0.5, 0.0],\n                       [0.0, 0.0, 0.2, 0.5, 1.0, 0.0],\n                       [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]\n    self.assertAllClose(dense, expected_result, rtol=1e-5, atol=1e-5)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/rates/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Functions to handle rates.""""""\n\nfrom tf_quant_finance.rates import cashflows\nfrom tf_quant_finance.rates import constant_fwd\nfrom tf_quant_finance.rates import forwards\nfrom tf_quant_finance.rates import hagan_west\nfrom tf_quant_finance.rates import swap_curve_bootstrap as swap_curve_boot_lib\nfrom tf_quant_finance.rates import swap_curve_fit as swap_curve_fit_lib\nfrom tf_quant_finance.rates.swap_curve_common import SwapCurveBuilderResult\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n\nswap_curve_fit = swap_curve_fit_lib.swap_curve_fit\nswap_curve_bootstrap = swap_curve_boot_lib.swap_curve_bootstrap\n\n_allowed_symbols = [\n    \'cashflows\',\n    \'forwards\',\n    \'hagan_west\',\n    \'constant_fwd\',\n    \'swap_curve_fit\',\n    \'swap_curve_bootstrap\',\n    \'SwapCurveBuilderResult\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/rates/cashflows.py,27,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Collection of functions to compute properties of cashflows.""""""\n\nimport tensorflow.compat.v2 as tf\n\n\ndef pv_from_yields(cashflows,\n                   times,\n                   yields,\n                   groups=None,\n                   dtype=None,\n                   name=None):\n  """"""Computes present value of cashflows given yields.\n\n  For a more complete description of the terminology as well as the mathematics\n  of pricing bonds, see Ref [1]. In particular, note that `yields` here refers\n  to the yield of the bond as defined in Section 4.4 of Ref [1]. This is\n  sometimes also referred to as the internal rate of return of a bond.\n\n  #### Example\n\n  The following example demonstrates the present value computation for two\n  bonds. Both bonds have 1000 face value with semi-annual coupons. The first\n  bond has 4% coupon rate and 2 year expiry. The second has 6% coupon rate and\n  3 year expiry. The yields to maturity (ytm) are 7% and 5% respectively.\n\n  ```python\n    dtype = np.float64\n\n    # The first element is the ytm of the first bond and the second is the\n    # yield of the second bond.\n    yields_to_maturity = np.array([0.07, 0.05], dtype=dtype)\n\n    # 2 and 3 year bonds with 1000 face value and 4%, 6% semi-annual coupons.\n    # Note that the first four entries in the cashflows are the cashflows of\n    # the first bond (group=0) and the next six are the cashflows of the second\n    # bond (group=1).\n    cashflows = np.array([20, 20, 20, 1020, 30, 30, 30, 30, 30, 1030],\n                         dtype=dtype)\n\n    # The times of the cashflows.\n    times = np.array([0.5, 1, 1.5, 2, 0.5, 1, 1.50, 2, 2.5, 3], dtype=dtype)\n\n    # Group entries take values between 0 and 1 (inclusive) as there are two\n    # bonds. One needs to assign each of the cashflow entries to one group or\n    # the other.\n    groups = np.array([0] * 4 + [1] * 6)\n\n    # Produces [942.712, 1025.778] as the values of the two bonds.\n    present_values = pv_from_yields(\n        cashflows, times, yields_to_maturity, groups=groups, dtype=dtype)\n  ```\n\n  #### References:\n\n  [1]: John C. Hull. Options, Futures and Other Derivatives. Ninth Edition.\n    June 2006.\n\n  Args:\n    cashflows: Real rank 1 `Tensor` of size `n`. The set of cashflows underlying\n      the bonds.\n    times: Real positive rank 1 `Tensor` of size `n`. The set of times at which\n      the corresponding cashflows occur quoted in years.\n    yields: Real rank 1 `Tensor` of size `1` if `groups` is None or of size `k`\n      if the maximum value in the `groups` is of `k-1`. The continuously\n      compounded yields to maturity/internal rate of returns corresponding to\n      each of the cashflow groups. The `i`th component is the yield to apply to\n      all the cashflows with group label `i` if `groups` is not None. If\n      `groups` is None, then this is a `Tensor` of size `[1]` and the only\n      component is the yield that applies to all the cashflows.\n    groups: Optional int `Tensor` of size `n` containing values between 0 and\n      `k-1` where `k` is the number of related cashflows.\n      Default value: None. This implies that all the cashflows are treated as a\n        single group.\n    dtype: `tf.Dtype`. If supplied the dtype for the `cashflows`, `times` and\n      yields (float32 or float64).\n      Default value: None which maps to the default dtype inferred by\n        Tensorflow.\n    name: Python str. The name to give to the ops created by this function.\n      Default value: None which maps to \'pv_from_yields\'.\n\n  Returns:\n    Real rank 1 `Tensor` of size `k` if groups is not `None` else of size `[1]`.\n      The present value of the cashflows. The `i`th component is the present\n      value of the cashflows in group `i` or to the entirety of the cashflows\n      if `groups` is None.\n  """"""\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'pv_from_yields\',\n      values=[cashflows, times, yields, groups]):\n    cashflows = tf.convert_to_tensor(cashflows, dtype=dtype, name=\'cashflows\')\n    times = tf.convert_to_tensor(times, dtype=dtype, name=\'times\')\n    yields = tf.convert_to_tensor(yields, dtype=dtype, name=\'yields\')\n    cashflow_yields = yields\n    if groups is not None:\n      groups = tf.convert_to_tensor(groups, name=\'groups\')\n      cashflow_yields = tf.gather(yields, groups)\n    discounted = cashflows * tf.math.exp(-times * cashflow_yields)\n    if groups is not None:\n      return tf.math.segment_sum(discounted, groups)\n    return tf.math.reduce_sum(discounted, keepdims=True)\n\n\ndef yields_from_pv(cashflows,\n                   times,\n                   present_values,\n                   groups=None,\n                   tolerance=1e-8,\n                   max_iterations=10,\n                   dtype=None,\n                   name=None):\n  """"""Computes yields to maturity from present values of cashflows.\n\n  For a complete description of the terminology as well as the mathematics\n  of computing bond yields, see Ref [1]. Note that `yields` here refers\n  to the yield of the bond as defined in Section 4.4 of Ref [1]. This is\n  sometimes also referred to as the internal rate of return of a bond.\n\n  #### Example\n\n  The following example demonstrates the yield computation for two\n  bonds. Both bonds have 1000 face value with semi-annual coupons. The first\n  bond has 4% coupon rate and 2 year expiry. The second has 6% coupon rate and\n  3 year expiry. The true yields to maturity (ytm) are 7% and 5% respectively.\n\n  ```python\n    dtype = np.float64\n\n    # The first element is the present value (PV) of the first bond and the\n    # second is the PV of the second bond.\n    present_values = np.array([942.71187528177757, 1025.7777300221542],\n                              dtype=dtype)\n\n    # 2 and 3 year bonds with 1000 face value and 4%, 6% semi-annual coupons.\n    # Note that the first four entries in the cashflows are the cashflows of\n    # the first bond (group=0) and the next six are the cashflows of the second\n    # bond (group=1).\n    cashflows = np.array([20, 20, 20, 1020, 30, 30, 30, 30, 30, 1030],\n                         dtype=dtype)\n\n    # The times of the cashflows.\n    times = np.array([0.5, 1, 1.5, 2, 0.5, 1, 1.50, 2, 2.5, 3], dtype=dtype)\n\n    # Group entries take values between 0 and 1 (inclusive) as there are two\n    # bonds. One needs to assign each of the cashflow entries to one group or\n    # the other.\n    groups = np.array([0] * 4 + [1] * 6)\n\n    # Expected yields = [0.07, 0.05]\n    yields = yields_from_pv(\n        cashflows, times, present_values, groups=groups, dtype=dtype)\n  ```\n\n  #### References:\n\n  [1]: John C. Hull. Options, Futures and Other Derivatives. Ninth Edition.\n    June 2006.\n\n  Args:\n    cashflows: Real rank 1 `Tensor` of size `n`. The set of cashflows underlying\n      the bonds.\n    times: Real positive rank 1 `Tensor` of size `n`. The set of times at which\n      the corresponding cashflows occur quoted in years.\n    present_values: Real rank 1 `Tensor` of size `k` where `k-1` is the maximum\n      value in the `groups` arg if supplied. If `groups` is not supplied, then\n      this is a `Tensor` of size `1`. The present values corresponding to each\n      of the cashflow groups. The `i`th component is the present value of all\n      the cashflows with group label `i` (or the present value of all the\n      cashflows if `groups=None`).\n    groups: Optional int `Tensor` of size `n` containing values between 0 and\n      `k-1` where `k` is the number of related cashflows.\n      Default value: None. This implies that all the cashflows are treated as a\n        single group.\n    tolerance: Positive real scalar `Tensor`. The tolerance for the estimated\n      yields. The yields are computed using a Newton root finder. The iterations\n      stop when the inferred yields change by less than this tolerance or the\n      maximum iterations are exhausted (whichever is earlier).\n      Default value: 1e-8.\n    max_iterations: Positive scalar int `Tensor`. The maximum number of\n      iterations to use to compute the yields. The iterations stop when the max\n      iterations is exhausted or the tolerance is reached (whichever is\n      earlier). Supply `None` to remove the limit on the number of iterations.\n      Default value: 10.\n    dtype: `tf.Dtype`. If supplied the dtype for the `cashflows`, `times` and\n      `present_values` (float32 or float64).\n      Default value: None which maps to the default dtype inferred by\n        Tensorflow.\n    name: Python str. The name to give to the ops created by this function.\n      Default value: None which maps to \'yields_from_pv\'.\n\n  Returns:\n    Real rank 1 `Tensor` of size `k`. The yield to maturity of the cashflows.\n      The `i`th component is the yield to maturity of the cashflows in group\n      `i`.\n  """"""\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'yields_from_pv\',\n      values=[\n          cashflows, times, present_values, groups, tolerance, max_iterations\n      ]):\n    cashflows = tf.convert_to_tensor(cashflows, dtype=dtype, name=\'cashflows\')\n    times = tf.convert_to_tensor(times, dtype=dtype, name=\'times\')\n    present_values = tf.convert_to_tensor(\n        present_values, dtype=dtype, name=\'present_values\')\n    if groups is None:\n      groups = tf.zeros_like(cashflows, dtype=tf.int32, name=\'groups\')\n    else:\n      groups = tf.convert_to_tensor(groups, name=\'groups\')\n\n    def pv_and_duration(yields):\n      cashflow_yields = tf.gather(yields, groups)\n      discounted = cashflows * tf.math.exp(-times * cashflow_yields)\n      durations = tf.math.segment_sum(discounted * times, groups)\n      pvs = tf.math.segment_sum(discounted, groups)\n      return pvs, durations\n\n    yields0 = tf.zeros_like(present_values)\n\n    def _cond(should_stop, yields):\n      del yields\n      return tf.math.logical_not(should_stop)\n\n    def _body(should_stop, yields):\n      del should_stop\n      pvs, durations = pv_and_duration(yields)\n      delta_yields = (pvs - present_values) / durations\n      next_should_stop = (tf.math.reduce_max(tf.abs(delta_yields)) <= tolerance)\n      return (next_should_stop, yields + delta_yields)\n\n    loop_vars = (tf.convert_to_tensor(False), yields0)\n    _, estimated_yields = tf.while_loop(\n        _cond,\n        _body,\n        loop_vars,\n        shape_invariants=(tf.TensorShape([]), tf.TensorShape([None])),\n        maximum_iterations=max_iterations,\n        parallel_iterations=1)\n    return estimated_yields\n'"
tf_quant_finance/rates/cashflows_test.py,2,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for cashflows module.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\nfrom tf_quant_finance.rates import cashflows as cashflows_lib\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass CashflowsTest(tf.test.TestCase):\n\n  def test_pv_from_yields_no_group(self):\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      yield_rate = 0.04\n      coupon_rate = 0.04\n      # Fifteen year bond with semi-annual coupons.\n      cashflows = np.array(\n          [coupon_rate * 500] * 29 + [1000 + coupon_rate * 500], dtype=dtype)\n      times = np.linspace(0.5, 15, num=30).astype(dtype)\n      expected_pv = 995.50315587\n      actual_pv = self.evaluate(\n          cashflows_lib.pv_from_yields(\n              cashflows, times, [yield_rate], dtype=dtype))\n      np.testing.assert_allclose(expected_pv, actual_pv)\n\n  def test_pv_from_yields_grouped(self):\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      yield_rates = [0.07, 0.05]\n      # 2 and 3 year bonds with 1000 face value and 4%, 6% semi-annual coupons.\n      cashflows = np.array([20, 20, 20, 1020, 30, 30, 30, 30, 30, 1030],\n                           dtype=dtype)\n      times = np.array([0.5, 1, 1.5, 2, 0.5, 1, 1.50, 2, 2.5, 3], dtype=dtype)\n      groups = np.array([0] * 4 + [1] * 6)\n      expected_pvs = np.array([942.71187528177757, 1025.7777300221542])\n      actual_pvs = self.evaluate(\n          cashflows_lib.pv_from_yields(\n              cashflows, times, yield_rates, groups=groups, dtype=dtype))\n      np.testing.assert_allclose(expected_pvs, actual_pvs)\n\n  def test_pv_zero_yields(self):\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      yield_rates = [0., 0.]\n      # 2 and 3 year bonds with 1000 face value and 4%, 6% semi-annual coupons.\n      cashflows = np.array([20, 20, 20, 1020, 30, 30, 30, 30, 30, 1030],\n                           dtype=dtype)\n      times = np.array([0.5, 1, 1.5, 2, 0.5, 1, 1.50, 2, 2.5, 3], dtype=dtype)\n      groups = np.array([0] * 4 + [1] * 6)\n      expected_pvs = np.array([1080., 1180.])\n      actual_pvs = self.evaluate(\n          cashflows_lib.pv_from_yields(\n              cashflows, times, yield_rates, groups=groups, dtype=dtype))\n      np.testing.assert_allclose(expected_pvs, actual_pvs)\n\n  def test_pv_infinite_yields(self):\n    """"""Tests in the limit of very large yields.""""""\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      yield_rates = [300., 300.]\n      # 2 and 3 year bonds with 1000 face value and 4%, 6% semi-annual coupons.\n      cashflows = np.array([20, 20, 20, 1020, 30, 30, 30, 30, 30, 1030],\n                           dtype=dtype)\n      times = np.array([0.5, 1, 1.5, 2, 0.5, 1, 1.50, 2, 2.5, 3], dtype=dtype)\n      groups = np.array([0] * 4 + [1] * 6)\n      expected_pvs = np.array([0., 0.])\n      actual_pvs = self.evaluate(\n          cashflows_lib.pv_from_yields(\n              cashflows, times, yield_rates, groups=groups, dtype=dtype))\n      np.testing.assert_allclose(expected_pvs, actual_pvs, atol=1e-9)\n\n  def test_yields_from_pvs_no_group(self):\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      coupon_rate = 0.04\n      # Fifteen year bond with semi-annual coupons.\n      cashflows = np.array(\n          [coupon_rate * 500] * 29 + [1000 + coupon_rate * 500], dtype=dtype)\n      pv = 995.50315587\n      times = np.linspace(0.5, 15, num=30).astype(dtype)\n      expected_yield_rate = 0.04\n      actual_yield_rate = self.evaluate(\n          cashflows_lib.yields_from_pv(cashflows, times, [pv], dtype=dtype))\n      np.testing.assert_allclose(expected_yield_rate, actual_yield_rate)\n\n  def test_yields_from_pv_grouped(self):\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      # 2 and 3 year bonds with 1000 face value and 4%, 6% semi-annual coupons.\n      cashflows = np.array([20, 20, 20, 1020, 30, 30, 30, 30, 30, 1030],\n                           dtype=dtype)\n      times = np.array([0.5, 1, 1.5, 2, 0.5, 1, 1.50, 2, 2.5, 3], dtype=dtype)\n      groups = np.array([0] * 4 + [1] * 6)\n      pvs = np.array([942.71187528177757, 1025.7777300221542])\n      expected_yield_rates = [0.07, 0.05]\n      actual_yield_rates = self.evaluate(\n          cashflows_lib.yields_from_pv(\n              cashflows, times, pvs, groups=groups, dtype=dtype))\n      np.testing.assert_allclose(\n          expected_yield_rates, actual_yield_rates, atol=1e-7)\n\n  def test_yield_saturated_pv(self):\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      # 2 and 3 year bonds with 1000 face value and 4%, 6% semi-annual coupons.\n      cashflows = np.array([20, 20, 20, 1020, 30, 30, 30, 30, 30, 1030],\n                           dtype=dtype)\n      times = np.array([0.5, 1, 1.5, 2, 0.5, 1, 1.50, 2, 2.5, 3], dtype=dtype)\n      groups = np.array([0] * 4 + [1] * 6)\n      pvs = np.array([1080., 1180.])\n      expected_yields = [0., 0.]\n      actual_yields = self.evaluate(\n          cashflows_lib.yields_from_pv(\n              cashflows, times, pvs, groups=groups, dtype=dtype))\n      np.testing.assert_allclose(expected_yields, actual_yields, atol=1e-9)\n\n  def test_yield_small_pv(self):\n    """"""Tests in the limit where implied yields are high.""""""\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      # 2 and 3 year bonds with 1000 face value and 4%, 6% semi-annual coupons.\n      cashflows = np.array([20, 20, 20, 1020, 30, 30, 30, 30, 30, 1030],\n                           dtype=dtype)\n      times = np.array([0.5, 1, 1.5, 2, 0.5, 1, 1.50, 2, 2.5, 3], dtype=dtype)\n      groups = np.array([0] * 4 + [1] * 6)\n      pvs = np.array([7.45333412e-05, 2.27476813e-08])\n      expected_yields = [25.0, 42.0]\n      actual_yields = self.evaluate(\n          cashflows_lib.yields_from_pv(\n              cashflows,\n              times,\n              pvs,\n              groups=groups,\n              dtype=dtype,\n              max_iterations=30))\n      np.testing.assert_allclose(expected_yields, actual_yields, atol=1e-9)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/rates/forwards.py,12,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Collection of functions to compute properties of cashflows.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math import segment_ops\n\n\ndef forward_rates_from_yields(yields,\n                              times,\n                              groups=None,\n                              dtype=None,\n                              name=None):\n  """"""Computes forward rates given a set of zero rates.\n\n  Denote the price of a zero coupon bond maturing at time `t` by `Z(t)`. Then\n  the zero rate to time `t` is defined as\n\n  ```None\n    r(t) = - ln(Z(t)) / t       (1)\n\n  ```\n\n  This is the (continuously compounded) interest rate that applies between time\n  `0` and time `t` as seen at time `0`. The forward rate between times `t1` and\n  `t2` is defined as the interest rate that applies to the period `[t1, t2]`\n  as seen from today. It is related to the zero coupon bond prices by\n\n  ```None\n    exp(-f(t1, t2)(t2-t1)) = Z(t2) / Z(t1)                 (2)\n    f(t1, t2) = - (ln Z(t2) - ln Z(t1)) / (t2 - t1)        (3)\n    f(t1, t2) = (t2 * r(t2) - t1 * r(t1)) / (t2 - t1)      (4)\n  ```\n\n  Given a sequence of increasing times `[t1, t2, ... tn]` and the zero rates\n  for those times, this function computes the forward rates that apply to the\n  consecutive time intervals i.e. `[0, t1], [t1, t2], ... [t_{n-1}, tn]` using\n  Eq. (4) above. Note that for the interval `[0, t1]` the forward rate is the\n  same as the zero rate.\n\n  Additionally, this function supports this computation for a batch of such\n  rates. Batching is made slightly complicated by the fact that different\n  zero curves may have different numbers of tenors (the parameter `n` above).\n  Instead of a batch as an extra dimension, we support the concept of groups\n  (also see documentation for `tf.segment_sum` which uses the same concept).\n\n  #### Example\n\n  The following example illustrates this method along with the concept of\n  groups. Assuming there are two sets of zero rates (e.g. for different\n  currencies) whose implied forward rates are needed. The first set has a total\n  of three marked tenors at `[0.25, 0.5, 1.0]`. The second set\n  has four marked tenors at `[0.25, 0.5, 1.0, 1.5]`.\n  Suppose, the zero rates for the first set are:\n  `[0.04, 0.041, 0.044]` and the second are `[0.022, 0.025, 0.028, 0.036]`.\n  Then this data is batched together as follows:\n  Groups: [0,    0    0,   1,    1,   1    1  ]\n  First three times for group 0, next four for group 1.\n  Times:  [0.25, 0.5, 1.0, 0.25, 0.5, 1.0, 1.5]\n  First three rates for group 0, next four for group 1.\n  Rates:  [0.04, 0.041, 0.044, 0.022, 0.025, 0.028, 0.036]\n\n\n  ```python\n    dtype = np.float64\n    groups = np.array([0, 0, 0, 1, 1, 1, 1])\n    times = np.array([0.25, 0.5, 1.0, 0.25, 0.5, 1.0, 1.5], dtype=dtype)\n    rates = np.array([0.04, 0.041, 0.044, 0.022, 0.025, 0.028, 0.036],\n                     dtype=dtype)\n    forward_rates = forward_rates_from_yields(\n        rates, times, groups=groups, dtype=dtype)\n  ```\n\n  #### References:\n\n  [1]: John C. Hull. Options, Futures and Other Derivatives. Ninth Edition.\n    June 2006.\n\n  Args:\n    yields: Real rank 1 `Tensor` of size `n`. The discount/zero rates.\n    times: Real positive rank 1 `Tensor` of size `n`. The set of times\n      corresponding to the supplied zero rates. If no `groups` is supplied, then\n      the whole array should be sorted in an increasing order. If `groups` are\n      supplied, then the times within a group should be in an increasing order.\n    groups: Optional int `Tensor` of size `n` containing values between 0 and\n      `k-1` where `k` is the number of different curves.\n      Default value: None. This implies that all the rates are treated as a\n        single group.\n    dtype: `tf.Dtype`. If supplied the dtype for the `yields` and `times`.\n      Default value: None which maps to the default dtype inferred by Tensorflow\n        (float32).\n    name: Python str. The name to give to the ops created by this function.\n      Default value: None which maps to \'forward_rates_from_yields\'.\n\n  Returns:\n    Real rank 1 `Tensor` of size `n` containing the forward rate that applies\n    for each successive time interval (within each group if groups are\n    specified).\n  """"""\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'forward_rates_from_yields\',\n      values=[yields, times, groups]):\n    yields = tf.convert_to_tensor(yields, dtype=dtype, name=\'yields\')\n    times = tf.convert_to_tensor(times, dtype=dtype, name=\'times\')\n    if groups is not None:\n      groups = tf.convert_to_tensor(groups, name=\'groups\')\n    # (t2 * r(t2) - t1 * r(t1)) / (t2 - t1)\n    rate_times = yields * times\n    diff_rate_times = segment_ops.segment_diff(\n        rate_times, order=1, exclusive=False, segment_ids=groups)\n    diff_times = segment_ops.segment_diff(\n        times, order=1, exclusive=False, segment_ids=groups)\n    return diff_rate_times / diff_times\n\n\ndef yields_from_forward_rates(discrete_forwards,\n                              times,\n                              groups=None,\n                              dtype=None,\n                              name=None):\n  """"""Computes yield rates from discrete forward rates.\n\n  Denote the price of a zero coupon bond maturing at time `t` by `Z(t)`. Then\n  the zero rate to time `t` is defined as\n\n  ```None\n    r(t) = - ln(Z(t)) / t       (1)\n\n  ```\n\n  This is the (continuously compounded) interest rate that applies between time\n  `0` and time `t` as seen at time `0`. The forward rate between times `t1` and\n  `t2` is defined as the interest rate that applies to the period `[t1, t2]`\n  as seen from today. It is related to the zero coupon bond prices by\n\n  ```None\n    exp(-f(t1, t2)(t2-t1)) = Z(t2) / Z(t1)                 (2)\n    f(t1, t2) = - (ln Z(t2) - ln Z(t1)) / (t2 - t1)        (3)\n    f(t1, t2) = (t2 * r(t2) - t1 * r(t1)) / (t2 - t1)      (4)\n  ```\n\n  Given a sequence of increasing times `[t1, t2, ... tn]` and the forward rates\n  for the consecutive time intervals, i.e. `[0, t1]`, `[t1, t2]` to\n  `[t_{n-1}, tn]`, this function computes the yields to maturity for maturities\n  `[t1, t2, ... tn]` using Eq. (4) above.\n\n  Additionally, this function supports this computation for a batch of such\n  forward rates. Batching is made slightly complicated by the fact that\n  different zero curves may have different numbers of tenors (the parameter `n`\n  above). Instead of a batch as an extra dimension, we support the concept of\n  groups (also see documentation for `tf.segment_sum` which uses the same\n  concept).\n\n  #### Example\n\n  The following example illustrates this method along with the concept of\n  groups. Assuming there are two sets of zero rates (e.g. for different\n  currencies) whose implied forward rates are needed. The first set has a total\n  of three marked tenors at `[0.25, 0.5, 1.0]`. The second set\n  has four marked tenors at `[0.25, 0.5, 1.0, 1.5]`.\n  Suppose, the forward rates for the first set are:\n  `[0.04, 0.041, 0.044]` and the second are `[0.022, 0.025, 0.028, 0.036]`.\n  Then this data is batched together as follows:\n  Groups:   [0,    0    0,   1,    1,   1    1  ]\n  First three times for group 0, next four for group 1.\n  Times:    [0.25, 0.5, 1.0, 0.25, 0.5, 1.0, 1.5]\n  First three discrete forwards for group 0, next four for group 1.\n  Forwards: [0.04, 0.042, 0.047, 0.022, 0.028, 0.031, 0.052]\n\n  ```python\n    dtype = np.float64\n    groups = np.array([0, 0, 0, 1, 1, 1, 1])\n    times = np.array([0.25, 0.5, 1.0, 0.25, 0.5, 1.0, 1.5], dtype=dtype)\n    discrete_forwards = np.array(\n        [0.04, 0.042, 0.047, 0.022, 0.028, 0.031, 0.052], dtype=dtype)\n    yields = yields_from_forward_rates(discrete_forwards, times,\n                                       groups=groups, dtype=dtype)\n    # Produces: [0.04, 0.041, 0.044, 0.022, 0.025, 0.028, 0.036]\n  ```\n\n  #### References:\n\n  [1]: John C. Hull. Options, Futures and Other Derivatives. Ninth Edition.\n    June 2006.\n\n  Args:\n    discrete_forwards: Real rank 1 `Tensor` of size `n`. The forward rates for\n      the time periods specified. Note that the first value applies between `0`\n      and time `times[0]`.\n    times: Real positive rank 1 `Tensor` of size `n`. The set of times\n      corresponding to the supplied zero rates. If no `groups` is supplied, then\n      the whole array should be sorted in an increasing order. If `groups` are\n      supplied, then the times within a group should be in an increasing order.\n    groups: Optional int `Tensor` of size `n` containing values between 0 and\n      `k-1` where `k` is the number of different curves.\n      Default value: None. This implies that all the rates are treated as a\n        single group.\n    dtype: `tf.Dtype`. If supplied the dtype for the `discrete_forwards` and\n      `times`.\n      Default value: None which maps to the default dtype inferred by\n      Tensorflow.\n    name: Python str. The name to give to the ops created by this function.\n      Default value: None which maps to \'yields_from_forward_rates\'.\n\n  Returns:\n    yields: Real rank 1 `Tensor` of size `n` containing the zero coupon yields\n    that for the supplied maturities (within each group if groups are\n    specified).\n  """"""\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'yields_from_forward_rates\',\n      values=[discrete_forwards, times, groups]):\n    discrete_forwards = tf.convert_to_tensor(discrete_forwards, dtype=dtype,\n                                             name=\'discrete_forwards\')\n    times = tf.convert_to_tensor(times, dtype=dtype, name=\'times\')\n    if groups is not None:\n      groups = tf.convert_to_tensor(groups, name=\'groups\')\n    # Strategy for solving this equation without loops.\n    # Define x_i = f_i (t_i - t_{i-1}) where f are the forward rates and\n    # t_{-1}=0. Also define y_i = r_i t_i\n    # Then the relationship between the forward rate and the yield can be\n    # written as: x_i = y_i - y_{i-1} which we need to solve for y.\n    # Hence, y_i = x_0 + x_1 + ... x_i.\n    intervals = segment_ops.segment_diff(\n        times, order=1, exclusive=False, segment_ids=groups)\n    x = intervals * discrete_forwards\n    y = segment_ops.segment_cumsum(x, exclusive=False, segment_ids=groups)\n    return y / times\n\n\n__all__ = [\'forward_rates_from_yields\', \'yields_from_forward_rates\']\n'"
tf_quant_finance/rates/forwards_test.py,2,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for rate forwards.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\nfrom tf_quant_finance.rates import forwards\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass ForwardRatesTest(tf.test.TestCase):\n\n  def test_forward_rates(self):\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      groups = np.array([0, 0, 0, 1, 1, 1, 1])\n      times = np.array([0.25, 0.5, 1.0, 0.25, 0.5, 1.0, 1.5], dtype=dtype)\n      rates = np.array([0.04, 0.041, 0.044, 0.022, 0.025, 0.028, 0.036],\n                       dtype=dtype)\n      forward_rates = self.evaluate(\n          forwards.forward_rates_from_yields(\n              rates, times, groups=groups, dtype=dtype))\n      expected_forward_rates = np.array(\n          [0.04, 0.042, 0.047, 0.022, 0.028, 0.031, 0.052], dtype=dtype)\n      np.testing.assert_allclose(\n          forward_rates, expected_forward_rates, atol=1e-6)\n\n  def test_forward_rates_no_batches(self):\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      times = np.array([0.25, 0.5, 1.0, 1.25, 1.5, 2.0, 2.5], dtype=dtype)\n      rates = np.array([0.04, 0.041, 0.044, 0.046, 0.046, 0.047, 0.050],\n                       dtype=dtype)\n      forward_rates = self.evaluate(\n          forwards.forward_rates_from_yields(rates, times, dtype=dtype))\n      expected_forward_rates = np.array(\n          [0.04, 0.042, 0.047, 0.054, 0.046, 0.05, 0.062], dtype=dtype)\n      np.testing.assert_allclose(\n          forward_rates, expected_forward_rates, atol=1e-6)\n\n  def test_yields_from_forwards(self):\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      groups = np.array([0, 0, 0, 1, 1, 1, 1])\n      times = np.array([0.25, 0.5, 1.0, 0.25, 0.5, 1.0, 1.5], dtype=dtype)\n      forward_rates = np.array([0.04, 0.042, 0.047, 0.022, 0.028, 0.031, 0.052],\n                               dtype=dtype)\n      expected_rates = np.array(\n          [0.04, 0.041, 0.044, 0.022, 0.025, 0.028, 0.036], dtype=dtype)\n      actual_rates = self.evaluate(\n          forwards.yields_from_forward_rates(\n              forward_rates, times, groups=groups, dtype=dtype))\n      np.testing.assert_allclose(actual_rates, expected_rates, atol=1e-6)\n\n  def test_yields_from_forward_rates_no_batches(self):\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      times = np.array([0.25, 0.5, 1.0, 1.25, 1.5, 2.0, 2.5], dtype=dtype)\n      forward_rates = np.array([0.04, 0.042, 0.047, 0.054, 0.046, 0.05, 0.062],\n                               dtype=dtype)\n      expected_rates = np.array(\n          [0.04, 0.041, 0.044, 0.046, 0.046, 0.047, 0.050], dtype=dtype)\n      actual_rates = self.evaluate(\n          forwards.yields_from_forward_rates(forward_rates, times, dtype=dtype))\n      np.testing.assert_allclose(actual_rates, expected_rates, atol=1e-6)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/rates/swap_curve_bootstrap.py,43,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Methods to construct a swap curve.\n\nBuilding swap curves is a core problem in mathematical finance. Swap\ncurves are built using the available market data in liquidly traded fixed income\nproducts. These include LIBOR rates, interest rate swaps, forward rate\nagreements (FRAs) or exchange traded futures contracts. This module contains\nmethods to build swap curve from market data.\n\nThe algorithm implemented here uses the bootstrap method to iteratively\nconstruct the swap curve. It decouples interpolation from bootstrapping so that\nany arbitrary interpolation scheme could be used to build the curve.\n\n#### References:\n\n[1]: Patrick Hagan & Graeme West. Interpolation Methods for Curve Construction.\n  Applied Mathematical Finance. Vol 13, No. 2, pp 89-129. June 2006.\n  https://www.researchgate.net/publication/24071726_Interpolation_Methods_for_Curve_Construction\n""""""\n\nimport collections\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math.interpolation import linear\n# TODO(b/148945638): Move common functionality for swap curve construction to a\n# separate python module.\nfrom tf_quant_finance.rates import swap_curve_common as scc\n\n\ndef swap_curve_bootstrap(float_leg_start_times,\n                         float_leg_end_times,\n                         fixed_leg_start_times,\n                         fixed_leg_end_times,\n                         fixed_leg_cashflows,\n                         present_values,\n                         present_values_settlement_times=None,\n                         float_leg_daycount_fractions=None,\n                         fixed_leg_daycount_fractions=None,\n                         float_leg_discount_rates=None,\n                         float_leg_discount_times=None,\n                         fixed_leg_discount_rates=None,\n                         fixed_leg_discount_times=None,\n                         curve_interpolator=None,\n                         initial_curve_rates=None,\n                         curve_tolerance=1e-8,\n                         maximum_iterations=50,\n                         dtype=None,\n                         name=None):\n  """"""Constructs the zero swap curve using bootstrap method.\n\n  A zero swap curve is a function of time which gives the interest rate that\n  can be used to project forward rates at arbitrary `t` for the valuation of\n  interest rate securities (e.g. FRAs, Interest rate futures, Swaps etc.).\n\n  Suppose we have a set of `N` Interest Rate Swaps (IRS) `S_i` with increasing\n  expiries whose market prices are known.\n  Suppose also that the `i`th IRS issues cashflows at times `T_{ij}` where\n  `1 <= j <= n_i` and `n_i` is the number of cashflows (including expiry)\n  for the `i`th swap.\n  Denote by `T_i` the time of final payment for the `i`th swap\n  (hence `T_i = T_{i,n_i}`). This function estimates a set of rates `r(T_i)`\n  such that when these rates are interpolated (using the user specified\n  interpolation method) to all other cashflow times, the computed value of the\n  swaps matches the market value of the swaps (within some tolerance).\n\n  The algorithm implemented here uses the bootstrap method to iteratively\n  construct the swap curve [1].\n\n  #### Example:\n\n  The following example illustrates the usage by building an implied swap curve\n  from four vanilla (fixed to float) LIBOR swaps.\n\n  ```python\n\n  dtype = np.float64\n\n  # Next we will set up LIBOR reset and payment times for four spot starting\n  # swaps with maturities 1Y, 2Y, 3Y, 4Y. The LIBOR rate spans 6M.\n\n  float_leg_start_times = [\n            np.array([0., 0.5], dtype=dtype),\n            np.array([0., 0.5, 1., 1.5], dtype=dtype),\n            np.array([0., 0.5, 1.0, 1.5, 2.0, 2.5], dtype=dtype),\n            np.array([0., 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5], dtype=dtype)\n        ]\n\n  float_leg_end_times = [\n            np.array([0.5, 1.0], dtype=dtype),\n            np.array([0.5, 1., 1.5, 2.0], dtype=dtype),\n            np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0], dtype=dtype),\n            np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0], dtype=dtype)\n        ]\n\n  # Next we will set up start and end times for semi-annual fixed coupons.\n\n  fixed_leg_start_times = [\n            np.array([0., 0.5], dtype=dtype),\n            np.array([0., 0.5, 1., 1.5], dtype=dtype),\n            np.array([0., 0.5, 1.0, 1.5, 2.0, 2.5], dtype=dtype),\n            np.array([0., 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5], dtype=dtype)\n        ]\n\n  fixed_leg_end_times = [\n            np.array([0.5, 1.0], dtype=dtype),\n            np.array([0.5, 1., 1.5, 2.0], dtype=dtype),\n            np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0], dtype=dtype),\n            np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0], dtype=dtype)\n        ]\n\n  # Next setup a trivial daycount for floating and fixed legs.\n\n  float_leg_daycount = [\n            np.array([0.5, 0.5], dtype=dtype),\n            np.array([0.5, 0.5, 0.5, 0.5], dtype=dtype),\n            np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5], dtype=dtype),\n            np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], dtype=dtype)\n        ]\n\n  fixed_leg_daycount = [\n            np.array([0.5, 0.5], dtype=dtype),\n            np.array([0.5, 0.5, 0.5, 0.5], dtype=dtype),\n            np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5], dtype=dtype),\n            np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], dtype=dtype)\n        ]\n\n  fixed_leg_cashflows = [\n        # 1 year swap with 2.855% semi-annual fixed payments.\n        np.array([-0.02855, -0.02855], dtype=dtype),\n        # 2 year swap with 3.097% semi-annual fixed payments.\n        np.array([-0.03097, -0.03097, -0.03097, -0.03097], dtype=dtype),\n        # 3 year swap with 3.1% semi-annual fixed payments.\n        np.array([-0.031, -0.031, -0.031, -0.031, -0.031, -0.031], dtype=dtype),\n        # 4 year swap with 3.2% semi-annual fixed payments.\n        np.array([-0.032, -0.032, -0.032, -0.032, -0.032, -0.032, -0.032,\n        -0.032], dtype=dtype)\n    ]\n\n  # The present values of the above IRS.\n    pvs = np.array([0., 0., 0., 0.], dtype=dtype)\n\n  # Initial state of the curve.\n  initial_curve_rates = np.array([0.01, 0.01, 0.01, 0.01], dtype=dtype)\n\n  results = swap_curve_bootstrap(float_leg_start_times, float_leg_end_times,\n                                 float_leg_daycount, fixed_leg_start_times,\n                                 fixed_leg_end_times, fixed_leg_cashflows,\n                                 fixed_leg_daycount, pvs, dtype=dtype,\n                                 initial_curve_rates=initial_curve_rates)\n\n  #### References:\n\n  [1]: Patrick Hagan & Graeme West. Interpolation Methods for Curve\n    Construction. Applied Mathematical Finance. Vol 13, No. 2, pp 89-129.\n    June 2006.\n    https://www.researchgate.net/publication/24071726_Interpolation_Methods_for_Curve_Construction\n\n  Args:\n    float_leg_start_times: List of `Tensor`s. Each `Tensor` must be of rank 1\n      and of the same real dtype. They may be of different sizes. Each `Tensor`\n      represents the beginning of the accrual period for the forward rate which\n      determines the floating payment. Each element in the list belong to a\n      unique swap to be used to build the curve.\n    float_leg_end_times: List of `Tensor`s. Each `Tensor` must be of rank 1 and\n      and the same shape and of the same real dtype as the corresponding element\n      in `float_leg_start_times`. Each `Tensor` represents the end of the\n      accrual period for the forward rate which determines the floating payment.\n    fixed_leg_start_times: List of `Tensor`s. Each `Tensor` must be of rank 1\n      and of the same real dtype. They may be of different sizes. Each `Tensor`\n      represents the begining of the accrual period fixed coupon.\n    fixed_leg_end_times: List of `Tensor`s. Each `Tensor` must be of the same\n      shape and type as `fixed_leg_start_times`. Each `Tensor` represents the\n      end of the accrual period for the fixed coupon.\n    fixed_leg_cashflows: List of `Tensor`s. The list must be of the same length\n      as the `fixed_leg_start_times`. Each `Tensor` must be of rank 1 and of the\n      same dtype as the `Tensor`s in `fixed_leg_start_times`. The input contains\n      fixed cashflows at each coupon payment time including notional (if any).\n      The sign should be negative (positive) to indicate net outgoing (incoming)\n      cashflow.\n    present_values: List containing scalar `Tensor`s of the same dtype as\n      elements of `fixed_leg_cashflows`. The length of the list must be the same\n      as the length of `fixed_leg_cashflows`. The input contains the market\n      price of the underlying instruments.\n    present_values_settlement_times: List containing scalar `Tensor`s of the\n      same dtype as elements of `present_values`. The length of the list must be\n      the same as the length of `present_values`. The settlement times for the\n      present values is the time from now when the instrument is traded to the\n      time that the purchase price is actually delivered. If not supplied, then\n      it is assumed that the settlement times are zero for every bond.\n      Default value: `None`, which is equivalent to zero settlement times.\n    float_leg_daycount_fractions: Optional list of `Tensor`s. Each `Tensor` must\n      be of the same shape and type as `float_leg_start_times`. They may be of\n      different sizes. Each `Tensor` represents the daycount fraction of the\n      forward rate which determines the floating payment.\n      Default value: `None`, If omitted the daycount fractions are computed as\n      the difference between float_leg_end_times and float_leg_start_times.\n    fixed_leg_daycount_fractions: Optional list of `Tensor`s. Each `Tensor` must\n      be of the same shape and type as `fixed_leg_start_times`. Each `Tensor`\n      represents the daycount fraction applicable for the fixed payment.\n      Default value: `None`, If omitted the daycount fractions are computed as\n      the difference between fixed_leg_end_times and fixed_leg_start_times.\n    float_leg_discount_rates: Optional `Tensor` of the same dtype as\n      `initial_discount_rates`. This input contains the continuously compounded\n      discount rates the will be used to discount the floating cashflows. This\n      allows the swap curve to constructed using an independent discount curve\n      (e.g. OIS curve).\n      Default value: `None`, in which case the cashflows are discounted using\n      the curve that is being constructed.\n    float_leg_discount_times: Optional `Tensor` of the same dtype and shape as\n      `float_leg_discount_rates`. This input contains the times corresponding to\n      the rates specified via the `float_leg_discount_rates`.\n    fixed_leg_discount_rates: Optional `Tensor` of the same dtype as\n      `initial_discount_rates`. This input contains the continuously compounded\n      discount rates the will be used to discount the fixed cashflows. This\n      allows the swap curve to constructed using an independent discount curve\n      (e.g. OIS curve).\n      Default value: `None`, in which case the cashflows are discounted using\n      the curve that is being constructed.\n    fixed_leg_discount_times: Optional `Tensor` of the same dtype and shape as\n      `fixed_leg_discount_rates`. This input contains the times corresponding to\n      the rates specified via the `fixed_leg_discount_rates`.\n    curve_interpolator: Optional Python callable used to interpolate the zero\n      swap rates at cashflow times. It should have the following interface:\n      yi = curve_interpolator(xi, x, y)\n      `x`, `y`, \'xi\', \'yi\' are all `Tensors` of real dtype. `x` and `y` are the\n      sample points and values (respectively) of the function to be\n      interpolated. `xi` are the points at which the interpolation is\n      desired and `yi` are the corresponding interpolated values returned by the\n      function.\n      Default value: `None`, which maps to linear interpolation.\n    initial_curve_rates: Optional `Tensor` of the same dtype and shape as\n      `present_values`. The starting guess for the discount rates used to\n      initialize the iterative procedure.\n      Default value: `None`. If not supplied, the yields to maturity for the\n        bonds is used as the initial value.\n    curve_tolerance: Optional positive scalar `Tensor` of same dtype as\n      elements of `bond_cashflows`. The absolute tolerance for terminating the\n      iterations used to fit the rate curve. The iterations are stopped when the\n      estimated discounts at the expiry times of the bond_cashflows change by a\n      amount smaller than `discount_tolerance` in an iteration.\n      Default value: 1e-8.\n    maximum_iterations: Optional positive integer `Tensor`. The maximum number\n      of iterations permitted when fitting the curve.\n      Default value: 50.\n    dtype: `tf.Dtype`. If supplied the dtype for the (elements of)\n      `float_leg_start_times`, and `fixed_leg_start_times`.\n      Default value: None which maps to the default dtype inferred by\n      TensorFlow.\n    name: Python str. The name to give to the ops created by this function.\n      Default value: `None` which maps to \'swap_curve\'.\n\n  Returns:\n    curve_builder_result: An instance of `SwapCurveBuilderResult` containing the\n      following attributes.\n      times: Rank 1 real `Tensor`. Times for the computed rates. These\n        are chosen to be the expiry times of the supplied instruments.\n      rates: Rank 1 `Tensor` of the same dtype as `times`.\n        The inferred zero rates.\n      discount_factor: Rank 1 `Tensor` of the same dtype as `times`.\n        The inferred discount factors.\n      initial_rates: Rank 1 `Tensor` of the same dtype as `times`. The\n        initial guess for the rates.\n      converged: Scalar boolean `Tensor`. Whether the procedure converged.\n        The procedure is said to have converged when the maximum absolute\n        difference in the discount factors from one iteration to the next falls\n        below the `discount_tolerance`.\n      failed: Scalar boolean `Tensor`. Whether the procedure failed. Procedure\n        may fail either because a NaN value was encountered for the discount\n        rates or the discount factors.\n      iterations: Scalar int32 `Tensor`. Number of iterations performed.\n\n  Raises:\n    ValueError: If the initial state of the curve is not\n    supplied to the function.\n\n  """"""\n\n  name = name or \'swap_curve_bootstrap\'\n  with tf.name_scope(name):\n\n    if curve_interpolator is None:\n      def default_interpolator(xi, x, y):\n        return linear.interpolate(xi, x, y, dtype=dtype)\n      curve_interpolator = default_interpolator\n\n    if present_values_settlement_times is None:\n      pv_settle_times = [tf.zeros_like(pv) for pv in present_values]\n    else:\n      pv_settle_times = present_values_settlement_times\n\n    if float_leg_daycount_fractions is None:\n      float_leg_daycount_fractions = [\n          y - x for x, y in zip(float_leg_start_times, float_leg_end_times)\n      ]\n\n    if fixed_leg_daycount_fractions is None:\n      fixed_leg_daycount_fractions = [\n          y - x for x, y in zip(fixed_leg_start_times, fixed_leg_end_times)\n      ]\n\n    float_leg_start_times = _convert_to_tensors(dtype, float_leg_start_times,\n                                                \'float_leg_start_times\')\n    float_leg_end_times = _convert_to_tensors(dtype, float_leg_end_times,\n                                              \'float_leg_end_times\')\n    float_leg_daycount_fractions = _convert_to_tensors(\n        dtype, float_leg_daycount_fractions, \'float_leg_daycount_fractions\')\n    fixed_leg_start_times = _convert_to_tensors(dtype, fixed_leg_start_times,\n                                                \'fixed_leg_start_times\')\n    fixed_leg_end_times = _convert_to_tensors(dtype, fixed_leg_end_times,\n                                              \'fixed_leg_end_times\')\n    fixed_leg_daycount_fractions = _convert_to_tensors(\n        dtype, fixed_leg_daycount_fractions, \'fixed_leg_daycount_fractions\')\n    fixed_leg_cashflows = _convert_to_tensors(dtype, fixed_leg_cashflows,\n                                              \'fixed_leg_cashflows\')\n    present_values = _convert_to_tensors(dtype, present_values,\n                                         \'present_values\')\n    pv_settle_times = _convert_to_tensors(dtype, pv_settle_times,\n                                          \'pv_settle_times\')\n\n    self_discounting_float_leg = False\n    self_discounting_fixed_leg = False\n    # Determine how the floating and fixed leg will be discounted. If separate\n    # discount curves for each leg are not specified, the curve will be self\n    # discounted using the swap curve.\n    if float_leg_discount_rates is None and fixed_leg_discount_rates is None:\n      self_discounting_float_leg = True\n      self_discounting_fixed_leg = True\n      float_leg_discount_rates = [0.0]\n      float_leg_discount_times = [0.]\n      fixed_leg_discount_rates = [0.]\n      fixed_leg_discount_times = [0.]\n    elif fixed_leg_discount_rates is None:\n      fixed_leg_discount_rates = float_leg_discount_rates\n      fixed_leg_discount_times = float_leg_discount_times\n    elif float_leg_discount_rates is None:\n      self_discounting_float_leg = True\n      float_leg_discount_rates = [0.]\n      float_leg_discount_times = [0.]\n\n    # Create tensors for discounting curves\n    float_leg_discount_rates = _convert_to_tensors(dtype,\n                                                   float_leg_discount_rates,\n                                                   \'float_disc_rates\')\n    float_leg_discount_times = _convert_to_tensors(dtype,\n                                                   float_leg_discount_times,\n                                                   \'float_disc_times\')\n    fixed_leg_discount_rates = _convert_to_tensors(dtype,\n                                                   fixed_leg_discount_rates,\n                                                   \'fixed_disc_rates\')\n    fixed_leg_discount_times = _convert_to_tensors(dtype,\n                                                   fixed_leg_discount_times,\n                                                   \'fixed_disc_times\')\n\n    if initial_curve_rates is not None:\n      initial_rates = tf.convert_to_tensor(\n          initial_curve_rates, dtype=dtype, name=\'initial_rates\')\n    else:\n      # TODO(b/144600429): Create a logic for a meaningful initial state of the\n      # curve\n      raise ValueError(\'Initial state of the curve is not specified.\')\n\n    return _build_swap_curve(float_leg_start_times,\n                             float_leg_end_times,\n                             float_leg_daycount_fractions,\n                             fixed_leg_start_times,\n                             fixed_leg_end_times,\n                             fixed_leg_cashflows,\n                             fixed_leg_daycount_fractions,\n                             float_leg_discount_rates,\n                             float_leg_discount_times,\n                             fixed_leg_discount_rates,\n                             fixed_leg_discount_times,\n                             self_discounting_float_leg,\n                             self_discounting_fixed_leg,\n                             present_values,\n                             pv_settle_times,\n                             curve_interpolator,\n                             initial_rates,\n                             curve_tolerance,\n                             maximum_iterations,\n                             dtype)\n\n\ndef _build_swap_curve(float_leg_start_times, float_leg_end_times,\n                      float_leg_daycount_fractions, fixed_leg_start_times,\n                      fixed_leg_end_times, fixed_leg_cashflows,\n                      fixed_leg_daycount_fractions, float_leg_discount_rates,\n                      float_leg_discount_times, fixed_leg_discount_rates,\n                      fixed_leg_discount_times, self_discounting_float_leg,\n                      self_discounting_fixed_leg, present_values,\n                      pv_settlement_times, curve_interpolator,\n                      initial_rates, curve_tolerance, maximum_iterations,\n                      dtype):\n  """"""Build the zero swap curve using the bootstrap method.""""""\n\n  # The procedure is recursive and as follows:\n  # 1. Start with an initial state of the swap curve. Set this as the current\n  #   swap curve.\n  # 2. From the current swap curve, compute the relevant forward rates and\n  #   discount factors (if cashflows are discounted using the swap curve). Use\n  #   the specified interpolation method to compute rates at intermediate times\n  #   as needed to calculate either the forward rate or the discount factors.\n  # 3. Using the above and the input present values of bootstarpping\n  #   instruments,compute the zero swap rate at expiry by inverting the swap\n  #   pricing formula. The following illustrates the procedure:\n\n  #   Assuming that a swap pays fixed payments c_i at times t_i (i=[1,...,n])\n  #   and receives floating payments at times t_j (j=[1,...,m]), then the\n  #   present value of the swap is given by\n\n  #   ```None\n  #   PV = sum_{j=1}^m (P_{j-1}/P_j - 1.) * P_j - sum_{i=1}^n a_i * c_i * P_i\n  #                                                                        (A)\n  #\n  #   ```\n  #   where P_i = exp(-r(t_i) * t_i) and a_i are the daycount fractions. We\n  #   update the current estimate of the rate at curve node t_k = t_n = t_m\n  #   by inverting the above equation:\n\n  #   ```None\n  #   P_k * (1 + a_i * c_i) = sum_{j=1}^{m - 1} (P_{j-1}/P_j - 1.) * P_j -\n  #                           sum_{i=1}^{n - 1} a_i * c_i * P_i - PV       (B)\n\n  #   ```\n  #   From Eq. (B), we get r(t_k) = -log(P_k) / t_k.\n  #   Using this as the next guess for the discount rates and we repeat the\n  #   procedure from Step (2) until convergence.\n\n  del fixed_leg_start_times, pv_settlement_times\n  curve_tensors = _create_curve_building_tensors(float_leg_start_times,\n                                                 float_leg_end_times,\n                                                 float_leg_daycount_fractions,\n                                                 fixed_leg_end_times,\n                                                 fixed_leg_cashflows,\n                                                 fixed_leg_daycount_fractions)\n\n  float_leg_calc_times_start = curve_tensors.float_leg_times_start\n  float_leg_calc_times_end = curve_tensors.float_leg_times_end\n  calc_fixed_leg_cashflows = curve_tensors.fixed_leg_cashflows\n  calc_fixed_leg_daycount = curve_tensors.fixed_leg_daycount\n  fixed_leg_calc_times = curve_tensors.fixed_leg_calc_times\n  calc_groups_float = curve_tensors.calc_groups_float\n  calc_groups_fixed = curve_tensors.calc_groups_fixed\n  last_float_leg_start_time = curve_tensors.last_float_leg_start_time\n  last_float_leg_end_time = curve_tensors.last_float_leg_end_time\n  last_fixed_leg_end_time = curve_tensors.last_fixed_leg_calc_time\n  last_fixed_leg_daycount = curve_tensors.last_fixed_leg_daycount\n  last_fixed_leg_cashflows = curve_tensors.last_fixed_leg_cashflows\n  expiry_times = curve_tensors.expiry_times\n\n  def _one_step(converged, failed, iteration, discount_factor):\n    """"""One step of the bootstrap iteration.""""""\n\n    x = -tf.math.log(discount_factor) / expiry_times\n    rates_start = curve_interpolator(float_leg_calc_times_start, expiry_times,\n                                     x)\n    rates_end = curve_interpolator(float_leg_calc_times_end, expiry_times, x)\n    rates_start_last = curve_interpolator(last_float_leg_start_time,\n                                          expiry_times, x)\n\n    float_cashflows = (\n        tf.math.exp(float_leg_calc_times_end * rates_end) /\n        tf.math.exp(float_leg_calc_times_start * rates_start) - 1.)\n\n    if self_discounting_float_leg:\n      float_discount_rates = rates_end\n    else:\n      float_discount_rates = curve_interpolator(float_leg_calc_times_end,\n                                                float_leg_discount_times,\n                                                float_leg_discount_rates)\n      last_float_discount_rate = curve_interpolator(last_float_leg_end_time,\n                                                    float_leg_discount_times,\n                                                    float_leg_discount_rates)\n      last_float_discount_factor = tf.math.exp(-last_float_discount_rate *\n                                               last_float_leg_end_time)\n    if self_discounting_fixed_leg:\n      fixed_discount_rates = curve_interpolator(fixed_leg_calc_times,\n                                                expiry_times, x)\n    else:\n      fixed_discount_rates = curve_interpolator(fixed_leg_calc_times,\n                                                fixed_leg_discount_times,\n                                                fixed_leg_discount_rates)\n      last_fixed_discount_rate = curve_interpolator(last_fixed_leg_end_time,\n                                                    fixed_leg_discount_times,\n                                                    fixed_leg_discount_rates)\n      last_fixed_discount_factor = tf.math.exp(-last_fixed_leg_end_time *\n                                               last_fixed_discount_rate)\n      last_fixed_leg_cashflow_pv = (\n          last_fixed_leg_daycount * last_fixed_leg_cashflows *\n          last_fixed_discount_factor)\n\n    calc_discounts_float_leg = tf.math.exp(-float_discount_rates *\n                                           float_leg_calc_times_end)\n    calc_discounts_fixed_leg = tf.math.exp(-fixed_discount_rates *\n                                           fixed_leg_calc_times)\n\n    float_pv = tf.math.segment_sum(float_cashflows * calc_discounts_float_leg,\n                                   calc_groups_float)\n    fixed_pv = tf.math.segment_sum(\n        calc_fixed_leg_daycount * calc_fixed_leg_cashflows *\n        calc_discounts_fixed_leg, calc_groups_fixed)\n\n    if self_discounting_float_leg and self_discounting_fixed_leg:\n      p_n_minus_1 = tf.math.exp(-rates_start_last * last_float_leg_start_time)\n      scale = last_fixed_leg_cashflows * last_fixed_leg_daycount - 1.\n      next_discount = (present_values - float_pv - fixed_pv -\n                       p_n_minus_1) / scale\n    elif self_discounting_float_leg:\n      p_n_minus_1 = tf.math.exp(-rates_start_last * last_float_leg_start_time)\n      next_discount = (float_pv + (fixed_pv + last_fixed_leg_cashflow_pv) -\n                       present_values + p_n_minus_1)\n    else:\n      p_n_minus_1 = tf.math.exp(-rates_start_last * last_float_leg_start_time)\n      scale = present_values - float_pv - (\n          fixed_pv + last_fixed_leg_cashflow_pv) + last_float_discount_factor\n      next_discount = p_n_minus_1 * last_float_discount_factor / scale\n\n    discount_diff = tf.math.abs(next_discount - discount_factor)\n    converged = (~tf.math.reduce_any(tf.math.is_nan(discount_diff)) &\n                 (tf.math.reduce_max(discount_diff) < curve_tolerance))\n\n    return (converged, failed, iteration + 1, next_discount)\n\n  def cond(converged, failed, iteration, x):\n    # Note we do not need to check iteration count here because that\n    # termination mode is imposed by the maximum_iterations parameter in the\n    # while loop.\n    del iteration, x\n    return ~tf.math.logical_or(converged, failed)\n\n  initial_vals = (False, False, 0, tf.math.exp(-initial_rates * expiry_times))\n  bootstrap_result = tf.compat.v2.while_loop(\n      cond, _one_step, initial_vals, maximum_iterations=maximum_iterations)\n\n  discount_factors = bootstrap_result[-1]\n  discount_rates = -tf.math.log(discount_factors) / expiry_times\n  results = scc.SwapCurveBuilderResult(\n      times=expiry_times,\n      rates=discount_rates,\n      discount_factors=discount_factors,\n      initial_rates=initial_rates,\n      converged=bootstrap_result[0],\n      failed=bootstrap_result[1],\n      iterations=bootstrap_result[2],\n      objective_value=tf.constant(0, dtype=dtype))\n  return results\n\n\ndef _convert_to_tensors(dtype, input_array, name):\n  """"""Converts the supplied list to a tensor.""""""\n\n  output_tensor = [\n      tf.convert_to_tensor(\n          x, dtype=dtype, name=name + \'_{}\'.format(i))\n      for i, x in enumerate(input_array)\n  ]\n\n  return output_tensor\n\n\nCurveFittingVars = collections.namedtuple(\n    \'CurveFittingVars\',\n    [\n        # The `Tensor` of maturities at which the curve will be built.\n        # Coorspond to maturities on the underlying instruments\n        \'expiry_times\',\n        # `Tensor` containing fixed leg cashflows from all instruments\n        # ""flattened"" (or concatenated)\n        \'fixed_leg_cashflows\',\n        # `Tensor` containing daycount associated with fixed leg cashflows\n        \'fixed_leg_daycount\',\n        # `Tensor` containing the times at which fixed cashflows are discouted\n        \'fixed_leg_calc_times\',\n        # `Tensor` containing the instrument settlement times expanded to match\n        # the dimensions of fixed leg cashflows\n        \'settle_times_fixed\',\n        # `Tensor` containing the start times of time-periods corresponding to\n        # floating cashflows for all instruments ""flattened"" (or concatenated)\n        \'float_leg_times_start\',\n        # `Tensor` containing the end times of time-periods corresponding to\n        # floating cashflows for all instruments ""flattened"" (or concatenated)\n        \'float_leg_times_end\',\n        # `Tensor` containing daycount associated with floating leg cashflows\n        \'float_leg_daycount\',\n        # `Tensor` containing the instrument settlement times expanded to match\n        # the dimensions of floating cashflows\n        \'settle_times_float\',\n        # `Tensor` containing the instrument index of each ""flattened"" `Tensor`\n        # for floating cashflows\n        \'calc_groups_float\',\n        # `Tensor` containing the instrument index of each ""flattened"" `Tensor`\n        # for fixed cashflows\n        \'calc_groups_fixed\',\n        # `Tensor` containing the start times of the final accrual periods of\n        # each insrument. These will be used for bootstrap iterations.\n        \'last_float_leg_start_time\',\n        # `Tensor` containing the end times of the final accrual periods of\n        # each insrument. These will be used for bootstrap iterations.\n        \'last_float_leg_end_time\',\n        # `Tensor` containing the daycount of the final accrual periods of\n        # each insrument. These will be used for bootstrap iterations.\n        \'last_float_leg_daycount\',\n        # `Tensor` containing the discounting times of the final fixed cashflow\n        # of each insrument. These will be used for bootstrap iterations.\n        \'last_fixed_leg_calc_time\',\n        # `Tensor` containing the daycount of the final fixed cashflow\n        # of each insrument. These will be used for bootstrap iterations.\n        \'last_fixed_leg_daycount\',\n        # `Tensor` containing the the final fixed cashflow of each instrument.\n        # These will be used for bootstrap iterations.\n        \'last_fixed_leg_cashflows\'\n    ])\n\n\ndef _create_curve_building_tensors(float_leg_start_times,\n                                   float_leg_end_times,\n                                   float_leg_daycount_fractions,\n                                   fixed_leg_end_times,\n                                   fixed_leg_cashflows,\n                                   fixed_leg_daycount_fractions):\n  """"""Helper function to create tensors needed for curve construction.""""""\n  calc_float_leg_daycount = []\n  float_leg_calc_times_start = []\n  float_leg_calc_times_end = []\n  calc_fixed_leg_cashflows = []\n  calc_fixed_leg_daycount = []\n  fixed_leg_calc_times = []\n  calc_groups_float = []\n  calc_groups_fixed = []\n  last_float_leg_start_time = []\n  last_float_leg_end_time = []\n  last_float_leg_daycount = []\n  last_fixed_leg_end_time = []\n  last_fixed_leg_daycount = []\n  last_fixed_leg_cashflows = []\n  expiry_times = []\n  num_instruments = len(float_leg_start_times)\n  for i in range(num_instruments):\n    calc_float_leg_daycount.append(float_leg_daycount_fractions[i][:-1])\n    float_leg_calc_times_start.append(float_leg_start_times[i][:-1])\n    float_leg_calc_times_end.append(float_leg_end_times[i][:-1])\n    calc_fixed_leg_cashflows.append(fixed_leg_cashflows[i][:-1])\n    calc_fixed_leg_daycount.append(fixed_leg_daycount_fractions[i][:-1])\n    fixed_leg_calc_times.append(fixed_leg_end_times[i][:-1])\n    last_float_leg_start_time.append(float_leg_start_times[i][-1])\n    last_float_leg_end_time.append(float_leg_end_times[i][-1])\n    last_float_leg_daycount.append(float_leg_daycount_fractions[i][-1])\n    last_fixed_leg_end_time.append(fixed_leg_end_times[i][-1])\n    last_fixed_leg_daycount.append(fixed_leg_daycount_fractions[i][-1])\n    last_fixed_leg_cashflows.append(fixed_leg_cashflows[i][-1])\n    expiry_times.append(\n        tf.math.maximum(float_leg_end_times[i][-1], fixed_leg_end_times[i][-1]))\n\n    calc_groups_float.append(\n        tf.fill(tf.shape(float_leg_start_times[i][:-1]), i))\n    calc_groups_fixed.append(tf.fill(tf.shape(fixed_leg_end_times[i][:-1]), i))\n\n  output = CurveFittingVars(\n      float_leg_daycount=tf.concat(calc_float_leg_daycount, axis=0),\n      float_leg_times_start=tf.concat(float_leg_calc_times_start, axis=0),\n      float_leg_times_end=tf.concat(float_leg_calc_times_end, axis=0),\n      fixed_leg_cashflows=tf.concat(calc_fixed_leg_cashflows, axis=0),\n      fixed_leg_daycount=tf.concat(calc_fixed_leg_daycount, axis=0),\n      fixed_leg_calc_times=tf.concat(fixed_leg_calc_times, axis=0),\n      settle_times_fixed=None,\n      settle_times_float=None,\n      expiry_times=tf.stack(expiry_times, axis=0),\n      calc_groups_float=tf.concat(calc_groups_float, axis=0),\n      calc_groups_fixed=tf.concat(calc_groups_fixed, axis=0),\n      last_float_leg_start_time=tf.stack(last_float_leg_start_time, axis=0),\n      last_float_leg_end_time=tf.stack(last_float_leg_end_time, axis=0),\n      last_float_leg_daycount=tf.stack(last_float_leg_daycount, axis=0),\n      last_fixed_leg_calc_time=tf.stack(last_fixed_leg_end_time, axis=0),\n      last_fixed_leg_daycount=tf.stack(last_fixed_leg_daycount, axis=0),\n      last_fixed_leg_cashflows=tf.stack(last_fixed_leg_cashflows, axis=0)\n      )\n\n  return output\n'"
tf_quant_finance/rates/swap_curve_common.py,0,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Common utilities and data structures for swap curve construction.""""""\n\nimport collections\n\nSwapCurveBuilderResult = collections.namedtuple(\n    \'SwapCurveBuilderResult\',\n    [\n        # Rank 1 real `Tensor`. Times for the computed rates.\n        \'times\',\n        # Rank 1 `Tensor` of the same dtype as `times`.\n        # The inferred zero rates.\n        \'rates\',\n        # Rank 1 `Tensor` of the same dtype as `times`.\n        # The inferred discount factors.\n        \'discount_factors\',\n        # Rank 1 `Tensor` of the same dtype as `times`. The\n        # initial guess for the rates.\n        \'initial_rates\',\n        # Scalar boolean `Tensor`. Whether the procedure converged.\n        \'converged\',\n        # Scalar boolean `Tensor`. Whether the procedure failed.\n        \'failed\',\n        # Scalar int32 `Tensor`. Number of iterations performed.\n        \'iterations\',\n        # Scalar real `Tensor`. The objective function at the optimal soultion.\n        \'objective_value\'\n    ])\n'"
tf_quant_finance/rates/swap_curve_fit.py,35,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Methods to construct a swap curve.\n\nBuilding swap curves is a core problem in mathematical finance. Swap\ncurves are built using the available market data in liquidly traded fixed income\nproducts. These include LIBOR rates, interest rate swaps, forward rate\nagreements (FRAs) or exchange traded futures contracts. This module contains\nmethods to build swap curve from market data.\n\nThe algorithm implemented here uses conjugate gradient optimization to minimize\nthe weighted least squares error between the input present values of the\ninstruments and the present values computed using the constructed swap curve.\n\n#### References:\n  [1]: Leif B.G. Andersen and Vladimir V. Piterbarg. Interest Rate Modeling,\n      Volume I: Foundations and Vanilla Models. Chapter 6. 2010.\n""""""\n\nimport collections\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math import make_val_and_grad_fn\nfrom tf_quant_finance.math import optimizer\nfrom tf_quant_finance.math.interpolation import linear\nfrom tf_quant_finance.rates import swap_curve_common as scc\n\n\ndef swap_curve_fit(float_leg_start_times,\n                   float_leg_end_times,\n                   float_leg_daycount_fractions,\n                   fixed_leg_start_times,\n                   fixed_leg_end_times,\n                   fixed_leg_daycount_fractions,\n                   fixed_leg_cashflows,\n                   present_values,\n                   present_values_settlement_times=None,\n                   float_leg_discount_rates=None,\n                   float_leg_discount_times=None,\n                   fixed_leg_discount_rates=None,\n                   fixed_leg_discount_times=None,\n                   optimize=None,\n                   curve_interpolator=None,\n                   initial_curve_rates=None,\n                   instrument_weights=None,\n                   curve_tolerance=1e-8,\n                   maximum_iterations=50,\n                   dtype=None,\n                   name=None):\n  """"""Constructs the zero swap curve using optimization.\n\n  A zero swap curve is a function of time which gives the interest rate that\n  can be used to project forward rates at arbitrary `t` for the valuation of\n  interest rate securities.\n\n  Suppose we have a set of `N` Interest Rate Swaps (IRS) `S_i` with increasing\n  expiries whose market prices are known.\n  Suppose also that the `i`th IRS issues cashflows at times `T_{ij}` where\n  `1 <= j <= n_i` and `n_i` is the number of cashflows (including expiry)\n  for the `i`th swap.\n  Denote by `T_i` the time of final payment for the `i`th swap\n  (hence `T_i = T_{i,n_i}`). This function estimates a set of rates `r(T_i)`\n  such that when these rates are interpolated to all other cashflow times,\n  the computed value of the swaps matches the market value of the swaps\n  (within some tolerance). Rates at intermediate times are interpolated using\n  the user specified interpolation method (the default interpolation method\n  is linear interpolation on rates).\n\n  #### Example:\n\n  The following example illustrates the usage by building an implied swap curve\n  from four vanilla (fixed to float) LIBOR swaps.\n\n  ```python\n\n  dtype = np.float64\n\n  # Next we will set up LIBOR reset and payment times for four spot starting\n  # swaps with maturities 1Y, 2Y, 3Y, 4Y. The LIBOR rate spans 6M.\n\n  float_leg_start_times = [\n            np.array([0., 0.5], dtype=dtype),\n            np.array([0., 0.5, 1., 1.5], dtype=dtype),\n            np.array([0., 0.5, 1.0, 1.5, 2.0, 2.5], dtype=dtype),\n            np.array([0., 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5], dtype=dtype)\n        ]\n\n  float_leg_end_times = [\n            np.array([0.5, 1.0], dtype=dtype),\n            np.array([0.5, 1., 1.5, 2.0], dtype=dtype),\n            np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0], dtype=dtype),\n            np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0], dtype=dtype)\n        ]\n\n  # Next we will set up start and end times for semi-annual fixed coupons.\n\n  fixed_leg_start_times = [\n            np.array([0., 0.5], dtype=dtype),\n            np.array([0., 0.5, 1., 1.5], dtype=dtype),\n            np.array([0., 0.5, 1.0, 1.5, 2.0, 2.5], dtype=dtype),\n            np.array([0., 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5], dtype=dtype)\n        ]\n\n  fixed_leg_end_times = [\n            np.array([0.5, 1.0], dtype=dtype),\n            np.array([0.5, 1., 1.5, 2.0], dtype=dtype),\n            np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0], dtype=dtype),\n            np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0], dtype=dtype)\n        ]\n\n  # Next setup a trivial daycount for floating and fixed legs.\n\n  float_leg_daycount = [\n            np.array([0.5, 0.5], dtype=dtype),\n            np.array([0.5, 0.5, 0.5, 0.5], dtype=dtype),\n            np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5], dtype=dtype),\n            np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], dtype=dtype)\n        ]\n\n  fixed_leg_daycount = [\n            np.array([0.5, 0.5], dtype=dtype),\n            np.array([0.5, 0.5, 0.5, 0.5], dtype=dtype),\n            np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5], dtype=dtype),\n            np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], dtype=dtype)\n        ]\n\n  fixed_leg_cashflows = [\n        # 1 year swap with 2.855% semi-annual fixed payments.\n        np.array([-0.02855, -0.02855], dtype=dtype),\n        # 2 year swap with 3.097% semi-annual fixed payments.\n        np.array([-0.03097, -0.03097, -0.03097, -0.03097], dtype=dtype),\n        # 3 year swap with 3.1% semi-annual fixed payments.\n        np.array([-0.031, -0.031, -0.031, -0.031, -0.031, -0.031], dtype=dtype),\n        # 4 year swap with 3.2% semi-annual fixed payments.\n        np.array([-0.032, -0.032, -0.032, -0.032, -0.032, -0.032, -0.032,\n        -0.032], dtype=dtype)\n    ]\n\n  # The present values of the above IRS.\n  pvs = np.array([0., 0., 0., 0.], dtype=dtype)\n\n  # Initial state of the curve.\n  initial_curve_rates = np.array([0.01, 0.01, 0.01, 0.01], dtype=dtype)\n\n  results = swap_curve_fit(float_leg_start_times, float_leg_end_times,\n                           float_leg_daycount, fixed_leg_start_times,\n                           fixed_leg_end_times, fixed_leg_cashflows,\n                           fixed_leg_daycount, pvs, dtype=dtype,\n                           initial_curve_rates=initial_curve_rates)\n\n  #### References:\n  [1]: Leif B.G. Andersen and Vladimir V. Piterbarg. Interest Rate Modeling,\n      Volume I: Foundations and Vanilla Models. Chapter 6. 2010.\n\n  Args:\n    float_leg_start_times: List of `Tensor`s. Each `Tensor` must be of rank 1\n      and of the same real dtype. They may be of different sizes. Each `Tensor`\n      represents the beginning of the accrual period for the forward rate which\n      determines the floating payment. Each element in the list belong to a\n      unique swap to be used to build the curve.\n    float_leg_end_times: List of `Tensor`s. Each `Tensor` must be of rank 1 and\n      and the same shape and of the same real dtype as the corresponding element\n      in `float_leg_start_times`. Each `Tensor` represents the end of the\n      accrual period for the forward rate which determines the floating payment.\n    float_leg_daycount_fractions: List of `Tensor`s. Each `Tensor` must be of\n      the same shape and type as `float_leg_start_times`. They may be of\n      different sizes. Each `Tensor` represents the daycount fraction of the\n      forward rate which determines the floating payment.\n    fixed_leg_start_times: List of `Tensor`s. Each `Tensor` must be of rank 1\n      and of the same real dtype. They may be of different sizes. Each `Tensor`\n      represents the begining of the accrual period fixed coupon.\n    fixed_leg_end_times: List of `Tensor`s. Each `Tensor` must be of the same\n      shape and type as `fixed_leg_start_times`. Each `Tensor` represents the\n      end of the accrual period for the fixed coupon.\n    fixed_leg_daycount_fractions: List of `Tensor`s. Each `Tensor` must be of\n      the same shape and type as `fixed_leg_start_times`. Each `Tensor`\n      represents the daycount fraction applicable for the fixed payment.\n    fixed_leg_cashflows: List of `Tensor`s. The list must be of the same length\n      as the `fixed_leg_start_times`. Each `Tensor` must be of rank 1 and of the\n      same dtype as the `Tensor`s in `fixed_leg_start_times`. The input contains\n      fixed cashflows at each coupon payment time including notional (if any).\n      The sign should be negative (positive) to indicate net outgoing (incoming)\n      cashflow.\n    present_values: List containing scalar `Tensor`s of the same dtype as\n      elements of `fixed_leg_cashflows`. The length of the list must be the same\n      as the length of `fixed_leg_cashflows`. The input contains the market\n      price of the underlying instruments.\n    present_values_settlement_times: List containing scalar `Tensor`s of the\n      same dtype as elements of `present_values`. The length of the list must be\n      the same as the length of `present_values`. The settlement times for the\n      present values is the time from now when the instrument is traded to the\n      time that the purchase price is actually delivered. If not supplied, then\n      it is assumed that the settlement times are zero for every bond.\n      Default value: `None` which is equivalent to zero settlement times.\n    float_leg_discount_rates: Optional `Tensor` of the same dtype as\n      `initial_discount_rates`. This input contains the continuously compounded\n      discount rates the will be used to discount the floating cashflows. This\n      allows the swap curve to constructed using an independent discount curve\n      (e.g. OIS curve). By default the cashflows are discounted using the curve\n      that is being constructed.\n    float_leg_discount_times: Optional `Tensor` of the same dtype and shape as\n      `float_leg_discount_rates`. This input contains the times corresponding to\n      the rates specified via the `float_leg_discount_rates`.\n    fixed_leg_discount_rates: Optional `Tensor` of the same dtype as\n      `initial_discount_rates`. This input contains the continuously compounded\n      discount rates the will be used to discount the fixed cashflows. This\n      allows the swap curve to constructed using an independent discount curve\n      (e.g. OIS curve). By default the cashflows are discounted using the curve\n      that is being constructed.\n    fixed_leg_discount_times: Optional `Tensor` of the same dtype and shape as\n      `fixed_leg_discount_rates`. This input contains the times corresponding to\n      the rates specified via the `fixed_leg_discount_rates`.\n    optimize: Optional Python callable which implements the algorithm used to\n      minimize the objective function during curve construction. It should have\n      the following interface:\n      result = optimize(value_and_gradients_function, initial_position,\n      tolerance, max_iterations)\n      `value_and_gradients_function` is a Python callable that accepts a point\n      as a real `Tensor` and returns a tuple of `Tensor`s of real dtype\n      containing the value of the function and its gradient at that point.\n      \'initial_position\' is a real `Tensor` containing the starting point of the\n      optimization, \'tolerance\' is a real scalar `Tensor` for stopping tolerance\n      for the procedure and `max_iterations` specifies the maximum number of\n      iterations.\n      `optimize` should return a namedtuple containing the items: `position` (a\n      tensor containing the optimal value), `converged` (a boolean indicating\n      whether the optimize converged according the specified criteria),\n      `failed` (a boolean indicating if the optimization resulted in a failure),\n      `num_iterations` (the number of iterations used), and `objective_value` (\n      the value of the objective function at the optimal value).\n      The default value for `optimize` is None and conjugate gradient algorithm\n      is used.\n    curve_interpolator: Optional Python callable used to interpolate the zero\n      swap rates at cashflow times. It should have the following interface:\n      yi = curve_interpolator(xi, x, y)\n      `x`, `y`, \'xi\', \'yi\' are all `Tensors` of real dtype. `x` and `y` are the\n      sample points and values (respectively) of the function to be\n      interpolated. `xi` are the points at which the interpolation is\n      desired and `yi` are the corresponding interpolated values returned by the\n      function. The default value for `curve_interpolator` is None in which\n      case linear interpolation is used.\n    initial_curve_rates: Optional `Tensor` of the same dtype and shape as\n      `present_values`. The starting guess for the discount rates used to\n      initialize the iterative procedure.\n      Default value: `None`. If not supplied, the yields to maturity for the\n        bonds is used as the initial value.\n    instrument_weights: Optional \'Tensor\' of the same dtype and shape as\n      `present_values`. This input contains the weight of each instrument in\n      computing the objective function for the conjugate gradient optimization.\n      By default the weights are set to be the inverse of maturities.\n    curve_tolerance: Optional positive scalar `Tensor` of same dtype as\n      elements of `bond_cashflows`. The absolute tolerance for terminating the\n      iterations used to fit the rate curve. The iterations are stopped when the\n      estimated discounts at the expiry times of the bond_cashflows change by a\n      amount smaller than `discount_tolerance` in an iteration.\n      Default value: 1e-8.\n    maximum_iterations: Optional positive integer `Tensor`. The maximum number\n      of iterations permitted when fitting the curve.\n      Default value: 50.\n    dtype: `tf.Dtype`. If supplied the dtype for the (elements of)\n      `float_leg_start_times`, and `fixed_leg_start_times`.\n      Default value: None which maps to the default dtype inferred by\n      TensorFlow.\n    name: Python str. The name to give to the ops created by this function.\n      Default value: `None` which maps to \'swap_curve\'.\n\n  Returns:\n    curve_builder_result: An instance of `SwapCurveBuilderResult` containing the\n      following attributes.\n      times: Rank 1 real `Tensor`. Times for the computed discount rates. These\n        are chosen to be the expiry times of the supplied cashflows.\n      discount_rates: Rank 1 `Tensor` of the same dtype as `times`.\n        The inferred discount rates.\n      discount_factor: Rank 1 `Tensor` of the same dtype as `times`.\n        The inferred discount factors.\n      initial_discount_rates: Rank 1 `Tensor` of the same dtype as `times`. The\n        initial guess for the discount rates.\n      converged: Scalar boolean `Tensor`. Whether the procedure converged.\n        The procedure is said to have converged when the maximum absolute\n        difference in the discount factors from one iteration to the next falls\n        below the `discount_tolerance`.\n      failed: Scalar boolean `Tensor`. Whether the procedure failed. Procedure\n        may fail either because a NaN value was encountered for the discount\n        rates or the discount factors.\n      iterations: Scalar int32 `Tensor`. Number of iterations performed.\n      objective_value: Scalar real `Tensor`. The value of the ibjective function\n        evaluated using the fitted swap curve.\n\n  Raises:\n    ValueError: If the initial state of the curve is not\n    supplied to the function.\n\n  """"""\n\n  with tf.name_scope(name or \'swap_curve\'):\n    if optimize is None:\n      optimize = optimizer.conjugate_gradient_minimize\n\n    present_values = _convert_to_tensors(dtype, present_values,\n                                         \'present_values\')\n    dtype = present_values[0].dtype\n    if present_values_settlement_times is None:\n      pv_settle_times = [tf.zeros_like(pv) for pv in present_values]\n    else:\n      pv_settle_times = present_values_settlement_times\n\n    float_leg_start_times = _convert_to_tensors(dtype, float_leg_start_times,\n                                                \'float_leg_start_times\')\n    float_leg_end_times = _convert_to_tensors(dtype, float_leg_end_times,\n                                              \'float_leg_end_times\')\n    float_leg_daycount_fractions = _convert_to_tensors(\n        dtype, float_leg_daycount_fractions, \'float_leg_daycount_fractions\')\n    fixed_leg_start_times = _convert_to_tensors(dtype, fixed_leg_start_times,\n                                                \'fixed_leg_start_times\')\n    fixed_leg_end_times = _convert_to_tensors(dtype, fixed_leg_end_times,\n                                              \'fixed_leg_end_times\')\n    fixed_leg_daycount_fractions = _convert_to_tensors(\n        dtype, fixed_leg_daycount_fractions, \'fixed_leg_daycount_fractions\')\n    fixed_leg_cashflows = _convert_to_tensors(dtype, fixed_leg_cashflows,\n                                              \'fixed_leg_cashflows\')\n\n    pv_settle_times = _convert_to_tensors(dtype, pv_settle_times,\n                                          \'pv_settle_times\')\n    if instrument_weights is None:\n      instrument_weights = _initialize_instrument_weights(float_leg_end_times,\n                                                          fixed_leg_end_times,\n                                                          dtype=dtype)\n    else:\n      instrument_weights = _convert_to_tensors(dtype, instrument_weights,\n                                               \'instrument_weights\')\n\n    if curve_interpolator is None:\n      def default_interpolator(xi, x, y):\n        return linear.interpolate(xi, x, y, dtype=dtype)\n      curve_interpolator = default_interpolator\n\n    self_discounting_float_leg = False\n    self_discounting_fixed_leg = False\n    # Determine how the floating and fixed leg will be discounted. If separate\n    # discount curves for each leg are not specified, the curve will be self\n    # discounted using the swap curve.\n    if float_leg_discount_rates is None and fixed_leg_discount_rates is None:\n      self_discounting_float_leg = True\n      self_discounting_fixed_leg = True\n      float_leg_discount_rates = [0.0]\n      float_leg_discount_times = [0.]\n      fixed_leg_discount_rates = [0.]\n      fixed_leg_discount_times = [0.]\n    elif fixed_leg_discount_rates is None:\n      fixed_leg_discount_rates = float_leg_discount_rates\n      fixed_leg_discount_times = float_leg_discount_times\n    elif float_leg_discount_rates is None:\n      self_discounting_float_leg = True\n      float_leg_discount_rates = [0.]\n      float_leg_discount_times = [0.]\n\n    # Create tensors for discounting curves\n    float_leg_discount_rates = _convert_to_tensors(dtype,\n                                                   float_leg_discount_rates,\n                                                   \'float_disc_rates\')\n    float_leg_discount_times = _convert_to_tensors(dtype,\n                                                   float_leg_discount_times,\n                                                   \'float_disc_times\')\n    fixed_leg_discount_rates = _convert_to_tensors(dtype,\n                                                   fixed_leg_discount_rates,\n                                                   \'fixed_disc_rates\')\n    fixed_leg_discount_times = _convert_to_tensors(dtype,\n                                                   fixed_leg_discount_times,\n                                                   \'fixed_disc_times\')\n\n    if initial_curve_rates is not None:\n      initial_rates = tf.convert_to_tensor(\n          initial_curve_rates, dtype=dtype, name=\'initial_rates\')\n    else:\n      # TODO(b/144600429): Create a logic for a meaningful initial state of the\n      # curve\n      raise ValueError(\'Initial state of the curve is not specified.\')\n\n    return _build_swap_curve(float_leg_start_times,\n                             float_leg_end_times,\n                             float_leg_daycount_fractions,\n                             fixed_leg_start_times,\n                             fixed_leg_end_times,\n                             fixed_leg_cashflows,\n                             fixed_leg_daycount_fractions,\n                             float_leg_discount_rates,\n                             float_leg_discount_times,\n                             fixed_leg_discount_rates,\n                             fixed_leg_discount_times,\n                             self_discounting_float_leg,\n                             self_discounting_fixed_leg,\n                             present_values,\n                             pv_settle_times,\n                             optimize,\n                             curve_interpolator,\n                             initial_rates,\n                             instrument_weights,\n                             curve_tolerance,\n                             maximum_iterations)\n\n\ndef _build_swap_curve(float_leg_start_times, float_leg_end_times,\n                      float_leg_daycount_fractions, fixed_leg_start_times,\n                      fixed_leg_end_times, fixed_leg_cashflows,\n                      fixed_leg_daycount_fractions, float_leg_discount_rates,\n                      float_leg_discount_times, fixed_leg_discount_rates,\n                      fixed_leg_discount_times, self_discounting_float_leg,\n                      self_discounting_fixed_leg, present_values,\n                      pv_settlement_times, optimize, curve_interpolator,\n                      initial_rates, instrument_weights, curve_tolerance,\n                      maximum_iterations):\n  """"""Build the zero swap curve.""""""\n  # The procedure uses optimization to estimate the swap curve as follows:\n  # 1. Start with an initial state of the swap curve.\n  # 2. Define a loss function which measures the deviations between model prices\n  #   of the IR swaps and their present values specified as input.\n  # 3. Use numerical optimization (currently conjugate gradient optimization) to\n  #   to build the swap curve such that the loss function is minimized.\n  del fixed_leg_start_times, float_leg_daycount_fractions\n  curve_tensors = _create_curve_building_tensors(\n      float_leg_start_times, float_leg_end_times, fixed_leg_end_times,\n      pv_settlement_times)\n  expiry_times = curve_tensors.expiry_times\n  calc_groups_float = curve_tensors.calc_groups_float\n  calc_groups_fixed = curve_tensors.calc_groups_fixed\n  settle_times_float = curve_tensors.settle_times_float\n  settle_times_fixed = curve_tensors.settle_times_fixed\n\n  float_leg_calc_times_start = tf.concat(float_leg_start_times, axis=0)\n  float_leg_calc_times_end = tf.concat(float_leg_end_times, axis=0)\n  calc_fixed_leg_cashflows = tf.concat(fixed_leg_cashflows, axis=0)\n  calc_fixed_leg_daycount = tf.concat(fixed_leg_daycount_fractions, axis=0)\n  fixed_leg_calc_times = tf.concat(fixed_leg_end_times, axis=0)\n\n  def _interpolate(x1, x_data, y_data):\n    return curve_interpolator(x1, x_data, y_data)\n\n  @make_val_and_grad_fn\n  def loss_function(x):\n    """"""Loss function for the optimization.""""""\n    # Currently the loss function is a weighted root mean squared difference\n    # between the model PV and market PV. The model PV is interest rate swaps is\n    # computed as follows:\n\n    # 1. Interpolate the swap curve at intermediate times required to compute\n    #   forward rates for the computation of floating cashflows.\n    # 2. Interpolate swap curve or the discount curve (if a separate discount\n    #   curve is specified) at intermediate cashflow times.\n    # 3. Compute the PV of the swap as the aggregate of floating and fixed legs.\n    # 4. Compute the loss (which is being minized) as the weighted root mean\n    #   squared difference between the model PV (computed above) and the market\n    #   PV (specified as input).\n\n    rates_start = _interpolate(float_leg_calc_times_start, expiry_times, x)\n    rates_end = _interpolate(float_leg_calc_times_end, expiry_times, x)\n    float_cashflows = (\n        tf.math.exp(float_leg_calc_times_end * rates_end) /\n        tf.math.exp(float_leg_calc_times_start * rates_start) - 1.)\n\n    if self_discounting_float_leg:\n      float_discount_rates = rates_end\n      float_settle_rates = _interpolate(settle_times_float, expiry_times, x)\n    else:\n      float_discount_rates = _interpolate(float_leg_calc_times_end,\n                                          float_leg_discount_times,\n                                          float_leg_discount_rates)\n      float_settle_rates = _interpolate(settle_times_float,\n                                        float_leg_discount_times,\n                                        float_leg_discount_rates)\n    if self_discounting_fixed_leg:\n      fixed_discount_rates = _interpolate(fixed_leg_calc_times, expiry_times, x)\n      fixed_settle_rates = _interpolate(settle_times_fixed, expiry_times, x)\n    else:\n      fixed_discount_rates = _interpolate(fixed_leg_calc_times,\n                                          fixed_leg_discount_times,\n                                          fixed_leg_discount_rates)\n      fixed_settle_rates = _interpolate(settle_times_fixed,\n                                        fixed_leg_discount_times,\n                                        fixed_leg_discount_rates)\n\n    # exp(-r(t) * t) / exp(-r(t_s) * t_s)\n    calc_discounts_float_leg = (\n        tf.math.exp(-float_discount_rates * float_leg_calc_times_end +\n                    float_settle_rates * settle_times_float))\n\n    calc_discounts_fixed_leg = (\n        tf.math.exp(-fixed_discount_rates * fixed_leg_calc_times +\n                    fixed_settle_rates * settle_times_fixed))\n\n    float_pv = tf.math.segment_sum(float_cashflows * calc_discounts_float_leg,\n                                   calc_groups_float)\n    fixed_pv = tf.math.segment_sum(\n        calc_fixed_leg_daycount * calc_fixed_leg_cashflows *\n        calc_discounts_fixed_leg, calc_groups_fixed)\n    swap_pv = float_pv + fixed_pv\n\n    value = tf.math.reduce_sum(input_tensor=instrument_weights *\n                               (swap_pv - present_values)**2)\n\n    return value\n\n  optimization_result = optimize(\n      loss_function, initial_position=initial_rates, tolerance=curve_tolerance,\n      max_iterations=maximum_iterations)\n\n  discount_rates = optimization_result.position\n  discount_factors = tf.math.exp(-discount_rates * expiry_times)\n  results = scc.SwapCurveBuilderResult(\n      times=expiry_times,\n      rates=discount_rates,\n      discount_factors=discount_factors,\n      initial_rates=initial_rates,\n      converged=optimization_result.converged,\n      failed=optimization_result.failed,\n      iterations=optimization_result.num_iterations,\n      objective_value=optimization_result.objective_value)\n  return results\n\n\ndef _convert_to_tensors(dtype, input_array, name):\n  """"""Converts the supplied list to a tensor.""""""\n\n  output_tensor = [\n      tf.convert_to_tensor(\n          x, dtype=dtype, name=name + \'_{}\'.format(i))\n      for i, x in enumerate(input_array)\n  ]\n\n  return output_tensor\n\n\ndef _initialize_instrument_weights(float_times, fixed_times, dtype):\n  """"""Function to compute default initial weights for optimization.""""""\n  weights = tf.ones(len(float_times), dtype=dtype)\n  one = tf.ones([], dtype=dtype)\n  float_times_last = tf.stack([times[-1] for times in float_times])\n  fixed_times_last = tf.stack([times[-1] for times in fixed_times])\n  weights = tf.maximum(one / float_times_last, one / fixed_times_last)\n  weights = tf.minimum(one, weights)\n  return tf.unstack(weights, name=\'instrument_weights\')\n\n\nCurveFittingVars = collections.namedtuple(\n    \'CurveFittingVars\',\n    [\n        # The `Tensor` of maturities at which the curve will be built.\n        # Coorspond to maturities on the underlying instruments\n        \'expiry_times\',\n        # `Tensor` containing the instrument index of each floating cashflow\n        \'calc_groups_float\',\n        # `Tensor` containing the instrument index of each fixed cashflow\n        \'calc_groups_fixed\',\n        # `Tensor` containing the settlement time of each floating cashflow\n        \'settle_times_float\',\n        # `Tensor` containing the settlement time of each fixed cashflow\n        \'settle_times_fixed\'\n    ])\n\n\ndef _create_curve_building_tensors(float_leg_start_times,\n                                   float_leg_end_times,\n                                   fixed_leg_end_times,\n                                   pv_settlement_times):\n  """"""Helper function to create tensors needed for curve construction.""""""\n  calc_groups_float = []\n  calc_groups_fixed = []\n  expiry_times = []\n  settle_times_float = []\n  settle_times_fixed = []\n  num_instruments = len(float_leg_start_times)\n  for i in range(num_instruments):\n    expiry_times.append(\n        tf.math.maximum(float_leg_end_times[i][-1], fixed_leg_end_times[i][-1]))\n\n    calc_groups_float.append(\n        tf.fill(tf.shape(float_leg_start_times[i]), i))\n    calc_groups_fixed.append(tf.fill(tf.shape(fixed_leg_end_times[i]), i))\n    settle_times_float.append(tf.fill(tf.shape(float_leg_start_times[i]),\n                                      pv_settlement_times[i]))\n    settle_times_fixed.append(tf.fill(tf.shape(fixed_leg_end_times[i]),\n                                      pv_settlement_times[i]))\n\n  expiry_times = tf.stack(expiry_times, axis=0)\n  calc_groups_float = tf.concat(calc_groups_float, axis=0)\n  calc_groups_fixed = tf.concat(calc_groups_fixed, axis=0)\n  settle_times_float = tf.concat(settle_times_float, axis=0)\n  settle_times_fixed = tf.concat(settle_times_fixed, axis=0)\n\n  return CurveFittingVars(expiry_times=expiry_times,\n                          calc_groups_float=calc_groups_float,\n                          calc_groups_fixed=calc_groups_fixed,\n                          settle_times_float=settle_times_float,\n                          settle_times_fixed=settle_times_fixed)\n'"
tf_quant_finance/rates/swap_curve_test.py,3,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for swap_curve.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass SwapCurveTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32\n      }, {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64\n      })\n  def test_correctness(self, dtype):\n    mats = [1., 2., 3., 5., 7., 10., 30.]\n    par_swap_rates = [2.855, 3.097, 3.134, 3.152, 3.181, 3.23, 3.27]\n    float_leg_start_times = [np.arange(0., x, 0.25, dtype) for x in mats]\n\n    float_leg_end_times = [\n        tf.convert_to_tensor(\n            np.arange(0.25, x + 0.1, 0.25, dtype)) for x in mats\n    ]\n\n    float_leg_dc = [\n        np.array(np.repeat(0.25, len(x)), dtype=dtype)\n        for x in float_leg_start_times\n    ]\n\n    fixed_leg_start_times = [np.arange(0., x, 0.5, dtype) for x in mats]\n\n    fixed_leg_end_times = [np.arange(0.5, x + 0.1, 0.5, dtype) for x in mats]\n\n    fixed_leg_dc = [\n        np.array(np.repeat(0.5, len(x)), dtype=dtype)\n        for x in fixed_leg_start_times\n    ]\n\n    fixed_leg_cashflows = [\n        np.array(np.repeat(-y / 100., len(x)), dtype=dtype)\n        for x, y in zip(fixed_leg_start_times, par_swap_rates)\n    ]\n\n    pvs = np.array(np.repeat(0., len(mats)), dtype=dtype)\n\n    initial_curve_rates = np.array(np.repeat(0.01, len(mats)), dtype=dtype)\n\n    results = self.evaluate(\n        tff.rates.swap_curve_fit(\n            float_leg_start_times,\n            float_leg_end_times,\n            float_leg_dc,\n            fixed_leg_start_times,\n            fixed_leg_end_times,\n            fixed_leg_cashflows,\n            fixed_leg_dc,\n            pvs,\n            dtype=dtype,\n            initial_curve_rates=initial_curve_rates))\n\n    np.testing.assert_allclose(results.times, [1.0, 2.0, 3.0, 5.0, 7.0, 10.0,\n                                               30.0])\n    self.assertFalse(results.failed)\n    expected_discount_rates = np.array([\n        0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n        0.03213901, 0.03257991\n    ],\n                                       dtype=dtype)\n    np.testing.assert_allclose(\n        results.rates, expected_discount_rates, atol=1e-6)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32\n      }, {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64\n      })\n  def test_OIS_discounting(self, dtype):\n    """"""Test the discouting of cashflows using a separate discounting curve.""""""\n    mats = [1., 2., 3., 5., 7., 10., 30.]\n    par_swap_rates = [2.855, 3.097, 3.134, 3.152, 3.181, 3.23, 3.27]\n\n    expected_discount_rates = np.array([\n        0.02844861, 0.03084989, 0.03121727, 0.0313961, 0.0316839, 0.03217002,\n        0.03256696\n    ],\n                                       dtype=np.float64)\n\n    float_leg_start_times = [np.arange(0., x, 0.25, dtype) for x in mats]\n\n    float_leg_end_times = [\n        np.arange(0.25, x + 0.1, 0.25, dtype) for x in mats\n    ]\n\n    float_leg_dc = [\n        np.array(np.repeat(0.25, len(x)), dtype=dtype)\n        for x in float_leg_start_times\n    ]\n\n    fixed_leg_start_times = [np.arange(0., x, 0.5, dtype) for x in mats]\n\n    fixed_leg_end_times = [np.arange(0.5, x + 0.1, 0.5, dtype) for x in mats]\n\n    fixed_leg_dc = [\n        np.array(np.repeat(0.5, len(x)), dtype=dtype)\n        for x in fixed_leg_start_times\n    ]\n\n    fixed_leg_cashflows = [\n        np.array(np.repeat(-y / 100., len(x)), dtype=dtype)\n        for x, y in zip(fixed_leg_start_times, par_swap_rates)\n    ]\n\n    pvs = np.array(np.repeat(0., len(mats)), dtype=dtype)\n\n    discount_rates = np.array([0.0, 0.0, 0.0, 0.0], dtype=dtype)\n    discount_times = np.array([1.0, 5.0, 10.0, 30.0], dtype=dtype)\n\n    initial_curve_rates = np.array(np.repeat(0.01, len(mats)), dtype=dtype)\n\n    results = self.evaluate(\n        tff.rates.swap_curve_fit(\n            float_leg_start_times,\n            float_leg_end_times,\n            float_leg_dc,\n            fixed_leg_start_times,\n            fixed_leg_end_times,\n            fixed_leg_cashflows,\n            fixed_leg_dc,\n            pvs,\n            float_leg_discount_rates=discount_rates,\n            float_leg_discount_times=discount_times,\n            dtype=dtype,\n            initial_curve_rates=initial_curve_rates))\n\n    np.testing.assert_allclose(results.times, [1.0, 2.0, 3.0, 5.0, 7.0, 10.0,\n                                               30.0])\n    self.assertFalse(results.failed)\n    np.testing.assert_allclose(\n        results.rates, expected_discount_rates, atol=1e-6)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32\n      }, {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64\n      })\n  def test_interpolation_const_fwd(self, dtype):\n    mats = [1., 2., 3., 5., 7., 10., 30.]\n    par_swap_rates = [2.855, 3.097, 3.134, 3.152, 3.181, 3.23, 3.27]\n    float_leg_start_times = [np.arange(0., x, 0.25, dtype) for x in mats]\n\n    float_leg_end_times = [\n        np.arange(0.25, x + 0.1, 0.25, dtype) for x in mats\n    ]\n\n    float_leg_dc = [\n        np.array(np.repeat(0.25, len(x)), dtype=dtype)\n        for x in float_leg_start_times\n    ]\n\n    fixed_leg_start_times = [np.arange(0., x, 0.5, dtype) for x in mats]\n\n    fixed_leg_end_times = [np.arange(0.5, x + 0.1, 0.5, dtype) for x in mats]\n\n    fixed_leg_dc = [\n        np.array(np.repeat(0.5, len(x)), dtype=dtype)\n        for x in fixed_leg_start_times\n    ]\n\n    fixed_leg_cashflows = [\n        np.array(np.repeat(-y / 100., len(x)), dtype=dtype)\n        for x, y in zip(fixed_leg_start_times, par_swap_rates)\n    ]\n\n    pvs = np.array(np.repeat(0., len(mats)), dtype=dtype)\n\n    initial_curve_rates = np.array(np.repeat(0.01, len(mats)), dtype=dtype)\n    def curve_interpolator(xi, x, y):\n      return tff.rates.constant_fwd.interpolate(xi, x, y, dtype=dtype)\n\n    results = self.evaluate(\n        tff.rates.swap_curve_fit(\n            float_leg_start_times,\n            float_leg_end_times,\n            float_leg_dc,\n            fixed_leg_start_times,\n            fixed_leg_end_times,\n            fixed_leg_cashflows,\n            fixed_leg_dc,\n            pvs,\n            dtype=dtype,\n            curve_interpolator=curve_interpolator,\n            initial_curve_rates=initial_curve_rates))\n\n    np.testing.assert_allclose(results.times, [1.0, 2.0, 3.0, 5.0, 7.0, 10.0,\n                                               30.0])\n    self.assertFalse(results.failed)\n    expected_discount_rates = np.array([\n        0.02834814, 0.03076991, 0.03113377, 0.03130508, 0.03160601,\n        0.03213445, 0.0325467\n    ],\n                                       dtype=dtype)\n\n    np.testing.assert_allclose(\n        results.rates, expected_discount_rates, atol=1e-6)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32\n      }, {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64\n      })\n  def test_settlement_times(self, dtype):\n    mats = [1., 2., 3., 5., 7., 10., 30.]\n    par_swap_rates = [2.855, 3.097, 3.134, 3.152, 3.181, 3.23, 3.27]\n    float_leg_start_times = [np.arange(0., x, 0.25, dtype) for x in mats]\n\n    float_leg_end_times = [\n        np.arange(0.25, x + 0.1, 0.25, dtype) for x in mats\n    ]\n\n    float_leg_dc = [\n        np.array(np.repeat(0.25, len(x)), dtype=dtype)\n        for x in float_leg_start_times\n    ]\n\n    fixed_leg_start_times = [np.arange(0., x, 0.5, dtype) for x in mats]\n\n    fixed_leg_end_times = [np.arange(0.5, x + 0.1, 0.5, dtype) for x in mats]\n\n    fixed_leg_dc = [\n        np.array(np.repeat(0.5, len(x)), dtype=dtype)\n        for x in fixed_leg_start_times\n    ]\n\n    fixed_leg_cashflows = [\n        np.array(np.repeat(-y / 100., len(x)), dtype=dtype)\n        for x, y in zip(fixed_leg_start_times, par_swap_rates)\n    ]\n\n    discount_rates_float = np.array([0.01, 0.01, 0.01, 0.01], dtype=dtype)\n    discount_times_float = np.array([1.0, 5.0, 10.0, 30.0], dtype=dtype)\n    discount_rates_fixed = np.array([0.02, 0.02, 0.02, 0.02], dtype=dtype)\n    discount_times_fixed = np.array([1.0, 5.0, 10.0, 30.0], dtype=dtype)\n\n    pvs = np.array(np.repeat(0., len(mats)), dtype=dtype)\n\n    settle_times = np.array(np.repeat(3./365., len(mats)), dtype=dtype)\n\n    initial_curve_rates = np.array(np.repeat(0.01, len(mats)), dtype=dtype)\n\n    results = self.evaluate(\n        tff.rates.swap_curve_fit(\n            float_leg_start_times,\n            float_leg_end_times,\n            float_leg_dc,\n            fixed_leg_start_times,\n            fixed_leg_end_times,\n            fixed_leg_cashflows,\n            fixed_leg_dc,\n            pvs,\n            float_leg_discount_rates=discount_rates_float,\n            float_leg_discount_times=discount_times_float,\n            fixed_leg_discount_rates=discount_rates_fixed,\n            fixed_leg_discount_times=discount_times_fixed,\n            present_values_settlement_times=settle_times,\n            dtype=dtype,\n            initial_curve_rates=initial_curve_rates))\n\n    np.testing.assert_allclose(results.times, [1.0, 2.0, 3.0, 5.0, 7.0, 10.0,\n                                               30.0])\n    self.assertFalse(results.failed)\n    expected_discount_rates = np.array([\n        0.02820418, 0.03044715, 0.0306564, 0.03052609, 0.030508, 0.03053637,\n        0.02808225\n    ],\n                                       dtype=dtype)\n\n    np.testing.assert_allclose(\n        results.rates, expected_discount_rates, atol=1e-6)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32\n      }, {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64\n      })\n  def test_correctness_bootstrap(self, dtype):\n    mats = [1., 2., 3., 5., 7., 10., 30.]\n    par_swap_rates = [2.855, 3.097, 3.134, 3.152, 3.181, 3.23, 3.27]\n    float_leg_start_times = [np.arange(0., x, 0.25, dtype) for x in mats]\n\n    float_leg_end_times = [\n        np.arange(0.25, x + 0.1, 0.25, dtype) for x in mats\n    ]\n\n    float_leg_dc = [\n        np.array(np.repeat(0.25, len(x)), dtype=dtype)\n        for x in float_leg_start_times\n    ]\n\n    fixed_leg_start_times = [np.arange(0., x, 0.5, dtype) for x in mats]\n\n    fixed_leg_end_times = [np.arange(0.5, x + 0.1, 0.5, dtype) for x in mats]\n\n    fixed_leg_dc = [\n        np.array(np.repeat(0.5, len(x)), dtype=dtype)\n        for x in fixed_leg_start_times\n    ]\n\n    fixed_leg_cashflows = [\n        np.array(np.repeat(-y / 100., len(x)), dtype=dtype)\n        for x, y in zip(fixed_leg_start_times, par_swap_rates)\n    ]\n\n    pvs = np.array(np.repeat(0., len(mats)), dtype=dtype)\n\n    initial_curve_rates = np.array(np.repeat(0.01, len(mats)), dtype=dtype)\n\n    results = self.evaluate(\n        tff.rates.swap_curve_bootstrap(\n            float_leg_start_times,\n            float_leg_end_times,\n            fixed_leg_start_times,\n            fixed_leg_end_times,\n            fixed_leg_cashflows,\n            pvs,\n            float_leg_daycount_fractions=float_leg_dc,\n            fixed_leg_daycount_fractions=fixed_leg_dc,\n            dtype=dtype,\n            initial_curve_rates=initial_curve_rates))\n\n    np.testing.assert_allclose(results.times, [1.0, 2.0, 3.0, 5.0, 7.0, 10.0,\n                                               30.0])\n    self.assertFalse(results.failed)\n    expected_discount_rates = np.array([\n        0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n        0.03213901, 0.03257991\n    ],\n                                       dtype=dtype)\n    np.testing.assert_allclose(\n        results.rates, expected_discount_rates, atol=1e-6)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32\n      }, {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64\n      })\n  def test_OIS_discounting_bootstrap(self, dtype):\n    """"""Test the discouting of cashflows using a separate discounting curve.""""""\n    mats = [1., 2., 3., 5., 7., 10., 30.]\n    par_swap_rates = [2.855, 3.097, 3.134, 3.152, 3.181, 3.23, 3.27]\n\n    expected_discount_rates = np.array([\n        0.02844861, 0.03084989, 0.03121727, 0.0313961, 0.0316839, 0.03217002,\n        0.03256696\n    ], dtype=np.float64)\n\n    float_leg_start_times = [np.arange(0., x, 0.25, dtype) for x in mats]\n\n    float_leg_end_times = [\n        np.arange(0.25, x + 0.1, 0.25, dtype) for x in mats\n    ]\n\n    float_leg_dc = [\n        np.array(np.repeat(0.25, len(x)), dtype=dtype)\n        for x in float_leg_start_times\n    ]\n\n    fixed_leg_start_times = [np.arange(0., x, 0.5, dtype) for x in mats]\n\n    fixed_leg_end_times = [np.arange(0.5, x + 0.1, 0.5, dtype) for x in mats]\n\n    fixed_leg_dc = [\n        np.array(np.repeat(0.5, len(x)), dtype=dtype)\n        for x in fixed_leg_start_times\n    ]\n\n    fixed_leg_cashflows = [\n        np.array(np.repeat(-y / 100., len(x)), dtype=dtype)\n        for x, y in zip(fixed_leg_start_times, par_swap_rates)\n    ]\n\n    pvs = np.array(np.repeat(0., len(mats)), dtype=dtype)\n\n    discount_rates = np.array([0.0, 0.0, 0.0, 0.0], dtype=dtype)\n    discount_times = np.array([1.0, 5.0, 10.0, 30.0], dtype=dtype)\n\n    initial_curve_rates = np.array(np.repeat(0.01, len(mats)), dtype=dtype)\n\n    results = self.evaluate(\n        tff.rates.swap_curve_bootstrap(\n            float_leg_start_times,\n            float_leg_end_times,\n            fixed_leg_start_times,\n            fixed_leg_end_times,\n            fixed_leg_cashflows,\n            pvs,\n            float_leg_daycount_fractions=float_leg_dc,\n            fixed_leg_daycount_fractions=fixed_leg_dc,\n            float_leg_discount_rates=discount_rates,\n            float_leg_discount_times=discount_times,\n            dtype=dtype,\n            initial_curve_rates=initial_curve_rates))\n\n    np.testing.assert_allclose(results.times, [1.0, 2.0, 3.0, 5.0, 7.0, 10.0,\n                                               30.0])\n    self.assertFalse(results.failed)\n    np.testing.assert_allclose(\n        results.rates, expected_discount_rates, atol=1e-6)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32\n      }, {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64\n      })\n  def test_missing_daycount_bootstrap(self, dtype):\n    mats = [1., 2., 3., 5., 7., 10., 30.]\n    par_swap_rates = [2.855, 3.097, 3.134, 3.152, 3.181, 3.23, 3.27]\n    float_leg_start_times = [np.arange(0., x, 0.25, dtype) for x in mats]\n\n    float_leg_end_times = [\n        np.arange(0.25, x + 0.1, 0.25, dtype) for x in mats\n    ]\n\n    fixed_leg_start_times = [np.arange(0., x, 0.5, dtype) for x in mats]\n\n    fixed_leg_end_times = [np.arange(0.5, x + 0.1, 0.5, dtype) for x in mats]\n\n    fixed_leg_cashflows = [\n        np.array(np.repeat(-y / 100., len(x)), dtype=dtype)\n        for x, y in zip(fixed_leg_start_times, par_swap_rates)\n    ]\n\n    pvs = np.array(np.repeat(0., len(mats)), dtype=dtype)\n\n    initial_curve_rates = np.array(np.repeat(0.01, len(mats)), dtype=dtype)\n\n    results = self.evaluate(\n        tff.rates.swap_curve_bootstrap(\n            float_leg_start_times,\n            float_leg_end_times,\n            fixed_leg_start_times,\n            fixed_leg_end_times,\n            fixed_leg_cashflows,\n            pvs,\n            dtype=dtype,\n            initial_curve_rates=initial_curve_rates))\n\n    np.testing.assert_allclose(results.times, [1.0, 2.0, 3.0, 5.0, 7.0, 10.0,\n                                               30.0])\n\n    self.assertFalse(results.failed)\n    expected_discount_rates = np.array(\n        [0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n         0.03213901, 0.03257991], dtype=dtype)\n    np.testing.assert_allclose(\n        results.rates, expected_discount_rates, atol=1e-6)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/black_scholes/approximations/__init__.py,0,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Approximations to the black scholes formula.""""""\n\nfrom tf_quant_finance.black_scholes.approximations.american_option import adesi_whaley\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'adesi_whaley\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/black_scholes/approximations/american_option.py,40,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Approximate formulas for American option pricing.""""""\n\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance.black_scholes import implied_vol_newton_root\nfrom tf_quant_finance.black_scholes import vanilla_prices\nfrom tf_quant_finance.math import gradient\n\n\n# TODO(b/151823233): remove `cost_of_carries` argument\ndef adesi_whaley(*,\n                 volatilities,\n                 strikes,\n                 expiries,\n                 spots=None,\n                 forwards=None,\n                 discount_rates=None,\n                 continuous_dividends=None,\n                 cost_of_carries=None,\n                 discount_factors=None,\n                 is_call_options=None,\n                 max_iterations=100,\n                 tolerance=1e-8,\n                 dtype=None,\n                 name=None):\n  """"""Computes American option prices using the Baron-Adesi Whaley approximation.\n\n  #### Example\n\n  ```python\n  spots = np.array([80.0, 90.0, 100.0, 110.0, 120.0])\n  strikes = np.array([100.0, 100.0, 100.0, 100.0, 100.0])\n  volatilities = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n  expiries = 0.25\n  cost_of_carries = -0.04\n  discount_rates = 0.08\n  computed_prices = adesi_whaley(\n      volatilities,\n      strikes,\n      expiries,\n      discount_rates,\n      cost_of_carries,\n      spots=spots,\n      dtype=tf.float64)\n  # Expected print output of computed prices:\n  # [0.03, 0.59, 3.52, 10.31, 20.0]\n  ```\n\n  #### References:\n  [1] Baron-Adesi, Whaley, Efficient Analytic Approximation of American Option\n    Values, The Journal of Finance, Vol XLII, No. 2, June 1987\n    https://deriscope.com/docs/Barone_Adesi_Whaley_1987.pdf\n\n  Args:\n    volatilities: Real `Tensor` of any shape and dtype. The volatilities to\n      expiry of the options to price.\n    strikes: A real `Tensor` of the same dtype and compatible shape as\n      `volatilities`. The strikes of the options to be priced.\n    expiries: A real `Tensor` of same dtype and compatible shape as\n      `volatilities`. The expiry of each option. The units should be such that\n      `expiry * volatility**2` is dimensionless.\n    spots: A real `Tensor` of any shape that broadcasts to the shape of the\n      `volatilities`. The current spot price of the underlying. Either this\n      argument or the `forwards` (but not both) must be supplied.\n    forwards: A real `Tensor` of any shape that broadcasts to the shape of\n      `volatilities`. The forwards to maturity. Either this argument or the\n      `spots` must be supplied but both must not be supplied.\n    discount_rates: An optional real `Tensor` of same dtype as the\n      `volatilities`. If not `None`, discount factors are calculated as e^(-rT),\n      where r are the discount rates, or risk free rates.\n      Default value: `None`, equivalent to r = 0 and discount factors = 1 when\n      discount_factors also not given.\n    continuous_dividends: An optional real `Tensor` of same dtype as the\n      `volatilities`. If not `None`, cost_of_carries is calculated as r - q,\n      where r are the discount rates and q is continuous_dividends. Either this\n      or cost_of_carries can be given.\n      Default value: `None`, equivalent to q = 0.\n    cost_of_carries: An optional real `Tensor` of same dtype as the\n      `volatilities`. Cost of storing a physical commodity, the cost of\n      interest paid when long, or the opportunity cost, or the cost of paying\n      dividends when short. If not `None`, and `spots` is supplied, used to\n      calculate forwards from spots: F = e^(bT) * S. If `None`, value assumed\n      to be equal to the discount rate - continuous_dividends\n      Default value: `None`, equivalent to b = r.\n    discount_factors: An optional real `Tensor` of same dtype as the\n      `volatilities`. If not `None`, these are the discount factors to expiry\n      (i.e. e^(-rT)). Mutually exclusive with discount_rate and cost_of_carry.\n      If neither is given, no discounting is applied (i.e. the undiscounted\n      option price is returned). If `spots` is supplied and `discount_factors`\n      is not `None` then this is also used to compute the forwards to expiry.\n      At most one of discount_rates and discount_factors can be supplied.\n      Default value: `None`, which maps to -log(discount_factors) / expiries\n    is_call_options: A boolean `Tensor` of a shape compatible with\n      `volatilities`. Indicates whether the option is a call (if True) or a put\n      (if False). If not supplied, call options are assumed.\n    max_iterations: positive `int`. The maximum number of iterations of Newton\'s\n      root finding method to find the critical spot price above and below which\n      the pricing formula is different.\n      Default value: 100\n    tolerance: Positive scalar `Tensor`. As with max_iterations, used with the\n      Newton root finder to find the critical spot price. The root finder will\n      judge an element to have converged if `|f(x_n) - a|` is less than\n      `tolerance` (where `f` is the target function as defined in [1] and\n      `x_n` is the estimated critical value), or if `x_n` becomes `nan`. When an\n      element is judged to have converged it will no longer be updated. If all\n      elements converge before `max_iterations` is reached then the root finder\n      will return early.\n      Default value: 1e-8\n    dtype: Optional `tf.DType`. If supplied, the dtype to be used for conversion\n      of any supplied non-`Tensor` arguments to `Tensor`.\n      Default value: None which maps to the default dtype inferred by\n       TensorFlow.\n    name: str. The name for the ops created by this function.\n      Default value: None which is mapped to the default name `adesi_whaley`.\n\n  Returns:\n    A 3-tuple containing the following items in order:\n       (a) option_prices: A `Tensor` of the same shape as `forwards`. The Black\n         Scholes price of the options.\n       (b) converged: A boolean `Tensor` of the same shape as `option_prices`\n         above. Indicates whether the corresponding adesi-whaley approximation\n         has converged to within tolerance.\n       (c) failed: A boolean `Tensor` of the same shape as `option_prices`\n         above. Indicates whether the corresponding options price is NaN or not\n         a finite number. Note that converged being True implies that failed\n         will be false. However, it may happen that converged is False but\n         failed is not True. This indicates the search did not converge in the\n         permitted number of iterations but may converge if the iterations are\n         increased.\n\n  Raises:\n    ValueError:\n      (a) If both `forwards` and `spots` are supplied or if neither is supplied.\n      (b) If both `continuous_dividends` and `cost_of_carries` are supplied.\n  """"""\n  if (spots is None) == (forwards is None):\n    raise ValueError(""Either spots or forwards must be supplied but not both."")\n  if (discount_rates is not None) and (discount_factors is not None):\n    raise ValueError(""At most one of discount_rates and discount_factors may ""\n                     ""be supplied"")\n  if (continuous_dividends is not None) and (cost_of_carries is not None):\n    raise ValueError(""At most one of continuous_dividends and cost_of_carries ""\n                     ""may be supplied"")\n  with tf.name_scope(name or ""adesi_whaley""):\n    volatilities = tf.convert_to_tensor(volatilities, dtype=dtype,\n                                        name=""volatilities"")\n    dtype = volatilities.dtype  # This dtype should be common for all inputs\n    strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=""strikes"")\n    expiries = tf.convert_to_tensor(expiries, dtype=dtype, name=""expiries"")\n    if discount_rates is not None:\n      discount_rates = tf.convert_to_tensor(discount_rates, dtype=dtype,\n                                            name=""discount_rates"")\n    elif discount_factors is not None:\n      discount_factors = tf.convert_to_tensor(discount_factors, dtype=dtype,\n                                              name=""discount_factors"")\n      discount_rates = -tf.math.log(discount_factors) / expiries\n    else:\n      discount_rates = tf.constant(0.0, dtype=dtype, name=""discount_rates"")\n\n    if cost_of_carries is not None:\n      cost_of_carries = tf.convert_to_tensor(cost_of_carries, dtype=dtype,\n                                             name=""cost_of_carries"")\n    else:\n      if continuous_dividends is not None:\n        continuous_dividends = tf.convert_to_tensor(continuous_dividends,\n                                                    dtype=dtype,\n                                                    name=""continuous_dividends"")\n        cost_of_carries = tf.convert_to_tensor(\n            discount_rates - continuous_dividends, name=""cost_of_carries"")\n      else:\n        cost_of_carries = tf.convert_to_tensor(\n            discount_rates, name=""cost_of_carries"")\n    # Set forwards and spots\n    if forwards is not None:\n      spots = tf.convert_to_tensor(\n          forwards * tf.exp(-cost_of_carries * expiries), dtype=dtype,\n          name=""spots"")\n    else:\n      spots = tf.convert_to_tensor(spots, dtype=dtype, name=""spots"")\n    if is_call_options is not None:\n      is_call_options = tf.convert_to_tensor(is_call_options, dtype=tf.bool)\n\n    # American option prices\n    am_prices, converged, failed = _adesi_whaley(\n        sigma=volatilities, x=strikes, t=expiries, s=spots, r=discount_rates,\n        b=cost_of_carries, is_call_options=is_call_options, dtype=dtype,\n        max_iterations=max_iterations, tolerance=tolerance)\n\n    # For call options where b >= r as per reference [1], only the European\n    # option price should be calclated, while for the rest the american price\n    # formula should be used. For this reason, the vanilla EU price is\n    # calculated for all the spot prices, (assuming they are all call options),\n    # and a subset of these will be used further down, if any of the date points\n    # fit the criteria that they are all call options with b >= r.\n    eu_prices = vanilla_prices.option_price(\n        volatilities=volatilities, strikes=strikes, expiries=expiries,\n        spots=spots, discount_rates=discount_rates,\n        cost_of_carries=cost_of_carries, dtype=dtype, name=name)\n    calculate_eu = tf.logical_and(is_call_options,\n                                  cost_of_carries >= discount_rates)\n    converged = tf.where(calculate_eu, True, converged)\n    failed = tf.where(calculate_eu, False, failed)\n    return tf.where(calculate_eu, eu_prices, am_prices), converged, failed\n\n\ndef _adesi_whaley(*,\n                  sigma,\n                  x,\n                  t,\n                  r,\n                  b,\n                  s,\n                  is_call_options,\n                  max_iterations,\n                  tolerance,\n                  dtype):\n  """"""Computes American option prices using the Baron-Adesi Whaley formula.""""""\n\n  # The naming convention will align variables with the variables named in\n  # reference [1], but made lower case, and differentiating between put and\n  # call option values with the suffix _put and _call.\n  # [1] https://deriscope.com/docs/Barone_Adesi_Whaley_1987.pdf\n  sign = tf.where(\n      is_call_options,\n      tf.constant(1, dtype=dtype),\n      tf.constant(-1, dtype=dtype))\n\n  q2, a2, s_crit, converged, failed = _adesi_whaley_critical_values(\n      sigma=sigma, x=x, t=t, r=r, b=b, sign=sign,\n      is_call_options=is_call_options,\n      max_iterations=max_iterations, tolerance=tolerance, dtype=dtype)\n\n  eu_prices = vanilla_prices.option_price(\n      volatilities=sigma, strikes=x, expiries=t, spots=s, discount_rates=r,\n      cost_of_carries=b, is_call_options=is_call_options, dtype=dtype)\n\n  # The divisive condition is different for put and call options\n  condition = tf.where(is_call_options, s < s_crit, s > s_crit)\n\n  american_prices = tf.where(\n      condition,\n      eu_prices + a2 * (s / s_crit) ** q2,\n      (s - x) * sign)\n\n  return american_prices, converged, failed\n\n\ndef _adesi_whaley_critical_values(*,\n                                  sigma,\n                                  x,\n                                  t,\n                                  r,\n                                  b,\n                                  sign,\n                                  is_call_options,\n                                  max_iterations=20,\n                                  tolerance=1e-8,\n                                  dtype):\n  """"""Computes critical value for the Baron-Adesi Whaley approximation.""""""\n\n  # The naming convention will align variables with the variables named in\n  # reference [1], but made lower case, and differentiating between put and\n  # call option values with the suffix _put and _call.\n  # [1] https://deriscope.com/docs/Barone_Adesi_Whaley_1987.pdf\n\n  m = 2 * r / sigma ** 2\n  n = 2 * b / sigma ** 2\n  k = 1 - tf.exp(- r * t)\n  q = _calc_q(n, m, sign, k)\n\n  def value_fn(s_crit):\n    return (vanilla_prices.option_price(volatilities=sigma,\n                                        strikes=x,\n                                        expiries=t,\n                                        spots=s_crit,\n                                        discount_rates=r,\n                                        cost_of_carries=b,\n                                        is_call_options=is_call_options,\n                                        dtype=dtype)\n            + sign * (1 - tf.math.exp((b - r) * t)\n                      * _ncdf(sign * _calc_d1(s_crit, x, sigma, b, t)))\n            * s_crit / q - sign * (s_crit - x))\n\n  def value_and_gradient_func(price):\n    return  gradient.value_and_gradient(value_fn, price)\n\n  # Calculate seed value for critical spot price for fewer iterations needed, as\n  # defined in reference [1] part II, section B.\n  # [1] https://deriscope.com/docs/Barone_Adesi_Whaley_1987.pdf\n  q_inf = _calc_q(n, m, sign)\n  s_inf = x / (1 - 1 / q_inf)\n  h = -(sign * b * t + 2 * sigma * tf.math.sqrt(t)) * sign * (x / (s_inf - x))\n  if is_call_options is None:\n    s_seed = x + (s_inf - x) * (1 - tf.math.exp(h))\n  else:\n    s_seed = tf.where(is_call_options,\n                      x + (s_inf - x) * (1 - tf.math.exp(h)),\n                      s_inf + (x - s_inf) * tf.math.exp(h))\n\n  s_crit, converged, failed = implied_vol_newton_root.newton_root_finder(\n      value_and_grad_func=value_and_gradient_func, initial_values=s_seed,\n      max_iterations=max_iterations, tolerance=tolerance, dtype=dtype)\n\n  a = (sign * s_crit / q) * (1 - tf.math.exp((b - r) * t) * _ncdf(\n      sign * _calc_d1(s_crit, x, sigma, b, t)))\n\n  return q, a, s_crit, converged, failed\n\n\ndef _calc_d1(s, x, sigma, b, t):\n  return (tf.math.log(s / x) + (b + sigma ** 2 / 2) * t) / (\n      sigma * tf.math.sqrt(t))\n\n\ndef _calc_q(n, m, sign, k=1):\n  return ((1 - n) + sign * tf.math.sqrt((n - 1) ** 2 + 4 * m / k)) / 2\n\n\ndef _ncdf(x):\n  return (tf.math.erf(x / _SQRT_2) + 1) / 2\n\n\ndef _npdf(x):\n  return tf.math.exp(-0.5 * x ** 2) / _SQRT_2_PI\n\n\n_SQRT_2_PI = np.sqrt(2 * np.pi, dtype=np.float64)\n_SQRT_2 = np.sqrt(2., dtype=np.float64)\n'"
tf_quant_finance/black_scholes/approximations/american_option_test.py,21,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for american_option.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.black_scholes.approximations.american_option import adesi_whaley\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass AmericanPrice(parameterized.TestCase, tf.test.TestCase):\n  """"""Tests for methods for the american pricing module.""""""\n\n  @parameterized.parameters(\n      (0.08, 0.2, 0.25,\n       [0.03, 0.59, 3.52, 10.31, 20.0, 20.42, 11.25, 4.40, 1.12, 0.18]),\n      (0.12, 0.2, 0.25,\n       [0.03, 0.59, 3.51, 10.29, 20.0, 20.25, 11.15, 4.35, 1.11, 0.18]),\n      (0.08, 0.4, 0.25,\n       [1.07, 3.28, 7.41, 13.50, 21.23, 21.46, 13.93, 8.27, 4.52, 2.30]),\n      (0.08, 0.2, 0.5,\n       [0.23, 1.39, 4.72, 10.96, 20.0, 20.98, 12.64, 6.37, 2.65, 0.92]),\n  )\n  def test_option_prices_neg_carries(self,\n                                     discount_rates,\n                                     volatilities,\n                                     expiries,\n                                     expected_prices):\n    """"""Tests the prices for negative cost_of_carries.""""""\n    spots = np.array([80.0, 90.0, 100.0, 110.0, 120.0] * 2)\n    strikes = np.array([100.0] * 10)\n    is_call_options = np.array([True] * 5 + [False] * 5)\n    cost_of_carries = -0.04\n    computed_prices, converged, failed = adesi_whaley(\n        volatilities=volatilities,\n        strikes=strikes,\n        expiries=expiries,\n        discount_rates=discount_rates,\n        cost_of_carries=cost_of_carries,\n        is_call_options=is_call_options,\n        spots=spots,\n        dtype=tf.float64)\n    expected_prices = np.array(expected_prices)\n    with self.subTest(name=\'ExpectedPrices\'):\n      self.assertAllClose(expected_prices, computed_prices,\n                          rtol=5e-3, atol=5e-3)\n    with self.subTest(name=\'AllConverged\'):\n      self.assertAllEqual(converged, tf.ones_like(computed_prices))\n    with self.subTest(name=\'NonFailed\'):\n      self.assertAllEqual(failed, tf.zeros_like(computed_prices))\n\n  @parameterized.parameters(\n      (0.08, 0.2, 0.25,\n       [0.05, 0.85, 4.44, 11.66, 20.90, 20.00, 10.18, 3.54, 0.80, 0.12]),\n      (0.12, 0.2, 0.25,\n       [0.05, 0.84, 4.40, 11.55, 20.69, 20.00, 10.16, 3.53, 0.79, 0.12]),\n      (0.08, 0.4, 0.25,\n       [1.29, 3.82, 8.35, 14.80, 22.72, 20.53, 12.93, 7.46, 3.96, 1.95]),\n      (0.08, 0.2, 0.5,\n       [0.41, 2.18, 6.50, 13.42, 22.06, 20.00, 10.71, 4.77, 1.76, 0.55]),\n  )\n  def test_option_prices_pos_carries(self,\n                                     discount_rates,\n                                     volatilities,\n                                     expiries,\n                                     expected_prices):\n    """"""Tests the prices for positive cost_of_carries.""""""\n    spots = np.array([80.0, 90.0, 100.0, 110.0, 120.0] * 2)\n    strikes = np.array([100.0] * 10)\n    is_call_options = [True] * 5 + [False] * 5\n    cost_of_carries = 0.04\n    computed_prices, converged, failed = adesi_whaley(\n        volatilities=volatilities,\n        strikes=strikes,\n        expiries=expiries,\n        discount_rates=discount_rates,\n        cost_of_carries=cost_of_carries,\n        spots=spots,\n        is_call_options=is_call_options,\n        dtype=tf.float64)\n    with self.subTest(name=\'ExpectedPrices\'):\n      self.assertAllClose(expected_prices, computed_prices,\n                          rtol=5e-3, atol=5e-3)\n    with self.subTest(name=\'AllConverged\'):\n      self.assertAllEqual(converged, tf.ones_like(computed_prices))\n    with self.subTest(name=\'NonFailed\'):\n      self.assertAllEqual(failed, tf.zeros_like(computed_prices))\n\n  @parameterized.parameters(\n      (0.08, 0.2, 0.25,\n       [0.04, 0.70, 3.93, 10.81, 20.02, 20.00, 10.58, 3.93, 0.94, 0.15]),\n      (0.12, 0.2, 0.25,\n       [0.04, 0.70, 3.90, 10.75, 20.0, 20.00, 10.53, 3.90, 0.93, 0.15]),\n      (0.08, 0.4, 0.25,\n       [1.17, 3.53, 7.84, 14.08, 21.86, 20.93, 13.39, 7.84, 4.23, 2.12]),\n      (0.08, 0.2, 0.5,\n       [0.30, 1.72, 5.48, 11.90, 20.34, 20.04, 11.48, 5.48, 2.15, 0.70]),\n  )\n  def test_option_prices_zero_cost_of_carries(self,\n                                              discount_rates,\n                                              volatilities,\n                                              expiries,\n                                              expected_prices):\n    """"""Tests the prices when cost_of_carries is zero.""""""\n    forwards = np.array([80.0, 90.0, 100.0, 110.0, 120.0] * 2)\n    strikes = np.array([100.0] * 10)\n    is_call_options = [True] * 5 + [False] * 5\n    cost_of_carries = 0.\n    computed_prices, converged, failed = adesi_whaley(\n        volatilities=volatilities,\n        strikes=strikes,\n        expiries=expiries,\n        discount_rates=discount_rates,\n        cost_of_carries=cost_of_carries,\n        forwards=forwards,\n        is_call_options=is_call_options,\n        dtype=tf.float64)\n    with self.subTest(name=\'ExpectedPrices\'):\n      self.assertAllClose(expected_prices, computed_prices,\n                          rtol=5e-3, atol=5e-3)\n    with self.subTest(name=\'AllConverged\'):\n      self.assertAllEqual(converged, tf.ones_like(computed_prices))\n    with self.subTest(name=\'NonFailed\'):\n      self.assertAllEqual(failed, tf.zeros_like(computed_prices))\n\n  @parameterized.parameters(\n      (tf.float64, 0.08, 0.2, 0.25, [20.0, 10.01, 3.22, 0.68, 0.10]),\n      (tf.float64, 0.12, 0.2, 0.25, [20.0, 10.0, 2.93, 0.58, 0.08]),\n      (tf.float64, 0.08, 0.4, 0.25, [20.25, 12.51, 7.10, 3.71, 1.81]),\n      (tf.float64, 0.08, 0.2, 0.5, [20.0, 10.23, 4.19, 1.45, 0.42]),\n      (tf.float32, 0.08, 0.2, 0.25, [20.0, 10.01, 3.22, 0.68, 0.10]),\n      (tf.float32, 0.12, 0.2, 0.25, [20.0, 10.0, 2.93, 0.58, 0.08]),\n      (tf.float32, 0.08, 0.4, 0.25, [20.25, 12.51, 7.10, 3.71, 1.81]),\n      (tf.float32, 0.08, 0.2, 0.5, [20.0, 10.23, 4.19, 1.45, 0.42]),\n  )\n  def test_option_prices_no_cost_of_carries(self,\n                                            dtype,\n                                            discount_rates,\n                                            volatilities,\n                                            expiries,\n                                            expected_prices):\n    """"""Tests the prices when no cost_of_carries is supplied.""""""\n    spots = np.array([80.0, 90.0, 100.0, 110.0, 120.0])\n    strikes = np.array([100.0, 100.0, 100.0, 100.0, 100.0])\n    is_call_options = False\n    computed_prices, converged, failed = adesi_whaley(\n        volatilities=volatilities,\n        strikes=strikes,\n        expiries=expiries,\n        discount_rates=discount_rates,\n        spots=spots,\n        is_call_options=is_call_options,\n        tolerance=1e-5,  # float32 does not converge to tolerance 1e-8\n        dtype=dtype)\n    with self.subTest(name=\'ExpectedPrices\'):\n      self.assertAllClose(expected_prices, computed_prices,\n                          rtol=5e-3, atol=5e-3)\n    with self.subTest(name=\'AllConverged\'):\n      self.assertAllEqual(converged, tf.ones_like(computed_prices))\n    with self.subTest(name=\'NonFailed\'):\n      self.assertAllEqual(failed, tf.zeros_like(computed_prices))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/experimental/finite_difference/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Finite difference methods.""""""\n\nfrom tf_quant_finance.experimental.finite_difference import methods\n\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'methods\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/experimental/finite_difference/methods.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Implementations of finite difference methods.""""""\n\nfrom autograd import elementwise_grad as egrad\nimport autograd.numpy as np\nimport pandas as pd\nimport sobol_seq\n\n\ndef rosenbrock(x):\n  """"""Rosenbrock function: test function for evaluating algorithms.""""""\n  x = np.array(x)\n  x_curr, x_next = x[..., :-1], x[..., 1:]\n  terms = 100 * np.square(x_next - np.square(x_curr)) + np.square(1 - x_curr)\n  return np.sum(terms, axis=-1)\n\n\ndef grid(num, ndim, large=False):\n  """"""Build a uniform grid with num points along each of ndim axes.""""""\n  if not large:\n    _check_not_too_large(np.power(num, ndim) * ndim)\n  x = np.linspace(0, 1, num, dtype=\'float64\')\n  w = 1 / (num - 1)\n  points = np.stack(\n      np.meshgrid(*[x for _ in range(ndim)], indexing=\'ij\'), axis=-1)\n  return points, w\n\n\ndef non_uniform_grid(num, ndim, skip=42, large=False):\n  """"""Build a non-uniform grid with num points of ndim dimensions.""""""\n  if not large:\n    _check_not_too_large(num * ndim)\n  return sobol_seq.i4_sobol_generate(ndim, num, skip=skip)\n\n\ndef autograd(f, ds, points):\n  """"""Evaluate derivatives of f on the given points.""""""\n  df_ds = lambda *args: f(np.stack(args, axis=-1))\n  for i in ds:\n    df_ds = egrad(df_ds, i)\n  ndim = points.shape[-1]\n  return df_ds(*[points[..., i] for i in range(ndim)])\n\n\ndef central(f0, ds, w):\n  """"""Apply central difference method to estimate derivatives.""""""\n  f = lambda o: shift(f0, o)\n  eye = np.eye(f0.ndim, dtype=int)\n  offsets = [-eye[d] for d in ds]\n\n  if not ds:\n    return f0\n  elif len(ds) == 1:  # First order derivatives.\n    i = offsets[0]\n    return (f(i) - f(-i)) / (2 * w)\n  elif len(ds) == 2:  # Second order derivatives.\n    i, j = offsets\n    w2 = np.square(w)\n    if ds[0] == ds[1]:  # d^2/dxdx\n      return (f(i) - 2 * f0 + f(-i)) / w2\n    else:  # d^2/dxdy\n      return (f(i + j) - f(i - j) - f(j - i) + f(-i - j)) / (4 * w2)\n  else:\n    raise NotImplementedError(ds)\n\n\ndef triangular(n):\n  """"""Compute the n-th triangular number.""""""\n  return np.floor_divide(n * (n + 1), 2)\n\n\ndef derivative_names(ndim):\n  """"""Iterate over derivative speficiations and their names.""""""\n  # Note: len(list(derivative_names(ndim)) == triangular(ndim + 1).\n  yield (), \'f\'  # Function value.\n  for i in range(ndim):\n    yield (i,), \'df/d%i\' % i  # First derivative along an axis.\n  for i in range(ndim):\n    yield (i, i), \'d^2f/d%i^2\' % i  # Second derivative along an axis.\n  for i, j in zip(*np.triu_indices(ndim, k=1)):\n    # Second derivarive along mixed axes.\n    yield (int(i), int(j)), \'d^2f/(d%i d%i)\' % (i, j)\n\n\ndef taylor_approx(target, stencil, values):\n  """"""Use taylor series to approximate up to second order derivatives.\n\n  Args:\n    target: An array of shape (..., n), a batch of n-dimensional points\n      where one wants to approximate function value and derivatives.\n    stencil: An array of shape broadcastable to (..., k, n), for each target\n      point a set of k = triangle(n + 1) points to use on its approximation.\n    values: An array of shape broadcastable to (..., k), the function value at\n      each of the stencil points.\n\n  Returns:\n    An array of shape (..., k), for each target point the approximated\n    function value, gradient and hessian evaluated at that point (flattened\n    and in the same order as returned by derivative_names).\n  """"""\n  # Broadcast arrays to their required shape.\n  batch_shape, ndim = target.shape[:-1], target.shape[-1]\n  stencil = np.broadcast_to(stencil, batch_shape + (triangular(ndim + 1), ndim))\n  values = np.broadcast_to(values, stencil.shape[:-1])\n\n  # Subtract target from each stencil point.\n  delta_x = stencil - np.expand_dims(target, axis=-2)\n  delta_xy = np.matmul(\n      np.expand_dims(delta_x, axis=-1), np.expand_dims(delta_x, axis=-2))\n  i = np.arange(ndim)\n  j, k = np.triu_indices(ndim, k=1)\n\n  # Build coefficients for the Taylor series equations, namely:\n  #   f(stencil) = coeffs @ [f(target), df/d0(target), ...]\n  coeffs = np.concatenate([\n      np.ones(delta_x.shape[:-1] + (1,)),  # f(target)\n      delta_x,  # df/di(target)\n      delta_xy[..., i, i] / 2,  # d^2f/di^2(target)\n      delta_xy[..., j, k],  # d^2f/{dj dk}(target)\n  ], axis=-1)\n\n  # Then: [f(target), df/d0(target), ...] = coeffs^{-1} @ f(stencil)\n  return np.squeeze(\n      np.matmul(np.linalg.inv(coeffs), values[..., np.newaxis]), axis=-1)\n\n\ndef non_uniform_approx_nearest(points, values):\n  """"""Approximate derivatives using nearest points in non-uniform grid.""""""\n  ndim = points.shape[-1]\n  k = triangular(ndim + 1)\n  diffs = np.expand_dims(points, axis=0) - np.expand_dims(points, axis=1)\n  norms = np.linalg.norm(diffs, axis=-1)\n  nearest_k = np.argpartition(norms, k)[..., :k]\n  return taylor_approx(points, points[nearest_k], values[nearest_k])\n\n\ndef central_errors(f, num, ndim, label=None):\n  """"""Build DataFrame of approximation errors with central differences method.""""""\n  points, w = grid(num, ndim)\n  values = f(points)\n\n  def name_errors():\n    for ds, name in derivative_names(ndim):\n      actual = autograd(f, ds, points)\n      approx = central(values, ds, w)\n      yield name, np.abs(actual - approx)\n\n  return _build_errors_df(name_errors(), label)\n\n\ndef non_uniform_errors(f, num, ndim, label=None):\n  """"""Build DataFrame of approximation errors with non uniform grid.""""""\n  points = non_uniform_grid(np.power(num, ndim), ndim)\n  values = f(points)\n  approx_all = non_uniform_approx_nearest(points, values)\n\n  def name_errors():\n    for (ds, name), approx in zip(derivative_names(ndim), approx_all.T):\n      actual = autograd(f, ds, points)\n      yield name, np.abs(actual - approx)\n\n  return _build_errors_df(name_errors(), label)\n\n\ndef _build_errors_df(name_errors, label):\n  """"""Helper to build errors DataFrame.""""""\n  series = []\n  percentiles = np.linspace(0, 100, 21)\n  index = percentiles / 100\n  for name, errors in name_errors:\n    series.append(pd.Series(\n        np.nanpercentile(errors, q=percentiles), index=index, name=name))\n  df = pd.concat(series, axis=1)\n  df.columns.name = \'derivative\'\n  df.index.name = \'quantile\'\n  df = df.stack().rename(\'error\').reset_index()\n  with np.errstate(divide=\'ignore\'):\n    df[\'log(error)\'] = np.log(df[\'error\'])\n  if label is not None:\n    df[\'label\'] = label\n  return df\n\n\ndef shift(x, offsets):\n  """"""Similar to np.roll, but fills with nan instead of rolling values over.\n\n  Also shifts along multiple axes at the same time.\n\n  Args:\n    x: The input array to shift.\n    offsets: How much to shift each axis, offsets[i] is the offset for the i-th\n      axis.\n\n  Returns:\n    An array with same shape as the input, with specified shifts applied.\n  """"""\n  def to_slice(offset):\n    return slice(offset, None) if offset >= 0 else slice(None, offset)\n\n  out = np.empty_like(x)\n  out.fill(np.nan)\n  ind_src = tuple(to_slice(-o) for o in offsets)\n  ind_dst = tuple(to_slice(o) for o in offsets)\n  out[ind_dst] = x[ind_src]\n  return out\n\n\ndef _check_not_too_large(num_values):\n  if num_values > 10e6:\n    raise ValueError(\'Attempting to create an array with more than 10M values\')\n'"
tf_quant_finance/experimental/instruments/__init__.py,0,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Instruments.""""""\n\nfrom tf_quant_finance.experimental.instruments import bond\nfrom tf_quant_finance.experimental.instruments import cap_floor\nfrom tf_quant_finance.experimental.instruments import cashflow_stream\nfrom tf_quant_finance.experimental.instruments import cms_swap\nfrom tf_quant_finance.experimental.instruments import eurodollar_futures\nfrom tf_quant_finance.experimental.instruments import floating_rate_note\nfrom tf_quant_finance.experimental.instruments import forward_rate_agreement\nfrom tf_quant_finance.experimental.instruments import interest_rate_swap\nfrom tf_quant_finance.experimental.instruments import overnight_index_linked_futures\nfrom tf_quant_finance.experimental.instruments import rate_curve\nfrom tf_quant_finance.experimental.instruments import rates_common\nfrom tf_quant_finance.experimental.instruments import swaption\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\nBond = bond.Bond\nCapAndFloor = cap_floor.CapAndFloor\nEurodollarFutures = eurodollar_futures.EurodollarFutures\nSwaption = swaption.Swaption\nOvernightIndexLinkedFutures = overnight_index_linked_futures.OvernightIndexLinkedFutures\nForwardRateAgreement = forward_rate_agreement.ForwardRateAgreement\nFloatingRateNote = floating_rate_note.FloatingRateNote\nRateCurve = rate_curve.RateCurve\nInterestRateMarket = rates_common.InterestRateMarket\nDayCountConvention = rates_common.DayCountConvention\nFixedCashflowStream = cashflow_stream.FixedCashflowStream\nFloatingCashflowStream = cashflow_stream.FloatingCashflowStream\nCMSCashflowStream = cms_swap.CMSCashflowStream\nCMSSwap = cms_swap.CMSSwap\nInterestRateSwap = interest_rate_swap.InterestRateSwap\nFixedCouponSpecs = rates_common.FixedCouponSpecs\nFloatCouponSpecs = rates_common.FloatCouponSpecs\nCMSCouponSpecs = rates_common.CMSCouponSpecs\nAverageType = rates_common.AverageType\nInterestRateModelType = rates_common.InterestRateModelType\nratecurve_from_discounting_function = rate_curve.ratecurve_from_discounting_function\n\n_allowed_symbols = [\n    \'Bond\',\n    \'CapAndFloor\',\n    \'CMSCashflowStream\',\n    \'CMSCouponSpecs\',\n    \'CMSSwap\',\n    \'EurodollarFutures\',\n    \'FloatingRateNote\',\n    \'ForwardRateAgreement\',\n    \'OvernightIndexLinkedFutures\',\n    \'RateCurve\',\n    \'Swaption\',\n    \'InterestRateMarket\',\n    \'InterestRateModelType\',\n    \'DayCountConvention\',\n    \'FixedCashflowStream\',\n    \'FloatingCashflowStream\',\n    \'InterestRateSwap\',\n    \'FixedCouponSpecs\',\n    \'FloatCouponSpecs\',\n    \'AverageType\',\n    \'ratecurve_from_discounting_function\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/experimental/instruments/bond.py,4,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Fixed rate bond.""""""\n\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance import datetime as dates\nfrom tf_quant_finance.experimental.instruments import cashflow_stream as cs\n\n\nclass Bond:\n  """"""Represents a batch of fixed coupon bonds.\n\n  Bonds are fixed income securities where the issuer makes periodic payments\n  (or coupons) on a pricipal amount (also known as the face value) based on a\n  fixed annualized interest rate. The payments are made periodically (for\n  example quarterly or semi-annually) where the last payment is typically made\n  at the maturity (or termination) of the contract at which time the principal\n  is also paid back.\n\n  For example, consider a fixed rate bond with settlement date T_0 and maturity\n  date T_n and equally spaced coupon payment dates T_1, T_2, ..., T_n such that\n\n  T_0 < T_1 < T_2 < ... < T_n and dt_i = T_(i+1) - T_i    (A)\n\n  The coupon accrual begins on T_0, T_1, ..., T_(n-1) and the payments are made\n  on T_1, T_2, ..., T_n (payment dates). The pricipal is also paid at T_n.\n\n  The Bond class can be used to create and price multiple bond securities\n  simultaneously. However all bonds within a Bond object must be priced using\n  a common reference and discount curve.\n\n  #### Example:\n  The following example illustrates the construction of an IRS instrument and\n  calculating its price.\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tf_quant_finance as tff\n  dates = tff.datetime\n  instruments = tff.experimental.instruments\n  rc = tff.experimental.instruments.rates_common\n\n  dtype = np.float64\n  start_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n  maturity_date = dates.convert_to_date_tensor([(2022, 2, 8)])\n  valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n  period_3m = dates.months(3)\n  period_6m = dates.months(6)\n  fix_spec = instruments.FixedCouponSpecs(\n              coupon_frequency=period_6m, currency=\'usd\',\n              notional=1., coupon_rate=0.03134,\n              daycount_convention=rc.DayCountConvention.ACTUAL_365,\n              businessday_rule=dates.BusinessDayConvention.NONE)\n\n  flt_spec = instruments.FloatCouponSpecs(\n              coupon_frequency=periods_3m, reference_rate_term=periods_3m,\n              reset_frequency=periods_3m, currency=\'usd\', notional=1.,\n              businessday_rule=dates.BusinessDayConvention.NONE,\n              coupon_basis=0., coupon_multiplier=1.,\n              daycount_convention=rc.DayCountConvention.ACTUAL_365)\n\n  swap = instruments.InterestRateSwap([(2020,2,2)], [(2023,2,2)], [fix_spec],\n                                      [flt_spec], dtype=np.float64)\n\n  curve_dates = valuation_date + dates.years(\n        [1, 2, 3, 5, 7, 10, 30])\n  reference_curve = instruments.RateCurve(\n      curve_dates,\n      np.array([\n        0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n        0.03213901, 0.03257991\n        ], dtype=dtype),\n      dtype=dtype)\n  market = instruments.InterestRateMarket(\n      reference_curve=reference_curve, discount_curve=reference_curve)\n\n  price = swap.price(valuation_date, market)\n  # Expected result: 1e-7\n  ```\n\n  #### References:\n  [1]: Leif B.G. Andersen and Vladimir V. Piterbarg. Interest Rate Modeling,\n      Volume I: Foundations and Vanilla Models. Chapter 5. 2010.\n  """"""\n\n  def __init__(self,\n               settlement_date,\n               maturity_date,\n               coupon_spec,\n               start_date=None,\n               first_coupon_date=None,\n               penultimate_coupon_date=None,\n               holiday_calendar=None,\n               dtype=None,\n               name=None):\n    """"""Initialize a batch of fixed coupon bonds.\n\n    Args:\n      settlement_date: A rank 1 `DateTensor` specifying the settlement date of\n        the bonds.\n      maturity_date: A rank 1 `DateTensor` specifying the maturity dates of the\n        bonds. The shape of the input should be the same as that of\n        `settlement_date`.\n      coupon_spec: A list of `FixedCouponSpecs` specifying the coupon payments.\n        The length of the list should be the same as the number of bonds\n        being created.\n      start_date: An optional `DateTensor` specifying the dates when the\n        interest starts to accrue for the coupons. The input can be used to\n        specify a forward start date for the coupons. The shape of the input\n        correspond to the numbercof instruments being created.\n        Default value: None in which case the coupons start to accrue from the\n        `settlement_date`.\n      first_coupon_date: An optional rank 1 `DateTensor` specifying the dates\n        when first coupon will be paid for bonds with irregular first coupon.\n      penultimate_coupon_date: An optional rank 1 `DateTensor` specifying the\n        dates when the penultimate coupon (or last regular coupon) will be paid\n        for bonds with irregular last coupon.\n      holiday_calendar: An instance of `dates.HolidayCalendar` to specify\n        weekends and holidays.\n        Default value: None in which case a holiday calendar would be created\n        with Saturday and Sunday being the holidays.\n      dtype: `tf.Dtype`. If supplied the dtype for the real variables or ops\n        either supplied to the bond object or created by the bond object.\n        Default value: None which maps to the default dtype inferred by\n        TensorFlow.\n      name: Python str. The name to give to the ops created by this class.\n        Default value: `None` which maps to \'bond\'.\n    """"""\n    self._name = name or \'bond\'\n\n    if holiday_calendar is None:\n      holiday_calendar = dates.create_holiday_calendar(\n          weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY)\n\n    with tf.name_scope(self._name):\n      self._dtype = dtype\n      self._settlement_date = dates.convert_to_date_tensor(settlement_date)\n      self._maturity_date = dates.convert_to_date_tensor(maturity_date)\n      self._holiday_calendar = holiday_calendar\n      self._setup(coupon_spec, start_date, first_coupon_date,\n                  penultimate_coupon_date)\n\n  def price(self, valuation_date, market, model=None, name=None):\n    """"""Returns the dirty price of the bonds on the valuation date.\n\n    Args:\n      valuation_date: A scalar `DateTensor` specifying the date on which\n        valuation is being desired.\n      market: A namedtuple of type `InterestRateMarket` which contains the\n        necessary information for pricing the bonds.\n      model: Reserved for future use.\n      name: Python str. The name to give to the ops created by this function.\n        Default value: `None` which maps to \'price\'.\n\n    Returns:\n      A Rank 1 `Tensor` of real dtype containing the dirty price of each bond\n      based on the input market data.\n    """"""\n\n    name = name or (self._name + \'_price\')\n    with tf.name_scope(name):\n      discount_curve = market.discount_curve\n      coupon_cf = self._cashflows.price(valuation_date, market, model)\n      principal_cf = (\n          self._notional * discount_curve.get_discount_factor(\n              self._maturity_date)\n          )\n      return coupon_cf + principal_cf\n\n  def _setup(self, coupon_spec, start_date, first_coupon_date,\n             penultimate_coupon_date):\n    """"""Setup bond cashflows.""""""\n    if start_date is None:\n      self._cashflows = cs.FixedCashflowStream(self._settlement_date,\n                                               self._maturity_date,\n                                               coupon_spec,\n                                               first_coupon_date,\n                                               penultimate_coupon_date,\n                                               dtype=self._dtype)\n    else:\n      self._cashflows = cs.FixedCashflowStream(start_date,\n                                               self._maturity_date,\n                                               coupon_spec,\n                                               first_coupon_date,\n                                               penultimate_coupon_date,\n                                               dtype=self._dtype)\n\n    self._notional = tf.convert_to_tensor([x.notional for x in coupon_spec],\n                                          dtype=self._dtype)\n'"
tf_quant_finance/experimental/instruments/bond_test.py,2,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for bond.py.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\ndates = tff.datetime\ninstruments = tff.experimental.instruments\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass BondTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_bond_correctness(self, dtype):\n    settlement_date = dates.convert_to_date_tensor([(2014, 1, 15)])\n    maturity_date = dates.convert_to_date_tensor([(2015, 1, 15)])\n    valuation_date = dates.convert_to_date_tensor([(2014, 1, 15)])\n    period_6m = dates.months(6)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=period_6m,\n        currency=\'usd\',\n        notional=100.,\n        coupon_rate=0.06,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n\n    bond_inst = instruments.Bond(settlement_date, maturity_date, [fix_spec],\n                                 dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([0, 6, 12])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.0, 0.005, 0.007], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(discount_curve=reference_curve)\n\n    price = self.evaluate(bond_inst.price(valuation_date, market))\n    np.testing.assert_allclose(price, 105.27397754, atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_bond_many(self, dtype):\n    settlement_date = dates.convert_to_date_tensor([(2014, 1, 15),\n                                                    (2014, 1, 15)])\n    maturity_date = dates.convert_to_date_tensor([(2015, 1, 15),\n                                                  (2015, 1, 15)])\n    valuation_date = dates.convert_to_date_tensor([(2014, 1, 15)])\n    period_6m = dates.months(6)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=period_6m,\n        currency=\'usd\',\n        notional=100.,\n        coupon_rate=0.06,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n\n    bond_inst = instruments.Bond(settlement_date, maturity_date,\n                                 [fix_spec, fix_spec],\n                                 dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([0, 6, 12])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.0, 0.005, 0.007], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(discount_curve=reference_curve)\n\n    price = self.evaluate(bond_inst.price(valuation_date, market))\n    np.testing.assert_allclose(price, [105.27397754, 105.27397754], atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_bond_stub_begin(self, dtype):\n    settlement_date = dates.convert_to_date_tensor([(2020, 1, 1)])\n    maturity_date = dates.convert_to_date_tensor([(2021, 2, 1)])\n    first_coupon_date = dates.convert_to_date_tensor([(2020, 2, 1)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 1, 1)])\n    period_6m = dates.months(6)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=period_6m,\n        currency=\'usd\',\n        notional=100.,\n        coupon_rate=0.06,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n\n    bond_inst = instruments.Bond(settlement_date, maturity_date,\n                                 [fix_spec],\n                                 first_coupon_date=first_coupon_date,\n                                 dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([0, 6, 12, 24])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.0, 0.025, 0.03, 0.035], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(discount_curve=reference_curve)\n\n    price = self.evaluate(bond_inst.price(valuation_date, market))\n    np.testing.assert_allclose(price, [103.12756228], atol=1e-6)\n\n    expected_coupon_dates = dates.convert_to_date_tensor([(2020, 2, 1),\n                                                          (2020, 8, 1),\n                                                          (2021, 2, 1)])\n    self.assertAllEqual(expected_coupon_dates.ordinal(),\n                        bond_inst._cashflows.payment_dates.ordinal())\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_bond_stub_end(self, dtype):\n    settlement_date = dates.convert_to_date_tensor([(2020, 1, 1)])\n    maturity_date = dates.convert_to_date_tensor([(2021, 2, 1)])\n    last_coupon_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 1, 1)])\n    period_6m = dates.months(6)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=period_6m,\n        currency=\'usd\',\n        notional=100.,\n        coupon_rate=0.06,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n\n    bond_inst = instruments.Bond(settlement_date, maturity_date,\n                                 [fix_spec],\n                                 penultimate_coupon_date=last_coupon_date,\n                                 dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([0, 6, 12, 24])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.0, 0.025, 0.03, 0.035], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(discount_curve=reference_curve)\n\n    price = self.evaluate(bond_inst.price(valuation_date, market))\n    np.testing.assert_allclose(price, [103.12769595], atol=1e-6)\n\n    expected_coupon_dates = dates.convert_to_date_tensor([(2020, 7, 1),\n                                                          (2021, 1, 1),\n                                                          (2021, 2, 1)])\n    self.assertAllEqual(expected_coupon_dates.ordinal(),\n                        bond_inst._cashflows.payment_dates.ordinal())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/experimental/instruments/cap_floor.py,29,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Cap and Floor.""""""\n\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance import black_scholes\nfrom tf_quant_finance import datetime as dates\nfrom tf_quant_finance.experimental.instruments import rates_common as rc\n\n\nclass CapAndFloor:\n  """"""Represents a batch of Caps and/or Floors.\n\n  An interest Cap (or Floor) is a portfolio of call (or put) options where the\n  underlying for the individual options are successive forward rates. The\n  individual options comprising a Cap are called Caplets and the corresponding\n  options comprising a Floor are called Floorlets. For example, a\n  caplet on forward rate `F(T_i, T_{i+1})` has the following payoff at time\n  `T_{i_1}`:\n\n  caplet payoff = tau_i * max[F(T_i, T_{i+1}), 0]\n\n  where `tau_i` is the daycount fraction.\n\n  The CapAndFloor class can be used to create and price multiple Caps/Floors\n  simultaneously. However all instruments within an object must be priced using\n  common reference/discount curve.\n\n  ### Example:\n  The following example illustrates the construction of an IRS instrument and\n  calculating its price.\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tf_quant_finance as tff\n  dates = tff.datetime\n  instruments = tff.experimental.instruments\n  rc = tff.experimental.instruments.rates_common\n\n  dtype = np.float64\n  notional = 100.0\n  maturity_date = dates.convert_to_date_tensor([(2022, 1, 15)])\n  start_date = dates.convert_to_date_tensor([(2021, 1, 15)])\n  valuation_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n\n  period3m = dates.months(3)\n  cap = instruments.CapAndFloor(\n      start_date,\n      maturity_date,\n      period3m,\n      0.005,\n      daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n      notional=notional,\n      dtype=dtype)\n  curve_dates = valuation_date + dates.months([0, 3, 12, 24])\n  reference_curve = instruments.RateCurve(\n      curve_dates,\n      np.array([0.005, 0.01, 0.015, 0.02], dtype=np.float64),\n      valuation_date=valuation_date,\n      dtype=np.float64)\n  market = instruments.InterestRateMarket(\n      reference_curve=reference_curve, discount_curve=reference_curve)\n\n  price = cap.price(\n            valuation_date,\n            market,\n            model=instruments.InterestRateModelType.LOGNORMAL_RATE,\n            pricing_context=0.5)\n  # Expected result: 1.0474063612452953\n  ```\n\n  ### References:\n  [1]: Leif B.G. Andersen and Vladimir V. Piterbarg. Interest Rate Modeling,\n      Volume I: Foundations and Vanilla Models. Chapter 5. 2010.\n  """"""\n\n  def __init__(self,\n               start_date,\n               maturity_date,\n               reset_frequency,\n               strike,\n               daycount_convention=None,\n               notional=None,\n               is_cap=None,\n               dtype=None,\n               name=None):\n    """"""Initializes a batch of Interest rate Caps (or Floors).\n\n    Args:\n      start_date: A scalar `InterestRateSwap` specifying the interest rate swaps\n        underlying the swaptions. The batch size of the swaptions being created\n        would be the same as the bacth size of the `swap`. For receiver\n        swaptions the receive_leg of the underlying swaps should be\n      maturity_date: A rank 1 `DateTensor` specifying the expiry dates\n        for each swaption. The shape of the input should be the same as the\n        batch size of the `swap` input.\n        Default value: None in which case the option expity date is the same as\n        the start date of each underlying swap.\n      reset_frequency: A rank 1 `PeriodTensor` specifying the frequency of\n        caplet resets and caplet payments.\n      strike: A scalar `Tensor` of real dtype specifying the strike rate against\n        which each caplet within the cap are exercised. The shape should be\n        compatible to the shape of `start_date`\n      daycount_convention: An optional `DayCountConvention` associated with the\n        underlying rate for the cap. Daycount is assumed to be the same for all\n        contracts in a given batch.\n        Default value: None in which case the daycount convention will default\n        to DayCountConvention.ACTUAL_360 for all contracts.\n      notional: An optional `Tensor` of real dtype specifying the notional\n        amount for the cap.\n        Default value: None in which case the notional is set to 1.\n      is_cap: An optional boolean `Tensor` of a shape compatible with\n        `start_date`. Indicates whether to compute the price of a Cap (if True)\n        or a Floor (if False).\n        Default value: None, it is assumed that every element is a Cap.\n      dtype: `tf.Dtype`. If supplied the dtype for the real variables or ops\n        either supplied to the Swaption object or created by the Swaption\n        object.\n        Default value: None which maps to the default dtype inferred by\n        TensorFlow.\n      name: Python str. The name to give to the ops created by this class.\n        Default value: `None` which maps to \'cap_and_floor\'.\n    """"""\n    self._name = name or \'cap_and_floor\'\n\n    with tf.name_scope(self._name):\n      self._dtype = dtype\n      self._start_date = dates.convert_to_date_tensor(start_date)\n      self._maturity_date = dates.convert_to_date_tensor(maturity_date)\n      if daycount_convention is None:\n        daycount_convention = rc.DayCountConvention.ACTUAL_360\n      self._daycount_convention = daycount_convention\n      self._strike = tf.convert_to_tensor(strike, dtype=self._dtype)\n      self._reset_frequency = reset_frequency\n      notional = notional or 1.0\n      self._notional = tf.convert_to_tensor(notional, dtype=self._dtype)\n      self._batch_size = self._start_date.shape.as_list()[0]\n      if is_cap is None:\n        is_cap = True\n      self._is_cap = tf.broadcast_to(\n          tf.convert_to_tensor(is_cap, dtype=tf.bool), self._start_date.shape)\n      self._setup_tensors()\n\n  def price(self, valuation_date, market, model=None, pricing_context=None,\n            name=None):\n    """"""Returns the present value of the Cap/Floor on the valuation date.\n\n    Args:\n      valuation_date: A scalar `DateTensor` specifying the date on which\n        valuation is being desired.\n      market: A namedtuple of type `InterestRateMarket` which contains the\n        necessary information for pricing the Cap/Floor.\n      model: An optional input of type `InterestRateModelType` to specify which\n        model to use for pricing.\n        Default value: `None` in which case `LOGNORMAL_RATE` model is used.\n      pricing_context: An optional input to provide additional parameters (such\n        as model parameters) relevant for pricing.\n      name: Python str. The name to give to the ops created by this function.\n        Default value: `None` which maps to `""price""`.\n\n    Returns:\n      A Rank 1 `Tensor` of real type containing the modeled price of each cap\n      (or floor) based on the input market data.\n\n    Raises:\n      ValueError: If an unsupported model is supplied to the function.\n    """"""\n\n    model = model or rc.InterestRateModelType.LOGNORMAL_RATE\n    name = name or (self._name + \'_price\')\n    with tf.name_scope(name):\n      valuation_date = dates.convert_to_date_tensor(valuation_date)\n      if model == rc.InterestRateModelType.LOGNORMAL_RATE:\n        caplet_prices = self._price_lognormal_rate(valuation_date, market,\n                                                   pricing_context)\n      else:\n        raise ValueError(f\'Unsupported model {model}.\')\n\n      return tf.math.segment_sum(caplet_prices, self._contract_index)\n\n  def _price_lognormal_rate(self, valuation_date, market, pricing_context):\n    """"""Computes caplet/floorlet prices using lognormal model for forward rates.\n\n    The function computes individual caplet prices for the batch of caps/floors\n    using the lognormal model for the forward rates. If the volatilities are\n    are supplied (through the input `pricing_context`) then they are used as\n    forward rate volatilies. Otherwise, volatilities are extracted using the\n    volatility surface for `market`.\n\n    Args:\n      valuation_date: A scalar `DateTensor` specifying the date on which\n        valuation is being desired.\n      market: A namedtuple of type `InterestRateMarket` which contains the\n        necessary information for pricing the Cap/Floor.\n      pricing_context: An optional input containing the black volatility for\n        for the forward rates.\n\n    Returns:\n      A Rank 1 `Tensor` of real type containing the price of each caplet\n      (or floorlet) based using the lognormal model for forward rates.\n    """"""\n\n    discount_curve = market.discount_curve\n\n    discount_factors = tf.where(\n        self._payment_dates > valuation_date,\n        discount_curve.get_discount_factor(self._payment_dates), 0.)\n\n    forward_rates = self._get_forward_rate(valuation_date, market)\n\n    if pricing_context is None:\n      volatility_surface = market.volatility_curve\n      black_vols = volatility_surface.interpolate(\n          self._reset_dates, self._strike, self._term)\n    else:\n      black_vols = tf.convert_to_tensor(pricing_context, dtype=self._dtype)\n\n    expiry_times = dates.daycount_actual_365_fixed(\n        start_date=valuation_date, end_date=self._reset_dates,\n        dtype=self._dtype)\n    caplet_prices = black_scholes.option_price(forwards=forward_rates,\n                                               strikes=self._strike,\n                                               volatilities=black_vols,\n                                               expiries=expiry_times,\n                                               is_call_options=self._is_cap)\n    intrinsic_value = tf.where(\n        self._is_cap, tf.math.maximum(forward_rates - self._strike, 0.0),\n        tf.math.maximum(self._strike - forward_rates, 0))\n    caplet_prices = tf.where(\n        self._payment_dates < valuation_date,\n        tf.constant(0., dtype=self._dtype),\n        tf.where(self._accrual_start_dates < valuation_date, intrinsic_value,\n                 caplet_prices))\n    caplet_prices = self._notional * self._daycount_fractions * caplet_prices\n    return discount_factors * caplet_prices\n\n  def _setup_tensors(self):\n    """"""Sets up tensors for efficient computations.""""""\n    date_schedule = dates.PeriodicSchedule(\n        start_date=self._start_date,\n        end_date=self._maturity_date,\n        tenor=self._reset_frequency).dates()\n\n    # rates reset at the begining of coupon period\n    reset_dates = date_schedule[:, :-1]\n    # payments occur at the end of the coupon period\n    payment_dates = date_schedule[:, 1:]\n    daycount_fractions = rc.get_daycount_fraction(\n        date_schedule[:, :-1],\n        date_schedule[:, 1:],\n        self._daycount_convention,\n        dtype=self._dtype)\n    contract_index = tf.repeat(\n        tf.range(0, self._batch_size),\n        payment_dates.shape.as_list()[-1])\n\n    self._num_caplets = daycount_fractions.shape.as_list()[-1]\n    # TODO(b/152164086): Use the functionality from dates library\n    self._rate_term = tf.repeat(tf.cast(reset_dates[:, 0].days_until(\n        payment_dates[:, 0]), dtype=self._dtype) / 365.0, self._num_caplets)\n    self._reset_dates = dates.DateTensor.reshape(reset_dates, [-1])\n    self._payment_dates = dates.DateTensor.reshape(payment_dates, [-1])\n    self._accrual_start_dates = dates.DateTensor.reshape(reset_dates, [-1])\n    self._accrual_end_dates = dates.DateTensor.reshape(payment_dates, [-1])\n    self._daycount_fractions = tf.reshape(daycount_fractions, [-1])\n    self._contract_index = contract_index\n    self._strike = tf.repeat(self._strike, self._num_caplets)\n    self._is_cap = tf.repeat(self._is_cap, self._num_caplets)\n\n  def _get_forward_rate(self, valuation_date, market):\n    """"""Returns the relevant forward rates from the market data.""""""\n\n    forward_rates = market.reference_curve.get_forward_rate(\n        self._accrual_start_dates,\n        self._accrual_end_dates,\n        self._daycount_fractions)\n\n    forward_rates = tf.where(self._daycount_fractions > 0.0, forward_rates,\n                             tf.zeros_like(forward_rates))\n\n    libor_rate = rc.get_rate_index(\n        market, self._start_date, rc.RateIndexType.LIBOR, dtype=self._dtype)\n    libor_rate = tf.repeat(\n        tf.convert_to_tensor(libor_rate, dtype=self._dtype), self._num_caplets)\n    forward_rates = tf.where(\n        self._accrual_end_dates < valuation_date,\n        tf.constant(0., dtype=self._dtype),\n        tf.where(self._accrual_start_dates < valuation_date, libor_rate,\n                 forward_rates))\n\n    return forward_rates\n'"
tf_quant_finance/experimental/instruments/cap_floor_test.py,2,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for cap_floor.py.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nfrom numpy import testing as np_testing\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\ndates = tff.datetime\ninstruments = tff.experimental.instruments\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass CapFloorTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(CapFloorTest, self).setUp()\n    self.maturity_date = [(2022, 1, 15)]\n    self.start_date = [(2021, 1, 15)]\n    self.valuation_date = [(2021, 1, 1)]\n\n  def get_market(self):\n    val_date = dates.convert_to_date_tensor(self.valuation_date)\n    curve_dates = val_date + dates.months([0, 3, 12, 24])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.005, 0.01, 0.015, 0.02], dtype=np.float64),\n        valuation_date=val_date,\n        dtype=np.float64)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n    return market\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_cap_correctness(self, dtype):\n    notional = 100.0\n\n    period3m = dates.months(3)\n\n    cap = instruments.CapAndFloor(\n        self.start_date,\n        self.maturity_date,\n        period3m,\n        0.005,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        notional=notional,\n        dtype=dtype)\n\n    price = self.evaluate(\n        cap.price(\n            self.valuation_date,\n            self.get_market(),\n            model=instruments.InterestRateModelType.LOGNORMAL_RATE,\n            pricing_context=0.5))\n    np_testing.assert_allclose(price, 1.0474063612452953, atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_floor_correctness(self, dtype):\n    notional = 100.0\n    period3m = dates.months(3)\n    cap = instruments.CapAndFloor(\n        self.start_date,\n        self.maturity_date,\n        period3m,\n        0.01,  # since this is a floor, we use different strike\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        notional=notional,\n        is_cap=False,\n        dtype=dtype)\n    price = self.evaluate(\n        cap.price(\n            self.valuation_date,\n            self.get_market(),\n            model=instruments.InterestRateModelType.LOGNORMAL_RATE,\n            pricing_context=0.5))\n    np_testing.assert_allclose(price, 0.01382758837128641, atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_cap_many(self, dtype):\n    notional = 100.\n    batch_maturity_date = dates.convert_to_date_tensor([(2022, 1, 15),\n                                                        (2022, 1, 15)])\n    batch_start_date = dates.convert_to_date_tensor([(2021, 1, 15),\n                                                     (2021, 1, 15)])\n    batch_valuation_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n\n    period3m = dates.months(3)\n    cap = instruments.CapAndFloor(\n        batch_start_date,\n        batch_maturity_date,\n        period3m,\n        [0.005, 0.01],\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        notional=notional,\n        dtype=dtype)\n    price = self.evaluate(\n        cap.price(\n            batch_valuation_date,\n            self.get_market(),\n            model=instruments.InterestRateModelType.LOGNORMAL_RATE,\n            pricing_context=0.5))\n    np_testing.assert_allclose(price,\n                               [1.0474063612452953, 0.5656630014452084],\n                               atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_cap_reset(self, dtype):\n    notional = 100.0\n    maturity_date = dates.convert_to_date_tensor([(2022, 1, 15),\n                                                  (2022, 1, 15)])\n    start_date = dates.convert_to_date_tensor([(2021, 1, 15),\n                                               (2021, 1, 15)])\n    valuation_date = dates.convert_to_date_tensor([(2021, 2, 1)])\n\n    period3m = dates.months(3)\n    cap = instruments.CapAndFloor(\n        start_date,\n        maturity_date,\n        period3m,\n        [0.005, 0.01],\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        notional=notional,\n        dtype=dtype)\n    curve_valuation_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    curve_dates = curve_valuation_date + dates.months([0, 3, 12, 24])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.005, 0.01, 0.015, 0.02], dtype=np.float64),\n        valuation_date=curve_valuation_date,\n        dtype=np.float64)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve,\n        discount_curve=reference_curve,\n        libor_rate=[0.006556, 0.006556])\n\n    price = self.evaluate(\n        cap.price(\n            valuation_date,\n            market,\n            model=instruments.InterestRateModelType.LOGNORMAL_RATE,\n            pricing_context=0.5))\n    np_testing.assert_allclose(price,\n                               [0.9389714183634128, 0.5354250398709062],\n                               atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_cap_fwd_rate(self, dtype):\n    notional = 100.0\n    period3m = dates.months(3)\n    cap = instruments.CapAndFloor(\n        self.start_date,\n        self.maturity_date,\n        period3m,\n        0.005,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        notional=notional,\n        dtype=dtype)\n    fwd_rates = self.evaluate(\n        cap._get_forward_rate(\n            dates.convert_to_date_tensor(self.valuation_date),\n            self.get_market()))\n    print(fwd_rates)\n    np_testing.assert_allclose(fwd_rates,\n                               [0.010966, 0.013824, 0.017164, 0.020266],\n                               atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_cap_price_lognormal_rate_model(self, dtype):\n    notional = 100.0\n    period3m = dates.months(3)\n    cap = instruments.CapAndFloor(\n        self.start_date,\n        self.maturity_date,\n        period3m,\n        0.005,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        notional=notional,\n        dtype=dtype)\n    price = self.evaluate(\n        cap._price_lognormal_rate(\n            dates.convert_to_date_tensor(self.valuation_date),\n            self.get_market(),\n            pricing_context=0.5))\n    print(price)\n    np_testing.assert_allclose(\n        price, [0.146671, 0.218595, 0.303358, 0.378782], atol=1e-6)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/experimental/instruments/cashflow_stream.py,53,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Cashflow streams.""""""\n\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance import datetime as dates\nfrom tf_quant_finance.experimental.instruments import rates_common as rc\n\n\nclass CashflowStream:\n  """"""Base class for Fixed or Floating cashflow streams.""""""\n\n  def _generate_schedule(self, cpn_frequency, roll_convention):\n    """"""Method to generate coupon dates.\n\n    Args:\n      cpn_frequency: A `PeriodTensor` specifying the frequency of coupon\n        payments.\n      roll_convention: Scalar of type `BusinessDayConvention` specifying how\n        dates are rolled if they fall on holidays.\n\n    Returns:\n      A tuple containing the generated date schedule and a boolean `Tensor`\n      of the same shape as the schedule specifying whether the coupons are\n      regular coupons.\n    """"""\n    if (self._first_coupon_date is None) and (self._penultimate_coupon_date is\n                                              None):\n      cpn_dates = dates.PeriodicSchedule(\n          start_date=self._start_date,\n          end_date=self._end_date,\n          tenor=cpn_frequency,\n          roll_convention=roll_convention).dates()\n      is_regular_cpn = tf.constant(\n          True, dtype=bool, shape=cpn_dates[:, :-1].shape)\n    elif self._first_coupon_date is not None:\n      cpn_dates = dates.PeriodicSchedule(\n          start_date=self._first_coupon_date,\n          end_date=self._end_date,\n          tenor=cpn_frequency,\n          roll_convention=roll_convention).dates()\n      cpn_dates = dates.DateTensor.concat(\n          [self._start_date.expand_dims(-1), cpn_dates], axis=1)\n\n      is_irregular_cpn = tf.constant(\n          False, dtype=bool, shape=self._start_date.shape)\n      is_regular_cpn = tf.concat([\n          tf.expand_dims(is_irregular_cpn, axis=-1),\n          tf.constant(True, dtype=bool, shape=cpn_dates[:, :-2].shape)\n      ],\n                                 axis=1)\n    else:\n      cpn_dates = dates.PeriodicSchedule(\n          start_date=self._start_date,\n          end_date=self._penultimate_coupon_date,\n          backward=True,\n          tenor=cpn_frequency,\n          roll_convention=roll_convention).dates()\n      cpn_dates = dates.DateTensor.concat(\n          [cpn_dates, self._end_date.expand_dims(-1)], axis=1)\n\n      is_irregular_cpn = tf.constant(\n          False, dtype=bool, shape=self._end_date.shape)\n      is_regular_cpn = tf.concat([\n          tf.constant(True, dtype=bool, shape=cpn_dates[:, :-2].shape),\n          tf.expand_dims(is_irregular_cpn, axis=-1)\n      ],\n                                 axis=1)\n\n    return cpn_dates, is_regular_cpn\n\n\nclass FixedCashflowStream(CashflowStream):\n  """"""Represents a batch of fixed stream of cashflows.\n\n  #### Example:\n  The following example illustrates the construction of an FixedCashflowStream\n  and calculating its present value.\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tf_quant_finance as tff\n  dates = tff.datetime\n  instruments = tff.experimental.instruments\n  rc = tff.experimental.instruments.rates_common\n\n  dtype = np.float64\n  start_date = dates.convert_to_date_tensor([(2020, 2, 2)])\n  maturity_date = dates.convert_to_date_tensor([(2023, 2, 2)])\n  valuation_date = dates.convert_to_date_tensor([(2020, 2, 2)])\n  period_6m = dates.months(6)\n  fix_spec = instruments.FixedCouponSpecs(\n              coupon_frequency=period_6m, currency=\'usd\',\n              notional=1.e6, coupon_rate=0.03134,\n              daycount_convention=rc.DayCountConvention.ACTUAL_365,\n              businessday_rule=dates.BusinessDayConvention.NONE)\n\n  cf_stream = instruments.FixedCashflowStream([start_date], [maturity_date],\n                                              [fix_spec], dtype=dtype)\n\n  curve_dates = valuation_date + dates.years([1, 2, 3, 5, 7, 10, 30])\n  reference_curve = instruments.RateCurve(\n      curve_dates,\n      np.array([\n        0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n        0.03213901, 0.03257991\n        ], dtype=dtype),\n      dtype=dtype)\n  market = instruments.InterestRateMarket(\n      reference_curve=reference_curve, discount_curve=reference_curve)\n\n  price = cf_stream.price(valuation_date, market)\n  # Expected result: 89259.267853547\n  ```\n  """"""\n\n  def __init__(self,\n               start_date,\n               end_date,\n               coupon_spec,\n               first_coupon_date=None,\n               penultimate_coupon_date=None,\n               dtype=None,\n               name=None):\n    """"""Initialize a batch of fixed cashflow streams.\n\n    Args:\n      start_date: A rank 1 `DateTensor` specifying the starting dates of the\n        accrual of the first coupon of the cashflow stream. The shape of the\n        input correspond to the numbercof streams being created.\n      end_date: A rank 1 `DateTensor` specifying the end dates for accrual of\n        the last coupon in each cashflow stream. The shape of the input should\n        be the same as that of `start_date`.\n      coupon_spec: A scalar or a list of `FixedCouponSpecs` specifying the\n        details of the coupon payment for the cashflow stream. If specified as\n        a list then the length of the list should be the same as the number of\n        streams being created and each coupon within the list must have the\n        same daycount_convention and businessday_rule. If specified as\n        a scalar, then the elements of the namedtuple must be of the same shape\n        as (or compatible to) the shape of `start_date`.\n      first_coupon_date: An optional rank 1 `DateTensor` specifying the payment\n        dates of the first coupon of the cashflow stream. Use this input for\n        cashflows with irregular first coupon.\n        Default value: None which implies regular first coupon.\n      penultimate_coupon_date: An optional rank 1 `DateTensor` specifying the\n        payment date of the penultimate (next to last) coupon of the cashflow\n        stream. Use this input for cashflows with irregular last coupon.\n        Default value: None which implies regular last coupon.\n      dtype: `tf.Dtype`. If supplied the dtype for the real variables or ops\n        either supplied to the FixedCashflowStream object or created by the\n        object.\n        Default value: None which maps to the default dtype inferred by\n        TensorFlow.\n      name: Python str. The name to give to the ops created by this class.\n        Default value: `None` which maps to \'fixed_cashflow_stream\'.\n    """"""\n\n    super(FixedCashflowStream, self).__init__()\n    self._name = name or \'fixed_cashflow_stream\'\n\n    with tf.name_scope(self._name):\n      self._start_date = dates.convert_to_date_tensor(start_date)\n      self._end_date = dates.convert_to_date_tensor(end_date)\n      self._batch_size = self._start_date.shape[0]\n      self._dtype = dtype\n      if first_coupon_date is None:\n        self._first_coupon_date = None\n      else:\n        self._first_coupon_date = dates.convert_to_date_tensor(\n            first_coupon_date)\n\n      if penultimate_coupon_date is None:\n        self._penultimate_coupon_date = None\n      else:\n        self._penultimate_coupon_date = dates.convert_to_date_tensor(\n            penultimate_coupon_date)\n\n      self._setup(coupon_spec)\n\n  def price(self, valuation_date, market, model=None, pricing_context=None,\n            name=None):\n    """"""Returns the present value of the stream on the valuation date.\n\n    Args:\n      valuation_date: A scalar `DateTensor` specifying the date on which\n        valuation is being desired.\n      market: A namedtuple of type `InterestRateMarket` which contains the\n        necessary information for pricing the cashflow stream.\n      model: Reserved for future use.\n      pricing_context: Additional context relevant for pricing.\n      name: Python str. The name to give to the ops created by this function.\n        Default value: `None` which maps to \'price\'.\n\n    Returns:\n      A Rank 1 `Tensor` of real type containing the modeled price of each stream\n      based on the input market data.\n    """"""\n\n    del model, pricing_context\n    name = name or (self._name + \'_price\')\n    with tf.name_scope(name):\n      discount_curve = market.discount_curve\n      discount_factors = discount_curve.get_discount_factor(\n          self._payment_dates)\n      future_cashflows = tf.cast(self._payment_dates >= valuation_date,\n                                 dtype=self._dtype)\n      cashflow_pvs = self._notional * (\n          future_cashflows * self._daycount_fractions * self._coupon_rate *\n          discount_factors)\n      return tf.math.reduce_sum(\n          tf.reshape(cashflow_pvs, (self._batch_size, self._num_cashflows)),\n          axis=1)\n\n  @property\n  def payment_dates(self):\n    return self._payment_dates\n\n  @property\n  def contract_index(self):\n    return self._contract_index\n\n  @property\n  def daycount_fractions(self):\n    return self._daycount_fractions\n\n  @property\n  def fixed_rate(self):\n    return self._fixed_rate\n\n  @property\n  def notional(self):\n    return self._notional\n\n  def _setup(self, coupon_spec):\n    """"""Setup tensors for efficient computations.""""""\n\n    if isinstance(coupon_spec, list):\n      cpn_frequency = dates.PeriodTensor.stack(\n          [x.coupon_frequency for x in coupon_spec], axis=0)\n      businessday_rule = coupon_spec[-1].businessday_rule\n      notional = tf.convert_to_tensor([x.notional for x in coupon_spec],\n                                      dtype=self._dtype)\n      fixed_rate = tf.convert_to_tensor([x.coupon_rate for x in coupon_spec],\n                                        dtype=self._dtype)\n      daycount_convention = coupon_spec[-1].daycount_convention\n    else:\n      cpn_frequency = coupon_spec.coupon_frequency\n      businessday_rule = coupon_spec.businessday_rule\n      notional = tf.broadcast_to(\n          tf.convert_to_tensor(coupon_spec.notional, dtype=self._dtype),\n          self._start_date.shape)\n      fixed_rate = tf.broadcast_to(\n          tf.convert_to_tensor(coupon_spec.coupon_rate, dtype=self._dtype),\n          self._start_date.shape)\n      daycount_convention = coupon_spec.daycount_convention\n\n    cpn_dates, _ = self._generate_schedule(cpn_frequency, businessday_rule)\n    payment_dates = cpn_dates[:, 1:]\n\n    notional = tf.repeat(notional, payment_dates.shape.as_list()[-1])\n    daycount_fractions = rc.get_daycount_fraction(\n        cpn_dates[:, :-1],\n        cpn_dates[:, 1:],\n        daycount_convention,\n        dtype=self._dtype)\n\n    coupon_rate = tf.expand_dims(fixed_rate, axis=-1)\n    coupon_rate = tf.repeat(coupon_rate, payment_dates.shape.as_list()[-1])\n    contract_index = tf.repeat(tf.range(0, self._batch_size),\n                               payment_dates.shape.as_list()[-1])\n\n    self._num_cashflows = payment_dates.shape.as_list()[-1]\n    self._payment_dates = payment_dates.reshape([-1])\n    self._notional = notional\n    self._daycount_fractions = tf.reshape(daycount_fractions, [-1])\n    self._coupon_rate = coupon_rate\n    self._fixed_rate = tf.convert_to_tensor(fixed_rate, dtype=self._dtype)\n    self._contract_index = contract_index\n\n\nclass FloatingCashflowStream(CashflowStream):\n  """"""Represents a batch of cashflows indexed to a floating rate.\n\n  #### Example:\n  The following example illustrates the construction of an floating\n  cashflow stream and calculating its present value.\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tf_quant_finance as tff\n  dates = tff.datetime\n  instruments = tff.experimental.instruments\n  rc = tff.experimental.instruments.rates_common\n\n  dtype = np.float64\n  start_date = dates.convert_to_date_tensor([(2020, 2, 2)])\n  maturity_date = dates.convert_to_date_tensor([(2023, 2, 2)])\n  valuation_date = dates.convert_to_date_tensor([(2020, 2, 2)])\n  period_3m = dates.months(3)\n  flt_spec = instruments.FloatCouponSpecs(\n              coupon_frequency=periods_3m, reference_rate_term=periods_3m,\n              reset_frequency=periods_3m, currency=\'usd\', notional=1.,\n              businessday_rule=dates.BusinessDayConvention.NONE,\n              coupon_basis=0., coupon_multiplier=1.,\n              daycount_convention=rc.DayCountConvention.ACTUAL_365)\n\n  cf_stream = instruments.FloatingCashflowStream([start_date], [maturity_date],\n                                                 [flt_spec], dtype=dtype)\n\n  curve_dates = valuation_date + dates.years([1, 2, 3, 5, 7, 10, 30])\n  reference_curve = instruments.RateCurve(\n      curve_dates,\n      np.array([\n        0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n        0.03213901, 0.03257991\n        ], dtype=dtype),\n      dtype=dtype)\n  market = instruments.InterestRateMarket(\n      reference_curve=reference_curve, discount_curve=reference_curve)\n\n  price = cf_stream.price(valuation_date, market)\n  # Expected result: 89259.685614769\n  ```\n  """"""\n\n  def __init__(self,\n               start_date,\n               end_date,\n               coupon_spec,\n               first_coupon_date=None,\n               penultimate_coupon_date=None,\n               dtype=None,\n               name=None):\n    """"""Initialize a batch of floating cashflow streams.\n\n    Args:\n      start_date: A rank 1 `DateTensor` specifying the starting dates of the\n        accrual of the first coupon of the cashflow stream. The shape of the\n        input correspond to the numbercof streams being created.\n      end_date: A rank 1 `DateTensor` specifying the end dates for accrual of\n        the last coupon in each cashflow stream. The shape of the input should\n        be the same as that of `start_date`.\n      coupon_spec: A scalar or a list of `FloatCouponSpecs` specifying the\n        details of the coupon payment for the cashflow stream. If specified as\n        a list then the length of the list should be the same as the number of\n        streams being created and each coupon within the list must have the\n        same daycount_convention and businessday_rule. If specified as\n        a scalar, then the elements of the namedtuple must be of the same shape\n        as (or compatible to) the shape of `start_date`.\n      first_coupon_date: An optional rank 1 `DateTensor` specifying the payment\n        dates of the first coupon of the cashflow stream. Use this input for\n        cashflows with irregular first coupon.\n        Default value: None which implies regular first coupon.\n      penultimate_coupon_date: An optional rank 1 `DateTensor` specifying the\n        payment date of the penultimate (next to last) coupon of the cashflow\n        stream. Use this input for cashflows with irregular last coupon.\n        Default value: None which implies regular last coupon.\n      dtype: `tf.Dtype`. If supplied the dtype for the real variables or ops\n        either supplied to the FloatingCashflowStream object or created by the\n        object.\n        Default value: None which maps to the default dtype inferred by\n        TensorFlow.\n      name: Python str. The name to give to the ops created by this class.\n        Default value: `None` which maps to \'floating_cashflow_stream\'.\n    """"""\n\n    super(FloatingCashflowStream, self).__init__()\n    self._name = name or \'floating_cashflow_stream\'\n\n    with tf.name_scope(self._name):\n      self._start_date = dates.convert_to_date_tensor(start_date)\n      self._end_date = dates.convert_to_date_tensor(end_date)\n      self._batch_size = self._start_date.shape[0]\n      if first_coupon_date is None:\n        self._first_coupon_date = None\n      else:\n        self._first_coupon_date = dates.convert_to_date_tensor(\n            first_coupon_date)\n\n      if penultimate_coupon_date is None:\n        self._penultimate_coupon_date = None\n      else:\n        self._penultimate_coupon_date = dates.convert_to_date_tensor(\n            penultimate_coupon_date)\n\n      self._dtype = dtype\n\n      self._setup(coupon_spec)\n\n  def price(self, valuation_date, market, model=None, pricing_context=None,\n            name=None):\n    """"""Returns the present value of the stream on the valuation date.\n\n    Args:\n      valuation_date: A scalar `DateTensor` specifying the date on which\n        valuation is being desired.\n      market: A namedtuple of type `InterestRateMarket` which contains the\n        necessary information for pricing the cashflow stream.\n      model: Reserved for future use.\n      pricing_context: Additional context relevant for pricing.\n      name: Python str. The name to give to the ops created by this function.\n        Default value: `None` which maps to \'price\'.\n\n    Returns:\n      A Rank 1 `Tensor` of real type containing the modeled price of each stream\n      contract based on the input market data.\n    """"""\n\n    del model, pricing_context\n    name = name or (self._name + \'_price\')\n    with tf.name_scope(name):\n      discount_curve = market.discount_curve\n      reference_curve = market.reference_curve\n      libor_rate = rc.get_rate_index(market, self._start_date,\n                                     rc.RateIndexType.LIBOR,\n                                     dtype=self._dtype)\n      libor_rate = tf.repeat(tf.convert_to_tensor(\n          libor_rate, dtype=self._dtype), self._num_cashflows)\n\n      discount_factors = discount_curve.get_discount_factor(self._payment_dates)\n      forward_rates = reference_curve.get_forward_rate(self._accrual_start_date,\n                                                       self._accrual_end_date,\n                                                       self._daycount_fractions)\n\n      forward_rates = tf.where(self._daycount_fractions > 0., forward_rates,\n                               tf.zeros_like(forward_rates))\n      # If coupon end date is before the valuation date, the payment is in the\n      # past. If valuation date is between coupon start date and coupon end\n      # date, then the rate has been fixed but not paid. Otherwise the rate is\n      # not fixed and should be read from the curve.\n      forward_rates = tf.where(\n          self._coupon_end_dates < valuation_date,\n          tf.constant(0., dtype=self._dtype),\n          tf.where(self._coupon_start_dates < valuation_date,\n                   libor_rate, forward_rates))\n\n      coupon_rate = self._coupon_multiplier * (\n          forward_rates + self._coupon_basis)\n\n      cashflow_pvs = self._notional * (\n          self._daycount_fractions * coupon_rate * discount_factors)\n      return tf.math.reduce_sum(\n          tf.reshape(cashflow_pvs, (self._batch_size, self._num_cashflows)),\n          axis=1)\n\n  @property\n  def notional(self):\n    return self._notional\n\n  def _setup(self, coupon_spec):\n    """"""Setup tensors for efficient computations.""""""\n\n    if isinstance(coupon_spec, list):\n      cpn_frequency = dates.PeriodTensor.stack(\n          [x.coupon_frequency for x in coupon_spec], axis=0)\n      businessday_rule = coupon_spec[-1].businessday_rule\n      ref_term = dates.PeriodTensor.stack(\n          [x.reference_rate_term for x in coupon_spec], axis=0)\n      daycount_convention = coupon_spec[-1].daycount_convention\n      notional = tf.convert_to_tensor([x.notional for x in coupon_spec],\n                                      dtype=self._dtype)\n      coupon_basis = tf.convert_to_tensor(\n          [x.coupon_basis for x in coupon_spec], dtype=self._dtype)\n      coupon_multiplier = tf.convert_to_tensor(\n          [x.coupon_multiplier for x in coupon_spec], dtype=self._dtype)\n    else:\n      cpn_frequency = coupon_spec.coupon_frequency\n      businessday_rule = coupon_spec.businessday_rule\n      ref_term = coupon_spec.reference_rate_term\n      daycount_convention = coupon_spec.daycount_convention\n      notional = tf.broadcast_to(\n          tf.convert_to_tensor(coupon_spec.notional, dtype=self._dtype),\n          self._start_date.shape)\n      coupon_basis = tf.broadcast_to(\n          tf.convert_to_tensor(coupon_spec.coupon_basis, dtype=self._dtype),\n          self._start_date.shape)\n      coupon_multiplier = tf.broadcast_to(\n          tf.convert_to_tensor(coupon_spec.coupon_multiplier,\n                               dtype=self._dtype), self._start_date.shape)\n\n    cpn_dates, is_regular_cpn = self._generate_schedule(cpn_frequency,\n                                                        businessday_rule)\n    accrual_start_dates = cpn_dates[:, :-1]\n\n    accrual_end_dates = cpn_dates[:, :-1] + ref_term.expand_dims(\n        axis=-1).broadcast_to(accrual_start_dates.shape)\n    coupon_start_dates = cpn_dates[:, :-1]\n    coupon_end_dates = cpn_dates[:, 1:]\n    accrual_end_dates = dates.DateTensor.where(is_regular_cpn,\n                                               accrual_end_dates,\n                                               coupon_end_dates)\n    payment_dates = cpn_dates[:, 1:]\n\n    daycount_fractions = rc.get_daycount_fraction(\n        cpn_dates[:, :-1],\n        cpn_dates[:, 1:],\n        daycount_convention,\n        dtype=self._dtype)\n\n    notional = tf.repeat(notional, payment_dates.shape.as_list()[-1])\n    coupon_basis = tf.repeat(coupon_basis, payment_dates.shape.as_list()[-1])\n    coupon_multiplier = tf.repeat(coupon_multiplier,\n                                  payment_dates.shape.as_list()[-1])\n\n    contract_index = tf.repeat(tf.range(0, self._batch_size),\n                               payment_dates.shape.as_list()[-1])\n\n    self._num_cashflows = daycount_fractions.shape.as_list()[-1]\n    self._coupon_start_dates = coupon_start_dates.reshape([-1])\n    self._coupon_end_dates = coupon_end_dates.reshape([-1])\n    self._payment_dates = payment_dates.reshape([-1])\n    self._accrual_start_date = accrual_start_dates.reshape([-1])\n    self._accrual_end_date = accrual_end_dates.reshape([-1])\n    self._notional = notional\n    self._daycount_fractions = tf.reshape(daycount_fractions, [-1])\n    self._coupon_basis = coupon_basis\n    self._coupon_multiplier = coupon_multiplier\n    self._contract_index = contract_index\n    self._is_regular_coupon = tf.reshape(is_regular_cpn, [-1])\n'"
tf_quant_finance/experimental/instruments/cashflow_stream_test.py,2,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for cashflow_stream.py.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\ndates = tff.datetime\ninstruments = tff.experimental.instruments\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass CashflowStreamTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_fixed_stream(self, dtype):\n    start_date = dates.convert_to_date_tensor([(2020, 2, 2)])\n    maturity_date = dates.convert_to_date_tensor([(2023, 2, 2)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 2)])\n    period_6m = dates.months(6)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=period_6m,\n        currency=\'usd\',\n        notional=1.,\n        coupon_rate=0.03134,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n\n    cf_stream = instruments.FixedCashflowStream(start_date, maturity_date,\n                                                [fix_spec],\n                                                dtype=dtype)\n\n    curve_dates = valuation_date + dates.years([1, 2, 3, 5, 7, 10, 30])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n            0.03213901, 0.03257991\n        ], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n\n    price = self.evaluate(cf_stream.price(valuation_date, market))\n    np.testing.assert_allclose(price, 0.089259267853547, atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_fixed_stream_many(self, dtype):\n    start_date = dates.convert_to_date_tensor([(2020, 2, 2), (2020, 2, 2)])\n    maturity_date = dates.convert_to_date_tensor([(2023, 2, 2), (2023, 2, 2)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 2)])\n    period_6m = dates.months(6)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=period_6m,\n        currency=\'usd\',\n        notional=1.,\n        coupon_rate=0.03134,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n\n    cf_stream = instruments.FixedCashflowStream(start_date, maturity_date,\n                                                [fix_spec, fix_spec],\n                                                dtype=dtype)\n\n    curve_dates = valuation_date + dates.years([1, 2, 3, 5, 7, 10, 30])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n            0.03213901, 0.03257991\n        ], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n\n    price = self.evaluate(cf_stream.price(valuation_date, market))\n    np.testing.assert_allclose(price, [0.089259267853547, 0.089259267853547],\n                               atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_floating_stream(self, dtype):\n    start_date = dates.convert_to_date_tensor([(2020, 2, 2)])\n    maturity_date = dates.convert_to_date_tensor([(2023, 2, 2)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 2)])\n    period_3m = dates.months(3)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period_3m,\n        reference_rate_term=period_3m,\n        reset_frequency=period_3m,\n        currency=\'usd\',\n        notional=1.,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0.,\n        coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    cf_stream = instruments.FloatingCashflowStream(start_date,\n                                                   maturity_date, [flt_spec],\n                                                   dtype=dtype)\n\n    curve_dates = valuation_date + dates.years([1, 2, 3, 5, 7, 10, 30])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n            0.03213901, 0.03257991\n        ], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n\n    price = self.evaluate(cf_stream.price(valuation_date, market))\n    np.testing.assert_allclose(price, 0.089259685614769, atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_floating_stream_many(self, dtype):\n    start_date = dates.convert_to_date_tensor([(2020, 2, 2), (2020, 2, 2)])\n    maturity_date = dates.convert_to_date_tensor([(2023, 2, 2), (2023, 2, 2)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 2)])\n    period_3m = dates.months(3)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period_3m,\n        reference_rate_term=period_3m,\n        reset_frequency=period_3m,\n        currency=\'usd\',\n        notional=1.,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0.,\n        coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    cf_stream = instruments.FloatingCashflowStream(start_date,\n                                                   maturity_date,\n                                                   [flt_spec, flt_spec],\n                                                   dtype=dtype)\n\n    curve_dates = valuation_date + dates.years([1, 2, 3, 5, 7, 10, 30])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n            0.03213901, 0.03257991\n        ], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n\n    price = self.evaluate(cf_stream.price(valuation_date, market))\n    np.testing.assert_allclose(price, [0.089259685614769, 0.089259685614769],\n                               atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_floating_stream_past_fixing(self, dtype):\n    start_date = dates.convert_to_date_tensor([(2020, 2, 2), (2020, 2, 1)])\n    maturity_date = dates.convert_to_date_tensor([(2023, 2, 2), (2023, 2, 2)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 7, 3)])\n    period_3m = dates.months(3)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period_3m,\n        reference_rate_term=period_3m,\n        reset_frequency=period_3m,\n        currency=\'usd\',\n        notional=1.,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0.,\n        coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    cf_stream = instruments.FloatingCashflowStream(start_date,\n                                                   maturity_date,\n                                                   [flt_spec, flt_spec],\n                                                   dtype=dtype)\n\n    curve_dates = valuation_date + dates.years([1, 2, 3, 5, 7, 10, 30])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n            0.03213901, 0.03257991\n        ], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve,\n        libor_rate=[0.01, 0.02])\n\n    price = self.evaluate(cf_stream.price(valuation_date, market))\n    np.testing.assert_allclose(price, [0.07720258, 0.08694714],\n                               atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_fixed_stream_past_fixing(self, dtype):\n    start_date = dates.convert_to_date_tensor([(2020, 2, 2), (2020, 2, 2)])\n    maturity_date = dates.convert_to_date_tensor([(2023, 2, 2), (2024, 2, 2)])\n    valuation_date = dates.convert_to_date_tensor([(2021, 3, 2)])\n    period_6m = dates.months(6)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=period_6m,\n        currency=\'usd\',\n        notional=1.,\n        coupon_rate=0.03134,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n\n    cf_stream = instruments.FixedCashflowStream(start_date, maturity_date,\n                                                [fix_spec, fix_spec],\n                                                dtype=dtype)\n\n    curve_dates = valuation_date + dates.years([1, 2, 3, 5, 7, 10, 30])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n            0.03213901, 0.03257991\n        ], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n\n    price = self.evaluate(cf_stream.price(valuation_date, market))\n    np.testing.assert_allclose(price, [0.06055127, 0.08939763],\n                               atol=1e-6)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/experimental/instruments/cms_swap.py,58,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Constant maturity swaps.""""""\n\nimport itertools\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\nfrom tf_quant_finance import black_scholes\nfrom tf_quant_finance import datetime as dates\nfrom tf_quant_finance.experimental.instruments import cashflow_stream as cs\nfrom tf_quant_finance.experimental.instruments import interest_rate_swap as irs\nfrom tf_quant_finance.experimental.instruments import rates_common as rc\nfrom tf_quant_finance.math import integration\n\n\nclass CMSCashflowStream(cs.CashflowStream):\n  """"""Represents a batch of cashflows indexed to a CMS rate.\n\n  #### Example:\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tf_quant_finance as tff\n  dates = tff.datetime\n  instruments = tff.experimental.instruments\n\n  start_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n  maturity_date = dates.convert_to_date_tensor([(2023, 1, 1)])\n  valuation_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n  p3m = dates.months(3)\n  p6m = dates.months(6)\n  p1y = dates.year()\n  fix_spec = instruments.FixedCouponSpecs(\n      coupon_frequency=p6m,\n      currency=\'usd\',\n      notional=1.,\n      coupon_rate=0.0,  # Not needed\n      daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n      businessday_rule=dates.BusinessDayConvention.NONE)\n  flt_spec = instruments.FloatCouponSpecs(\n      coupon_frequency=p3m,\n      reference_rate_term=p3m,\n      reset_frequency=p3m,\n      currency=\'usd\',\n      notional=1.,\n      businessday_rule=dates.BusinessDayConvention.NONE,\n      coupon_basis=0.,\n      coupon_multiplier=1.,\n      daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n  cms_spec = instruments.CMSCouponSpecs(\n      coupon_frequency=p3m,\n      tenor=p1y,\n      float_leg=flt_spec,\n      fixed_leg=fix_spec,\n      notional=1.,\n      coupon_basis=0.,\n      coupon_multiplier=1.,\n      businessday_rule=None,\n      daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n  cms = instruments.CMSCashflowStream(\n      start_date, maturity_date, [cms_spec], dtype=dtype)\n\n  curve_dates = valuation_date + dates.years([0, 1, 2, 3, 5])\n  reference_curve = instruments.RateCurve(\n      curve_dates,\n      np.array([\n          0.02, 0.02, 0.025, 0.03, 0.035\n      ], dtype=np.float64),\n      valuation_date=valuation_date,\n      dtype=np.float64)\n  market = instruments.InterestRateMarket(\n      reference_curve=reference_curve, discount_curve=reference_curve)\n\n  price = cms.price(valuation_date, market)\n  # Expected result: 55512.6295434207\n  ```\n  """"""\n\n  def __init__(self,\n               start_date,\n               end_date,\n               coupon_spec,\n               dtype=None,\n               name=None):\n    """"""Initialize a batch of CMS cashflow streams.\n\n    Args:\n      start_date: A rank 1 `DateTensor` specifying the starting dates of the\n        accrual of the first coupon of the cashflow stream. The shape of the\n        input correspond to the numbercof streams being created.\n      end_date: A rank 1 `DateTensor` specifying the end dates for accrual of\n        the last coupon in each cashflow stream. The shape of the input should\n        be the same as that of `start_date`.\n      coupon_spec: A list of `CMSCouponSpecs` specifying the details of the\n        coupon payment for the cashflow stream. The length of the list should\n        be the same as the number of streams being created. Each coupon within\n        the list must have the same daycount_convention and businessday_rule.\n      dtype: `tf.Dtype`. If supplied the dtype for the real variables or ops\n        either supplied to the FloatingCashflowStream object or created by the\n        object.\n        Default value: None which maps to the default dtype inferred by\n        TensorFlow.\n      name: Python str. The name to give to the ops created by this class.\n        Default value: `None` which maps to \'floating_cashflow_stream\'.\n    """"""\n\n    super(CMSCashflowStream, self).__init__()\n    self._name = name or \'cms_cashflow_stream\'\n\n    with tf.name_scope(self._name):\n      self._start_date = dates.convert_to_date_tensor(start_date)\n      self._end_date = dates.convert_to_date_tensor(end_date)\n      self._batch_size = self._start_date.shape[0]\n      self._first_coupon_date = None\n      self._penultimate_coupon_date = None\n      self._dtype = dtype\n\n      self._setup(coupon_spec)\n\n  def price(self, valuation_date, market, model=None, pricing_context=None,\n            name=None):\n    """"""Returns the present value of the stream on the valuation date.\n\n    Args:\n      valuation_date: A scalar `DateTensor` specifying the date on which\n        valuation is being desired.\n      market: A namedtuple of type `InterestRateMarket` which contains the\n        necessary information for pricing the cashflow stream.\n      model: An optional input of type `InterestRateModelType` to specify which\n        model to use for pricing.\n        Default value: `None` in which case `NORMAL_RATE` model is used.\n      pricing_context: An optional input to provide additional parameters (such\n        as model parameters) relevant for pricing.\n      name: Python str. The name to give to the ops created by this function.\n        Default value: `None` which maps to \'price\'.\n\n    Returns:\n      A Rank 1 `Tensor` of real type containing the modeled price of each stream\n      contract based on the input market data.\n    """"""\n\n    name = name or (self._name + \'_price\')\n    with tf.name_scope(name):\n      valuation_date = dates.convert_to_date_tensor(valuation_date)\n      discount_curve = market.discount_curve\n      past_fixing = rc.get_rate_index(\n          market, self._start_date, rc.RateIndexType.SWAP, dtype=self._dtype)\n      past_fixing = tf.repeat(\n          tf.convert_to_tensor(past_fixing, dtype=self._dtype),\n          self._num_cashflows)\n\n      discount_factors = discount_curve.get_discount_factor(self._payment_dates)\n      cms_rates = self._swap.par_rate(valuation_date, market, model)\n\n      cms_rates = tf.where(self._daycount_fractions > 0., cms_rates,\n                           tf.zeros_like(cms_rates))\n      # If coupon end date is before the valuation date, the payment is in the\n      # past. If valuation date is between coupon start date and coupon end\n      # date, then the rate has been fixed but not paid. Otherwise the rate is\n      # not fixed and should be read from the curve.\n      cms_rates = tf.where(\n          self._coupon_end_dates < valuation_date,\n          tf.constant(0., dtype=self._dtype),\n          tf.where(self._coupon_start_dates < valuation_date,\n                   past_fixing, cms_rates))\n      cms_rates = self._adjust_convexity(\n          valuation_date, market, model, pricing_context, cms_rates,\n          discount_factors)\n\n      coupon_rate = self._coupon_multiplier * (\n          cms_rates + self._coupon_basis)\n\n      cashflow_pvs = self._notional * (\n          self._daycount_fractions * coupon_rate * discount_factors)\n      return tf.math.segment_sum(cashflow_pvs, self._contract_index)\n\n  def _adjust_convexity(self, valuation_date, market, model, pricing_context,\n                        cms_rates, discount_factors):\n    """"""Computes the convexity adjusted cms rate.""""""\n    if model is None:\n      return cms_rates\n    elif model in (\n        rc.InterestRateModelType.LOGNORMAL_SMILE_CONSISTENT_REPLICATION,\n        rc.InterestRateModelType.NORMAL_SMILE_CONSISTENT_REPLICATION):\n      return self._convexity_smile_replication(\n          valuation_date, market, model, cms_rates, pricing_context)\n    else:\n      level = self._swap.annuity(valuation_date, market, None)\n      expiry_time = dates.daycount_actual_365_fixed(\n          start_date=valuation_date,\n          end_date=self._coupon_start_dates,\n          dtype=self._dtype)\n      with tf.GradientTape() as g:\n        g.watch(cms_rates)\n        fx = self._fs(cms_rates)\n      dfx = tf.squeeze(g.gradient(fx, cms_rates))\n      swap_vol = tf.convert_to_tensor(pricing_context, dtype=self._dtype)\n      if model == rc.InterestRateModelType.LOGNORMAL_RATE:\n        cms_rates = cms_rates + dfx * level * (cms_rates**2) * (\n            tf.math.exp(swap_vol**2 * expiry_time) - 1.0) / discount_factors\n      else:\n        cms_rates = cms_rates + dfx * level * (\n            swap_vol**2 * expiry_time) / discount_factors\n      return cms_rates\n\n  def _convexity_smile_replication(self, valuation_date, market, model,\n                                   cms_rates, pricing_context):\n    """"""Calculate CMS convexity correction by static replication.""""""\n    normal_model = (\n        model == rc.InterestRateModelType.NORMAL_SMILE_CONSISTENT_REPLICATION)\n    swap_vol = tf.convert_to_tensor(pricing_context, dtype=self._dtype)\n    expiry_time = dates.daycount_actual_365_fixed(\n        start_date=valuation_date,\n        end_date=self._coupon_start_dates,\n        dtype=self._dtype)\n    lower = tf.zeros_like(cms_rates) + 1e-6\n    # TODO(b/154407973): Improve the logic to compute the upper limit.\n    rate_limit = 2000.0\n    upper = rate_limit * cms_rates\n    num_points = 10001\n\n    def _call_replication():\n      def _intfun_call(x):\n        d2fx = self._f_atm_second_derivative(x, cms_rates)\n\n        forwards = tf.broadcast_to(tf.expand_dims(cms_rates, -1), x.shape)\n        expiries = tf.broadcast_to(tf.expand_dims(expiry_time, -1), x.shape)\n        option_val = _option_prices(\n            volatilities=swap_vol, strikes=x, expiries=expiries,\n            forwards=forwards, is_normal_model=normal_model,\n            dtype=self._dtype)\n        return d2fx * option_val\n\n      intval_c = integration.integrate(\n          _intfun_call, cms_rates, upper, num_points=num_points)\n      dfk = self._f_atm_first_derivative(cms_rates, cms_rates)\n      c_k = _option_prices(volatilities=swap_vol, strikes=cms_rates,\n                           expiries=expiry_time, forwards=cms_rates,\n                           is_normal_model=normal_model,\n                           dtype=self._dtype)\n      return (1.0 + dfk) * c_k + intval_c\n\n    def _put_replication():\n      def _intfun_put(x):\n        d2fx = self._f_atm_second_derivative(x, cms_rates)\n\n        forwards = tf.broadcast_to(tf.expand_dims(cms_rates, -1), x.shape)\n        expiries = tf.broadcast_to(tf.expand_dims(expiry_time, -1), x.shape)\n        option_val = _option_prices(\n            volatilities=swap_vol, strikes=x, expiries=expiries,\n            forwards=forwards, is_call_options=False,\n            is_normal_model=normal_model, dtype=self._dtype)\n        return d2fx * option_val\n\n      intval_p = integration.integrate(\n          _intfun_put, lower, cms_rates, num_points=num_points)\n      dfk = self._f_atm_first_derivative(cms_rates, cms_rates)\n      p_k = _option_prices(volatilities=swap_vol, strikes=cms_rates,\n                           expiries=expiry_time, forwards=cms_rates,\n                           is_call_options=False, is_normal_model=normal_model,\n                           dtype=self._dtype)\n      return (1.0 + dfk) * p_k - intval_p\n\n    call_rep = _call_replication()\n    put_rep = _put_replication()\n\n    return cms_rates + (call_rep - put_rep)\n\n  def _setup(self, coupon_spec):\n    """"""Setup tensors for efficient computations.""""""\n\n    cpn_frequency = dates.PeriodTensor.stack(\n        [x.coupon_frequency for x in coupon_spec], axis=0)\n    cpn_dates, _ = self._generate_schedule(cpn_frequency,\n                                           coupon_spec[-1].businessday_rule)\n    cms_start_dates = cpn_dates[:, :-1]\n    cms_term = dates.PeriodTensor.stack([x.tenor for x in coupon_spec], axis=0)\n\n    cms_end_dates = cpn_dates[:, :-1] + cms_term.expand_dims(\n        axis=-1).broadcast_to(cms_start_dates.shape)\n    coupon_start_dates = cpn_dates[:, :-1]\n    coupon_end_dates = cpn_dates[:, 1:]\n    payment_dates = cpn_dates[:, 1:]\n\n    daycount_fractions = rc.get_daycount_fraction(\n        coupon_start_dates,\n        coupon_end_dates,\n        coupon_spec[-1].daycount_convention,\n        dtype=self._dtype)\n\n    notional = tf.repeat(\n        tf.convert_to_tensor([x.notional for x in coupon_spec],\n                             dtype=self._dtype),\n        payment_dates.shape.as_list()[-1])\n\n    coupon_basis = tf.repeat(tf.convert_to_tensor(\n        [x.coupon_basis for x in coupon_spec], dtype=self._dtype),\n                             payment_dates.shape.as_list()[-1])\n\n    coupon_multiplier = tf.repeat(tf.convert_to_tensor(\n        [x.coupon_multiplier for x in coupon_spec], dtype=self._dtype),\n                                  payment_dates.shape.as_list()[-1])\n\n    contract_index = tf.repeat(\n        tf.range(0, len(coupon_spec)),\n        payment_dates.shape.as_list()[-1])\n\n    cms_fixed_leg = [x.fixed_leg for x in coupon_spec]\n    cms_float_leg = [x.float_leg for x in coupon_spec]\n    self._num_cashflows = daycount_fractions.shape.as_list()[-1]\n    self._swap = irs.InterestRateSwap(\n        cms_start_dates.reshape([-1]),\n        cms_end_dates.reshape([-1]),\n        list(itertools.chain.from_iterable(\n            itertools.repeat(i, self._num_cashflows) for i in cms_fixed_leg)),\n        list(itertools.chain.from_iterable(\n            itertools.repeat(i, self._num_cashflows) for i in cms_float_leg)),\n        dtype=self._dtype\n        )\n    self._coupon_start_dates = coupon_start_dates.reshape([-1])\n    self._coupon_end_dates = coupon_end_dates.reshape([-1])\n    self._payment_dates = payment_dates.reshape([-1])\n    self._notional = notional\n    self._daycount_fractions = tf.reshape(daycount_fractions, [-1])\n    self._coupon_basis = coupon_basis\n    self._coupon_multiplier = coupon_multiplier\n    self._contract_index = contract_index\n\n    # convexity related values\n    def term_to_years(t):\n      frac = tf.where(t.period_type() == dates.PeriodType.MONTH,\n                      tf.constant(1. / 12., dtype=self._dtype),\n                      tf.where(\n                          t.period_type() == dates.PeriodType.YEAR,\n                          tf.constant(1., dtype=self._dtype),\n                          tf.constant(0., dtype=self._dtype)))\n      return frac * tf.cast(t.quantity(), dtype=self._dtype)\n\n    cms_fixed_leg_frequency = dates.PeriodTensor.stack(\n        [x.fixed_leg.coupon_frequency for x in coupon_spec], axis=0)\n    self._delta = term_to_years(cpn_frequency)\n    self._tau = term_to_years(cms_fixed_leg_frequency)\n    self._cms_periods = term_to_years(cms_term) / self._tau\n\n  def _fs(self, s):\n    """"""Equation 2.13(a) from Hagen\'s paper.""""""\n    g = tf.where(s == 0., self._tau * self._cms_periods,\n                 (1/s) * (1. - 1./(1.0 + self._tau * s)**self._cms_periods))\n    return 1./(g * (1. + self._tau*s)**(self._delta / self._tau))\n\n  def _f_atm(self, s, cms_rates):\n    """"""Equation 2.19(b) from Hagan\'s paper.""""""\n    return (s - tf.expand_dims(cms_rates, -1)) * (\n        self._fs(s) / self._fs(tf.expand_dims(cms_rates, -1)) - 1.0)\n\n  def _f_atm_first_derivative(self, s, cms_rates):\n    """"""Computes first order derivative of _f_atm.""""""\n    with tf.GradientTape() as g:\n      g.watch(s)\n      fx = self._f_atm(s, cms_rates)\n    dfx = tf.squeeze(g.gradient(fx, s))\n    return dfx\n\n  def _f_atm_second_derivative(self, s, cms_rates):\n    """"""Computes second order derivative of _f_atm.""""""\n    with tf.GradientTape() as g:\n      g.watch(s)\n      with tf.GradientTape() as gg:\n        gg.watch(s)\n        fx = self._f_atm(s, cms_rates)\n      dfx = tf.squeeze(gg.gradient(fx, s))\n    d2fx = tf.squeeze(g.gradient(dfx, s))\n    return d2fx\n\n\nclass CMSSwap(irs.InterestRateSwap):\n  """"""Represents a batch of CMS Swaps.\n\n  A CMS swap is a swap contract where the floating leg payments are based on the\n  constant maturity swap (CMS) rate. The CMS rate refers to a future fixing of\n  swap rate of a fixed maturity, i.e. the breakeven swap rate on a standard\n  fixed-to-float swap of the specified maturity [1].\n\n  Let S_(i,m) denote a swap rate with maturity `m` (years) on the fixing date\n  `T_i. Consider a CMS swap with the starting date T_0 and maturity date T_n\n  and regularly spaced coupon payment dates T_1, T_2, ..., T_n such that\n\n  T_0 < T_1 < T_2 < ... < T_n and dt_i = T_(i+1) - T_i    (A)\n\n  The CMS rate, S_(i, m), is fixed on T_0, T_1, ..., T_(n-1) and floating\n  payments made are on T_1, T_2, ..., T_n (payment dates) with the i-th payment\n  being equal to tau_i * S_(i, m) where tau_i is the year fraction between\n  [T_i, T_(i+1)].\n\n  The CMSSwap class can be used to create and price multiple CMS swaps\n  simultaneously. However all CMS swaps within an object must be priced using\n  a common reference and discount curve.\n\n  #### Example:\n  The following example illustrates the construction of an CMS swap and\n  calculating its price.\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tf_quant_finance as tff\n  dates = tff.datetime\n  instruments = tff.experimental.instruments\n  rc = tff.experimental.instruments.rates_common\n\n  dtype = np.float64\n  start_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n  maturity_date = dates.convert_to_date_tensor([(2023, 1, 1)])\n  valuation_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n  p3m = dates.months(3)\n  p6m = dates.months(6)\n  p1y = dates.year()\n  fix_spec = instruments.FixedCouponSpecs(\n      coupon_frequency=p6m,\n      currency=\'usd\',\n      notional=1.,\n      coupon_rate=0.02,\n      daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n      businessday_rule=dates.BusinessDayConvention.NONE)\n  flt_spec = instruments.FloatCouponSpecs(\n      coupon_frequency=p3m,\n      reference_rate_term=p3m,\n      reset_frequency=p3m,\n      currency=\'usd\',\n      notional=1.,\n      businessday_rule=dates.BusinessDayConvention.NONE,\n      coupon_basis=0.,\n      coupon_multiplier=1.,\n      daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n  cms_spec = instruments.CMSCouponSpecs(\n      coupon_frequency=p3m,\n      tenor=p1y,\n      float_leg=flt_spec,\n      fixed_leg=fix_spec,\n      notional=1.e6,\n      coupon_basis=0.,\n      coupon_multiplier=1.,\n      businessday_rule=None,\n      daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n  cms = instruments.CMSSwap(\n      start_date, maturity_date, [fix_spec],\n      [cms_spec], dtype=dtype)\n\n  curve_dates = valuation_date + dates.years([0, 1, 2, 3, 5])\n  reference_curve = instruments.RateCurve(\n      curve_dates,\n      np.array([\n          0.02, 0.02, 0.025, 0.03, 0.035\n      ], dtype=np.float64),\n      valuation_date=valuation_date,\n      dtype=np.float64)\n  market = instruments.InterestRateMarket(\n      reference_curve=reference_curve, discount_curve=reference_curve)\n\n  price = cms.price(valuation_date, market)\n  # Expected result: 16629.820479418966\n  ```\n\n  #### References:\n  [1]: Leif B.G. Andersen and Vladimir V. Piterbarg. Interest Rate Modeling,\n      Volume I: Foundations and Vanilla Models. Chapter 5. 2010.\n  """"""\n\n  def __init__(self,\n               start_date,\n               maturity_date,\n               pay_leg,\n               receive_leg,\n               holiday_calendar=None,\n               dtype=None,\n               name=None):\n    """"""Initialize a batch of CMS swap contracts.\n\n    Args:\n      start_date: A rank 1 `DateTensor` specifying the dates for the inception\n        (start of the accrual) of the swap cpntracts. The shape of the input\n        correspond to the numbercof instruments being created.\n      maturity_date: A rank 1 `DateTensor` specifying the maturity dates for\n        each contract. The shape of the input should be the same as that of\n        `start_date`.\n      pay_leg: A list of either `FixedCouponSpecs`, `FloatCouponSpecs` or\n        `CMSCouponSpecs` specifying the coupon payments for the payment leg of\n        the swap. The length of the list should be the same as the number of\n        instruments being created.\n      receive_leg: A list of either `FixedCouponSpecs` or `FloatCouponSpecs` or\n        `CMSCouponSpecs` specifying the coupon payments for the receiving leg\n        of the swap. The length of the list should be the same as the number of\n        instruments being created.\n      holiday_calendar: An instance of `dates.HolidayCalendar` to specify\n        weekends and holidays.\n        Default value: None in which case a holiday calendar would be created\n        with Saturday and Sunday being the holidays.\n      dtype: `tf.Dtype`. If supplied the dtype for the real variables or ops\n        either supplied to the IRS object or created by the IRS object.\n        Default value: None which maps to the default dtype inferred by\n        TensorFlow.\n      name: Python str. The name to give to the ops created by this class.\n        Default value: `None` which maps to \'cms_swap\'.\n    """"""\n    self._name = name or \'cms_swap\'\n\n    if holiday_calendar is None:\n      holiday_calendar = dates.create_holiday_calendar(\n          weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY)\n\n    with tf.name_scope(self._name):\n      self._dtype = dtype\n      self._start_date = dates.convert_to_date_tensor(start_date)\n      self._maturity_date = dates.convert_to_date_tensor(maturity_date)\n      self._holiday_calendar = holiday_calendar\n      self._floating_leg = None\n      self._fixed_leg = None\n      self._cms_leg = None\n      self._pay_leg = self._setup_leg(pay_leg)\n      self._receive_leg = self._setup_leg(receive_leg)\n\n  def price(self, valuation_date, market, model=None, pricing_context=None,\n            name=None):\n    """"""Returns the present value of the instrument on the valuation date.\n\n    Args:\n      valuation_date: A scalar `DateTensor` specifying the date on which\n        valuation is being desired.\n      market: A namedtuple of type `InterestRateMarket` which contains the\n        necessary information for pricing the interest rate swap.\n      model: An optional input of type `InterestRateModelType` to specify the\n        model to use for `convexity correction` while pricing individual\n        swaplets of the cms swap. When `model` is\n        `InterestRateModelType.LOGNORMAL_SMILE_CONSISTENT_REPLICATION` or\n        `InterestRateModelType.NORMAL_SMILE_CONSISTENT_REPLICATION`, the\n        function uses static replication (from lognormal and normal swaption\n        implied volatility data respectively) as described in [1]. When `model`\n        is `InterestRateModelType.LOGNORMAL_RATE` or\n        `InterestRateModelType.NORMAL_RATE`, the function uses analytic\n        approximations for the convexity adjustment based on lognormal and\n        normal swaption rate dyanmics respectively [1].\n        Default value: `None` in which case convexity correction is not used.\n      pricing_context: Additional context relevant for pricing.\n      name: Python str. The name to give to the ops created by this function.\n        Default value: `None` which maps to \'price\'.\n\n    Returns:\n      A Rank 1 `Tensor` of real type containing the modeled price of each IRS\n      contract based on the input market data.\n\n    #### References:\n    [1]: Patrick S. Hagan. Convexity conundrums: Pricing cms swaps, caps and\n    floors. WILMOTT magazine.\n    """"""\n\n    name = name or (self._name + \'_price\')\n    with tf.name_scope(name):\n      return super(CMSSwap, self).price(valuation_date, market, model,\n                                        pricing_context, name)\n\n  def _setup_leg(self, leg):\n    """"""Setup swap legs.""""""\n    if isinstance(leg[0], rc.CMSCouponSpecs):\n      new_leg = CMSCashflowStream(\n          self._start_date, self._maturity_date, leg, dtype=self._dtype)\n      self._cms_leg = new_leg\n    else:\n      new_leg = super(CMSSwap, self)._setup_leg(leg)\n\n    return new_leg\n\n\n# TODO(b/152333799): Use the library model for pricing.\ndef _option_prices(*,\n                   volatilities=None,\n                   strikes=None,\n                   forwards=None,\n                   expiries=None,\n                   is_call_options=True,\n                   is_normal_model=True,\n                   dtype=None):\n  """"""Computes prices of European options using normal model.\n\n  Args:\n    volatilities: Real `Tensor` of any shape and dtype. The volatilities to\n      expiry of the options to price.\n    strikes: A real `Tensor` of the same dtype and compatible shape as\n      `volatilities`. The strikes of the options to be priced.\n    forwards: A real `Tensor` of any shape that broadcasts to the shape of\n      `volatilities`. The forwards to maturity. Either this argument or the\n    expiries: A real `Tensor` of same dtype and compatible shape as\n      `volatilities`. The expiry of each option. The units should be such that\n      `expiry * volatility**2` is dimensionless.\n    is_call_options: A boolean `Tensor` of a shape compatible with\n      `volatilities`. Indicates whether the option is a call (if True) or a put\n      (if False). If not supplied, call options are assumed.\n    is_normal_model: A boolean `Tensor` of a shape compatible with\n      `volatilities`. Indicates whether the options should be priced using\n      normal model (if True) or lognormal model (if False). If not supplied,\n      normal model is assumed.\n    dtype: Optional `tf.DType`. If supplied, the dtype to be used for conversion\n      of any supplied non-`Tensor` arguments to `Tensor`.\n      Default value: `None` which maps to the default dtype inferred by\n        TensorFlow.\n\n  Returns:\n    Options prices computed using normal model for the underlying.\n  """"""\n  dtype = dtype or tf.constant(0.0).dtype\n  def _ncdf(x):\n    sqrt_2 = tf.math.sqrt(tf.constant(2.0, dtype=dtype))\n    return (tf.math.erf(x / sqrt_2) + 1) / 2\n\n  sqrt_var = tf.math.sqrt(expiries) * volatilities\n  d = (forwards - strikes) / sqrt_var\n  mu = tf.constant(0., dtype=dtype)\n  loc = tf.constant(1., dtype=dtype)\n  value = tf.where(\n      is_normal_model,\n      tf.where(is_call_options, (forwards - strikes) * _ncdf(d) +\n               sqrt_var * tfp.distributions.Normal(mu, loc).prob(d),\n               (strikes - forwards) * _ncdf(-d) +\n               sqrt_var * tfp.distributions.Normal(mu, loc).prob(d)),\n      black_scholes.option_price(\n          volatilities=volatilities,\n          strikes=strikes,\n          expiries=expiries,\n          forwards=forwards,\n          is_call_options=is_call_options,\n          dtype=dtype))\n  value = tf.where(\n      expiries > 0, value,\n      tf.where(is_call_options, tf.maximum(forwards - strikes, 0.0),\n               tf.maximum(strikes - forwards, 0.0)))\n  return value\n'"
tf_quant_finance/experimental/instruments/cms_swap_test.py,2,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for cms_swap.py.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\ndates = tff.datetime\ninstruments = tff.experimental.instruments\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass CMSSwapTest(tf.test.TestCase, parameterized.TestCase):\n\n  def get_cms_coupon_spec(self, fix_rate, fix_leg_freq=\'6m\'):\n    p3m = dates.months(3)\n    p6m = dates.months(6)\n    p1y = dates.year()\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=p6m if fix_leg_freq == \'6m\' else p3m,\n        currency=\'usd\',\n        notional=1.,\n        coupon_rate=fix_rate,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=p3m,\n        reference_rate_term=p3m,\n        reset_frequency=p3m,\n        currency=\'usd\',\n        notional=1.,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0.,\n        coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n    cms_spec = instruments.CMSCouponSpecs(\n        coupon_frequency=p3m,\n        tenor=p1y,\n        float_leg=flt_spec,\n        fixed_leg=fix_spec,\n        notional=1.,\n        coupon_basis=0.,\n        coupon_multiplier=1.,\n        businessday_rule=None,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n    return cms_spec\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_cms_stream_no_convexity(self, dtype):\n    start_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    maturity_date = dates.convert_to_date_tensor([(2023, 1, 1)])\n    valuation_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n\n    cms = instruments.CMSCashflowStream(\n        start_date, maturity_date, [self.get_cms_coupon_spec(0.0)],\n        dtype=dtype)\n\n    curve_dates = valuation_date + dates.years([0, 1, 2, 3, 5])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02, 0.02, 0.025, 0.03, 0.035\n        ], dtype=np.float64),\n        valuation_date=valuation_date,\n        dtype=np.float64)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n\n    price = self.evaluate(cms.price(valuation_date, market))\n    np.testing.assert_allclose(price, 0.0555126295434207, atol=1e-7)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_cms_stream_many(self, dtype):\n    start_date = dates.convert_to_date_tensor([(2021, 1, 1), (2021, 1, 1)])\n    maturity_date = dates.convert_to_date_tensor([(2023, 1, 1), (2022, 1, 1)])\n    valuation_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n\n    cms_spec = self.get_cms_coupon_spec(0.0)\n    cms = instruments.CMSCashflowStream(\n        start_date, maturity_date, [cms_spec, cms_spec], dtype=dtype)\n\n    curve_dates = valuation_date + dates.years([0, 1, 2, 3, 5])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02, 0.02, 0.025, 0.03, 0.035\n        ], dtype=np.float64),\n        valuation_date=valuation_date,\n        dtype=np.float64)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n\n    price = self.evaluate(cms.price(valuation_date, market))\n    np.testing.assert_allclose(price,\n                               [0.0555126295434207, 0.022785926686551876],\n                               atol=1e-7)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_cms_stream_past_fixing(self, dtype):\n    start_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    maturity_date = dates.convert_to_date_tensor([(2023, 1, 1)])\n    valuation_date = dates.convert_to_date_tensor([(2021, 1, 10)])\n\n    cms = instruments.CMSCashflowStream(\n        start_date, maturity_date, [self.get_cms_coupon_spec(0.0)], dtype=dtype)\n\n    curve_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    curve_dates = curve_date + dates.years([0, 1, 2, 3, 5])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02, 0.02, 0.025, 0.03, 0.035\n        ], dtype=np.float64),\n        valuation_date=curve_date,\n        dtype=np.float64)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve,\n        discount_curve=reference_curve,\n        swap_rate=0.01)\n\n    price = self.evaluate(cms.price(valuation_date, market))\n    np.testing.assert_allclose(price, 0.053034387186703995, atol=1e-7)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_cms_to_fixed_swap(self, dtype):\n    start_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    maturity_date = dates.convert_to_date_tensor([(2023, 1, 1)])\n    valuation_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    p6m = dates.months(6)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=p6m,\n        currency=\'usd\',\n        notional=1.,\n        coupon_rate=0.02,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n    cms_spec = self.get_cms_coupon_spec(0.0)\n\n    cms = instruments.CMSSwap(\n        start_date, maturity_date, [fix_spec],\n        [cms_spec], dtype=dtype)\n\n    curve_dates = valuation_date + dates.years([0, 1, 2, 3, 5])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02, 0.02, 0.025, 0.03, 0.035\n        ], dtype=np.float64),\n        valuation_date=valuation_date,\n        dtype=np.float64)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n\n    price = self.evaluate(cms.price(valuation_date, market))\n    np.testing.assert_allclose(price, 0.016629820479418966, atol=1e-7)\n\n  @parameterized.named_parameters(\n      (\'LognormalRate\', instruments.InterestRateModelType.LOGNORMAL_RATE, 0.15,\n       0.0006076380479708987),\n      (\'NormalRate\', instruments.InterestRateModelType.NORMAL_RATE, 0.003,\n       0.0005958813803308),\n      (\'ReplicationLn\',\n       instruments.InterestRateModelType.LOGNORMAL_SMILE_CONSISTENT_REPLICATION,\n       0.15, 0.0006076275782589),\n      (\'ReplicationNormal\',\n       instruments.InterestRateModelType.NORMAL_SMILE_CONSISTENT_REPLICATION,\n       0.003, 0.0005956153840977),\n  )\n  def test_cms_convexity_model(self, model, parameter, expected):\n    dtype = np.float64\n    start_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    maturity_date = dates.convert_to_date_tensor([(2031, 1, 1)])\n    valuation_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    p3m = dates.months(3)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=p3m,\n        currency=\'usd\',\n        notional=1.,\n        coupon_rate=0.02,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n    cms_spec = self.get_cms_coupon_spec(0.0, \'3m\')\n\n    cms = instruments.CMSSwap(\n        start_date, maturity_date, [fix_spec],\n        [cms_spec], dtype=dtype)\n\n    curve_dates = valuation_date + dates.years([0, 360])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02, 0.02\n        ], dtype=np.float64),\n        valuation_date=valuation_date,\n        dtype=np.float64)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n\n    price = self.evaluate(cms.price(valuation_date, market, model=model,\n                                    pricing_context=parameter))\n    np.testing.assert_allclose(price, expected, atol=1e-7)\n\n  @parameterized.named_parameters(\n      (\'None\', None, 0.15, 0.0009080833232544),\n      (\'LognormalRate\', instruments.InterestRateModelType.LOGNORMAL_RATE, 0.15,\n       0.0011142108850073),\n      (\'NormalRate\', instruments.InterestRateModelType.NORMAL_RATE, 0.003,\n       0.0010975575420384),\n      (\'ReplicationLn\',\n       instruments.InterestRateModelType.LOGNORMAL_SMILE_CONSISTENT_REPLICATION,\n       0.15, 0.0011141562723458),\n      (\'ReplicationNormal\',\n       instruments.InterestRateModelType.NORMAL_SMILE_CONSISTENT_REPLICATION,\n       0.003, 0.0010973379723202),\n  )\n  def test_cms_convexity_model_6m(self, model, parameter, expected):\n    dtype = np.float64\n    start_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    maturity_date = dates.convert_to_date_tensor([(2031, 1, 1)])\n    valuation_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    p3m = dates.months(3)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=p3m,\n        currency=\'usd\',\n        notional=1.,\n        coupon_rate=0.02,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n    cms_spec = self.get_cms_coupon_spec(0.0)\n\n    cms = instruments.CMSSwap(\n        start_date, maturity_date, [fix_spec],\n        [cms_spec], dtype=dtype)\n\n    curve_dates = valuation_date + dates.years([0, 360])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02, 0.02\n        ], dtype=np.float64),\n        valuation_date=valuation_date,\n        dtype=np.float64)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n\n    price = self.evaluate(cms.price(valuation_date, market, model=model,\n                                    pricing_context=parameter))\n    np.testing.assert_allclose(price, expected, atol=1e-7)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/experimental/instruments/eurodollar_futures.py,3,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Eurodollar futures contract.""""""\n\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance import datetime as dates\nfrom tf_quant_finance.experimental.instruments import rates_common as rc\n\n\nclass EurodollarFutures:\n  """"""Represents a collection of Eurodollar futures contracts.\n\n  Interest rate futures are exchange traded futures contracts on Libor rates\n  liquidly traded on exchanges such as Chicago Mercantile Exchange (CME) or\n  London International Financial Futures and Options Exchange (LIFFE). Contracts\n  on CME on a US Dollar spot Libor rate are called Eurodollar (ED) Futures.\n  An ED future contract at maturity T settles at the price\n  100 * (1 - F(T, T1, T2))\n  where F(T, T1, T2) is the spot Libor rate at time T with start T1 and\n  maturity T2 (ref [1]).\n\n  The EurodollarFutures class can used to create and price multiple contracts\n  simultaneously. However all contracts within an object must be priced using a\n  common reference curve.\n\n  #### Example:\n  The following example illustrates the construction of an ED future instrument\n  and calculating its price.\n\n  ```python\n\n  import numpy as np\n  import tensorflow as tf\n  import tf_quant_finance as tff\n\n  dates = tff.datetime\n  instruments = tff.experimental.instruments\n\n  dtype = np.float64\n  notional = 1.\n  expiry_date = dates.convert_to_date_tensor([(2021, 2, 8)])\n  valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n  rate_term = dateslib.months(3)\n\n  edfuture = instruments.EurodollarFutures(\n      expiry_date, notional, rate_term=rate_term, dtype=dtype)\n\n  curve_dates = valuation_date + tff.datetime.months([1, 2, 3, 12, 24, 60])\n  reference_curve = instruments.RateCurve(\n      curve_dates,\n      np.array([0.02, 0.025, 0.0275, 0.03, 0.035, 0.0325], dtype=dtype),\n      dtype=dtype)\n\n  market = instruments.InterestRateMarket(reference_curve=reference_curve,\n                                          discount_curve=None)\n\n  price = self.evaluate(edfuture.price(valuation_date, market))\n\n  #### References:\n  [1]: Leif B.G. Andersen and Vladimir V. Piterbarg. Interest Rate Modeling,\n      Volume I: Foundations and Vanilla Models. Chapter 5. 2010.\n  """"""\n\n  def __init__(self,\n               expiry_date,\n               contract_notional=1.,\n               daycount_convention=None,\n               rate_term=None,\n               maturity_date=None,\n               dtype=None,\n               name=None):\n    """"""Initialize the Eurodollar futures object.\n\n    Args:\n      expiry_date: A Rank 1 `DateTensor` specifying the dates on which the\n        futures contracts expire.\n      contract_notional: An optional scalar or Rank 1 `Tensor` of real dtype\n        specifying the unit (or size) for the contract. For example for\n        eurodollar futures traded on CME, the contract notional is $2500. If\n        `contract_notional` is entered as a scalar, it is assumed that the input\n        is the same for all of the contracts.\n        Default value: 1.0\n      daycount_convention: An optional `DayCountConvention` corresponding\n        to the day count convention for the underlying rate for each contract.\n        Daycount is assumed to be the same for all contracts in a given batch.\n        Default value: None in which case each the day count convention of\n        DayCountConvention.ACTUAL_360 is used for each contract.\n      rate_term: An optional Rank 1 `PeriodTensor` specifying the term (or\n        tenor) of the rate that determines the settlement of each contract.\n        Default value: `None` in which case the the rate is assumed to be for\n        the period [expiry_date, maturity_date].\n      maturity_date: An optional Rank 1 `DateTensor` specifying the maturity of\n        the underlying forward rate for each contract. This input should be\n        specified if the input `rate_term` is `None`. If both `maturity_date`\n        and `rate_term` are specified, an error is raised.\n        Default value: `None`\n      dtype: `tf.Dtype`. If supplied the dtype for the real variables or ops\n        either supplied to the EurodollarFuture object or created by the\n        EurodollarFuture object.\n        Default value: None which maps to the default dtype inferred by\n        TensorFlow.\n      name: Python str. The name to give to the ops created by this class.\n        Default value: `None` which maps to \'eurodollar_future\'.\n\n    Raises:\n      ValueError: If both `maturity_date` and `rate_term` are unspecified or\n      if both `maturity_date` and `rate_term` are specified.\n    """"""\n    self._name = name or \'eurodollar_futures\'\n\n    if (rate_term is None) == (maturity_date is None):\n      msg = (\'Error creating the EurodollarFutures contract. \'\n             \'Either rate_term or maturity_date is required.\')\n      raise ValueError(msg)\n\n    if rate_term is not None and maturity_date is not None:\n      msg = (\'Error creating the EurodollarFutures contract.\'\n             \' Both rate_term or maturity_date are specified.\')\n      raise ValueError(msg)\n\n    with tf.name_scope(self._name):\n      self._dtype = dtype\n      self._contract_notional = tf.convert_to_tensor(\n          contract_notional, dtype=self._dtype)\n      self._expiry_date = dates.convert_to_date_tensor(expiry_date)\n      self._accrual_start_date = self._expiry_date\n      if rate_term is None:\n        self._accrual_end_date = dates.convert_to_date_tensor(maturity_date)\n      else:\n        # TODO (b/150291959): Add businessday and holiday conventions\n        self._accrual_end_date = self._accrual_start_date + rate_term\n\n      if daycount_convention is None:\n        daycount_convention = rc.DayCountConvention.ACTUAL_360\n\n      self._daycount_convention = daycount_convention\n      self._daycount_fraction = rc.get_daycount_fraction(\n          self._accrual_start_date, self._accrual_end_date,\n          self._daycount_convention, self._dtype)\n\n  def price(self, valuation_date, market, model=None):\n    """"""Returns the price of the contract on the valuation date.\n\n    Args:\n      valuation_date: A scalar `DateTensor` specifying the date on which\n        valuation is being desired.\n      market: A namedtuple of type `InterestRateMarket` which contains the\n        necessary information for pricing the FRA instrument.\n      model: Reserved for future use.\n\n    Returns:\n      A Rank 1 `Tensor` of real type containing the modeled price of each\n      futures contract based on the input market data.\n    """"""\n\n    del model, valuation_date\n\n    reference_curve = market.reference_curve\n\n    fwd_rate = reference_curve.get_forward_rate(self._accrual_start_date,\n                                                self._accrual_end_date,\n                                                self._daycount_fraction)\n\n    return 100. * self._contract_notional * (1. - fwd_rate)\n'"
tf_quant_finance/experimental/instruments/eurodollar_futures_test.py,2,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for eurodollar_futures.py.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\ndates = tff.datetime\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass EurodollarFuturesTest(tf.test.TestCase):\n\n  def test_edf_correctness(self):\n    dtype = np.float64\n    notional = 1.\n    expiry_date = tff.datetime.convert_to_date_tensor(\n        [(2021, 2, 8)])\n    valuation_date = tff.datetime.convert_to_date_tensor([(2020, 2, 8)])\n    rate_term = tff.datetime.months(3)\n    edfuture = tff.experimental.instruments.EurodollarFutures(\n        expiry_date,\n        contract_notional=notional,\n        rate_term=rate_term,\n        dtype=dtype)\n\n    curve_dates = valuation_date + tff.datetime.months([1, 2, 3, 12, 24, 60])\n    reference_curve = tff.experimental.instruments.RateCurve(\n        curve_dates,\n        np.array([0.02, 0.025, 0.0275, 0.03, 0.035, 0.0325], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = tff.experimental.instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=None)\n\n    price = self.evaluate(edfuture.price(valuation_date, market))\n    np.testing.assert_allclose(price, 96.41051344, atol=1e-6)\n\n  def test_edf_explicit_maturity(self):\n    dtype = np.float64\n    notional = 1.\n    expiry_date = tff.datetime.convert_to_date_tensor(\n        [(2021, 2, 8)])\n    valuation_date = tff.datetime.convert_to_date_tensor([(2020, 2, 8)])\n    maturity_date = tff.datetime.convert_to_date_tensor([(2021, 5, 8)])\n    edfuture = tff.experimental.instruments.EurodollarFutures(\n        expiry_date, contract_notional=notional, maturity_date=maturity_date,\n        dtype=dtype)\n\n    curve_dates = valuation_date + tff.datetime.months([1, 2, 3, 12, 24, 60])\n    reference_curve = tff.experimental.instruments.RateCurve(\n        curve_dates,\n        np.array([0.02, 0.025, 0.0275, 0.03, 0.035, 0.0325], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = tff.experimental.instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=None)\n\n    price = self.evaluate(edfuture.price(valuation_date, market))\n    np.testing.assert_allclose(price, 96.41051344, atol=1e-6)\n\n  def test_edf_many(self):\n    dtype = np.float64\n    notional = 1.\n    expiry_date = tff.datetime.convert_to_date_tensor(\n        [(2021, 2, 8), (2021, 2, 8)])\n    valuation_date = tff.datetime.convert_to_date_tensor([(2020, 2, 8)])\n    maturity_date = tff.datetime.convert_to_date_tensor(\n        [(2021, 5, 8), (2021, 5, 8)])\n\n    edfuture = tff.experimental.instruments.EurodollarFutures(\n        expiry_date, contract_notional=notional, maturity_date=maturity_date,\n        dtype=dtype)\n\n    curve_dates = valuation_date + tff.datetime.months([1, 2, 3, 12, 24, 60])\n    reference_curve = tff.experimental.instruments.RateCurve(\n        curve_dates,\n        np.array([0.02, 0.025, 0.0275, 0.03, 0.035, 0.0325], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = tff.experimental.instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=None)\n\n    price = self.evaluate(edfuture.price(valuation_date, market))\n    np.testing.assert_allclose(price, [96.41051344, 96.41051344], atol=1e-6)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/experimental/instruments/floating_rate_note.py,4,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Floating rate note.""""""\n\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance import datetime as dates\nfrom tf_quant_finance.experimental.instruments import cashflow_stream as cs\n\n\nclass FloatingRateNote:\n  """"""Represents a batch of floating rate notes.\n\n  Floating rate notes are bond securities where the value of the coupon is not\n  fixed at the time of issuance but is rather reset for every coupon period\n  typically based on a benchmark index such as LIBOR rate [1].\n\n  For example, consider a floating rate note with settlement date T_0 and\n  maturity date T_n and equally spaced coupon payment dates T_1, T_2, ..., T_n\n  such that\n\n  T_0 < T_1 < T_2 < ... < T_n and dt_i = T_(i+1) - T_i    (A)\n\n  The floating rate is fixed on T_0, T_1, ..., T_(n-1) and the payments are\n  typically made on T_1, T_2, ..., T_n (payment dates) and the i-th coupon\n  payment is given by:\n\n  c_i = N * tau_i * L[T_{i-1}, T_i]                        (B)\n\n  where N is the notional amount, tau_i is the daycount fraction for the period\n  [T_{i-1}, T_i] and L[T_{i-1}, T_i] is the flotaing rate reset at T_{i-1}.\n\n  The FloatingRateNote class can be used to create and price multiple FRNs\n  simultaneously. However all FRNs within a FloatingRateNote object must be\n  priced using a common reference and discount curve.\n\n  #### Example:\n  The following example illustrates the construction of an IRS instrument and\n  calculating its price.\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tf_quant_finance as tff\n  dates = tff.datetime\n  instruments = tff.experimental.instruments\n  rc = tff.experimental.instruments.rates_common\n\n  dtype = np.float64\n  settlement_date = dates.convert_to_date_tensor([(2021, 1, 15)])\n  maturity_date = dates.convert_to_date_tensor([(2022, 1, 15)])\n  valuation_date = dates.convert_to_date_tensor([(2021, 1, 15)])\n  period_3m = dates.months(3)\n  flt_spec = instruments.FloatCouponSpecs(\n      coupon_frequency=period_3m,\n      reference_rate_term=period_3m,\n      reset_frequency=period_3m,\n      currency=\'usd\',\n      notional=100.,\n      businessday_rule=dates.BusinessDayConvention.NONE,\n      coupon_basis=0.,\n      coupon_multiplier=1.,\n      daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n  frn = instruments.FloatingRateNote(settlement_date, maturity_date,\n                                     [flt_spec],\n                                     dtype=dtype)\n\n  curve_dates = valuation_date + dates.months([0, 6, 12, 36])\n  reference_curve = instruments.RateCurve(\n      curve_dates,\n      np.array([0.0, 0.005, 0.007, 0.015], dtype=dtype),\n      valuation_date=valuation_date,\n      dtype=dtype)\n  market = instruments.InterestRateMarket(discount_curve=reference_curve,\n                                          reference_curve=reference_curve)\n\n  price = frn.price(valuation_date, market)\n  # Expected result: 100.\n  ```\n\n  #### References:\n  [1]: Tomas Bjork. Arbitrage theory in continuous time, Second edition.\n      Chapter 20. 2004.\n  """"""\n\n  def __init__(self,\n               settlement_date,\n               maturity_date,\n               coupon_spec,\n               start_date=None,\n               first_coupon_date=None,\n               penultimate_coupon_date=None,\n               holiday_calendar=None,\n               dtype=None,\n               name=None):\n    """"""Initialize a batch of floating rate notes (FRNs).\n\n    Args:\n      settlement_date: A rank 1 `DateTensor` specifying the settlement date of\n        the FRNs.\n      maturity_date: A rank 1 `DateTensor` specifying the maturity dates of the\n        FRNs. The shape of the input should be the same as that of\n        `settlement_date`.\n      coupon_spec: A list of `FloatCouponSpecs` specifying the coupon payments.\n        The length of the list should be the same as the number of FRNs\n        being created.\n      start_date: An optional `DateTensor` specifying the dates when the\n        interest starts to accrue for the coupons. The input can be used to\n        specify a forward start date for the coupons. The shape of the input\n        correspond to the numbercof instruments being created.\n        Default value: None in which case the coupons start to accrue from the\n        `settlement_date`.\n      first_coupon_date: An optional rank 1 `DateTensor` specifying the dates\n        when first coupon will be paid for FRNs with irregular first coupon.\n      penultimate_coupon_date: An optional rank 1 `DateTensor` specifying the\n        dates when the penultimate coupon (or last regular coupon) will be paid\n        for FRNs with irregular last coupon.\n      holiday_calendar: An instance of `dates.HolidayCalendar` to specify\n        weekends and holidays.\n        Default value: None in which case a holiday calendar would be created\n        with Saturday and Sunday being the holidays.\n      dtype: `tf.Dtype`. If supplied the dtype for the real variables or ops\n        either supplied to the bond object or created by the bond object.\n        Default value: None which maps to the default dtype inferred by\n        TensorFlow.\n      name: Python str. The name to give to the ops created by this class.\n        Default value: `None` which maps to \'floating_rate_note\'.\n    """"""\n    self._name = name or \'floating_rate_note\'\n\n    if holiday_calendar is None:\n      holiday_calendar = dates.create_holiday_calendar(\n          weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY)\n\n    with tf.name_scope(self._name):\n      self._dtype = dtype\n      self._settlement_date = dates.convert_to_date_tensor(settlement_date)\n      self._maturity_date = dates.convert_to_date_tensor(maturity_date)\n      self._holiday_calendar = holiday_calendar\n      self._setup(coupon_spec, start_date, first_coupon_date,\n                  penultimate_coupon_date)\n\n  def price(self, valuation_date, market, model=None, name=None):\n    """"""Returns the price of the FRNs on the valuation date.\n\n    Args:\n      valuation_date: A scalar `DateTensor` specifying the date on which\n        valuation is being desired.\n      market: A namedtuple of type `InterestRateMarket` which contains the\n        necessary information for pricing the FRNs.\n      model: Reserved for future use.\n      name: Python str. The name to give to the ops created by this function.\n        Default value: `None` which maps to \'price\'.\n\n    Returns:\n      A Rank 1 `Tensor` of real dtype containing the price of each FRN\n      based on the input market data.\n    """"""\n\n    name = name or (self._name + \'_price\')\n    with tf.name_scope(name):\n      discount_curve = market.discount_curve\n      coupon_cf = self._cashflows.price(valuation_date, market, model)\n      principal_cf = (\n          self._notional * discount_curve.get_discount_factor(\n              self._maturity_date)\n          )\n      return coupon_cf + principal_cf\n\n  def _setup(self, coupon_spec, start_date, first_coupon_date,\n             penultimate_coupon_date):\n    """"""Setup bond cashflows.""""""\n    if start_date is None:\n      start_date = self._settlement_date\n    self._cashflows = cs.FloatingCashflowStream(\n        start_date,\n        self._maturity_date,\n        coupon_spec,\n        first_coupon_date=first_coupon_date,\n        penultimate_coupon_date=penultimate_coupon_date,\n        dtype=self._dtype)\n\n    self._notional = tf.convert_to_tensor([x.notional for x in coupon_spec],\n                                          dtype=self._dtype)\n'"
tf_quant_finance/experimental/instruments/floating_rate_note_test.py,2,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for floating_rate_note.py.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\ndates = tff.datetime\ninstruments = tff.experimental.instruments\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass FloatingRateNoteTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_frn_correctness(self, dtype):\n    settlement_date = dates.convert_to_date_tensor([(2021, 1, 15)])\n    maturity_date = dates.convert_to_date_tensor([(2022, 1, 15)])\n    valuation_date = dates.convert_to_date_tensor([(2021, 1, 15)])\n    period_3m = dates.months(3)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period_3m,\n        reference_rate_term=period_3m,\n        reset_frequency=period_3m,\n        currency=\'usd\',\n        notional=100.,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0.,\n        coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    frn = instruments.FloatingRateNote(settlement_date, maturity_date,\n                                       [flt_spec],\n                                       dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([0, 6, 12, 36])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.0, 0.005, 0.007, 0.015], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(discount_curve=reference_curve,\n                                            reference_curve=reference_curve)\n\n    price = self.evaluate(frn.price(valuation_date, market))\n    np.testing.assert_allclose(price, 100.0, atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_frn_correctness_fwd_start(self, dtype):\n    settlement_date = dates.convert_to_date_tensor([(2021, 1, 15)])\n    start_date = dates.convert_to_date_tensor([(2021, 4, 15)])\n    maturity_date = dates.convert_to_date_tensor([(2022, 1, 15)])\n    valuation_date = dates.convert_to_date_tensor([(2021, 1, 15)])\n    period_3m = dates.months(3)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period_3m,\n        reference_rate_term=period_3m,\n        reset_frequency=period_3m,\n        currency=\'usd\',\n        notional=100.,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0.,\n        coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    frn = instruments.FloatingRateNote(settlement_date, maturity_date,\n                                       [flt_spec],\n                                       start_date=start_date,\n                                       dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([0, 6, 12, 36])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.0, 0.005, 0.007, 0.015], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(discount_curve=reference_curve,\n                                            reference_curve=reference_curve)\n\n    price = self.evaluate(frn.price(valuation_date, market))\n    np.testing.assert_allclose(price, 99.9387155246714656, atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_frn_many(self, dtype):\n    settlement_date = dates.convert_to_date_tensor([(2021, 1, 15),\n                                                    (2021, 1, 15)])\n    maturity_date = dates.convert_to_date_tensor([(2022, 1, 15),\n                                                  (2022, 1, 15)])\n    valuation_date = dates.convert_to_date_tensor([(2021, 1, 15)])\n    period_3m = dates.months(3)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period_3m,\n        reference_rate_term=period_3m,\n        reset_frequency=period_3m,\n        currency=\'usd\',\n        notional=100.,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0.,\n        coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    frn = instruments.FloatingRateNote(settlement_date, maturity_date,\n                                       [flt_spec, flt_spec],\n                                       dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([0, 6, 12, 36])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.0, 0.005, 0.007, 0.015], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(discount_curve=reference_curve,\n                                            reference_curve=reference_curve)\n\n    price = self.evaluate(frn.price(valuation_date, market))\n    np.testing.assert_allclose(price, [100.0, 100.0], atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_frn_basis(self, dtype):\n    settlement_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    maturity_date = dates.convert_to_date_tensor([(2022, 1, 1)])\n    valuation_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    period_3m = dates.months(3)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period_3m,\n        reference_rate_term=period_3m,\n        reset_frequency=period_3m,\n        currency=\'usd\',\n        notional=100.,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0.01,\n        coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    frn = instruments.FloatingRateNote(settlement_date, maturity_date,\n                                       [flt_spec],\n                                       dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([0, 6, 12, 36])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.001, 0.005, 0.007, 0.015], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(discount_curve=reference_curve,\n                                            reference_curve=reference_curve)\n\n    price = self.evaluate(frn.price(valuation_date, market))\n    np.testing.assert_allclose(price, 100.996314114175, atol=1e-7)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_frn_stub_begin(self, dtype):\n    settlement_date = dates.convert_to_date_tensor([(2021, 3, 1)])\n    maturity_date = dates.convert_to_date_tensor([(2022, 1, 1)])\n    valuation_date = dates.convert_to_date_tensor([(2021, 3, 1)])\n    first_coupon_date = dates.convert_to_date_tensor([(2021, 4, 1)])\n    period_3m = dates.months(3)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period_3m,\n        reference_rate_term=period_3m,\n        reset_frequency=period_3m,\n        currency=\'usd\',\n        notional=100.,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0.01,\n        coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    frn = instruments.FloatingRateNote(settlement_date, maturity_date,\n                                       [flt_spec],\n                                       first_coupon_date=first_coupon_date,\n                                       dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([0, 6, 12, 36])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.001, 0.005, 0.007, 0.015], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(discount_curve=reference_curve,\n                                            reference_curve=reference_curve)\n\n    price = self.evaluate(frn.price(valuation_date, market))\n    np.testing.assert_allclose(price, 100.83591541528823, atol=1e-7)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_frn_stub_end(self, dtype):\n    settlement_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    maturity_date = dates.convert_to_date_tensor([(2022, 2, 1)])\n    valuation_date = dates.convert_to_date_tensor([(2021, 1, 1)])\n    penultimate_coupon_date = dates.convert_to_date_tensor([(2022, 1, 1)])\n    period_3m = dates.months(3)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period_3m,\n        reference_rate_term=period_3m,\n        reset_frequency=period_3m,\n        currency=\'usd\',\n        notional=100.,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0.01,\n        coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    frn = instruments.FloatingRateNote(\n        settlement_date,\n        maturity_date, [flt_spec],\n        penultimate_coupon_date=penultimate_coupon_date,\n        dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([0, 6, 12, 36])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.001, 0.005, 0.007, 0.015], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = instruments.InterestRateMarket(discount_curve=reference_curve,\n                                            reference_curve=reference_curve)\n\n    price = self.evaluate(frn.price(valuation_date, market))\n    np.testing.assert_allclose(price, 101.08057198860133, atol=1e-7)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/experimental/instruments/forward_rate_agreement.py,5,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Forward Rate Agreement.""""""\n\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance import datetime as dates\nfrom tf_quant_finance.experimental.instruments import rates_common as rc\n\n\nclass ForwardRateAgreement:\n  """"""Represents a batch of Forward Rate Agreements (FRA).\n\n  An FRA is a contract for the period [T, T+tau] where the holder exchanges a\n  fixed rate (agreed at the start of the contract) against a floating payment\n  determined at time T based on the spot Libor rate for term `tau`. The\n  cashflows are exchanged at the settlement time T_s, which is either equal to T\n  or close to T. The FRA are structured so that the payments are made in T+tau\n  dollars (ref [1]).\n\n  The ForwardRateAgreement class can be used to create and price multiple FRAs\n  simultaneously. However all FRAs within a FRA object must be priced using\n  a common reference and discount curve.\n\n  #### Example:\n  The following example illustrates the construction of a FRA instrument and\n  calculating its price.\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tf_quant_finance as tff\n  dates = tff.datetime\n\n  dtype = np.float64\n  notional = 1.\n  settlement_date = dates.convert_to_date_tensor([(2021, 2, 8)])\n  fixing_date = dates.convert_to_date_tensor([(2021, 2, 8)])\n  valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n  fixed_rate = 0.02\n  rate_term = rate_term = dates.months(3)\n\n  fra = tff.experimental.instruments.ForwardRateAgreement(\n        notional, settlement_date, fixing_date, fixed_rate,\n        rate_term=rate_term, dtype=dtype)\n  curve_dates = valuation_date + dates.months([1, 2, 3, 12, 24, 60])\n  reference_curve = tff.experimental.instruments.RateCurve(\n      curve_dates,\n      np.array([0.02, 0.025, 0.0275, 0.03, 0.035, 0.0325], dtype=dtype),\n      dtype=dtype)\n  market = tff.experimental.instruments.InterestRateMarket(\n      reference_curve=reference_curve, discount_curve=reference_curve)\n\n  price = fra.price(valuation_date, market)\n  # Expected result: 0.00378275\n  ```\n\n  #### References:\n  [1]: Leif B.G. Andersen and Vladimir V. Piterbarg. Interest Rate Modeling,\n      Volume I: Foundations and Vanilla Models. Chapter 5. 2010.\n  """"""\n\n  def __init__(self,\n               settlement_date,\n               fixing_date,\n               fixed_rate,\n               notional=1.,\n               daycount_convention=None,\n               rate_term=None,\n               maturity_date=None,\n               dtype=None,\n               name=None):\n    """"""Initialize the batch of FRA contracts.\n\n    Args:\n      settlement_date: A rank 1 `DateTensor` specifying the dates on which\n        cashflows are settled. The shape of the input correspond to the number\n        of instruments being created.\n      fixing_date: A rank 1 `DateTensor` specifying the dates on which forward\n        rate will be fixed. The shape of the inout should be the same as that of\n        `settlement_date`.\n      fixed_rate: A rank 1 `Tensor` of real dtype specifying the fixed rate\n        payment agreed at the initiation of the individual contracts. The shape\n        should be the same as that of `settlement_date`.\n      notional: A scalar or a rank 1 `Tensor` of real dtype specifying the\n        notional amount for each contract. When the notional is specified as a\n        scalar, it is assumed that all contracts have the same notional. If the\n        notional is in the form of a `Tensor`, then the shape must be the same\n        as `settlement_date`.\n        Default value: 1.0\n      daycount_convention: An optional `DayCountConvention` to determine\n        how cashflows are accrued for each contract. Daycount is assumed to be\n        the same for all contracts in a given batch.\n        Default value: None in which case the daycount convention will default\n        to DayCountConvention.ACTUAL_360 for all contracts.\n      rate_term: An optional rank 1 `PeriodTensor` specifying the term (or the\n        tenor) of the Libor rate that determines the floating cashflow. The\n        shape of the input should be the same as `settlement_date`.\n        Default value: `None` in which case the the forward rate is determined\n        for the period [settlement_date, maturity_date].\n      maturity_date: An optional rank 1 `DateTensor` specifying the maturity of\n        the underlying forward rate for each contract. This input is only used\n        if the input `rate_term` is `None`.\n        Default value: `None`\n      dtype: `tf.Dtype`. If supplied the dtype for the real variables or ops\n        either supplied to the FRA object or created by the FRA object.\n        Default value: None which maps to the default dtype inferred by\n        TensorFlow.\n      name: Python str. The name to give to the ops created by this class.\n        Default value: `None` which maps to \'forward_rate_agreement\'.\n\n    Raises:\n      ValueError: If both `maturity_date` and `rate_term` are unspecified.\n    """"""\n    self._name = name or \'forward_rate_agreement\'\n\n    if rate_term is None and maturity_date is None:\n      raise ValueError(\n          \'Error creating FRA. Either rate_term or maturity_date is required.\')\n\n    with tf.name_scope(self._name):\n      self._dtype = dtype\n      self._notional = tf.convert_to_tensor(notional, dtype=self._dtype)\n      self._fixing_date = dates.convert_to_date_tensor(fixing_date)\n      self._settlement_date = dates.convert_to_date_tensor(settlement_date)\n      self._accrual_start_date = dates.convert_to_date_tensor(settlement_date)\n      if rate_term is None:\n        self._accrual_end_date = dates.convert_to_date_tensor(maturity_date)\n      else:\n        self._accrual_end_date = self._accrual_start_date + rate_term\n\n      # TODO (b/150216422): Fix tf.repeat to work with python enums\n      if daycount_convention is None:\n        daycount_convention = rc.DayCountConvention.ACTUAL_360\n\n      self._fixed_rate = tf.convert_to_tensor(fixed_rate, dtype=self._dtype,\n                                              name=\'fixed_rate\')\n      self._daycount_convention = daycount_convention\n      self._daycount_fraction = rc.get_daycount_fraction(\n          self._accrual_start_date, self._accrual_end_date,\n          self._daycount_convention, self._dtype)\n\n  def price(self, valuation_date, market, model=None):\n    """"""Returns the present value of the instrument on the valuation date.\n\n    Args:\n      valuation_date: A scalar `DateTensor` specifying the date on which\n        valuation is being desired.\n      market: A namedtuple of type `InterestRateMarket` which contains the\n        necessary information for pricing the FRA instrument.\n      model: Reserved for future use.\n\n    Returns:\n      A Rank 1 `Tensor` of real type containing the modeled price of each FRA\n      contract based on the input market data.\n    """"""\n\n    del model, valuation_date\n\n    reference_curve = market.reference_curve\n    discount_curve = market.discount_curve\n\n    fwd_rate = reference_curve.get_forward_rate(self._accrual_start_date,\n                                                self._accrual_end_date,\n                                                self._daycount_fraction)\n    discount_at_settlement = discount_curve.get_discount_factor(\n        self._settlement_date)\n\n    return discount_at_settlement * self._notional * (\n        fwd_rate - self._fixed_rate) * self._daycount_fraction / (\n            1. + self._daycount_fraction * fwd_rate)\n'"
tf_quant_finance/experimental/instruments/forward_rate_agreement_test.py,4,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for forward_rate_agreement.py.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\ndates = tff.datetime\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass ForwardRateAgreementTest(tf.test.TestCase):\n\n  def test_fra_correctness(self):\n    dtype = np.float64\n    notional = 1.\n    settlement_date = dates.convert_to_date_tensor(\n        [(2021, 2, 8)])\n    fixing_date = dates.convert_to_date_tensor([(2021, 2, 8)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n    fixed_rate = 0.02\n    rate_term = dates.months(3)\n    fra = tff.experimental.instruments.ForwardRateAgreement(\n        settlement_date, fixing_date, fixed_rate, notional=notional,\n        rate_term=rate_term, dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([1, 2, 3, 12, 24, 60])\n    reference_curve = tff.experimental.instruments.RateCurve(\n        curve_dates,\n        np.array([0.02, 0.025, 0.0275, 0.03, 0.035, 0.0325], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = tff.experimental.instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n    price = self.evaluate(fra.price(valuation_date, market))\n    np.testing.assert_allclose(price, 0.00377957, atol=1e-6)\n\n  def test_fra_explicit_maturity(self):\n    dtype = np.float64\n    notional = 1.\n    settlement_date = dates.convert_to_date_tensor(\n        [(2021, 2, 8)])\n    fixing_date = dates.convert_to_date_tensor([(2021, 2, 8)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n    fixed_rate = 0.02\n    maturity_date = dates.convert_to_date_tensor(\n        [(2021, 5, 8)])\n    fra = tff.experimental.instruments.ForwardRateAgreement(\n        settlement_date, fixing_date, fixed_rate, notional=notional,\n        maturity_date=maturity_date, dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([1, 2, 3, 12, 24, 60])\n    reference_curve = tff.experimental.instruments.RateCurve(\n        curve_dates,\n        np.array([0.02, 0.025, 0.0275, 0.03, 0.035, 0.0325], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = tff.experimental.instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n    price = self.evaluate(fra.price(valuation_date, market))\n    np.testing.assert_allclose(price, 0.00377957, atol=1e-6)\n\n  def test_fra_many(self):\n    dtype = np.float64\n    notional = 1.\n    settlement_date = dates.convert_to_date_tensor(\n        [(2021, 2, 8), (2021, 5, 8), (2021, 8, 8)])\n    fixing_date = dates.convert_to_date_tensor(\n        [(2021, 2, 8), (2021, 5, 8), (2021, 8, 8)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n    fixed_rate = tf.convert_to_tensor([0.02, 0.021, 0.022], dtype=dtype)\n    rate_term = dates.months([3, 3, 3])\n    fra = tff.experimental.instruments.ForwardRateAgreement(\n        settlement_date, fixing_date, fixed_rate, notional=notional,\n        rate_term=rate_term, dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([1, 2, 3, 12, 24, 60])\n    reference_curve = tff.experimental.instruments.RateCurve(\n        curve_dates,\n        np.array([0.02, 0.025, 0.0275, 0.03, 0.035, 0.0325], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = tff.experimental.instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n    price = self.evaluate(fra.price(valuation_date, market))\n    np.testing.assert_allclose(price, [0.00377957, 0.0042278427, 0.004548173],\n                               atol=1e-6)\n\n  def test_fra_many_act365(self):\n    dtype = np.float64\n    notional = 1.\n    settlement_date = dates.convert_to_date_tensor(\n        [(2021, 2, 8), (2021, 5, 8), (2021, 8, 8)])\n    fixing_date = dates.convert_to_date_tensor(\n        [(2021, 2, 8), (2021, 5, 8), (2021, 8, 8)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n    fixed_rate = tf.convert_to_tensor([0.02, 0.021, 0.022], dtype=dtype)\n    rate_term = dates.months([3, 3, 3])\n    fra = tff.experimental.instruments.ForwardRateAgreement(\n        settlement_date,\n        fixing_date,\n        fixed_rate,\n        notional=notional,\n        rate_term=rate_term,\n        dtype=dtype,\n        daycount_convention=tff.experimental.instruments.DayCountConvention\n        .ACTUAL_365)\n\n    curve_dates = valuation_date + dates.months([1, 2, 3, 12, 24, 60])\n    reference_curve = tff.experimental.instruments.RateCurve(\n        curve_dates,\n        np.array([0.02, 0.025, 0.0275, 0.03, 0.035, 0.0325], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = tff.experimental.instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n    price = self.evaluate(fra.price(valuation_date, market))\n    np.testing.assert_allclose(price, [0.003844721, 0.004297866, 0.00462077292],\n                               atol=1e-6)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/experimental/instruments/interest_rate_swap.py,5,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Interest rate swap.""""""\n\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance import datetime as dates\nfrom tf_quant_finance.experimental.instruments import cashflow_stream as cs\nfrom tf_quant_finance.experimental.instruments import rates_common as rc\n\n\nclass InterestRateSwap:\n  """"""Represents a batch of Interest Rate Swaps (IRS).\n\n  An Interest rate swap (IRS) is a contract between two counterparties for an\n  exchange of a series of payments over a period of time. The payments are made\n  periodically (for example quarterly or semi-annually) where the last payment\n  is made at the maturity (or termination) of the contract. In the case of\n  fixed-for-floating IRS, one counterparty pays a fixed rate while the other\n  counterparty\'s payments are linked to a floating index, most commonly the\n  LIBOR rate. On the other hand, in the case of interest rate basis swap, the\n  payments of both counterparties are linked to a floating index. Typically, the\n  floating rate is observed (or fixed) at the begining of each period while the\n  payments are made at the end of each period [1].\n\n  For example, consider a vanilla swap with the starting date T_0 and maturity\n  date T_n and equally spaced coupon payment dates T_1, T_2, ..., T_n such that\n\n  T_0 < T_1 < T_2 < ... < T_n and dt_i = T_(i+1) - T_i    (A)\n\n  The floating rate is fixed on T_0, T_1, ..., T_(n-1) and both the fixed and\n  floating payments are made on T_1, T_2, ..., T_n (payment dates).\n\n  The InterestRateSwap class can be used to create and price multiple IRS\n  simultaneously. The class supports vanilla fixed-for-floating swaps as\n  well as basis swaps. However all IRS within an IRS object must be priced using\n  a common reference and discount curve.\n\n  #### Example (non batch):\n  The following example illustrates the construction of an IRS instrument and\n  calculating its price.\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tf_quant_finance as tff\n  dates = tff.datetime\n  instruments = tff.experimental.instruments\n\n  dtype = np.float64\n  start_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n  maturity_date = dates.convert_to_date_tensor([(2022, 2, 8)])\n  valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n  period_3m = dates.months(3)\n  period_6m = dates.months(6)\n  fix_spec = instruments.FixedCouponSpecs(\n              coupon_frequency=period_6m, currency=\'usd\',\n              notional=1., coupon_rate=0.03134,\n              daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n              businessday_rule=dates.BusinessDayConvention.NONE)\n\n  flt_spec = instruments.FloatCouponSpecs(\n              coupon_frequency=period_3m, reference_rate_term=period_3m,\n              reset_frequency=period_3m, currency=\'usd\', notional=1.,\n              businessday_rule=dates.BusinessDayConvention.NONE,\n              coupon_basis=0., coupon_multiplier=1.,\n              daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n  swap = instruments.InterestRateSwap([(2020,2,2)], [(2023,2,2)], [fix_spec],\n                                      [flt_spec], dtype=np.float64)\n\n  curve_dates = valuation_date + dates.years([1, 2, 3, 5, 7, 10, 30])\n  reference_curve = instruments.RateCurve(\n      curve_dates,\n      np.array([\n        0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n        0.03213901, 0.03257991\n        ], dtype=dtype),\n      valuation_date=valuation_date,\n      dtype=dtype)\n  market = instruments.InterestRateMarket(\n      reference_curve=reference_curve, discount_curve=reference_curve)\n\n  price = swap.price(valuation_date, market)\n  # Expected result: 1e-7\n  ```\n\n  #### Example (batch):\n  The following example illustrates the construction and pricing of IRS using\n  batches.\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tf_quant_finance as tff\n  dates = tff.datetime\n  instruments = tff.experimental.instruments\n\n  dtype = np.float64\n  notional = 1.0\n  maturity_date = dates.convert_to_date_tensor([(2023, 2, 8), (2027, 2, 8)])\n  start_date = dates.convert_to_date_tensor([(2020, 2, 8), (2020, 2, 8)])\n  valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n\n  period3m = dates.months([3, 3])\n  period6m = dates.months([6, 6])\n  fix_spec = instruments.FixedCouponSpecs(\n      coupon_frequency=period6m, currency=\'usd\',\n      notional=notional,\n      coupon_rate=[0.03134, 0.03181],\n      daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n      businessday_rule=dates.BusinessDayConvention.NONE)\n  flt_spec = instruments.FloatCouponSpecs(\n      coupon_frequency=period3m, reference_rate_term=period3m,\n      reset_frequency=period3m, currency=\'usd\',\n      notional=notional,\n      businessday_rule=dates.BusinessDayConvention.NONE,\n      coupon_basis=0.0, coupon_multiplier=1.0,\n      daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n  swap = instruments.InterestRateSwap(start_date, maturity_date,\n                                      fix_spec, flt_spec,\n                                      dtype=dtype)\n  curve_dates = valuation_date + dates.years([1, 2, 3, 5, 7, 10, 30])\n  reference_curve = instruments.RateCurve(\n      curve_dates,\n      np.array([\n        0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n        0.03213901, 0.03257991\n        ], dtype=dtype),\n        valuation_date=valuation_date,\n      dtype=dtype)\n  market = instruments.InterestRateMarket(\n      reference_curve=reference_curve, discount_curve=reference_curve)\n\n  price = swap.price(valuation_date, market)\n  # Expected result: [1.0e-7, 1.0e-7]\n  ```\n\n  #### References:\n  [1]: Leif B.G. Andersen and Vladimir V. Piterbarg. Interest Rate Modeling,\n      Volume I: Foundations and Vanilla Models. Chapter 5. 2010.\n  """"""\n\n  def __init__(self,\n               start_date,\n               maturity_date,\n               pay_leg,\n               receive_leg,\n               holiday_calendar=None,\n               dtype=None,\n               name=None):\n    """"""Initialize a batch of IRS contracts.\n\n    Args:\n      start_date: A rank 1 `DateTensor` specifying the dates for the inception\n        (start of the accrual) of the swap contracts. The shape of the input\n        correspond to the number of instruments being created.\n      maturity_date: A rank 1 `DateTensor` specifying the maturity dates for\n        each contract. The shape of the input should be the same as that of\n        `start_date`.\n      pay_leg: A scalar or a list of either `FixedCouponSpecs` or\n        `FloatCouponSpecs` specifying the coupon payments for the payment leg\n        of the swap. If specified as a list then the length of the list should\n        be the same as the number of instruments being created. If specified as\n        a scalar, then the elements of the namedtuple must be of the same shape\n        as (or compatible to) the shape of `start_date`.\n      receive_leg: A scalar or a list of either `FixedCouponSpecs` or\n        `FloatCouponSpecs` specifying the coupon payments for the receiving leg\n        of the swap. If specified as a list then the length of the list should\n        be the same as the number of instruments being created. If specified as\n        a scalar, then the elements of the namedtuple must be of the same shape\n        as (or compatible with) the shape of `start_date`.\n      holiday_calendar: An instance of `dates.HolidayCalendar` to specify\n        weekends and holidays.\n        Default value: None in which case a holiday calendar would be created\n        with Saturday and Sunday being the holidays.\n      dtype: `tf.Dtype`. If supplied the dtype for the real variables or ops\n        either supplied to the IRS object or created by the IRS object.\n        Default value: None which maps to the default dtype inferred by\n        TensorFlow.\n      name: Python str. The name to give to the ops created by this class.\n        Default value: `None` which maps to \'interest_rate_swap\'.\n    """"""\n    self._name = name or \'interest_rate_swap\'\n\n    if holiday_calendar is None:\n      holiday_calendar = dates.create_holiday_calendar(\n          weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY)\n\n    with tf.name_scope(self._name):\n      self._dtype = dtype\n      self._start_date = dates.convert_to_date_tensor(start_date)\n      self._maturity_date = dates.convert_to_date_tensor(maturity_date)\n      self._holiday_calendar = holiday_calendar\n      self._floating_leg = None\n      self._fixed_leg = None\n      self._pay_leg = self._setup_leg(pay_leg)\n      self._receive_leg = self._setup_leg(receive_leg)\n      self._is_payer = isinstance(self._pay_leg, cs.FixedCashflowStream)\n\n  def price(self, valuation_date, market, model=None, pricing_context=None,\n            name=None):\n    """"""Returns the present value of the instrument on the valuation date.\n\n    Args:\n      valuation_date: A scalar `DateTensor` specifying the date on which\n        valuation is being desired.\n      market: A namedtuple of type `InterestRateMarket` which contains the\n        necessary information for pricing the interest rate swap.\n      model: Reserved for future use.\n      pricing_context: Additional context relevant for pricing.\n      name: Python str. The name to give to the ops created by this function.\n        Default value: `None` which maps to \'price\'.\n\n    Returns:\n      A Rank 1 `Tensor` of real type containing the modeled price of each IRS\n      contract based on the input market data.\n    """"""\n\n    name = name or (self._name + \'_price\')\n    with tf.name_scope(name):\n      valuation_date = dates.convert_to_date_tensor(valuation_date)\n      pay_cf = self._pay_leg.price(valuation_date, market, model,\n                                   pricing_context)\n      receive_cf = self._receive_leg.price(valuation_date, market, model,\n                                           pricing_context)\n      return receive_cf - pay_cf\n\n  def annuity(self, valuation_date, market, model=None):\n    """"""Returns the annuity of each swap on the vauation date.""""""\n    valuation_date = dates.convert_to_date_tensor(valuation_date)\n    return self._annuity(valuation_date, market, model, True)\n\n  def par_rate(self, valuation_date, market, model=None):\n    """"""Returns the par swap rate for the swap.""""""\n\n    valuation_date = dates.convert_to_date_tensor(valuation_date)\n    swap_annuity = self._annuity(valuation_date, market, model, False)\n    float_pv = self._floating_leg.price(valuation_date, market, model)\n\n    return float_pv / swap_annuity\n\n  @property\n  def term(self):\n    return tf.cast(self._start_date.days_until(self._maturity_date),\n                   dtype=self._dtype) / 365.\n\n  @property\n  def fixed_rate(self):\n    return self._fixed_leg.fixed_rate\n\n  @property\n  def notional(self):\n    return self._floating_leg.notional\n\n  @property\n  def is_payer(self):\n    return self._is_payer\n\n  def _setup_leg(self, leg):\n    """"""Setup swap legs.""""""\n    leg_instance = leg[0] if isinstance(leg, list) else leg\n    if isinstance(leg_instance, rc.FixedCouponSpecs):\n      leg_ = cs.FixedCashflowStream(\n          self._start_date, self._maturity_date, leg, dtype=self._dtype)\n      self._fixed_leg = leg_\n    elif isinstance(leg_instance, rc.FloatCouponSpecs):\n      leg_ = cs.FloatingCashflowStream(\n          self._start_date, self._maturity_date, leg, dtype=self._dtype)\n      self._floating_leg = leg_\n    else:\n      raise ValueError(\'Unreconized leg type.\')\n\n    return leg_\n\n  def _annuity(self, valuation_date, market, model=None, unit_notional=True):\n    """"""Returns the annuity of each swap on the vauation date.""""""\n    del valuation_date, model\n\n    if unit_notional:\n      notional = 1.\n    else:\n      notional = self._fixed_leg.notional\n\n    if self._fixed_leg is not None:\n      discount_curve = market.discount_curve\n      discount_factors = discount_curve.get_discount_factor(\n          self._fixed_leg.payment_dates)\n      return tf.math.segment_sum(\n          notional * discount_factors * self._fixed_leg.daycount_fractions,\n          self._fixed_leg.contract_index)\n    else:\n      return 0.\n'"
tf_quant_finance/experimental/instruments/interest_rate_swap_test.py,2,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for interest_rate_swap.py.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\ndates = tff.datetime\ninstruments = tff.experimental.instruments\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass InterestRateSwapTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(InterestRateSwapTest, self).setUp()\n    self.maturity_date = [(2023, 2, 8)]\n    self.start_date = [(2020, 2, 8)]\n    self.valuation_date = [(2020, 2, 8)]\n\n  def get_market(self):\n    val_date = dates.convert_to_date_tensor(self.valuation_date)\n    curve_dates = val_date + dates.years([1, 2, 3, 5, 7, 10, 30])\n\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n            0.03213901, 0.03257991\n        ], dtype=np.float64),\n        valuation_date=val_date,\n        dtype=np.float64)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n    return market\n\n  def test_irs_correctness(self):\n    dtype = np.float64\n    notional = 1.\n\n    period3m = dates.months(3)\n    period6m = dates.months(6)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=period6m, currency=\'usd\', notional=notional,\n        coupon_rate=0.03134,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period3m, reference_rate_term=period3m,\n        reset_frequency=period3m, currency=\'usd\', notional=notional,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0., coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    swap = instruments.InterestRateSwap(self.start_date, self.maturity_date,\n                                        [fix_spec], [flt_spec],\n                                        dtype=dtype)\n\n    price = self.evaluate(swap.price(self.valuation_date, self.get_market()))\n    np.testing.assert_allclose(price, 1.e-7, atol=1e-6)\n\n  def test_irs_correctness_scalar_spec(self):\n    dtype = np.float64\n    notional = 1.\n\n    period3m = dates.months(3)\n    period6m = dates.months(6)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=period6m, currency=\'usd\', notional=notional,\n        coupon_rate=0.03134,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period3m, reference_rate_term=period3m,\n        reset_frequency=period3m, currency=\'usd\', notional=notional,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0., coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    swap = instruments.InterestRateSwap(self.start_date, self.maturity_date,\n                                        fix_spec, flt_spec,\n                                        dtype=dtype)\n\n    price = self.evaluate(swap.price(self.valuation_date, self.get_market()))\n    np.testing.assert_allclose(price, 1.e-7, atol=1e-6)\n\n  def test_irs_correctness_batch(self):\n    dtype = np.float64\n    notional = 1.0\n    maturity_date = dates.convert_to_date_tensor([(2023, 2, 8), (2027, 2, 8)])\n    start_date = dates.convert_to_date_tensor([(2020, 2, 8), (2020, 2, 8)])\n\n    period3m = dates.months([3, 3])\n    period6m = dates.months([6, 6])\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=period6m, currency=\'usd\',\n        notional=notional,\n        coupon_rate=[0.03134, 0.03181],\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period3m, reference_rate_term=period3m,\n        reset_frequency=period3m, currency=\'usd\',\n        notional=notional,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0.0, coupon_multiplier=1.0,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    swap = instruments.InterestRateSwap(start_date, maturity_date,\n                                        fix_spec, flt_spec,\n                                        dtype=dtype)\n\n    price = self.evaluate(swap.price(self.valuation_date, self.get_market()))\n    np.testing.assert_allclose(price, [1.0e-7, 1.0e-7], atol=1e-6)\n\n  def test_irs_parrate(self):\n    dtype = np.float64\n    notional = 1.\n\n    period3m = dates.months(3)\n    period6m = dates.months(6)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=period6m, currency=\'usd\', notional=notional,\n        coupon_rate=0.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period3m, reference_rate_term=period3m,\n        reset_frequency=period3m, currency=\'usd\', notional=notional,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0., coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    swap = instruments.InterestRateSwap(self.start_date, self.maturity_date,\n                                        [fix_spec], [flt_spec],\n                                        dtype=dtype)\n\n    price = self.evaluate(swap.par_rate(self.valuation_date,\n                                        self.get_market()))\n    np.testing.assert_allclose(price, 0.03134, atol=1e-6)\n\n  def test_irs_parrate_many(self):\n    dtype = np.float64\n    notional = 1.\n    maturity_date = dates.convert_to_date_tensor([(2023, 2, 8), (2025, 2, 8)])\n    start_date = dates.convert_to_date_tensor([(2020, 2, 8), (2020, 2, 8)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n\n    period3m = dates.months(3)\n    period6m = dates.months(6)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=period6m, currency=\'usd\', notional=notional,\n        coupon_rate=0.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period3m, reference_rate_term=period3m,\n        reset_frequency=period3m, currency=\'usd\', notional=notional,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0., coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    swap = instruments.InterestRateSwap(start_date, maturity_date,\n                                        [fix_spec, fix_spec],\n                                        [flt_spec, flt_spec],\n                                        dtype=dtype)\n\n    curve_dates = valuation_date + dates.years([1, 2, 3, 5, 7, 10, 30])\n\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n            0.03213901, 0.03257991\n        ], dtype=np.float64),\n        valuation_date=valuation_date,\n        dtype=np.float64)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n\n    price = self.evaluate(swap.par_rate(valuation_date, market))\n    np.testing.assert_allclose(price, [0.03134, 0.03152], atol=1e-6)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/experimental/instruments/overnight_index_linked_futures.py,10,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Futures contracts on overnight rates.""""""\n\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance import datetime as dates\nfrom tf_quant_finance.experimental.instruments import rates_common as rc\n\n\nclass OvernightIndexLinkedFutures:\n  """"""Represents a collection of futures linked to an average of overnight rates.\n\n  Overnight index futures are exchange traded futures contracts where the\n  underlying reference rates are the published overnight rates such as\n  Secured Overnight Financing Rate (SOFR), Effective Fed Funds Rate (EFFR) etc.\n  These contracts are generally cash settled where the settlement price is\n  evaluated on the basis of realized reference rate values during the contract\n  reference period (or delivery period). Typically the settlement price is\n  based on componding the published daily reference rate during the delivery\n  period or based on the arithmetic average of the reference rate during the\n  delivery period.\n  An overnight index future contract on the settlement date T settles at the\n  price\n\n  `100 * (1 - R)`\n\n  If R is evaluated based on compunding the realized index values during the\n  reference period then:\n\n  `R = [Product[(1 + tau_i * r_i), 1 <= i <= N] - 1] / Sum[tau_i, 1 <= i <= N]`\n\n  If R is evaulated based on the arithmetic average of the realized index\n  during the reference period, then:\n\n  `R = Sum(r_i, 1 <= i <= N)  / N`\n\n  where `i` is the variable indexing the business days within the delivery\n  period, tau_i denotes the year fractions between successive business days\n  taking into account the appropriate daycount convention and N is the number of\n  calendar days in the delivery period. See [1] for SOFR futures on CME.\n\n  The OvernightIndexLinkedFutures class can be used to create and price multiple\n  contracts simultaneously. However all contracts within an object must be\n  priced using a common reference curve.\n\n  #### Example:\n  The following example illustrates the construction of an overnight index\n  future instrument and calculating its price.\n\n  ```python\n\n  import numpy as np\n  import tensorflow as tf\n  import tf_quant_finance as tff\n\n  dates = tff.datetime\n  instruments = tff.experimental.instruments\n\n  dtype = np.float64\n  notional = 1.\n  contract_start_date = dates.convert_to_date_tensor([(2021, 2, 8)])\n  contract_end_date = dates.convert_to_date_tensor([(2021, 5, 8)])\n  valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n\n  future = instruments.OvernightIndexLinkedFutures(\n      contract_start_date, contract_end_date, dtype=dtype)\n\n  curve_dates = valuation_date + dates.months([1, 2, 3, 12, 24, 60])\n  reference_curve = instruments.RateCurve(\n      curve_dates,\n      np.array([0.02, 0.025, 0.0275, 0.03, 0.035, 0.0325], dtype=dtype),\n      dtype=dtype)\n\n  market = instruments.InterestRateMarket(reference_curve=reference_curve,\n                                          discount_curve=None)\n\n  price = future.price(valuation_date, market)\n\n  #### References:\n  [1]: SOFR futures settlement calculation.\n  https://www.cmegroup.com/education/files/sofr-futures-settlement-calculation-methodologies.pdf\n  """"""\n\n  def __init__(self,\n               contract_start_date,\n               contract_end_date,\n               daycount_convention=None,\n               averaging_type=None,\n               contract_unit=1.,\n               holiday_calendar=None,\n               dtype=None,\n               name=None):\n    """"""Initialize the Overnight index futures object.\n\n    Args:\n      contract_start_date: A Rank 1 `DateTensor` specifying the start dates of\n        the reference period (or delivery period) of each futures contract. The\n        published overnight index during the reference period determines the\n        final settlement price of the futures contract.\n      contract_end_date: A Rank 1 `DateTensor` specifying the ending dates of\n        the reference period (or delivery period) of each futures contract.\n      daycount_convention: An optional scalar `DayCountConvention` corresponding\n        to the day count convention for the underlying rate for each contract.\n        Default value: None in which case each the day count convention equal to\n        DayCountConvention.ACTUAL_360 is used.\n      averaging_type: An optional `AverageType` corresponding to how the\n        final settlement rate is computed from daily rates.\n        Default value: None, in which case `AverageType.COMPOUNDING` is used.\n      contract_unit: An optional scalar or Rank 1 `Tensor` of real dtype\n        specifying the notional amount for the contract. If the notional is\n        entered as a scalar, it is assumed that all of the contracts have a\n        notional equal to the input value.\n        Default value: 1.0\n      holiday_calendar: An instance of `dates.HolidayCalenday` to specify\n        weekends and holidays.\n        Default value: None in which case a holiday calendar would be created\n        with Saturday and Sunday being the holidays.\n      dtype: `tf.Dtype`. If supplied the dtype for the real variables or ops\n        either supplied to the EurodollarFuture object or created by the\n        EurodollarFuture object.\n        Default value: None which maps to the default dtype inferred by\n        TensorFlow.\n      name: Python str. The name to give to the ops created by this class.\n        Default value: `None` which maps to \'eurodollar_future\'.\n    """"""\n    self._name = name or \'overnight_rate_futures\'\n\n    with tf.compat.v2.name_scope(self._name):\n      self._contract_unit = tf.convert_to_tensor(\n          contract_unit, dtype=dtype)\n      self._dtype = dtype if dtype else self._contract_unit.dtype\n      self._start_date = dates.convert_to_date_tensor(contract_start_date)\n      self._end_date = dates.convert_to_date_tensor(contract_end_date)\n      self._batch_size = self._start_date.shape[0]\n\n      if daycount_convention is None:\n        daycount_convention = rc.DayCountConvention.ACTUAL_360\n\n      if averaging_type is None:\n        averaging_type = rc.AverageType.COMPOUNDING\n\n      if holiday_calendar is None:\n        holiday_calendar = dates.create_holiday_calendar(\n            weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY)\n\n      self._daycount_convention = daycount_convention\n      self._averaging_type = averaging_type\n      self._holiday_calendar = holiday_calendar\n      self._rate_tenor = dates.day()\n\n      self._setup()\n\n  def price(self, valuation_date, market, model=None, name=None):\n    """"""Returns the price of the contract on the valuation date.\n\n    Args:\n      valuation_date: A scalar `DateTensor` specifying the date on which\n        valuation is being desired.\n      market: An object of type `InterestRateMarket` which contains the\n        necessary information for pricing the FRA instrument.\n      model: Reserved for future use.\n      name: Python string. The name to give this op.\n        Default value: `None` which maps to `price`.\n\n    Returns:\n      A Rank 1 `Tensor` of real type containing the modeled price of each\n      futures contract based on the input market data.\n    """"""\n\n    del model, valuation_date\n\n    name = name or (self._name + \'_price\')\n    with tf.name_scope(name):\n      reference_curve = market.reference_curve\n\n      df1 = reference_curve.get_discount_factor(self._accrual_start_dates)\n      df2 = reference_curve.get_discount_factor(self._accrual_end_dates)\n\n      fwd_rates = (df1 / df2 - 1.) / self._accrual_daycount\n\n      total_accrual = tf.math.segment_sum(self._daycount_fractions,\n                                          self._contract_idx)\n      if self._averaging_type == rc.AverageType.ARITHMETIC_AVERAGE:\n\n        settlement_rate = tf.math.segment_sum(\n            fwd_rates * self._daycount_fractions,\n            self._contract_idx) / total_accrual\n      else:\n        settlement_rate = (tf.math.segment_prod(\n            1. + fwd_rates * self._daycount_fractions, self._contract_idx) -\n                           1.) / total_accrual\n\n      return 100. * (1. - settlement_rate)\n\n  def _setup(self):\n    """"""Setup relevant tensors for efficient computations.""""""\n\n    reset_dates = []\n    contract_idx = []\n    daycount_fractions = []\n    for i in range(self._batch_size):\n      instr_reset_dates = dates.PeriodicSchedule(\n          start_date=self._start_date[i] + self._rate_tenor,\n          end_date=self._end_date[i],\n          tenor=self._rate_tenor,\n          holiday_calendar=self._holiday_calendar,\n          roll_convention=dates.BusinessDayConvention.FOLLOWING).dates()\n\n      # Append the start_date of the contract\n      instr_reset_dates = dates.DateTensor.concat([\n          self._start_date[i].expand_dims(axis=0),\n          instr_reset_dates], axis=0)\n\n      # Add one day beyond the end of the delivery period to compute the\n      # accrual on the last day of the delivery.\n      one_period_past_enddate = self._end_date[i] + self._rate_tenor\n      instr_reset_dates = dates.DateTensor.concat([\n          instr_reset_dates,\n          one_period_past_enddate.expand_dims(axis=0)], axis=0)\n\n      instr_daycount_fractions = rc.get_daycount_fraction(\n          instr_reset_dates[:-1], instr_reset_dates[1:],\n          self._daycount_convention, self._dtype)\n\n      reset_dates.append(instr_reset_dates[:-1])\n      daycount_fractions.append(instr_daycount_fractions)\n      contract_idx.append(tf.fill(tf.shape(instr_daycount_fractions), i))\n\n    self._reset_dates = dates.DateTensor.concat(reset_dates, axis=0)\n    self._accrual_start_dates = self._reset_dates\n    self._accrual_end_dates = self._reset_dates + self._rate_tenor\n    self._accrual_daycount = rc.get_daycount_fraction(\n        self._accrual_start_dates, self._accrual_end_dates,\n        self._daycount_convention, self._dtype)\n    self._daycount_fractions = tf.concat(daycount_fractions, axis=0)\n    self._contract_idx = tf.concat(contract_idx, axis=0)\n'"
tf_quant_finance/experimental/instruments/overnight_index_linked_futures_test.py,2,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for overnight_index_linked_futures.py.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\ndates = tff.datetime\ninstruments = tff.experimental.instruments\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass OvernightIndexLinkedFuturesTest(tf.test.TestCase,\n                                      parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_fut_compounded(self, dtype):\n    cal = dates.create_holiday_calendar(weekend_mask=dates.WeekendMask.NONE)\n\n    start_date = dates.convert_to_date_tensor([(2020, 5, 1)])\n    end_date = dates.convert_to_date_tensor([(2020, 5, 31)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n    indexfuture = instruments.OvernightIndexLinkedFutures(\n        start_date,\n        end_date,\n        holiday_calendar=cal,\n        averaging_type=instruments.AverageType.COMPOUNDING,\n        dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([1, 2, 6])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.02, 0.025, 0.015], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = tff.experimental.instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=None)\n\n    price = self.evaluate(indexfuture.price(valuation_date, market))\n    np.testing.assert_allclose(price, 98.64101997, atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_fut_averaged(self, dtype):\n    cal = dates.create_holiday_calendar(weekend_mask=dates.WeekendMask.NONE)\n\n    start_date = dates.convert_to_date_tensor([(2020, 5, 1)])\n    end_date = dates.convert_to_date_tensor([(2020, 5, 31)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n    indexfuture = instruments.OvernightIndexLinkedFutures(\n        start_date,\n        end_date,\n        averaging_type=instruments.AverageType.ARITHMETIC_AVERAGE,\n        holiday_calendar=cal,\n        dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([1, 2, 6])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.02, 0.025, 0.015], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = tff.experimental.instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=None)\n\n    price = self.evaluate(indexfuture.price(valuation_date, market))\n    np.testing.assert_allclose(price, 98.6417886, atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_fut_compounded_calendar(self, dtype):\n    cal = dates.create_holiday_calendar(\n        weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY)\n\n    start_date = dates.convert_to_date_tensor([(2020, 5, 1)])\n    end_date = dates.convert_to_date_tensor([(2020, 5, 31)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n    indexfuture = instruments.OvernightIndexLinkedFutures(\n        start_date,\n        end_date,\n        holiday_calendar=cal,\n        averaging_type=instruments.AverageType.COMPOUNDING,\n        dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([1, 2, 6])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.02, 0.025, 0.015], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = tff.experimental.instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=None)\n\n    price = self.evaluate(indexfuture.price(valuation_date, market))\n    np.testing.assert_allclose(price, 98.6332129, atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_fut_averaged_calendar(self, dtype):\n    cal = dates.create_holiday_calendar(\n        weekend_mask=dates.WeekendMask.SATURDAY_SUNDAY)\n\n    start_date = dates.convert_to_date_tensor([(2020, 5, 1)])\n    end_date = dates.convert_to_date_tensor([(2020, 5, 31)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n    indexfuture = instruments.OvernightIndexLinkedFutures(\n        start_date,\n        end_date,\n        averaging_type=instruments.AverageType.ARITHMETIC_AVERAGE,\n        holiday_calendar=cal,\n        dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([1, 2, 6])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.02, 0.025, 0.015], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = tff.experimental.instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=None)\n\n    price = self.evaluate(indexfuture.price(valuation_date, market))\n    np.testing.assert_allclose(price, 98.63396465, atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_fut_many(self, dtype):\n    cal = dates.create_holiday_calendar(weekend_mask=dates.WeekendMask.NONE)\n\n    start_date = dates.convert_to_date_tensor([(2020, 5, 1), (2020, 5, 1)])\n    end_date = dates.convert_to_date_tensor([(2020, 5, 31), (2020, 5, 31)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n    indexfuture = instruments.OvernightIndexLinkedFutures(\n        start_date,\n        end_date,\n        holiday_calendar=cal,\n        averaging_type=instruments.AverageType.COMPOUNDING,\n        dtype=dtype)\n\n    curve_dates = valuation_date + dates.months([1, 2, 6])\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([0.02, 0.025, 0.015], dtype=dtype),\n        valuation_date=valuation_date,\n        dtype=dtype)\n    market = tff.experimental.instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=None)\n\n    price = self.evaluate(indexfuture.price(valuation_date, market))\n    np.testing.assert_allclose(price, [98.64101997, 98.64101997], atol=1e-6)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/experimental/instruments/rate_curve.py,10,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Interest rate curve defintion.""""""\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance import datetime as dates\nfrom tf_quant_finance.math.interpolation import linear\n\n\nclass RateCurve(object):\n  """"""Represents an interest rate curve.""""""\n\n  def __init__(self,\n               maturity_dates,\n               rates,\n               valuation_date,\n               compounding=None,\n               interpolator=None,\n               dtype=None,\n               name=None):\n    """"""Initializes the interest rate curve.\n\n    Args:\n      maturity_dates: A `DateTensor` containing the maturity dates on which the\n        curve is specified.\n      rates: A `Tensor` of real dtype specifying the rates (or yields)\n        corresponding to the input maturities. The shape of this input should\n        match the shape of `maturity_dates`.\n      valuation_date: A scalar `DateTensor` specifying the valuation (or\n        settlement) date for the curve.\n      compounding: Optional scalar `Tensor` of dtype int32 specifying the\n        componding frequency of the input rates. Use compounding=0 for\n        continuously compounded rates. If compounding is different than 0, then\n        rates are converted to continuously componded rates to perform\n        interpolation.\n        Default value: If omitted, the default value is 0.\n      interpolator: Optional Python callable specifying the desired\n        interpolation method. It should have the following interface: yi =\n        interpolator(xi, x, y) `x`, `y`, \'xi\', \'yi\' are all `Tensors` of real\n        dtype. `x` and `y` are the sample points and values (respectively) of\n        the function to be interpolated. `xi` are the points at which the\n        interpolation is desired and `yi` are the corresponding interpolated\n        values returned by the function.\n        Default value: None in which case linear interpolation is used.\n      dtype: `tf.Dtype`. Optional input specifying the dtype of the `rates`\n        input.\n      name: Python str. The name to give to the ops created by this function.\n        Default value: `None` which maps to \'rate_curve\'.\n    """"""\n    self._name = name or \'rate_curve\'\n    with tf.compat.v1.name_scope(self._name):\n      self._dtype = dtype\n      if interpolator is None:\n        def default_interpolator(xi, x, y):\n          return linear.interpolate(xi, x, y, dtype=dtype)\n        interpolator = default_interpolator\n\n      if compounding is None:\n        compounding = 0\n\n      self._dates = dates.convert_to_date_tensor(maturity_dates)\n      self._valuation_date = dates.convert_to_date_tensor(\n          valuation_date)\n\n      self._times = self._get_time(self._dates)\n      self._rates = tf.convert_to_tensor(rates, dtype=self._dtype,\n                                         name=\'curve_rates\')\n\n      if compounding > 0:\n        self._rates = tf.where(\n            self._times > 0.,\n            tf.math.log(\n                (1. + self._rates / compounding)**(compounding * self._rates)) /\n            self._times, self._rates)\n      self._interpolator = interpolator\n\n  def get_rates(self, interpolation_dates):\n    """"""Returns interpolated rates at `interpolation_dates`.""""""\n\n    idates = dates.convert_to_date_tensor(interpolation_dates)\n    times = self._get_time(idates)\n    return self._interpolator(times, self._times, self._rates)\n\n  def get_discount_factor(self, interpolation_dates):\n    """"""Returns discount factors at `interpolation_dates`.""""""\n\n    idates = dates.convert_to_date_tensor(interpolation_dates)\n    times = self._get_time(idates)\n    return tf.math.exp(-self.get_rates(idates) * times)\n\n  def get_forward_rate(self, start_date, maturity_date, daycount_fraction=None):\n    """"""Returns the simply accrued forward rate between [start_dt, maturity_dt].\n\n    Args:\n      start_date: A `DateTensor` specifying the start of the accrual period\n        for the forward rate.\n      maturity_date: A `DateTensor` specifying the end of the accrual period\n        for the forward rate. The shape of `maturity_date` must be the same\n        as the shape of the `DateTensor` `start_date`.\n      daycount_fraction: An optional `Tensor` of real dtype specifying the\n        time between `start_date` and `maturity_date` in years computed using\n        the forward rate\'s day count basis. The shape of the input should be\n        the same as that of `start_date` and `maturity_date`.\n        Default value: `None`, in which case the daycount fraction is computed\n        using `ACTUAL_365` convention.\n\n    Returns:\n      A real tensor of same shape as the inputs containing the simply compounded\n      forward rate.\n    """"""\n    start_date = dates.convert_to_date_tensor(start_date)\n    maturity_date = dates.convert_to_date_tensor(maturity_date)\n    if daycount_fraction is None:\n      daycount_fraction = dates.daycount_actual_365_fixed(\n          start_date=start_date, end_date=maturity_date, dtype=self._dtype)\n    else:\n      daycount_fraction = tf.convert_to_tensor(daycount_fraction, self._dtype)\n    dfstart = self.get_discount_factor(start_date)\n    dfmaturity = self.get_discount_factor(maturity_date)\n    return (dfstart / dfmaturity - 1.) / daycount_fraction\n\n  @property\n  def valuation_date(self):\n    return self._valuation_date\n\n  def _get_time(self, desired_dates):\n    """"""Computes the year fraction from the curve\'s valuation date.""""""\n\n    return dates.daycount_actual_365_fixed(\n        start_date=self._valuation_date,\n        end_date=desired_dates,\n        dtype=self._dtype)\n\n\nclass RateCurveFromDiscountingFunction(RateCurve):\n  """"""Implements `RateCurve` class using discounting function.""""""\n\n  def __init__(self, maturity_dates, rates, valuation_date, discount_fn,\n               dtype):\n    super(RateCurveFromDiscountingFunction, self).__init__(\n        maturity_dates, rates, valuation_date, dtype=dtype)\n    self._discount_fn = discount_fn\n\n  def get_discount_factor(self, interpolation_dates):\n    return self._discount_fn(interpolation_dates)\n\n\ndef ratecurve_from_discounting_function(discount_fn, dtype=None):\n  """"""Returns `RateCurve` object using the supplied function for discounting.\n\n  Args:\n    discount_fn: A python callable which takes a `DateTensor` as an input and\n      returns the corresponding discount factor as an output.\n    dtype: `tf.Dtype`. Optional input specifying the dtype of the real tensors\n      and ops.\n\n  Returns:\n    An object of class `RateCurveFromDiscountingFunction` which uses the\n    supplied function for discounting.\n  """"""\n\n  dtype = dtype or tf.constant(0.0).dtype\n  pseudo_maturity_dates = dates.convert_to_date_tensor([(2020, 1, 1)])\n  pseudo_rates = tf.convert_to_tensor([0.0], dtype=dtype)\n  pseudo_valuation_date = dates.convert_to_date_tensor((2020, 1, 1))\n\n  return RateCurveFromDiscountingFunction(\n      pseudo_maturity_dates, pseudo_rates, pseudo_valuation_date,\n      discount_fn, dtype)\n'"
tf_quant_finance/experimental/instruments/rate_curve_test.py,3,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for rate_curve.py.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\ndates = tff.datetime\ninstruments = tff.experimental.instruments\n\n\n@test_util.run_all_in_graph_and_eager_modes\ndef get_curve(dtype, ext_discount):\n  valuation_date = dates.convert_to_date_tensor([(2020, 1, 1)])\n\n  curve_dates = valuation_date + dates.years([0, 1, 2])\n  curve_rates = np.array([0.0, 0.01, 0.02], dtype=np.float64)\n\n  def my_discount_function(idates):\n    idates = dates.convert_to_date_tensor(idates)\n    curve_times = dates.daycount_actual_365_fixed(\n        start_date=valuation_date, end_date=curve_dates, dtype=dtype)\n    itimes = dates.daycount_actual_365_fixed(\n        start_date=valuation_date, end_date=idates, dtype=dtype)\n    irates = tff.math.interpolation.linear.interpolate(\n        itimes, curve_times, curve_rates, dtype=dtype)\n    return tf.math.exp(-irates * itimes)\n\n  if ext_discount:\n    fn = my_discount_function\n    return instruments.ratecurve_from_discounting_function(fn, dtype)\n  else:\n    curve = instruments.RateCurve(\n        curve_dates,\n        curve_rates,\n        valuation_date=valuation_date,\n        dtype=np.float64)\n    return curve\n\n\nclass RateCurveTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64, False),\n  )\n  def test_rate_curve(self, dtype, ext_discount):\n    curve = get_curve(dtype, ext_discount)\n    values = self.evaluate(\n        curve.get_rates([(2020, 6, 1), (2021, 6, 1), (2025, 1, 1)]))\n    np.testing.assert_allclose(values, [0.0041530054644809, 0.0141369863013699,\n                                        0.02], atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64, False),\n      (\'DoublePrecisionExternalDiscount\', np.float64, True),\n  )\n  def test_discount_factor(self, dtype, ext_discount):\n    curve = get_curve(dtype, ext_discount)\n    values = self.evaluate(\n        curve.get_discount_factor([(2020, 6, 1), (2021, 6, 1), (2025, 1, 1)]))\n    np.testing.assert_allclose(values, [0.9982720239040115, 0.9801749825461190,\n                                        0.9047382632042100], atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64, False),\n      (\'DoublePrecisionExternalDiscount\', np.float64, True),\n  )\n  def test_fwd_rates(self, dtype, ext_discount):\n    start_dates = [(2020, 6, 1), (2021, 6, 1), (2025, 1, 1)]\n    maturity_dates = [(2020, 7, 1), (2021, 8, 1), (2025, 3, 1)]\n    curve = get_curve(dtype, ext_discount)\n    values = self.evaluate(\n        curve.get_forward_rate(start_dates, maturity_dates))\n    np.testing.assert_allclose(values, [0.0091291063032444, 0.0300477964192536,\n                                        0.0200323636336040], atol=1e-6)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/experimental/instruments/rates_common.py,3,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Common rates related utilities.""""""\n\nimport collections\nimport enum\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance import datetime as dates\n\n\nInterestRateMarket = collections.namedtuple(\n    \'InterestRateMarket\',\n    [\n        # Instance of class RateCurve. The curve used for computing the forward\n        # expectation of Libor rate.\n        \'reference_curve\',\n        # Instance of class RateCurve. The curve used for discounting cashflows.\n        \'discount_curve\',\n        # Scalar of real dtype containing the past fixing of libor rate\n        \'libor_rate\',\n        # Scalar of real dtype containing the past fixing of swap rate\n        \'swap_rate\',\n        # Instance of class VolatiltyCube. Market implied black volatilities.\n        \'volatility_curve\'\n    ])\nInterestRateMarket.__new__.__defaults__ = (None, None, None, None, None)\n\n# TODO(b/151954834): Change to `attrs` or `dataclasses`\nFixedCouponSpecs = collections.namedtuple(\n    \'FixedCouponSpecs\',\n    [\n        # Scalar or rank 1 `dates.PeriodTensor` specifying the frequency of\n        # the cashflow payments\n        \'coupon_frequency\',\n        # String specifying the currency of cashflows\n        \'currency\',\n        # Scalar or rank 1 `Tensor` of real dtype specifying the notional for\n        # the payments\n        \'notional\',\n        # Scalar or rank 1 `Tensor` of real dtype specifying the coupon rate\n        \'coupon_rate\',\n        # Scalar of type `DayCountConvention` specifying the applicable\n        # daycount convention\n        \'daycount_convention\',\n        # Scalar of type `BusinessDayConvention` specifying how dates are rolled\n        # if they fall on holidays\n        \'businessday_rule\'\n    ])\n\nFloatCouponSpecs = collections.namedtuple(\n    \'FloatCouponSpecs\',\n    [\n        # Scalar or rank 1 `dates.PeriodTensor` specifying the frequency of\n        # the cashflow payments\n        \'coupon_frequency\',\n        # Scalar or rank 1 `dates.PeriodTensor` specifying the term of the\n        # underlying rate which determines the coupon payment\n        \'reference_rate_term\',\n        # Scalar or rank 1 `dates.PeriodTensor` specifying the frequency with\n        # which the underlying rate resets\n        \'reset_frequency\',\n        # String specifying the currency of cashflows\n        \'currency\',\n        # Scalar or rank 1 `Tensor` of real dtype specifying the notional for\n        # the payments\n        \'notional\',\n        # Scalar of type `DayCountConvention` specifying the daycount\n        # convention of the underlying rate\n        \'daycount_convention\',\n        # Scalar of type `BusinessDayConvention` specifying how dates are rolled\n        # if they fall on holidays\n        \'businessday_rule\',\n        # Scalar of real dtype specifying the fixed basis (in decimals)\n        \'coupon_basis\',\n        # Scalar of real dtype\n        \'coupon_multiplier\'\n    ])\n\nCMSCouponSpecs = collections.namedtuple(\n    \'CMSCouponSpecs\',\n    [\n        # Scalar of type `dates.PeriodTensor` specifying the frequency of\n        # the cashflow payments\n        \'coupon_frequency\',\n        # Scalar `dates.PeriodTensor` specifying the tenor of the CMS rate\n        \'tenor\',\n        # Scalar of type `instruments.FloatCouponSpecs` specifying the floating\n        # leg of the CMS\n        \'float_leg\',\n        # Scalar of type `instruments.FixedCouponSpecs` specifying the fixed\n        # leg of the CMS\n        \'fixed_leg\',\n        # Scalar of real dtype specifying the notional for the payments\n        \'notional\',\n        # Scalar of type `DayCountConvention` specifying the daycount\n        # convention of the underlying rate\n        \'daycount_convention\',\n        # Scalar of real dtype specifying the fixed basis (in decimals)\n        \'coupon_basis\',\n        # Scalar of real dtype\n        \'coupon_multiplier\',\n        # Scalar of type `BusinessDayConvention` specifying how dates are rolled\n        # if they fall on holidays\n        \'businessday_rule\'\n    ])\n\n\nclass AverageType(enum.Enum):\n  """"""Averaging types.""""""\n  # Componded rate\n  COMPOUNDING = 1\n\n  # Arthmatic average\n  ARITHMETIC_AVERAGE = 2\n\n\nclass DayCountConvention(enum.Enum):\n  """"""Day count conventions for accrual.""""""\n  # Actual/360 day count basis\n  ACTUAL_360 = 1\n\n  # Acutal/365 day count basis\n  ACTUAL_365 = 2\n\n  # 30/360 ISDA day count basis\n  THIRTY_360_ISDA = 3\n\n\nclass RateIndexType(enum.Enum):\n  """"""Interest rate indexes.""""""\n  # LIBOR rates\n  LIBOR = 1\n\n  # Swap rates\n  SWAP = 2\n\n\nclass InterestRateModelType(enum.Enum):\n  """"""Models for pricing interest rate derivatives.""""""\n  # Lognormal model for the underlying rate\n  LOGNORMAL_RATE = 1\n\n  # Normal model for the underlying rate\n  NORMAL_RATE = 2\n\n  # Smile consistent replication (lognormal vols)\n  LOGNORMAL_SMILE_CONSISTENT_REPLICATION = 3\n\n  # Smile consistent replication (normal vols)\n  NORMAL_SMILE_CONSISTENT_REPLICATION = 4\n\n\ndef elapsed_time(date_1, date_2, dtype):\n  """"""Computes elapsed time between two date tensors.""""""\n  days_in_year = 365.\n  return tf.cast(date_1.days_until(date_2), dtype=dtype) / (\n      days_in_year)\n\n\n# TODO(b/149644030): Use daycounts.py for this.\ndef get_daycount_fraction(date_start, date_end, convention, dtype):\n  """"""Return the day count fraction between two dates.""""""\n  if convention == DayCountConvention.ACTUAL_365:\n    return dates.daycount_actual_365_fixed(\n        start_date=date_start, end_date=date_end, dtype=dtype)\n  elif convention == DayCountConvention.ACTUAL_360:\n    return dates.daycount_actual_360(\n        start_date=date_start, end_date=date_end, dtype=dtype)\n  elif convention == DayCountConvention.THIRTY_360_ISDA:\n    return dates.daycount_thirty_360_isda(\n        start_date=date_start, end_date=date_end, dtype=dtype)\n  else:\n    raise ValueError(\'Daycount convention not implemented.\')\n\n\ndef get_rate_index(market,\n                   valuation_date,\n                   rate_type=None,\n                   currency=None,\n                   dtype=None):\n  """"""Return the relevant rate from the market data.""""""\n  del currency\n  if rate_type == RateIndexType.LIBOR:\n    rate = market.libor_rate or tf.zeros(valuation_date.shape, dtype=dtype)\n  elif rate_type == RateIndexType.SWAP:\n    rate = market.swap_rate or tf.zeros(valuation_date.shape, dtype=dtype)\n  else:\n    raise ValueError(\'Unrecognized rate type.\')\n  return rate\n\n\ndef get_implied_volatility_data(market,\n                                valuation_date=None,\n                                volatility_type=None,\n                                currency=None):\n  """"""Return the implied colatility date from the market data.""""""\n  del valuation_date, volatility_type, currency\n  vol_date = market.volatility_curve\n  return vol_date\n'"
tf_quant_finance/experimental/instruments/swaption.py,4,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""European swaptions.""""""\n\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance import black_scholes\nfrom tf_quant_finance import datetime as dates\nfrom tf_quant_finance.experimental.instruments import rates_common as rc\n\n\nclass Swaption:\n  """"""Represents a batch of European Swaptions.\n\n  A European Swaption is a contract that gives the holder an option to enter a\n  swap contract at a future date at a prespecified fixed rate. A swaption that\n  grants the holder to pay fixed rate and receive floating rate is called a\n  payer swaption while the swaption that grants the holder to receive fixed and\n  pay floating payments is called the receiver swaption. Typically the start\n  date (or the inception date) of the swap concides with the expiry of the\n  swaption [1].\n\n  The Swaption class can be used to create and price multiple swaptions\n  simultaneously. However all swaptions within an object must be priced using\n  common reference/discount curve.\n\n  ### Example:\n  The following example illustrates the construction of an European swaption\n  and calculating its price using the Black model.\n\n  ```python\n  import numpy as np\n  import tensorflow as tf\n  import tf_quant_finance as tff\n  dates = tff.datetime\n  instruments = tff.experimental.instruments\n  rc = tff.experimental.instruments.rates_common\n\n  dtype = np.float64\n  notional = 1.e6\n  maturity_date = dates.convert_to_date_tensor([(2025, 2, 8)])\n  start_date = dates.convert_to_date_tensor([(2022, 2, 8)])\n  expiry_date = dates.convert_to_date_tensor([(2022, 2, 8)])\n  valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n\n  period3m = dates.months(3)\n  period6m = dates.months(6)\n  fix_spec = instruments.FixedCouponSpecs(\n      coupon_frequency=period6m, currency=\'usd\', notional=notional,\n      coupon_rate=0.03134,\n      daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n      businessday_rule=dates.BusinessDayConvention.NONE)\n  flt_spec = instruments.FloatCouponSpecs(\n      coupon_frequency=period3m, reference_rate_term=period3m,\n      reset_frequency=period3m, currency=\'usd\', notional=notional,\n      businessday_rule=dates.BusinessDayConvention.NONE,\n      coupon_basis=0., coupon_multiplier=1.,\n      daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n  swap = instruments.InterestRateSwap(start_date, maturity_date,\n                                      [fix_spec], [flt_spec],\n                                      dtype=dtype)\n  swaption = instruments.Swaption(swap, expiry_date, dtype=dtype)\n\n  curve_dates = valuation_date + dates.years([1, 2, 3, 5, 7, 10, 30])\n\n  reference_curve = instruments.RateCurve(\n      curve_dates,\n      np.array([\n          0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n          0.03213901, 0.03257991\n      ], dtype=np.float64),\n      valuation_date=valuation_date,\n      dtype=np.float64)\n  market = instruments.InterestRateMarket(\n      reference_curve=reference_curve, discount_curve=reference_curve)\n\n  price = swaption.price(\n          valuation_date,\n          market,\n          model=instruments.InterestRateModelType.LOGNORMAL_RATE,\n          pricing_context=0.5))\n  # Expected result: 24145.254011\n  ```\n\n  ### References:\n  [1]: Leif B.G. Andersen and Vladimir V. Piterbarg. Interest Rate Modeling,\n      Volume I: Foundations and Vanilla Models. Chapter 5. 2010.\n  """"""\n\n  def __init__(self,\n               swap,\n               expiry_date=None,\n               dtype=None,\n               name=None):\n    """"""Initialize a batch of European swaptions.\n\n    Args:\n      swap: An instance of `InterestRateSwap` specifying the interest rate\n        swaps underlying the swaptions. The batch size of the swaptions being\n        created would be the same as the batch size of the `swap`.\n      expiry_date: An optional rank 1 `DateTensor` specifying the expiry dates\n        for each swaption. The shape of the input should be the same as the\n        batch size of the `swap` input.\n        Default value: None in which case the option expity date is the same as\n        the start date of each underlying swap.\n      dtype: `tf.Dtype`. If supplied the dtype for the real variables or ops\n        either supplied to the Swaption object or created by the Swaption\n        object.\n        Default value: None which maps to the default dtype inferred by\n        TensorFlow.\n      name: Python str. The name to give to the ops created by this class.\n        Default value: `None` which maps to \'swaption\'.\n    """"""\n    self._name = name or \'swaption\'\n\n    with tf.name_scope(self._name):\n      self._dtype = dtype\n      self._expiry_date = dates.convert_to_date_tensor(expiry_date)\n      self._swap = swap\n\n  def price(self, valuation_date, market, model=None, pricing_context=None,\n            name=None):\n    """"""Returns the present value of the swaption on the valuation date.\n\n    Args:\n      valuation_date: A scalar `DateTensor` specifying the date on which\n        valuation is being desired.\n      market: A namedtuple of type `InterestRateMarket` which contains the\n        necessary information for pricing the FRA instrument.\n      model: An optional input of type `InterestRateModelType` to specify which\n        model to use for pricing.\n        Default value: `None` in which case LOGNORMAL_RATE model is used.\n      pricing_context: An optional input to provide additional parameters (such\n        as model parameters) relevant for pricing.\n      name: Python str. The name to give to the ops created by this function.\n        Default value: `None` which maps to \'price\'.\n\n    Returns:\n      A Rank 1 `Tensor` of real type containing the modeled price of each IRS\n      contract based on the input market data.\n\n    Raises:\n      ValueError: If an unsupported model is supplied to the function.\n    """"""\n    model = model or rc.InterestRateModelType.LOGNORMAL_RATE\n    name = name or (self._name + \'_price\')\n    with tf.name_scope(name):\n      swap_annuity = self._swap.annuity(valuation_date, market, model)\n      forward_swap_rate = self._swap.par_rate(valuation_date, market, model)\n      strike = self._swap.fixed_rate\n\n      expiry_time = dates.daycount_actual_365_fixed(\n          start_date=valuation_date,\n          end_date=self._expiry_date,\n          dtype=self._dtype)\n      # Ideally we would like the model to tell us how to price the option.\n      # The default for European swaptions should be SABR, but the current\n      # implementation needs some work.\n      if model == rc.InterestRateModelType.LOGNORMAL_RATE:\n        option_value = self._price_lognormal_rate(market, pricing_context,\n                                                  forward_swap_rate,\n                                                  strike, expiry_time)\n      else:\n        raise ValueError(\'Unsupported model.\')\n\n      return self._swap.notional[-1] * swap_annuity * option_value\n\n  def _price_lognormal_rate(self, market, pricing_context,\n                            forward_swap_rate, strike,\n                            expiry_time):\n    """"""Price the swaption using lognormal model for rate.""""""\n\n    # Ideally we would like the model to tell what piece of market data is\n    # needed. For example, a Black lognormal model will tell us to pick\n    # lognormal vols and Black normal model should tell us to pick normal\n    # vols.\n    if pricing_context is None:\n      swaption_vol_cube = rc.get_implied_volatility_data(market)\n      term = self._swap.swap_term\n      black_vols = swaption_vol_cube.interpolate(self._expiry_date, strike,\n                                                 term)\n    else:\n      black_vols = tf.convert_to_tensor(pricing_context, dtype=self._dtype)\n    return black_scholes.option_price(volatilities=black_vols,\n                                      strikes=strike,\n                                      expiries=expiry_time,\n                                      forwards=forward_swap_rate,\n                                      is_call_options=self._swap.is_payer,\n                                      dtype=self._dtype\n                                      )\n'"
tf_quant_finance/experimental/instruments/swaption_test.py,2,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for swaption.py.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\ndates = tff.datetime\ninstruments = tff.experimental.instruments\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass SwaptionTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_swaption_correctness(self, dtype):\n    notional = 1.e6\n    maturity_date = dates.convert_to_date_tensor([(2025, 2, 8)])\n    start_date = dates.convert_to_date_tensor([(2022, 2, 8)])\n    expiry_date = dates.convert_to_date_tensor([(2022, 2, 8)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n\n    period3m = dates.months(3)\n    period6m = dates.months(6)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=period6m, currency=\'usd\', notional=notional,\n        coupon_rate=0.03134,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period3m, reference_rate_term=period3m,\n        reset_frequency=period3m, currency=\'usd\', notional=notional,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0., coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    swap = instruments.InterestRateSwap(start_date, maturity_date,\n                                        [fix_spec], [flt_spec],\n                                        dtype=dtype)\n    swaption = instruments.Swaption(swap, expiry_date, dtype=dtype)\n\n    curve_dates = valuation_date + dates.years(\n        [1, 2, 3, 5, 7, 10, 30])\n\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n            0.03213901, 0.03257991\n        ], dtype=np.float64),\n        valuation_date=valuation_date,\n        dtype=np.float64)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n\n    price = self.evaluate(\n        swaption.price(\n            valuation_date,\n            market,\n            model=instruments.InterestRateModelType.LOGNORMAL_RATE,\n            pricing_context=0.5))\n    np.testing.assert_allclose(price, 24145.254011, atol=1e-6)\n\n  @parameterized.named_parameters(\n      (\'DoublePrecision\', np.float64),\n  )\n  def test_swaption_many(self, dtype):\n    notional = 1.e6\n    maturity_date = dates.convert_to_date_tensor([(2025, 2, 8), (2025, 2, 8)])\n    start_date = dates.convert_to_date_tensor([(2022, 2, 8), (2022, 2, 8)])\n    expiry_date = dates.convert_to_date_tensor([(2022, 2, 8), (2022, 2, 8)])\n    valuation_date = dates.convert_to_date_tensor([(2020, 2, 8)])\n\n    period3m = dates.months(3)\n    period6m = dates.months(6)\n    fix_spec = instruments.FixedCouponSpecs(\n        coupon_frequency=period6m, currency=\'usd\', notional=notional,\n        coupon_rate=0.03134,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365,\n        businessday_rule=dates.BusinessDayConvention.NONE)\n    flt_spec = instruments.FloatCouponSpecs(\n        coupon_frequency=period3m, reference_rate_term=period3m,\n        reset_frequency=period3m, currency=\'usd\', notional=notional,\n        businessday_rule=dates.BusinessDayConvention.NONE,\n        coupon_basis=0., coupon_multiplier=1.,\n        daycount_convention=instruments.DayCountConvention.ACTUAL_365)\n\n    swap = instruments.InterestRateSwap(start_date, maturity_date,\n                                        [fix_spec, fix_spec],\n                                        [flt_spec, flt_spec],\n                                        dtype=dtype)\n    swaption = instruments.Swaption(swap, expiry_date, dtype=dtype)\n\n    curve_dates = valuation_date + dates.years([1, 2, 3, 5, 7, 10, 30])\n\n    reference_curve = instruments.RateCurve(\n        curve_dates,\n        np.array([\n            0.02834814, 0.03077457, 0.03113739, 0.03130794, 0.03160892,\n            0.03213901, 0.03257991\n        ], dtype=np.float64),\n        valuation_date=valuation_date,\n        dtype=np.float64)\n    market = instruments.InterestRateMarket(\n        reference_curve=reference_curve, discount_curve=reference_curve)\n\n    price = self.evaluate(\n        swaption.price(\n            valuation_date,\n            market,\n            model=instruments.InterestRateModelType.LOGNORMAL_RATE,\n            pricing_context=[0.5, 0.5]))\n    np.testing.assert_allclose(price, [24145.254011, 24145.254011], atol=1e-6)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/experimental/lsm_algorithm/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""LSM algorithm methods.""""""\n\nfrom tf_quant_finance.experimental.lsm_algorithm.lsm import least_square_mc\nfrom tf_quant_finance.experimental.lsm_algorithm.lsm import make_polynomial_basis\nfrom tf_quant_finance.experimental.lsm_algorithm.lsm_v2 import least_square_mc as least_square_mc_v2\nfrom tf_quant_finance.experimental.lsm_algorithm.lsm_v2 import make_polynomial_basis as make_polynomial_basis_v2\nfrom tf_quant_finance.experimental.lsm_algorithm.payoff import make_basket_put_payoff\n\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    ""least_square_mc"",\n    ""least_square_mc_v2"",\n    ""make_basket_put_payoff"",\n    ""make_polynomial_basis"",\n    ""make_polynomial_basis_v2"",\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/experimental/lsm_algorithm/lsm.py,52,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Implementation of the regression MC algorithm of Longstaff and Schwartz.""""""\n\nimport collections\nimport tensorflow.compat.v2 as tf\n\n\nLsmLoopVars = collections.namedtuple(\n    \'LsmLoopVars\',\n    [\n        # int32. The LSM algorithm iterates backwards over times where an option\n        # can be exercised, this tracks progress.\n        \'exercise_index\',\n        # [num_samples, num_exercise_times, payoff_dim] shaped tensor. Tracks\n        # the optimal cashflow of each sample path for each payoff dimension at\n        # each exercise time.\n        \'cashflow\'\n    ])\n\n\ndef make_polynomial_basis(degree):\n  """"""Produces a callable from samples to polynomial basis for use in regression.\n\n  The output callable accepts a `Tensor` `X` of shape `[num_samples, dim]`,\n  computes a centered value `Y = X - mean(X, axis=0)` and outputs a `Tensor`\n  of shape `[degree * dim, num_samples]`, where\n  ```\n  Z[i*j, k] = X[k, j]**(degree - i) * X[k, j]**i, 0<=i<degree - 1, 0<=j<dim\n  ```\n  For example, if `degree` and `dim` are both equal to 2, the polynomial basis\n  is `1, X, X**2, Y, Y**2, X * Y, X**2 * Y, X * Y**2`, where `X` and `Y` are\n  the spatial axes.\n\n  #### Example\n  ```python\n  basis = make_polynomial_basis(2)\n  x = [1.0, 2.0, 3.0, 4.0]\n  x = tf.expand_dims(x, -1)\n  basis(x)\n  # Expected result:\n  [[ 1.0, 1.0, 1.0, 1.0], [-1.5, -0.5, 0.5, 1.5]]\n  ```\n\n  Args:\n    degree: An `int32` scalar `Tensor`. The degree of the desired polynomial\n      basis.\n\n  Returns:\n    A callable from `Tensor`s of shape `[num_samples, dim]` to `Tensor`s of\n    shape `[degree * dim, num_samples]`.\n\n  Raises:\n    ValueError: If `degree` is less than `1`.\n  """"""\n  tf.debugging.assert_greater_equal(\n      degree, 0,\n      message=\'Degree of polynomial basis can not be negative.\')\n  def basis(sample_paths):\n    """"""Computes polynomial basis expansion at the given sample points.\n\n    Args:\n      sample_paths: A `Tensor`s of either `flot32` or `float64` dtype and of\n        shape `[num_samples, dim]` where `dim` has to be statically known.\n\n    Returns:\n      A `Tensor`s of shape `[degree * dim, num_samples]`.\n    """"""\n    samples = tf.convert_to_tensor(sample_paths)\n    dim = samples.shape.as_list()[-1]\n    grid = tf.range(0, degree + 1, dtype=samples.dtype)\n\n    samples_centered = samples - tf.math.reduce_mean(samples, axis=0)\n    samples_centered = tf.expand_dims(samples_centered, -2)\n    grid = tf.meshgrid(*(dim * [grid]))\n    grid = tf.reshape(tf.stack(grid, -1), [-1, dim])\n    # Shape [num_samples, degree * dim]\n    basis_expansion = tf.reduce_prod(samples_centered**grid, -1)\n    return  tf.transpose(basis_expansion)\n  return basis\n\n\ndef least_square_mc(sample_paths,\n                    exercise_times,\n                    payoff_fn,\n                    basis_fn,\n                    discount_factors=None,\n                    dtype=None,\n                    name=None):\n  """"""Values Amercian style options using the LSM algorithm.\n\n  The Least-Squares Monte-Carlo (LSM) algorithm is a Monte-Carlo approach to\n  valuation of American style options. Using the sample paths of underlying\n  assets, and a user supplied payoff function it attempts to find the optimal\n  exercise point along each sample path. With optimal exercise points known,\n  the option is valued as the average payoff assuming optimal exercise\n  discounted to present value.\n\n  #### Example. American put option price through Monte Carlo\n  ```python\n  # Let the underlying model be a Black-Scholes process\n  # dS_t / S_t = rate dt + sigma**2 dW_t, S_0 = 1.0\n  # with `rate = 0.1`, and volatility `sigma = 1.0`.\n  # Define drift and volatility functions for log(S_t)\n  rate = 0.1\n  def drift_fn(_, x):\n    return rate - tf.ones_like(x) / 2.\n  def vol_fn(_, x):\n    return tf.expand_dims(tf.ones_like(x), -1)\n  # Use Euler scheme to propagate 100000 paths for 1 year into the future\n  times = np.linspace(0., 1, num=50)\n  num_samples = 100000\n  log_paths = tf.function(tff.models.euler_sampling.sample)(\n          dim=1,\n          drift_fn=drift_fn, volatility_fn=vol_fn,\n          random_type=tff.math.random.RandomType.PSEUDO_ANTITHETIC,\n          times=times, num_samples=num_samples, seed=42, time_step=0.01)\n  # Compute exponent to get samples of `S_t`\n  paths = tf.math.exp(log_paths)\n  # American put option price for strike 1.1 and expiry 1 (assuming actual day\n  # count convention and no settlement adjustment)\n  strike = [1.1]\n  exercise_times = tf.range(times.shape[-1])\n  discount_factors = tf.exp(-rate * times)\n  payoff_fn = make_basket_put_payoff(strike)\n  basis_fn = make_polynomial_basis(10)\n  lsm_price(paths, exercise_times, payoff_fn, basis_fn,\n            discount_factors=discount_factors)\n  # Expected value: [0.397]\n  # European put option price\n  tff.black_scholes.option_price(volatilities=[1], strikes=strikes,\n                                 expiries=[1], spots=[1.],\n                                 discount_factors=discount_factors[-1],\n                                 is_call_options=False,\n                                 dtype=tf.float64)\n  # Expected value: [0.379]\n  ```\n  #### References\n\n  [1] Longstaff, F.A. and Schwartz, E.S., 2001. Valuing American options by\n  simulation: a simple least-squares approach. The review of financial studies,\n  14(1), pp.113-147.\n\n  Args:\n    sample_paths: A `Tensor` of shape `[num_samples, num_times, dim]`, the\n      sample paths of the underlying ito process of dimension `dim` at\n      `num_times` different points.\n    exercise_times: An `int32` `Tensor` of shape `[num_exercise_times]`.\n      Contents must be a subset of the integers `[0,...,num_times - 1]`,\n      representing the ticks at which the option may be exercised.\n    payoff_fn: Callable from a `Tensor` of shape `[num_samples, num_times, dim]`\n      and an integer scalar positive `Tensor` (representing the current time\n      index) to a `Tensor` of shape `[num_samples, payoff_dim]`\n      of the same dtype as `samples`. The output represents the payout resulting\n      from exercising the option at time `S`. The `payoff_dim` allows multiple\n      options on the same underlying asset (i.e., `samples`) to be valued in\n      parallel.\n    basis_fn: Callable from a `Tensor` of shape `[num_samples, dim]` to a\n      `Tensor` of shape `[basis_size, num_samples]` of the same dtype as\n      `samples`. The result being the design matrix used in regression of the\n      continuation value of options.\n    discount_factors: A `Tensor` of shape `[num_exercise_times]` and the same\n      `dtype` as `samples`, the k-th element of which represents the discount\n      factor at time tick `k`.\n      Default value: `None` which maps to a one-`Tensor` of the same `dtype`\n        as `samples` and shape `[num_exercise_times]`.\n    dtype: Optional `dtype`. Either `tf.float32` or `tf.float64`. The `dtype`\n      If supplied, represents the `dtype` for the input and output `Tensor`s.\n      Default value: `None`, which means that the `dtype` inferred by TensorFlow\n      is used.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` which is mapped to the default name\n      \'least_square_mc\'.\n  Returns:\n    A `Tensor` of shape `[num_samples, payoff_dim]` of the same dtype as\n    `samples`.\n  """"""\n  with tf.compat.v1.name_scope(name, default_name=\'least_square_mc\',\n                               values=[sample_paths, exercise_times]):\n    # Conversion of the inputs to tensors\n    sample_paths = tf.convert_to_tensor(sample_paths,\n                                        dtype=dtype, name=\'sample_paths\')\n    exercise_times = tf.convert_to_tensor(exercise_times, name=\'exercise_times\')\n    num_times = exercise_times.shape.as_list()[-1]\n    if discount_factors is None:\n      discount_factors = tf.ones(shape=exercise_times.shape,\n                                 dtype=sample_paths.dtype,\n                                 name=\'discount_factors\')\n    else:\n      discount_factors = tf.convert_to_tensor(\n          discount_factors, dtype=dtype, name=\'discount_factors\')\n    discount_factors = tf.concat([[1], discount_factors], -1)\n    # Initialise cashflow as the payoff at final sample.\n    tick = exercise_times[num_times - 1]\n    # Calculate the payoff of each path if exercised now. Shape\n    # [num_samples, payoff_dim]\n    exercise_value = payoff_fn(sample_paths, tick)\n    zeros = tf.zeros(exercise_value.shape + [num_times - 1],\n                     dtype=exercise_value.dtype)\n    exercise_value = tf.expand_dims(exercise_value, -1)\n\n    # Shape [num_samples, payoff_dim, num_exercise]\n    cashflow = tf.concat([zeros, exercise_value], -1)\n    # Starting state for loop iteration.\n    lsm_loop_vars = LsmLoopVars(exercise_index=num_times - 1, cashflow=cashflow)\n    def loop_body(exercise_index, cashflow):\n      return _lsm_loop_body(sample_paths, exercise_times, discount_factors,\n                            payoff_fn, basis_fn,\n                            num_times, exercise_index, cashflow)\n\n    loop_value = tf.while_loop(lsm_loop_cond, loop_body, lsm_loop_vars,\n                               maximum_iterations=num_times)\n    present_values = continuation_value_fn(\n        loop_value.cashflow, discount_factors, 0)\n    return tf.math.reduce_mean(present_values, axis=0)\n\n\ndef lsm_loop_cond(exercise_index, cashflow):\n  """"""Condition to exit a countdown loop when the exercise date hits zero.""""""\n  del cashflow\n  return exercise_index > 0\n\n\ndef continuation_value_fn(cashflow, discount_factors, exercise_index):\n  """"""Returns the discounted value of the right hand part of the cashflow tensor.\n\n  Args:\n    cashflow: A real `Tensor` of shape\n      `[num_samples, payoff_dim, num_exercise]`. Tracks the optimal cashflow of\n       each sample path for each payoff dimension at each exercise time.\n    discount_factors: A `Tensor` of shape `[num_exercise_times + 1]` and the\n      same `dtype` as `samples`, the `k`-th element of which represents the\n      discount factor at time tick `k + 1`. `discount_factors[0]` is `1` which\n      is the discount factor at time `0`.\n    exercise_index: An integer scalar `Tensor` representing the index of the\n      exercise time of interest. Should be less than `num_exercise_times`.\n\n  Returns:\n    A `[num_samples, payoff_dim]` `Tensor` whose entries represent the sum of\n    those elements to the right of `exercise_index` in `cashflow`, discounted to\n    the time indexed by `exercise_index`. When `exercise_index` is zero, the\n    return represents the sum of the cashflow discounted to present value for\n    each sample path.\n  """"""\n  total_discount_factors = (discount_factors[exercise_index + 1:]\n                            / discount_factors[exercise_index])\n  return tf.math.reduce_sum(\n      cashflow[..., exercise_index:] * total_discount_factors, axis=2)\n\n\ndef expected_exercise_fn(design, continuation_value, exercise_value):\n  """"""Returns the expected continuation value for each path.\n\n  Args:\n    design: A real `Tensor` of shape `[basis_size, num_samples]`.\n    continuation_value: A `Tensor` of shape `[num_samples, payoff_dim]` and of\n      the same dtype as `design`. The optimal value of the option conditional on\n      not exercising now or earlier, taking future information into account.\n    exercise_value: A `Tensor` of the same shape and dtype as\n      `continuation_value`. Value of the option if exercised immideately at\n      the current time\n\n  Returns:\n    A `Tensor` of the same shape and dtype as `continuation_value` whose\n    `(n, v)`-th entry represents the expected continuation value of sample path\n    `n` under the `v`-th payoff scheme.\n  """"""\n  # We wish to value each option under different payoffs, expressed through a\n  # multidimensional payoff function. While the basis calculated from the sample\n  # paths is the same for each payoff, the LSM algorithm requires us to fit a\n  # regression model only on the in-the-money paths, which are payoff dependent,\n  # hence we create multiple copies of the regression design (basis) matrix and\n  # zero out rows for out of the money paths under each payoff.\n  batch_design = tf.broadcast_to(\n      tf.expand_dims(design, -1), design.shape + [continuation_value.shape[-1]])\n  mask = tf.cast(exercise_value > 0, design.dtype)\n  # Zero out contributions from samples we\'d never exercise at this point (i.e.,\n  # these extra observations do not change the regression coefficients).\n  masked = tf.transpose(batch_design * mask, perm=(2, 1, 0))\n  # For design matrix X and response y, the coefficients beta of the best linear\n  # unbiased estimate are contained in the equation X\'X beta = X\'y. Here `lhs`\n  # is X\'X and `rhs` is X\'y, or rather a tensor of such left hand and right hand\n  # sides, one for each payoff dimension.\n  lhs = tf.matmul(masked, masked, transpose_a=True)\n  # Use pseudo inverse for the regression matrix to ensure stability of the\n  # algorithm.\n  lhs_pinv = tf.linalg.pinv(lhs)\n  rhs = tf.matmul(\n      masked,\n      tf.expand_dims(tf.transpose(continuation_value), -1),\n      transpose_a=True)\n  beta = tf.linalg.matmul(lhs_pinv, rhs)\n  continuation = tf.matmul(tf.transpose(batch_design, perm=(2, 1, 0)), beta)\n  return tf.maximum(tf.transpose(tf.squeeze(continuation, -1)), 0.0)\n\n\ndef _updated_cashflow(num_times, exercise_index, exercise_value,\n                      expected_continuation, cashflow):\n  """"""Revises the cashflow tensor where options will be exercised earlier.""""""\n  do_exercise_bool = exercise_value > expected_continuation\n  do_exercise = tf.cast(do_exercise_bool, exercise_value.dtype)\n  # Shape [num_samples, payoff_dim]\n  scaled_do_exercise = tf.where(do_exercise_bool, exercise_value,\n                                tf.zeros_like(exercise_value))\n  # This picks out the samples where we now wish to exercise.\n  # Shape [num_samples, payoff_dim, 1]\n  new_samp_masked = tf.expand_dims(scaled_do_exercise, 2)\n  # This should be one on the current time step and zero otherwise.\n  # This is an array with nonzero entries showing newly exercised payoffs.\n  pad_shape = scaled_do_exercise.shape.as_list()\n  zeros_before = tf.zeros(pad_shape + [exercise_index - 1],\n                          dtype=scaled_do_exercise.dtype)\n  zeros_after = tf.zeros(pad_shape + [num_times - exercise_index],\n                         dtype=scaled_do_exercise.dtype)\n  new_cash = tf.concat([zeros_before, new_samp_masked, zeros_after], -1)\n\n  # Has shape [num_samples, payoff_dim, 1]\n  old_samp_masker = tf.expand_dims(1 - do_exercise, 2)\n  # Broadcast to shape [num_samples, payoff_dim, num_times - exercise_index]\n  old_samp_masker_after = tf.broadcast_to(\n      old_samp_masker, pad_shape + [num_times - exercise_index])\n  # Has shape `[num_samples, payoff_dim, exercise_index]`\n  zeros_before = tf.zeros(pad_shape + [exercise_index],\n                          dtype=scaled_do_exercise.dtype)\n  # Shape [num_samples, payoff_dim, num_times]\n  old_mask = tf.concat([zeros_before,\n                        old_samp_masker_after], -1)\n  # Shape [num_samples, payoff_dim, num_times]\n  old_cash = old_mask * cashflow\n  return new_cash + old_cash\n\n\ndef _lsm_loop_body(sample_paths, exercise_times, discount_factors, payoff_fn,\n                   basis_fn, num_times, exercise_index, cashflow):\n  """"""Finds the optimal exercise point and updates `cashflow`.""""""\n\n  # Index of the sample path that the exercise index maps to.\n  tick = exercise_times[exercise_index - 1]\n  # Calculate the payoff of each path if exercised now.\n  # Shape [num_samples, payoff_dim]\n  exercise_value = payoff_fn(sample_paths, tick)\n  # Present value of hanging on to the options (using future information).\n  # Shape `[num_samples, payoff_dim]`\n  continuation_value = continuation_value_fn(cashflow, discount_factors,\n                                             exercise_index)\n  # Create a design matrix for regression based on the sample paths.\n  # Shape `[num_samples, basis_size]`\n  design = basis_fn(sample_paths[:, tick, :])\n  # Expected present value of hanging on the options.\n  # Shape `[num_samples, payoff_dim]`\n  expected_continuation = expected_exercise_fn(design, continuation_value,\n                                               exercise_value)\n  # Update the cashflow matrix to reflect where earlier exercise is optimal.\n  # Shape `[num_samples, payoff_dim, num_exercise]`\n  rev_cash = _updated_cashflow(num_times, exercise_index, exercise_value,\n                               expected_continuation,\n                               cashflow)\n  rev_cash.set_shape(cashflow.shape)\n  return LsmLoopVars(exercise_index=exercise_index - 1, cashflow=rev_cash)\n'"
tf_quant_finance/experimental/lsm_algorithm/lsm_test.py,14,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for the regression Monte Carlo algorithm.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.experimental.lsm_algorithm import lsm\nfrom tf_quant_finance.experimental.lsm_algorithm import payoff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass LsmTest(tf.test.TestCase):\n\n  def setUp(self):\n    """"""Sets `samples` as in the Longstaff-Schwartz paper.""""""\n    super(LsmTest, self).setUp()\n    # See Longstaff, F.A. and Schwartz, E.S., 2001. Valuing American options by\n    # simulation: a simple least-squares approach.\n    samples = [[1.0, 1.09, 1.08, 1.34],\n               [1.0, 1.16, 1.26, 1.54],\n               [1.0, 1.22, 1.07, 1.03],\n               [1.0, 0.93, 0.97, 0.92],\n               [1.0, 1.11, 1.56, 1.52],\n               [1.0, 0.76, 0.77, 0.90],\n               [1.0, 0.92, 0.84, 1.01],\n               [1.0, 0.88, 1.22, 1.34]]\n    # Expand dims to reflect that `samples` represent sample paths of\n    # a 1-dimensional process\n    self.samples = np.expand_dims(samples, -1)\n    # Interest rates between exercise times\n    interest_rates = [0.06, 0.06, 0.06]\n    # Corresponding discount factors\n    self.discount_factors = np.exp(-np.cumsum(interest_rates))\n\n  def test_loop_condition(self):\n    """"""Tests that the loop will stop countdown at zero and not before.""""""\n    self.assertTrue(lsm.lsm_loop_cond(1, None))\n    self.assertFalse(lsm.lsm_loop_cond(0, None))\n\n  def test_continuation_value(self):\n    """"""Tests continuation value returns the discounted sum of later payoffs.""""""\n    exercise_index = 2\n    for dtype in (np.float32, np.float64):\n      discount_factors = tf.constant(\n          [1.0, 0.9, 0.8, 0.7, 0.6], dtype=dtype)\n      cashflow = tf.ones(shape=[10, 5, 4], dtype=dtype)\n      continuation_value = lsm.continuation_value_fn(cashflow,\n                                                     discount_factors,\n                                                     exercise_index)\n      expected_continuation = 1.625 * np.ones([10, 5])\n      self.assertAllClose(\n          continuation_value, expected_continuation, rtol=1e-8, atol=1e-8)\n\n  def test_expected_continuation(self):\n    """"""Tests that expected continuation works in V=1 case.\n\n    In particular this verifies that the regression done to get the expected\n    continuation value is performed on those elements which have a positive\n    exercise value.\n    """"""\n    for dtype in (np.float32, np.float64):\n      a = tf.range(start=-2, limit=3, delta=1, dtype=dtype)\n      design = tf.concat([a, a], axis=0)\n      design = tf.concat([[tf.ones_like(design), design]], axis=1)\n\n      # These values ensure that the expected continuation value is `(1,...,1).`\n      exercise_now = tf.expand_dims(\n          tf.concat([tf.ones_like(a), tf.zeros_like(a)], axis=0), -1)\n      cashflow = tf.expand_dims(\n          tf.concat([tf.ones_like(a), -tf.ones_like(a)], axis=0), -1)\n\n      expected_exercise = lsm.expected_exercise_fn(\n          design, cashflow, exercise_now)\n      self.assertAllClose(expected_exercise, tf.ones_like(cashflow))\n\n  def test_european_option_put(self):\n    """"""Tests that LSM price of European put option is computed as expected.""""""\n    # This is the same example as in Section 1 of\n    # Longstaff, F.A. and Schwartz, E.S., 2001. Valuing American options by\n    # simulation: a simple least-squares approach. The review of financial\n    # studies, 14(1), pp.113-147.\n    basis_fn = lsm.make_polynomial_basis(2)\n    for dtype in (np.float32, np.float64):\n      payoff_fn = payoff.make_basket_put_payoff([1.1], dtype=dtype)\n      # Option price\n      european_put_price = lsm.least_square_mc(\n          self.samples, [3], payoff_fn, basis_fn,\n          discount_factors=[self.discount_factors[-1]], dtype=dtype)\n      self.assertAllClose(european_put_price, [0.0564],\n                          rtol=1e-4, atol=1e-4)\n\n  def test_american_option_put(self):\n    """"""Tests that LSM price of American put option is computed as expected.""""""\n    # This is the same example as in Section 1 of\n    # Longstaff, F.A. and Schwartz, E.S., 2001. Valuing American options by\n    # simulation: a simple least-squares approach. The review of financial\n    # studies, 14(1), pp.113-147.\n    basis_fn = lsm.make_polynomial_basis(2)\n    for dtype in (np.float32, np.float64):\n      payoff_fn = payoff.make_basket_put_payoff([1.1], dtype=dtype)\n      # Option price\n      american_put_price = lsm.least_square_mc(\n          self.samples, [1, 2, 3], payoff_fn, basis_fn,\n          discount_factors=self.discount_factors, dtype=dtype)\n      self.assertAllClose(american_put_price, [0.1144],\n                          rtol=1e-4, atol=1e-4)\n\n  def test_american_basket_option_put(self):\n    """"""Tests the LSM price of American Basket put option.""""""\n    # This is the same example as in Section 1 of\n    # Longstaff, F.A. and Schwartz, E.S., 2001. Valuing American options by\n    # simulation: a simple least-squares approach. The review of financial\n    # studies, 14(1), pp.113-147.\n    # This is the minimum number of basis functions for the tests to pass.\n    basis_fn = lsm.make_polynomial_basis(10)\n    exercise_times = [1, 2, 3]\n    dtype = np.float64\n    payoff_fn = payoff.make_basket_put_payoff([1.1, 1.2, 1.3], dtype=dtype)\n    # Create a 2-d process which is simply follows the `samples` paths:\n    samples = tf.convert_to_tensor(self.samples, dtype=dtype)\n    samples_2d = tf.concat([samples, samples], -1)\n    # Price American basket option\n    american_basket_put_price = lsm.least_square_mc(\n        samples_2d, exercise_times, payoff_fn, basis_fn,\n        discount_factors=self.discount_factors, dtype=dtype)\n    # Since the marginal processes of `samples_2d` are 100% correlated, the\n    # price should be the same as of the American option computed for\n    # `samples`\n    american_put_price = lsm.least_square_mc(\n        self.samples, exercise_times, payoff_fn, basis_fn,\n        discount_factors=self.discount_factors, dtype=dtype)\n    self.assertAllClose(american_basket_put_price, american_put_price,\n                        rtol=1e-4, atol=1e-4)\n    self.assertAllEqual(american_basket_put_price.shape, [3])\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/experimental/lsm_algorithm/lsm_v2.py,56,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Implementation of the regression MC of Longstaff and Schwartz.\n\nThis version of the algorithm is XLA-compatible but might be slower in some\ncases than the previous version without XLA compilation. The following major\ndifferences with the previous version are:\n\n1. All tensors inside `_continuation_value_fn`, `_expected_exercise_fn` and\n  `_updated_cashflow` are now of static shape. This increases the memory\n  consumption of the algorithm but avoids dynamically shaped tensors.\n2. `make_polynomial_basis` outputs the function that has an additional\n  `time_index` argument. This is necessary to avoid dynamically shaped tensors.\n""""""\n\nimport collections\nimport tensorflow.compat.v2 as tf\n\nLsmLoopVars = collections.namedtuple(\n    ""LsmLoopVars"",\n    [\n        ""exercise_index"",  # int. The LSM algorithm iterates backwards over\n        # times where an option can be exercised, this tracks progress.\n        ""cashflow""  # (N, V, K) shaped tensor. Tracks the optimal cashflow\n        # of each sample path for each payoff dimension at each\n        # exercise time.\n    ])\n\n\ndef make_polynomial_basis(degree):\n  """"""Produces a callable from samples to polynomial basis for use in regression.\n\n  The output callable accepts a `Tensor` `X` of shape `[num_samples, dim]`,\n  computes a centered value `Y = X - mean(X, axis=0)` and outputs a `Tensor`\n  of shape `[degree * dim, num_samples]`, where\n  ```\n  Z[i*j, k] = X[k, j]**(degree - i) * X[k, j]**i, 0<=i<degree - 1, 0<=j<dim\n  ```\n  For example, if `degree` and `dim` are both equal to 2, the polynomial basis\n  is `1, X, X**2, Y, Y**2, X * Y, X**2 * Y, X * Y**2`, where `X` and `Y` are\n  the spatial axes.\n\n  #### Example\n  ```python\n  basis = make_polynomial_basis(2)\n  x = [1.0, 2.0, 3.0, 4.0]\n  x = tf.expand_dims(x, axis=-1)\n  basis(x)\n  # Expected result:\n  [[ 1.0, 1.0, 1.0, 1.0], [-1.5, -0.5, 0.5, 1.5]]\n  ```\n\n  Args:\n    degree: An `int32` scalar `Tensor`. The degree of the desired polynomial\n      basis.\n\n  Returns:\n    A callable from `Tensor`s of shape `[num_samples, dim]` to `Tensor`s of\n    shape `[(degree + 1)**dim, num_samples]`.\n\n  Raises:\n    ValueError: If `degree` is less than `1`.\n  """"""\n  tf.debugging.assert_greater_equal(\n      degree, 0,\n      message=""Degree of polynomial basis can not be negative."")\n  def basis(sample_paths, time_index):\n    """"""Computes polynomial basis expansion at the given sample points.\n\n    Args:\n      sample_paths: A `Tensor` of either `flaot32` or `float64` dtype and of\n        shape `[num_samples, num_times, dim]`.\n      time_index: An integer scalar `Tensor` that corresponds to the time\n        coordinate at which the basis function is computed.\n\n    Returns:\n      A `Tensor`s of shape `[(degree + 1)**dim, num_samples]`.\n    """"""\n    sample_paths = tf.convert_to_tensor(sample_paths,\n                                        name=""sample_paths"")\n    shape = sample_paths.shape.as_list()\n    num_samples = shape[0]\n    dim = shape[-1]\n    slice_samples = tf.slice(sample_paths, [0, time_index, 0],\n                             [num_samples, 1, dim])\n    slice_samples = tf.squeeze(slice_samples, 1)\n    dim = sample_paths.shape.as_list()[-1]\n    samples_centered = slice_samples - tf.math.reduce_mean(slice_samples,\n                                                           axis=0)\n    samples_centered = tf.expand_dims(samples_centered, axis=-2)\n    grid = tf.range(degree + 1, dtype=samples_centered.dtype)\n    # Creates a grid of \'power\' expansions, i.e., a `Tensor` of shape\n    # [(degree + 1)**dim, dim] with entries [k_1, .., k_dim] where\n    ## 0 <= k_i <= dim.\n    grid = tf.meshgrid(*(dim * [grid]))\n    # Shape [(degree + 1)**3, dim]\n    grid = tf.reshape(tf.stack(grid, -1), [-1, dim])\n    # `samples_centered` has shape [num_samples, 1, dim],\n    # `samples_centered**grid` has shape `[num_samples, (degree + 1)**dim, dim]`\n    # so that the output shape is [num_samples, (degree + 1)**dim]\n    basis_expansion = tf.reduce_prod(samples_centered**grid, -1)\n    return  tf.transpose(basis_expansion)\n  return basis\n\n\ndef least_square_mc(sample_paths,\n                    exercise_times,\n                    payoff_fn,\n                    basis_fn,\n                    discount_factors=None,\n                    dtype=None,\n                    name=None):\n  """"""Values Amercian style options using the LSM algorithm.\n\n  The Least-Squares Monte-Carlo (LSM) algorithm is a Monte-Carlo approach to\n  valuation of American style options. Using the sample paths of underlying\n  assets, and a user supplied payoff function it attempts to find the optimal\n  exercise point along each sample path. With optimal exercise points known,\n  the option is valued as the average payoff assuming optimal exercise\n  discounted to present value.\n\n  #### Example. American put option price through Monte Carlo\n  ```python\n  # Let the underlying model be a Black-Scholes process\n  # dS_t / S_t = rate dt + sigma**2 dW_t, S_0 = 1.0\n  # with `rate = 0.1`, and volatility `sigma = 1.0`.\n  # Define drift and volatility functions for log(S_t)\n  rate = 0.1\n  def drift_fn(_, x):\n    return rate - tf.ones_like(x) / 2.\n  def vol_fn(_, x):\n    return tf.expand_dims(tf.ones_like(x), axis=-1)\n  # Use Euler scheme to propagate 100000 paths for 1 year into the future\n  times = np.linspace(0., 1, num=50)\n  num_samples = 100000\n  log_paths = tf.function(tff.models.euler_sampling.sample)(\n          dim=1,\n          drift_fn=drift_fn, volatility_fn=vol_fn,\n          random_type=tff.math.random.RandomType.PSEUDO_ANTITHETIC,\n          times=times, num_samples=num_samples, seed=42, time_step=0.01)\n  # Compute exponent to get samples of `S_t`\n  paths = tf.math.exp(log_paths)\n  # American put option price for strike 1.1 and expiry 1 (assuming actual day\n  # count convention and no settlement adjustment)\n  strike = [1.1]\n  exercise_times = tf.range(times.shape[-1])\n  discount_factors = tf.exp(-rate * times)\n  payoff_fn = make_basket_put_payoff(strike)\n  basis_fn = make_polynomial_basis(10)\n  least_square_mc(paths, exercise_times, payoff_fn, basis_fn,\n                  discount_factors=discount_factors)\n  # Expected value: [0.397]\n  # European put option price\n  tff.black_scholes.option_price(volatilities=[1], strikes=strikes,\n                                 expiries=[1], spots=[1.],\n                                 discount_factors=discount_factors[-1],\n                                 is_call_options=False,\n                                 dtype=tf.float64)\n  # Expected value: [0.379]\n  ```\n  #### References\n\n  [1] Longstaff, F.A. and Schwartz, E.S., 2001. Valuing American options by\n  simulation: a simple least-squares approach. The review of financial studies,\n  14(1), pp.113-147.\n\n  Args:\n    sample_paths: A `Tensor` of shape `[num_samples, num_times, dim]`, the\n      sample paths of the underlying ito process of dimension `dim` at\n      `num_times` different points.\n    exercise_times: An `int32` `Tensor` of shape `[num_exercise_times]`.\n      Contents must be a subset of the integers `[0,...,num_times - 1]`,\n      representing the time indices at which the option may be exercised.\n    payoff_fn: Callable from a `Tensor` of shape `[num_samples, S, dim]`\n      (where S <= num_times) to a `Tensor` of shape `[num_samples, payoff_dim]`\n      of the same dtype as `samples`. The output represents the payout resulting\n      from exercising the option at time `S`. The `payoff_dim` allows multiple\n      options on the same underlying asset (i.e., `samples`) to be valued in\n      parallel.\n    basis_fn: Callable from a `Tensor` of the same shape and `dtype` as\n      `sample_paths` and a positive integer `Tenor` (representing a current\n      time index) to a `Tensor` of shape `[basis_size, num_samples]` of the same\n      dtype as `sample_paths`. The result being the design matrix used in\n      regression of the continuation value of options.\n    discount_factors: A `Tensor` of shape `[num_exercise_times]` and the same\n      `dtype` as `samples`, the k-th element of which represents the discount\n      factor at time index `k`.\n      Default value: `None` which maps to a one-`Tensor` of the same `dtype`\n        as `samples` and shape `[num_exercise_times]`.\n    dtype: Optional `dtype`. Either `tf.float32` or `tf.float64`. The `dtype`\n      If supplied, represents the `dtype` for the input and output `Tensor`s.\n      Default value: `None`, which means that the `dtype` inferred by TensorFlow\n      is used.\n    name: Python `str` name prefixed to Ops created by this function.\n      Default value: `None` which is mapped to the default name\n      \'least_square_mc\'.\n\n  Returns:\n    A `Tensor` of shape `[num_samples, payoff_dim]` of the same dtype as\n    `samples`.\n  """"""\n  name = name or ""least_square_mc""\n  with tf.name_scope(name):\n    # Conversion of the inputs to tensors\n    sample_paths = tf.convert_to_tensor(sample_paths,\n                                        dtype=dtype, name=""sample_paths"")\n    dtype = sample_paths.dtype\n    exercise_times = tf.convert_to_tensor(exercise_times, name=""exercise_times"")\n    num_times = exercise_times.shape.as_list()[-1]\n    if discount_factors is None:\n      discount_factors = tf.ones(shape=exercise_times.shape,\n                                 dtype=dtype,\n                                 name=""discount_factors"")\n    else:\n      discount_factors = tf.convert_to_tensor(\n          discount_factors, dtype=dtype, name=""discount_factors"")\n    discount_factors = tf.concat([[1], discount_factors], axis=-1)\n    # Initialise cashflow as the payoff at final sample.\n    time_index = exercise_times[num_times - 1]\n    # Calculate the payoff of each path if exercised now. Shape\n    # [num_samples, payoff_dim]\n    exercise_value = payoff_fn(sample_paths, time_index)\n    zeros = tf.zeros(exercise_value.shape + [num_times - 1],\n                     dtype=dtype)\n    exercise_value = tf.expand_dims(exercise_value, axis=-1)\n\n    # Shape [num_samples, payoff_dim, num_exercise]\n    cashflow = tf.concat([zeros, exercise_value], axis=-1)\n    # Starting state for loop iteration.\n    lsm_loop_vars = LsmLoopVars(exercise_index=num_times - 1, cashflow=cashflow)\n    def loop_body(exercise_index, cashflow):\n      return _lsm_loop_body(\n          sample_paths=sample_paths,\n          exercise_times=exercise_times,\n          discount_factors=discount_factors,\n          payoff_fn=payoff_fn,\n          basis_fn=basis_fn,\n          num_times=num_times,\n          exercise_index=exercise_index,\n          cashflow=cashflow)\n\n    loop_value = tf.while_loop(_lsm_loop_cond, loop_body, lsm_loop_vars)\n    present_values = _continuation_value_fn(\n        loop_value.cashflow, discount_factors, 0)\n    return tf.math.reduce_mean(present_values, axis=0)\n\n\ndef _lsm_loop_cond(exercise_index, cashflow):\n  """"""Condition to exit a countdown loop when the exercise date hits zero.""""""\n  del cashflow\n  return exercise_index > 0\n\n\ndef _continuation_value_fn(cashflow, discount_factors, exercise_index):\n  """"""Returns the discounted value of the right hand part of the cashflow tensor.\n\n  Args:\n    cashflow: A real `Tensor` of shape\n      `[num_samples, payoff_dim, num_exercise]`. Tracks the optimal cashflow of\n       each sample path for each payoff dimension at each exercise time.\n    discount_factors: A `Tensor` of shape `[num_exercise_times + 1]` and the\n      same `dtype` as `samples`, the `k`-th element of which represents the\n      discount factor at time index `k + 1`. `discount_factors[0]` is `1` which\n      is the discount factor at time `0`.\n    exercise_index: An integer scalar `Tensor` representing the index of the\n      exercise time of interest. Should be less than `num_exercise_times`.\n\n  Returns:\n    A `[num_samples, payoff_dim]` `Tensor` whose entries represent the sum of\n    those elements to the right of `exercise_index` in `cashflow`, discounted to\n    the time indexed by `exercise_index`. When `exercise_index` is zero, the\n    return represents the sum of the cashflow discounted to present value for\n    each sample path.\n  """"""\n  _, _, num_cashflow = cashflow.shape.as_list()\n  disc_factors_are_used = tf.range(num_cashflow + 1) >= exercise_index + 1\n  total_discount_factors = (\n      tf.where(disc_factors_are_used, discount_factors, 0)\n      / discount_factors[exercise_index])\n  cashflows_are_used = tf.range(num_cashflow) >= exercise_index\n  cashflow_masked = tf.where(cashflows_are_used, cashflow, 0)\n  return tf.math.reduce_sum(\n      cashflow_masked * total_discount_factors[1:], axis=2)\n\n\ndef _expected_exercise_fn(design, continuation_value, exercise_value):\n  """"""Returns the expected continuation value for each path.\n\n  Args:\n    design: A real `Tensor` of shape `[basis_size, num_samples]`.\n    continuation_value: A `Tensor` of shape `[num_samples, payoff_dim]` and of\n      the same dtype as `design`. The optimal value of the option conditional on\n      not exercising now or earlier, taking future information into account.\n    exercise_value: A `Tensor` of the same shape and dtype as\n      `continuation_value`. Value of the option if exercised immideately at\n      the current time\n\n  Returns:\n    A `Tensor` of the same shape and dtype as `continuation_value` whose\n    `(n, v)`-th entry represents the expected continuation value of sample path\n    `n` under the `v`-th payoff scheme.\n  """"""\n  batch_design = tf.broadcast_to(\n      design[..., None], design.shape + [continuation_value.shape[-1]])\n  mask = tf.cast(exercise_value > 0, design.dtype)\n  # Zero out contributions from samples we\'d never exercise at this point (i.e.,\n  # these extra observations do not change the regression coefficients).\n  masked = tf.transpose(batch_design * mask, perm=(2, 1, 0))\n  # For design matrix X and response y, the coefficients beta of the best linear\n  # unbiased estimate are contained in the equation X\'X beta = X\'y. Here `lhs`\n  # is X\'X and `rhs` is X\'y, or rather a tensor of such left hand and right hand\n  # sides, one for each payoff dimension.\n  lhs = tf.matmul(masked, masked, transpose_a=True)\n  # Use pseudo inverse for the regression matrix to ensure stability of the\n  # algorithm.\n  lhs_pinv = tf.linalg.pinv(lhs)\n  rhs = tf.matmul(\n      masked,\n      tf.expand_dims(tf.transpose(continuation_value), axis=-1),\n      transpose_a=True)\n  beta = tf.matmul(lhs_pinv, rhs)\n  continuation = tf.matmul(tf.transpose(batch_design, perm=(2, 1, 0)), beta)\n  return tf.maximum(tf.transpose(tf.squeeze(continuation, -1)), 0.0)\n\n\ndef _updated_cashflow(num_times, exercise_index, exercise_value,\n                      expected_continuation, cashflow):\n  """"""Revises the cashflow tensor where options will be exercised earlier.""""""\n  do_exercise_bool = exercise_value > expected_continuation\n  do_exercise = tf.cast(do_exercise_bool, exercise_value.dtype)\n  # Shape [num_samples, payoff_dim]\n  scaled_do_exercise = tf.where(do_exercise_bool, exercise_value,\n                                tf.zeros_like(exercise_value))\n  # This picks out the samples where we now wish to exercise.\n  # Shape [num_samples, payoff_dim, 1]\n  new_samp_masked = tf.expand_dims(scaled_do_exercise, axis=2)\n  # This should be one on the current time step and zero otherwise.\n  # This is an array with nonzero entries showing newly exercised payoffs.\n  zeros = tf.zeros_like(cashflow)\n  mask = tf.equal(tf.range(0, num_times), exercise_index - 1)\n  new_cash = tf.where(mask, new_samp_masked, zeros)\n  # Has shape [num_samples, payoff_dim, 1]\n  old_mask = tf.expand_dims(1 - do_exercise, axis=2)\n  mask = tf.range(0, num_times) >= exercise_index\n  old_mask = tf.where(mask, old_mask, zeros)\n  # Shape [num_samples, payoff_dim, num_times]\n  old_cash = old_mask * cashflow\n  return new_cash + old_cash\n\n\ndef _lsm_loop_body(sample_paths, exercise_times, discount_factors, payoff_fn,\n                   basis_fn, num_times, exercise_index, cashflow):\n  """"""Finds the optimal exercise point and updates `cashflow`.""""""\n\n  # Index of the sample path that the exercise index maps to.\n  time_index = exercise_times[exercise_index - 1]\n  # Calculate the payoff of each path if exercised now.\n  # Shape [num_samples, payoff_dim]\n  exercise_value = payoff_fn(sample_paths, time_index)\n  # Present value of hanging on to the options (using future information).\n  # Shape `[num_samples, payoff_dim]`\n  continuation_value = _continuation_value_fn(cashflow, discount_factors,\n                                              exercise_index)\n  # Create a design matrix for regression based on the sample paths.\n  # Shape `[num_samples, basis_size]`\n  design = basis_fn(sample_paths, time_index)\n\n  # Expected present value of hanging on the options.\n  expected_continuation = _expected_exercise_fn(design, continuation_value,\n                                                exercise_value)\n  # Update the cashflow matrix to reflect where earlier exercise is optimal.\n  # Shape `[num_samples, payoff_dim, num_exercise]`.\n  rev_cash = _updated_cashflow(num_times, exercise_index, exercise_value,\n                               expected_continuation,\n                               cashflow)\n  return LsmLoopVars(exercise_index=exercise_index - 1, cashflow=rev_cash)\n'"
tf_quant_finance/experimental/lsm_algorithm/lsm_v2_test.py,6,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for the regression Monte Carlo algorithm.""""""\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nimport tf_quant_finance as tff\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\nlsm_algorithm = tff.experimental.lsm_algorithm\n\n_SAMPLES = [[1.0, 1.09, 1.08, 1.34],\n            [1.0, 1.16, 1.26, 1.54],\n            [1.0, 1.22, 1.07, 1.03],\n            [1.0, 0.93, 0.97, 0.92],\n            [1.0, 1.11, 1.56, 1.52],\n            [1.0, 0.76, 0.77, 0.90],\n            [1.0, 0.92, 0.84, 1.01],\n            [1.0, 0.88, 1.22, 1.34]]\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass LsmTest(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    """"""Sets `samples` as in the Longstaff-Schwartz paper.""""""\n    super(LsmTest, self).setUp()\n    # See Longstaff, F.A. and Schwartz, E.S., 2001. Valuing American options by\n    # simulation: a simple least-squares approach.\n    # Expand dims to reflect that `samples` represent sample paths of\n    # a 1-dimensional process\n    self.samples = np.expand_dims(_SAMPLES, -1)\n    # Interest rates between exercise times\n    interest_rates = [0.06, 0.06, 0.06]\n    # Corresponding discount factors\n    self.discount_factors = np.exp(-np.cumsum(interest_rates))\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32\n      }, {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64\n      })\n  def test_european_option_put(self, dtype):\n    """"""Tests that LSM price of European put option is computed as expected.""""""\n    # This is the same example as in Section 1 of\n    # Longstaff, F.A. and Schwartz, E.S., 2001. Valuing American options by\n    # simulation: a simple least-squares approach. The review of financial\n    # studies, 14(1), pp.113-147.\n    basis_fn = lsm_algorithm.make_polynomial_basis_v2(2)\n    payoff_fn = lsm_algorithm.make_basket_put_payoff([1.1], dtype=dtype)\n    # Option price\n    european_put_price = lsm_algorithm.least_square_mc_v2(\n        self.samples, [3], payoff_fn, basis_fn,\n        discount_factors=[self.discount_factors[-1]], dtype=dtype)\n    self.assertAllClose(european_put_price, [0.0564],\n                        rtol=1e-4, atol=1e-4)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32\n      }, {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64\n      })\n  def test_american_option_put(self, dtype):\n    """"""Tests that LSM price of American put option is computed as expected.""""""\n    # This is the same example as in Section 1 of\n    # Longstaff, F.A. and Schwartz, E.S., 2001. Valuing American options by\n    # simulation: a simple least-squares approach. The review of financial\n    # studies, 14(1), pp.113-147.\n    basis_fn = lsm_algorithm.make_polynomial_basis_v2(2)\n    payoff_fn = lsm_algorithm.make_basket_put_payoff([1.1], dtype=dtype)\n    # Option price\n    american_put_price = lsm_algorithm.least_square_mc_v2(\n        self.samples, [1, 2, 3], payoff_fn, basis_fn,\n        discount_factors=self.discount_factors, dtype=dtype)\n    self.assertAllClose(american_put_price, [0.1144],\n                        rtol=1e-4, atol=1e-4)\n\n  def test_american_option_put_xla(self):\n    """"""Tests that LSM price of American put option with xla compilation.""""""\n    # This is the same example as above. Here we compile the graph with the XLA\n    # compiler.\n    basis_fn = lsm_algorithm.make_polynomial_basis_v2(2)\n    dtype = np.float64\n    payoff_fn = lsm_algorithm.make_basket_put_payoff([1.1], dtype=dtype)\n    # Option price function\n    def american_put_price_fn(samples):\n      return lsm_algorithm.least_square_mc_v2(\n          samples, [1, 2, 3], payoff_fn, basis_fn,\n          discount_factors=self.discount_factors, dtype=dtype)\n    # Compile the graph with XLA\n    @tf.function\n    def xla_compiled_op(samples):\n      return tf.xla.experimental.compile(american_put_price_fn, [samples])[0]\n    # Option price\n    american_put_price = xla_compiled_op(self.samples)\n    self.assertAllClose(american_put_price, [0.1144],\n                        rtol=1e-4, atol=1e-4)\n\n  def test_american_basket_option_put(self):\n    """"""Tests the LSM price of American Basket put option.""""""\n    # This is the same example as in Section 1 of\n    # Longstaff, F.A. and Schwartz, E.S., 2001. Valuing American options by\n    # simulation: a simple least-squares approach. The review of financial\n    # studies, 14(1), pp.113-147.\n    # This is the minimum number of basis functions for the tests to pass.\n    basis_fn = lsm_algorithm.make_polynomial_basis_v2(10)\n    exercise_times = [1, 2, 3]\n    dtype = np.float64\n    payoff_fn = lsm_algorithm.make_basket_put_payoff([1.1, 1.2, 1.3],\n                                                     dtype=dtype)\n    # Create a 2-d process which is simply follows the `samples` paths:\n    samples = tf.convert_to_tensor(self.samples, dtype=dtype)\n    samples_2d = tf.concat([samples, samples], -1)\n    # Price American basket option\n    american_basket_put_price = lsm_algorithm.least_square_mc_v2(\n        samples_2d, exercise_times, payoff_fn, basis_fn,\n        discount_factors=self.discount_factors, dtype=dtype)\n    # Since the marginal processes of `samples_2d` are 100% correlated, the\n    # price should be the same as of the American option computed for\n    # `samples`\n    american_put_price = lsm_algorithm.least_square_mc_v2(\n        self.samples, exercise_times, payoff_fn, basis_fn,\n        discount_factors=self.discount_factors, dtype=dtype)\n    with self.subTest(name=\'Price\'):\n      self.assertAllClose(american_basket_put_price, american_put_price,\n                          rtol=1e-4, atol=1e-4)\n    with self.subTest(name=\'Shape\'):\n      self.assertAllEqual(american_basket_put_price.shape, [3])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/experimental/lsm_algorithm/payoff.py,10,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Payoff functions.""""""\n\nimport functools\nimport tensorflow.compat.v2 as tf\n\n\ndef make_basket_put_payoff(strike_price, dtype=None, name=None):\n  """"""Produces a callable from samples to payoff of a simple basket put option.\n\n  Args:\n    strike_price: A `Tensor` of `dtype` consistent with `samples` and shape\n      `[num_samples, num_strikes]`.\n    dtype: Optional `dtype`. Either `tf.float32` or `tf.float64`. The `dtype`\n      If supplied, represents the `dtype` for the \'strike_price\' as well as\n      for the input argument of the output payoff callable.\n      Default value: `None`, which means that the `dtype` inferred by TensorFlow\n      is used.\n    name: Python `str` name prefixed to Ops created by the callable created\n      by this function.\n      Default value: `None` which is mapped to the default name \'put_valuer\'\n\n  Returns:\n    A callable from `Tensor` of shape `[num_samples, num_exercise_times, dim]`\n    and a scalar `Tensor` representing current time to a `Tensor` of shape\n    `[num_samples, num_strikes]`.\n  """"""\n  strike_price = tf.convert_to_tensor(strike_price, dtype=dtype,\n                                      name=""strike_price"")\n  put_valuer = functools.partial(_put_valuer, strike_price=strike_price,\n                                 dtype=dtype, name=name)\n\n  return put_valuer\n\n\ndef _put_valuer(sample_paths, time_index, strike_price, dtype=None, name=None):\n  """"""Produces a callable from samples to payoff of a simple basket put option.\n\n  Args:\n    sample_paths: A `Tensor` of either `flaot32` or `float64` dtype and of\n      shape `[num_samples, num_times, dim]`.\n    time_index: An integer scalar `Tensor` that corresponds to the time\n      coordinate at which the basis function is computed.\n    strike_price: A `Tensor` of the same `dtype` as `sample_paths` and shape\n      `[num_samples, num_strikes]`.\n    dtype: Optional `dtype`. Either `tf.float32` or `tf.float64`. The `dtype`\n      If supplied, represents the `dtype` for the \'strike_price\' as well as\n      for the input argument of the output payoff callable.\n      Default value: `None`, which means that the `dtype` inferred by TensorFlow\n      is used.\n    name: Python `str` name prefixed to Ops created by the callable created\n      by this function.\n      Default value: `None` which is mapped to the default name \'put_valuer\'\n\n  Returns:\n    A callable from `Tensor` of shape `[num_samples, num_exercise_times, dim]`\n    and a scalar `Tensor` representing current time to a `Tensor` of shape\n    `[num_samples, num_strikes]`.\n  """"""\n  name = name or ""put_valuer""\n  with tf.name_scope(name):\n    strike_price = tf.convert_to_tensor(strike_price, dtype=dtype,\n                                        name=""strike_price"")\n    sample_paths = tf.convert_to_tensor(sample_paths, dtype=dtype,\n                                        name=""sample_paths"")\n    num_samples, _, dim = sample_paths.shape.as_list()\n\n    slice_sample_paths = tf.slice(sample_paths, [0, time_index, 0],\n                                  [num_samples, 1, dim])\n    slice_sample_paths = tf.squeeze(slice_sample_paths, 1)\n    average = tf.math.reduce_mean(slice_sample_paths, axis=-1, keepdims=True)\n    return tf.nn.relu(strike_price - average)\n'"
tf_quant_finance/experimental/lsm_algorithm/payoff_test.py,4,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Payoff function tests.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.experimental.lsm_algorithm import payoff as payoff_utils\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass PayoffTest(tf.test.TestCase):\n\n  def test_put_payoff_function(self):\n    """"""Tests the put payoff function returns the right numbers.""""""\n    # See Longstaff, F.A. and Schwartz, E.S., 2001. Valuing American options by\n    # simulation: a simple least-squares approach.\n    samples = [[1.0, 1.09, 1.08, 1.34],\n               [1.0, 1.16, 1.26, 1.54],\n               [1.0, 1.22, 1.07, 1.03],\n               [1.0, 0.93, 0.97, 0.92],\n               [1.0, 1.11, 1.56, 1.52],\n               [1.0, 0.76, 0.77, 0.90],\n               [1.0, 0.92, 0.84, 1.01],\n               [1.0, 0.88, 1.22, 1.34]]\n    # Expand dims to reflect that `samples` represent sample paths of\n    # a 1-dimensional process\n    for dtype in (np.float32, np.float64):\n      # Create payoff functions for 2 different strike values\n      payoff_fn = payoff_utils.make_basket_put_payoff([1.1, 1.2], dtype=dtype)\n      sample_paths = tf.convert_to_tensor(samples, dtype=dtype)\n      sample_paths = tf.expand_dims(sample_paths, -1)\n      # Actual payoff\n      payoff = payoff_fn(sample_paths, 3)\n      # Expected payoffs at the final time\n      expected_payoff = [[0, 0],\n                         [0, 0],\n                         [0.07, 0.17],\n                         [0.18, 0.28],\n                         [0, 0],\n                         [0.2, 0.3],\n                         [0.09, 0.19],\n                         [0, 0]]\n    self.assertAllClose(expected_payoff, payoff, rtol=1e-8, atol=1e-8)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/math/integration/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Numerical integration methods.""""""\n\n\nfrom tf_quant_finance.math.integration.integrate import integrate\nfrom tf_quant_finance.math.integration.integrate import IntegrationMethod\nfrom tf_quant_finance.math.integration.simpson import simpson\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'integrate\',\n    \'simpson\',\n    \'IntegrationMethod\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/math/integration/integrate.py,3,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Algorithms for numeric integration in TensorFlow.""""""\n\n\nimport enum\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math.integration.simpson import simpson\n\n\n@enum.unique\nclass IntegrationMethod(enum.Enum):\n  """"""Specifies which algorithm to use for the numeric integration.\n\n  * `COMPOSITE_SIMPSONS_RULE`: Composite Simpson\'s 1/3 rule.\n  """"""\n  COMPOSITE_SIMPSONS_RULE = 1\n\n\ndef integrate(func,\n              lower,\n              upper,\n              method=IntegrationMethod.COMPOSITE_SIMPSONS_RULE,\n              dtype=None,\n              name=None,\n              **kwargs):\n  """"""Evaluates definite integral.\n\n  #### Example\n  ```python\n    f = lambda x: x*x\n    a = tf.constant(0.0)\n    b = tf.constant(3.0)\n    integrate(f, a, b) # 9.0\n  ```\n\n  Args:\n    func: Python callable representing a function to be integrated. It must be a\n      callable of a single `Tensor` parameter and return a `Tensor` of the same\n      shape and dtype as its input. It will be called with a `Tesnor` of shape\n      `lower.shape + [n]` (where n is integer number of points) and of the same\n      `dtype` as `lower`.\n    lower: `Tensor` or Python float representing the lower limits of\n      integration. `func` will be integrated between each pair of points defined\n      by `lower` and `upper`.\n    upper: `Tensor` of the same shape and dtype as `lower` or Python float\n      representing the upper limits of intergation.\n    method: Integration method. Instance of IntegrationMethod enum. Default is\n      IntegrationMethod.COMPOSITE_SIMPSONS_RULE.\n    dtype: Dtype of result. Must be real dtype. Defaults to dtype of `lower`.\n    name: Python str. The name to give to the ops created by this function.\n      Default value: None which maps to \'integrate\'.\n    **kwargs: Additional parameters for specific integration method.\n\n  Returns:\n    `Tensor` of the same shape and dtype as `lower`, containing the value of the\n    definite integral.\n\n  Raises: ValueError if `method` was not recognized.\n  """"""\n  with tf.compat.v1.name_scope(\n      name, default_name=\'integrate\', values=[lower, upper]):\n    if method == IntegrationMethod.COMPOSITE_SIMPSONS_RULE:\n      return simpson(func, lower, upper, dtype=dtype, **kwargs)\n    else:\n      raise ValueError(\'Unknown method: %s.\' % method)\n'"
tf_quant_finance/math/integration/integration_test.py,17,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for numeric integration methods.""""""\n\nimport collections\nimport math\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\ntff_int = tff.math.integration\n\nIntegrationTestCase = collections.namedtuple(\'IntegrationTestCase\', [\n    \'func\',\n    \'lower\',\n    \'upper\',\n    \'antiderivative\',\n])\n\n# pylint:disable=g-long-lambda\nBASIC_TEST_CASES = [\n    IntegrationTestCase(\n        func=lambda x: tf.exp(2 * x + 1),\n        lower=1.0,\n        upper=3.0,\n        antiderivative=lambda x: np.exp(2 * x + 1) / 2,\n    ),\n    IntegrationTestCase(\n        func=lambda x: x**5,\n        lower=-10.0,\n        upper=100.0,\n        antiderivative=lambda x: x**6 / 6,\n    ),\n    IntegrationTestCase(\n        func=lambda x: (x**3 + x**2 - 4 * x + 1) / (x**2 + 1)**2,\n        lower=0.0,\n        upper=10.0,\n        antiderivative=lambda x: sum([\n            2.5 / (x**2 + 1),\n            0.5 * np.log(x**2 + 1),\n            np.arctan(x),\n        ]),\n    ),\n    IntegrationTestCase(\n        func=lambda x: (tf.sinh(2 * x) + 3 * tf.sinh(x)) /\n        (tf.cosh(x)**2 + 2 * tf.cosh(0.5 * x)**2),\n        lower=2.0,\n        upper=4.0,\n        antiderivative=lambda x: sum([\n            np.log(np.cosh(x)**2 + np.cosh(x) + 1),\n            (4 / np.sqrt(3)) * np.arctan((1 + 2 * np.cosh(x)) / np.sqrt(3.0)),\n        ]),\n    ),\n    IntegrationTestCase(\n        func=lambda x: tf.exp(2 * x) * tf.math.sqrt(tf.exp(x) + tf.exp(2 * x)),\n        lower=2.0,\n        upper=4.0,\n        antiderivative=lambda x: sum([\n            np.sqrt((np.exp(x) + np.exp(2 * x))**3) / 3,\n            -(1 + 2 * np.exp(x)) * np.sqrt(np.exp(x) + np.exp(2 * x)) / 8,\n            np.log(np.sqrt(1 + np.exp(x)) + np.exp(0.5 * x)) / 8,\n        ]),\n    ),\n    IntegrationTestCase(\n        func=lambda x: tf.exp(-x**2),\n        lower=0.0,\n        upper=1.0,\n        antiderivative=lambda x: 0.5 * np.sqrt(np.pi) * math.erf(x),\n    ),\n]\n\nTEST_CASE_RAPID_CHANGE = IntegrationTestCase(\n    func=lambda x: 1.0 / tf.sqrt(x + 1e-6),\n    lower=0.0,\n    upper=1.0,\n    antiderivative=lambda x: 2.0 * np.sqrt(x + 1e-6),\n)\n\n\nclass IntegrationTest(tf.test.TestCase):\n\n  def _test_batches_and_types(self, integrate_function, args):\n    """"""Checks handling batches and dtypes.""""""\n    dtypes = [np.float32, np.float64, np.complex64, np.complex128]\n    a = [[0.0, 0.0], [0.0, 0.0]]\n    b = [[np.pi / 2, np.pi], [1.5 * np.pi, 2 * np.pi]]\n    a = [a, a]\n    b = [b, b]\n    k = tf.constant([[[[1.0]]], [[[2.0]]]])\n    func = lambda x: tf.cast(k, dtype=x.dtype) * tf.sin(x)\n    ans = [[[1.0, 2.0], [1.0, 0.0]], [[2.0, 4.0], [2.0, 0.0]]]\n\n    results = []\n    for dtype in dtypes:\n      lower = tf.constant(a, dtype=dtype)\n      upper = tf.constant(b, dtype=dtype)\n      results.append(integrate_function(func, lower, upper, **args))\n\n    results = self.evaluate(results)\n\n    for i in range(len(results)):\n      assert results[i].dtype == dtypes[i]\n      assert np.allclose(results[i], ans, atol=1e-3)\n\n  def _test_accuracy(self, integrate_function, args, test_case, max_rel_error):\n    func = test_case.func\n    lower = tf.constant(test_case.lower, dtype=tf.float64)\n    upper = tf.constant(test_case.upper, dtype=tf.float64)\n    exact = test_case.antiderivative(\n        test_case.upper) - test_case.antiderivative(test_case.lower)\n    approx = integrate_function(func, lower, upper, **args)\n    approx = self.evaluate(approx)\n    assert np.abs(approx - exact) <= np.abs(exact) * max_rel_error\n\n  def _test_gradient(self, integrate_function, args):\n    """"""Checks that integration result can be differentiated.""""""\n\n    # We consider I(a) = int_0^1 cos(ax) dx.\n    # Then dI/da = (a*cos(a) - sin(a))/a^2.\n    def integral(a):\n      return integrate_function(\n          lambda x: tf.cos(a * x), 0.0, 1.0, dtype=tf.float64, **args)\n\n    a = tf.constant(0.5, dtype=tf.float64)\n    di_da = tff.math.fwd_gradient(integral, a)\n\n    true_di_da = lambda a: (a * np.cos(a) - np.sin(a)) / (a**2)\n    self.assertAllClose(self.evaluate(di_da), true_di_da(0.5))\n\n  def test_integrate_batches_and_types(self):\n    self._test_batches_and_types(tff_int.integrate, {})\n    for method in tff_int.IntegrationMethod:\n      self._test_batches_and_types(tff_int.integrate, {\'method\': method})\n\n  def test_integrate_accuracy(self):\n    for test_case in BASIC_TEST_CASES:\n      self._test_accuracy(tff_int.integrate, {}, test_case, 1e-8)\n      for method in tff_int.IntegrationMethod:\n        self._test_accuracy(tff_int.integrate, {\'method\': method}, test_case,\n                            1e-8)\n\n  def test_integrate_gradient(self):\n    for method in tff_int.IntegrationMethod:\n      self._test_gradient(tff_int.integrate, {\'method\': method})\n\n  def test_integrate_int_limits(self):\n    for method in tff_int.IntegrationMethod:\n      result = tff_int.integrate(tf.sin, 0, 1, method=method, dtype=tf.float64)\n      result = self.evaluate(result)\n      self.assertAllClose(0.459697694, result)\n\n  def test_simpson_batches_and_types(self):\n    self._test_batches_and_types(tff_int.simpson, {})\n\n  def test_simpson_accuracy(self):\n    for test_case in BASIC_TEST_CASES:\n      self._test_accuracy(tff_int.simpson, {}, test_case, 1e-8)\n\n  def test_simpson_rapid_change(self):\n    self._test_accuracy(tff_int.simpson, {\'num_points\': 1001},\n                        TEST_CASE_RAPID_CHANGE, 2e-1)\n    self._test_accuracy(tff_int.simpson, {\'num_points\': 10001},\n                        TEST_CASE_RAPID_CHANGE, 3e-2)\n    self._test_accuracy(tff_int.simpson, {\'num_points\': 100001},\n                        TEST_CASE_RAPID_CHANGE, 5e-4)\n    self._test_accuracy(tff_int.simpson, {\'num_points\': 1000001},\n                        TEST_CASE_RAPID_CHANGE, 3e-6)\n\n  def test_simpson_gradient(self):\n    self._test_gradient(tff_int.simpson, {})\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/math/integration/simpson.py,21,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Composite Simpson\'s algorithm for numeric integration.""""""\n\n\nimport tensorflow.compat.v2 as tf\n\n\ndef simpson(func, lower, upper, num_points=1001, dtype=None, name=None):\n  """"""Evaluates definite integral using composite Simpson\'s 1/3 rule.\n\n  Integrates `func` using composite Simpson\'s 1/3 rule [1].\n\n  Evaluates function at points of evenly spaced grid of `num_points` points,\n  then uses obtained values to interpolate `func` with quadratic polynomials\n  and integrates these polynomials.\n\n  #### References\n  [1] Weisstein, Eric W. ""Simpson\'s Rule."" From MathWorld - A Wolfram Web\n      Resource. http://mathworld.wolfram.com/SimpsonsRule.html\n\n  #### Example\n  ```python\n    f = lambda x: x*x\n    a = tf.constant(0.0)\n    b = tf.constant(3.0)\n    integrate(f, a, b, num_points=1001) # 9.0\n  ```\n\n  Args:\n    func: Python callable representing a function to be integrated. It must be a\n      callable of a single `Tensor` parameter and return a `Tensor` of the same\n      shape and dtype as its input. It will be called with a `Tesnor` of shape\n      `lower.shape + [n]` (where n is integer number of points) and of the same\n      `dtype` as `lower`.\n    lower: `Tensor` or Python float representing the lower limits of\n      integration. `func` will be integrated between each pair of points defined\n      by `lower` and `upper`.\n    upper: `Tensor` of the same shape and dtype as `lower` or Python float\n      representing the upper limits of intergation.\n    num_points: Scalar int32 `Tensor`. Number of points at which function `func`\n      will be evaluated. Must be odd and at least 3.\n      Default value: 1001.\n    dtype: Optional `tf.Dtype`. If supplied, the dtype for the `lower` and\n      `upper`. Result will have the same dtype.\n      Default value: None which maps to dtype of `lower`.\n    name: Python str. The name to give to the ops created by this function.\n      Default value: None which maps to \'integrate_simpson_composite\'.\n\n  Returns:\n    `Tensor` of shape `func_batch_shape + limits_batch_shape`, containing\n      value of the definite integral.\n\n  """"""\n  with tf.compat.v1.name_scope(\n      name, default_name=\'integrate_simpson_composite\', values=[lower, upper]):\n    lower = tf.convert_to_tensor(lower, dtype=dtype, name=\'lower\')\n    dtype = lower.dtype\n    upper = tf.convert_to_tensor(upper, dtype=dtype, name=\'upper\')\n    num_points = tf.convert_to_tensor(\n        num_points, dtype=tf.int32, name=\'num_points\')\n\n    assertions = [\n        tf.debugging.assert_greater_equal(num_points, 3),\n        tf.debugging.assert_equal(num_points % 2, 1),\n    ]\n\n    with tf.compat.v1.control_dependencies(assertions):\n      dx = (upper - lower) / (tf.cast(num_points, dtype=dtype) - 1)\n      dx_expand = tf.expand_dims(dx, -1)\n      lower_exp = tf.expand_dims(lower, -1)\n      grid = lower_exp + dx_expand * tf.cast(tf.range(num_points), dtype=dtype)\n      weights_first = tf.constant([1.0], dtype=dtype)\n      weights_mid = tf.tile(\n          tf.constant([4.0, 2.0], dtype=dtype), [(num_points - 3) // 2])\n      weights_last = tf.constant([4.0, 1.0], dtype=dtype)\n      weights = tf.concat([weights_first, weights_mid, weights_last], axis=0)\n\n    return tf.reduce_sum(func(grid) * weights, axis=-1) * dx / 3\n'"
tf_quant_finance/math/interpolation/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Ops related to interpolation.""""""\n\n\nfrom tf_quant_finance.math.interpolation import cubic\nfrom tf_quant_finance.math.interpolation import linear\n\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'cubic\',\n    \'linear\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/math/optimizer/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Optimization methods.""""""\n\n\nfrom tensorflow_probability.python.optimizer import bfgs_minimize\nfrom tensorflow_probability.python.optimizer import converged_all\nfrom tensorflow_probability.python.optimizer import converged_any\nfrom tensorflow_probability.python.optimizer import differential_evolution_minimize\nfrom tensorflow_probability.python.optimizer import differential_evolution_one_step\nfrom tensorflow_probability.python.optimizer import lbfgs_minimize\nfrom tensorflow_probability.python.optimizer import linesearch\nfrom tensorflow_probability.python.optimizer import nelder_mead_minimize\nfrom tensorflow_probability.python.optimizer import nelder_mead_one_step\n\nfrom tf_quant_finance.math.optimizer.conjugate_gradient import ConjugateGradientParams\nfrom tf_quant_finance.math.optimizer.conjugate_gradient import minimize as conjugate_gradient_minimize\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'bfgs_minimize\',\n    \'differential_evolution_minimize\',\n    \'differential_evolution_one_step\',\n    \'conjugate_gradient_minimize\',\n    \'converged_all\',\n    \'converged_any\',\n    \'lbfgs_minimize\',\n    \'linesearch\',\n    \'nelder_mead_minimize\',\n    \'nelder_mead_one_step\',\n    \'ConjugateGradientParams\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/math/optimizer/conjugate_gradient.py,51,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""The Conjugate Gradient optimization algorithm.\n\nReferences:\n[HZ2006] Hager, William W., and Hongchao Zhang. ""Algorithm 851: CG_DESCENT,\n  a conjugate gradient method with guaranteed descent.""\n  http://users.clas.ufl.edu/hager/papers/CG/cg_compare.pdf\n[HZ2013] W. W. Hager and H. Zhang (2013) The limited memory conjugate gradient\n  method.\n  https://pdfs.semanticscholar.org/8769/69f3911777e0ff0663f21b67dff30518726b.pdf\n[JuliaLineSearches] Line search methods in Julia.\n  https://github.com/JuliaNLSolvers/LineSearches.jl\n""""""\n\n\nimport collections\n\nimport attr\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow_probability.python.optimizer import converged_all\nfrom tensorflow_probability.python.optimizer import linesearch\n\nOptimizerResult = collections.namedtuple(\n    \'OptimizerResult\',\n    [\n        # Scalar boolean tensor indicating whether the minimum\n        # was found within tolerance.\n        \'converged\',\n        # Scalar boolean tensor indicating whether a line search\n        # step failed to find a suitable step size satisfying Wolfe\n        # conditions. In the absence of any constraints on the\n        # number of objective evaluations permitted, this value will\n        # be the complement of `converged`. However, if there is\n        # a constraint and the search stopped due to available\n        # evaluations being exhausted, both `failed` and `converged`\n        # will be simultaneously False.\n        \'failed\',\n        # The number of iterations.\n        \'num_iterations\',\n        # The total number of objective evaluations performed.\n        \'num_objective_evaluations\',\n        # A tensor containing the last argument value found during the search.\n        # If the search converged, then this value is the argmin of the\n        # objective function (within some tolerance).\n        \'position\',\n        # A tensor containing the value of the objective\n        # function at the `position`. If the search\n        # converged, then this is the (local) minimum of\n        # the objective function.\n        \'objective_value\',\n        # A tensor containing the gradient of the\n        # objective function at the\n        # `final_position`. If the search converged\n        # the max-norm of this tensor should be\n        # below the tolerance.\n        \'objective_gradient\',\n    ])\n\n# Internal state of optimizer.\n_OptimizerState = collections.namedtuple(\n    \'_OptimizerState\',\n    [\n        # Fields from OptimizerResult.\n        \'converged\',\n        \'failed\',\n        \'num_iterations\',\n        \'num_objective_evaluations\',\n        # Position (x_k in [HZ2006]).\n        \'position\',\n        # Objective (f_k in [HZ2006]).\n        \'objective_value\',\n        # Gradient (g_k in [HZ2006]).\n        \'objective_gradient\',\n        # Direction, along which to go at the next step (d_k in [HZ2006]).\n        \'direction\',\n        # Previous step length (a_{k-1} in [HZ2006]).\n        \'prev_step\',\n    ])\n\n# A namedtuple to hold information about point needed for linesearch method.\nValueAndGradient = collections.namedtuple(\n    \'ValueAndGradient\',\n    [\n        # Point at which line function is evaluated.\n        \'x\',\n        # Value of function.\n        \'f\',\n        # Directional derivative.\n        \'df\',\n        # Full gradient evaluated at that point.\n        \'full_gradient\'\n    ])\n\n\n@attr.s\nclass ConjugateGradientParams(object):\n  """"""Adjustable parameters of conjugate gradient algorithm.""""""\n  # Real number. Sufficient decrease parameter for Wolfe conditions.\n  # Corresponds to `delta` in [HZ2006].\n  # Range (0, 0.5). Defaults to 0.1.\n  sufficient_decrease_param = attr.ib(default=0.1)\n  # Real number. Curvature parameter for Wolfe conditions.\n  # Corresponds to \'sigma\' in [HZ2006].\n  # Range [`delta`, 1). Defaults to 0.9.\n  curvature_param = attr.ib(default=0.9)\n  # Real number. Used to estimate the threshold at which the line search\n  # switches to approximate Wolfe conditions.\n  # Corresponds to \'epsilon\' in [HZ2006].\n  # Range (0, inf). Defaults to 1e-6.\n  threshold_use_approximate_wolfe_condition = attr.ib(default=1e-6)\n  # Real number. Shrinkage parameter for line search.\n  # Corresponds to \'gamma\' in [HZ2006].\n  # Range (0, 1). Defaults to 0.66.\n  shrinkage_param = attr.ib(default=0.66)\n  # Real number. Parameter used in to calculate lower bound for coefficient\n  # \'beta_k\', used to calculate next direction.\n  # Corresponds to \'eta\' in [HZ2013].\n  # Range (0, inf). Defaults to 0.4.\n  direction_update_param = attr.ib(default=0.4)\n  # Real number. Used in line search to expand the initial interval in case it\n  # does not bracket a minimum.\n  # Corresponds to \'rho\' in [HZ2006].\n  # Range (1.0, inf). Defaults to 5.0.\n  expansion_param = attr.ib(default=5.0)\n  # Real scalar `Tensor`. Factor used in initial guess for line search to\n  # multiply previous step to get right point for quadratic interpolation.\n  # Corresponds to \'psi_1\' in [HZ2006].\n  # Range (0, 1). Defaults to 0.2.\n  initial_guess_small_factor = attr.ib(default=0.2)\n  # Real number. Factor used in initial guess for line search to multiply\n  # previous step if qudratic interpolation failed.\n  # Corresponds to \'psi_2\' in [HZ2006].\n  # Range (1, inf). Defaults to 2.0.\n  initial_guess_step_multiplier = attr.ib(default=2.0)\n  # Boolean. Whether to try quadratic interpolation when finding initial step\n  # for line search.\n  # Corresponds to \'QuadStep\' in [HZ2006].\n  # Defaults  to `True`.\n  quad_step = attr.ib(default=True)\n\n\ndef minimize(value_and_gradients_function,\n             initial_position,\n             tolerance=1e-8,\n             x_tolerance=0,\n             f_relative_tolerance=0,\n             max_iterations=50,\n             parallel_iterations=1,\n             stopping_condition=None,\n             params=None,\n             name=None):\n  """"""Minimizes a differentiable function.\n\n  Implementation of algorithm described in [HZ2006]. Updated formula for next\n  search direction were taken from [HZ2013].\n\n  Supports batches with 1-dimensional batch shape.\n\n  #### References:\n  [HZ2006] Hager, William W., and Hongchao Zhang. ""Algorithm 851: CG_DESCENT,\n    a conjugate gradient method with guaranteed descent.""\n    http://users.clas.ufl.edu/hager/papers/CG/cg_compare.pdf\n  [HZ2013] W. W. Hager and H. Zhang (2013) The limited memory conjugate gradient\n    method.\n    https://pdfs.semanticscholar.org/8769/69f3911777e0ff0663f21b67dff30518726b.pdf\n\n  ### Usage:\n  The following example demonstrates this optimizer attempting to find the\n  minimum for a simple two dimensional quadratic objective function.\n\n  ```python\n    minimum = np.array([1.0, 1.0])  # The center of the quadratic bowl.\n    scales = np.array([2.0, 3.0])  # The scales along the two axes.\n\n    # The objective function and the gradient.\n    def quadratic(x):\n      value = tf.reduce_sum(scales * (x - minimum) ** 2)\n      return value, tf.gradients(value, x)[0]\n\n    start = tf.constant([0.6, 0.8])  # Starting point for the search.\n    optim_results = conjugate_gradient.minimize(\n        quadratic, initial_position=start, tolerance=1e-8)\n\n    with tf.Session() as session:\n      results = session.run(optim_results)\n      # Check that the search converged\n      assert(results.converged)\n      # Check that the argmin is close to the actual value.\n      np.testing.assert_allclose(results.position, minimum)\n  ```\n\n  Args:\n    value_and_gradients_function:  A Python callable that accepts a point as a\n      real `Tensor` and returns a tuple of `Tensor`s of real dtype containing\n      the value of the function and its gradient at that point. The function to\n      be minimized. The input should be of shape `[..., n]`, where `n` is the\n      size of the domain of input points, and all others are batching\n      dimensions. The first component of the return value should be a real\n      `Tensor` of matching shape `[...]`. The second component (the gradient)\n      should also be of shape `[..., n]` like the input value to the function.\n    initial_position: Real `Tensor` of shape `[..., n]`. The starting point, or\n      points when using batching dimensions, of the search procedure. At these\n      points the function value and the gradient norm should be finite.\n    tolerance: Scalar `Tensor` of real dtype. Specifies the gradient tolerance\n      for the procedure. If the supremum norm of the gradient vector is below\n      this number, the algorithm is stopped.\n    x_tolerance: Scalar `Tensor` of real dtype. If the absolute change in the\n      position between one iteration and the next is smaller than this number,\n      the algorithm is stopped.\n    f_relative_tolerance: Scalar `Tensor` of real dtype. If the relative change\n      in the objective value between one iteration and the next is smaller than\n      this value, the algorithm is stopped.\n    max_iterations: Scalar positive int32 `Tensor`. The maximum number of\n      iterations.\n    parallel_iterations: Positive integer. The number of iterations allowed to\n      run in parallel.\n    stopping_condition: (Optional) A Python function that takes as input two\n      Boolean tensors of shape `[...]`, and returns a Boolean scalar tensor. The\n      input tensors are `converged` and `failed`, indicating the current status\n      of each respective batch member; the return value states whether the\n      algorithm should stop. The default is tfp.optimizer.converged_all which\n      only stops when all batch members have either converged or failed. An\n      alternative is tfp.optimizer.converged_any which stops as soon as one\n      batch member has converged, or when all have failed.\n    params: ConjugateGradientParams object with adjustable parameters of the\n      algorithm. If not supplied, default parameters will be used.\n    name: (Optional) Python str. The name prefixed to the ops created by this\n      function. If not supplied, the default name \'minimize\' is used.\n\n  Returns:\n    optimizer_results: A namedtuple containing the following items:\n      converged: boolean tensor of shape `[...]` indicating for each batch\n        member whether the minimum was found within tolerance.\n      failed:  boolean tensor of shape `[...]` indicating for each batch\n        member whether a line search step failed to find a suitable step size\n        satisfying Wolfe conditions. In the absence of any constraints on the\n        number of objective evaluations permitted, this value will\n        be the complement of `converged`. However, if there is\n        a constraint and the search stopped due to available\n        evaluations being exhausted, both `failed` and `converged`\n        will be simultaneously False.\n      num_objective_evaluations: The total number of objective\n        evaluations performed.\n      position: A tensor of shape `[..., n]` containing the last argument value\n        found during the search from each starting point. If the search\n        converged, then this value is the argmin of the objective function.\n      objective_value: A tensor of shape `[...]` with the value of the\n        objective function at the `position`. If the search converged, then\n        this is the (local) minimum of the objective function.\n      objective_gradient: A tensor of shape `[..., n]` containing the gradient\n        of the objective function at the `position`. If the search converged\n        the max-norm of this tensor should be below the tolerance.\n\n  """"""\n  with tf.compat.v1.name_scope(name, \'minimize\', [initial_position, tolerance]):\n    if params is None:\n      params = ConjugateGradientParams()\n\n    initial_position = tf.convert_to_tensor(\n        value=initial_position, name=\'initial_position\')\n    dtype = initial_position.dtype\n    tolerance = tf.convert_to_tensor(\n        value=tolerance, dtype=dtype, name=\'grad_tolerance\')\n    f_relative_tolerance = tf.convert_to_tensor(\n        value=f_relative_tolerance, dtype=dtype, name=\'f_relative_tolerance\')\n    x_tolerance = tf.convert_to_tensor(\n        value=x_tolerance, dtype=dtype, name=\'x_tolerance\')\n    max_iterations = tf.convert_to_tensor(\n        value=max_iterations, name=\'max_iterations\')\n    stopping_condition = stopping_condition or converged_all\n    delta = tf.convert_to_tensor(\n        params.sufficient_decrease_param, dtype=dtype, name=\'delta\')\n    sigma = tf.convert_to_tensor(\n        params.curvature_param, dtype=dtype, name=\'sigma\')\n    eps = tf.convert_to_tensor(\n        params.threshold_use_approximate_wolfe_condition,\n        dtype=dtype,\n        name=\'sigma\')\n    eta = tf.convert_to_tensor(\n        params.direction_update_param, dtype=dtype, name=\'eta\')\n    psi_1 = tf.convert_to_tensor(\n        params.initial_guess_small_factor, dtype=dtype, name=\'psi_1\')\n    psi_2 = tf.convert_to_tensor(\n        params.initial_guess_step_multiplier, dtype=dtype, name=\'psi_2\')\n\n    f0, df0 = value_and_gradients_function(initial_position)\n    converged = tf.norm(df0, axis=-1) < tolerance\n\n    initial_state = _OptimizerState(\n        converged=converged,\n        failed=tf.zeros_like(converged),  # All false.\n        num_iterations=tf.convert_to_tensor(value=0),\n        num_objective_evaluations=tf.convert_to_tensor(value=1),\n        position=initial_position,\n        objective_value=f0,\n        objective_gradient=df0,\n        direction=-df0,\n        prev_step=tf.ones_like(f0),\n    )\n\n    def _cond(state):\n      """"""Continue if iterations remain and stopping condition is not met.""""""\n      return (\n          (state.num_iterations < max_iterations)\n          & tf.logical_not(stopping_condition(state.converged, state.failed)))\n\n    def _body(state):\n      """"""Main optimization loop.""""""\n      # We use notation of [HZ2006] for brevity.\n      x_k = state.position\n      d_k = state.direction\n      f_k = state.objective_value\n      g_k = state.objective_gradient\n      a_km1 = state.prev_step  # Means a_{k-1}.\n\n      # Define scalar function, which is objective restricted to direction.\n      def ls_func(alpha):\n        pt = x_k + tf.expand_dims(alpha, axis=-1) * d_k\n        objective_value, gradient = value_and_gradients_function(pt)\n        return ValueAndGradient(\n            x=alpha,\n            f=objective_value,\n            df=_dot(gradient, d_k),\n            full_gradient=gradient)\n\n      # Generate initial guess for line search.\n      # [HZ2006] suggests to generate first initial guess separately, but\n      # [JuliaLineSearches] generates it as if previous step length was 1, and\n      # we do the same.\n      phi_0 = f_k\n      dphi_0 = _dot(g_k, d_k)\n      ls_val_0 = ValueAndGradient(\n          x=tf.zeros_like(phi_0), f=phi_0, df=dphi_0, full_gradient=g_k)\n      step_guess_result = _init_step(ls_val_0, a_km1, ls_func, psi_1, psi_2,\n                                     params.quad_step)\n      init_step = step_guess_result.step\n\n      # Check if initial step size already satisfies Wolfe condition, and in\n      # that case don\'t perform line search.\n      c = init_step.x\n      phi_lim = phi_0 + eps * tf.abs(phi_0)\n      phi_c = init_step.f\n      dphi_c = init_step.df\n      # Original Wolfe conditions, T1 in [HZ2006].\n      suff_decrease_1 = delta * dphi_0 >= (phi_c - phi_0) / c\n      curvature = dphi_c >= sigma * dphi_0\n      wolfe1 = suff_decrease_1 & curvature\n      # Approximate Wolfe conditions, T2 in [HZ2006].\n      suff_decrease_2 = (2 * delta - 1) * dphi_0 >= dphi_c\n      curvature = dphi_c >= sigma * dphi_0\n      wolfe2 = suff_decrease_2 & curvature & (phi_c <= phi_lim)\n      wolfe = wolfe1 | wolfe2\n      skip_line_search = (step_guess_result.may_terminate\n                          & wolfe) | state.failed | state.converged\n\n      # Call Hager-Zhang line search (L0-L3 in [HZ2006]).\n      # Parameter theta from [HZ2006] is not adjustable, it\'s always 0.5.\n      ls_result = linesearch.hager_zhang(\n          ls_func,\n          value_at_zero=ls_val_0,\n          converged=skip_line_search,\n          initial_step_size=init_step.x,\n          value_at_initial_step=init_step,\n          shrinkage_param=params.shrinkage_param,\n          expansion_param=params.expansion_param,\n          sufficient_decrease_param=delta,\n          curvature_param=sigma,\n          threshold_use_approximate_wolfe_condition=eps)\n\n      # Moving to the next point, using step length from line search.\n      # If line search was skipped, take step length from initial guess.\n      # To save objective evaluation, use objective value and gradient returned\n      # by line search or initial guess.\n      a_k = tf.compat.v1.where(\n          skip_line_search, init_step.x, ls_result.left.x)\n      x_kp1 = state.position + tf.expand_dims(a_k, -1) * d_k\n      f_kp1 = tf.compat.v1.where(\n          skip_line_search, init_step.f, ls_result.left.f)\n      g_kp1 = tf.compat.v1.where(skip_line_search, init_step.full_gradient,\n                                 ls_result.left.full_gradient)\n\n      # Evaluate next direction.\n      # Use formulas (2.7)-(2.11) from [HZ2013] with P_k=I.\n      y_k = g_kp1 - g_k\n      d_dot_y = _dot(d_k, y_k)\n      b_k = (_dot(y_k, g_kp1) -\n             _norm_sq(y_k) * _dot(g_kp1, d_k) / d_dot_y) / d_dot_y\n      eta_k = eta * _dot(d_k, g_k) / _norm_sq(d_k)\n      b_k = tf.maximum(b_k, eta_k)\n      d_kp1 = -g_kp1 + tf.expand_dims(b_k, -1) * d_k\n\n      # Check convergence criteria.\n      grad_converged = _norm_inf(g_kp1) <= tolerance\n      x_converged = (_norm_inf(x_kp1 - x_k) <= x_tolerance)\n      f_converged = (\n          tf.math.abs(f_kp1 - f_k) <= f_relative_tolerance * tf.math.abs(f_k))\n      converged = grad_converged | x_converged | f_converged\n\n      # Construct new state for next iteration.\n      new_state = _OptimizerState(\n          converged=converged,\n          failed=state.failed,\n          num_iterations=state.num_iterations + 1,\n          num_objective_evaluations=state.num_objective_evaluations +\n          step_guess_result.func_evals + ls_result.func_evals,\n          position=tf.compat.v1.where(state.converged, x_k, x_kp1),\n          objective_value=tf.compat.v1.where(state.converged, f_k, f_kp1),\n          objective_gradient=tf.compat.v1.where(state.converged, g_k, g_kp1),\n          direction=d_kp1,\n          prev_step=a_k)\n      return (new_state,)\n\n    final_state = tf.while_loop(\n        _cond, _body, (initial_state,),\n        parallel_iterations=parallel_iterations)[0]\n    return OptimizerResult(\n        converged=final_state.converged,\n        failed=final_state.failed,\n        num_iterations=final_state.num_iterations,\n        num_objective_evaluations=final_state.num_objective_evaluations,\n        position=final_state.position,\n        objective_value=final_state.objective_value,\n        objective_gradient=final_state.objective_gradient)\n\n# A namedtuple with result of guessing initial step.\n_StepGuessResult = collections.namedtuple(\n    \'_StepGuessResult\',\n    [\n        # ValueAndGradient describing the initial guess.\n        \'step\',\n        # Whether initial guess is ""good enogh"" to use. Used internally by\n        # _init_step, must have all components `True` when returned.\n        \'can_take\',\n        # If true, means that before performing line search we have to check\n        # whether Wolfe conditions are already satisfied, and in that case don\'t\n        # perform line search.\n        # Set to true if step was obtained by quandratic interpolation.\n        \'may_terminate\',\n        # Number of function calls made to determine initial step.\n        \'func_evals\',\n    ])\n\n\ndef _init_step(pos, prev_step, func, psi_1, psi_2, quad_step):\n  """"""Finds initial step size for line seacrh at given point.\n\n  Corresponds to I1-I2 in [HZ2006].\n\n  Args:\n    pos: ValueAndGradient for current point.\n    prev_step: Step size at previous iteration.\n    func: Callable taking real `Tensor` and returning ValueAndGradient,\n      describes scalar function for line search.\n    psi_1: Real scalar `Tensor`. Factor to multiply previous step to get right\n      point for quadratic interpolation.\n    psi_2: Real scalar `Tesnor`. Factor to multiply previous step if qudratic\n      interpolation failed.\n    quad_step: Boolean. Whether to try quadratic interpolation.\n\n  Returns:\n    _StepGuessResult namedtuple containing initial guess and additional data.\n  """"""\n  phi_0 = pos.f\n  derphi_0 = pos.df\n  step = func(psi_1 * prev_step)\n  can_take = step.f > phi_0\n  result = _StepGuessResult(\n      step=step,\n      func_evals=1,\n      can_take=can_take,\n      may_terminate=tf.zeros_like(can_take))\n\n  # Try to approximate function with a parabola and take its minimum as initial\n  # guess.\n  if quad_step:\n    # Quadratic coefficient of parabola. If it\'s positive, parabola is convex\n    # and has minimum.\n    q_koef = step.f - phi_0 - step.x * derphi_0\n    quad_step_success = tf.logical_and(step.f <= phi_0, q_koef > 0.0)\n\n    def update_result_1():\n      new_x = tf.compat.v1.where(\n          quad_step_success,\n          -0.5 * (derphi_0 * step.x**2) / q_koef, result.step.x)\n      return _StepGuessResult(\n          step=func(new_x),\n          func_evals=result.func_evals + 1,\n          can_take=tf.logical_or(result.can_take, quad_step_success),\n          may_terminate=tf.logical_or(result.may_terminate, quad_step_success))\n\n    result = tf.cond(\n        tf.reduce_any(quad_step_success), update_result_1, lambda: result)\n\n  def update_result_2():\n    new_x = tf.compat.v1.where(can_take, result.step.x, psi_2 * prev_step)\n    return _StepGuessResult(\n        step=func(new_x),\n        func_evals=result.func_evals + 1,\n        can_take=tf.ones_like(can_take),\n        may_terminate=result.may_terminate)\n\n  # According to [HZ2006] we should fall back to psi_2*prev_step when quadratic\n  # interpolation failed. However, [JuliaLineSearches] retains guess\n  # psi_1*prev_step if func(psi_1 * prev_step) > func(0), because then local\n  # minimum is within (0, psi_1*prev_step).\n  result = tf.cond(\n      tf.reduce_all(result.can_take), lambda: result, update_result_2)\n\n  return result\n\n\ndef _dot(x, y):\n  """"""Evaluates scalar product.""""""\n  return tf.reduce_sum(x * y, axis=-1)\n\n\ndef _norm(x):\n  """"""Evaluates L2 norm.""""""\n  return tf.norm(x, axis=-1)\n\n\ndef _norm_sq(x):\n  """"""Evaluates L2 norm squared.""""""\n  return tf.reduce_sum(tf.square(x), axis=-1)\n\n\ndef _norm_inf(x):\n  """"""Evaluates inf-norm.""""""\n  return tf.reduce_max(tf.abs(x), axis=-1)\n'"
tf_quant_finance/math/optimizer/conjugate_gradient_test.py,39,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for Fletcher-Reeves algorithm.""""""\n\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\ndef _norm(x):\n  return np.linalg.norm(x, np.inf)\n\n\n# Test functions.\ndef _rosenbrock(x):\n  """"""See https://en.wikipedia.org/wiki/Rosenbrock_function.""""""\n  term1 = 100 * tf.reduce_sum(tf.square(x[1:] - tf.square(x[:-1])))\n  term2 = tf.reduce_sum(tf.square(1 - x[:-1]))\n  return term1 + term2\n\n\ndef _himmelblau(coord):\n  """"""See https://en.wikipedia.org/wiki/Himmelblau%27s_function.""""""\n  x, y = coord[..., 0], coord[..., 1]\n  return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n\n\ndef _mc_cormick(coord):\n  """"""See https://www.sfu.ca/~ssurjano/mccorm.html.""""""\n  x = coord[0]\n  y = coord[1]\n  return tf.sin(x + y) + tf.square(x - y) - 1.5 * x + 2.5 * y + 1\n\n\ndef _beale(coord):\n  """"""See https://www.sfu.ca/~ssurjano/beale.html.""""""\n  x = coord[0]\n  y = coord[1]\n  term1 = (1.5 - x + x * y)**2\n  term2 = (2.25 - x + x * y**2)**2\n  term3 = (2.625 - x + x * y**3)**2\n  return term1 + term2 + term3\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass ConjugateGradientTest(tf.test.TestCase):\n\n  def _check_algorithm(self,\n                       func=None,\n                       start_point=None,\n                       gtol=1e-4,\n                       expected_argmin=None):\n    """"""Runs algorithm on given test case and verifies result.""""""\n    val_grad_func = lambda x: tff.math.value_and_gradient(func, x)\n    start_point = tf.constant(start_point, dtype=tf.float64)\n    expected_argmin = np.array(expected_argmin, dtype=np.float64)\n\n    f_call_ctr = tf.Variable(0, dtype=tf.int32)\n\n    def val_grad_func_with_counter(x):\n      with tf.compat.v1.control_dependencies(\n          [tf.compat.v1.assign_add(f_call_ctr, 1)]):\n        return val_grad_func(x)\n\n    result = tff.math.optimizer.conjugate_gradient_minimize(\n        val_grad_func_with_counter,\n        start_point,\n        tolerance=gtol,\n        max_iterations=200)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    result = self.evaluate(result)\n    f_call_ctr = self.evaluate(f_call_ctr)\n\n    # Check that minimum is found.\n    with self.subTest(name=""Position""):\n      self.assertAllClose(result.position, expected_argmin, rtol=1e-3,\n                          atol=1e-3)\n    # Check that gradient norm is below tolerance.\n    grad_norm = np.max(result.objective_gradient)\n    with self.subTest(name=""GradientNorm""):\n      self.assertLessEqual(grad_norm, gtol)\n    # Check that number of function calls, declared by algorithm, is correct.\n    with self.subTest(name=""NumberOfEvals""):\n      self.assertEqual(result.num_objective_evaluations, f_call_ctr)\n    # Check returned function and gradient values.\n    pos = tf.constant(result.position, dtype=tf.float64)\n    f_at_pos, grad_at_pos = self.evaluate(val_grad_func(pos))\n    with self.subTest(name=""ObjectiveValue""):\n      self.assertAllClose(result.objective_value, f_at_pos)\n    with self.subTest(name=""ObjectiveGradient""):\n      self.assertAllClose(result.objective_gradient, grad_at_pos)\n    # Check that all converged and none failed.\n    with self.subTest(name=""AllConverged""):\n      self.assertTrue(np.all(result.converged))\n    with self.subTest(""NoneFailed""):\n      self.assertFalse(np.any(result.failed))\n\n  def test_univariate(self):\n    self._check_algorithm(\n        func=lambda x: (x[0] - 20)**2,\n        start_point=[100.0],\n        expected_argmin=[20.0])\n\n  def test_quadratics(self):\n\n    def test_random_quadratic(dim, seed):\n      """"""Generates random test case for function x^T A x + b x.""""""\n      np.random.seed(seed)\n      a = np.random.uniform(size=(dim, dim))\n      a = np.array(\n          np.dot(a, a.T), dtype=np.float64)  # Must be positive semidefinite.\n      b = np.array(np.random.uniform(size=(dim,)), dtype=np.float64)\n      argmin = -np.dot(np.linalg.inv(a), b)\n      a = tf.constant(a)\n      b = tf.constant(b)\n\n      def paraboloid(x):\n        return 0.5 * tf.einsum(""i,ij,j->"", x, a, x) + tf.einsum(""i,i->"", b, x)\n\n      self._check_algorithm(\n          start_point=np.random.uniform(size=(dim,)),\n          func=paraboloid,\n          expected_argmin=argmin)\n\n    test_random_quadratic(2, 43)\n    test_random_quadratic(3, 43)\n    test_random_quadratic(4, 43)\n    test_random_quadratic(5, 43)\n    test_random_quadratic(10, 43)\n    test_random_quadratic(15, 43)\n\n  def test_paraboloid_4th_order(self):\n    self._check_algorithm(\n        func=lambda x: tf.reduce_sum(x**4),\n        start_point=[1, 2, 3, 4, 5],\n        expected_argmin=[0, 0, 0, 0, 0],\n        gtol=1e-10)\n\n  def test_logistic_regression(self):\n    dim = 5\n    n_objs = 10000\n    np.random.seed(1)\n    betas = np.random.randn(dim)  # The true beta\n    intercept = np.random.randn()  # The true intercept\n    features = np.random.randn(n_objs, dim)  # The feature matrix\n    probs = 1 / (1 + np.exp(\n        -np.matmul(features, np.expand_dims(betas, -1)) - intercept))\n    labels = np.random.binomial(1, probs)  # The true labels\n    regularization = 0.8\n    feat = tf.constant(features)\n    lab = tf.constant(labels, dtype=feat.dtype)\n\n    def f_negative_log_likelihood(params):\n      intercept, beta = params[0], params[1:]\n      logit = tf.matmul(feat, tf.expand_dims(beta, -1)) + intercept\n      log_likelihood = tf.reduce_sum(\n          tf.nn.sigmoid_cross_entropy_with_logits(labels=lab, logits=logit))\n      l2_penalty = regularization * tf.reduce_sum(beta**2)\n      total_loss = log_likelihood + l2_penalty\n      return total_loss\n    start_point = np.ones(dim + 1)\n    argmin = [\n        -2.38636155, 1.61778325, -0.60694238, -0.51523609, -1.09832275,\n        0.88892742\n    ]\n\n    self._check_algorithm(\n        func=f_negative_log_likelihood,\n        start_point=start_point,\n        expected_argmin=argmin,\n        gtol=1e-5)\n\n  def test_data_fitting(self):\n    """"""Tests MLE estimation for a simple geometric GLM.""""""\n    n, dim = 100, 3\n    dtype = tf.float64\n    np.random.seed(234095)\n    x = np.random.choice([0, 1], size=[dim, n])\n    s = 0.01 * np.sum(x, 0)\n    p = 1. / (1 + np.exp(-s))\n    y = np.random.geometric(p)\n    x_data = tf.convert_to_tensor(value=x, dtype=dtype)\n    y_data = tf.expand_dims(tf.convert_to_tensor(value=y, dtype=dtype), -1)\n\n    def neg_log_likelihood(state):\n      state_ext = tf.expand_dims(state, 0)\n      linear_part = tf.matmul(state_ext, x_data)\n      linear_part_ex = tf.stack([tf.zeros_like(linear_part), linear_part],\n                                axis=0)\n      term1 = tf.squeeze(\n          tf.matmul(tf.reduce_logsumexp(linear_part_ex, axis=0), y_data), -1)\n      term2 = (0.5 * tf.reduce_sum(state_ext * state_ext, axis=-1) -\n               tf.reduce_sum(linear_part, axis=-1))\n      return tf.squeeze(term1 + term2)\n\n    self._check_algorithm(\n        func=neg_log_likelihood,\n        start_point=np.ones(shape=[dim]),\n        expected_argmin=[-0.020460034354, 0.171708568111, 0.021200423717])\n\n  def test_rosenbrock_2d_v1(self):\n    self._check_algorithm(\n        func=_rosenbrock,\n        start_point=[-1.2, 2],\n        expected_argmin=[1.0, 1.0])\n\n  def test_rosenbrock_2d_v2(self):\n    self._check_algorithm(\n        func=_rosenbrock,\n        start_point=[7, -12],\n        expected_argmin=[1.0, 1.0])\n\n  def test_rosenbock_7d(self):\n    self._check_algorithm(\n        func=_rosenbrock,\n        start_point=np.zeros(7),\n        expected_argmin=np.ones(7))\n\n  def test_himmelblau_v1(self):\n    self._check_algorithm(\n        func=_himmelblau,\n        start_point=[4, 3],\n        expected_argmin=[3.0, 2.0],\n        gtol=1e-8)\n\n  def test_himmelblau_v2(self):\n    self._check_algorithm(\n        func=_himmelblau,\n        start_point=[-2, 3],\n        expected_argmin=[-2.805118, 3.131312],\n        gtol=1e-8)\n\n  def test_himmelblau_v3(self):\n    self._check_algorithm(\n        func=_himmelblau,\n        start_point=[-3, -3],\n        expected_argmin=[-3.779310, -3.283186],\n        gtol=1e-8)\n\n  def test_himmelblau_v4(self):\n    self._check_algorithm(\n        func=_himmelblau,\n        start_point=[3, -1],\n        expected_argmin=[3.584428, -1.848126],\n        gtol=1e-8)\n\n  def test_mc_cormick(self):\n    self._check_algorithm(\n        func=_mc_cormick,\n        start_point=[0, 0],\n        expected_argmin=[-0.54719, -1.54719])\n\n  def test_beale(self):\n    self._check_algorithm(\n        func=_beale,\n        start_point=[-1.0, -1.0],\n        expected_argmin=[3.0, 0.5],\n        gtol=1e-8)\n\n  def test_himmelblau_batch_all(self):\n    self._check_algorithm(\n        func=_himmelblau,\n        start_point=[[1, 1], [-2, 2], [-1, -1], [1, -2]],\n        expected_argmin=[[3, 2], [-2.805118, 3.131312], [-3.779310, -3.283186],\n                         [3.584428, -1.848126]],\n        gtol=1e-8)\n\n  def test_himmelblau_batch_any(self):\n    val_grad_func = tff.math.make_val_and_grad_fn(_himmelblau)\n    starts = tf.constant([[1, 1], [-2, 2], [-1, -1], [1, -2]], dtype=tf.float64)\n    expected_minima = np.array([[3, 2], [-2.805118, 3.131312],\n                                [-3.779310, -3.283186], [3.584428, -1.848126]],\n                               dtype=np.float64)\n\n    # Run with `converged_any` stopping condition, to stop as soon as any of\n    # the batch members have converged.\n    batch_results = tff.math.optimizer.conjugate_gradient_minimize(\n        val_grad_func,\n        initial_position=starts,\n        stopping_condition=tff.math.optimizer.converged_any,\n        tolerance=1e-8)\n    batch_results = self.evaluate(batch_results)\n\n    self.assertFalse(np.any(batch_results.failed))  # None have failed.\n    self.assertTrue(np.any(batch_results.converged))  # At least one converged.\n    self.assertFalse(np.all(batch_results.converged))  # But not all did.\n\n    # Converged points are near expected minima.\n    for actual, expected in zip(batch_results.position[batch_results.converged],\n                                expected_minima[batch_results.converged]):\n      self.assertArrayNear(actual, expected, 1e-5)\n    self.assertEqual(batch_results.num_iterations, 7)\n    self.assertEqual(batch_results.num_objective_evaluations, 27)\n\n  def test_dynamic_shapes(self):\n    """"""Can build op with dynamic shapes in graph mode.""""""\n    if tf.executing_eagerly():\n      return\n    minimum = np.array([1.0, 1.0])\n    scales = np.array([2.0, 3.0])\n\n    @tff.math.make_val_and_grad_fn\n    def quadratic(x):\n      return tf.reduce_sum(input_tensor=scales * (x - minimum)**2)\n\n    # Test with a vector of unknown dimension.\n    start = tf.compat.v1.placeholder(tf.float32, shape=[None])\n    op = tff.math.optimizer.conjugate_gradient_minimize(\n        quadratic, initial_position=start, tolerance=1e-8)\n    self.assertFalse(op.position.shape.is_fully_defined())\n\n    with self.cached_session() as session:\n      results = session.run(op, feed_dict={start: [0.6, 0.8]})\n    self.assertTrue(results.converged)\n    self.assertLessEqual(_norm(results.objective_gradient), 1e-8)\n    self.assertArrayNear(results.position, minimum, 1e-5)\n\n  def test_multiple_functions(self):\n    # Define 3 independednt quadratic functions, each with its own minimum.\n    minima = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n    func = lambda x: tf.reduce_sum(tf.square(x - minima), axis=1)\n    self._check_algorithm(\n        func=func, start_point=np.zeros_like(minima), expected_argmin=minima)\n\n  def test_float32(self):\n    minimum = np.array([1.0, 1.0], dtype=np.float32)\n    scales = np.array([2.0, 3.0], dtype=np.float32)\n    start = np.zeros_like(minimum)\n\n    @tff.math.make_val_and_grad_fn\n    def quadratic(x):\n      return tf.reduce_sum(input_tensor=scales * (x - minimum)**2)\n\n    result = tff.math.optimizer.conjugate_gradient_minimize(\n        quadratic, initial_position=start)\n    self.assertEqual(result.position.dtype, tf.float32)\n    self.assertArrayNear(self.evaluate(result.position), minimum, 1e-5)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/math/optimizer/optimizer_test.py,9,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for tff.math.optimizer.""""""\n\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\ntff_math = tff.math\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass OptimizerTest(tf.test.TestCase):\n  """"""Tests for optimization algorithms.""""""\n\n  def test_bfgs_minimize(self):\n    """"""Use BFGS algorithm to find all four minima of Himmelblau\'s function.""""""\n\n    @tff_math.make_val_and_grad_fn\n    def himmelblau(coord):\n      x, y = coord[..., 0], coord[..., 1]\n      return (x * x + y - 11) ** 2 + (x + y * y - 7) ** 2\n\n    start = tf.constant([[1, 1],\n                         [-2, 2],\n                         [-1, -1],\n                         [1, -2]], dtype=\'float64\')\n\n    results = self.evaluate(tff_math.optimizer.bfgs_minimize(\n        himmelblau, initial_position=start,\n        stopping_condition=tff_math.optimizer.converged_all,\n        tolerance=1e-8))\n\n    expected_minima = np.array([[3, 2],\n                                [-2.805118, 3.131312],\n                                [-3.779310, -3.283186],\n                                [3.584428, -1.848126]])\n\n    self.assertTrue(results.converged.all())\n    self.assertEqual(results.position.shape, expected_minima.shape)\n    self.assertNDArrayNear(results.position, expected_minima, 1e-5)\n\n  def test_lbfgs_minimize(self):\n    """"""Use L-BFGS algorithm to optimize randomly generated quadratic bowls.""""""\n\n    np.random.seed(12345)\n    dim = 10\n    batches = 50\n    minima = np.random.randn(batches, dim)\n    scales = np.exp(np.random.randn(batches, dim))\n\n    @tff_math.make_val_and_grad_fn\n    def quadratic(x):\n      return tf.reduce_sum(input_tensor=scales * (x - minima) ** 2, axis=-1)\n\n    start = tf.ones((batches, dim), dtype=\'float64\')\n\n    results = self.evaluate(tff_math.optimizer.lbfgs_minimize(\n        quadratic, initial_position=start,\n        stopping_condition=tff_math.optimizer.converged_any,\n        tolerance=1e-8))\n\n    self.assertTrue(results.converged.any())\n    self.assertEqual(results.position.shape, minima.shape)\n    self.assertNDArrayNear(\n        results.position[results.converged], minima[results.converged], 1e-5)\n\n  def test_nelder_mead_minimize(self):\n    """"""Use Nelder Mead algorithm to optimize the Rosenbrock function.""""""\n\n    def rosenbrock(coord):\n      x, y = coord[0], coord[1]\n      return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2\n\n    start = tf.constant([-1.0, 1.0])\n    results = self.evaluate(tff_math.optimizer.nelder_mead_minimize(\n        rosenbrock,\n        initial_vertex=start,\n        func_tolerance=1e-12))\n    self.assertTrue(results.converged)\n    self.assertArrayNear(results.position, [1.0, 1.0], 1e-5)\n\n  def test_differential_evolution(self):\n    """"""Use differential evolution algorithm to minimize a quadratic function.""""""\n    minimum = np.array([1.0, 1.0])\n    scales = np.array([2.0, 3.0])\n    def quadratic(x):\n      return tf.reduce_sum(\n          scales * tf.math.squared_difference(x, minimum), axis=-1)\n\n    initial_population = tf.random.uniform([40, 2], seed=1243)\n    results = self.evaluate(tff_math.optimizer.differential_evolution_minimize(\n        quadratic,\n        initial_population=initial_population,\n        func_tolerance=1e-12,\n        seed=2484))\n    self.assertTrue(results.converged)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/math/pde/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PDE solver methods.""""""\n\nfrom tf_quant_finance.math.pde import boundary_conditions\nfrom tf_quant_finance.math.pde import fd_solvers\nfrom tf_quant_finance.math.pde import grids\nfrom tf_quant_finance.math.pde import steppers\n\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'grids\',\n    \'steppers\',\n    \'fd_solvers\',\n    \'boundary_conditions\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/math/pde/boundary_conditions.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Helper functions to construct boundary conditions of PDEs.""""""\n\nimport functools\n\n\ndef dirichlet(boundary_values_fn):\n  """"""Wrapper for Dirichlet boundary conditions to be used in PDE solvers.\n\n  Example: the boundary value is 1 on both boundaries.\n\n  ```python\n  def lower_boundary_fn(t, location_grid):\n    return 1\n\n  def upper_boundary_fn(t, location_grid):\n    return 0\n\n  solver = fd_solvers.solve_forward(...,\n      boundary_conditions = [(dirichlet(lower_boundary_fn),\n                              dirichlet(upper_boundary_fn))],\n      ...)\n  ```\n\n  Also can be used as a decorator:\n\n  ```python\n  @dirichlet\n  def lower_boundary_fn(t, location_grid):\n    return 1\n\n  @dirichlet\n  def upper_boundary_fn(t, location_grid):\n    return 0\n\n  solver = fd_solvers.solve_forward(...,\n      boundary_conditions = [(lower_boundary_fn, upper_boundary_fn)],\n      ...)\n  ```\n\n  Args:\n    boundary_values_fn: Callable returning the boundary values at given time.\n      Accepts two arguments - the moment of time and the current coordinate\n      grid.\n      Returns a number, a zero-rank Tensor or a Tensor of shape\n      `batch_shape + grid_shape\'`, where `grid_shape\'` is grid_shape excluding\n      the axis orthogonal to the boundary. For example, in 3D the value grid\n      shape is `batch_shape + (z_size, y_size, x_size)`, and the boundary\n      tensors on the planes `y = y_min` and `y = y_max` should be either scalars\n      or have shape `batch_shape + (z_size, x_size)`. In 1D case this reduces\n      to just `batch_shape`.\n\n  Returns:\n    Callable suitable for PDE solvers.\n  """"""\n  @functools.wraps(boundary_values_fn)\n  def fn(t, x):\n    # The boundary condition has the form alpha V + beta V_n = gamma, and we\n    # should return a tuple (alpha, beta, gamma). In this case alpha = 1 and\n    # beta = 0.\n    return 1, None, boundary_values_fn(t, x)\n\n  return fn\n\n\ndef neumann(boundary_normal_derivative_fn):\n  """"""Wrapper for Neumann boundary condition to be used in PDE solvers.\n\n  Example: the normal boundary derivative is 1 on both boundaries (i.e.\n  `dV/dx = 1` on upper boundary, `dV/dx = -1` on lower boundary).\n\n  ```python\n  def lower_boundary_fn(t, location_grid):\n    return 1\n\n  def upper_boundary_fn(t, location_grid):\n    return 1\n\n  solver = fd_solvers.step_back(...,\n      boundary_conditions = [(neumann(lower_boundary_fn),\n                              neumann(upper_boundary_fn))],\n      ...)\n  ```\n\n  Also can be used as a decorator:\n\n  ```python\n  @neumann\n  def lower_boundary_fn(t, location_grid):\n    return 1\n\n  @neumann\n  def upper_boundary_fn(t, location_grid):\n    return 1\n\n  solver = fd_solvers.solve_forward(...,\n      boundary_conditions = [(lower_boundary_fn, upper_boundary_fn)],\n      ...)\n  ```\n\n  Args:\n    boundary_normal_derivative_fn: Callable returning the values of the\n      derivative with respect to the exterior normal to the boundary at the\n      given time.\n      Accepts two arguments - the moment of time and the current coordinate\n      grid.\n      Returns a number, a zero-rank Tensor or a Tensor of shape\n      `batch_shape + grid_shape\'`, where `grid_shape\'` is grid_shape excluding\n      the axis orthogonal to the boundary. For example, in 3D the value grid\n      shape is `batch_shape + (z_size, y_size, x_size)`, and the boundary\n      tensors on the planes `y = y_min` and `y = y_max` should be either scalars\n      or have shape `batch_shape + (z_size, x_size)`. In 1D case this reduces\n      to just `batch_shape`.\n\n  Returns:\n    Callable suitable for PDE solvers.\n  """"""\n  @functools.wraps(boundary_normal_derivative_fn)\n  def fn(t, x):\n    # The boundary condition has the form alpha V_n + beta V_n = gamma, and we\n    # should return a tuple (alpha, beta, gamma). In this case alpha = 0 and\n    # beta = 1.\n    return None, 1, boundary_normal_derivative_fn(t, x)\n\n  return fn\n\n__all__ = [""dirichlet"", ""neumann""]\n'"
tf_quant_finance/math/pde/fd_solvers.py,28,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Functions for solving linear parabolic PDEs.""""""\n\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance.math.pde.steppers.douglas_adi import douglas_adi_step\nfrom tf_quant_finance.math.pde.steppers.oscillation_damped_crank_nicolson import oscillation_damped_crank_nicolson_step\n\n\ndef solve_backward(start_time,\n                   end_time,\n                   coord_grid,\n                   values_grid,\n                   num_steps=None,\n                   start_step_count=0,\n                   time_step=None,\n                   one_step_fn=None,\n                   boundary_conditions=None,\n                   values_transform_fn=None,\n                   second_order_coeff_fn=None,\n                   first_order_coeff_fn=None,\n                   zeroth_order_coeff_fn=None,\n                   inner_second_order_coeff_fn=None,\n                   inner_first_order_coeff_fn=None,\n                   maximum_steps=None,\n                   swap_memory=True,\n                   dtype=None,\n                   name=None):\n  """"""Evolves a grid of function values backwards in time according to a PDE.\n\n  Evolves a discretized solution of following second order linear\n  partial differential equation:\n\n  ```None\n    dV/dt + Sum[a_ij d2(A_ij V)/dx_i dx_j, 1 <= i, j <=n] +\n       Sum[b_i d(B_i V)/dx_i, 1 <= i <= n] + c V = 0.\n  ```\n  from time `t0` to time `t1<t0` (i.e. backwards in time). Here `a_ij`,\n  `A_ij`, `b_i`, `B_i` and `c` are coefficients that may depend on spatial\n  variables `x` and time `t`.\n\n  The solution `V(t,x)` is assumed to be discretized on an `n`-dimensional\n  rectangular grid. A rectangular grid, G, in n-dimensions may be described\n  by specifying the coordinates of the points along each axis. For example,\n  a 2 x 4 grid in two dimensions can be specified by taking the cartesian\n  product of [1, 3] and [5, 6, 7, 8] to yield the grid points with\n  coordinates: `[(1, 5), (1, 6), (1, 7), (1, 8), (3, 5) ... (3, 8)]`.\n\n  This function allows batching of solutions. In this context, batching means\n  the ability to represent and evolve multiple independent functions `V`\n  (e.g. V1, V2 ...) simultaneously. A single discretized solution is specified\n  by stating its values at each grid point. This can be represented as a\n  `Tensor` of shape [d1, d2, ... dn] where di is the grid size along the `i`th\n  axis. A batch of such solutions is represented by a `Tensor` of shape:\n  [K, d1, d2, ... dn] where `K` is the batch size. This method only requires\n  that the input parameter `values_grid` be broadcastable with shape\n  [K, d1, ... dn].\n\n  The evolution of the solution from `t0` to `t1` is done by discretizing the\n  differential equation to a difference equation along the spatial and\n  temporal axes. The temporal discretization is given by a (sequence of)\n  time steps [dt_1, dt_2, ... dt_k] such that the sum of the time steps is\n  equal to the total time step `t0 - t1`. If a uniform time step is used,\n  it may equivalently be specified by stating the number of steps (n_steps)\n  to take. This method provides both options via the `time_step`\n  and `num_steps` parameters.\n\n  The mapping between the arguments of this method and the above\n  equation are described in the Args section below.\n\n  #### Example. European call option pricing.\n  ```python\n  import tensorflow.compat.v2 as tf\n  import tf_quant_finance as tff\n  pde = tff.math.pde\n\n  num_equations = 2  # Number of PDE\n  num_grid_points = 1024  # Number of grid points\n  dtype = tf.float64\n\n  # Build a log-uniform grid\n  s_min = 0.01\n  s_max = 300.\n  grid = pde.grids.uniform_grid(minimums=[s_min],\n                                maximums=[s_max],\n                                sizes=[num_grid_points],\n                                dtype=dtype)\n\n  # Specify volatilities and interest rates for the options\n  strike = tf.constant([[50], [100]], dtype)\n  volatility = tf.constant([[0.3], [0.15]], dtype)\n  rate = tf.constant([[0.01], [0.03]], dtype)\n  expiry = 1.0\n\n  # For batching multiple PDEs, we need to stack the grid values\n  # so that final_values[i] is the grid for the ith strike.\n  s = grid[0]\n  final_value_grid = tf.nn.relu(s - strike)\n\n  # Define parabolic equation coefficients. In this case the coefficients\n  # can be computed exactly but the same functions as below can be used to\n  # get approximate values for general case.\n  # We again need to use `tf.meshgrid` to batch the coefficients.\n  def second_order_coeff_fn(t, grid):\n    del t\n    s = grid[0]\n    return [[volatility**2 * s**2 / 2]]\n\n  def first_order_coeff_fn(t, grid):\n    del t\n    s = grid[0]\n    return [rate * s]\n\n  def zeroth_order_coeff_fn(t, grid):\n    del t, grid\n    return -rate\n\n  @pde.boundary_conditions.dirichlet\n  def lower_boundary_fn(t, grid):\n    del t, grid\n    return 0\n\n  @pde.boundary_conditions.dirichlet\n  def upper_boundary_fn(t, grid):\n    del grid\n    return tf.squeeze(s_max - strike * tf.exp(-rate * (expiry - t)))\n\n\n  # Estimate European call option price\n  estimate = pde.fd_solvers.solve_backward(\n    start_time=expiry,\n    end_time=0,\n    coord_grid=grid,\n    values_grid=final_value_grid,\n    time_step=0.01,\n    boundary_conditions=[(lower_boundary_fn, upper_boundary_fn)],\n    second_order_coeff_fn=second_order_coeff_fn,\n    first_order_coeff_fn=first_order_coeff_fn,\n    zeroth_order_coeff_fn=zeroth_order_coeff_fn,\n    dtype=dtype)[0]\n\n  # Extract estimates for some of the grid locations and compare to the\n  # true option price\n  value_grid_first_option = estimate[0, :]\n  value_grid_second_option = estimate[1, :]\n  ```\n\n  See more examples in `pde_solvers.pdf`.\n\n  Args:\n    start_time: Real positive scalar `Tensor`. The start time of the grid.\n      Corresponds to time `t0` above.\n    end_time: Real scalar `Tensor` smaller than the `start_time` and greater\n      than zero. The time to step back to. Corresponds to time `t1` above.\n    coord_grid: List of `n` rank 1 real `Tensor`s. `n` is the dimension of the\n      domain. The i-th `Tensor` has shape, `[d_i]` where `d_i` is the size of\n      the grid along axis `i`. The coordinates of the grid points. Corresponds\n      to the spatial grid `G` above.\n    values_grid: Real `Tensor` containing the function values at time\n      `start_time` which have to be evolved to time `end_time`. The shape of the\n      `Tensor` must broadcast with `B + [d_1, d_2, ..., d_n]`. `B` is the batch\n      dimensions (one or more), which allow multiple functions (with potentially\n      different boundary/final conditions and PDE coefficients) to be evolved\n      simultaneously.\n    num_steps: Positive int scalar `Tensor`. The number of time steps to take\n      when moving from `start_time` to `end_time`. Either this argument or the\n      `time_step` argument must be supplied (but not both). If num steps is\n      `k>=1`, uniform time steps of size `(t0 - t1)/k` are taken to evolve the\n      solution from `t0` to `t1`. Corresponds to the `n_steps` parameter above.\n    start_step_count: A scalar integer `Tensor`. Number of steps performed so\n      far.\n    time_step: The time step to take. Either this argument or the `num_steps`\n      argument must be supplied (but not both). The type of this argument may\n      be one of the following (in order of generality):\n        (a) None in which case `num_steps` must be supplied.\n        (b) A positive real scalar `Tensor`. The maximum time step to take.\n          If the value of this argument is `dt`, then the total number of steps\n          taken is N = (t0 - t1) / dt rounded up to the nearest integer. The\n          first N-1 steps are of size dt and the last step is of size\n          `t0 - t1 - (N-1) * dt`.\n        (c) A callable accepting the current time and returning the size of the\n          step to take. The input and the output are real scalar `Tensor`s.\n    one_step_fn: The transition kernel. A callable that consumes the following\n      arguments by keyword:\n        1. \'time\': Current time\n        2. \'next_time\': The next time to step to. For the backwards in time\n          evolution, this time will be smaller than the current time.\n        3. \'coord_grid\': The coordinate grid.\n        4. \'values_grid\': The values grid.\n        5. \'second_order_coeff_fn\': Callable returning the coefficients of the\n          second order terms of the PDE. See the spec of the\n          `second_order_coeff_fn` argument below.\n        6. \'first_order_coeff_fn\': Callable returning the coefficients of the\n          first order terms of the PDE. See the spec of the\n          `first_order_coeff_fn` argument below.\n        7. \'zeroth_order_coeff_fn\': Callable returning the coefficient of the\n          zeroth order term of the PDE. See the spec of the\n          `zeroth_order_coeff_fn` argument below.\n        8. \'num_steps_performed\': A scalar integer `Tensor`. Number of steps\n          performed so far.\n       The callable should return a sequence of two `Tensor`s. The first one\n       is a `Tensor` of the same `dtype` and `shape` as `coord_grid` and\n       represents a new coordinate grid after one iteration. The second `Tensor`\n       is of the same shape and `dtype` as`values_grid` and represents an\n       approximate solution of the equation after one iteration.\n       Default value: None, which means Crank-Nicolson scheme with oscillation\n       damping is used for 1D problems, and Douglas ADI scheme with `theta=0.5`\n       - for multidimensional problems.\n    boundary_conditions: The boundary conditions. Only rectangular boundary\n      conditions are supported.\n      A list of tuples of size `n` (space dimension\n      of the PDE). Each tuple consists of two callables representing the\n      boundary conditions at the minimum and maximum values of the spatial\n      variable indexed by the position in the list. E.g. for `n=2`, the length\n      of `boundary_conditions` should be 2, `boundary_conditions[0][0]`\n      describes the boundary `(y_min, x)`, and `boundary_conditions[1][0]`- the\n      boundary `(y, x_min)`. The boundary conditions are accepted in the form\n      `alpha(t, x) V + beta(t, x) V_n = gamma(t, x)`, where `V_n` is the\n      derivative with respect to the exterior normal to the boundary.\n      Each callable receives the current time `t` and the `coord_grid` at the\n      current time, and should return a tuple of `alpha`, `beta`, and `gamma`.\n      Each can be a number, a zero-rank `Tensor` or a `Tensor` whose shape is\n      the grid shape with the corresponding dimension removed.\n      For example, for a two-dimensional grid of shape `(b, ny, nx)`, where `b`\n      is the batch size, `boundary_conditions[0][i]` with `i = 0, 1` should\n      return a tuple of either numbers, zero-rank tensors or tensors of shape\n      `(b, nx)`. Similarly for `boundary_conditions[1][i]`, except the tensor\n      shape should be `(b, ny)`. `alpha` and `beta` can also be `None` in case\n      of Neumann and Dirichlet conditions, respectively.\n      Default value: None, which means Dirichlet conditions with zero value on\n      all boundaries are applied.\n    values_transform_fn: An optional callable applied to transform the solution\n      values at each time step. The callable is invoked after the time step has\n      been performed. The callable should accept the time of the grid, the\n      coordinate grid, and the values grid and should return a tuple of the\n      the coordinate grid and updated value grid.\n    second_order_coeff_fn: Callable returning the coefficients of the second\n      order terms of the PDE (i.e. `a_{ij}(t, x)` above) at given time `t`.\n      The callable accepts the following arguments:\n        `t`: The time at which the coefficient should be evaluated.\n        `coord_grid`: a `Tensor` representing a grid of locations `r` at which\n          the coefficient should be evaluated.\n      Returns the object `a` such that `a[i][j]` is defined and\n      `a[i][j]=a_{ij}(r, t)`, where `0 <= i < n_dims` and `i <= j < n_dims`.\n      For example, the object may be a list of lists or a rank 2 Tensor.\n      `a[i][j]` is assumed to be symmetrical, and only the elements with\n      `j >= i` will be used, so elements with `j < i` can be `None`.\n      Each `a[i][j]` should be a Number, a `Tensor` broadcastable to the shape\n      of `coord_grid`, or `None` if corresponding term is absent in the\n      equation. Also, the callable itself may be None, meaning there are no\n      second-order derivatives in the equation.\n      For example, for a 2D equation with the following second order terms\n      ```\n      a_xx V_xx + 2 a_xy V_xy + a_yy V_yy\n      ```\n       the callable may return either\n      `[[a_yy, a_xy], [a_xy, a_xx]]` or `[[a_yy, a_xy], [None, a_xx]]`.\n      Default value: None. If both `second_order_coeff_fn` and\n        `inner_second_order_coeff_fn` are None, it means the second-order term\n        is absent. If only one of them is `None`, it is assumed to be `1`.\n    first_order_coeff_fn: Callable returning the coefficients of the\n      first order terms of the PDE (i.e. `mu_i(t, r)` above) evaluated at given\n      time `t`.\n      The callable accepts the following arguments:\n        `t`: The time at which the coefficient should be evaluated.\n        `locations_grid`: a `Tensor` representing a grid of locations `r` at\n          which the coefficient should be evaluated.\n      Returns a list or an 1D `Tensor`, `i`-th element of which represents\n      `b_i(t, r)`. Each element is a `Tensor` broadcastable to the shape of\n      `locations_grid`, or None if corresponding term is absent in the\n      equation. The callable itself may be None, meaning there are no\n      first-order derivatives in the equation.\n      Default value: None. If both `first_order_coeff_fn` and\n        `inner_first_order_coeff_fn` are None, it means the first-order term is\n        absent. If only one of them is `None`, it is assumed to be `1`.\n    zeroth_order_coeff_fn: Callable returning the coefficient of the\n      zeroth order term of the PDE (i.e. `c(t, r)` above) evaluated at given\n      time `t`.\n      The callable accepts the following arguments:\n        `t`: The time at which the coefficient should be evaluated.\n        `locations_grid`: a `Tensor` representing a grid of locations `r` at\n          which the coefficient should be evaluated.\n      Should return a `Tensor` broadcastable to the shape of `locations_grid`.\n      May return None or be None if the shift term is absent in the equation.\n      Default value: None, meaning absent zeroth order term.\n    inner_second_order_coeff_fn: Callable returning the coefficients under the\n      second derivatives (i.e. `A_ij(t, x)` above) at given time `t`. The\n      requirements are the same as for `second_order_coeff_fn`.\n    inner_first_order_coeff_fn: Callable returning the coefficients under the\n      first derivatives (i.e. `B_i(t, x)` above) at given time `t`. The\n      requirements are the same as for `first_order_coeff_fn`.\n    maximum_steps: Optional int `Tensor`. The maximum number of time steps that\n      might be taken. This argument is only used if the `num_steps` is not used\n      and `time_step` is a callable otherwise it is ignored. It is useful to\n      supply this argument to ensure that the time stepping loop can be\n      optimized. If the argument is supplied and used, the time loop with\n      execute at most these many steps so it is important to ensure that this\n      parameter is an upper bound on the number of expected steps.\n    swap_memory: Whether GPU-CPU memory swap is enabled for this op. See\n      equivalent flag in `tf.while_loop` documentation for more details. Useful\n      when computing a gradient of the op.\n    dtype: The dtype to use.\n      Default value: None, which means dtype will be inferred from\n      `values_grid`.\n    name: The name to give to the ops.\n      Default value: None which means `solve_backward` is used.\n\n  Returns:\n    The final values grid, final coordinate grid, final time and number of steps\n    performed.\n\n  Raises:\n    ValueError if neither num steps nor time steps are provided or if both\n    are provided.\n  """"""\n  values_grid = tf.convert_to_tensor(values_grid, dtype=dtype)\n  start_time = tf.convert_to_tensor(\n      start_time, dtype=values_grid.dtype, name=\'start_time\')\n  end_time = tf.math.maximum(\n      tf.math.minimum(\n          tf.convert_to_tensor(end_time, dtype=values_grid.dtype,\n                               name=\'end_time\'),\n          start_time), 0)\n  return _solve(_time_direction_backward_fn,\n                start_time,\n                end_time,\n                coord_grid,\n                values_grid,\n                num_steps,\n                start_step_count,\n                time_step,\n                one_step_fn,\n                boundary_conditions,\n                values_transform_fn,\n                second_order_coeff_fn,\n                first_order_coeff_fn,\n                zeroth_order_coeff_fn,\n                inner_second_order_coeff_fn,\n                inner_first_order_coeff_fn,\n                maximum_steps,\n                swap_memory,\n                name or \'solve_backward\')\n\n\ndef solve_forward(start_time,\n                  end_time,\n                  coord_grid,\n                  values_grid,\n                  num_steps=None,\n                  start_step_count=0,\n                  time_step=None,\n                  one_step_fn=None,\n                  boundary_conditions=None,\n                  values_transform_fn=None,\n                  second_order_coeff_fn=None,\n                  first_order_coeff_fn=None,\n                  zeroth_order_coeff_fn=None,\n                  inner_second_order_coeff_fn=None,\n                  inner_first_order_coeff_fn=None,\n                  maximum_steps=None,\n                  swap_memory=True,\n                  dtype=None,\n                  name=None):\n  """"""Evolves a grid of function values forward in time according to a PDE.\n\n  Evolves a discretized solution of following second order linear\n  partial differential equation:\n\n  ```None\n    dV/dt + Sum[a_ij d2(A_ij V)/dx_i dx_j, 1 <= i, j <=n] +\n       Sum[b_i d(B_i V)/dx_i, 1 <= i <= n] + c V = 0.\n  ```\n  from time `t0` to time `t1 > t0` (i.e. forward in time). Here `a_ij`,\n  `A_ij`, `b_i`, `B_i` and `c` are coefficients that may depend on spatial\n  variables `x` and time `t`.\n\n  See more details in `solve_backwards()`: other than the forward time\n  direction, the specification is the same.\n\n  Args:\n    start_time: Real scalar `Tensor`. The start time of the grid.\n      Corresponds to time `t0` above.\n    end_time: Real scalar `Tensor` larger than the `start_time`.\n       The time to evolve forward to. Corresponds to time `t1` above.\n    coord_grid: List of `n` rank 1 real `Tensor`s. `n` is the dimension of the\n      domain. The i-th `Tensor` has shape, `[d_i]` where `d_i` is the size of\n      the grid along axis `i`. The coordinates of the grid points. Corresponds\n      to the spatial grid `G` above.\n    values_grid: Real `Tensor` containing the function values at time\n      `start_time` which have to be evolved to time `end_time`. The shape of the\n      `Tensor` must broadcast with `B + [d_1, d_2, ..., d_n]`. `B` is the batch\n      dimensions (one or more), which allow multiple functions (with potentially\n      different boundary/final conditions and PDE coefficients) to be evolved\n      simultaneously.\n    num_steps: Positive int scalar `Tensor`. The number of time steps to take\n      when moving from `start_time` to `end_time`. Either this argument or the\n      `time_step` argument must be supplied (but not both). If num steps is\n      `k>=1`, uniform time steps of size `(t1 - t0)/k` are taken to evolve the\n      solution from `t0` to `t1`. Corresponds to the `n_steps` parameter above.\n    start_step_count: A scalar integer `Tensor`. Number of steps performed so\n      far.\n    time_step: The time step to take. Either this argument or the `num_steps`\n      argument must be supplied (but not both). The type of this argument may\n      be one of the following (in order of generality):\n        (a) None in which case `num_steps` must be supplied.\n        (b) A positive real scalar `Tensor`. The maximum time step to take.\n          If the value of this argument is `dt`, then the total number of steps\n          taken is N = (t1 - t0) / dt rounded up to the nearest integer. The\n          first N-1 steps are of size dt and the last step is of size\n          `t1 - t0 - (N-1) * dt`.\n        (c) A callable accepting the current time and returning the size of the\n          step to take. The input and the output are real scalar `Tensor`s.\n    one_step_fn: The transition kernel. A callable that consumes the following\n      arguments by keyword:\n        1. \'time\': Current time\n        2. \'next_time\': The next time to step to. For the backwards in time\n          evolution, this time will be smaller than the current time.\n        3. \'coord_grid\': The coordinate grid.\n        4. \'values_grid\': The values grid.\n        5. \'second_order_coeff_fn\': Callable returning the coefficients of the\n          second order terms of the PDE. See the spec of the\n          `second_order_coeff_fn` argument below.\n        6. \'first_order_coeff_fn\': Callable returning the coefficients of the\n          first order terms of the PDE. See the spec of the\n          `first_order_coeff_fn` argument below.\n        7. \'zeroth_order_coeff_fn\': Callable returning the coefficient of the\n          zeroth order term of the PDE. See the spec of the\n          `zeroth_order_coeff_fn` argument below.\n        8. \'num_steps_performed\': A scalar integer `Tensor`. Number of steps\n          performed so far.\n       The callable should return a sequence of two `Tensor`s. The first one\n       is a `Tensor` of the same `dtype` and `shape` as `coord_grid` and\n       represents a new coordinate grid after one iteration. The second `Tensor`\n       is of the same shape and `dtype` as`values_grid` and represents an\n       approximate solution of the equation after one iteration.\n       Default value: None, which means Crank-Nicolson scheme with oscillation\n       damping is used for 1D problems, and Douglas ADI scheme with `theta=0.5`\n       - for multidimensional problems.\n    boundary_conditions: The boundary conditions. Only rectangular boundary\n      conditions are supported.\n      A list of tuples of size `n` (space dimension\n      of the PDE). Each tuple consists of two callables representing the\n      boundary conditions at the minimum and maximum values of the spatial\n      variable indexed by the position in the list. E.g. for `n=2`, the length\n      of `boundary_conditions` should be 2, `boundary_conditions[0][0]`\n      describes the boundary `(y_min, x)`, and `boundary_conditions[1][0]`- the\n      boundary `(y, x_min)`. The boundary conditions are accepted in the form\n      `alpha(t, x) V + beta(t, x) V_n = gamma(t, x)`, where `V_n` is the\n      derivative with respect to the exterior normal to the boundary.\n      Each callable receives the current time `t` and the `coord_grid` at the\n      current time, and should return a tuple of `alpha`, `beta`, and `gamma`.\n      Each can be a number, a zero-rank `Tensor` or a `Tensor` whose shape is\n      the grid shape with the corresponding dimension removed.\n      For example, for a two-dimensional grid of shape `(b, ny, nx)`, where `b`\n      is the batch size, `boundary_conditions[0][i]` with `i = 0, 1` should\n      return a tuple of either numbers, zero-rank tensors or tensors of shape\n      `(b, nx)`. Similarly for `boundary_conditions[1][i]`, except the tensor\n      shape should be `(b, ny)`. `alpha` and `beta` can also be `None` in case\n      of Neumann and Dirichlet conditions, respectively.\n      Default value: None, which means Dirichlet conditions with zero value on\n      all boundaries are applied.\n    values_transform_fn: An optional callable applied to transform the solution\n      values at each time step. The callable is invoked after the time step has\n      been performed. The callable should accept the time of the grid, the\n      coordinate grid and the values grid and should return the values grid. All\n      input arguments to be passed by keyword.\n      It returns the updated value grid and the coordinate grid, which may be\n      updated as well.\n    second_order_coeff_fn: Callable returning the coefficients of the second\n      order terms of the PDE (i.e. `a_{ij}(t, x)` above) at given time `t`.\n      The callable accepts the following arguments:\n        `t`: The time at which the coefficient should be evaluated.\n        `coord_grid`: a `Tensor` representing a grid of locations `r` at which\n          the coefficient should be evaluated.\n      Returns the object `a` such that `a[i][j]` is defined and\n      `a[i][j]=a_{ij}(r, t)`, where `0 <= i < n_dims` and `i <= j < n_dims`.\n      For example, the object may be a list of lists or a rank 2 Tensor.\n      `a[i][j]` is assumed to be symmetrical, and only the elements with\n      `j >= i` will be used, so elements with `j < i` can be `None`.\n      Each `a[i][j]` should be a Number, a `Tensor` broadcastable to the shape\n      of `coord_grid`, or `None` if corresponding term is absent in the\n      equation. Also, the callable itself may be None, meaning there are no\n      second-order derivatives in the equation.\n      For example, for a 2D equation with the following second order terms\n      ```\n      a_xx V_xx + 2 a_xy V_xy + a_yy V_yy\n      ```\n       the callable may return either\n      `[[a_yy, a_xy], [a_xy, a_xx]]` or `[[a_yy, a_xy], [None, a_xx]]`.\n      Default value: None. If both `second_order_coeff_fn` and\n        `inner_second_order_coeff_fn` are None, it means the second-order term\n        is absent. If only one of them is `None`, it is assumed to be `1`.\n    first_order_coeff_fn: Callable returning the coefficients of the\n      first order terms of the PDE (i.e. `mu_i(t, r)` above) evaluated at given\n      time `t`.\n      The callable accepts the following arguments:\n        `t`: The time at which the coefficient should be evaluated.\n        `locations_grid`: a `Tensor` representing a grid of locations `r` at\n          which the coefficient should be evaluated.\n      Returns a list or an 1D `Tensor`, `i`-th element of which represents\n      `b_i(t, r)`. Each element is a `Tensor` broadcastable to the shape of\n      `locations_grid`, or None if corresponding term is absent in the\n      equation. The callable itself may be None, meaning there are no\n      first-order derivatives in the equation.\n      Default value: None. If both `first_order_coeff_fn` and\n        `inner_first_order_coeff_fn` are None, it means the first-order term is\n        absent. If only one of them is `None`, it is assumed to be `1`.\n    zeroth_order_coeff_fn: Callable returning the coefficient of the\n      zeroth order term of the PDE (i.e. `c(t, r)` above) evaluated at given\n      time `t`.\n      The callable accepts the following arguments:\n        `t`: The time at which the coefficient should be evaluated.\n        `locations_grid`: a `Tensor` representing a grid of locations `r` at\n          which the coefficient should be evaluated.\n      Should return a `Tensor` broadcastable to the shape of `locations_grid`.\n      May return None or be None if the shift term is absent in the equation.\n      Default value: None, meaning absent zeroth order term.\n    inner_second_order_coeff_fn: Callable returning the coefficients under the\n      second derivatives (i.e. `A_ij(t, x)` above) at given time `t`. The\n      requirements are the same as for `second_order_coeff_fn`.\n    inner_first_order_coeff_fn: Callable returning the coefficients under the\n      first derivatives (i.e. `B_i(t, x)` above) at given time `t`. The\n      requirements are the same as for `first_order_coeff_fn`.\n    maximum_steps: Optional int `Tensor`. The maximum number of time steps that\n      might be taken. This argument is only used if the `num_steps` is not used\n      and `time_step` is a callable otherwise it is ignored. It is useful to\n      supply this argument to ensure that the time stepping loop can be\n      optimized. If the argument is supplied and used, the time loop with\n      execute at most these many steps so it is important to ensure that this\n      parameter is an upper bound on the number of expected steps.\n    swap_memory: Whether GPU-CPU memory swap is enabled for this op. See\n      equivalent flag in `tf.while_loop` documentation for more details. Useful\n      when computing a gradient of the op.\n    dtype: The dtype to use.\n      Default value: None, which means dtype will be inferred from\n      `values_grid`.\n    name: The name to give to the ops.\n      Default value: None which means `solve_forward` is used.\n\n  Returns:\n    The final values grid, final coordinate grid, final time and number of steps\n    performed.\n\n  Raises:\n    ValueError if neither num steps nor time steps are provided or if both\n    are provided.\n  """"""\n  values_grid = tf.convert_to_tensor(values_grid, dtype=dtype)\n  start_time = tf.convert_to_tensor(\n      start_time, dtype=values_grid.dtype, name=\'start_time\')\n  end_time = tf.math.maximum(\n      tf.convert_to_tensor(end_time, dtype=values_grid.dtype, name=\'end_time\'),\n      start_time)\n  return _solve(_time_direction_forward_fn,\n                start_time,\n                end_time,\n                coord_grid,\n                values_grid,\n                num_steps,\n                start_step_count,\n                time_step,\n                one_step_fn,\n                boundary_conditions,\n                values_transform_fn,\n                second_order_coeff_fn,\n                first_order_coeff_fn,\n                zeroth_order_coeff_fn,\n                inner_second_order_coeff_fn,\n                inner_first_order_coeff_fn,\n                maximum_steps,\n                swap_memory,\n                name or \'solve_forward\')\n\n\ndef _solve(\n    time_direction_fn,\n    start_time,\n    end_time,\n    coord_grid,\n    values_grid,\n    num_steps=None,\n    start_step_count=0,\n    time_step=None,\n    one_step_fn=None,\n    boundary_conditions=None,\n    values_transform_fn=None,\n    second_order_coeff_fn=None,\n    first_order_coeff_fn=None,\n    zeroth_order_coeff_fn=None,\n    inner_second_order_coeff_fn=None,\n    inner_first_order_coeff_fn=None,\n    maximum_steps=None,\n    swap_memory=True,\n    name=None):\n  """"""Common code for solve_backward and solve_forward.""""""\n  if (num_steps is None) == (time_step is None):\n    raise ValueError(\'Exactly one of num_steps or time_step\'\n                     \' should be supplied.\')\n\n  coord_grid = [\n      tf.convert_to_tensor(dim_grid, dtype=values_grid.dtype)\n      for dim_grid in coord_grid\n  ]\n\n  n_dims = len(coord_grid)\n  if one_step_fn is None:\n    if n_dims == 1:\n      one_step_fn = oscillation_damped_crank_nicolson_step()\n    else:\n      one_step_fn = douglas_adi_step(theta=0.5)\n\n  if boundary_conditions is None:\n\n    def zero_dirichlet(t, grid):\n      del t, grid\n      return 1, None, tf.constant(0, dtype=values_grid.dtype)\n\n    boundary_conditions = [(zero_dirichlet, zero_dirichlet)] * n_dims\n\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'solve\',\n      values=[\n          start_time,\n          end_time,\n          coord_grid,\n          values_grid,\n          num_steps,\n          time_step,\n      ]):\n    time_step_fn, est_max_steps = _get_time_steps_info(start_time, end_time,\n                                                       num_steps, time_step,\n                                                       time_direction_fn)\n    if est_max_steps is None and maximum_steps is not None:\n      est_max_steps = maximum_steps\n\n    def loop_cond(should_stop, time, x_grid, f_grid, steps_performed):\n      del time, x_grid, f_grid, steps_performed\n      return tf.logical_not(should_stop)\n\n    def loop_body(should_stop, time, x_grid, f_grid, steps_performed):\n      """"""Propagates the grid in time.""""""\n      del should_stop\n      next_should_stop, t_next = time_step_fn(time)\n      next_xs, next_fs = one_step_fn(\n          time=time,\n          next_time=t_next,\n          coord_grid=x_grid,\n          value_grid=f_grid,\n          boundary_conditions=boundary_conditions,\n          second_order_coeff_fn=second_order_coeff_fn,\n          first_order_coeff_fn=first_order_coeff_fn,\n          zeroth_order_coeff_fn=zeroth_order_coeff_fn,\n          inner_second_order_coeff_fn=inner_second_order_coeff_fn,\n          inner_first_order_coeff_fn=inner_first_order_coeff_fn,\n          num_steps_performed=steps_performed)\n\n      if values_transform_fn is not None:\n        next_xs, next_fs = values_transform_fn(t_next, next_xs, next_fs)\n      return next_should_stop, t_next, next_xs, next_fs, steps_performed + 1\n\n    # If the start time is already equal to end time, no stepping is needed.\n    # solve_backward, solve_forward already took care of the case when end_time\n    # is on the ""wrong side"" of start_time.\n    should_already_stop = (start_time == end_time)\n    initial_args = (should_already_stop, start_time, coord_grid, values_grid,\n                    start_step_count)\n    (_, final_time, final_coords, final_values,\n     steps_performed) = tf.while_loop(\n         loop_cond,\n         loop_body,\n         initial_args,\n         swap_memory=swap_memory,\n         maximum_iterations=est_max_steps)\n    return final_values, final_coords, final_time, steps_performed\n\n\ndef _is_callable(var_or_fn):\n  """"""Returns whether an object is callable or not.""""""\n  # Python 2.7 as well as Python 3.x with x > 2 support \'callable\'.\n  # In between, callable was removed hence we need to do a more expansive check\n  if hasattr(var_or_fn, \'__call__\'):\n    return True\n  try:\n    return callable(var_or_fn)\n  except NameError:\n    return False\n\n\ndef _get_time_steps_info(start_time, end_time, num_steps, time_step,\n                         time_direction_fn):\n  """"""Creates a callable to step through time and estimates the max steps.""""""\n  # time_direction_fn must be one of _time_step_forward_fn and\n  # _time_step_backward_fn\n  dt = None\n  estimated_max_steps = None\n  interval = tf.math.abs(end_time - start_time)\n  if num_steps is not None:\n    dt = interval / tf.cast(num_steps, dtype=start_time.dtype)\n    estimated_max_steps = num_steps\n  if time_step is not None and not _is_callable(time_step):\n    dt = time_step\n    estimated_max_steps = tf.cast(tf.math.ceil(interval / dt), dtype=tf.int32)\n  if dt is not None:\n    raw_time_step_fn = lambda _: dt\n  else:\n    raw_time_step_fn = time_step\n\n  def time_step_fn(t):\n    # t is the current time.\n    # t_next is the next time\n    dt = raw_time_step_fn(t)\n    should_stop, t_next = time_direction_fn(t, dt, end_time)\n    return should_stop, t_next\n\n  return time_step_fn, estimated_max_steps\n\n\ndef _time_direction_forward_fn(t, dt, end_time):\n  t_next = tf.math.minimum(end_time, t + dt)\n  return t_next >= end_time, t_next\n\n\ndef _time_direction_backward_fn(t, dt, end_time):\n  t_next = tf.math.maximum(end_time, t - dt)\n  return t_next <= end_time, t_next\n\n\n__all__ = [\'solve_backward\', \'solve_forward\']\n'"
tf_quant_finance/math/pde/grids.py,70,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Functions to create grids suitable for PDE pricing.""""""\n\nimport numpy as np\nfrom six.moves import range\nimport tensorflow.compat.v2 as tf\n\n\ndef uniform_grid(minimums,\n                 maximums,\n                 sizes,\n                 dtype=None,\n                 validate_args=False,\n                 name=None):\n  """"""Creates a grid spec for a uniform grid.\n\n  A uniform grid is characterized by having a constant gap between neighboring\n  points along each axis.\n\n  Note that the shape of all three parameters must be fully defined and equal\n  to each other. The shape is used to determine the dimension of the grid.\n\n  Args:\n    minimums: Real `Tensor` of rank 1 containing the lower end points of the\n      grid. Must have the same shape as those of `maximums` and `sizes` args.\n    maximums: `Tensor` of the same dtype and shape as `minimums`. The upper\n      endpoints of the grid.\n    sizes: Integer `Tensor` of the same shape as `minimums`. The size of the\n      grid in each axis. Each entry must be greater than or equal to 2 (i.e. the\n      sizes include the end points). For example, if minimums = [0.] and\n      maximums = [1.] and sizes = [3], the grid will have three points at [0.0,\n      0.5, 1.0].\n    dtype: Optional tf.dtype. The default dtype to use for the grid.\n    validate_args: Python boolean indicating whether to validate the supplied\n      arguments. The validation checks performed are (a) `maximums` > `minimums`\n      (b) `sizes` >= 2.\n    name: Python str. The name prefixed to the ops created by this function. If\n      not supplied, the default name \'uniform_grid_spec\' is used.\n\n  Returns:\n    The grid locations as projected along each axis. One `Tensor` of shape\n    `[..., n]`, where `n` is the number of points along that axis. The first\n    dimensions are the batch shape. The grid itself can be seen as a cartesian\n    product of the locations array.\n\n  Raises:\n    ValueError if the shape of maximums, minimums and sizes are not fully\n    defined or they are not identical to each other or they are not rank 1.\n  """"""\n  with tf.compat.v1.name_scope(name, \'uniform_grid\',\n                               [minimums, maximums, sizes]):\n    minimums = tf.convert_to_tensor(minimums, dtype=dtype, name=\'minimums\')\n    maximums = tf.convert_to_tensor(maximums, dtype=dtype, name=\'maximums\')\n    sizes = tf.convert_to_tensor(sizes, name=\'sizes\')\n    # Check that the shape of `sizes` is statically defined.\n    if not _check_shapes_fully_defined(minimums, maximums, sizes):\n      raise ValueError(\'The shapes of minimums, maximums and sizes \'\n                       \'must be fully defined.\')\n\n    if not (minimums.shape == maximums.shape and minimums.shape == sizes.shape):\n      raise ValueError(\'The shapes of minimums, maximums and sizes must be \'\n                       \'identical.\')\n\n    if len(minimums.shape.as_list()) != 1:\n      raise ValueError(\'The minimums, maximums and sizes must all be rank 1.\')\n\n    control_deps = []\n    if validate_args:\n      control_deps = [\n          tf.compat.v1.debugging.assert_greater(maximums, minimums),\n          tf.compat.v1.debugging.assert_greater_equal(sizes, 2)\n      ]\n    with tf.compat.v1.control_dependencies(control_deps):\n      dim = sizes.shape[0]\n      locations = [\n          tf.linspace(minimums[i], maximums[i], num=sizes[i])\n          for i in range(dim)\n      ]\n      return locations\n\n\ndef log_uniform_grid(minimums,\n                     maximums,\n                     sizes,\n                     dtype=None,\n                     validate_args=False,\n                     name=None):\n  """"""Creates a grid spec for a uniform grid in a log-space.\n\n  A log-uniform grid is characterized by having a constant gap between\n  neighboring points along each axis in the log-space, i.e., the logarithm of\n  output grid is the uniform grid.\n\n  Note that the shape of all three parameters must be fully defined and equal\n  to each other. The shape is used to determine the dimension of the grid.\n  Note that all the parameters are supplied and returned for the original space\n  and not the log-space.\n\n  #### Examples\n\n  ```python\n  dtype = np.float64\n  min_x, max_x, sizes = [0.1], [3.0], [5]\n  # Here min_x and max_x are in the original space and *not* in the log-space.\n  grid = log_uniform_grid(min_x, max_x, sizes,dtype=dtype)\n  with tf.Session() as sess:\n    grid = sess.run(grid)\n  # Note that the minimum and maximum grid locations are the same as min_x and\n  # max_x.\n  print(\'locations: \', grid.locations)\n  # locations:  [array([ 0.1, 0.234, 0.548, 1.282, 3.0])]\n  print(\'grid: \', grid.grid)\n  # grid: array([[ 0.1], [0.234], [0.548], [1.282], [ 3.0]])\n  print(\'deltas: \', grid.deltas)\n  # deltas: [array([ 0.134, 0.314, 0.734, 1.718])]\n  ```\n\n  Args:\n    minimums: Real `Tensor` of rank 1 containing the lower end points of the\n      output grid. Must have the same shape as those of `maximums` and `sizes`\n      args.\n    maximums: `Tensor` of the same dtype and shape as `minimums`. The upper\n      endpoints of the output grid.\n    sizes: Integer `Tensor` of the same shape as `minimums`. The size of the\n      grid in each axis. Each entry must be greater than or equal to 2 (i.e. the\n      sizes include the end points).\n    dtype: Optional tf.dtype. The default dtype to use for the grid.\n    validate_args: Python boolean indicating whether to validate the supplied\n      arguments. The validation checks performed are (a) `maximums` > `minimums`\n      (b) `minimums` > 0.0 (c) `sizes` >= 2.\n    name: Python str. The name prefixed to the ops created by this function. If\n      not supplied, the default name \'uniform_grid_spec\' is used.\n\n  Returns:\n    The grid locations as projected along each axis. One `Tensor` of shape\n    `[..., n]`, where `n` is the number of points along that axis. The first\n    dimensions are the batch shape. The grid itself can be seen as a cartesian\n    product of the locations array.\n  Raises:\n    ValueError if the shape of maximums, minimums and sizes are not fully\n    defined or they are not identical to each other or they are not rank 1.\n  """"""\n  with tf.compat.v1.name_scope(name, \'log_uniform_grid\',\n                               [minimums, maximums, sizes]):\n    minimums = tf.convert_to_tensor(minimums, dtype=dtype, name=\'minimums\')\n    maximums = tf.convert_to_tensor(maximums, dtype=dtype, name=\'maximums\')\n    sizes = tf.convert_to_tensor(sizes, name=\'sizes\')\n    # Check that the shape of `sizes` is statically defined.\n    if not _check_shapes_fully_defined(minimums, maximums, sizes):\n      raise ValueError(\'The shapes of minimums, maximums and sizes \'\n                       \'must be fully defined.\')\n\n    if not (minimums.shape == maximums.shape and minimums.shape == sizes.shape):\n      raise ValueError(\'The shapes of minimums, maximums and sizes must be \'\n                       \'identical.\')\n\n    if len(minimums.shape.as_list()) != 1:\n      raise ValueError(\'The minimums, maximums and sizes must all be rank 1.\')\n\n    control_deps = []\n    if validate_args:\n      control_deps = [\n          tf.compat.v1.debugging.assert_greater(maximums, minimums),\n          tf.compat.v1.debugging.assert_greater(minimums,\n                                                tf.constant(0, dtype=dtype)),\n          tf.compat.v1.debugging.assert_greater_equal(sizes, 2)\n      ]\n    # Generate a uniform grid in the log-space taking into account that the\n    # arguments were already validated.\n    with tf.compat.v1.control_dependencies(control_deps):\n      dim = sizes.shape[0]\n      log_maximums = tf.math.log(maximums)\n      log_minimums = tf.math.log(minimums)\n\n      locations = [\n          tf.exp(tf.linspace(log_minimums[i], log_maximums[i], num=sizes[i]))\n          for i in range(dim)\n      ]\n      return locations\n\n\ndef rectangular_grid(axis_locations,\n                     dtype=None,\n                     name=None):\n  """"""Specifies parameters for an axis-wise non-uniform grid in n-dimensions.\n\n  This specifies rectangular grids formed by taking the cartesian product\n  of points along each axis. For example, in two dimensions, one may specify\n  a grid by specifying points along the x-axis as [0.0, 1.0, 1.3] and along the\n  y-axis by [3.0, 3.6, 4.3, 7.0]. Taking the cartesian product of the two,\n  produces a 3 x 4 grid which is rectangular but non-uniform along each axis.\n\n  The points along each axis should be in ascending order and there must be at\n  least two points specified along each axis. If `validate_args` is set to\n  True, both these conditions are explicitly verified.\n\n  Args:\n    axis_locations: A Python iterable of rank 1 real `Tensor`s. The number of\n      `Tensor`s in the list is the dimension of the grid. The i\'th element\n      specifies the coordinates of the points of the grid along that axis. Each\n      `Tensor` must have at least two elements.\n    dtype: Optional tf.dtype. The default dtype to use for the grid..\n    name: Python str. The name prefixed to the ops created by this class. If not\n      supplied, the default name \'rectangular_grid\' is used.\n\n  Returns:\n    The grid locations as projected along each axis. One `Tensor` of shape\n    `[..., n]`, where `n` is the number of points along that axis. The first\n    dimensions are the batch shape. The grid itself can be seen as a cartesian\n    product of the locations array.\n\n  Raises:\n    ValueError if `axis_locations` is empty.\n  """"""\n  with tf.compat.v1.name_scope(name, \'rectangular_grid\', [axis_locations]):\n    if not axis_locations:\n      raise ValueError(\'The axis locations parameter cannot be empty.\')\n    locations = [\n        tf.convert_to_tensor(\n            location, dtype=dtype, name=\'location_axis_{}\'.format(i))\n        for i, location in enumerate(axis_locations)\n    ]\n    return locations\n\n\ndef uniform_grid_with_extra_point(minimums,\n                                  maximums,\n                                  sizes,\n                                  extra_grid_point,\n                                  dtype=None,\n                                  validate_args=False,\n                                  name=None):\n  """"""Creates a grid spec for a uniform grid with an extra grid point.\n\n  A uniform grid is characterized by having a constant gap between neighboring\n  points along each axis. An extra grid point is useful, for example, when\n  computing sensitivities for a value through a grid pricing method\n\n  #### Examples\n\n  ```python\n  dtype = np.float64\n  extra_locations = tf.constant([[1, 2], [2, 3]], dtype=dtype)\n  min_x, max_x, sizes = [[0, 0], [0, 0]], [[10, 5], [100, 5]], [3, 2]\n  # Here min_x and max_x are in the original space and *not* in the log-space.\n  grid = uniform_grid_with_extra_point(\n      min_x, max_x, sizes,\n      extra_grid_point=extra_locations, dtype=dtype)\n  with tf.Session() as sess:\n    grid = sess.run(grid)\n  # Note that the minimum and maximum grid locations are the same as min_x and\n  # max_x.\n  print(grid.locations[0])\n  # [[0, 1, 5, 10], [0, 2, 50, 100]]\n  print(grid.locations[1])\n  # [[0, 2, 5], [0, 3, 5]]\n  ```\n\n  Note that the shape of all four parameters must be fully defined and equal\n  to each other. The shape is used to determine the dimension of the grid.\n\n  Args:\n    minimums: Real `Tensor` of rank 1 or 2 containing the lower end points of\n      the grid. Must have the same shape as those of `maximums`. When rank is 2\n      the first dimension is the batch dimension.\n    maximums: `Tensor` of the same dtype and shape as `minimums`. The upper\n      endpoints of the grid.\n    sizes: Integer rank 1 `Tensor` of the same shape as `minimums`. The size of\n      the grid in each axis. Each entry must be greater than or equal to 2 (i.e.\n      the sizes include the end points). For example, if minimums = [0.] and\n      maximums = [1.] and sizes = [3], the grid will have three points at [0.0,\n      0.5, 1.0].\n    extra_grid_point: A `Tensor` of the same `dtype` as `minimums` and of shape\n      `[batch_size, n]`, where `batch_shape` is a positive integer and `n` is\n      the number of points along a dimension. These are the extra points added\n      to the grid, so that the output grid `locations` have shape `[batch_shape,\n      n+1]`.\n    dtype: Optional tf.dtype. The default dtype to use for the grid.\n    validate_args: Python boolean indicating whether to validate the supplied\n      arguments. The validation checks performed are (a) `maximums` > `minimums`\n      (b) `sizes` >= 2.\n    name: Python str. The name prefixed to the ops created by this function. If\n      not supplied, the default name \'uniform_grid_spec\' is used.\n\n  Returns:\n    The grid locations as projected along each axis. One `Tensor` of shape\n    `[..., n]`, where `n` is the number of points along that axis. The first\n    dimensions are the batch shape. The grid itself can be seen as a cartesian\n    product of the locations array.\n\n  Raises:\n    ValueError if the shape of maximums, minimums and sizes are not fully\n    defined or they are not identical to each other or they are not rank 1.\n  """"""\n  with tf.compat.v1.name_scope(name, \'uniform_grid\',\n                               [minimums, maximums, sizes]):\n    minimums = tf.convert_to_tensor(minimums, dtype=dtype, name=\'minimums\')\n    maximums = tf.convert_to_tensor(maximums, dtype=dtype, name=\'maximums\')\n    extra_grid_point = tf.convert_to_tensor(\n        extra_grid_point, dtype=dtype, name=\'extra_grid_point\')\n    batch_shape = tf.shape(extra_grid_point)[0]\n    sizes = tf.convert_to_tensor(sizes, name=\'sizes\')\n    # Check that the shape of `sizes` is statically defined.\n    if not _check_shapes_fully_defined(minimums, maximums, sizes):\n      raise ValueError(\'The shapes of minimums, maximums and sizes \'\n                       \'must be fully defined.\')\n\n    if minimums.shape != maximums.shape:\n      raise ValueError(\'The shapes of minimums and maximums must be identical.\')\n\n    control_deps = []\n    if validate_args:\n      control_deps = [\n          tf.compat.v1.debugging.assert_greater(maximums, minimums),\n          tf.compat.v1.debugging.assert_greater_equal(sizes, 2)\n      ]\n    locations = []\n    with tf.compat.v1.control_dependencies(control_deps):\n      dim = sizes.shape[0]\n      for i in range(dim):\n        locations.append(\n            tf.expand_dims(minimums[..., i], -1) +\n            tf.expand_dims((maximums[..., i] - minimums[..., i]), -1) * tf\n            .linspace(tf.constant(0., dtype=minimums.dtype), 1.0, num=sizes[i]))\n      # Broadcast `locations` to the shape `[batch_shape, size]`\n      for i, location in enumerate(locations):\n        locations[i] = location + tf.zeros([batch_shape, sizes[i]], dtype=dtype)\n      # Add `extra_grid_point` to `locations`\n      for i, location in enumerate(locations):\n        location_update = tf.sort(\n            tf.concat(\n                [location, tf.expand_dims(extra_grid_point[:, i], -1)], -1), -1)\n        locations[i] = location_update\n      return locations\n\n\ndef log_uniform_grid_with_extra_point(minimums,\n                                      maximums,\n                                      sizes,\n                                      extra_grid_point,\n                                      dtype=None,\n                                      validate_args=False,\n                                      name=None):\n  """"""Creates a grid for a uniform grid in a log-space with an extra grid point.\n\n  A log-uniform grid is characterized by having a constant gap between\n  neighboring points along each axis in the log-space, i.e., the logarithm of\n  output grid is the uniform grid.  An extra grid point is useful, for example,\n  when computing sensitivities for a value through a grid pricing method.\n\n  Note that the shape of all three parameters must be fully defined and equal\n  to each other. The shape is used to determine the dimension of the grid.\n  Note that all the parameters are supplied and returned for the original space\n  and not the log-space.\n\n  #### Examples\n\n  ```python\n  dtype = np.float64\n  extra_locations = tf.constant([[0.5, 2], [2, 3]], dtype=dtype)\n  min_x, max_x, sizes = [[0.1, 0.1], [0.01, 0.1]], [[10, 5], [100, 5]], [3, 2]\n  # Here min_x and max_x are in the original space and *not* in the log-space.\n  grid = log_uniform_grid_with_extra_point(\n      min_x, max_x, sizes,\n      extra_grid_point=extra_locations, dtype=dtype)\n  with tf.Session() as sess:\n    grid = sess.run(grid)\n  # Note that the minimum and maximum grid locations are the same as min_x and\n  # max_x.\n  print(grid.locations[0])\n  # [[0.1, 0.5, 1.0, 10.0], [0.01, 1.0, 2.0, 100.0]]\n  print(grid.locations[1])\n  # [[0.1, 2, 5], [0.1, 3, 5]]\n  ```\n\n  Args:\n    minimums: Real `Tensor` of rank 1 or 2 containing the lower end points of\n      the grid. Must have the same shape as those of `maximums`. When rank is 2\n      the first dimension is the batch dimension.\n    maximums: `Tensor` of the same dtype and shape as `minimums`. The upper\n      endpoints of the grid.\n    sizes: Integer rank 1 `Tensor` of the same shape as `minimums`. The size of\n      the grid in each axis. Each entry must be greater than or equal to 2 (i.e.\n      the sizes include the end points).\n    extra_grid_point: A `Tensor` of the same `dtype` as `minimums` and of shape\n      `[batch_size, n]`, where `batch_shape` is a positive integer and `n` is\n      the number of points along a dimension. These are the extra points added\n      to the grid, so that the output grid `locations` have shape `[batch_shape,\n      n+1]`.\n    dtype: Optional tf.dtype. The default dtype to use for the grid.\n    validate_args: Python boolean indicating whether to validate the supplied\n      arguments. The validation checks performed are (a) `maximums` > `minimums`\n      (b) `minimums` > 0.0 (c) `sizes` >= 2.\n    name: Python str. The name prefixed to the ops created by this function. If\n      not supplied, the default name \'uniform_grid_spec\' is used.\n\n  Returns:\n    The grid locations as projected along each axis. One `Tensor` of shape\n    `[..., n]`, where `n` is the number of points along that axis. The first\n    dimensions are the batch shape. The grid itself can be seen as a cartesian\n    product of the locations array.\n\n  Raises:\n    ValueError if the shape of maximums, minimums and sizes are not fully\n    defined or they are not identical to each other or they are not rank 1.\n  """"""\n  with tf.compat.v1.name_scope(name, \'log_uniform_grid\',\n                               [minimums, maximums, sizes]):\n    minimums = tf.convert_to_tensor(minimums, dtype=dtype, name=\'minimums\')\n    maximums = tf.convert_to_tensor(maximums, dtype=dtype, name=\'maximums\')\n    sizes = tf.convert_to_tensor(sizes, name=\'sizes\')\n    extra_grid_point = tf.convert_to_tensor(\n        extra_grid_point, dtype=dtype, name=\'extra_grid_point\')\n    batch_shape = tf.shape(extra_grid_point)[0]\n    # Check that the shape of `sizes` is statically defined.\n    if not _check_shapes_fully_defined(minimums, maximums, sizes):\n      raise ValueError(\'The shapes of minimums, maximums and sizes \'\n                       \'must be fully defined.\')\n\n    if minimums.shape != maximums.shape:\n      raise ValueError(\'The shapes of minimums and maximums must be identical.\')\n\n    control_deps = []\n    if validate_args:\n      control_deps = [\n          tf.compat.v1.debugging.assert_greater(maximums, minimums),\n          tf.compat.v1.debugging.assert_greater(minimums,\n                                                tf.constant(0, dtype=dtype)),\n          tf.compat.v1.debugging.assert_greater_equal(sizes, 2)\n      ]\n    # Generate a uniform grid in the log-space taking into account that the\n    # arguments were already validated.\n    locations = []\n    with tf.compat.v1.control_dependencies(control_deps):\n      dim = sizes.shape[0]\n      log_maximums = tf.math.log(maximums)\n      log_minimums = tf.math.log(minimums)\n      for i in range(dim):\n        locations.append(\n            tf.expand_dims(log_minimums[..., i], -1) +\n            tf.expand_dims((log_maximums[..., i] - log_minimums[..., i]), -1) *\n            tf.linspace(\n                tf.constant(0., dtype=minimums.dtype), 1.0, num=sizes[i]))\n      # Broadcast `locations` to the shape `[batch_shape, size]`\n      for i, location in enumerate(locations):\n        locations[i] = location + tf.zeros([batch_shape, sizes[i]], dtype=dtype)\n      # Add `extra_grid_point` to `locations`\n      for i, location in enumerate(locations):\n        location_update = tf.sort(\n            tf.concat(\n                [location,\n                 tf.expand_dims(tf.math.log(\n                     extra_grid_point[:, i]), -1)], -1), -1)\n        locations[i] = tf.exp(location_update)\n      return locations\n\n\ndef _check_shapes_fully_defined(*args):\n  """"""Checks if all the arguments have fully defined shapes.""""""\n  return np.all([arg.shape.is_fully_defined() for arg in args])\n\n\n__all__ = [\n    \'uniform_grid\',\n    \'uniform_grid_with_extra_point\',\n    \'rectangular_grid\',\n    \'log_uniform_grid\',\n    \'log_uniform_grid_with_extra_point\',\n]\n'"
tf_quant_finance/math/random_ops/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Ops related to random or quasi random sampling.""""""\n\n\nfrom tf_quant_finance.math.random_ops import halton\nfrom tf_quant_finance.math.random_ops import sobol\nfrom tf_quant_finance.math.random_ops.multivariate_normal import multivariate_normal as mv_normal_sample\nfrom tf_quant_finance.math.random_ops.multivariate_normal import RandomType\nfrom tf_quant_finance.math.random_ops.stateless import stateless_random_shuffle\nfrom tf_quant_finance.math.random_ops.uniform import uniform\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'halton\',\n    \'sobol\',\n    \'mv_normal_sample\',\n    \'RandomType\',\n    \'stateless_random_shuffle\',\n    \'uniform\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/math/random_ops/multivariate_normal.py,26,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Multivariate Normal distribution with various random types.""""""\n\nimport enum\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance.math.random_ops import halton\nfrom tf_quant_finance.math.random_ops import sobol\n\n\n_SQRT_2 = np.sqrt(2.)\n\n\n@enum.unique\nclass RandomType(enum.Enum):\n  """"""Types of random number sequences.\n\n  * `PSEUDO`: The standard Tensorflow random generator.\n  * `STATELESS`: The stateless Tensorflow random generator.\n  * `HALTON`: The standard Halton sequence.\n  * `HALTON_RANDOMIZED`: The randomized Halton sequence.\n  * `SOBOL`: The standard Sobol sequence.\n  * `PSEUDO_ANTITHETIC`: PSEUDO random numbers along with antithetic variates.\n  """"""\n  PSEUDO = 0\n  STATELESS = 1\n  HALTON = 2\n  HALTON_RANDOMIZED = 3\n  SOBOL = 4\n  PSEUDO_ANTITHETIC = 5\n  STATELESS_ANTITHETIC = 6\n\n\ndef multivariate_normal(sample_shape,\n                        mean=None,\n                        covariance_matrix=None,\n                        scale_matrix=None,\n                        random_type=None,\n                        validate_args=False,\n                        seed=None,\n                        dtype=None,\n                        name=None,\n                        **kwargs):\n  """"""Generates draws from a multivariate Normal distribution.\n\n  Draws samples from the multivariate Normal distribution on `R^k` with the\n  supplied mean and covariance parameters. Allows generating either\n  (pseudo) random or quasi-random draws based on the `random_type` parameter.\n\n  #### Example:\n\n  ```python\n\n  # A single batch example.\n  sample_shape = [10]  # Generates 10 draws.\n  mean = [0.1, 0.2]  # The mean of the distribution. A single batch.\n  covariance = [[1.0, 0.1], [0.1, 0.9]]\n  # Produces a Tensor of shape [10, 2] containing 10 samples from the\n  # 2 dimensional normal. The draws are generated using the standard random\n  # number generator in TF.\n  sample = multivariate_normal(sample_shape, mean=mean,\n                               covariance_matrix=covariance,\n                               random_type=RandomType.PSEUDO)\n\n  # Produces a Tensor of shape [10, 2] containing 10 samples from the\n  # 2 dimensional normal. Here the draws are generated using the stateless\n  # random number generator. Note that a seed parameter is required and may\n  # not be omitted. For the fixed seed, the same numbers will be produced\n  # regardless of the rest of the graph or across independent sessions.\n  sample_stateless = multivariate_normal(sample_shape, mean=mean,\n                                         covariance_matrix=covariance,\n                                         random_type=RandomType.STATELESS,\n                                         seed=1234)\n\n  # A multi-batch example. We can simultaneously draw from more than one\n  # set of parameters similarly to the behaviour in tensorflow distributions\n  # library.\n  sample_shape = [5, 4]  # Twenty samples arranged as a 5x4 matrix.\n  means = [[1.0, -1.0], [0.0, 2.0],[0.3, 1.4]]  # A batch of three mean vectors.\n  # This demonstrates the broadcasting of the parameters. While we have\n  # a batch of 3 mean vectors, we supply only one covariance matrix. This means\n  # that three distributions have different means but the same covariance.\n  covariances = [[1.0, 0.1], [0.1, 1.0]]\n\n  # Produces a Tensor of shape [5, 4, 3, 2] containing 20 samples from the\n  # batch of 3 bivariate normals.\n  sample_batch = multivariate_normal(sample_shape, mean=means,\n                                     covariance_matrix=covariance)\n\n  Args:\n    sample_shape: Rank 1 `Tensor` of positive `int32`s. Should specify a valid\n      shape for a `Tensor`. The shape of the samples to be drawn.\n    mean: Real `Tensor` of rank at least 1 or None. The shape of the `Tensor` is\n      interpreted as `batch_shape + [k]` where `k` is the dimension of domain.\n      The mean value(s) of the distribution(s) to draw from.\n      Default value: None which is mapped to a zero mean vector.\n    covariance_matrix: Real `Tensor` of rank at least 2 or None. Symmetric\n      positive definite `Tensor` of  same `dtype` as `mean`. The strict upper\n      triangle of `covariance_matrix` is ignored, so if `covariance_matrix` is\n      not symmetric no error will be raised (unless `validate_args is True`).\n      `covariance_matrix` has shape `batch_shape + [k, k]` where `b >= 0` and\n      `k` is the event size.\n      Default value: None which is mapped to the identity covariance.\n    scale_matrix: Real `Tensor` of rank at least 2 or None. If supplied, it\n      should be positive definite `Tensor` of same `dtype` as `mean`. The\n      covariance matrix is related to the scale matrix by `covariance =\n      scale_matrix * Transpose(scale_matrix)`.\n      Default value: None which corresponds to an identity covariance matrix.\n    random_type: Enum value of `RandomType`. The type of draw to generate.\n      For `PSEUDO_ANTITHETIC` and `STATELESS_ANTITHETIC` the first dimension of\n      `sample_shape` is expected to be an even positive integer. The antithetic\n      pairs are then contained in the slices of the `output` tensor as\n      `output[:(sample_shape[0] / 2), ...]` and\n      `output[(sample_shape[0] / 2):, ...]`.\n      Default value: None which is mapped to `RandomType.PSEUDO`.\n    validate_args: Python `bool`. When `True`, distribution parameters are\n      checked for validity despite possibly degrading runtime performance. When\n      `False` invalid inputs may silently render incorrect outputs.\n      Default value: False.\n    seed: Seed for the random number generator. The seed is\n      only relevant if `random_type` is one of\n      `[STATELESS, PSEUDO, HALTON_RANDOMIZED, PSEUDO_ANTITHETIC,\n        STATELESS_ANTITHETIC]`. For `PSEUDO`, `PSEUDO_ANTITHETIC` and\n      `HALTON_RANDOMIZED` the seed should be a Python integer. For\n      `STATELESS` and  `STATELESS_ANTITHETIC `must be supplied as an integer\n      `Tensor` of shape `[2]`.\n      Default value: `None` which means no seed is set.\n    dtype: Optional `dtype`. The dtype of the input and output tensors.\n      Default value: None which maps to the default dtype inferred by\n      TensorFlow.\n    name: Python `str` name prefixed to Ops created by this class.\n      Default value: None which is mapped to the default name\n        \'multivariate_normal\'.\n    **kwargs: parameters, specific to a random type:\n      (1) `skip` is an `int` 0-d `Tensor`. The number of initial points of the\n      Sobol or Halton sequence to skip. Used only when `random_type` is \'SOBOL\',\n      \'HALTON\', or \'HALTON_RANDOMIZED\', otherwise ignored.\n      (2) `randomization_params` is an instance of\n      `tff.math.random.HaltonParams` that fully describes the randomization\n      behavior. Used only when `random_type` is \'HALTON_RANDOMIZED\', otherwise\n      ignored (see halton.sample args for more details). If this parameter is\n      provided when random_type is `HALTON_RANDOMIZED`, the `seed` parameter is\n      ignored.\n      Default value: `None`. In this case with randomized = True, the necessary\n        randomization parameters will be computed from scratch.\n\n  Returns:\n    A `Tensor` of shape `sample_shape + batch_shape + [k]`. The draws from the\n    multivariate normal distribution.\n\n  Raises:\n    ValueError:\n      (a) If all of `mean`, `covariance_matrix` and `scale_matrix` are None.\n      (b) If both `covariance_matrix` and `scale_matrix` are specified.\n      (c) If `random_type` is either `RandomType.STATELESS` or\n      `RandomType.STATELESS_ANTITHETIC` and `seed` is not supplied\n    NotImplementedError: If `random_type` is neither RandomType.PSEUDO,\n      RandomType.STATELESS, RandomType.PSEUDO_ANTITHETIC,\n      RandomType.STATELESS_ANTITHETIC, RandomType.SOBOL, RandomType.HALTON, nor\n      RandomType.HALTON_RANDOMIZED.\n  """"""\n  random_type = RandomType.PSEUDO if random_type is None else random_type\n  # Convert sample_shape to a list\n  sample_shape = list(sample_shape)\n  if mean is None and covariance_matrix is None and scale_matrix is None:\n    raise ValueError(\'At least one of mean, covariance_matrix or scale_matrix\'\n                     \' must be specified.\')\n\n  if covariance_matrix is not None and scale_matrix is not None:\n    raise ValueError(\'Only one of covariance matrix or scale matrix\'\n                     \' must be specified\')\n\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'multivariate_normal\',\n      values=[sample_shape, mean, covariance_matrix, scale_matrix]):\n    if mean is not None:\n      mean = tf.convert_to_tensor(mean, dtype=dtype, name=\'mean\')\n    if random_type in [RandomType.PSEUDO, RandomType.STATELESS]:\n      return _mvnormal_pseudo(\n          sample_shape,\n          mean,\n          covariance_matrix=covariance_matrix,\n          scale_matrix=scale_matrix,\n          random_type=random_type,\n          seed=seed,\n          dtype=dtype)\n    elif random_type in [RandomType.PSEUDO_ANTITHETIC,\n                         RandomType.STATELESS_ANTITHETIC]:\n      return _mvnormal_pseudo_antithetic(\n          sample_shape,\n          mean,\n          covariance_matrix=covariance_matrix,\n          scale_matrix=scale_matrix,\n          random_type=random_type,\n          seed=seed,\n          dtype=dtype)\n    elif random_type == RandomType.SOBOL:\n      return _mvnormal_sobol(\n          sample_shape,\n          mean,\n          covariance_matrix=covariance_matrix,\n          scale_matrix=scale_matrix,\n          validate_args=validate_args,\n          dtype=dtype,\n          **kwargs)\n    elif random_type == RandomType.HALTON:\n      return _mvnormal_halton(\n          sample_shape,\n          mean,\n          randomized=False,\n          seed=seed,\n          covariance_matrix=covariance_matrix,\n          scale_matrix=scale_matrix,\n          validate_args=validate_args,\n          dtype=dtype,\n          **kwargs)\n    elif random_type == RandomType.HALTON_RANDOMIZED:\n      return _mvnormal_halton(\n          sample_shape,\n          mean,\n          randomized=True,\n          seed=seed,\n          covariance_matrix=covariance_matrix,\n          scale_matrix=scale_matrix,\n          validate_args=validate_args,\n          dtype=dtype,\n          **kwargs)\n    else:\n      raise NotImplementedError(\n          \'Only STATELESS, PSEUDO, PSEUDO_ANTITHETIC, STATELESS_ANTITHETIC,  \'\n          \'HALTON, HALTON_RANDOMIZED, and SOBOL random types are currently \'\n          \'supported. Supplied: {}\'. format(random_type))\n\n\ndef _mvnormal_pseudo(sample_shape,\n                     mean,\n                     covariance_matrix=None,\n                     scale_matrix=None,\n                     random_type=RandomType.PSEUDO,\n                     seed=None,\n                     dtype=None):\n  """"""Returns normal draws using the tfp multivariate normal distribution.""""""\n  (mean, scale_matrix,\n   batch_shape, _, dtype) = _process_mean_scale(mean, scale_matrix,\n                                                covariance_matrix, dtype)\n  output_shape = sample_shape + batch_shape\n  if random_type == RandomType.PSEUDO:\n    samples = tf.random.normal(shape=output_shape,\n                               dtype=dtype,\n                               seed=seed)\n  else:\n    if seed is None:\n      raise ValueError(\'`seed` should be specified if the `random_type` is \'\n                       \'`STATELESS` or `STATELESS_ANTITHETIC`\')\n    samples = tf.random.stateless_normal(\n        shape=output_shape, dtype=dtype, seed=seed)\n  if scale_matrix is None:\n    return mean + samples\n  else:\n    return mean + tf.linalg.matvec(scale_matrix, samples)\n\n\ndef _mvnormal_pseudo_antithetic(sample_shape,\n                                mean,\n                                covariance_matrix=None,\n                                scale_matrix=None,\n                                random_type=RandomType.PSEUDO_ANTITHETIC,\n                                seed=None,\n                                dtype=None):\n  """"""Returns normal draws with the antithetic samples.""""""\n  sample_shape = tf.TensorShape(sample_shape).as_list()\n  sample_zero_dim = sample_shape[0]\n  # For the antithetic sampler `sample_shape` is split evenly between\n  # samples and their antithetic counterparts. In order to do the splitting\n  # we expect the first dimension of `sample_shape` to be even.\n  is_even_dim = tf.compat.v1.debugging.assert_equal(\n      sample_zero_dim % 2,\n      0,\n      message=\'First dimension of `sample_shape` should be even for \'\n      \'PSEUDO_ANTITHETIC random type\')\n  with tf.control_dependencies([is_even_dim]):\n    antithetic_shape = [sample_zero_dim // 2] + sample_shape[1:]\n  if random_type == RandomType.PSEUDO_ANTITHETIC:\n    random_type_sample = RandomType.PSEUDO\n  else:\n    random_type_sample = RandomType.STATELESS\n  result = _mvnormal_pseudo(\n      antithetic_shape,\n      mean,\n      covariance_matrix=covariance_matrix,\n      scale_matrix=scale_matrix,\n      random_type=random_type_sample,\n      seed=seed,\n      dtype=dtype)\n  if mean is None:\n    return tf.concat([result, -result], axis=0)\n  else:\n    return tf.concat([result, 2 * mean - result], axis=0)\n\n\ndef _mvnormal_sobol(sample_shape,\n                    mean,\n                    covariance_matrix=None,\n                    scale_matrix=None,\n                    validate_args=False,\n                    dtype=None,\n                    **kwargs):\n  """"""Returns normal draws using Sobol low-discrepancy sequences.""""""\n  return _mvnormal_quasi(sample_shape,\n                         mean,\n                         random_type=RandomType.SOBOL,\n                         seed=None,\n                         covariance_matrix=covariance_matrix,\n                         scale_matrix=scale_matrix,\n                         validate_args=validate_args,\n                         dtype=dtype,\n                         **kwargs)\n\n\ndef _mvnormal_halton(sample_shape,\n                     mean,\n                     randomized,\n                     seed=None,\n                     covariance_matrix=None,\n                     scale_matrix=None,\n                     validate_args=False,\n                     dtype=None,\n                     **kwargs):\n  """"""Returns normal draws using Halton low-discrepancy sequences.""""""\n  random_type = (RandomType.HALTON_RANDOMIZED if randomized\n                 else RandomType.HALTON)\n  return _mvnormal_quasi(sample_shape,\n                         mean,\n                         random_type,\n                         seed=seed,\n                         covariance_matrix=covariance_matrix,\n                         scale_matrix=scale_matrix,\n                         validate_args=validate_args,\n                         dtype=dtype,\n                         **kwargs)\n\n\ndef _mvnormal_quasi(sample_shape,\n                    mean,\n                    random_type,\n                    seed,\n                    covariance_matrix=None,\n                    scale_matrix=None,\n                    validate_args=False,\n                    dtype=None,\n                    **kwargs):\n  """"""Returns normal draws using low-discrepancy sequences.""""""\n  (mean, scale_matrix,\n   batch_shape, dim, dtype) = _process_mean_scale(mean, scale_matrix,\n                                                  covariance_matrix, dtype)\n  # Reverse elements of the batch shape\n  batch_shape_reverse = tf.TensorShape(reversed(batch_shape))\n  # Transposed shape of the output\n  output_shape_t = tf.concat([batch_shape_reverse, sample_shape], -1)\n  # Number of quasi random samples\n  num_samples = tf.reduce_prod(output_shape_t) // dim\n  # Number of initial low discrepancy sequence numbers to skip\n  if \'skip\' in kwargs:\n    skip = kwargs[\'skip\']\n  else:\n    skip = 0\n  if random_type == RandomType.SOBOL:\n    # Shape [num_samples, dim] of the Sobol samples\n    low_discrepancy_seq = sobol.sample(\n        dim=dim, num_results=num_samples, skip=skip,\n        dtype=dtype)\n  else:  # HALTON or HALTON_RANDOMIZED random_dtype\n    if \'randomization_params\' in kwargs:\n      randomization_params = kwargs[\'randomization_params\']\n    else:\n      randomization_params = None\n    randomized = random_type == RandomType.HALTON_RANDOMIZED\n    # Shape [num_samples, dim] of the Sobol samples\n    low_discrepancy_seq, _ = halton.sample(\n        dim=dim,\n        sequence_indices=tf.range(skip, skip + num_samples),\n        randomized=randomized,\n        randomization_params=randomization_params,\n        seed=seed,\n        validate_args=validate_args,\n        dtype=dtype)\n\n  # Transpose to the shape [dim, num_samples]\n  low_discrepancy_seq = tf.transpose(low_discrepancy_seq)\n  size_sample = tf.size(sample_shape)\n  size_batch = tf.size(batch_shape)\n  # Permutation for `output_shape_t` to the output shape\n  permutation = tf.concat([tf.range(size_batch, size_batch + size_sample),\n                           tf.range(size_batch - 1, -1, -1)], -1)\n  # Reshape Sobol samples to the correct output shape\n  low_discrepancy_seq = tf.transpose(\n      tf.reshape(low_discrepancy_seq, output_shape_t),\n      permutation)\n  # Apply inverse Normal CDF to Sobol samples to obtain the corresponding\n  # Normal samples\n  samples = tf.math.erfinv((low_discrepancy_seq - 0.5) * 2)* _SQRT_2\n  if scale_matrix is None:\n    return mean + samples\n  else:\n    return mean + tf.linalg.matvec(scale_matrix, samples)\n\n\ndef _process_mean_scale(mean, scale_matrix, covariance_matrix, dtype):\n  """"""Extracts correct mean, scale, batch_shape, dimension, and dtype.""""""\n  if scale_matrix is not None:\n    scale_matrix = tf.convert_to_tensor(scale_matrix, dtype=dtype,\n                                        name=\'scale_matrix\')\n  else:\n    if covariance_matrix is not None:\n      covariance_matrix = tf.convert_to_tensor(covariance_matrix, dtype=dtype,\n                                               name=\'covariance_matrix\')\n      scale_matrix = tf.linalg.cholesky(covariance_matrix)\n  if mean is None:\n    mean = 0.0\n    # batch_shape includes the dimension of the samples\n    batch_shape = scale_matrix.shape.as_list()[:-1]\n    dim = scale_matrix.shape.as_list()[-1]\n    dtype = scale_matrix.dtype\n  else:\n    batch_shape = mean.shape.as_list()\n    dim = mean.shape.as_list()[-1]\n    dtype = mean.dtype\n  return mean, scale_matrix, batch_shape, dim, dtype\n'"
tf_quant_finance/math/random_ops/multivariate_normal_test.py,3,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for random.multivariate_normal.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\ntff_rnd = tff.math.random\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass RandomTest(parameterized.TestCase, tf.test.TestCase):\n\n  def test_shapes(self):\n    """"""Tests the sample shapes.""""""\n    sample_no_batch = self.evaluate(\n        tff_rnd.mv_normal_sample([2, 4], mean=[0.2, 0.1]))\n    self.assertEqual(sample_no_batch.shape, (2, 4, 2))\n    sample_batch = self.evaluate(\n        tff_rnd.mv_normal_sample(\n            [2, 4], mean=[[0.2, 0.1], [0., -0.1], [0., 0.1]]))\n    self.assertEqual(sample_batch.shape, (2, 4, 3, 2))\n\n  def test_mean_default(self):\n    """"""Tests that the default value of mean is 0.""""""\n    covar = np.array([[1.0, 0.1], [0.1, 1.0]])\n    sample = self.evaluate(\n        tff_rnd.mv_normal_sample(\n            [40000], covariance_matrix=covar, seed=1234))\n    with self.subTest(""Shape""):\n      np.testing.assert_array_equal(sample.shape, [40000, 2])\n    with self.subTest(""Mean""):\n      self.assertArrayNear(np.mean(sample, axis=0), [0.0, 0.0], 1e-2)\n    with self.subTest(""Covariance""):\n      self.assertArrayNear(\n          np.cov(sample, rowvar=False).reshape([-1]), covar.reshape([-1]), 2e-2)\n\n  def test_covariance_default(self):\n    """"""Tests that the default value of the covariance matrix is identity.""""""\n    mean = np.array([[1.0, 0.1], [0.1, 1.0]])\n    sample = self.evaluate(\n        tff_rnd.mv_normal_sample([10000], mean=mean))\n\n    np.testing.assert_array_equal(sample.shape, [10000, 2, 2])\n    np.testing.assert_array_almost_equal(\n        np.mean(sample, axis=0), mean, decimal=1)\n    np.testing.assert_array_almost_equal(\n        np.cov(sample[:, 0, :], rowvar=False), np.eye(2), decimal=1)\n    np.testing.assert_array_almost_equal(\n        np.cov(sample[:, 1, :], rowvar=False), np.eye(2), decimal=1)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""PSEUDO"",\n          ""random_type"": tff_rnd.RandomType.PSEUDO,\n          ""seed"": 4567,\n      }, {\n          ""testcase_name"": ""STATELESS"",\n          ""random_type"": tff_rnd.RandomType.STATELESS,\n          ""seed"": [1, 4567]\n      })\n  def test_general_mean_covariance(self, random_type, seed):\n    """"""Tests that the sample is correctly generated for pseudo and stateless.""""""\n    mean = np.array([[1.0, 0.1], [0.1, 1.0]])\n    covar = np.array([\n        [[0.9, -0.1], [-0.1, 1.0]],\n        [[1.1, -0.3], [-0.3, 0.6]],\n    ])\n    size = 30000\n    sample = self.evaluate(\n        tff_rnd.mv_normal_sample(\n            [size], mean=mean, covariance_matrix=covar,\n            random_type=random_type, seed=seed))\n\n    np.testing.assert_array_equal(sample.shape, [size, 2, 2])\n    np.testing.assert_array_almost_equal(\n        np.mean(sample, axis=0), mean, decimal=1)\n    np.testing.assert_array_almost_equal(\n        np.cov(sample[:, 0, :], rowvar=False), covar[0], decimal=1)\n    np.testing.assert_array_almost_equal(\n        np.cov(sample[:, 1, :], rowvar=False), covar[1], decimal=1)\n\n  def test_mean_and_scale(self):\n    """"""Tests sample for scale specification.""""""\n    mean = np.array([[1.0, 0.1], [0.1, 1.0]])\n    scale = np.array([[0.4, -0.1], [0.22, 1.38]])\n\n    covariance = np.matmul(scale, scale.transpose())\n    size = 30000\n    sample = self.evaluate(\n        tff_rnd.mv_normal_sample(\n            [size], mean=mean, scale_matrix=scale, seed=7534))\n\n    np.testing.assert_array_equal(sample.shape, [size, 2, 2])\n    np.testing.assert_array_almost_equal(\n        np.mean(sample, axis=0), mean, decimal=1)\n    np.testing.assert_array_almost_equal(\n        np.cov(sample[:, 0, :], rowvar=False), covariance, decimal=1)\n    np.testing.assert_array_almost_equal(\n        np.cov(sample[:, 1, :], rowvar=False), covariance, decimal=1)\n\n  def test_mean_default_sobol(self):\n    """"""Tests that the default value of mean is 0.""""""\n    covar = np.array([[1.0, 0.1], [0.1, 1.0]])\n    # The number of initial points of the Sobol sequence to skip\n    skip = 1000\n    sample = self.evaluate(\n        tff_rnd.mv_normal_sample(\n            [10000], covariance_matrix=covar,\n            random_type=tff_rnd.RandomType.SOBOL,\n            skip=skip))\n    np.testing.assert_array_equal(sample.shape, [10000, 2])\n    self.assertArrayNear(np.mean(sample, axis=0), [0.0, 0.0], 1e-2)\n    self.assertArrayNear(\n        np.cov(sample, rowvar=False).reshape([-1]), covar.reshape([-1]), 2e-2)\n\n  def test_mean_and_scale_sobol(self):\n    """"""Tests sample for scale specification.""""""\n    mean = np.array([[1.0, 0.1], [0.1, 1.0], [2.0, 0.3], [0., 0.]])\n    scale = np.array([[0.4, -0.1], [0.22, 1.38]])\n    covariance = np.matmul(scale, scale.transpose())\n    sample_shape = [2, 3, 5000]\n    sample = self.evaluate(\n        tff_rnd.mv_normal_sample(\n            sample_shape, mean=mean, scale_matrix=scale,\n            random_type=tff_rnd.RandomType.SOBOL))\n\n    np.testing.assert_array_equal(sample.shape, sample_shape + [4, 2])\n    np.testing.assert_array_almost_equal(\n        np.mean(sample, axis=(0, 1, 2)), mean, decimal=1)\n    for i in range(4):\n      np.testing.assert_array_almost_equal(\n          np.cov(sample[0, 1, :, i, :], rowvar=False), covariance, decimal=1)\n\n  def test_mean_default_halton(self):\n    """"""Tests that the default value of mean is 0.""""""\n    covar = np.array([[1.0, 0.1], [0.1, 1.0]])\n    # The number of initial points of the Sobol sequence to skip\n    skip = 1000\n    sample = self.evaluate(\n        tff_rnd.mv_normal_sample(\n            [10000], covariance_matrix=covar,\n            random_type=tff_rnd.RandomType.HALTON_RANDOMIZED,\n            skip=skip))\n    np.testing.assert_array_equal(sample.shape, [10000, 2])\n    self.assertArrayNear(np.mean(sample, axis=0), [0.0, 0.0], 1e-2)\n    self.assertArrayNear(\n        np.cov(sample, rowvar=False).reshape([-1]), covar.reshape([-1]), 2e-2)\n\n  def test_mean_default_halton_randomization_params(self):\n    """"""Tests that the default value of mean is 0.""""""\n    dtype = np.float32\n    covar = np.array([[1.0, 0.1], [0.1, 1.0]], dtype=dtype)\n    num_samples = 10000\n    # Set up randomization parameters\n    randomization_params = tff.math.random.halton.sample(\n        2, num_samples, randomized=True, seed=42)[1]\n    sample = self.evaluate(\n        tff_rnd.mv_normal_sample(\n            [num_samples], covariance_matrix=covar,\n            random_type=tff_rnd.RandomType.HALTON_RANDOMIZED,\n            randomization_params=randomization_params))\n    np.testing.assert_array_equal(sample.shape, [num_samples, 2])\n    self.assertArrayNear(np.mean(sample, axis=0), [0.0, 0.0], 1e-2)\n    self.assertArrayNear(\n        np.cov(sample, rowvar=False).reshape([-1]), covar.reshape([-1]), 2e-2)\n\n  def test_mean_and_scale_halton(self):\n    """"""Tests sample for scale specification.""""""\n    mean = np.array([[1.0, 0.1], [0.1, 1.0], [2.0, 0.3], [0., 0.]])\n    scale = np.array([[0.4, -0.1], [0.22, 1.38]])\n    covariance = np.matmul(scale, scale.transpose())\n    sample_shape = [2, 3, 5000]\n    sample = self.evaluate(\n        tff_rnd.mv_normal_sample(\n            sample_shape, mean=mean, scale_matrix=scale,\n            random_type=tff_rnd.RandomType.HALTON))\n\n    np.testing.assert_array_equal(sample.shape, sample_shape + [4, 2])\n    np.testing.assert_array_almost_equal(\n        np.mean(sample, axis=(0, 1, 2)), mean, decimal=1)\n    for i in range(4):\n      np.testing.assert_array_almost_equal(\n          np.cov(sample[0, 2, :, i, :], rowvar=False), covariance, decimal=1)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""PSEUDO"",\n          ""random_type"": tff_rnd.RandomType.PSEUDO_ANTITHETIC,\n          ""seed"": 42,\n      }, {\n          ""testcase_name"": ""STATELESS"",\n          ""random_type"": tff_rnd.RandomType.STATELESS_ANTITHETIC,\n          ""seed"": [1, 42]\n      })\n  def test_mean_and_scale_antithetic(self, random_type, seed):\n    """"""Tests antithetic sampler for scale specification.""""""\n    mean = np.array([[1.0, 0.1], [0.1, 1.0]])\n    scale = np.array([[0.4, -0.1], [0.22, 1.38]])\n\n    covariance = np.matmul(scale, scale.transpose())\n    size = 30000\n    sample = self.evaluate(\n        tff_rnd.mv_normal_sample(\n            [size], mean=mean, scale_matrix=scale,\n            random_type=random_type, seed=seed))\n    with self.subTest(""Shape""):\n      np.testing.assert_array_equal(sample.shape, [size, 2, 2])\n    # Antithetic combination of samples should be equal to the `mean`\n    antithetic_size = size // 2\n    antithetic_combination = (sample[:antithetic_size, ...]\n                              + sample[antithetic_size:, ...]) / 2\n    with self.subTest(""Partition""):\n      np.testing.assert_allclose(\n          antithetic_combination,\n          mean + np.zeros([antithetic_size, 2, 2]), 1e-10, 1e-10)\n    # Get the antithetic pairs and verify normality\n    with self.subTest(""Mean""):\n      np.testing.assert_array_almost_equal(\n          np.mean(sample[:antithetic_size, ...], axis=0), mean, decimal=1)\n    with self.subTest(""CovariancePart1""):\n      np.testing.assert_array_almost_equal(\n          np.cov(sample[:antithetic_size, 0, :], rowvar=False),\n          covariance, decimal=1)\n    with self.subTest(""CovariancePart2""):\n      np.testing.assert_array_almost_equal(\n          np.cov(sample[:antithetic_size, 1, :], rowvar=False),\n          covariance, decimal=1)\n\n  def test_antithetic_sample_requires_even_dim(self):\n    """"""Error is trigerred if the first dim of sample_shape is odd.""""""\n    mean = np.array([[1.0, 0.1], [0.1, 1.0]])\n    scale = np.array([[0.4, -0.1], [0.22, 1.38]])\n    sample_shape = [11, 100]\n    # Should fail: The first dimension of `sample_shape` should be even.\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      self.evaluate(\n          tff_rnd.mv_normal_sample(\n              sample_shape, mean=mean, scale_matrix=scale,\n              random_type=tff_rnd.RandomType.PSEUDO_ANTITHETIC))\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/math/random_ops/stateless.py,8,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Stateless random ops.\n\nImplement some of the stateless ops, which produce random numbers as a\ndeterministic function of seed.\n""""""\n\n\nimport tensorflow.compat.v2 as tf\n\n\ndef stateless_random_shuffle(input_tensor, seed, name=None):\n  """"""Produces stateless random shuffle of the 1st dimension of an input Tensor.\n\n  This is a stateless version of `tf.random_shuffle`. If run twice with the same\n  seed, produces the same result.\n\n  Example\n  ```python\n  identity_shuffle = tf.range(100)\n  random_shuffle = stateless_random_shuffle(identity_shuffle, seed=(42, 2))\n  ```\n\n  Args:\n    input_tensor: float32, float64, int32 or int64 1-D Tensor.\n    seed: int32 or int64 Tensor of shape [2].\n    name: Python `str` name prefixed to ops created by this function.\n\n  Returns:\n    A Tensor of the same shape and dtype as `input_tensor`.\n  """"""\n  with tf.compat.v1.name_scope(name,\n                               default_name=\'stateless_random_shuffle\',\n                               values=[input_tensor, seed]):\n    input_tensor = tf.convert_to_tensor(input_tensor, name=\'input_tensor\')\n    seed = tf.convert_to_tensor(seed, name=\'random_seed\')\n    uniforms = tf.random.stateless_uniform(\n        shape=[tf.shape(input_tensor)[0]], seed=seed, dtype=tf.float64)\n  return tf.gather(input_tensor, tf.argsort(uniforms, stable=True, axis=0))\n'"
tf_quant_finance/math/random_ops/stateless_test.py,16,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for random.stateless.""""""\n\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\ntff_rnd = tff.math.random\n\n\nclass StatelessRandomOpsTest(tf.test.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testOutputIsPermutation(self):\n    """"""Checks that stateless_random_shuffle outputs a permutation.""""""\n    for dtype in (tf.int32, tf.int64, tf.float32, tf.float64):\n      identity_permutation = tf.range(10, dtype=dtype)\n      random_shuffle_seed_1 = tff_rnd.stateless_random_shuffle(\n          identity_permutation, seed=tf.constant((1, 42), tf.int64))\n      random_shuffle_seed_2 = tff_rnd.stateless_random_shuffle(\n          identity_permutation, seed=tf.constant((2, 42), tf.int64))\n      # Check that the shuffles are of the correct dtype\n      for shuffle in (random_shuffle_seed_1, random_shuffle_seed_2):\n        np.testing.assert_equal(shuffle.dtype, dtype.as_numpy_dtype)\n      random_shuffle_seed_1 = self.evaluate(random_shuffle_seed_1)\n      random_shuffle_seed_2 = self.evaluate(random_shuffle_seed_2)\n      identity_permutation = self.evaluate(identity_permutation)\n      # Check that the shuffles are different\n      self.assertTrue(\n          np.abs(random_shuffle_seed_1 - random_shuffle_seed_2).max())\n      # Check that the shuffles are indeed permutations\n      for shuffle in (random_shuffle_seed_1, random_shuffle_seed_2):\n        self.assertAllEqual(set(shuffle), set(identity_permutation))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testOutputIsStateless(self):\n    """"""Checks that stateless_random_shuffle is stateless.""""""\n    random_permutation_next_call = None\n    for dtype in (tf.int32, tf.int64, tf.float32, tf.float64):\n      random_permutation = tff_rnd.stateless_random_shuffle(\n          tf.range(10, dtype=dtype), seed=(100, 42))\n      random_permutation_first_call = self.evaluate(random_permutation)\n      if random_permutation_next_call is not None:\n        # Checks that the values are the same across different dtypes\n        np.testing.assert_array_equal(random_permutation_first_call,\n                                      random_permutation_next_call)\n      random_permutation_next_call = self.evaluate(random_permutation)\n      np.testing.assert_array_equal(random_permutation_first_call,\n                                    random_permutation_next_call)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testOutputIsIndependentOfInputValues(self):\n    """"""stateless_random_shuffle output is independent of input_tensor values.""""""\n    # Generate sorted array of random numbers to control that the result\n    # is independent of `input_tesnor` values\n    np.random.seed(25)\n    random_input = np.random.normal(size=[10])\n    random_input.sort()\n    for dtype in (tf.int32, tf.int64, tf.float32, tf.float64):\n      # Permutation of a sequence [0, 1, .., 9]\n      random_permutation = tff_rnd.stateless_random_shuffle(\n          tf.range(10, dtype=dtype), seed=(100, 42))\n      random_permutation = self.evaluate(random_permutation)\n      # Shuffle `random_input` with the same seed\n      random_shuffle_control = tff_rnd.stateless_random_shuffle(\n          random_input, seed=(100, 42))\n      random_shuffle_control = self.evaluate(random_shuffle_control)\n      # Checks that the generated permutation does not depend on the underlying\n      # values\n      np.testing.assert_array_equal(\n          np.argsort(random_permutation), np.argsort(random_shuffle_control))\n\n  @test_util.run_v1_only(""Sessions are not available in TF2.0"")\n  def testOutputIsStatelessSession(self):\n    """"""Checks that stateless_random_shuffle is stateless across Sessions.""""""\n    random_permutation_next_call = None\n    for dtype in (tf.int32, tf.int64, tf.float32, tf.float64):\n      random_permutation = tff_rnd.stateless_random_shuffle(\n          tf.range(10, dtype=dtype), seed=tf.constant((100, 42), tf.int64))\n      with tf.compat.v1.Session() as sess:\n        random_permutation_first_call = sess.run(random_permutation)\n      if random_permutation_next_call is not None:\n        # Checks that the values are the same across different dtypes\n        np.testing.assert_array_equal(random_permutation_first_call,\n                                      random_permutation_next_call)\n      with tf.compat.v1.Session() as sess:\n        random_permutation_next_call = sess.run(random_permutation)\n      np.testing.assert_array_equal(random_permutation_first_call,\n                                    random_permutation_next_call)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testMultiDimensionalShape(self):\n    """"""Check that stateless_random_shuffle works with multi-dim shapes.""""""\n    for dtype in (tf.int32, tf.int64, tf.float32, tf.float64):\n      input_permutation = tf.constant([[[1], [2], [3]], [[4], [5], [6]]],\n                                      dtype=dtype)\n      random_shuffle = tff_rnd.stateless_random_shuffle(\n          input_permutation, seed=(1, 42))\n      random_permutation_first_call = self.evaluate(random_shuffle)\n      random_permutation_next_call = self.evaluate(random_shuffle)\n      input_permutation = self.evaluate(input_permutation)\n      # Check that the dtype is correct\n      np.testing.assert_equal(random_permutation_first_call.dtype,\n                              dtype.as_numpy_dtype)\n      # Check that the shuffles are the same\n      np.testing.assert_array_equal(random_permutation_first_call,\n                                    random_permutation_next_call)\n      # Check that the output shape is correct\n      np.testing.assert_equal(random_permutation_first_call.shape,\n                              input_permutation.shape)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/math/random_ops/uniform.py,10,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Uniform distribution with various random types.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math.random_ops import halton\nfrom tf_quant_finance.math.random_ops import sobol\nfrom tf_quant_finance.math.random_ops.multivariate_normal import RandomType\n\n\ndef uniform(\n    dim,\n    sample_shape,\n    random_type=None,\n    dtype=None,\n    seed=None,\n    name=None,\n    **kwargs):\n  """"""Generates draws from a uniform distribution on [0, 1).\n\n  Allows generating either (pseudo) random or quasi-random draws based on the\n  `random_type` parameter. Dimension parameter `dim` is required since for\n  quasi-random draws one needs to know the dimensionality of the space as\n  opposed to just sample shape.\n\n  #### Example:\n\n  ```python\n  sample_shape = [10]  # Generates 10 draws.\n\n  # `Tensor` of shape [10, 1]\n  uniform_samples = uniform(1, sample_shape)\n\n  # `Tensor` of shape [10, 5]\n  sobol_samples = uniform(5, sample_shape, RandomType.SOBOL)\n  ```\n\n  Args:\n    dim: A positive Python `int` representing each sample\'s `event_size.`\n    sample_shape: Rank 1 `Tensor` of positive `int32`s. Should specify a valid\n      shape for a `Tensor`. The shape of the samples to be drawn.\n    random_type: Enum value of `RandomType`. The type of draw to generate.\n      Default value: None which is mapped to `RandomType.PSEUDO`.\n    dtype: Optional `dtype` (eithier `tf.float32` or `tf.float64`). The dtype of\n      the output `Tensor`.\n      Default value: `None` which maps to `tf.float32`.\n    seed: Seed for the random number generator. The seed is\n      only relevant if `random_type` is one of\n      `[STATELESS, PSEUDO, HALTON_RANDOMIZED]`. For `PSEUDO`, and\n      `HALTON_RANDOMIZED` the seed should be a Python integer. For\n      `STATELESS` must be supplied as an integer `Tensor` of shape `[2]`.\n      Default value: `None` which means no seed is set.\n    name: Python `str` name prefixed to ops created by this class.\n      Default value: `None` which is mapped to the default name\n        `uniform_distribution`.\n    **kwargs: parameters, specific to a random type:\n      (1) `skip` is an `int` 0-d `Tensor`. The number of initial points of the\n      Sobol or Halton sequence to skip. Used only when `random_type` is \'SOBOL\',\n      \'HALTON\', or \'HALTON_RANDOMIZED\', otherwise ignored.\n      (2) `randomization_params` is an instance of\n      `tff.math.random.HaltonParams` that fully describes the randomization\n      behavior. Used only when `random_type` is \'HALTON_RANDOMIZED\', otherwise\n      ignored (see halton.sample args for more details). If this parameter is\n      provided when random_type is `HALTON_RANDOMIZED`, the `seed` parameter is\n      ignored.\n      Default value: `None`. In this case with randomized = True, the necessary\n        randomization parameters will be computed from scratch.\n\n  Returns:\n    samples: A `Tensor` of shape `sample_shape + [dim]`. The draws\n      from the uniform distribution of the requested random type.\n\n  Raises:\n    ValueError: if `random_type` is `STATELESS` and the `seed` is `None`.\n  """"""\n  random_type = RandomType.PSEUDO if random_type is None else random_type\n  dtype = dtype or tf.float32\n  with tf.compat.v1.name_scope(name, default_name=\'uniform_distribution\',\n                               values=[sample_shape]):\n\n    if random_type == RandomType.PSEUDO:\n      return tf.random.uniform(\n          shape=sample_shape + [dim], dtype=dtype, seed=seed)\n    elif random_type == RandomType.STATELESS:\n      if seed is None:\n        raise ValueError(\'`seed` must be supplied if the `random_type` is \'\n                         \'STATELESS.\')\n      return tf.random.stateless_uniform(\n          shape=sample_shape + [dim], dtype=dtype, seed=seed)\n    # TODO(b/145104222): Add anthithetic sampling for the uniform distribution.\n    elif random_type == RandomType.PSEUDO_ANTITHETIC:\n      raise NotImplementedError(\n          \'At the moment antithetic sampling is not supported for the uniform \'\n          \'distribution.\')\n    else:\n      return _quasi_uniform(dim=dim,\n                            sample_shape=sample_shape,\n                            random_type=random_type,\n                            dtype=dtype,\n                            seed=seed,\n                            **kwargs)\n\n\ndef _quasi_uniform(\n    dim,\n    sample_shape,\n    random_type,\n    dtype,\n    seed=None,\n    **kwargs):\n  """"""Quasi random draws from a uniform distribution on [0, 1).""""""\n  # Shape of the output\n  output_shape = tf.concat([sample_shape] + [[dim]], -1)\n  # Number of quasi random samples\n  num_samples = tf.reduce_prod(sample_shape)\n  # Number of initial low discrepancy sequence numbers to skip\n  if \'skip\' in kwargs:\n    skip = kwargs[\'skip\']\n  else:\n    skip = 0\n  if random_type == RandomType.SOBOL:\n    # Shape [num_samples, dim] of the Sobol samples\n    low_discrepancy_seq = sobol.sample(\n        dim=dim, num_results=num_samples, skip=skip,\n        dtype=dtype)\n  else:  # HALTON or HALTON_RANDOMIZED random_dtype\n    if \'randomization_params\' in kwargs:\n      randomization_params = kwargs[\'randomization_params\']\n    else:\n      randomization_params = None\n    randomized = random_type == RandomType.HALTON_RANDOMIZED\n    # Shape [num_samples, dim] of the Sobol samples\n    low_discrepancy_seq, _ = halton.sample(\n        dim=dim,\n        sequence_indices=tf.range(skip, skip + num_samples),\n        randomized=randomized,\n        randomization_params=randomization_params,\n        seed=seed,\n        dtype=dtype)\n  return  tf.reshape(low_discrepancy_seq, output_shape)\n'"
tf_quant_finance/math/random_ops/uniform_test.py,2,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for uniform sampling.""""""\n\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\ntff_rnd = tff.math.random\n\n\n# TODO(b/145134423): Increase test coverage.\n@test_util.run_all_in_graph_and_eager_modes\nclass UniformTest(tf.test.TestCase):\n\n  def test_uniform(self):\n    """"""Tests uniform pseudo random numbers.""""""\n    sample_shape = [2, 3, 5000]\n    dim = 10\n    sample = self.evaluate(\n        tff_rnd.uniform(dim=dim,\n                        sample_shape=sample_shape,\n                        seed=101))\n    np.testing.assert_array_equal(sample.shape, sample_shape + [dim])\n    expected_mean = 0.5 * np.ones(sample_shape[:-1] + [dim])\n    np.testing.assert_array_almost_equal(\n        np.mean(sample, axis=2), expected_mean, decimal=2)\n\n  def test_stateless(self):\n    """"""Tests stateless pseudo random numbers.""""""\n    sample_shape = [2, 3, 5000]\n    dim = 10\n    sample = self.evaluate(\n        tff_rnd.uniform(dim=dim,\n                        sample_shape=sample_shape,\n                        random_type=tff_rnd.RandomType.STATELESS,\n                        seed=[2, 2]))\n    np.testing.assert_array_equal(sample.shape, sample_shape + [dim])\n    expected_mean = 0.5 * np.ones(sample_shape[:-1] + [dim])\n    np.testing.assert_array_almost_equal(\n        np.mean(sample, axis=2), expected_mean, decimal=2)\n\n  def test_sobol(self):\n    """"""Tests Sobol samples.""""""\n    # The number of initial points of the Sobol sequence to skip\n    skip = 1000\n    for dtype in [np.float32, np.float64]:\n      sample = tff_rnd.uniform(dim=5,\n                               sample_shape=[100],\n                               random_type=tff_rnd.RandomType.SOBOL,\n                               skip=skip,\n                               dtype=dtype)\n      expected_samples = tff_rnd.sobol.sample(dim=5,\n                                              num_results=100,\n                                              skip=skip,\n                                              dtype=dtype)\n      self.assertAllClose(sample, expected_samples)\n      self.assertEqual(sample.dtype.as_numpy_dtype, dtype)\n\n  def test_halton_randomization_params(self):\n    """"""Tests samples for the randomized Halton sequence.""""""\n    dtype = np.float32\n    num_samples = 10000\n    # Set up randomization parameters (this is an optional arg)\n    randomization_params = tff.math.random.halton.sample(\n        2, num_samples, randomized=True, seed=42)[1]\n    sample = self.evaluate(\n        tff_rnd.uniform(dim=2,\n                        sample_shape=[num_samples],\n                        random_type=tff_rnd.RandomType.HALTON_RANDOMIZED,\n                        randomization_params=randomization_params,\n                        dtype=dtype))\n    np.testing.assert_array_equal(sample.shape, [num_samples, 2])\n    self.assertArrayNear(np.mean(sample, axis=0), [0.5, 0.5], 1e-2)\n\n  def test_halton(self):\n    """"""Tests samples for halton random numbers.""""""\n    sample_shape = [2, 3, 1000]  # Need less samples than for the uniform case\n    dim = 10\n    sample = self.evaluate(\n        tff_rnd.uniform(dim=dim,\n                        sample_shape=sample_shape,\n                        random_type=tff_rnd.RandomType.HALTON))\n    np.testing.assert_array_equal(sample.shape, sample_shape + [dim])\n    expected_mean = 0.5 * np.ones([dim])\n    np.testing.assert_array_almost_equal(\n        np.mean(sample, axis=(0, 1, 2)), expected_mean, decimal=3)\n    expected_mean = 0.5 * np.ones(sample_shape[:-1] + [dim])\n    np.testing.assert_array_almost_equal(\n        np.mean(sample, axis=2), expected_mean, decimal=2)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/models/geometric_brownian_motion/__init__.py,0,b''
tf_quant_finance/models/geometric_brownian_motion/geometric_brownian_motion_test.py,25,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for Geometric Brownian Motion.""""""\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass GeometricBrownianMotionTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""SinglePrecision"",\n          ""dtype"": np.float32,\n      }, {\n          ""testcase_name"": ""DoublePrecision"",\n          ""dtype"": np.float64,\n      })\n  def test_univariate_drift_and_volatility(self, dtype):\n    """"""Tests univariate GBM drift and volatility functions.""""""\n    process = tff.models.GeometricBrownianMotion(0.05, 0.5, dtype=dtype)\n    drift_fn = process.drift_fn()\n    volatility_fn = process.volatility_fn()\n    state = np.array([[1.], [2.], [3.]], dtype=dtype)\n    with self.subTest(""Drift""):\n      drift = drift_fn(0.2, state)\n      expected_drift = state * 0.05\n      self.assertAllClose(drift, expected_drift, atol=1e-8, rtol=1e-8)\n    with self.subTest(""Volatility""):\n      vol = volatility_fn(0.2, state)\n      expected_vol = state * 0.5\n      self.assertAllClose(vol, np.expand_dims(expected_vol, axis=-1),\n                          atol=1e-8, rtol=1e-8)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""SinglePrecision"",\n          ""dtype"": np.float32,\n      }, {\n          ""testcase_name"": ""DoublePrecision"",\n          ""dtype"": np.float64,\n      })\n  def test_multivariate_drift_and_volatility(self, dtype):\n    """"""Tests multivariate GBM drift and volatility functions.""""""\n    means = [0.05, 0.02]\n    volatilities = [0.1, 0.2]\n    corr_matrix = [[1, 0.1], [0.1, 1]]\n    process = tff.models.MultivariateGeometricBrownianMotion(\n        dim=2, means=means, volatilities=volatilities, corr_matrix=corr_matrix,\n        dtype=tf.float64)\n    drift_fn = process.drift_fn()\n    volatility_fn = process.volatility_fn()\n    state = np.array([[1., 2.], [3., 4.], [5., 6.]], dtype=dtype)\n    with self.subTest(""Drift""):\n      drift = drift_fn(0.2, state)\n      expected_drift = np.array(means) * state\n      self.assertAllClose(drift, expected_drift, atol=1e-8, rtol=1e-8)\n    with self.subTest(""Volatility""):\n      vol = volatility_fn(0.2, state)\n      expected_vol = np.expand_dims(\n          np.array(volatilities) * state,\n          axis=-1) * np.linalg.cholesky(corr_matrix)\n      self.assertAllClose(vol, expected_vol, atol=1e-8, rtol=1e-8)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""SinglePrecision"",\n          ""dtype"": np.float32,\n      }, {\n          ""testcase_name"": ""DoublePrecision"",\n          ""dtype"": np.float64,\n      })\n  def test_multivariate_drift_and_volatility_no_corr(self, dtype):\n    """"""Tests multivariate GBM drift and volatility functions.""""""\n    means = [0.05, 0.02]\n    volatilities = [0.1, 0.2]\n    corr_matrix = [[1, 0.0], [0.0, 1]]\n    process = tff.models.MultivariateGeometricBrownianMotion(\n        dim=2, means=means, volatilities=volatilities,\n        dtype=tf.float64)\n    drift_fn = process.drift_fn()\n    volatility_fn = process.volatility_fn()\n    state = np.array([[1., 2.], [3., 4.], [5., 6.]], dtype=dtype)\n    with self.subTest(""Drift""):\n      drift = drift_fn(0.2, state)\n      expected_drift = np.array(means) * state\n      self.assertAllClose(drift, expected_drift, atol=1e-8, rtol=1e-8)\n    with self.subTest(""Volatility""):\n      vol = volatility_fn(0.2, state)\n      expected_vol = np.expand_dims(\n          np.array(volatilities) * state,\n          axis=-1) * np.linalg.cholesky(corr_matrix)\n      self.assertAllClose(vol, expected_vol, atol=1e-8, rtol=1e-8)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""SinglePrecision"",\n          ""dtype"": np.float32,\n      }, {\n          ""testcase_name"": ""DoublePrecision"",\n          ""dtype"": np.float64,\n      })\n  def test_univariate_sample_mean_and_variance(self, dtype):\n    """"""Tests the mean and vol of the univariate GBM sampled paths.""""""\n    process = tff.models.GeometricBrownianMotion(0.05, 0.5, dtype=dtype)\n    samples = process.sample_paths(\n        times=[0.1, 0.5, 1.0], initial_state=2.0,\n        random_type=tff.math.random.RandomType.SOBOL, num_samples=10000)\n    log_s = tf.math.log(samples)\n    mean = tf.reduce_mean(log_s, axis=0, keepdims=True)\n    var = tf.reduce_mean((log_s - mean)**2, axis=0)\n    expected_mean = ((process._mu - process._sigma**2 / 2)\n                     * np.array([0.1, 0.5, 1.0]) + np.log(2.))\n    expected_var = process._sigma**2 * np.array([0.1, 0.5, 1.0])\n    with self.subTest(""Drift""):\n      self.assertAllClose(tf.squeeze(mean), expected_mean, atol=1e-3, rtol=1e-3)\n    with self.subTest(""Variance""):\n      self.assertAllClose(tf.squeeze(var), expected_var, atol=1e-3, rtol=1e-3)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""SinglePrecision"",\n          ""dtype"": np.float32,\n      }, {\n          ""testcase_name"": ""DoublePrecision"",\n          ""dtype"": np.float64,\n      })\n  def test_multivariate_sample_mean_and_variance(self, dtype):\n    """"""Tests the mean and vol of the univariate GBM sampled paths.""""""\n    means = 0.05\n    volatilities = [0.1, 0.2]\n    corr_matrix = [[1, 0.1], [0.1, 1]]\n    process = tff.models.MultivariateGeometricBrownianMotion(\n        dim=2, means=means, volatilities=volatilities, corr_matrix=corr_matrix,\n        dtype=dtype)\n    times = [0.1, 0.5, 1.0]\n    initial_state = [1.0, 2.0]\n    samples = process.sample_paths(\n        times=times, initial_state=initial_state,\n        random_type=tff.math.random.RandomType.SOBOL, num_samples=10000)\n    log_s = tf.math.log(samples)\n    mean = tf.reduce_mean(log_s, axis=0, keepdims=True)\n    var = tf.reduce_mean((log_s - mean)**2, axis=0)\n    expected_mean = ((process._means - process._vols**2 / 2)\n                     * np.array(np.expand_dims(times, -1))\n                     + np.log(initial_state))\n    expected_var = process._vols**2 * np.array(np.expand_dims(times, -1))\n    with self.subTest(""Drift""):\n      self.assertAllClose(tf.squeeze(mean), expected_mean, atol=1e-3, rtol=1e-3)\n    with self.subTest(""Variance""):\n      self.assertAllClose(tf.squeeze(var), expected_var, atol=1e-3, rtol=1e-3)\n    with self.subTest(""Correlations""):\n      samples = self.evaluate(samples)\n      for i in range(len(times)):\n        corr = np.corrcoef(samples[:, i, :], rowvar=False)\n      self.assertAllClose(corr, corr_matrix, atol=1e-2, rtol=1e-2)\n\n  def test_univariate_xla_compatible(self):\n    """"""Tests that univariate GBM sampling is XLA-compatible.""""""\n    process = tff.models.GeometricBrownianMotion(0.05, 0.5, dtype=tf.float64)\n    @tf.function\n    def sample_fn():\n      return process.sample_paths(\n          times=[0.1, 0.5, 1.0], initial_state=2.0, num_samples=10000)\n    samples = tf.xla.experimental.compile(sample_fn)[0]\n    log_s = tf.math.log(samples)\n    mean = tf.reduce_mean(log_s, axis=0)\n    expected_mean = ((process._mu - process._sigma**2 / 2)\n                     * np.array([0.1, 0.5, 1.0]) + np.log(2.))\n    self.assertAllClose(tf.squeeze(mean), expected_mean, atol=1e-2, rtol=1e-2)\n\n  def test_multiivariate_xla_compatible(self):\n    """"""Tests that multiivariate GBM sampling is XLA-compatible.""""""\n    corr_matrix = [[1, 0.1], [0.1, 1]]\n    process = tff.models.MultivariateGeometricBrownianMotion(\n        dim=2, means=0.05, volatilities=[0.1, 0.2], corr_matrix=corr_matrix,\n        dtype=tf.float64)\n    times = [0.1, 0.5, 1.0]\n    initial_state = [1.0, 2.0]\n    @tf.function\n    def sample_fn():\n      return process.sample_paths(\n          times=times, initial_state=initial_state, num_samples=10000)\n    samples = tf.xla.experimental.compile(sample_fn)[0]\n    log_s = tf.math.log(samples)\n    mean = tf.reduce_mean(log_s, axis=0)\n    expected_mean = ((process._means - process._vols**2 / 2)\n                     * np.array(np.expand_dims(times, -1))\n                     + np.log(initial_state))\n    self.assertAllClose(mean, expected_mean, atol=1e-2, rtol=1e-2)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n\n'"
tf_quant_finance/models/geometric_brownian_motion/multivariate_geometric_brownian_motion.py,31,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Multivariate Geometric Brownian Motion.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math.pde import fd_solvers\nfrom tf_quant_finance.models import ito_process\nfrom tf_quant_finance.models import utils\n\n\nclass MultivariateGeometricBrownianMotion(ito_process.ItoProcess):\n  """"""Multivariate Geometric Brownian Motion.\n\n  Represents a d-dimensional Ito process:\n\n  ```None\n    dX_i(t) = means_i * X_i(t) * dt + volatilities_i * X_i(t) * dW_i(t),\n    1 <= i <= d\n  ```\n\n  where `W(t) = (W_1(t), .., W_d(t))` is a d-dimensional Brownian motion with\n  a correlation matrix `corr_matrix`, `means` and `volatilities` are `Tensor`s\n  that correspond to mean and volatility of a Geometric Brownian Motion `X_i`\n\n  ## Example\n\n  ```python\n  import tf_quant_finance as tff\n  corr_matrix = [[1, 0.1], [0.1, 1]]\n  process = tff.MultivariateGeometricBrownianMotion(\n      means=1, volatilities=[0.1, 0.2],\n      corr_matrix=corr_matrix,\n      dtype=tf.float64)\n  times = [0.1, 0.2, 1.0]\n  initial_state=[1.0, 2.0]\n  samples = process.sample_paths(times=times,\n                                 initial_state=initial_state,\n                                 random_type=tff.math.random.RandomType.SOBOL,\n                                 num_samples=100000)\n  ```\n  """"""\n\n  def __init__(self,\n               dim,\n               means=0.0,\n               volatilities=1.0,\n               corr_matrix=None,\n               dtype=None,\n               name=None):\n    """"""Initializes the Multivariate Geometric Brownian Motion.\n\n    Args:\n      dim: A Python scalar. The dimensionality of the process\n      means:  A real `Tensor` of shape broadcastable to `[dim]`.\n        Corresponds to the vector of means of the GBM components `X_i`.\n      Default value: 0.0.\n      volatilities: A `Tensor` of the same `dtype` as `means` and of shape\n        broadcastable to `[dim]`. Corresponds to the volatilities of the GBM\n        components `X_i`.\n        Default value: 1.0.\n      corr_matrix: An optional `Tensor` of the same `dtype` as `means` and of\n        shape `[dim, dim]`. Correlation of the GBM components `W_i`.\n        Default value: `None` which maps to a process with\n        independent GBM components `X_i`.\n      dtype: The default dtype to use when converting values to `Tensor`s.\n        Default value: `None` which means that default dtypes inferred by\n          TensorFlow are used.\n      name: Python string. The name to give to the ops created by this class.\n        Default value: `None` which maps to the default name\n        \'multivariate_geometric_brownian_motion\'.\n    Raises:\n      ValueError: If `corr_matrix` is supplied and is not of shape `[dim, dim]`\n    """"""\n    self._name = name or ""multivariate_geometric_brownian_motion""\n    with tf.name_scope(self._name):\n      self._means = tf.convert_to_tensor(means, dtype=dtype,\n                                         name=""means"")\n      self._dtype = self._means.dtype\n      self._vols = tf.convert_to_tensor(volatilities, dtype=self._dtype,\n                                        name=""volatilities"")\n      self._dim = dim\n      if corr_matrix is None:\n        self._corr_matrix = None\n      else:\n        self._corr_matrix = tf.convert_to_tensor(corr_matrix, dtype=self._dtype,\n                                                 name=""corr_matrix"")\n        if self._corr_matrix.shape.as_list() != [dim, dim]:\n          raise ValueError(""`corr_matrix` must be of shape [{0}, {0}] but is ""\n                           ""of shape {1}"".format(\n                               dim, self._corr_matrix.shape.as_list()))\n\n  def dim(self):\n    """"""The dimension of the process.""""""\n    return self._dim\n\n  def dtype(self):\n    """"""The data type of process realizations.""""""\n    return self._dtype\n\n  def name(self):\n    """"""The name to give to ops created by this class.""""""\n    return self._name\n\n  def drift_fn(self):\n    """"""Python callable calculating instantaneous drift.""""""\n    def _drift_fn(t, x):\n      """"""Drift function of the GBM.""""""\n      del t\n      return self._means * x\n    return _drift_fn\n\n  def volatility_fn(self):\n    """"""Python callable calculating the instantaneous volatility.""""""\n    def _vol_fn(t, x):\n      """"""Volatility function of the GBM.""""""\n      del t\n      # Shape [num_samples, dim]\n      vols = self._vols * x\n      if self._corr_matrix is not None:\n        vols = tf.expand_dims(vols, axis=-1)\n        cholesky = tf.linalg.cholesky(self._corr_matrix)\n        return vols * cholesky\n      else:\n        return tf.linalg.diag(vols)\n    return _vol_fn\n\n  def sample_paths(self,\n                   times,\n                   initial_state=None,\n                   num_samples=1,\n                   random_type=None,\n                   seed=None,\n                   skip=0,\n                   name=None):\n    """"""Returns a sample of paths from the process.\n\n    Args:\n      times: Rank 1 `Tensor` of positive real values. The times at which the\n        path points are to be evaluated.\n      initial_state: A `Tensor` of the same `dtype` as `times` and of shape\n        broadcastable with `[num_samples, dim]`. Represents the initial state of\n        the Ito process.\n      Default value: `None` which maps to a initial state of ones.\n      num_samples: Positive scalar `int`. The number of paths to draw.\n      random_type: Enum value of `RandomType`. The type of (quasi)-random\n        number generator to use to generate the paths.\n        Default value: None which maps to the standard pseudo-random numbers.\n      seed: Seed for the random number generator. The seed is\n        only relevant if `random_type` is one of\n        `[STATELESS, PSEUDO, HALTON_RANDOMIZED, PSEUDO_ANTITHETIC,\n          STATELESS_ANTITHETIC]`. For `PSEUDO`, `PSEUDO_ANTITHETIC` and\n        `HALTON_RANDOMIZED` the seed should be an Python integer. For\n        `STATELESS` and  `STATELESS_ANTITHETIC `must be supplied as an integer\n        `Tensor` of shape `[2]`.\n        Default value: `None` which means no seed is set.\n      skip: `int32` 0-d `Tensor`. The number of initial points of the Sobol or\n        Halton sequence to skip. Used only when `random_type` is \'SOBOL\',\n        \'HALTON\', or \'HALTON_RANDOMIZED\', otherwise ignored.\n        Default value: 0.\n      name: Str. The name to give this op.\n        Default value: `sample_paths`.\n\n    Returns:\n      A `Tensor`s of shape [num_samples, k, dim] where `k` is the size\n      of the `times`.\n    """"""\n    name = name or (self._name + ""_sample_path"")\n    with tf.name_scope(name):\n      times = tf.convert_to_tensor(times, self._dtype)\n      if initial_state is None:\n        initial_state = tf.ones([num_samples, self._dim], dtype=self._dtype,\n                                name=""initial_state"")\n      else:\n        initial_state = (\n            tf.convert_to_tensor(initial_state, dtype=self._dtype,\n                                 name=""initial_state"")\n            + tf.zeros([num_samples, 1], dtype=self._dtype))\n      # Shape [num_samples, 1, dim]\n      initial_state = tf.expand_dims(initial_state, axis=1)\n      num_requested_times = times.shape[0]\n      return self._sample_paths(\n          times=times, num_requested_times=num_requested_times,\n          initial_state=initial_state,\n          num_samples=num_samples, random_type=random_type,\n          seed=seed, skip=skip)\n\n  def _sample_paths(self,\n                    times,\n                    num_requested_times,\n                    initial_state,\n                    num_samples,\n                    random_type,\n                    seed,\n                    skip):\n    """"""Returns a sample of paths from the process.""""""\n    # Normal draws needed for sampling.\n    # Shape [num_requested_times, num_samples, dim]\n    normal_draws = utils.generate_mc_normal_draws(\n        num_normal_draws=self._dim, num_time_steps=num_requested_times,\n        num_sample_paths=num_samples, random_type=random_type,\n        seed=seed,\n        dtype=self._dtype, skip=skip)\n    times = tf.concat([[0], times], -1)\n    # Time increments\n    # Shape [num_requested_times, 1, 1]\n    dt = tf.expand_dims(tf.expand_dims(times[1:] - times[:-1], axis=-1),\n                        axis=-1)\n    if self._corr_matrix is None:\n      stochastic_increment = normal_draws\n    else:\n      cholesky = tf.linalg.cholesky(self._corr_matrix)\n      stochastic_increment = tf.linalg.matvec(cholesky, normal_draws)\n\n    # The logarithm of all the increments between the times.\n    # Shape [num_requested_times, num_samples, dim]\n    log_increments = ((self._means - self._vols**2 / 2) * dt\n                      + tf.sqrt(dt) * self._vols\n                      * stochastic_increment)\n\n    # Since the implementation of tf.math.cumsum is single-threaded we\n    # use lower-triangular matrix multiplication instead\n    once = tf.ones([num_requested_times, num_requested_times],\n                   dtype=self._dtype)\n    lower_triangular = tf.linalg.band_part(once, -1, 0)\n    cumsum = tf.linalg.matvec(lower_triangular,\n                              tf.transpose(log_increments))\n    cumsum = tf.transpose(cumsum, [1, 2, 0])\n    samples = initial_state * tf.math.exp(cumsum)\n    return samples\n\n  def fd_solver_backward(self,\n                         start_time,\n                         end_time,\n                         coord_grid,\n                         values_grid,\n                         discounting=None,\n                         one_step_fn=None,\n                         boundary_conditions=None,\n                         start_step_count=0,\n                         num_steps=None,\n                         time_step=None,\n                         values_transform_fn=None,\n                         dtype=None,\n                         name=None,\n                         **kwargs):\n    """"""Returns a solver for Feynman-Kac PDE associated to the process.\n\n    This method applies a finite difference method to solve the final value\n    problem as it appears in the Feynman-Kac formula associated to this Ito\n    process. The Feynman-Kac PDE is closely related to the backward Kolomogorov\n    equation associated to the stochastic process and allows for the inclusion\n    of a discounting function.\n\n    For more details of the Feynman-Kac theorem see [1]. The PDE solved by this\n    method is:\n\n    ```None\n      V_t + Sum[mu_i(t, x) V_i, 1<=i<=n] +\n        (1/2) Sum[ D_{ij} V_{ij}, 1 <= i,j <= n] - r(t, x) V = 0\n    ```\n\n    In the above, `V_t` is the derivative of `V` with respect to `t`,\n    `V_i` is the partial derivative with respect to `x_i` and `V_{ij}` the\n    (mixed) partial derivative with respect to `x_i` and `x_j`. `mu_i` is the\n    drift of this process and `D_{ij}` are the components of the diffusion\n    tensor:\n\n    ```None\n      D_{ij}(t,x) = (Sigma(t,x) . Transpose[Sigma(t,x)])_{ij}\n    ```\n\n    This method evolves a spatially discretized solution of the above PDE from\n    time `t0` to time `t1 < t0` (i.e. backwards in time).\n    The solution `V(t,x)` is assumed to be discretized on an `n`-dimensional\n    rectangular grid. A rectangular grid, G, in n-dimensions may be described\n    by specifying the coordinates of the points along each axis. For example,\n    a 2 x 4 grid in two dimensions can be specified by taking the cartesian\n    product of [1, 3] and [5, 6, 7, 8] to yield the grid points with\n    coordinates: `[(1, 5), (1, 6), (1, 7), (1, 8), (3, 5) ... (3, 8)]`.\n\n    This method allows batching of solutions. In this context, batching means\n    the ability to represent and evolve multiple independent functions `V`\n    (e.g. V1, V2 ...) simultaneously. A single discretized solution is specified\n    by stating its values at each grid point. This can be represented as a\n    `Tensor` of shape [d1, d2, ... dn] where di is the grid size along the `i`th\n    axis. A batch of such solutions is represented by a `Tensor` of shape:\n    [K, d1, d2, ... dn] where `K` is the batch size. This method only requires\n    that the input parameter `values_grid` be broadcastable with shape\n    [K, d1, ... dn].\n\n    The evolution of the solution from `t0` to `t1` is often done by\n    discretizing the differential equation to a difference equation along\n    the spatial and temporal axes. The temporal discretization is given by a\n    (sequence of) time steps [dt_1, dt_2, ... dt_k] such that the sum of the\n    time steps is equal to the total time step `t0 - t1`. If a uniform time\n    step is used, it may equivalently be specified by stating the number of\n    steps (n_steps) to take. This method provides both options via the\n    `time_step` and `num_steps` parameters. However, not all methods need\n    discretization along time direction (e.g. method of lines) so this argument\n    may not be applicable to some implementations.\n\n    The workhorse of this method is the `one_step_fn`. For the commonly used\n    methods, see functions in `math.pde.steppers` module.\n\n    The mapping between the arguments of this method and the above\n    equation are described in the Args section below.\n\n    For a simple instructive example of implementation of this method, see\n    `models.GenericItoProcess.fd_solver_backward`.\n\n    # TODO(b/142309558): Complete documentation.\n\n    Args:\n      start_time: Real positive scalar `Tensor`. The start time of the grid.\n        Corresponds to time `t0` above.\n      end_time: Real scalar `Tensor` smaller than the `start_time` and greater\n        than zero. The time to step back to. Corresponds to time `t1` above.\n      coord_grid: List of `n` rank 1 real `Tensor`s. `n` is the dimension of the\n        domain. The i-th `Tensor` has shape, `[d_i]` where `d_i` is the size of\n        the grid along axis `i`. The coordinates of the grid points. Corresponds\n        to the spatial grid `G` above.\n      values_grid: Real `Tensor` containing the function values at time\n        `start_time` which have to be stepped back to time `end_time`. The shape\n        of the `Tensor` must broadcast with `[K, d_1, d_2, ..., d_n]`. The first\n        axis of size `K` is the values batch dimension and allows multiple\n        functions (with potentially different boundary/final conditions) to be\n        stepped back simultaneously.\n      discounting: Callable corresponding to `r(t,x)` above. If not supplied,\n        zero discounting is assumed.\n      one_step_fn: The transition kernel. A callable that consumes the following\n        arguments by keyword:\n          1. \'time\': Current time\n          2. \'next_time\': The next time to step to. For the backwards in time\n            evolution, this time will be smaller than the current time.\n          3. \'coord_grid\': The coordinate grid.\n          4. \'values_grid\': The values grid.\n          5. \'boundary_conditions\': The boundary conditions.\n          6. \'quadratic_coeff\': A callable returning the quadratic coefficients\n            of the PDE (i.e. `(1/2)D_{ij}(t, x)` above). The callable accepts\n            the time and  coordinate grid as keyword arguments and returns a\n            `Tensor` with shape that broadcasts with `[dim, dim]`.\n          7. \'linear_coeff\': A callable returning the linear coefficients of the\n            PDE (i.e. `mu_i(t, x)` above). Accepts time and coordinate grid as\n            keyword arguments and returns a `Tensor` with shape that broadcasts\n            with `[dim]`.\n          8. \'constant_coeff\': A callable returning the coefficient of the\n            linear homogenous term (i.e. `r(t,x)` above). Same spec as above.\n            The `one_step_fn` callable returns a 2-tuple containing the next\n            coordinate grid, next values grid.\n      boundary_conditions: A list of size `dim` containing boundary conditions.\n        The i\'th element of the list is a 2-tuple containing the lower and upper\n        boundary condition for the boundary along the i`th axis.\n      start_step_count: Scalar integer `Tensor`. Initial value for the number of\n        time steps performed.\n        Default value: 0 (i.e. no previous steps performed).\n      num_steps: Positive int scalar `Tensor`. The number of time steps to take\n        when moving from `start_time` to `end_time`. Either this argument or the\n        `time_step` argument must be supplied (but not both). If num steps is\n        `k>=1`, uniform time steps of size `(t0 - t1)/k` are taken to evolve the\n        solution from `t0` to `t1`. Corresponds to the `n_steps` parameter\n        above.\n      time_step: The time step to take. Either this argument or the `num_steps`\n        argument must be supplied (but not both). The type of this argument may\n        be one of the following (in order of generality): (a) None in which case\n          `num_steps` must be supplied. (b) A positive real scalar `Tensor`. The\n          maximum time step to take. If the value of this argument is `dt`, then\n          the total number of steps taken is N = (t0 - t1) / dt rounded up to\n          the nearest integer. The first N-1 steps are of size dt and the last\n          step is of size `t0 - t1 - (N-1) * dt`. (c) A callable accepting the\n          current time and returning the size of the step to take. The input and\n          the output are real scalar `Tensor`s.\n      values_transform_fn: An optional callable applied to transform the\n        solution values at each time step. The callable is invoked after the\n        time step has been performed. The callable should accept the time of the\n        grid, the coordinate grid and the values grid and should return the\n        values grid. All input arguments to be passed by keyword.\n      dtype: The dtype to use.\n      name: The name to give to the ops.\n        Default value: None which means `solve_backward` is used.\n      **kwargs: Additional keyword args:\n        (1) pde_solver_fn: Function to solve the PDE that accepts all the above\n          arguments by name and returns the same tuple object as required below.\n          Defaults to `tff.math.pde.fd_solvers.solve_backward`.\n\n    Returns:\n      A tuple object containing at least the following attributes:\n        final_values_grid: A `Tensor` of same shape and dtype as `values_grid`.\n          Contains the final state of the values grid at time `end_time`.\n        final_coord_grid: A list of `Tensor`s of the same specification as\n          the input `coord_grid`. Final state of the coordinate grid at time\n          `end_time`.\n        step_count: The total step count (i.e. the sum of the `start_step_count`\n          and the number of steps performed in this call.).\n        final_time: The final time at which the evolution stopped. This value\n          is given by `max(min(end_time, start_time), 0)`.\n    """"""\n    pde_solver_fn = kwargs.get(""pde_solver_fn"", fd_solvers.solve_backward)\n\n    second_order_coeff_fn, first_order_coeff_fn, zeroth_order_coeff_fn = (\n        _backward_pde_coeffs(self._drift_fn, self._volatility_fn, discounting))\n\n    return pde_solver_fn(\n        start_time=start_time,\n        end_time=end_time,\n        coord_grid=coord_grid,\n        values_grid=values_grid,\n        num_steps=num_steps,\n        start_step_count=start_step_count,\n        time_step=time_step,\n        one_step_fn=one_step_fn,\n        boundary_conditions=boundary_conditions,\n        values_transform_fn=values_transform_fn,\n        second_order_coeff_fn=second_order_coeff_fn,\n        first_order_coeff_fn=first_order_coeff_fn,\n        zeroth_order_coeff_fn=zeroth_order_coeff_fn,\n        dtype=dtype,\n        name=name)\n\n  def fd_solver_forward(self,\n                        start_time,\n                        end_time,\n                        coord_grid,\n                        values_grid,\n                        one_step_fn=None,\n                        boundary_conditions=None,\n                        start_step_count=0,\n                        num_steps=None,\n                        time_step=None,\n                        values_transform_fn=None,\n                        dtype=None,\n                        name=None,\n                        **kwargs):\n    r""""""Returns a solver for the Fokker Plank equation of this process.\n\n    The Fokker Plank equation (also known as the Kolmogorov Forward equation)\n    associated to this Ito process is given by:\n\n    ```None\n      V_t + Sum[(mu_i(t, x) V)_i, 1<=i<=n]\n        - (1/2) Sum[ (D_{ij} V)_{ij}, 1 <= i,j <= n] = 0\n    ```\n\n    with the initial value condition $$V(0, x) = u(x)$$.\n\n    This method evolves a spatially discretized solution of the above PDE from\n    time `t0` to time `t1 > t0` (i.e. forwards in time).\n    The solution `V(t,x)` is assumed to be discretized on an `n`-dimensional\n    rectangular grid. A rectangular grid, G, in n-dimensions may be described\n    by specifying the coordinates of the points along each axis. For example,\n    a 2 x 4 grid in two dimensions can be specified by taking the cartesian\n    product of [1, 3] and [5, 6, 7, 8] to yield the grid points with\n    coordinates: `[(1, 5), (1, 6), (1, 7), (1, 8), (3, 5) ... (3, 8)]`.\n\n    Batching of solutions is supported. In this context, batching means\n    the ability to represent and evolve multiple independent functions `V`\n    (e.g. V1, V2 ...) simultaneously. A single discretized solution is specified\n    by stating its values at each grid point. This can be represented as a\n    `Tensor` of shape [d1, d2, ... dn] where di is the grid size along the `i`th\n    axis. A batch of such solutions is represented by a `Tensor` of shape:\n    [K, d1, d2, ... dn] where `K` is the batch size. This method only requires\n    that the input parameter `values_grid` be broadcastable with shape\n    [K, d1, ... dn].\n\n    The evolution of the solution from `t0` to `t1` is often done by\n    discretizing the differential equation to a difference equation along\n    the spatial and temporal axes. The temporal discretization is given by a\n    (sequence of) time steps [dt_1, dt_2, ... dt_k] such that the sum of the\n    time steps is equal to the total time step `t1 - t0`. If a uniform time\n    step is used, it may equivalently be specified by stating the number of\n    steps (n_steps) to take. This method provides both options via the\n    `time_step` and `num_steps` parameters. However, not all methods need\n    discretization along time direction (e.g. method of lines) so this argument\n    may not be applicable to some implementations.\n\n    The workhorse of this method is the `one_step_fn`. For the commonly used\n    methods, see functions in `math.pde.steppers` module.\n\n    The mapping between the arguments of this method and the above\n    equation are described in the Args section below.\n\n    For a simple instructive example of implementation of this method, see\n    `models.GenericItoProcess.fd_solver_forward`.\n\n    # TODO(b/142309558): Complete documentation.\n\n    Args:\n      start_time: Real positive scalar `Tensor`. The start time of the grid.\n        Corresponds to time `t0` above.\n      end_time: Real scalar `Tensor` smaller than the `start_time` and greater\n        than zero. The time to step back to. Corresponds to time `t1` above.\n      coord_grid: List of `n` rank 1 real `Tensor`s. `n` is the dimension of the\n        domain. The i-th `Tensor` has shape, `[d_i]` where `d_i` is the size of\n        the grid along axis `i`. The coordinates of the grid points. Corresponds\n        to the spatial grid `G` above.\n      values_grid: Real `Tensor` containing the function values at time\n        `start_time` which have to be stepped back to time `end_time`. The shape\n        of the `Tensor` must broadcast with `[K, d_1, d_2, ..., d_n]`. The first\n        axis of size `K` is the values batch dimension and allows multiple\n        functions (with potentially different boundary/final conditions) to be\n        stepped back simultaneously.\n      one_step_fn: The transition kernel. A callable that consumes the following\n        arguments by keyword:\n          1. \'time\': Current time\n          2. \'next_time\': The next time to step to. For the backwards in time\n            evolution, this time will be smaller than the current time.\n          3. \'coord_grid\': The coordinate grid.\n          4. \'values_grid\': The values grid.\n          5. \'quadratic_coeff\': A callable returning the quadratic coefficients\n            of the PDE (i.e. `(1/2)D_{ij}(t, x)` above). The callable accepts\n            the time and  coordinate grid as keyword arguments and returns a\n            `Tensor` with shape that broadcasts with `[dim, dim]`.\n          6. \'linear_coeff\': A callable returning the linear coefficients of the\n            PDE (i.e. `mu_i(t, x)` above). Accepts time and coordinate grid as\n            keyword arguments and returns a `Tensor` with shape that broadcasts\n            with `[dim]`.\n          7. \'constant_coeff\': A callable returning the coefficient of the\n            linear homogenous term (i.e. `r(t,x)` above). Same spec as above.\n            The `one_step_fn` callable returns a 2-tuple containing the next\n            coordinate grid, next values grid.\n      boundary_conditions: A list of size `dim` containing boundary conditions.\n        The i\'th element of the list is a 2-tuple containing the lower and upper\n        boundary condition for the boundary along the i`th axis.\n      start_step_count: Scalar integer `Tensor`. Initial value for the number of\n        time steps performed.\n        Default value: 0 (i.e. no previous steps performed).\n      num_steps: Positive int scalar `Tensor`. The number of time steps to take\n        when moving from `start_time` to `end_time`. Either this argument or the\n        `time_step` argument must be supplied (but not both). If num steps is\n        `k>=1`, uniform time steps of size `(t0 - t1)/k` are taken to evolve the\n        solution from `t0` to `t1`. Corresponds to the `n_steps` parameter\n        above.\n      time_step: The time step to take. Either this argument or the `num_steps`\n        argument must be supplied (but not both). The type of this argument may\n        be one of the following (in order of generality): (a) None in which case\n          `num_steps` must be supplied. (b) A positive real scalar `Tensor`. The\n          maximum time step to take. If the value of this argument is `dt`, then\n          the total number of steps taken is N = (t1 - t0) / dt rounded up to\n          the nearest integer. The first N-1 steps are of size dt and the last\n          step is of size `t1 - t0 - (N-1) * dt`. (c) A callable accepting the\n          current time and returning the size of the step to take. The input and\n          the output are real scalar `Tensor`s.\n      values_transform_fn: An optional callable applied to transform the\n        solution values at each time step. The callable is invoked after the\n        time step has been performed. The callable should accept the time of the\n        grid, the coordinate grid and the values grid and should return the\n        values grid. All input arguments to be passed by keyword.\n      dtype: The dtype to use.\n      name: The name to give to the ops.\n        Default value: None which means `solve_forward` is used.\n      **kwargs: Additional keyword args:\n        (1) pde_solver_fn: Function to solve the PDE that accepts all the above\n          arguments by name and returns the same tuple object as required below.\n          Defaults to `tff.math.pde.fd_solvers.solve_forward`.\n\n    Returns:\n      A tuple object containing at least the following attributes:\n        final_values_grid: A `Tensor` of same shape and dtype as `values_grid`.\n          Contains the final state of the values grid at time `end_time`.\n        final_coord_grid: A list of `Tensor`s of the same specification as\n          the input `coord_grid`. Final state of the coordinate grid at time\n          `end_time`.\n        step_count: The total step count (i.e. the sum of the `start_step_count`\n          and the number of steps performed in this call.).\n        final_time: The final time at which the evolution stopped. This value\n          is given by `max(min(end_time, start_time), 0)`.\n    """"""\n    pde_solver_fn = kwargs.get(""pde_solver_fn"", fd_solvers.solve_forward)\n\n    backward_second_order, backward_first_order, backward_zeroth_order = (\n        _backward_pde_coeffs(self._drift_fn, self._volatility_fn,\n                             discounting=None))\n\n    # Transform backward to forward equation.\n    inner_second_order_coeff_fn = lambda t, x: -backward_second_order(t, x)\n    inner_first_order_coeff_fn = backward_first_order\n    zeroth_order_coeff_fn = backward_zeroth_order\n\n    return pde_solver_fn(\n        start_time=start_time,\n        end_time=end_time,\n        coord_grid=coord_grid,\n        values_grid=values_grid,\n        num_steps=num_steps,\n        start_step_count=start_step_count,\n        time_step=time_step,\n        one_step_fn=one_step_fn,\n        boundary_conditions=boundary_conditions,\n        values_transform_fn=values_transform_fn,\n        inner_second_order_coeff_fn=inner_second_order_coeff_fn,\n        inner_first_order_coeff_fn=inner_first_order_coeff_fn,\n        zeroth_order_coeff_fn=zeroth_order_coeff_fn,\n        dtype=dtype,\n        name=name)\n\n\ndef _backward_pde_coeffs(drift_fn, volatility_fn, discounting):\n  """"""Returns coeffs of the backward PDE.""""""\n  def second_order_coeff_fn(t, coord_grid):\n    sigma = volatility_fn(t, _coord_grid_to_mesh_grid(coord_grid))\n    sigma_times_sigma_t = tf.linalg.matmul(sigma, sigma, transpose_b=True)\n\n    # We currently have [dim, dim] as innermost dimensions, but the returned\n    # tensor must have [dim, dim] as outermost dimensions.\n    rank = len(sigma.shape.as_list())\n    perm = [rank - 2, rank - 1] + list(range(rank - 2))\n    sigma_times_sigma_t = tf.transpose(sigma_times_sigma_t, perm)\n    return sigma_times_sigma_t / 2\n\n  def first_order_coeff_fn(t, coord_grid):\n    mu = drift_fn(t, _coord_grid_to_mesh_grid(coord_grid))\n\n    # We currently have [dim] as innermost dimension, but the returned\n    # tensor must have [dim] as outermost dimension.\n    rank = len(mu.shape.as_list())\n    perm = [rank - 1] + list(range(rank - 1))\n    mu = tf.transpose(mu, perm)\n    return mu\n\n  def zeroth_order_coeff_fn(t, coord_grid):\n    if not discounting:\n      return None\n    return -discounting(t, _coord_grid_to_mesh_grid(coord_grid))\n\n  return second_order_coeff_fn, first_order_coeff_fn, zeroth_order_coeff_fn\n\n\ndef _coord_grid_to_mesh_grid(coord_grid):\n  if len(coord_grid) == 1:\n    return tf.expand_dims(coord_grid[0], -1)\n  return tf.stack(values=tf.meshgrid(*coord_grid, indexing=""ij""), axis=-1)\n'"
tf_quant_finance/models/geometric_brownian_motion/univariate_geometric_brownian_motion.py,24,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Geometric Brownian Motion model.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math.pde import fd_solvers\nfrom tf_quant_finance.models import ito_process\nfrom tf_quant_finance.models import utils\n\n\nclass GeometricBrownianMotion(ito_process.ItoProcess):\n  """"""Geometric Brownian Motion.\n\n  Represents the 1-dimensional Ito process:\n\n  ```None\n    dX(t) = mu * X(t) * dt + sigma * X(t) * dW(t),\n  ```\n\n  where `W(t)` is a 1D Brownian motion, `mu` and `sigma` are constant `Tensor`s.\n\n  ## Example\n\n  ```python\n  import tf_quant_finance as tff\n  process = GeometricBrownianMotion(0.05, 1.0, dtype=tf.float64)\n  samples = process.sample_paths([0.1, 0.5, 1.0],\n                                 initial_state=1.5,\n                                 random_type=random.RandomType.SOBOL,\n                                 num_samples=1000000)\n  ```\n  """"""\n\n  def __init__(self,\n               mu,\n               sigma,\n               dtype=None,\n               name=None):\n    """"""Initializes the Geometric Brownian Motion.\n\n    Args:\n      mu: Scalar real `Tensor`. Corresponds to the mean of the Ito process.\n      sigma: Scalar real `Tensor` of the same `dtype` as `mu`. Corresponds to\n        the volatility of the process.\n      dtype: The default dtype to use when converting values to `Tensor`s.\n        Default value: `None` which means that default dtypes inferred by\n          TensorFlow are used.\n      name: Python string. The name to give to the ops created by this class.\n        Default value: `None` which maps to the default name\n        \'geometric_brownian_motion\'.\n    """"""\n    self._name = name or ""geometric_brownian_motion""\n    with tf.name_scope(self._name):\n      self._mu = tf.convert_to_tensor(mu, dtype=dtype, name=""mu"")\n      self._dtype = self._mu.dtype\n      self._sigma = tf.convert_to_tensor(sigma, dtype=self._dtype, name=""sigma"")\n      self._dim = 1\n\n  def dim(self):\n    """"""The dimension of the process.""""""\n    return self._dim\n\n  def dtype(self):\n    """"""The data type of process realizations.""""""\n    return self._dtype\n\n  def name(self):\n    """"""The name to give to ops created by this class.""""""\n    return self._name\n\n  def drift_fn(self):\n    """"""Python callable calculating instantaneous drift.""""""\n    def _drift_fn(t, x):\n      """"""Drift function of the GBM.""""""\n      del t\n      return self._mu * x\n    return _drift_fn\n\n  def volatility_fn(self):\n    """"""Python callable calculating the instantaneous volatility.""""""\n    def _vol_fn(t, x):\n      """"""Volatility function of the GBM.""""""\n      del t\n      vol = self._sigma * tf.expand_dims(x, -1)\n      return vol\n    return _vol_fn\n\n  def sample_paths(self,\n                   times,\n                   initial_state=None,\n                   num_samples=1,\n                   random_type=None,\n                   seed=None,\n                   skip=0,\n                   name=None):\n    """"""Returns a sample of paths from the process.\n\n    Args:\n      times: Rank 1 `Tensor` of positive real values. The times at which the\n        path points are to be evaluated.\n      initial_state: A `Tensor` of the same `dtype` as `times` and of shape\n        broadcastable with `[num_samples]`. Represents the initial state of the\n        Ito process.\n      Default value: `None` which maps to a initial state of ones.\n      num_samples: Positive scalar `int`. The number of paths to draw.\n      random_type: Enum value of `RandomType`. The type of (quasi)-random\n        number generator to use to generate the paths.\n        Default value: None which maps to the standard pseudo-random numbers.\n      seed: Seed for the random number generator. The seed is\n        only relevant if `random_type` is one of\n        `[STATELESS, PSEUDO, HALTON_RANDOMIZED, PSEUDO_ANTITHETIC,\n          STATELESS_ANTITHETIC]`. For `PSEUDO`, `PSEUDO_ANTITHETIC` and\n        `HALTON_RANDOMIZED` the seed should be an Python integer. For\n        `STATELESS` and  `STATELESS_ANTITHETIC `must be supplied as an integer\n        `Tensor` of shape `[2]`.\n        Default value: `None` which means no seed is set.\n      skip: `int32` 0-d `Tensor`. The number of initial points of the Sobol or\n        Halton sequence to skip. Used only when `random_type` is \'SOBOL\',\n        \'HALTON\', or \'HALTON_RANDOMIZED\', otherwise ignored.\n        Default value: 0.\n      name: Str. The name to give this op.\n        Default value: `sample_paths`.\n\n    Returns:\n      A `Tensor`s of shape [num_samples, k, 1] where `k` is the size\n      of the `times`.\n    """"""\n    name = name or (self._name + ""_sample_path"")\n    with tf.name_scope(name):\n      times = tf.convert_to_tensor(times, self._dtype)\n      if initial_state is None:\n        initial_state = tf.zeros([num_samples, 1], dtype=self._dtype,\n                                 name=""initial_state"")\n      else:\n        initial_state = (\n            tf.convert_to_tensor(initial_state, dtype=self._dtype,\n                                 name=""initial_state"")\n            + tf.zeros([num_samples, 1], dtype=self._dtype))\n      num_requested_times = times.shape[0]\n      return self._sample_paths(\n          times=times, num_requested_times=num_requested_times,\n          initial_state=initial_state,\n          num_samples=num_samples, random_type=random_type,\n          seed=seed, skip=skip)\n\n  def _sample_paths(self,\n                    times,\n                    num_requested_times,\n                    initial_state,\n                    num_samples,\n                    random_type,\n                    seed,\n                    skip):\n    """"""Returns a sample of paths from the process.""""""\n    # Normal draws needed for sampling\n    normal_draws = utils.generate_mc_normal_draws(\n        num_normal_draws=1, num_time_steps=num_requested_times,\n        num_sample_paths=num_samples, random_type=random_type,\n        seed=seed,\n        dtype=self._dtype, skip=skip)\n    times = tf.concat([[0], times], -1)\n    dt = times[1:] - times[:-1]\n    # The logarithm of all the increments between the times.\n    log_increments = ((self._mu - self._sigma**2 / 2) * dt\n                      + tf.sqrt(dt) * self._sigma\n                      * tf.transpose(tf.squeeze(normal_draws, -1)))\n    # Since the implementation of tf.math.cumsum is single-threaded we\n    # use lower-triangular matrix multiplication instead\n    once = tf.ones([num_requested_times, num_requested_times],\n                   dtype=self._dtype)\n    lower_triangular = tf.linalg.band_part(once, -1, 0)\n    cumsum = tf.linalg.matvec(lower_triangular,\n                              log_increments)\n    samples = initial_state * tf.math.exp(cumsum)\n    return tf.expand_dims(samples, -1)\n\n  # TODO(b/152967694): Remove the duplicate methods.\n  def fd_solver_backward(self,\n                         start_time,\n                         end_time,\n                         coord_grid,\n                         values_grid,\n                         discounting=None,\n                         one_step_fn=None,\n                         boundary_conditions=None,\n                         start_step_count=0,\n                         num_steps=None,\n                         time_step=None,\n                         values_transform_fn=None,\n                         dtype=None,\n                         name=None,\n                         **kwargs):\n    """"""Returns a solver for Feynman-Kac PDE associated to the process.\n\n    This method applies a finite difference method to solve the final value\n    problem as it appears in the Feynman-Kac formula associated to this Ito\n    process. The Feynman-Kac PDE is closely related to the backward Kolomogorov\n    equation associated to the stochastic process and allows for the inclusion\n    of a discounting function.\n\n    For more details of the Feynman-Kac theorem see [1]. The PDE solved by this\n    method is:\n\n    ```None\n      V_t + Sum[mu_i(t, x) V_i, 1<=i<=n] +\n        (1/2) Sum[ D_{ij} V_{ij}, 1 <= i,j <= n] - r(t, x) V = 0\n    ```\n\n    In the above, `V_t` is the derivative of `V` with respect to `t`,\n    `V_i` is the partial derivative with respect to `x_i` and `V_{ij}` the\n    (mixed) partial derivative with respect to `x_i` and `x_j`. `mu_i` is the\n    drift of this process and `D_{ij}` are the components of the diffusion\n    tensor:\n\n    ```None\n      D_{ij}(t,x) = (Sigma(t,x) . Transpose[Sigma(t,x)])_{ij}\n    ```\n\n    This method evolves a spatially discretized solution of the above PDE from\n    time `t0` to time `t1 < t0` (i.e. backwards in time).\n    The solution `V(t,x)` is assumed to be discretized on an `n`-dimensional\n    rectangular grid. A rectangular grid, G, in n-dimensions may be described\n    by specifying the coordinates of the points along each axis. For example,\n    a 2 x 4 grid in two dimensions can be specified by taking the cartesian\n    product of [1, 3] and [5, 6, 7, 8] to yield the grid points with\n    coordinates: `[(1, 5), (1, 6), (1, 7), (1, 8), (3, 5) ... (3, 8)]`.\n\n    This method allows batching of solutions. In this context, batching means\n    the ability to represent and evolve multiple independent functions `V`\n    (e.g. V1, V2 ...) simultaneously. A single discretized solution is specified\n    by stating its values at each grid point. This can be represented as a\n    `Tensor` of shape [d1, d2, ... dn] where di is the grid size along the `i`th\n    axis. A batch of such solutions is represented by a `Tensor` of shape:\n    [K, d1, d2, ... dn] where `K` is the batch size. This method only requires\n    that the input parameter `values_grid` be broadcastable with shape\n    [K, d1, ... dn].\n\n    The evolution of the solution from `t0` to `t1` is often done by\n    discretizing the differential equation to a difference equation along\n    the spatial and temporal axes. The temporal discretization is given by a\n    (sequence of) time steps [dt_1, dt_2, ... dt_k] such that the sum of the\n    time steps is equal to the total time step `t0 - t1`. If a uniform time\n    step is used, it may equivalently be specified by stating the number of\n    steps (n_steps) to take. This method provides both options via the\n    `time_step` and `num_steps` parameters. However, not all methods need\n    discretization along time direction (e.g. method of lines) so this argument\n    may not be applicable to some implementations.\n\n    The workhorse of this method is the `one_step_fn`. For the commonly used\n    methods, see functions in `math.pde.steppers` module.\n\n    The mapping between the arguments of this method and the above\n    equation are described in the Args section below.\n\n    For a simple instructive example of implementation of this method, see\n    `models.GenericItoProcess.fd_solver_backward`.\n\n    # TODO(b/142309558): Complete documentation.\n\n    Args:\n      start_time: Real positive scalar `Tensor`. The start time of the grid.\n        Corresponds to time `t0` above.\n      end_time: Real scalar `Tensor` smaller than the `start_time` and greater\n        than zero. The time to step back to. Corresponds to time `t1` above.\n      coord_grid: List of `n` rank 1 real `Tensor`s. `n` is the dimension of the\n        domain. The i-th `Tensor` has shape, `[d_i]` where `d_i` is the size of\n        the grid along axis `i`. The coordinates of the grid points. Corresponds\n        to the spatial grid `G` above.\n      values_grid: Real `Tensor` containing the function values at time\n        `start_time` which have to be stepped back to time `end_time`. The shape\n        of the `Tensor` must broadcast with `[K, d_1, d_2, ..., d_n]`. The first\n        axis of size `K` is the values batch dimension and allows multiple\n        functions (with potentially different boundary/final conditions) to be\n        stepped back simultaneously.\n      discounting: Callable corresponding to `r(t,x)` above. If not supplied,\n        zero discounting is assumed.\n      one_step_fn: The transition kernel. A callable that consumes the following\n        arguments by keyword:\n          1. \'time\': Current time\n          2. \'next_time\': The next time to step to. For the backwards in time\n            evolution, this time will be smaller than the current time.\n          3. \'coord_grid\': The coordinate grid.\n          4. \'values_grid\': The values grid.\n          5. \'boundary_conditions\': The boundary conditions.\n          6. \'quadratic_coeff\': A callable returning the quadratic coefficients\n            of the PDE (i.e. `(1/2)D_{ij}(t, x)` above). The callable accepts\n            the time and  coordinate grid as keyword arguments and returns a\n            `Tensor` with shape that broadcasts with `[dim, dim]`.\n          7. \'linear_coeff\': A callable returning the linear coefficients of the\n            PDE (i.e. `mu_i(t, x)` above). Accepts time and coordinate grid as\n            keyword arguments and returns a `Tensor` with shape that broadcasts\n            with `[dim]`.\n          8. \'constant_coeff\': A callable returning the coefficient of the\n            linear homogenous term (i.e. `r(t,x)` above). Same spec as above.\n            The `one_step_fn` callable returns a 2-tuple containing the next\n            coordinate grid, next values grid.\n      boundary_conditions: A list of size `dim` containing boundary conditions.\n        The i\'th element of the list is a 2-tuple containing the lower and upper\n        boundary condition for the boundary along the i`th axis.\n      start_step_count: Scalar integer `Tensor`. Initial value for the number of\n        time steps performed.\n        Default value: 0 (i.e. no previous steps performed).\n      num_steps: Positive int scalar `Tensor`. The number of time steps to take\n        when moving from `start_time` to `end_time`. Either this argument or the\n        `time_step` argument must be supplied (but not both). If num steps is\n        `k>=1`, uniform time steps of size `(t0 - t1)/k` are taken to evolve the\n        solution from `t0` to `t1`. Corresponds to the `n_steps` parameter\n        above.\n      time_step: The time step to take. Either this argument or the `num_steps`\n        argument must be supplied (but not both). The type of this argument may\n        be one of the following (in order of generality): (a) None in which case\n          `num_steps` must be supplied. (b) A positive real scalar `Tensor`. The\n          maximum time step to take. If the value of this argument is `dt`, then\n          the total number of steps taken is N = (t0 - t1) / dt rounded up to\n          the nearest integer. The first N-1 steps are of size dt and the last\n          step is of size `t0 - t1 - (N-1) * dt`. (c) A callable accepting the\n          current time and returning the size of the step to take. The input and\n          the output are real scalar `Tensor`s.\n      values_transform_fn: An optional callable applied to transform the\n        solution values at each time step. The callable is invoked after the\n        time step has been performed. The callable should accept the time of the\n        grid, the coordinate grid and the values grid and should return the\n        values grid. All input arguments to be passed by keyword.\n      dtype: The dtype to use.\n      name: The name to give to the ops.\n        Default value: None which means `solve_backward` is used.\n      **kwargs: Additional keyword args:\n        (1) pde_solver_fn: Function to solve the PDE that accepts all the above\n          arguments by name and returns the same tuple object as required below.\n          Defaults to `tff.math.pde.fd_solvers.solve_backward`.\n\n    Returns:\n      A tuple object containing at least the following attributes:\n        final_values_grid: A `Tensor` of same shape and dtype as `values_grid`.\n          Contains the final state of the values grid at time `end_time`.\n        final_coord_grid: A list of `Tensor`s of the same specification as\n          the input `coord_grid`. Final state of the coordinate grid at time\n          `end_time`.\n        step_count: The total step count (i.e. the sum of the `start_step_count`\n          and the number of steps performed in this call.).\n        final_time: The final time at which the evolution stopped. This value\n          is given by `max(min(end_time, start_time), 0)`.\n    """"""\n    pde_solver_fn = kwargs.get(""pde_solver_fn"", fd_solvers.solve_backward)\n\n    second_order_coeff_fn, first_order_coeff_fn, zeroth_order_coeff_fn = (\n        _backward_pde_coeffs(self._drift_fn, self._volatility_fn, discounting))\n\n    return pde_solver_fn(\n        start_time=start_time,\n        end_time=end_time,\n        coord_grid=coord_grid,\n        values_grid=values_grid,\n        num_steps=num_steps,\n        start_step_count=start_step_count,\n        time_step=time_step,\n        one_step_fn=one_step_fn,\n        boundary_conditions=boundary_conditions,\n        values_transform_fn=values_transform_fn,\n        second_order_coeff_fn=second_order_coeff_fn,\n        first_order_coeff_fn=first_order_coeff_fn,\n        zeroth_order_coeff_fn=zeroth_order_coeff_fn,\n        dtype=dtype,\n        name=name)\n\n  def fd_solver_forward(self,\n                        start_time,\n                        end_time,\n                        coord_grid,\n                        values_grid,\n                        one_step_fn=None,\n                        boundary_conditions=None,\n                        start_step_count=0,\n                        num_steps=None,\n                        time_step=None,\n                        values_transform_fn=None,\n                        dtype=None,\n                        name=None,\n                        **kwargs):\n    r""""""Returns a solver for the Fokker Plank equation of this process.\n\n    The Fokker Plank equation (also known as the Kolmogorov Forward equation)\n    associated to this Ito process is given by:\n\n    ```None\n      V_t + Sum[(mu_i(t, x) V)_i, 1<=i<=n]\n        - (1/2) Sum[ (D_{ij} V)_{ij}, 1 <= i,j <= n] = 0\n    ```\n\n    with the initial value condition $$V(0, x) = u(x)$$.\n\n    This method evolves a spatially discretized solution of the above PDE from\n    time `t0` to time `t1 > t0` (i.e. forwards in time).\n    The solution `V(t,x)` is assumed to be discretized on an `n`-dimensional\n    rectangular grid. A rectangular grid, G, in n-dimensions may be described\n    by specifying the coordinates of the points along each axis. For example,\n    a 2 x 4 grid in two dimensions can be specified by taking the cartesian\n    product of [1, 3] and [5, 6, 7, 8] to yield the grid points with\n    coordinates: `[(1, 5), (1, 6), (1, 7), (1, 8), (3, 5) ... (3, 8)]`.\n\n    Batching of solutions is supported. In this context, batching means\n    the ability to represent and evolve multiple independent functions `V`\n    (e.g. V1, V2 ...) simultaneously. A single discretized solution is specified\n    by stating its values at each grid point. This can be represented as a\n    `Tensor` of shape [d1, d2, ... dn] where di is the grid size along the `i`th\n    axis. A batch of such solutions is represented by a `Tensor` of shape:\n    [K, d1, d2, ... dn] where `K` is the batch size. This method only requires\n    that the input parameter `values_grid` be broadcastable with shape\n    [K, d1, ... dn].\n\n    The evolution of the solution from `t0` to `t1` is often done by\n    discretizing the differential equation to a difference equation along\n    the spatial and temporal axes. The temporal discretization is given by a\n    (sequence of) time steps [dt_1, dt_2, ... dt_k] such that the sum of the\n    time steps is equal to the total time step `t1 - t0`. If a uniform time\n    step is used, it may equivalently be specified by stating the number of\n    steps (n_steps) to take. This method provides both options via the\n    `time_step` and `num_steps` parameters. However, not all methods need\n    discretization along time direction (e.g. method of lines) so this argument\n    may not be applicable to some implementations.\n\n    The workhorse of this method is the `one_step_fn`. For the commonly used\n    methods, see functions in `math.pde.steppers` module.\n\n    The mapping between the arguments of this method and the above\n    equation are described in the Args section below.\n\n    For a simple instructive example of implementation of this method, see\n    `models.GenericItoProcess.fd_solver_forward`.\n\n    # TODO(b/142309558): Complete documentation.\n\n    Args:\n      start_time: Real positive scalar `Tensor`. The start time of the grid.\n        Corresponds to time `t0` above.\n      end_time: Real scalar `Tensor` smaller than the `start_time` and greater\n        than zero. The time to step back to. Corresponds to time `t1` above.\n      coord_grid: List of `n` rank 1 real `Tensor`s. `n` is the dimension of the\n        domain. The i-th `Tensor` has shape, `[d_i]` where `d_i` is the size of\n        the grid along axis `i`. The coordinates of the grid points. Corresponds\n        to the spatial grid `G` above.\n      values_grid: Real `Tensor` containing the function values at time\n        `start_time` which have to be stepped back to time `end_time`. The shape\n        of the `Tensor` must broadcast with `[K, d_1, d_2, ..., d_n]`. The first\n        axis of size `K` is the values batch dimension and allows multiple\n        functions (with potentially different boundary/final conditions) to be\n        stepped back simultaneously.\n      one_step_fn: The transition kernel. A callable that consumes the following\n        arguments by keyword:\n          1. \'time\': Current time\n          2. \'next_time\': The next time to step to. For the backwards in time\n            evolution, this time will be smaller than the current time.\n          3. \'coord_grid\': The coordinate grid.\n          4. \'values_grid\': The values grid.\n          5. \'quadratic_coeff\': A callable returning the quadratic coefficients\n            of the PDE (i.e. `(1/2)D_{ij}(t, x)` above). The callable accepts\n            the time and  coordinate grid as keyword arguments and returns a\n            `Tensor` with shape that broadcasts with `[dim, dim]`.\n          6. \'linear_coeff\': A callable returning the linear coefficients of the\n            PDE (i.e. `mu_i(t, x)` above). Accepts time and coordinate grid as\n            keyword arguments and returns a `Tensor` with shape that broadcasts\n            with `[dim]`.\n          7. \'constant_coeff\': A callable returning the coefficient of the\n            linear homogenous term (i.e. `r(t,x)` above). Same spec as above.\n            The `one_step_fn` callable returns a 2-tuple containing the next\n            coordinate grid, next values grid.\n      boundary_conditions: A list of size `dim` containing boundary conditions.\n        The i\'th element of the list is a 2-tuple containing the lower and upper\n        boundary condition for the boundary along the i`th axis.\n      start_step_count: Scalar integer `Tensor`. Initial value for the number of\n        time steps performed.\n        Default value: 0 (i.e. no previous steps performed).\n      num_steps: Positive int scalar `Tensor`. The number of time steps to take\n        when moving from `start_time` to `end_time`. Either this argument or the\n        `time_step` argument must be supplied (but not both). If num steps is\n        `k>=1`, uniform time steps of size `(t0 - t1)/k` are taken to evolve the\n        solution from `t0` to `t1`. Corresponds to the `n_steps` parameter\n        above.\n      time_step: The time step to take. Either this argument or the `num_steps`\n        argument must be supplied (but not both). The type of this argument may\n        be one of the following (in order of generality): (a) None in which case\n          `num_steps` must be supplied. (b) A positive real scalar `Tensor`. The\n          maximum time step to take. If the value of this argument is `dt`, then\n          the total number of steps taken is N = (t1 - t0) / dt rounded up to\n          the nearest integer. The first N-1 steps are of size dt and the last\n          step is of size `t1 - t0 - (N-1) * dt`. (c) A callable accepting the\n          current time and returning the size of the step to take. The input and\n          the output are real scalar `Tensor`s.\n      values_transform_fn: An optional callable applied to transform the\n        solution values at each time step. The callable is invoked after the\n        time step has been performed. The callable should accept the time of the\n        grid, the coordinate grid and the values grid and should return the\n        values grid. All input arguments to be passed by keyword.\n      dtype: The dtype to use.\n      name: The name to give to the ops.\n        Default value: None which means `solve_forward` is used.\n      **kwargs: Additional keyword args:\n        (1) pde_solver_fn: Function to solve the PDE that accepts all the above\n          arguments by name and returns the same tuple object as required below.\n          Defaults to `tff.math.pde.fd_solvers.solve_forward`.\n\n    Returns:\n      A tuple object containing at least the following attributes:\n        final_values_grid: A `Tensor` of same shape and dtype as `values_grid`.\n          Contains the final state of the values grid at time `end_time`.\n        final_coord_grid: A list of `Tensor`s of the same specification as\n          the input `coord_grid`. Final state of the coordinate grid at time\n          `end_time`.\n        step_count: The total step count (i.e. the sum of the `start_step_count`\n          and the number of steps performed in this call.).\n        final_time: The final time at which the evolution stopped. This value\n          is given by `max(min(end_time, start_time), 0)`.\n    """"""\n    pde_solver_fn = kwargs.get(""pde_solver_fn"", fd_solvers.solve_forward)\n\n    backward_second_order, backward_first_order, backward_zeroth_order = (\n        _backward_pde_coeffs(self._drift_fn, self._volatility_fn,\n                             discounting=None))\n\n    # Transform backward to forward equation.\n    inner_second_order_coeff_fn = lambda t, x: -backward_second_order(t, x)\n    inner_first_order_coeff_fn = backward_first_order\n    zeroth_order_coeff_fn = backward_zeroth_order\n\n    return pde_solver_fn(\n        start_time=start_time,\n        end_time=end_time,\n        coord_grid=coord_grid,\n        values_grid=values_grid,\n        num_steps=num_steps,\n        start_step_count=start_step_count,\n        time_step=time_step,\n        one_step_fn=one_step_fn,\n        boundary_conditions=boundary_conditions,\n        values_transform_fn=values_transform_fn,\n        inner_second_order_coeff_fn=inner_second_order_coeff_fn,\n        inner_first_order_coeff_fn=inner_first_order_coeff_fn,\n        zeroth_order_coeff_fn=zeroth_order_coeff_fn,\n        dtype=dtype,\n        name=name)\n\n\ndef _backward_pde_coeffs(drift_fn, volatility_fn, discounting):\n  """"""Returns coeffs of the backward PDE.""""""\n  def second_order_coeff_fn(t, coord_grid):\n    sigma = volatility_fn(t, _coord_grid_to_mesh_grid(coord_grid))\n    sigma_times_sigma_t = tf.linalg.matmul(sigma, sigma, transpose_b=True)\n\n    # We currently have [dim, dim] as innermost dimensions, but the returned\n    # tensor must have [dim, dim] as outermost dimensions.\n    rank = len(sigma.shape.as_list())\n    perm = [rank - 2, rank - 1] + list(range(rank - 2))\n    sigma_times_sigma_t = tf.transpose(sigma_times_sigma_t, perm)\n    return sigma_times_sigma_t / 2\n\n  def first_order_coeff_fn(t, coord_grid):\n    mu = drift_fn(t, _coord_grid_to_mesh_grid(coord_grid))\n\n    # We currently have [dim] as innermost dimension, but the returned\n    # tensor must have [dim] as outermost dimension.\n    rank = len(mu.shape.as_list())\n    perm = [rank - 1] + list(range(rank - 1))\n    mu = tf.transpose(mu, perm)\n    return mu\n\n  def zeroth_order_coeff_fn(t, coord_grid):\n    if not discounting:\n      return None\n    return -discounting(t, _coord_grid_to_mesh_grid(coord_grid))\n\n  return second_order_coeff_fn, first_order_coeff_fn, zeroth_order_coeff_fn\n\n\ndef _coord_grid_to_mesh_grid(coord_grid):\n  if len(coord_grid) == 1:\n    return tf.expand_dims(coord_grid[0], -1)\n  return tf.stack(values=tf.meshgrid(*coord_grid, indexing=""ij""), axis=-1)\n'"
tf_quant_finance/models/heston/__init__.py,0,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Heston model.""""""\n\nfrom tf_quant_finance.models.heston import approximations\nfrom tf_quant_finance.models.heston.heston_model import HestonModel\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'approximations\',\n    \'HestonModel\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/models/heston/heston_model.py,52,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Heston model with piecewise constant parameters.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math import piecewise\nfrom tf_quant_finance.math import random_ops as random\nfrom tf_quant_finance.models import generic_ito_process\nfrom tf_quant_finance.models import utils\n\n\n_SQRT_2 = np.sqrt(2., dtype=np.float64)\n\n\nclass HestonModel(generic_ito_process.GenericItoProcess):\n  """"""Heston Model with piecewise constant parameters.\n\n  Represents the Ito process:\n\n  ```None\n    dX(t) = -V(t) / 2 * dt + sqrt(V(t)) * dW_{X}(t),\n    dV(t) = kappa(t) * (theta(t) - V(t)) * dt\n            + epsilon(t) * sqrt(V(t)) * dW_{V}(t)\n  ```\n\n  where `W_{X}` and `W_{V}` are 1D Brownian motions with a correlation\n  `rho(t)`. `kappa`, `theta`, `epsilon`, and `rho` are positive piecewise\n  constant functions of time. Here `V(t)` represents the process variance at\n  time `t` and `X` represents logarithm of the spot price at time `t`.\n\n  `kappa` corresponds to the mean reversion rate, `theta` is the long run\n  price variance, and `epsilon` is the volatility of the volatility.\n\n  See [1] and [2] for details.\n\n  #### Example\n\n  ```python\n  import tf_quant_finance as tff\n  import numpy as np\n  epsilon = PiecewiseConstantFunc(\n      jump_locations=[0.5], values=[1, 1.1], dtype=np.float64)\n  process = HestonModel(kappa=0.5, theta=0.04, epsilon=epsilon, rho=0.1,\n                        dtype=np.float64)\n  times = np.linspace(0.0, 1.0, 1000)\n  num_samples = 10000  # number of trajectories\n  sample_paths = process.sample_paths(\n      times,\n      time_step=0.01,\n      num_samples=num_samples,\n      initial_state=np.array([1.0, 0.04]),\n      random_type=random.RandomType.SOBOL)\n  ```\n\n  #### References:\n    [1]: Cristian Homescu. Implied volatility surface: construction\n      methodologies and characteristics.\n      arXiv: https://arxiv.org/pdf/1107.1834.pdf\n    [2]: Leif Andersen. Efficient Simulation of the Heston Stochastic\n      Volatility Models. 2006.\n      Link:\n      http://www.ressources-actuarielles.net/ext/isfa/1226.nsf/d512ad5b22d73cc1c1257052003f1aed/1826b88b152e65a7c12574b000347c74/$FILE/LeifAndersenHeston.pdf\n  """"""\n\n  def __init__(self,\n               kappa,\n               theta,\n               epsilon,\n               rho,\n               dtype=None,\n               name=None):\n    """"""Initializes the Heston Model.\n\n    #### References:\n      [1]: Leif Andersen. Efficient Simulation of the Heston Stochastic\n        Volatility Models. 2006.\n        Link:\n        http://www.ressources-actuarielles.net/ext/isfa/1226.nsf/d512ad5b22d73cc1c1257052003f1aed/1826b88b152e65a7c12574b000347c74/$FILE/LeifAndersenHeston.pdf\n    Args:\n      kappa: Scalar real `Tensor` or an instant of batch-free left-continuous\n        `PiecewiseConstantFunc`. Should contain a positive value.\n        Corresponds to the mean reversion rate.\n      theta: Scalar real `Tensor` or an instant of batch-free left-continuous\n        `PiecewiseConstantFunc`. Should contain positive a value of the same\n        `dtype` as `kappa`.\n        Corresponds to the lond run price variance.\n      epsilon: Scalar real `Tensor` or an instant of batch-free left-continuous\n        `PiecewiseConstantFunc`. Should contain positive a value of the same\n        `dtype` as `kappa`.\n        Corresponds to the volatility of the volatility.\n      rho: Scalar real `Tensor` or an instant of batch-free left-continuous\n        `PiecewiseConstantFunc`. Should contain a value in range (-1, 1) of the\n        same `dtype` as `kappa`.\n        Corresponds to the correlation between dW_{X}` and `dW_{V}`.\n      dtype: The default dtype to use when converting values to `Tensor`s.\n        Default value: `None` which means that default dtypes inferred by\n          TensorFlow are used.\n      name: Python string. The name to give to the ops created by this class.\n        Default value: `None` which maps to the default name `heston_model`.\n    """"""\n    self._name = name or \'heston_model\'\n    with tf.compat.v1.name_scope(self._name,\n                                 values=[kappa, theta, epsilon, rho]):\n      self._dtype = dtype or None\n      self._kappa = kappa if isinstance(\n          kappa, piecewise.PiecewiseConstantFunc) else tf.convert_to_tensor(\n              kappa, dtype=self._dtype, name=\'kappa\')\n      self._theta = theta if isinstance(\n          theta, piecewise.PiecewiseConstantFunc) else tf.convert_to_tensor(\n              theta, dtype=self._dtype, name=\'theta\')\n      self._epsilon = epsilon if isinstance(\n          epsilon, piecewise.PiecewiseConstantFunc) else tf.convert_to_tensor(\n              epsilon, dtype=self._dtype, name=\'epsilon\')\n      self._rho = rho if isinstance(\n          rho, piecewise.PiecewiseConstantFunc) else tf.convert_to_tensor(\n              rho, dtype=self._dtype, name=\'rho\')\n\n    def _vol_fn(t, x):\n      """"""Volatility function of the Heston Process.""""""\n      # For correlated brownian motions W_{X} and W_{V} with correlation\n      # `rho(t)`, one can write\n      # W_{V}(t) = rho(t) * W_{X}(t) + sqrt(1 - rho(t)**2) * W_{Z}(t)\n      # where W_{Z}(t) is an independent from W_{X} and W{V} Brownian motion\n      # Volatility matrix for Heston model is then\n      # [[sqrt(V(t)), 0],\n      #  [epsilon(t) * rho(t) * sqrt(V(t)), epsilon(t) * sqrt(1-rho**2) * V(t)]]\n      vol = tf.sqrt(tf.abs(x[..., 1]))\n      zeros = tf.zeros_like(vol)\n      # Get parameter values at time `t`\n      rho, epsilon = _get_parameters([t], self._rho, self._epsilon)  # pylint: disable=unbalanced-tuple-unpacking\n      rho, epsilon = rho[0], epsilon[0]\n      # First column of the volatility matrix\n      vol_matrix_1 = tf.stack([vol, epsilon * rho * vol], -1)\n      # Second column of the volatility matrix\n      vol_matrix_2 = tf.stack([zeros, epsilon * tf.sqrt(1 - rho**2) * vol], -1)\n      vol_matrix = tf.stack([vol_matrix_1, vol_matrix_2], -1)\n      return vol_matrix\n\n    def _drift_fn(t, x):\n      var = x[..., 1]\n      # Get parameter values at time `t`\n      kappa, theta = _get_parameters([t], self._kappa, self._theta)  # pylint: disable=unbalanced-tuple-unpacking\n      kappa, theta = kappa[0], theta[0]\n      log_spot_drift = -var / 2\n      var_drift = kappa * (theta - var)\n      drift = tf.stack([log_spot_drift, var_drift], -1)\n      return drift\n\n    super(HestonModel, self).__init__(2, _drift_fn, _vol_fn, dtype, name)\n\n  def sample_paths(self,\n                   times,\n                   initial_state,\n                   num_samples=1,\n                   random_type=None,\n                   seed=None,\n                   time_step=None,\n                   skip=0,\n                   tolerance=1e-6,\n                   name=None):\n    """"""Returns a sample of paths from the process.\n\n    Using Quadratic-Exponential (QE) method described in [1] generates samples\n    paths started at time zero and returns paths values at the specified time\n    points.\n\n    Args:\n      times: Rank 1 `Tensor` of positive real values. The times at which the\n        path points are to be evaluated.\n      initial_state: A rank 1 `Tensor` with two elements where the first element\n        corresponds to the initial value of the log spot `X(0)` and the second\n        to the starting variance value `V(0)`.\n      num_samples: Positive scalar `int`. The number of paths to draw.\n      random_type: Enum value of `RandomType`. The type of (quasi)-random\n        number generator to use to generate the paths.\n        Default value: None which maps to the standard pseudo-random numbers.\n      seed: Seed for the random number generator. The seed is\n        only relevant if `random_type` is one of\n        `[STATELESS, PSEUDO, HALTON_RANDOMIZED, PSEUDO_ANTITHETIC,\n          STATELESS_ANTITHETIC]`. For `PSEUDO`, `PSEUDO_ANTITHETIC` and\n        `HALTON_RANDOMIZED` the seed should be an Python integer. For\n        `STATELESS` and  `STATELESS_ANTITHETIC `must be supplied as an integer\n        `Tensor` of shape `[2]`.\n        Default value: `None` which means no seed is set.\n      time_step: Positive Python float to denote time discretization parameter.\n      skip: `int32` 0-d `Tensor`. The number of initial points of the Sobol or\n        Halton sequence to skip. Used only when `random_type` is \'SOBOL\',\n        \'HALTON\', or \'HALTON_RANDOMIZED\', otherwise ignored.\n      tolerance: Scalar positive real `Tensor`. Specifies minimum time tolerance\n        for which the stochastic process `X(t) != X(t + tolerance)`.\n      Default value: 1e-6.\n      name: Str. The name to give this op.\n        Default value: `sample_paths`.\n\n    Returns:\n      A `Tensor`s of shape [num_samples, k, 2] where `k` is the size\n      of the `times`. For each sample and time the first dimension represents\n      the simulated log-state trajectories of the spot price `X(t)`, whereas the\n      second one represents the simulated variance trajectories `V(t)`.\n\n    Raises:\n      ValueError: If `time_step` is not supplied.\n\n    #### References:\n      [1]: Leif Andersen. Efficient Simulation of the Heston Stochastic\n        Volatility Models. 2006.\n    """"""\n    if random_type is None:\n      random_type = random.RandomType.PSEUDO\n    if time_step is None:\n      raise ValueError(\'`time_step` can not be `None` when calling \'\n                       \'sample_paths of HestonModel.\')\n    # Note: all the notations below are the same as in [1].\n    name = name or (self._name + \'_sample_path\')\n    with tf.name_scope(name):\n      time_step = tf.convert_to_tensor(time_step, self._dtype)\n      times = tf.convert_to_tensor(times, self._dtype)\n      current_log_spot = (\n          tf.convert_to_tensor(initial_state[..., 0], dtype=self._dtype)\n          + tf.zeros([num_samples], dtype=self._dtype))\n      current_vol = (\n          tf.convert_to_tensor(initial_state[..., 1], dtype=self._dtype)\n          + tf.zeros([num_samples], dtype=self._dtype))\n      num_requested_times = times.shape[0]\n      times, keep_mask = _prepare_grid(\n          times, time_step, times.dtype,\n          self._kappa, self._theta, self._epsilon, self._rho)\n      return self._sample_paths(\n          times, num_requested_times,\n          current_log_spot, current_vol,\n          num_samples, random_type, keep_mask, seed, skip, tolerance)\n\n  def _sample_paths(self,\n                    times,\n                    num_requested_times,\n                    current_log_spot,\n                    current_vol,\n                    num_samples,\n                    random_type,\n                    keep_mask,\n                    seed,\n                    skip,\n                    tolerance):\n    """"""Returns a sample of paths from the process.""""""\n    # Note: all the notations below are the same as in [1].\n    dt = times[1:] - times[:-1]\n    # Compute the parameters at `times`. Here + tf.reduce_min(dt) / 2 ensures\n    # that the value is constant between `times`.\n    kappa, theta, epsilon, rho = _get_parameters(  # pylint: disable=unbalanced-tuple-unpacking\n        times + tf.reduce_min(dt) / 2,\n        self._kappa, self._theta, self._epsilon, self._rho)\n    # In order random_type which is not PSEUDO,  sequence of independent random\n    # normals should be generated upfront.\n    if dt.shape.is_fully_defined():\n      steps_num = dt.shape.as_list()[-1]\n    else:\n      steps_num = tf.shape(dt)[-1]\n      # TODO(b/148133811): Re-enable Sobol test when TF 2.2 is released.\n      if random_type == random.RandomType.SOBOL:\n        raise ValueError(\'Sobol sequence for Euler sampling is temporarily \'\n                         \'unsupported when `time_step` or `times` have a \'\n                         \'non-constant value\')\n    if random_type != random.RandomType.PSEUDO:\n      # Note that at each iteration we need 3 random draws.\n      normal_draws = utils.generate_mc_normal_draws(\n          num_normal_draws=3, num_time_steps=steps_num,\n          num_sample_paths=num_samples, random_type=random_type,\n          seed=seed,\n          dtype=self.dtype(), skip=skip)\n    else:\n      normal_draws = None\n    cond_fn = lambda i, *args: i < steps_num\n    def body_fn(i, written_count, current_vol, current_log_spot, vol_paths,\n                log_spot_paths):\n      """"""Simulate Heston process to the next time point.""""""\n      time_step = dt[i]\n      if normal_draws is None:\n        normals = random.mv_normal_sample(\n            (num_samples,),\n            mean=tf.zeros([3], dtype=kappa.dtype), seed=seed)\n      else:\n        normals = normal_draws[i]\n      def _next_vol_fn():\n        return _update_variance(\n            kappa[i], theta[i], epsilon[i], rho[i],\n            current_vol, time_step, normals[..., :2])\n      # Do not update variance if `time_step > tolerance`\n      next_vol = tf.cond(time_step > tolerance,\n                         _next_vol_fn,\n                         lambda: current_vol)\n      def _next_log_spot_fn():\n        return _update_log_spot(\n            kappa[i], theta[i], epsilon[i], rho[i],\n            current_vol, next_vol, current_log_spot, time_step,\n            normals[..., -1])\n      # Do not update state if `time_step > tolerance`\n      next_log_spot = tf.cond(time_step > tolerance,\n                              _next_log_spot_fn,\n                              lambda: current_log_spot)\n      # Update volatility paths\n      vol_paths = utils.maybe_update_along_axis(\n          tensor=vol_paths,\n          do_update=keep_mask[i + 1],\n          ind=written_count,\n          axis=1,\n          new_tensor=tf.expand_dims(next_vol, axis=1))\n      # Update log-spot paths\n      log_spot_paths = utils.maybe_update_along_axis(\n          tensor=log_spot_paths,\n          do_update=keep_mask[i + 1],\n          ind=written_count,\n          axis=1,\n          new_tensor=tf.expand_dims(next_log_spot, axis=1))\n      written_count += tf.cast(keep_mask[i + 1], dtype=tf.int32)\n      return (i + 1, written_count,\n              next_vol, next_log_spot, vol_paths, log_spot_paths)\n\n    shape = (num_samples, num_requested_times)\n    log_spot_paths = tf.zeros(shape, dtype=self._dtype)\n    vol_paths = tf.zeros(shape, dtype=self._dtype)\n    _, _, _, _, vol_paths, log_spot_paths = tf.while_loop(\n        cond_fn, body_fn, (0, 0, current_vol, current_log_spot,\n                           vol_paths, log_spot_paths),\n        maximum_iterations=steps_num)\n    return tf.stack([log_spot_paths, vol_paths], -1)\n\n\ndef _get_parameters(times, *params):\n  """"""Gets parameter values at at specified `times`.""""""\n  result = []\n  for param in params:\n    if isinstance(param, piecewise.PiecewiseConstantFunc):\n      result.append(param(times))\n    else:\n      result.append(param * tf.ones_like(times))\n  return result\n\n\ndef _update_variance(\n    kappa, theta, epsilon, rho,\n    current_vol, time_step, normals, psi_c=1.5):\n  """"""Updates variance value.""""""\n  del rho\n  psi_c = tf.convert_to_tensor(psi_c, dtype=kappa.dtype)\n  scaled_time = tf.exp(-kappa * time_step)\n  epsilon_squared = epsilon**2\n  m = theta + (current_vol - theta) * scaled_time\n  s_squared = (\n      current_vol * epsilon_squared * scaled_time / kappa\n      * (1 - scaled_time) + theta * epsilon_squared / 2 / kappa\n      * (1 - scaled_time)**2)\n  psi = s_squared / m**2\n  uniforms = 0.5 * (1 + tf.math.erf(normals[..., 0] / _SQRT_2))\n  cond = psi < psi_c\n  # Result where `cond` is true\n  psi_inv = 2 / psi\n  b_squared = psi_inv - 1 + tf.sqrt(psi_inv * (psi_inv - 1))\n\n  a = m / (1 + b_squared)\n  next_var_true = a * (tf.sqrt(b_squared) + tf.squeeze(normals[..., 1]))**2\n  # Result where `cond` is false\n  p = (psi - 1) / (psi + 1)\n  beta = (1 - p) / m\n  next_var_false = tf.where(uniforms > p,\n                            tf.math.log(1 - p) - tf.math.log(1 - uniforms),\n                            tf.zeros_like(uniforms)) / beta\n  next_var = tf.where(cond, next_var_true, next_var_false)\n  return next_var\n\n\ndef _update_log_spot(\n    kappa, theta, epsilon, rho,\n    current_vol, next_vol, current_log_spot, time_step, normals,\n    gamma_1=0.5, gamma_2=0.5):\n  """"""Updates log-spot value.""""""\n  k_0 = - rho * kappa * theta / epsilon * time_step\n  k_1 = (gamma_1 * time_step\n         * (kappa * rho / epsilon - 0.5)\n         - rho / epsilon)\n  k_2 = (gamma_2 * time_step\n         * (kappa * rho / epsilon - 0.5)\n         + rho / epsilon)\n  k_3 = gamma_1 * time_step * (1 - rho**2)\n  k_4 = gamma_2 * time_step * (1 - rho**2)\n\n  next_log_spot = (\n      current_log_spot + k_0 + k_1 * current_vol + k_2 * next_vol\n      + tf.sqrt(k_3 * current_vol + k_4 * next_vol) * normals)\n  return next_log_spot\n\n\ndef _prepare_grid(times, time_step, dtype, *params):\n  """"""Prepares grid of times for path generation.\n\n  Args:\n    times:  Rank 1 `Tensor` of increasing positive real values. The times at\n      which the path points are to be evaluated.\n    time_step: Rank 0 real `Tensor`. Maximal distance between points in\n      resulting grid.\n    dtype: `tf.Dtype` of the input and output `Tensor`s.\n    *params: Parameters of the Heston model. Either scalar `Tensor`s of the\n      same `dtype` or instances of `PiecewiseConstantFunc`.\n\n  Returns:\n    Tuple `(all_times, mask)`.\n    `all_times` is a 1-D real `Tensor` containing all points from \'times`, the\n    uniform grid of points between `[0, times[-1]]` with grid size equal to\n    `time_step`, and jump locations of piecewise constant parameters The\n    `Tensor` is sorted in ascending order and may contain duplicates.\n    `mask` is a boolean 1-D `Tensor` of the same shape as \'all_times\', showing\n    which elements of \'all_times\' correspond to THE values from `times`.\n    Guarantees that times[0]=0 and mask[0]=False.\n  """"""\n  grid = tf.range(0.0, times[-1], time_step, dtype=dtype)\n  additional_times = []\n  for param in params:\n    if isinstance(param, piecewise.PiecewiseConstantFunc):\n      additional_times.append(param.jump_locations())\n  all_times = tf.concat([grid, times] + additional_times, axis=0)\n  additional_times_mask = [\n      tf.zeros_like(times, dtype=tf.bool) for times in additional_times]\n  mask = tf.concat([\n      tf.zeros_like(grid, dtype=tf.bool),\n      tf.ones_like(times, dtype=tf.bool)\n  ] + additional_times_mask, axis=0)\n  perm = tf.argsort(all_times, stable=True)\n  all_times = tf.gather(all_times, perm)\n  mask = tf.gather(mask, perm)\n  return all_times, mask\n'"
tf_quant_finance/models/heston/heston_model_test.py,7,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for Heston Model.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\nHestonModel = tff.models.HestonModel\ngrids = tff.math.pde.grids\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass HestonModelTest(tf.test.TestCase):\n\n  def test_volatility(self):\n    """"""Tests volatility stays close to its mean for small vol of vol.""""""\n    theta = 0.05\n    process = HestonModel(\n        kappa=1.0, theta=theta, epsilon=0.00001,\n        rho=-0.0, dtype=np.float64)\n    years = 1.0\n    times = np.linspace(0.0, years, int(365 * years))\n    num_samples = 2\n    paths = process.sample_paths(\n        times,\n        time_step=0.01,\n        num_samples=num_samples,\n        initial_state=np.array([np.log(100), 0.045]),\n        seed=None)\n    # For small values of epsilon, volatility should stay close to theta\n    volatility_trace = self.evaluate(paths)[..., 1]\n    max_deviation = np.max(abs(volatility_trace[:, 50:] - theta))\n    self.assertAlmostEqual(\n        max_deviation, 0.0, places=2)\n\n  def test_state(self):\n    """"""Tests state behaves like GBM for small vol of vol.""""""\n    theta = 1.0\n    process = HestonModel(\n        kappa=1.0, theta=theta, epsilon=0.00001,\n        rho=-0.0, dtype=np.float64)\n    times = [0.0, 0.5, 1.0]\n    num_samples = 1000\n    start_value = 100\n    paths = process.sample_paths(\n        times,\n        time_step=0.001,\n        num_samples=num_samples,\n        initial_state=np.array([np.log(start_value), 1.0]),\n        seed=None)\n    # For small values of epsilon, state should behave like Geometric\n    # Brownian Motion with volatility `theta`.\n    state_trace = self.evaluate(paths)[..., 0]\n    # Starting point should be the same\n    np.testing.assert_allclose(state_trace[:, 0], np.log(100), 1e-8)\n    for i in (1, 2):\n      # Mean and variance of the approximating Geometric Brownian Motions\n      gbm_mean = start_value\n      gbm_std = start_value * np.sqrt((np.exp(times[i]) - 1))\n      np.testing.assert_allclose(np.mean(np.exp(state_trace[:, i])),\n                                 gbm_mean, 1.0)\n      np.testing.assert_allclose(np.std(np.exp(state_trace[:, 1])),\n                                 gbm_std, 2.0)\n\n  def test_piecewise_and_dtype(self):\n    """"""Tests that piecewise constant coefficients can be handled.""""""\n    for dtype in (np.float32, np.float64):\n      kappa = tff.math.piecewise.PiecewiseConstantFunc(\n          jump_locations=[0.5], values=[1, 1.1], dtype=dtype)\n      theta = tff.math.piecewise.PiecewiseConstantFunc(\n          jump_locations=[0.5], values=[1, 0.9], dtype=dtype)\n      epsilon = tff.math.piecewise.PiecewiseConstantFunc(\n          jump_locations=[0.3], values=[0.1, 0.2], dtype=dtype)\n      rho = tff.math.piecewise.PiecewiseConstantFunc(\n          jump_locations=[0.5], values=[0.4, 0.6], dtype=dtype)\n      process = HestonModel(\n          kappa=kappa, theta=theta, epsilon=epsilon,\n          rho=rho, dtype=dtype)\n      times = [0.1, 1.0]\n      num_samples = 100\n      initial_state = np.array([np.log(100), 0.045], dtype=dtype)\n      paths = process.sample_paths(\n          times,\n          time_step=0.1,\n          num_samples=num_samples,\n          initial_state=initial_state,\n          seed=None)\n      paths = self.evaluate(paths)\n      state_trace, volatility_trace = paths[..., 0], paths[..., 0]\n      self.assertEqual(volatility_trace.dtype, dtype)\n      self.assertEqual(state_trace.dtype, dtype)\n      # Check drift and volatility calculation\n      self.assertAllClose(\n          process.drift_fn()(times[0], initial_state),\n          np.array([-0.0225, 0.955]))\n      self.assertAllClose(\n          process.volatility_fn()(times[0], initial_state),\n          np.array([[0.21213203, 0.],\n                    [0.00848528, 0.01944222]]))\n\n  def test_compare_monte_carlo_to_backward_pde(self):\n    dtype = tf.float64\n    kappa = 0.3\n    theta = 0.05\n    epsilon = 0.02\n    rho = 0.1\n    maturity_time = 1.0\n    initial_log_spot = 3.0\n    initial_vol = 0.05\n    strike = 15\n    discounting = 0.5\n\n    heston = HestonModel(\n        kappa=kappa, theta=theta, epsilon=epsilon, rho=rho, dtype=dtype)\n    initial_state = np.array([initial_log_spot, initial_vol])\n    samples = heston.sample_paths(\n        times=[maturity_time / 2, maturity_time],\n        initial_state=initial_state,\n        time_step=0.01,\n        num_samples=1000,\n        random_type=tff.math.random.RandomType.PSEUDO_ANTITHETIC,\n        seed=42)\n    self.assertEqual(samples.shape, [1000, 2, 2])\n    log_spots = samples[:, -1, 0]\n    monte_carlo_price = (\n        tf.constant(np.exp(-discounting * maturity_time), dtype=dtype) *\n        tf.math.reduce_mean(tf.nn.relu(tf.math.exp(log_spots) - strike)))\n\n    s_min, s_max = 2, 4\n    v_min, v_max = 0.03, 0.07\n    grid_size_s, grid_size_v = 101, 101\n    time_step = 0.01\n\n    grid = grids.uniform_grid(minimums=[s_min, v_min],\n                              maximums=[s_max, v_max],\n                              sizes=[grid_size_s, grid_size_v],\n                              dtype=dtype)\n\n    s_mesh, _ = tf.meshgrid(grid[0], grid[1], indexing=""ij"")\n    final_value_grid = tf.nn.relu(tf.math.exp(s_mesh) - strike)\n    value_grid = heston.fd_solver_backward(\n        start_time=1.0,\n        end_time=0.0,\n        coord_grid=grid,\n        values_grid=final_value_grid,\n        time_step=time_step,\n        discounting=lambda *args: discounting)[0]\n    pde_price = value_grid[int(grid_size_s / 2), int(grid_size_v / 2)]\n\n    self.assertAllClose(monte_carlo_price, pde_price, atol=0.1, rtol=0.1)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/models/hull_white/__init__.py,0,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""TensorFlow Quantitative Finance tools to build Hull White type models.""""""\n\nfrom tf_quant_finance.models.hull_white.one_factor import HullWhiteModel1F\nfrom tf_quant_finance.models.hull_white.vector_hull_white import VectorHullWhiteModel\nfrom tf_quant_finance.models.hull_white.zero_coupon_bond_option import bond_option_price\n\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'HullWhiteModel1F\',\n    \'VectorHullWhiteModel\',\n    \'bond_option_price\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/models/hull_white/hull_white_test.py,21,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for Hull White Module.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n# @test_util.run_all_in_graph_and_eager_modes\nclass HullWhiteTest(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    self.mean_reversion = [0.1, 0.05]\n    self.volatility = [0.01, 0.02]\n    self.volatility_time_dep_1d = [0.01, 0.02, 0.01]\n    self.instant_forward_rate_1d_fn = lambda *args: [0.01]\n    self.instant_forward_rate_2d_fn = lambda *args: [0.01, 0.01]\n    self.initial_state = [0.01, 0.01]\n    # See D. Brigo, F. Mercurio. Interest Rate Models. 2007.\n    def _true_mean(t):\n      dtype = np.float64\n      a = dtype(self.mean_reversion)\n      sigma = dtype(self.volatility)\n      initial_state = dtype(self.initial_state)\n      return (dtype(self.instant_forward_rate_2d_fn(t))\n              + (sigma * sigma / 2 / a**2)\n              * (1.0 - np.exp(-a * t))**2\n              - self.instant_forward_rate_2d_fn(0) * np.exp(-a * t)\n              + initial_state *  np.exp(-a * t))\n    self.true_mean = _true_mean\n    def _true_var(t):\n      dtype = np.float64\n      a = dtype(self.mean_reversion)\n      sigma = dtype(self.volatility)\n      return (sigma * sigma / 2 / a) * (1.0 - np.exp(-2 * a * t))\n    self.true_var = _true_var\n\n    def _true_std_time_dep(t, intervals, vol, k):\n      res = np.zeros_like(t, dtype=np.float64)\n      for i, tt in enumerate(t):\n        var = 0.0\n        for j in range(len(intervals) - 1):\n          if tt >= intervals[j] and tt < intervals[j + 1]:\n            var = var + vol[j]**2 / 2 / k * (\n                np.exp(2 * k * tt) - np.exp(2 * k * intervals[j]))\n            break\n          else:\n            var = var + vol[j]**2 / 2 / k * (\n                np.exp(2 * k * intervals[j + 1]) - np.exp(2 * k * intervals[j]))\n        else:\n          var = var + vol[-1]**2/2/k *(np.exp(2*k*tt)-np.exp(2*k*intervals[-1]))\n        res[i] = np.exp(-k*tt) * np.sqrt(var)\n\n      return res\n    self.true_std_time_dep = _true_std_time_dep\n\n    def _true_zcb_std(t, tau, v, k):\n      e_tau = np.exp(-k*tau)\n      et = np.exp(k*t)\n      val = v/k * (1. - e_tau*et) * np.sqrt((1.-1./et/et)/k/2)\n      return val\n    self.true_zcb_std = _true_zcb_std\n\n    super(HullWhiteTest, self).setUp()\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'STATELESS\',\n          \'random_type\': tff.math.random.RandomType.STATELESS_ANTITHETIC,\n          \'seed\': [1, 2],\n      }, {\n          \'testcase_name\': \'HALTON\',\n          \'random_type\': tff.math.random.RandomType.HALTON,\n          \'seed\': None,\n      })\n  def test_mean_and_variance_1d(self, random_type, seed):\n    """"""Tests model with piecewise constant parameters in 1 dimension.""""""\n    for dtype in [tf.float32, tf.float64]:\n      # exact discretization is not supported for time-dependent specification\n      # of mean reversion rate.\n      mean_reversion = tff.math.piecewise.PiecewiseConstantFunc(\n          [], values=[self.mean_reversion[0]], dtype=dtype)\n      volatility = tff.math.piecewise.PiecewiseConstantFunc(\n          [0.1, 2.0], values=3 * [self.volatility[0]], dtype=dtype)\n      process = tff.models.hull_white.HullWhiteModel1F(\n          mean_reversion=mean_reversion,\n          volatility=volatility,\n          initial_discount_rate_fn=self.instant_forward_rate_1d_fn,\n          dtype=dtype)\n      paths = process.sample_paths(\n          [0.1, 0.5, 1.0],\n          num_samples=50000,\n          random_type=random_type,\n          seed=seed,\n          skip=1000000)\n      self.assertEqual(paths.dtype, dtype)\n      self.assertAllEqual(paths.shape, [50000, 3, 1])\n      paths = self.evaluate(paths)\n      paths = paths[:, -1, :]  # Extract paths values for the terminal time\n      mean = np.mean(paths, axis=0)\n      variance = np.var(paths, axis=0)\n      self.assertAllClose(mean, [self.true_mean(1.0)[0]], rtol=1e-4, atol=1e-4)\n      self.assertAllClose(variance,\n                          [self.true_var(1.0)[0]], rtol=1e-4, atol=1e-4)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'STATELESS\',\n          \'random_type\': tff.math.random.RandomType.STATELESS_ANTITHETIC,\n          \'seed\': [1, 2],\n      }, {\n          \'testcase_name\': \'HALTON\',\n          \'random_type\': tff.math.random.RandomType.HALTON,\n          \'seed\': None,\n      })\n  def test_variance_zcb_1d(self, random_type, seed):\n    """"""Tests discount bond variance in 1 dimension.""""""\n    for dtype in [tf.float64, tf.float64]:\n      def discount_fn(x):\n        return 0.01 * tf.ones_like(x, dtype=dtype)  # pylint: disable=cell-var-from-loop\n      mr = 0.03\n      vol = 0.015\n      mean_reversion = tff.math.piecewise.PiecewiseConstantFunc(\n          [], values=[mr], dtype=dtype)\n      volatility = tff.math.piecewise.PiecewiseConstantFunc(\n          [0.1, 2.0], values=3 * [vol], dtype=dtype)\n      process = tff.models.hull_white.HullWhiteModel1F(\n          mean_reversion=mean_reversion,\n          volatility=volatility,\n          initial_discount_rate_fn=discount_fn,\n          dtype=dtype)\n      curve_times = np.array([0.0, 1.0, 2.0, 3.0])\n      paths, _ = process.sample_discount_curve_paths(\n          [0.1, 1.0, 2.0],\n          curve_times,\n          num_samples=500000,\n          random_type=random_type,\n          seed=seed,\n          skip=1000000)\n      self.assertEqual(paths.dtype, dtype)\n      self.assertAllEqual(paths.shape, [500000, 4, 3, 1])\n      paths = self.evaluate(tf.math.log(paths))\n      std_zcb = np.std(paths, axis=0)[:, 2, 0]\n      expected_std = self.true_zcb_std(2.0, 2.0 + curve_times, vol, mr)\n      self.assertAllClose(std_zcb, expected_std, rtol=1e-4, atol=1e-4)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'STATELESS\',\n          \'random_type\': tff.math.random.RandomType.STATELESS_ANTITHETIC,\n          \'seed\': [1, 2],\n      }, {\n          \'testcase_name\': \'HALTON\',\n          \'random_type\': tff.math.random.RandomType.HALTON,\n          \'seed\': None,\n      })\n  def test_time_dependent_1d(self, random_type, seed):\n    """"""Tests model with time dependent vol in 1 dimension.""""""\n    for dtype in [tf.float32, tf.float64]:\n      def discount_fn(x):\n        return 0.01 * tf.ones_like(x, dtype=dtype)  # pylint: disable=cell-var-from-loop\n      mean_reversion = tff.math.piecewise.PiecewiseConstantFunc(\n          [], values=[self.mean_reversion[0]], dtype=dtype)\n      volatility = tff.math.piecewise.PiecewiseConstantFunc(\n          [0.1, 2.0], values=self.volatility_time_dep_1d, dtype=dtype)\n      process = tff.models.hull_white.HullWhiteModel1F(\n          mean_reversion=mean_reversion,\n          volatility=volatility,\n          initial_discount_rate_fn=discount_fn,\n          dtype=dtype)\n      times = np.array([0.1, 1.0, 2.0, 3.0])\n      paths = process.sample_paths(\n          times,\n          num_samples=500000,\n          random_type=random_type,\n          seed=seed,\n          skip=1000000)\n      self.assertEqual(paths.dtype, dtype)\n      self.assertAllEqual(paths.shape, [500000, 4, 1])\n      paths = self.evaluate(paths)\n      r_std = np.squeeze(np.std(paths, axis=0))\n      expected_std = self.true_std_time_dep(\n          times, np.array([0.0, 0.1, 2.0]),\n          np.array(self.volatility_time_dep_1d), 0.1)\n      self.assertAllClose(r_std, expected_std, rtol=1e-4, atol=1e-4)\n\n  def test_mean_variance_correlation_piecewise_2d(self):\n    """"""Tests model with piecewise constant parameters in 2 dimensions.""""""\n    for dtype in [tf.float32, tf.float64]:\n      # Mean reversion without batch dimesnion\n      mean_reversion = self.mean_reversion\n      # Volatility with batch dimesnion\n      volatility = tff.math.piecewise.PiecewiseConstantFunc(\n          [[0.1, 0.2, 0.5], [0.1, 2.0, 3.0]],\n          values=[4 * [self.volatility[0]],\n                  4 * [self.volatility[1]]], dtype=dtype)\n      expected_corr_matrix = [[1., 0.5], [0.5, 1.]]\n      corr_matrix = tff.math.piecewise.PiecewiseConstantFunc(\n          [0.5, 1.0], values=3 * [expected_corr_matrix], dtype=dtype)\n      process = tff.models.hull_white.VectorHullWhiteModel(\n          dim=2,\n          mean_reversion=mean_reversion,\n          volatility=volatility,\n          corr_matrix=corr_matrix,\n          initial_discount_rate_fn=self.instant_forward_rate_2d_fn,\n          dtype=dtype)\n      paths = process.sample_paths(\n          [0.1, 0.5, 1.0],\n          num_samples=50000,\n          random_type=tff.math.random.RandomType.SOBOL,\n          skip=1000000)\n      self.assertEqual(paths.dtype, dtype)\n      self.assertAllEqual(paths.shape, [50000, 3, 2])\n      paths = self.evaluate(paths)\n      paths = paths[:, -1, :]  # Extract paths values for the terminal time\n      mean = np.mean(paths, axis=0)\n      estimated_corr_matrix = np.corrcoef(paths[:, 0], paths[:, 1])\n      variance = np.var(paths, axis=0)\n      self.assertAllClose(mean, self.true_mean(1.0), rtol=1e-4, atol=1e-4)\n      self.assertAllClose(variance,\n                          self.true_var(1.0), rtol=1e-4, atol=1e-4)\n      self.assertAllClose(estimated_corr_matrix, expected_corr_matrix,\n                          rtol=1e-4, atol=1e-4)\n\n  def test_mean_variance_correlation_constant_piecewise_2d(self):\n    """"""Tests model with piecewise constant or constant parameters in 2.""""""\n    for dtype in [tf.float32, tf.float64]:\n      tf.random.set_seed(10)  # Fix global random seed\n      mean_reversion = self.mean_reversion\n      volatility = tff.math.piecewise.PiecewiseConstantFunc(\n          2 * [[0.2, 1.0]], values=np.array(3 * [self.volatility]).transpose(),\n          dtype=dtype)\n      expected_corr_matrix = [[1., 0.5], [0.5, 1.]]\n      corr_matrix = tff.math.piecewise.PiecewiseConstantFunc(\n          [0.5, 2.0], values=[expected_corr_matrix,\n                              [[1., 0.6], [0.6, 1.]],\n                              [[1., 0.9], [0.9, 1.]]], dtype=dtype)\n      process = tff.models.hull_white.VectorHullWhiteModel(\n          dim=2,\n          mean_reversion=mean_reversion,\n          volatility=volatility,\n          corr_matrix=corr_matrix,\n          initial_discount_rate_fn=self.instant_forward_rate_2d_fn,\n          dtype=dtype)\n      paths = process.sample_paths(\n          [0.1, 0.5, 1.0],\n          num_samples=500000,\n          random_type=tff.math.random.RandomType.PSEUDO_ANTITHETIC,\n          seed=42)\n      self.assertEqual(paths.dtype, dtype)\n      self.assertAllEqual(paths.shape, [500000, 3, 2])\n      paths = self.evaluate(paths)\n      estimated_corr_matrix = np.corrcoef(paths[:, 1, 0], paths[:, 1, 1])\n      paths = paths[:, -1, :]  # Extract paths values for the terminal time\n      mean = np.mean(paths, axis=0)\n      variance = np.var(paths, axis=0)\n      self.assertAllClose(mean, self.true_mean(1.0), rtol=1e-3, atol=1e-3)\n      self.assertAllClose(variance,\n                          self.true_var(1.0), rtol=1e-3, atol=1e-3)\n      self.assertAllClose(estimated_corr_matrix, expected_corr_matrix,\n                          rtol=1e-2, atol=1e-2)\n\n  def test_mean_variance_correlation_generic_2d(self):\n    """"""Tests model with generic parameters in 2 dimensions.""""""\n    for dtype in [tf.float32, tf.float64]:\n      # Mean reversion without batch dimesnion\n      mean_reversion = tff.math.piecewise.PiecewiseConstantFunc(\n          [0.1, 2.0], values=3 * [self.mean_reversion], dtype=dtype)\n      # Volatility with batch dimesnion\n      volatility = tff.math.piecewise.PiecewiseConstantFunc(\n          [[0.1, 0.2, 0.5], [0.1, 2.0, 3.0]],\n          values=[4 * [self.volatility[0]],\n                  4 * [self.volatility[1]]], dtype=dtype)\n      def corr_matrix(t):\n        one = tf.ones_like(t)\n        row1 = tf.stack([one, 0.5 * t], axis=-1)\n        row2 = tf.reverse(row1, [0])\n        corr_matrix = tf.stack([row1, row2], axis=-1)\n        return corr_matrix\n      process = tff.models.hull_white.VectorHullWhiteModel(\n          dim=2,\n          mean_reversion=mean_reversion,\n          volatility=volatility,\n          corr_matrix=corr_matrix,\n          initial_discount_rate_fn=self.instant_forward_rate_2d_fn,\n          dtype=dtype)\n      times = [0.1, 0.5]\n      paths = process.sample_paths(\n          times,\n          num_samples=50000,\n          random_type=tff.math.random.RandomType.SOBOL,\n          skip=100000,\n          time_step=0.01)\n      self.assertEqual(paths.dtype, dtype)\n      self.assertAllEqual(paths.shape, [50000, 2, 2])\n      paths = self.evaluate(paths)\n      paths = paths[:, -1, :]  # Extract paths values for the terminal time\n      mean = np.mean(paths, axis=0)\n      variance = np.var(paths, axis=0)\n      self.assertAllClose(mean, self.true_mean(times[-1]), rtol=1e-3, atol=1e-3)\n      self.assertAllClose(variance,\n                          self.true_var(times[-1]), rtol=1e-3, atol=1e-3)\n\n  def test_invalid_batch_size_piecewise(self):\n    """"""Tests that the batch dimension should be [2] if it is not empty.""""""\n    dtype = tf.float64\n    # Batch shape is [1]. Should be [] or [2]\n    mean_reversion = tff.math.piecewise.PiecewiseConstantFunc(\n        [[0.1, 2.0]], values=[3 * [self.mean_reversion]], dtype=dtype)\n    # Volatility with batch dimesnion\n    volatility = self.volatility\n    with self.assertRaises(ValueError):\n      tff.models.hull_white.VectorHullWhiteModel(\n          dim=2,\n          mean_reversion=mean_reversion,\n          volatility=volatility,\n          corr_matrix=None,\n          initial_discount_rate_fn=self.instant_forward_rate_2d_fn,\n          dtype=dtype)\n\n  def test_invalid_batch_rank_piecewise(self):\n    """"""Tests that the batch rank should be 1 if it is not empty.""""""\n    dtype = tf.float64\n    # Batch rank is 2\n    mean_reversion = tff.math.piecewise.PiecewiseConstantFunc(\n        [[[0.1, 2.0]]], values=[[3 * [self.mean_reversion]]], dtype=dtype)\n    # Volatility with batch dimesnion\n    volatility = self.volatility\n    with self.assertRaises(ValueError):\n      tff.models.hull_white.VectorHullWhiteModel(\n          dim=2,\n          mean_reversion=mean_reversion,\n          volatility=volatility,\n          corr_matrix=None,\n          initial_discount_rate_fn=self.instant_forward_rate_2d_fn,\n          dtype=dtype)\n\n  def test_time_step_not_supplied(self):\n    """"""Tests that the `time_step` should be supplied if Euler scheme is used.""""""\n    dtype = tf.float64\n    def volatility_fn(t):\n      del t\n      return self.volatility\n    process = tff.models.hull_white.VectorHullWhiteModel(\n        dim=2,\n        mean_reversion=self.mean_reversion,\n        volatility=volatility_fn,\n        initial_discount_rate_fn=self.instant_forward_rate_2d_fn,\n        dtype=dtype)\n    with self.assertRaises(ValueError):\n      process.sample_paths(\n          [0.1, 2.0],\n          num_samples=100)\n\n  def test_times_wrong_rank(self):\n    """"""Tests that the `times` should be a rank 1 `Tensor`.""""""\n    dtype = tf.float64\n    process = tff.models.hull_white.VectorHullWhiteModel(\n        dim=2,\n        mean_reversion=self.mean_reversion,\n        volatility=self.volatility,\n        initial_discount_rate_fn=self.instant_forward_rate_2d_fn,\n        dtype=dtype)\n    with self.assertRaises(ValueError):\n      process.sample_paths(\n          [[0.1, 2.0]],\n          num_samples=100)\n\n  def test_time_dependent_mr(self):\n    """"""Tests that time depemdent mr uses generic sampling.""""""\n    dtype = tf.float64\n    mean_reversion = tff.math.piecewise.PiecewiseConstantFunc(\n        [1.0], values=[0.03, 0.04], dtype=dtype)\n\n    process = tff.models.hull_white.VectorHullWhiteModel(\n        dim=1,\n        mean_reversion=mean_reversion,\n        volatility=[0.015],\n        corr_matrix=None,\n        initial_discount_rate_fn=self.instant_forward_rate_1d_fn,\n        dtype=dtype)\n    self.assertTrue(process._sample_with_generic)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/models/hull_white/one_factor.py,2,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""One factor Hull-White model with time-dependent parameters.""""""\n\nfrom tf_quant_finance.models.hull_white import vector_hull_white\n\n\nclass HullWhiteModel1F(vector_hull_white.VectorHullWhiteModel):\n  r""""""One Factor Hull-White Model.\n\n  Represents the Ito process:\n\n  ```None\n    dr(t) = (theta(t) - a(t) * r(t)) dt + sigma(t) * dW_{r}(t)\n  ```\n  where `W_{r}` is a 1D Brownian motion.\n  `theta`, `a`, `sigma`, are positive functions of time.\n  `a` correspond to the mean-reversion rate, `sigma` is the volatility of\n  the process, `theta(t)` is the function that determines long run behaviour\n  of the process `r(t)` and is defined to match the market data through the\n  instantaneous forward rate matching:\n\n  ```None\n  \\theta = df(t) / dt + a * f(t) + 0.5 * sigma**2 / a\n           * (1 - exp(-2 * a *t)), 1 <= i <= n\n  ```\n  where `f(t)` is the instantaneous forward rate at time `0` for a maturity\n  `t` and `df(t)/dt` is the gradient of `f` with respect to the maturity.\n  See Section 3.3.1 of [1] for details.\n\n  If the parameters `a` and `sigma` are piecewise constant functions, the\n  process is sampled exactly. Otherwise, Euler sampling is used.\n\n  #### Example. Hull-White processes with piecewise constant coefficients.\n\n  ```python\n  import numpy as np\n  import tensorflow.compat.v2 as tf\n  import tf_quant_finance as tff\n\n  dtype = tf.float64\n  # Mean-reversion is a piecewise constant function.\n  mean_reversion = tff.math.piecewise.PiecewiseConstantFunc(\n      jump_locations=[1, 2, 3, 4],\n      values=[0.1, 0.2, 0.3, 0.4, 0.5],\n      dtype=dtype)\n  # Volatility is a piecewise constant function.\n  volatility = tff.math.piecewise.PiecewiseConstantFunc(\n      jump_locations=[0.1, 2.],\n      values=[0.1, 0.2, 0.1],\n      dtype=dtype)\n  instant_forward_rate_fn = lambda *args: [0.01]\n  process = tff.models.hull_white.HullWhiteModel1F(\n      mean_reversion=mean_reversion,\n      volatility=volatility, instant_forward_rate_fn=instant_forward_rate_fn,\n      dtype=dtype)\n  # Sample 10000 paths using Sobol numbers as a random type.\n  times = np.linspace(0., 1.0, 10)\n  num_samples = 10000  # number of trajectories\n  paths = process.sample_paths(\n      times,\n      num_samples=num_samples,\n      initial_state=[0.1],\n      random_type=tff.math.random.RandomType.SOBOL)\n  # Compute mean for each Hull-White process at the terminal value\n  tf.math.reduce_mean(paths[:, -1, 0], axis=0)\n  # Expected value: 0.10928\n  ```\n\n  #### References:\n    [1]: D. Brigo, F. Mercurio. Interest Rate Models. 2007.\n  """"""\n\n  def __init__(self,\n               mean_reversion,\n               volatility,\n               initial_discount_rate_fn,\n               dtype=None,\n               name=None):\n    """"""Initializes Hull-White Model.\n\n    Args:\n      mean_reversion: A real positive `Tensor` of shape `[1]` or a Python\n        callable. The callable can be one of the following:\n          (a) A left-continuous piecewise constant object (e.g.,\n          `tff.math.piecewise.PiecewiseConstantFunc`) that has a property\n          `is_piecewise_constant` set to `True`. In this case the object\n          should have a method `jump_locations(self)` that returns a\n          `Tensor` of shape `[num_jumps]`. `mean_reversion(t)` should return a\n          `Tensor` `t.shape`,\n          where `t` is a rank 1 `Tensor` of the same `dtype` as the output.\n          See example in the class docstring.\n         (b) A callable that accepts scalars (stands for time `t`) and returns a\n         `Tensor` of shape `[1]`.\n        Corresponds to the mean reversion rate.\n      volatility: A real positive `Tensor` of the same `dtype` as\n        `mean_reversion` or a callable with the same specs as above.\n        Corresponds to the lond run price variance.\n      initial_discount_rate_fn: A Python callable that accepts expiry time as a\n        real `Tensor` of the same `dtype` as `mean_reversion` and returns a\n        `Tensor` of same shape as the output.\n        Corresponds to the initial discount rates at time `t=0` such that\n        P(0,t) = exp(-y(t) * t) where P(0,t) denotes the initial discount bond\n        prices.\n      dtype: The default dtype to use when converting values to `Tensor`s.\n        Default value: `None` which means that default dtypes inferred by\n          TensorFlow are used.\n      name: Python string. The name to give to the ops created by this class.\n        Default value: `None` which maps to the default name `hull_white_model`.\n    """"""\n    super(HullWhiteModel1F, self).__init__(\n        1, mean_reversion, volatility, initial_discount_rate_fn,\n        corr_matrix=None, dtype=dtype, name=name)\n'"
tf_quant_finance/models/hull_white/vector_hull_white.py,106,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Vector of correlated Hull-White models with time-dependent parameters.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math import gradient\nfrom tf_quant_finance.math import piecewise\nfrom tf_quant_finance.math import random_ops as random\nfrom tf_quant_finance.models import euler_sampling\nfrom tf_quant_finance.models import generic_ito_process\nfrom tf_quant_finance.models import utils\n\n\nclass VectorHullWhiteModel(generic_ito_process.GenericItoProcess):\n  r""""""Ensemble of correlated Hull-White Models.\n\n  Represents the Ito process:\n\n  ```None\n    dr_i(t) = (theta_i(t) - a_i(t) * r_i(t)) dt + sigma_i(t) * dW_{r_i}(t),\n    1 <= i <= n,\n  ```\n  where `W_{r_i}` are 1D Brownian motions with a correlation matrix `Rho(t)`.\n  For each `i`, `r_i` is the Hull-White process.\n  `theta_i`, `a_i`, `sigma_i`, `Rho` are positive functions of time.\n  `a_i` correspond to the mean-reversion rate, `sigma_i` is the volatility of\n  the process, `theta_i(t)` is the function that determines long run behaviour\n  of the process `r(t) = (r_1(t), ..., r_n(t))`\n  and is computed to match the initial (at t=0) discount curve:\n\n  ```None\n  \\theta_i = df_i(t) / dt + a_i * f_i(t) + 0.5 * sigma_i**2 / a_i\n             * (1 - exp(-2 * a_i *t)), 1 <= i <= n\n  ```\n  where `f_i(t)` is the instantaneous forward rate at time `0` for a maturity\n  `t` and `df_i(t)/dt` is the gradient of `f_i` with respect to the maturity.\n  See Section 3.3.1 of [1] for details.\n\n  The price at time `t` of a zero-coupon bond maturing at `T` is given by\n  (Ref. [2]):\n\n  ```None\n  P(t,T) = P(0,T) / P(0,t) *\n           exp(-(r(t) - f(0,t)) * G(t,T) - 0.5 * y(t) * G(t,T)^2)\n\n  y(t) = int_0^t [exp(-2 int_u^t (a(s) ds)) sigma(u)^2 du]\n\n  G(t,T) = int_t^T [exp(-int_t^u a(s) ds) du]\n  ```\n\n  If mean-reversion, `a_i`, is constant and the volatility (`sigma_i`), and\n  correlation (`Rho`) are piecewise constant functions, the process is sampled\n  exactly. Otherwise, Euler sampling is used.\n\n  For `n=1` this class represents Hull-White Model (see\n  tff.models.hull_white.HullWhiteModel1F).\n\n  #### Example. Two correlated Hull-White processes.\n\n  ```python\n  import numpy as np\n  import tensorflow.compat.v2 as tf\n  import tf_quant_finance as tff\n\n  dtype = tf.float64\n  # Mean-reversion is constant for the two processes. `mean_reversion(t)`\n  # has shape `[dim] + t.shape`.\n  mean_reversion = tff.math.piecewise.PiecewiseConstantFunc(\n        jump_locations=[[], []],\n        values=[[0.03], [0.1]],\n        dtype=dtype)\n  # Volatility is a piecewise constant function with jumps at the same locations\n  # for both Hull-White processes. `volatility(t)` has shape `[dim] + t.shape`.\n  volatility = tff.math.piecewise.PiecewiseConstantFunc(\n      jump_locations=[[0.1, 2.], [0.1, 2.]],\n      values=[[0.01, 0.02, 0.01], [0.01, 0.015, 0.01]],\n      dtype=dtype)\n  # Correlation matrix is constant\n  corr_matrix = [[1., 0.1], [0.1, 1.]]\n  initial_discount_rate_fn = lambda *args: [0.01, 0.015]\n  process = VectorHullWhiteModel(\n      dim=2, mean_reversion=mean_reversion,\n      volatility=volatility,\n      initial_discount_rate_fn=initial_discount_rate_fn,\n      corr_matrix=None,\n      dtype=dtype)\n  # Sample 10000 paths using Sobol numbers as a random type.\n  times = np.linspace(0., 1.0, 10)\n  num_samples = 10000  # number of trajectories\n  paths = process.sample_paths(\n      times,\n      num_samples=num_samples,\n      initial_state=None)\n  # Compute mean for each Hull-White process at the terminal value\n  tf.math.reduce_mean(paths[:, -1, :], axis=0)\n  # Expected value: [0.01013373 0.01494516]\n  ```\n\n  #### References:\n    [1]: D. Brigo, F. Mercurio. Interest Rate Models. 2007.\n    [2]: Leif B. G. Andersen and Vladimir V. Piterbarg. Interest Rate Modeling.\n    Volume II: Term Structure Models.\n  """"""\n\n  def __init__(self,\n               dim,\n               mean_reversion,\n               volatility,\n               initial_discount_rate_fn,\n               corr_matrix=None,\n               dtype=None,\n               name=None):\n    """"""Initializes the Correlated Hull-White Model.\n\n    Args:\n      dim: A Python scalar which corresponds to the number of correlated\n        Hull-White Models.\n      mean_reversion: A real positive `Tensor` of shape `[dim]` or a Python\n        callable. The callable can be one of the following:\n          (a) A left-continuous piecewise constant object (e.g.,\n          `tff.math.piecewise.PiecewiseConstantFunc`) that has a property\n          `is_piecewise_constant` set to `True`. In this case the object\n          should have a method `jump_locations(self)` that returns a\n          `Tensor` of shape `[dim, num_jumps]` or `[num_jumps]`\n          In the first case, `mean_reversion(t)` should return a `Tensor`\n          of shape `[dim] + t.shape`, and in the second, `t.shape + [dim]`,\n          where `t` is a rank 1 `Tensor` of the same `dtype` as the output.\n          See example in the class docstring.\n         (b) A callable that accepts scalars (stands for time `t`) and returns a\n         `Tensor` of shape `[dim]`.\n        Corresponds to the mean reversion rate.\n      volatility: A real positive `Tensor` of the same `dtype` as\n        `mean_reversion` or a callable with the same specs as above.\n        Corresponds to the lond run price variance.\n      initial_discount_rate_fn: A Python callable that accepts expiry time as\n        a real `Tensor` of the same `dtype` as `mean_reversion` and returns a\n        `Tensor` of shape `input_shape + dim`.\n        Corresponds to the zero coupon bond yield at the present time for the\n        input expiry time.\n      corr_matrix: A `Tensor` of shape `[dim, dim]` and the same `dtype` as\n        `mean_reversion` or a Python callable. The callable can be one of\n        the following:\n          (a) A left-continuous piecewise constant object (e.g.,\n          `tff.math.piecewise.PiecewiseConstantFunc`) that has a property\n          `is_piecewise_constant` set to `True`. In this case the object\n          should have a method `jump_locations(self)` that returns a\n          `Tensor` of shape `[num_jumps]`. `corr_matrix(t)` should return a\n          `Tensor` of shape `t.shape + [dim]`, where `t` is a rank 1 `Tensor`\n          of the same `dtype` as the output.\n         (b) A callable that accepts scalars (stands for time `t`) and returns a\n         `Tensor` of shape `[dim, dim]`.\n        Corresponds to the correlation matrix `Rho`.\n      dtype: The default dtype to use when converting values to `Tensor`s.\n        Default value: `None` which means that default dtypes inferred by\n          TensorFlow are used.\n      name: Python string. The name to give to the ops created by this class.\n        Default value: `None` which maps to the default name `hull_white_model`.\n\n    Raises:\n      ValueError:\n        (a) If either `mean_reversion`, `volatility`, or `corr_matrix` is\n          a piecewise constant function where `jump_locations` have batch shape\n          of rank > 1.\n        (b): If batch rank of the `jump_locations` is `[n]` with `n` different\n          from `dim`.\n    """"""\n    self._name = name or \'hull_white_model\'\n    with tf.name_scope(self._name):\n      self._dtype = dtype or None\n      # If the parameter is callable but not a piecewise constant use\n      # generic sampling method (e.g., Euler).\n      self._sample_with_generic = False\n      def _instant_forward_rate_fn(t):\n        t = tf.convert_to_tensor(t, dtype=self._dtype)\n        def _log_zero_coupon_bond(x):\n          r = tf.convert_to_tensor(\n              initial_discount_rate_fn(x), dtype=self._dtype)\n          return -r * x\n\n        rate = -gradient.fwd_gradient(\n            _log_zero_coupon_bond, t, use_gradient_tape=True,\n            unconnected_gradients=tf.UnconnectedGradients.ZERO)\n        return rate\n\n      def _initial_discount_rate_fn(t):\n        return tf.convert_to_tensor(\n            initial_discount_rate_fn(t), dtype=self._dtype)\n\n      self._instant_forward_rate_fn = _instant_forward_rate_fn\n      self._initial_discount_rate_fn = _initial_discount_rate_fn\n      self._mean_reversion, sample_with_generic = _input_type(\n          mean_reversion, dim=dim, dtype=dtype, name=\'mean_reversion\')\n\n      # Update flag to whether to sample with a generic sampler.\n      self._sample_with_generic |= sample_with_generic\n      # Get the volatility type\n      self._volatility, sample_with_generic = _input_type(\n          volatility, dim=dim, dtype=dtype, name=\'volatility\')\n\n      # Update flag to whether to sample with a generic sampler.\n      self._sample_with_generic |= sample_with_generic\n      if corr_matrix is not None:\n        # Get correlation matrix type\n        self._corr_matrix, sample_with_generic = _input_type(\n            corr_matrix, dim=dim, dtype=dtype, name=\'corr_matrix\')\n        # Update flag to whether to sample with a generic sampler.\n        self._sample_with_generic |= sample_with_generic\n      else:\n        self._corr_matrix = None\n\n      if not self._sample_with_generic:\n        self._exact_discretization_setup(dim)\n\n    # Volatility function\n    def _vol_fn(t, x):\n      """"""Volatility function of correlated Hull-White.""""""\n      # Get parameter values at time `t`\n      volatility = _get_parameters(tf.expand_dims(t, -1), self._volatility)[0]\n      volatility = tf.transpose(volatility)\n      if self._corr_matrix is not None:\n        corr_matrix = _get_parameters(tf.expand_dims(t, -1), self._corr_matrix)\n        corr_matrix = corr_matrix[0]\n        corr_matrix = tf.linalg.cholesky(corr_matrix)\n      else:\n        corr_matrix = tf.eye(self._dim, dtype=volatility.dtype)\n\n      return volatility * corr_matrix + tf.zeros(\n          x.shape.as_list()[:-1] + [self._dim, self._dim],\n          dtype=volatility.dtype)\n\n    # Drift function\n    def _drift_fn(t, x):\n      """"""Drift function of correlated Hull-White.""""""\n      # Get parameter values at time `t`\n      mean_reversion, volatility = _get_parameters(  # pylint: disable=unbalanced-tuple-unpacking\n          tf.expand_dims(t, -1), self._mean_reversion, self._volatility)\n      fwd_rates = self._instant_forward_rate_fn(t)\n      fwd_rates_grad = gradient.fwd_gradient(\n          self._instant_forward_rate_fn, t, use_gradient_tape=True,\n          unconnected_gradients=tf.UnconnectedGradients.ZERO)\n      drift = fwd_rates_grad + mean_reversion * fwd_rates\n      drift += (volatility**2 / 2 / mean_reversion\n                * (1 - tf.math.exp(-2 * mean_reversion * t))\n                - mean_reversion * x)\n      return drift\n    super(VectorHullWhiteModel, self).__init__(dim, _drift_fn, _vol_fn,\n                                               dtype, name)\n\n  @property\n  def mean_reversion(self):\n    return self._mean_reversion\n\n  @property\n  def volatility(self):\n    return self._volatility\n\n  def sample_paths(self,\n                   times,\n                   num_samples=1,\n                   random_type=None,\n                   seed=None,\n                   skip=0,\n                   time_step=None,\n                   name=None):\n    """"""Returns a sample of paths from the correlated Hull-White process.\n\n    Uses exact sampling if `self.mean_reversion` is constant and\n    `self.volatility` and `self.corr_matrix` are all `Tensor`s or piecewise\n    constant functions, and Euler scheme sampling otherwise.\n\n    The exact sampling implements the algorithm and notations in [1], section\n    10.1.6.1.\n\n    Args:\n      times: Rank 1 `Tensor` of positive real values. The times at which the\n        path points are to be evaluated.\n      num_samples: Positive scalar `int32` `Tensor`. The number of paths to\n        draw.\n      random_type: Enum value of `RandomType`. The type of (quasi)-random\n        number generator to use to generate the paths.\n        Default value: `None` which maps to the standard pseudo-random numbers.\n      seed: Seed for the random number generator. The seed is\n        only relevant if `random_type` is one of\n        `[STATELESS, PSEUDO, HALTON_RANDOMIZED, PSEUDO_ANTITHETIC,\n          STATELESS_ANTITHETIC]`. For `PSEUDO`, `PSEUDO_ANTITHETIC` and\n        `HALTON_RANDOMIZED` the seed should be an Python integer. For\n        `STATELESS` and  `STATELESS_ANTITHETIC `must be supplied as an integer\n        `Tensor` of shape `[2]`.\n        Default value: `None` which means no seed is set.\n      skip: `int32` 0-d `Tensor`. The number of initial points of the Sobol or\n        Halton sequence to skip. Used only when `random_type` is \'SOBOL\',\n        \'HALTON\', or \'HALTON_RANDOMIZED\', otherwise ignored.\n        Default value: `0`.\n      time_step: Scalar real `Tensor`. Maximal distance between time grid points\n        in Euler scheme. Used only when Euler scheme is applied.\n      Default value: `None`.\n      name: Python string. The name to give this op.\n        Default value: `sample_paths`.\n\n    Returns:\n      A `Tensor` of shape [num_samples, k, dim] where `k` is the size\n      of the `times` and `dim` is the dimension of the process.\n\n    Raises:\n      ValueError:\n        (a) If `times` has rank different from `1`.\n        (b) If Euler scheme is used by times is not supplied.\n    """"""\n    # Note: all the notations below are the same as in [2].\n    name = name or self._name + \'_sample_path\'\n    with tf.name_scope(name):\n      times = tf.convert_to_tensor(times, self._dtype)\n      if len(times.shape) != 1:\n        raise ValueError(\'`times` should be a rank 1 Tensor. \'\n                         \'Rank is {} instead.\'.format(len(times.shape)))\n      if self._sample_with_generic:\n        if time_step is None:\n          raise ValueError(\'`time_step` can not be `None` when at least one of \'\n                           \'the parameters is a generic callable.\')\n        initial_state = self._instant_forward_rate_fn(0.0)\n        return euler_sampling.sample(dim=self._dim,\n                                     drift_fn=self._drift_fn,\n                                     volatility_fn=self._volatility_fn,\n                                     times=times,\n                                     time_step=time_step,\n                                     num_samples=num_samples,\n                                     initial_state=initial_state,\n                                     random_type=random_type,\n                                     seed=seed,\n                                     skip=skip,\n                                     dtype=self._dtype)\n      return self._sample_paths(\n          times, None, num_samples, random_type, skip, seed)\n\n  def sample_discount_curve_paths(self,\n                                  times,\n                                  curve_times,\n                                  num_samples=1,\n                                  random_type=None,\n                                  seed=None,\n                                  skip=0,\n                                  name=None):\n    """"""Returns a sample of simulated discount curves for the Hull-white model.\n\n    Args:\n      times: Rank 1 `Tensor` of positive real values. The times at which the\n        discount curves are to be evaluated.\n      curve_times: Rank 1 `Tensor` of positive real values. The maturities\n        at which discount curve is computed at each simulation time.\n      num_samples: Positive scalar `int`. The number of paths to draw.\n      random_type: Enum value of `RandomType`. The type of (quasi)-random\n        number generator to use to generate the paths.\n        Default value: None which maps to the standard pseudo-random numbers.\n      seed: Seed for the random number generator. The seed is\n        only relevant if `random_type` is one of\n        `[STATELESS, PSEUDO, HALTON_RANDOMIZED, PSEUDO_ANTITHETIC,\n          STATELESS_ANTITHETIC]`. For `PSEUDO`, `PSEUDO_ANTITHETIC` and\n        `HALTON_RANDOMIZED` the seed should be an Python integer. For\n        `STATELESS` and  `STATELESS_ANTITHETIC` must be supplied as an integer\n        `Tensor` of shape `[2]`.\n        Default value: `None` which means no seed is set.\n      skip: `int32` 0-d `Tensor`. The number of initial points of the Sobol or\n        Halton sequence to skip. Used only when `random_type` is \'SOBOL\',\n        \'HALTON\', or \'HALTON_RANDOMIZED\', otherwise ignored.\n        Default value: `0`.\n      name: Str. The name to give this op.\n        Default value: `sample_discount_curve_paths`.\n\n    Returns:\n      A tuple containing two `Tensor`s. The first element is a `Tensor` of\n      shape [num_samples, m, k, dim] and contains the simulated bond curves\n      where `m` is the size of `curve_times`, `k` is the size of `times` and\n      `dim` is the dimension of the process. The second element is a `Tensor`\n      of shape [num_samples, k, dim] and contains the simulated short rate\n      paths.\n\n    ### References:\n      [1]: Leif B.G. Andersen and Vladimir V. Piterbarg. Interest Rate Modeling,\n      Volume II: Term Structure Models. 2010.\n    """"""\n    name = name or self._name + \'_sample_discount_curve_paths\'\n    with tf.name_scope(name):\n      times = tf.convert_to_tensor(times, self._dtype)\n      curve_times = tf.convert_to_tensor(curve_times, self._dtype)\n      return self._sample_paths(times, curve_times, num_samples,\n                                random_type, skip, seed)\n\n  def _sample_paths(self,\n                    times,\n                    curve_times,\n                    num_samples,\n                    random_type,\n                    skip,\n                    seed):\n    """"""Returns a sample of paths from the process.""""""\n    # Note: all the notations below are the same as in [1].\n    num_requested_times = times.shape[0]\n    params = [self._mean_reversion, self._volatility, self._corr_matrix]\n    if self._corr_matrix is not None:\n      params = params + [self._corr_matrix]\n    times, keep_mask = _prepare_grid(\n        times, params)\n    # Add zeros as a starting location\n    dt = times[1:] - times[:-1]\n    if dt.shape.is_fully_defined():\n      steps_num = dt.shape.as_list()[-1]\n    else:\n      steps_num = tf.shape(dt)[-1]\n      # TODO(b/148133811): Re-enable Sobol test when TF 2.2 is released.\n      if random_type == random.RandomType.SOBOL:\n        raise ValueError(\'Sobol sequence for Euler sampling is temporarily \'\n                         \'unsupported when `time_step` or `times` have a \'\n                         \'non-constant value\')\n    # In order to use low-discrepancy random_type we need to generate the\n    # sequence of independent random normals upfront. We also precompute random\n    # numbers for stateless random type in order to ensure independent samples\n    # for multiple function calls whith different seeds.\n    if random_type in (random.RandomType.SOBOL,\n                       random.RandomType.HALTON,\n                       random.RandomType.HALTON_RANDOMIZED,\n                       random.RandomType.STATELESS,\n                       random.RandomType.STATELESS_ANTITHETIC):\n      normal_draws = utils.generate_mc_normal_draws(\n          num_normal_draws=self._dim, num_time_steps=steps_num,\n          num_sample_paths=num_samples, random_type=random_type,\n          seed=seed,\n          dtype=self._dtype, skip=skip)\n    else:\n      normal_draws = None\n    # The below is OK because we support exact discretization with piecewise\n    # constant mr and vol.\n    mean_reversion = self._mean_reversion(times)\n    volatility = self._volatility(times)\n    if self._corr_matrix is not None:\n      corr_matrix = _get_parameters(\n          times + tf.math.reduce_min(dt) / 2, self._corr_matrix)[0]\n      corr_matrix_root = tf.linalg.cholesky(corr_matrix)\n    else:\n      corr_matrix_root = None\n\n    exp_x_t = self._conditional_mean_x(times, mean_reversion, volatility)\n    var_x_t = self._conditional_variance_x(times, mean_reversion, volatility)\n    y_t = self._compute_yt(times, mean_reversion, volatility)\n    if self._dim == 1:\n      mean_reversion = tf.expand_dims(mean_reversion, axis=0)\n\n    cond_fn = lambda i, *args: i < tf.size(dt)\n    def body_fn(i, written_count,\n                current_x,\n                rate_paths):\n      """"""Simulate hull-white process to the next time point.""""""\n      if normal_draws is None:\n        normals = random.mv_normal_sample(\n            (num_samples,),\n            mean=tf.zeros((self._dim,), dtype=mean_reversion.dtype),\n            random_type=random_type, seed=seed)\n      else:\n        normals = normal_draws[i]\n\n      if corr_matrix_root is not None:\n        normals = tf.linalg.matvec(corr_matrix_root[i], normals)\n\n      next_x = (tf.math.exp(-mean_reversion[:, i + 1] * dt[i]) * current_x\n                + exp_x_t[:, i] + tf.math.sqrt(var_x_t[:, i]) * normals)\n      f_0_t = self._instant_forward_rate_fn(times[i + 1])\n\n      # Update `rate_paths`\n      rate_paths = utils.maybe_update_along_axis(\n          tensor=rate_paths,\n          do_update=keep_mask[i + 1],\n          ind=written_count,\n          axis=1,\n          new_tensor=tf.expand_dims(next_x, axis=1) + f_0_t)\n      written_count += tf.cast(keep_mask[i + 1], dtype=tf.int32)\n      return (i + 1, written_count, next_x, rate_paths)\n\n    rate_paths = tf.zeros((num_samples, num_requested_times, self._dim),\n                          dtype=self._dtype)\n    initial_x = tf.zeros((num_samples, self._dim), dtype=self._dtype)\n    # TODO(b/157232803): Use tf.cumsum instead?\n    _, _, _, rate_paths = tf.while_loop(\n        cond_fn, body_fn, (0, 0, initial_x, rate_paths))\n\n    if curve_times is not None:\n      # Discount curve paths are desired.\n      return self._bond_reconstitution(\n          times, curve_times, mean_reversion, rate_paths, y_t), rate_paths\n    else:\n      return rate_paths\n\n  def _bond_reconstitution(self,\n                           times,\n                           curve_times,\n                           mean_reversion,\n                           rate_paths,\n                           y_t):\n    """"""Compute discount bond prices using Eq. 10.18 in Ref [2].""""""\n    num_curve_nodes = curve_times.shape.as_list()[0]  # m\n    num_sim_steps = times[1:].shape.as_list()[0]  # k\n    t = tf.reshape(\n        tf.repeat(tf.expand_dims(times[1:], axis=-1), self._dim, axis=-1),\n        (1, 1, num_sim_steps, self._dim))\n    curve_times = tf.reshape(curve_times, (1, num_curve_nodes, 1, 1))\n    curve_times = tf.repeat(curve_times, self._dim, axis=-1)\n    f_0_t = self._instant_forward_rate_fn(t)\n    x_t = tf.expand_dims(rate_paths, axis=1) - f_0_t\n    p_0_t = tf.math.exp(-self._initial_discount_rate_fn(t) * t)\n    p_0_t_tau = tf.math.exp(-self._initial_discount_rate_fn(curve_times + t) *\n                            (curve_times + t)) / p_0_t\n    # Transpose so the `dim` is the trailing dimension.\n    kappa = tf.transpose(mean_reversion[:, 1:])\n    kappa = tf.reshape(kappa, (1, 1, num_sim_steps, self._dim))\n    g_t_tau = (1. - tf.math.exp(-kappa * curve_times)) / kappa\n    term1 = x_t * g_t_tau\n    y_t = tf.reshape(tf.transpose(y_t[:, 1:]), (1, 1, num_sim_steps, self._dim))\n    term2 = y_t * g_t_tau**2\n    p_t_tau = p_0_t_tau * tf.math.exp(-term1 - 0.5 * term2)\n    return p_t_tau\n\n  def _exact_discretization_setup(self, dim):\n    """"""Initial setup for efficient computations.""""""\n    self._zero_padding = tf.zeros((dim, 1), dtype=self._dtype)\n    self._jump_locations = tf.concat(\n        [self._volatility.jump_locations(),\n         self._mean_reversion.jump_locations()], axis=-1)\n    self._jump_values_vol = self._volatility(self._jump_locations)\n    self._jump_values_mr = self._mean_reversion(self._jump_locations)\n    if dim == 1:\n      self._padded_knots = tf.concat([\n          self._zero_padding,\n          tf.expand_dims(self._jump_locations[:-1], axis=0)\n      ], axis=1)\n      self._jump_values_vol = tf.expand_dims(self._jump_values_vol, axis=0)\n      self._jump_values_mr = tf.expand_dims(self._jump_values_mr, axis=0)\n      self._jump_locations = tf.expand_dims(self._jump_locations, axis=0)\n\n    else:\n      self._padded_knots = tf.concat(\n          [self._zero_padding, self._jump_locations[:, :-1]], axis=1)\n\n  def _compute_yt(self, t, mr_t, sigma_t):\n    """"""Computes y(t) as described in [1], section 10.1.6.1.""""""\n    t = tf.repeat(tf.expand_dims(t, axis=0), self._dim, axis=0)\n    time_index = tf.searchsorted(self._jump_locations, t)\n    y_between_vol_knots = self._y_integral(\n        self._padded_knots, self._jump_locations, self._jump_values_vol,\n        self._jump_values_mr)\n    y_at_vol_knots = tf.concat(\n        [self._zero_padding,\n         _cumsum_using_matvec(y_between_vol_knots)], axis=1)\n\n    vn = tf.concat(\n        [self._zero_padding, self._jump_locations], axis=1)\n    y_t = self._y_integral(\n        tf.gather(vn, time_index, batch_dims=1), t, sigma_t, mr_t)\n    y_t = y_t + tf.gather(y_at_vol_knots, time_index, batch_dims=1)\n    return tf.math.exp(-2 * mr_t * t) * y_t\n\n  def _conditional_mean_x(self, t, mr_t, sigma_t):\n    """"""Computes the drift term in [1], Eq. 10.39.""""""\n    t = tf.repeat(tf.expand_dims(t, axis=0), self._dim, axis=0)\n    time_index = tf.searchsorted(self._jump_locations, t)\n    vn = tf.concat([self._zero_padding, self._jump_locations], axis=1)\n    y_between_vol_knots = self._y_integral(self._padded_knots,\n                                           self._jump_locations,\n                                           self._jump_values_vol,\n                                           self._jump_values_mr)\n\n    y_at_vol_knots = tf.concat(\n        [self._zero_padding,\n         _cumsum_using_matvec(y_between_vol_knots)], axis=1)\n\n    ex_between_vol_knots = self._ex_integral(self._padded_knots,\n                                             self._jump_locations,\n                                             self._jump_values_vol,\n                                             self._jump_values_mr,\n                                             y_at_vol_knots[:, :-1])\n\n    ex_at_vol_knots = tf.concat(\n        [self._zero_padding,\n         _cumsum_using_matvec(ex_between_vol_knots)], axis=1)\n\n    c = tf.gather(y_at_vol_knots, time_index, batch_dims=1)\n    exp_x_t = self._ex_integral(\n        tf.gather(vn, time_index, batch_dims=1), t, sigma_t, mr_t, c)\n    exp_x_t = exp_x_t + tf.gather(ex_at_vol_knots, time_index, batch_dims=1)\n    exp_x_t = (exp_x_t[:, 1:] - exp_x_t[:, :-1]) * tf.math.exp(\n        -tf.broadcast_to(mr_t, t.shape)[:, 1:] * t[:, 1:])\n    return exp_x_t\n\n  def _y_integral(self, t0, t, vol, k):\n    """"""Computes int_t0^t sigma(u)^2 exp(2*k*u) du.""""""\n    return (vol * vol) / (2 * k) * (\n        tf.math.exp(2 * k * t) - tf.math.exp(2 * k * t0))\n\n  def _ex_integral(self, t0, t, vol, k, y_t0):\n    """"""Function computes the integral for the drift calculation.""""""\n    # Computes int_t0^t (exp(k*s)*y(s)) ds,\n    # where y(s)=y(t0) + int_t0^s exp(-2*(s-u)) vol(u)^2 du.""""""\n    value = (\n        tf.math.exp(k * t) - tf.math.exp(k * t0) + tf.math.exp(2 * k * t0) *\n        (tf.math.exp(-k * t) - tf.math.exp(-k * t0)))\n    value = value * vol**2 / (2 * k * k) + y_t0 * (tf.math.exp(-k * t0) -\n                                                   tf.math.exp(-k * t)) / k\n    return value\n\n  def _conditional_variance_x(self, t, mr_t, sigma_t):\n    """"""Computes the variance of x(t), see [1], Eq. 10.41.""""""\n    t = tf.repeat(tf.expand_dims(t, axis=0), self._dim, axis=0)\n    var_x_between_vol_knots = self._variance_int(self._padded_knots,\n                                                 self._jump_locations,\n                                                 self._jump_values_vol,\n                                                 self._jump_values_mr)\n    varx_at_vol_knots = tf.concat(\n        [self._zero_padding,\n         _cumsum_using_matvec(var_x_between_vol_knots)],\n        axis=1)\n\n    time_index = tf.searchsorted(self._jump_locations, t)\n    vn = tf.concat(\n        [self._zero_padding,\n         self._jump_locations], axis=1)\n\n    var_x_t = self._variance_int(\n        tf.gather(vn, time_index, batch_dims=1), t, sigma_t, mr_t)\n    var_x_t = var_x_t + tf.gather(varx_at_vol_knots, time_index, batch_dims=1)\n\n    var_x_t = (var_x_t[:, 1:] - var_x_t[:, :-1]) * tf.math.exp(\n        -2 * tf.broadcast_to(mr_t, t.shape)[:, 1:] * t[:, 1:])\n    return var_x_t\n\n  def _variance_int(self, t0, t, vol, k):\n    """"""Computes int_t0^t exp(2*k*s) vol(s)^2 ds.""""""\n    return vol * vol / (2 * k) * (\n        tf.math.exp(2 * k * t) - tf.math.exp(2 * k * t0))\n\n\ndef _get_parameters(times, *params):\n  """"""Gets parameter values at at specified `times`.""""""\n  res = []\n  for param in params:\n    if isinstance(param, piecewise.PiecewiseConstantFunc):\n      jump_locations = param.jump_locations()\n      if len(jump_locations.shape) > 1:\n        # If `jump_locations` has batch dimension, transpose the result\n        # Shape [num_times, dim]\n        res.append(tf.transpose(param(times)))\n      else:\n        # Shape [num_times, dim]\n        res.append(param(times))\n    elif callable(param):\n      # Used only in drift and volatility computation.\n      # Here `times` is of shape [1]\n      t = tf.squeeze(times)\n      # The result has to have shape [1] + param.shape\n      res.append(tf.expand_dims(param(t), 0))\n    else:\n      res.append(param + tf.zeros(times.shape + param.shape, dtype=times.dtype))\n  return res\n\n\ndef _prepare_grid(times, *params):\n  """"""Prepares grid of times for path generation.\n\n  Args:\n    times:  Rank 1 `Tensor` of increasing positive real values. The times at\n      which the path points are to be evaluated.\n    *params: Parameters of the Heston model. Either scalar `Tensor`s of the\n      same `dtype` or instances of `PiecewiseConstantFunc`.\n\n  Returns:\n    Tuple `(all_times, mask)`.\n    `all_times` is a 1-D real `Tensor` containing all points from \'times`, the\n    uniform grid of points between `[0, times[-1]]` with grid size equal to\n    `time_step`, and jump locations of piecewise constant parameters The\n    `Tensor` is sorted in ascending order and may contain duplicates.\n    `mask` is a boolean 1-D `Tensor` of the same shape as \'all_times\', showing\n    which elements of \'all_times\' correspond to THE values from `times`.\n    Guarantees that times[0]=0 and mask[0]=False.\n  """"""\n  additional_times = []\n  for param in params:\n    if hasattr(param, \'is_piecewise_constant\'):\n      if param.is_piecewise_constant:\n        # Flatten all jump locations\n        additional_times.append(tf.reshape(param.jump_locations(), [-1]))\n  zeros = tf.constant([0], dtype=times.dtype)\n  all_times = tf.concat([zeros] + [times] + additional_times, axis=0)\n  additional_times_mask = [\n      tf.zeros_like(times, dtype=tf.bool) for times in additional_times]\n  mask = tf.concat([\n      tf.cast(zeros, dtype=tf.bool),\n      tf.ones_like(times, dtype=tf.bool)\n  ] + additional_times_mask, axis=0)\n  perm = tf.argsort(all_times, stable=True)\n  all_times = tf.gather(all_times, perm)\n  mask = tf.gather(mask, perm)\n  return all_times, mask\n\n\ndef _input_type(param, dim, dtype, name):\n  """"""Checks if the input parameter is a callable or piecewise constant.""""""\n  # If the parameter is callable but not a piecewise constant use\n  # generic sampling method (e.g., Euler).\n  sample_with_generic = False\n  if hasattr(param, \'is_piecewise_constant\'):\n    if param.is_piecewise_constant:\n      jumps_shape = param.jump_locations().shape\n      if len(jumps_shape) > 2:\n        raise ValueError(\n            \'Batch rank of `jump_locations` should be `1` for all piecewise \'\n            \'constant arguments but {} instead\'.format(len(jumps_shape[:-1])))\n      if len(jumps_shape) == 2:\n        if dim != jumps_shape[0]:\n          raise ValueError(\n              \'Batch shape of `jump_locations` should be either empty or \'\n              \'`[{0}]` but `[{1}]` instead\'.format(dim, jumps_shape[0]))\n      if name == \'mean_reversion\' and jumps_shape[0] > 0:\n        # Exact discretization currently not supported with time-dependent mr\n        sample_with_generic = True\n      return param, sample_with_generic\n    else:\n      sample_with_generic = True\n  elif callable(param):\n    sample_with_generic = True\n  else:\n    # Otherwise, input is a `Tensor`, return a `PiecewiseConstantFunc`.\n    param = tf.convert_to_tensor(param, dtype=dtype, name=name)\n    param_shape = param.shape.as_list()\n    if len(param_shape) > 1:\n      # For `Tensor` inputs we assume constant parameters, so this would be an\n      # error\n      raise ValueError(\n          \'Rank of {} should be `1`, but instead is {}\'.format(\n              name, len(param_shape)))\n    if param_shape[0] != dim:\n      # This is an error, we need as many parameters as the number of `dim`\n      raise ValueError(\n          \'Length of {} ({}) should be the same as `dims`({}).\'.format(\n              name, param_shape[0], dim))\n    jump_locations = [] if dim == 1 else [[]] * dim\n    values = param if dim == 1 else tf.expand_dims(param, axis=-1)\n    param = piecewise.PiecewiseConstantFunc(\n        jump_locations=jump_locations, values=values,\n        dtype=dtype)\n\n  return param, sample_with_generic\n\n\ndef _cumsum_using_matvec(input_tensor):\n  """"""Computes cumsum using matrix algebra.""""""\n  dtype = input_tensor.dtype\n  axis_length = input_tensor.shape.as_list()[-1]\n  ones = tf.ones([axis_length, axis_length], dtype=dtype)\n  lower_triangular = tf.linalg.band_part(ones, -1, 0)\n  cumsum = tf.linalg.matvec(lower_triangular, input_tensor)\n  return cumsum\n'"
tf_quant_finance/models/hull_white/zero_coupon_bond_option.py,70,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Pricing of zero coupon bond options using Hull-White model.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance.models.hull_white import vector_hull_white\n\n\ndef _ncdf(x):\n  """"""Implements the cumulative normal distribution function.""""""\n  return (tf.math.erf(x / _SQRT_2) + 1) / 2\n\n\ndef _prepare_indices(idx0, idx1, idx2, idx3):\n  """"""Prepare indices to get relevant slice from discount curve simulations.""""""\n  len0 = idx0.shape.as_list()[0]\n  len1 = idx1.shape.as_list()[0]\n  len3 = idx3.shape.as_list()[0]\n  idx0 = tf.repeat(idx0, len1*len3)\n  idx1 = tf.tile(tf.repeat(idx1, len3), [len0])\n  idx2 = tf.tile(tf.repeat(idx2, len3), [len0])\n  idx3 = tf.tile(idx3, [len0*len1])\n\n  return tf.stack([idx0, idx1, idx2, idx3], axis=-1)\n\n\ndef _cumprod_using_matvec(input_tensor):\n  """"""Computes cumprod using matrix algebra.""""""\n  dtype = input_tensor.dtype\n  axis_length = input_tensor.shape.as_list()[-1]\n  ones = tf.ones([axis_length, axis_length], dtype=dtype)\n  lower_triangular = tf.linalg.band_part(ones, -1, 0)\n  cumsum = tf.linalg.matvec(lower_triangular, tf.math.log(input_tensor))\n  return tf.math.exp(cumsum)\n\n\n_SQRT_2 = np.sqrt(2.0, dtype=np.float64)\n\n\ndef bond_option_price(*,\n                      strikes=None,\n                      expiries=None,\n                      maturities=None,\n                      discount_rate_fn=None,\n                      dim=None,\n                      mean_reversion=None,\n                      volatility=None,\n                      is_call_options=True,\n                      use_analytic_pricing=True,\n                      num_samples=1,\n                      random_type=None,\n                      seed=None,\n                      skip=0,\n                      time_step=None,\n                      dtype=None,\n                      name=None):\n  """"""Calculates European bond option prices using the Hull-White model.\n\n  Bond options are fixed income securities which give the holder a right to\n  exchange at a future date (the option expiry) a zero coupon bond for a fixed\n  price (the strike of the option). The maturity date of the bond is after the\n  the expiry of the option. If `P(t,T)` denotes the price at time `t` of a zero\n  coupon bond with maturity `T`, then the payoff from the option at option\n  expiry, `T0`, is given by:\n\n  ```None\n  payoff = max(P(T0, T) - X, 0)\n  ```\n  where `X` is the strike price of the option.\n\n  #### Example\n\n  ````python\n  import numpy as np\n  import tensorflow.compat.v2 as tf\n  import tf_quant_finance as tff\n\n  dtype = tf.float64\n\n  discount_rate_fn = lambda x: 0.01 * tf.ones_like(x, dtype=dtype)\n  expiries = np.array([1.0])\n  maturities = np.array([5.0])\n  strikes = np.exp(-0.01 * maturities) / np.exp(-0.01 * expiries)\n  price = tff.models.hull_white.bond_option_price(\n      strikes=strikes,\n      expiries=expiries,\n      maturities=maturities,\n      dim=1,\n      mean_reversion=[0.03],\n      volatility=[0.02],\n      discount_rate_fn=discount_rate_fn,\n      use_analytic_pricing=True,\n      dtype=dtype)\n  # Expected value: [[0.02817777]]\n  ````\n\n  Args:\n    strikes: A real `Tensor` of any shape and dtype. The strike price of the\n      options. The shape of this input determines the number (and shape) of the\n      options to be priced and the output.\n    expiries: A real `Tensor` of the same dtype and compatible shape as\n      `strikes`.  The time to expiry of each bond option.\n    maturities: A real `Tensor` of the same dtype and compatible shape as\n      `strikes`.  The time to maturity of the underlying zero coupon bonds.\n    discount_rate_fn: A Python callable that accepts expiry time as a real\n      `Tensor` and returns a `Tensor` of shape `input_shape + dim`. Computes\n      the zero coupon bond yield at the present time for the input expiry time.\n    dim: A Python scalar which corresponds to the number of Hull-White Models\n      to be used for pricing.\n    mean_reversion: A real positive `Tensor` of shape `[dim]` or a Python\n      callable. The callable can be one of the following:\n      (a) A left-continuous piecewise constant object (e.g.,\n      `tff.math.piecewise.PiecewiseConstantFunc`) that has a property\n      `is_piecewise_constant` set to `True`. In this case the object should\n      have a method `jump_locations(self)` that returns a `Tensor` of shape\n      `[dim, num_jumps]` or `[num_jumps]`. In the first case,\n      `mean_reversion(t)` should return a `Tensor` of shape `[dim] + t.shape`,\n      and in the second, `t.shape + [dim]`, where `t` is a rank 1 `Tensor` of\n      the same `dtype` as the output. See example in the class docstring.\n      (b) A callable that accepts scalars (stands for time `t`) and returns a\n      `Tensor` of shape `[dim]`.\n      Corresponds to the mean reversion rate.\n    volatility: A real positive `Tensor` of the same `dtype` as\n      `mean_reversion` or a callable with the same specs as above.\n      Corresponds to the lond run price variance.\n    is_call_options: A boolean `Tensor` of a shape compatible with\n      `strikes`. Indicates whether the option is a call (if True) or a put\n      (if False). If not supplied, call options are assumed.\n    use_analytic_pricing: A Python boolean specifying if analytic valuation\n      should be performed. Analytic valuation is only supported for constant\n      `mean_reversion` and piecewise constant `volatility`. If the input is\n      `False`, then valuation using Monte-Carlo simulations is performed.\n    num_samples: Positive scalar `int32` `Tensor`. The number of simulation\n      paths during Monte-Carlo valuation. This input is ignored during analytic\n      valuation.\n      Default value: The default value is 1.\n    random_type: Enum value of `RandomType`. The type of (quasi)-random\n      number generator to use to generate the simulation paths. This input is\n      relevant only for Monte-Carlo valuation and ignored during analytic\n      valuation.\n      Default value: `None` which maps to the standard pseudo-random numbers.\n    seed: Seed for the random number generator. The seed is only relevant if\n      `random_type` is one of\n      `[STATELESS, PSEUDO, HALTON_RANDOMIZED, PSEUDO_ANTITHETIC,\n        STATELESS_ANTITHETIC]`. For `PSEUDO`, `PSEUDO_ANTITHETIC` and\n      `HALTON_RANDOMIZED` the seed should be an Python integer. For\n      `STATELESS` and  `STATELESS_ANTITHETIC `must be supplied as an integer\n      `Tensor` of shape `[2]`. This input is relevant only for Monte-Carlo\n      valuation and ignored during analytic valuation.\n      Default value: `None` which means no seed is set.\n    skip: `int32` 0-d `Tensor`. The number of initial points of the Sobol or\n      Halton sequence to skip. Used only when `random_type` is \'SOBOL\',\n      \'HALTON\', or \'HALTON_RANDOMIZED\', otherwise ignored.\n      Default value: `0`.\n    time_step: Scalar real `Tensor`. Maximal distance between time grid points\n      in Euler scheme. Relevant when Euler scheme is used for simulation. This\n      input is ignored during analytic valuation.\n      Default value: `None`.\n    dtype: The default dtype to use when converting values to `Tensor`s.\n      Default value: `None` which means that default dtypes inferred by\n      TensorFlow are used.\n    name: Python string. The name to give to the ops created by this class.\n      Default value: `None` which maps to the default name\n      `hw_bond_option_price`.\n\n  Returns:\n    A `Tensor` of real dtype and shape  `strikes.shape + [dim]` containing the\n    computed option prices.\n  """"""\n  name = name or \'hw_bond_option_price\'\n  if dtype is None:\n    dtype = tf.convert_to_tensor([0.0]).dtype\n  with tf.name_scope(name):\n    strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\'strikes\')\n    expiries = tf.convert_to_tensor(expiries, dtype=dtype, name=\'expiries\')\n    maturities = tf.convert_to_tensor(maturities, dtype=dtype,\n                                      name=\'maturities\')\n    is_call_options = tf.convert_to_tensor(is_call_options, dtype=tf.bool,\n                                           name=\'is_call_options\')\n    model = vector_hull_white.VectorHullWhiteModel(\n        dim,\n        mean_reversion=mean_reversion,\n        volatility=volatility,\n        initial_discount_rate_fn=discount_rate_fn,\n        dtype=dtype)\n\n    if use_analytic_pricing:\n      return _analytic_valuation(\n          discount_rate_fn, model, strikes, expiries, maturities, dim,\n          is_call_options)\n\n    if time_step is None:\n      raise ValueError(\'`time_step` must be provided for simulation \'\n                       \'based bond option valuation.\')\n\n    sim_times, _ = tf.unique(tf.reshape(expiries, shape=[-1]))\n    longest_expiry = tf.reduce_max(sim_times)\n    sim_times, _ = tf.unique(\n        tf.concat([sim_times, tf.range(\n            time_step, longest_expiry, time_step)], axis=0))\n    sim_times = tf.sort(sim_times)\n    tau = maturities - expiries\n    curve_times_builder, _ = tf.unique(tf.reshape(tau, shape=[-1]))\n    curve_times = tf.sort(curve_times_builder)\n\n    p_t_tau, r_t = model.sample_discount_curve_paths(\n        times=sim_times,\n        curve_times=curve_times,\n        num_samples=num_samples,\n        random_type=random_type,\n        seed=seed,\n        skip=skip)\n\n    dt_builder = tf.concat(\n        [tf.convert_to_tensor([0.0], dtype=dtype),\n         sim_times[1:] - sim_times[:-1]], axis=0)\n    dt = tf.expand_dims(tf.expand_dims(dt_builder, axis=-1), axis=0)\n    discount_factors_builder = tf.math.exp(-r_t * dt)\n    # Transpose before (and after) because we want the cumprod along axis=1\n    # and `matvec` operates on the last axis.\n    discount_factors_builder = tf.transpose(\n        _cumprod_using_matvec(\n            tf.transpose(discount_factors_builder, [0, 2, 1])), [0, 2, 1])\n\n    # make discount factors the same shape as `p_t_tau`. This involves adding\n    # an extra dimenstion (corresponding to `curve_times`).\n    discount_factors_builder = tf.expand_dims(\n        discount_factors_builder,\n        axis=1)\n    discount_factors_simulated = tf.repeat(\n        discount_factors_builder, p_t_tau.shape.as_list()[1], axis=1)\n\n    # `sim_times` and `curve_times` are sorted for simulation. We need to\n    # select the indices corresponding to our input.\n    sim_time_index = tf.searchsorted(sim_times, tf.reshape(expiries, [-1]))\n    curve_time_index = tf.searchsorted(curve_times, tf.reshape(tau, [-1]))\n\n    gather_index = _prepare_indices(\n        tf.range(0, num_samples), curve_time_index, sim_time_index,\n        tf.range(0, dim))\n\n    payoff_discount_factors_builder = tf.gather_nd(\n        discount_factors_simulated, gather_index)\n    payoff_discount_factors = tf.reshape(\n        payoff_discount_factors_builder,\n        [num_samples] + strikes.shape + [dim])\n    payoff_bond_price_builder = tf.gather_nd(p_t_tau, gather_index)\n    payoff_bond_price = tf.reshape(\n        payoff_bond_price_builder, [num_samples]+strikes.shape+[dim])\n\n    is_call_options = tf.transpose(\n        tf.broadcast_to(is_call_options, [dim] + strikes.shape))\n\n    strikes = tf.reshape(strikes, [1] + strikes.shape + [1])\n    payoff = tf.where(\n        is_call_options,\n        tf.math.maximum(payoff_bond_price - strikes, 0.0),\n        tf.math.maximum(strikes - payoff_bond_price, 0.0))\n    option_value = tf.math.reduce_mean(\n        payoff_discount_factors * payoff, axis=0)\n\n    return option_value\n\n\ndef _analytic_valuation(discount_rate_fn, model, strikes, expiries, maturities,\n                        dim, is_call_options):\n  """"""Performs analytic valuation.""""""\n\n  discount_factor_expiry = tf.math.exp(\n      -discount_rate_fn(expiries) * expiries)\n  input_shape = expiries.shape\n  variance = _bond_option_variance(\n      model, tf.reshape(expiries, shape=[-1]), tf.reshape(maturities, [-1]),\n      dim)\n  variance = tf.reshape(variance, [dim] + input_shape)\n  discount_factor_maturity = tf.math.exp(-discount_rate_fn(maturities) *\n                                         maturities)\n  forward_bond_price = discount_factor_maturity / discount_factor_expiry\n  sqrt_variance = tf.math.sqrt(variance)\n  d1 = (tf.expand_dims(tf.math.log(forward_bond_price / strikes), axis=0) +\n        0.5 * variance) / sqrt_variance\n  d2 = d1 - tf.math.sqrt(variance)\n  option_value_call = (\n      tf.expand_dims(discount_factor_maturity, axis=0) * _ncdf(d1) -\n      tf.expand_dims(strikes * discount_factor_expiry, axis=0) * _ncdf(d2))\n  option_value_put = (\n      tf.expand_dims(strikes * discount_factor_expiry, axis=0) * _ncdf(-d2)\n      - tf.expand_dims(discount_factor_maturity, axis=0) * _ncdf(-d1))\n\n  is_call_options = tf.broadcast_to(is_call_options, [dim] + strikes.shape)\n  option_value = tf.where(is_call_options, option_value_call,\n                          option_value_put)\n\n  # Make `dim` as the last dimension and return.\n  return tf.transpose(\n      option_value,\n      perm=[i for i in range(1, len(option_value.shape.as_list()))] + [0])\n\n\n# TODO(b/158501671): Clean-up this implementation.\ndef _bond_option_variance(model, option_expiry, bond_maturity, dim):\n  """"""Computes black equivalent variance for bond options.\n\n  Black equivalent variance is definied as the variance to use in the Black\n  formula to obtain the model implied price of European bond options.\n\n  Args:\n    model: An instance of `VectorHullWhiteModel`.\n    option_expiry: A rank 1 `Tensor` of real dtype specifying the time to\n      expiry of each option.\n    bond_maturity: A rank 1 `Tensor` of real dtype specifying the time to\n      maturity of underlying zero coupon bonds.\n    dim: Dimensionality of the Hull-White process.\n\n  Returns:\n    A rank 1 `Tensor` of same dtype and shape as the inputs with computed\n    Black-equivalent variance for the underlying options.\n  """"""\n  # pylint: disable=protected-access\n  if model._sample_with_generic:\n    raise ValueError(\'The paramerization of `mean_reversion` and/or \'\n                     \'`volatility` does not support analytic computation \'\n                     \'of bond option variance.\')\n  mean_reversion = model.mean_reversion(option_expiry)\n  volatility = model.volatility(option_expiry)\n\n  option_expiry = tf.repeat(tf.expand_dims(option_expiry, axis=0), dim, axis=0)\n  bond_maturity = tf.repeat(tf.expand_dims(bond_maturity, axis=0), dim, axis=0)\n\n  var_between_vol_knots = model._variance_int(model._padded_knots,\n                                              model._jump_locations,\n                                              model._jump_values_vol,\n                                              model._jump_values_mr)\n  varx_at_vol_knots = tf.concat(\n      [model._zero_padding,\n       vector_hull_white._cumsum_using_matvec(var_between_vol_knots)],\n      axis=1)\n\n  time_index = tf.searchsorted(model._jump_locations, option_expiry)\n  vn = tf.concat(\n      [model._zero_padding,\n       model._jump_locations], axis=1)\n\n  var_expiry = model._variance_int(\n      tf.gather(vn, time_index, batch_dims=1), option_expiry,\n      volatility, mean_reversion)\n  var_expiry = var_expiry + tf.gather(\n      varx_at_vol_knots, time_index, batch_dims=1)\n  var_expiry = var_expiry * (\n      tf.math.exp(-mean_reversion*option_expiry) - tf.math.exp(\n          -mean_reversion*bond_maturity))**2 / mean_reversion**2\n  # gpylint: enable=protected-access\n  return var_expiry\n'"
tf_quant_finance/models/hull_white/zero_coupon_bond_option_test.py,20,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for zero_coupon_bond_option.py.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n# @test_util.run_all_in_graph_and_eager_modes\nclass HullWhiteBondOptionTest(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    self.mean_reversion_1d = [0.03]\n    self.volatility_1d = [0.02]\n    self.volatility_time_dep_1d = [0.01, 0.02]\n    self.mean_reversion_2d = [0.03, 0.06]\n    self.volatility_2d = [0.02, 0.01]\n\n    super(HullWhiteBondOptionTest, self).setUp()\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'analytic\',\n          \'use_analytic_pricing\': True,\n          \'error_tol\': 1e-8,\n      }, {\n          \'testcase_name\': \'simulation\',\n          \'use_analytic_pricing\': False,\n          \'error_tol\': 1e-4,\n      })\n  def test_correctness_1d(self, use_analytic_pricing, error_tol):\n    """"""Tests model with constant parameters in 1 dimension.""""""\n    dtype = tf.float64\n\n    discount_rate_fn = lambda x: 0.01 * tf.ones_like(x, dtype=dtype)\n    expiries = np.array([1.0])\n    maturities = np.array([5.0])\n    strikes = np.exp(-0.01 * maturities) / np.exp(-0.01 * expiries)\n    price = tff.models.hull_white.bond_option_price(\n        strikes=strikes,\n        expiries=expiries,\n        maturities=maturities,\n        dim=1,\n        mean_reversion=self.mean_reversion_1d,\n        volatility=self.volatility_1d,\n        discount_rate_fn=discount_rate_fn,\n        use_analytic_pricing=use_analytic_pricing,\n        num_samples=500000,\n        time_step=0.1,\n        random_type=tff.math.random.RandomType.PSEUDO_ANTITHETIC,\n        dtype=dtype)\n    self.assertEqual(price.dtype, dtype)\n    self.assertAllEqual(price.shape, [1, 1])\n    price = self.evaluate(price)\n    self.assertAllClose(price, [[0.02817777]], rtol=error_tol, atol=error_tol)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'analytic\',\n          \'use_analytic_pricing\': True,\n          \'error_tol\': 1e-8,\n      }, {\n          \'testcase_name\': \'simulation\',\n          \'use_analytic_pricing\': False,\n          \'error_tol\': 1e-4,\n      })\n  def test_xla(self, use_analytic_pricing, error_tol):\n    """"""Tests model with XLA.""""""\n    dtype = tf.float64\n\n    discount_rate_fn = lambda x: 0.01 * tf.ones_like(x, dtype=dtype)\n    expiries = np.array([1.0])\n    maturities = np.array([5.0])\n    strikes = np.exp(-0.01 * maturities) / np.exp(-0.01 * expiries)\n    @tf.function\n    def xla_fn():\n      return tff.models.hull_white.bond_option_price(\n          strikes=strikes,\n          expiries=expiries,\n          maturities=maturities,\n          dim=1,\n          mean_reversion=self.mean_reversion_1d,\n          volatility=self.volatility_1d,\n          discount_rate_fn=discount_rate_fn,\n          use_analytic_pricing=use_analytic_pricing,\n          num_samples=500000,\n          time_step=0.1,\n          random_type=tff.math.random.RandomType.PSEUDO_ANTITHETIC,\n          dtype=dtype)\n    price_xla = tf.xla.experimental.compile(xla_fn)\n    price = self.evaluate(price_xla)[0]\n    self.assertAllClose(price, [[0.02817777]], rtol=error_tol, atol=error_tol)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'analytic\',\n          \'use_analytic_pricing\': True,\n          \'error_tol\': 1e-8,\n      }, {\n          \'testcase_name\': \'simulation\',\n          \'use_analytic_pricing\': False,\n          \'error_tol\': 1e-4,\n      })\n  def test_correctness_time_dep_1d(self, use_analytic_pricing, error_tol):\n    """"""Tests model with piecewise constant volatility in 1 dimension.""""""\n    dtype = tf.float64\n\n    discount_rate_fn = lambda x: 0.01 * tf.ones_like(x, dtype=dtype)\n    expiries = np.array([1.0])\n    maturities = np.array([5.0])\n    strikes = np.exp(-0.01 * maturities) / np.exp(-0.01 * expiries)\n    volatility = tff.math.piecewise.PiecewiseConstantFunc(\n        jump_locations=[0.5], values=self.volatility_time_dep_1d,\n        dtype=dtype)\n    price = tff.models.hull_white.bond_option_price(\n        strikes=strikes,\n        expiries=expiries,\n        maturities=maturities,\n        dim=1,\n        mean_reversion=self.mean_reversion_1d,\n        volatility=volatility,\n        discount_rate_fn=discount_rate_fn,\n        use_analytic_pricing=use_analytic_pricing,\n        num_samples=500000,\n        time_step=0.1,\n        random_type=tff.math.random.RandomType.PSEUDO_ANTITHETIC,\n        dtype=dtype)\n    self.assertEqual(price.dtype, dtype)\n    self.assertAllEqual(price.shape, [1, 1])\n    price = self.evaluate(price)\n    self.assertAllClose(price, [[0.02237839]], rtol=error_tol, atol=error_tol)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'analytic\',\n          \'use_analytic_pricing\': True,\n          \'error_tol\': 1e-8,\n      }, {\n          \'testcase_name\': \'simulation\',\n          \'use_analytic_pricing\': False,\n          \'error_tol\': 1e-4,\n      })\n  def test_1d_batch(self, use_analytic_pricing, error_tol):\n    """"""Tests model with 1d batch of options.""""""\n    dtype = tf.float64\n\n    discount_rate_fn = lambda x: 0.01 * tf.ones_like(x, dtype=dtype)\n    expiries = np.array([1.0, 1.0, 1.0])\n    maturities = np.array([5.0, 5.0, 5.0])\n    strikes = np.exp(-0.01 * maturities) / np.exp(-0.01 * expiries)\n    price = tff.models.hull_white.bond_option_price(\n        strikes=strikes,\n        expiries=expiries,\n        maturities=maturities,\n        dim=1,\n        mean_reversion=self.mean_reversion_1d,\n        volatility=self.volatility_1d,\n        discount_rate_fn=discount_rate_fn,\n        use_analytic_pricing=use_analytic_pricing,\n        num_samples=500000,\n        time_step=0.1,\n        random_type=tff.math.random.RandomType.PSEUDO_ANTITHETIC,\n        dtype=dtype)\n    self.assertEqual(price.dtype, dtype)\n    self.assertAllEqual(price.shape, [3, 1])\n    price = self.evaluate(price)\n    self.assertAllClose(price, [[0.02817777], [0.02817777], [0.02817777]],\n                        rtol=error_tol, atol=error_tol)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'analytic\',\n          \'use_analytic_pricing\': True,\n          \'error_tol\': 1e-8,\n      }, {\n          \'testcase_name\': \'simulation\',\n          \'use_analytic_pricing\': False,\n          \'error_tol\': 1e-4,\n      })\n  def test_2d_batch(self, use_analytic_pricing, error_tol):\n    """"""Tests model with 2d batch of options.""""""\n    dtype = tf.float64\n\n    discount_rate_fn = lambda x: 0.01 * tf.ones_like(x, dtype=dtype)\n    expiries = np.array([[1.0, 1.0], [2.0, 2.0]])\n    maturities = np.array([[5.0, 5.0], [4.0, 4.0]])\n    strikes = np.exp(-0.01 * maturities) / np.exp(-0.01 * expiries)\n    price = tff.models.hull_white.bond_option_price(\n        strikes=strikes,\n        expiries=expiries,\n        maturities=maturities,\n        dim=1,\n        mean_reversion=self.mean_reversion_1d,\n        volatility=self.volatility_1d,\n        discount_rate_fn=discount_rate_fn,\n        use_analytic_pricing=use_analytic_pricing,\n        num_samples=500000,\n        time_step=0.1,\n        random_type=tff.math.random.RandomType.PSEUDO_ANTITHETIC,\n        dtype=dtype)\n    self.assertEqual(price.dtype, dtype)\n    self.assertAllEqual(price.shape, [2, 2, 1])\n    price = self.evaluate(price)\n    expected = [[[0.02817777], [0.02817777]], [[0.02042677], [0.02042677]]]\n    self.assertAllClose(price, expected, rtol=error_tol, atol=error_tol)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'analytic\',\n          \'use_analytic_pricing\': True,\n          \'error_tol\': 1e-8,\n      }, {\n          \'testcase_name\': \'simulation\',\n          \'use_analytic_pricing\': False,\n          \'error_tol\': 1e-4,\n      })\n  def test_correctness_2d(self, use_analytic_pricing, error_tol):\n    """"""Tests model with constant parameters in 2 dimension.""""""\n    dtype = tf.float64\n\n    discount_rate_fn = lambda x: 0.01 * tf.ones_like(x, dtype=dtype)\n    expiries = np.array([1.0])\n    maturities = np.array([5.0])\n    strikes = np.exp(-0.01 * maturities) / np.exp(-0.01 * expiries)\n    price = tff.models.hull_white.bond_option_price(\n        strikes=strikes,\n        expiries=expiries,\n        maturities=maturities,\n        dim=2,\n        mean_reversion=self.mean_reversion_2d,\n        volatility=self.volatility_2d,\n        discount_rate_fn=discount_rate_fn,\n        use_analytic_pricing=use_analytic_pricing,\n        num_samples=500000,\n        time_step=0.1,\n        random_type=tff.math.random.RandomType.PSEUDO_ANTITHETIC,\n        dtype=dtype)\n    self.assertEqual(price.dtype, dtype)\n    self.assertAllEqual(price.shape, [1, 2])\n    price = self.evaluate(price)\n    self.assertAllClose(price, [[0.02817777, 0.01309971]],\n                        rtol=error_tol, atol=error_tol)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'analytic\',\n          \'use_analytic_pricing\': True,\n          \'error_tol\': 1e-8,\n      }, {\n          \'testcase_name\': \'simulation\',\n          \'use_analytic_pricing\': False,\n          \'error_tol\': 1e-4,\n      })\n  def test_mixed_1d_batch_2d(self, use_analytic_pricing, error_tol):\n    """"""Tests mixed 1d batch with constant parameters in 2 dimension.""""""\n    dtype = tf.float64\n\n    discount_rate_fn = lambda x: 0.01 * tf.ones_like(x, dtype=dtype)\n    expiries = np.array([1.0, 1.0, 2.0])\n    maturities = np.array([5.0, 6.0, 4.0])\n    strikes = np.exp(-0.01 * maturities) / np.exp(-0.01 * expiries)\n    price = tff.models.hull_white.bond_option_price(\n        strikes=strikes,\n        expiries=expiries,\n        maturities=maturities,\n        dim=2,\n        mean_reversion=self.mean_reversion_2d,\n        volatility=self.volatility_2d,\n        discount_rate_fn=discount_rate_fn,\n        use_analytic_pricing=use_analytic_pricing,\n        num_samples=500000,\n        time_step=0.1,\n        random_type=tff.math.random.RandomType.PSEUDO_ANTITHETIC,\n        dtype=dtype)\n    self.assertEqual(price.dtype, dtype)\n    self.assertAllEqual(price.shape, [3, 2])\n    price = self.evaluate(price)\n    expected = [[0.02817777, 0.01309971], [0.03436008, 0.01575343],\n                [0.02042677, 0.00963248]]\n    self.assertAllClose(price, expected, rtol=error_tol, atol=error_tol)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'analytic\',\n          \'use_analytic_pricing\': True,\n          \'error_tol\': 1e-8,\n      }, {\n          \'testcase_name\': \'simulation\',\n          \'use_analytic_pricing\': False,\n          \'error_tol\': 1e-4,\n      })\n  def test_call_put(self, use_analytic_pricing, error_tol):\n    """"""Tests mixed 1d batch with constant parameters in 2 dimension.""""""\n    dtype = tf.float64\n\n    discount_rate_fn = lambda x: 0.01 * tf.ones_like(x, dtype=dtype)\n    expiries = np.array([1.0, 1.0, 2.0])\n    maturities = np.array([5.0, 6.0, 4.0])\n    strikes = np.exp(-0.01 * maturities) / np.exp(-0.01 * expiries) -0.01\n    price = tff.models.hull_white.bond_option_price(\n        strikes=strikes,\n        expiries=expiries,\n        maturities=maturities,\n        is_call_options=[True, False, False],\n        dim=2,\n        mean_reversion=self.mean_reversion_2d,\n        volatility=self.volatility_2d,\n        discount_rate_fn=discount_rate_fn,\n        use_analytic_pricing=use_analytic_pricing,\n        num_samples=500000,\n        time_step=0.1,\n        random_type=tff.math.random.RandomType.PSEUDO_ANTITHETIC,\n        dtype=dtype)\n    self.assertEqual(price.dtype, dtype)\n    self.assertAllEqual(price.shape, [3, 2])\n    price = self.evaluate(price)\n    expected = [[0.03325893, 0.01857569], [0.02945686, 0.01121538],\n                [0.01579646, 0.0054692]]\n    self.assertAllClose(price, expected, rtol=error_tol, atol=error_tol)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/models/legacy/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Legacy tools to build Diffusion Models.\n\nTODO(b/147222558): brownian_motion.py currently uses an old version of\nItoProcess interface. It should be updated to use the new interface, or merged\nwith generic_ito_process.py. Then the ""legacy"" package should be removed.\n""""""\n\nfrom tf_quant_finance.models.legacy.brownian_motion import BrownianMotion\nfrom tf_quant_finance.models.legacy.ito_process import ItoProcess\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'ItoProcess\',\n    \'BrownianMotion\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/models/legacy/brownian_motion.py,16,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""N-dimensional Brownian Motion.\n\nImplements the Ito process defined by:\n\n```\n  dX_i = a_i(t) dt + Sum[S_{ij}(t) dW_{j}, 1 <= j <= n] for each i in {1,..,n}\n```\n\nwhere `dW_{j}, 1 <= j <= n` are n independent 1D Brownian increments. The\ncoefficient `a_i` is the drift and the matrix `S_{ij}` is the volatility of the\nprocess.\n\nFor more details, see Ref [1].\n\n#### References:\n  [1]: Brent Oksendal. Stochastic Differential Equations: An Introduction with\n    Applications. Springer. 2010.\n""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math.random_ops import multivariate_normal as mvn\nfrom tf_quant_finance.models.legacy import brownian_motion_utils as bmu\nfrom tf_quant_finance.models.legacy import ito_process\n\n\nclass BrownianMotion(ito_process.ItoProcess):\n  """"""The multi dimensional Brownian Motion.""""""\n\n  def __init__(self,\n               dim=1,\n               drift=None,\n               volatility=None,\n               total_drift_fn=None,\n               total_covariance_fn=None,\n               dtype=None,\n               name=None):\n    """"""Initializes the Brownian motion class.\n\n    Represents the Ito process:\n\n    ```None\n      dX_i = a_i(t) dt + Sum(S_{ij}(t) dW_j for j in [1 ... n]), 1 <= i <= n\n\n    ```\n\n    `a_i(t)` is the drift rate of this process and the `S_{ij}(t)` is the\n    volatility matrix. Associated to these parameters are the integrated\n    drift and covariance functions. These are defined as:\n\n    ```None\n      total_drift_{i}(t1, t2) = Integrate(a_{i}(t), t1 <= t <= t2)\n      total_covariance_{ij}(t1, t2) = Integrate(inst_covariance_{ij}(t),\n                                                     t1 <= t <= t2)\n      inst_covariance_{ij}(t) = (S.S^T)_{ij}(t)\n    ```\n\n    Sampling from the Brownian motion process with time dependent parameters\n    can be done efficiently if the total drift and total covariance functions\n    are supplied. If the parameters are constant, the total parameters can be\n    automatically inferred and it is not worth supplying then explicitly.\n\n    Currently, it is not possible to infer the total drift and covariance from\n    the instantaneous values if the latter are functions of time. In this case,\n    we use a generic sampling method (Euler-Maruyama) which may be\n    inefficient. It is advisable to supply the total covariance and total drift\n    in the time dependent case where possible.\n\n    #### Example\n    The following is an example of a 1 dimensional brownian motion using default\n    arguments of zero drift and unit volatility.\n\n    ```python\n    process = bm.BrownianMotion()\n    times = np.array([0.2, 0.33, 0.7, 0.9, 1.88])\n    num_samples = 10000\n    with tf.Session() as sess:\n      paths = sess.run(process.sample_paths(\n          times,\n          num_samples=num_samples,\n          initial_state=np.array(0.1),\n          seed=1234))\n\n    # Compute the means at the specified times.\n    means = np.mean(paths, axis=0)\n    print (means)  # Mean values will be near 0.1 for each time\n\n    # Compute the covariances at the given times\n    covars = np.cov(paths.reshape([num_samples, 5]), rowvar=False)\n\n    # covars is a 5 x 5 covariance matrix.\n    # Expected result is that Covar(X(t), X(t\')) = min(t, t\')\n    expected = np.minimum(times.reshape([-1, 1]), times.reshape([1, -1]))\n    print (""Computed Covars: {}, True Covars: {}"".format(covars, expected))\n    ```\n\n    Args:\n      dim: Python int greater than or equal to 1. The dimension of the Brownian\n        motion.\n        Default value: 1 (i.e. a one dimensional brownian process).\n      drift: The drift of the process. The type and shape of the value must be\n        one of the following (in increasing order of generality) (a) A real\n        scalar `Tensor`. This corresponds to a time and component independent\n        drift. Every component of the Brownian motion has the same drift rate\n        equal to this value. (b) A real `Tensor` of shape `[dim]`. This\n        corresponds to a time independent drift with the `i`th component as the\n        drift rate of the `i`th component of the Brownian motion. (c) A Python\n        callable accepting a single positive `Tensor` of general shape (referred\n        to as `times_shape`) and returning a `Tensor` of shape `times_shape +\n        [dim]`. The input argument is the times at which the drift needs to be\n        evaluated. This case corresponds to a general time and direction\n        dependent drift rate.\n        Default value: None which maps to zero drift.\n      volatility: The volatility of the process. The type and shape of the\n        supplied value must be one of the following (in increasing order of\n        generality) (a) A positive real scalar `Tensor`. This corresponds to a\n        time independent, diagonal volatility matrix. The `(i, j)` component of\n        the full volatility matrix is equal to zero if `i != j` and equal to the\n        supplied value otherwise. (b) A positive real `Tensor` of shape `[dim]`.\n        This corresponds to a time independent volatility matrix with zero\n        correlation. The `(i, j)` component of the full volatility matrix is\n        equal to zero `i != j` and equal to the `i`th component of the supplied\n        value otherwise. (c) A positive definite real `Tensor` of shape `[dim,\n        dim]`. The full time independent volatility matrix. (d) A Python\n        callable accepting a single positive `Tensor` of general shape (referred\n        to as `times_shape`) and returning a `Tensor` of shape `times_shape +\n        [dim, dim]`. The input argument are the times at which the volatility\n        needs to be evaluated. This case corresponds to a general time and axis\n        dependent volatility matrix.\n        Default value: None which maps to a volatility matrix equal to identity.\n      total_drift_fn: Optional Python callable to compute the integrated drift\n        rate between two times. The callable should accept two real `Tensor`\n        arguments. The first argument contains the start times and the second,\n        the end times of the time intervals for which the total drift is to be\n        computed. Both the `Tensor` arguments are of the same dtype and shape.\n        The return value of the callable should be a real `Tensor` of the same\n        dtype as the input arguments and of shape `times_shape + [dim]` where\n        `times_shape` is the shape of the times `Tensor`. Note that it is an\n        error to supply this parameter if the `drift` is not supplied.\n        Default value: None.\n      total_covariance_fn: A Python callable returning the integrated covariance\n        rate between two times. The callable should accept two real `Tensor`\n        arguments. The first argument is the start times and the second is the\n        end times of the time intervals for which the total covariance is\n        needed. Both the `Tensor` arguments are of the same dtype and shape. The\n        return value of the callable is a real `Tensor` of the same dtype as the\n        input arguments and of shape `times_shape + [dim, dim]` where\n        `times_shape` is the shape of the times `Tensor`. Note that it is an\n        error to suppy this argument if the `volatility` is not supplied.\n        Default value: None.\n      dtype: The default dtype to use when converting values to `Tensor`s.\n        Default value: None which means that default dtypes inferred by\n          TensorFlow are used.\n      name: str. The name scope under which ops created by the methods of this\n        class are nested.\n        Default value: None which maps to the default name `brownian_motion`.\n\n    Raises:\n      ValueError if the dimension is less than 1 or if total drift is supplied\n        but drift is not supplied or if the total covariance is supplied but\n        but volatility is not supplied.\n    """"""\n    super(BrownianMotion, self).__init__()\n\n    if dim < 1:\n      raise ValueError(\'Dimension must be 1 or greater.\')\n    if drift is None and total_drift_fn is not None:\n      raise ValueError(\'total_drift_fn must not be supplied if drift\'\n                       \' is not supplied.\')\n    if volatility is None and total_covariance_fn is not None:\n      raise ValueError(\'total_covariance_fn must not be supplied if drift\'\n                       \' is not supplied.\')\n    self._dim = dim\n    self._dtype = dtype\n    self._name = name or \'brownian_motion\'\n\n    drift_fn, total_drift_fn = bmu.construct_drift_data(drift, total_drift_fn,\n                                                        dim, dtype)\n    self._drift_fn = drift_fn\n    self._total_drift_fn = total_drift_fn\n\n    vol_fn, total_covar_fn = bmu.construct_vol_data(volatility,\n                                                    total_covariance_fn, dim,\n                                                    dtype)\n    self._volatility_fn = vol_fn\n    self._total_covariance_fn = total_covar_fn\n\n  # Override\n  def dim(self):\n    """"""The dimension of the process.""""""\n    return self._dim\n\n  # Override\n  def dtype(self):\n    """"""The data type of process realizations.""""""\n    return self._dtype\n\n  # Override\n  def name(self):\n    """"""The name to give to the ops created by this class.""""""\n    return self._name\n\n  # Override\n  def drift_fn(self):\n    return lambda t, x: self._drift_fn(t)\n\n  # Override\n  def volatility_fn(self):\n    return lambda t, x: self._volatility_fn(t)\n\n  def total_drift_fn(self):\n    """"""The integrated drift of the process.\n\n    Returns:\n      None or a Python callable. None is returned if the input drift was a\n      callable and no total drift function was supplied.\n      The callable returns the integrated drift rate between two times.\n      It accepts two real `Tensor` arguments. The first argument is the\n      left end point and the second is the right end point of the time interval\n      for which the total drift is needed. Both the `Tensor` arguments are of\n      the same dtype and shape. The return value of the callable is\n      a real `Tensor` of the same dtype as the input arguments and of shape\n      `times_shape + [dim]` where `times_shape` is the shape of the times\n      `Tensor`.\n    """"""\n    return self._total_drift_fn\n\n  def total_covariance_fn(self):\n    """"""The total covariance of the process between two times.\n\n    Returns:\n      A Python callable returning the integrated covariances between two times.\n      The callable accepts two real `Tensor` arguments. The first argument\n      is the left end point and the second is the right end point of the time\n      interval for which the total covariance is needed.\n\n      The shape of the two input arguments and their dtypes must match.\n      The output of the callable is a `Tensor` of shape\n      `times_shape + [dim, dim]` containing the integrated covariance matrix\n      between the start times and end times.\n    """"""\n    return self._total_covariance_fn\n\n  # Override\n  def sample_paths(self,\n                   times,\n                   num_samples=1,\n                   initial_state=None,\n                   random_type=None,\n                   seed=None,\n                   swap_memory=True,\n                   name=None,\n                   **kwargs):\n    """"""Returns a sample of paths from the process.\n\n    Generates samples of paths from the process at the specified time points.\n\n    Args:\n      times: Rank 1 `Tensor` of increasing positive real values. The times at\n        which the path points are to be evaluated.\n      num_samples: Positive scalar `int`. The number of paths to draw.\n      initial_state: `Tensor` of shape `[dim]`. The initial state of the\n        process.\n        Default value: None which maps to a zero initial state.\n      random_type: Enum value of `RandomType`. The type of (quasi)-random number\n        generator to use to generate the paths.\n        Default value: None which maps to the standard pseudo-random numbers.\n      seed: Python `int`. The random seed to use. If not supplied, no seed is\n        set.\n      swap_memory: Whether GPU-CPU memory swap is enabled for this op. See\n        equivalent flag in `tf.while_loop` documentation for more details.\n        Useful when computing a gradient of the op since `tf.while_loop` is used\n        to propagate stochastic process in time.\n      name: str. The name to give this op. If not supplied, default name of\n        `sample_paths` is used.\n      **kwargs: parameters, specific to Euler schema: `grid_step` is rank 0 real\n        `Tensor` - maximal distance between points in grid in Euler schema. Note\n        that Euler sampling is only used if it is not possible to do exact\n        sampling because total drift or total covariance are unavailable.\n\n    Returns:\n     A real `Tensor` of shape [num_samples, k, n] where `k` is the size of the\n        `times`, `n` is the dimension of the process.\n    """"""\n    if self._total_drift_fn is None or self._total_covariance_fn is None:\n      return super(BrownianMotion, self).sample_paths(\n          times,\n          num_samples=num_samples,\n          initial_state=initial_state,\n          random_type=random_type,\n          seed=seed,\n          name=name,\n          **kwargs)\n\n    default_name = self._name + \'_sample_path\'\n    with tf.compat.v1.name_scope(\n        name, default_name=default_name, values=[times, initial_state]):\n      end_times = tf.convert_to_tensor(times, dtype=self.dtype())\n      start_times = tf.concat(\n          [tf.zeros([1], dtype=end_times.dtype), end_times[:-1]], axis=0)\n      paths = self._exact_sampling(end_times, start_times, num_samples,\n                                   initial_state, random_type, seed)\n      if initial_state is not None:\n        return paths + initial_state\n      return paths\n\n  def _exact_sampling(self, end_times, start_times, num_samples, initial_state,\n                      random_type, seed):\n    """"""Returns a sample of paths from the process.""""""\n    non_decreasing = tf.debugging.assert_greater_equal(\n        end_times, start_times, message=\'Sampling times must be non-decreasing\')\n    starts_non_negative = tf.debugging.assert_greater_equal(\n        start_times,\n        tf.zeros_like(start_times),\n        message=\'Sampling times must not be < 0.\')\n    with tf.compat.v1.control_dependencies(\n        [starts_non_negative, non_decreasing]):\n      drifts = self._total_drift_fn(start_times, end_times)\n      covars = self._total_covariance_fn(start_times, end_times)\n      # path_deltas are of shape [num_samples, size(times), dim].\n      path_deltas = mvn.multivariate_normal((num_samples,),\n                                            mean=drifts,\n                                            covariance_matrix=covars,\n                                            random_type=random_type,\n                                            seed=seed)\n      paths = tf.cumsum(path_deltas, axis=1)\n    return paths\n\n  # Override\n  def fd_solver_backward(self,\n                         final_time,\n                         discounting_fn=None,\n                         grid_spec=None,\n                         time_step=None,\n                         time_step_fn=None,\n                         values_batch_size=1,\n                         name=None,\n                         **kwargs):\n    """"""Returns a solver for solving Feynman-Kac PDE associated to the process.\n\n    Represents the PDE\n\n    ```None\n      V_t + Sum[a_i(t) V_i, 1<=i<=n] +\n        (1/2) Sum[ D_{ij}(t) V_{ij}, 1 <= i,j <= n] - r(t, x) V = 0\n    ```\n\n    In the above, `V_t` is the derivative of `V` with respect to `t`,\n    `V_i` is the partial derivative with respect to `x_i` and `V_{ij}` the\n    (mixed) partial derivative with respect to `x_i` and `x_j`. `D_{ij}` are\n    the components of the diffusion tensor:\n\n    ```None\n      D_{ij}(t) = (Sigma . Transpose[Sigma])_{ij}(t)\n    ```\n\n    This method provides a finite difference solver to solve the above\n    differential equation. Whereas the coefficients `mu` and `D` are properties\n    of the SDE itself, the function `r(t, x)` may be arbitrarily specified\n    by the user (the parameter `discounting_fn` to this method).\n\n    Args:\n      final_time: Positive scalar real `Tensor`. The time of the final value.\n        The solver is initialized to this final time.\n      discounting_fn: Python callable corresponding to the function `r(t, x)` in\n        the description above. The callable accepts two positional arguments.\n        The first argument is the time at which the discount rate function is\n        needed. The second argument contains the values of the state at which\n        the discount is to be computed.\n        Default value: None which maps to `r(t, x) = 0`.\n      grid_spec: An iterable convertible to a tuple containing at least the\n        attributes named \'grid\', \'dim\' and \'sizes\'. For a full description of\n        the fields and expected types, see `grids.GridSpec` which provides the\n        canonical specification of this object.\n      time_step: A real positive scalar `Tensor` or None. The fixed\n        discretization parameter along the time dimension. Either this argument\n        or the `time_step_fn` must be specified. It is an error to specify both.\n        Default value: None.\n      time_step_fn: A callable accepting an instance of `grids.GridStepperState`\n        and returning the size of the next time step as a real scalar tensor.\n        This argument allows usage of a non-constant time step while stepping\n        back. If not specified, the `time_step` parameter must be specified. It\n        is an error to specify both.\n        Default value: None.\n      values_batch_size: A positive Python int. The batch size of values to be\n        propagated simultaneously.\n        Default value: 1.\n      name: Python str. The name to give this op.\n        Default value: None which maps to `fd_solver_backward`.\n      **kwargs: Any other keyword args needed.\n\n    Returns:\n      An instance of `BackwardGridStepper` configured for solving the\n      Feynman-Kac PDE associated to this process.\n    """"""\n    # TODO(b/141669934): Implement the method once the boundary conditions\n    # specification is complete.\n    raise NotImplementedError(\'Finite difference solver not implemented\')\n\n\ndef _prefer_static_shape(tensor):\n  """"""Returns the static shape if fully specified else the dynamic shape.""""""\n  tensor = tf.convert_to_tensor(tensor)\n  static_shape = tensor.shape\n  if static_shape.is_fully_defined():\n    return static_shape\n  return tf.shape(tensor)\n\n\ndef _prefer_static_rank(tensor):\n  """"""Returns the static rank if fully specified else the dynamic rank.""""""\n  tensor = tf.convert_to_tensor(tensor)\n  if tensor.shape.rank is None:\n    return tf.rank(tensor)\n  return tensor.shape.rank\n'"
tf_quant_finance/models/legacy/brownian_motion_test.py,17,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for `sample_paths` of `ItoProcess`.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\nfrom tf_quant_finance.models.legacy.brownian_motion import BrownianMotion\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass BrownianMotionTest(tf.test.TestCase):\n\n  def assertArrayEqual(self, arg1, arg2):\n    self.assertArrayNear(arg1, arg2, 0.)\n\n  def test_default_construction_1d(self):\n    """"""Tests the default parameters.""""""\n    process = BrownianMotion()\n    self.assertEqual(process.dim(), 1)\n    drift_fn = process.drift_fn()\n    # Drift should be zero.\n    t0 = np.array([0.2, 0.7, 0.9])\n    t1 = t0 + [0.1, 0.8, 0.3]\n    drifts = self.evaluate(drift_fn(t0, None))\n    total_drift_fn = process.total_drift_fn()\n    self.assertAlmostEqual(\n        self.evaluate(total_drift_fn(0.4, 0.5)), 0.0, places=7)\n    self.assertArrayNear(drifts, np.zeros([3]), 1e-10)\n    variances = self.evaluate(process.total_covariance_fn()(t0, t1))\n    self.assertArrayNear(variances, t1 - t0, 1e-10)\n    self.assertAlmostEqual(\n        self.evaluate(process.total_covariance_fn()(0.41, 0.55)),\n        0.14,\n        places=7)\n\n  def test_default_construction_2d(self):\n    """"""Tests the default parameters for 2 dimensional Brownian Motion.""""""\n    process = BrownianMotion(dim=2)\n    self.assertEqual(process.dim(), 2)\n    drift_fn = process.total_drift_fn()\n    # Drifts should be zero.\n    t0 = np.array([0.2, 0.7, 0.9])\n    delta_t = np.array([0.1, 0.8, 0.3])\n    t1 = t0 + delta_t\n    drifts = self.evaluate(drift_fn(t0, t1))\n    self.assertEqual(drifts.shape, (3, 2))\n    self.assertArrayNear(drifts.reshape([-1]), np.zeros([3 * 2]), 1e-10)\n    variances = self.evaluate(process.total_covariance_fn()(t0, t1))\n    self.assertEqual(variances.shape, (3, 2, 2))\n    expected_variances = np.eye(2) * delta_t.reshape([-1, 1, 1])\n    print(variances, expected_variances)\n\n    self.assertArrayNear(\n        variances.reshape([-1]), expected_variances.reshape([-1]), 1e-10)\n\n  def test_path_properties_1d(self):\n    """"""Tests path samples have the right properties.""""""\n    process = BrownianMotion()\n    times = np.array([0.2, 0.33, 0.7, 0.9, 1.88])\n    num_samples = 10000\n    paths = self.evaluate(\n        process.sample_paths(\n            times,\n            num_samples=num_samples,\n            initial_state=np.array(0.1),\n            seed=1234))\n    self.assertArrayEqual(paths.shape, (num_samples, 5, 1))\n    self.assertArrayNear(\n        np.mean(paths, axis=0).reshape([-1]),\n        np.zeros(5) + 0.1, 0.05)\n\n    covars = np.cov(paths.reshape([num_samples, 5]), rowvar=False)\n    # Expected covariances are: cov_{ij} = min(t_i, t_j)\n    expected = np.minimum(times.reshape([-1, 1]), times.reshape([1, -1]))\n    self.assertArrayNear(covars.reshape([-1]), expected.reshape([-1]), 0.05)\n\n  def test_time_dependent_construction(self):\n    """"""Tests with time dependent drift and variance.""""""\n\n    def vol_fn(t):\n      return tf.expand_dims(0.2 - 0.1 * tf.exp(-t), axis=-1)\n\n    def variance_fn(t0, t1):\n      # The instantaneous volatility is 0.2 - 0.1 e^(-t).\n      tot_var = (t1 - t0) * 0.04 - (tf.exp(-2 * t1) - tf.exp(-2 * t0)) * 0.005\n      tot_var += 0.04 * (tf.exp(-t1) - tf.exp(-t0))\n      return tf.reshape(tot_var, [-1, 1, 1])\n\n    process = BrownianMotion(\n        dim=1, drift=0.1, volatility=vol_fn, total_covariance_fn=variance_fn)\n    t0 = np.array([0.2, 0.7, 0.9])\n    delta_t = np.array([0.1, 0.8, 0.3])\n    t1 = t0 + delta_t\n    drifts = self.evaluate(process.total_drift_fn()(t0, t1))\n    self.assertArrayNear(drifts, 0.1 * delta_t, 1e-10)\n    variances = self.evaluate(process.total_covariance_fn()(t0, t1))\n    self.assertArrayNear(\n        variances.reshape([-1]), [0.00149104, 0.02204584, 0.00815789], 1e-8)\n\n  def test_paths_time_dependent(self):\n    """"""Tests path properties with time dependent drift and variance.""""""\n\n    def vol_fn(t):\n      return tf.expand_dims(0.2 - 0.1 * tf.exp(-t), axis=-1)\n\n    def variance_fn(t0, t1):\n      # The instantaneous volatility is 0.2 - 0.1 e^(-t).\n      tot_var = (t1 - t0) * 0.04 - (tf.exp(-2 * t1) - tf.exp(-2 * t0)) * 0.005\n      tot_var += 0.04 * (tf.exp(-t1) - tf.exp(-t0))\n      return tf.reshape(tot_var, [-1, 1, 1])\n\n    process = BrownianMotion(\n        dim=1, drift=0.1, volatility=vol_fn, total_covariance_fn=variance_fn)\n    times = np.array([0.2, 0.33, 0.7, 0.9, 1.88])\n    num_samples = 10000\n    paths = self.evaluate(\n        process.sample_paths(\n            times,\n            num_samples=num_samples,\n            initial_state=np.array(0.1),\n            seed=12134))\n\n    self.assertArrayEqual(paths.shape, (num_samples, 5, 1))\n    self.assertArrayNear(\n        np.mean(paths, axis=0).reshape([-1]), 0.1 + times * 0.1, 0.05)\n\n    covars = np.cov(paths.reshape([num_samples, 5]), rowvar=False)\n    # Expected covariances are: cov_{ij} = variance_fn(0, min(t_i, t_j))\n    min_times = np.minimum(times.reshape([-1, 1]),\n                           times.reshape([1, -1])).reshape([-1])\n    expected_covars = self.evaluate(\n        variance_fn(tf.zeros_like(min_times), min_times))\n    self.assertArrayNear(covars.reshape([-1]), expected_covars, 0.005)\n\n  def test_paths_multi_dim(self):\n    """"""Tests path properties for 2 dimensional brownian motion.\n\n    We construct the following 2 dimensional time dependent brownian motion.\n\n    dX_1 = mu_1 dt + s11 dW_1 + s12 dW_2\n    dX_2 = mu_2 dt + s21 dW_1 + s22 dW_2\n\n    mu_1, mu_2 are constants. s11, s12, s21, s22 are all linear functions of\n    time. Let s11 = a11 t + b11 and similarly for the other three coefficients.\n    Define the matrices:\n      A = [[a11, a12], [a21, a22]], B = [[b11, b12], [b21, b22]]\n\n    Then the total covariance from 0 to time T is:\n\n    Total Covariance(0,T) = A.A\' T**3 / 3 + (A.B\'+B.A\') T**2 / 2 + B.B\' T\n\n    where A\', B\' are the transposes of A and B.\n    """"""\n\n    mu = np.array([0.2, 0.7])\n    a_mat = np.array([[0.4, 0.1], [0.3, 0.2]])\n    b_mat = np.array([[0.33, -0.03], [0.21, 0.5]])\n    c1 = np.matmul(a_mat, a_mat.transpose()) / 3\n    c2 = np.matmul(a_mat, b_mat.transpose()) / 2\n    c2 += c2.transpose()\n    c3 = np.matmul(b_mat, b_mat.transpose())\n\n    def vol_fn(t):\n      return a_mat * tf.reshape(t, [-1, 1, 1]) + b_mat\n\n    def tot_cov_fn(t0, t1):\n      t0 = tf.reshape(t0, [-1, 1, 1])\n      t1 = tf.reshape(t1, [-1, 1, 1])\n      return c1 * (t1**3 - t0**3) + c2 * (t1**2 - t0**2) + c3 * (t1 - t0)\n\n    process = BrownianMotion(\n        dim=2, drift=mu, volatility=vol_fn, total_covariance_fn=tot_cov_fn)\n    times = np.array([0.1, 0.21, 0.32, 0.43, 0.55])\n    num_samples = 10000\n    initial_state = np.array([0.1, -1.1])\n    paths = self.evaluate(\n        process.sample_paths(\n            times,\n            num_samples=num_samples,\n            initial_state=initial_state,\n            seed=12134))\n\n    self.assertArrayEqual(paths.shape, (num_samples, 5, 2))\n    expected_means = np.reshape(times, [-1, 1]) * mu + initial_state\n    self.assertArrayNear(\n        np.mean(paths, axis=0).reshape([-1]), expected_means.reshape([-1]),\n        0.005)\n    # For the covariance comparison, it is much simpler to transpose the\n    # time axis with event dimension.\n    paths = np.transpose(paths, [0, 2, 1])\n    # covars is of shape [10, 10].\n    # It has the block structure [ [<X,X>, <X,Y>], [<Y,X>, <Y,Y>]]\n    # where X is the first component of the brownian motion and Y is\n    # second component. <X,X> means the 5x5 covariance matrix of X(t1), ...X(t5)\n    # and similarly for the other components.\n    covars = np.cov(paths.reshape([num_samples, -1]), rowvar=False)\n\n    # Expected covariances are: cov_{ij} = variance_fn(0, min(t_i, t_j))\n    min_times = np.minimum(times.reshape([-1, 1]),\n                           times.reshape([1, -1])).reshape([-1])\n    # Of shape [25, 2, 2]. We need to do a transpose.\n    expected_covars = self.evaluate(\n        tot_cov_fn(tf.zeros_like(min_times), min_times))\n    expected_covars = np.transpose(expected_covars, (1, 2, 0))\n\n    xx_actual = covars[:5, :5].reshape([-1])\n    xx_expected = expected_covars[0, 0, :]\n    self.assertArrayNear(xx_actual, xx_expected, 0.005)\n\n    xy_actual = covars[:5, 5:].reshape([-1])\n    xy_expected = expected_covars[0, 1, :]\n    self.assertArrayNear(xy_actual, xy_expected, 0.005)\n\n    yy_actual = covars[5:, 5:].reshape([-1])\n    yy_expected = expected_covars[1, 1, :]\n\n    self.assertArrayNear(yy_actual, yy_expected, 0.005)\n\n  def test_unsorted_times_is_error(self):\n    """"""Tests that supplying unsorted times in sample_paths is an error.""""""\n    process = BrownianMotion()\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      self.evaluate(process.sample_paths([0.1, 0.09, 1.0], num_samples=1))\n\n  def test_negative_times_is_error(self):\n    """"""Tests that supplying negative times in sample_paths is an error.""""""\n    process = BrownianMotion()\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      self.evaluate(process.sample_paths([-0.1, 0.09, 1.0], num_samples=1))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/models/legacy/brownian_motion_utils.py,26,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utility functions needed for brownian motion and related processes.""""""\n\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance.math import gradient\n\n\ndef is_callable(var_or_fn):\n  """"""Returns whether an object is callable or not.""""""\n  # Python 2.7 as well as Python 3.x with x > 2 support \'callable\'.\n  # In between, callable was removed hence we need to do a more expansive check\n  if hasattr(var_or_fn, \'__call__\'):\n    return True\n  try:\n    return callable(var_or_fn)\n  except NameError:\n    return False\n\n\ndef outer_multiply(x, y):\n  """"""Performs an outer multiplication of two tensors.\n\n  Given two `Tensor`s, `S` and `T` of shape `s` and `t` respectively, the outer\n  product `P` is a `Tensor` of shape `s + t` whose components are given by:\n\n  ```none\n  P_{i1,...ik, j1, ... , jm} = S_{i1...ik} T_{j1, ... jm}\n  ```\n\n  Args:\n    x: A `Tensor` of any shape and numeric dtype.\n    y: A `Tensor` of any shape and the same dtype as `x`.\n\n  Returns:\n    outer_product: A `Tensor` of shape Shape[x] + Shape[y] and the same dtype\n      as `x`.\n  """"""\n  x_shape = tf.shape(x)\n  padded_shape = tf.concat(\n      [x_shape, tf.ones(tf.rank(y), dtype=x_shape.dtype)], axis=0)\n  return tf.reshape(x, padded_shape) * y\n\n\ndef construct_drift_data(drift, total_drift_fn, dim, dtype):\n  """"""Constructs drift functions.""""""\n  # Six cases based on drift being (None, callable, constant) and total drift\n  # being (None, callable).\n  # 1. Neither drift nor total drift given -> return defaults.\n  # 2. total drift is none and drift is a callable -> raise error.\n  # 3. total drift is none and drift is a constant -> calculate.\n  # 4. total drift is given and drift is none -> differentiate.\n  # 5. total drift is given and drift is callable -> return them.\n  # 6. total drift is given and drift is constant -> wrap and return.\n\n  # Case 1, 2 and 3.\n  if total_drift_fn is None:\n    # Case 1\n    if drift is None:\n      return _default_drift_data(dim, dtype)\n    if is_callable(drift):\n      # Case 2: drift is a function and total_drift_fn needs to be computed.\n      # We need numerical integration for this which will be added later.\n      # For now, return None.\n      # TODO(b/141091950): Add numerical integration.\n      return drift, None\n    # Case 3. Drift is a constant.\n    def total_drift(t1, t2):\n      dt = tf.convert_to_tensor(t2 - t1, dtype=dtype)\n      return outer_multiply(dt, tf.ones([dim], dtype=dt.dtype) * drift)\n\n    return _make_drift_fn_from_const(drift, dim, dtype), total_drift\n\n  # Total drift is not None\n  # Case 4.\n  if drift is None:\n    def drift_from_total_drift(t):\n      start_time = tf.zeros_like(t)\n      return gradient.fwd_gradient(lambda x: total_drift_fn(start_time, x), t)\n\n    return drift_from_total_drift, total_drift_fn\n\n  # Case 5\n  if is_callable(drift):\n    return drift, total_drift_fn\n\n  return _make_drift_fn_from_const(drift, dim, dtype), total_drift_fn\n\n\ndef construct_vol_data(volatility, total_covariance_fn, dim, dtype):\n  """"""Constructs volatility data.\n\n  This function resolves the supplied arguments in to the following ten cases:\n  (vol -> volatility, total_covar -> total_covariance_fn)\n  1. vol and total_covar are both None -> Return default values.\n  2. total_covar is supplied and vol is None -> compute vol from total covar.\n  3. total_covar is supplied and vol is a callable -> Return supplied values.\n  4. total_covar is supplied and vol is a scalar constant.\n  5. total_covar is supplied and vol is a vector constant.\n  6. total_covar is supplied and vol is a matrix constant.\n  7. total_covar is not supplied and vol is a callable -> Return None for total\n    covariance function.\n  8. total_covar is not supplied and vol is a scalar constant.\n  9. total_covar is not supplied and vol is a vector constant.\n  10. total_covar is not supplied and vol is a matrix.\n\n  For cases 4, 5 and 6 we create an appropriate volatility fn. For cases 8\n  through to 10 we do the same but also create an appropriate covariance\n  function.\n\n  Args:\n    volatility: The volatility specification. None or a callable or a scalar,\n      vector or matrix.\n    total_covariance_fn: The total covariance function. Either None or a\n      callable.\n    dim: int. The dimension of the process.\n    dtype: The default dtype to use.\n\n  Returns:\n    A tuple of two callables:\n      volatility_fn: A function accepting a time argument and returning\n        the volatility at that time.\n      total_covariance_fn: A function accepting two time arguments and\n        returning the total covariance between the two times.\n  """"""\n  # Case 1\n  if volatility is None and total_covariance_fn is None:\n    return _default_vol_data(dim, dtype)\n\n  if total_covariance_fn is not None:\n    # Case 2\n    if volatility is None:\n      vol_fn = _volatility_fn_from_total_covar_fn(total_covariance_fn)\n      return vol_fn, total_covariance_fn\n    # Case 3\n    if is_callable(volatility):\n      return volatility, total_covariance_fn\n    # Cases 4, 5, 6\n    return _construct_vol_data_const_vol(volatility, total_covariance_fn, dim,\n                                         dtype)\n\n  # Case 7.\n  if is_callable(volatility):\n    # TODO(b/141091950): Add numerical integration.\n    return volatility, None\n\n  # Cases 8-10\n  return _construct_vol_data_const_vol(volatility, None, dim, dtype)\n\n\ndef _make_drift_fn_from_const(drift_const, dim, dtype):\n  drift_const = tf.convert_to_tensor(drift_const, dtype=dtype)\n  # This trivial multiplication ensures that drift_const is of shape at\n  # least [dim].\n  drift_const = tf.ones([dim], dtype=drift_const.dtype) * drift_const\n  return lambda t: outer_multiply(tf.ones_like(t), drift_const)\n\n\ndef _volatility_fn_from_total_covar_fn(total_covariance_fn):\n  """"""Volatility function from total covariance function.""""""\n\n  def vol_fn(time):\n    # We should consider changing the start time to be some small dt behind\n    # the time. In case the total covariance is being computed by a numerical\n    # integration, this will mean that we spend less time iterating.\n    start_time = tf.zeros_like(time)\n    total_covar_fn = lambda t: total_covariance_fn(start_time, t)\n    vol_sq = gradient.fwd_gradient(total_covar_fn, time)\n    return tf.linalg.cholesky(vol_sq, name=\'volatility\')\n\n  return vol_fn\n\n\ndef _default_drift_data(dimension, dtype):\n  """"""Constructs a function which returns a zero drift.""""""\n\n  def zero_drift(time):\n    shape = tf.concat([tf.shape(time), [dimension]], axis=0)\n    time = tf.convert_to_tensor(time, dtype=dtype)\n    return tf.zeros(shape, dtype=time.dtype)\n\n  return zero_drift, (lambda t1, t2: zero_drift(t1))\n\n\ndef _default_vol_data(dimension, dtype):\n  """"""Unit volatility and corresponding covariance functions.""""""\n\n  def unit_vol(time):\n    shape = tf.concat([tf.shape(time), [dimension, dimension]], axis=0)\n    out_dtype = tf.convert_to_tensor(time, dtype=dtype).dtype\n    return tf.ones(shape, dtype=out_dtype)\n\n  def covar(start_time, end_time):\n    dt = tf.convert_to_tensor(end_time - start_time, dtype=dtype, name=\'dt\')\n    return outer_multiply(dt, tf.eye(dimension, dtype=dt.dtype))\n\n  return unit_vol, covar\n\n\ndef _ensure_matrix(volatility, dim, dtype):\n  """"""Converts a volatility tensor to the right shape.""""""\n  # Works only for static rank.\n  rank = len(volatility.shape)\n  if not rank:\n    return tf.eye(dim, dtype=dtype) * volatility\n  if rank == 1:\n    return tf.linalg.tensor_diag(volatility)\n  # It is of rank 2 at least\n  return volatility\n\n\ndef _covar_from_vol(volatility, dim, dtype):\n  rank = len(volatility.shape)\n  if not rank:\n    return volatility * volatility * tf.eye(dim, dtype=dtype)\n  if rank == 1:\n    return tf.linalg.tensor_diag(volatility * volatility)\n  return tf.linalg.matmul(volatility, volatility, transpose_b=True)\n\n\ndef _construct_vol_data_const_vol(volatility, total_covariance_fn, dim, dtype):\n  """"""Constructs vol data when constant volatility is supplied.""""""\n  volatility_matrix = _ensure_matrix(volatility, dim, dtype)\n\n  def vol_fn(time):\n    return outer_multiply(tf.ones_like(time), volatility_matrix)\n\n  if total_covariance_fn is not None:\n    return vol_fn, total_covariance_fn\n\n  # Need to compute total covariance.\n  covariance_matrix = _covar_from_vol(volatility, dim, dtype)\n  covar_fn = lambda t1, t2: outer_multiply(t2 - t1, covariance_matrix)\n  return vol_fn, covar_fn\n'"
tf_quant_finance/models/legacy/brownian_motion_utils_test.py,26,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for `sample_paths` of `ItoProcess`.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\nfrom tf_quant_finance.models.legacy import brownian_motion_utils as bm_utils\n\n\n# Test classes and functions for testing callables.\nclass _TestClass(object):\n  pass\n\n\nclass _TestClass2(object):\n\n  def __call__(self, x):\n    return x * x\n\n\ndef _test_fn(x, y, z):\n  return x + 2 * y + 4 * z\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass BrownianMotionUtilsTest(tf.test.TestCase):\n\n  def test_is_callable(self):\n    arg1 = lambda x: x * x\n    self.assertTrue(bm_utils.is_callable(arg1))\n    arg2 = _TestClass()\n    self.assertFalse(bm_utils.is_callable(arg2))\n    arg3 = _TestClass2()\n    self.assertTrue(bm_utils.is_callable(arg3))\n    self.assertTrue(bm_utils.is_callable(_test_fn))\n    self.assertFalse(bm_utils.is_callable(2.0))\n\n  def test_outer_multiply(self):\n    # Scalar - scalar\n    scalars = tf.constant(2.0), tf.constant(3.0)\n    vectors = tf.constant([1.0, 2.0]), tf.constant([3.0, 5.0, 1.0])\n    matrices = (tf.constant([[1., 2],\n                             [2., 3]]), tf.constant([[1., 2, 3], [4, 5, 6]]))\n    result1 = self.evaluate(bm_utils.outer_multiply(scalars[0], scalars[1]))\n    self.assertEqual(result1, 6)\n    # Scalar - vector\n    result2 = self.evaluate(bm_utils.outer_multiply(scalars[0], vectors[1]))\n    np.testing.assert_allclose(result2, [6.0, 10.0, 2.0])\n    # Vector - scalar\n    result3 = self.evaluate(bm_utils.outer_multiply(vectors[0], scalars[1]))\n    np.testing.assert_allclose(result3, [3.0, 6.0])\n\n    # Vector - vector\n    result4 = self.evaluate(bm_utils.outer_multiply(vectors[0], vectors[1]))\n    np.testing.assert_allclose(result4, [[3.0, 5.0, 1.0], [6.0, 10.0, 2.0]])\n\n    # Vector - vector non commutative\n    result5 = self.evaluate(bm_utils.outer_multiply(vectors[1], vectors[0]))\n    np.testing.assert_allclose(result5, [[3.0, 6.0], [5.0, 10.0], [1.0, 2.0]])\n\n    # Vector - matrix\n    result6 = self.evaluate(bm_utils.outer_multiply(vectors[0], matrices[0]))\n    np.testing.assert_allclose(result6, [[[1., 2], [2, 3]], [[2, 4], [4, 6]]])\n\n    # Matrix - matrix\n    result7 = self.evaluate(bm_utils.outer_multiply(matrices[1], matrices[0]))\n    np.testing.assert_allclose(\n        result7,\n        [[[[1., 2], [2., 3]], [[2., 4], [4., 6]], [[3., 6], [6., 9]]],\n         [[[4., 8], [8., 12]], [[5., 10], [10., 15]], [[6., 12], [12., 18]]]])\n\n  def test_construct_drift_default(self):\n    dtypes = [tf.float64, tf.float32]\n    for dtype in dtypes:\n      drift_fn, total_drift_fn = bm_utils.construct_drift_data(\n          None, None, 2, dtype)\n      times = tf.constant([0.3, 0.9, 1.5], dtype=dtype)\n      drift_vals = self.evaluate(drift_fn(times))\n      np.testing.assert_array_equal(drift_vals.shape, [3, 2])\n      np.testing.assert_allclose(drift_vals, [[0.0, 0], [0, 0], [0, 0]])\n      total_vals = self.evaluate(total_drift_fn(times - 0.2, times))\n      np.testing.assert_array_equal(total_vals.shape, [3, 2])\n      np.testing.assert_allclose(total_vals, [[0.0, 0], [0, 0], [0, 0]])\n\n  def test_construct_drift_constant(self):\n    dtypes = [tf.float64, tf.float32]\n\n    def make_total_drift_fn(v, dtype):\n\n      def fn(t1, t2):\n        return bm_utils.outer_multiply(t2 - t1, v * tf.ones([2], dtype=dtype))\n\n      return fn\n\n    for dtype in dtypes:\n      drift_const = tf.constant(2.0, dtype=dtype)\n      drift_fn, total_drift_fn = bm_utils.construct_drift_data(\n          drift_const, None, 2, dtype)\n      times = tf.constant([0.3, 0.9, 1.5], dtype=dtype)\n      drift_vals = self.evaluate(drift_fn(times))\n      np.testing.assert_array_equal(drift_vals.shape, [3, 2])\n      np.testing.assert_allclose(drift_vals, [[2.0, 2], [2, 2], [2, 2]])\n      total_vals = self.evaluate(total_drift_fn(times - 0.2, times))\n      np.testing.assert_array_equal(total_vals.shape, [3, 2])\n      np.testing.assert_allclose(\n          total_vals, [[0.4, 0.4], [0.4, 0.4], [0.4, 0.4]], atol=1e-7)\n      # Tests if both are supplied. Note we deliberately supply inconsistent\n      # data to ensure that the method doesn\'t do anything clever.\n\n      drift_fn_alt, total_drift_fn_alt = bm_utils.construct_drift_data(\n          drift_const, make_total_drift_fn(4.0, dtype), 2, dtype)\n      drift_vals = self.evaluate(drift_fn_alt(times))\n      np.testing.assert_array_equal(drift_vals.shape, [3, 2])\n      np.testing.assert_allclose(drift_vals, [[2.0, 2], [2, 2], [2, 2]])\n      total_vals_alt = self.evaluate(total_drift_fn_alt(times - 0.2, times))\n      np.testing.assert_array_equal(total_vals.shape, [3, 2])\n      np.testing.assert_allclose(\n          total_vals_alt, [[0.8, 0.8], [0.8, 0.8], [0.8, 0.8]], atol=1e-5)\n\n  def test_construct_drift_callable(self):\n    dtype = tf.float64\n    a, b = 0.1, -0.8\n\n    def test_drift_fn(t):\n      return tf.expand_dims(t * a + b, axis=-1)\n\n    def test_total_drift_fn(t1, t2):\n      res = (t2**2 - t1**2) * a / 2 + (t2 - t1) * b\n      return tf.expand_dims(res, axis=-1)\n\n    drift_fn, total_drift_fn = bm_utils.construct_drift_data(\n        test_drift_fn, test_total_drift_fn, 1, dtype)\n    times = tf.constant([0.0, 1.0, 2.0], dtype=dtype)\n    drift_vals = self.evaluate(drift_fn(times))\n    np.testing.assert_array_equal(drift_vals.shape, [3, 1])\n    np.testing.assert_allclose(drift_vals, [[-0.8], [-0.7], [-0.6]])\n\n    t1 = tf.constant([1.0, 2.0, 3.0], dtype=dtype)\n    t2 = tf.constant([1.5, 3.0, 5.0], dtype=dtype)\n\n    total_vals = self.evaluate(total_drift_fn(t1, t2))\n    np.testing.assert_array_equal(total_vals.shape, [3, 1])\n    np.testing.assert_allclose(\n        total_vals, [[-0.3375], [-0.55], [-0.8]], atol=1e-7)\n    # Tests that total drift is None if drift is a callable and no total_drift\n    # is supplied\n\n    _, total_drift = bm_utils.construct_drift_data(test_drift_fn, None, 1,\n                                                   dtype)\n    self.assertIsNone(total_drift)\n\n  # Tests for volatility. There are 10 cases.\n  def test_construct_vol_defaults(self):\n    # Check the None, None\n    dtype = np.float64\n    vol_fn, _ = bm_utils.construct_vol_data(None, None, 2, dtype)\n    times = tf.constant([0.5, 1.0, 2.0, 3.0], dtype=dtype)\n    vols = self.evaluate(vol_fn(times))\n    np.testing.assert_array_equal(vols.shape, [4, 2, 2])\n\n  def test_construct_vol_covar_and_no_vol(self):\n    # Check the None, None\n    dtype = np.float64\n    covar_matrix = np.array([[0.15, 0.30], [0.30, 0.60]]).astype(dtype)\n\n    def covar_fn(t1, t2):\n      return bm_utils.outer_multiply(t2 - t1, covar_matrix)\n\n    vol_fn, covar_fn = bm_utils.construct_vol_data(None, covar_fn, 2, dtype)\n\n    times = tf.constant([0.5, 1.0, 2.0, 3.0], dtype=dtype)\n    vols = self.evaluate(vol_fn(times))\n    np.testing.assert_array_equal(vols.shape, [4, 2, 2])\n    # Directly testing vols is not sensible because given a covariance matrix\n    # there are many volatility matrices that would lead to the same covar.\n    # So we instead check the implied covariance.\n    for i in range(4):\n      actual_covar = np.matmul(vols[i], vols[i].transpose())\n      np.testing.assert_allclose(actual_covar, covar_matrix)\n    times2 = times + 0.31415\n    # Check that the total covariance is correct.\n    tc = self.evaluate(covar_fn(times, times2))\n    np.testing.assert_array_equal(tc.shape, [4, 2, 2])\n    # Directly testing vols is not sensible because given a covariance matrix\n    # there are many volatility matrices that would lead to the same covar.\n    # So we instead check the implied covariance.\n    for i in range(4):\n      np.testing.assert_allclose(tc[i], covar_matrix * 0.31415)\n\n  def test_construct_vol_covar_and_vol_callables(self):\n    dtype = np.float64\n    vol_matrix = np.array([[1.0, 0.21, -0.33], [0.61, 1.5, 1.77],\n                           [-0.3, 1.19, -0.55]]).astype(dtype)\n    covar_matrix = np.matmul(vol_matrix, vol_matrix.transpose())\n    vol_fn = lambda time: bm_utils.outer_multiply(time, vol_matrix)\n\n    def tc_fn(t1, t2):\n      return bm_utils.outer_multiply((t2**2 - t1**2) / 2, covar_matrix)\n\n    times = np.array([[0.12, 0.44], [0.48, 1.698]]).astype(dtype)\n    actual_vol_fn, actual_tc_fn = bm_utils.construct_vol_data(\n        vol_fn, tc_fn, 3, dtype)\n    actual_vols = self.evaluate(actual_vol_fn(times))\n    np.testing.assert_array_equal(actual_vols.shape, [2, 2, 3, 3])\n    np.testing.assert_allclose(actual_vols, self.evaluate(vol_fn(times)))\n    times2 = times + np.array([[0.12, 0.34], [0.56, 0.78]]).astype(dtype)\n    actual_tc = self.evaluate(actual_tc_fn(times, times2))\n    np.testing.assert_array_equal(actual_tc.shape, [2, 2, 3, 3])\n    np.testing.assert_allclose(actual_tc,\n                               self.evaluate(actual_tc_fn(times, times2)))\n\n  def test_construct_vol_covar_and_scalar_vol(self):\n    dtype = np.float64\n    vol = tf.constant(0.94, dtype=dtype)\n    # Note that the total covariance function we supply is deliberately not\n    # the one that is implied by the volatility function.\n    np.random.seed(1235)\n    dim = 5\n    vol_matrix = np.random.randn(dim, dim)\n    covar_matrix = np.matmul(vol_matrix, vol_matrix.transpose())\n\n    def tc_fn(t1, t2):\n      return bm_utils.outer_multiply(t2 - t1, covar_matrix)\n\n    times = np.array([[0.12, 0.44], [0.48, 1.698]]).astype(dtype)\n    actual_vol_fn, actual_tc_fn = bm_utils.construct_vol_data(\n        vol, tc_fn, dim, dtype)\n    actual_vols = self.evaluate(actual_vol_fn(times))\n    np.testing.assert_array_equal(actual_vols.shape, [2, 2, dim, dim])\n    for i in range(2):\n      for j in range(2):\n        np.testing.assert_allclose(actual_vols[i, j],\n                                   np.eye(dim).astype(dtype) * 0.94)\n\n    actual_tc = self.evaluate(actual_tc_fn(times, times + 1.0))\n    np.testing.assert_array_equal(actual_tc.shape, [2, 2, dim, dim])\n    for i in range(2):\n      for j in range(2):\n        np.testing.assert_allclose(actual_tc[i, j], covar_matrix)\n\n  def test_construct_vol_covar_and_vector_vol(self):\n    dtype = np.float64\n    vol = np.array([0.94, 1.1, 0.42], dtype=dtype)\n    # Note that the total covariance function we supply is deliberately not\n    # the one that is implied by the volatility function.\n    np.random.seed(5321)\n    dim = 3\n    vol_matrix = np.random.randn(dim, dim)\n    covar_matrix = np.matmul(vol_matrix, vol_matrix.transpose())\n\n    def tc_fn(t1, t2):\n      return bm_utils.outer_multiply(t2 - t1, covar_matrix)\n\n    times = np.array([[0.12], [0.48]]).astype(dtype)\n    actual_vol_fn, actual_tc_fn = bm_utils.construct_vol_data(\n        tf.constant(vol), tc_fn, dim, dtype)\n    actual_vols = self.evaluate(actual_vol_fn(times))\n    np.testing.assert_array_equal(actual_vols.shape, [2, 1, dim, dim])\n    for i in range(2):\n      np.testing.assert_allclose(actual_vols[i, 0], np.diag(vol))\n    actual_tc = self.evaluate(actual_tc_fn(times, times + 0.22))\n    np.testing.assert_array_equal(actual_tc.shape, [2, 1, dim, dim])\n    for i in range(2):\n      np.testing.assert_allclose(actual_tc[i, 0], covar_matrix * 0.22)\n\n  def test_construct_vol_covar_and_vol_matrix(self):\n    dtype = np.float64\n    vol_matrix = np.array([[1.0, 0.21, -0.33], [0.61, 1.5, 1.77],\n                           [-0.3, 1.19, -0.55]]).astype(dtype)\n    covar_matrix = np.matmul(vol_matrix, vol_matrix.transpose())\n\n    def tc_fn(t1, t2):\n      return bm_utils.outer_multiply((t2 - t1), covar_matrix)\n\n    times = np.array([[0.12, 0.44], [0.48, 1.698]]).astype(dtype)\n    actual_vol_fn, actual_tc_fn = bm_utils.construct_vol_data(\n        vol_matrix, tc_fn, 3, dtype)\n    actual_vols = self.evaluate(actual_vol_fn(times))\n    np.testing.assert_array_equal(actual_vols.shape, [2, 2, 3, 3])\n    for i in range(2):\n      for j in range(2):\n        np.testing.assert_allclose(actual_vols[i, j], vol_matrix)\n\n    times2 = times + np.array([[0.12, 0.34], [0.56, 0.78]]).astype(dtype)\n    actual_tc = self.evaluate(actual_tc_fn(times, times2))\n    np.testing.assert_array_equal(actual_tc.shape, [2, 2, 3, 3])\n    np.testing.assert_allclose(actual_tc,\n                               self.evaluate(actual_tc_fn(times, times2)))\n\n  def test_construct_vol_no_covar_vol_callable(self):\n    vol_fn = tf.sin\n    _, total_cov = bm_utils.construct_vol_data(vol_fn, None, 1, tf.float32)\n    self.assertIsNone(total_cov)\n\n  def test_construct_vol_no_covar_and_scalar_vol(self):\n    dtype = np.float64\n    vol = tf.constant(0.94, dtype=dtype)\n    np.random.seed(1235)\n    dim = 5\n    covar_matrix = np.eye(dim) * 0.94 * 0.94\n    times = np.array([[0.12, 0.44], [0.48, 1.698]]).astype(dtype)\n    actual_vol_fn, actual_tc_fn = bm_utils.construct_vol_data(\n        vol, None, dim, dtype)\n    actual_vols = self.evaluate(actual_vol_fn(times))\n    np.testing.assert_array_equal(actual_vols.shape, [2, 2, dim, dim])\n    for i in range(2):\n      for j in range(2):\n        np.testing.assert_allclose(actual_vols[i, j],\n                                   np.eye(dim).astype(dtype) * 0.94)\n\n    actual_tc = self.evaluate(actual_tc_fn(times, times + 1.0))\n    np.testing.assert_array_equal(actual_tc.shape, [2, 2, dim, dim])\n    for i in range(2):\n      for j in range(2):\n        np.testing.assert_allclose(actual_tc[i, j], covar_matrix)\n\n  def test_construct_vol_no_covar_and_vector_vol(self):\n    dtype = np.float64\n    vol = np.array([0.94, 1.1, 0.42], dtype=dtype)\n    np.random.seed(5321)\n    dim = 3\n    vol_matrix = np.diag(vol)\n    covar_matrix = np.matmul(vol_matrix, vol_matrix.transpose())\n\n    times = np.array([[0.12], [0.48]]).astype(dtype)\n    actual_vol_fn, actual_tc_fn = bm_utils.construct_vol_data(\n        tf.constant(vol), None, dim, dtype)\n    actual_vols = self.evaluate(actual_vol_fn(times))\n    np.testing.assert_array_equal(actual_vols.shape, [2, 1, dim, dim])\n    for i in range(2):\n      np.testing.assert_allclose(actual_vols[i, 0], np.diag(vol))\n    actual_tc = self.evaluate(actual_tc_fn(times, times + 0.22))\n    np.testing.assert_array_equal(actual_tc.shape, [2, 1, dim, dim])\n    for i in range(2):\n      np.testing.assert_allclose(actual_tc[i, 0], covar_matrix * 0.22)\n\n  def test_construct_vol_no_covar_and_vol_matrix(self):\n    dtype = np.float64\n    vol_matrix = np.array([[1.0, 0.21, -0.33], [0.61, 1.5, 1.77],\n                           [-0.3, 1.19, -0.55]]).astype(dtype)\n    covar_matrix = np.matmul(vol_matrix, vol_matrix.transpose())\n    times = np.array([[0.12, 0.44], [0.48, 1.698]]).astype(dtype)\n    actual_vol_fn, actual_tc_fn = bm_utils.construct_vol_data(\n        vol_matrix, None, 3, dtype)\n    actual_vols = self.evaluate(actual_vol_fn(times))\n    np.testing.assert_array_equal(actual_vols.shape, [2, 2, 3, 3])\n    for i in range(2):\n      for j in range(2):\n        np.testing.assert_allclose(actual_vols[i, j], vol_matrix)\n\n    dt = np.array([[0.12, 0.34], [0.56, 0.78]]).astype(dtype)\n    times2 = times + dt\n    actual_tc = self.evaluate(actual_tc_fn(times, times2))\n    np.testing.assert_array_equal(actual_tc.shape, [2, 2, 3, 3])\n    for i in range(2):\n      for j in range(2):\n        np.testing.assert_allclose(actual_tc[i, j], covar_matrix * dt[i, j])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/models/legacy/ito_process.py,33,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Defines Ito processes.\n\nIto processes underlie most quantitative finance models. This module defines\na framework for describing Ito processes. An Ito process is usually defined\nvia an Ito SDE:\n\n```\n  dX = a(t, X_t) dt + b(t, X_t) dW_t\n\n```\n\nwhere `a(t, x)` is a function taking values in `R^n`, `b(t, X_t)` is a function\ntaking values in `n x n` matrices. For a complete mathematical definition,\nincluding the regularity conditions that must be imposed on the coefficients\n`a(t, X)` and `b(t, X)`, see Ref [1].\n\n#### References:\n  [1]: Brent Oksendal. Stochastic Differential Equations: An Introduction with\n    Applications. Springer. 2010.\n""""""\n\nimport abc\nimport six\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math import random_ops\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass ItoProcess(object):\n  """"""Base class for Ito processes.\n\n    Represents a general Ito process:\n\n    ```None\n      dX_i = a_i(t, X) dt + Sum(S_{ij}(t, X) dW_j for 1 <= j <= n), 1 <= i <= n\n    ```\n\n  The vector coefficient `a_i` is referred to as the drift of the process and\n  the matrix `S_{ij}` as the volatility of the process. For the process to be\n  well defined, these coefficients need to satisfy certain technical conditions\n  which may be found in Ref. [1]. The vector `dW_j` represents independent\n  Brownian increments.\n\n  #### Example. 2-dimensional Ito process of the form\n\n  ```none\n  dX_1 = mu_1 * sqrt(t) dt + s11 * dW_1 + s12 * dW_2\n  dX_2 = mu_2 * sqrt(t) dt + s21 * dW_1 + s22 * dW_2\n  ```\n\n  class SimpleItoProcess(ito_process.ItoProcess):\n    def __init__(self, dim, drift_fn, vol_fn, dtype=tf.float64):\n      self._dim = dim\n      self._drift_fn = drift_fn\n      self._vol_fn = vol_fn\n      self._dtype = dtype\n\n    def dim(self):\n      return self._dim\n\n    def drift_fn(self):\n      return self._drift_fn\n\n    def volatility_fn(self):\n      return self._vol_fn\n\n    def dtype(self):\n      return self._dtype\n\n    def name(self):\n      return \'simple_ito_process\'\n\n  mu = np.array([0.2, 0.7])\n  s = np.array([[0.3, 0.1], [0.1, 0.3]])\n  num_samples = 10000\n  dim = 2\n  dtype=tf.float64\n\n  # Define drift and volatility functions\n  def drift_fn(t, x):\n    return mu * tf.sqrt(t) * tf.ones([num_samples, dim], dtype=dtype)\n\n  def vol_fn(t, x):\n    return s * tf.ones([num_samples, dim, dim], dtype=dtype)\n\n  # Initialize `SimpleItoProcess`\n  process = SimpleItoProcess(dim=2, drift_fn=drift_fn, vol_fn=vol_fn,\n                             dtype=dtype)\n  # Set starting location\n  x0 = np.array([0.1, -1.1])\n  # Sample `num_samples` paths at specified `times` locations using built-in\n  # Euler scheme.\n  times = [0.1, 1.0, 2.0]\n  paths = process.sample_paths(\n            times,\n            num_samples=num_samples,\n            initial_state=x0,\n            grid_step=0.01,\n            seed=42)\n\n  #### References\n  [1]: Brent Oksendal. Stochastic Differential Equations: An Introduction with\n    Applications. Springer. 2010.\n  """"""\n\n  @abc.abstractmethod\n  def dim(self):\n    """"""The dimension of the process.""""""\n    return None\n\n  @abc.abstractmethod\n  def dtype(self):\n    """"""The data type of process realizations.""""""\n    pass\n\n  @abc.abstractmethod\n  def name(self):\n    """"""The name to give to the ops created by this class.""""""\n    pass\n\n  def drift_fn(self):\n    """"""Python callable calculating instantaneous drift.\n\n    The callable should accept two real `Tensor` arguments of the same dtype.\n    The first argument is the scalar time t, the second argument is the value of\n    Ito process X - tensor of shape `batch_shape + [dim]`. The result is value\n    of drift a(t, X). The return value of the callable is a real `Tensor` of the\n    same dtype as the input arguments and of shape `batch_shape + [dim]`.\n    """"""\n    pass\n\n  def volatility_fn(self):\n    """"""Python callable calculating the instantaneous volatility.\n\n    The callable should accept two real `Tensor` arguments of the same dtype and\n    shape `times_shape`. The first argument is the scalar time t, the second\n    argument is the value of Ito process X - tensor of shape `batch_shape +\n    [dim]`. The result is value of drift S(t, X). The return value of the\n    callable is a real `Tensor` of the same dtype as the input arguments and of\n    shape `batch_shape + [dim, dim]`.\n    """"""\n    pass\n\n  def sample_paths(self,\n                   times,\n                   num_samples=1,\n                   initial_state=None,\n                   random_type=None,\n                   seed=None,\n                   swap_memory=True,\n                   name=None,\n                   **kwargs):\n    """"""Returns a sample of paths from the process.\n\n    The default implementation uses Euler schema. However, for particular types\n    of Ito processes more efficient schemes can be used.\n\n    Args:\n      times: Rank 1 `Tensor` of increasing positive real values. The times at\n        which the path points are to be evaluated.\n      num_samples: Positive scalar `int`. The number of paths to draw.\n      initial_state: `Tensor` of shape `[dim]`. The initial state of the\n        process.\n        Default value: None which maps to a zero initial state.\n      random_type: Enum value of `RandomType`. The type of (quasi)-random number\n        generator to use to generate the paths.\n        Default value: None which maps to the standard pseudo-random numbers.\n      seed: Python `int`. The random seed to use. If not supplied, no seed is\n        set.\n      swap_memory: Whether GPU-CPU memory swap is enabled for this op. See\n        equivalent flag in `tf.while_loop` documentation for more details.\n        Useful when computing a gradient of the op since `tf.while_loop` is used\n        to propagate stochastic process in time.\n      name: str. The name to give this op. If not supplied, default name of\n        `sample_paths` is used.\n      **kwargs: parameters, specific to Euler schema: `grid_step` is rank 0 real\n        `Tensor` - maximal distance between points in grid in Euler schema.\n\n    Returns:\n     A real `Tensor` of shape [num_samples, k, n] where `k` is the size of the\n        `times`, `n` is the dimension of the process.\n    """"""\n    if self.drift_fn() is None or self.volatility_fn() is None:\n      raise NotImplementedError(\n          \'In order to use Euler scheme, both drift_fn and volatility_fn \'\n          \'should be provided.\')\n    default_name = self.name() + \'_sample_paths\'\n    with tf.compat.v1.name_scope(\n        name, default_name=default_name, values=[times, initial_state]):\n      if initial_state is None:\n        initial_state = tf.zeros(self._dim, dtype=self._dtype)\n      times = tf.convert_to_tensor(times, dtype=self._dtype)\n      initial_state = tf.convert_to_tensor(\n          initial_state, dtype=self._dtype, name=\'initial_state\')\n      num_requested_times = tf.shape(times)[-1]\n      grid_step = kwargs[\'grid_step\']\n      times, keep_mask = self._prepare_grid(times, grid_step)\n      return self._sample_paths(times, grid_step, keep_mask,\n                                num_requested_times, num_samples, initial_state,\n                                random_type, seed, swap_memory)\n\n  def fd_solver_backward(self,\n                         final_time,\n                         discounting_fn=None,\n                         grid_spec=None,\n                         time_step=None,\n                         time_step_fn=None,\n                         values_batch_size=1,\n                         name=None,\n                         **kwargs):\n    """"""Returns a solver for solving Feynman-Kac PDE associated to the process.\n\n    Constructs a finite differences grid solver for solving the final value\n    problem as it appears in the Feynman-Kac formula associated to this Ito\n    process. The Feynman-Kac PDE is closely related to the backward Kolomogorov\n    equation associated to the stochastic process and allows for the inclusion\n    of a discount rate.\n\n    A limited version of Feynman-Kac formula states the following.\n    Consider the expectation\n\n    ```None\n      V(x, t, T) = E_Q[e^{-R(t, T)} f(X_T) | X_t = x]\n    ```\n\n    where `Q` is a probability measure under which `W_j` is an n-dimensional\n    Wiener process, `X` is an n-dimensional Ito process satisfying:\n\n    ```None\n      dX_i = mu_i dt + Sum[Sigma_{ij} dW_j, 1 <= j <= n]\n    ```\n\n    and `R(t, T)` is a positive stochastic process given by:\n\n    ```None\n      R(t,T) = Integral[ r(s, X_s) ds, t <= s <= T]\n    ```\n\n    This expectation is the solution of the following second order linear\n    partial differential equation:\n\n    ```None\n      V_t + Sum[mu_i(t, x) V_i, 1<=i<=n] +\n        (1/2) Sum[ D_{ij} V_{ij}, 1 <= i,j <= n] - r(t, x) V = 0\n    ```\n\n    In the above, `V_t` is the derivative of `V` with respect to `t`,\n    `V_i` is the partial derivative with respect to `x_i` and `V_{ij}` the\n    (mixed) partial derivative with respect to `x_i` and `x_j`. `D_{ij}` are\n    the components of the diffusion tensor:\n\n    ```None\n      D_{ij} = (Sigma . Transpose[Sigma])_{ij}\n    ```\n\n    This method provides a finite difference solver to solve the above\n    differential equation. Whereas the coefficients `mu` and `D` are properties\n    of the SDE itself, the function `r(t, x)` may be arbitrarily specified\n    by the user (the parameter `discounting_fn` to this method).\n\n    Args:\n      final_time: Positive scalar real `Tensor`. The time of the final value.\n        The solver is initialized to this final time.\n      discounting_fn: Python callable corresponding to the function `r(t, x)` in\n        the description above. The callable accepts two positional arguments.\n        The first argument is the time at which the discount rate function is\n        needed. The second argument contains the values of the state at which\n        the discount is to be computed.\n        Default value: None which maps to `r(t, x) = 0`.\n      grid_spec: An iterable convertible to a tuple containing at least the\n        attributes named \'grid\', \'dim\' and \'sizes\'. For a full description of\n        the fields and expected types, see `grids.GridSpec` which provides the\n        canonical specification of this object.\n      time_step: A real positive scalar `Tensor` or None. The fixed\n        discretization parameter along the time dimension. Either this argument\n        or the `time_step_fn` must be specified. It is an error to specify both.\n        Default value: None.\n      time_step_fn: A callable accepting an instance of `grids.GridStepperState`\n        and returning the size of the next time step as a real scalar tensor.\n        This argument allows usage of a non-constant time step while stepping\n        back. If not specified, the `time_step` parameter must be specified. It\n        is an error to specify both.\n        Default value: None.\n      values_batch_size: A positive Python int. The batch size of values to be\n        propagated simultaneously.\n        Default value: 1.\n      name: Python str. The name to give this op.\n        Default value: None which maps to `fd_solver_backward`.\n      **kwargs: Any other keyword args needed.\n\n    Returns:\n      An instance of `BackwardGridStepper` configured for solving the\n      Feynman-Kac PDE associated to this process.\n    """"""\n    raise NotImplementedError(\'Backward Finite difference solver not \'\n                              \'implemented for ItoProcess.\')\n\n  def _sample_paths(self, times, grid_step, keep_mask, num_requested_times,\n                    num_samples, initial_state, random_type, seed, swap_memory):\n    """"""Returns a sample of paths from the process.""""""\n    dt = times[1:] - times[:-1]\n    sqrt_dt = tf.sqrt(dt)\n    current_state = initial_state + tf.zeros(\n        [num_samples, self.dim()], dtype=initial_state.dtype)\n    steps_num = tf.shape(dt)[-1]\n    wiener_mean = tf.zeros((self.dim(), 1), dtype=self._dtype)\n\n    cond_fn = lambda i, *args: i < steps_num\n\n    def step_fn(i, written_count, current_state, result):\n      """"""Performs one step of Euler scheme.""""""\n      current_time = times[i + 1]\n      dw = random_ops.mv_normal_sample((num_samples,),\n                                       mean=wiener_mean,\n                                       random_type=random_type,\n                                       seed=seed)\n      dw = dw * sqrt_dt[i]\n      dt_inc = dt[i] * self.drift_fn()(current_time, current_state)  # pylint: disable=not-callable\n      dw_inc = tf.squeeze(\n          tf.matmul(self.volatility_fn()(current_time, current_state), dw), -1)  # pylint: disable=not-callable\n      next_state = current_state + dt_inc + dw_inc\n\n      def write_next_state_to_result():\n        # Replace result[:, written_count, :] with next_state.\n        one_hot = tf.one_hot(written_count, depth=num_requested_times)\n        mask = tf.expand_dims(one_hot > 0, axis=-1)\n        return tf.where(mask, tf.expand_dims(next_state, axis=1), result)\n\n      # Keep only states for times requested by user.\n      result = tf.cond(keep_mask[i + 1],\n                       write_next_state_to_result,\n                       lambda: result)\n      written_count += tf.cast(keep_mask[i + 1], dtype=tf.int32)\n      return i + 1, written_count, next_state, result\n\n    # Maximum number iterations is passed to the while loop below. It improves\n    # performance of the while loop on a GPU and is needed for XLA-compilation\n    # comptatiblity\n    maximum_iterations = (\n        tf.cast(1. / grid_step, dtype=tf.int32) + tf.size(times))\n    result = tf.zeros((num_samples, num_requested_times, self.dim()))\n    _, _, _, result = tf.compat.v1.while_loop(\n        cond_fn,\n        step_fn, (0, 0, current_state, result),\n        maximum_iterations=maximum_iterations,\n        swap_memory=swap_memory)\n\n    return result\n\n  def _prepare_grid(self, times, grid_step):\n    """"""Prepares grid of times for path generation.\n\n    Args:\n      times:  Rank 1 `Tensor` of increasing positive real values. The times at\n        which the path points are to be evaluated.\n      grid_step: Rank 0 real `Tensor`. Maximal distance between points in\n        resulting grid.\n\n    Returns:\n      Tuple `(all_times, mask)`.\n      `all_times` is 1-D real `Tensor` containing all points from \'times` and\n      whose intervals are at most `grid_step`.\n      `mask` is a boolean 1-D tensor of the same shape as \'all_times\', showing\n      which elements of \'all_times\' correspond to values from `times`.\n      Guarantees that times[0]=0 and grid_step[0]=False.\n      \'all_times` is sorted ascending and may contain duplicates.\n    """"""\n    grid = tf.range(0.0, times[-1], grid_step, dtype=self._dtype)\n    all_times = tf.concat([grid, times], axis=0)\n    mask = tf.concat([\n        tf.zeros_like(grid, dtype=tf.bool),\n        tf.ones_like(times, dtype=tf.bool)\n    ],\n                     axis=0)\n    perm = tf.argsort(all_times, stable=True)\n    all_times = tf.gather(all_times, perm)\n    mask = tf.gather(mask, perm)\n    return all_times, mask\n'"
tf_quant_finance/rates/constant_fwd/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Constant forward interpolation.""""""\n\nfrom tf_quant_finance.rates.constant_fwd.constant_fwd_interpolation import interpolate\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'interpolate\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/rates/constant_fwd/constant_fwd_interpolation.py,15,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Constant forward interpolation method.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math.interpolation.linear import linear_interpolation\n\n\ndef interpolate(interpolation_times,\n                reference_times,\n                reference_yields,\n                dtype=None,\n                name=None):\n  """"""Performs the constant forward interpolation for supplied points.\n\n  Given an interest rate yield curve whose maturities and the corresponding\n  (continuously compounded) yields are in `reference_times` and\n  `reference_yields`, this function returns interpolated yields at\n  `interpolation_times` using the constant forward interpolation.\n\n  Let `t_i, i=1,...,n` and `y_i, i=1,...,n` denote the reference_times and\n  reference_yields respectively. If `t` is a maturity for which the\n  interpolation is desired such that t_{i-1} <= t <= t_i, then constant forward\n  interpolation produces the corresponding yield, `y_t`, such that the forward\n  rate in the interval `[t_{i-1},t]` is the same as the forward rate in the\n  interval `[t_{i-1},t_i]`. Mathematically, this is the same as linearly\n  interpolating `t*y_t` using the curve `t_i, t_i*y_i`.\n\n  `reference_times` must be strictly increasing but `reference_yields` don\'t\n  need to be because we don\'t require the rate curve to be monotonic.\n\n  #### Examples\n\n  ```python\n  interpolation_times = [1, 3, 6, 7, 8, 15, 18, 25, 30]\n  # `reference_times` must be increasing, but `reference_yields` don\'t need to\n  # be.\n  reference_times = [0.0, 2.0, 6.0, 8.0, 18.0, 30.0]\n  reference_yields = [0.01, 0.02, 0.015, 0.014, 0.02, 0.025]\n  result = interpolate(interpolation_times, reference_times, reference_yields)\n  ```\n\n  Args:\n    interpolation_times: The times at which interpolation is desired. A N-D\n      `Tensor` of real dtype where the first N-1 dimensions represent the\n      batching dimensions.\n    reference_times: Maturities in the input yield curve. A N-D `Tensor` of\n      real dtype where the first N-1 dimensions represent the batching\n      dimensions. Should be sorted in increasing order.\n    reference_yields: Continuously compounded yields corresponding to\n      `reference_times`. A N-D `Tensor` of real dtype. Should have the\n      compatible shape as `x_data`.\n    dtype: Optional tf.dtype for `interpolation_times`, reference_times`,\n      and `reference_yields`. If not specified, the dtype of the inputs will be\n      used.\n    name: Python str. The name prefixed to the ops created by this function. If\n      not supplied, the default name \'constant_fwd_interpolation\' is used.\n\n  Returns:\n    A N-D `Tensor` of real dtype with the same shape as `interpolations_times`\n      containing the interpolated yields.\n  """"""\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'constant_fwd_interpolation\',\n      values=[interpolation_times, reference_times, reference_yields]):\n    interpolation_times = tf.convert_to_tensor(interpolation_times, dtype=dtype)\n    reference_times = tf.convert_to_tensor(reference_times, dtype=dtype)\n    reference_yields = tf.broadcast_to(\n        tf.convert_to_tensor(reference_yields, dtype=dtype),\n        shape=tf.shape(reference_times))\n\n    # Currently only flat extrapolation is being supported. We achieve this by\n    # clipping the interpolation times between minimum and maximum times of the\n    # input curve.\n    reference_times_min = tf.reduce_min(\n        reference_times, axis=tf.rank(reference_times) - 1, keepdims=True)\n    reference_times_max = tf.reduce_max(\n        reference_times, axis=tf.rank(reference_times) - 1, keepdims=True)\n    interpolation_times = tf.minimum(\n        tf.broadcast_to(reference_times_max, interpolation_times.shape),\n        tf.maximum(\n            tf.broadcast_to(reference_times_min, interpolation_times.shape),\n            interpolation_times))\n\n    interpolated_prod = linear_interpolation.interpolate(\n        interpolation_times, reference_times,\n        reference_times * reference_yields, dtype=dtype)\n    interpolated = interpolated_prod / interpolation_times\n    return interpolated\n'"
tf_quant_finance/rates/constant_fwd/constant_fwd_interpolation_test.py,2,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for Constant Fwd Interpolation.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\nfrom tf_quant_finance.rates.constant_fwd import constant_fwd_interpolation\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass ConstantFwdInterpolationTest(tf.test.TestCase):\n\n  def test_correctness(self):\n    interpolation_times = [1., 3., 6., 7., 8., 15., 18., 25., 30.]\n\n    reference_times = [0.0, 2.0, 6.0, 8.0, 18.0, 30.0]\n    reference_yields = [0.01, 0.02, 0.015, 0.014, 0.02, 0.025]\n    result = self.evaluate(\n        constant_fwd_interpolation.interpolate(interpolation_times,\n                                               reference_times,\n                                               reference_yields))\n    expected_result = np.array(\n        [0.02, 0.0175, 0.015, 0.01442857, 0.014, 0.01904, 0.02, 0.0235, 0.025])\n    np.testing.assert_allclose(result, expected_result, atol=1e-6)\n\n  def test_extrapolation(self):\n    interpolation_times = [0.5, 35.0]\n\n    reference_times = [1.0, 2.0, 6.0, 8.0, 18.0, 30.0]\n    reference_yields = [0.01, 0.02, 0.015, 0.014, 0.02, 0.025]\n    result = self.evaluate(\n        constant_fwd_interpolation.interpolate(interpolation_times,\n                                               reference_times,\n                                               reference_yields))\n    expected_result = np.array([0.01, 0.025])\n    np.testing.assert_allclose(result, expected_result, atol=1e-6)\n\n  def test_batching(self):\n    interpolation_times = [[1., 3., 6., 7.], [8., 15., 18., 25.]]\n\n    reference_times = [[0.0, 2.0, 6.0, 8.0, 18.0, 30.0],\n                       [0.0, 2.0, 6.0, 8.0, 18.0, 30.0]]\n    reference_yields = [[0.01, 0.02, 0.015, 0.014, 0.02, 0.025],\n                        [0.01, 0.02, 0.015, 0.014, 0.02, 0.025]]\n    result = self.evaluate(\n        constant_fwd_interpolation.interpolate(interpolation_times,\n                                               reference_times,\n                                               reference_yields))\n    expected_result = np.array(\n        [[0.02, 0.0175, 0.015, 0.01442857], [0.014, 0.01904, 0.02, 0.0235]])\n    np.testing.assert_allclose(result, expected_result, atol=1e-6)\n\n  def test_batching_and_extrapolation(self):\n    interpolation_times = [[0.5, 1., 3., 6., 7.], [8., 15., 18., 25., 35.]]\n\n    reference_times = [[1.0, 2.0, 6.0, 8.0, 18.0, 30.0],\n                       [1.0, 2.0, 6.0, 8.0, 18.0, 30.0]]\n    reference_yields = [[0.01, 0.02, 0.015, 0.014, 0.02, 0.025],\n                        [0.005, 0.02, 0.015, 0.014, 0.02, 0.025]]\n    result = self.evaluate(\n        constant_fwd_interpolation.interpolate(interpolation_times,\n                                               reference_times,\n                                               reference_yields))\n    expected_result = np.array(\n        [[0.01, 0.01, 0.0175, 0.015, 0.01442857],\n         [0.014, 0.01904, 0.02, 0.0235, 0.025]])\n    np.testing.assert_allclose(result, expected_result, atol=1e-6)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/rates/hagan_west/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Hagan West algorithm for rate interpolation and bootstrapping.""""""\n\nfrom tf_quant_finance.rates.hagan_west import bond_curve as bond_curve_lib\nfrom tf_quant_finance.rates.hagan_west import monotone_convex\n\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\nbond_curve = bond_curve_lib.bond_curve\nCurveBuilderResult = bond_curve_lib.CurveBuilderResult\n\n_allowed_symbols = [\n    \'bond_curve\',\n    \'monotone_convex\',\n    \'CurveBuilderResult\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/rates/hagan_west/bond_curve.py,38,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Methods to construct a discount curve from bonds.\n\nBuilding discount curves is a core problem in mathematical finance. Discount\ncurves are built using the available market data in liquidly traded rates\nproducts. These include bonds, swaps, forward rate agreements (FRAs) or\neurodollar futures contracts. This module contains methods to build rate curve\nfrom (potentially coupon bearing) bonds data.\n\nA discount curve is a function of time which gives the interest rate that\napplies to a unit of currency deposited today for a period of  time `t`.\nThe traded price of bonds implicitly contains the market view on the discount\nrates. The purpose of discount curve construction is to extract this\ninformation.\n\nThe algorithm implemented here is based on the Monotone Convex Interpolation\nmethod described by Hagan and West in Ref [1, 2].\n\n#### References:\n\n[1]: Patrick Hagan & Graeme West. Interpolation Methods for Curve Construction.\n  Applied Mathematical Finance. Vol 13, No. 2, pp 89-129. June 2006.\n  https://www.researchgate.net/publication/24071726_Interpolation_Methods_for_Curve_Construction\n[2]: Patrick Hagan & Graeme West. Methods for Constructing a Yield Curve.\n  Wilmott Magazine, pp. 70-81. May 2008.\n""""""\n\nimport collections\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.rates import cashflows\nfrom tf_quant_finance.rates.hagan_west import monotone_convex\n\nCurveBuilderResult = collections.namedtuple(\n    \'CurveBuilderResult\',\n    [\n        # Rank 1 real `Tensor`. Times for the computed discount rates.\n        \'times\',\n        # Rank 1 `Tensor` of the same dtype as `times`.\n        # The inferred discount rates.\n        \'discount_rates\',\n        # Rank 1 `Tensor` of the same dtype as `times`.\n        # The inferred discount factors.\n        \'discount_factors\',\n        # Rank 1 `Tensor` of the same dtype as `times`. The\n        # initial guess for the discount rates.\n        \'initial_discount_rates\',\n        # Scalar boolean `Tensor`. Whether the procedure converged.\n        \'converged\',\n        # Scalar boolean `Tensor`. Whether the procedure failed.\n        \'failed\',\n        # Scalar int32 `Tensor`. Number of iterations performed.\n        \'iterations\'\n    ])\n\n\ndef bond_curve(bond_cashflows,\n               bond_cashflow_times,\n               present_values,\n               present_values_settlement_times=None,\n               initial_discount_rates=None,\n               discount_tolerance=1e-8,\n               maximum_iterations=50,\n               validate_args=False,\n               dtype=None,\n               name=None):\n  """"""Constructs the bond discount rate curve using the Hagan-West algorithm.\n\n\n  A discount curve is a function of time which gives the interest rate that\n  applies to a unit of currency deposited today for a period of  time `t`.\n  The traded price of bonds implicitly contains the market view on the discount\n  rates. The purpose of discount curve construction is to extract this\n  information.\n\n  Suppose we have a set of `N` bonds `B_i` with increasing expiries whose market\n  prices are known.\n  Suppose also that the `i`th bond issues cashflows at times `T_{ij}` where\n  `1 <= j <= n_i` and `n_i` is the number of cashflows (including expiry)\n  for the `i`th bond.\n  Denote by `T_i` the time of final payment for the `i`th bond\n  (hence `T_i = T_{i,n_i}`). This function estimates a set of rates `r(T_i)`\n  such that when these rates are interpolated to all other cashflow times using\n  the Monotone Convex interpolation scheme (Ref [1, 2]), the computed value of\n  the bonds matches the market value of the bonds (within some tolerance).\n\n  The algorithm implemented here is based on the Monotone Convex Interpolation\n  method described by Hagan and West in Ref [1, 2].\n\n\n  ### Limitations\n\n  The fitting algorithm suggested in Hagan and West has a few limitations that\n  are worth keeping in mind.\n\n    1. Non-convexity: The implicit loss function that is minimized by the\n      procedure is non-convex. Practically this means that for a given level of\n      tolerance, it is possible to find distinct values for the discount rates\n      all of which price the given cashflows to within tolerance. Depending\n      on the initial values chosen, the procedure of Hagan-West can converge to\n      different minima.\n    2. Stability: The procedure iterates by computing the rate to expiry of\n      a bond given the approximate rates for the coupon dates. If the initial\n      guess is widely off or even if it isn\'t but the rates are artificially\n      large, it can happen that the discount factor estimated at an iteration\n      step (see Eq. 14 in Ref. [2]) is negative. Hence no real discount rate\n      can be found to continue the iterations. Additionally, it can be shown\n      that the procedure diverges if the final cashflow is not larger than\n      all the intermediate cashflows. While this situation does not arise in\n      the case of bond cashflows, it is an important consideration from a\n      mathematical perspective. For the details of the stability and\n      convergence of the scheme see the associated technical note.\n      TODO(b/139052353): Write the technical note and add a reference here.\n\n  #### Example:\n\n  The following example demonstrates the usage by building the implied curve\n  from four coupon bearing bonds.\n\n  ```python\n\n  dtype=np.float64\n\n  # These need to be sorted by expiry time.\n  cashflow_times = [\n      np.array([0.25, 0.5, 0.75, 1.0], dtype=dtype),\n      np.array([0.5, 1.0, 1.5, 2.0], dtype=dtype),\n      np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0], dtype=dtype),\n      np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0], dtype=dtype)\n  ]\n\n  cashflows = [\n      # 1 year bond with 5% three monthly coupon.\n      np.array([12.5, 12.5, 12.5, 1012.5], dtype=dtype),\n      # 2 year bond with 6% semi-annual coupon.\n      np.array([30, 30, 30, 1030], dtype=dtype),\n      # 3 year bond with 8% semi-annual coupon.\n      np.array([40, 40, 40, 40, 40, 1040], dtype=dtype),\n      # 4 year bond with 3% semi-annual coupon.\n      np.array([15, 15, 15, 15, 15, 15, 15, 1015], dtype=dtype)\n  ]\n\n  # The present values of the above cashflows.\n  pvs = np.array([\n      999.68155223943393, 1022.322872470043, 1093.9894418810143,\n      934.20885689015677\n  ], dtype=dtype)\n\n  results = bond_curve(cashflows, cashflow_times, pvs)\n\n  # The curve times are the expiries of the supplied cashflows.\n  np.testing.assert_allclose(results.times, [1.0, 2.0, 3.0, 4.0])\n\n  expected_discount_rates = np.array([5.0, 4.75, 4.53333333, 4.775],\n                                     dtype=dtype) / 100\n\n  np.testing.assert_allclose(results.discount_rates, expected_discount_rates,\n                             atol=1e-6)\n  ```\n\n  #### References:\n\n  [1]: Patrick Hagan & Graeme West. Interpolation Methods for Curve\n    Construction. Applied Mathematical Finance. Vol 13, No. 2, pp 89-129.\n    June 2006.\n  https://www.researchgate.net/publication/24071726_Interpolation_Methods_for_Curve_Construction\n  [2]: Patrick Hagan & Graeme West. Methods for Constructing a Yield Curve.\n    Wilmott Magazine, pp. 70-81. May 2008.\n  https://www.researchgate.net/profile/Patrick_Hagan3/publication/228463045_Methods_for_constructing_a_yield_curve/links/54db8cda0cf23fe133ad4d01.pdf\n\n  Args:\n    bond_cashflows: List of `Tensor`s. Each `Tensor` must be of rank 1 and of\n      the same real dtype. They may be of different sizes. Each `Tensor`\n      represents the bond cashflows defining a particular bond. The elements of\n      the list are the bonds to be used to build the curve.\n    bond_cashflow_times: List of `Tensor`s. The list must be of the same length\n      as the `bond_cashflows` and each `Tensor` in the list must be of the same\n      length as the `Tensor` at the same index in the `bond_cashflows` list.\n      Each `Tensor` must be of rank 1 and of the same dtype as the `Tensor`s in\n      `bond_cashflows` and contain strictly positive and increasing values. The\n      times of the bond cashflows for the bonds must in an ascending order.\n    present_values: List containing scalar `Tensor`s of the same dtype as\n      elements of `bond_cashflows`. The length of the list must be the same as\n      the length of `bond_cashflows`. The market price (i.e the all-in or dirty\n      price) of the bond cashflows supplied in the `bond_cashflows`.\n    present_values_settlement_times: List containing scalar `Tensor`s of the\n      same dtype as elements of `bond_cashflows`. The length of the list must be\n      the same as the length of `bond_cashflows`. The settlement times for the\n      present values is the time from now when the bond is traded to the time\n      that the purchase price is actually delivered. If not supplied, then it is\n      assumed that the settlement times are zero for every bond.\n      Default value: `None` which is equivalent to zero settlement times.\n    initial_discount_rates: Optional `Tensor` of the same dtype and shape as\n      `present_values`. The starting guess for the discount rates used to\n      initialize the iterative procedure.\n      Default value: `None`. If not supplied, the yields to maturity for the\n        bonds is used as the initial value.\n    discount_tolerance: Optional positive scalar `Tensor` of same dtype as\n      elements of `bond_cashflows`. The absolute tolerance for terminating the\n      iterations used to fit the rate curve. The iterations are stopped when the\n      estimated discounts at the expiry times of the bond_cashflows change by a\n      amount smaller than `discount_tolerance` in an iteration.\n      Default value: 1e-8.\n    maximum_iterations: Optional positive integer `Tensor`. The maximum number\n      of iterations permitted when fitting the curve.\n      Default value: 50.\n    validate_args: Optional boolean flag to enable validation of the input\n      arguments. The checks performed are: (1) There are no cashflows which\n      expire before or at the corresponding settlement time (or at time 0 if\n      settlement time is not provided). (2) Cashflow times for each bond form\n      strictly increasing sequence. (3) Final cashflow for each bond is larger\n      than any other cashflow for that bond.\n      Default value: False.\n    dtype: `tf.Dtype`. If supplied the dtype for the (elements of)\n      `bond_cashflows`, `bond_cashflow_times` and `present_values`.\n      Default value: None which maps to the default dtype inferred by TensorFlow\n        (float32).\n    name: Python str. The name to give to the ops created by this function.\n      Default value: None which maps to \'hagan_west\'.\n\n  Returns:\n    curve_builder_result: An instance of `CurveBuilderResult` containing the\n      following attributes.\n      times: Rank 1 real `Tensor`. Times for the computed discount rates. These\n        are chosen to be the expiry times of the supplied cashflows.\n      discount_rates: Rank 1 `Tensor` of the same dtype as `times`.\n        The inferred discount rates.\n      discount_factor: Rank 1 `Tensor` of the same dtype as `times`.\n        The inferred discount factors.\n      initial_discount_rates: Rank 1 `Tensor` of the same dtype as `times`. The\n        initial guess for the discount rates.\n      converged: Scalar boolean `Tensor`. Whether the procedure converged.\n        The procedure is said to have converged when the maximum absolute\n        difference in the discount factors from one iteration to the next falls\n        below the `discount_tolerance`.\n      failed: Scalar boolean `Tensor`. Whether the procedure failed. Procedure\n        may fail either because a NaN value was encountered for the discount\n        rates or the discount factors.\n      iterations: Scalar int32 `Tensor`. Number of iterations performed.\n\n  Raises:\n    ValueError: If the `cashflows` and `cashflow_times` are not all of the same\n      length greater than or equal to two. Also raised if the\n      `present_values_settlement_times` is not None and not of the same length\n      as the `cashflows`.\n    tf.errors.InvalidArgumentError: In case argument validation is requested and\n      conditions explained in the corresponding section of Args comments are not\n      met.\n  """"""\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'bond_curve\',\n      values=[\n          bond_cashflows, bond_cashflow_times, present_values,\n          present_values_settlement_times\n      ]):\n    if present_values_settlement_times is None:\n      pv_settle_times = [tf.zeros_like(pv) for pv in present_values]\n    else:\n      pv_settle_times = present_values_settlement_times\n\n    args = _convert_to_tensors(dtype, bond_cashflows, bond_cashflow_times,\n                               present_values, pv_settle_times)\n\n    bond_cashflows, bond_cashflow_times, present_values, pv_settle_times = args\n\n    # Always perform static validation.\n    _perform_static_validation(bond_cashflows, bond_cashflow_times,\n                               present_values, pv_settle_times)\n\n    control_inputs = []\n    if validate_args:\n      control_inputs = _validate_args_control_deps(bond_cashflows,\n                                                   bond_cashflow_times,\n                                                   pv_settle_times)\n\n    if initial_discount_rates is not None:\n      initial_rates = tf.convert_to_tensor(\n          initial_discount_rates, dtype=dtype, name=\'initial_rates\')\n    else:\n      # Note that we ignore the pv settlement times for this computation.\n      # This should be OK so long as the settlement times are not too large\n      # compared to the bond expiry times. Ignoring the settlement times amounts\n      # to overestimating the starting point if the true discount curve is\n      # positive.\n      initial_rates = _initial_discount_rates(\n          bond_cashflows,\n          bond_cashflow_times,\n          present_values,\n          name=\'initial_rates\')\n\n    with tf.compat.v1.control_dependencies(control_inputs):\n      return _build_discount_curve(bond_cashflows, bond_cashflow_times,\n                                   present_values, pv_settle_times,\n                                   initial_rates, discount_tolerance,\n                                   maximum_iterations)\n\n\ndef _build_discount_curve(bond_cashflows, bond_cashflow_times, present_values,\n                          pv_settle_times, initial_discount_rates,\n                          discount_tolerance, maximum_iterations):\n  """"""Estimates the discount curve.\n\n  The procedure is recursive and as follows:\n  1. Assume some initial set of discount rates/discount factors.\n    Set this as the current yield curve.\n  2. From the current yield curve, interpolate to get the discount rates\n    for each time at which bond_cashflows occur.\n  3. Using these discounts and the known bond prices, compute the discount\n    rate to expiry of each bond by inverting the bond pricing formula as\n    follows. We know that the bond price satisfies (`P` is the present value,\n    `r_i` is the discount rate to time `t_i`, `c_i` is the cashflow occurring at\n    time `t_i`.):\n\n    ```None\n      P e^{-r_0 t_0} = c_1 e^{-r_1 t_1} + ... + c_n e^{-r_n t_n}        (A)\n\n    ```\n    Assuming we have estimated r_0, r_1, r_2, ..., r_{n-1}, we can invert the\n    above equation to calculate r_n. We write this in a suggestive form\n    suitable for the implementation below.\n\n    ```None\n      -c_n z_n = -P z_0 + c_1 z_1 + c_2 z_2 + ... + c_{n-1} z_{n-1}     (B)\n\n    ```\n    where\n\n    ```None\n      z_i = e^{-r_i t_i}      (C)\n\n    ```\n    The RHS of Eq. (B) looks like the PV of cashflows\n    `[-P, c_1, c_2, ... c_{n-1}]` paid out at times `[t_0, t_1, ..., t_{n-1}]`.\n\n    Concatenate these ""synthetic"" cashflow times for each bond:\n\n    `Ts = [t1_0, t1_1, ... t1_{n1-1}] + [t2_0, t2_1, ... t2_{n2-1}] ...`\n\n    Also concatenate the synthetic bond cashflows as:\n\n    `Cs = [-P1, c1_1, ..., c1_{n1-1}] + [-P2, c2_1, ..., c2_{n2-1}] ...`\n\n    Then compute `Rs = InterpolateRates[Ts], Zs = exp(-Rs * Ts)`\n\n    Let `Zns = [z_n1, z_n2, ... ], Cns = [c1_n, c2_n, ...]` be the discount\n    factors to expiry and the final cashflow of each bond.\n    We can derive `Zns = - SegmentSum(Cs * Zs) / Cns`.\n\n    From that, we get Rns = -log(Zns) / Tns.\n    Using this as the next guess for the discount rates and we repeat the\n    procedure from Step (1) until convergence.\n\n  Args:\n    bond_cashflows: List of `Tensor`s. Each `Tensor` must be of rank 1 and of\n      the same real dtype. They may be of different sizes. Each `Tensor`\n      represents the bond cashflows defining a particular bond. The elements of\n      the list are the bonds to be used to build the curve.\n    bond_cashflow_times: List of `Tensor`s. The list must be of the same length\n      as the `bond_cashflows` and each `Tensor` in the list must be of the same\n      length as the `Tensor` at the same index in the `bond_cashflows` list.\n      Each `Tensor` must be of rank 1 and of the same dtype as the `Tensor`s in\n      `bond_cashflows` and contain strictly positive and increasing values. The\n      times of the bond cashflows for the bonds must in an ascending order.\n    present_values: List containing scalar `Tensor`s of the same dtype as\n      elements of `bond_cashflows`. The length of the list must be the same as\n      the length of `bond_cashflows`. The market price (i.e the all-in or dirty\n      price) of the bond cashflows supplied in the `bond_cashflows`.\n    pv_settle_times:   List containing scalar `Tensor`s of the same dtype as\n      elements of `bond_cashflows`. The length of the list must be the same as\n      the length of `bond_cashflows`. The settlement times for the present\n      values is the time from now when the bond is traded to the time that the\n      purchase price is actually delivered.\n    initial_discount_rates: Rank 1 `Tensor` of same shape and dtype as\n      `pv_settle_times`. The initial guess for the discount rates to bond expiry\n      times.\n    discount_tolerance: Positive scalar `Tensor` of same dtype as\n      `initial_discount_factors`. The absolute tolerance for terminating the\n      iterations used to fit the rate curve. The iterations are stopped when the\n      estimated discounts at the expiry times of the bond cashflows change by a\n      amount smaller than `discount_tolerance` in an iteration.\n    maximum_iterations: Positive scalar `tf.int32` `Tensor`. The maximum number\n      of iterations permitted.\n\n  Returns:\n    curve_builder_result: An instance of `CurveBuilderResult` containing the\n      following attributes.\n      times: Rank 1 real `Tensor`. Times for the computed discount rates.\n      discount_rates: Rank 1 `Tensor` of the same dtype as `times`.\n        The inferred discount rates.\n      discount_factor: Rank 1 `Tensor` of the same dtype as `times`.\n        The inferred discount factors.\n      initial_discount_rates: Rank 1 `Tensor` of the same dtype as `times`. The\n        initial guess for the discount rates.\n      converged: Scalar boolean `Tensor`. Whether the procedure converged.\n        The procedure is said to have converged when the maximum absolute\n        difference in the discount factors from one iteration to the next falls\n        below the `discount_tolerance`.\n      failed: Scalar boolean `Tensor`. Whether the procedure failed. Procedure\n        may fail either because a NaN value was encountered for the discount\n        rates or the discount factors.\n      iterations: Scalar `tf.int32` `Tensor`. Number of iterations performed.\n  """"""\n  calc_bond_cashflows = []  # Cs\n  calc_times = []  # Ts\n  expiry_times = []  # Tns\n  expiry_bond_cashflows = []  # Cns\n  calc_groups = []\n  num_bonds = len(bond_cashflows)\n  for i in range(num_bonds):\n    calc_bond_cashflows.extend([[-present_values[i]], bond_cashflows[i][:-1]])\n    calc_times.extend([[pv_settle_times[i]], bond_cashflow_times[i][:-1]])\n    expiry_times.append(bond_cashflow_times[i][-1])\n    expiry_bond_cashflows.append(bond_cashflows[i][-1])\n    calc_groups.append(tf.fill(tf.shape(bond_cashflows[i]), i))\n\n  calc_bond_cashflows = tf.concat(calc_bond_cashflows, axis=0)\n  calc_times = tf.concat(calc_times, axis=0)\n  expiry_times = tf.stack(expiry_times, axis=0)\n  expiry_bond_cashflows = tf.stack(expiry_bond_cashflows, axis=0)\n  calc_groups = tf.concat(calc_groups, axis=0)\n\n  def one_step(converged, failed, iteration, expiry_discounts):\n    """"""One step of the iteration.""""""\n    expiry_rates = -tf.math.log(expiry_discounts) / expiry_times\n    failed = tf.math.reduce_any(\n        tf.math.is_nan(expiry_rates) | tf.math.is_nan(expiry_discounts))\n    calc_rates = monotone_convex.interpolate_yields(\n        calc_times, expiry_times, yields=expiry_rates)\n    calc_discounts = tf.math.exp(-calc_rates * calc_times)\n    next_expiry_discounts = -tf.math.segment_sum(\n        calc_bond_cashflows * calc_discounts,\n        calc_groups) / expiry_bond_cashflows\n    discount_diff = tf.math.abs(next_expiry_discounts - expiry_discounts)\n    converged = (~tf.math.reduce_any(tf.math.is_nan(discount_diff)) &\n                 (tf.math.reduce_max(discount_diff) < discount_tolerance))\n    return converged, failed, iteration + 1, next_expiry_discounts\n\n  def cond(converged, failed, iteration, expiry_discounts):\n    del expiry_discounts, iteration\n    # Note we do not need to check iteration count here because that\n    # termination mode is imposed by the maximum_iterations parameter in the\n    # while loop.\n    return ~tf.math.logical_or(converged, failed)\n\n  initial_discount_factors = tf.math.exp(-initial_discount_rates * expiry_times)\n  initial_vals = (False, False, 0, initial_discount_factors)\n  loop_result = tf.while_loop(\n      cond, one_step, initial_vals, maximum_iterations=maximum_iterations)\n  discount_factors = loop_result[-1]\n  discount_rates = -tf.math.log(discount_factors) / expiry_times\n  results = CurveBuilderResult(\n      times=expiry_times,\n      discount_rates=discount_rates,\n      discount_factors=discount_factors,\n      initial_discount_rates=initial_discount_rates,\n      converged=loop_result[0],\n      failed=loop_result[1],\n      iterations=loop_result[2])\n  return results\n\n\ndef _initial_discount_rates(bond_cashflows,\n                            bond_cashflow_times,\n                            present_values,\n                            name=\'initial_discount_rates\'):\n  """"""Constructs a guess for the initial rates as the yields to maturity.""""""\n  n = len(bond_cashflows)\n  groups = []\n  for i in range(n):\n    groups.append(tf.fill(tf.shape(bond_cashflows[i]), i))\n  bond_cashflows = tf.concat(bond_cashflows, axis=0)\n  bond_cashflow_times = tf.concat(bond_cashflow_times, axis=0)\n  groups = tf.concat(groups, axis=0)\n  return cashflows.yields_from_pv(\n      bond_cashflows,\n      bond_cashflow_times,\n      present_values,\n      groups=groups,\n      name=name)\n\n\ndef _perform_static_validation(bond_cashflows, bond_cashflow_times,\n                               present_values, pv_settle_times):\n  """"""Performs static validation on the arguments.""""""\n  if len(bond_cashflows) != len(bond_cashflow_times):\n    raise ValueError(\n        \'Cashflow times and bond_cashflows must be of the same length.\'\n        \'bond_cashflows are of size\'\n        \' {} and times of size {}\'.format(\n            len(bond_cashflows), len(bond_cashflow_times)))\n\n  if len(bond_cashflows) != len(present_values):\n    raise ValueError(\n        \'Present values and bond_cashflows must be of the same length.\'\n        \'bond_cashflows are of size\'\n        \' {} and PVs of size {}\'.format(\n            len(bond_cashflows), len(present_values)))\n\n  if len(present_values) != len(pv_settle_times):\n    raise ValueError(\n        \'Present value settlement times and present values must be of\'\n        \'the same length. Settlement times are of size\'\n        \' {} and PVs of size {}\'.format(\n            len(pv_settle_times), len(present_values)))\n\n  if len(bond_cashflows) < 2:\n    raise ValueError(\n        \'At least two bonds must be supplied to calibrate the curve.\'\n        \'Found {}.\'.format(len(bond_cashflows)))\n\n\ndef _validate_args_control_deps(bond_cashflows, bond_cashflow_times,\n                                pv_settle_times):\n  """"""Returns assertions for the validity of the arguments.""""""\n  cashflows_are_strictly_increasing = []\n  cashflow_after_settlement = []\n  final_cashflow_is_the_largest = []\n  for bond_index, bond_cashflow in enumerate(bond_cashflows):\n    times = bond_cashflow_times[bond_index]\n    time_difference = times[1:] - times[:-1]\n    cashflows_are_strictly_increasing.append(\n        tf.debugging.assert_positive(time_difference))\n    cashflow_after_settlement.append(\n        tf.debugging.assert_greater(times[0], pv_settle_times[bond_index]))\n    final_cashflow_is_the_largest.append(\n        tf.debugging.assert_greater(\n            tf.fill(tf.shape(bond_cashflow[:-1]),\n                    bond_cashflow[-1]), bond_cashflow[:-1]))\n  return (cashflow_after_settlement + cashflows_are_strictly_increasing +\n          final_cashflow_is_the_largest)\n\n\ndef _convert_to_tensors(dtype, bond_cashflows, bond_cashflow_times,\n                        present_values, pv_settle_times):\n  """"""Converts each element of the supplied lists to a tensor.""""""\n\n  bond_cashflows = [\n      tf.convert_to_tensor(\n          cashflow, dtype=dtype, name=\'cashflows_bond_{}\'.format(i))\n      for i, cashflow in enumerate(bond_cashflows)\n  ]\n  bond_cashflow_times = [\n      tf.convert_to_tensor(\n          cashflow_times, dtype=dtype, name=\'cashflow_times_bond_{}\'.format(i))\n      for i, cashflow_times in enumerate(bond_cashflow_times)\n  ]\n  present_values = [\n      tf.convert_to_tensor(pv, dtype=dtype, name=\'pv_bond_{}\'.format(i))\n      for i, pv in enumerate(present_values)\n  ]\n  pv_settle_times = [\n      tf.convert_to_tensor(\n          pv_time, dtype=dtype, name=\'pv_settle_time_bond_{}\'.format(i))\n      for i, pv_time in enumerate(pv_settle_times)\n  ]\n\n  return bond_cashflows, bond_cashflow_times, present_values, pv_settle_times\n'"
tf_quant_finance/rates/hagan_west/bond_curve_test.py,11,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for bond_curve.""""""\n\nimport math\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\nfrom tf_quant_finance.rates.hagan_west import bond_curve\nfrom tf_quant_finance.rates.hagan_west import monotone_convex\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass BondCurveTest(tf.test.TestCase):\n\n  def test_cashflow_times_cashflow_before_settelment_error(self):\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        self.evaluate(\n            bond_curve.bond_curve(\n                bond_cashflows=[\n                    np.array([12.5, 12.5, 12.5, 1012.5], dtype=dtype),\n                    np.array([30.0, 30.0, 30.0, 1030.0], dtype=dtype)\n                ],\n                bond_cashflow_times=[\n                    np.array([0.25, 0.5, 0.75, 1.0], dtype=dtype),\n                    np.array([0.5, 1.0, 1.5, 2.0], dtype=dtype)\n                ],\n                present_values=np.array([999.0, 1022.0], dtype=dtype),\n                present_values_settlement_times=np.array([0.25, 0.25],\n                                                         dtype=dtype),\n                validate_args=True,\n                dtype=dtype))\n\n  def test_cashflow_times_are_strongly_ordered_error(self):\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        self.evaluate(\n            bond_curve.bond_curve(\n                bond_cashflows=[\n                    np.array([12.5, 12.5, 12.5, 1012.5], dtype=dtype),\n                    np.array([30.0, 30.0, 30.0, 1030.0], dtype=dtype)\n                ],\n                bond_cashflow_times=[\n                    np.array([0.25, 0.5, 0.75, 1.0], dtype=dtype),\n                    np.array([0.5, 1.0, 1.5, 1.5], dtype=dtype)\n                ],\n                present_values=np.array([999.0, 1022.0], dtype=dtype),\n                validate_args=True,\n                dtype=dtype))\n\n  def test_final_cashflow_is_the_largest_error(self):\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        self.evaluate(\n            bond_curve.bond_curve(\n                bond_cashflows=[\n                    np.array([12.5, 12.5, 12.5, 1012.5], dtype=dtype),\n                    np.array([30.0, 30.0, 30.0, 3.0], dtype=dtype)\n                ],\n                bond_cashflow_times=[\n                    np.array([0.25, 0.5, 0.75, 1.0], dtype=dtype),\n                    np.array([0.5, 1.0, 1.5, 2.0], dtype=dtype)\n                ],\n                present_values=np.array([999.0, 1022.0], dtype=dtype),\n                validate_args=True,\n                dtype=dtype))\n\n  def test_correctness(self):\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      cashflows = [\n          # 1 year bond with 5% three monthly coupon.\n          np.array([12.5, 12.5, 12.5, 1012.5], dtype=dtype),\n          # 2 year bond with 6% semi-annual coupon.\n          np.array([30, 30, 30, 1030], dtype=dtype),\n          # 3 year bond with 8% semi-annual coupon.\n          np.array([40, 40, 40, 40, 40, 1040], dtype=dtype),\n          # 4 year bond with 3% semi-annual coupon.\n          np.array([15, 15, 15, 15, 15, 15, 15, 1015], dtype=dtype)\n      ]\n      cashflow_times = [\n          np.array([0.25, 0.5, 0.75, 1.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0], dtype=dtype)\n      ]\n      pvs = np.array([\n          999.68155223943393, 1022.322872470043, 1093.9894418810143,\n          934.20885689015677\n      ],\n                     dtype=dtype)\n      results = self.evaluate(\n          bond_curve.bond_curve(\n              cashflows, cashflow_times, pvs, validate_args=True, dtype=dtype))\n\n      np.testing.assert_allclose(results.times, [1.0, 2.0, 3.0, 4.0])\n\n      self.assertTrue(results.converged)\n      self.assertFalse(results.failed)\n      expected_discount_rates = np.array([5.0, 4.75, 4.53333333, 4.775],\n                                         dtype=dtype) / 100\n      expected_discount_factors = np.exp(-expected_discount_rates *\n                                         [1.0, 2.0, 3.0, 4.0])\n      np.testing.assert_allclose(\n          results.discount_rates, expected_discount_rates, atol=1e-6)\n      np.testing.assert_allclose(\n          results.discount_factors, expected_discount_factors, atol=1e-6)\n\n  def test_unstable(self):\n    """"""Demonstrates the instability of Hagan West for extreme cases.""""""\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      cashflows = [\n          # 1 year bond with 5% three monthly coupon.\n          np.array([12.5, 12.5, 12.5, 1012.5], dtype=dtype),\n          # 2 year bond with 6% semi-annual coupon.\n          np.array([30, 30, 30, 1030], dtype=dtype),\n          # 3 year bond with 8% semi-annual coupon.\n          np.array([40, 40, 40, 40, 40, 1040], dtype=dtype),\n          # 4 year bond with 3% semi-annual coupon.\n          np.array([15, 15, 15, 15, 15, 15, 15, 1015], dtype=dtype)\n      ]\n      cashflow_times = [\n          np.array([0.25, 0.5, 0.75, 1.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0], dtype=dtype)\n      ]\n      # Computed with discount rates of [5.0, 4.75, 4.53333333, 4.775]\n      # which are 100 times the values in the previous test case.\n      pvs = np.array([\n          11.561316110080888, 2.6491572753698067, 3.4340789041846866,\n          1.28732090544209\n      ],\n                     dtype=dtype)\n      true_discount_rates = np.array([5.0, 4.75, 4.53333333, 4.775],\n                                     dtype=dtype)\n      # Check failure with default initial rates.\n      results_default = self.evaluate(\n          bond_curve.bond_curve(\n              cashflows,\n              cashflow_times,\n              pvs,\n              maximum_iterations=100,\n              validate_args=True,\n              dtype=dtype))\n      self.assertFalse(results_default.converged)\n      self.assertTrue(results_default.failed)\n      self.assertFalse(np.isnan(results_default.discount_rates[0]))\n      self.assertTrue(np.isnan(results_default.discount_rates[1]))\n\n      # It even fails if we underestimate the result even marginally.\n      # However the behaviour is different if we start above the true values.\n      # See next test.\n      results_close = self.evaluate(\n          bond_curve.bond_curve(\n              cashflows,\n              cashflow_times,\n              pvs,\n              initial_discount_rates=true_discount_rates * 0.9999,\n              maximum_iterations=100))\n      self.assertFalse(results_close.converged)\n      self.assertTrue(results_close.failed)\n      self.assertFalse(np.isnan(results_close.discount_rates[0]))\n      self.assertFalse(np.isnan(results_close.discount_rates[1]))\n      self.assertTrue(np.isnan(results_close.discount_rates[2]))\n\n  def test_non_convex(self):\n    """"""Demonstrates the nonconvexity of Hagan West for extreme cases.""""""\n    # This is the same example as the previous one but with different starting\n    # point.\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      cashflows = [\n          # 1 year bond with 5% three monthly coupon.\n          np.array([12.5, 12.5, 12.5, 1012.5], dtype=dtype),\n          # 2 year bond with 6% semi-annual coupon.\n          np.array([30, 30, 30, 1030], dtype=dtype),\n          # 3 year bond with 8% semi-annual coupon.\n          np.array([40, 40, 40, 40, 40, 1040], dtype=dtype),\n          # 4 year bond with 3% semi-annual coupon.\n          np.array([15, 15, 15, 15, 15, 15, 15, 1015], dtype=dtype)\n      ]\n      cashflow_times = [\n          np.array([0.25, 0.5, 0.75, 1.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0], dtype=dtype)\n      ]\n      # Computed with discount rates of [5.0, 4.75, 4.53333333, 4.775]\n      # which are 100 times the values in the previous test case.\n      pvs = np.array([\n          11.561316110080888, 2.6491572753698067, 3.4340789041846866,\n          1.28732090544209\n      ],\n                     dtype=dtype)\n      true_discount_rates = np.array([5.0, 4.75, 4.53333333, 4.775],\n                                     dtype=dtype)\n      initial_rates = true_discount_rates * 1.01\n      # Check failure with default initial rates.\n      results = self.evaluate(\n          bond_curve.bond_curve(\n              cashflows,\n              cashflow_times,\n              pvs,\n              initial_discount_rates=initial_rates,\n              maximum_iterations=100,\n              validate_args=True,\n              dtype=dtype))\n      self.assertTrue(results.converged)\n      self.assertFalse(results.failed)\n      # It converges to a different set of rates.\n      np.testing.assert_allclose(\n          results.discount_rates,\n          [4.9610328, 4.17662715, 2.84038942, 2.38737021],\n          atol=1e-6)\n\n      # However, the actual bond prices with the returned rates are indeed\n      # correct.\n      implied_pvs = self.evaluate(\n          _compute_pv(cashflows, cashflow_times, results.discount_rates,\n                      np.array([1.0, 2.0, 3.0, 4.0], dtype=dtype)))\n\n      np.testing.assert_allclose(implied_pvs, pvs, rtol=1e-5)\n\n  def test_flat_curve(self):\n    """"""Checks that flat curves work.""""""\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      cashflows = [\n          # 1 year bond with 5% three monthly coupon.\n          np.array([12.5, 12.5, 12.5, 1012.5], dtype=dtype),\n          # 2 year bond with 6% semi-annual coupon.\n          np.array([30, 30, 30, 1030], dtype=dtype),\n          # 3 year bond with 8% semi-annual coupon.\n          np.array([40, 40, 40, 40, 40, 1040], dtype=dtype),\n          # 4 year bond with 3% semi-annual coupon.\n          np.array([15, 15, 15, 15, 15, 15, 15, 1015], dtype=dtype)\n      ]\n      cashflow_times = [\n          np.array([0.25, 0.5, 0.75, 1.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0], dtype=dtype)\n      ]\n      # Computed with a flat curve of 15%.\n      pvs = np.array([906.27355957, 840.6517334, 823.73626709, 635.7076416],\n                     dtype=dtype)\n      true_discount_rates = np.array([0.15] * 4, dtype=dtype)\n      results = self.evaluate(\n          bond_curve.bond_curve(\n              cashflows, cashflow_times, pvs, validate_args=True, dtype=dtype))\n      self.assertTrue(results.converged)\n      self.assertFalse(results.failed)\n      np.testing.assert_allclose(\n          results.discount_rates, true_discount_rates, atol=1e-6)\n\n  def test_negative_rates(self):\n    """"""Checks that method works even if the actual rates are negative.""""""\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      cashflows = [\n          # 1 year bond with 5% three monthly coupon.\n          np.array([12.5, 12.5, 12.5, 1012.5], dtype=dtype),\n          # 2 year bond with 6% semi-annual coupon.\n          np.array([30, 30, 30, 1030], dtype=dtype),\n          # 3 year bond with 8% semi-annual coupon.\n          np.array([40, 40, 40, 40, 40, 1040], dtype=dtype),\n          # 4 year bond with 3% semi-annual coupon.\n          np.array([15, 15, 15, 15, 15, 15, 15, 1015], dtype=dtype)\n      ]\n      cashflow_times = [\n          np.array([0.25, 0.5, 0.75, 1.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0], dtype=dtype),\n          np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0], dtype=dtype)\n      ]\n      pvs = np.array(\n          [1029.54933442, 1097.95320227, 1268.65376174, 1249.84175959],\n          dtype=dtype)\n      true_discount_rates = np.array([0.02, 0.01, -0.01, -0.03], dtype=dtype)\n      results = self.evaluate(\n          bond_curve.bond_curve(\n              cashflows, cashflow_times, pvs, validate_args=True, dtype=dtype))\n      self.assertTrue(results.converged)\n      self.assertFalse(results.failed)\n      np.testing.assert_allclose(\n          results.discount_rates, true_discount_rates, atol=1e-4)\n\n  def test_negative_forwards(self):\n    """"""Checks that method works if the rates are positive by fwds are not.""""""\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      true_discount_rates = np.array([0.12, 0.09, 0.02, 0.01, 0.01318182],\n                                     dtype=dtype)\n      # Note the implied forward rates for this rate curve are:\n      # [0.12, 0.06, -0.05, -0.01, 0.02]\n      cashflows = [\n          np.array([1.2, 10.], dtype=dtype),\n          np.array([1.1, 2.2, 1.4, 15.5], dtype=dtype),\n          np.array([1.22, 0.45, 2.83, 96.0], dtype=dtype),\n          np.array([12.33, 9.84, 1.15, 11.87, 0.66, 104.55], dtype=dtype),\n          np.array([5.84, 0.23, 5.23, 114.95], dtype=dtype)\n      ]\n      cashflow_times = [\n          np.array([0.15, 0.25], dtype=dtype),\n          np.array([0.1, 0.2, 0.4, 0.5], dtype=dtype),\n          np.array([0.22, 0.45, 0.93, 1.0], dtype=dtype),\n          np.array([0.33, 0.84, 0.92, 1.22, 1.45, 1.5], dtype=dtype),\n          np.array([0.43, 0.77, 1.3, 2.2], dtype=dtype)\n      ]\n      pvs = np.array(\n          [10.88135262, 19.39268844, 98.48426722, 137.91938533, 122.63546542],\n          dtype=dtype)\n      results = self.evaluate(\n          bond_curve.bond_curve(\n              cashflows, cashflow_times, pvs, validate_args=True, dtype=dtype))\n      self.assertTrue(results.converged)\n      self.assertFalse(results.failed)\n      self.assertEqual(results.iterations, 6)\n      np.testing.assert_allclose(\n          results.discount_rates, true_discount_rates, atol=1e-6)\n      np.testing.assert_allclose(\n          results.times, [0.25, 0.5, 1., 1.5, 2.2], atol=1e-6)\n\n  def test_zero_coupon_bond(self):\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      cashflows = [\n          # 6 months bond with no coupons.\n          np.array([1020], dtype=dtype),\n          # 1 year bond with 5% semi-annual coupon.\n          np.array([25, 1025], dtype=dtype),\n          # 2 year bond with 8% annual coupon.\n          np.array([80, 1080], dtype=dtype),\n          # 3 year bond with 3% annual coupon.\n          np.array([30, 30, 1030], dtype=dtype)\n      ]\n      cashflow_times = [\n          np.array([0.5], dtype=dtype),\n          np.array([0.5, 1.0], dtype=dtype),\n          np.array([1.0, 2.0], dtype=dtype),\n          np.array([1.0, 2.0, 3.0], dtype=dtype)\n      ]\n      pvs = np.array([1000.0, 1000.0, 1000.0, 1000.0], dtype=dtype)\n      # We can calculate discount rates going step-by-step.\n      r1 = -math.log(pvs[0] / cashflows[0][0]) / cashflow_times[0]\n      r2 = -(\n          math.log(\n              (pvs[1] - cashflows[1][0] * math.exp(-r1 * cashflow_times[1][0]))\n              / cashflows[1][1]) / cashflow_times[1][1])\n      r3 = -(\n          math.log(\n              (pvs[2] - cashflows[2][0] * math.exp(-r2 * cashflow_times[2][0]))\n              / cashflows[2][1]) / cashflow_times[2][1])\n      r4 = -(\n          math.log(\n              (pvs[3] - cashflows[3][0] * math.exp(-r2 * cashflow_times[3][0]) -\n               cashflows[3][1] * math.exp(-r3 * cashflow_times[3][1])) /\n              cashflows[3][2]) / cashflow_times[3][2])\n      true_discount_rates = np.array([r1, r2, r3, r4], dtype=dtype)\n\n      results = self.evaluate(\n          bond_curve.bond_curve(\n              cashflows, cashflow_times, pvs, validate_args=True, dtype=dtype))\n      self.assertTrue(results.converged)\n      self.assertFalse(results.failed)\n      self.assertEqual(results.iterations, 4)\n      np.testing.assert_allclose(\n          results.discount_rates, true_discount_rates, atol=1e-6)\n      np.testing.assert_allclose(results.times, [0.5, 1.0, 2.0, 3.0], atol=1e-6)\n\n  def test_only_zero_coupon_bonds(self):\n    dtypes = [np.float64, np.float32]\n    for dtype in dtypes:\n      cashflows = [\n          # 1 year bond with no coupons.\n          np.array([1010], dtype=dtype),\n          # 2 year bond with no coupons.\n          np.array([1030], dtype=dtype),\n          # 3 year bond with no coupons.\n          np.array([1020], dtype=dtype),\n          # 4 year bond with no coupons.\n          np.array([1040], dtype=dtype)\n      ]\n      cashflow_times = [\n          np.array([1.0], dtype=dtype),\n          np.array([2.0], dtype=dtype),\n          np.array([3.0], dtype=dtype),\n          np.array([4.0], dtype=dtype)\n      ]\n      true_discount_rates = np.array([0.001, 0.2, 0.03, 0.0], dtype=dtype)\n      pvs = np.array([(cashflows[i][0] * math.exp(-rate * cashflow_times[i][0]))\n                      for i, rate in enumerate(true_discount_rates)],\n                     dtype=dtype)\n      results = self.evaluate(\n          bond_curve.bond_curve(\n              cashflows, cashflow_times, pvs, validate_args=True, dtype=dtype))\n      self.assertTrue(results.converged)\n      self.assertFalse(results.failed)\n      self.assertEqual(results.iterations, 1)\n      np.testing.assert_allclose(\n          results.discount_rates, true_discount_rates, atol=1e-6)\n      np.testing.assert_allclose(results.times, [1.0, 2.0, 3.0, 4.0], atol=1e-6)\n\n\ndef _compute_pv(cashflows, cashflow_times, reference_rates, reference_times):\n  times = tf.concat(cashflow_times, axis=0)\n  groups = tf.concat([\n      tf.zeros_like(cashflow, dtype=tf.int32) + i\n      for i, cashflow in enumerate(cashflows)\n  ],\n                     axis=0)\n  rates = monotone_convex.interpolate_yields(\n      times, reference_times, yields=reference_rates)\n  discounts = tf.math.exp(-times * rates)\n  cashflows = tf.concat(cashflows, axis=0)\n  return tf.math.segment_sum(discounts * cashflows, groups)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/rates/hagan_west/monotone_convex.py,59,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""The monotone convex interpolation method.\n\nThe monotone convex method is a scheme devised by Hagan and West (Ref [1]). It\nis a commonly used method to interpolate interest rate yield curves. For\nmore details see Refs [1, 2].\n\nIt is important to point out that the monotone convex method *does not* solve\nthe standard interpolation problem but a modified one as described below.\n\nSuppose we are given a strictly increasing sequence of scalars (which we will\nrefer to as time) `[t_1, t_2, ... t_n]` and a set of values\n`[f_1, f_2, ... f_n]`.\nThe aim is to find a function `f(t)` defined on the interval `[0, t_n]` which\nsatisfies (in addition to continuity and positivity conditions, see Section 6\nof Ref [2] for more details) the following\n\n```\n  Integral[f(u), t_{i-1} <= u <= t_i] = f_i,  with t_0 = 0\n\n```\n\nIn the context of interest rate curve building, `f(t)` corresponds to the\ninstantaneous forward rate at time `t` and the `f_i` correspond to the\ndiscrete forward rates that apply to the time period `[t_{i-1}, t_i]`.\n\nThis implementation of the method currently supports batching along the\ninterpolation times but not along the interpolated curves (i.e. it is possible\nto evaluate the `f(t)` for `t` as a vector of times but not build multiple\ncurves at the same time).\n\n\n#### References:\n\n[1]: Patrick Hagan & Graeme West. Interpolation Methods for Curve Construction.\n  Applied Mathematical Finance. Vol 13, No. 2, pp 89-129. June 2006.\n  https://www.researchgate.net/publication/24071726_Interpolation_Methods_for_Curve_Construction\n[2]: Patrick Hagan & Graeme West. Methods for Constructing a Yield Curve.\n  Wilmott Magazine, pp. 70-81. May 2008.\n""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math import piecewise\nfrom tf_quant_finance.math.diff_ops import diff\nfrom tf_quant_finance.rates import forwards\n\n\ndef interpolate(times,\n                interval_values,\n                interval_times,\n                validate_args=False,\n                dtype=None,\n                name=None):\n  """"""Performs the monotone convex interpolation.\n\n  The monotone convex method is a scheme devised by Hagan and West (Ref [1]). It\n  is a commonly used method to interpolate interest rate yield curves. For\n  more details see Refs [1, 2].\n\n  It is important to point out that the monotone convex method *does not* solve\n  the standard interpolation problem but a modified one as described below.\n\n  Suppose we are given a strictly increasing sequence of scalars (which we will\n  refer to as time) `[t_1, t_2, ... t_n]` and a set of values\n  `[f_1, f_2, ... f_n]`.\n  The aim is to find a function `f(t)` defined on the interval `[0, t_n]` which\n  satisfies (in addition to continuity and positivity conditions) the following\n\n  ```None\n    Integral[f(u), t_{i-1} <= u <= t_i] = f_i,  with t_0 = 0\n\n  ```\n\n  In the context of interest rate curve building, `f(t)` corresponds to the\n  instantaneous forward rate at time `t` and the `f_i` correspond to the\n  discrete forward rates that apply to the time period `[t_{i-1}, t_i]`.\n  Furthermore, the integral of the forward curve is related to the yield curve\n  by\n\n  ```None\n    Integral[f(u), 0 <= u <= t] = r(t) * t\n\n  ```\n\n  where `r(t)` is the interest rate that applies between `[0, t]` (the yield of\n  a zero coupon bond paying a unit of currency at time `t`).\n\n  This function computes both the interpolated value and the integral along\n  the segment containing the supplied time. Specifically, given a time `t` such\n  that `t_k <= t <= t_{k+1}`, this function computes the interpolated value\n  `f(t)` and the value `Integral[f(u), t_k <= u <= t]`.\n\n  This implementation of the method currently supports batching along the\n  interpolation times but not along the interpolated curves (i.e. it is possible\n  to evaluate the `f(t)` for `t` as a vector of times but not build multiple\n  curves at the same time).\n\n  #### Example\n\n  ```python\n  interval_times = tf.constant([0.25, 0.5, 1.0, 2.0, 3.0], dtype=dtype)\n  interval_values = tf.constant([0.05, 0.051, 0.052, 0.053, 0.055],\n                                dtype=dtype)\n  times = tf.constant([0.25, 0.5, 1.0, 2.0, 3.0, 1.1], dtype=dtype)\n  # Returns the following two values:\n  # interpolated = [0.0505, 0.05133333, 0.05233333, 0.054, 0.0555, 0.05241]\n  # integrated =  [0, 0, 0, 0, 0.055, 0.005237]\n  # Note that the first four integrated values are zero. This is because\n  # those interpolation time are at the start of their containing interval.\n  # The fourth value (i.e. at 3.0) is not zero because this is the last\n  # interval (i.e. it is the integral from 2.0 to 3.0).\n  interpolated, integrated = interpolate(\n      times, interval_values, interval_times)\n  ```\n\n  #### References:\n\n  [1]: Patrick Hagan & Graeme West. Interpolation Methods for Curve\n    Construction. Applied Mathematical Finance. Vol 13, No. 2, pp 89-129.\n    June 2006.\n    https://www.researchgate.net/publication/24071726_Interpolation_Methods_for_Curve_Construction\n  [2]: Patrick Hagan & Graeme West. Methods for Constructing a Yield Curve.\n    Wilmott Magazine, pp. 70-81. May 2008.\n\n  Args:\n    times: Non-negative rank 1 `Tensor` of any size. The times for which the\n      interpolation has to be performed.\n    interval_values: Rank 1 `Tensor` of the same shape and dtype as\n      `interval_times`. The values associated to each of the intervals specified\n      by the `interval_times`. Must have size at least 2.\n    interval_times: Strictly positive rank 1 `Tensor` of real dtype containing\n      increasing values. The endpoints of the intervals (i.e. `t_i` above.).\n      Note that the left end point of the first interval is implicitly assumed\n      to be 0. Must have size at least 2.\n    validate_args: Python bool. If true, adds control dependencies to check that\n      the `times` are bounded by the `interval_endpoints`.\n      Default value: False\n    dtype: `tf.Dtype` to use when converting arguments to `Tensor`s. If not\n      supplied, the default Tensorflow conversion will take place. Note that\n      this argument does not do any casting.\n      Default value: None.\n    name: Python `str` name prefixed to Ops created by this class.\n      Default value: None which is mapped to the default name \'interpolation\'.\n\n  Returns:\n    A 2-tuple containing\n      interpolated_values: Rank 1 `Tensor` of the same size and dtype as the\n        `times`. The interpolated values at the supplied times.\n      integrated_values: Rank 1 `Tensor` of the same size and dtype as the\n        `times`. The integral of the interpolated function. The integral is\n        computed from the largest interval time that is smaller than the time\n        up to the given time.\n  """"""\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'interpolate\',\n      values=[times, interval_times, interval_values]):\n    times = tf.convert_to_tensor(times, dtype=dtype, name=\'times\')\n    interval_times = tf.convert_to_tensor(\n        interval_times, dtype=dtype, name=\'interval_times\')\n    interval_values = tf.convert_to_tensor(\n        interval_values, dtype=dtype, name=\'interval_values\')\n    control_deps = []\n    if validate_args:\n      control_deps = [\n          tf.compat.v1.debugging.assert_non_negative(times),\n          tf.compat.v1.debugging.assert_positive(interval_times)\n      ]\n    with tf.compat.v1.control_dependencies(control_deps):\n      # Step 1: Find the values at the endpoints.\n      endpoint_values = _interpolate_adjacent(interval_times, interval_values)\n      endpoint_times = tf.concat([[0.0], interval_times], axis=0)\n      intervals = piecewise.find_interval_index(\n          times, endpoint_times, last_interval_is_closed=True)\n      # Comparing to the notation used in the paper:\n      # f_left -> f_{i-1}\n      # f_right -> f_i\n      # t_left -> t_{i-1}\n      # t_right -> t_i\n      # fd -> f^d_i\n      # g0 -> g0\n      # g1 -> g1\n      # g1plus2g0 -> g1 + 2 g0 (boundary line A)\n      # g0plus2g1 -> g0 + 2 g1 (boundary line B)\n      # x -> x\n      f_left = tf.gather(endpoint_values, intervals)\n      f_right = tf.gather(endpoint_values, intervals + 1)\n      # fd is the discrete forward associated to each interval.\n      fd = tf.gather(interval_values, intervals)\n      t_left = tf.gather(endpoint_times, intervals)\n      t_right = tf.gather(endpoint_times, intervals + 1)\n      interval_lengths = (t_right - t_left)\n      x = (times - t_left) / interval_lengths\n\n      # TODO(b/140410758): The calculation below can be done more efficiently\n      # if we instead do the following:\n      # 1. Subdivide the regions further so that each subregion corresponds\n      #   to a single quadratic in x. (Region 2, 3 and 4 get divided into 2\n      #   pieces for a total of 7 cases.\n      # 2. For each interval (i.e. [t_i, t{i+1}]) the case that applies to\n      #   a point falling in that region can be decided and the corresponding\n      #   quadratic coefficients computed once and for all.\n      # 3. The above information is built once for the supplied forwards.\n      # 4. Given the above information and a set of times to interpolate for,\n      #   we map each time to the appropriate interval and compute the quadratic\n      #   function value using that x.\n\n      g0 = f_left - fd\n      g1 = f_right - fd\n      g1plus2g0 = g1 + 2 * g0\n      g0plus2g1 = g0 + 2 * g1\n\n      result = tf.zeros_like(times)\n      integrated = tf.zeros_like(times)\n\n      # The method uses quadratic splines to do the interpolation.\n      # The specific spline used depends on the relationship between the\n      # boundary values (`g0` and `g1` above).\n      # The two dimensional plane determined by these two values is divided\n      # into four wedge sections referred to as region 1, 2, 3 and 4 below.\n      # For details of how the regions are defined, see Fig. 4 in Ref [2].\n      is_region_1, region_1_value, integrated_value_1 = _region_1(\n          g1plus2g0, g0plus2g1, g0, g1, x)\n\n      result = tf.where(is_region_1, region_1_value, result)\n      integrated = tf.where(is_region_1, integrated_value_1, integrated)\n\n      is_region_2, region_2_value, integrated_value_2 = _region_2(\n          g1plus2g0, g0plus2g1, g0, g1, x)\n\n      result = tf.where(is_region_2, region_2_value, result)\n      integrated = tf.where(is_region_2, integrated_value_2, integrated)\n\n      is_region_3, region_3_value, integrated_value_3 = _region_3(\n          g1plus2g0, g0plus2g1, g0, g1, x)\n\n      result = tf.where(is_region_3, region_3_value, result)\n      integrated = tf.where(is_region_3, integrated_value_3, integrated)\n\n      is_region_4, region_4_value, integrated_value_4 = _region_4(\n          g1plus2g0, g0plus2g1, g0, g1, x)\n      result = tf.where(is_region_4, region_4_value, result)\n      integrated = tf.where(is_region_4, integrated_value_4, integrated)\n\n      # g0 = g1 = 0 requires special handling. Checking if the values are\n      # legitimatey zero requires we pay close attention to the numerical\n      # precision issues.\n      g0_eps = tf.abs(tf.math.nextafter(fd, f_left) - fd) * 1.1\n      g1_eps = tf.abs(tf.math.nextafter(fd, f_right) - fd) * 1.1\n\n      is_origin = ((tf.abs(g0) <= g0_eps) & (tf.abs(g1) <= g1_eps))\n\n      result = tf.where(is_origin, tf.zeros_like(result), result)\n      integrated = tf.where(is_origin, tf.zeros_like(integrated), integrated)\n\n      return (result + fd, (integrated + fd * x) * interval_lengths)\n\n\ndef interpolate_forward_rate(interpolation_times,\n                             reference_times,\n                             yields=None,\n                             discrete_forwards=None,\n                             validate_args=False,\n                             dtype=None,\n                             name=None):\n  """"""Interpolates instantaneous forward rate to supplied times .\n\n    Applies the Hagan West procedure to interpolate either a zero coupon yield\n    curve or a discrete forward curve to a given set of times to compute\n    the instantaneous forward rate for those times.\n\n    A zero coupon yield curve is specified by a set\n    of times and the yields on zero coupon bonds expiring at those\n    times. A discrete forward rate curve specifies the interest rate that\n    applies between two times in the future as seen from the current time.\n    The relation between the two sets of curve is as follows. Suppose the\n    yields on zero coupon bonds expiring at times `[t_1, ..., t_n]` are\n    `[r_1, ..., r_n]`, then the forward rate between time `[t_i, t_{i+1}]` is\n    denoted `f(0; t_i, t_{i+1})` and given by\n\n    ```None\n      f(0; t_i, t_{i+1}) = (r_{i+1} t_{i+1} - r_i t_i) / (t_{i+1} - t_i)\n    ```\n    This function uses the Hagan West algorithm to perform the interpolation.\n    This scheme interpolates on the forward curve. If `yields` are specified\n    instead of `discrete_forwards` then they are first converted to the\n    discrete forwards before interpolation.\n    For more details on the interpolation procedure, see Ref. [1].\n\n  #### Example\n\n  ```python\n    dtype = np.float64\n    # Market data.\n    reference_times = np.array([1.0, 2.0, 3.0, 4,0, 5.0], dtype=dtype)\n    yields = np.array([2.75, 4.0, 4.75, 5.0, 4.75], dtype=dtype) / 100\n\n    # Times for which the interpolated values are required.\n    interpolation_times = np.array([0.3, 1.3, 2.1, 4.5], dtype=dtype)\n    interpolated_forwards = interpolate_forward_rates(\n        interpolation_times,\n        reference_times=reference_times,\n        yields=yields)\n\n    # Produces: [0.0229375, 0.05010625, 0.0609, 0.03625].\n  ```\n\n  #### References:\n\n  [1]: Patrick Hagan & Graeme West. Methods for Constructing a Yield Curve.\n    Wilmott Magazine, pp. 70-81. May 2008.\n\n  Args:\n    interpolation_times: Non-negative rank 1 `Tensor` of any size. The times for\n      which the interpolation has to be performed.\n    reference_times: Strictly positive rank 1 `Tensor` of real dtype containing\n      increasing values. The expiry times of the underlying zero coupon bonds.\n    yields: Optional rank 1 `Tensor` of the same shape and dtype as\n      `reference_times`, if supplied. The yield rate of zero coupon bonds\n      expiring at the corresponding time in the `reference_times`. Either this\n      argument or the `discrete_forwards` must be supplied (but not both).\n      Default value: None.\n    discrete_forwards: Optional rank 1 `Tensor` of the same shape and dtype as\n      `reference_times`, if supplied. The `i`th component of the `Tensor` is the\n      forward rate that applies between `reference_times[i-1]` and\n      `reference_times[i]` for `i>0` and between time `0` and\n      `reference_times[0]` for `i=0`. Either this argument or the `yields` must\n      be specified (but not both).\n      Default value: None.\n    validate_args: Python bool. If true, adds control dependencies to check that\n      the `times` are bounded by the `reference_times`.\n      Default value: False\n    dtype: `tf.Dtype` to use when converting arguments to `Tensor`s. If not\n      supplied, the default Tensorflow conversion will take place. Note that\n      this argument does not do any casting.\n      Default value: None.\n    name: Python `str` name prefixed to Ops created by this class.\n      Default value: None which is mapped to the default name\n        \'interpolate_forward_rate\'.\n\n  Returns:\n      interpolated_forwards: Rank 1 `Tensor` of the same size and dtype as the\n        `interpolation_times`. The interpolated instantaneous forwards at the\n        `interpolation_times`.\n\n  Raises:\n    ValueError if neither `yields` nor `discrete_forwards` are specified or if\n    both are specified.\n  """"""\n\n  if (yields is None) == (discrete_forwards is None):\n    raise ValueError(\'Exactly one of yields or discrete forwards must\'\n                     \' be supplied.\')\n\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'interpolate_forward_rate\',\n      values=[interpolation_times, reference_times, yields, discrete_forwards]):\n    if discrete_forwards is not None:\n      discrete_forwards = tf.convert_to_tensor(discrete_forwards, dtype=dtype)\n    reference_times = tf.convert_to_tensor(reference_times, dtype=dtype)\n    interpolation_times = tf.convert_to_tensor(interpolation_times, dtype=dtype)\n    if yields is not None:\n      yields = tf.convert_to_tensor(yields, dtype=dtype)\n      discrete_forwards = forwards.forward_rates_from_yields(\n          yields, reference_times, dtype=dtype)\n    interpolated_forwards, _ = interpolate(\n        interpolation_times,\n        discrete_forwards,\n        reference_times,\n        validate_args=validate_args,\n        dtype=dtype)\n    return interpolated_forwards\n\n\ndef interpolate_yields(interpolation_times,\n                       reference_times,\n                       yields=None,\n                       discrete_forwards=None,\n                       validate_args=False,\n                       dtype=None,\n                       name=None):\n  """"""Interpolates the yield curve to the supplied times.\n\n    Applies the Hagan West procedure to interpolate either a zero coupon yield\n    curve or a discrete forward curve to a given set of times.\n    A zero coupon yield curve is specified by a set\n    of times and the yields on zero coupon bonds expiring at those\n    times. A discrete forward rate curve specifies the interest rate that\n    applies between two times in the future as seen from the current time.\n    The relation between the two sets of curve is as follows. Suppose the\n    yields on zero coupon bonds expiring at times `[t_1, ..., t_n]` are\n    `[r_1, ..., r_n]`, then the forward rate between time `[t_i, t_{i+1}]` is\n    denoted `f(0; t_i, t_{i+1})` and given by\n\n    ```None\n      f(0; t_i, t_{i+1}) = (r_{i+1} t_{i+1} - r_i t_i) / (t_{i+1} - t_i)\n    ```\n\n    This function uses the Hagan West algorithm to perform the interpolation.\n    The interpolation proceeds in two steps. Firstly the discrete forward\n    curve is bootstrapped and an instantaneous forward curve is built. From the\n    instantaneous forward curve, the interpolated yield values are inferred\n    using the relation:\n\n    ```None\n      r(t) = (1/t) * Integrate[ f(s), 0 <= s <= t]\n    ```\n\n    The above equation connects the instantaneous forward curve `f(t)` to the\n    yield curve `r(t)`. The Hagan West procedure uses the Monotone Convex\n    interpolation to create a continuous forward curve. This is then integrated\n    to compute the implied yield rate.\n\n    For more details on the interpolation procedure, see Ref. [1].\n\n  #### Example\n\n  ```python\n    dtype = np.float64\n    reference_times = np.array([1.0, 2.0, 3.0, 4.0], dtype=dtype)\n    yields = np.array([5.0, 4.75, 4.53333333, 4.775], dtype=dtype)\n\n    # Times for which the interpolated values are required.\n    interpolation_times = np.array([0.25, 0.5, 1.0, 2.0], dtype=dtype)\n\n    interpolated = interpolate_yields(\n        interpolation_times, reference_times, yields=yields)\n    # Produces [5.1171875, 5.09375, 5.0, 4.75]\n  ```\n\n  #### References:\n\n  [1]: Patrick Hagan & Graeme West. Methods for Constructing a Yield Curve.\n    Wilmott Magazine, pp. 70-81. May 2008.\n    https://www.researchgate.net/profile/Patrick_Hagan3/publication/228463045_Methods_for_constructing_a_yield_curve/links/54db8cda0cf23fe133ad4d01.pdf\n\n  Args:\n    interpolation_times: Non-negative rank 1 `Tensor` of any size. The times for\n      which the interpolation has to be performed.\n    reference_times: Strictly positive rank 1 `Tensor` of real dtype containing\n      increasing values. The expiry times of the underlying zero coupon bonds.\n    yields: Optional rank 1 `Tensor` of the same shape and dtype as\n      `reference_times`, if supplied. The yield rate of zero coupon bonds\n      expiring at the corresponding time in the `reference_times`. Either this\n      argument or the `discrete_forwards` must be supplied (but not both).\n      Default value: None.\n    discrete_forwards: Optional rank 1 `Tensor` of the same shape and dtype as\n      `reference_times`, if supplied. The `i`th component of the `Tensor` is the\n      forward rate that applies between `reference_times[i-1]` and\n      `reference_times[i]` for `i>0` and between time `0` and\n      `reference_times[0]` for `i=0`. Either this argument or the `yields` must\n      be specified (but not both).\n      Default value: None.\n    validate_args: Python bool. If true, adds control dependencies to check that\n      the `times` are bounded by the `reference_times`.\n      Default value: False\n    dtype: `tf.Dtype` to use when converting arguments to `Tensor`s. If not\n      supplied, the default Tensorflow conversion will take place. Note that\n      this argument does not do any casting.\n      Default value: None.\n    name: Python `str` name prefixed to Ops created by this class.\n      Default value: None which is mapped to the default name\n        \'interpolate_forward_rate\'.\n\n  Returns:\n      interpolated_forwards: Rank 1 `Tensor` of the same size and dtype as the\n        `interpolation_times`. The interpolated instantaneous forwards at the\n        `interpolation_times`.\n\n  Raises:\n    ValueError if neither `yields` nor `discrete_forwards` are specified or if\n    both are specified.\n  """"""\n\n  if (yields is None) == (discrete_forwards is None):\n    raise ValueError(\'Exactly one of yields or discrete forwards must\'\n                     \' be supplied.\')\n\n  with tf.compat.v1.name_scope(\n      name,\n      default_name=\'interpolate_forward_rate\',\n      values=[interpolation_times, reference_times, yields, discrete_forwards]):\n    if discrete_forwards is not None:\n      discrete_forwards = tf.convert_to_tensor(discrete_forwards, dtype=dtype)\n      reference_yields = forwards.yields_from_forward_rates(\n          discrete_forwards, reference_times, dtype=dtype)\n    reference_times = tf.convert_to_tensor(reference_times, dtype=dtype)\n    interpolation_times = tf.convert_to_tensor(interpolation_times, dtype=dtype)\n\n    if yields is not None:\n      reference_yields = tf.convert_to_tensor(yields, dtype=dtype)\n      discrete_forwards = forwards.forward_rates_from_yields(\n          reference_yields, reference_times, dtype=dtype)\n\n    _, integrated_adjustments = interpolate(\n        interpolation_times,\n        discrete_forwards,\n        reference_times,\n        validate_args=validate_args,\n        dtype=dtype)\n\n    extended_times = tf.concat([[0.0], reference_times], axis=0)\n    extended_yields = tf.concat([[0.0], reference_yields], axis=0)\n    intervals = piecewise.find_interval_index(\n        interpolation_times, extended_times, last_interval_is_closed=True)\n    base_values = tf.gather(extended_yields * extended_times, intervals)\n    interpolated = tf.math.divide_no_nan(base_values + integrated_adjustments,\n                                         interpolation_times)\n    return interpolated\n\n\ndef _interpolate_adjacent(times, values, name=None):\n  """"""Interpolates linearly between adjacent values.\n\n  Suppose `times` are `[t_1, t_2, ..., t_n]` an array of length `n` and\n  values are `[f_1, ... f_n]` of length `n`. This function associates\n  each of the values to the midpoint of the interval i.e. `f_i` is associated\n  to the midpoint of the interval `[t_i, t_{i+1}]`. Then it calculates the\n  values at the interval boundaries by linearly interpolating between adjacent\n  intervals. The first interval is considered to be `[0, t_1]`. The values at\n  the endpoints (i.e. result[0] and result[n]) are computed as follows:\n  `result[0] = values[0] - 0.5 * (result[1] - values[0])` and\n  `result[n] = values[n-1] - 0.5 * (result[n-1] - values[n-1])`.\n  The rationale for these specific values is discussed in Ref. [1].\n\n  Args:\n    times: A rank 1 `Tensor` of real dtype. The times at which the interpolated\n      values are to be computed. The values in the array should be positive and\n      monotonically increasing.\n    values: A rank 1 `Tensor` of the same dtype and shape as `times`. The values\n      assigned to the midpoints of the time intervals.\n    name: Python `str` name prefixed to Ops created by this class.\n      Default value: None which is mapped to the default name\n        \'interpolate_adjacent\'.\n\n  Returns:\n    interval_values: The values interpolated from the supplied midpoint values\n      as described above. A `Tensor` of the same dtype as `values` but shape\n      `[n+1]` where `[n]` is the shape of `values`. The `i`th component of the\n      is the value associated to the time point `t_{i+1}` with `t_0 = 0`.\n  """"""\n  with tf.compat.v1.name_scope(\n      name, default_name=\'interpolate_adjacent\', values=[times, values]):\n    dt1 = diff(times, order=1, exclusive=False)\n    dt2 = diff(times, order=2, exclusive=False)[1:]\n    weight_right = dt1[:-1] / dt2\n    weight_left = dt1[1:] / dt2\n    interior_values = weight_right * values[1:] + weight_left * values[:-1]\n    value_0 = values[0] - 0.5 * (interior_values[0] - values[0])\n    value_n = values[-1] - 0.5 * (interior_values[-1] - values[-1])\n    return tf.concat([[value_0], interior_values, [value_n]], axis=0)\n\n\ndef _region_1(g1plus2g0, g0plus2g1, g0, g1, x):\n  """"""Computes conditional and value for points in region 1.""""""\n  is_region_1 = (((g1plus2g0 < 0) & (g0plus2g1 >= 0)) | ((g1plus2g0 > 0) &\n                                                         (g0plus2g1 <= 0)))\n  # Reference: Eq. 27 in Ref [2]\n  # Quadratic form a x^2 + b x + c\n  a, b, c = 3 * (g0 + g1), -2 * g1plus2g0, g0\n  region_1_value = (a * x + b) * x + c\n  # Integrated value is: x(1-x)(g0 (1-x) - g1 x)\n  one_minus_x = 1 - x\n  integrated_value = x * one_minus_x * (g0 * one_minus_x - g1 * x)\n  return is_region_1, region_1_value, integrated_value\n\n\ndef _region_2(g1plus2g0, g0plus2g1, g0, g1, x):\n  """"""Computes conditional and value for points in region 2.""""""\n  del g0plus2g1\n  # Reference: Eq. 28, 29 in Ref [2]\n  is_region_2 = (((g0 < 0) & (g1plus2g0 >= 0)) | ((g0 > 0) & (g1plus2g0 <= 0)))\n  eta = g1plus2g0 / (g1 - g0)\n  x_floor = tf.math.maximum(x, eta)\n  ratio = (x_floor - eta) / (1 - eta)\n  region_2_value = g0 + (g1 - g0) * tf.math.square(ratio)\n  # Integral is: g0 x + 1/3 (g1 - g0) (1-eta) [(x-eta)/(1-eta)]^3 for x > eta\n  # and g0 x for x < eta.\n  coeff = (g1 - g0) * (1 - eta) / 3\n  integrated_value = g0 * x + coeff * (x_floor**3)\n  return is_region_2, region_2_value, integrated_value\n\n\ndef _region_3(g1plus2g0, g0plus2g1, g0, g1, x):\n  """"""Computes conditional and value for points in region 3.""""""\n  del g1plus2g0\n  # Reference: Eq. 30, 31 in Ref [2]\n  is_region_3 = (((g1 <= 0) & (g0plus2g1 > 0)) | ((g1 >= 0) & (g0plus2g1 < 0)))\n  eta = 3 * g1 / (g1 - g0)\n  x_cap = tf.math.minimum(x, eta)\n  ratio = (eta - x_cap) / eta\n  # Replace NaN values (corresponding to g1 == 0) with zeros.\n  ratio = tf.where(tf.math.is_nan(ratio), tf.zeros_like(ratio), ratio)\n  region_3_value = g1 + (g0 - g1) * tf.math.square(ratio)\n  integrated_value = g1 * x + eta * (g0 - g1) / 3 * (1 - ratio**3)\n  return is_region_3, region_3_value, integrated_value\n\n\ndef _region_4(g1plus2g0, g0plus2g1, g0, g1, x):\n  """"""Computes conditional and value for points in region 4.""""""\n  del g1plus2g0, g0plus2g1\n  # Reference: Eq. 32 in Ref [2]\n  is_region_4 = (((g0 >= 0) & (g1 > 0)) | ((g0 <= 0) & (g1 < 0)))\n  eta = g1 / (g0 + g1)\n  x_cap = tf.math.minimum(x, eta)\n  x_floor = tf.math.maximum(x, eta)\n  shift = -0.5 * (eta * g0 + (1 - eta) * g1)\n  ratio_cap = (eta - x_cap) / eta\n  ratio_floor = (x_floor - eta) / (1 - eta)\n  region_4_value = (\n      shift + (g0 - shift) * tf.math.square(ratio_cap) +\n      (g1 - shift) * tf.math.square(ratio_floor))\n  # Integrated value: A x + (g0-A) eta ((eta-x)/eta)^3 / 3 for x < eta\n  # and A x + (g1-A) (1-eta) [(x-eta)/(1-eta)]^3 / 3 for x > eta\n  integrated_value = (\n      shift * x + (g0 - shift) * eta * (1 - ratio_cap**3) / 3 + (g1 - shift) *\n      (1 - eta) * (ratio_floor**3) / 3)\n  return is_region_4, region_4_value, integrated_value\n\n\n__all__ = [\n    \'diff\',\n    \'interpolate\',\n    \'interpolate_forward_rate\',\n    \'interpolate_yields\',\n]\n'"
tf_quant_finance/rates/hagan_west/monotone_convex_test.py,40,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for monotone_convex module.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\nfrom tf_quant_finance.rates.hagan_west import monotone_convex\n\n\nclass MonotoneConvexTest(tf.test.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_interpolate_adjacent(self):\n    dtype = tf.float64\n    times = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0], dtype=dtype)\n    values = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0], dtype=dtype)\n    actual = self.evaluate(monotone_convex._interpolate_adjacent(times, values))\n    expected = [0.75, 1.5, 2.5, 3.5, 4.5, 5.25]\n    np.testing.assert_array_equal(actual, expected)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_interpolation(self):\n    dtype = tf.float64\n    interval_times = tf.constant([0.25, 0.5, 1.0, 2.0, 3.0, 4.0], dtype=dtype)\n    interval_values = tf.constant([0.05, 0.051, 0.052, 0.053, 0.055, 0.055],\n                                  dtype=dtype)\n    test_times = tf.constant([0.25, 0.5, 1.0, 2.0, 3.0, 4.0, 1.1], dtype=dtype)\n    expected = [0.0505, 0.05133333, 0.05233333, 0.055, 0.055, 0.055, 0.05241]\n    actual, integrated_actual = self.evaluate(\n        monotone_convex.interpolate(test_times, interval_values,\n                                    interval_times))\n    np.testing.assert_allclose(actual, expected)\n    integrated_expected = [0, 0, 0, 0, 0, 0.055, 0.005237]\n    np.testing.assert_allclose(integrated_actual, integrated_expected)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_interpolation_two_intervals(self):\n    dtype = tf.float64\n    interval_times = tf.constant([0.25, 0.5], dtype=dtype)\n    interval_values = tf.constant([0.05, 0.051], dtype=dtype)\n    test_times = tf.constant([0.25, 0.5], dtype=dtype)\n    actual, integrated_actual = self.evaluate(\n        monotone_convex.interpolate(test_times, interval_values,\n                                    interval_times))\n    expected = [0.0505, 0.05125]\n    np.testing.assert_allclose(actual, expected)\n    integrated_expected = [0.0, 0.01275]\n    np.testing.assert_allclose(integrated_actual, integrated_expected)\n\n  @test_util.deprecated_graph_mode_only\n  def test_interpolation_differentiable(self):\n    dtype = tf.float64\n    interval_times = tf.constant([0.25, 0.5, 1.0, 2.0, 3.0], dtype=dtype)\n    knot_1y = tf.constant([0.052], dtype=dtype)\n    interval_values = tf.concat([\n        tf.constant([0.05, 0.051], dtype=dtype), knot_1y,\n        tf.constant([0.053, 0.055], dtype=dtype)\n    ],\n                                axis=0)\n    test_time = tf.constant([1.1, 2.7], dtype=dtype)\n    interpolated, _ = monotone_convex.interpolate(test_time, interval_values,\n                                                  interval_times)\n    gradient_1y = self.evaluate(tf.convert_to_tensor(\n        tf.gradients(interpolated[0], knot_1y)[0]))\n    gradient_zero = self.evaluate(tf.convert_to_tensor(\n        tf.gradients(interpolated[1], knot_1y)[0]))\n\n    self.assertAlmostEqual(gradient_1y[0], 0.42)\n    self.assertAlmostEqual(gradient_zero[0], 0.0)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_integrated_value(self):\n    dtype = np.float64\n    interval_times = tf.constant([0.25, 0.5, 1.0, 2.0, 3.0], dtype=dtype)\n    interval_values = tf.constant([0.05, 0.051, 0.052, 0.053, 0.055],\n                                  dtype=dtype)\n    # Checks that the integral is correct by computing it using a Riemann sum.\n    test_times = tf.constant(np.linspace(1.0, 1.1, num=51), dtype=dtype)\n    dt = np.array(0.002, dtype=dtype)\n    values, integrated = self.evaluate(\n        monotone_convex.interpolate(test_times, interval_values,\n                                    interval_times))\n    actual = integrated[-1]\n    expected = np.sum(dt * (values[1:] + values[:-1]) / 2)\n    self.assertAlmostEqual(actual, expected)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_flat_values(self):\n    dtype = np.float64\n    interval_times = np.array([0.3, 1.0, 1.43, 3.7, 9.2, 12.48], dtype=dtype)\n    interval_values = np.array([8.0] * 6, dtype=dtype)\n    test_times = tf.constant(\n        [0.1, 1.1, 1.22, 0.45, 1.8, 3.8, 7.45, 7.73, 9.6, 11.7, 12.],\n        dtype=dtype)\n    actual, _ = self.evaluate(\n        monotone_convex.interpolate(test_times, interval_values,\n                                    interval_times))\n    expected = np.zeros([11], dtype=dtype) + 8.\n    np.testing.assert_allclose(actual, expected)\n\n  def test_interpolated_forwards_with_discrete_forwards(self):\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      reference_times = tf.constant([0.25, 0.5, 1.0, 2.0, 3.0], dtype=dtype)\n      discrete_forwards = tf.constant([0.05, 0.051, 0.052, 0.053, 0.055],\n                                      dtype=dtype)\n      test_times = tf.constant([0.25, 0.5, 1.0, 2.0, 3.0, 1.1], dtype=dtype)\n      expected = np.array(\n          [0.0505, 0.05133333, 0.05233333, 0.054, 0.0555, 0.05241], dtype=dtype)\n      actual = self.evaluate(\n          monotone_convex.interpolate_forward_rate(\n              test_times, reference_times, discrete_forwards=discrete_forwards))\n      np.testing.assert_allclose(actual, expected, rtol=1e-5)\n\n  def test_interpolated_forwards_with_yields(self):\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      reference_times = np.array([1.0, 2.0, 3.0, 4.0, 5.0], dtype=dtype)\n      yields = np.array([2.75, 4.0, 4.75, 5.0, 4.75], dtype=dtype) / 100\n\n      # Times for which the interpolated values are required.\n      interpolation_times = np.array([0.3, 1.3, 2.1, 4.5], dtype=dtype)\n      actual = self.evaluate(\n          monotone_convex.interpolate_forward_rate(\n              interpolation_times,\n              reference_times=reference_times,\n              yields=yields))\n      expected = np.array([0.0229375, 0.05010625, 0.0609, 0.03625], dtype=dtype)\n      np.testing.assert_allclose(actual, expected, rtol=1e-5)\n\n  def test_interpolated_yields_with_discrete_forwards(self):\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      reference_times = tf.constant([1.0, 2.0, 3.0, 4.0], dtype=dtype)\n      discrete_forwards = tf.constant([5, 4.5, 4.1, 5.5], dtype=dtype)\n      test_times = tf.constant(\n          [0.25, 0.5, 1.0, 2.0, 3.0, 1.1, 2.5, 2.9, 3.6, 4.0], dtype=dtype)\n      expected = np.array([\n          5.1171875, 5.09375, 5.0, 4.75, 4.533333, 4.9746, 4.624082, 4.535422,\n          4.661777, 4.775\n      ],\n                          dtype=dtype)\n      actual = self.evaluate(\n          monotone_convex.interpolate_yields(\n              test_times, reference_times, discrete_forwards=discrete_forwards))\n      np.testing.assert_allclose(actual, expected, rtol=1e-5)\n\n  def test_interpolated_yields_with_yields(self):\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      reference_times = np.array([1.0, 2.0, 3.0, 4.0], dtype=dtype)\n      yields = np.array([5.0, 4.75, 4.53333333, 4.775], dtype=dtype)\n\n      # Times for which the interpolated values are required.\n      interpolation_times = tf.constant(\n          [0.25, 0.5, 1.0, 2.0, 3.0, 1.1, 2.5, 2.9, 3.6, 4.0], dtype=dtype)\n      expected = np.array([\n          5.1171875, 5.09375, 5.0, 4.75, 4.533333, 4.9746, 4.624082, 4.535422,\n          4.661777, 4.775\n      ],\n                          dtype=dtype)\n      actual = self.evaluate(\n          monotone_convex.interpolate_yields(\n              interpolation_times, reference_times, yields=yields))\n      np.testing.assert_allclose(actual, expected, rtol=1e-5)\n\n  def test_interpolated_yields_flat_curve(self):\n    """"""Checks the interpolation for flat curves.""""""\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      reference_times = np.array([0.3, 1.0, 1.43, 3.7, 9.2, 12.48], dtype=dtype)\n      yields = np.array([8.0] * 6, dtype=dtype)\n\n      # Times for which the interpolated values are required.\n      interpolation_times = tf.constant(\n          [0.1, 1.1, 1.22, 0.45, 1.8, 3.8, 7.45, 7.73, 9.6, 11.7, 12.],\n          dtype=dtype)\n      expected = np.array([8.0] * 11, dtype=dtype)\n      actual = self.evaluate(\n          monotone_convex.interpolate_yields(\n              interpolation_times, reference_times, yields=yields))\n      np.testing.assert_allclose(actual, expected, rtol=1e-5)\n\n  def test_interpolated_yields_zero_time(self):\n    """"""Checks the interpolation for yield curve is non-NaN for 0 time.""""""\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      reference_times = np.array([0.3, 1.0, 1.43, 3.7, 9.2, 12.48], dtype=dtype)\n      yields = np.array([3.0, 3.1, 2.9, 4.1, 4.3, 5.1], dtype=dtype)\n\n      # Times for which the interpolated values are required.\n      interpolation_times = tf.constant([0.0], dtype=dtype)\n      actual = self.evaluate(\n          monotone_convex.interpolate_yields(\n              interpolation_times, reference_times, yields=yields))\n      np.testing.assert_allclose(actual, [0.0], rtol=1e-8)\n\n  def test_interpolated_yields_consistency(self):\n    dtypes = [np.float32, np.float64]\n    for dtype in dtypes:\n      reference_times = np.array([1.0, 2.0, 3.0, 4.0], dtype=dtype)\n      yields = np.array([5.0, 4.75, 4.53333333, 4.775], dtype=dtype)\n\n      # Times for which the interpolated values are required.\n      interpolation_times_1 = tf.constant([0.25, 0.5, 1.0, 2.0, 3.0],\n                                          dtype=dtype)\n      interpolation_times_2 = tf.constant([1.1, 2.5, 2.9, 3.6, 4.0],\n                                          dtype=dtype)\n      expected = np.array([\n          5.1171875, 5.09375, 5.0, 4.75, 4.533333, 4.9746, 4.624082, 4.535422,\n          4.661777, 4.775\n      ],\n                          dtype=dtype)\n      actual_1 = monotone_convex.interpolate_yields(\n          interpolation_times_1, reference_times, yields=yields)\n      actual_2 = monotone_convex.interpolate_yields(\n          interpolation_times_2, reference_times, yields=yields)\n      actual = self.evaluate(tf.concat([actual_1, actual_2], axis=0))\n      np.testing.assert_allclose(actual, expected, rtol=1e-5)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/math/interpolation/cubic/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Cubic spline interpolation methods.""""""\n\n\nfrom tf_quant_finance.math.interpolation.cubic.cubic_interpolation import build as build_spline\nfrom tf_quant_finance.math.interpolation.cubic.cubic_interpolation import interpolate\nfrom tf_quant_finance.math.interpolation.cubic.cubic_interpolation import SplineParameters\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'build_spline\',\n    \'interpolate\',\n    \'SplineParameters\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/math/interpolation/cubic/cubic_interpolation.py,43,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Cubic Spline interpolation framework.""""""\n\n\nimport collections\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nSplineParameters = collections.namedtuple(\n    ""SplineParameters"",\n    [\n        # A real `Tensor` of shape batch_shape + [num_points] containing\n        # X-coordinates of the spline.\n        ""x_data"",\n        # A `Tensor` of the same shape and `dtype` as `x_data` containing\n        # Y-coordinates of the spline.\n        ""y_data"",\n        # A `Tensor` of the same shape and `dtype` as `x_data` containing\n        # spline interpolation coefficients\n        ""spline_coeffs""\n    ])\n\n\ndef build(x_data, y_data, validate_args=False, dtype=None, name=None):\n  """"""Builds a SplineParameters interpolation object.\n\n  Given a `Tensor` of state points `x_data` and corresponding values `y_data`\n  creates an object that contains iterpolation coefficients. The object can be\n  used by the `interpolate` function to get interpolated values for a set of\n  state points `x` using the cubic spline interpolation algorithm.\n  It assumes that the second derivative at the first and last spline points\n  are zero. The basic logic is explained in [1] (see also, e.g., [2]).\n\n  Repeated entries in `x_data` are allowed for the boundary values of `x_data`.\n  For example, `x_data` can be `[1., 1., 2, 3. 4., 4., 4.]` but not\n  `[1., 2., 2., 3.]`. The repeated values play no role in interpolation and are\n  useful only for interpolating multiple splines with different numbers of data\n  point. It is user responsibility to verify that the corresponding\n  values of `y_data` are the same for the repeated values of `x_data`.\n\n  Typical Usage Example:\n\n  ```python\n  import tensorflow.compat.v2 as tf\n  import numpy as np\n\n  x_data = np.linspace(-5.0, 5.0,  num=11)\n  y_data = 1.0/(1.0 + x_data**2)\n  spline = cubic_interpolation.build(x_data, y_data)\n  x_args = [3.3, 3.4, 3.9]\n\n  y = cubic_interpolation.interpolate(x_args, spline)\n  ```\n\n  #### References:\n  [1]: R. Sedgewick, Algorithms in C, 1990, p. 545-550.\n    Link: http://index-of.co.uk/Algorithms/Algorithms%20in%20C.pdf\n  [2]: R. Pienaar, M Choudhry. Fitting the term structure of interest rates:\n    the practical implementation of cubic spline methodology.\n    Link:\n    http://yieldcurve.com/mktresearch/files/PienaarChoudhry_CubicSpline2.pdf\n\n  Args:\n    x_data: A real `Tensor` of shape `[..., num_points]` containing\n      X-coordinates of points to fit the splines to. The values have to\n      be monotonically non-decreasing along the last dimension.\n    y_data: A `Tensor` of the same shape and `dtype` as `x_data` containing\n      Y-coordinates of points to fit the splines to.\n    validate_args: Python `bool`. When `True`, verifies if elements of `x_data`\n      are sorted in the last dimension in non-decreasing order despite possibly\n      degrading runtime performance.\n      Default value: False.\n    dtype: Optional dtype for both `x_data` and `y_data`.\n      Default value: `None` which maps to the default dtype inferred by\n      TensorFlow.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` which is mapped to the default name\n      `cubic_spline_build`.\n\n  Returns:\n    An instance of `SplineParameters`.\n  """"""\n  # Main body of build\n  with tf.compat.v1.name_scope(\n      name, default_name=""cubic_spline_build"", values=[x_data, y_data]):\n    x_data = tf.convert_to_tensor(x_data, dtype=dtype, name=""x_data"")\n    y_data = tf.convert_to_tensor(y_data, dtype=dtype, name=""y_data"")\n    # Sanity check inputs\n    if validate_args:\n      assert_sanity_check = [_validate_arguments(x_data)]\n    else:\n      assert_sanity_check = []\n\n    with tf.compat.v1.control_dependencies(assert_sanity_check):\n      spline_coeffs = _calculate_spline_coeffs(x_data, y_data)\n\n    return SplineParameters(x_data=x_data, y_data=y_data,\n                            spline_coeffs=spline_coeffs)\n\n\ndef interpolate(x_values,\n                spline_data,\n                optimize_for_tpu=False,\n                dtype=None,\n                name=None):\n  """"""Interpolates spline values for the given `x_values` and the `spline_data`.\n\n  Constant extrapolation is performed for the values outside the domain\n  `spline_data.x_data`. This means that for `x > max(spline_data.x_data)`,\n  `interpolate(x, spline_data) = spline_data.y_data[-1]`\n  and for  `x < min(spline_data.x_data)`,\n  `interpolate(x, spline_data) = spline_data.y_data[0]`.\n\n  For the interpolation formula refer to p.548 of [1].\n\n  #### References:\n  [1]: R. Sedgewick, Algorithms in C, 1990, p. 545-550.\n    Link: http://index-of.co.uk/Algorithms/Algorithms%20in%20C.pdf\n\n  Args:\n    x_values: A real `Tensor` of shape `batch_shape + [num_points]`.\n    spline_data: An instance of `SplineParameters`. `spline_data.x_data` should\n      have the same batch shape as `x_values`.\n    optimize_for_tpu: A Python bool. If `True`, the algorithm uses one-hot\n      encoding to lookup indices of `x_values` in `spline_data.x_data`. This\n      significantly improves performance of the algorithm on a TPU device but\n      may slow down performance on the CPU.\n      Default value: `False`.\n    dtype: Optional dtype for `x_values`.\n      Default value: `None` which maps to the default dtype inferred by\n      TensorFlow.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` which is mapped to the default name\n      `cubic_spline_interpolate`.\n\n  Returns:\n      A `Tensor` of the same shape and `dtype` as `x_values`. Represents\n      the interpolated values.\n\n  Raises:\n    ValueError:\n      If `x_values` batch shape is different from `spline_data.x_data` batch\n      shape.\n  """"""\n  name = name or ""cubic_spline_interpolate""\n  with tf.name_scope(name):\n    x_values = tf.convert_to_tensor(x_values, dtype=dtype, name=""x_values"")\n    dtype = x_values.dtype\n    # Unpack the spline data\n    x_data = spline_data.x_data\n    y_data = spline_data.y_data\n    spline_coeffs = spline_data.spline_coeffs\n    # Check that all the x_values are within the boundaries\n    if x_values.shape.as_list()[:-1] != x_data.shape.as_list()[:-1]:\n      msg = (""The input tensor has a different number of rows than the ""\n             ""number of splines: {} != {}"")\n      raise ValueError(msg.format(x_values.shape.as_list()[:-1],\n                                  x_data.shape.as_list()[:-1]))\n    # Determine the splines to use.\n    indices = tf.searchsorted(x_data, x_values, side=""right"") - 1\n    # This selects all elements for the start of the spline interval.\n    # Make sure indices lie in the permissible range\n    indices_lower = tf.maximum(indices, 0)\n    # This selects all elements for the end of the spline interval.\n    # Make sure indices lie in the permissible range\n    indices_upper = tf.minimum(indices + 1, x_data.shape.as_list()[-1] - 1)\n    # Prepare indices for `tf.gather_nd` or `tf.one_hot`\n    # TODO(b/156720909): Extract get_slice logic into a common utilities module\n    # for cubic and linear interpolation\n    if optimize_for_tpu:\n      x_data_size = x_data.shape.as_list()[-1]\n      lower_encoding = tf.one_hot(indices_lower, x_data_size,\n                                  dtype=dtype)\n      upper_encoding = tf.one_hot(indices_upper, x_data_size,\n                                  dtype=dtype)\n    else:\n      index_matrix = _prepare_indices(indices)\n      lower_encoding = tf.concat(\n          [index_matrix, tf.expand_dims(indices_lower, -1)], -1)\n      upper_encoding = tf.concat(\n          [index_matrix, tf.expand_dims(indices_upper, -1)], -1)\n\n    # Calculate dx and dy.\n    # Simplified logic:\n    # dx = x_data[indices + 1] - x_data[indices]\n    # dy = y_data[indices + 1] - y_data[indices]\n    # indices is a tensor with different values per row/spline\n    # Hence use a selection matrix with gather_nd\n    def get_slice(x, encoding):\n      if optimize_for_tpu:\n        return tf.math.reduce_sum(tf.expand_dims(x, axis=-2) * encoding,\n                                  axis=-1)\n      else:\n        return tf.gather_nd(x, encoding)\n    x0 = get_slice(x_data, lower_encoding)\n    x1 = get_slice(x_data, upper_encoding)\n    dx = x1 - x0\n\n    y0 = get_slice(y_data, lower_encoding)\n    y1 = get_slice(y_data, upper_encoding)\n    dy = y1 - y0\n\n    spline_coeffs0 = get_slice(spline_coeffs, lower_encoding)\n    spline_coeffs1 = get_slice(spline_coeffs, upper_encoding)\n\n    t = (x_values - x0) / dx\n    t = tf.where(dx > 0, t, tf.zeros_like(t))\n    df = ((t + 1.0) * spline_coeffs1 * 2.0) - ((t - 2.0) * spline_coeffs0 * 2.0)\n    df1 = df * t * (t - 1) / 6.0\n    result = y0 + (t * dy) + (dx * dx * df1)\n    # Use constant extrapolation outside the domain\n    upper_bound = tf.expand_dims(\n        tf.reduce_max(x_data, -1), -1) + tf.zeros_like(result)\n    lower_bound = tf.expand_dims(\n        tf.reduce_min(x_data, -1), -1) + tf.zeros_like(result)\n    result = tf.where(tf.logical_and(x_values <= upper_bound,\n                                     x_values >= lower_bound),\n                      result, tf.where(x_values > upper_bound, y0, y1))\n    return result\n\n\ndef _calculate_spline_coeffs(x_data, y_data):\n  """"""Calculates the coefficients for the spline interpolation.\n\n  These are the values of the second derivative of the spline at `x_data`.\n  See p.548 of [1].\n\n  Below is an outline of the function when number of observations if equal to 7.\n  The coefficients are obtained by building and solving a tridiagonal linear\n  system of equations with symmetric matrix\n\n   w2,  dx2,   0,   0,   0\n   dx2,  w3, dx3,   0,   0\n   0,  dx3,   w4, dx4,   0\n   0,    0,  dx4,  w5, dx5\n   0,    0,    0, dx5,  w6\n\n   where:\n   wn = 2 * (x_data[n-2] + x_data[n-1])\n   dxn = x_data[n-1] - x_data[n-2]\n\n   and the right hand side of the equation is:\n   [[3*( (d2-d1)/X1 - (d1-d0)/x0],\n    [3*( (d3-d2)/X2 - (d2-d1)/x1],\n    ...\n   ]\n\n   with di = y_data[..., i]\n\n   Solve for `spline_coeffs`, so that  matrix * spline_coeffs = rhs\n\n   the solution is the `spline_coeffs` parameter of the spline equation:\n\n   y_pred = a(spline_coeffs) * t^3 + b(spline_coeffs) * t^2\n            + c(spline_coeffs) * t + d(spline_coeffs)\n   with t being the proportion of the difference between the x value of\n   the spline used and the nx_value of the next spline:\n\n   t = (x_values - x_data[:,n]) / (x_data[:,n+1]-x_data[:,n])\n\n   and `a`, `b`, `c`, and `d` are functions of `spline_coeffs` and `x_data` and\n   are provided in the `interpolate` function.\n\n  #### References:\n  [1]: R. Sedgewick, Algorithms in C, 1990, p. 545-550.\n    Link: http://index-of.co.uk/Algorithms/Algorithms%20in%20C.pdf\n\n  Args:\n    x_data: A real `Tensor` of shape `[..., num_points]` containing\n      X-coordinates of points to fit the splines to. The values have to\n      be monotonically non-decreasing along the last dimension.\n    y_data: A `Tensor` of the same shape and `dtype` as `x_data` containing\n      Y-coordinates of points to fit the splines to.\n\n  Returns:\n     A `Tensor` of the same shape and `dtype` as `x_data`. Represents the\n     spline coefficients for the cubic spline interpolation.\n  """"""\n\n  # `dx` is the distances between the x points. It is 1 element shorter than\n  # `x_data`\n  dx = x_data[..., 1:] - x_data[..., :-1]\n\n  # `diag_values` are the diagonal values 2 * (x_data[i+1] - x_data[i-1])\n  # its length 2 shorter\n\n  diag_values = 2.0 * (x_data[..., 2:] - x_data[..., :-2])\n  superdiag = dx[..., 1:]\n  subdiag = dx[..., :-1]\n\n  corr_term = tf.logical_or(tf.equal(superdiag, 0), tf.equal(subdiag, 0))\n  diag_values_corr = tf.where(corr_term,\n                              tf.ones_like(diag_values), diag_values)\n  superdiag_corr = tf.where(tf.equal(subdiag, 0),\n                            tf.zeros_like(superdiag), superdiag)\n  subdiag_corr = tf.where(tf.equal(superdiag, 0),\n                          tf.zeros_like(subdiag), subdiag)\n  diagonals = tf.stack([superdiag_corr, diag_values_corr, subdiag_corr],\n                       axis=-2)\n\n  # determine the rhs of the equation\n  dd = (y_data[..., 1:] - y_data[..., :-1]) / dx\n  dd = tf.where(tf.equal(dx, 0), tf.zeros_like(dd), dd)\n  # rhs is a column vector:\n  # [[-3((y1-y0)/dx0 - (y2-y1)/dx0], ...]\n  rhs = -3 * (dd[..., :-1] - dd[..., 1:])\n  rhs = tf.where(corr_term, tf.zeros_like(rhs), rhs)\n  # Partial pivoting is unnecessary since the matrix is diagonally dominant.\n  spline_coeffs = tf.linalg.tridiagonal_solve(diagonals, rhs,\n                                              partial_pivoting=False)\n  # Reshape `spline_coeffs`\n  zero = tf.zeros_like(dx[..., :1], dtype=x_data.dtype)\n  spline_coeffs = tf.concat([zero, spline_coeffs, zero], axis=-1)\n  return spline_coeffs\n\n\ndef _validate_arguments(x_data):\n  """"""Checks that input arguments are in the non-decreasing order.""""""\n  diffs = x_data[..., 1:] - x_data[..., :-1]\n  return tf.compat.v1.debugging.assert_greater_equal(\n      diffs,\n      tf.zeros_like(diffs),\n      message=""x_data is not sorted in non-decreasing order."")\n\n\ndef _prepare_indices(indices):\n  """"""Prepares `tf.searchsorted` output for index argument of `tf.gather_nd`.""""""\n  batch_shape = indices.shape.as_list()[:-1]\n  num_points = indices.shape.as_list()[-1]\n  batch_shape_reverse = indices.shape.as_list()[:-1]\n  batch_shape_reverse.reverse()\n  index_matrix = tf.constant(\n      np.flip(np.transpose(np.indices(batch_shape_reverse)), -1),\n      dtype=indices.dtype)\n  batch_rank = len(batch_shape)\n  # Broadcast index matrix to the shape of\n  # `batch_shape + [num_points] + [batch_rank]`\n  broadcasted_shape = batch_shape + [num_points] + [batch_rank]\n  index_matrix = tf.expand_dims(index_matrix, -2) + tf.zeros(\n      broadcasted_shape, dtype=indices.dtype)\n  return index_matrix\n'"
tf_quant_finance/math/interpolation/cubic/cubic_interpolation_test.py,12,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for cubic spline interpolation.""""""\n\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass CubicInterpolationTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (""default_interpolation"", False),\n      (""one_hot_interpolation"", True),\n  )\n  def test_error_calc(self, optimize_for_tpu):\n    """"""Test the deviation of the interpolated values from the actual.""""""\n    sampling_points = 1000\n    spline_x = np.linspace(0.0, 10.0, num=11, dtype=np.float64)\n    spline_y = [1.0 / (1.0 + x * x) for x in spline_x]\n    x_series = np.array([spline_x])\n    y_series = np.array([spline_y])\n    spline = tff.math.interpolation.cubic.build_spline(x_series, y_series)\n\n    # There is an error if we go to 10.0\n    test_range_x = np.linspace(0.0, 9.99, num=sampling_points, dtype=np.float64)\n    search_args = tf.constant(np.array([test_range_x]), dtype=tf.float64)\n    projected_y = tff.math.interpolation.cubic.interpolate(\n        search_args, spline, optimize_for_tpu=optimize_for_tpu)\n    expected_y = tf.constant([[1.0 / (1.0 + x * x) for x in test_range_x]],\n                             dtype=tf.float64)\n    errors = expected_y - projected_y\n    deviation = self.evaluate(tfp.stats.stddev(errors[0], sample_axis=0))\n    limit = 0.02\n    self.assertLess(deviation, limit)\n\n  @parameterized.named_parameters(\n      (""default_interpolation"", False),\n      (""one_hot_interpolation"", True),\n  )\n  def test_spline_batch(self, optimize_for_tpu):\n    """"""Tests batching of four splines.""""""\n    for dtype in (np.float32, np.float64):\n      x_data = np.linspace(-11, 12, 24)\n      x_data = np.reshape(x_data, [2, 2, 6])\n      y_data = 1.0 / (1.0 + x_data * x_data)\n      search_args = np.array([[[-10.5, -5.], [-4.5, 1]],\n                              [[1.5, 2.], [7.5, 12.]]])\n\n      spline = tff.math.interpolation.cubic.build_spline(\n          x_data, y_data, dtype=dtype)\n      result = tff.math.interpolation.cubic.interpolate(\n          search_args, spline,\n          optimize_for_tpu=optimize_for_tpu, dtype=dtype)\n\n      expected = np.array([[[0.00900778, 0.02702703],\n                            [0.04705774, 1.]],\n                           [[0.33135411, 0.2],\n                            [0.01756963, 0.00689655]]],\n                          dtype=dtype)\n      self.assertEqual(result.dtype.as_numpy_dtype, dtype)\n      result = self.evaluate(result)\n      np.testing.assert_almost_equal(expected, result)\n\n  @parameterized.named_parameters(\n      (""default_interpolation"", False),\n      (""one_hot_interpolation"", True),\n  )\n  def test_invalid_interpolate_parameter_shape(self, optimize_for_tpu):\n    """"""Tests batch shape of spline and interpolation should be the same.""""""\n    x_data1 = np.linspace(-5.0, 5.0, num=11)\n    x_data2 = np.linspace(0.0, 10.0, num=11)\n    x_series = np.array([x_data1, x_data2])\n    y_data1 = 1.0 / (1.0 + x_data1**2)\n    y_data2 = 1.0 / (2.0 + x_data2**2)\n    y_series = np.array([y_data1, y_data2])\n    x_data_series = tf.stack(x_series, axis=0)\n    y_data_series = tf.stack(y_series, axis=0)\n    search_args = tf.constant([[-1.2, 0.0, 0.3]], dtype=tf.float64)\n    x_test = tf.stack(search_args, axis=0)\n    spline = tff.math.interpolation.cubic.build_spline(x_data_series,\n                                                       y_data_series)\n\n    msg = ""Failed to catch that the test vector has less rows than x_points""\n    with self.assertRaises(ValueError, msg=msg):\n      tff.math.interpolation.cubic.interpolate(\n          x_test, spline)\n\n  def test_invalid_spline_x_points(self):\n    """"""Tests a spline where the x_points are not increasing.""""""\n    x_data = tf.constant([[1.0, 2.0, 1.5, 3.0, 4.0]], dtype=tf.float64)\n    y_data = tf.constant([[1.0, 1.0, 1.0, 1.0, 1.0]], dtype=tf.float64)\n\n    msg = ""Failed to detect invalid x_data sequence""\n    with self.assertRaises(tf.errors.InvalidArgumentError, msg=msg):\n      self.evaluate(\n          tff.math.interpolation.cubic.build_spline(\n              x_data, y_data, validate_args=True)[2])\n\n  @parameterized.named_parameters(\n      (""default_interpolation"", False),\n      (""one_hot_interpolation"", True),\n  )\n  def test_duplicate_x_points(self, optimize_for_tpu):\n    """"""Tests a spline where there are x_points of the same value.""""""\n    # Repeated boundary values are allowed\n    x_data = np.array([[1.0, 1.0, 2.0, 3.0, 4.0, 4.0, 4.0],\n                       [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]])\n    y_data = np.array([[3.0, 3.0, 1.0, 3.0, 2.0, 3.0, 2.0],\n                       [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]])\n    spline = tff.math.interpolation.cubic.build_spline(x_data, y_data)\n    x_values = np.array([[0.0, 1.0, 1.5, 2.0, 2.5, 3.5, 4.0, 5.0],\n                         [0.0, 1.0, 1.5, 2.0, 2.5, 3.5, 4.0, 5.0]])\n    interpolated = tff.math.interpolation.cubic.interpolate(\n        x_values, spline, optimize_for_tpu=optimize_for_tpu)\n    expected = np.array([[3.0, 3.0, 1.525, 1.0, 1.925, 2.9, 2.0, 2.0],\n                         [1.0, 1.0, 1.5, 2.0, 2.5, 3.5, 4.0, 5.0]])\n    interpolated = self.evaluate(interpolated)\n    np.testing.assert_almost_equal(expected, interpolated)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/math/interpolation/linear/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Linear interpolation.""""""\n\n\nfrom tf_quant_finance.math.interpolation.linear.linear_interpolation import interpolate\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'interpolate\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/math/interpolation/linear/linear_interpolation.py,45,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Linear interpolation method.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\n\ndef interpolate(x,\n                x_data,\n                y_data,\n                left_slope=None,\n                right_slope=None,\n                validate_args=False,\n                optimize_for_tpu=False,\n                dtype=None,\n                name=None):\n  """"""Performs linear interpolation for supplied points.\n\n  Given a set of knots whose x- and y- coordinates are in `x_data` and `y_data`,\n  this function returns y-values for x-coordinates in `x` via piecewise\n  linear interpolation.\n\n  `x_data` must be non decreasing, but `y_data` don\'t need to be because we do\n  not require the function approximated by these knots to be monotonic.\n\n  #### Examples\n\n  ```python\n  x = [-10, -1, 1, 3, 6, 7, 8, 15, 18, 25, 30, 35]\n  x_data = [-1, 2, 6, 8, 18, 30.0]\n  y_data = [10, -1, -5, 7, 9, 20]\n\n  result = linear_interpolation(x, x_data, y_data)\n  # [ 10, 10, 2.66666667, -2, -5, 1, 7, 8.4, 9, 15.41666667, 20, 20]\n  ```\n\n  Args:\n    x: x-coordinates for which we need to get interpolation. A N-D `Tensor` of\n      real dtype. First N-1 dimensions represent batching dimensions.\n    x_data: x coordinates. A N-D `Tensor` of real dtype. Should be sorted\n      in non decreasing order. First N-1 dimensions represent batching\n      dimensions.\n    y_data: y coordinates. A N-D `Tensor` of real dtype. Should have the\n      compatible shape as `x_data`. First N-1 dimensions represent batching\n      dimensions.\n    left_slope: The slope to use for extrapolation with x-coordinate smaller\n      than the min `x_data`. It\'s a 0-D or N-D `Tensor`.\n      Default value: `None`, which maps to `0.0` meaning constant extrapolation,\n      i.e. extrapolated value will be the leftmost `y_data`.\n    right_slope: The slope to use for extrapolation with x-coordinate greater\n      than the max `x_data`. It\'s a 0-D or N-D `Tensor`.\n      Default value: `None` which maps to `0.0` meaning constant extrapolation,\n      i.e. extrapolated value will be the rightmost `y_data`.\n    validate_args: Python `bool` that indicates whether the function performs\n      the check if the shapes of `x_data` and `y_data` are equal and that the\n      elements in `x_data` are non decreasing. If this value is set to `False`\n      and the elements in `x_data` are not increasing, the result of linear\n      interpolation may be wrong.\n      Default value: `False`.\n    optimize_for_tpu: A Python bool. If `True`, the algorithm uses one-hot\n      encoding to lookup indices of `x_values` in `x_data`. This significantly\n      improves performance of the algorithm on a TPU device but may slow down\n      performance on the CPU.\n      Default value: `False`.\n    dtype: Optional tf.dtype for `x`, x_data`, `y_data`, `left_slope` and\n      `right_slope`.\n      Default value: `None` which means that the `dtype` inferred by TensorFlow\n      is used.\n    name: Python str. The name prefixed to the ops created by this function.\n      Default value: `None` which maps to \'linear_interpolation\'.\n\n  Returns:\n    A N-D `Tensor` of real dtype corresponding to the x-values in `x`.\n  """"""\n  name = name or \'linear_interpolation\'\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(x, dtype=dtype, name=\'x\')\n    dtype = dtype or x.dtype\n    x_data = tf.convert_to_tensor(x_data, dtype=dtype, name=\'x_data\')\n    y_data = tf.convert_to_tensor(y_data, dtype=dtype, name=\'y_data\')\n    batch_shape = x.shape.as_list()[:-1]\n    if not batch_shape:\n      x = tf.expand_dims(x, 0)\n      x_data = tf.expand_dims(x_data, 0)\n      y_data = tf.expand_dims(y_data, 0)\n\n    if left_slope is None:\n      left_slope = tf.constant(0.0, dtype=x.dtype, name=\'left_slope\')\n    else:\n      left_slope = tf.convert_to_tensor(left_slope, dtype=dtype,\n                                        name=\'left_slope\')\n    if right_slope is None:\n      right_slope = tf.constant(0.0, dtype=x.dtype, name=\'right_slope\')\n    else:\n      right_slope = tf.convert_to_tensor(right_slope, dtype=dtype,\n                                         name=\'right_slope\')\n    control_deps = []\n    if validate_args:\n      # Check that `x_data` elements is non-decreasing\n      diffs = x_data[..., 1:] - x_data[..., :-1]\n      assertion = tf.compat.v1.debugging.assert_greater_equal(\n          diffs,\n          tf.zeros_like(diffs),\n          message=\'x_data is not sorted in non-decreasing order.\')\n      control_deps.append(assertion)\n      # Check that the shapes of `x_data` and `y_data` are equal\n      control_deps.append(\n          tf.compat.v1.assert_equal(tf.shape(x_data), tf.shape(y_data)))\n\n    with tf.control_dependencies(control_deps):\n      # Get upper bound indices for `x`.\n      upper_indices = tf.searchsorted(x_data, x, side=\'left\', out_type=tf.int32)\n      x_data_size = x_data.shape.as_list()[-1]\n      at_min = tf.equal(upper_indices, 0)\n      at_max = tf.equal(upper_indices, x_data_size)\n      # Create tensors in order to be used by `tf.where`.\n      # `values_min` are extrapolated values for x-coordinates less than or\n      # equal to `x_data[..., 0]`.\n      # `values_max` are extrapolated values for x-coordinates greater than\n      # `x_data[..., -1]`.\n\n      values_min = tf.expand_dims(y_data[..., 0], -1) + left_slope * (\n          x - tf.broadcast_to(\n              tf.expand_dims(x_data[..., 0], -1), shape=tf.shape(x)))\n      values_max = tf.expand_dims(y_data[..., -1], -1) + right_slope * (\n          x - tf.broadcast_to(\n              tf.expand_dims(x_data[..., -1], -1), shape=tf.shape(x)))\n\n      # `tf.where` evaluates all branches, need to cap indices to ensure it\n      # won\'t go out of bounds.\n      capped_lower_indices = tf.math.maximum(upper_indices - 1, 0)\n      capped_upper_indices = tf.math.minimum(upper_indices, x_data_size - 1)\n      # Prepare indices for `tf.gather_nd` or `tf.one_hot`\n      # TODO(b/156720909): Extract get_slice logic into a common utilities\n      # module for cubic and linear interpolation\n      if optimize_for_tpu:\n        lower_encoding = tf.one_hot(capped_lower_indices, x_data_size,\n                                    dtype=dtype)\n        upper_encoding = tf.one_hot(capped_upper_indices, x_data_size,\n                                    dtype=dtype)\n      else:\n        index_matrix = _prepare_indices(upper_indices)\n        lower_encoding = tf.concat(\n            [index_matrix, tf.expand_dims(capped_lower_indices, -1)], -1)\n\n        upper_encoding = tf.concat(\n            [index_matrix, tf.expand_dims(capped_upper_indices, -1)], -1)\n      def get_slice(x, encoding):\n        if optimize_for_tpu:\n          return tf.math.reduce_sum(tf.expand_dims(x, axis=-2) * encoding,\n                                    axis=-1)\n        else:\n          return tf.gather_nd(x, encoding)\n      x_data_lower = get_slice(x_data, lower_encoding)\n      x_data_upper = get_slice(x_data, upper_encoding)\n      y_data_lower = get_slice(y_data, lower_encoding)\n      y_data_upper = get_slice(y_data, upper_encoding)\n\n      # Nan in unselected branches could propagate through gradient calculation,\n      # hence we need to clip the values to ensure no nan would occur. In this\n      # case we need to ensure there is no division by zero.\n      x_data_diff = x_data_upper - x_data_lower\n      floor_x_diff = tf.where(at_min | at_max, x_data_diff + 1, x_data_diff)\n      interpolated = y_data_lower + (x - x_data_lower) * (\n          y_data_upper - y_data_lower) / floor_x_diff\n\n      interpolated = tf.where(at_min, values_min, interpolated)\n      interpolated = tf.where(at_max, values_max, interpolated)\n      if batch_shape:\n        return interpolated\n      else:\n        return tf.squeeze(interpolated, 0)\n\n\ndef _prepare_indices(indices):\n  """"""Prepares `tf.searchsorted` output for index argument of `tf.gather_nd`.""""""\n  batch_shape = indices.shape.as_list()[:-1]\n  num_points = indices.shape.as_list()[-1]\n  batch_shape_reverse = indices.shape.as_list()[:-1]\n  batch_shape_reverse.reverse()\n  index_matrix = tf.constant(\n      np.flip(np.transpose(np.indices(batch_shape_reverse)), -1),\n      dtype=indices.dtype)\n  batch_rank = len(batch_shape)\n  # Broadcast index matrix to the shape of\n  # `batch_shape + [num_points] + [batch_rank]`\n  broadcasted_shape = batch_shape + [num_points] + [batch_rank]\n  index_matrix = tf.expand_dims(index_matrix, -2) + tf.zeros(\n      broadcasted_shape, dtype=indices.dtype)\n  return index_matrix\n'"
tf_quant_finance/math/interpolation/linear/linear_interpolation_test.py,24,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for nomisma_quant_finance.math.interpolation.linear.interpolate.""""""\n\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass LinearInterpolation(tf.test.TestCase, parameterized.TestCase):\n  """"""Tests for methods in linear_interpolation module.""""""\n\n  @parameterized.named_parameters(\n      (\'default_interpolation\', False),\n      (\'one_hot_interpolation\', True),\n  )\n  def test_linear_interpolation_const_extrapolation_default_dtype(\n      self, optimize_for_tpu):\n    """"""Tests linear interpolation with const extrapolation.""""""\n    x = [-10.0, -1.0, 1.0, 3.0, 6.0, 7.0, 8.0, 15.0, 18.0, 25.0, 30.0, 35.0]\n    x_data = [-1.0, 2.0, 6.0, 8.0, 18.0, 30.0]\n    y_data = [10.0, -1.0, -5.0, 7.0, 9.0, 20.0]\n    result = self.evaluate(\n        tff.math.interpolation.linear.interpolate(\n            x, x_data, y_data,\n            optimize_for_tpu=optimize_for_tpu))\n    self.assertAllClose(result,\n                        [np.interp(x_coord, x_data, y_data) for x_coord in x],\n                        1e-8)\n    # All above real would be converted to float32.\n    self.assertIsInstance(result[0], np.float32)\n\n  @parameterized.named_parameters(\n      (\'default_interpolation\', False),\n      (\'one_hot_interpolation\', True),\n  )\n  def test_linear_interpolation_const_extrapolation(\n      self, optimize_for_tpu):\n    """"""Tests linear interpolation with const extrapolation.""""""\n    x = [-10, -1, 1, 3, 6, 7, 8, 15, 18, 25, 30, 35]\n    x_data = [-1, 2, 6, 8, 18, 30]\n    y_data = [10, -1, -5, 7, 9, 20]\n    result = self.evaluate(tff.math.interpolation.linear.interpolate(\n        x, x_data, y_data,\n        optimize_for_tpu=optimize_for_tpu,\n        dtype=tf.float32))\n    self.assertAllClose(result,\n                        [np.interp(x_coord, x_data, y_data) for x_coord in x],\n                        1e-8)\n    self.assertIsInstance(result[0], np.float32)\n\n  @parameterized.named_parameters(\n      (\'default_interpolation\', False),\n      (\'one_hot_interpolation\', True),\n  )\n  def test_linear_interpolation_nonconst_extrapolation(\n      self, optimize_for_tpu):\n    """"""Tests linear interpolation with nonconst extrapolation.""""""\n    x = [-10, -2, -1, 1, 3, 6, 7, 8, 15, 18, 25, 30, 31, 35]\n    x_data = np.array([-1, 2, 6, 8, 18, 30])\n    y_data_as_list = [10, -1, -5, 7, 9, 20]\n    y_data = tf.convert_to_tensor(y_data_as_list, dtype=tf.float64)\n    left_slope = 2.0\n    right_slope = -3.0\n    result = self.evaluate(\n        tff.math.interpolation.linear.interpolate(\n            x,\n            x_data,\n            y_data,\n            left_slope=left_slope,\n            right_slope=right_slope,\n            optimize_for_tpu=optimize_for_tpu,\n            dtype=tf.float64))\n    expected_left = 10.0 + left_slope * (np.array([-10.0, -2.0]) - (-1.0))\n    expected_right = 20.0 + right_slope * (np.array([31.0, 35.0]) - 30.0)\n    expected_middle = [\n        np.interp(x_coord, x_data, y_data_as_list) for x_coord in x[2:-2]\n    ]\n    self.assertAllClose(\n        result, np.concatenate([expected_left, expected_middle,\n                                expected_right]), 1e-8)\n\n  @parameterized.named_parameters(\n      (\'default_interpolation\', False),\n      (\'one_hot_interpolation\', True),\n  )\n  def test_linear_interpolation_repeating_values(\n      self, optimize_for_tpu):\n    """"""Tests linear interpolation with repeating values in x_data.""""""\n    x = [1.5]\n    # Points (1, 1) and (2, 3) should be used to interpolate x=1.5.\n    x_data = [0, 1, 1, 2, 2, 3]\n    y_data = [0, 0, 1, 2, 3, 3]\n\n    result = self.evaluate(\n        tff.math.interpolation.linear.interpolate(\n            x, x_data, y_data, optimize_for_tpu=optimize_for_tpu,\n            dtype=tf.float32))\n    self.assertAllClose(result, [1.5], 1e-8)\n\n  @parameterized.named_parameters(\n      (\'default_interpolation\', False),\n      (\'one_hot_interpolation\', True),\n  )\n  def test_linear_interpolation_unequal_lengths_xys(\n      self, optimize_for_tpu):\n    """"""Tests incompatible `x_data` and `y_data`.""""""\n    x = [1, 2]\n    x_data = [-1, 2, 6, 8, 18]\n    y_data = [10, -1, -5, 7, 9, 20]\n    with self.assertRaises((tf.errors.InvalidArgumentError, ValueError)):\n      self.evaluate(\n          tff.math.interpolation.linear.interpolate(\n              x, x_data, y_data, validate_args=True,\n              optimize_for_tpu=optimize_for_tpu,\n              dtype=tf.float64))\n\n  @parameterized.named_parameters(\n      (\'default_interpolation\', False),\n      (\'one_hot_interpolation\', True),\n  )\n  def test_linear_interpolation_empty_xys(self, optimize_for_tpu):\n    """"""Tests an error would be thrown if knots are empty.""""""\n    x = [1, 2]\n    x_data = []\n    y_data = []\n    with self.assertRaises((tf.errors.InvalidArgumentError, ValueError)):\n      self.evaluate(\n          tff.math.interpolation.linear.interpolate(\n              x, x_data, y_data,\n              optimize_for_tpu=optimize_for_tpu,\n              dtype=tf.float64))\n\n  @parameterized.named_parameters(\n      (\'default_interpolation\', False),\n      (\'one_hot_interpolation\', True),\n  )\n  def test_linear_interpolation_const_extrapolation_batching(\n      self, optimize_for_tpu):\n    """"""Tests linear interpolation with const extrapolation and batching.""""""\n    x = [[0, 1.5, 4], [4, 5.5, 8]]\n    x_data = [[1, 2, 3], [5, 6, 7]]\n    y_data = [[0, 2, 4], [1, 2, 3]]\n    result = self.evaluate(\n        tff.math.interpolation.linear.interpolate(\n            x, x_data, y_data,\n            optimize_for_tpu=optimize_for_tpu,\n            dtype=tf.float32))\n    self.assertAllClose(result, np.array([[0, 1, 4], [1, 1.5, 3]]), 1e-8)\n\n  @parameterized.named_parameters(\n      (\'default_interpolation\', False),\n      (\'one_hot_interpolation\', True),\n  )\n  def test_linear_interpolation_multiple_batching_dimensions(\n      self, optimize_for_tpu):\n    """"""Tests linear interpolation with multiple batching dimensions.""""""\n    for dtype in (np.float32, np.float64):\n      x = np.array([[[1.5], [3.5]]], dtype=dtype)\n      x_data = np.array([[[1, 2], [3, 4]]], dtype=dtype)\n      y_data = np.array([[[0, 1], [2, 3]]], dtype=dtype)\n      result = self.evaluate(\n          tff.math.interpolation.linear.interpolate(\n              x, x_data, y_data,\n              optimize_for_tpu=optimize_for_tpu))\n      self.assertEqual(result.dtype, dtype)\n      self.assertAllClose(result, np.array([[[0.5], [2.5]]]), 1e-8)\n\n  @parameterized.named_parameters(\n      (\'default_interpolation\', False),\n      (\'one_hot_interpolation\', True),\n  )\n  def test_linear_interpolation_non_const_extrapolation_batching(\n      self, optimize_for_tpu):\n    """"""Tests linear interpolation with non-const extrapolation and batching.""""""\n    x = [[0, 1.5, 4], [4, 5.5, 8]]\n    x_data = [[1, 2, 3], [5, 6, 7]]\n    y_data = [[0, 2, 4], [1, 2, 3]]\n    left_slope = [[1], [1]]\n    right_slope = [[-1], [-1]]\n\n    result = self.evaluate(\n        tff.math.interpolation.linear.interpolate(\n            x, x_data, y_data, left_slope, right_slope,\n            optimize_for_tpu=optimize_for_tpu,\n            dtype=tf.float32))\n    self.assertAllClose(result, np.array([[-1, 1, 3], [0, 1.5, 2]]), 1e-8)\n\n  @parameterized.named_parameters(\n      (\'default_interpolation\', False),\n      (\'one_hot_interpolation\', True),\n  )\n  def test_linear_interpolation_x_data_not_increasing(\n      self, optimize_for_tpu):\n    """"""Tests linear interpolation when x_data is not increasing.""""""\n    x = [[0, 1.5, 4], [4, 5.5, 8]]\n    x_data = [[1, 2, 3], [5, 7, 6]]\n    y_data = [[0, 2, 4], [1, 2, 3]]\n\n    with self.assertRaises((tf.errors.InvalidArgumentError, ValueError)):\n      self.evaluate(\n          tff.math.interpolation.linear.interpolate(\n              x, x_data, y_data,\n              validate_args=True,\n              optimize_for_tpu=optimize_for_tpu,\n              dtype=tf.float32))\n\n  @parameterized.named_parameters(\n      (\'default_interpolation\', False),\n      (\'one_hot_interpolation\', True),\n  )\n  def test_valid_gradients(self, optimize_for_tpu):\n    """"""Tests none of the gradients is nan.""""""\n\n    # In this example, `x[0]` and `x[1]` are both less than or equal to\n    # `x_data[0]`. `x[-2]` and `x[-1]` are both greater than or equal to\n    # `x_data[-1]`. They are set up this way to test none of the tf.where\n    # branches of the implementation have any nan. An unselected nan could still\n    # propagate through gradient calculation with the end result being nan.\n    x = [[-10.0, -1.0, 1.0, 3.0, 6.0, 7.0], [8.0, 15.0, 18.0, 25.0, 30.0, 35.0]]\n    x_data = [[-1.0, 2.0, 6.0], [8.0, 18.0, 30.0]]\n\n    def _value_helper_fn(y_data):\n      """"""A helper function that returns sum of squared interplated values.""""""\n\n      interpolated_values = tff.math.interpolation.linear.interpolate(\n          x, x_data, y_data,\n          optimize_for_tpu=optimize_for_tpu,\n          dtype=tf.float64)\n      return tf.reduce_sum(tf.math.square(interpolated_values))\n\n    y_data = tf.convert_to_tensor([[10.0, -1.0, -5.0], [7.0, 9.0, 20.0]],\n                                  dtype=tf.float64)\n    if tf.executing_eagerly():\n      with tf.GradientTape(watch_accessed_variables=False) as tape:\n        tape.watch(y_data)\n        value = _value_helper_fn(y_data=y_data)\n        gradients = tape.gradient(value, y_data)\n    else:\n      value = _value_helper_fn(y_data=y_data)\n      gradients = tf.gradients(value, y_data)[0]\n\n    gradients = tf.convert_to_tensor(gradients)\n\n    self.assertFalse(self.evaluate(tf.reduce_any(tf.math.is_nan(gradients))))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/math/pde/steppers/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Steppers for PDE solvers.""""""\n\nfrom tf_quant_finance.math.pde.steppers import composite_stepper\nfrom tf_quant_finance.math.pde.steppers import crank_nicolson\nfrom tf_quant_finance.math.pde.steppers import douglas_adi\nfrom tf_quant_finance.math.pde.steppers import explicit\nfrom tf_quant_finance.math.pde.steppers import extrapolation\nfrom tf_quant_finance.math.pde.steppers import implicit\nfrom tf_quant_finance.math.pde.steppers import multidim_parabolic_equation_stepper\nfrom tf_quant_finance.math.pde.steppers import oscillation_damped_crank_nicolson\nfrom tf_quant_finance.math.pde.steppers import parabolic_equation_stepper\nfrom tf_quant_finance.math.pde.steppers import weighted_implicit_explicit\n\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'composite_stepper\',\n    \'crank_nicolson\',\n    \'douglas_adi\',\n    \'explicit\',\n    \'extrapolation\',\n    \'implicit\',\n    \'multidim_parabolic_equation_stepper\',\n    \'oscillation_damped_crank_nicolson\',\n    \'parabolic_equation_stepper\',\n    \'weighted_implicit_explicit\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/math/pde/steppers/composite_stepper.py,1,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Composition of two time marching schemes.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math.pde.steppers.parabolic_equation_stepper import parabolic_equation_step\n\n\ndef composite_scheme_step(first_scheme_steps, first_scheme, second_scheme):\n  """"""Composes two time marching schemes.\n\n  Applies a step of parabolic PDE solver using `first_scheme` if number of\n  performed steps is less than `first_scheme_steps`, and using `second_scheme`\n  otherwise.\n\n  Args:\n    first_scheme_steps: A Python integer. Number of steps to apply\n      `first_scheme` on.\n    first_scheme: First time marching scheme (see `time_marching_scheme`\n      argument of `parabolic_equation_step`).\n    second_scheme: Second time marching scheme (see `time_marching_scheme`\n      argument of `parabolic_equation_step`).\n\n  Returns:\n     Callable to be used in finite-difference PDE solvers (see fd_solvers.py).\n  """"""\n\n  def step_fn(\n      time,\n      next_time,\n      coord_grid,\n      value_grid,\n      boundary_conditions,\n      second_order_coeff_fn,\n      first_order_coeff_fn,\n      zeroth_order_coeff_fn,\n      inner_second_order_coeff_fn,\n      inner_first_order_coeff_fn,\n      num_steps_performed,\n      dtype=None,\n      name=None):\n    """"""Performs the step.""""""\n    name = name or \'composite_scheme_step\'\n\n    def scheme(*args, **kwargs):\n      return tf.cond(num_steps_performed < first_scheme_steps,\n                     lambda: first_scheme(*args, **kwargs),\n                     lambda: second_scheme(*args, **kwargs))\n    return parabolic_equation_step(\n        time,\n        next_time,\n        coord_grid,\n        value_grid,\n        boundary_conditions,\n        second_order_coeff_fn,\n        first_order_coeff_fn,\n        zeroth_order_coeff_fn,\n        inner_second_order_coeff_fn,\n        inner_first_order_coeff_fn,\n        time_marching_scheme=scheme,\n        dtype=dtype,\n        name=name)\n\n  return step_fn\n\n\n__all__ = [\'composite_scheme_step\']\n'"
tf_quant_finance/math/pde/steppers/crank_nicolson.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Crank-Nicolson time marching scheme for parabolic PDEs.""""""\n\nfrom tf_quant_finance.math.pde.steppers.parabolic_equation_stepper import parabolic_equation_step\nfrom tf_quant_finance.math.pde.steppers.weighted_implicit_explicit import weighted_implicit_explicit_scheme\n\n\ndef crank_nicolson_step():\n  """"""Creates a stepper function with Crank-Nicolson time marching scheme.\n\n  Crank-Nicolson time marching scheme is one of the the most widely used schemes\n  for 1D PDEs. Given a space-discretized equation\n\n  ```\n  du/dt = A(t) u(t) + b(t)\n  ```\n  (here `u` is a value vector, `A` and `b` are the matrix and the vector defined\n  by the PDE), it approximates the right-hand side as an average of values taken\n  before and after the time step:\n\n  ```\n  (u(t2) - u(t1)) / (t2 - t1) = (A(t1) u(t1) + b(t1) + A(t2) u(t2) + b(t2)) / 2.\n  ```\n\n  Crank-Nicolson has second order accuracy and is stable.\n\n  More details can be found in `weighted_implicit_explicit.py` describing the\n  weighted implicit-explicit scheme - Crank-Nicolson scheme is a special case\n  with `theta = 0.5`.\n\n  Returns:\n    Callable to be used in finite-difference PDE solvers (see fd_solvers.py).\n  """"""\n\n  def step_fn(\n      time,\n      next_time,\n      coord_grid,\n      value_grid,\n      boundary_conditions,\n      second_order_coeff_fn,\n      first_order_coeff_fn,\n      zeroth_order_coeff_fn,\n      inner_second_order_coeff_fn,\n      inner_first_order_coeff_fn,\n      num_steps_performed,\n      dtype=None,\n      name=None):\n    """"""Performs the step.""""""\n    del num_steps_performed\n    name = name or \'crank_nicolson_step\'\n    return parabolic_equation_step(time,\n                                   next_time,\n                                   coord_grid,\n                                   value_grid,\n                                   boundary_conditions,\n                                   second_order_coeff_fn,\n                                   first_order_coeff_fn,\n                                   zeroth_order_coeff_fn,\n                                   inner_second_order_coeff_fn,\n                                   inner_first_order_coeff_fn,\n                                   time_marching_scheme=crank_nicolson_scheme,\n                                   dtype=dtype,\n                                   name=name)\n  return step_fn\n\n\ncrank_nicolson_scheme = weighted_implicit_explicit_scheme(theta=0.5)\n\n\n__all__ = [\'crank_nicolson_step\', \'crank_nicolson_scheme\']\n'"
tf_quant_finance/math/pde/steppers/douglas_adi.py,16,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Douglas ADI method for solving multidimensional parabolic PDEs.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tf_quant_finance.math.pde.steppers.multidim_parabolic_equation_stepper import multidim_parabolic_equation_step\n\n\ndef douglas_adi_step(theta=0.5):\n  """"""Creates a stepper function with Crank-Nicolson time marching scheme.\n\n  Douglas ADI scheme is the simplest time marching scheme for solving parabolic\n  PDEs with multiple spatial dimensions. The time step consists of several\n  substeps: the first one is fully explicit, and the following `N` steps are\n  implicit with respect to contributions of one of the `N` axes (hence ""ADI"" -\n  alternating direction implicit). See `douglas_adi_scheme` below for more\n  details.\n\n  Args:\n    theta: positive Number. `theta = 0` corresponds to fully explicit scheme.\n    The larger `theta` the stronger are the corrections by the implicit\n    substeps. The recommended value is `theta = 0.5`, because the scheme is\n    second order accurate in that case, unless mixed second derivative terms are\n    present in the PDE.\n  Returns:\n    Callable to be used in finite-difference PDE solvers (see fd_solvers.py).\n  """"""\n  def _step_fn(\n      time,\n      next_time,\n      coord_grid,\n      value_grid,\n      boundary_conditions,\n      second_order_coeff_fn,\n      first_order_coeff_fn,\n      zeroth_order_coeff_fn,\n      inner_second_order_coeff_fn,\n      inner_first_order_coeff_fn,\n      num_steps_performed,\n      dtype=None,\n      name=None):\n    """"""Performs the step.""""""\n    del num_steps_performed\n    name = name or \'douglas_adi_step\'\n    return multidim_parabolic_equation_step(time,\n                                            next_time,\n                                            coord_grid,\n                                            value_grid,\n                                            boundary_conditions,\n                                            douglas_adi_scheme(theta),\n                                            second_order_coeff_fn,\n                                            first_order_coeff_fn,\n                                            zeroth_order_coeff_fn,\n                                            inner_second_order_coeff_fn,\n                                            inner_first_order_coeff_fn,\n                                            dtype=dtype,\n                                            name=name)\n  return _step_fn\n\n\ndef douglas_adi_scheme(theta):\n  """"""Applies Douglas time marching scheme (see [1] and Eq. 3.1 in [2]).\n\n  Time marching schemes solve the space-discretized equation\n  `du/dt = A(t) u(t) + b(t)` where `u` and `b` are vectors and `A` is a matrix;\n  see more details in multidim_parabolic_equation_stepper.py.\n\n  In Douglas scheme (as well as other ADI schemes), the matrix `A` is\n  represented as sum `A = sum_i A_i + A_mixed`. `A_i` is the contribution of\n  terms with partial derivatives w.r.t. dimension `i`, and `A_mixed` is the\n  contribution of all the mixed-derivative terms. The shift term is split evenly\n  between `A_i`. Similarly, inhomogeneous term is represented as sum `b = sum_i\n  b_i`, where `b_i` comes from boundary conditions on boundary orthogonal to\n  dimension `i`.\n\n  Given the current values vector u(t1), the step is defined as follows\n  (using the notation of Eq. 3.1 in [2]):\n  `Y_0 = (1 + A(t1) dt) U_{n-1} + b(t1) dt`,\n  `Y_j = Y_{j-1} + theta dt (A_j(t2) Y_j - A_j(t1) U_{n-1} + b_j(t2) - b_j(t1))`\n  for each spatial dimension `j`, and\n  `U_n = Y_{n_dims-1}`.\n\n  Here the parameter `theta` is a non-negative number, `U_{n-1} = u(t1)`,\n  `U_n = u(t2)`, and `dt = t2 - t1`.\n\n  Note: Douglas scheme is only first-order accurate if mixed terms are\n  present. More advanced schemes, such as Craig-Sneyd scheme, are needed to\n  achieve the second-order accuracy.\n\n  #### References:\n  [1] Douglas Jr., Jim (1962), ""Alternating direction methods for three space\n    variables"", Numerische Mathematik, 4 (1): 41-63\n  [2] Tinne Haentjens, Karek J. in\'t Hout. ADI finite difference schemes for\n    the Heston-Hull-White PDE. https://arxiv.org/abs/1111.4087\n\n  Args:\n    theta: Number between 0 and 1 (see the step definition above). `theta = 0`\n      corresponds to fully-explicit scheme.\n\n  Returns:\n    A callable consumes the following arguments by keyword:\n      1. inner_value_grid: Grid of solution values at the current time of\n        the same `dtype` as `value_grid` and shape of `value_grid[..., 1:-1]`.\n      2. t1: Time before the step.\n      3. t2: Time after the step.\n      4. equation_params_fn: A callable that takes a scalar `Tensor` argument\n        representing time, and constructs the tridiagonal matrix `A`\n        (a tuple of three `Tensor`s, main, upper, and lower diagonals)\n        and the inhomogeneous term `b`. All of the `Tensor`s are of the same\n        `dtype` as `inner_value_grid` and of the shape broadcastable with the\n        shape of `inner_value_grid`.\n      5. n_dims: A Python integer, the spatial dimension of the PDE.\n    The callable returns a `Tensor` of the same shape and `dtype` a\n    `values_grid` and represents an approximate solution `u(t2)`.\n  """"""\n\n  if theta < 0 or theta > 1:\n    raise ValueError(\'Theta should be in the interval [0, 1].\')\n\n  def _marching_scheme(value_grid, t1, t2, equation_params_fn, n_dims):\n    """"""Constructs the Douglas ADI time marching scheme.""""""\n    current_grid = value_grid\n    matrix_params_t1, inhomog_terms_t1 = equation_params_fn(t1)\n    matrix_params_t2, inhomog_terms_t2 = equation_params_fn(t2)\n\n    # Explicit substep: Y_0 = (1 + A(t1) dt) U_{n-1} + b(t1) dt,\n    # where dt = t2 - t1\n    for i in range(n_dims - 1):\n      for j in range(i + 1, n_dims):\n        mixed_term = matrix_params_t1[i][j]\n        if mixed_term is not None:\n          current_grid += _apply_mixed_term_explicitly(value_grid, mixed_term,\n                                                       t2 - t1, i, j, n_dims)\n\n    # These are A_i(t1) * U_{n-1} * dt; caching them because they appear again\n    # later in the correction substeps.\n    explicit_contributions = []\n\n    for i in range(n_dims):\n      superdiag, diag, subdiag = (matrix_params_t1[i][i][d] for d in range(3))\n      contribution = _apply_tridiag_matrix_explicitly(\n          value_grid, superdiag, diag, subdiag, i, n_dims) * (t2 - t1)\n      explicit_contributions.append(contribution)\n      current_grid += contribution\n\n    for inhomog_term in inhomog_terms_t1:\n      current_grid += inhomog_term * (t2 - t1)\n\n    # Correction substeps. For each dimension i:\n    # Y_i = (1 - theta * A_i(t2) * dt)^(-1) *\n    #      (Y_{i-1} - theta * dt * A_i(t1) * U_{n-1} + dt * (b_i(t2) - b_i(t1)))\n    if theta == 0:\n      return current_grid\n\n    for i in range(n_dims):\n      inhomog_term_delta = (inhomog_terms_t2[i] - inhomog_terms_t2[i])\n      superdiag, diag, subdiag = (matrix_params_t2[i][i][d] for d in range(3))\n      current_grid = _apply_correction(theta, current_grid,\n                                       explicit_contributions[i],\n                                       superdiag, diag, subdiag,\n                                       inhomog_term_delta, t1, t2, i, n_dims)\n\n    return current_grid\n  return _marching_scheme\n\n\ndef _apply_mixed_term_explicitly(values, mixed_term, delta_t, dim1, dim2,\n                                 n_dims):\n  """"""Applies mixed term explicitly.""""""\n  mixed_term_pp, mixed_term_pm, mixed_term_mp, mixed_term_mm = mixed_term\n  batch_rank = len(values.shape) - n_dims\n  dim1 += batch_rank\n  dim2 += batch_rank\n  shift_right = _shift(values, dim1, 1)\n  shift_right_down = _shift(shift_right, dim2, 1)\n  shift_right_up = _shift(shift_right, dim2, -1)\n  shift_left = _shift(values, dim1, -1)\n  shift_left_down = _shift(shift_left, dim2, 1)\n  shift_left_up = _shift(shift_left, dim2, -1)\n  return (mixed_term_mm * shift_right_down +\n          mixed_term_mp * shift_right_up +\n          mixed_term_pm * shift_left_down +\n          mixed_term_pp * shift_left_up) * delta_t\n\n\ndef _apply_tridiag_matrix_explicitly(values, superdiag, diag, subdiag,\n                                     dim, n_dims):\n  """"""Applies tridiagonal matrix explicitly.""""""\n  perm = _get_permutation(values, n_dims, dim)\n\n  # Make the given dimension the last one in the tensors, treat all the\n  # other spatial dimensions as batch dimensions.\n  if perm is not None:\n    values = tf.transpose(values, perm)\n    superdiag, diag, subdiag = (\n        tf.transpose(c, perm) for c in (superdiag, diag, subdiag))\n\n  values = tf.squeeze(\n      tf.linalg.tridiagonal_matmul((superdiag, diag, subdiag),\n                                   tf.expand_dims(values, -1),\n                                   diagonals_format=\'sequence\'), -1)\n\n  # Transpose back to how it was.\n  if perm is not None:\n    values = tf.transpose(values, perm)\n  return values\n\n\ndef _apply_correction(theta, values, explicit_contribution, superdiag, diag,\n                      subdiag, inhomog_term_delta, t1, t2, dim, n_dims):\n  """"""Applies correction for the given dimension.""""""\n  rhs = (\n      values - theta * explicit_contribution +\n      theta * inhomog_term_delta * (t2 - t1))\n\n  # Make the given dimension the last one in the tensors, treat all the\n  # other spatial dimensions as batch dimensions.\n  perm = _get_permutation(values, n_dims, dim)\n  if perm is not None:\n    rhs = tf.transpose(rhs, perm)\n    superdiag, diag, subdiag = (\n        tf.transpose(c, perm) for c in (superdiag, diag, subdiag))\n\n  subdiag = -theta * subdiag * (t2 - t1)\n  diag = 1 - theta * diag * (t2 - t1)\n  superdiag = -theta * superdiag * (t2 - t1)\n  result = tf.linalg.tridiagonal_solve((superdiag, diag, subdiag),\n                                       rhs,\n                                       diagonals_format=\'sequence\',\n                                       partial_pivoting=False)\n\n  # Transpose back to how it was.\n  if perm is not None:\n    result = tf.transpose(result, perm)\n  return result\n\n\ndef _shift(tensor, axis, delta):\n  """"""Shifts the given tensor, filling it with zeros on the other side.\n\n  Args:\n    tensor: `Tensor`.\n    axis: Axis to shift along.\n    delta: Shift size. May be negative: the sign determines the direction of the\n      shift.\n\n  Returns:\n    Shifted `Tensor`.\n\n  Example:\n  ```\n  t = [[1, 2, 3]\n       [4, 5, 6]\n       [7, 8, 9]]\n  _shift(t, 1, 2) = [[0, 0, 1]\n                     [0, 0, 4]\n                     [0, 0, 7]]\n  _shift(t, 0, -1) = [[4, 5, 6]\n                      [7, 8, 9]\n                      [0, 0, 0]]\n\n  TODO(b/144087751): implement this in C++. Perhaps we can add a parameter to\n  tf.roll, so that it fills ""the other side"" with zeros.\n  """"""\n  rank = len(tensor.shape)\n  zeros_shape = np.zeros(rank)\n  for d in range(rank):\n    if d == axis:\n      zeros_shape[d] = np.abs(delta)\n    else:\n      zeros_shape[d] = tf.compat.dimension_value(tensor.shape[d])\n\n  zeros = tf.zeros(zeros_shape, dtype=tensor.dtype)\n\n  slice_begin = np.zeros(rank, dtype=np.int32)\n  slice_size = -np.ones(rank, dtype=np.int32)\n  if delta > 0:\n    slice_size[axis] = tf.compat.dimension_value(tensor.shape[axis]) - delta\n    return tf.concat((zeros, tf.slice(tensor, slice_begin, slice_size)),\n                     axis=axis)\n  else:\n    slice_begin[axis] = -delta\n    return tf.concat((tf.slice(tensor, slice_begin, slice_size), zeros),\n                     axis=axis)\n\n\ndef _get_permutation(tensor, n_dims, active_dim):\n  """"""Returns the permutation that swaps the active and the last dimensions.\n\n  Args:\n    tensor: `Tensor` having a statically known rank.\n    n_dims: Number of spatial dimensions.\n    active_dim: The active spatial dimension.\n\n  Returns:\n    A list representing the permutation, or `None` if no permutation needed.\n\n  For example, with \'tensor` having rank 5, `n_dims = 3` and `active_dim = 1`\n  yields [0, 1, 2, 4, 3]. Explanation: we start with [0, 1, 2, 3, 4], where the\n  last n_dims=3 dimensions are spatial dimensions, and the first two are batch\n  dimensions. Among the spatial dimensions, we take the one at index 1, which\n  is ""3"", and swap it with the last dimension ""4"".\n  """"""\n  if not tensor.shape:\n    raise ValueError(""Tensor\'s rank should be static"")\n  rank = len(tensor.shape)\n  batch_rank = rank - n_dims\n  if active_dim == n_dims - 1:\n    return None\n  perm = np.arange(rank)\n  perm[rank - 1] = batch_rank + active_dim\n  perm[batch_rank + active_dim] = rank - 1\n  return perm\n\n\n__all__ = [\'douglas_adi_step\', \'douglas_adi_scheme\']\n'"
tf_quant_finance/math/pde/steppers/douglas_adi_scheme_test.py,9,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Time marching schemes for finite difference methods for parabolic PDEs.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\ndouglas_adi_scheme = tff.math.pde.steppers.douglas_adi.douglas_adi_scheme\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass DouglasAdiSchemeTest(tf.test.TestCase):\n\n  def test_douglas_step_2d(self):\n    u = np.arange(1, 17, dtype=np.float32).reshape(4, 4)\n    d = np.arange(11, 27, dtype=np.float32).reshape(4, 4)\n    dx = np.array([d, -3 * d, 2 * d])\n    dy = np.array([2 * d, -6 * d, 4 * d])\n    dxy = np.arange(-8, 8, dtype=np.float32).reshape(4, 4)\n    bx = np.arange(2, 18, dtype=np.float32).reshape(4, 4)\n    by = np.arange(5, 21, dtype=np.float32).reshape(4, 4)\n    theta = 0.3\n\n    def equation_params_fn(t):\n      del t\n      return ([[_tfconst(dy), _spread_mixed_term(_tfconst(dxy))],\n               [None, _tfconst(dx)]],\n              [_tfconst(by), _tfconst(bx)])\n\n    scheme = douglas_adi_scheme(theta=theta)\n    actual = self.evaluate(\n        scheme(value_grid=tf.constant(u, dtype=tf.float32), t1=0, t2=1,\n               equation_params_fn=equation_params_fn,\n               n_dims=2))\n    expected = self._simplified_douglas_step_2d(u, dx, dy, dxy, bx, by,\n                                                0, 1, theta)\n    self.assertLess(np.max(np.abs(expected - actual)), 0.01)\n\n  def test_douglas_step_3d(self):\n    u = np.arange(0, 80, dtype=np.float32).reshape(4, 4, 5)\n    d = np.arange(10, 90, dtype=np.float32).reshape(4, 4, 5)\n    dx = np.array([d, -3 * d, 2 * d])\n    dy = 2 * dx\n    dz = 3 * dx\n    dxy = np.arange(-20, 60, dtype=np.float32).reshape(4, 4, 5)\n    dyz = 2 * dxy\n    dxz = 3 * dxy\n    bx = np.arange(20, 100, dtype=np.float32).reshape(4, 4, 5)\n    by = np.arange(30, 110, dtype=np.float32).reshape(4, 4, 5)\n    bz = np.arange(40, 120, dtype=np.float32).reshape(4, 4, 5)\n    theta = 0.3\n\n    def equation_params_fn(t):\n      del t\n      dyz_spread = _spread_mixed_term(_tfconst(dyz))\n      dxz_spread = _spread_mixed_term(_tfconst(dxz))\n      dxy_spread = _spread_mixed_term(_tfconst(dxy))\n      return ([[_tfconst(dz), dyz_spread, dxz_spread],\n               [dyz_spread, _tfconst(dy), dxy_spread],\n               [dxz_spread, dxy_spread, _tfconst(dx)]],\n              [_tfconst(bz), _tfconst(by), _tfconst(bx)])\n\n    scheme = douglas_adi_scheme(theta=theta)\n    actual = self.evaluate(\n        scheme(value_grid=tf.constant(u, dtype=tf.float32), t1=0, t2=1,\n               equation_params_fn=equation_params_fn, n_dims=3))\n    expected = self._simplified_douglas_step_3d(u, dx, dy, dz, dxy, dyz, dxz,\n                                                bx, by, bz, 0, 1, theta)\n    self.assertLess(np.max(np.abs(expected - actual)), 0.01)\n\n  def _simplified_douglas_step_2d(self, u, dx, dy, dxy, bx, by, t1, t2,\n                                  theta):\n    # Simplified version of the step to test against: fixed number of\n    # dimensions, np instead of tf, for loops, etc.\n    dt = t2 - t1\n\n    # u0 = (1 + A * dt) u + b * dt\n    dx_contrib = (\n        dx[0] * _np_shift(u, 1, -1) + dx[1] * u + dx[2] * _np_shift(u, 1, 1))\n    dy_contrib = (\n        dy[0] * _np_shift(u, 0, -1) + dy[1] * u + dy[2] * _np_shift(u, 0, 1))\n    dxy_contrib = dxy * (\n        _np_shift(_np_shift(u, 0, 1), 1, 1) - _np_shift(\n            _np_shift(u, 0, 1), 1, -1) - _np_shift(_np_shift(u, 0, -1), 1, 1) +\n        _np_shift(_np_shift(u, 0, -1), 1, -1))\n    u0 = u + (dx_contrib + dy_contrib + dxy_contrib) * dt\n    u0 += (bx + by) * dt\n\n    # u1 = (1 - theta * dt * A_y)^(-1) (u0 - theta * dt * A_y u)\n    theta *= dt  # Theta is always multiplied by dt.\n    rhs = u0 - theta * (\n        dy[0] * _np_shift(u, 0, -1) + dy[1] * u + dy[2] * _np_shift(u, 0, 1))\n\n    u1 = np.zeros_like(u0)\n    for i in range(u0.shape[1]):\n      diags = np.array(\n          [-theta * dy[0, :, i], 1 - theta * dy[1, :, i], -theta * dy[2, :, i]])\n      u1[:, i] = self._np_tridiagonal_solve(diags, rhs[:, i])\n\n    # u2 = (1 - theta * dt * A_x)^(-1) (u1 - theta * dt * A_x u)\n    rhs = u1 - theta * (\n        dx[0] * _np_shift(u, 1, -1) + dx[1] * u + dx[2] * _np_shift(u, 1, 1))\n    u2 = np.zeros_like(u0)\n    for i in range(u0.shape[0]):\n      diags = np.array(\n          [-theta * dx[0, i], 1 - theta * dx[1, i], -theta * dx[2, i]])\n      u2[i] = self._np_tridiagonal_solve(diags, rhs[i])\n    return u2\n\n  def _np_tridiagonal_solve(self, diags, rhs):\n    return self.evaluate(\n        tf.linalg.tridiagonal_solve(\n            tf.constant(diags, dtype=tf.float32),\n            tf.constant(rhs, dtype=tf.float32)))\n\n  def _simplified_douglas_step_3d(self, u, dx, dy, dz, dxy, dyz, dxz, bx,\n                                  by, bz, t1, t2, theta):\n    # Simplified version of the step to test against: fixed number of\n    # dimensions, np instead of tf, for loops, etc.\n    dt = t2 - t1\n\n    # u0 = (1 + dt * A) u + b * dt\n    def tridiag_contrib(tridiag_term, dim):\n      return (tridiag_term[0] * _np_shift(u, dim, -1) + tridiag_term[1] * u +\n              tridiag_term[2] * _np_shift(u, dim, 1)) * dt\n\n    def mixed_term_contrib(mixed_term, dim1, dim2):\n      return mixed_term * (_np_shift(_np_shift(u, dim1, 1), dim2, 1) -\n                           _np_shift(_np_shift(u, dim1, 1), dim2, -1) -\n                           _np_shift(_np_shift(u, dim1, -1), dim2, 1) +\n                           _np_shift(_np_shift(u, dim1, -1), dim2, -1)) * dt\n\n    u0 = (\n        u + tridiag_contrib(dx, 2) + tridiag_contrib(dy, 1) +\n        tridiag_contrib(dz, 0) + mixed_term_contrib(dxy, 2, 1) +\n        mixed_term_contrib(dyz, 1, 0) + mixed_term_contrib(dxz, 2, 0))\n    u0 += (bx + by + bz) * dt\n\n    # u1 = (1 - theta * dt * A_z)^(-1) (u0 - theta * dt * A_z u)\n    theta *= dt  # Theta is always multiplied by dt.\n    rhs = u0 - theta * (\n        dz[0] * _np_shift(u, 0, -1) + dz[1] * u + dz[2] * _np_shift(u, 0, 1))\n\n    u1 = np.zeros_like(u0)\n    for y in range(u0.shape[1]):\n      for x in range(u0.shape[2]):\n        diags = np.array([\n            -theta * dz[0, :, y, x], 1 - theta * dz[1, :, y, x],\n            -theta * dz[2, :, y, x]\n        ])\n        u1[:, y, x] = self._np_tridiagonal_solve(diags, rhs[:, y, x])\n\n    # u2 = (1 - theta * dt * A_y)^(-1) (u1 - theta * dt * A_y u)\n    rhs = u1 - theta * (\n        dy[0] * _np_shift(u, 1, -1) + dy[1] * u + dy[2] * _np_shift(u, 1, 1))\n    u2 = np.zeros_like(u0)\n    for z in range(u0.shape[0]):\n      for x in range(u0.shape[2]):\n        diags = np.array([\n            -theta * dy[0, z, :, x], 1 - theta * dy[1, z, :, x],\n            -theta * dy[2, z, :, x]\n        ])\n        u2[z, :, x] = self._np_tridiagonal_solve(diags, rhs[z, :, x])\n\n    # u3 = (1 - theta * dt * A_x)^(-1) (u2 - theta * dt * A_x u)\n    rhs = u2 - theta * (\n        dx[0] * _np_shift(u, 2, -1) + dx[1] * u + dx[2] * _np_shift(u, 2, 1))\n    u3 = np.zeros_like(u0)\n    for z in range(u0.shape[0]):\n      for y in range(u0.shape[1]):\n        diags = np.array([\n            -theta * dx[0, z, y, :], 1 - theta * dx[1, z, y, :],\n            -theta * dx[2, z, y, :]\n        ])\n        u3[z, y, :] = self._np_tridiagonal_solve(diags, rhs[z, y, :])\n\n    return u3\n\n  def test_douglas_step_with_batching(self):\n    u = np.arange(0, 80, dtype=np.float32).reshape(4, 4, 5)\n    d = np.arange(10, 90, dtype=np.float32).reshape(4, 4, 5)\n    dx = np.array([d, -3 * d, 2 * d])\n    dy = 2 * dx\n    dxy = (np.arange(-20, 60, dtype=np.float32).reshape(4, 4, 5)) * 4\n    theta = 0.3\n    bx = np.arange(20, 100, dtype=np.float32).reshape(4, 4, 5)\n    by = np.arange(30, 110, dtype=np.float32).reshape(4, 4, 5)\n\n    def equation_params_fn(t):\n      del t\n      return ([[_tfconst(dy), _spread_mixed_term(_tfconst(dxy))],\n               [None, _tfconst(dx)]],\n              [_tfconst(by), _tfconst(bx)])\n\n    scheme = douglas_adi_scheme(theta=theta)\n    actual = self.evaluate(\n        scheme(value_grid=tf.constant(u, dtype=tf.float32), t1=0, t2=1,\n               equation_params_fn=equation_params_fn, n_dims=2))\n    expected = np.zeros_like(u)\n    for i in range(4):\n      expected[i] = self._simplified_douglas_step_2d(u[i], dx[:, i], dy[:, i],\n                                                     dxy[i], bx[i],\n                                                     by[i], 0, 1, theta)\n\n    self.assertLess(np.max(np.abs(expected - actual)), 0.01)\n\n\ndef _np_shift(values, axis, delta):\n  values = np.roll(values, delta, axis)\n  sl = [slice(None)] * values.ndim\n  if delta > 0:\n    sl[axis] = slice(None, delta)\n  else:\n    sl[axis] = slice(delta, None)\n  values[sl] = 0\n  return values\n\n\ndef _tfconst(np_array):\n  return tf.constant(np_array, dtype=tf.float32)\n\n\ndef _spread_mixed_term(term):\n  # Turns non-diagonal element into a tuple of 4 elements representing 4\n  # diagonal points.\n  return term, -term, -term, term\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/math/pde/steppers/explicit.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Explicit time marching scheme for parabolic PDEs.""""""\n\nfrom tf_quant_finance.math.pde.steppers.parabolic_equation_stepper import parabolic_equation_step\nfrom tf_quant_finance.math.pde.steppers.weighted_implicit_explicit import weighted_implicit_explicit_scheme\n\n\ndef explicit_step():\n  """"""Creates a stepper function with explicit time marching scheme.\n\n  Explicit time marching scheme is the simplest scheme for 1D PDEs.\n  Given a space-discretized equation\n\n  ```\n  du/dt = A(t) u(t) + b(t)\n  ```\n  (here `u` is a value vector, `A` and `b` are the matrix and the vector defined\n  by the PDE), it approximates the right-hand side with its value before the\n  time step:\n\n  ```\n  (u(t2) - u(t1)) / (t2 - t1) = A(t1) u(t1) + b(t1)\n  ```\n  This scheme avoids any matrix inversions and thus is much faster than other\n  schemes. It is, however, stable only with small time steps and is only first\n  order accurate. Usually, Crank-Nicolson or Extrapolation schemes are\n  preferable.\n\n  More details can be found in `weighted_implicit_explicit.py` describing the\n  weighted implicit-explicit scheme - explicit scheme is a special case\n  with `theta = 1`.\n\n  Returns:\n    Callable to be used in finite-difference PDE solvers (see fd_solvers.py).\n  """"""\n  def step_fn(time,\n              next_time,\n              coord_grid,\n              value_grid,\n              boundary_conditions,\n              second_order_coeff_fn,\n              first_order_coeff_fn,\n              zeroth_order_coeff_fn,\n              inner_second_order_coeff_fn,\n              inner_first_order_coeff_fn,\n              num_steps_performed,\n              dtype=None,\n              name=None):\n    """"""Performs the step.""""""\n    del num_steps_performed\n    name = name or \'explicit_step\'\n    return parabolic_equation_step(time,\n                                   next_time,\n                                   coord_grid,\n                                   value_grid,\n                                   boundary_conditions,\n                                   second_order_coeff_fn,\n                                   first_order_coeff_fn,\n                                   zeroth_order_coeff_fn,\n                                   inner_second_order_coeff_fn,\n                                   inner_first_order_coeff_fn,\n                                   time_marching_scheme=explicit_scheme,\n                                   dtype=dtype,\n                                   name=name)\n  return step_fn\n\nexplicit_scheme = weighted_implicit_explicit_scheme(theta=1)\n\n\n__all__ = [\'explicit_step\', \'explicit_scheme\']\n'"
tf_quant_finance/math/pde/steppers/extrapolation.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Extrapolation time marching scheme for parabolic PDEs.""""""\n\nfrom tf_quant_finance.math.pde.steppers.implicit import implicit_scheme\nfrom tf_quant_finance.math.pde.steppers.parabolic_equation_stepper import parabolic_equation_step\n\n\ndef extrapolation_step():\n  """"""Creates a stepper function with Extrapolation time marching scheme.\n\n  Extrapolation scheme combines two half-steps and the full time step to obtain\n  desirable properties. See more details below in `extrapolation_scheme`.\n\n  It is slower than Crank-Nicolson scheme, but deals better with value grids\n  that have discontinuities. Consider also `oscillation_damped_crank_nicolson`,\n  an efficient combination of Crank-Nicolson and Extrapolation schemes.\n\n  Returns:\n    Callable to be used in finite-difference PDE solvers (see fd_solvers.py).\n  """"""\n  def step_fn(\n      time,\n      next_time,\n      coord_grid,\n      value_grid,\n      boundary_conditions,\n      second_order_coeff_fn,\n      first_order_coeff_fn,\n      zeroth_order_coeff_fn,\n      inner_second_order_coeff_fn,\n      inner_first_order_coeff_fn,\n      num_steps_performed,\n      dtype=None,\n      name=None):\n    """"""Performs the step.""""""\n    del num_steps_performed\n    name = name or \'extrapolation_step\'\n    return parabolic_equation_step(time,\n                                   next_time,\n                                   coord_grid,\n                                   value_grid,\n                                   boundary_conditions,\n                                   second_order_coeff_fn,\n                                   first_order_coeff_fn,\n                                   zeroth_order_coeff_fn,\n                                   inner_second_order_coeff_fn,\n                                   inner_first_order_coeff_fn,\n                                   time_marching_scheme=extrapolation_scheme,\n                                   dtype=dtype,\n                                   name=name)\n  return step_fn\n\n\ndef extrapolation_scheme(value_grid, t1, t2, equation_params_fn):\n  """"""Constructs extrapolation implicit-explicit scheme.\n\n  Performs two implicit half-steps, one full implicit step, and combines them\n  with such coefficients that ensure second-order errors. More computationally\n  expensive than Crank-Nicolson scheme, but provides a better approximation for\n  high-wavenumber components, which results in absence of oscillations typical\n  for Crank-Nicolson scheme in case of non-smooth initial conditions. See [1]\n  for details.\n\n  #### References:\n  [1]: D. Lawson, J & Ll Morris, J. The Extrapolation of First Order Methods\n  for Parabolic Partial Differential Equations. I. 1978\n  SIAM Journal on Numerical Analysis. 15. 1212-1224.\n  https://epubs.siam.org/doi/abs/10.1137/0715082\n\n  Args:\n    value_grid: A `Tensor` of real dtype. Grid of solution values at the current\n      time.\n    t1: Time before the step.\n    t2: Time after the step.\n    equation_params_fn: A callable that takes a scalar `Tensor` argument\n      representing time and constructs the tridiagonal matrix `A`\n      (a tuple of three `Tensor`s, main, upper, and lower diagonals)\n      and the inhomogeneous term `b`. All of the `Tensor`s are of the same\n      `dtype` as `inner_value_grid` and of the shape broadcastable with the\n      shape of `inner_value_grid`.\n\n  Returns:\n    A `Tensor` of the same shape and `dtype` a\n    `values_grid` and represents an approximate solution `u(t2)`.\n  """"""\n  first_half_step = implicit_scheme(value_grid, t1, (t1 + t2) / 2,\n                                    equation_params_fn)\n  two_half_steps = implicit_scheme(first_half_step, (t1 + t2) / 2, t2,\n                                   equation_params_fn)\n\n  full_step = implicit_scheme(value_grid, t1, t2, equation_params_fn)\n  return 2 * two_half_steps - full_step\n\n\n__all__ = [\'extrapolation_scheme\', \'extrapolation_step\']\n'"
tf_quant_finance/math/pde/steppers/implicit.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Implicit time marching scheme for parabolic PDEs.""""""\n\nfrom tf_quant_finance.math.pde.steppers.parabolic_equation_stepper import parabolic_equation_step\nfrom tf_quant_finance.math.pde.steppers.weighted_implicit_explicit import weighted_implicit_explicit_scheme\n\n\ndef implicit_step():\n  """"""Creates a stepper function with implicit time marching scheme.\n\n  Given a space-discretized equation\n\n  ```\n  du/dt = A(t) u(t) + b(t)\n  ```\n  (here `u` is a value vector, `A` and `b` are the matrix and the vector defined\n  by the PDE), the implicit time marching scheme approximates the right-hand\n  side with its value after the time step:\n\n  ```\n  (u(t2) - u(t1)) / (t2 - t1) = A(t2) u(t2) + b(t2)\n  ```\n  This scheme is stable, but is only first order accurate.\n  Usually, Crank-Nicolson or Extrapolation schemes are preferable.\n\n  More details can be found in `weighted_implicit_explicit.py` describing the\n  weighted implicit-explicit scheme - implicit scheme is a special case\n  with `theta = 0`.\n\n  Returns:\n    Callable to be used in finite-difference PDE solvers (see fd_solvers.py).\n  """"""\n  def step_fn(\n      time,\n      next_time,\n      coord_grid,\n      value_grid,\n      boundary_conditions,\n      second_order_coeff_fn,\n      first_order_coeff_fn,\n      zeroth_order_coeff_fn,\n      inner_second_order_coeff_fn,\n      inner_first_order_coeff_fn,\n      num_steps_performed,\n      dtype=None,\n      name=None):\n    """"""Performs the step.""""""\n    del num_steps_performed\n    name = name or \'implicit_step\'\n    return parabolic_equation_step(time,\n                                   next_time,\n                                   coord_grid,\n                                   value_grid,\n                                   boundary_conditions,\n                                   second_order_coeff_fn,\n                                   first_order_coeff_fn,\n                                   zeroth_order_coeff_fn,\n                                   inner_second_order_coeff_fn,\n                                   inner_first_order_coeff_fn,\n                                   time_marching_scheme=implicit_scheme,\n                                   dtype=dtype,\n                                   name=name)\n  return step_fn\n\nimplicit_scheme = weighted_implicit_explicit_scheme(theta=0)\n\n\n__all__ = [\'implicit_scheme\', \'implicit_step\']\n'"
tf_quant_finance/math/pde/steppers/multidim_parabolic_equation_stepper.py,26,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Stepper for multidimensional parabolic PDE solving.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\n\ndef multidim_parabolic_equation_step(\n    time,\n    next_time,\n    coord_grid,\n    value_grid,\n    boundary_conditions,\n    time_marching_scheme,\n    second_order_coeff_fn=None,\n    first_order_coeff_fn=None,\n    zeroth_order_coeff_fn=None,\n    inner_second_order_coeff_fn=None,\n    inner_first_order_coeff_fn=None,\n    dtype=None,\n    name=None):\n  """"""Performs one step in time to solve a multidimensional PDE.\n\n  Typically one doesn\'t need to use this function directly, unless they have\n  a custom time marching scheme. A simple stepper function for multidimensional\n  PDEs can be found in `douglas_adi.py`.\n\n  The PDE is of the form\n\n  ```None\n    dV/dt + Sum[a_ij d2(A_ij V)/dx_i dx_j, 1 <= i, j <=n] +\n       Sum[b_i d(B_i V)/dx_i, 1 <= i <= n] + c V = 0.\n  ```\n  from time `t0` to time `t1`. The solver can go both forward and backward in\n  time. Here `a_ij`, `A_ij`, `b_i`, `B_i` and `c` are coefficients that may\n  depend on spatial variables `x` and time `t`.\n\n  Here `V` is the unknown function, `V_{...}` denotes partial derivatives\n  w.r.t. dimensions specified in curly brackets, `i` and `j` denote spatial\n  dimensions, `r` is the spatial radius-vector.\n\n  Args:\n    time: Real scalar `Tensor`. The time before the step.\n    next_time: Real scalar `Tensor`. The time after the step.\n    coord_grid: List of `n` rank 1 real `Tensor`s. `n` is the dimension of the\n      domain. The i-th `Tensor` has shape, `[d_i]` where `d_i` is the size of\n      the grid along axis `i`. The coordinates of the grid points. Corresponds\n      to the spatial grid `G` above.\n    value_grid: Real `Tensor` containing the function values at time\n      `time` which have to be evolved to time `next_time`. The shape of the\n      `Tensor` must broadcast with `B + [d_1, d_2, ..., d_n]`. `B` is the batch\n      dimensions (one or more), which allow multiple functions (with potentially\n      different boundary/final conditions and PDE coefficients) to be evolved\n      simultaneously.\n    boundary_conditions: The boundary conditions. Only rectangular boundary\n      conditions are supported.\n      A list of tuples of size `n` (space dimension\n      of the PDE). Each tuple consists of two callables representing the\n      boundary conditions at the minimum and maximum values of the spatial\n      variable indexed by the position in the list. E.g. for `n=2`, the length\n      of `boundary_conditions` should be 2, `boundary_conditions[0][0]`\n      describes the boundary `(y_min, x)`, and `boundary_conditions[1][0]`- the\n      boundary `(y, x_min)`. The boundary conditions are accepted in the form\n      `alpha(t, x) V + beta(t, x) V_n = gamma(t, x)`, where `V_n` is the\n      derivative with respect to the exterior normal to the boundary.\n      Each callable receives the current time `t` and the `coord_grid` at the\n      current time, and should return a tuple of `alpha`, `beta`, and `gamma`.\n      Each can be a number, a zero-rank `Tensor` or a `Rensor` whose shape is\n      the grid shape with the corresponding dimension removed.\n      For example, for a two-dimensional grid of shape `(b, ny, nx)`, where `b`\n      is the batch size, `boundary_conditions[0][0]` should return a tuple of\n      either numbers, zero-rank tensors or tensors of shape `(b, nx)`. Similarly\n      for `boundary_conditions[1][0]`, except the tensor shape should be\n      `(b, ny)`. `alpha` and `beta` can also be `None` in case of Neumann and\n      Dirichlet conditions, respectively.\n    time_marching_scheme: A callable which represents the time marching scheme\n      for solving the PDE equation. If `u(t)` is space-discretized vector of the\n      solution of a PDE, a time marching scheme approximately solves the\n      equation `du/dt = A(t) u(t) + b(t)` for `u(t2)` given `u(t1)`, or vice\n      versa if going backwards in time. Here `A` is a banded matrix containing\n      contributions from the current and neighboring points in space, `b` is an\n      arbitrary vector (inhomogeneous term).\n      Multidimensional time marching schemes are usually based on the idea of\n      ADI (alternating direction implicit) method: the time step is split into\n      substeps, and in each substep only one dimension is treated ""implicitly"",\n      while all the others are treated ""explicitly"". This way one has to solve\n      only tridiagonal systems of equations, but not more complicated banded\n      ones. A few examples of time marching schemes (Douglas, Craig-Sneyd, etc.)\n      can be found in [1].\n      The callable consumes the following arguments by keyword:\n        1. inner_value_grid: Grid of solution values at the current time of\n          the same `dtype` as `value_grid` and shape of `value_grid[..., 1:-1]`.\n        2. t1: Lesser of the two times defining the step.\n        3. t2: Greater of the two times defining the step.\n        4. equation_params_fn: A callable that takes a scalar `Tensor` argument\n          representing time and returns a tuple of two elements.\n          The first one represents `A`. The length must be the number of\n          dimensions (`n_dims`), and A[i] must have length `n_dims - i`.\n          `A[i][0]` is a tridiagonal matrix representing influence of the\n          neighboring points along the dimension `i`. It is a tuple of\n          superdiagonal, diagonal, and subdiagonal parts of the tridiagonal\n          matrix. The shape of these tensors must be same as of `value_grid`.\n          superdiagonal[..., -1] and subdiagonal[..., 0] are ignored.\n          `A[i][j]` with `i < j < n_dims` are tuples of four Tensors with same\n          shape as `value_grid` representing the influence of four points placed\n          diagonally from the given point in the plane of dimensions `i` and\n          `j`. Denoting `k`, `l` the indices of a given grid point in the plane,\n          the four Tensors represent contributions of points `(k+1, l+1)`,\n          `(k+1, l-1)`, `(k-1, l+1)`, and `(k-1, l-1)`, in this order.\n          The second element in the tuple is a list of contributions to `b(t)`\n          associated with each dimension. E.g. if `b(t)` comes from boundary\n          conditions, then it is split correspondingly. Each element in the list\n          is a Tensor with the shape of `value_grid`.\n          For example a 2D problem with `value_grid.shape = (B, ny, nx)`, where\n          `B` is the batch size. The elements `Aij` are non-zero if `i = j` or\n          `i` is a neighbor of `j` in the x-y plane. Depict these non-zero\n          elements on the grid as follows:\n          ```\n          a_mm    a_y-   a_mp\n          a_x-    a_0    a_x+\n          a_pm   a_y+   a_pp\n          ```\n          The callable should return\n          ```\n          ([[(a_y-, a_0y, a_y+), (a_pp, a_pm, a_mp, a_pp)],\n            [None, (a_x-, a_0x, a_x+)]],\n          [b_y, b_x])\n          ```\n          where `a_0x + a_0y = a_0` (the splitting is arbitrary). Note that\n          there is no need to repeat the non-diagonal term\n          `(a_pp, a_pm, a_mp, a_pp)` for the second time: it\'s replaced with\n          `None`.\n          All the elements `a_...` may be different for each point in the grid,\n          so they are `Tensors` of shape `(B, ny, nx)`. `b_y` and `b_x` are also\n          `Tensors` of that shape.\n        5. n_dims: A Python integer, the spatial dimension of the PDE.\n      The callable should return a `Tensor` of the same shape and `dtype` as\n      `values_grid` that represents an approximate solution of the\n      space-discretized PDE.\n    second_order_coeff_fn: Callable returning the second order coefficient\n      `a_{ij}(t, r)` evaluated at given time `t`.\n      The callable accepts the following arguments:\n        `t`: The time at which the coefficient should be evaluated.\n        `locations_grid`: a `Tensor` representing a grid of locations `r` at\n          which the coefficient should be evaluated.\n      Returns an object `A` such that `A[i][j]` is defined and\n      `A[i][j]=a_{ij}(r, t)`, where `0 <= i < n_dims` and `i <= j < n_dims`.\n      For example, the object may be a list of lists or a rank 2 Tensor.\n      Only the elements with `j >= i` will be used, and it is assumed that\n      `a_{ji} = a_{ij}`, so `A[i][j] with `j < i` may return `None`.\n      Each `A[i][j]` should be a Number, a `Tensor` broadcastable to the\n      shape of the grid represented by `locations_grid`, or `None` if\n      corresponding term is absent in the equation. Also, the callable itself\n      may be None, meaning there are no second-order derivatives in the\n      equation.\n      For example, for `n_dims=2`, the callable may return either\n      `[[a_yy, a_xy], [a_xy, a_xx]]` or `[[a_yy, a_xy], [None, a_xx]]`.\n    first_order_coeff_fn: Callable returning the first order coefficients\n      `b_{i}(t, r)` evaluated at given time `t`.\n      The callable accepts the following arguments:\n        `t`: The time at which the coefficient should be evaluated.\n        `locations_grid`: a `Tensor` representing a grid of locations `r` at\n          which the coefficient should be evaluated.\n      Returns a list or an 1D `Tensor`, `i`-th element of which represents\n      `b_{i}(t, r)`. Each element should be a Number, a `Tensor` broadcastable\n       to the shape of of the grid represented by `locations_grid`, or None if\n       corresponding term is absent in the equation. The callable itself may be\n       None, meaning there are no first-order derivatives in the equation.\n    zeroth_order_coeff_fn: Callable returning the zeroth order coefficient\n      `c(t, r)` evaluated at given time `t`.\n      The callable accepts the following arguments:\n        `t`: The time at which the coefficient should be evaluated.\n        `locations_grid`: a `Tensor` representing a grid of locations `r` at\n          which the coefficient should be evaluated.\n      Should return a Number or a `Tensor` broadcastable to the shape of\n      the grid represented by `locations_grid`. May also return None or be None\n      if the shift term is absent in the equation.\n    inner_second_order_coeff_fn: Callable returning the coefficients under the\n      second derivatives (i.e. `A_ij(t, x)` above) at given time `t`. The\n      requirements are the same as for `second_order_coeff_fn`.\n    inner_first_order_coeff_fn: Callable returning the coefficients under the\n      first derivatives (i.e. `B_i(t, x)` above) at given time `t`. The\n      requirements are the same as for `first_order_coeff_fn`.\n    dtype: The dtype to use.\n    name: The name to give to the ops.\n      Default value: None which means `parabolic_equation_step` is used.\n\n  Returns:\n    A sequence of two `Tensor`s. The first one is a `Tensor` of the same\n    `dtype` and `shape` as `coord_grid` and represents a new coordinate grid\n    after one iteration. The second `Tensor` is of the same shape and `dtype`\n    as`values_grid` and represents an approximate solution of the equation after\n    one iteration.\n\n  #### References:\n  [1] Tinne Haentjens, Karek J. in\'t Hout. ADI finite difference schemes\n  for the Heston-Hull-White PDE. https://arxiv.org/abs/1111.4087\n  """"""\n  with tf.compat.v1.name_scope(\n      name, \'multidim_parabolic_equation_step\',\n      values=[time, next_time, coord_grid, value_grid]):\n\n    time = tf.convert_to_tensor(time, dtype=dtype, name=\'time\')\n    next_time = tf.convert_to_tensor(next_time, dtype=dtype, name=\'next_time\')\n    coord_grid = [tf.convert_to_tensor(x, dtype=dtype,\n                                       name=\'coord_grid_axis_{}\'.format(ind))\n                  for ind, x in enumerate(coord_grid)]\n    value_grid = tf.convert_to_tensor(value_grid, dtype=dtype,\n                                      name=\'value_grid\')\n\n    n_dims = len(coord_grid)\n\n    # Sanitize the coeff callables.\n    second_order_coeff_fn = (second_order_coeff_fn or\n                             (lambda *args: [[None] * n_dims] * n_dims))\n    first_order_coeff_fn = (first_order_coeff_fn or\n                            (lambda *args: [None] * n_dims))\n    zeroth_order_coeff_fn = zeroth_order_coeff_fn or (lambda *args: None)\n    inner_second_order_coeff_fn = (\n        inner_second_order_coeff_fn or\n        (lambda *args: [[None] * n_dims] * n_dims))\n    inner_first_order_coeff_fn = (\n        inner_first_order_coeff_fn or (lambda *args: [None] * n_dims))\n\n    batch_rank = len(value_grid.shape.as_list()) - len(coord_grid)\n\n    def equation_params_fn(t):\n      return _construct_discretized_equation_params(\n          coord_grid, value_grid, boundary_conditions, second_order_coeff_fn,\n          first_order_coeff_fn, zeroth_order_coeff_fn,\n          inner_second_order_coeff_fn, inner_first_order_coeff_fn, batch_rank,\n          t)\n\n    inner_grid_in = _trim_boundaries(value_grid, batch_rank)\n\n    inner_grid_out = time_marching_scheme(\n        value_grid=inner_grid_in,\n        t1=time,\n        t2=next_time,\n        equation_params_fn=equation_params_fn,\n        n_dims=n_dims)\n\n    updated_value_grid = _apply_boundary_conditions_after_step(\n        value_grid, inner_grid_out, coord_grid, boundary_conditions, batch_rank,\n        next_time)\n\n    return coord_grid, updated_value_grid\n\n\ndef _construct_discretized_equation_params(\n    coord_grid, value_grid, boundary_conditions, second_order_coeff_fn,\n    first_order_coeff_fn, zeroth_order_coeff_fn, inner_second_order_coeff_fn,\n    inner_first_order_coeff_fn, batch_rank, t):\n  """"""Constructs parameters of discretized equation.""""""\n  second_order_coeffs = second_order_coeff_fn(t, coord_grid)\n  first_order_coeffs = first_order_coeff_fn(t, coord_grid)\n  zeroth_order_coeffs = zeroth_order_coeff_fn(t, coord_grid)\n  inner_second_order_coeffs = inner_second_order_coeff_fn(t, coord_grid)\n  inner_first_order_coeffs = inner_first_order_coeff_fn(t, coord_grid)\n\n  matrix_params = []\n  inhomog_terms = []\n\n  zeroth_order_coeffs = _prepare_pde_coeff(zeroth_order_coeffs, value_grid)\n  if zeroth_order_coeffs is not None:\n    zeroth_order_coeffs = _trim_boundaries(\n        zeroth_order_coeffs, from_dim=batch_rank)\n\n  n_dims = len(coord_grid)\n  for dim in range(n_dims):\n    # 1. Construct contributions of dV/dx_dim and d^2V/dx_dim^2. This yields\n    # a tridiagonal matrix.\n    delta = _get_grid_delta(coord_grid, dim)  # Non-uniform grids not supported.\n\n    second_order_coeff = second_order_coeffs[dim][dim]\n    first_order_coeff = first_order_coeffs[dim]\n    inner_second_order_coeff = inner_second_order_coeffs[dim][dim]\n    inner_first_order_coeff = inner_first_order_coeffs[dim]\n\n    superdiag, diag, subdiag = (\n        _construct_tridiagonal_matrix(value_grid, second_order_coeff,\n                                      first_order_coeff,\n                                      inner_second_order_coeff,\n                                      inner_first_order_coeff, delta, dim,\n                                      batch_rank, n_dims))\n\n    # 2. Account for boundary conditions on boundaries orthogonal to dim.\n    # This modifies the first and last row of the tridiagonal matrix and also\n    # yields a contribution to the inhomogeneous term\n    (superdiag, diag, subdiag), inhomog_term_contribution = (\n        _apply_boundary_conditions_to_tridiagonal_and_inhomog_terms(\n            value_grid, dim, batch_rank, boundary_conditions, coord_grid,\n            superdiag, diag, subdiag, delta, t))\n\n    # 3. Evenly distribute shift term among tridiagonal matrices of each\n    # dimension. The minus sign is because we move the shift term to rhs.\n    if zeroth_order_coeffs is not None:\n      # pylint: disable=invalid-unary-operand-type\n      diag += -zeroth_order_coeffs / n_dims\n\n    matrix_params_row = [None] * dim + [(superdiag, diag, subdiag)]\n\n    # 4. Construct contributions of mixed terms, d^2V/(dx_dim dx_dim2).\n    for dim2 in range(dim + 1, n_dims):\n      mixed_coeff = second_order_coeffs[dim][dim2]\n      inner_mixed_coeff = inner_second_order_coeffs[dim][dim2]\n      mixed_term_contrib = (\n          _construct_contribution_of_mixed_term(mixed_coeff, inner_mixed_coeff,\n                                                coord_grid, value_grid, dim,\n                                                dim2, batch_rank, n_dims))\n      matrix_params_row.append(mixed_term_contrib)\n\n    matrix_params.append(matrix_params_row)\n    inhomog_terms.append(inhomog_term_contribution)\n\n  return matrix_params, inhomog_terms\n\n\ndef _construct_tridiagonal_matrix(value_grid, second_order_coeff,\n                                  first_order_coeff, inner_second_order_coeff,\n                                  inner_first_order_coeff, delta, dim,\n                                  batch_rank, n_dims):\n  """"""Constructs contributions of first and non-mixed second order terms.""""""\n  second_order_coeff = _prepare_pde_coeff(second_order_coeff, value_grid)\n  first_order_coeff = _prepare_pde_coeff(first_order_coeff, value_grid)\n  inner_second_order_coeff = _prepare_pde_coeff(inner_second_order_coeff,\n                                                value_grid)\n  inner_first_order_coeff = _prepare_pde_coeff(inner_first_order_coeff,\n                                               value_grid)\n\n  zeros = tf.zeros_like(value_grid)\n  zeros = _trim_boundaries(zeros, from_dim=batch_rank)\n\n  def create_trimming_shifts(dim_shift):\n    # See _trim_boundaries. We need to apply shift only to the dimension `dim`.\n    shifts = [0] * n_dims\n    shifts[dim] = dim_shift\n    return shifts\n\n  # Discretize first-order term.\n  if first_order_coeff is None and inner_first_order_coeff is None:\n    # No first-order term.\n    superdiag_first_order = zeros\n    diag_first_order = zeros\n    subdiag_first_order = zeros\n  else:\n    superdiag_first_order = -1 / (2 * delta)\n    subdiag_first_order = 1 / (2 * delta)\n    diag_first_order = -superdiag_first_order - subdiag_first_order\n    if first_order_coeff is not None:\n      first_order_coeff = _trim_boundaries(\n          first_order_coeff, from_dim=batch_rank)\n      superdiag_first_order *= first_order_coeff\n      subdiag_first_order *= first_order_coeff\n      diag_first_order *= first_order_coeff\n    if inner_first_order_coeff is not None:\n      superdiag_first_order *= _trim_boundaries(\n          inner_first_order_coeff,\n          from_dim=batch_rank,\n          shifts=create_trimming_shifts(1))\n      subdiag_first_order *= _trim_boundaries(\n          inner_first_order_coeff,\n          from_dim=batch_rank,\n          shifts=create_trimming_shifts(-1))\n      diag_first_order *= _trim_boundaries(\n          inner_first_order_coeff, from_dim=batch_rank)\n\n  # Discretize second-order term.\n  if second_order_coeff is None and inner_second_order_coeff is None:\n    # No second-order term.\n    superdiag_second_order = zeros\n    diag_second_order = zeros\n    subdiag_second_order = zeros\n  else:\n    superdiag_second_order = -1 / (delta * delta)\n    subdiag_second_order = -1 / (delta * delta)\n    diag_second_order = -superdiag_second_order - subdiag_second_order\n    if second_order_coeff is not None:\n      second_order_coeff = _trim_boundaries(\n          second_order_coeff, from_dim=batch_rank)\n      superdiag_second_order *= second_order_coeff\n      subdiag_second_order *= second_order_coeff\n      diag_second_order *= second_order_coeff\n    if inner_second_order_coeff is not None:\n      superdiag_second_order *= _trim_boundaries(\n          inner_second_order_coeff,\n          from_dim=batch_rank,\n          shifts=create_trimming_shifts(1))\n      subdiag_second_order *= _trim_boundaries(\n          inner_second_order_coeff,\n          from_dim=batch_rank,\n          shifts=create_trimming_shifts(-1))\n      diag_second_order *= _trim_boundaries(\n          inner_second_order_coeff, from_dim=batch_rank)\n\n  superdiag = superdiag_first_order + superdiag_second_order\n  subdiag = subdiag_first_order + subdiag_second_order\n  diag = diag_first_order + diag_second_order\n  return superdiag, diag, subdiag\n\n\ndef _construct_contribution_of_mixed_term(outer_coeff, inner_coeff, coord_grid,\n                                          value_grid, dim1, dim2, batch_rank,\n                                          n_dims):\n  """"""Constructs contribution of a mixed derivative term.""""""\n  if outer_coeff is None and inner_coeff is None:\n    return None\n  delta_dim1 = _get_grid_delta(coord_grid, dim1)\n  delta_dim2 = _get_grid_delta(coord_grid, dim2)\n\n  outer_coeff = _prepare_pde_coeff(outer_coeff, value_grid)\n  inner_coeff = _prepare_pde_coeff(inner_coeff, value_grid)\n\n  # The contribution of d2V/dx_dim1 dx_dim2 is\n  # mixed_coeff / (4 * delta_dim1 * delta_dim2), but there is also\n  # d2V/dx_dim2 dx_dim1, so the contribution is doubled.\n  # Also, the minus is because of moving to the rhs.\n  contrib = -1 / (2 * delta_dim1 * delta_dim2)\n\n  if outer_coeff is not None:\n    outer_coeff = _trim_boundaries(outer_coeff, batch_rank)\n    contrib *= outer_coeff\n\n  if inner_coeff is None:\n    return contrib, -contrib, -contrib, contrib\n\n  def create_trimming_shifts(dim1_shift, dim2_shift):\n    # See _trim_boundaries. We need to apply shifts to dimensions dim1 and\n    # dim2.\n    shifts = [0] * n_dims\n    shifts[dim1] = dim1_shift\n    shifts[dim2] = dim2_shift\n    return shifts\n\n  # When there are inner coefficients in mixed terms, the contributions of four\n  # diagonally placed points are significantly different. Below we use indices\n  # ""p"" and ""m"" to denote shifts by +1 and -1 in grid indices on the\n  # (dim1, dim2) plane. E.g. if the current point has grid indices (k, l),\n  # contrib_pm is the contribution of the point (k + 1, l - 1).\n  contrib_pp = contrib * _trim_boundaries(\n      inner_coeff, from_dim=batch_rank, shifts=create_trimming_shifts(1, 1))\n  contrib_pm = -contrib * _trim_boundaries(\n      inner_coeff, from_dim=batch_rank, shifts=create_trimming_shifts(1, -1))\n  contrib_mp = -contrib * _trim_boundaries(\n      inner_coeff, from_dim=batch_rank, shifts=create_trimming_shifts(-1, 1))\n  contrib_mm = contrib * _trim_boundaries(\n      inner_coeff, from_dim=batch_rank, shifts=create_trimming_shifts(-1, -1))\n\n  return contrib_pp, contrib_pm, contrib_mp, contrib_mm\n\n\ndef _apply_boundary_conditions_to_tridiagonal_and_inhomog_terms(\n    value_grid, dim, batch_rank, boundary_conditions, coord_grid, superdiag,\n    diag, subdiag, delta, t):\n  """"""Updates contributions according to boundary conditions.""""""\n  # This is analogous to _apply_boundary_conditions_to_discretized_equation in\n  # pde_kernels.py. The difference is that we work with the given spatial\n  # dimension. In particular, in all the tensor slices we have to slice\n  # into the dimension `batch_rank + dim` instead of the last dimension.\n\n  # Retrieve the boundary conditions in the form alpha V + beta V\' = gamma.\n\n  alpha_l, beta_l, gamma_l = boundary_conditions[dim][0](t, coord_grid)\n  alpha_u, beta_u, gamma_u = boundary_conditions[dim][1](t, coord_grid)\n\n  alpha_l, beta_l, gamma_l, alpha_u, beta_u, gamma_u = (\n      _prepare_boundary_conditions(b, value_grid, batch_rank, dim)\n      for b in (alpha_l, beta_l, gamma_l, alpha_u, beta_u, gamma_u))\n\n  def reshape_fn(bound_coeff):\n    """"""Reshapes boundary coefficient.""""""\n    # Say the grid shape is (b, nz, ny, nx), and dim = 1.\n    # The boundary condition coefficients are expected to have shape\n    # (b, nz, nx). We need to:\n    # - Trim the boundaries: nz -> nz-2, nx -> nx-2, because we work with\n    # the inner part of the grid here.\n    # - Expand dimension batch_rank+dim=2, because broadcasting won\'t always\n    # do this correctly in subsequent computations: if a has shape (5, 1) and\n    # b has shape (5,) then a*b has shape (5, 5)!\n    # Thus this function turns (b, nz, nx) into (b, nz-2, 1, nx-2).\n    return _reshape_boundary_conds(\n        bound_coeff, trim_from=batch_rank, expand_dim_at=batch_rank + dim)\n\n  alpha_l, beta_l, gamma_l, alpha_u, beta_u, gamma_u = map(\n      reshape_fn, (alpha_l, beta_l, gamma_l, alpha_u, beta_u, gamma_u))\n\n  dim += batch_rank\n  subdiag_first = _slice(subdiag, dim, 0, 1)\n  superdiag_last = _slice(superdiag, dim, -1, 0)\n  diag_inner = _slice(diag, dim, 1, -1)\n\n  if beta_l is None and beta_u is None:\n    # Dirichlet conditions on both boundaries. In this case there are no\n    # corrections to the tridiagonal matrix, so we can take a shortcut.\n    first_inhomog_element = subdiag_first * gamma_l / alpha_l\n    last_inhomog_element = superdiag_last * gamma_u / alpha_u\n    inhomog_term = tf.concat(\n        (first_inhomog_element, tf.zeros_like(diag_inner),\n         last_inhomog_element), dim)\n    return (superdiag, diag, subdiag), inhomog_term\n\n  # A few more slices we\'re going to need.\n  subdiag_last = _slice(subdiag, dim, -1, 0)\n  subdiag_except_last = _slice(subdiag, dim, 0, -1)\n  superdiag_first = _slice(superdiag, dim, 0, 1)\n  superdiag_except_first = _slice(superdiag, dim, 1, 0)\n  diag_first = _slice(diag, dim, 0, 1)\n  diag_last = _slice(diag, dim, -1, 0)\n\n  # Convert the boundary conditions into the form v0 = xi1 v1 + xi2 v2 + eta,\n  # and calculate corrections to the tridiagonal matrix and the inhomogeneous\n  # term.\n  xi1, xi2, eta = _discretize_boundary_conditions(delta, delta, alpha_l,\n                                                  beta_l, gamma_l)\n  diag_first_correction = subdiag_first * xi1\n  superdiag_correction = subdiag_first * xi2\n  first_inhomog_element = subdiag_first * eta\n  xi1, xi2, eta = _discretize_boundary_conditions(delta, delta, alpha_u,\n                                                  beta_u, gamma_u)\n  diag_last_correction = superdiag_last * xi1\n  subdiag_correction = superdiag_last * xi2\n  last_inhomog_element = superdiag_last * eta\n  diag = tf.concat((diag_first + diag_first_correction, diag_inner,\n                    diag_last + diag_last_correction), dim)\n  superdiag = tf.concat(\n      (superdiag_first + superdiag_correction, superdiag_except_first), dim)\n  subdiag = tf.concat(\n      (subdiag_except_last, subdiag_last + subdiag_correction), dim)\n  inhomog_term = tf.concat((first_inhomog_element, tf.zeros_like(diag_inner),\n                            last_inhomog_element), dim)\n  return (superdiag, diag, subdiag), inhomog_term\n\n\ndef _apply_boundary_conditions_after_step(value_grid_in, inner_grid_out,\n                                          coord_grid, boundary_conditions,\n                                          batch_rank, t):\n  """"""Calculates and appends boundary values after making a step.""""""\n  # After we\'ve updated the values in the inner part of the grid according to\n  # the PDE, we append the boundary values calculated using the boundary\n  # conditions.\n  # This is done using the discretized form of the boundary conditions,\n  # v0 = xi1 v1 + xi2 v2 + eta.\n  # This is analogous to _apply_boundary_conditions_after_step in\n  # pde_kernels.py, except we have to restore the boundaries in each\n  # dimension. For example, for n_dims=2, inner_grid_out has dimensions\n  # (b, ny-2, nx-2), which then becomes (b, ny, nx-2) and finally (b, ny, nx).\n  grid = inner_grid_out\n  for dim in range(len(coord_grid)):\n    grid = _apply_boundary_conditions_after_step_to_dim(dim, batch_rank,\n                                                        boundary_conditions,\n                                                        coord_grid,\n                                                        value_grid_in, grid, t)\n\n  return grid\n\n\ndef _apply_boundary_conditions_after_step_to_dim(dim, batch_rank,\n                                                 boundary_conditions,\n                                                 coord_grid, value_grid_in,\n                                                 current_value_grid_out, t):\n  """"""Calculates and appends boundaries orthogonal to `dim`.""""""\n  # E.g. for n_dims = 3, and dim = 1, the expected input grid shape is\n  # (b, nx, ny-2, nz-2), and the output shape is (b, nx, ny, nz-2).\n  lower_value_first = _slice(current_value_grid_out, batch_rank + dim, 0, 1)\n  lower_value_second = _slice(current_value_grid_out, batch_rank + dim, 1, 2)\n  upper_value_first = _slice(current_value_grid_out, batch_rank + dim, -1, 0)\n  upper_value_second = _slice(current_value_grid_out, batch_rank + dim, -2, -1)\n\n  alpha_l, beta_l, gamma_l = boundary_conditions[dim][0](t, coord_grid)\n  alpha_u, beta_u, gamma_u = boundary_conditions[dim][1](t, coord_grid)\n\n  alpha_l, beta_l, gamma_l, alpha_u, beta_u, gamma_u = (\n      _prepare_boundary_conditions(b, value_grid_in, batch_rank, dim)\n      for b in (alpha_l, beta_l, gamma_l, alpha_u, beta_u, gamma_u))\n\n  def reshape_fn(bound_coeff):\n    # Say the grid shape is (b, nz, ny-2, nx-2), and dim = 1: we have already\n    # restored the z-boundaries and now are restoring the y-boundaries.\n    # The boundary condition coefficients are expected to have the shape\n    # (b, nz, nx). We need to:\n    # - Trim the boundaries which we haven\'t yet restored: nx -> nx-2.\n    # - Expand dimension batch_rank+dim=2, because broadcasting won\'t always\n    # do this correctly in subsequent computations.\n    # Thus this function turns (b, nz, nx) into (b, nz, 1, nx-2).\n    return _reshape_boundary_conds(\n        bound_coeff,\n        trim_from=batch_rank + dim,\n        expand_dim_at=batch_rank + dim)\n\n  alpha_l, beta_l, gamma_l, alpha_u, beta_u, gamma_u = map(\n      reshape_fn, (alpha_l, beta_l, gamma_l, alpha_u, beta_u, gamma_u))\n\n  delta = _get_grid_delta(coord_grid, dim)\n  xi1, xi2, eta = _discretize_boundary_conditions(delta, delta, alpha_l,\n                                                  beta_l, gamma_l)\n  first_value = (xi1 * lower_value_first + xi2 * lower_value_second + eta)\n  xi1, xi2, eta = _discretize_boundary_conditions(delta, delta, alpha_u,\n                                                  beta_u, gamma_u)\n  last_value = (xi1 * upper_value_first + xi2 * upper_value_second + eta)\n  return tf.concat((first_value, current_value_grid_out, last_value),\n                   batch_rank + dim)\n\n\ndef _get_grid_delta(coord_grid, dim):\n  # Retrieves delta along given dimension, assuming the grid is uniform.\n  return coord_grid[dim][1] - coord_grid[dim][0]\n\n\ndef _prepare_pde_coeff(raw_coeff, value_grid):\n  # Converts values received from second_order_coeff_fn and similar Callables\n  # into a format usable further down in the pipeline.\n  if raw_coeff is None:\n    return None\n  dtype = value_grid.dtype\n  coeff = tf.convert_to_tensor(raw_coeff, dtype=dtype)\n  coeff = tf.broadcast_to(coeff, tf.shape(value_grid))\n  return coeff\n\n\ndef _prepare_boundary_conditions(boundary_tensor, value_grid, batch_rank, dim):\n  """"""Prepares values received from boundary_condition callables.""""""\n  if boundary_tensor is None:\n    return None\n  boundary_tensor = tf.convert_to_tensor(boundary_tensor, value_grid.dtype)\n  # Broadcast to the shape of the boundary: it is the shape of value grid with\n  # one dimension removed.\n  dim_to_remove = batch_rank + dim\n  broadcast_shape = []\n  # Shape slicing+concatenation seems error-prone, so let\'s do it simply.\n  for i, size in enumerate(value_grid.shape):\n    if i != dim_to_remove:\n      broadcast_shape.append(size)\n  return tf.broadcast_to(boundary_tensor, broadcast_shape)\n\n\ndef _discretize_boundary_conditions(dx0, dx1, alpha, beta, gamma):\n  """"""Discretizes boundary conditions.""""""\n  # Converts a boundary condition given as alpha V + beta V_n = gamma,\n  # where V_n is the derivative w.r.t. the normal to the boundary into\n  # v0 = xi1 v1 + xi2 v2 + eta,\n  # where v0 is the value on the boundary point of the grid, v1 and v2 - values\n  # on the next two points on the grid.\n  # The expressions are exactly the same for both boundaries.\n\n  if beta is None:\n    # Dirichlet condition.\n    if alpha is None:\n      raise ValueError(\n          ""Invalid boundary conditions: alpha and beta can\'t both be None."")\n    zeros = tf.zeros_like(gamma)\n    return zeros, zeros, gamma / alpha\n\n  denom = beta * dx1 * (2 * dx0 + dx1)\n  if alpha is not None:\n    denom += alpha * dx0 * dx1 * (dx0 + dx1)\n  xi1 = beta * (dx0 + dx1) * (dx0 + dx1) / denom\n  xi2 = -beta * dx0 * dx0 / denom\n  eta = gamma * dx0 * dx1 * (dx0 + dx1) / denom\n  return xi1, xi2, eta\n\n\ndef _reshape_boundary_conds(raw_coeff, trim_from, expand_dim_at):\n  """"""Reshapes boundary condition coefficients.""""""\n  # If the coefficient is None, a number or a rank-0 tensor, return as-is.\n  if (not tf.is_tensor(raw_coeff)\n      or len(raw_coeff.shape.as_list()) == 0):  # pylint: disable=g-explicit-length-test\n    return raw_coeff\n  # See explanation why we trim boundaries and expand dims in places where this\n  # function is used.\n  coeff = _trim_boundaries(raw_coeff, trim_from)\n  coeff = tf.expand_dims(coeff, expand_dim_at)\n  return coeff\n\n\ndef _slice(tensor, dim, start, end):\n  """"""Slices the tensor along given dimension.""""""\n  # Performs a slice along the dimension dim. E.g. for tensor t of rank 3,\n  # _slice(t, 1, 3, 5) is same as t[:, 3:5].\n  # For a slice unbounded to the right, set end=0: _slice(t, 1, -3, 0) is same\n  # as t[:, -3:].\n  rank = len(tensor.shape.as_list())\n  if start < 0:\n    start += tf.compat.dimension_value(tensor.shape.as_list()[dim])\n  if end <= 0:\n    end += tf.compat.dimension_value(tensor.shape.as_list()[dim])\n  slice_begin = np.zeros(rank, dtype=np.int32)\n  slice_begin[dim] = start\n  slice_size = -np.ones(rank, dtype=np.int32)\n  slice_size[dim] = end - start\n  return tf.slice(tensor, slice_begin, slice_size)\n\n\ndef _trim_boundaries(tensor, from_dim, shifts=None):\n  """"""Trims tensor boundaries starting from given dimension.""""""\n  # For example, if tensor has shape (a, b, c, d) and from_dim=1, then the\n  # output tensor has shape (a, b-2, c-2, d-2).\n  # For example _trim_boundaries_with_shifts(t, 1) with a rank-4\n  # tensor t yields t[:, 1:-1, 1:-1, 1:-1].\n  #\n  # If shifts is specified, the slices applied are shifted. shifts is an array\n  # of length rank(tensor) - from_dim, with values -1, 0, or 1, meaning slices\n  # [:-2], [-1, 1], and [2:], respectively.\n  # For example _trim_boundaries_with_shifts(t, 1, (1, 0, -1)) with a rank-4\n  #  tensor t yields t[:, 2:, 1:-1, :-2].\n  rank = len(tensor.shape.as_list())\n  slice_begin = np.zeros(rank, dtype=np.int32)\n  slice_size = np.zeros(rank, dtype=np.int32)\n  for i in range(from_dim):\n    slice_size[i] = tf.compat.dimension_value(tensor.shape.as_list()[i])\n  for i in range(from_dim, rank):\n    slice_begin[i] = 1\n    slice_size[i] = tf.compat.dimension_value(tensor.shape.as_list()[i]) - 2\n    if shifts is not None:\n      slice_begin[i] += shifts[i - from_dim]\n  return tf.slice(tensor, slice_begin, slice_size)\n\n\n__all__ = [\'multidim_parabolic_equation_step\']\n'"
tf_quant_finance/math/pde/steppers/multidim_parabolic_equation_stepper_test.py,102,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for multidimensional parabolic PDE solvers.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\nfd_solvers = tff.math.pde.fd_solvers\ndirichlet = tff.math.pde.boundary_conditions.dirichlet\nneumann = tff.math.pde.boundary_conditions.neumann\ngrids = tff.math.pde.grids\ndouglas_adi_step = tff.math.pde.steppers.douglas_adi.douglas_adi_step\n\n_SQRT2 = np.sqrt(2)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass MultidimParabolicEquationStepperTest(tf.test.TestCase):\n\n  def testAnisotropicDiffusion(self):\n    """"""Tests solving 2d diffusion equation.\n\n    The equation is `u_{t} + Dx u_{xx} + Dy u_{yy} = 0`.\n    The final condition is a gaussian centered at (0, 0) with variance sigma.\n    The variance along each dimension should evolve as\n    `sigma + 2 Dx (t_final - t)` and `sigma + 2 Dy (t_final - t)`.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[-10, -20],\n        maximums=[10, 20],\n        sizes=[201, 301],\n        dtype=tf.float32)\n    ys = self.evaluate(grid[0])\n    xs = self.evaluate(grid[1])\n\n    diff_coeff_x = 0.4  # Dx\n    diff_coeff_y = 0.25  # Dy\n    time_step = 0.1\n    final_t = 1\n    final_variance = 1\n\n    def quadratic_coeff_fn(t, location_grid):\n      del t, location_grid\n      u_xx = diff_coeff_x\n      u_yy = diff_coeff_y\n      u_xy = None\n      return [[u_yy, u_xy], [u_xy, u_xx]]\n\n    final_values = tf.expand_dims(\n        tf.constant(\n            np.outer(\n                _gaussian(ys, final_variance), _gaussian(xs, final_variance)),\n            dtype=tf.float32),\n        axis=0)\n    bound_cond = [(_zero_boundary, _zero_boundary),\n                  (_zero_boundary, _zero_boundary)]\n    step_fn = douglas_adi_step(theta=0.5)\n    result = fd_solvers.solve_backward(\n        start_time=final_t,\n        end_time=0,\n        coord_grid=grid,\n        values_grid=final_values,\n        time_step=time_step,\n        one_step_fn=step_fn,\n        boundary_conditions=bound_cond,\n        second_order_coeff_fn=quadratic_coeff_fn,\n        dtype=grid[0].dtype)\n\n    variance_x = final_variance + 2 * diff_coeff_x * final_t\n    variance_y = final_variance + 2 * diff_coeff_y * final_t\n    expected = np.outer(_gaussian(ys, variance_y), _gaussian(xs, variance_x))\n\n    self._assertClose(expected, result)\n\n  def testSimpleDrift(self):\n    """"""Tests solving 2d drift equation.\n\n    The equation is `u_{t} + vx u_{x} + vy u_{y} = 0`.\n    The final condition is a gaussian centered at (0, 0) with variance sigma.\n    The gaussian should drift with velocity `[vx, vy]`.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[-10, -20],\n        maximums=[10, 20],\n        sizes=[201, 301],\n        dtype=tf.float32)\n    ys = self.evaluate(grid[0])\n    xs = self.evaluate(grid[1])\n\n    time_step = 0.01\n    final_t = 3\n    variance = 1\n    vx = 0.1\n    vy = 0.3\n\n    def first_order_coeff_fn(t, location_grid):\n      del t, location_grid\n      return [vy, vx]\n\n    final_values = tf.expand_dims(\n        tf.constant(\n            np.outer(_gaussian(ys, variance), _gaussian(xs, variance)),\n            dtype=tf.float32),\n        axis=0)\n\n    bound_cond = [(_zero_boundary, _zero_boundary),\n                  (_zero_boundary, _zero_boundary)]\n\n    result = fd_solvers.solve_backward(\n        start_time=final_t,\n        end_time=0,\n        coord_grid=grid,\n        values_grid=final_values,\n        time_step=time_step,\n        one_step_fn=douglas_adi_step(theta=0.5),\n        boundary_conditions=bound_cond,\n        first_order_coeff_fn=first_order_coeff_fn,\n        dtype=grid[0].dtype)\n\n    expected = np.outer(\n        _gaussian(ys + vy * final_t, variance),\n        _gaussian(xs + vx * final_t, variance))\n\n    self._assertClose(expected, result)\n\n  # These four tests below run _testAnisotropicDiffusion with different formats\n  # of quadratic term.\n  def testAnisotropicDiffusion_TwoDimList(self):\n    def pack_second_order_coeff_fn(u_yy, u_xy, u_xx):\n      return [[u_yy, u_xy], [u_xy, u_xx]]\n    self._testDiffusionInDiagonalDirection(pack_second_order_coeff_fn)\n\n  def testAnisotropicDiffusion_TwoDimList_WithoutRedundantElement(self):\n    def pack_second_order_coeff_fn(u_yy, u_xy, u_xx):\n      return [[u_yy, u_xy], [None, u_xx]]\n    self._testDiffusionInDiagonalDirection(pack_second_order_coeff_fn)\n\n  def testAnisotropicDiffusion_ListOfTensors(self):\n    def pack_second_order_coeff_fn(u_yy, u_xy, u_xx):\n      return [tf.constant([u_yy, u_xy], dtype=tf.float32),\n              tf.constant([u_xy, u_xx], dtype=tf.float32)]\n    self._testDiffusionInDiagonalDirection(pack_second_order_coeff_fn)\n\n  def testAnisotropicDiffusion_2DTensor(self):\n    def pack_second_order_coeff_fn(u_yy, u_xy, u_xx):\n      return tf.convert_to_tensor([[u_yy, u_xy], [u_xy, u_xx]],\n                                  dtype=tf.float32)\n    self._testDiffusionInDiagonalDirection(pack_second_order_coeff_fn)\n\n  # pylint: disable=g-doc-args\n  def _testDiffusionInDiagonalDirection(self, pack_second_order_coeff_fn):\n    """"""Tests solving 2d diffusion equation involving mixed terms.\n\n    The equation is `u_{t} + D u_{xx} / 2 +  D u_{yy} / 2 + D u_{xy} = 0`.\n    The final condition is a gaussian centered at (0, 0) with variance sigma.\n\n    The equation can be rewritten as `u_{t} + D u_{zz} = 0`, where\n    `z = (x + y) / sqrt(2)`.\n\n    Thus variance should evolve as `sigma + 2D(t_final - t)` along z dimension\n    and stay unchanged in the orthogonal dimension:\n    `u(x, y, t) = gaussian((x + y)/sqrt(2), sigma + 2D(t_final - t)) *\n    gaussian((x - y)/sqrt(2), sigma)`.\n    """"""\n    dtype = tf.float32\n\n    grid = grids.uniform_grid(\n        minimums=[-10, -20], maximums=[10, 20], sizes=[201, 301], dtype=dtype)\n    ys = self.evaluate(grid[0])\n    xs = self.evaluate(grid[1])\n\n    diff_coeff = 1  # D\n    time_step = 0.1\n    final_t = 3\n    final_variance = 1\n\n    def second_order_coeff_fn(t, location_grid):\n      del t, location_grid\n      return pack_second_order_coeff_fn(diff_coeff / 2, diff_coeff / 2,\n                                        diff_coeff / 2)\n\n    variance_along_diagonal = final_variance + 2 * diff_coeff * final_t\n\n    def expected_fn(x, y):\n      return (_gaussian((x + y) / _SQRT2, variance_along_diagonal) * _gaussian(\n          (x - y) / _SQRT2, final_variance))\n\n    expected = np.array([[expected_fn(x, y) for x in xs] for y in ys])\n\n    final_values = tf.expand_dims(\n        tf.constant(\n            np.outer(\n                _gaussian(ys, final_variance), _gaussian(xs, final_variance)),\n            dtype=dtype),\n        axis=0)\n    bound_cond = [(_zero_boundary, _zero_boundary),\n                  (_zero_boundary, _zero_boundary)]\n    step_fn = douglas_adi_step(theta=0.5)\n    result = fd_solvers.solve_backward(\n        start_time=final_t,\n        end_time=0,\n        coord_grid=grid,\n        values_grid=final_values,\n        time_step=time_step,\n        one_step_fn=step_fn,\n        boundary_conditions=bound_cond,\n        second_order_coeff_fn=second_order_coeff_fn,\n        dtype=grid[0].dtype)\n\n    self._assertClose(expected, result)\n\n  def testShiftTerm(self):\n    """"""Simple test for the shift term.\n\n    The equation is `u_{t} + a u = 0`, the solution is\n    `u(x, y, t) = exp(-a(t - t_final)) u(x, y, t_final)`\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[-10, -20],\n        maximums=[10, 20],\n        sizes=[201, 301],\n        dtype=tf.float32)\n    ys = self.evaluate(grid[0])\n    xs = self.evaluate(grid[1])\n\n    time_step = 0.1\n    final_t = 1\n    variance = 1\n    a = 2\n\n    def zeroth_order_coeff_fn(t, location_grid):\n      del t, location_grid\n      return a\n\n    expected = (\n        np.outer(_gaussian(ys, variance), _gaussian(xs, variance)) *\n        np.exp(a * final_t))\n\n    final_values = tf.expand_dims(\n        tf.constant(\n            np.outer(_gaussian(ys, variance), _gaussian(xs, variance)),\n            dtype=tf.float32),\n        axis=0)\n    bound_cond = [(_zero_boundary, _zero_boundary),\n                  (_zero_boundary, _zero_boundary)]\n    step_fn = douglas_adi_step(theta=0.5)\n    result = fd_solvers.solve_backward(\n        start_time=final_t,\n        end_time=0,\n        coord_grid=grid,\n        values_grid=final_values,\n        time_step=time_step,\n        one_step_fn=step_fn,\n        boundary_conditions=bound_cond,\n        zeroth_order_coeff_fn=zeroth_order_coeff_fn,\n        dtype=grid[0].dtype)\n\n    self._assertClose(expected, result)\n\n  def testNoTimeDependence(self):\n    """"""Test for the case where all terms (quadratic, linear, shift) are null.""""""\n    grid = grids.uniform_grid(\n        minimums=[-10, -20],\n        maximums=[10, 20],\n        sizes=[201, 301],\n        dtype=tf.float32)\n    ys = self.evaluate(grid[0])\n    xs = self.evaluate(grid[1])\n\n    time_step = 0.1\n    final_t = 1\n    variance = 1\n\n    final_cond = np.outer(_gaussian(ys, variance), _gaussian(xs, variance))\n    final_values = tf.expand_dims(tf.constant(final_cond, dtype=tf.float32),\n                                  axis=0)\n    bound_cond = [(_zero_boundary, _zero_boundary),\n                  (_zero_boundary, _zero_boundary)]\n    step_fn = douglas_adi_step(theta=0.5)\n    result = fd_solvers.solve_backward(\n        start_time=final_t,\n        end_time=0,\n        coord_grid=grid,\n        values_grid=final_values,\n        time_step=time_step,\n        one_step_fn=step_fn,\n        boundary_conditions=bound_cond,\n        dtype=grid[0].dtype)\n    expected = final_cond  # No time dependence.\n    self._assertClose(expected, result)\n\n  def testAnisotropicDiffusion_WithDirichletBoundaries(self):\n    """"""Tests solving 2d diffusion equation with Dirichlet boundary conditions.\n\n    The equation is `u_{t} + u_{xx} + 2 u_{yy} = 0`.\n    The final condition is `u(t=1, x, y) = e * sin(x/sqrt(2)) * cos(y / 2)`.\n    The following function satisfies this PDE and final condition:\n    `u(t, x, y) = exp(t) * sin(x / sqrt(2)) * cos(y / 2)`.\n    We impose Dirichlet boundary conditions using this function:\n    `u(t, x_min, y) = exp(t) * sin(x_min / sqrt(2)) * cos(y / 2)`, etc.\n    The other tests below are similar, but with other types of boundary\n    conditions.\n    """"""\n    time_step = 0.01\n    final_t = 1\n    x_min = -20\n    x_max = 20\n    y_min = -10\n    y_max = 10\n\n    grid = grids.uniform_grid(\n        minimums=[y_min, x_min],\n        maximums=[y_max, x_max],\n        sizes=[201, 301],\n        dtype=tf.float32)\n    ys = self.evaluate(grid[0])\n    xs = self.evaluate(grid[1])\n\n    def second_order_coeff_fn(t, location_grid):\n      del t, location_grid\n      return [[2, None], [None, 1]]\n\n    @dirichlet\n    def lower_bound_x(t, location_grid):\n      del location_grid\n      return tf.exp(t) * np.sin(x_min / _SQRT2) * tf.sin(ys / 2)\n\n    @dirichlet\n    def upper_bound_x(t, location_grid):\n      del location_grid\n      return tf.exp(t) * np.sin(x_max / _SQRT2) * tf.sin(ys / 2)\n\n    @dirichlet\n    def lower_bound_y(t, location_grid):\n      del location_grid\n      return tf.exp(t) * tf.sin(xs / _SQRT2) * np.sin(y_min / 2)\n\n    @dirichlet\n    def upper_bound_y(t, location_grid):\n      del location_grid\n      return tf.exp(t) * tf.sin(xs / _SQRT2) * np.sin(y_max / 2)\n\n    expected = np.outer(np.sin(ys / 2), np.sin(xs / _SQRT2))\n\n    final_values = tf.expand_dims(\n        tf.constant(\n            np.outer(np.sin(ys / 2), np.sin(xs / _SQRT2)) * np.exp(final_t),\n            dtype=tf.float32),\n        axis=0)\n    bound_cond = [(lower_bound_y, upper_bound_y),\n                  (lower_bound_x, upper_bound_x)]\n    step_fn = douglas_adi_step(theta=0.5)\n    result = fd_solvers.solve_backward(\n        start_time=final_t,\n        end_time=0,\n        coord_grid=grid,\n        values_grid=final_values,\n        time_step=time_step,\n        one_step_fn=step_fn,\n        boundary_conditions=bound_cond,\n        second_order_coeff_fn=second_order_coeff_fn,\n        dtype=grid[0].dtype)\n\n    self._assertClose(expected, result)\n\n  def testAnisotropicDiffusion_WithNeumannBoundaries(self):\n    """"""Tests solving 2d diffusion equation with Neumann boundary conditions.""""""\n    time_step = 0.01\n    final_t = 1\n    x_min = -20\n    x_max = 20\n    y_min = -10\n    y_max = 10\n\n    grid = grids.uniform_grid(\n        minimums=[y_min, x_min],\n        maximums=[y_max, x_max],\n        sizes=[201, 301],\n        dtype=tf.float32)\n    ys = self.evaluate(grid[0])\n    xs = self.evaluate(grid[1])\n\n    def second_order_coeff_fn(t, location_grid):\n      del t, location_grid\n      return [[2, None], [None, 1]]\n\n    @neumann\n    def lower_bound_x(t, location_grid):\n      del location_grid\n      return -tf.exp(t) * np.cos(x_min / _SQRT2) * tf.sin(ys / 2) / _SQRT2\n\n    @neumann\n    def upper_bound_x(t, location_grid):\n      del location_grid\n      return tf.exp(t) * np.cos(x_max / _SQRT2) * tf.sin(ys / 2) / _SQRT2\n\n    @neumann\n    def lower_bound_y(t, location_grid):\n      del location_grid\n      return -tf.exp(t) * tf.sin(xs / _SQRT2) * np.cos(y_min / 2) / 2\n\n    @neumann\n    def upper_bound_y(t, location_grid):\n      del location_grid\n      return tf.exp(t) * tf.sin(xs / _SQRT2) * np.cos(y_max / 2) / 2\n\n    expected = np.outer(np.sin(ys / 2), np.sin(xs / _SQRT2))\n\n    final_values = tf.expand_dims(\n        tf.constant(\n            np.outer(np.sin(ys / 2), np.sin(xs / _SQRT2)) * np.exp(final_t),\n            dtype=tf.float32),\n        axis=0)\n    bound_cond = [(lower_bound_y, upper_bound_y),\n                  (lower_bound_x, upper_bound_x)]\n    step_fn = douglas_adi_step(theta=0.5)\n    result = fd_solvers.solve_backward(\n        start_time=final_t,\n        end_time=0,\n        coord_grid=grid,\n        values_grid=final_values,\n        time_step=time_step,\n        one_step_fn=step_fn,\n        boundary_conditions=bound_cond,\n        second_order_coeff_fn=second_order_coeff_fn,\n        dtype=grid[0].dtype)\n\n    self._assertClose(expected, result)\n\n  def testAnisotropicDiffusion_WithMixedBoundaries(self):\n    """"""Tests solving 2d diffusion equation with mixed boundary conditions.""""""\n    time_step = 0.01\n    final_t = 1\n    x_min = -20\n    x_max = 20\n    y_min = -10\n    y_max = 10\n\n    grid = grids.uniform_grid(\n        minimums=[y_min, x_min],\n        maximums=[y_max, x_max],\n        sizes=[201, 301],\n        dtype=tf.float32)\n    ys = self.evaluate(grid[0])\n    xs = self.evaluate(grid[1])\n\n    def second_order_coeff_fn(t, location_grid):\n      del t, location_grid\n      return [[2, None], [None, 1]]\n\n    @dirichlet\n    def lower_bound_x(t, location_grid):\n      del location_grid\n      return tf.exp(t) * np.sin(x_min / _SQRT2) * tf.sin(ys / 2)\n\n    @neumann\n    def upper_bound_x(t, location_grid):\n      del location_grid\n      return tf.exp(t) * np.cos(x_max / _SQRT2) * tf.sin(ys / 2) / _SQRT2\n\n    @neumann\n    def lower_bound_y(t, location_grid):\n      del location_grid\n      return -tf.exp(t) * tf.sin(xs / _SQRT2) * np.cos(y_min / 2) / 2\n\n    @dirichlet\n    def upper_bound_y(t, location_grid):\n      del location_grid\n      return tf.exp(t) * tf.sin(xs / _SQRT2) * np.sin(y_max / 2)\n\n    expected = np.outer(np.sin(ys / 2), np.sin(xs / _SQRT2))\n\n    final_values = tf.expand_dims(\n        tf.constant(\n            np.outer(np.sin(ys / 2), np.sin(xs / _SQRT2)) * np.exp(final_t),\n            dtype=tf.float32),\n        axis=0)\n    bound_cond = [(lower_bound_y, upper_bound_y),\n                  (lower_bound_x, upper_bound_x)]\n    step_fn = douglas_adi_step(theta=0.5)\n    result = fd_solvers.solve_backward(\n        start_time=final_t,\n        end_time=0,\n        coord_grid=grid,\n        values_grid=final_values,\n        time_step=time_step,\n        one_step_fn=step_fn,\n        boundary_conditions=bound_cond,\n        second_order_coeff_fn=second_order_coeff_fn,\n        dtype=grid[0].dtype)\n\n    self._assertClose(expected, result)\n\n  def testAnisotropicDiffusion_WithRobinBoundaries(self):\n    """"""Tests solving 2d diffusion equation with Robin boundary conditions.""""""\n    time_step = 0.01\n    final_t = 1\n    x_min = -20\n    x_max = 20\n    y_min = -10\n    y_max = 10\n\n    grid = grids.uniform_grid(\n        minimums=[y_min, x_min],\n        maximums=[y_max, x_max],\n        sizes=[201, 301],\n        dtype=tf.float32)\n    ys = self.evaluate(grid[0])\n    xs = self.evaluate(grid[1])\n\n    def second_order_coeff_fn(t, location_grid):\n      del t, location_grid\n      return [[2, None], [None, 1]]\n\n    def lower_bound_x(t, location_grid):\n      del location_grid\n      f = tf.exp(t) * tf.sin(ys / 2) * (\n          np.sin(x_min / _SQRT2) - np.cos(x_min / _SQRT2) / _SQRT2)\n      return 1, 1, f\n\n    def upper_bound_x(t, location_grid):\n      del location_grid\n      f = tf.exp(t) * tf.sin(ys / 2) * (\n          np.sin(x_max / _SQRT2) + 2 * np.cos(x_max / _SQRT2) / _SQRT2)\n      return 1, 2, f\n\n    def lower_bound_y(t, location_grid):\n      del location_grid\n      f = tf.exp(t) * tf.sin(xs / _SQRT2) * (\n          np.sin(y_min / 2) - 3 * np.cos(y_min / 2) / 2)\n      return 1, 3, f\n\n    def upper_bound_y(t, location_grid):\n      del location_grid\n      f = tf.exp(t) * tf.sin(\n          xs / _SQRT2) * (2 * np.sin(y_max / 2) + 3 * np.cos(y_max / 2) / 2)\n      return 2, 3, f\n\n    expected = np.outer(np.sin(ys / 2), np.sin(xs / _SQRT2))\n\n    final_values = tf.expand_dims(\n        tf.constant(\n            np.outer(np.sin(ys / 2), np.sin(xs / _SQRT2)) * np.exp(final_t),\n            dtype=tf.float32),\n        axis=0)\n    bound_cond = [(lower_bound_y, upper_bound_y),\n                  (lower_bound_x, upper_bound_x)]\n    step_fn = douglas_adi_step(theta=0.5)\n    result = fd_solvers.solve_backward(\n        start_time=final_t,\n        end_time=0,\n        coord_grid=grid,\n        values_grid=final_values,\n        time_step=time_step,\n        one_step_fn=step_fn,\n        boundary_conditions=bound_cond,\n        second_order_coeff_fn=second_order_coeff_fn,\n        dtype=grid[0].dtype)\n\n    self._assertClose(expected, result)\n\n  def _assertClose(self, expected, stepper_result):\n    actual = self.evaluate(stepper_result[0])\n    self.assertLess(np.max(np.abs(actual - expected)) / np.max(expected), 0.01)\n\n  def testAnisotropicDiffusion_InForwardDirection(self):\n    """"""Tests solving 2d diffusion equation in forward direction.\n\n    The equation is `u_{t} - Dx u_{xx} - Dy u_{yy} = 0`.\n    The initial condition is a gaussian centered at (0, 0) with variance sigma.\n    The variance along each dimension should evolve as `sigma + 2 Dx (t - t_0)`\n    and `sigma + 2 Dy (t - t_0)`.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[-10, -20],\n        maximums=[10, 20],\n        sizes=[201, 301],\n        dtype=tf.float32)\n    ys = self.evaluate(grid[0])\n    xs = self.evaluate(grid[1])\n\n    diff_coeff_x = 0.4  # Dx\n    diff_coeff_y = 0.25  # Dy\n    time_step = 0.1\n    final_t = 1.0\n    initial_variance = 1\n\n    def quadratic_coeff_fn(t, location_grid):\n      del t, location_grid\n      u_xx = -diff_coeff_x\n      u_yy = -diff_coeff_y\n      u_xy = None\n      return [[u_yy, u_xy], [u_xy, u_xx]]\n\n    final_values = tf.expand_dims(\n        tf.constant(\n            np.outer(\n                _gaussian(ys, initial_variance),\n                _gaussian(xs, initial_variance)),\n            dtype=tf.float32),\n        axis=0)\n    bound_cond = [(_zero_boundary, _zero_boundary),\n                  (_zero_boundary, _zero_boundary)]\n    step_fn = douglas_adi_step(theta=0.5)\n    result = fd_solvers.solve_forward(\n        start_time=0.0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=final_values,\n        time_step=time_step,\n        one_step_fn=step_fn,\n        boundary_conditions=bound_cond,\n        second_order_coeff_fn=quadratic_coeff_fn,\n        dtype=grid[0].dtype)\n\n    variance_x = initial_variance + 2 * diff_coeff_x * final_t\n    variance_y = initial_variance + 2 * diff_coeff_y * final_t\n    expected = np.outer(_gaussian(ys, variance_y), _gaussian(xs, variance_x))\n\n    self._assertClose(expected, result)\n\n  def testReferenceEquation(self):\n    """"""Tests the equation used as reference for a few further tests.\n\n    We solve the heat equation `u_t = u_xx + u_yy` on x = [0...1], y = [0...1]\n    with boundary conditions `u(x, y, t=0) = (1/2 - |x-1/2|)(1/2-|y-1/2|), and\n    zero Dirichlet on all spatial boundaries.\n\n    The exact solution of the diffusion equation with zero-Dirichlet rectangular\n    boundaries is `u(x, y, t) = u(x, t) * u(y, t)`,\n    `u(z, t) = sum_{n=1..inf} b_n sin(pi n z) exp(-n^2 pi^2 t)`,\n    `b_n = 2 integral_{0..1} sin(pi n z) u(z, t=0) dz.`\n\n    The initial conditions are taken so that the integral easily calculates, and\n    the sum can be approximated by a few first terms (given large enough `t`).\n    See the result in _reference_heat_equation_solution.\n\n    Using this solution helps to simplify the tests, as we don\'t have to\n    maintain complicated boundary conditions in each test or tweak the\n    parameters to keep the ""support"" of the function far from boundaries.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[0, 0], maximums=[1, 1], sizes=[201, 301], dtype=tf.float32)\n    ys, xs = grid\n\n    final_t = 0.1\n    time_step = 0.002\n\n    def second_order_coeff_fn(t, coord_grid):\n      del t, coord_grid\n      return [[-1, None], [None, -1]]\n\n    initial = _reference_2d_pde_initial_cond(xs, ys)\n    expected = _reference_2d_pde_solution(xs, ys, final_t)\n    actual = fd_solvers.solve_forward(\n        start_time=0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=initial,\n        time_step=time_step,\n        second_order_coeff_fn=second_order_coeff_fn)[0]\n\n    self.assertAllClose(expected, actual, atol=1e-3, rtol=1e-3)\n\n  def testReference_WithExponentMultiplier(self):\n    """"""Tests solving diffusion equation with an exponent multiplier.\n\n    Take the heat equation `v_{t} - v_{xx} - v_{yy} = 0` and substitute\n    `v = exp(x + 2y) u`.\n    This yields `u_{t} - u_{xx} - u_{yy} - 2u_{x} - 4u_{y} - 5u = 0`. The test\n    compares numerical solution of this equation to the exact one, which is the\n    diffusion equation solution times `exp(-x-2y)`.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[0, 0], maximums=[1, 1], sizes=[201, 301], dtype=tf.float32)\n    ys, xs = grid\n\n    final_t = 0.1\n    time_step = 0.002\n\n    def second_order_coeff_fn(t, coord_grid):\n      del t, coord_grid\n      return [[-1, None], [None, -1]]\n\n    def first_order_coeff_fn(t, coord_grid):\n      del t, coord_grid\n      return [-4, -2]\n\n    def zeroth_order_coeff_fn(t, coord_grid):\n      del t, coord_grid\n      return -5\n\n    exp = _dir_prod(tf.exp(-2 * ys), tf.exp(-xs))\n    initial = exp * _reference_2d_pde_initial_cond(xs, ys)\n    expected = exp * _reference_2d_pde_solution(xs, ys, final_t)\n\n    actual = fd_solvers.solve_forward(\n        start_time=0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=initial,\n        time_step=time_step,\n        second_order_coeff_fn=second_order_coeff_fn,\n        first_order_coeff_fn=first_order_coeff_fn,\n        zeroth_order_coeff_fn=zeroth_order_coeff_fn)[0]\n\n    self.assertAllClose(expected, actual, atol=1e-3, rtol=1e-3)\n\n  def testInnerSecondOrderCoeff(self):\n    """"""Tests handling inner_second_order_coeff.\n\n    As in previous test, take the diffusion equation\n    `v_{t} - v_{xx} - v_{yy} = 0` and substitute `v = exp(x + 2y) u`, but this\n    time keep exponent under the derivative:\n    `u_{t} - exp(-x)[exp(x)u]_{xx} - exp(-2y)[exp(2y)u]_{yy} = 0`.\n    Expect the same solution as in previous test.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[0, 0], maximums=[1, 1], sizes=[201, 251], dtype=tf.float32)\n    ys, xs = grid\n\n    final_t = 0.1\n    time_step = 0.002\n\n    def second_order_coeff_fn(t, coord_grid):\n      del t\n      y, x = tf.meshgrid(*coord_grid, indexing=\'ij\')\n      return [[-tf.exp(-2 * y), None], [None, -tf.exp(-x)]]\n\n    def inner_second_order_coeff_fn(t, coord_grid):\n      del t\n      y, x = tf.meshgrid(*coord_grid, indexing=\'ij\')\n      return [[tf.exp(2 * y), None], [None, tf.exp(x)]]\n\n    exp = _dir_prod(tf.exp(-2 * ys), tf.exp(-xs))\n    initial = exp * _reference_2d_pde_initial_cond(xs, ys)\n    expected = exp * _reference_2d_pde_solution(xs, ys, final_t)\n\n    actual = fd_solvers.solve_forward(\n        start_time=0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=initial,\n        time_step=time_step,\n        second_order_coeff_fn=second_order_coeff_fn,\n        inner_second_order_coeff_fn=inner_second_order_coeff_fn)[0]\n\n    self.assertAllClose(expected, actual, atol=1e-3, rtol=1e-3)\n\n  def testInnerFirstAndSecondOrderCoeff(self):\n    """"""Tests handling both inner_first_order_coeff and inner_second_order_coeff.\n\n    We saw previously that the solution of\n    `u_{t} - u_{xx} - u_{yy} - 2u_{x} - 4u_{y} - 5u = 0` is\n    `u = exp(-x-2y) v`, where `v` solves the diffusion equation. Substitute now\n    `u = exp(-x-2y) v` without expanding the derivatives:\n    `v_{t} - exp(x)[exp(-x)v]_{xx} - exp(2y)[exp(-2y)v]_{yy} -\n      2exp(x)[exp(-x)v]_{x} - 4exp(2y)[exp(-2y)v]_{y} - 5v = 0`.\n    Solve this equation and expect the solution of the diffusion equation.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[0, 0], maximums=[1, 1], sizes=[201, 251], dtype=tf.float32)\n    ys, xs = grid\n\n    final_t = 0.1\n    time_step = 0.002\n\n    def second_order_coeff_fn(t, coord_grid):\n      del t\n      y, x = tf.meshgrid(*coord_grid, indexing=\'ij\')\n      return [[-tf.exp(2 * y), None], [None, -tf.exp(x)]]\n\n    def inner_second_order_coeff_fn(t, coord_grid):\n      del t\n      y, x = tf.meshgrid(*coord_grid, indexing=\'ij\')\n      return [[tf.exp(-2 * y), None], [None, tf.exp(-x)]]\n\n    def first_order_coeff_fn(t, coord_grid):\n      del t\n      y, x = tf.meshgrid(*coord_grid, indexing=\'ij\')\n      return [-4 * tf.exp(2 * y), -2 * tf.exp(x)]\n\n    def inner_first_order_coeff_fn(t, coord_grid):\n      del t\n      y, x = tf.meshgrid(*coord_grid, indexing=\'ij\')\n      return [tf.exp(-2 * y), tf.exp(-x)]\n\n    def zeroth_order_coeff_fn(t, coord_grid):\n      del t, coord_grid\n      return -5\n\n    initial = _reference_2d_pde_initial_cond(xs, ys)\n    expected = _reference_2d_pde_solution(xs, ys, final_t)\n\n    actual = fd_solvers.solve_forward(\n        start_time=0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=initial,\n        time_step=time_step,\n        second_order_coeff_fn=second_order_coeff_fn,\n        first_order_coeff_fn=first_order_coeff_fn,\n        zeroth_order_coeff_fn=zeroth_order_coeff_fn,\n        inner_second_order_coeff_fn=inner_second_order_coeff_fn,\n        inner_first_order_coeff_fn=inner_first_order_coeff_fn)[0]\n\n    self.assertAllClose(expected, actual, atol=1e-3, rtol=1e-3)\n\n  def testReferenceEquation_WithTransformationYieldingMixedTerm(self):\n    """"""Tests an equation with mixed terms against exact solution.\n\n    Take the reference equation `v_{t} = v_{xx} + v_{yy}` and substitute\n    `v(x, y, t) = u(x, 2y - x, t)`. This yields\n    `u_{t} = u_{xx} + 5u_{zz} - 2u_{xz}`, where `z = 2y - x`.\n    Having `u(x, z, t) = v(x, (x+z)/2, t)` where `v(x, y, t)` is the known\n    solution of the reference equation, we derive the boundary conditions\n    and the expected solution for `u(x, y, t)`.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[0, 0], maximums=[1, 1], sizes=[201, 251], dtype=tf.float32)\n\n    final_t = 0.1\n    time_step = 0.002\n\n    def second_order_coeff_fn(t, coord_grid):\n      del t, coord_grid\n      return [[-5, 1], [None, -1]]\n\n    @dirichlet\n    def boundary_lower_z(t, coord_grid):\n      x = coord_grid[1]\n      return _reference_pde_solution(x, t) * _reference_pde_solution(x / 2, t)\n\n    @dirichlet\n    def boundary_upper_z(t, coord_grid):\n      x = coord_grid[1]\n      return _reference_pde_solution(x, t) * _reference_pde_solution(\n          (x + 1) / 2, t)\n\n    z_mesh, x_mesh = tf.meshgrid(grid[0], grid[1], indexing=\'ij\')\n    initial = (\n        _reference_pde_initial_cond(x_mesh) * _reference_pde_initial_cond(\n            (x_mesh + z_mesh) / 2))\n    expected = (\n        _reference_pde_solution(x_mesh, final_t) * _reference_pde_solution(\n            (x_mesh + z_mesh) / 2, final_t))\n\n    actual = fd_solvers.solve_forward(\n        start_time=0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=initial,\n        time_step=time_step,\n        second_order_coeff_fn=second_order_coeff_fn,\n        boundary_conditions=[(boundary_lower_z, boundary_upper_z),\n                             (_zero_boundary, _zero_boundary)])[0]\n\n    self.assertAllClose(expected, actual, atol=1e-3, rtol=1e-3)\n\n  def testInnerMixedSecondOrderCoeffs(self):\n    """"""Tests handling coefficients under the mixed second derivative.\n\n    Take the equation from the previous test,\n    `u_{t} = u_{xx} + 5u_{zz} - 2u_{xz}` and substitute `u = exp(xz) w`,\n    leaving the exponent under the derivatives:\n    `w_{t} = exp(-xz) [exp(xz) u]_{xx} + 5 exp(-xz) [exp(xz) u]_{zz}\n    - 2 exp(-xz) [exp(xz) u]_{xz}`.\n    We now have a coefficient under the mixed derivative. Test that the solution\n    is `w = exp(-xz) u`, where u is from the previous test.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[0, 0], maximums=[1, 1], sizes=[201, 251], dtype=tf.float32)\n\n    final_t = 0.1\n    time_step = 0.002\n\n    def second_order_coeff_fn(t, coord_grid):\n      del t,\n      z, x = tf.meshgrid(*coord_grid, indexing=\'ij\')\n      exp = tf.math.exp(-z * x)\n      return [[-5 * exp, exp], [None, -exp]]\n\n    def inner_second_order_coeff_fn(t, coord_grid):\n      del t,\n      z, x = tf.meshgrid(*coord_grid, indexing=\'ij\')\n      exp = tf.math.exp(z * x)\n      return [[exp, exp], [None, exp]]\n\n    @dirichlet\n    def boundary_lower_z(t, coord_grid):\n      x = coord_grid[1]\n      return _reference_pde_solution(x, t) * _reference_pde_solution(x / 2, t)\n\n    @dirichlet\n    def boundary_upper_z(t, coord_grid):\n      x = coord_grid[1]\n      return tf.exp(-x) * _reference_pde_solution(\n          x, t) * _reference_pde_solution((x + 1) / 2, t)\n\n    z, x = tf.meshgrid(*grid, indexing=\'ij\')\n    exp = tf.math.exp(-z * x)\n    initial = exp * (\n        _reference_pde_initial_cond(x) * _reference_pde_initial_cond(\n            (x + z) / 2))\n    expected = exp * (\n        _reference_pde_solution(x, final_t) * _reference_pde_solution(\n            (x + z) / 2, final_t))\n\n    actual = fd_solvers.solve_forward(\n        start_time=0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=initial,\n        time_step=time_step,\n        second_order_coeff_fn=second_order_coeff_fn,\n        inner_second_order_coeff_fn=inner_second_order_coeff_fn,\n        boundary_conditions=[(boundary_lower_z, boundary_upper_z),\n                             (_zero_boundary, _zero_boundary)])[0]\n\n    self.assertAllClose(expected, actual, atol=1e-3, rtol=1e-3)\n\n  def testCompareExpandedAndNotExpandedPdes(self):\n    """"""Tests comparing PDEs with expanded derivatives and without.\n\n    The equation is\n    `u_{t} + [x u]_{x} + [y^2 u]_{y} - [sin(x) u]_{xx} - [cos(y) u]_yy\n     + [x^3 y^2 u]_{xy} = 0`.\n    Solve the equation, expand the derivatives and solve the equation again.\n    Expect the results to be equal.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[0, 0], maximums=[1, 1], sizes=[201, 251], dtype=tf.float32)\n\n    final_t = 0.1\n    time_step = 0.002\n    y, x = grid\n\n    initial = _reference_2d_pde_initial_cond(x, y)  # arbitrary\n\n    def inner_second_order_coeff_fn(t, coord_grid):\n      del t\n      y, x = tf.meshgrid(*coord_grid, indexing=\'ij\')\n      return [[-tf.math.cos(y), x**3 * y**2 / 2], [None, -tf.math.sin(x)]]\n\n    def inner_first_order_coeff_fn(t, coord_grid):\n      del t\n      y, x = tf.meshgrid(*coord_grid, indexing=\'ij\')\n      return [y**2, x]\n\n    result_not_expanded = fd_solvers.solve_forward(\n        start_time=0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=initial,\n        time_step=time_step,\n        inner_second_order_coeff_fn=inner_second_order_coeff_fn,\n        inner_first_order_coeff_fn=inner_first_order_coeff_fn)[0]\n\n    def second_order_coeff_fn(t, coord_grid):\n      del t\n      y, x = tf.meshgrid(*coord_grid, indexing=\'ij\')\n      return [[-tf.math.cos(y), x**3 * y**2 / 2], [None, -tf.math.sin(x)]]\n\n    def first_order_coeff_fn(t, coord_grid):\n      del t\n      y, x = tf.meshgrid(*coord_grid, indexing=\'ij\')\n      return [y**2 * (1 + 3 * x**2) + 2 * tf.math.sin(y),\n              x * (1 + 2 * x**2 * y) - 2 * tf.math.cos(x)]\n\n    def zeroth_order_coeff_fn(t, coord_grid):\n      del t\n      y, x = tf.meshgrid(*coord_grid, indexing=\'ij\')\n      return 1 + 2 * y + tf.math.sin(x) + tf.math.cos(x) + 6 * x**2 * y\n\n    result_expanded = fd_solvers.solve_forward(\n        start_time=0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=initial,\n        time_step=time_step,\n        second_order_coeff_fn=second_order_coeff_fn,\n        first_order_coeff_fn=first_order_coeff_fn,\n        zeroth_order_coeff_fn=zeroth_order_coeff_fn)[0]\n\n    self.assertAllClose(\n        result_not_expanded, result_expanded, atol=1e-3, rtol=1e-3)\n\n\ndef _gaussian(xs, variance):\n  return np.exp(-np.square(xs) / (2 * variance)) / np.sqrt(2 * np.pi * variance)\n\n\n@dirichlet\ndef _zero_boundary(t, locations):\n  del t, locations\n  return 0\n\n\ndef _reference_2d_pde_initial_cond(xs, ys):\n  """"""Initial conditions for the reference 2d diffusion equation.""""""\n  return _dir_prod(\n      _reference_pde_initial_cond(ys), _reference_pde_initial_cond(xs))\n\n\ndef _reference_2d_pde_solution(xs, ys, t, num_terms=5):\n  return _dir_prod(\n      _reference_pde_solution(ys, t, num_terms),\n      _reference_pde_solution(xs, t, num_terms))\n\n\ndef _reference_pde_initial_cond(xs):\n  """"""Initial conditions for the reference diffusion equation.""""""\n  return -tf.math.abs(xs - 0.5) + 0.5\n\n\ndef _reference_pde_solution(xs, t, num_terms=5):\n  """"""Solution for the reference diffusion equation.""""""\n  u = tf.zeros_like(xs)\n  for k in range(num_terms):\n    n = 2 * k + 1\n    term = tf.math.sin(np.pi * n * xs) * tf.math.exp(-n**2 * np.pi**2 * t)\n    term *= 4 / (np.pi**2 * n**2)\n    if k % 2 == 1:\n      term *= -1\n    u += term\n  return u\n\n\ndef _dir_prod(a, b):\n  """"""Calculates the direct product of two Tensors.""""""\n  return tf.tensordot(a, b, ([], []))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/math/pde/steppers/oscillation_damped_crank_nicolson.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Crank-Nicolson with oscillation damping time marching scheme.""""""\n\nfrom tf_quant_finance.math.pde.steppers.composite_stepper import composite_scheme_step\nfrom tf_quant_finance.math.pde.steppers.crank_nicolson import crank_nicolson_scheme\nfrom tf_quant_finance.math.pde.steppers.extrapolation import extrapolation_scheme\n\n\ndef oscillation_damped_crank_nicolson_step(extrapolation_steps=1):\n  """"""Scheme similar to Crank-Nicolson, but ensuring damping of oscillations.\n\n  Performs first (or first few) steps with Extrapolation scheme, then proceeds\n  with Crank-Nicolson scheme. This combines absence of oscillations by virtue\n  of Extrapolation scheme with lower computational cost of Crank-Nicolson\n  scheme.\n\n  See [1], [2] ([2] mostly discusses using fully implicit scheme on the first\n  step, but mentions using extrapolation scheme for better accuracy in the end).\n\n  #### References:\n  [1]: D. Lawson, J & Ll Morris, J. The Extrapolation of First Order Methods for\n    Parabolic Partial Differential Equations. I. 1978 SIAM Journal on Numerical\n    Analysis. 15. 1212-1224.\n    https://epubs.siam.org/doi/abs/10.1137/0715082\n  [2]: B. Giles, Michael & Carter, Rebecca. Convergence analysis of\n    Crank-Nicolson and Rannacher time-marching. J. Comput. Finance. 9. 2005.\n    https://core.ac.uk/download/pdf/1633712.pdf\n\n  Args:\n    extrapolation_steps: number of first steps to which to apply the\n      Extrapolation scheme. Defaults to `1`.\n\n  Returns:\n    Callable to use as `one_step_fn` in fd_solvers.\n  """"""\n  return composite_scheme_step(extrapolation_steps, extrapolation_scheme,\n                               crank_nicolson_scheme)\n\n__all__ = [""oscillation_damped_crank_nicolson_step""]\n'"
tf_quant_finance/math/pde/steppers/parabolic_equation_stepper.py,18,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Stepper for parabolic PDEs solving.""""""\n\nimport tensorflow.compat.v2 as tf\n\n\ndef parabolic_equation_step(\n    time,\n    next_time,\n    coord_grid,\n    value_grid,\n    boundary_conditions,\n    second_order_coeff_fn,\n    first_order_coeff_fn,\n    zeroth_order_coeff_fn,\n    inner_second_order_coeff_fn,\n    inner_first_order_coeff_fn,\n    time_marching_scheme,\n    dtype=None,\n    name=None):\n  """"""Performs one step of the parabolic PDE solver.\n\n  Typically one doesn\'t need to call this function directly, unless they have\n  a custom time marching scheme. A simple stepper function for one-dimensional\n  PDEs can be found in `crank_nicolson.py`.\n\n  For a given solution (given by the `value_grid`) of a parabolic PDE at a given\n  `time` on a given `coord_grid` computes an approximate solution at the\n  `next_time` on the same coordinate grid. The parabolic differential equation\n  is of the form:\n\n  ```none\n   dV/dt + a * d2(A * V)/dx2 + b * d(B * V)/dx + c * V = 0\n  ```\n  Here `a`, `A`, `b`, `B`, and `c` are known coefficients which may depend on\n  `x` and `t`; `V = V(t, x)` is the solution to be found.\n\n  Args:\n    time: Real scalar `Tensor`. The time before the step.\n    next_time: Real scalar `Tensor`. The time after the step.\n    coord_grid: List of `n` rank 1 real `Tensor`s. `n` is the dimension of the\n      domain. The i-th `Tensor` has shape, `[d_i]` where `d_i` is the size of\n      the grid along axis `i`. The coordinates of the grid points. Corresponds\n      to the spatial grid `G` above.\n    value_grid: Real `Tensor` containing the function values at time\n      `time` which have to be evolved to time `next_time`. The shape of the\n      `Tensor` must broadcast with `B + [d_1, d_2, ..., d_n]`. `B` is the batch\n      dimensions (one or more), which allow multiple functions (with potentially\n      different boundary/final conditions and PDE coefficients) to be evolved\n      simultaneously.\n    boundary_conditions: The boundary conditions. Only rectangular boundary\n      conditions are supported. A list of tuples of size 1. The list element is\n      a tuple that consists of two callables representing the\n      boundary conditions at the minimum and maximum values of the spatial\n      variable indexed by the position in the list. `boundary_conditions[0][0]`\n      describes the boundary at `x_min`, and `boundary_conditions[0][1]` the\n      boundary at `x_max`. The boundary conditions are accepted in the form\n      `alpha(t) V + beta(t) V_n = gamma(t)`, where `V_n` is the derivative\n      with respect to the exterior normal to the boundary.\n      Each callable receives the current time `t` and the `coord_grid` at the\n      current time, and should return a tuple of `alpha`, `beta`, and `gamma`.\n      Each can be a number, a zero-rank `Tensor` or a `Tensor` of the batch\n      shape.\n      For example, for a grid of shape `(b, n)`, where `b` is the batch size,\n      `boundary_conditions[0][0]` should return a tuple of either numbers,\n      zero-rank tensors or tensors of shape `(b, n)`.\n      `alpha` and `beta` can also be `None` in case of Neumann and\n      Dirichlet conditions, respectively.\n    second_order_coeff_fn: Callable returning the second order coefficient\n      `a(t, r)` evaluated at given time `t`.\n      The callable accepts the following arguments:\n        `t`: The time at which the coefficient should be evaluated.\n        `locations_grid`: a `Tensor` representing a grid of locations `r` at\n          which the coefficient should be evaluated.\n      Returns an object `A` such that `A[0][0]` is defined and equals\n      `a(r, t)`. `A[0][0]` should be a Number, a `Tensor` broadcastable to the\n      shape of the grid represented by `locations_grid`, or `None` if\n      corresponding term is absent in the equation. Also, the callable itself\n      may be None, meaning there are no second-order derivatives in the\n      equation.\n    first_order_coeff_fn: Callable returning the first order coefficient\n      `b(t, r)` evaluated at given time `t`.\n      The callable accepts the following arguments:\n        `t`: The time at which the coefficient should be evaluated.\n        `locations_grid`: a `Tensor` representing a grid of locations `r` at\n          which the coefficient should be evaluated.\n      Returns a list or an 1D `Tensor`, `0`-th element of which represents\n      `b(t, r)`. This element should be a Number, a `Tensor` broadcastable\n       to the shape of the grid represented by `locations_grid`, or None if\n       corresponding term is absent in the equation. The callable itself may be\n       None, meaning there are no first-order derivatives in the equation.\n    zeroth_order_coeff_fn: Callable returning the zeroth order coefficient\n      `c(t, r)` evaluated at given time `t`.\n      The callable accepts the following arguments:\n        `t`: The time at which the coefficient should be evaluated.\n        `locations_grid`: a `Tensor` representing a grid of locations `r` at\n          which the coefficient should be evaluated.\n      Should return a Number or a `Tensor` broadcastable to the shape of\n      the grid represented by `locations_grid`. May also return None or be None\n      if the shift term is absent in the equation.\n    inner_second_order_coeff_fn: Callable returning the coefficients under the\n      second derivatives (i.e. `A(t, x)` above) at given time `t`. The\n      requirements are the same as for `second_order_coeff_fn`.\n    inner_first_order_coeff_fn: Callable returning the coefficients under the\n      first derivatives (i.e. `B(t, x)` above) at given time `t`. The\n      requirements are the same as for `first_order_coeff_fn`.\n    time_marching_scheme: A callable which represents the time marching scheme\n      for solving the PDE equation. If `u(t)` is space-discretized vector of the\n      solution of the PDE, this callable approximately solves the equation\n      `du/dt = A(t) u(t)` for `u(t1)` given `u(t2)`. Here `A` is a tridiagonal\n      matrix. The callable consumes the following arguments by keyword:\n        1. inner_value_grid: Grid of solution values at the current time of\n          the same `dtype` as `value_grid` and shape of `value_grid[..., 1:-1]`.\n        2. t1: Time before the step.\n        3. t2: Time after the step.\n        4. equation_params_fn: A callable that takes a scalar `Tensor` argument\n          representing time, and constructs the tridiagonal matrix `A`\n          (a tuple of three `Tensor`s, main, upper, and lower diagonals)\n          and the inhomogeneous term `b`. All of the `Tensor`s are of the same\n          `dtype` as `values_inner_value_gridgrid` and of the shape\n          broadcastable with the shape of `inner_value_grid`.\n      The callable should return a `Tensor` of the same shape and `dtype` a\n      `value_grid` and represents an approximate solution of the PDE after one\n      iteraton.\n    dtype: The dtype to use.\n    name: The name to give to the ops.\n      Default value: None which means `parabolic_equation_step` is used.\n\n  Returns:\n    A sequence of two `Tensor`s. The first one is a `Tensor` of the same\n    `dtype` and `shape` as `coord_grid` and represents a new coordinate grid\n    after one iteration. The second `Tensor` is of the same shape and `dtype`\n    as`value_grid` and represents an approximate solution of the equation after\n    one iteration.\n  """"""\n  with tf.compat.v1.name_scope(name, \'parabolic_equation_step\',\n                               [time, next_time, coord_grid, value_grid]):\n    time = tf.convert_to_tensor(time, dtype=dtype, name=\'time\')\n    next_time = tf.convert_to_tensor(next_time, dtype=dtype, name=\'next_time\')\n    coord_grid = [tf.convert_to_tensor(x, dtype=dtype,\n                                       name=\'coord_grid_axis_{}\'.format(ind))\n                  for ind, x in enumerate(coord_grid)]\n    value_grid = tf.convert_to_tensor(value_grid, dtype=dtype,\n                                      name=\'value_grid\')\n\n    inner_grid_in = value_grid[..., 1:-1]\n    coord_grid_deltas = coord_grid[0][1:] - coord_grid[0][:-1]\n\n    def equation_params_fn(t):\n      return _construct_space_discretized_eqn_params(\n          coord_grid, coord_grid_deltas, value_grid, boundary_conditions,\n          second_order_coeff_fn, first_order_coeff_fn, zeroth_order_coeff_fn,\n          inner_second_order_coeff_fn, inner_first_order_coeff_fn, t)\n\n    inner_grid_out = time_marching_scheme(\n        value_grid=inner_grid_in,\n        t1=time,\n        t2=next_time,\n        equation_params_fn=equation_params_fn)\n\n    updated_value_grid = _apply_boundary_conditions_after_step(\n        inner_grid_out, boundary_conditions,\n        coord_grid, coord_grid_deltas, next_time)\n    return coord_grid, updated_value_grid\n\n\ndef _construct_space_discretized_eqn_params(\n    coord_grid, coord_grid_deltas, value_grid, boundary_conditions,\n    second_order_coeff_fn, first_order_coeff_fn, zeroth_order_coeff_fn,\n    inner_second_order_coeff_fn, inner_first_order_coeff_fn, t):\n  """"""Constructs the tridiagonal matrix and the inhomogeneous term.""""""\n  # The space-discretized PDE has the form dv/dt = A(t) v(t) + b(t), where\n  # v(t) is V(t, x) discretized by x, A(t) is a tridiagonal matrix and b(t) is\n  # a vector. A(t) and b(t) depend on the PDE coefficients and the boundary\n  # conditions. This function constructs A(t) and b(t). See construction of\n  # A(t) e.g. in [Forsyth, Vetzal][1] (we denote `beta` and `gamma` from the\n  # paper as `dx_coef` and `dxdx_coef`).\n\n  # Get forward, backward and total differences.\n  forward_deltas = coord_grid_deltas[1:]\n  backward_deltas = coord_grid_deltas[:-1]\n  # Note that sum_deltas = 2 * central_deltas.\n  sum_deltas = forward_deltas + backward_deltas\n\n  # 3-diagonal matrix construction. See matrix `M` in [Forsyth, Vetzal][1].\n  #  The `tridiagonal` matrix is of shape\n  # `[value_dim, 3, num_grid_points]`.\n\n  # Get the PDE coefficients and broadcast them to the shape of value grid.\n  second_order_coeff_fn = second_order_coeff_fn or (lambda *args: [[None]])\n  first_order_coeff_fn = first_order_coeff_fn or (lambda *args: [None])\n  zeroth_order_coeff_fn = zeroth_order_coeff_fn or (lambda *args: None)\n  inner_second_order_coeff_fn = inner_second_order_coeff_fn or (\n      lambda *args: [[None]])\n  inner_first_order_coeff_fn = inner_first_order_coeff_fn or (\n      lambda *args: [None])\n\n  second_order_coeff = _prepare_pde_coeffs(\n      second_order_coeff_fn(t, coord_grid)[0][0], value_grid)\n  first_order_coeff = _prepare_pde_coeffs(\n      first_order_coeff_fn(t, coord_grid)[0], value_grid)\n  zeroth_order_coeff = _prepare_pde_coeffs(\n      zeroth_order_coeff_fn(t, coord_grid), value_grid)\n  inner_second_order_coeff = _prepare_pde_coeffs(\n      inner_second_order_coeff_fn(t, coord_grid)[0][0], value_grid)\n  inner_first_order_coeff = _prepare_pde_coeffs(\n      inner_first_order_coeff_fn(t, coord_grid)[0], value_grid)\n\n  zeros = tf.zeros_like(value_grid[..., 1:-1])\n\n  # Discretize zeroth-order term.\n  if zeroth_order_coeff is None:\n    diag_zeroth_order = zeros\n  else:\n    # Minus is due to moving to rhs.\n    diag_zeroth_order = -zeroth_order_coeff[..., 1:-1]\n\n  # Discretize first-order term.\n  if first_order_coeff is None and inner_first_order_coeff is None:\n    # No first-order term.\n    superdiag_first_order = zeros\n    diag_first_order = zeros\n    subdiag_first_order = zeros\n  else:\n    superdiag_first_order = -backward_deltas / (sum_deltas * forward_deltas)\n    subdiag_first_order = forward_deltas / (sum_deltas * backward_deltas)\n    diag_first_order = -superdiag_first_order - subdiag_first_order\n    if first_order_coeff is not None:\n      superdiag_first_order *= first_order_coeff[..., 1:-1]\n      subdiag_first_order *= first_order_coeff[..., 1:-1]\n      diag_first_order *= first_order_coeff[..., 1:-1]\n    if inner_first_order_coeff is not None:\n      superdiag_first_order *= inner_first_order_coeff[..., 2:]\n      subdiag_first_order *= inner_first_order_coeff[..., :-2]\n      diag_first_order *= inner_first_order_coeff[..., 1:-1]\n\n  # Discretize second-order term.\n  if second_order_coeff is None and inner_second_order_coeff is None:\n    # No second-order term.\n    superdiag_second_order = zeros\n    diag_second_order = zeros\n    subdiag_second_order = zeros\n  else:\n    superdiag_second_order = -2 / (sum_deltas * forward_deltas)\n    subdiag_second_order = -2 / (sum_deltas * backward_deltas)\n    diag_second_order = -superdiag_second_order - subdiag_second_order\n    if second_order_coeff is not None:\n      superdiag_second_order *= second_order_coeff[..., 1:-1]\n      subdiag_second_order *= second_order_coeff[..., 1:-1]\n      diag_second_order *= second_order_coeff[..., 1:-1]\n    if inner_second_order_coeff is not None:\n      superdiag_second_order *= inner_second_order_coeff[..., 2:]\n      subdiag_second_order *= inner_second_order_coeff[..., :-2]\n      diag_second_order *= inner_second_order_coeff[..., 1:-1]\n\n  superdiag = superdiag_first_order + superdiag_second_order\n  subdiag = subdiag_first_order + subdiag_second_order\n  diag = diag_zeroth_order + diag_first_order + diag_second_order\n\n  return _apply_boundary_conditions_to_discretized_equation(\n      value_grid, boundary_conditions, coord_grid, coord_grid_deltas, diag,\n      superdiag, subdiag, t)\n\n\ndef _apply_boundary_conditions_to_discretized_equation(\n    value_grid, boundary_conditions,\n    coord_grid, coord_grid_deltas, diagonal, upper_diagonal, lower_diagonal, t):\n  """"""Updates space-discretized equation according to boundary conditions.""""""\n  # Without taking into account the boundary conditions, the space-discretized\n  # PDE has the form dv/dt = A(t) v(t), where v(t) is V(t, x) discretized by\n  # x, and A is the tridiagonal matrix defined by coefficients of the PDE.\n  # Boundary conditions change the first and the last row of A and introduce\n  # the inhomogeneous term, so the equation becomes dv/dt = A\'(t) v(t) + b(t),\n  # where A\' is the modified matrix, and b is a vector.\n  # This function receives A and returns A\' and b.\n\n  # Retrieve the boundary conditions in the form alpha V + beta V\' = gamma.\n  alpha_l, beta_l, gamma_l = boundary_conditions[0][0](t, coord_grid)\n  alpha_u, beta_u, gamma_u = boundary_conditions[0][1](t, coord_grid)\n\n  alpha_l, beta_l, gamma_l, alpha_u, beta_u, gamma_u = (\n      _prepare_boundary_conditions(b, value_grid)\n      for b in (alpha_l, beta_l, gamma_l, alpha_u, beta_u, gamma_u))\n\n  if beta_l is None and beta_u is None:\n    # Dirichlet conditions on both boundaries. In this case there are no\n    # corrections to the tridiagonal matrix, so we can take a shortcut.\n    first_inhomog_element = lower_diagonal[..., 0] * gamma_l / alpha_l\n    last_inhomog_element = upper_diagonal[..., -1] * gamma_u / alpha_u\n    inhomog_term = _append_first_and_last(first_inhomog_element,\n                                          tf.zeros_like(diagonal[..., 1:-1]),\n                                          last_inhomog_element)\n    return (diagonal, upper_diagonal, lower_diagonal), inhomog_term\n\n  # Convert the boundary conditions into the form v0 = xi1 v1 + xi2 v2 + eta,\n  # and calculate corrections to the tridiagonal matrix and the inhomogeneous\n  # term.\n  xi1, xi2, eta = _discretize_boundary_conditions(coord_grid_deltas[0],\n                                                  coord_grid_deltas[1],\n                                                  alpha_l,\n                                                  beta_l, gamma_l)\n  diag_first_correction = lower_diagonal[..., 0] * xi1\n  upper_diag_correction = lower_diagonal[..., 0] * xi2\n  first_inhomog_element = lower_diagonal[..., 0] * eta\n  xi1, xi2, eta = _discretize_boundary_conditions(coord_grid_deltas[-1],\n                                                  coord_grid_deltas[-2],\n                                                  alpha_u,\n                                                  beta_u, gamma_u)\n  diag_last_correction = upper_diagonal[..., -1] * xi1\n  lower_diag_correction = upper_diagonal[..., -1] * xi2\n  last_inhomog_element = upper_diagonal[..., -1] * eta\n  diagonal = _append_first_and_last(diagonal[..., 0] + diag_first_correction,\n                                    diagonal[..., 1:-1],\n                                    diagonal[..., -1] + diag_last_correction)\n  upper_diagonal = _append_first(\n      upper_diagonal[..., 0] + upper_diag_correction, upper_diagonal[..., 1:])\n  lower_diagonal = _append_last(\n      lower_diagonal[..., :-1],\n      lower_diagonal[..., -1] + lower_diag_correction)\n  inhomog_term = _append_first_and_last(first_inhomog_element,\n                                        tf.zeros_like(diagonal[..., 1:-1]),\n                                        last_inhomog_element)\n  return (diagonal, upper_diagonal, lower_diagonal), inhomog_term\n\n\ndef _apply_boundary_conditions_after_step(\n    inner_grid_out, boundary_conditions,\n    coord_grid, coord_grid_deltas, time_after_step):\n  """"""Calculates and appends boundary values after making a step.""""""\n  # After we\'ve updated the values in the inner part of the grid according to\n  # the PDE, we append the boundary values calculated using the boundary\n  # conditions.\n  # This is done using the discretized form of the boundary conditions,\n  # v0 = xi1 v1 + xi2 v2 + eta.\n\n  alpha, beta, gamma = boundary_conditions[0][0](time_after_step,\n                                                 coord_grid)\n  alpha, beta, gamma = (\n      _prepare_boundary_conditions(b, inner_grid_out)\n      for b in (alpha, beta, gamma))\n  xi1, xi2, eta = _discretize_boundary_conditions(coord_grid_deltas[0],\n                                                  coord_grid_deltas[1],\n                                                  alpha, beta, gamma)\n  first_value = (\n      xi1 * inner_grid_out[..., 0] + xi2 * inner_grid_out[..., 1] + eta)\n  alpha, beta, gamma = boundary_conditions[0][1](time_after_step,\n                                                 coord_grid)\n  alpha, beta, gamma = (\n      _prepare_boundary_conditions(b, inner_grid_out)\n      for b in (alpha, beta, gamma))\n  xi1, xi2, eta = _discretize_boundary_conditions(coord_grid_deltas[-1],\n                                                  coord_grid_deltas[-2],\n                                                  alpha, beta, gamma)\n  last_value = (\n      xi1 * inner_grid_out[..., -1] + xi2 * inner_grid_out[..., -2] + eta)\n  return _append_first_and_last(first_value, inner_grid_out, last_value)\n\n\ndef _prepare_pde_coeffs(raw_coeffs, value_grid):\n  """"""Prepares values received from second_order_coeff_fn and similar.""""""\n  if raw_coeffs is None:\n    return None\n  dtype = value_grid.dtype\n  coeffs = tf.convert_to_tensor(raw_coeffs, dtype=dtype)\n\n  broadcast_shape = tf.shape(value_grid)\n  coeffs = tf.broadcast_to(coeffs, broadcast_shape)\n  return coeffs\n\n\ndef _prepare_boundary_conditions(boundary_tensor, value_grid):\n  """"""Prepares values received from boundary_condition callables.""""""\n  if boundary_tensor is None:\n    return None\n  boundary_tensor = tf.convert_to_tensor(boundary_tensor, value_grid.dtype)\n  # Broadcast to batch dimensions.\n  broadcast_shape = tf.shape(value_grid)[:-1]\n  return tf.broadcast_to(boundary_tensor, broadcast_shape)\n\n\ndef _discretize_boundary_conditions(dx0, dx1, alpha, beta, gamma):\n  """"""Discretizes boundary conditions.""""""\n  # Converts a boundary condition given as alpha V + beta V_n = gamma,\n  # where V_n is the derivative w.r.t. the normal to the boundary into\n  # v0 = xi1 v1 + xi2 v2 + eta,\n  # where v0 is the value on the boundary point of the grid, v1 and v2 - values\n  # on the next two points on the grid.\n  # The expressions are exactly the same for both boundaries.\n\n  if beta is None:\n    # Dirichlet condition.\n    if alpha is None:\n      raise ValueError(\n          ""Invalid boundary conditions: alpha and beta can\'t both be None."")\n    zeros = tf.zeros_like(gamma)\n    return zeros, zeros, gamma / alpha\n\n  denom = beta * dx1 * (2 * dx0 + dx1)\n  if alpha is not None:\n    denom += alpha * dx0 * dx1 * (dx0 + dx1)\n  xi1 = beta * (dx0 + dx1) * (dx0 + dx1) / denom\n  xi2 = -beta * dx0 * dx0 / denom\n  eta = gamma * dx0 * dx1 * (dx0 + dx1) / denom\n  return xi1, xi2, eta\n\n\ndef _append_first_and_last(first, inner, last):\n  return tf.concat((tf.expand_dims(first, -1), inner, tf.expand_dims(last, -1)),\n                   -1)\n\n\ndef _append_first(first, rest):\n  return tf.concat((tf.expand_dims(first, -1), rest), -1)\n\n\ndef _append_last(rest, last):\n  return tf.concat((rest, tf.expand_dims(last, -1)), -1)\n\n\n__all__ = [\'parabolic_equation_step\']\n'"
tf_quant_finance/math/pde/steppers/parabolic_equation_stepper_test.py,56,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for 1-D parabolic PDE solvers.""""""\n\nimport math\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\nfd_solvers = tff.math.pde.fd_solvers\ndirichlet = tff.math.pde.boundary_conditions.dirichlet\nneumann = tff.math.pde.boundary_conditions.neumann\ngrids = tff.math.pde.grids\ncrank_nicolson_step = tff.math.pde.steppers.crank_nicolson.crank_nicolson_step\nexplicit_step = tff.math.pde.steppers.explicit.explicit_step\nextrapolation_step = tff.math.pde.steppers.extrapolation.extrapolation_step\nimplicit_step = tff.math.pde.steppers.implicit.implicit_step\ncrank_nicolson_with_oscillation_damping_step = tff.math.pde.steppers.oscillation_damped_crank_nicolson.oscillation_damped_crank_nicolson_step\nweighted_implicit_explicit_step = tff.math.pde.steppers.weighted_implicit_explicit.weighted_implicit_explicit_step\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass ParabolicEquationStepperTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'Implicit\',\n          \'one_step_fn\': implicit_step(),\n          \'time_step\': 0.001\n      }, {\n          \'testcase_name\': \'Explicit\',\n          \'one_step_fn\': explicit_step(),\n          \'time_step\': 0.001\n      }, {\n          \'testcase_name\': \'Weighted\',\n          \'one_step_fn\': weighted_implicit_explicit_step(theta=0.3),\n          \'time_step\': 0.001\n      }, {\n          \'testcase_name\': \'CrankNicolson\',\n          \'one_step_fn\': crank_nicolson_step(),\n          \'time_step\': 0.01\n      }, {\n          \'testcase_name\': \'Extrapolation\',\n          \'one_step_fn\': extrapolation_step(),\n          \'time_step\': 0.01\n      }, {\n          \'testcase_name\': \'CrankNicolsonWithOscillationDamping\',\n          \'one_step_fn\': crank_nicolson_with_oscillation_damping_step(),\n          \'time_step\': 0.01\n      })\n  def testHeatEquationWithVariousSchemes(self, one_step_fn, time_step):\n    """"""Test solving heat equation with various time marching schemes.\n\n    Tests solving heat equation with the boundary conditions\n    `u(x, t=1) = e * sin(x)`, `u(-2 pi n - pi / 2, t) = -e^t`, and\n    `u(2 pi n + pi / 2, t) = -e^t` with some integer `n` for `u(x, t=0)`.\n\n    The exact solution is `u(x, t=0) = sin(x)`.\n\n    All time marching schemes should yield reasonable results given small enough\n    time steps. First-order accurate schemes (explicit, implicit, weighted with\n    theta != 0.5) require smaller time step than second-order accurate ones\n    (Crank-Nicolson, Extrapolation).\n\n    Args:\n      one_step_fn: one_step_fn representing a time marching scheme to use.\n      time_step: time step for given scheme.\n    """"""\n    def final_cond_fn(x):\n      return math.e * math.sin(x)\n\n    def expected_result_fn(x):\n      return tf.sin(x)\n\n    @dirichlet\n    def lower_boundary_fn(t, x):\n      del x\n      return -tf.exp(t)\n\n    @dirichlet\n    def upper_boundary_fn(t, x):\n      del x\n      return tf.exp(t)\n\n    grid = grids.uniform_grid(\n        minimums=[-10.5 * math.pi],\n        maximums=[10.5 * math.pi],\n        sizes=[1000],\n        dtype=np.float32)\n    self._testHeatEquation(\n        grid=grid,\n        final_t=1,\n        time_step=time_step,\n        final_cond_fn=final_cond_fn,\n        expected_result_fn=expected_result_fn,\n        one_step_fn=one_step_fn,\n        lower_boundary_fn=lower_boundary_fn,\n        upper_boundary_fn=upper_boundary_fn,\n        error_tolerance=1e-3)\n\n  def testHeatEquation_WithNeumannBoundaryConditions(self):\n    """"""Test for Neumann boundary conditions.\n\n    Tests solving heat equation with the following boundary conditions:\n    `u(x, t=1) = e * sin(x)`, `u_x(0, t) = e^t`, and\n    `u_x(2 pi n + pi/2, t) = 0`, where `n` is some integer.\n\n    The exact solution `u(x, t=0) = e^t sin(x)`.\n    """"""\n\n    def final_cond_fn(x):\n      return math.e * math.sin(x)\n\n    def expected_result_fn(x):\n      return tf.sin(x)\n\n    @neumann\n    def lower_boundary_fn(t, x):\n      del x\n      return -tf.exp(t)\n\n    @neumann\n    def upper_boundary_fn(t, x):\n      del t, x\n      return 0\n\n    grid = grids.uniform_grid(\n        minimums=[0.0], maximums=[10.5 * math.pi], sizes=[1000],\n        dtype=np.float32)\n    self._testHeatEquation(\n        grid,\n        final_t=1,\n        time_step=0.01,\n        final_cond_fn=final_cond_fn,\n        expected_result_fn=expected_result_fn,\n        one_step_fn=crank_nicolson_step(),\n        lower_boundary_fn=lower_boundary_fn,\n        upper_boundary_fn=upper_boundary_fn,\n        error_tolerance=1e-3)\n\n  def testHeatEquation_WithMixedBoundaryConditions(self):\n    """"""Test for mixed boundary conditions.\n\n    Tests solving heat equation with the following boundary conditions:\n    `u(x, t=1) = e * sin(x)`, `u_x(0, t) = e^t`, and\n    `u(2 pi n + pi/2, t) = e^t`, where `n` is some integer.\n\n    The exact solution `u(x, t=0) = e^t sin(x)`.\n    """"""\n\n    def final_cond_fn(x):\n      return math.e * math.sin(x)\n\n    def expected_result_fn(x):\n      return tf.sin(x)\n\n    @neumann\n    def lower_boundary_fn(t, x):\n      del x\n      return -tf.exp(t)\n\n    @dirichlet\n    def upper_boundary_fn(t, x):\n      del x\n      return tf.exp(t)\n\n    grid = grids.uniform_grid(minimums=[0], maximums=[10.5 * math.pi],\n                              sizes=[1000], dtype=np.float32)\n    self._testHeatEquation(\n        grid,\n        final_t=1,\n        time_step=0.01,\n        final_cond_fn=final_cond_fn,\n        expected_result_fn=expected_result_fn,\n        one_step_fn=crank_nicolson_step(),\n        lower_boundary_fn=lower_boundary_fn,\n        upper_boundary_fn=upper_boundary_fn,\n        error_tolerance=1e-3)\n\n  def testHeatEquation_WithRobinBoundaryConditions(self):\n    """"""Test for Robin boundary conditions.\n\n    Tests solving heat equation with the following boundary conditions:\n    `u(x, t=1) = e * sin(x)`, `u_x(0, t) + 2u(0, t) = e^t`, and\n    `2u(x_max, t) + u_x(x_max, t) = 2*e^t`, where `x_max = 2 pi n + pi/2` with\n    some integer `n`.\n\n    The exact solution `u(x, t=0) = e^t sin(x)`.\n    """"""\n\n    def final_cond_fn(x):\n      return math.e * math.sin(x)\n\n    def expected_result_fn(x):\n      return tf.sin(x)\n\n    def lower_boundary_fn(t, x):\n      del x\n      return 2, -1, tf.exp(t)\n\n    def upper_boundary_fn(t, x):\n      del x\n      return 2, 1, 2 * tf.exp(t)\n\n    grid = grids.uniform_grid(minimums=[0], maximums=[4.5 * math.pi],\n                              sizes=[1000], dtype=np.float64)\n    self._testHeatEquation(\n        grid,\n        final_t=1,\n        time_step=0.01,\n        final_cond_fn=final_cond_fn,\n        expected_result_fn=expected_result_fn,\n        one_step_fn=crank_nicolson_step(),\n        lower_boundary_fn=lower_boundary_fn,\n        upper_boundary_fn=upper_boundary_fn,\n        error_tolerance=1e-2)\n\n  def testHeatEquation_WithRobinBoundaryConditions_AndLogUniformGrid(self):\n    """"""Same as previous, but with log-uniform grid.""""""\n\n    def final_cond_fn(x):\n      return math.e * math.sin(x)\n\n    def expected_result_fn(x):\n      return tf.sin(x)\n\n    def lower_boundary_fn(t, x):\n      del x\n      return 2, -1, tf.exp(t)\n\n    def upper_boundary_fn(t, x):\n      del x\n      return 2, 1, 2 * tf.exp(t)\n\n    grid = grids.log_uniform_grid(\n        minimums=[2 * math.pi],\n        maximums=[4.5 * math.pi],\n        sizes=[1000],\n        dtype=np.float64)\n    self._testHeatEquation(\n        grid=grid,\n        final_t=1,\n        time_step=0.01,\n        final_cond_fn=final_cond_fn,\n        expected_result_fn=expected_result_fn,\n        one_step_fn=crank_nicolson_step(),\n        lower_boundary_fn=lower_boundary_fn,\n        upper_boundary_fn=upper_boundary_fn,\n        error_tolerance=1e-2)\n\n  def testHeatEquation_WithRobinBoundaryConditions_AndExtraPointInGrid(self):\n    """"""Same as previous, but with grid with an extra point.\n\n    We add an extra point in a uniform grid so that grid[1]-grid[0] and\n    grid[2]-grid[1] are significantly different. This is important for testing\n    the discretization of boundary conditions: both deltas participate there.\n    """"""\n\n    def final_cond_fn(x):\n      return math.e * math.sin(x)\n\n    def expected_result_fn(x):\n      return tf.sin(x)\n\n    def lower_boundary_fn(t, x):\n      del x\n      return 2, -1, tf.exp(t)\n\n    def upper_boundary_fn(t, x):\n      del x\n      return 2, 1, 2 * tf.exp(t)\n\n    x_min = 0\n    x_max = 4.5 * math.pi\n    num_points = 1001\n    locations = np.linspace(x_min, x_max, num=num_points - 1)\n    delta = locations[1] - locations[0]\n    locations = np.insert(locations, 1, locations[0] + delta / 3)\n\n    grid = [tf.constant(locations)]\n\n    self._testHeatEquation(\n        grid=grid,\n        final_t=1,\n        time_step=0.01,\n        final_cond_fn=final_cond_fn,\n        expected_result_fn=expected_result_fn,\n        one_step_fn=crank_nicolson_step(),\n        lower_boundary_fn=lower_boundary_fn,\n        upper_boundary_fn=upper_boundary_fn,\n        error_tolerance=1e-2)\n\n  def testCrankNicolsonOscillationDamping(self):\n    """"""Tests the Crank-Nicolson oscillation damping.\n\n    Oscillations arise in Crank-Nicolson scheme when the initial (or final)\n    conditions have discontinuities. We use Heaviside step function as initial\n    conditions. The exact solution of the heat equation with unbounded x is\n    ```None\n    u(x, t) = (1 + erf(x/2sqrt(t))/2\n    ```\n    We take large enough x_min, x_max to be able to use this as a reference\n    solution.\n\n    CrankNicolsonWithOscillationDamping produces much smaller error than\n    the usual crank_nicolson_scheme.\n    """"""\n\n    final_t = 1\n    x_min = -10\n    x_max = 10\n    dtype = np.float32\n\n    def final_cond_fn(x):\n      return 0.0 if x < 0 else 1.0\n\n    def expected_result_fn(x):\n      return 1 / 2 + tf.math.erf(x / (2 * tf.sqrt(dtype(final_t)))) / 2\n\n    @dirichlet\n    def lower_boundary_fn(t, x):\n      del t, x\n      return 0\n\n    @dirichlet\n    def upper_boundary_fn(t, x):\n      del t, x\n      return 1\n\n    grid = grids.uniform_grid(\n        minimums=[x_min], maximums=[x_max], sizes=[1000], dtype=dtype)\n\n    self._testHeatEquation(\n        grid=grid,\n        final_t=final_t,\n        time_step=0.01,\n        final_cond_fn=final_cond_fn,\n        expected_result_fn=expected_result_fn,\n        one_step_fn=crank_nicolson_with_oscillation_damping_step(),\n        lower_boundary_fn=lower_boundary_fn,\n        upper_boundary_fn=upper_boundary_fn,\n        error_tolerance=1e-3)\n\n  def _testHeatEquation(self,\n                        grid,\n                        final_t,\n                        time_step,\n                        final_cond_fn,\n                        expected_result_fn,\n                        one_step_fn,\n                        lower_boundary_fn,\n                        upper_boundary_fn,\n                        error_tolerance=1e-3):\n    """"""Helper function with details of testing heat equation solving.""""""\n    # Define coefficients for a PDE V_{t} + V_{XX} = 0.\n    def second_order_coeff_fn(t, x):\n      del t, x\n      return [[1]]\n\n    xs = self.evaluate(grid)[0]\n    final_values = tf.constant([final_cond_fn(x) for x in xs],\n                               dtype=grid[0].dtype)\n\n    result = fd_solvers.solve_backward(\n        start_time=final_t,\n        end_time=0,\n        coord_grid=grid,\n        values_grid=final_values,\n        num_steps=None,\n        start_step_count=0,\n        time_step=time_step,\n        one_step_fn=one_step_fn,\n        boundary_conditions=[(lower_boundary_fn, upper_boundary_fn)],\n        values_transform_fn=None,\n        second_order_coeff_fn=second_order_coeff_fn,\n        dtype=grid[0].dtype)\n\n    actual = self.evaluate(result[0])\n    expected = self.evaluate(expected_result_fn(xs))\n    self.assertLess(np.max(np.abs(actual - expected)), error_tolerance)\n\n  def testDocStringExample(self):\n    """"""Tests that the European Call option price is computed correctly.""""""\n    num_equations = 2  # Number of PDE\n    num_grid_points = 1024  # Number of grid points\n    dtype = np.float64\n    # Build a log-uniform grid\n    s_max = 300.\n    grid = grids.log_uniform_grid(minimums=[0.01], maximums=[s_max],\n                                  sizes=[num_grid_points],\n                                  dtype=dtype)\n    # Specify volatilities and interest rates for the options\n    volatility = np.array([0.3, 0.15], dtype=dtype).reshape([-1, 1])\n    rate = np.array([0.01, 0.03], dtype=dtype).reshape([-1, 1])\n    expiry = 1.0\n    strike = np.array([50, 100], dtype=dtype).reshape([-1, 1])\n\n    def second_order_coeff_fn(t, location_grid):\n      del t\n      return [[tf.square(volatility) * tf.square(location_grid[0]) / 2]]\n\n    def first_order_coeff_fn(t, location_grid):\n      del t\n      return [rate * location_grid[0]]\n\n    def zeroth_order_coeff_fn(t, location_grid):\n      del t, location_grid\n      return -rate\n\n    @dirichlet\n    def lower_boundary_fn(t, location_grid):\n      del t, location_grid\n      return 0\n\n    @dirichlet\n    def upper_boundary_fn(t, location_grid):\n      return tf.squeeze(location_grid[0][-1]\n                        - strike * tf.exp(-rate * (expiry - t)))\n\n    final_values = tf.nn.relu(grid[0] - strike)\n    # Broadcast to the shape of value dimension, if necessary.\n    final_values += tf.zeros([num_equations, num_grid_points],\n                             dtype=dtype)\n    # Estimate European call option price\n    estimate = fd_solvers.solve_backward(\n        start_time=expiry,\n        end_time=0,\n        coord_grid=grid,\n        values_grid=final_values,\n        num_steps=None,\n        start_step_count=0,\n        time_step=tf.constant(0.01, dtype=dtype),\n        one_step_fn=crank_nicolson_step(),\n        boundary_conditions=[(lower_boundary_fn, upper_boundary_fn)],\n        values_transform_fn=None,\n        second_order_coeff_fn=second_order_coeff_fn,\n        first_order_coeff_fn=first_order_coeff_fn,\n        zeroth_order_coeff_fn=zeroth_order_coeff_fn,\n        dtype=dtype)[0]\n    estimate = self.evaluate(estimate)\n    # Extract estimates for some of the grid locations and compare to the\n    # true option price\n    value_grid_first_option = estimate[0, :]\n    value_grid_second_option = estimate[1, :]\n    # Get two grid locations (correspond to spot 51.9537332 and 106.25407758,\n    # respectively).\n    loc_1 = 849\n    loc_2 = 920\n    # True call option price (obtained using black_scholes_price function)\n    call_price = [7.35192484, 11.75642136]\n\n    self.assertAllClose(\n        call_price, [value_grid_first_option[loc_1],\n                     value_grid_second_option[loc_2]],\n        rtol=1e-03, atol=1e-03)\n\n  def testEuropeanCallDynamicVol(self):\n    """"""Price for the European Call option with time-dependent volatility.""""""\n    num_equations = 1  # Number of PDE\n    num_grid_points = 1024  # Number of grid points\n    dtype = np.float64\n    # Build a log-uniform grid\n    s_max = 300.\n    grid = grids.log_uniform_grid(minimums=[0.01], maximums=[s_max],\n                                  sizes=[num_grid_points],\n                                  dtype=dtype)\n    # Specify volatilities and interest rates for the options\n    expiry = 1.0\n    strike = 50.0\n\n    # Volatility is of the form  `sigma**2(t) = 1 / 6 + 1 / 2 * t**2`.\n    def second_order_coeff_fn(t, location_grid):\n      return [[(1. / 6 + t**2 / 2) * tf.square(location_grid[0]) / 2]]\n\n    @dirichlet\n    def lower_boundary_fn(t, location_grid):\n      del t, location_grid\n      return 0\n\n    @dirichlet\n    def upper_boundary_fn(t, location_grid):\n      del t\n      return location_grid[0][-1] - strike\n\n    final_values = tf.nn.relu(grid[0] - strike)\n    # Broadcast to the shape of value dimension, if necessary.\n    final_values += tf.zeros([num_equations, num_grid_points],\n                             dtype=dtype)\n    # Estimate European call option price\n    estimate = fd_solvers.solve_backward(\n        start_time=expiry,\n        end_time=0,\n        coord_grid=grid,\n        values_grid=final_values,\n        num_steps=None,\n        start_step_count=0,\n        time_step=tf.constant(0.01, dtype=dtype),\n        one_step_fn=crank_nicolson_step(),\n        boundary_conditions=[(lower_boundary_fn, upper_boundary_fn)],\n        values_transform_fn=None,\n        second_order_coeff_fn=second_order_coeff_fn,\n        dtype=dtype)[0]\n\n    value_grid = self.evaluate(estimate)[0, :]\n    # Get two grid locations (correspond to spot 51.9537332 and 106.25407758,\n    # respectively).\n    loc_1 = 849\n    # True call option price (obtained using black_scholes_price function)\n    call_price = 12.582092\n    self.assertAllClose(call_price, value_grid[loc_1], rtol=1e-02, atol=1e-02)\n\n  def testHeatEquation_InForwardDirection(self):\n    """"""Test solving heat equation with various time marching schemes.\n\n    Tests solving heat equation with the boundary conditions\n    `u(x, t=1) = e * sin(x)`, `u(-2 pi n - pi / 2, t) = -e^t`, and\n    `u(2 pi n + pi / 2, t) = -e^t` with some integer `n` for `u(x, t=0)`.\n\n    The exact solution is `u(x, t=0) = sin(x)`.\n\n    All time marching schemes should yield reasonable results given small enough\n    time steps. First-order accurate schemes (explicit, implicit, weighted with\n    theta != 0.5) require smaller time step than second-order accurate ones\n    (Crank-Nicolson, Extrapolation).\n    """"""\n    final_time = 1.0\n\n    def initial_cond_fn(x):\n      return tf.sin(x)\n\n    def expected_result_fn(x):\n      return np.exp(-final_time) * tf.sin(x)\n\n    @dirichlet\n    def lower_boundary_fn(t, x):\n      del x\n      return -tf.exp(-t)\n\n    @dirichlet\n    def upper_boundary_fn(t, x):\n      del x\n      return tf.exp(-t)\n\n    grid = grids.uniform_grid(\n        minimums=[-10.5 * math.pi],\n        maximums=[10.5 * math.pi],\n        sizes=[1000],\n        dtype=np.float32)\n\n    def second_order_coeff_fn(t, x):\n      del t, x\n      return [[-1]]\n\n    final_values = initial_cond_fn(grid[0])\n\n    result = fd_solvers.solve_forward(\n        start_time=0.0,\n        end_time=final_time,\n        coord_grid=grid,\n        values_grid=final_values,\n        time_step=0.01,\n        boundary_conditions=[(lower_boundary_fn, upper_boundary_fn)],\n        second_order_coeff_fn=second_order_coeff_fn)[0]\n\n    actual = self.evaluate(result)\n    expected = self.evaluate(expected_result_fn(grid[0]))\n    self.assertLess(np.max(np.abs(actual - expected)), 1e-3)\n\n  def testReferenceEquation(self):\n    """"""Tests the equation used as reference for a few further tests.\n\n    We solve the diffusion equation `u_t = u_xx` on x = [0...1] with boundary\n    conditions `u(x<=1/2, t=0) = x`, `u(x>1/2, t=0) = 1 - x`,\n    `u(x=0, t) = u(x=1, t) = 0`.\n\n    The exact solution of the diffusion equation with zero-Dirichlet boundaries\n    is:\n    `u(x, t) = sum_{n=1..inf} b_n sin(pi n x) exp(-n^2 pi^2 t)`,\n    `b_n = 2 integral_{0..1} sin(pi n x) u(x, t=0) dx.`\n\n    The initial conditions are taken so that the integral easily calculates, and\n    the sum can be approximated by a few first terms (given large enough `t`).\n    See the result in _reference_heat_equation_solution.\n\n    Using this solution helps to simplify the tests, as we don\'t have to\n    maintain complicated boundary conditions in each test or tweak the\n    parameters to keep the ""support"" of the function far from boundaries.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[0], maximums=[1], sizes=[501], dtype=tf.float32)\n    xs = grid[0]\n\n    final_t = 0.1\n    time_step = 0.001\n\n    def second_order_coeff_fn(t, coord_grid):\n      del t, coord_grid\n      return [[-1]]\n\n    initial = _reference_pde_initial_cond(xs)\n    expected = _reference_pde_solution(xs, final_t)\n    actual = fd_solvers.solve_forward(\n        start_time=0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=initial,\n        time_step=time_step,\n        second_order_coeff_fn=second_order_coeff_fn)[0]\n\n    self.assertAllClose(expected, actual, atol=1e-3, rtol=1e-3)\n\n  def testReference_WithExponentMultiplier(self):\n    """"""Tests solving diffusion equation with an exponent multiplier.\n\n    Take the heat equation `v_{t} - v_{xx} = 0` and substitute `v = exp(x) u`.\n    This yields `u_{t} - u_{xx} - 2u_{x} - u = 0`. The test compares numerical\n    solution of this equation to the exact one, which is the diffusion equation\n    solution times `exp(-x)`.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[0], maximums=[1], sizes=[501], dtype=tf.float32)\n    xs = grid[0]\n\n    final_t = 0.1\n    time_step = 0.001\n\n    def second_order_coeff_fn(t, coord_grid):\n      del t, coord_grid\n      return [[-1]]\n\n    def first_order_coeff_fn(t, coord_grid):\n      del t, coord_grid\n      return [-2]\n\n    def zeroth_order_coeff_fn(t, coord_grid):\n      del t, coord_grid\n      return -1\n\n    initial = tf.exp(-xs) * _reference_pde_initial_cond(xs)\n    expected = tf.exp(-xs) * _reference_pde_solution(xs, final_t)\n\n    actual = fd_solvers.solve_forward(\n        start_time=0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=initial,\n        time_step=time_step,\n        second_order_coeff_fn=second_order_coeff_fn,\n        first_order_coeff_fn=first_order_coeff_fn,\n        zeroth_order_coeff_fn=zeroth_order_coeff_fn)[0]\n\n    self.assertAllClose(expected, actual, atol=1e-3, rtol=1e-3)\n\n  def testInnerSecondOrderCoeff(self):\n    """"""Tests handling inner_second_order_coeff.\n\n    As in previous test, take the diffusion equation `v_{t} - v_{xx} = 0` and\n    substitute `v = exp(x) u`, but this time keep exponent under the derivative:\n    `u_{t} - exp(-x)[exp(x)u]_{xx} = 0`. Expect the same solution as in\n    previous test.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[0], maximums=[1], sizes=[501], dtype=tf.float32)\n    xs = grid[0]\n\n    final_t = 0.1\n    time_step = 0.001\n\n    def second_order_coeff_fn(t, coord_grid):\n      del t\n      x = coord_grid[0]\n      return [[-tf.exp(-x)]]\n\n    def inner_second_order_coeff_fn(t, coord_grid):\n      del t\n      x = coord_grid[0]\n      return [[tf.exp(x)]]\n\n    initial = tf.exp(-xs) * _reference_pde_initial_cond(xs)\n    expected = tf.exp(-xs) * _reference_pde_solution(xs, final_t)\n\n    actual = fd_solvers.solve_forward(\n        start_time=0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=initial,\n        time_step=time_step,\n        second_order_coeff_fn=second_order_coeff_fn,\n        inner_second_order_coeff_fn=inner_second_order_coeff_fn)[0]\n\n    self.assertAllClose(expected, actual, atol=1e-3, rtol=1e-3)\n\n  def testInnerFirstAndSecondOrderCoeff(self):\n    """"""Tests handling both inner_first_order_coeff and inner_second_order_coeff.\n\n    We saw previously that the solution of `u_{t} - u_{xx} - 2u_{x} - u = 0` is\n    `u = exp(-x) v`, where v solves the diffusion equation. Substitute now\n    `u = exp(-x) v` without expanding the derivatives:\n    `v_{t} - exp(x)[exp(-x)v]_{xx} - 2exp(x)[exp(-x)v]_{x} - v = 0`.\n    Solve this equation and expect the solution of the diffusion equation.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[0], maximums=[1], sizes=[501], dtype=tf.float32)\n    xs = grid[0]\n\n    final_t = 0.1\n    time_step = 0.001\n\n    def second_order_coeff_fn(t, coord_grid):\n      del t\n      x = coord_grid[0]\n      return [[-tf.exp(x)]]\n\n    def inner_second_order_coeff_fn(t, coord_grid):\n      del t\n      x = coord_grid[0]\n      return [[tf.exp(-x)]]\n\n    def first_order_coeff_fn(t, coord_grid):\n      del t\n      x = coord_grid[0]\n      return [-2 * tf.exp(x)]\n\n    def inner_first_order_coeff_fn(t, coord_grid):\n      del t\n      x = coord_grid[0]\n      return [tf.exp(-x)]\n\n    def zeroth_order_coeff_fn(t, coord_grid):\n      del t, coord_grid\n      return -1\n\n    initial = _reference_pde_initial_cond(xs)\n    expected = _reference_pde_solution(xs, final_t)\n\n    actual = fd_solvers.solve_forward(\n        start_time=0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=initial,\n        time_step=time_step,\n        second_order_coeff_fn=second_order_coeff_fn,\n        first_order_coeff_fn=first_order_coeff_fn,\n        zeroth_order_coeff_fn=zeroth_order_coeff_fn,\n        inner_second_order_coeff_fn=inner_second_order_coeff_fn,\n        inner_first_order_coeff_fn=inner_first_order_coeff_fn)[0]\n\n    self.assertAllClose(expected, actual, atol=1e-3, rtol=1e-3)\n\n  def testCompareExpandedAndNotExpandedPdes(self):\n    """"""Tests comparing PDEs with expanded derivatives and without.\n\n    Take equation `u_{t} - [x^2 u]_{xx} + [x u]_{x} = 0`.\n    Expanding the derivatives yields `u_{t} - x^2 u_{xx} - 3x u_{x} - u = 0`.\n    Solve both equations and expect the results to be equal.\n    """"""\n    grid = grids.uniform_grid(\n        minimums=[0], maximums=[1], sizes=[501], dtype=tf.float32)\n    xs = grid[0]\n\n    final_t = 0.1\n    time_step = 0.001\n\n    initial = _reference_pde_initial_cond(xs)  # arbitrary\n\n    def inner_second_order_coeff_fn(t, coord_grid):\n      del t\n      x = coord_grid[0]\n      return [[-tf.square(x)]]\n\n    def inner_first_order_coeff_fn(t, coord_grid):\n      del t\n      x = coord_grid[0]\n      return [x]\n\n    result_not_expanded = fd_solvers.solve_forward(\n        start_time=0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=initial,\n        time_step=time_step,\n        inner_second_order_coeff_fn=inner_second_order_coeff_fn,\n        inner_first_order_coeff_fn=inner_first_order_coeff_fn)[0]\n\n    def second_order_coeff_fn(t, coord_grid):\n      del t\n      x = coord_grid[0]\n      return [[-tf.square(x)]]\n\n    def first_order_coeff_fn(t, coord_grid):\n      del t\n      x = coord_grid[0]\n      return [-3 * x]\n\n    def zeroth_order_coeff_fn(t, coord_grid):\n      del t, coord_grid\n      return -1\n\n    result_expanded = fd_solvers.solve_forward(\n        start_time=0,\n        end_time=final_t,\n        coord_grid=grid,\n        values_grid=initial,\n        time_step=time_step,\n        second_order_coeff_fn=second_order_coeff_fn,\n        first_order_coeff_fn=first_order_coeff_fn,\n        zeroth_order_coeff_fn=zeroth_order_coeff_fn)[0]\n\n    self.assertAllClose(\n        result_not_expanded, result_expanded, atol=1e-3, rtol=1e-3)\n\n\ndef _reference_pde_initial_cond(xs):\n  """"""Initial conditions for the reference diffusion equation.""""""\n  return -tf.math.abs(xs - 0.5) + 0.5\n\n\ndef _reference_pde_solution(xs, t, num_terms=5):\n  """"""Solution for the reference diffusion equation.""""""\n  u = tf.zeros_like(xs)\n  for k in range(num_terms):\n    n = 2 * k + 1\n    term = tf.math.sin(np.pi * n * xs) * tf.math.exp(-n**2 * np.pi**2 * t)\n    term *= 4 / (np.pi**2 * n**2)\n    if k % 2 == 1:\n      term *= -1\n    u += term\n  return u\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/math/pde/steppers/time_marching_schemes_test.py,36,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for parabolic PDE time marching schemes.""""""\n\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\nfd_solvers = tff.math.pde.fd_solvers\ndirichlet = tff.math.pde.boundary_conditions.dirichlet\nneumann = tff.math.pde.boundary_conditions.neumann\ngrids = tff.math.pde.grids\ncrank_nicolson_scheme = tff.math.pde.steppers.crank_nicolson.crank_nicolson_scheme\nexplicit_scheme = tff.math.pde.steppers.explicit.explicit_scheme\nextrapolation_scheme = tff.math.pde.steppers.extrapolation.extrapolation_scheme\nimplicit_scheme = tff.math.pde.steppers.implicit.implicit_scheme\nweighted_implicit_explicit_scheme = tff.math.pde.steppers.weighted_implicit_explicit.weighted_implicit_explicit_scheme\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass TimeMarchingSchemeTest(tf.test.TestCase, parameterized.TestCase):\n\n  parameters = {\n      \'testcase_name\': \'Implicit\',\n      \'scheme\': implicit_scheme,\n      \'accuracy_order\': 1\n  }, {\n      \'testcase_name\': \'Explicit\',\n      \'scheme\': explicit_scheme,\n      \'accuracy_order\': 1\n  }, {\n      \'testcase_name\': \'Weighted\',\n      \'scheme\': weighted_implicit_explicit_scheme(theta=0.3),\n      \'accuracy_order\': 1\n  }, {\n      \'testcase_name\': \'CrankNicolson\',\n      \'scheme\': crank_nicolson_scheme,\n      \'accuracy_order\': 2\n  }, {\n      \'testcase_name\': \'Extrapolation\',\n      \'scheme\': extrapolation_scheme,\n      \'accuracy_order\': 2\n  }\n  # Not including CompositeTimeMarchingScheme, its correctness is tested\n  # elsewhere.\n\n  @parameterized.named_parameters(*parameters)\n  def testHomogeneous(self, scheme, accuracy_order):\n    # Tests solving du/dt = At for a time step.\n    # Compares with exact solution u(t) = exp(At) u(0).\n\n    # Time step should be small enough to ""resolve"" different orders of accuracy\n    time_step = 0.0001\n    u = tf.constant([1, 2, -1, -2], dtype=tf.float64)\n    matrix = tf.constant(\n        [[1, -1, 0, 0], [3, 1, 2, 0], [0, -2, 1, 4], [0, 0, 3, 1]],\n        dtype=tf.float64)\n\n    tridiag_form = self._convert_to_tridiagonal_format(matrix)\n    actual = self.evaluate(\n        scheme(u, 0, time_step, lambda t: (tridiag_form, None)))\n    expected = self.evaluate(\n        tf.squeeze(\n            tf.matmul(tf.linalg.expm(matrix * time_step), tf.expand_dims(u,\n                                                                         1))))\n\n    error_tolerance = 30 * time_step**(accuracy_order + 1)\n    self.assertLess(np.max(np.abs(actual - expected)), error_tolerance)\n\n  @parameterized.named_parameters(*parameters)\n  def testHomogeneousBackwards(self, scheme, accuracy_order):\n    # Tests solving du/dt = At for a backward time step.\n    # Compares with exact solution u(0) = exp(-At) u(t).\n    time_step = 0.0001\n    u = tf.constant([1, 2, -1, -2], dtype=tf.float64)\n    matrix = tf.constant(\n        [[1, -1, 0, 0], [3, 1, 2, 0], [0, -2, 1, 4], [0, 0, 3, 1]],\n        dtype=tf.float64)\n\n    tridiag_form = self._convert_to_tridiagonal_format(matrix)\n    actual = self.evaluate(\n        scheme(u, time_step, 0, lambda t: (tridiag_form, None)))\n\n    expected = self.evaluate(\n        tf.squeeze(\n            tf.matmul(\n                tf.linalg.expm(-matrix * time_step), tf.expand_dims(u, 1))))\n\n    error_tolerance = 30 * time_step**(accuracy_order + 1)\n    self.assertLess(np.max(np.abs(actual - expected)), error_tolerance)\n\n  @parameterized.named_parameters(*parameters)\n  def testInhomogeneous(self, scheme, accuracy_order):\n    # Tests solving du/dt = At + b for a time step.\n    # Compares with exact solution u(t) = exp(At) u(0) + (exp(At) - 1) A^(-1) b.\n    time_step = 0.0001\n    u = tf.constant([1, 2, -1, -2], dtype=tf.float64)\n    matrix = tf.constant(\n        [[1, -1, 0, 0], [3, 1, 2, 0], [0, -2, 1, 4], [0, 0, 3, 1]],\n        dtype=tf.float64)\n    b = tf.constant([1, -1, -2, 2], dtype=tf.float64)\n\n    tridiag_form = self._convert_to_tridiagonal_format(matrix)\n    actual = self.evaluate(scheme(u, 0, time_step, lambda t: (tridiag_form, b)))\n\n    exponent = tf.linalg.expm(matrix * time_step)\n    eye = tf.eye(4, 4, dtype=tf.float64)\n    u = tf.expand_dims(u, 1)\n    b = tf.expand_dims(b, 1)\n    expected = (\n        tf.matmul(exponent, u) +\n        tf.matmul(exponent - eye, tf.matmul(tf.linalg.inv(matrix), b)))\n    expected = self.evaluate(tf.squeeze(expected))\n\n    error_tolerance = 30 * time_step**(accuracy_order + 1)\n    self.assertLess(np.max(np.abs(actual - expected)), error_tolerance)\n\n  @parameterized.named_parameters(*parameters)\n  def testInhomogeneousBackwards(self, scheme, accuracy_order):\n    # Tests solving du/dt = At + b for a backward time step.\n    # Compares with exact solution u(0) = exp(-At) u(t)\n    # + (exp(-At) - 1) A^(-1) b.\n    time_step = 0.0001\n    u = tf.constant([1, 2, -1, -2], dtype=tf.float64)\n    matrix = tf.constant(\n        [[1, -1, 0, 0], [3, 1, 2, 0], [0, -2, 1, 4], [0, 0, 3, 1]],\n        dtype=tf.float64)\n    b = tf.constant([1, -1, -2, 2], dtype=tf.float64)\n\n    tridiag_form = self._convert_to_tridiagonal_format(matrix)\n    actual = self.evaluate(scheme(u, time_step, 0, lambda t: (tridiag_form, b)))\n\n    exponent = tf.linalg.expm(-matrix * time_step)\n    eye = tf.eye(4, 4, dtype=tf.float64)\n    u = tf.expand_dims(u, 1)\n    b = tf.expand_dims(b, 1)\n    expected = (\n        tf.matmul(exponent, u) +\n        tf.matmul(exponent - eye, tf.matmul(tf.linalg.inv(matrix), b)))\n    expected = self.evaluate(tf.squeeze(expected))\n\n    error_tolerance = 30 * time_step**(accuracy_order + 1)\n    self.assertLess(np.max(np.abs(actual - expected)), error_tolerance)\n\n  def _convert_to_tridiagonal_format(self, matrix):\n    matrix_np = self.evaluate(matrix)\n    n = matrix_np.shape[0]\n    superdiag = [matrix_np[i, i + 1] for i in range(n - 1)] + [0]\n    diag = [matrix_np[i, i] for i in range(n)]\n    subdiag = [0] + [matrix_np[i + 1, i] for i in range(n - 1)]\n    return tuple(\n        tf.constant(v, dtype=matrix.dtype) for v in (diag, superdiag, subdiag))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/math/pde/steppers/weighted_implicit_explicit.py,5,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Weighted implicit-explicit time marching scheme for parabolic PDEs.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math.pde.steppers.parabolic_equation_stepper import parabolic_equation_step\n\n\ndef weighted_implicit_explicit_step(theta):\n  """"""Creates a stepper function with weighted implicit-explicit scheme.\n\n  Given a space-discretized equation\n\n  ```\n  du/dt = A(t) u(t) + b(t)\n  ```\n  (here `u` is a value vector, `A` and `b` are the matrix and the vector defined\n  by the PDE), the scheme approximates the right-hand side as a weighted average\n  of values taken before and after a time step:\n\n  ```\n  (u(t2) - u(t1)) / (t2 - t1) = theta * (A(t1) u(t1) + b(t1))\n     + (1 - theta) (A(t2) u(t2) + b(t2)).\n  ```\n\n  Includes as particular cases the implicit (`theta = 0`), explicit\n  (`theta = 1`), and Crank-Nicolson (`theta = 0.5`) schemes.\n\n  The scheme is stable for `theta >= 0.5`, is second order accurate if\n  `theta = 0.5` (i.e. in Crank-Nicolson case), and first order accurate\n  otherwise.\n\n  More details can be found in `weighted_implicit_explicit_scheme` below.\n\n  Args:\n    theta: A float in range `[0, 1]`. A parameter used to mix implicit and\n      explicit schemes together. Value of `0.0` corresponds to the fully\n      implicit scheme, `1.0` to the fully explicit, and `0.5` to the\n      Crank-Nicolson scheme.\n\n  Returns:\n    Callable to be used in finite-difference PDE solvers (see fd_solvers.py).\n  """"""\n  scheme = weighted_implicit_explicit_scheme(theta)\n  def step_fn(\n      time,\n      next_time,\n      coord_grid,\n      value_grid,\n      boundary_conditions,\n      second_order_coeff_fn,\n      first_order_coeff_fn,\n      zeroth_order_coeff_fn,\n      inner_second_order_coeff_fn,\n      inner_first_order_coeff_fn,\n      num_steps_performed,\n      dtype=None,\n      name=None):\n    """"""Performs the step.""""""\n    del num_steps_performed\n    name = name or \'weighted_implicit_explicit_scheme\'\n    return parabolic_equation_step(time,\n                                   next_time,\n                                   coord_grid,\n                                   value_grid,\n                                   boundary_conditions,\n                                   second_order_coeff_fn,\n                                   first_order_coeff_fn,\n                                   zeroth_order_coeff_fn,\n                                   inner_second_order_coeff_fn,\n                                   inner_first_order_coeff_fn,\n                                   time_marching_scheme=scheme,\n                                   dtype=dtype,\n                                   name=name)\n  return step_fn\n\n\ndef weighted_implicit_explicit_scheme(theta):\n  """"""Constructs weighted implicit-explicit scheme.\n\n  Approximates the space-discretized equation of `du/dt = A(t) u(t) + b(t)` as\n  ```\n  (u(t2) - u(t1)) / (t2 - t1) = theta * (A u(t1) + b)\n     + (1 - theta) (A u(t2) + b),\n  ```\n  where `A = A((t1 + t2)/2)`, `b = b((t1 + t2)/2)`, and `theta` is a float\n  between `0` and `1`.\n\n  Note that typically `A` and `b` are evaluated at `t1` and `t2` in\n  the explicit and implicit terms respectively (the two terms of the right-hand\n  side). Instead, we evaluate them at the midpoint `(t1 + t2)/2`, which saves\n  some computation. One can check that evaluating at midpoint doesn\'t change the\n  order of accuracy of the scheme: it is still second order accurate in\n  `t2 - t1` if `theta = 0.5` and first order accurate otherwise.\n\n  The solution is the following:\n  `u(t2) = (1 - (1 - theta) dt A)^(-1) * (1 + theta dt A) u(t1) + dt b`.\n\n  The main bottleneck here is inverting the matrix `(1 - (1 - theta) dt A)`.\n  This matrix is tridiagonal (each point is influenced by the two neighbouring\n  points), and thus the inversion can be efficiently performed using\n  `tf.linalg.tridiagonal_solve`.\n\n  #### References:\n  [1] I.V. Puzynin, A.V. Selin, S.I. Vinitsky, A high-order accuracy method for\n  numerical solving of the time-dependent Schrodinger equation, Comput. Phys.\n  Commun. 123 (1999), 1.\n  https://www.sciencedirect.com/science/article/pii/S0010465599002246\n\n  Args:\n    theta: A float in range `[0, 1]`. A parameter used to mix implicit and\n      explicit schemes together. Value of `0.0` corresponds to the fully\n      implicit scheme, `1.0` to the fully explicit, and `0.5` to the\n      Crank-Nicolson scheme.\n\n  Returns:\n    A callable that consumes the following arguments by keyword:\n      1. value_grid: Grid of values at time `t1`, i.e. `u(t1)`.\n      2. t1: Time before the step.\n      3. t2: Time after the step.\n      4. equation_params_fn: A callable that takes a scalar `Tensor` argument\n        representing time, and constructs the tridiagonal matrix `A`\n        (a tuple of three `Tensor`s, main, upper, and lower diagonals)\n        and the inhomogeneous term `b`. All of the `Tensor`s are of the same\n        `dtype` as `value_grid` and of the shape broadcastable with the\n        shape of `value_grid`.\n    The callable returns a `Tensor` of the same shape and `dtype` as\n    `value_grid` and represents an approximate solution `u(t2)`.\n  """"""\n  if theta < 0 or theta > 1:\n    raise ValueError(\n        \'`theta` should be in [0, 1]. Supplied: {}\'.format(theta))\n\n  def _marching_scheme(value_grid, t1, t2, equation_params_fn):\n    """"""Constructs the time marching scheme.""""""\n    (diag, superdiag, subdiag), inhomog_term = equation_params_fn(\n        (t1 + t2) / 2)\n\n    if theta == 0:  # fully implicit scheme\n      rhs = value_grid\n    else:\n      rhs = _weighted_scheme_explicit_part(value_grid, diag, superdiag, subdiag,\n                                           theta, t1, t2)\n\n    if inhomog_term is not None:\n      rhs += inhomog_term * (t2 - t1)\n    if theta < 1:\n      # Note that if theta is `0`, `rhs` equals to the `value_grid`, so that the\n      # fully implicit step is performed.\n      return _weighted_scheme_implicit_part(rhs, diag, superdiag, subdiag,\n                                            theta, t1, t2)\n    return rhs\n\n  return _marching_scheme\n\n\ndef _weighted_scheme_explicit_part(vec, diag, upper, lower, theta, t1, t2):\n  """"""Explicit step of the weighted implicit-explicit scheme.\n\n  Args:\n    vec: A real dtype `Tensor` of shape `[num_equations, num_grid_points - 2]`.\n      Represents the multiplied vector. ""- 2"" accounts for the boundary points,\n      which the time-marching schemes do not touch.\n    diag: A real dtype `Tensor` of the shape\n      `[num_equations, num_grid_points - 2]`. Represents the main diagonal of\n      a 3-diagonal matrix of the discretized PDE.\n    upper: A real dtype `Tensor` of the shape\n      `[num_equations, num_grid_points - 2]`. Represents the upper diagonal of\n      a 3-diagonal matrix of the discretized PDE.\n    lower:  A real dtype `Tensor` of the shape\n      `[num_equations, num_grid_points - 2]`. Represents the lower diagonal of\n      a 3-diagonal matrix of the discretized PDE.\n    theta: A Python float between 0 and 1.\n    t1: Smaller of the two times defining the step.\n    t2: Greater of the two times defining the step.\n\n  Returns:\n    A tensor of the same shape and dtype as `vec`.\n  """"""\n  multiplier = theta * (t2 - t1)\n  diag = 1 + multiplier * diag\n  upper = multiplier * upper\n  lower = multiplier * lower\n\n  # Multiply the tridiagonal matrix by the vector.\n  diag_part = diag * vec\n  zeros = tf.zeros_like(lower[..., :1])\n  lower_part = tf.concat((zeros, lower[..., 1:] * vec[..., :-1]), axis=-1)\n  upper_part = tf.concat((upper[..., :-1] * vec[..., 1:], zeros), axis=-1)\n  return lower_part + diag_part + upper_part\n\n\ndef _weighted_scheme_implicit_part(vec, diag, upper, lower, theta, t1, t2):\n  """"""Implicit step of the weighted implicit-explicit scheme.\n\n  Args:\n    vec: A real dtype `Tensor` of shape `[num_equations, num_grid_points - 2]`.\n      Represents the multiplied vector. ""- 2"" accounts for the boundary points,\n      which the time-marching schemes do not touch.\n    diag: A real dtype `Tensor` of the shape\n      `[num_equations, num_grid_points - 2]`. Represents the main diagonal of\n      a 3-diagonal matrix of the discretized PDE.\n    upper: A real dtype `Tensor` of the shape\n      `[num_equations, num_grid_points - 2]`. Represents the upper diagonal of\n      a 3-diagonal matrix of the discretized PDE.\n    lower:  A real dtype `Tensor` of the shape\n      `[num_equations, num_grid_points - 2]`. Represents the lower diagonal of\n      a 3-diagonal matrix of the discretized PDE.\n    theta: A Python float between 0 and 1.\n    t1: Smaller of the two times defining the step.\n    t2: Greater of the two times defining the step.\n\n  Returns:\n    A tensor of the same shape and dtype as `vec`.\n  """"""\n  multiplier = (1 - theta) * (t1 - t2)\n  diag = 1 + multiplier * diag\n  upper = multiplier * upper\n  lower = multiplier * lower\n  return tf.linalg.tridiagonal_solve([upper, diag, lower],\n                                     vec,\n                                     diagonals_format=\'sequence\',\n                                     transpose_rhs=True,\n                                     partial_pivoting=False)\n\n\n__all__ = [\n    \'weighted_implicit_explicit_scheme\',\n    \'weighted_implicit_explicit_step\',\n]\n'"
tf_quant_finance/math/random_ops/halton/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Halton sampling.""""""\n\n\nfrom tf_quant_finance.math.random_ops.halton.halton_impl import HaltonParams\nfrom tf_quant_finance.math.random_ops.halton.halton_impl import sample\n\n\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'sample\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/math/random_ops/halton/halton_impl.py,59,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Quasi Monte Carlo support: Halton sequence.""""""\n\nimport collections\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math.random_ops import stateless\n\n__all__ = [\n    \'sample\',\n]\n\n# The maximum dimension we support. This is limited by the number of primes\n# in the _PRIMES array.\n_MAX_DIMENSION = 1000\n\n# The maximum sequence index we support, depending on data type.\n_MAX_INDEX_BY_DTYPE = {tf.float32: 2**24 - 1, np.float32: 2**24 - 1,\n                       tf.float64: 2**53 - 1, np.float64: 2**53 - 1}\n\n# The number of coefficients we use to represent each Halton number when\n# expressed in the (prime) base for an event dimension. In theory this should be\n# infinite, but in practice it is useful to cap this based on data type.\n_NUM_COEFFS_BY_DTYPE = {tf.float32: 24, np.float32: 24,\n                        tf.float64: 54, np.float64: 54}\n\n# Parameters that can be reused with subsequent calls to halton.sample().\nHaltonParams = collections.namedtuple(\n    \'HaltonParams\',\n    [\n        \'perms\',  # Uniform iid sample from the space of permutations as\n        # returned by _get_permutations() below. A tensor of shape\n        # [_MAX_SIZES_BY_AXES[dtype], sum(_PRIMES)] and dtype\n        # tf.float32 or tf.float64.\n        \'zero_correction\',  # A scaled uniform random tensor of shape\n        # [dim] and  dtype tf.float32 or tf.float64. Used to\n        # appropriately adjust the randomization described\n        # in Owen (2017), see below for further details.\n    ])\n\n\ndef sample(dim,\n           num_results=None,\n           sequence_indices=None,\n           randomized=True,\n           randomization_params=None,\n           seed=None,\n           validate_args=False,\n           dtype=None,\n           name=None):\n  r""""""Returns a sample from the `dim` dimensional Halton sequence.\n\n  Warning: The sequence elements take values only between 0 and 1. Care must be\n  taken to appropriately transform the domain of a function if it differs from\n  the unit cube before evaluating integrals using Halton samples. It is also\n  important to remember that quasi-random numbers without randomization are not\n  a replacement for pseudo-random numbers in every context. Quasi random numbers\n  are completely deterministic and typically have significant negative\n  autocorrelation unless randomization is used.\n\n  Computes the members of the low discrepancy Halton sequence in dimension\n  `dim`. The `dim`-dimensional sequence takes values in the unit hypercube in\n  `dim` dimensions. Currently, only dimensions up to 1000 are supported. The\n  prime base for the k-th axes is the k-th prime starting from 2. For example,\n  if `dim` = 3, then the bases will be [2, 3, 5] respectively and the first\n  element of the non-randomized sequence will be: [0.5, 0.333, 0.2]. For a more\n  complete description of the Halton sequences see\n  [here](https://en.wikipedia.org/wiki/Halton_sequence). For low discrepancy\n  sequences and their applications see\n  [here](https://en.wikipedia.org/wiki/Low-discrepancy_sequence).\n\n  If `randomized` is true, this function produces a scrambled version of the\n  Halton sequence introduced by [Owen (2017)][1]. For the advantages of\n  randomization of low discrepancy sequences see [here](\n  https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method#Randomization_of_quasi-Monte_Carlo).\n\n  The number of samples produced is controlled by the `num_results` and\n  `sequence_indices` parameters. The user must supply either `num_results` or\n  `sequence_indices` but not both.\n  The former is the number of samples to produce starting from the first\n  element. If `sequence_indices` is given instead, the specified elements of\n  the sequence are generated. For example, sequence_indices=tf.range(10) is\n  equivalent to specifying n=10.\n\n  #### Examples\n\n  ```python\n  import tensorflow.compat.v2 as tf\n  import tensorflow_probability as tfp\n\n  # Produce the first 1000 members of the Halton sequence in 3 dimensions.\n  num_results = 1000\n  dim = 3\n  sample, params = qmc.halton.sample(\n    dim,\n    num_results=num_results,\n    seed=127)\n\n  # Evaluate the integral of x_1 * x_2^2 * x_3^3  over the three dimensional\n  # hypercube.\n  powers = tf.range(1.0, limit=dim + 1)\n  integral = tf.reduce_mean(tf.reduce_prod(sample ** powers, axis=-1))\n  true_value = 1.0 / tf.reduce_prod(powers + 1.0)\n  with tf.Session() as session:\n    values = session.run((integral, true_value))\n\n  # Produces a relative absolute error of 1.7%.\n  print (""Estimated: %f, True Value: %f"" % values)\n\n  # Now skip the first 1000 samples and recompute the integral with the next\n  # thousand samples. The sequence_indices argument can be used to do this.\n\n\n  sequence_indices = tf.range(start=1000, limit=1000 + num_results,\n                              dtype=tf.int32)\n  sample_leaped, _ = qmc.halton.sample(\n      dim,\n      sequence_indices=sequence_indices,\n      randomization_params=params)\n\n  integral_leaped = tf.reduce_mean(tf.reduce_prod(sample_leaped ** powers,\n                                                  axis=-1))\n  with tf.Session() as session:\n    values = session.run((integral_leaped, true_value))\n  # Now produces a relative absolute error of 0.05%.\n  print (""Leaped Estimated: %f, True Value: %f"" % values)\n  ```\n\n  Args:\n    dim: Positive Python `int` representing each sample\'s `event_size.` Must not\n      be greater than 1000.\n    num_results: (Optional) Positive scalar `Tensor` of dtype int32. The number\n      of samples to generate. Either this parameter or sequence_indices must be\n      specified but not both. If this parameter is None, then the behaviour is\n      determined by the `sequence_indices`.\n      Default value: `None`.\n    sequence_indices: (Optional) `Tensor` of dtype int32 and rank 1. The\n      elements of the sequence to compute specified by their position in the\n      sequence. The entries index into the Halton sequence starting with 0 and\n      hence, must be whole numbers. For example, sequence_indices=[0, 5, 6] will\n      produce the first, sixth and seventh elements of the sequence. If this\n      parameter is None, then the `num_results` parameter must be specified\n      which gives the number of desired samples starting from the first sample.\n      Default value: `None`.\n    randomized: (Optional) bool indicating whether to produce a randomized\n      Halton sequence. If True, applies the randomization described in [Owen\n      (2017)][1]. If True, either seed or randomization_params must be\n      specified. This is because the randomization uses stateless random number\n      generation which requires an explicitly specified seed.\n      Default value: `True`.\n    randomization_params: (Optional) Instance of `HaltonParams` that fully\n      describes the randomization behavior. If provided and randomized is True,\n      seed will be ignored and these will be used instead of computing them from\n      scratch. If randomized is False, this parameter has no effect.\n      Default value: `None`. In this case with randomized = True, the necessary\n        randomization parameters will be computed from scratch.\n    seed: (Optional) Python integer to seed the random number generator. Must be\n      specified if `randomized` is True and randomization_params is not\n      specified. Ignored if randomized is False or randomization_params is\n      specified.\n      Default value: `None`.\n    validate_args: If True, checks that maximum index is not exceeded and that\n      the dimension `dim` is less than 1 or greater than 1000.\n      Default value: `False`.\n    dtype: Optional `dtype`. The dtype of the output `Tensor` (either `float32`\n    or `float64`).\n      Default value: `None` which maps to the `float32`.\n    name:  (Optional) Python `str` describing ops managed by this function. If\n      not supplied the name of this function is used.\n      Default value: ""halton_sample"".\n\n  Returns:\n    halton_elements: Elements of the Halton sequence. `Tensor` of supplied dtype\n      and `shape` `[num_results, dim]` if `num_results` was specified or shape\n      `[s, dim]` where s is the size of `sequence_indices` if `sequence_indices`\n      were specified.\n    randomization_params: None if randomized is False. If randomized is True\n      and randomization_params was supplied as an argument, returns that.\n      Otherwise returns the computed randomization_params, an instance of\n      `HaltonParams` that fully describes the randomization behavior.\n\n  Raises:\n    ValueError: if both `sequence_indices` and `num_results` were specified.\n    ValueError: if `randomization` is True but `seed` is not specified.\n    InvalidArgumentError: if `validate_args` is True and the maximum supported\n      sequence index is exceeded.\n\n  #### References\n\n  [1]: Art B. Owen. A randomized Halton algorithm in R. _arXiv preprint\n       arXiv:1706.02808_, 2017. https://arxiv.org/abs/1706.02808\n  """"""\n  if (num_results is None) == (sequence_indices is None):\n    raise ValueError(\'Either `num_results` or `sequence_indices` must be\'\n                     \' specified but not both.\')\n  dtype = dtype or tf.float32\n\n  with tf.compat.v1.name_scope(\n      name, \'halton_sample\', values=[num_results, sequence_indices]):\n    # Here and in the following, the shape layout is as follows:\n    # [sample dimension, event dimension, coefficient dimension].\n    # The coefficient dimension is an intermediate axes which will hold the\n    # weights of the starting integer when expressed in the (prime) base for\n    # an event dimension.\n    if num_results is not None:\n      num_results = tf.convert_to_tensor(value=num_results,\n                                         dtype=tf.int32,\n                                         name=\'name_results\')\n    if sequence_indices is not None:\n      sequence_indices = tf.convert_to_tensor(value=sequence_indices,\n                                              dtype=tf.int32,\n                                              name=\'sequence_indices\')\n    indices = _get_indices(num_results, sequence_indices, dtype)\n\n    runtime_assertions = []\n    if validate_args:\n      runtime_assertions.append(\n          tf.compat.v1.assert_less_equal(\n              tf.reduce_max(indices),\n              tf.constant(_MAX_INDEX_BY_DTYPE[dtype], dtype=dtype),\n              message=(\n                  \'Maximum sequence index exceeded. Maximum index for dtype %s \'\n                  \'is %d.\' % (dtype, _MAX_INDEX_BY_DTYPE[dtype]))))\n      runtime_assertions.append(\n          tf.compat.v1.assert_greater_equal(\n              dim, 1, message=\'`dim` should be greater than 1\'))\n      runtime_assertions.append(\n          tf.compat.v1.assert_less_equal(\n              dim, _MAX_DIMENSION,\n              message=\'`dim` should be less or equal than %d\' % _MAX_DIMENSION))\n\n    with tf.compat.v1.control_dependencies(runtime_assertions):\n      radixes = tf.convert_to_tensor(_PRIMES, dtype=dtype, name=\'radixes\')\n      radixes = tf.reshape(radixes[0:dim], shape=[dim, 1])\n\n      max_sizes_by_axes = tf.convert_to_tensor(\n          _MAX_SIZES_BY_AXES[dtype],\n          dtype=dtype,\n          name=\'max_sizes_by_axes\')[:dim]\n      max_size = tf.reduce_max(max_sizes_by_axes)\n\n      # The powers of the radixes that we will need. Note that there is a bit\n      # of an excess here. Suppose we need the place value coefficients of 7\n      # in base 2 and 3. For 2, we will have 3 digits but we only need 2 digits\n      # for base 3. However, we can only create rectangular tensors so we\n      # store both expansions in a [2, 3] tensor. This leads to the problem that\n      # we might end up attempting to raise large numbers to large powers. For\n      # example, base 2 expansion of 1024 has 10 digits. If we were in 10\n      # dimensions, then the 10th prime (29) we will end up computing 29^10 even\n      # though we don\'t need it. We avoid this by setting the exponents for each\n      # axes to 0 beyond the maximum value needed for that dimension.\n      exponents_by_axes = tf.tile([tf.range(max_size, dtype=dtype)], [dim, 1])\n\n      # The mask is true for those coefficients that are irrelevant.\n      weight_mask = exponents_by_axes >= max_sizes_by_axes\n      capped_exponents = tf.where(weight_mask, tf.zeros_like(exponents_by_axes),\n                                  exponents_by_axes)\n      weights = radixes**capped_exponents\n      # The following computes the base b expansion of the indices. Suppose,\n      # x = a0 + a1*b + a2*b^2 + ... Then, performing a floor div of x with\n      # the vector (1, b, b^2, b^3, ...) will produce\n      # (a0 + s1 * b, a1 + s2 * b, ...) where s_i are coefficients we don\'t care\n      # about. Noting that all a_i < b by definition of place value expansion,\n      # we see that taking the elements mod b of the above vector produces the\n      # place value expansion coefficients.\n      coeffs = tf.compat.v1.floor_div(indices, weights)\n      coeffs *= 1. - tf.cast(weight_mask, dtype)\n      coeffs %= radixes\n      if not randomized:\n        coeffs /= radixes\n        return tf.reduce_sum(input_tensor=coeffs / weights, axis=-1), None\n\n      if randomization_params is None:\n        perms, zero_correction = None, None\n      else:\n        perms, zero_correction = randomization_params\n      coeffs, perms = _randomize(coeffs, radixes, seed, perms=perms)\n      # Remove the contribution from randomizing the trailing zero for the\n      # axes where max_size_by_axes < max_size. This will be accounted\n      # for separately below (using zero_correction).\n      coeffs *= 1. - tf.cast(weight_mask, dtype)\n      coeffs /= radixes\n      base_values = tf.reduce_sum(input_tensor=coeffs / weights, axis=-1)\n\n      # The randomization used in Owen (2017) does not leave 0 invariant. While\n      # we have accounted for the randomization of the first `max_size_by_axes`\n      # coefficients, we still need to correct for the trailing zeros. Luckily,\n      # this is equivalent to adding a uniform random value scaled so the first\n      # `max_size_by_axes` coefficients are zero. The following statements\n      # perform this correction.\n      if zero_correction is None:\n        if seed is None:\n          zero_correction = tf.random.uniform([dim, 1], dtype=dtype)\n        else:\n          zero_correction = tf.random.stateless_uniform([dim, 1],\n                                                        seed=(seed, seed),\n                                                        dtype=dtype)\n        zero_correction /= radixes**max_sizes_by_axes\n        zero_correction = tf.reshape(zero_correction, [-1])\n\n      return base_values + zero_correction, HaltonParams(perms, zero_correction)\n\n\ndef _randomize(coeffs, radixes, seed, perms=None):\n  """"""Applies the Owen (2017) randomization to the coefficients.""""""\n  given_dtype = coeffs.dtype\n  coeffs = tf.cast(coeffs, dtype=tf.int32)\n  num_coeffs = _NUM_COEFFS_BY_DTYPE[given_dtype]\n  radixes = tf.reshape(tf.cast(radixes, dtype=tf.int32), shape=[-1])\n  if perms is None:\n    perms = _get_permutations(num_coeffs, radixes, seed)\n    perms = tf.reshape(perms, shape=[-1])\n  radix_sum = tf.reduce_sum(input_tensor=radixes)\n  radix_offsets = tf.reshape(tf.cumsum(radixes, exclusive=True), shape=[-1, 1])\n  offsets = radix_offsets + tf.range(num_coeffs) * radix_sum\n  permuted_coeffs = tf.gather(perms, coeffs + offsets)\n  return tf.cast(permuted_coeffs, dtype=given_dtype), perms\n\n\ndef _get_permutations(num_results, dims, seed):\n  """"""Uniform iid sample from the space of permutations.\n\n  Draws a sample of size `num_results` from the group of permutations of degrees\n  specified by the `dims` tensor. These are packed together into one tensor\n  such that each row is one sample from each of the dimensions in `dims`. For\n  example, if dims = [2,3] and num_results = 2, the result is a tensor of shape\n  [2, 2 + 3] and the first row of the result might look like:\n  [1, 0, 2, 0, 1]. The first two elements are a permutation over 2 elements\n  while the next three are a permutation over 3 elements.\n\n  Args:\n    num_results: A positive scalar `Tensor` of integral type. The number of\n      draws from the discrete uniform distribution over the permutation groups.\n    dims: A 1D `Tensor` of the same dtype as `num_results`. The degree of the\n      permutation groups from which to sample.\n    seed: (Optional) Python integer to seed the random number generator.\n\n  Returns:\n    permutations: A `Tensor` of shape `[num_results, sum(dims)]` and the same\n    dtype as `dims`.\n  """"""\n  sample_range = tf.range(num_results)\n\n  def generate_one(d):\n\n    def fn(i):\n      if seed is None:\n        return tf.random.shuffle(tf.range(d))\n      else:\n        return stateless.stateless_random_shuffle(tf.range(d),\n                                                  seed=(seed + i, d))\n\n    return tf.map_fn(\n        fn, sample_range, parallel_iterations=1 if seed is not None else 10)\n\n  return tf.concat([generate_one(d) for d in tf.unstack(dims)], axis=-1)\n\n\ndef _get_indices(num_results, sequence_indices, dtype, name=None):\n  """"""Generates starting points for the Halton sequence procedure.\n\n  The k\'th element of the sequence is generated starting from a positive integer\n  which must be distinct for each `k`. It is conventional to choose the starting\n  point as `k` itself (or `k+1` if k is zero based). This function generates\n  the starting integers for the required elements and reshapes the result for\n  later use.\n\n  Args:\n    num_results: Positive scalar `Tensor` of dtype int32. The number of samples\n      to generate. If this parameter is supplied, then `sequence_indices` should\n      be None.\n    sequence_indices: `Tensor` of dtype int32 and rank 1. The entries index into\n      the Halton sequence starting with 0 and hence, must be whole numbers. For\n      example, sequence_indices=[0, 5, 6] will produce the first, sixth and\n      seventh elements of the sequence. If this parameter is not None then `n`\n      must be None.\n    dtype: The dtype of the sample. One of `float32` or `float64`. Default is\n      `float32`.\n    name: Python `str` name which describes ops created by this function.\n\n  Returns:\n    indices: `Tensor` of dtype `dtype` and shape = `[n, 1, 1]`.\n  """"""\n  with tf.compat.v1.name_scope(name, \'_get_indices\',\n                               [num_results, sequence_indices]):\n    if sequence_indices is None:\n      num_results = tf.cast(num_results, dtype=dtype)\n      sequence_indices = tf.range(num_results, dtype=dtype)\n    else:\n      sequence_indices = tf.cast(sequence_indices, dtype)\n\n    # Shift the indices so they are 1 based.\n    indices = sequence_indices + 1\n\n    # Reshape to make space for the event dimension and the place value\n    # coefficients.\n    return tf.reshape(indices, [-1, 1, 1])\n\n\ndef _base_expansion_size(num, bases):\n  """"""Computes the number of terms in the place value expansion.\n\n  Let num = a0 + a1 b + a2 b^2 + ... ak b^k be the place value expansion of\n  `num` in base b (ak <> 0). This function computes and returns `k+1` for each\n  base `b` specified in `bases`.\n\n  This can be inferred from the base `b` logarithm of `num` as follows:\n    $$k = Floor(log_b (num)) + 1  = Floor( log(num) / log(b)) + 1$$\n\n  Args:\n    num: Scalar numpy array of dtype either `float32` or `float64`. The number\n      to compute the base expansion size of.\n    bases: Numpy array of the same dtype as num. The bases to compute the size\n      against.\n\n  Returns:\n    Tensor of same dtype and shape as `bases` containing the size of num when\n    written in that base.\n  """"""\n  return np.floor(np.log(num) / np.log(bases)) + 1\n\n\n# First 1000 prime numbers.\n_PRIMES = np.array([\n    2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71,\n    73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151,\n    157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233,\n    239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317,\n    331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419,\n    421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503,\n    509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607,\n    613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701,\n    709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811,\n    821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911,\n    919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997, 1009, 1013,\n    1019, 1021, 1031, 1033, 1039, 1049, 1051, 1061, 1063, 1069, 1087, 1091,\n    1093, 1097, 1103, 1109, 1117, 1123, 1129, 1151, 1153, 1163, 1171, 1181,\n    1187, 1193, 1201, 1213, 1217, 1223, 1229, 1231, 1237, 1249, 1259, 1277,\n    1279, 1283, 1289, 1291, 1297, 1301, 1303, 1307, 1319, 1321, 1327, 1361,\n    1367, 1373, 1381, 1399, 1409, 1423, 1427, 1429, 1433, 1439, 1447, 1451,\n    1453, 1459, 1471, 1481, 1483, 1487, 1489, 1493, 1499, 1511, 1523, 1531,\n    1543, 1549, 1553, 1559, 1567, 1571, 1579, 1583, 1597, 1601, 1607, 1609,\n    1613, 1619, 1621, 1627, 1637, 1657, 1663, 1667, 1669, 1693, 1697, 1699,\n    1709, 1721, 1723, 1733, 1741, 1747, 1753, 1759, 1777, 1783, 1787, 1789,\n    1801, 1811, 1823, 1831, 1847, 1861, 1867, 1871, 1873, 1877, 1879, 1889,\n    1901, 1907, 1913, 1931, 1933, 1949, 1951, 1973, 1979, 1987, 1993, 1997,\n    1999, 2003, 2011, 2017, 2027, 2029, 2039, 2053, 2063, 2069, 2081, 2083,\n    2087, 2089, 2099, 2111, 2113, 2129, 2131, 2137, 2141, 2143, 2153, 2161,\n    2179, 2203, 2207, 2213, 2221, 2237, 2239, 2243, 2251, 2267, 2269, 2273,\n    2281, 2287, 2293, 2297, 2309, 2311, 2333, 2339, 2341, 2347, 2351, 2357,\n    2371, 2377, 2381, 2383, 2389, 2393, 2399, 2411, 2417, 2423, 2437, 2441,\n    2447, 2459, 2467, 2473, 2477, 2503, 2521, 2531, 2539, 2543, 2549, 2551,\n    2557, 2579, 2591, 2593, 2609, 2617, 2621, 2633, 2647, 2657, 2659, 2663,\n    2671, 2677, 2683, 2687, 2689, 2693, 2699, 2707, 2711, 2713, 2719, 2729,\n    2731, 2741, 2749, 2753, 2767, 2777, 2789, 2791, 2797, 2801, 2803, 2819,\n    2833, 2837, 2843, 2851, 2857, 2861, 2879, 2887, 2897, 2903, 2909, 2917,\n    2927, 2939, 2953, 2957, 2963, 2969, 2971, 2999, 3001, 3011, 3019, 3023,\n    3037, 3041, 3049, 3061, 3067, 3079, 3083, 3089, 3109, 3119, 3121, 3137,\n    3163, 3167, 3169, 3181, 3187, 3191, 3203, 3209, 3217, 3221, 3229, 3251,\n    3253, 3257, 3259, 3271, 3299, 3301, 3307, 3313, 3319, 3323, 3329, 3331,\n    3343, 3347, 3359, 3361, 3371, 3373, 3389, 3391, 3407, 3413, 3433, 3449,\n    3457, 3461, 3463, 3467, 3469, 3491, 3499, 3511, 3517, 3527, 3529, 3533,\n    3539, 3541, 3547, 3557, 3559, 3571, 3581, 3583, 3593, 3607, 3613, 3617,\n    3623, 3631, 3637, 3643, 3659, 3671, 3673, 3677, 3691, 3697, 3701, 3709,\n    3719, 3727, 3733, 3739, 3761, 3767, 3769, 3779, 3793, 3797, 3803, 3821,\n    3823, 3833, 3847, 3851, 3853, 3863, 3877, 3881, 3889, 3907, 3911, 3917,\n    3919, 3923, 3929, 3931, 3943, 3947, 3967, 3989, 4001, 4003, 4007, 4013,\n    4019, 4021, 4027, 4049, 4051, 4057, 4073, 4079, 4091, 4093, 4099, 4111,\n    4127, 4129, 4133, 4139, 4153, 4157, 4159, 4177, 4201, 4211, 4217, 4219,\n    4229, 4231, 4241, 4243, 4253, 4259, 4261, 4271, 4273, 4283, 4289, 4297,\n    4327, 4337, 4339, 4349, 4357, 4363, 4373, 4391, 4397, 4409, 4421, 4423,\n    4441, 4447, 4451, 4457, 4463, 4481, 4483, 4493, 4507, 4513, 4517, 4519,\n    4523, 4547, 4549, 4561, 4567, 4583, 4591, 4597, 4603, 4621, 4637, 4639,\n    4643, 4649, 4651, 4657, 4663, 4673, 4679, 4691, 4703, 4721, 4723, 4729,\n    4733, 4751, 4759, 4783, 4787, 4789, 4793, 4799, 4801, 4813, 4817, 4831,\n    4861, 4871, 4877, 4889, 4903, 4909, 4919, 4931, 4933, 4937, 4943, 4951,\n    4957, 4967, 4969, 4973, 4987, 4993, 4999, 5003, 5009, 5011, 5021, 5023,\n    5039, 5051, 5059, 5077, 5081, 5087, 5099, 5101, 5107, 5113, 5119, 5147,\n    5153, 5167, 5171, 5179, 5189, 5197, 5209, 5227, 5231, 5233, 5237, 5261,\n    5273, 5279, 5281, 5297, 5303, 5309, 5323, 5333, 5347, 5351, 5381, 5387,\n    5393, 5399, 5407, 5413, 5417, 5419, 5431, 5437, 5441, 5443, 5449, 5471,\n    5477, 5479, 5483, 5501, 5503, 5507, 5519, 5521, 5527, 5531, 5557, 5563,\n    5569, 5573, 5581, 5591, 5623, 5639, 5641, 5647, 5651, 5653, 5657, 5659,\n    5669, 5683, 5689, 5693, 5701, 5711, 5717, 5737, 5741, 5743, 5749, 5779,\n    5783, 5791, 5801, 5807, 5813, 5821, 5827, 5839, 5843, 5849, 5851, 5857,\n    5861, 5867, 5869, 5879, 5881, 5897, 5903, 5923, 5927, 5939, 5953, 5981,\n    5987, 6007, 6011, 6029, 6037, 6043, 6047, 6053, 6067, 6073, 6079, 6089,\n    6091, 6101, 6113, 6121, 6131, 6133, 6143, 6151, 6163, 6173, 6197, 6199,\n    6203, 6211, 6217, 6221, 6229, 6247, 6257, 6263, 6269, 6271, 6277, 6287,\n    6299, 6301, 6311, 6317, 6323, 6329, 6337, 6343, 6353, 6359, 6361, 6367,\n    6373, 6379, 6389, 6397, 6421, 6427, 6449, 6451, 6469, 6473, 6481, 6491,\n    6521, 6529, 6547, 6551, 6553, 6563, 6569, 6571, 6577, 6581, 6599, 6607,\n    6619, 6637, 6653, 6659, 6661, 6673, 6679, 6689, 6691, 6701, 6703, 6709,\n    6719, 6733, 6737, 6761, 6763, 6779, 6781, 6791, 6793, 6803, 6823, 6827,\n    6829, 6833, 6841, 6857, 6863, 6869, 6871, 6883, 6899, 6907, 6911, 6917,\n    6947, 6949, 6959, 6961, 6967, 6971, 6977, 6983, 6991, 6997, 7001, 7013,\n    7019, 7027, 7039, 7043, 7057, 7069, 7079, 7103, 7109, 7121, 7127, 7129,\n    7151, 7159, 7177, 7187, 7193, 7207, 7211, 7213, 7219, 7229, 7237, 7243,\n    7247, 7253, 7283, 7297, 7307, 7309, 7321, 7331, 7333, 7349, 7351, 7369,\n    7393, 7411, 7417, 7433, 7451, 7457, 7459, 7477, 7481, 7487, 7489, 7499,\n    7507, 7517, 7523, 7529, 7537, 7541, 7547, 7549, 7559, 7561, 7573, 7577,\n    7583, 7589, 7591, 7603, 7607, 7621, 7639, 7643, 7649, 7669, 7673, 7681,\n    7687, 7691, 7699, 7703, 7717, 7723, 7727, 7741, 7753, 7757, 7759, 7789,\n    7793, 7817, 7823, 7829, 7841, 7853, 7867, 7873, 7877, 7879, 7883, 7901,\n    7907, 7919\n],\n                   dtype=np.int32)\n\n# For each supported data type, we store the maximum number of digits we might\n# need for each dimension. See _base_expansion_size() for more details.\n_MAX_SIZES_BY_AXES = {\n    dtype: _base_expansion_size(_MAX_INDEX_BY_DTYPE[dtype],\n                                np.expand_dims(_PRIMES, 1))\n    for dtype in _MAX_INDEX_BY_DTYPE\n}\n\nassert len(_PRIMES) == _MAX_DIMENSION\n'"
tf_quant_finance/math/random_ops/halton/halton_test.py,34,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for random.halton.""""""\n\nimport numpy as np\nfrom six.moves import range\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\nfrom tf_quant_finance.math import random\n\ntfd = tfp.distributions\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass HaltonSequenceTest(tf.test.TestCase):\n\n  def test_known_values_small_bases(self):\n    # The first five elements of the non-randomized random.halton sequence\n    # with base 2 and 3.\n    expected = np.array([[1. / 2, 1. / 3], [1. / 4, 2. / 3], [3. / 4, 1. / 9],\n                         [1. / 8, 4. / 9], [5. / 8, 7. / 9]],\n                        dtype=np.float32)\n    sample, _ = random.halton.sample(2, num_results=5, randomized=False)\n    self.assertAllClose(expected, self.evaluate(sample), rtol=1e-6)\n\n  def test_dynamic_num_samples(self):\n    """"""Tests that num_samples argument supports Tensors.""""""\n    # The first five elements of the non-randomized random.halton sequence\n    # with base 2 and 3.\n    expected = np.array([[1. / 2, 1. / 3], [1. / 4, 2. / 3], [3. / 4, 1. / 9],\n                         [1. / 8, 4. / 9], [5. / 8, 7. / 9]],\n                        dtype=np.float32)\n    sample, _ = random.halton.sample(2, num_results=tf.constant(5),\n                                     randomized=False)\n    self.assertAllClose(expected, self.evaluate(sample), rtol=1e-6)\n\n  def test_sequence_indices(self):\n    """"""Tests access of sequence elements by index.""""""\n    dim = 5\n    indices = tf.range(10, dtype=tf.int32)\n    sample_direct, _ = random.halton.sample(dim, num_results=10,\n                                            randomized=False)\n    sample_from_indices, _ = random.halton.sample(\n        dim, sequence_indices=indices, randomized=False)\n    self.assertAllClose(\n        self.evaluate(sample_direct),\n        self.evaluate(sample_from_indices),\n        rtol=1e-6)\n\n  def test_dtypes_works_correctly(self):\n    """"""Tests that all supported dtypes work without error.""""""\n    dim = 3\n    sample_float32, _ = random.halton.sample(\n        dim, num_results=10, dtype=tf.float32, seed=11)\n    sample_float64, _ = random.halton.sample(\n        dim, num_results=10, dtype=tf.float64, seed=21)\n    self.assertEqual(self.evaluate(sample_float32).dtype, np.float32)\n    self.assertEqual(self.evaluate(sample_float64).dtype, np.float64)\n\n  def test_normal_integral_mean_and_var_correctly_estimated(self):\n    n = int(1000)\n    # This test is almost identical to the similarly named test in\n    # monte_carlo_test.py. The only difference is that we use the Halton\n    # samples instead of the random samples to evaluate the expectations.\n    # MC with pseudo random numbers converges at the rate of 1/ Sqrt(N)\n    # (N=number of samples). For QMC in low dimensions, the expected convergence\n    # rate is ~ 1/N. Hence we should only need 1e3 samples as compared to the\n    # 1e6 samples used in the pseudo-random monte carlo.\n    mu_p = tf.constant([-1., 1.], dtype=tf.float64)\n    mu_q = tf.constant([0., 0.], dtype=tf.float64)\n    sigma_p = tf.constant([0.5, 0.5], dtype=tf.float64)\n    sigma_q = tf.constant([1., 1.], dtype=tf.float64)\n    p = tfd.Normal(loc=mu_p, scale=sigma_p)\n    q = tfd.Normal(loc=mu_q, scale=sigma_q)\n\n    cdf_sample, _ = random.halton.sample(2, num_results=n, dtype=tf.float64,\n                                         seed=1729)\n    q_sample = q.quantile(cdf_sample)\n\n    # Compute E_p[X].\n    e_x = tf.reduce_mean(q_sample * p.prob(q_sample) / q.prob(q_sample), 0)\n\n    # Compute E_p[X^2 - E_p[X]^2].\n    e_x2 = tf.reduce_mean(q_sample**2 * p.prob(q_sample) / q.prob(q_sample)\n                          - e_x**2, 0)\n    stddev = tf.sqrt(e_x2)\n\n    # Keep the tolerance levels the same as in monte_carlo_test.py.\n    self.assertEqual(p.batch_shape, e_x.shape)\n    self.assertAllClose(self.evaluate(p.mean()), self.evaluate(e_x), rtol=0.01)\n    self.assertAllClose(\n        self.evaluate(p.stddev()), self.evaluate(stddev), rtol=0.02)\n\n  def test_docstring_example(self):\n    # Produce the first 1000 members of the Halton sequence in 3 dimensions.\n    num_results = 1000\n    dim = 3\n    sample, params = random.halton.sample(dim, num_results=num_results,\n                                          seed=127)\n\n    # Evaluate the integral of x_1 * x_2^2 * x_3^3  over the three dimensional\n    # hypercube.\n    powers = tf.range(1., limit=dim + 1)\n    integral = tf.reduce_mean(\n        input_tensor=tf.reduce_prod(input_tensor=sample**powers, axis=-1))\n    true_value = 1. / tf.reduce_prod(input_tensor=powers + 1.)\n\n    # Produces a relative absolute error of 1.7%.\n    self.assertAllClose(\n        self.evaluate(integral), self.evaluate(true_value), rtol=0.02)\n\n    # Now skip the first 1000 samples and recompute the integral with the next\n    # thousand samples. The sequence_indices argument can be used to do this.\n\n    sequence_indices = tf.range(\n        start=1000, limit=1000 + num_results, dtype=tf.int32)\n    sample_leaped, _ = random.halton.sample(\n        dim, sequence_indices=sequence_indices, randomization_params=params)\n\n    integral_leaped = tf.reduce_mean(\n        input_tensor=tf.reduce_prod(\n            input_tensor=sample_leaped**powers, axis=-1))\n    self.assertAllClose(\n        self.evaluate(integral_leaped), self.evaluate(true_value), rtol=0.05)\n\n  def test_randomized_qmc_basic(self):\n    """"""Tests the randomization of the random.halton sequences.""""""\n    # This test is identical to the example given in Owen (2017), Figure 5.\n    dim = 20\n    num_results = 2000\n    replica = 5\n    seed = 121117\n\n    values = []\n    for i in range(replica):\n      sample, _ = random.halton.sample(dim, num_results=num_results,\n                                       seed=seed + i)\n      f = tf.reduce_mean(\n          input_tensor=tf.reduce_sum(input_tensor=sample, axis=1)**2)\n      values.append(self.evaluate(f))\n    self.assertAllClose(np.mean(values), 101.6667, atol=np.std(values) * 2)\n\n  def test_partial_sum_func_qmc(self):\n    """"""Tests the QMC evaluation of (x_j + x_{j+1} ...+x_{n})^2.\n\n    A good test of QMC is provided by the function:\n\n      f(x_1,..x_n, x_{n+1}, ..., x_{n+m}) = (x_{n+1} + ... x_{n+m} - m / 2)^2\n\n    with the coordinates taking values in the unit interval. The mean and\n    variance of this function (with the uniform distribution over the\n    unit-hypercube) is exactly calculable:\n\n      <f> = m / 12, Var(f) = m (5m - 3) / 360\n\n    The purpose of the ""shift"" (if n > 0) in the coordinate dependence of the\n    function is to provide a test for Halton sequence which exhibit more\n    dependence in the higher axes.\n\n    This test confirms that the mean squared error of RQMC estimation falls\n    as O(N^(2-e)) for any e>0.\n    """"""\n    n, m = 5, 5\n    dim = n + m\n    num_results_lo, num_results_hi = 500, 5000\n    replica = 10\n    true_mean = m / 12.\n    seed_lo = 1925\n    seed_hi = 898128\n\n    def func_estimate(x):\n      return tf.reduce_mean(\n          input_tensor=tf.math.squared_difference(\n              tf.reduce_sum(input_tensor=x[:, -m:], axis=-1), m / 2.))\n\n    estimates = []\n    for i in range(replica):\n      sample_lo, _ = random.halton.sample(\n          dim, num_results=num_results_lo, seed=seed_lo + i)\n      sample_hi, _ = random.halton.sample(\n          dim, num_results=num_results_hi, seed=seed_hi + i)\n      f_lo, f_hi = func_estimate(sample_lo), func_estimate(sample_hi)\n      estimates.append((self.evaluate(f_lo), self.evaluate(f_hi)))\n    var_lo, var_hi = np.mean((np.array(estimates) - true_mean)**2, axis=0)\n\n    # Expect that the variance scales as N^2 so var_hi / var_lo ~ k / 10^2\n    # with k a fudge factor accounting for the residual N dependence\n    # of the QMC error and the sampling error.\n    log_rel_err = np.log(100 * var_hi / var_lo)\n    self.assertAllClose(log_rel_err, 0., atol=1.2)\n\n  def test_seed_implies_deterministic_results(self):\n    dim = 20\n    num_results = 100\n    seed = 1925\n    sample1, _ = random.halton.sample(dim, num_results=num_results, seed=seed)\n    sample2, _ = random.halton.sample(dim, num_results=num_results, seed=seed)\n    [sample1_, sample2_] = self.evaluate([sample1, sample2])\n    self.assertAllClose(sample1_, sample2_, atol=0., rtol=1e-6)\n\n  def test_randomization_does_not_depend_on_sequence_indices(self):\n    dim = 2\n    seed = 9427\n    sample1, _ = random.halton.sample(dim, sequence_indices=[0], seed=seed)\n    # For sample2, we generate an additional row at index=1000 then discard it.\n    sample2, _ = random.halton.sample(dim, sequence_indices=[0, 1000],\n                                      seed=seed)\n    sample2 = sample2[:1, :]\n    self.assertAllClose(\n        self.evaluate(sample1), self.evaluate(sample2), rtol=1e-6)\n\n  def test_many_small_batches_same_as_one_big_batch(self):\n    dim = 2\n    num_results_per_batch = 1\n    num_batches = 3\n    seed = 1925\n    sample1, _ = random.halton.sample(\n        dim, num_results_per_batch * num_batches, seed=seed)\n    batch_indices = (\n        tf.range(i * num_results_per_batch, (i + 1) * num_results_per_batch)\n        for i in range(num_batches))\n    sample2 = (\n        random.halton.sample(dim, sequence_indices=sequence_indices, seed=seed)\n        for sequence_indices in batch_indices)\n    result_set1 = set(tuple(row) for row in self.evaluate(sample1))\n    result_set2 = set()\n    for batch, _ in sample2:\n      result_set2.update(tuple(row) for row in self.evaluate(batch))\n    self.assertEqual(result_set1, result_set2)\n\n  def test_max_index_exceeded_raises(self):\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      sample, _ = random.halton.sample(\n          1,\n          sequence_indices=[2**30],\n          dtype=tf.float32,\n          randomized=False,\n          validate_args=True)\n      self.evaluate(sample)\n\n  def test_dim_is_negative(self):\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      sample, _ = random.halton.sample(\n          -1,\n          num_results=10,\n          dtype=tf.float32,\n          randomized=False,\n          validate_args=True)\n      self.evaluate(sample)\n\n  def test_dim_too_big(self):\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      sample, _ = random.halton.sample(\n          1001,\n          num_results=10,\n          dtype=tf.float32,\n          randomized=False,\n          validate_args=True)\n      self.evaluate(sample)\n\n  def test_reusing_params_returns_same_points(self):\n    dim = 20\n    num_results = 100\n    seed1, seed2 = 1925, 62278\n    sample1, params = random.halton.sample(dim, num_results=num_results,\n                                           seed=seed1)\n    # We expect the same result because seed2 will be ignored when\n    # randomization_params is supplied.\n    sample2, _ = random.halton.sample(\n        dim, num_results=num_results, seed=seed2, randomization_params=params)\n    [sample1_, sample2_] = self.evaluate([sample1, sample2])\n    self.assertAllClose(sample1_, sample2_, atol=0., rtol=1e-6)\n\n  def test_using_params_with_randomization_false_does_not_randomize(self):\n    dim = 20\n    num_results = 100\n    sample_plain, _ = random.halton.sample(\n        dim, num_results=num_results, randomized=False)\n    seed = 87226\n    _, params = random.halton.sample(dim, num_results=num_results, seed=seed)\n    sample_with_params, _ = random.halton.sample(\n        dim,\n        num_results=num_results,\n        randomized=False,\n        randomization_params=params)\n    self.assertAllClose(\n        self.evaluate(sample_plain),\n        self.evaluate(sample_with_params),\n        atol=0.,\n        rtol=1e-6)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_quant_finance/math/random_ops/sobol/__init__.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Sobol sampling.""""""\n\n\nfrom tf_quant_finance.math.random_ops.sobol.sobol_impl import sample\n\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'sample\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/math/random_ops/sobol/sobol_impl.py,28,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Quasi Monte Carlo support: Sobol sequence.\n\nA TensorFlow implementation of Sobol sequences, a type of quasi-random\nlow-discrepancy sequence: https://en.wikipedia.org/wiki/Sobol_sequence.\n""""""\n\n\nimport logging\nimport os\n\nimport numpy as np\nfrom six.moves import range\nimport tensorflow.compat.v2 as tf\n\n_LN_2 = np.log(2.)\n\n\ndef sample(dim,\n           num_results,\n           skip=0,\n           validate_args=False,\n           dtype=None,\n           name=None):\n  """"""Returns num_results samples from the Sobol sequence of dimension dim.\n\n  Uses the original ordering of points, not the more commonly used Gray code\n  ordering. Derived from notes by Joe & Kuo[1].\n\n  [1] describes bitwise operations on binary floats. The implementation below\n  achieves this by transforming the floats into ints, being careful to align\n  the digits so the bitwise operations are correct, then transforming back to\n  floats.\n\n  Args:\n    dim: Positive Python `int` representing each sample\'s `event_size.`\n    num_results: Positive scalar `Tensor` of dtype int32. The number of Sobol\n      points to return in the output.\n    skip: Positive scalar `Tensor` of dtype int32. The number of initial points\n      of the Sobol sequence to skip.\n    validate_args: Python `bool`. When `True`, input `Tensor\'s` are checked for\n      validity despite possibly degrading runtime performance. The checks verify\n      that `dim >= 1`, `num_results >= 1`, and `skip >= 0`. When `False` invalid\n      inputs may silently render incorrect outputs.\n      Default value: False.\n    dtype: Optional `dtype`. The dtype of the output `Tensor` (either `float32`\n      or `float64`).\n      Default value: `None` which maps to the `float32`.\n    name: Python `str` name prefixed to ops created by this function.\n\n  Returns:\n    `Tensor` of samples from Sobol sequence with `shape` [n, dim].\n\n  #### References\n\n  [1]: S. Joe and F. Y. Kuo. Notes on generating Sobol sequences. August 2008.\n       https://web.maths.unsw.edu.au/~fkuo/sobol/joe-kuo-notes.pdf\n  """"""\n  with tf.compat.v1.name_scope(name, \'sobol_sample\', [dim, num_results, skip]):\n    num_results = tf.convert_to_tensor(\n        num_results, dtype=tf.int32, name=\'num_results\')\n    skip = tf.convert_to_tensor(skip, dtype=tf.int32, name=\'skip\')\n    control_dependencies = []\n    if validate_args:\n      if dim < 1:\n        raise ValueError(\n            \'Dimension must be greater than zero. Supplied {}\'.format(dim))\n      control_dependencies.append(tf.compat.v1.debugging.assert_greater(\n          num_results, 0,\n          message=\'Number of results `num_results` must be greater than zero.\'))\n      control_dependencies.append(tf.compat.v1.debugging.assert_greater(\n          skip, 0,\n          message=\'`skip` must be non-negative.\'))\n\n    with tf.compat.v1.control_dependencies(control_dependencies):\n      if validate_args:\n        # The following tf.maximum ensures that the graph can be executed\n        # even with ignored control dependencies\n        num_results = tf.maximum(num_results, 1, name=\'fix_num_results\')\n        skip = tf.maximum(skip, 0, name=\'fix_skip\')\n      direction_numbers = tf.convert_to_tensor(\n          _compute_direction_numbers(dim), name=\'direction_numbers\')\n      # Number of digits actually needed for binary representation.\n      num_digits = tf.cast(\n          tf.math.ceil(\n              tf.math.log(tf.cast(skip + num_results + 1, tf.float64)) / _LN_2),\n          tf.int32)\n    # Direction numbers, reshaped and with the digits shifted as needed for the\n    # bitwise xor operations below. Note that here and elsewhere we use bit\n    # shifts rather than powers of two because exponentiating integers is not\n    # currently supported on GPUs.\n    direction_numbers = tf.bitwise.left_shift(\n        direction_numbers[:dim, :num_digits], tf.range(num_digits - 1, -1, -1))\n    direction_numbers = tf.expand_dims(tf.transpose(a=direction_numbers), 1)\n\n    # Build the binary matrix corresponding to the i values in Joe & Kuo[1]. It\n    # is a matrix of the binary representation of the numbers (skip+1, skip+2,\n    # ..., skip+num_results). For example, with skip=0 and num_results=6 the\n    # binary matrix (before expanding the dimension) looks like this:\n    # [[1 0 1 0 1 0]\n    #  [0 1 1 0 0 1]\n    #  [0 0 0 1 1 1]]\n    irange = tf.range(skip + 1, skip + num_results + 1)\n    dig_range = tf.expand_dims(tf.range(num_digits), 1)\n    binary_matrix = tf.bitwise.bitwise_and(\n        1, tf.bitwise.right_shift(irange, dig_range))\n    binary_matrix = tf.expand_dims(binary_matrix, -1)\n\n    # Multiply and bitwise-xor everything together. We use while_loop rather\n    # than foldl(bitwise_xor(...)) because the latter is not currently supported\n    # on GPUs.\n    product = direction_numbers * binary_matrix\n\n    def _cond(partial_result, i):\n      del partial_result\n      return i < num_digits\n\n    def _body(partial_result, i):\n      return tf.bitwise.bitwise_xor(partial_result, product[i, :, :]), i + 1\n\n    result, _ = tf.while_loop(_cond, _body, (product[0, :, :], 1))\n    # Shift back from integers to floats.\n    dtype = dtype or tf.float32\n    return (tf.cast(result, dtype)\n            / tf.cast(tf.bitwise.left_shift(1, num_digits), dtype))\n\n\n# TODO(b/135590027): Add option to store these instead of recomputing each time.\ndef _compute_direction_numbers(dim):\n  """"""Returns array of direction numbers for dimension dim.\n\n  These are the m_kj values in the Joe & Kuo notes[1], not the v_kj values. So\n  these refer to the \'abuse of notation\' mentioned in the notes -- it is a\n  matrix of integers, not floats. The variable names below are intended to match\n  the notation in the notes as closely as possible.\n\n  Args:\n    dim: int, dimension.\n\n  Returns:\n    `numpy.array` of direction numbers with `shape` [dim, 32].\n  """"""\n  m = np.empty((dim, 32), dtype=np.int32)\n  m[0, :] = np.ones(32, dtype=np.int32)\n  for k in range(dim - 1):\n    a_k = _PRIMITIVE_POLYNOMIAL_COEFFICIENTS[k]\n    deg = np.int32(np.floor(np.log2(a_k)))  # degree of polynomial\n    m[k + 1, :deg] = _INITIAL_DIRECTION_NUMBERS[:deg, k]\n    for j in range(deg, 32):\n      m[k + 1, j] = m[k + 1, j - deg]\n      for i in range(deg):\n        if (a_k >> i) & 1:\n          m[k + 1, j] = np.bitwise_xor(m[k + 1, j], m[k + 1, j - deg + i] <<\n                                       (deg - i))\n  return m\n\n\ndef _get_sobol_data_path():\n  """"""Returns path of file \'new-joe-kuo-6.21201\'.\n\n     Location of file \'new-joe-kuo-6.21201\' depends on the environment in\n     which this code is executed. In Google internal environment file\n     \'new-joe-kuo-6.21201\' is accessible using the\n     \'third_party/sobol_data/new-joe-kuo-6.21201\' file path.\n\n     However, this doesn\'t work in the pip package. In pip package the directory\n     \'third_party\' is a subdirectory of directory \'tf_quant_finance\' and in\n     this case we construct a file path relative to the __file__ file path.\n\n     If this library is installed in editable mode with `pip install -e .`, then\n     the directory \'third_party` will be at the top level of the repository\n     and need to search a level further up relative to __file__.\n  """"""\n  filename = \'new-joe-kuo-6.21201\'\n  path1 = os.path.join(\'third_party\', \'sobol_data\', filename)\n  path2 = os.path.abspath(\n      os.path.join(\n          os.path.dirname(__file__), \'..\', \'..\', \'..\',\n          \'third_party\', \'sobol_data\', filename))\n  path3 = os.path.abspath(\n      os.path.join(\n          os.path.dirname(__file__), \'..\', \'..\', \'..\', \'..\',\n          \'third_party\', \'sobol_data\', filename))\n\n  paths = [path1, path2, path3]\n  for path in paths:\n    if os.path.exists(path):\n      return path\n\n\ndef _load_sobol_data():\n  """"""Parses file \'new-joe-kuo-6.21201\'.""""""\n  path = _get_sobol_data_path()\n  if path is None:\n    logging.warning(\'Unable to find path to sobol data file.\')\n    return NotImplemented, NotImplemented\n  header_line = True\n  # Primitive polynomial coefficients.\n  polynomial_coefficients = np.zeros(shape=(21200,), dtype=np.int64)\n  # Initial direction numbers.\n  direction_numbers = np.zeros(shape=(18, 21200), dtype=np.int64)\n  index = 0\n  with open(path) as f:\n    for line in f:\n      # Skip first line (header).\n      if header_line:\n        header_line = False\n        continue\n      tokens = line.split()\n      s, a = tokens[1:3]\n      polynomial_coefficients[index] = 2**int(s) + 2 * int(a) + 1\n      for i, m_i in enumerate(tokens[3:]):\n        direction_numbers[i, index] = int(m_i)\n      index += 1\n  return polynomial_coefficients, direction_numbers\n\n\n(_PRIMITIVE_POLYNOMIAL_COEFFICIENTS,\n _INITIAL_DIRECTION_NUMBERS) = _load_sobol_data()\n'"
tf_quant_finance/math/random_ops/sobol/sobol_test.py,12,"b'# Lint as: python3\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for quasirandom.sobol.""""""\n\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\nfrom tf_quant_finance.math import random\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass SampleSobolSequenceTest(tf.test.TestCase):\n\n  def test_known_values_small_dimension(self):\n    # The first five elements of the non-randomized Sobol sequence\n    # with dimension 2\n    for dtype in [np.float16, np.float32, np.float64]:\n      sample = random.sobol.sample(2, 5, dtype=dtype)\n      # These are in the original order, not Gray code order.\n      expected = np.array([[0.5, 0.5], [0.25, 0.75], [0.75, 0.25],\n                           [0.125, 0.625], [0.625, 0.125]],\n                          dtype=dtype)\n      self.assertAllClose(expected, self.evaluate(sample), rtol=1e-6)\n      self.assertEqual(sample.dtype.as_numpy_dtype, dtype)\n\n  def test_more_known_values(self):\n    # The first 31 elements of the non-randomized Sobol sequence\n    # with dimension 5\n    sample = random.sobol.sample(5, 31)\n    # These are in the Gray code order.\n    expected = [[0.5, 0.5, 0.5, 0.5, 0.5], [0.75, 0.25, 0.25, 0.25, 0.75],\n                [0.25, 0.75, 0.75, 0.75, 0.25],\n                [0.375, 0.375, 0.625, 0.875, 0.375],\n                [0.875, 0.875, 0.125, 0.375, 0.875],\n                [0.625, 0.125, 0.875, 0.625, 0.625],\n                [0.125, 0.625, 0.375, 0.125, 0.125],\n                [0.1875, 0.3125, 0.9375, 0.4375, 0.5625],\n                [0.6875, 0.8125, 0.4375, 0.9375, 0.0625],\n                [0.9375, 0.0625, 0.6875, 0.1875, 0.3125],\n                [0.4375, 0.5625, 0.1875, 0.6875, 0.8125],\n                [0.3125, 0.1875, 0.3125, 0.5625, 0.9375],\n                [0.8125, 0.6875, 0.8125, 0.0625, 0.4375],\n                [0.5625, 0.4375, 0.0625, 0.8125, 0.1875],\n                [0.0625, 0.9375, 0.5625, 0.3125, 0.6875],\n                [0.09375, 0.46875, 0.46875, 0.65625, 0.28125],\n                [0.59375, 0.96875, 0.96875, 0.15625, 0.78125],\n                [0.84375, 0.21875, 0.21875, 0.90625, 0.53125],\n                [0.34375, 0.71875, 0.71875, 0.40625, 0.03125],\n                [0.46875, 0.09375, 0.84375, 0.28125, 0.15625],\n                [0.96875, 0.59375, 0.34375, 0.78125, 0.65625],\n                [0.71875, 0.34375, 0.59375, 0.03125, 0.90625],\n                [0.21875, 0.84375, 0.09375, 0.53125, 0.40625],\n                [0.15625, 0.15625, 0.53125, 0.84375, 0.84375],\n                [0.65625, 0.65625, 0.03125, 0.34375, 0.34375],\n                [0.90625, 0.40625, 0.78125, 0.59375, 0.09375],\n                [0.40625, 0.90625, 0.28125, 0.09375, 0.59375],\n                [0.28125, 0.28125, 0.15625, 0.21875, 0.71875],\n                [0.78125, 0.78125, 0.65625, 0.71875, 0.21875],\n                [0.53125, 0.03125, 0.40625, 0.46875, 0.46875],\n                [0.03125, 0.53125, 0.90625, 0.96875, 0.96875]]\n    # Because sobol.sample computes points in the original order,\n    # not Gray code order, we ignore the order and only check that the sets of\n    # rows are equal.\n    self.assertAllClose(\n        sorted(tuple(row) for row in expected),\n        sorted(tuple(row) for row in self.evaluate(sample)),\n        rtol=1e-6)\n\n  def test_skip(self):\n    dim = 10\n    n = 50\n    skip = 17\n    sample_noskip = random.sobol.sample(dim, n + skip)\n    sample_skip = random.sobol.sample(dim, n, skip)\n\n    self.assertAllClose(\n        self.evaluate(sample_noskip[skip:, :]), self.evaluate(sample_skip))\n\n  def test_normal_integral_mean_and_var_correctly_estimated(self):\n    n = int(1000)\n    # This test is almost identical to the similarly named test in\n    # monte_carlo_test.py. The only difference is that we use the Sobol\n    # samples instead of the random samples to evaluate the expectations.\n    # MC with pseudo random numbers converges at the rate of 1/ Sqrt(N)\n    # (N=number of samples). For QMC in low dimensions, the expected convergence\n    # rate is ~ 1/N. Hence we should only need 1e3 samples as compared to the\n    # 1e6 samples used in the pseudo-random monte carlo.\n    dtype = tf.float64\n    mu_p = tf.constant([-1., 1.], dtype=dtype)\n    mu_q = tf.constant([0., 0.], dtype=dtype)\n    sigma_p = tf.constant([0.5, 0.5], dtype=dtype)\n    sigma_q = tf.constant([1., 1.], dtype=dtype)\n    p = tfp.distributions.Normal(loc=mu_p, scale=sigma_p)\n    q = tfp.distributions.Normal(loc=mu_q, scale=sigma_q)\n\n    cdf_sample = random.sobol.sample(2, n, dtype=dtype)\n    q_sample = q.quantile(cdf_sample)\n\n    # Compute E_p[X].\n    e_x = tf.reduce_mean(q_sample * p.prob(q_sample) / q.prob(q_sample), 0)\n\n    # Compute E_p[X^2 - E_p[X]^2].\n    e_x2 = tf.reduce_mean(q_sample**2 * p.prob(q_sample) / q.prob(q_sample)\n                          - e_x**2, 0)\n    stddev = tf.sqrt(e_x2)\n\n    # Keep the tolerance levels the same as in monte_carlo_test.py.\n    self.assertEqual(p.batch_shape, e_x.shape)\n    self.assertAllClose(self.evaluate(p.mean()), self.evaluate(e_x), rtol=0.01)\n    self.assertAllClose(\n        self.evaluate(p.stddev()), self.evaluate(stddev), rtol=0.02)\n\n  def test_two_dimensional_projection(self):\n    # This test fails for Halton sequences, where two-dimensional projections of\n    # high dimensional samples are perfectly correlated. So with Halton samples,\n    # the integral below is incorrecly computed to be 1/3 rather than the\n    # correct 1/4.\n    dim = 170\n    n = 1000\n    sample = random.sobol.sample(dim, n)\n    x = self.evaluate(sample[:, dim - 2])\n    y = self.evaluate(sample[:, dim - 1])\n    corr = np.corrcoef(x, y)[1, 0]\n    self.assertAllClose(corr, 0.0, atol=0.05)\n    self.assertAllClose((x * y).mean(), 0.25, rtol=0.05)\n\n  def test_dim_should_be_positive(self):\n    """"""Error is trigerred if dim < 1.""""""\n    with self.assertRaises(ValueError):\n      self.evaluate(random.sobol.sample(0, 5, validate_args=True))\n\n  def test_skip_should_be_non_negative(self):\n    """"""Error is trigerred if skip < 0.""""""\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      self.evaluate(random.sobol.sample(2, 5, skip=-10, validate_args=True))\n\n  def test_num_results_should_be_positive(self):\n    """"""Error is trigerred if num_results < 1.""""""\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      self.evaluate(random.sobol.sample(2, 0, validate_args=True))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/models/heston/approximations/__init__.py,0,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Approximations to the Heston model.""""""\n\nfrom tf_quant_finance.models.heston.approximations.european_option import european_option_price\nfrom tensorflow.python.util.all_util import remove_undocumented  # pylint: disable=g-direct-tensorflow-import\n\n_allowed_symbols = [\n    \'european_option_price\',\n]\n\nremove_undocumented(__name__, _allowed_symbols)\n'"
tf_quant_finance/models/heston/approximations/european_option.py,46,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Method for semi-analytical Heston option price.""""""\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nfrom tf_quant_finance.math import integration\n\n_PI_ = np.pi\n_COMPOSITE_SIMPSONS_RULE = integration.IntegrationMethod.COMPOSITE_SIMPSONS_RULE\n\n\ndef european_option_price(*,\n                          strikes=None,\n                          expiries=None,\n                          is_call_options=None,\n                          variances=None,\n                          kappas=None,\n                          thetas=None,\n                          sigmas=None,\n                          rhos=None,\n                          spots=None,\n                          forwards=None,\n                          discount_rates=None,\n                          continuous_dividends=None,\n                          cost_of_carries=None,\n                          discount_factors=None,\n                          integration_method=None,\n                          dtype=None,\n                          name=None,\n                          **kwargs):\n  """"""Calculates European option prices under the Heston model.\n\n  Heston originally published in 1993 his eponymous model [3]. He provided\n  a semi- analytical formula for pricing European option via Fourier transform\n  under his model. However, as noted by Albrecher [1], the characteristic\n  function used in Heston paper can suffer numerical issues because of the\n  discontinuous nature of the square root function in the complex plane, and a\n  second version of the characteric function which doesn\'t suffer this\n  shortcoming should be used instead. Attari [2] further refined the numerical\n  method by reducing the number of numerical integrations (only one Fourier\n  transform instead of two) and with an integrand function decaying\n  quadratically instead of linearly. Attari\'s numerical method is implemented\n  here.\n\n  Heston model:\n  ```\n    dF/F = sqrt(V) * dW_1\n    dV = kappa * (theta - V) * dt * sigma * sqrt(V) * dW_2\n    <dW_1,dW_2> = rho *dt\n  ```\n  The variance V follows a square root process.\n\n  #### Example\n  ```python\n  import tf_quant_finance as tff\n  import numpy as np\n  prices = tff.models.heston.approximations.european_option_price(\n      variances=0.11,\n      strikes=102.0,\n      expiries=1.2,\n      forwards=100.0,\n      is_call_options=True,\n      kappas=2.0,\n      thetas=0.5,\n      sigmas=0.15,\n      rhos=0.3,\n      discount_factors=1.0,\n      dtype=np.float64)\n  # Expected print output of prices:\n  # 24.82219619\n  ```\n  #### References\n  [1] Hansjorg Albrecher, The Little Heston Trap\n  https://perswww.kuleuven.be/~u0009713/HestonTrap.pdf\n  [2] Mukarram Attari, Option Pricing Using Fourier Transforms: A Numerically\n  Efficient Simplification\n  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=520042\n  [3] Steven L. Heston, A Closed-Form Solution for Options with Stochastic\n  Volatility with Applications to Bond and Currency Options\n  http://faculty.baruch.cuny.edu/lwu/890/Heston93.pdf\n  Args:\n    strikes: A real `Tensor` of any shape and dtype. The strikes of the options\n      to be priced.\n    expiries: A real `Tensor` of the same dtype and compatible shape as\n      `strikes`.  The expiry of each option.\n    is_call_options: A boolean `Tensor` of a shape compatible with\n      `strikes`. Indicates whether the option is a call (if True) or a put\n      (if False). If not supplied, call options are assumed.\n    variances: A real `Tensor` of the same dtype and compatible shape as\n      `strikes`. The initial value of the variance.\n    kappas: A real `Tensor` of the same dtype and compatible shape as\n      `strikes`. The mean reversion strength of the variance square root\n      process.\n    thetas: A real `Tensor` of the same dtype and compatible shape as\n      `strikes`. The mean reversion level of the variance square root process.\n    sigmas: A real `Tensor` of the same dtype and compatible shape as\n      `strikes`. The volatility of the variance square root process (volatility\n      of volatility)\n    rhos: A real `Tensor` of the same dtype and compatible shape as\n      `strikes`. The correlation between spot and variance.\n        spots: A real `Tensor` of any shape that broadcasts to the shape of the\n      `volatilities`. The current spot price of the underlying. Either this\n      argument or the `forwards` (but not both) must be supplied.\n    forwards: A real `Tensor` of any shape that broadcasts to the shape of\n      `strikes`. The forwards to maturity. Either this argument or the\n      `spots` must be supplied but both must not be supplied.\n    discount_rates: An optional real `Tensor` of same dtype as the\n      `strikes` and of the shape that broadcasts with `strikes`.\n      If not `None`, discount factors are calculated as e^(-rT),\n      where r are the discount rates, or risk free rates. At most one of\n      discount_rates and discount_factors can be supplied.\n      Default value: `None`, equivalent to r = 0 and discount factors = 1 when\n      discount_factors also not given.\n    continuous_dividends: An optional real `Tensor` of same dtype as the\n      `strikes` and of the shape that broadcasts with `strikes`.\n      If not `None`, `cost_of_carries` is calculated as r - q,\n      where r are the `discount_rates` and q is `continuous_dividends`. Either\n      this or `cost_of_carries` can be given.\n      Default value: `None`, equivalent to q = 0.\n    cost_of_carries: An optional real `Tensor` of same dtype as the\n      `strikes` and of the shape that broadcasts with `strikes`.\n      Cost of storing a physical commodity, the cost of interest paid when\n      long, or the opportunity cost, or the cost of paying dividends when short.\n      If not `None`, and `spots` is supplied, used to calculate forwards from\n      `spots`: F = e^(bT) * S, where F is the forwards price, b is the cost of\n      carries, T is expiries and S is the spot price. If `None`, value assumed\n      to be equal to the `discount_rate` - `continuous_dividends`\n      Default value: `None`, equivalent to b = r.\n    discount_factors: An optional real `Tensor` of same dtype as the\n      `strikes`. If not `None`, these are the discount factors to expiry\n      (i.e. e^(-rT)). Mutually exclusive with discount_rate and cost_of_carry.\n      If neither is given, no discounting is applied (i.e. the undiscounted\n      option price is returned). If `spots` is supplied and `discount_factors`\n      is not `None` then this is also used to compute the forwards to expiry.\n      At most one of discount_rates and discount_factors can be supplied.\n      Default value: `None`, which maps to -log(discount_factors) / expiries\n    integration_method: An instance of `math.integration.IntegrationMethod`.\n      Default value: `None` which maps to the Simpsons integration rule.\n    dtype: Optional `tf.DType`. If supplied, the dtype to be used for conversion\n      of any supplied non-`Tensor` arguments to `Tensor`.\n      Default value: None which maps to the default dtype inferred by\n      TensorFlow.\n    name: str. The name for the ops created by this function.\n      Default value: None which is mapped to the default name\n      `heston_price`.\n    **kwargs: Additional parameters for the underlying integration method.\n      If not supplied and `integration_method` is Simpson, then uses\n      `IntegrationMethod.COMPOSITE_SIMPSONS_RULE` with `num_points=1001`, and\n      bounds `lower=1e-9`, `upper=100`.\n  Returns:\n    A `Tensor` of the same shape as the input data which is the price of\n    European options under the Heston model.\n  """"""\n  if (spots is None) == (forwards is None):\n    raise ValueError(\'Either spots or forwards must be supplied but not both.\')\n  if (discount_rates is not None) and (discount_factors is not None):\n    raise ValueError(\'At most one of discount_rates and discount_factors may \'\n                     \'be supplied\')\n  if (continuous_dividends is not None) and (cost_of_carries is not None):\n    raise ValueError(\'At most one of continuous_dividends and cost_of_carries \'\n                     \'may be supplied\')\n\n  with tf.compat.v1.name_scope(name, default_name=\'eu_option_price\'):\n    strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\'strikes\')\n    dtype = strikes.dtype\n    expiries = tf.convert_to_tensor(expiries, dtype=dtype, name=\'expiries\')\n    kappas = tf.convert_to_tensor(kappas, dtype=dtype, name=\'kappas\')\n    thetas = tf.convert_to_tensor(thetas, dtype=dtype, name=\'thetas\')\n    sigmas = tf.convert_to_tensor(sigmas, dtype=dtype, name=\'sigmas\')\n    rhos = tf.convert_to_tensor(rhos, dtype=dtype, name=\'rhos\')\n    variances = tf.convert_to_tensor(variances, dtype=dtype, name=\'variances\')\n    forwards = tf.convert_to_tensor(forwards, dtype=dtype, name=\'forwards\')\n\n    if discount_factors is not None:\n      discount_factors = tf.convert_to_tensor(\n          discount_factors, dtype=dtype, name=\'discount_factors\')\n\n    if discount_rates is not None:\n      discount_rates = tf.convert_to_tensor(\n          discount_rates, dtype=dtype, name=\'discount_rates\')\n    elif discount_factors is not None:\n      discount_rates = -tf.math.log(discount_factors) / expiries\n    else:\n      discount_rates = tf.convert_to_tensor(\n          0.0, dtype=dtype, name=\'discount_rates\')\n\n    if continuous_dividends is None:\n      continuous_dividends = tf.convert_to_tensor(\n          0.0, dtype=dtype, name=\'continuous_dividends\')\n\n    if cost_of_carries is not None:\n      cost_of_carries = tf.convert_to_tensor(\n          cost_of_carries, dtype=dtype, name=\'cost_of_carries\')\n    else:\n      cost_of_carries = discount_rates - continuous_dividends\n\n    if discount_factors is None:\n      discount_factors = tf.exp(-discount_rates * expiries)  # pylint: disable=invalid-unary-operand-type\n\n    if forwards is not None:\n      forwards = tf.convert_to_tensor(forwards, dtype=dtype, name=\'forwards\')\n    else:\n      spots = tf.convert_to_tensor(spots, dtype=dtype, name=\'spots\')\n      forwards = spots * tf.exp(cost_of_carries * expiries)\n\n    # Cast as complex for the characteristic function calculation\n    expiries_real = tf.complex(expiries, tf.zeros_like(expiries))\n    kappas_real = tf.complex(kappas, tf.zeros_like(kappas))\n    thetas_real = tf.complex(thetas, tf.zeros_like(thetas))\n    sigmas_real = tf.complex(sigmas, tf.zeros_like(sigmas))\n    rhos_real = tf.complex(rhos, tf.zeros_like(rhos))\n    variances_real = tf.complex(variances, tf.zeros_like(variances))\n\n    # Prepare inputs to build an integrand_function\n    expiries_real = tf.expand_dims(expiries_real, -1)\n    kappas_real = tf.expand_dims(kappas_real, -1)\n    thetas_real = tf.expand_dims(thetas_real, -1)\n    sigmas_real = tf.expand_dims(sigmas_real, -1)\n    rhos_real = tf.expand_dims(rhos_real, -1)\n    variances_real = tf.expand_dims(variances_real, -1)\n    if integration_method is None:\n      integration_method = _COMPOSITE_SIMPSONS_RULE\n    if integration_method == _COMPOSITE_SIMPSONS_RULE:\n      if \'num_points\' not in kwargs:\n        kwargs[\'num_points\'] = 1001\n      if \'lower\' not in kwargs:\n        kwargs[\'lower\'] = 1e-9\n      if \'upper\' not in kwargs:\n        kwargs[\'upper\'] = 100\n    def char_fun(u):\n      # Using \'second formula\' for the (first) characteristic function of\n      # log( spot_T / forwards )\n      # (noted \'phi_2\' in \'The Little Heston Trap\', (Albrecher))\n      u_real = tf.complex(u, tf.zeros_like(u))\n      u_imag = tf.complex(tf.zeros_like(u), u)\n      s = rhos_real * sigmas_real * u_imag\n      # TODO(b/156221007): investigate why s_kappa = (s - kappas_real)**2 leads\n      # to a wrong result in graph mode.\n      s_kappa = (s - kappas_real) * s - (s - kappas_real) * kappas_real\n      d = s_kappa - sigmas_real ** 2 * (-u_imag - u_real ** 2)\n      d = tf.math.sqrt(d)\n      g = (kappas_real - s - d) / (kappas_real - s + d)\n      a = kappas_real * thetas_real\n      h = g * tf.math.exp(-d * expiries_real)\n      m = 2 * tf.math.log((1 - h) / (1 - g))\n      c = (a / sigmas_real ** 2) * ((kappas_real - s - d) * expiries_real - m)\n      e = (1 - tf.math.exp(-d * expiries_real))\n      d_new = (kappas_real - s - d) / sigmas_real ** 2 * (e / (1 - h))\n      return tf.math.exp(c + d_new * variances_real)\n\n    def integrand_function(u, k):\n      # Note that with [2], integrand is in 1 / u**2,\n      # which converges faster than Heston 1993 (which is in 1 /u)\n      char_fun_complex = char_fun(u)\n      char_fun_real_part = tf.math.real(char_fun_complex)\n      char_fun_imag_part = tf.math.imag(char_fun_complex)\n\n      a = (char_fun_real_part + char_fun_imag_part / u) * tf.math.cos(u * k)\n      b = (char_fun_imag_part - char_fun_real_part / u) * tf.math.sin(u * k)\n\n      return (a + b) / (1.0 + u * u)\n\n    k = tf.expand_dims(tf.math.log(strikes / forwards), axis=-1)\n\n    integral = integration.integrate(\n        lambda u: integrand_function(u, k),\n        method=integration_method,\n        dtype=dtype,\n        **kwargs)\n    undiscounted_call_prices = forwards - strikes * (0.5 + integral / _PI_)\n\n    if is_call_options is None:\n      return undiscounted_call_prices * discount_factors\n    else:\n      is_call_options = tf.convert_to_tensor(is_call_options, dtype=tf.bool,\n                                             name=\'is_call_options\')\n      # Use call-put parity for Put\n      undiscounted_put_prices = undiscounted_call_prices - forwards + strikes\n\n      undiscount_prices = tf.where(\n          is_call_options,\n          undiscounted_call_prices,\n          undiscounted_put_prices)\n      return undiscount_prices * discount_factors\n'"
tf_quant_finance/models/heston/approximations/european_option_test.py,2,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for Heston Price method.""""""\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\nimport tf_quant_finance as tff\n\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass HestonPriceTest(parameterized.TestCase, tf.test.TestCase):\n  """"""Tests for Heston Price method.""""""\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64,\n      },\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32,\n      })\n  def test_docstring(self, dtype):\n    prices = tff.models.heston.approximations.european_option_price(\n        variances=0.11,\n        strikes=102.0,\n        expiries=1.2,\n        forwards=100.0,\n        is_call_options=True,\n        kappas=2.0,\n        thetas=0.5,\n        sigmas=0.15,\n        rhos=0.3,\n        discount_factors=1.0,\n        dtype=dtype)\n    # Computed using scipy\n    self.assertAllClose(prices, 24.822196, rtol=1e-5, atol=1e-5)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'DoublePrecision\',\n          \'dtype\': np.float64,\n      },\n      {\n          \'testcase_name\': \'SinglePrecision\',\n          \'dtype\': np.float32,\n      })\n  def test_heston_price(self, dtype):\n    kappas = np.array([0.1, 10.0], dtype=dtype)\n    thetas = np.array([0.1, 0.5], dtype=dtype)\n    variances = np.array([0.1, 0.5], dtype=dtype)\n    forwards = np.array([10.0], dtype=dtype)\n    sigmas = np.array([1.0, 0.9], dtype=dtype)\n    strikes = np.array([9.7, 10.0], dtype=dtype)\n    expiries = np.array([1.0], dtype=dtype)\n    discount_factors = np.array([0.99, 0.98], dtype=dtype)\n    rhos = np.array([0.5, 0.1], dtype=dtype)\n\n    tff_prices = self.evaluate(\n        tff.models.heston.approximations.european_option_price(\n            kappas=kappas,\n            thetas=thetas,\n            sigmas=sigmas,\n            rhos=rhos,\n            variances=variances,\n            forwards=forwards,\n            expiries=expiries,\n            strikes=strikes,\n            discount_factors=discount_factors,\n            is_call_options=np.asarray([True, False], dtype=np.bool)))\n    # Computed using scipy\n    scipy_prices = [1.07475678, 2.708217]\n    np.testing.assert_allclose(\n        tff_prices,\n        scipy_prices, rtol=1e-5, atol=1e-5)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_quant_finance/examples/demos/option_pricing_basic/common/__init__.py,0,b''
tf_quant_finance/examples/demos/option_pricing_basic/common/datatypes.py,0,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Common data types needed across multiple containers in this project.""""""\n\nimport dataclasses\nimport numpy as np\n\n\n@dataclasses.dataclass\nclass OptionBatch:\n  """"""Represents a batch of options.""""""\n\n  strike: np.ndarray  # float64 array.\n  call_put_flag: np.ndarray  # Boolean array. True if Call, False otherwise.\n  expiry_date: np.ndarray  # int array containing date ordinals.\n  trade_id: np.ndarray  # int32 array.\n  underlier_id: np.ndarray  # int32 array. The identifier for the underlying.\n\n\n@dataclasses.dataclass\nclass OptionMarketData:\n  """"""Represents market data to be used to price the batch of options.""""""\n\n  underlier_id: np.ndarray  # int32 array. The identifier for an underlying.\n  spot: np.ndarray  # double array. The spot price of the underlier.\n  volatility: np.ndarray  # double array. The volatility of the underlier.\n  rate: np.ndarray  # double array. The risk free rate.\n\n\n@dataclasses.dataclass\nclass ComputeData:\n  """"""Carries a compute request from the downloader to the calculator.""""""\n\n  market_data_path: str  # Path to the market data file to use.\n  portfolio_path: str  # Path to the portfolio file to compute.\n'"
tf_quant_finance/examples/demos/option_pricing_basic/data_generator/data_generation.py,0,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A binary to generate random options data.\n\nNote that Google Cloud SDK, storage client library and tf_quant_finance should\nalready be installed (see `requirements.txt`). In case the generated data needs\nto be written to GCS, authentication steps must be performed prior to running\nthis script as described at:\nhttps://cloud.google.com/storage/docs/reference/libraries\n\nThis script is meant to be run from inside a docker container\n(see the Dockerfile). If running outside a container, the PYTHONPATH must be\nmodified to include the parent folder so the modules in the `common` folder are\nvisible. For example, `PYTHONPATH=$PYTHONPATH:/my/path/option_pricing_basic/`.\n""""""\n\nimport datetime\nfrom os import path\nimport tempfile\nfrom typing import Tuple, List\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nfrom common import datatypes\nimport dataclasses\nimport numpy as np\nimport tf_quant_finance as tff\n\nfrom google.cloud import storage\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    \'output_path\', \'\',\n    \'Location where the output data is stored. If the path \'\n    \'begins with gs:// it is assumed to be GCS. If a GCS path is \'\n    \'specified, temporary local files will be created in a platform \'\n    \'specific temporary directory.\')\nflags.DEFINE_integer(\'num_underliers\', 1000, \'Number of different underliers \'\n                     \'for the options.\')\nflags.DEFINE_integer(\'options_per_file\', 1000000,\n                     \'Number of options in each file.\')\nflags.DEFINE_integer(\'num_files\', 50, \'Number of files to generate.\')\nflags.DEFINE_string(\n    \'output_file_prefix\', \'portfolio\',\n    \'Prefix for generating portfolio file names. The filename will \'\n    \'be generated by adding a number to the prefix \'\n    \'(e.g. portfolio_1-of-20.tfrecords).\')\nflags.DEFINE_string(\n    \'market_data_file_name\', \'market_data.tfrecords\',\n    \'Name of the market data file. \'\n    \'It will be stored in the same location as the portfolio file.\')\nflags.DEFINE_integer(\'random_seed\', None,\n                     \'An optional random seed to control the data generation.\')\n\n\ndef is_gcs_path(gcs_path: str) -> bool:\n  return gcs_path.startswith(\'gs://\')\n\n\ndef generate_market_data(num_underliers: int) -> datatypes.OptionMarketData:\n  spots = np.random.rand(num_underliers) * 900 + 100  # Between 100.0 and 1000.0\n  volatility = np.random.rand(\n      num_underliers) * 0.57 + 0.03  # Between 3% and 60%\n  rate = np.random.rand(num_underliers) * 0.15  # Between 0% and 15%.\n  return datatypes.OptionMarketData(\n      underlier_id=np.arange(num_underliers),\n      spot=spots,\n      volatility=volatility,\n      rate=rate)\n\n\ndef generate_portfolio(num_instruments: int,\n                       market_data: datatypes.OptionMarketData,\n                       start_instrument_id: int = 0) -> datatypes.OptionBatch:\n  """"""Generates a random portfolio.""""""\n  underlier_ids = np.random.choice(\n      market_data.underlier_id, size=num_instruments)\n  # Choose strikes to be within +/- 40% of the spot.\n  call_put_flags = np.random.rand(num_instruments) > 0.5\n  # datetime.date.toordinal uses the same ordinals as tff\'s dates module.\n  today = datetime.date.today().toordinal()\n  expiry_date = today + np.random.randint(\n      1, high=365 * 10, size=num_instruments)\n  strike_multipliers = np.random.rand(num_instruments) * 0.8 + 0.6\n  underlier_spots = market_data.spot[underlier_ids]\n  strikes = underlier_spots * strike_multipliers\n  trade_ids = np.arange(num_instruments) + start_instrument_id\n  return datatypes.OptionBatch(\n      strike=strikes,\n      call_put_flag=call_put_flags,\n      expiry_date=expiry_date,\n      trade_id=trade_ids,\n      underlier_id=underlier_ids)\n\n\ndef split_bucket_path(gcs_path: str) -> Tuple[str, str]:\n  gcs_path = gcs_path.replace(\'gs://\', \'\')  # Remove the prefix.\n  pieces = gcs_path.split(\'/\')\n  bucket_name = pieces[0]\n  remainder = \'\' if len(pieces) == 1 else path.join(*pieces[1:])\n  return bucket_name, remainder\n\n\ndef upload_to_gcs(gcs_path: str, local_files: List[str]) -> None:\n  """"""Uploads data to a gcs bucket.""""""\n  client = storage.Client()\n  bucket_name, gcs_path = split_bucket_path(gcs_path)\n  bucket = client.bucket(bucket_name)\n  for local_path in local_files:\n    file_name = path.basename(local_path)\n    full_blob_name = path.join(gcs_path, file_name)\n    target_blob = bucket.blob(full_blob_name)\n    target_blob.upload_from_filename(local_path)\n    logging.info(\'Uploaded %s to gs://%s/%s.\',\n                 local_path, bucket_name, full_blob_name)\n\n\ndef write_local(local_file_path: str, data_class: object) -> None:\n  """"""Writes a dataclass object to file.""""""\n  if not dataclasses.is_dataclass(data_class):\n    raise ValueError(\'Object to write must be a dataclass\')\n  data = dataclasses.asdict(data_class)\n  with tff.experimental.io.ArrayDictWriter(local_file_path) as writer:\n    writer.write(data)\n\n\ndef main(argv):\n  """"""Generates synthetic pricing data.""""""\n  del argv\n  np.random.seed(seed=FLAGS.random_seed)\n  # Generate and write market data.\n  output_path = FLAGS.output_path\n  local_directory = (tempfile.gettempdir() if is_gcs_path(output_path)\n                     else output_path)\n  market_data_file_path = path.join(local_directory,\n                                    FLAGS.market_data_file_name)\n  num_underliers = FLAGS.num_underliers\n  market_data = generate_market_data(num_underliers)\n  write_local(market_data_file_path, market_data)\n  logging.info(\'Wrote market data to %s\', market_data_file_path)\n  # Generate the portfolio conditioned on the market data.\n  num_files = FLAGS.num_files\n  output_file_prefix = FLAGS.output_file_prefix\n  file_name_gen = (\n      lambda i: f\'{output_file_prefix}_{i+1}-of-{num_files}.tfrecords\')\n  local_full_path_gen = lambda i: path.join(local_directory, file_name_gen(i))\n\n  local_paths = []\n  options_per_file = FLAGS.options_per_file\n  for i in range(num_files):\n    start_id = i * options_per_file\n    portfolio = generate_portfolio(\n        options_per_file, market_data, start_instrument_id=start_id)\n    local_path = local_full_path_gen(i)\n    write_local(local_path, portfolio)\n    logging.info(\'Wrote portfolio shard to %s\', local_path)\n    local_paths.append(local_path)\n\n  # Upload stuff to GCS\n  if is_gcs_path(output_path):\n    logging.info(\'Uploading data to gcs\')\n    upload_to_gcs(output_path, [market_data_file_path])\n    upload_to_gcs(output_path, local_paths)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_quant_finance/examples/demos/option_pricing_basic/downloader/app/main.py,0,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Script to handle file processing requests and queuing work.\n\nThis binary does two main things:\n\n1. It listens for job requests. These contain the gcs path to portfolio and\n  market data files to be processed.\n2. It downloads those file and queues it for downstream processing.\n\nAn example call using curl\ncurl --header ""Content-Type: application/json"" --request POST \\\n  --data \'{""market-data-file"": ""gs://BUCKET_NAME/market_data_file_name"", \\\n    ""portfolio-file"": ""gs://BUCKET_NAME/portfolio_file_name"", \\\n      ""force-refresh"": true }\' http://localhost:8080/jobreq\n""""""  # pylint: disable=g-docstring-has-escape\n\nimport os\nfrom os import path\nimport time\nfrom typing import Dict, List, Tuple, Union\n\nfrom common import datatypes\nimport dataclasses\nimport flask\nimport zmq\n\nfrom google.cloud import storage\n\napp = flask.Flask(__name__)\n\nDOWNLOAD_BASE_PATH = \'/var/tmp/downloads\'\n\n# Communicates with the container doing the calculations at IPC_PATH + IPC_NAME.\n# To use shared memory, IPC_PATH can be changed to /dev/shm (and enabling it in\n# docker run).\nIPC_PATH = \'/var/tmp/ipc\'\nIPC_NAME = \'jobs\'\n\n# Port at which it receives requests.\nPORT = os.environ.get(\'PORT\') or 8080\n\napp.logger.setLevel(\'INFO\')\napp.logger.info(f\'Downloaded files will be saved at {DOWNLOAD_BASE_PATH}\')\n\nif not path.exists(DOWNLOAD_BASE_PATH):\n  os.makedirs(DOWNLOAD_BASE_PATH)\n\nif not path.exists(IPC_PATH):\n  os.makedirs(IPC_PATH)\n\n\n#\xc2\xa0Initialize the IPC socket.\ndef init_socket():\n  context = zmq.Context()\n  sender = context.socket(zmq.PUSH)\n  channel = \'ipc://\' + path.join(IPC_PATH, IPC_NAME)\n  app.logger.info(f\'Pricer requests will be sent at {channel}\')\n  sender.bind(channel)\n  return sender\n\n\nSENDER = init_socket()\n\n\n@dataclasses.dataclass\nclass JobRequest:\n  """"""Specifies the request expected by the process route.\n\n  Note that all underscores in field names are replaced by dashes in the JSON.\n  """"""\n  # Full GCS path (with gs:// prefix) to portfolio file.\n  portfolio_file: str\n  # Full GCS path (with gs:// prefix) to market data.\n  market_data_file: str\n  # Whether to force download of the files even if they exist locally.\n  force_refresh: bool = False\n\n  @classmethod\n  def from_dict(cls, data: Dict[str, Union[str, bool]]) -> \'JobRequest\':\n    del cls\n    return JobRequest(\n        portfolio_file=data.get(\'portfolio-file\', \'\'),\n        market_data_file=data.get(\'market-data-file\', \'\'),\n        force_refresh=data.get(\'force-refresh\', False))\n\n  def to_dict(self) -> Dict[str, Union[str, bool]]:\n    return {\n        \'portfolio-file\': self.portfolio_file,\n        \'market-data-file\': self.market_data_file,\n        \'force-refresh\': self.force_refresh\n    }\n\n  def validate(self) -> Tuple[bool, str]:\n    """"""Validates the data in the request.""""""\n    is_ok = True\n    messages: List[str] = []\n    if not self.portfolio_file:\n      is_ok = False\n      messages.append(\'Portfolio file must be specified.\')\n    if not self.market_data_file:\n      is_ok = False\n      messages.append(\'Market data file must be specified.\')\n    return is_ok, \' \'.join(messages)\n\n  def market_data_local_path(self) -> str:\n    return self._local_path_from_gcs_path(self.market_data_file)\n\n  def portfolio_local_path(self) -> str:\n    return self._local_path_from_gcs_path(self.portfolio_file)\n\n  def market_data_exists_locally(self) -> bool:\n    return path.exists(self.market_data_local_path())\n\n  def portfolio_exists_locally(self) -> bool:\n    return path.exists(self.portfolio_local_path())\n\n  def download_if_needed(self) -> str:\n    """"""Downloads specified data and returns an error message if any.""""""\n    is_valid, err_message = self.validate()\n    if not is_valid:\n      return err_message\n    if (not self.force_refresh and self.market_data_exists_locally() and\n        self.portfolio_exists_locally()):\n      return \'\'\n    market_data_path = self.market_data_local_path()\n    self._ensure_base_path_exists(market_data_path)\n    self._download_from_gcs(self.market_data_file, market_data_path)\n    portfolio_data_path = self.portfolio_local_path()\n    self._ensure_base_path_exists(portfolio_data_path)\n    self._download_from_gcs(self.portfolio_file, portfolio_data_path)\n    return \'\'\n\n  def _split_gcs_path(self, gcs_path: str) -> Tuple[str, str]:\n    gcs_path = gcs_path.replace(\'gs://\', \'\')  # Remove the prefix.\n    pieces = gcs_path.split(\'/\')\n    bucket_name = pieces[0]\n    remainder = \'\' if len(pieces) == 1 else path.join(*pieces[1:])\n    return bucket_name, remainder\n\n  def _ensure_base_path_exists(self, local_file_path: str) -> None:\n    dir_path = os.path.dirname(local_file_path)\n    if not path.exists(dir_path):\n      os.makedirs(dir_path)\n\n  def _download_from_gcs(self, gcs_file: str, local_file: str):\n    client = storage.Client()\n    bucket_name, blob_name = self._split_gcs_path(gcs_file)\n    bucket = client.bucket(bucket_name)\n    target_blob = bucket.blob(blob_name)\n    start_time = time.time()\n    target_blob.download_to_filename(local_file)\n    end_time = time.time()\n    app.logger.info(\'Downloaded %s to %s in %f seconds.\', gcs_file, local_file,\n                    end_time - start_time)\n\n  def _local_path_from_gcs_path(self, gcs_path: str) -> str:\n    return path.join(DOWNLOAD_BASE_PATH, gcs_path.replace(\'gs://\', \'\'))\n\n\ndef _make_response(status, message):\n  return flask.jsonify(dict(status=status, message=message))\n\n\n@app.route(\'/jobreq\', methods=[\'POST\'])\ndef forward_request():\n  """"""Forwards incoming job requests to the pricer.""""""\n  request_data = flask.request.get_json()\n  job_request = JobRequest.from_dict(request_data)\n  err = job_request.download_if_needed()\n  if err:\n    return _make_response(\'Error\', err), 400\n  else:\n    # Send work to the compute process.\n    app.logger.info(\'Forwarding compute request to backend.\')\n    data_obj = datatypes.ComputeData(\n        market_data_path=job_request.market_data_local_path(),\n        portfolio_path=job_request.portfolio_local_path())\n    SENDER.send_pyobj(data_obj)\n    app.logger.info(\'Sent request to backend.\')\n    return _make_response(\'OK\', \'\'), 201\n\n\n@app.route(\'/\')\ndef is_alive():\n  return \'It is alive\\n\', 200\n\n\nif __name__ == \'__main__\':\n  app.run(debug=False, host=\'0.0.0.0\', port=PORT)\n'"
tf_quant_finance/examples/demos/option_pricing_basic/pricer/app/main.py,0,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Computes option prices using TensorFlow Finance.""""""\n\nimport os\nfrom os import path\n\nfrom absl import app\nfrom absl import logging\nfrom common import datatypes\nimport pricers\nimport tf_quant_finance as tff\nimport zmq\n\nRESULTS_BASE_PATH = \'/var/tmp/results\'\n\nif not path.exists(RESULTS_BASE_PATH):\n  os.makedirs(RESULTS_BASE_PATH)\n\n# Communicates with the container doing the calculations at IPC_PATH + IPC_NAME.\n# To use shared memory, IPC_PATH can be changed to /dev/shm (and enabling it in\n# docker run).\nIPC_PATH = \'/var/tmp/ipc\'\nIPC_NAME = \'jobs\'\n\n# The size of the option batches to be priced. If this is 0 or negative,\n# then variable sized inputs can be supplied but it is more efficient to set\n# an explicit batch size here.\nBATCH_SIZE = os.environ.get(\'OPTION_BATCH_SIZE\', 1000000)\n\n# The number of assets on which the options are written. If this is 0 or\n# negative, the market data can contain variable number of assets (across\n# different job requests). It is more efficient to set an explicit value here.\nNUM_ASSETS = os.environ.get(\'NUM_ASSETS\', 1000)\n\nlogging.set_verbosity(logging.INFO)\n\n\ndef main(argv):\n  del argv\n  batch_size = None if BATCH_SIZE <= 0 else BATCH_SIZE\n  num_assets = None if NUM_ASSETS <= 0 else NUM_ASSETS\n  pricer = pricers.TffOptionPricer(batch_size=batch_size, num_assets=num_assets)\n  context = zmq.Context()\n  receiver = context.socket(zmq.PULL)\n  channel = \'ipc://\' + path.join(IPC_PATH, IPC_NAME)\n  receiver.connect(channel)\n  logging.info(\'Pricer ready and listening at %s\', channel)\n\n  while True:\n    logging.info(\'Waiting for inputs...\')\n    inputs: datatypes.ComputeRequest = receiver.recv_pyobj()\n    print(\'Received input data...\')\n    with pricers.Timer() as read_timer:\n      market_data = tff.experimental.io.ArrayDictReader(\n          inputs.market_data_path).next()\n      portfolio = tff.experimental.io.ArrayDictReader(\n          inputs.portfolio_path).next()\n    logging.info(\'Read input data in %f ms\', read_timer.elapsed_ms)\n    with pricers.Timer() as compute_timer:\n      prices = pricer.price(market_data[\'spot\'], market_data[\'volatility\'],\n                            market_data[\'rate\'], portfolio[\'underlier_id\'],\n                            portfolio[\'strike\'], portfolio[\'call_put_flag\'],\n                            portfolio[\'expiry_date\'])\n      results = {\'trade_id\': portfolio[\'trade_id\'], \'prices\': prices}\n      output_path = path.join(RESULTS_BASE_PATH,\n                              \'results_\' + path.basename(inputs.portfolio_path))\n    logging.info(\'Computed prices in %f ms\', compute_timer.elapsed_ms)\n    with pricers.Timer() as write_timer:\n      with tff.experimental.io.ArrayDictWriter(output_path) as writer:\n        writer.write(results)\n    logging.info(\'Wrote output results to %s in %f ms\', output_path,\n                 write_timer.elapsed_ms)\n\n    total_ms = (\n        read_timer.elapsed_ms + compute_timer.elapsed_ms +\n        write_timer.elapsed_ms)\n    logging.info(\'Processed request of size %d in %f total ms.\',\n                 portfolio[\'underlier_id\'].size, total_ms)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_quant_finance/examples/demos/option_pricing_basic/pricer/app/pricers.py,5,"b'# Lint as: python3\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Computes option prices using TensorFlow Finance.""""""\n\nimport datetime\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nimport tf_quant_finance as tff\n\n\nclass Timer:\n  """"""A simple timer.""""""\n\n  def __init__(self):\n    self.start_time = 0\n    self.end_time = 0\n\n  def __enter__(self) -> ""Timer"":\n    self.start_time = time.time()\n    return self\n\n  def __exit__(self, unused_type, unused_value, unused_traceback):\n    del unused_type, unused_value, unused_traceback\n    self.end_time = time.time()\n\n  @property\n  def elapsed_ms(self) -> float:\n    """"""Returns the elapsed time in milliseconds.""""""\n    return (self.end_time - self.start_time) * 1000\n\n\ndef _price(spot_mkt, vol_mkt, rate_mkt, underliers, strikes, call_put_flag,\n           expiry_ordinals):\n  """"""Prices the options.""""""\n  # Get mkt data for each option\n  spots = tf.gather(spot_mkt, underliers)\n  vols = tf.gather(vol_mkt, underliers)\n  rates = tf.gather(rate_mkt, underliers)\n  # Convert expiries into time.\n  expiry_ordinals = tf.cast(expiry_ordinals, dtype=tf.int32)\n  expiry_dates = tff.datetime.dates_from_ordinals(expiry_ordinals)\n  pricing_date = tff.datetime.dates_from_datetimes([datetime.date.today()])\n  expiry_times = tff.datetime.daycount_actual_360(\n      start_date=pricing_date, end_date=expiry_dates, dtype=np.float64)\n  prices = tff.black_scholes.option_price(\n      volatilities=vols,\n      strikes=strikes,\n      expiries=expiry_times,\n      spots=spots,\n      discount_rates=rates,\n      is_call_options=call_put_flag)\n  return prices\n\n\nclass TffOptionPricer:\n  """"""Prices options using TFF.""""""\n\n  def __init__(self, batch_size=1000000, num_assets=1000):\n    dtype = np.float64\n    self._pricer = tf.function(_price)\n    # Do a warm-up. This initializes a bunch of stuff which improves performance\n    # at request serving time.\n    if batch_size is not None and num_assets is not None:\n      self._pricer(\n          np.zeros([num_assets], dtype=dtype),  # spot_mkt\n          np.zeros([num_assets], dtype=dtype),  # vol_mkt\n          np.zeros([num_assets], dtype=dtype),  # rate_mkt\n          np.zeros([batch_size], dtype=np.int32),  # underliers\n          np.zeros([batch_size], dtype=dtype),  # strikes\n          np.zeros([batch_size], dtype=np.bool),  # call_put_flag\n          np.ones([batch_size], dtype=np.int32))  # expiry_ordinals\n\n  def price(self, spot_mkt, vol_mkt, rate_mkt, underliers, strikes,\n            call_put_flag, expiry_ordinals):\n    """"""Prices options.""""""\n    prices = self._pricer(spot_mkt, vol_mkt, rate_mkt, underliers, strikes,\n                          call_put_flag, expiry_ordinals)\n    return prices\n'"
