file_path,api_count,code
1-1.NNLM/NNLM-Tensor.py,15,"b'# code by Tae Hwan Jung @graykode\nimport tensorflow as tf\nimport numpy as np\n\ntf.reset_default_graph()\n\nsentences = [ ""i like dog"", ""i love coffee"", ""i hate milk""]\n\nword_list = "" "".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\nnumber_dict = {i: w for i, w in enumerate(word_list)}\nn_class = len(word_dict) # number of Vocabulary\n\n# NNLM Parameter\nn_step = 2 # number of steps [\'i like\', \'i love\', \'i hate\']\nn_hidden = 2 # number of hidden units\n\ndef make_batch(sentences):\n    input_batch = []\n    target_batch = []\n\n    for sen in sentences:\n        word = sen.split()\n        input = [word_dict[n] for n in word[:-1]]\n        target = word_dict[word[-1]]\n\n        input_batch.append(np.eye(n_class)[input])\n        target_batch.append(np.eye(n_class)[target])\n\n    return input_batch, target_batch\n\n# Model\nX = tf.placeholder(tf.float32, [None, n_step, n_class]) # [batch_size, number of steps, number of Vocabulary]\nY = tf.placeholder(tf.float32, [None, n_class])\n\ninput = tf.reshape(X, shape=[-1, n_step * n_class]) # [batch_size, n_step * n_class]\nH = tf.Variable(tf.random_normal([n_step * n_class, n_hidden]))\nd = tf.Variable(tf.random_normal([n_hidden]))\nU = tf.Variable(tf.random_normal([n_hidden, n_class]))\nb = tf.Variable(tf.random_normal([n_class]))\n\ntanh = tf.nn.tanh(d + tf.matmul(input, H)) # [batch_size, n_hidden]\nmodel = tf.matmul(tanh, U) + b # [batch_size, n_class]\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\noptimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\nprediction =tf.argmax(model, 1)\n\n# Training\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\ninput_batch, target_batch = make_batch(sentences)\n\nfor epoch in range(5000):\n    _, loss = sess.run([optimizer, cost], feed_dict={X: input_batch, Y: target_batch})\n    if (epoch + 1)%1000 == 0:\n        print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n\n# Predict\npredict =  sess.run([prediction], feed_dict={X: input_batch})\n\n# Test\ninput = [sen.split()[:2] for sen in sentences]\nprint([sen.split()[:2] for sen in sentences], \'->\', [number_dict[n] for n in predict[0]])'"
1-1.NNLM/NNLM-Torch.py,0,"b'# code by Tae Hwan Jung @graykode\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\ndtype = torch.FloatTensor\n\nsentences = [ ""i like dog"", ""i love coffee"", ""i hate milk""]\n\nword_list = "" "".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\nnumber_dict = {i: w for i, w in enumerate(word_list)}\nn_class = len(word_dict) # number of Vocabulary\n\n# NNLM Parameter\nn_step = 2 # n-1 in paper\nn_hidden = 2 # h in paper\nm = 2 # m in paper\n\ndef make_batch(sentences):\n    input_batch = []\n    target_batch = []\n\n    for sen in sentences:\n        word = sen.split()\n        input = [word_dict[n] for n in word[:-1]]\n        target = word_dict[word[-1]]\n\n        input_batch.append(input)\n        target_batch.append(target)\n\n    return input_batch, target_batch\n\n# Model\nclass NNLM(nn.Module):\n    def __init__(self):\n        super(NNLM, self).__init__()\n        self.C = nn.Embedding(n_class, m)\n        self.H = nn.Parameter(torch.randn(n_step * m, n_hidden).type(dtype))\n        self.W = nn.Parameter(torch.randn(n_step * m, n_class).type(dtype))\n        self.d = nn.Parameter(torch.randn(n_hidden).type(dtype))\n        self.U = nn.Parameter(torch.randn(n_hidden, n_class).type(dtype))\n        self.b = nn.Parameter(torch.randn(n_class).type(dtype))\n\n    def forward(self, X):\n        X = self.C(X)\n        X = X.view(-1, n_step * m) # [batch_size, n_step * n_class]\n        tanh = torch.tanh(self.d + torch.mm(X, self.H)) # [batch_size, n_hidden]\n        output = self.b + torch.mm(X, self.W) + torch.mm(tanh, self.U) # [batch_size, n_class]\n        return output\n\nmodel = NNLM()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ninput_batch, target_batch = make_batch(sentences)\ninput_batch = Variable(torch.LongTensor(input_batch))\ntarget_batch = Variable(torch.LongTensor(target_batch))\n\n# Training\nfor epoch in range(5000):\n\n    optimizer.zero_grad()\n    output = model(input_batch)\n\n    # output : [batch_size, n_class], target_batch : [batch_size] (LongTensor, not one-hot)\n    loss = criterion(output, target_batch)\n    if (epoch + 1)%1000 == 0:\n        print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n\n    loss.backward()\n    optimizer.step()\n\n# Predict\npredict = model(input_batch).data.max(1, keepdim=True)[1]\n\n# Test\nprint([sen.split()[:2] for sen in sentences], \'->\', [number_dict[n.item()] for n in predict.squeeze()])\n'"
1-2.Word2Vec/Word2Vec-Skipgram-Tensor(NCE_loss).py,11,"b'\'\'\'\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n  reference : https://github.com/golbin/TensorFlow-Tutorials/blob/master/04%20-%20Neural%20Network%20Basic/03%20-%20Word2Vec.py\n\'\'\'\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntf.reset_default_graph()\n\n# 3 Words Sentence\nsentences = [ ""i like dog"", ""i like cat"", ""i like animal"",\n              ""dog cat animal"", ""apple cat dog like"", ""dog fish milk like"",\n              ""dog cat eyes like"", ""i like apple"", ""apple i hate"",\n              ""apple i movie book music like"", ""cat dog hate"", ""cat dog like""]\n\nword_sequence = "" "".join(sentences).split()\nword_list = "" "".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\n\n# Word2Vec Parameter\nbatch_size = 20\nembedding_size = 2 # To show 2 dim embedding graph\nnum_sampled = 10 # for negative sampling, less than batch_size\nvoc_size = len(word_list)\n\ndef random_batch(data, size):\n    random_inputs = []\n    random_labels = []\n    random_index = np.random.choice(range(len(data)), size, replace=False)\n\n    for i in random_index:\n        random_inputs.append(data[i][0])  # target\n        random_labels.append([data[i][1]])  # context word\n\n    return random_inputs, random_labels\n\n# Make skip gram of one size window\nskip_grams = []\nfor i in range(1, len(word_sequence) - 1):\n    target = word_dict[word_sequence[i]]\n    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n\n    for w in context:\n        skip_grams.append([target, w])\n\n# Model\ninputs = tf.placeholder(tf.int32, shape=[batch_size])\nlabels = tf.placeholder(tf.int32, shape=[batch_size, 1]) # To use tf.nn.nce_loss, [batch_size, 1]\n\nembeddings = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\nselected_embed = tf.nn.embedding_lookup(embeddings, inputs)\n\nnce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\nnce_biases = tf.Variable(tf.zeros([voc_size]))\n\n# Loss and optimizer\ncost = tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_biases, labels, selected_embed, num_sampled, voc_size))\noptimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n\n# Training\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    for epoch in range(5000):\n        batch_inputs, batch_labels = random_batch(skip_grams, batch_size)\n        _, loss = sess.run([optimizer, cost], feed_dict={inputs: batch_inputs, labels: batch_labels})\n\n        if (epoch + 1) % 1000 == 0:\n            print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n\n    trained_embeddings = embeddings.eval()\n\nfor i, label in enumerate(word_list):\n    x, y = trained_embeddings[i]\n    plt.scatter(x, y)\n    plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords=\'offset points\', ha=\'right\', va=\'bottom\')\nplt.show()'"
1-2.Word2Vec/Word2Vec-Skipgram-Tensor(Softmax).py,11,"b'\'\'\'\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n\'\'\'\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntf.reset_default_graph()\n\n# 3 Words Sentence\nsentences = [ ""i like dog"", ""i like cat"", ""i like animal"",\n              ""dog cat animal"", ""apple cat dog like"", ""dog fish milk like"",\n              ""dog cat eyes like"", ""i like apple"", ""apple i hate"",\n              ""apple i movie book music like"", ""cat dog hate"", ""cat dog like""]\n\nword_sequence = "" "".join(sentences).split()\nword_list = "" "".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\n\n# Word2Vec Parameter\nbatch_size = 20\nembedding_size = 2 # To show 2 dim embedding graph\nvoc_size = len(word_list)\n\ndef random_batch(data, size):\n    random_inputs = []\n    random_labels = []\n    random_index = np.random.choice(range(len(data)), size, replace=False)\n\n    for i in random_index:\n        random_inputs.append(np.eye(voc_size)[data[i][0]])  # target\n        random_labels.append(np.eye(voc_size)[data[i][1]])  # context word\n\n    return random_inputs, random_labels\n\n# Make skip gram of one size window\nskip_grams = []\nfor i in range(1, len(word_sequence) - 1):\n    target = word_dict[word_sequence[i]]\n    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n\n    for w in context:\n        skip_grams.append([target, w])\n\n# Model\ninputs = tf.placeholder(tf.float32, shape=[None, voc_size])\nlabels = tf.placeholder(tf.float32, shape=[None, voc_size])\n\n# W and WT is not Traspose relationship\nW = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\nWT = tf.Variable(tf.random_uniform([embedding_size, voc_size], -1.0, 1.0))\n\nhidden_layer = tf.matmul(inputs, W) # [batch_size, embedding_size]\noutput_layer = tf.matmul(hidden_layer, WT) # [batch_size, voc_size]\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output_layer, labels=labels))\noptimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    for epoch in range(5000):\n        batch_inputs, batch_labels = random_batch(skip_grams, batch_size)\n        _, loss = sess.run([optimizer, cost], feed_dict={inputs: batch_inputs, labels: batch_labels})\n\n        if (epoch + 1)%1000 == 0:\n            print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n\n        trained_embeddings = W.eval()\n\nfor i, label in enumerate(word_list):\n    x, y = trained_embeddings[i]\n    plt.scatter(x, y)\n    plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords=\'offset points\', ha=\'right\', va=\'bottom\')\nplt.show()'"
1-2.Word2Vec/Word2Vec-Skipgram-Torch(Softmax).py,0,"b'\'\'\'\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n\'\'\'\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\n\ndtype = torch.FloatTensor\n\n# 3 Words Sentence\nsentences = [ ""i like dog"", ""i like cat"", ""i like animal"",\n              ""dog cat animal"", ""apple cat dog like"", ""dog fish milk like"",\n              ""dog cat eyes like"", ""i like apple"", ""apple i hate"",\n              ""apple i movie book music like"", ""cat dog hate"", ""cat dog like""]\n\nword_sequence = "" "".join(sentences).split()\nword_list = "" "".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\n\n# Word2Vec Parameter\nbatch_size = 20  # To show 2 dim embedding graph\nembedding_size = 2  # To show 2 dim embedding graph\nvoc_size = len(word_list)\n\ndef random_batch(data, size):\n    random_inputs = []\n    random_labels = []\n    random_index = np.random.choice(range(len(data)), size, replace=False)\n\n    for i in random_index:\n        random_inputs.append(np.eye(voc_size)[data[i][0]])  # target\n        random_labels.append(data[i][1])  # context word\n\n    return random_inputs, random_labels\n\n# Make skip gram of one size window\nskip_grams = []\nfor i in range(1, len(word_sequence) - 1):\n    target = word_dict[word_sequence[i]]\n    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n\n    for w in context:\n        skip_grams.append([target, w])\n\n# Model\nclass Word2Vec(nn.Module):\n    def __init__(self):\n        super(Word2Vec, self).__init__()\n\n        # W and WT is not Traspose relationship\n        self.W = nn.Parameter(-2 * torch.rand(voc_size, embedding_size) + 1).type(dtype) # voc_size > embedding_size Weight\n        self.WT = nn.Parameter(-2 * torch.rand(embedding_size, voc_size) + 1).type(dtype) # embedding_size > voc_size Weight\n\n    def forward(self, X):\n        # X : [batch_size, voc_size]\n        hidden_layer = torch.matmul(X, self.W) # hidden_layer : [batch_size, embedding_size]\n        output_layer = torch.matmul(hidden_layer, self.WT) # output_layer : [batch_size, voc_size]\n        return output_layer\n\nmodel = Word2Vec()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training\nfor epoch in range(5000):\n\n    input_batch, target_batch = random_batch(skip_grams, batch_size)\n\n    input_batch = Variable(torch.Tensor(input_batch))\n    target_batch = Variable(torch.LongTensor(target_batch))\n\n    optimizer.zero_grad()\n    output = model(input_batch)\n\n    # output : [batch_size, voc_size], target_batch : [batch_size] (LongTensor, not one-hot)\n    loss = criterion(output, target_batch)\n    if (epoch + 1)%1000 == 0:\n        print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n\n    loss.backward()\n    optimizer.step()\n\nfor i, label in enumerate(word_list):\n    W, WT = model.parameters()\n    x,y = float(W[i][0]), float(W[i][1])\n    plt.scatter(x, y)\n    plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords=\'offset points\', ha=\'right\', va=\'bottom\')\nplt.show()\n'"
2-1.TextCNN/TextCNN-Tensor.py,23,"b'\'\'\'\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n  Reference : https://github.com/ioatr/textcnn\n\'\'\'\nimport tensorflow as tf\nimport numpy as np\n\ntf.reset_default_graph()\n\n# Text-CNN Parameter\nembedding_size = 2 # n-gram\nsequence_length = 3\nnum_classes = 2 # 0 or 1\nfilter_sizes = [2,2,2] # n-gram window\nnum_filters = 3\n\n# 3 words sentences (=sequence_length is 3)\nsentences = [""i love you"",""he loves me"", ""she likes baseball"", ""i hate you"",""sorry for that"", ""this is awful""]\nlabels = [1,1,1,0,0,0] # 1 is good, 0 is not good.\n\nword_list = "" "".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\nvocab_size = len(word_dict)\n\ninputs = []\nfor sen in sentences:\n    inputs.append(np.asarray([word_dict[n] for n in sen.split()]))\n\noutputs = []\nfor out in labels:\n    outputs.append(np.eye(num_classes)[out]) # ONE-HOT : To using Tensor Softmax Loss function\n\n# Model\nX = tf.placeholder(tf.int32, [None, sequence_length])\nY = tf.placeholder(tf.int32, [None, num_classes])\n\nW = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\nembedded_chars = tf.nn.embedding_lookup(W, X) # [batch_size, sequence_length, embedding_size]\nembedded_chars = tf.expand_dims(embedded_chars, -1) # add channel(=1) [batch_size, sequence_length, embedding_size, 1]\n\npooled_outputs = []\nfor i, filter_size in enumerate(filter_sizes):\n    filter_shape = [filter_size, embedding_size, 1, num_filters]\n    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1))\n    b = tf.Variable(tf.constant(0.1, shape=[num_filters]))\n\n    conv = tf.nn.conv2d(embedded_chars, # [batch_size, sequence_length, embedding_size, 1]\n                        W,              # [filter_size(n-gram window), embedding_size, 1, num_filters(=3)]\n                        strides=[1, 1, 1, 1],\n                        padding=\'VALID\')\n    h = tf.nn.relu(tf.nn.bias_add(conv, b))\n    pooled = tf.nn.max_pool(h,\n                            ksize=[1, sequence_length - filter_size + 1, 1, 1], # [batch_size, filter_height, filter_width, channel]\n                            strides=[1, 1, 1, 1],\n                            padding=\'VALID\')\n    pooled_outputs.append(pooled) # dim of pooled : [batch_size(=6), output_height(=1), output_width(=1), channel(=1)]\n\nnum_filters_total = num_filters * len(filter_sizes)\nh_pool = tf.concat(pooled_outputs, num_filters) # h_pool : [batch_size(=6), output_height(=1), output_width(=1), channel(=1) * 3]\nh_pool_flat = tf.reshape(h_pool, [-1, num_filters_total]) # [batch_size, ]\n\n# Model-Training\nWeight = tf.get_variable(\'W\', shape=[num_filters_total, num_classes], \n                    initializer=tf.contrib.layers.xavier_initializer())\nBias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\nmodel = tf.nn.xw_plus_b(h_pool_flat, Weight, Bias)  \ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\noptimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n\n# Model-Predict\nhypothesis = tf.nn.softmax(model)\npredictions = tf.argmax(hypothesis, 1)\n# Training\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\nfor epoch in range(5000):\n    _, loss = sess.run([optimizer, cost], feed_dict={X: inputs, Y: outputs})\n    if (epoch + 1)%1000 == 0:\n        print(\'Epoch:\', \'%06d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n\n# Test\ntest_text = \'sorry hate you\'\ntests = []\ntests.append(np.asarray([word_dict[n] for n in test_text.split()]))\n\npredict = sess.run([predictions], feed_dict={X: tests})\nresult = predict[0][0]\nif result == 0:\n    print(test_text,""is Bad Mean..."")\nelse:\n    print(test_text,""is Good Mean!!"")'"
2-1.TextCNN/TextCNN-Torch.py,0,"b'\'\'\'\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n\'\'\'\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\ndtype = torch.FloatTensor\n\n# Text-CNN Parameter\nembedding_size = 2 # n-gram\nsequence_length = 3\nnum_classes = 2  # 0 or 1\nfilter_sizes = [2, 2, 2] # n-gram window\nnum_filters = 3\n\n# 3 words sentences (=sequence_length is 3)\nsentences = [""i love you"", ""he loves me"", ""she likes baseball"", ""i hate you"", ""sorry for that"", ""this is awful""]\nlabels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n\nword_list = "" "".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\nvocab_size = len(word_dict)\n\ninputs = []\nfor sen in sentences:\n    inputs.append(np.asarray([word_dict[n] for n in sen.split()]))\n\ntargets = []\nfor out in labels:\n    targets.append(out) # To using Torch Softmax Loss function\n\ninput_batch = Variable(torch.LongTensor(inputs))\ntarget_batch = Variable(torch.LongTensor(targets))\n\n\nclass TextCNN(nn.Module):\n    def __init__(self):\n        super(TextCNN, self).__init__()\n\n        self.num_filters_total = num_filters * len(filter_sizes)\n        self.W = nn.Parameter(torch.empty(vocab_size, embedding_size).uniform_(-1, 1)).type(dtype)\n        self.Weight = nn.Parameter(torch.empty(self.num_filters_total, num_classes).uniform_(-1, 1)).type(dtype)\n        self.Bias = nn.Parameter(0.1 * torch.ones([num_classes])).type(dtype)\n\n    def forward(self, X):\n        embedded_chars = self.W[X] # [batch_size, sequence_length, sequence_length]\n        embedded_chars = embedded_chars.unsqueeze(1) # add channel(=1) [batch, channel(=1), sequence_length, embedding_size]\n\n        pooled_outputs = []\n        for filter_size in filter_sizes:\n            # conv : [input_channel(=1), output_channel(=3), (filter_height, filter_width), bias_option]\n            conv = nn.Conv2d(1, num_filters, (filter_size, embedding_size), bias=True)(embedded_chars)\n            h = F.relu(conv)\n            # mp : ((filter_height, filter_width))\n            mp = nn.MaxPool2d((sequence_length - filter_size + 1, 1))\n            # pooled : [batch_size(=6), output_height(=1), output_width(=1), output_channel(=3)]\n            pooled = mp(h).permute(0, 3, 2, 1)\n            pooled_outputs.append(pooled)\n\n        h_pool = torch.cat(pooled_outputs, len(filter_sizes)) # [batch_size(=6), output_height(=1), output_width(=1), output_channel(=3) * 3]\n        h_pool_flat = torch.reshape(h_pool, [-1, self.num_filters_total]) # [batch_size(=6), output_height * output_width * (output_channel * 3)]\n\n        model = torch.mm(h_pool_flat, self.Weight) + self.Bias # [batch_size, num_classes]\n        return model\n\nmodel = TextCNN()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training\nfor epoch in range(5000):\n    optimizer.zero_grad()\n    output = model(input_batch)\n\n    # output : [batch_size, num_classes], target_batch : [batch_size] (LongTensor, not one-hot)\n    loss = criterion(output, target_batch)\n    if (epoch + 1) % 1000 == 0:\n        print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n\n    loss.backward()\n    optimizer.step()\n\n# Test\ntest_text = \'sorry hate you\'\ntests = [np.asarray([word_dict[n] for n in test_text.split()])]\ntest_batch = Variable(torch.LongTensor(tests))\n\n# Predict\npredict = model(test_batch).data.max(1, keepdim=True)[1]\nif predict[0][0] == 0:\n    print(test_text,""is Bad Mean..."")\nelse:\n    print(test_text,""is Good Mean!!"")'"
3-1.TextRNN/TextRNN-Tensor.py,14,"b'\'\'\'\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n\'\'\'\nimport tensorflow as tf\nimport numpy as np\n\ntf.reset_default_graph()\n\nsentences = [ ""i like dog"", ""i love coffee"", ""i hate milk""]\n\nword_list = "" "".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\nnumber_dict = {i: w for i, w in enumerate(word_list)}\nn_class = len(word_dict)\n\n# TextRNN Parameter\nn_step = 2 # number of cells(= number of Step)\nn_hidden = 5 # number of hidden units in one cell\n\ndef make_batch(sentences):\n    input_batch = []\n    target_batch = []\n    \n    for sen in sentences:\n        word = sen.split()\n        input = [word_dict[n] for n in word[:-1]]\n        target = word_dict[word[-1]]\n\n        input_batch.append(np.eye(n_class)[input])\n        target_batch.append(np.eye(n_class)[target])\n\n    return input_batch, target_batch\n\n# Model\nX = tf.placeholder(tf.float32, [None, n_step, n_class]) # [batch_size, n_step, n_class]\nY = tf.placeholder(tf.float32, [None, n_class])         # [batch_size, n_class]\n\nW = tf.Variable(tf.random_normal([n_hidden, n_class]))\nb = tf.Variable(tf.random_normal([n_class]))\n\ncell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\noutputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n\n# outputs : [batch_size, n_step, n_hidden]\noutputs = tf.transpose(outputs, [1, 0, 2]) # [n_step, batch_size, n_hidden]\noutputs = outputs[-1] # [batch_size, n_hidden]\nmodel = tf.matmul(outputs, W) + b # model : [batch_size, n_class]\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\noptimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n\nprediction = tf.cast(tf.argmax(model, 1), tf.int32)\n\n# Training\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\ninput_batch, target_batch = make_batch(sentences)\n\nfor epoch in range(5000):\n    _, loss = sess.run([optimizer, cost], feed_dict={X: input_batch, Y: target_batch})\n    if (epoch + 1)%1000 == 0:\n        print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n        \ninput = [sen.split()[:2] for sen in sentences]\n\npredict =  sess.run([prediction], feed_dict={X: input_batch})\nprint([sen.split()[:2] for sen in sentences], \'->\', [number_dict[n] for n in predict[0]])'"
3-1.TextRNN/TextRNN-Torch.py,0,"b'\'\'\'\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n\'\'\'\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\ndtype = torch.FloatTensor\n\nsentences = [ ""i like dog"", ""i love coffee"", ""i hate milk""]\n\nword_list = "" "".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\nnumber_dict = {i: w for i, w in enumerate(word_list)}\nn_class = len(word_dict)\n\n# TextRNN Parameter\nbatch_size = len(sentences)\nn_step = 2 # number of cells(= number of Step)\nn_hidden = 5 # number of hidden units in one cell\n\ndef make_batch(sentences):\n    input_batch = []\n    target_batch = []\n\n    for sen in sentences:\n        word = sen.split()\n        input = [word_dict[n] for n in word[:-1]]\n        target = word_dict[word[-1]]\n\n        input_batch.append(np.eye(n_class)[input])\n        target_batch.append(target)\n\n    return input_batch, target_batch\n\n# to Torch.Tensor\ninput_batch, target_batch = make_batch(sentences)\ninput_batch = Variable(torch.Tensor(input_batch))\ntarget_batch = Variable(torch.LongTensor(target_batch))\n\nclass TextRNN(nn.Module):\n    def __init__(self):\n        super(TextRNN, self).__init__()\n\n        self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden)\n        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n\n    def forward(self, hidden, X):\n        X = X.transpose(0, 1) # X : [n_step, batch_size, n_class]\n        outputs, hidden = self.rnn(X, hidden)\n        # outputs : [n_step, batch_size, num_directions(=1) * n_hidden]\n        # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n        outputs = outputs[-1] # [batch_size, num_directions(=1) * n_hidden]\n        model = torch.mm(outputs, self.W) + self.b # model : [batch_size, n_class]\n        return model\n\nmodel = TextRNN()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training\nfor epoch in range(5000):\n    optimizer.zero_grad()\n\n    # hidden : [num_layers * num_directions, batch, hidden_size]\n    hidden = Variable(torch.zeros(1, batch_size, n_hidden))\n    # input_batch : [batch_size, n_step, n_class]\n    output = model(hidden, input_batch)\n\n    # output : [batch_size, n_class], target_batch : [batch_size] (LongTensor, not one-hot)\n    loss = criterion(output, target_batch)\n    if (epoch + 1) % 1000 == 0:\n        print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n\n    loss.backward()\n    optimizer.step()\n\ninput = [sen.split()[:2] for sen in sentences]\n\n# Predict\nhidden = Variable(torch.zeros(1, batch_size, n_hidden))\npredict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\nprint([sen.split()[:2] for sen in sentences], \'->\', [number_dict[n.item()] for n in predict.squeeze()])'"
3-2.TextLSTM/TextLSTM-Tensor.py,14,"b""'''\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n'''\nimport tensorflow as tf\nimport numpy as np\n\ntf.reset_default_graph()\n\nchar_arr = [c for c in 'abcdefghijklmnopqrstuvwxyz']\nword_dict = {n: i for i, n in enumerate(char_arr)}\nnumber_dict = {i: w for i, w in enumerate(char_arr)}\nn_class = len(word_dict) # number of class(=number of vocab)\n\nseq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']\n\n# TextLSTM Parameters\nn_step = 3\nn_hidden = 128\n\ndef make_batch(seq_data):\n    input_batch, target_batch = [], []\n\n    for seq in seq_data:\n        input = [word_dict[n] for n in seq[:-1]] # 'm', 'a' , 'k' is input\n        target = word_dict[seq[-1]] # 'e' is target\n        input_batch.append(np.eye(n_class)[input])\n        target_batch.append(np.eye(n_class)[target])\n\n    return input_batch, target_batch\n\n# Model\nX = tf.placeholder(tf.float32, [None, n_step, n_class]) # [batch_size, n_step, n_class]\nY = tf.placeholder(tf.float32, [None, n_class])         # [batch_size, n_class]\n\nW = tf.Variable(tf.random_normal([n_hidden, n_class]))\nb = tf.Variable(tf.random_normal([n_class]))\n\ncell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\noutputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n\n# outputs : [batch_size, n_step, n_hidden]\noutputs = tf.transpose(outputs, [1, 0, 2]) # [n_step, batch_size, n_hidden]\noutputs = outputs[-1] # [batch_size, n_hidden]\nmodel = tf.matmul(outputs, W) + b # model : [batch_size, n_class]\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\noptimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n\nprediction = tf.cast(tf.argmax(model, 1), tf.int32)\n\n# Training\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\ninput_batch, target_batch = make_batch(seq_data)\n\nfor epoch in range(1000):\n    _, loss = sess.run([optimizer, cost], feed_dict={X: input_batch, Y: target_batch})\n    if (epoch + 1)%100 == 0:\n        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n\ninputs = [sen[:3] for sen in seq_data]\n\npredict =  sess.run([prediction], feed_dict={X: input_batch})\nprint(inputs, '->', [number_dict[n] for n in predict[0]])"""
3-2.TextLSTM/TextLSTM-Torch.py,0,"b""'''\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n'''\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\ndtype = torch.FloatTensor\n\nchar_arr = [c for c in 'abcdefghijklmnopqrstuvwxyz']\nword_dict = {n: i for i, n in enumerate(char_arr)}\nnumber_dict = {i: w for i, w in enumerate(char_arr)}\nn_class = len(word_dict) # number of class(=number of vocab)\n\nseq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']\n\n# TextLSTM Parameters\nn_step = 3\nn_hidden = 128\n\ndef make_batch(seq_data):\n    input_batch, target_batch = [], []\n\n    for seq in seq_data:\n        input = [word_dict[n] for n in seq[:-1]] # 'm', 'a' , 'k' is input\n        target = word_dict[seq[-1]] # 'e' is target\n        input_batch.append(np.eye(n_class)[input])\n        target_batch.append(target)\n\n    return Variable(torch.Tensor(input_batch)), Variable(torch.LongTensor(target_batch))\n\nclass TextLSTM(nn.Module):\n    def __init__(self):\n        super(TextLSTM, self).__init__()\n\n        self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden)\n        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n\n    def forward(self, X):\n        input = X.transpose(0, 1)  # X : [n_step, batch_size, n_class]\n\n        hidden_state = Variable(torch.zeros(1, len(X), n_hidden))   # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n        cell_state = Variable(torch.zeros(1, len(X), n_hidden))     # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n\n        outputs, (_, _) = self.lstm(input, (hidden_state, cell_state))\n        outputs = outputs[-1]  # [batch_size, n_hidden]\n        model = torch.mm(outputs, self.W) + self.b  # model : [batch_size, n_class]\n        return model\n\ninput_batch, target_batch = make_batch(seq_data)\n\nmodel = TextLSTM()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\noutput = model(input_batch)\n\n# Training\nfor epoch in range(1000):\n    optimizer.zero_grad()\n\n    output = model(input_batch)\n    loss = criterion(output, target_batch)\n    if (epoch + 1) % 100 == 0:\n        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n\n    loss.backward()\n    optimizer.step()\n\ninputs = [sen[:3] for sen in seq_data]\n\npredict = model(input_batch).data.max(1, keepdim=True)[1]\nprint(inputs, '->', [number_dict[n.item()] for n in predict.squeeze()])"""
3-3.Bi-LSTM/Bi-LSTM-Tensor.py,16,"b""'''\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n'''\nimport tensorflow as tf\nimport numpy as np\n\ntf.reset_default_graph()\n\nsentence = (\n    'Lorem ipsum dolor sit amet consectetur adipisicing elit '\n    'sed do eiusmod tempor incididunt ut labore et dolore magna '\n    'aliqua Ut enim ad minim veniam quis nostrud exercitation'\n)\n\nword_dict = {w: i for i, w in enumerate(list(set(sentence.split())))}\nnumber_dict = {i: w for i, w in enumerate(list(set(sentence.split())))}\nn_class = len(word_dict)\nn_step = len(sentence.split())\nn_hidden = 5\n\ndef make_batch(sentence):\n    input_batch = []\n    target_batch = []\n\n    words = sentence.split()\n    for i, word in enumerate(words[:-1]):\n        input = [word_dict[n] for n in words[:(i + 1)]]\n        input = input + [0] * (n_step - len(input))\n        target = word_dict[words[i + 1]]\n        input_batch.append(np.eye(n_class)[input])\n        target_batch.append(np.eye(n_class)[target])\n\n    return input_batch, target_batch\n\n# Bi-LSTM Model\nX = tf.placeholder(tf.float32, [None, n_step, n_class])\nY = tf.placeholder(tf.float32, [None, n_class])\n\nW = tf.Variable(tf.random_normal([n_hidden * 2, n_class]))\nb = tf.Variable(tf.random_normal([n_class]))\n\nlstm_fw_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\nlstm_bw_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n\n# outputs : [batch_size, len_seq, n_hidden], states : [batch_size, n_hidden]\noutputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell,lstm_bw_cell, X, dtype=tf.float32)\n\noutputs = tf.concat([outputs[0], outputs[1]], 2) # output[0] : lstm_fw, output[1] : lstm_bw\noutputs = tf.transpose(outputs, [1, 0, 2]) # [n_step, batch_size, n_hidden]\noutputs = outputs[-1] # [batch_size, n_hidden]\n\nmodel = tf.matmul(outputs, W) + b\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\noptimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n\nprediction = tf.cast(tf.argmax(model, 1), tf.int32)\n\n# Training\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\ninput_batch, target_batch = make_batch(sentence)\n\nfor epoch in range(10000):\n    _, loss = sess.run([optimizer, cost], feed_dict={X: input_batch, Y: target_batch})\n    if (epoch + 1)%1000 == 0:\n        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n\npredict =  sess.run([prediction], feed_dict={X: input_batch})\nprint(sentence)\nprint([number_dict[n] for n in [pre for pre in predict[0]]])"""
3-3.Bi-LSTM/Bi-LSTM-Torch.py,0,"b""'''\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n'''\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\ndtype = torch.FloatTensor\n\nsentence = (\n    'Lorem ipsum dolor sit amet consectetur adipisicing elit '\n    'sed do eiusmod tempor incididunt ut labore et dolore magna '\n    'aliqua Ut enim ad minim veniam quis nostrud exercitation'\n)\n\nword_dict = {w: i for i, w in enumerate(list(set(sentence.split())))}\nnumber_dict = {i: w for i, w in enumerate(list(set(sentence.split())))}\nn_class = len(word_dict)\nmax_len = len(sentence.split())\nn_hidden = 5\n\ndef make_batch(sentence):\n    input_batch = []\n    target_batch = []\n\n    words = sentence.split()\n    for i, word in enumerate(words[:-1]):\n        input = [word_dict[n] for n in words[:(i + 1)]]\n        input = input + [0] * (max_len - len(input))\n        target = word_dict[words[i + 1]]\n        input_batch.append(np.eye(n_class)[input])\n        target_batch.append(target)\n\n    return Variable(torch.Tensor(input_batch)), Variable(torch.LongTensor(target_batch))\n\nclass BiLSTM(nn.Module):\n    def __init__(self):\n        super(BiLSTM, self).__init__()\n\n        self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden, bidirectional=True)\n        self.W = nn.Parameter(torch.randn([n_hidden * 2, n_class]).type(dtype))\n        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n\n    def forward(self, X):\n        input = X.transpose(0, 1)  # input : [n_step, batch_size, n_class]\n\n        hidden_state = Variable(torch.zeros(1*2, len(X), n_hidden))   # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n        cell_state = Variable(torch.zeros(1*2, len(X), n_hidden))     # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n\n        outputs, (_, _) = self.lstm(input, (hidden_state, cell_state))\n        outputs = outputs[-1]  # [batch_size, n_hidden]\n        model = torch.mm(outputs, self.W) + self.b  # model : [batch_size, n_class]\n        return model\n\ninput_batch, target_batch = make_batch(sentence)\n\nmodel = BiLSTM()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training\nfor epoch in range(10000):\n    optimizer.zero_grad()\n    output = model(input_batch)\n    loss = criterion(output, target_batch)\n    if (epoch + 1) % 1000 == 0:\n        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n\n    loss.backward()\n    optimizer.step()\n\npredict = model(input_batch).data.max(1, keepdim=True)[1]\nprint(sentence)\nprint([number_dict[n.item()] for n in predict.squeeze()])"""
4-1.Seq2Seq/Seq2Seq-Tensor.py,18,"b""'''\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n  reference : https://github.com/golbin/TensorFlow-Tutorials/blob/master/10%20-%20RNN/03%20-%20Seq2Seq.py\n'''\nimport tensorflow as tf\nimport numpy as np\n\ntf.reset_default_graph()\n# S: Symbol that shows starting of decoding input\n# E: Symbol that shows starting of decoding output\n# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n\nchar_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\nnum_dic = {n: i for i, n in enumerate(char_arr)}\n\nseq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n\n# Seq2Seq Parameter\nn_step = 5\nn_hidden = 128\nn_class = len(num_dic) # number of class(=number of vocab)\n\ndef make_batch(seq_data):\n    input_batch, output_batch, target_batch = [], [], []\n\n    for seq in seq_data:\n        for i in range(2):\n            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n\n        input = [num_dic[n] for n in seq[0]]\n        output = [num_dic[n] for n in ('S' + seq[1])]\n        target = [num_dic[n] for n in (seq[1] + 'E')]\n\n        input_batch.append(np.eye(n_class)[input])\n        output_batch.append(np.eye(n_class)[output])\n\n        target_batch.append(target)\n\n    return input_batch, output_batch, target_batch\n\n# Model\nenc_input = tf.placeholder(tf.float32, [None, None, n_class]) # [batch_size, max_len(=encoder_step), n_class]\ndec_input = tf.placeholder(tf.float32, [None, None, n_class]) # [batch_size, max_len+1(=decoder_step) (becase of 'S' or 'E'), n_class]\ntargets = tf.placeholder(tf.int64, [None, None]) # [batch_size, max_len+1], not one-hot\n\nwith tf.variable_scope('encode'):\n    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n    _, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype=tf.float32)\n    # encoder state will go to decoder initial_state, enc_states : [batch_size, n_hidden(=128)]\n\nwith tf.variable_scope('decode'):\n    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n    outputs, _ = tf.nn.dynamic_rnn(dec_cell, dec_input, initial_state=enc_states, dtype=tf.float32)\n    # outputs : [batch_size, max_len+1, n_hidden(=128)]\n\nmodel = tf.layers.dense(outputs, n_class, activation=None) # model : [batch_size, max_len+1, n_class]\n\ncost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets))\noptimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n\n# Training\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\ninput_batch, output_batch, target_batch = make_batch(seq_data)\n\nfor epoch in range(5000):\n    _, loss = sess.run([optimizer, cost], feed_dict={enc_input: input_batch, dec_input: output_batch, targets: target_batch})\n    if (epoch + 1)%1000 == 0:\n        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n\n# Test\ndef translate(word):\n    seq_data = [word, 'P' * len(word)]\n\n    input_batch, output_batch, _ = make_batch([seq_data])\n    prediction = tf.argmax(model, 2)\n\n    result = sess.run(prediction, feed_dict={enc_input: input_batch, dec_input: output_batch})\n\n    decoded = [char_arr[i] for i in result[0]]\n    end = decoded.index('E')\n    translated = ''.join(decoded[:end])\n\n    return translated.replace('P','')\n\nprint('test')\nprint('man ->', translate('man'))\nprint('mans ->', translate('mans'))\nprint('king ->', translate('king'))\nprint('black ->', translate('black'))\nprint('upp ->', translate('upp'))"""
4-1.Seq2Seq/Seq2Seq-Torch.py,0,"b""# code by Tae Hwan Jung(Jeff Jung) @graykode\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\ndtype = torch.FloatTensor\n# S: Symbol that shows starting of decoding input\n# E: Symbol that shows starting of decoding output\n# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n\nchar_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\nnum_dic = {n: i for i, n in enumerate(char_arr)}\n\nseq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n\n# Seq2Seq Parameter\nn_step = 5\nn_hidden = 128\nn_class = len(num_dic)\nbatch_size = len(seq_data)\n\ndef make_batch(seq_data):\n    input_batch, output_batch, target_batch = [], [], []\n\n    for seq in seq_data:\n        for i in range(2):\n            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n\n        input = [num_dic[n] for n in seq[0]]\n        output = [num_dic[n] for n in ('S' + seq[1])]\n        target = [num_dic[n] for n in (seq[1] + 'E')]\n\n        input_batch.append(np.eye(n_class)[input])\n        output_batch.append(np.eye(n_class)[output])\n        target_batch.append(target) # not one-hot\n\n    # make tensor\n    return Variable(torch.Tensor(input_batch)), Variable(torch.Tensor(output_batch)), Variable(torch.LongTensor(target_batch))\n\n# Model\nclass Seq2Seq(nn.Module):\n    def __init__(self):\n        super(Seq2Seq, self).__init__()\n\n        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n        self.fc = nn.Linear(n_hidden, n_class)\n\n    def forward(self, enc_input, enc_hidden, dec_input):\n        enc_input = enc_input.transpose(0, 1) # enc_input: [max_len(=n_step, time step), batch_size, n_class]\n        dec_input = dec_input.transpose(0, 1) # dec_input: [max_len(=n_step, time step), batch_size, n_class]\n\n        # enc_states : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n        # outputs : [max_len+1(=6), batch_size, num_directions(=1) * n_hidden(=128)]\n        outputs, _ = self.dec_cell(dec_input, enc_states)\n\n        model = self.fc(outputs) # model : [max_len+1(=6), batch_size, n_class]\n        return model\n\n\ninput_batch, output_batch, target_batch = make_batch(seq_data)\n\nmodel = Seq2Seq()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nfor epoch in range(5000):\n    # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n    hidden = Variable(torch.zeros(1, batch_size, n_hidden))\n\n    optimizer.zero_grad()\n    # input_batch : [batch_size, max_len(=n_step, time step), n_class]\n    # output_batch : [batch_size, max_len+1(=n_step, time step) (becase of 'S' or 'E'), n_class]\n    # target_batch : [batch_size, max_len+1(=n_step, time step)], not one-hot\n    output = model(input_batch, hidden, output_batch)\n    # output : [max_len+1, batch_size, n_class]\n    output = output.transpose(0, 1) # [batch_size, max_len+1(=6), n_class]\n    loss = 0\n    for i in range(0, len(target_batch)):\n        # output[i] : [max_len+1, n_class, target_batch[i] : max_len+1]\n        loss += criterion(output[i], target_batch[i])\n    if (epoch + 1) % 1000 == 0:\n        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n    loss.backward()\n    optimizer.step()\n\n\n# Test\ndef translate(word):\n    input_batch, output_batch, _ = make_batch([[word, 'P' * len(word)]])\n\n    # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n    hidden = Variable(torch.zeros(1, 1, n_hidden))\n    output = model(input_batch, hidden, output_batch)\n    # output : [max_len+1(=6), batch_size(=1), n_class]\n\n    predict = output.data.max(2, keepdim=True)[1] # select n_class dimension\n    decoded = [char_arr[i] for i in predict]\n    end = decoded.index('E')\n    translated = ''.join(decoded[:end])\n\n    return translated.replace('P', '')\n\nprint('test')\nprint('man ->', translate('man'))\nprint('mans ->', translate('mans'))\nprint('king ->', translate('king'))\nprint('black ->', translate('black'))\nprint('upp ->', translate('upp'))"""
4-2.Seq2Seq(Attention)/Seq2Seq(Attention)-Tensor.py,33,"b'# code by Tae Hwan Jung(Jeff Jung) @graykode\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntf.reset_default_graph()\n# S: Symbol that shows starting of decoding input\n# E: Symbol that shows starting of decoding output\n# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\nsentences = [\'ich mochte ein bier P\', \'S i want a beer\', \'i want a beer E\']\n\nword_list = "" "".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\nnumber_dict = {i: w for i, w in enumerate(word_list)}\nn_class = len(word_dict)  # vocab list\n\n# Parameter\nn_step = 5  # maxium number of words in one sentence(=number of time steps)\nn_hidden = 128\n\ndef make_batch(sentences):\n    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n    return input_batch, output_batch, target_batch\n\n# Model\nenc_inputs = tf.placeholder(tf.float32, [None, None, n_class])  # [batch_size, n_step, n_class]\ndec_inputs = tf.placeholder(tf.float32, [None, None, n_class])  # [batch_size, n_step, n_class]\ntargets = tf.placeholder(tf.int64, [1, n_step])  # [batch_size, n_step], not one-hot\n\n# Linear for attention\nattn = tf.Variable(tf.random_normal([n_hidden, n_hidden]))\nout = tf.Variable(tf.random_normal([n_hidden * 2, n_class]))\n\ndef get_att_score(dec_output, enc_output):  # enc_output [n_step, n_hidden]\n    score = tf.squeeze(tf.matmul(enc_output, attn), 0)  # score : [n_hidden]\n    dec_output = tf.squeeze(dec_output, [0, 1])  # dec_output : [n_hidden]\n    return tf.tensordot(dec_output, score, 1)  # inner product make scalar value\n\ndef get_att_weight(dec_output, enc_outputs):\n    attn_scores = []  # list of attention scalar : [n_step]\n    enc_outputs = tf.transpose(enc_outputs, [1, 0, 2])  # enc_outputs : [n_step, batch_size, n_hidden]\n    for i in range(n_step):\n        attn_scores.append(get_att_score(dec_output, enc_outputs[i]))\n\n    # Normalize scores to weights in range 0 to 1\n    return tf.reshape(tf.nn.softmax(attn_scores), [1, 1, -1])  # [1, 1, n_step]\n\nmodel = []\nAttention = []\nwith tf.variable_scope(\'encode\'):\n    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n    # enc_outputs : [batch_size(=1), n_step(=decoder_step), n_hidden(=128)]\n    # enc_hidden : [batch_size(=1), n_hidden(=128)]\n    enc_outputs, enc_hidden = tf.nn.dynamic_rnn(enc_cell, enc_inputs, dtype=tf.float32)\n\nwith tf.variable_scope(\'decode\'):\n    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n\n    inputs = tf.transpose(dec_inputs, [1, 0, 2])\n    hidden = enc_hidden\n    for i in range(n_step):\n        # time_major True mean inputs shape: [max_time, batch_size, ...]\n        dec_output, hidden = tf.nn.dynamic_rnn(dec_cell, tf.expand_dims(inputs[i], 1),\n                                               initial_state=hidden, dtype=tf.float32, time_major=True)\n        attn_weights = get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n        Attention.append(tf.squeeze(attn_weights))\n\n        # matrix-matrix product of matrices [1, 1, n_step] x [1, n_step, n_hidden] = [1, 1, n_hidden]\n        context = tf.matmul(attn_weights, enc_outputs)\n        dec_output = tf.squeeze(dec_output, 0)  # [1, n_step]\n        context = tf.squeeze(context, 1)  # [1, n_hidden]\n\n        model.append(tf.matmul(tf.concat((dec_output, context), 1), out))  # [n_step, batch_size(=1), n_class]\n\ntrained_attn = tf.stack([Attention[0], Attention[1], Attention[2], Attention[3], Attention[4]], 0)  # to show attention matrix\nmodel = tf.transpose(model, [1, 0, 2])  # model : [n_step, n_class]\nprediction = tf.argmax(model, 2)\ncost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets))\noptimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n\n# Training and Test\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    for epoch in range(2000):\n        input_batch, output_batch, target_batch = make_batch(sentences)\n        _, loss, attention = sess.run([optimizer, cost, trained_attn],\n                                      feed_dict={enc_inputs: input_batch, dec_inputs: output_batch, targets: target_batch})\n\n        if (epoch + 1) % 400 == 0:\n            print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n\n    predict_batch = [np.eye(n_class)[[word_dict[n] for n in \'P P P P P\'.split()]]]\n    result = sess.run(prediction, feed_dict={enc_inputs: input_batch, dec_inputs: predict_batch})\n    print(sentences[0].split(), \'->\', [number_dict[n] for n in result[0]])\n\n    # Show Attention\n    fig = plt.figure(figsize=(5, 5))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attention, cmap=\'viridis\')\n    ax.set_xticklabels([\'\'] + sentences[0].split(), fontdict={\'fontsize\': 14})\n    ax.set_yticklabels([\'\'] + sentences[2].split(), fontdict={\'fontsize\': 14})\n    plt.show()'"
4-2.Seq2Seq(Attention)/Seq2Seq(Attention)-Torch.py,0,"b'# code by Tae Hwan Jung(Jeff Jung) @graykode\n# Reference : https://github.com/hunkim/PyTorchZeroToAll/blob/master/14_2_seq2seq_att.py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\ndtype = torch.FloatTensor\n# S: Symbol that shows starting of decoding input\n# E: Symbol that shows starting of decoding output\n# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\nsentences = [\'ich mochte ein bier P\', \'S i want a beer\', \'i want a beer E\']\n\nword_list = "" "".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\nnumber_dict = {i: w for i, w in enumerate(word_list)}\nn_class = len(word_dict)  # vocab list\n\n# Parameter\nn_hidden = 128\n\ndef make_batch(sentences):\n    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n\n    # make tensor\n    return Variable(torch.Tensor(input_batch)), Variable(torch.Tensor(output_batch)), Variable(torch.LongTensor(target_batch))\n\nclass Attention(nn.Module):\n    def __init__(self):\n        super(Attention, self).__init__()\n        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n\n        # Linear for attention\n        self.attn = nn.Linear(n_hidden, n_hidden)\n        self.out = nn.Linear(n_hidden * 2, n_class)\n\n    def forward(self, enc_inputs, hidden, dec_inputs):\n        enc_inputs = enc_inputs.transpose(0, 1)  # enc_inputs: [n_step(=n_step, time step), batch_size, n_class]\n        dec_inputs = dec_inputs.transpose(0, 1)  # dec_inputs: [n_step(=n_step, time step), batch_size, n_class]\n\n        # enc_outputs : [n_step, batch_size, num_directions(=1) * n_hidden], matrix F\n        # enc_hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n\n        trained_attn = []\n        hidden = enc_hidden\n        n_step = len(dec_inputs)\n        model = Variable(torch.empty([n_step, 1, n_class]))\n\n        for i in range(n_step):  # each time step\n            # dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]\n            # hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]\n            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n            attn_weights = self.get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n            trained_attn.append(attn_weights.squeeze().data.numpy())\n\n            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]\n            context = attn_weights.bmm(enc_outputs.transpose(0, 1))\n            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1), num_directions(=1) * n_hidden]\n            context = context.squeeze(1)  # [1, num_directions(=1) * n_hidden]\n            model[i] = self.out(torch.cat((dec_output, context), 1))\n\n        # make model shape [n_step, n_class]\n        return model.transpose(0, 1).squeeze(0), trained_attn\n\n    def get_att_weight(self, dec_output, enc_outputs):  # get attention weight one \'dec_output\' with \'enc_outputs\'\n        n_step = len(enc_outputs)\n        attn_scores = Variable(torch.zeros(n_step))  # attn_scores : [n_step]\n\n        for i in range(n_step):\n            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n\n        # Normalize scores to weights in range 0 to 1\n        return F.softmax(attn_scores).view(1, 1, -1)\n\n    def get_att_score(self, dec_output, enc_output):  # enc_outputs [batch_size, num_directions(=1) * n_hidden]\n        score = self.attn(enc_output)  # score : [batch_size, n_hidden]\n        return torch.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value\n\ninput_batch, output_batch, target_batch = make_batch(sentences)\n\n# hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\nhidden = Variable(torch.zeros(1, 1, n_hidden))\n\nmodel = Attention()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Train\nfor epoch in range(2000):\n    optimizer.zero_grad()\n    output, _ = model(input_batch, hidden, output_batch)\n\n    loss = criterion(output, target_batch.squeeze(0))\n    if (epoch + 1) % 400 == 0:\n        print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n\n    loss.backward()\n    optimizer.step()\n\n# Test\ntest_batch = [np.eye(n_class)[[word_dict[n] for n in \'SPPPP\']]]\ntest_batch = Variable(torch.Tensor(test_batch))\npredict, trained_attn = model(input_batch, hidden, test_batch)\npredict = predict.data.max(1, keepdim=True)[1]\nprint(sentences[0], \'->\', [number_dict[n.item()] for n in predict.squeeze()])\n\n# Show Attention\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(1, 1, 1)\nax.matshow(trained_attn, cmap=\'viridis\')\nax.set_xticklabels([\'\'] + sentences[0].split(), fontdict={\'fontsize\': 14})\nax.set_yticklabels([\'\'] + sentences[2].split(), fontdict={\'fontsize\': 14})\nplt.show()'"
4-3.Bi-LSTM(Attention)/Bi-LSTM(Attention)-Tensor.py,23,"b'\'\'\'\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n  Reference : https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/models/LSTM_Attn.py\n\'\'\'\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntf.reset_default_graph()\n\n# Bi-LSTM(Attention) Parameters\nembedding_dim = 2\nn_hidden = 5 # number of hidden units in one cell\nn_step = 3 # all sentence is consist of 3 words\nn_class = 2  # 0 or 1\n\n# 3 words sentences (=sequence_length is 3)\nsentences = [""i love you"", ""he loves me"", ""she likes baseball"", ""i hate you"", ""sorry for that"", ""this is awful""]\nlabels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n\nword_list = "" "".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\nvocab_size = len(word_dict)\n\ninput_batch = []\nfor sen in sentences:\n    input_batch.append(np.asarray([word_dict[n] for n in sen.split()]))\n\ntarget_batch = []\nfor out in labels:\n    target_batch.append(np.eye(n_class)[out]) # ONE-HOT : To using Tensor Softmax Loss function\n\n# LSTM Model\nX = tf.placeholder(tf.int32, [None, n_step])\nY = tf.placeholder(tf.int32, [None, n_class])\nout = tf.Variable(tf.random_normal([n_hidden * 2, n_class]))\n\nembedding = tf.Variable(tf.random_uniform([vocab_size, embedding_dim]))\ninput = tf.nn.embedding_lookup(embedding, X) # [batch_size, len_seq, embedding_dim]\n\nlstm_fw_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\nlstm_bw_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n\n# output : [batch_size, len_seq, n_hidden], states : [batch_size, n_hidden]\noutput, final_state = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell,lstm_bw_cell, input, dtype=tf.float32)\n\n# Attention\noutput = tf.concat([output[0], output[1]], 2)                             # output[0] : lstm_fw, output[1] : lstm_bw\nfinal_hidden_state = tf.concat([final_state[1][0], final_state[1][1]], 1) # final_hidden_state : [batch_size, n_hidden * num_directions(=2)]\nfinal_hidden_state = tf.expand_dims(final_hidden_state, 2)                # final_hidden_state : [batch_size, n_hidden * num_directions(=2), 1]\n\nattn_weights = tf.squeeze(tf.matmul(output, final_hidden_state), 2) # attn_weights : [batch_size, n_step]\nsoft_attn_weights = tf.nn.softmax(attn_weights, 1)\ncontext = tf.matmul(tf.transpose(output, [0, 2, 1]), tf.expand_dims(soft_attn_weights, 2)) # context : [batch_size, n_hidden * num_directions(=2), 1]\ncontext = tf.squeeze(context, 2) # [batch_size, n_hidden * num_directions(=2)]\n\nmodel = tf.matmul(context, out)\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\noptimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n\n# Model-Predict\nhypothesis = tf.nn.softmax(model)\npredictions = tf.argmax(hypothesis, 1)\n\n# Training\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    for epoch in range(5000):\n        _, loss, attention = sess.run([optimizer, cost, soft_attn_weights], feed_dict={X: input_batch, Y: target_batch})\n        if (epoch + 1)%1000 == 0:\n            print(\'Epoch:\', \'%06d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n\n    # Test\n    test_text = \'sorry hate you\'\n    tests = [np.asarray([word_dict[n] for n in test_text.split()])]\n\n    predict = sess.run([predictions], feed_dict={X: tests})\n    result = predict[0][0]\n    if result == 0:\n        print(test_text,""is Bad Mean..."")\n    else:\n        print(test_text,""is Good Mean!!"")\n\n    fig = plt.figure(figsize=(6, 3)) # [batch_size, n_step]\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attention, cmap=\'viridis\')\n    ax.set_xticklabels([\'\'] + [\'first_word\', \'second_word\', \'third_word\'], fontdict={\'fontsize\': 14}, rotation=90)\n    ax.set_yticklabels([\'\'] + [\'batch_1\', \'batch_2\', \'batch_3\', \'batch_4\', \'batch_5\', \'batch_6\'], fontdict={\'fontsize\': 14})\n    plt.show()'"
4-3.Bi-LSTM(Attention)/Bi-LSTM(Attention)-Torch.py,0,"b'\'\'\'\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n  Reference : https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/models/LSTM_Attn.py\n\'\'\'\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\ndtype = torch.FloatTensor\n\n# Bi-LSTM(Attention) Parameters\nembedding_dim = 2\nn_hidden = 5 # number of hidden units in one cell\nnum_classes = 2  # 0 or 1\n\n# 3 words sentences (=sequence_length is 3)\nsentences = [""i love you"", ""he loves me"", ""she likes baseball"", ""i hate you"", ""sorry for that"", ""this is awful""]\nlabels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n\nword_list = "" "".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\nvocab_size = len(word_dict)\n\ninputs = []\nfor sen in sentences:\n    inputs.append(np.asarray([word_dict[n] for n in sen.split()]))\n\ntargets = []\nfor out in labels:\n    targets.append(out) # To using Torch Softmax Loss function\n\ninput_batch = Variable(torch.LongTensor(inputs))\ntarget_batch = Variable(torch.LongTensor(targets))\n\nclass BiLSTM_Attention(nn.Module):\n    def __init__(self):\n        super(BiLSTM_Attention, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, n_hidden, bidirectional=True)\n        self.out = nn.Linear(n_hidden * 2, num_classes)\n\n    # lstm_output : [batch_size, n_step, n_hidden * num_directions(=2)], F matrix\n    def attention_net(self, lstm_output, final_state):\n        hidden = final_state.view(-1, n_hidden * 2, 1)   # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, n_step]\n        soft_attn_weights = F.softmax(attn_weights, 1)\n        # [batch_size, n_hidden * num_directions(=2), n_step] * [batch_size, n_step, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n        return context, soft_attn_weights.data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n\n    def forward(self, X):\n        input = self.embedding(X) # input : [batch_size, len_seq, embedding_dim]\n        input = input.permute(1, 0, 2) # input : [len_seq, batch_size, embedding_dim]\n\n        hidden_state = Variable(torch.zeros(1*2, len(X), n_hidden)) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n        cell_state = Variable(torch.zeros(1*2, len(X), n_hidden)) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n\n        # final_hidden_state, final_cell_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n        output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state))\n        output = output.permute(1, 0, 2) # output : [batch_size, len_seq, n_hidden]\n        attn_output, attention = self.attention_net(output, final_hidden_state)\n        return self.out(attn_output), attention # model : [batch_size, num_classes], attention : [batch_size, n_step]\n\nmodel = BiLSTM_Attention()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training\nfor epoch in range(5000):\n    optimizer.zero_grad()\n    output, attention = model(input_batch)\n    loss = criterion(output, target_batch)\n    if (epoch + 1) % 1000 == 0:\n        print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n\n    loss.backward()\n    optimizer.step()\n\n# Test\ntest_text = \'sorry hate you\'\ntests = [np.asarray([word_dict[n] for n in test_text.split()])]\ntest_batch = Variable(torch.LongTensor(tests))\n\n# Predict\npredict, _ = model(test_batch)\npredict = predict.data.max(1, keepdim=True)[1]\nif predict[0][0] == 0:\n    print(test_text,""is Bad Mean..."")\nelse:\n    print(test_text,""is Good Mean!!"")\n    \nfig = plt.figure(figsize=(6, 3)) # [batch_size, n_step]\nax = fig.add_subplot(1, 1, 1)\nax.matshow(attention, cmap=\'viridis\')\nax.set_xticklabels([\'\']+[\'first_word\', \'second_word\', \'third_word\'], fontdict={\'fontsize\': 14}, rotation=90)\nax.set_yticklabels([\'\']+[\'batch_1\', \'batch_2\', \'batch_3\', \'batch_4\', \'batch_5\', \'batch_6\'], fontdict={\'fontsize\': 14})\nplt.show()'"
5-1.Transformer/Transformer(Greedy_decoder)-Torch.py,0,"b'\'\'\'\n  code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612\n  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n              https://github.com/JayParks/transformer\n\'\'\'\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\n\ndtype = torch.FloatTensor\n# S: Symbol that shows starting of decoding input\n# E: Symbol that shows starting of decoding output\n# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\nsentences = [\'ich mochte ein bier P\', \'S i want a beer\', \'i want a beer E\']\n\n# Transformer Parameters\n# Padding Should be Zero index\nsrc_vocab = {\'P\' : 0, \'ich\' : 1, \'mochte\' : 2, \'ein\' : 3, \'bier\' : 4}\nsrc_vocab_size = len(src_vocab)\n\ntgt_vocab = {\'P\' : 0, \'i\' : 1, \'want\' : 2, \'a\' : 3, \'beer\' : 4, \'S\' : 5, \'E\' : 6}\nnumber_dict = {i: w for i, w in enumerate(tgt_vocab)}\ntgt_vocab_size = len(tgt_vocab)\n\nsrc_len = 5\ntgt_len = 5\n\nd_model = 512  # Embedding Size\nd_ff = 2048 # FeedForward dimension\nd_k = d_v = 64  # dimension of K(=Q), V\nn_layers = 6  # number of Encoder of Decoder Layer\nn_heads = 8  # number of heads in Multi-Head Attention\n\ndef make_batch(sentences):\n    input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]\n    return Variable(torch.LongTensor(input_batch)), Variable(torch.LongTensor(output_batch)), Variable(torch.LongTensor(target_batch))\n\ndef get_sinusoid_encoding_table(n_position, d_model):\n    def cal_angle(position, hid_idx):\n        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n    def get_posi_angle_vec(position):\n        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n\n    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n    return torch.FloatTensor(sinusoid_table)\n\ndef get_attn_pad_mask(seq_q, seq_k):\n    # print(seq_q)\n    batch_size, len_q = seq_q.size()\n    batch_size, len_k = seq_k.size()\n    # eq(zero) is PAD token\n    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n\ndef get_attn_subsequent_mask(seq):\n    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n    return subsequent_mask\n\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self):\n        super(ScaledDotProductAttention, self).__init__()\n\n    def forward(self, Q, K, V, attn_mask):\n        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n        attn = nn.Softmax(dim=-1)(scores)\n        context = torch.matmul(attn, V)\n        return context, attn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super(MultiHeadAttention, self).__init__()\n        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n        self.W_K = nn.Linear(d_model, d_k * n_heads)\n        self.W_V = nn.Linear(d_model, d_v * n_heads)\n    def forward(self, Q, K, V, attn_mask):\n        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n        residual, batch_size = Q, Q.size(0)\n        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n\n        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n\n        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n        output = nn.Linear(n_heads * d_v, d_model)(context)\n        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n\nclass PoswiseFeedForwardNet(nn.Module):\n    def __init__(self):\n        super(PoswiseFeedForwardNet, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n\n    def forward(self, inputs):\n        residual = inputs # inputs : [batch_size, len_q, d_model]\n        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n        output = self.conv2(output).transpose(1, 2)\n        return nn.LayerNorm(d_model)(output + residual)\n\nclass EncoderLayer(nn.Module):\n    def __init__(self):\n        super(EncoderLayer, self).__init__()\n        self.enc_self_attn = MultiHeadAttention()\n        self.pos_ffn = PoswiseFeedForwardNet()\n\n    def forward(self, enc_inputs, enc_self_attn_mask):\n        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n        return enc_outputs, attn\n\nclass DecoderLayer(nn.Module):\n    def __init__(self):\n        super(DecoderLayer, self).__init__()\n        self.dec_self_attn = MultiHeadAttention()\n        self.dec_enc_attn = MultiHeadAttention()\n        self.pos_ffn = PoswiseFeedForwardNet()\n\n    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n        dec_outputs = self.pos_ffn(dec_outputs)\n        return dec_outputs, dec_self_attn, dec_enc_attn\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_len+1, d_model),freeze=True)\n        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n\n    def forward(self, enc_inputs): # enc_inputs : [batch_size x source_len]\n        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))\n        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n        enc_self_attns = []\n        for layer in self.layers:\n            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n            enc_self_attns.append(enc_self_attn)\n        return enc_outputs, enc_self_attns\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(tgt_len+1, d_model),freeze=True)\n        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n\n    def forward(self, dec_inputs, enc_inputs, enc_outputs): # dec_inputs : [batch_size x target_len]\n        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(torch.LongTensor([[5,1,2,3,4]]))\n        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)\n        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs)\n        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n\n        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n\n        dec_self_attns, dec_enc_attns = [], []\n        for layer in self.layers:\n            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n            dec_self_attns.append(dec_self_attn)\n            dec_enc_attns.append(dec_enc_attn)\n        return dec_outputs, dec_self_attns, dec_enc_attns\n\nclass Transformer(nn.Module):\n    def __init__(self):\n        super(Transformer, self).__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False)\n    def forward(self, enc_inputs, dec_inputs):\n        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n        dec_logits = self.projection(dec_outputs) # dec_logits : [batch_size x src_vocab_size x tgt_vocab_size]\n        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n\ndef greedy_decoder(model, enc_input, start_symbol):\n    """"""\n    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don\'t know the\n    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.\n    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n    :param model: Transformer Model\n    :param enc_input: The encoder input\n    :param start_symbol: The start symbol. In this example it is \'S\' which corresponds to index 4\n    :return: The target input\n    """"""\n    enc_outputs, enc_self_attns = model.encoder(enc_input)\n    dec_input = torch.zeros(1, 5).type_as(enc_input.data)\n    next_symbol = start_symbol\n    for i in range(0, 5):\n        dec_input[0][i] = next_symbol\n        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n        projected = model.projection(dec_outputs)\n        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\n        next_word = prob.data[i]\n        next_symbol = next_word.item()\n    return dec_input\n\ndef showgraph(attn):\n    attn = attn[-1].squeeze(0)[0]\n    attn = attn.squeeze(0).data.numpy()\n    fig = plt.figure(figsize=(n_heads, n_heads)) # [n_heads, n_heads]\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap=\'viridis\')\n    ax.set_xticklabels([\'\']+sentences[0].split(), fontdict={\'fontsize\': 14}, rotation=90)\n    ax.set_yticklabels([\'\']+sentences[2].split(), fontdict={\'fontsize\': 14})\n    plt.show()\n\nmodel = Transformer()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nfor epoch in range(20):\n    optimizer.zero_grad()\n    enc_inputs, dec_inputs, target_batch = make_batch(sentences)\n    outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n    loss = criterion(outputs, target_batch.contiguous().view(-1))\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n    loss.backward()\n    optimizer.step()\n\n# Test\ngreedy_dec_input = greedy_decoder(model, enc_inputs, start_symbol=tgt_vocab[""S""])\npredict, _, _, _ = model(enc_inputs, greedy_dec_input)\npredict = predict.data.max(1, keepdim=True)[1]\nprint(sentences[0], \'->\', [number_dict[n.item()] for n in predict.squeeze()])\n\nprint(\'first head of last state enc_self_attns\')\nshowgraph(enc_self_attns)\n\nprint(\'first head of last state dec_self_attns\')\nshowgraph(dec_self_attns)\n\nprint(\'first head of last state dec_enc_attns\')\nshowgraph(dec_enc_attns)'"
5-1.Transformer/Transformer-Torch.py,0,"b""'''\n  code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612\n  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n              https://github.com/JayParks/transformer\n'''\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\n\ndtype = torch.FloatTensor\n# S: Symbol that shows starting of decoding input\n# E: Symbol that shows starting of decoding output\n# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\nsentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n\n# Transformer Parameters\n# Padding Should be Zero\nsrc_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4}\nsrc_vocab_size = len(src_vocab)\n\ntgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'S' : 5, 'E' : 6}\nnumber_dict = {i: w for i, w in enumerate(tgt_vocab)}\ntgt_vocab_size = len(tgt_vocab)\n\nsrc_len = 5\ntgt_len = 5\n\nd_model = 512  # Embedding Size\nd_ff = 2048 # FeedForward dimension\nd_k = d_v = 64  # dimension of K(=Q), V\nn_layers = 6  # number of Encoder of Decoder Layer\nn_heads = 8  # number of heads in Multi-Head Attention\n\ndef make_batch(sentences):\n    input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]\n    return Variable(torch.LongTensor(input_batch)), Variable(torch.LongTensor(output_batch)), Variable(torch.LongTensor(target_batch))\n\ndef get_sinusoid_encoding_table(n_position, d_model):\n    def cal_angle(position, hid_idx):\n        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n    def get_posi_angle_vec(position):\n        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n\n    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n    return torch.FloatTensor(sinusoid_table)\n\ndef get_attn_pad_mask(seq_q, seq_k):\n    batch_size, len_q = seq_q.size()\n    batch_size, len_k = seq_k.size()\n    # eq(zero) is PAD token\n    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n\ndef get_attn_subsequent_mask(seq):\n    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n    return subsequent_mask\n\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self):\n        super(ScaledDotProductAttention, self).__init__()\n\n    def forward(self, Q, K, V, attn_mask):\n        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n        attn = nn.Softmax(dim=-1)(scores)\n        context = torch.matmul(attn, V)\n        return context, attn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super(MultiHeadAttention, self).__init__()\n        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n        self.W_K = nn.Linear(d_model, d_k * n_heads)\n        self.W_V = nn.Linear(d_model, d_v * n_heads)\n    def forward(self, Q, K, V, attn_mask):\n        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n        residual, batch_size = Q, Q.size(0)\n        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n\n        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n\n        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n        output = nn.Linear(n_heads * d_v, d_model)(context)\n        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n\nclass PoswiseFeedForwardNet(nn.Module):\n    def __init__(self):\n        super(PoswiseFeedForwardNet, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n\n    def forward(self, inputs):\n        residual = inputs # inputs : [batch_size, len_q, d_model]\n        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n        output = self.conv2(output).transpose(1, 2)\n        return nn.LayerNorm(d_model)(output + residual)\n\nclass EncoderLayer(nn.Module):\n    def __init__(self):\n        super(EncoderLayer, self).__init__()\n        self.enc_self_attn = MultiHeadAttention()\n        self.pos_ffn = PoswiseFeedForwardNet()\n\n    def forward(self, enc_inputs, enc_self_attn_mask):\n        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n        return enc_outputs, attn\n\nclass DecoderLayer(nn.Module):\n    def __init__(self):\n        super(DecoderLayer, self).__init__()\n        self.dec_self_attn = MultiHeadAttention()\n        self.dec_enc_attn = MultiHeadAttention()\n        self.pos_ffn = PoswiseFeedForwardNet()\n\n    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n        dec_outputs = self.pos_ffn(dec_outputs)\n        return dec_outputs, dec_self_attn, dec_enc_attn\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_len+1, d_model),freeze=True)\n        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n\n    def forward(self, enc_inputs): # enc_inputs : [batch_size x source_len]\n        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))\n        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n        enc_self_attns = []\n        for layer in self.layers:\n            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n            enc_self_attns.append(enc_self_attn)\n        return enc_outputs, enc_self_attns\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(tgt_len+1, d_model),freeze=True)\n        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n\n    def forward(self, dec_inputs, enc_inputs, enc_outputs): # dec_inputs : [batch_size x target_len]\n        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(torch.LongTensor([[5,1,2,3,4]]))\n        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)\n        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs)\n        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n\n        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n\n        dec_self_attns, dec_enc_attns = [], []\n        for layer in self.layers:\n            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n            dec_self_attns.append(dec_self_attn)\n            dec_enc_attns.append(dec_enc_attn)\n        return dec_outputs, dec_self_attns, dec_enc_attns\n\nclass Transformer(nn.Module):\n    def __init__(self):\n        super(Transformer, self).__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False)\n    def forward(self, enc_inputs, dec_inputs):\n        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n        dec_logits = self.projection(dec_outputs) # dec_logits : [batch_size x src_vocab_size x tgt_vocab_size]\n        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n\nmodel = Transformer()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ndef showgraph(attn):\n    attn = attn[-1].squeeze(0)[0]\n    attn = attn.squeeze(0).data.numpy()\n    fig = plt.figure(figsize=(n_heads, n_heads)) # [n_heads, n_heads]\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    ax.set_xticklabels(['']+sentences[0].split(), fontdict={'fontsize': 14}, rotation=90)\n    ax.set_yticklabels(['']+sentences[2].split(), fontdict={'fontsize': 14})\n    plt.show()\n\nfor epoch in range(20):\n    optimizer.zero_grad()\n    enc_inputs, dec_inputs, target_batch = make_batch(sentences)\n    outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n    loss = criterion(outputs, target_batch.contiguous().view(-1))\n    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n    loss.backward()\n    optimizer.step()\n\n# Test\npredict, _, _, _ = model(enc_inputs, dec_inputs)\npredict = predict.data.max(1, keepdim=True)[1]\nprint(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n\nprint('first head of last state enc_self_attns')\nshowgraph(enc_self_attns)\n\nprint('first head of last state dec_self_attns')\nshowgraph(dec_self_attns)\n\nprint('first head of last state dec_enc_attns')\nshowgraph(dec_enc_attns)"""
5-2.BERT/BERT-Torch.py,0,"b'\'\'\'\n  code by Tae Hwan Jung(Jeff Jung) @graykode\n  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n              https://github.com/JayParks/transformer, https://github.com/dhlee347/pytorchic-bert\n\'\'\'\nimport math\nimport re\nfrom random import *\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\n# BERT Parameters\nmaxlen = 30\nbatch_size = 6\nmax_pred = 5 # max tokens of prediction\nn_layers = 6\nn_heads = 12\nd_model = 768\nd_ff = 768*4 # 4*d_model, FeedForward dimension\nd_k = d_v = 64  # dimension of K(=Q), V\nn_segments = 2\n\ntext = (\n    \'Hello, how are you? I am Romeo.\\n\'\n    \'Hello, Romeo My name is Juliet. Nice to meet you.\\n\'\n    \'Nice meet you too. How are you today?\\n\'\n    \'Great. My baseball team won the competition.\\n\'\n    \'Oh Congratulations, Juliet\\n\'\n    \'Thanks you Romeo\'\n)\nsentences = re.sub(""[.,!?\\\\-]"", \'\', text.lower()).split(\'\\n\') # filter \'.\', \',\', \'?\', \'!\'\nword_list = list(set("" "".join(sentences).split()))\nword_dict = {\'[PAD]\' : 0, \'[CLS]\' : 1, \'[SEP]\' : 2, \'[MASK]\' : 3}\nfor i, w in enumerate(word_list):\n    word_dict[w] = i + 4\nnumber_dict = {i: w for i, w in enumerate(word_dict)}\nvocab_size = len(word_dict)\n\ntoken_list = list()\nfor sentence in sentences:\n    arr = [word_dict[s] for s in sentence.split()]\n    token_list.append(arr)\n\n# sample IsNext and NotNext to be same in small batch size\ndef make_batch():\n    batch = []\n    positive = negative = 0\n    while positive != batch_size/2 or negative != batch_size/2:\n        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\n        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n        input_ids = [word_dict[\'[CLS]\']] + tokens_a + [word_dict[\'[SEP]\']] + tokens_b + [word_dict[\'[SEP]\']]\n        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n\n        # MASK LM\n        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n        cand_maked_pos = [i for i, token in enumerate(input_ids)\n                          if token != word_dict[\'[CLS]\'] and token != word_dict[\'[SEP]\']]\n        shuffle(cand_maked_pos)\n        masked_tokens, masked_pos = [], []\n        for pos in cand_maked_pos[:n_pred]:\n            masked_pos.append(pos)\n            masked_tokens.append(input_ids[pos])\n            if random() < 0.8:  # 80%\n                input_ids[pos] = word_dict[\'[MASK]\'] # make mask\n            elif random() < 0.5:  # 10%\n                index = randint(0, vocab_size - 1) # random index in vocabulary\n                input_ids[pos] = word_dict[number_dict[index]] # replace\n\n        # Zero Paddings\n        n_pad = maxlen - len(input_ids)\n        input_ids.extend([0] * n_pad)\n        segment_ids.extend([0] * n_pad)\n\n        # Zero Padding (100% - 15%) tokens\n        if max_pred > n_pred:\n            n_pad = max_pred - n_pred\n            masked_tokens.extend([0] * n_pad)\n            masked_pos.extend([0] * n_pad)\n\n        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n            positive += 1\n        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n            negative += 1\n    return batch\n# Proprecessing Finished\n\ndef get_attn_pad_mask(seq_q, seq_k):\n    batch_size, len_q = seq_q.size()\n    batch_size, len_k = seq_k.size()\n    # eq(zero) is PAD token\n    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n\ndef gelu(x):\n    ""Implementation of the gelu activation function by Hugging Face""\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\nclass Embedding(nn.Module):\n    def __init__(self):\n        super(Embedding, self).__init__()\n        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n        self.norm = nn.LayerNorm(d_model)\n\n    def forward(self, x, seg):\n        seq_len = x.size(1)\n        pos = torch.arange(seq_len, dtype=torch.long)\n        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n        return self.norm(embedding)\n\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self):\n        super(ScaledDotProductAttention, self).__init__()\n\n    def forward(self, Q, K, V, attn_mask):\n        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n        attn = nn.Softmax(dim=-1)(scores)\n        context = torch.matmul(attn, V)\n        return context, attn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super(MultiHeadAttention, self).__init__()\n        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n        self.W_K = nn.Linear(d_model, d_k * n_heads)\n        self.W_V = nn.Linear(d_model, d_v * n_heads)\n    def forward(self, Q, K, V, attn_mask):\n        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n        residual, batch_size = Q, Q.size(0)\n        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n\n        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n\n        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n        output = nn.Linear(n_heads * d_v, d_model)(context)\n        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n\nclass PoswiseFeedForwardNet(nn.Module):\n    def __init__(self):\n        super(PoswiseFeedForwardNet, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n        return self.fc2(gelu(self.fc1(x)))\n\nclass EncoderLayer(nn.Module):\n    def __init__(self):\n        super(EncoderLayer, self).__init__()\n        self.enc_self_attn = MultiHeadAttention()\n        self.pos_ffn = PoswiseFeedForwardNet()\n\n    def forward(self, enc_inputs, enc_self_attn_mask):\n        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n        return enc_outputs, attn\n\nclass BERT(nn.Module):\n    def __init__(self):\n        super(BERT, self).__init__()\n        self.embedding = Embedding()\n        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n        self.fc = nn.Linear(d_model, d_model)\n        self.activ1 = nn.Tanh()\n        self.linear = nn.Linear(d_model, d_model)\n        self.activ2 = gelu\n        self.norm = nn.LayerNorm(d_model)\n        self.classifier = nn.Linear(d_model, 2)\n        # decoder is shared with embedding layer\n        embed_weight = self.embedding.tok_embed.weight\n        n_vocab, n_dim = embed_weight.size()\n        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n        self.decoder.weight = embed_weight\n        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n\n    def forward(self, input_ids, segment_ids, masked_pos):\n        output = self.embedding(input_ids, segment_ids)\n        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n        for layer in self.layers:\n            output, enc_self_attn = layer(output, enc_self_attn_mask)\n        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n        # it will be decided by first token(CLS)\n        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n\n        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n        # get masked position from final output of transformer.\n        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n\n        return logits_lm, logits_clsf\n\nmodel = BERT()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nbatch = make_batch()\ninput_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\ninput_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens), \\\n    torch.LongTensor(masked_pos), torch.LongTensor(isNext)\n\nfor epoch in range(100):\n    optimizer.zero_grad()\n    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n    loss_lm = (loss_lm.float()).mean()\n    loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n    loss = loss_lm + loss_clsf\n    if (epoch + 1) % 10 == 0:\n        print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.6f}\'.format(loss))\n    loss.backward()\n    optimizer.step()\n\n# Predict mask tokens ans isNext\ninput_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[0]\nprint(text)\nprint([number_dict[w] for w in input_ids if number_dict[w] != \'[PAD]\'])\n\nlogits_lm, logits_clsf = model(torch.LongTensor([input_ids]), \\\n                               torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))\nlogits_lm = logits_lm.data.max(2)[1][0].data.numpy()\nprint(\'masked tokens list : \',[pos for pos in masked_tokens if pos != 0])\nprint(\'predict masked tokens list : \',[pos for pos in logits_lm if pos != 0])\n\nlogits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\nprint(\'isNext : \', True if isNext else False)\nprint(\'predict isNext : \',True if logits_clsf else False)\n'"
