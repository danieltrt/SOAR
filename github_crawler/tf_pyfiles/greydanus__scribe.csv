file_path,api_count,code
model.py,70,"b'import numpy as np\nimport numpy.matlib\nimport math\nimport random\nimport os\nimport xml.etree.ElementTree as ET\n\nimport tensorflow as tf\nfrom utils import *\n\nclass Model():\n\tdef __init__(self, args, logger):\n\t\tself.logger = logger\n\n\t\t# ----- transfer some of the args params over to the model\n\n\t\t# model params\n\t\tself.rnn_size = args.rnn_size\n\t\tself.train = args.train\n\t\tself.nmixtures = args.nmixtures\n\t\tself.kmixtures = args.kmixtures\n\t\tself.batch_size = args.batch_size if self.train else 1 # training/sampling specific\n\t\tself.tsteps = args.tsteps if self.train else 1 # training/sampling specific\n\t\tself.alphabet = args.alphabet\n\t\t# training params\n\t\tself.dropout = args.dropout\n\t\tself.grad_clip = args.grad_clip\n\t\t# misc\n\t\tself.tsteps_per_ascii = args.tsteps_per_ascii\n\t\tself.data_dir = args.data_dir\n\n\t\tself.graves_initializer = tf.truncated_normal_initializer(mean=0., stddev=.075, seed=None, dtype=tf.float32)\n\t\tself.window_b_initializer = tf.truncated_normal_initializer(mean=-3.0, stddev=.25, seed=None, dtype=tf.float32) # hacky initialization\n\n\t\tself.logger.write(\'\\tusing alphabet{}\'.format(self.alphabet))\n\t\tself.char_vec_len = len(self.alphabet) + 1 #plus one for <UNK> token\n\t\tself.ascii_steps = args.tsteps/args.tsteps_per_ascii\n\n\n\t\t# ----- build the basic recurrent network architecture\n\t\tcell_func = tf.contrib.rnn.LSTMCell # could be GRUCell or RNNCell\n\t\tself.cell0 = cell_func(args.rnn_size, state_is_tuple=True, initializer=self.graves_initializer)\n\t\tself.cell1 = cell_func(args.rnn_size, state_is_tuple=True, initializer=self.graves_initializer)\n\t\tself.cell2 = cell_func(args.rnn_size, state_is_tuple=True, initializer=self.graves_initializer)\n\n\t\tif (self.train and self.dropout < 1): # training mode\n\t\t\tself.cell0 = tf.contrib.rnn.DropoutWrapper(self.cell0, output_keep_prob = self.dropout)\n\t\t\tself.cell1 = tf.contrib.rnn.DropoutWrapper(self.cell1, output_keep_prob = self.dropout)\n\t\t\tself.cell2 = tf.contrib.rnn.DropoutWrapper(self.cell2, output_keep_prob = self.dropout)\n\n\t\tself.input_data = tf.placeholder(dtype=tf.float32, shape=[None, self.tsteps, 3])\n\t\tself.target_data = tf.placeholder(dtype=tf.float32, shape=[None, self.tsteps, 3])\n\t\tself.istate_cell0 = self.cell0.zero_state(batch_size=self.batch_size, dtype=tf.float32)\n\t\tself.istate_cell1 = self.cell1.zero_state(batch_size=self.batch_size, dtype=tf.float32)\n\t\tself.istate_cell2 = self.cell2.zero_state(batch_size=self.batch_size, dtype=tf.float32)\n\n\t\t#slice the input volume into separate vols for each tstep\n\t\tinputs = [tf.squeeze(input_, [1]) for input_ in tf.split(self.input_data, self.tsteps, 1)]\n\t\t#build cell0 computational graph\n\t\touts_cell0, self.fstate_cell0 = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.istate_cell0, self.cell0, loop_function=None, scope=\'cell0\')\n\n\n\t# ----- build the gaussian character window\n\t\tdef get_window(alpha, beta, kappa, c):\n\t\t\t# phi -> [? x 1 x ascii_steps] and is a tf matrix\n\t\t\t# c -> [? x ascii_steps x alphabet] and is a tf matrix\n\t\t\tascii_steps = c.get_shape()[1].value #number of items in sequence\n\t\t\tphi = get_phi(ascii_steps, alpha, beta, kappa)\n\t\t\twindow = tf.matmul(phi,c)\n\t\t\twindow = tf.squeeze(window, [1]) # window ~ [?,alphabet]\n\t\t\treturn window, phi\n\n\t\t#get phi for all t,u (returns a [1 x tsteps] matrix) that defines the window\n\t\tdef get_phi(ascii_steps, alpha, beta, kappa):\n\t\t\t# alpha, beta, kappa -> [?,kmixtures,1] and each is a tf variable\n\t\t\tu = np.linspace(0,ascii_steps-1,ascii_steps) # weight all the U items in the sequence\n\t\t\tkappa_term = tf.square( tf.subtract(kappa,u))\n\t\t\texp_term = tf.multiply(-beta,kappa_term)\n\t\t\tphi_k = tf.multiply(alpha, tf.exp(exp_term))\n\t\t\tphi = tf.reduce_sum(phi_k,1, keep_dims=True)\n\t\t\treturn phi # phi ~ [?,1,ascii_steps]\n\n\t\tdef get_window_params(i, out_cell0, kmixtures, prev_kappa, reuse=True):\n\t\t\thidden = out_cell0.get_shape()[1]\n\t\t\tn_out = 3*kmixtures\n\t\t\twith tf.variable_scope(\'window\',reuse=reuse):\n\t\t\t\twindow_w = tf.get_variable(""window_w"", [hidden, n_out], initializer=self.graves_initializer)\n\t\t\t\twindow_b = tf.get_variable(""window_b"", [n_out], initializer=self.window_b_initializer)\n\t\t\tabk_hats = tf.nn.xw_plus_b(out_cell0, window_w, window_b) # abk_hats ~ [?,n_out]\n\t\t\tabk = tf.exp(tf.reshape(abk_hats, [-1, 3*kmixtures,1])) # abk_hats ~ [?,n_out] = ""alpha, beta, kappa hats""\n\n\t\t\talpha, beta, kappa = tf.split(abk, 3, 1) # alpha_hat, etc ~ [?,kmixtures]\n\t\t\tkappa = kappa + prev_kappa\n\t\t\treturn alpha, beta, kappa # each ~ [?,kmixtures,1]\n\n\t\tself.init_kappa = tf.placeholder(dtype=tf.float32, shape=[None, self.kmixtures, 1]) \n\t\tself.char_seq = tf.placeholder(dtype=tf.float32, shape=[None, self.ascii_steps, self.char_vec_len])\n\t\tprev_kappa = self.init_kappa\n\t\tprev_window = self.char_seq[:,0,:]\n\n\t\t#add gaussian window result\n\t\treuse = False\n\t\tfor i in range(len(outs_cell0)):\n\t\t\t[alpha, beta, new_kappa] = get_window_params(i, outs_cell0[i], self.kmixtures, prev_kappa, reuse=reuse)\n\t\t\twindow, phi = get_window(alpha, beta, new_kappa, self.char_seq)\n\t\t\touts_cell0[i] = tf.concat((outs_cell0[i],window), 1) #concat outputs\n\t\t\touts_cell0[i] = tf.concat((outs_cell0[i],inputs[i]), 1) #concat input data\n\t\t\tprev_kappa = new_kappa\n\t\t\tprev_window = window\n\t\t\treuse = True\n\t\t#save some attention mechanism params (useful for sampling/debugging later)\n\t\tself.window = window\n\t\tself.phi = phi\n\t\tself.new_kappa = new_kappa\n\t\tself.alpha = alpha\n\n\n\t# ----- finish building LSTMs 2 and 3\n\t\touts_cell1, self.fstate_cell1 = tf.contrib.legacy_seq2seq.rnn_decoder(outs_cell0, self.istate_cell1, self.cell1, loop_function=None, scope=\'cell1\')\n\n\t\touts_cell2, self.fstate_cell2 = tf.contrib.legacy_seq2seq.rnn_decoder(outs_cell1, self.istate_cell2, self.cell2, loop_function=None, scope=\'cell2\')\n\n\t# ----- start building the Mixture Density Network on top (start with a dense layer to predict the MDN params)\n\t\tn_out = 1 + self.nmixtures * 6 # params = end_of_stroke + 6 parameters per Gaussian\n\t\twith tf.variable_scope(\'mdn_dense\'):\n\t\t\tmdn_w = tf.get_variable(""output_w"", [self.rnn_size, n_out], initializer=self.graves_initializer)\n\t\t\tmdn_b = tf.get_variable(""output_b"", [n_out], initializer=self.graves_initializer)\n\n\t\tout_cell2 = tf.reshape(tf.concat(outs_cell2, 1), [-1, args.rnn_size]) #concat outputs for efficiency\n\t\toutput = tf.nn.xw_plus_b(out_cell2, mdn_w, mdn_b) #data flows through dense nn\n\n\n\t# ----- build mixture density cap on top of second recurrent cell\n\t\tdef gaussian2d(x1, x2, mu1, mu2, s1, s2, rho):\n\t\t\t# define gaussian mdn (eq 24, 25 from http://arxiv.org/abs/1308.0850)\n\t\t\tx_mu1 = tf.subtract(x1, mu1)\n\t\t\tx_mu2 = tf.subtract(x2, mu2)\n\t\t\tZ = tf.square(tf.div(x_mu1, s1)) + \\\n\t\t\t    tf.square(tf.div(x_mu2, s2)) - \\\n\t\t\t    2*tf.div(tf.multiply(rho, tf.multiply(x_mu1, x_mu2)), tf.multiply(s1, s2))\n\t\t\trho_square_term = 1-tf.square(rho)\n\t\t\tpower_e = tf.exp(tf.div(-Z,2*rho_square_term))\n\t\t\tregularize_term = 2*np.pi*tf.multiply(tf.multiply(s1, s2), tf.sqrt(rho_square_term))\n\t\t\tgaussian = tf.div(power_e, regularize_term)\n\t\t\treturn gaussian\n\n\t\tdef get_loss(pi, x1_data, x2_data, eos_data, mu1, mu2, sigma1, sigma2, rho, eos):\n\t\t\t# define loss function (eq 26 of http://arxiv.org/abs/1308.0850)\n\t\t\tgaussian = gaussian2d(x1_data, x2_data, mu1, mu2, sigma1, sigma2, rho)\n\t\t\tterm1 = tf.multiply(gaussian, pi)\n\t\t\tterm1 = tf.reduce_sum(term1, 1, keep_dims=True) #do inner summation\n\t\t\tterm1 = -tf.log(tf.maximum(term1, 1e-20)) # some errors are zero -> numerical errors.\n\n\t\t\tterm2 = tf.multiply(eos, eos_data) + tf.multiply(1-eos, 1-eos_data) #modified Bernoulli -> eos probability\n\t\t\tterm2 = -tf.log(term2) #negative log error gives loss\n\n\t\t\treturn tf.reduce_sum(term1 + term2) #do outer summation\n\n\t\t# now transform dense NN outputs into params for MDN\n\t\tdef get_mdn_coef(Z):\n\t\t\t# returns the tf slices containing mdn dist params (eq 18...23 of http://arxiv.org/abs/1308.0850)\n\t\t\teos_hat = Z[:, 0:1] #end of sentence tokens\n\t\t\tpi_hat, mu1_hat, mu2_hat, sigma1_hat, sigma2_hat, rho_hat = tf.split(Z[:, 1:], 6, 1)\n\t\t\tself.pi_hat, self.sigma1_hat, self.sigma2_hat = \\\n\t\t\t\t\t\t\t\t\t\tpi_hat, sigma1_hat, sigma2_hat # these are useful for bias method during sampling\n\n\t\t\teos = tf.sigmoid(-1*eos_hat) # technically we gained a negative sign\n\t\t\tpi = tf.nn.softmax(pi_hat) # softmax z_pi:\n\t\t\tmu1 = mu1_hat; mu2 = mu2_hat # leave mu1, mu2 as they are\n\t\t\tsigma1 = tf.exp(sigma1_hat); sigma2 = tf.exp(sigma2_hat) # exp for sigmas\n\t\t\trho = tf.tanh(rho_hat) # tanh for rho (squish between -1 and 1)rrr\n\n\t\t\treturn [eos, pi, mu1, mu2, sigma1, sigma2, rho]\n\n\t\t# reshape target data (as we did the input data)\n\t\tflat_target_data = tf.reshape(self.target_data,[-1, 3])\n\t\t[x1_data, x2_data, eos_data] = tf.split(flat_target_data, 3, 1) #we might as well split these now\n\n\t\t[self.eos, self.pi, self.mu1, self.mu2, self.sigma1, self.sigma2, self.rho] = get_mdn_coef(output)\n\n\t\tloss = get_loss(self.pi, x1_data, x2_data, eos_data, self.mu1, self.mu2, self.sigma1, self.sigma2, self.rho, self.eos)\n\t\tself.cost = loss / (self.batch_size * self.tsteps)\n\n\t\t# ----- bring together all variables and prepare for training\n\t\tself.learning_rate = tf.Variable(0.0, trainable=False)\n\t\tself.decay = tf.Variable(0.0, trainable=False)\n\t\tself.momentum = tf.Variable(0.0, trainable=False)\n\n\t\ttvars = tf.trainable_variables()\n\t\tgrads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), self.grad_clip)\n\n\t\tif args.optimizer == \'adam\':\n\t\t\tself.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n\t\telif args.optimizer == \'rmsprop\':\n\t\t\tself.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate, decay=self.decay, momentum=self.momentum)\n\t\telse:\n\t\t\traise ValueError(""Optimizer type not recognized"")\n\t\tself.train_op = self.optimizer.apply_gradients(zip(grads, tvars))\n\n\t\t# ----- some TensorFlow I/O\n\t\tself.sess = tf.InteractiveSession()\n\t\tself.saver = tf.train.Saver(tf.global_variables())\n\t\tself.sess.run(tf.global_variables_initializer())\n\n\t\t# ----- for restoring previous models\n\tdef try_load_model(self, save_path):\n\t\tload_was_success = True # yes, I\'m being optimistic\n\t\tglobal_step = 0\n\t\ttry:\n\t\t\tsave_dir = \'/\'.join(save_path.split(\'/\')[:-1])\n\t\t\tckpt = tf.train.get_checkpoint_state(save_dir)\n\t\t\tload_path = ckpt.model_checkpoint_path\n\t\t\tself.saver.restore(self.sess, load_path)\n\t\texcept:\n\t\t\tself.logger.write(""no saved model to load. starting new session"")\n\t\t\tload_was_success = False\n\t\telse:\n\t\t\tself.logger.write(""loaded model: {}"".format(load_path))\n\t\t\tself.saver = tf.train.Saver(tf.global_variables())\n\t\t\tglobal_step = int(load_path.split(\'-\')[-1])\n\t\treturn load_was_success, global_step\n'"
run.py,4,"b'import numpy as np\nimport tensorflow as tf\n\nimport argparse\nimport time\nimport os\n\nfrom model import Model\nfrom utils import *\nfrom sample import *\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\n\t#general model params\n\tparser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', help=\'train the model\')\n\tparser.add_argument(\'--sample\', dest=\'train\', action=\'store_false\', help=\'sample from the model\')\n\tparser.add_argument(\'--rnn_size\', type=int, default=100, help=\'size of RNN hidden state\')\n\tparser.add_argument(\'--tsteps\', type=int, default=150, help=\'RNN time steps (for backprop)\')\n\tparser.add_argument(\'--nmixtures\', type=int, default=8, help=\'number of gaussian mixtures\')\n\n\t# window params\n\tparser.add_argument(\'--kmixtures\', type=int, default=1, help=\'number of gaussian mixtures for character window\')\n\tparser.add_argument(\'--alphabet\', type=str, default=\' abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\', \\\n\t\t\t\t\t\thelp=\'default is a-z, A-Z, space, and <UNK> tag\')\n\tparser.add_argument(\'--tsteps_per_ascii\', type=int, default=25, help=\'expected number of pen points per character\')\n\n\t# training params\n\tparser.add_argument(\'--batch_size\', type=int, default=32, help=\'batch size for each gradient step\')\n\tparser.add_argument(\'--nbatches\', type=int, default=500, help=\'number of batches per epoch\')\n\tparser.add_argument(\'--nepochs\', type=int, default=250, help=\'number of epochs\')\n\tparser.add_argument(\'--dropout\', type=float, default=0.85, help=\'probability of keeping neuron during dropout\')\n\n\tparser.add_argument(\'--grad_clip\', type=float, default=10., help=\'clip gradients to this magnitude\')\n\tparser.add_argument(\'--optimizer\', type=str, default=\'rmsprop\', help=""ctype of optimizer: \'rmsprop\' \'adam\'"")\n\tparser.add_argument(\'--learning_rate\', type=float, default=1e-4, help=\'learning rate\')\n\tparser.add_argument(\'--lr_decay\', type=float, default=1.0, help=\'decay rate for learning rate\')\n\tparser.add_argument(\'--decay\', type=float, default=0.95, help=\'decay rate for rmsprop\')\n\tparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'momentum for rmsprop\')\n\n\t#book-keeping\n\tparser.add_argument(\'--data_scale\', type=int, default=50, help=\'amount to scale data down before training\')\n\tparser.add_argument(\'--log_dir\', type=str, default=\'./logs/\', help=\'location, relative to execution, of log files\')\n\tparser.add_argument(\'--data_dir\', type=str, default=\'./data\', help=\'location, relative to execution, of data\')\n\tparser.add_argument(\'--save_path\', type=str, default=\'saved/model.ckpt\', help=\'location to save model\')\n\tparser.add_argument(\'--save_every\', type=int, default=500, help=\'number of batches between each save\')\n\n\t#sampling\n\tparser.add_argument(\'--text\', type=str, default=\'\', help=\'string for sampling model (defaults to test cases)\')\n\tparser.add_argument(\'--style\', type=int, default=-1, help=\'optionally condition model on a preset style (using data in styles.p)\')\n\tparser.add_argument(\'--bias\', type=float, default=1.0, help=\'higher bias means neater, lower means more diverse (range is 0-5)\')\n\tparser.add_argument(\'--sleep_time\', type=int, default=60*5, help=\'time to sleep between running sampler\')\n\tparser.set_defaults(train=True)\n\targs = parser.parse_args()\n\n\ttrain_model(args) if args.train else sample_model(args)\n\ndef train_model(args):\n\tlogger = Logger(args) # make logging utility\n\tlogger.write(""\\nTRAINING MODE..."")\n\tlogger.write(""{}\\n"".format(args))\n\tlogger.write(""loading data..."")\n\tdata_loader = DataLoader(args, logger=logger)\n\t\n\tlogger.write(""building model..."")\n\tmodel = Model(args, logger=logger)\n\n\tlogger.write(""attempt to load saved model..."")\n\tload_was_success, global_step = model.try_load_model(args.save_path)\n\n\tv_x, v_y, v_s, v_c = data_loader.validation_data()\n\tvalid_inputs = {model.input_data: v_x, model.target_data: v_y, model.char_seq: v_c}\n\n\tlogger.write(""training..."")\n\tmodel.sess.run(tf.assign(model.decay, args.decay ))\n\tmodel.sess.run(tf.assign(model.momentum, args.momentum ))\n\trunning_average = 0.0 ; remember_rate = 0.99\n\tfor e in range(global_step/args.nbatches, args.nepochs):\n\t\tmodel.sess.run(tf.assign(model.learning_rate, args.learning_rate * (args.lr_decay ** e)))\n\t\tlogger.write(""learning rate: {}"".format(model.learning_rate.eval()))\n\n\t\tc0, c1, c2 = model.istate_cell0.c.eval(), model.istate_cell1.c.eval(), model.istate_cell2.c.eval()\n\t\th0, h1, h2 = model.istate_cell0.h.eval(), model.istate_cell1.h.eval(), model.istate_cell2.h.eval()\n\t\tkappa = np.zeros((args.batch_size, args.kmixtures, 1))\n\n\t\tfor b in range(global_step%args.nbatches, args.nbatches):\n\t\t\t\n\t\t\ti = e * args.nbatches + b\n\t\t\tif global_step is not 0 : i+=1 ; global_step = 0\n\n\t\t\tif i % args.save_every == 0 and (i > 0):\n\t\t\t\tmodel.saver.save(model.sess, args.save_path, global_step = i) ; logger.write(\'SAVED MODEL\')\n\n\t\t\tstart = time.time()\n\t\t\tx, y, s, c = data_loader.next_batch()\n\n\t\t\tfeed = {model.input_data: x, model.target_data: y, model.char_seq: c, model.init_kappa: kappa, \\\n\t\t\t\t\tmodel.istate_cell0.c: c0, model.istate_cell1.c: c1, model.istate_cell2.c: c2, \\\n\t\t\t\t\tmodel.istate_cell0.h: h0, model.istate_cell1.h: h1, model.istate_cell2.h: h2}\n\n\t\t\t[train_loss, _] = model.sess.run([model.cost, model.train_op], feed)\n\t\t\tfeed.update(valid_inputs)\n\t\t\tfeed[model.init_kappa] = np.zeros((args.batch_size, args.kmixtures, 1))\n\t\t\t[valid_loss] = model.sess.run([model.cost], feed)\n\t\t\t\n\t\t\trunning_average = running_average*remember_rate + train_loss*(1-remember_rate)\n\n\t\t\tend = time.time()\n\t\t\tif i % 10 is 0: logger.write(""{}/{}, loss = {:.3f}, regloss = {:.5f}, valid_loss = {:.3f}, time = {:.3f}"" \\\n\t\t\t\t.format(i, args.nepochs * args.nbatches, train_loss, running_average, valid_loss, end - start) )\n\ndef sample_model(args, logger=None):\n\tif args.text == \'\':\n\t\tstrings = [\'call me ishmael some years ago\', \'A project by Sam Greydanus\', \'mmm mmm mmm mmm mmm mmm mmm\', \\\n\t\t\t\'What I cannot create I do not understand\', \'You know nothing Jon Snow\'] # test strings\n\telse:\n\t\tstrings = [args.text]\n\n\tlogger = Logger(args) if logger is None else logger # instantiate logger, if None\n\tlogger.write(""\\nSAMPLING MODE..."")\n\tlogger.write(""loading data..."")\n\t\n\tlogger.write(""building model..."")\n\tmodel = Model(args, logger)\n\n\tlogger.write(""attempt to load saved model..."")\n\tload_was_success, global_step = model.try_load_model(args.save_path)\n\n\tif load_was_success:\n\t\tfor s in strings:\n\t\t\tstrokes, phis, windows, kappas = sample(s, model, args)\n\n\t\t\tw_save_path = \'{}figures/iter-{}-w-{}\'.format(args.log_dir, global_step, s[:10].replace(\' \', \'_\'))\n\t\t\tg_save_path = \'{}figures/iter-{}-g-{}\'.format(args.log_dir, global_step, s[:10].replace(\' \', \'_\'))\n\t\t\tl_save_path = \'{}figures/iter-{}-l-{}\'.format(args.log_dir, global_step, s[:10].replace(\' \', \'_\'))\n\n\t\t\twindow_plots(phis, windows, save_path=w_save_path)\n\t\t\tgauss_plot(strokes, \'Heatmap for ""{}""\'.format(s), figsize = (2*len(s),4), save_path=g_save_path)\n\t\t\tline_plot(strokes, \'Line plot for ""{}""\'.format(s), figsize = (len(s),2), save_path=l_save_path)\n\n\t\t\t# make sure that kappas are reasonable\n\t\t\tlogger.write( ""kappas: \\n{}"".format(str(kappas[min(kappas.shape[0]-1, args.tsteps_per_ascii),:])) )\n\telse:\n\t\tlogger.write(""load failed, sampling canceled"")\n\n\tif True:\n\t\ttf.reset_default_graph()\n\t\ttime.sleep(args.sleep_time)\n\t\tsample_model(args, logger=logger)\n\nif __name__ == \'__main__\':\n\tmain()\n'"
sample.py,0,"b'import numpy as np\nimport tensorflow as tf\nimport cPickle as pickle\n\nfrom utils import *\n\ndef sample_gaussian2d(mu1, mu2, s1, s2, rho):\n    mean = [mu1, mu2]\n    cov = [[s1*s1, rho*s1*s2], [rho*s1*s2, s2*s2]]\n    x = np.random.multivariate_normal(mean, cov, 1)\n    return x[0][0], x[0][1]\n\ndef get_style_states(model, args):\n    c0, c1, c2 = model.istate_cell0.c.eval(), model.istate_cell1.c.eval(), model.istate_cell2.c.eval()\n    h0, h1, h2 = model.istate_cell0.h.eval(), model.istate_cell1.h.eval(), model.istate_cell2.h.eval()\n    if args.style is -1: return [c0, c1, c2, h0, h1, h2] #model \'chooses\' random style\n\n    with open(os.path.join(args.data_dir, \'styles.p\'),\'r\') as f:\n        style_strokes, style_strings = pickle.load(f)\n\n    style_strokes, style_string = style_strokes[args.style], style_strings[args.style]\n    style_onehot = [to_one_hot(style_string, model.ascii_steps, args.alphabet)]\n        \n    style_stroke = np.zeros((1, 1, 3), dtype=np.float32)\n    style_kappa = np.zeros((1, args.kmixtures, 1))\n    prime_len = 500 # must be <= 700\n    \n    for i in xrange(prime_len):\n        style_stroke[0][0] = style_strokes[i,:]\n        feed = {model.input_data: style_stroke, model.char_seq: style_onehot, model.init_kappa: style_kappa, \\\n                model.istate_cell0.c: c0, model.istate_cell1.c: c1, model.istate_cell2.c: c2, \\\n                model.istate_cell0.h: h0, model.istate_cell1.h: h1, model.istate_cell2.h: h2}\n        fetch = [model.new_kappa, \\\n                 model.fstate_cell0.c, model.fstate_cell1.c, model.fstate_cell2.c,\n                 model.fstate_cell0.h, model.fstate_cell1.h, model.fstate_cell2.h]\n        [style_kappa, c0, c1, c2, h0, h1, h2] = model.sess.run(fetch, feed)\n    return [c0, c1, c2, np.zeros_like(h0), np.zeros_like(h1), np.zeros_like(h2)] #only the c vectors should be primed\n\ndef sample(input_text, model, args):\n    # initialize some parameters\n    one_hot = [to_one_hot(input_text, model.ascii_steps, args.alphabet)]         # convert input string to one-hot vector\n    [c0, c1, c2, h0, h1, h2] = get_style_states(model, args) # get numpy zeros states for all three LSTMs\n    kappa = np.zeros((1, args.kmixtures, 1))   # attention mechanism\'s read head should start at index 0\n    prev_x = np.asarray([[[0, 0, 1]]], dtype=np.float32)     # start with a pen stroke at (0,0)\n\n    strokes, pis, windows, phis, kappas = [], [], [], [], [] # the data we\'re going to generate will go here\n\n    finished = False ; i = 0\n    while not finished:\n        feed = {model.input_data: prev_x, model.char_seq: one_hot, model.init_kappa: kappa, \\\n                model.istate_cell0.c: c0, model.istate_cell1.c: c1, model.istate_cell2.c: c2, \\\n                model.istate_cell0.h: h0, model.istate_cell1.h: h1, model.istate_cell2.h: h2}\n        fetch = [model.pi_hat, model.mu1, model.mu2, model.sigma1_hat, model.sigma2_hat, model.rho, model.eos, \\\n                 model.window, model.phi, model.new_kappa, model.alpha, \\\n                 model.fstate_cell0.c, model.fstate_cell1.c, model.fstate_cell2.c,\\\n                 model.fstate_cell0.h, model.fstate_cell1.h, model.fstate_cell2.h]\n        [pi_hat, mu1, mu2, sigma1_hat, sigma2_hat, rho, eos, window, phi, kappa, alpha, \\\n                 c0, c1, c2, h0, h1, h2] = model.sess.run(fetch, feed)\n        \n        #bias stuff:\n        sigma1 = np.exp(sigma1_hat - args.bias) ; sigma2 = np.exp(sigma2_hat - args.bias)\n        pi_hat *= 1 + args.bias # apply bias\n        pi = np.zeros_like(pi_hat) # need to preallocate\n        pi[0] = np.exp(pi_hat[0]) / np.sum(np.exp(pi_hat[0]), axis=0) # softmax\n        \n        # choose a component from the MDN\n        idx = np.random.choice(pi.shape[1], p=pi[0])\n\teos = 1 if 0.35 < eos[0][0] else 0 # use 0.5 as arbitrary boundary\n        x1, x2 = sample_gaussian2d(mu1[0][idx], mu2[0][idx], sigma1[0][idx], sigma2[0][idx], rho[0][idx])\n            \n        # store the info at this time step\n        windows.append(window)\n        phis.append(phi[0])\n        kappas.append(kappa[0].T)\n        pis.append(pi[0])\n        strokes.append([mu1[0][idx], mu2[0][idx], sigma1[0][idx], sigma2[0][idx], rho[0][idx], eos])\n        \n        # test if finished (has the read head seen the whole ascii sequence?)\n        # main_kappa_idx = np.where(alpha[0]==np.max(alpha[0]));\n        # finished = True if kappa[0][main_kappa_idx] > len(input_text) else False\n        finished = True if i > args.tsteps else False\n        \n        # new input is previous output\n        prev_x[0][0] = np.array([x1, x2, eos], dtype=np.float32)\n        i+=1\n\n    windows = np.vstack(windows)\n    phis = np.vstack(phis)\n    kappas = np.vstack(kappas)\n    strokes = np.vstack(strokes)\n\n    # the network predicts the displacements between pen points, so do a running sum over the time dimension\n    strokes[:,:2] = np.cumsum(strokes[:,:2], axis=0)\n    return strokes, phis, windows, kappas\n\n\n# plots parameters from the attention mechanism\ndef window_plots(phis, windows, save_path=\'.\'):\n    import matplotlib.cm as cm\n    import matplotlib as mpl\n    mpl.use(\'Agg\')\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(16,4))\n    plt.subplot(121)\n    plt.title(\'Phis\', fontsize=20)\n    plt.xlabel(""ascii #"", fontsize=15)\n    plt.ylabel(""time steps"", fontsize=15)\n    plt.imshow(phis, interpolation=\'nearest\', aspect=\'auto\', cmap=cm.jet)\n    plt.subplot(122)\n    plt.title(\'Soft attention window\', fontsize=20)\n    plt.xlabel(""one-hot vector"", fontsize=15)\n    plt.ylabel(""time steps"", fontsize=15)\n    plt.imshow(windows, interpolation=\'nearest\', aspect=\'auto\', cmap=cm.jet)\n    plt.savefig(save_path)\n    plt.clf() ; plt.cla()\n\n# a heatmap for the probabilities of each pen point in the sequence\ndef gauss_plot(strokes, title, figsize = (20,2), save_path=\'.\'):\n    import matplotlib.mlab as mlab\n    import matplotlib.cm as cm\n    import matplotlib as mpl\n    mpl.use(\'Agg\')\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=figsize) #\n    buff = 1 ; epsilon = 1e-4\n    minx, maxx = np.min(strokes[:,0])-buff, np.max(strokes[:,0])+buff\n    miny, maxy = np.min(strokes[:,1])-buff, np.max(strokes[:,1])+buff\n    delta = abs(maxx-minx)/400. ;\n\n    x = np.arange(minx, maxx, delta)\n    y = np.arange(miny, maxy, delta)\n    X, Y = np.meshgrid(x, y)\n    Z = np.zeros_like(X)\n    for i in range(strokes.shape[0]):\n        gauss = mlab.bivariate_normal(X, Y, mux=strokes[i,0], muy=strokes[i,1], \\\n            sigmax=strokes[i,2], sigmay=strokes[i,3], sigmaxy=0) # sigmaxy=strokes[i,4] gives error\n        Z += gauss/(np.max(gauss) + epsilon)\n\n    plt.title(title, fontsize=20)\n    plt.imshow(Z)\n    plt.savefig(save_path)\n    plt.clf() ; plt.cla()\n\n# plots the stroke data (handwriting!)\ndef line_plot(strokes, title, figsize = (20,2), save_path=\'.\'):\n    import matplotlib as mpl\n    mpl.use(\'Agg\')\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=figsize)\n    eos_preds = np.where(strokes[:,-1] == 1)\n    eos_preds = [0] + list(eos_preds[0]) + [-1] #add start and end indices\n    for i in range(len(eos_preds)-1):\n        start = eos_preds[i]+1\n        stop = eos_preds[i+1]\n        plt.plot(strokes[start:stop,0], strokes[start:stop,1],\'b-\', linewidth=2.0) #draw a stroke\n    plt.title(title,  fontsize=20)\n    plt.gca().invert_yaxis()\n    plt.savefig(save_path)\n    plt.clf() ; plt.cla()\n'"
utils.py,0,"b'import numpy as np\nimport math\nimport random\nimport os\nimport cPickle as pickle\nimport xml.etree.ElementTree as ET\n\nfrom utils import *\n\nclass DataLoader():\n    def __init__(self, args, logger, limit = 500):\n        self.data_dir = args.data_dir\n        self.alphabet = args.alphabet\n        self.batch_size = args.batch_size\n        self.tsteps = args.tsteps\n        self.data_scale = args.data_scale # scale data down by this factor\n        self.ascii_steps = args.tsteps/args.tsteps_per_ascii\n        self.logger = logger\n        self.limit = limit # removes large noisy gaps in the data\n\n        data_file = os.path.join(self.data_dir, ""strokes_training_data.cpkl"")\n        stroke_dir = self.data_dir + ""/lineStrokes""\n        ascii_dir = self.data_dir + ""/ascii""\n\n        if not (os.path.exists(data_file)) :\n            self.logger.write(""\\tcreating training data cpkl file from raw source"")\n            self.preprocess(stroke_dir, ascii_dir, data_file)\n\n        self.load_preprocessed(data_file)\n        self.reset_batch_pointer()\n\n    def preprocess(self, stroke_dir, ascii_dir, data_file):\n        # create data file from raw xml files from iam handwriting source.\n        self.logger.write(""\\tparsing dataset..."")\n        \n        # build the list of xml files\n        filelist = []\n        # Set the directory you want to start from\n        rootDir = stroke_dir\n        for dirName, subdirList, fileList in os.walk(rootDir):\n            for fname in fileList:\n                filelist.append(dirName+""/""+fname)\n\n        # function to read each individual xml file\n        def getStrokes(filename):\n            tree = ET.parse(filename)\n            root = tree.getroot()\n\n            result = []\n\n            x_offset = 1e20\n            y_offset = 1e20\n            y_height = 0\n            for i in range(1, 4):\n                x_offset = min(x_offset, float(root[0][i].attrib[\'x\']))\n                y_offset = min(y_offset, float(root[0][i].attrib[\'y\']))\n                y_height = max(y_height, float(root[0][i].attrib[\'y\']))\n            y_height -= y_offset\n            x_offset -= 100\n            y_offset -= 100\n\n            for stroke in root[1].findall(\'Stroke\'):\n                points = []\n                for point in stroke.findall(\'Point\'):\n                    points.append([float(point.attrib[\'x\'])-x_offset,float(point.attrib[\'y\'])-y_offset])\n                result.append(points)\n            return result\n        \n        # function to read each individual xml file\n        def getAscii(filename, line_number):\n            with open(filename, ""r"") as f:\n                s = f.read()\n            s = s[s.find(""CSR""):]\n            if len(s.split(""\\n"")) > line_number+2:\n                s = s.split(""\\n"")[line_number+2]\n                return s\n            else:\n                return """"\n                \n        # converts a list of arrays into a 2d numpy int16 array\n        def convert_stroke_to_array(stroke):\n            n_point = 0\n            for i in range(len(stroke)):\n                n_point += len(stroke[i])\n            stroke_data = np.zeros((n_point, 3), dtype=np.int16)\n\n            prev_x = 0\n            prev_y = 0\n            counter = 0\n\n            for j in range(len(stroke)):\n                for k in range(len(stroke[j])):\n                    stroke_data[counter, 0] = int(stroke[j][k][0]) - prev_x\n                    stroke_data[counter, 1] = int(stroke[j][k][1]) - prev_y\n                    prev_x = int(stroke[j][k][0])\n                    prev_y = int(stroke[j][k][1])\n                    stroke_data[counter, 2] = 0\n                    if (k == (len(stroke[j])-1)): # end of stroke\n                        stroke_data[counter, 2] = 1\n                    counter += 1\n            return stroke_data\n\n        # build stroke database of every xml file inside iam database\n        strokes = []\n        asciis = []\n        for i in range(len(filelist)):\n            if (filelist[i][-3:] == \'xml\'):\n                stroke_file = filelist[i]\n#                 print \'processing \'+stroke_file\n                stroke = convert_stroke_to_array(getStrokes(stroke_file))\n                \n                ascii_file = stroke_file.replace(""lineStrokes"",""ascii"")[:-7] + "".txt""\n                line_number = stroke_file[-6:-4]\n                line_number = int(line_number) - 1\n                ascii = getAscii(ascii_file, line_number)\n                if len(ascii) > 10:\n                    strokes.append(stroke)\n                    asciis.append(ascii)\n                else:\n                    self.logger.write(""\\tline length was too short. line was: "" + ascii)\n                \n        assert(len(strokes)==len(asciis)), ""There should be a 1:1 correspondence between stroke data and ascii labels.""\n        f = open(data_file,""wb"")\n        pickle.dump([strokes,asciis], f, protocol=2)\n        f.close()\n        self.logger.write(""\\tfinished parsing dataset. saved {} lines"".format(len(strokes)))\n\n\n    def load_preprocessed(self, data_file):\n        f = open(data_file,""rb"")\n        [self.raw_stroke_data, self.raw_ascii_data] = pickle.load(f)\n        f.close()\n\n        # goes thru the list, and only keeps the text entries that have more than tsteps points\n        self.stroke_data = []\n        self.ascii_data = []\n        self.valid_stroke_data = []\n        self.valid_ascii_data = []\n        counter = 0\n\n        # every 1 in 20 (5%) will be used for validation data\n        cur_data_counter = 0\n        for i in range(len(self.raw_stroke_data)):\n            data = self.raw_stroke_data[i]\n            if len(data) > (self.tsteps+2):\n                # removes large gaps from the data\n                data = np.minimum(data, self.limit)\n                data = np.maximum(data, -self.limit)\n                data = np.array(data,dtype=np.float32)\n                data[:,0:2] /= self.data_scale\n                cur_data_counter = cur_data_counter + 1\n                if cur_data_counter % 20 == 0:\n                  self.valid_stroke_data.append(data)\n                  self.valid_ascii_data.append(self.raw_ascii_data[i])\n                else:\n                    self.stroke_data.append(data)\n                    self.ascii_data.append(self.raw_ascii_data[i])\n\n        # minus 1, since we want the ydata to be a shifted version of x data\n        self.num_batches = int(len(self.stroke_data) / self.batch_size)\n        self.logger.write(""\\tloaded dataset:"")\n        self.logger.write(""\\t\\t{} train individual data points"".format(len(self.stroke_data)))\n        self.logger.write(""\\t\\t{} valid individual data points"".format(len(self.valid_stroke_data)))\n        self.logger.write(""\\t\\t{} batches"".format(self.num_batches))\n\n    def validation_data(self):\n        # returns validation data\n        x_batch = []\n        y_batch = []\n        ascii_list = []\n        for i in range(self.batch_size):\n            valid_ix = i%len(self.valid_stroke_data)\n            data = self.valid_stroke_data[valid_ix]\n            x_batch.append(np.copy(data[:self.tsteps]))\n            y_batch.append(np.copy(data[1:self.tsteps+1]))\n            ascii_list.append(self.valid_ascii_data[valid_ix])\n        one_hots = [to_one_hot(s, self.ascii_steps, self.alphabet) for s in ascii_list]\n        return x_batch, y_batch, ascii_list, one_hots\n\n    def next_batch(self):\n        # returns a randomized, tsteps-sized portion of the training data\n        x_batch = []\n        y_batch = []\n        ascii_list = []\n        for i in xrange(self.batch_size):\n            data = self.stroke_data[self.idx_perm[self.pointer]]\n            idx = random.randint(0, len(data)-self.tsteps-2)\n            x_batch.append(np.copy(data[:self.tsteps]))\n            y_batch.append(np.copy(data[1:self.tsteps+1]))\n            ascii_list.append(self.ascii_data[self.idx_perm[self.pointer]])\n            self.tick_batch_pointer()\n        one_hots = [to_one_hot(s, self.ascii_steps, self.alphabet) for s in ascii_list]\n        return x_batch, y_batch, ascii_list, one_hots\n\n    def tick_batch_pointer(self):\n        self.pointer += 1\n        if (self.pointer >= len(self.stroke_data)):\n            self.reset_batch_pointer()\n    def reset_batch_pointer(self):\n        self.idx_perm = np.random.permutation(len(self.stroke_data))\n        self.pointer = 0\n\n# utility function for converting input ascii characters into vectors the network can understand.\n# index position 0 means ""unknown""\ndef to_one_hot(s, ascii_steps, alphabet):\n    steplimit=3e3; s = s[:3e3] if len(s) > 3e3 else s # clip super-long strings\n    seq = [alphabet.find(char) + 1 for char in s]\n    if len(seq) >= ascii_steps:\n        seq = seq[:ascii_steps]\n    else:\n        seq = seq + [0]*(ascii_steps - len(seq))\n    one_hot = np.zeros((ascii_steps,len(alphabet)+1))\n    one_hot[np.arange(ascii_steps),seq] = 1\n    return one_hot\n\n# abstraction for logging\nclass Logger():\n    def __init__(self, args):\n        self.logf = \'{}train_scribe.txt\'.format(args.log_dir) if args.train else \'{}sample_scribe.txt\'.format(args.log_dir)\n        with open(self.logf, \'w\') as f: f.write(""Scribe: Realistic Handriting in Tensorflow\\n     by Sam Greydanus\\n\\n\\n"")\n\n    def write(self, s, print_it=True):\n        if print_it:\n            print s\n        with open(self.logf, \'a\') as f:\n            f.write(s + ""\\n"")\n'"
