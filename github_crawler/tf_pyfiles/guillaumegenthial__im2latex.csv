file_path,api_count,code
build.py,0,"b'import click\n\n\nfrom model.utils.data_generator import DataGenerator\nfrom model.utils.text import build_vocab, write_vocab\nfrom model.utils.image import build_images\nfrom model.utils.general import Config\n\n\n@click.command()\n@click.option(\'--data\', default=""configs/data_small.json"",\n        help=\'Path to data json config\')\n@click.option(\'--vocab\', default=""configs/vocab_small.json"",\n        help=\'Path to vocab json config\')\ndef main(data, vocab):\n    data_config = Config(data)\n\n    # datasets\n    train_set = DataGenerator(\n        path_formulas=data_config.path_formulas_train,\n        dir_images=data_config.dir_images_train,\n        path_matching=data_config.path_matching_train)\n    test_set  = DataGenerator(\n        path_formulas=data_config.path_formulas_test,\n        dir_images=data_config.dir_images_test,\n        path_matching=data_config.path_matching_test)\n    val_set   = DataGenerator(\n        path_formulas=data_config.path_formulas_val,\n        dir_images=data_config.dir_images_val,\n        path_matching=data_config.path_matching_val)\n\n    # produce images and matching files\n    train_set.build(buckets=data_config.buckets)\n    test_set.build(buckets=data_config.buckets)\n    val_set.build(buckets=data_config.buckets)\n\n    # vocab\n    vocab_config = Config(vocab)\n    vocab = build_vocab([train_set], min_count=vocab_config.min_count_tok)\n    write_vocab(vocab, vocab_config.path_vocab)\n\n\nif __name__ == ""__main__"":\n    main()'"
evaluate_img.py,0,"b'import click\n\n\nfrom model.utils.data_generator import DataGenerator\nfrom model.img2seq import Img2SeqModel\nfrom model.utils.general import Config\nfrom model.utils.text import Vocab, load_formulas\nfrom model.utils.image import greyscale, build_images\n\nfrom model.evaluation.text import score_files\nfrom model.evaluation.image import score_dirs\n\n\n@click.command()\n@click.option(\'--results\', default=""results/small/"", help=\'Dir to results\')\ndef main(results):\n    # restore config and model\n    dir_output = results\n\n    config_data  = Config(dir_output + ""data.json"")\n    config_vocab = Config(dir_output + ""vocab.json"")\n    config_model = Config(dir_output + ""model.json"")\n\n    vocab = Vocab(config_vocab)\n    model = Img2SeqModel(config_model, dir_output, vocab)\n    model.build_pred()\n    model.restore_session(dir_output + ""model.weights/"")\n\n    # load dataset\n    test_set = DataGenerator(path_formulas=config_data.path_formulas_test,\n            dir_images=config_data.dir_images_test, img_prepro=greyscale,\n            max_iter=config_data.max_iter, bucket=config_data.bucket_test,\n            path_matching=config_data.path_matching_test,\n            max_len=config_data.max_length_formula,\n            form_prepro=vocab.form_prepro,)\n\n\n    # build images from formulas\n    formula_ref = dir_output + ""formulas_test/ref.txt""\n    formula_hyp = dir_output + ""formulas_test/hyp_0.txt""\n    images_ref  = dir_output + ""images_test/ref/""\n    images_test = dir_output + ""images_test/hyp_0/""\n    build_images(load_formulas(formula_ref), images_ref)\n    build_images(load_formulas(formula_hyp), images_test)\n\n    # score the repositories\n    scores = score_dirs(images_ref, images_test, greyscale)\n    msg = "" - "".join([""{} {:04.2f}"".format(k, v) for k, v in scores.items()])\n    model.logger.info(""- Eval Img: {}"".format(msg))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
evaluate_txt.py,0,"b'import click\n\n\nfrom model.utils.data_generator import DataGenerator\nfrom model.img2seq import Img2SeqModel\nfrom model.utils.general import Config\nfrom model.utils.text import Vocab\nfrom model.utils.image import greyscale\n\nfrom model.utils.text import load_formulas\nfrom model.evaluation.text import score_files\n\n\n@click.command()\n@click.option(\'--results\', default=""results/small/"", help=\'Dir to results\')\ndef main(results):\n    # restore config and model\n    dir_output = results\n\n    config_data  = Config(dir_output + ""data.json"")\n    config_vocab = Config(dir_output + ""vocab.json"")\n    config_model = Config(dir_output + ""model.json"")\n\n    vocab = Vocab(config_vocab)\n    model = Img2SeqModel(config_model, dir_output, vocab)\n    model.build_pred()\n    model.restore_session(dir_output + ""model.weights/"")\n\n    # load dataset\n    test_set = DataGenerator(path_formulas=config_data.path_formulas_test,\n            dir_images=config_data.dir_images_test, img_prepro=greyscale,\n            max_iter=config_data.max_iter, bucket=config_data.bucket_test,\n            path_matching=config_data.path_matching_test,\n            max_len=config_data.max_length_formula,\n            form_prepro=vocab.form_prepro,)\n\n    # use model to write predictions in files\n    config_eval = Config({""dir_answers"": dir_output + ""formulas_test/"",\n                          ""batch_size"": 20})\n    files, perplexity = model.write_prediction(config_eval, test_set)\n    formula_ref, formula_hyp = files[0], files[1]\n\n    # score the ref and prediction files\n    scores = score_files(formula_ref, formula_hyp)\n    scores[""perplexity""] = perplexity\n    msg = "" - "".join([""{} {:04.2f}"".format(k, v) for k, v in scores.items()])\n    model.logger.info(""- Test Txt: {}"".format(msg))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
predict.py,0,"b'from scipy.misc import imread\nimport PIL\nfrom PIL import Image\n\n\nfrom model.img2seq import Img2SeqModel\nfrom model.utils.general import Config, run\nfrom model.utils.text import Vocab\nfrom model.utils.image import greyscale, crop_image, pad_image, \\\n    downsample_image, TIMEOUT\n\n\n\ndef interactive_shell(model):\n    """"""Creates interactive shell to play with model\n    """"""\n    model.logger.info(""""""\nThis is an interactive mode.\nTo exit, enter \'exit\'.\nEnter a path to a file\ninput> data/images_test/0.png"""""")\n\n    while True:\n        try:\n            # for python 2\n            img_path = raw_input(""input> "")\n        except NameError:\n            # for python 3\n            img_path = input(""input> "")\n\n        if img_path == ""exit"":\n            break\n\n        if img_path[-3:] == ""png"":\n            img = imread(img_path)\n\n        elif img_path[-3:] == ""pdf"":\n            # call magick to convert the pdf into a png file\n            buckets = [\n            [240, 100], [320, 80], [400, 80], [400, 100], [480, 80], [480, 100],\n            [560, 80], [560, 100], [640, 80], [640, 100], [720, 80], [720, 100],\n            [720, 120], [720, 200], [800, 100], [800, 320], [1000, 200],\n            [1000, 400], [1200, 200], [1600, 200], [1600, 1600]\n            ]\n\n            dir_output = ""tmp/""\n            name = img_path.split(\'/\')[-1].split(\'.\')[0]\n            run(""magick convert -density {} -quality {} {} {}"".format(200, 100,\n                img_path, dir_output+""{}.png"".format(name)), TIMEOUT)\n            img_path = dir_output + ""{}.png"".format(name)\n            crop_image(img_path, img_path)\n            pad_image(img_path, img_path, buckets=buckets)\n            downsample_image(img_path, img_path, 2)\n\n            img = imread(img_path)\n\n\n        img = greyscale(img)\n        hyps = model.predict(img)\n\n        model.logger.info(hyps[0])\n\n\nif __name__ == ""__main__"":\n    # restore config and model\n    dir_output = ""results/small/""\n    config_vocab = Config(dir_output + ""vocab.json"")\n    config_model = Config(dir_output + ""model.json"")\n    vocab = Vocab(config_vocab)\n\n    model = Img2SeqModel(config_model, dir_output, vocab)\n    model.build_pred()\n    model.restore_session(dir_output + ""model.weights/"")\n\n    interactive_shell(model)\n'"
train.py,0,"b'import click\n\n\nfrom model.utils.data_generator import DataGenerator\nfrom model.img2seq import Img2SeqModel\nfrom model.utils.lr_schedule import LRSchedule\nfrom model.utils.general import Config\nfrom model.utils.text import Vocab\nfrom model.utils.image import greyscale\n\n\n@click.command()\n@click.option(\'--data\', default=""configs/data_small.json"",\n        help=\'Path to data json config\')\n@click.option(\'--vocab\', default=""configs/vocab_small.json"",\n        help=\'Path to vocab json config\')\n@click.option(\'--training\', default=""configs/training_small.json"",\n        help=\'Path to training json config\')\n@click.option(\'--model\', default=""configs/model.json"",\n        help=\'Path to model json config\')\n@click.option(\'--output\', default=""results/small/"",\n        help=\'Dir for results and model weights\')\ndef main(data, vocab, training, model, output):\n    # Load configs\n    dir_output = output\n    config = Config([data, vocab, training, model])\n    config.save(dir_output)\n    vocab = Vocab(config)\n\n    # Load datasets\n    train_set = DataGenerator(path_formulas=config.path_formulas_train,\n            dir_images=config.dir_images_train, img_prepro=greyscale,\n            max_iter=config.max_iter, bucket=config.bucket_train,\n            path_matching=config.path_matching_train,\n            max_len=config.max_length_formula,\n            form_prepro=vocab.form_prepro)\n    val_set = DataGenerator(path_formulas=config.path_formulas_val,\n            dir_images=config.dir_images_val, img_prepro=greyscale,\n            max_iter=config.max_iter, bucket=config.bucket_val,\n            path_matching=config.path_matching_val,\n            max_len=config.max_length_formula,\n            form_prepro=vocab.form_prepro)\n\n    # Define learning rate schedule\n    n_batches_epoch = ((len(train_set) + config.batch_size - 1) //\n                        config.batch_size)\n    lr_schedule = LRSchedule(lr_init=config.lr_init,\n            start_decay=config.start_decay*n_batches_epoch,\n            end_decay=config.end_decay*n_batches_epoch,\n            end_warm=config.end_warm*n_batches_epoch,\n            lr_warm=config.lr_warm,\n            lr_min=config.lr_min)\n\n    # Build model and train\n    model = Img2SeqModel(config, dir_output, vocab)\n    model.build_train(config)\n    model.train(config, train_set, val_set, lr_schedule)\n\n\nif __name__ == ""__main__"":\n    main()'"
model/__init__.py,0,b''
model/base.py,17,"b'import os\nimport sys\nimport time\nimport tensorflow as tf\n\n\nfrom .utils.general import init_dir, get_logger\n\n\nclass BaseModel(object):\n    """"""Generic class for tf models""""""\n\n    def __init__(self, config, dir_output):\n        """"""Defines self._config\n\n        Args:\n            config: (Config instance) class with hyper parameters,\n                vocab and embeddings\n\n        """"""\n        self._config = config\n        self._dir_output = dir_output\n        init_dir(self._dir_output)\n        self.logger = get_logger(self._dir_output + ""model.log"")\n        tf.reset_default_graph() # saveguard if previous model was defined\n\n\n    def build_train(self, config=None):\n        """"""To overwrite with model-specific logic\n\n        This logic must define\n            - self.loss\n            - self.lr\n            - etc.\n        """"""\n        raise NotImplementedError\n\n\n    def build_pred(self, config=None):\n        """"""Similar to build_train but no need to define train_op""""""\n        raise NotImplementedError\n\n\n    def _add_train_op(self, lr_method, lr, loss, clip=-1):\n        """"""Defines self.train_op that performs an update on a batch\n\n        Args:\n            lr_method: (string) sgd method, for example ""adam""\n            lr: (tf.placeholder) tf.float32, learning rate\n            loss: (tensor) tf.float32 loss to minimize\n            clip: (python float) clipping of gradient. If < 0, no clipping\n\n        """"""\n        _lr_m = lr_method.lower() # lower to make sure\n\n        with tf.variable_scope(""train_step""):\n            if _lr_m == \'adam\': # sgd method\n                optimizer = tf.train.AdamOptimizer(lr)\n            elif _lr_m == \'adagrad\':\n                optimizer = tf.train.AdagradOptimizer(lr)\n            elif _lr_m == \'sgd\':\n                optimizer = tf.train.GradientDescentOptimizer(lr)\n            elif _lr_m == \'rmsprop\':\n                optimizer = tf.train.RMSPropOptimizer(lr)\n            else:\n                raise NotImplementedError(""Unknown method {}"".format(_lr_m))\n\n            # for batch norm beta gamma\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):\n                if clip > 0: # gradient clipping if clip is positive\n                    grads, vs     = zip(*optimizer.compute_gradients(loss))\n                    grads, gnorm  = tf.clip_by_global_norm(grads, clip)\n                    self.train_op = optimizer.apply_gradients(zip(grads, vs))\n                else:\n                    self.train_op = optimizer.minimize(loss)\n\n\n    def init_session(self):\n        """"""Defines self.sess, self.saver and initialize the variables""""""\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n        self.saver = tf.train.Saver()\n\n\n    def restore_session(self, dir_model):\n        """"""Reload weights into session\n\n        Args:\n            sess: tf.Session()\n            dir_model: dir with weights\n\n        """"""\n        self.logger.info(""Reloading the latest trained model..."")\n        self.saver.restore(self.sess, dir_model)\n\n\n    def save_session(self):\n        """"""Saves session""""""\n        # check dir one last time\n        dir_model = self._dir_output + ""model.weights/""\n        init_dir(dir_model)\n\n        # logging\n        sys.stdout.write(""\\r- Saving model..."")\n        sys.stdout.flush()\n\n        # saving\n        self.saver.save(self.sess, dir_model)\n\n        # logging\n        sys.stdout.write(""\\r"")\n        sys.stdout.flush()\n        self.logger.info(""- Saved model in {}"".format(dir_model))\n\n\n    def close_session(self):\n        """"""Closes the session""""""\n        self.sess.close()\n\n\n    def _add_summary(self):\n        """"""Defines variables for Tensorboard\n\n        Args:\n            dir_output: (string) where the results are written\n\n        """"""\n        self.merged      = tf.summary.merge_all()\n        self.file_writer = tf.summary.FileWriter(self._dir_output,\n                self.sess.graph)\n\n\n    def train(self, config, train_set, val_set, lr_schedule):\n        """"""Global training procedure\n\n        Calls method self.run_epoch and saves weights if score improves.\n        All the epoch-logic including the lr_schedule update must be done in\n        self.run_epoch\n\n        Args:\n            config: Config instance contains params as attributes\n            train_set: Dataset instance\n            val_set: Dataset instance\n            lr_schedule: LRSchedule instance that takes care of learning proc\n\n        Returns:\n            best_score: (float)\n\n        """"""\n        best_score = None\n\n        for epoch in range(config.n_epochs):\n            # logging\n            tic = time.time()\n            self.logger.info(""Epoch {:}/{:}"".format(epoch+1, config.n_epochs))\n\n            # epoch\n            score = self._run_epoch(config, train_set, val_set, epoch,\n                    lr_schedule)\n\n            # save weights if we have new best score on eval\n            if best_score is None or score >= best_score:\n                best_score = score\n                self.logger.info(""- New best score ({:04.2f})!"".format(\n                        best_score))\n                self.save_session()\n            if lr_schedule.stop_training:\n                self.logger.info(""- Early Stopping."")\n                break\n\n            # logging\n            toc = time.time()\n            self.logger.info(""- Elapsed time: {:04.2f}, lr: {:04.5f}"".format(\n                            toc-tic, lr_schedule.lr))\n\n        return best_score\n\n\n    def _run_epoch(config, train_set, val_set, epoch, lr_schedule):\n        """"""Model_specific method to overwrite\n\n        Performs an epoch of training\n\n        Args:\n            config: Config\n            train_set: Dataset instance\n            val_set: Dataset instance\n            epoch: (int) id of the epoch, starting at 0\n            lr_schedule: LRSchedule instance that takes care of learning proc\n\n        Returns:\n            score: (float) model will select weights that achieve the highest\n                score\n\n        """"""\n        raise NotImplementedError\n\n\n    def evaluate(self, config, test_set):\n        """"""Evaluates model on test set\n\n        Calls method run_evaluate on test_set and takes care of logging\n\n        Args:\n            config: Config\n            test_set: instance of class Dataset\n\n        Return:\n            scores: (dict) scores[""acc""] = 0.85 for instance\n\n        """"""\n        # logging\n        sys.stdout.write(""\\r- Evaluating..."")\n        sys.stdout.flush()\n\n        # evaluate\n        scores = self._run_evaluate(config, test_set)\n\n        # logging\n        sys.stdout.write(""\\r"")\n        sys.stdout.flush()\n        msg = "" - "".join([""{} {:04.2f}"".format(k, v)\n                for k, v in scores.items()])\n        self.logger.info(""- Eval: {}"".format(msg))\n\n        return scores\n\n\n    def _run_evaluate(config, test_set):\n        """"""Model-specific method to overwrite\n\n        Performs an epoch of evaluation\n\n        Args:\n            config: Config\n            test_set: Dataset instance\n\n        Returns:\n            scores: (dict) scores[""acc""] = 0.85 for instance\n\n        """"""\n        raise NotImplementedError\n'"
model/decoder.py,20,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\nimport tensorflow.contrib.layers as layers\nfrom tensorflow.contrib.rnn import GRUCell, LSTMCell\n\n\nfrom components.dynamic_decode import dynamic_decode\nfrom components.attention_mechanism import AttentionMechanism\nfrom components.attention_cell import AttentionCell\nfrom components.greedy_decoder_cell import GreedyDecoderCell\nfrom components.beam_search_decoder_cell import BeamSearchDecoderCell\n\n\nclass Decoder(object):\n    """"""Implements this paper https://arxiv.org/pdf/1609.04938.pdf""""""\n\n    def __init__(self, config, n_tok, id_end):\n        self._config = config\n        self._n_tok = n_tok\n        self._id_end = id_end\n        self._tiles = 1 if config.decoding == ""greedy"" else config.beam_size\n\n\n    def __call__(self, training, img, formula, dropout):\n        """"""Decodes an image into a sequence of token\n\n        Args:\n            training: (tf.placeholder) bool\n            img: encoded image (tf.Tensor) shape = (N, H, W, C)\n            formula: (tf.placeholder), shape = (N, T)\n\n        Returns:\n            pred_train: (tf.Tensor), shape = (?, ?, vocab_size) logits of each class\n            pret_test: (structure)\n                - pred.test.logits, same as pred_train\n                - pred.test.ids, shape = (?, config.max_length_formula)\n\n        """"""\n        dim_embeddings = self._config.attn_cell_config.get(""dim_embeddings"")\n        E = tf.get_variable(""E"", initializer=embedding_initializer(),\n                shape=[self._n_tok, dim_embeddings], dtype=tf.float32)\n\n        start_token = tf.get_variable(""start_token"", dtype=tf.float32,\n                shape=[dim_embeddings], initializer=embedding_initializer())\n\n        batch_size = tf.shape(img)[0]\n\n        # training\n        with tf.variable_scope(""attn_cell"", reuse=False):\n            embeddings = get_embeddings(formula, E, dim_embeddings,\n                    start_token, batch_size)\n            attn_meca = AttentionMechanism(img,\n                    self._config.attn_cell_config[""dim_e""])\n            recu_cell = LSTMCell(self._config.attn_cell_config[""num_units""])\n            attn_cell = AttentionCell(recu_cell, attn_meca, dropout,\n                    self._config.attn_cell_config, self._n_tok)\n\n            train_outputs, _ = tf.nn.dynamic_rnn(attn_cell, embeddings,\n                    initial_state=attn_cell.initial_state())\n\n        # decoding\n        with tf.variable_scope(""attn_cell"", reuse=True):\n            attn_meca = AttentionMechanism(img=img,\n                    dim_e=self._config.attn_cell_config[""dim_e""],\n                    tiles=self._tiles)\n            recu_cell = LSTMCell(self._config.attn_cell_config[""num_units""],\n                    reuse=True)\n            attn_cell = AttentionCell(recu_cell, attn_meca, dropout,\n                    self._config.attn_cell_config, self._n_tok)\n            if self._config.decoding == ""greedy"":\n                decoder_cell = GreedyDecoderCell(E, attn_cell, batch_size,\n                        start_token, id_end)\n            elif self._config.decoding == ""beam_search"":\n                decoder_cell = BeamSearchDecoderCell(E, attn_cell, batch_size,\n                        start_token, self._id_end, self._config.beam_size,\n                        self._config.div_gamma, self._config.div_prob)\n\n            test_outputs, _ = dynamic_decode(decoder_cell,\n                    self._config.max_length_formula+1)\n\n        return train_outputs, test_outputs\n\n\ndef get_embeddings(formula, E, dim, start_token, batch_size):\n    """"""Returns the embedding of the n-1 first elements in the formula concat\n    with the start token\n\n    Args:\n        formula: (tf.placeholder) tf.uint32\n        E: tf.Variable (matrix)\n        dim: (int) dimension of embeddings\n        start_token: tf.Variable\n        batch_size: tf variable extracted from placeholder\n\n    Returns:\n        embeddings_train: tensor\n\n    """"""\n    formula_ = tf.nn.embedding_lookup(E, formula)\n    start_token_ = tf.reshape(start_token, [1, 1, dim])\n    start_tokens = tf.tile(start_token_, multiples=[batch_size, 1, 1])\n    embeddings = tf.concat([start_tokens, formula_[:, :-1, :]], axis=1)\n\n    return embeddings\n\n\ndef embedding_initializer():\n    """"""Returns initializer for embeddings""""""\n    def _initializer(shape, dtype, partition_info=None):\n        E = tf.random_uniform(shape, minval=-1.0, maxval=1.0, dtype=dtype)\n        E = tf.nn.l2_normalize(E, -1)\n        return E\n\n    return _initializer\n'"
model/encoder.py,21,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import GRUCell, LSTMCell\n\n\nfrom .components.positional import add_timing_signal_nd\n\n\nclass Encoder(object):\n    """"""Class with a __call__ method that applies convolutions to an image""""""\n\n    def __init__(self, config):\n        self._config = config\n\n\n    def __call__(self, training, img, dropout):\n        """"""Applies convolutions to the image\n\n        Args:\n            training: (tf.placeholder) tf.bool\n            img: batch of img, shape = (?, height, width, channels), of type\n                tf.uint8\n\n        Returns:\n            the encoded images, shape = (?, h\', w\', c\')\n\n        """"""\n        img = tf.cast(img, tf.float32) / 255.\n\n        with tf.variable_scope(""convolutional_encoder""):\n            # conv + max pool -> /2\n            out = tf.layers.conv2d(img, 64, 3, 1, ""SAME"",\n                    activation=tf.nn.relu)\n            out = tf.layers.max_pooling2d(out, 2, 2, ""SAME"")\n\n            # conv + max pool -> /2\n            out = tf.layers.conv2d(out, 128, 3, 1, ""SAME"",\n                    activation=tf.nn.relu)\n            out = tf.layers.max_pooling2d(out, 2, 2, ""SAME"")\n\n            # regular conv -> id\n            out = tf.layers.conv2d(out, 256, 3, 1, ""SAME"",\n                    activation=tf.nn.relu)\n\n            out = tf.layers.conv2d(out, 256, 3, 1, ""SAME"",\n                    activation=tf.nn.relu)\n\n            if self._config.encoder_cnn == ""vanilla"":\n                out = tf.layers.max_pooling2d(out, (2, 1), (2, 1), ""SAME"")\n\n            out = tf.layers.conv2d(out, 512, 3, 1, ""SAME"",\n                    activation=tf.nn.relu)\n\n            if self._config.encoder_cnn == ""vanilla"":\n                out = tf.layers.max_pooling2d(out, (1, 2), (1, 2), ""SAME"")\n\n            if self._config.encoder_cnn == ""cnn"":\n                # conv with stride /2 (replaces the 2 max pool)\n                out = tf.layers.conv2d(out, 512, (2, 4), 2, ""SAME"")\n\n            # conv\n            out = tf.layers.conv2d(out, 512, 3, 1, ""VALID"",\n                    activation=tf.nn.relu)\n\n            if self._config.positional_embeddings:\n                # from tensor2tensor lib - positional embeddings\n                out = add_timing_signal_nd(out)\n\n        return out\n'"
model/img2seq.py,14,"b'import sys\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.layers as layers\n\n\nfrom .utils.general import Config, Progbar, minibatches\nfrom .utils.image import pad_batch_images\nfrom .utils.text import pad_batch_formulas\nfrom .evaluation.text import score_files, write_answers, truncate_end\n\n\nfrom .encoder import Encoder\nfrom .decoder import Decoder\nfrom .base import BaseModel\n\n\nclass Img2SeqModel(BaseModel):\n    """"""Specialized class for Img2Seq Model""""""\n\n    def __init__(self, config, dir_output, vocab):\n        """"""\n        Args:\n            config: Config instance defining hyperparams\n            vocab: Vocab instance defining useful vocab objects like tok_to_id\n\n        """"""\n        super(Img2SeqModel, self).__init__(config, dir_output)\n        self._vocab = vocab\n\n\n    def build_train(self, config):\n        """"""Builds model""""""\n        self.logger.info(""Building model..."")\n\n        self.encoder = Encoder(self._config)\n        self.decoder = Decoder(self._config, self._vocab.n_tok,\n                self._vocab.id_end)\n\n        self._add_placeholders_op()\n        self._add_pred_op()\n        self._add_loss_op()\n\n        self._add_train_op(config.lr_method, self.lr, self.loss,\n                config.clip)\n        self.init_session()\n\n        self.logger.info(""- done."")\n\n\n    def build_pred(self):\n        self.logger.info(""Building model..."")\n\n        self.encoder = Encoder(self._config)\n        self.decoder = Decoder(self._config, self._vocab.n_tok,\n                self._vocab.id_end)\n\n        self._add_placeholders_op()\n        self._add_pred_op()\n        self._add_loss_op()\n\n        self.init_session()\n\n        self.logger.info(""- done."")\n\n\n\n    def _add_placeholders_op(self):\n        """"""\n        Add placeholder attributes\n        """"""\n        # hyper params\n        self.lr = tf.placeholder(tf.float32, shape=(),\n            name=\'lr\')\n        self.dropout = tf.placeholder(tf.float32, shape=(),\n            name=\'dropout\')\n        self.training = tf.placeholder(tf.bool, shape=(),\n            name=""training"")\n\n\n        # input of the graph\n        self.img = tf.placeholder(tf.uint8, shape=(None, None, None, 1),\n            name=\'img\')\n        self.formula = tf.placeholder(tf.int32, shape=(None, None),\n            name=\'formula\')\n        self.formula_length = tf.placeholder(tf.int32, shape=(None, ),\n            name=\'formula_length\')\n\n        # tensorboard\n        tf.summary.scalar(""lr"", self.lr)\n\n\n    def _get_feed_dict(self, img, training, formula=None, lr=None, dropout=1):\n        """"""Returns a dict""""""\n        img = pad_batch_images(img)\n\n        fd = {\n            self.img: img,\n            self.dropout: dropout,\n            self.training: training,\n        }\n\n        if formula is not None:\n            formula, formula_length = pad_batch_formulas(formula,\n                    self._vocab.id_pad, self._vocab.id_end)\n            # print img.shape, formula.shape\n            fd[self.formula] = formula\n            fd[self.formula_length] = formula_length\n        if lr is not None:\n            fd[self.lr] = lr\n\n        return fd\n\n\n    def _add_pred_op(self):\n        """"""Defines self.pred""""""\n        encoded_img = self.encoder(self.training, self.img, self.dropout)\n        train, test = self.decoder(self.training, encoded_img, self.formula,\n                self.dropout)\n\n        self.pred_train = train\n        self.pred_test  = test\n\n\n    def _add_loss_op(self):\n        """"""Defines self.loss""""""\n        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=self.pred_train, labels=self.formula)\n\n        mask = tf.sequence_mask(self.formula_length)\n        losses = tf.boolean_mask(losses, mask)\n\n        # loss for training\n        self.loss = tf.reduce_mean(losses)\n\n        # # to compute perplexity for test\n        self.ce_words = tf.reduce_sum(losses) # sum of CE for each word\n        self.n_words = tf.reduce_sum(self.formula_length) # number of words\n\n        # for tensorboard\n        tf.summary.scalar(""loss"", self.loss)\n\n\n\n    def _run_epoch(self, config, train_set, val_set, epoch, lr_schedule):\n        """"""Performs an epoch of training\n\n        Args:\n            config: Config instance\n            train_set: Dataset instance\n            val_set: Dataset instance\n            epoch: (int) id of the epoch, starting at 0\n            lr_schedule: LRSchedule instance that takes care of learning proc\n\n        Returns:\n            score: (float) model will select weights that achieve the highest\n                score\n\n        """"""\n        # logging\n        batch_size = config.batch_size\n        nbatches = (len(train_set) + batch_size - 1) // batch_size\n        prog = Progbar(nbatches)\n\n        # iterate over dataset\n        for i, (img, formula) in enumerate(minibatches(train_set, batch_size)):\n            # get feed dict\n            fd = self._get_feed_dict(img, training=True, formula=formula,\n                    lr=lr_schedule.lr, dropout=config.dropout)\n\n            # update step\n            _, loss_eval = self.sess.run([self.train_op, self.loss],\n                    feed_dict=fd)\n            prog.update(i + 1, [(""loss"", loss_eval), (""perplexity"",\n                    np.exp(loss_eval)), (""lr"", lr_schedule.lr)])\n\n            # update learning rate\n            lr_schedule.update(batch_no=epoch*nbatches + i)\n\n        # logging\n        self.logger.info(""- Training: {}"".format(prog.info))\n\n        # evaluation\n        config_eval = Config({""dir_answers"": self._dir_output + ""formulas_val/"",\n                ""batch_size"": config.batch_size})\n        scores = self.evaluate(config_eval, val_set)\n        score = scores[config.metric_val]\n        lr_schedule.update(score=score)\n\n        return score\n\n\n\n    def write_prediction(self, config, test_set):\n        """"""Performs an epoch of evaluation\n\n        Args:\n            config: (Config) with batch_size and dir_answers\n            test_set:(Dataset) instance\n\n        Returns:\n            files: (list) of path to files\n            perp: (float) perplexity on test set\n\n        """"""\n        # initialize containers of references and predictions\n        if self._config.decoding == ""greedy"":\n            refs, hyps = [], [[]]\n        elif self._config.decoding == ""beam_search"":\n            refs, hyps = [], [[] for i in range(self._config.beam_size)]\n\n        # iterate over the dataset\n        n_words, ce_words = 0, 0 # sum of ce for all words + nb of words\n        for img, formula in minibatches(test_set, config.batch_size):\n            fd = self._get_feed_dict(img, training=False, formula=formula,\n                    dropout=1)\n            ce_words_eval, n_words_eval, ids_eval = self.sess.run(\n                    [self.ce_words, self.n_words, self.pred_test.ids],\n                    feed_dict=fd)\n\n            # TODO(guillaume): move this logic into tf graph\n            if self._config.decoding == ""greedy"":\n                ids_eval = np.expand_dims(ids_eval, axis=1)\n\n            elif self._config.decoding == ""beam_search"":\n                ids_eval = np.transpose(ids_eval, [0, 2, 1])\n\n            n_words += n_words_eval\n            ce_words += ce_words_eval\n            for form, preds in zip(formula, ids_eval):\n                refs.append(form)\n                for i, pred in enumerate(preds):\n                    hyps[i].append(pred)\n\n        files = write_answers(refs, hyps, self._vocab.id_to_tok,\n                config.dir_answers, self._vocab.id_end)\n\n        perp = - np.exp(ce_words / float(n_words))\n\n        return files, perp\n\n\n    def _run_evaluate(self, config, test_set):\n        """"""Performs an epoch of evaluation\n\n        Args:\n            test_set: Dataset instance\n            params: (dict) with extra params in it\n                - ""dir_name"": (string)\n\n        Returns:\n            scores: (dict) scores[""acc""] = 0.85 for instance\n\n        """"""\n        files, perp = self.write_prediction(config, test_set)\n        scores = score_files(files[0], files[1])\n        scores[""perplexity""] = perp\n\n        return scores\n\n\n    def predict_batch(self, images):\n        if self._config.decoding == ""greedy"":\n            hyps = [[]]\n        elif self._config.decoding == ""beam_search"":\n            hyps = [[] for i in range(self._config.beam_size)]\n\n        fd = self._get_feed_dict(images, training=False, dropout=1)\n        ids_eval, = self.sess.run([self.pred_test.ids], feed_dict=fd)\n\n        if self._config.decoding == ""greedy"":\n            ids_eval = np.expand_dims(ids_eval, axis=1)\n\n        elif self._config.decoding == ""beam_search"":\n            ids_eval = np.transpose(ids_eval, [0, 2, 1])\n\n        for preds in ids_eval:\n            for i, pred in enumerate(preds):\n                p = truncate_end(pred, self._vocab.id_end)\n                p = "" "".join([self._vocab.id_to_tok[idx] for idx in p])\n                hyps[i].append(p)\n\n        return hyps\n\n\n    def predict(self, img):\n        preds = self.predict_batch([img])\n        preds_ = []\n        # extract only one element (no batch)\n        for hyp in preds:\n            preds_.append(hyp[0])\n\n        return preds_\n'"
model/components/__init__.py,0,b''
model/components/attention_cell.py,12,"b'import tensorflow as tf\nimport collections\nfrom tensorflow.contrib.rnn import RNNCell, LSTMStateTuple\n\n\nAttentionState = collections.namedtuple(""AttentionState"", (""cell_state"", ""o""))\n\n\nclass AttentionCell(RNNCell):\n    def __init__(self, cell, attention_mechanism, dropout, attn_cell_config,\n        num_proj, dtype=tf.float32):\n        """"""\n        Args:\n            cell: (RNNCell)\n            attention_mechanism: (AttentionMechanism)\n            dropout: (tf.float)\n            attn_cell_config: (dict) hyper params\n\n        """"""\n        # variables and tensors\n        self._cell                = cell\n        self._attention_mechanism = attention_mechanism\n        self._dropout             = dropout\n\n        # hyperparameters and shapes\n        self._n_channels     = self._attention_mechanism._n_channels\n        self._dim_e          = attn_cell_config[""dim_e""]\n        self._dim_o          = attn_cell_config[""dim_o""]\n        self._num_units      = attn_cell_config[""num_units""]\n        self._dim_embeddings = attn_cell_config[""dim_embeddings""]\n        self._num_proj       = num_proj\n        self._dtype          = dtype\n\n        # for RNNCell\n        self._state_size = AttentionState(self._cell._state_size, self._dim_o)\n\n\n    @property\n    def state_size(self):\n        return self._state_size\n\n\n    @property\n    def output_size(self):\n        return self._num_proj\n\n\n    @property\n    def output_dtype(self):\n        return self._dtype\n\n\n    def initial_state(self):\n        """"""Returns initial state for the lstm""""""\n        initial_cell_state = self._attention_mechanism.initial_cell_state(self._cell)\n        initial_o          = self._attention_mechanism.initial_state(""o"", self._dim_o)\n\n        return AttentionState(initial_cell_state, initial_o)\n\n\n    def step(self, embedding, attn_cell_state):\n        """"""\n        Args:\n            embedding: shape = (batch_size, dim_embeddings) embeddings\n                from previous time step\n            attn_cell_state: (AttentionState) state from previous time step\n\n        """"""\n        prev_cell_state, o = attn_cell_state\n\n        scope = tf.get_variable_scope()\n        with tf.variable_scope(scope):\n            # compute new h\n            x                     = tf.concat([embedding, o], axis=-1)\n            new_h, new_cell_state = self._cell.__call__(x, prev_cell_state)\n            new_h = tf.nn.dropout(new_h, self._dropout)\n\n            # compute attention\n            c = self._attention_mechanism.context(new_h)\n\n            # compute o\n            o_W_c = tf.get_variable(""o_W_c"", dtype=tf.float32,\n                    shape=(self._n_channels, self._dim_o))\n            o_W_h = tf.get_variable(""o_W_h"", dtype=tf.float32,\n                    shape=(self._num_units, self._dim_o))\n\n            new_o = tf.tanh(tf.matmul(new_h, o_W_h) + tf.matmul(c, o_W_c))\n            new_o = tf.nn.dropout(new_o, self._dropout)\n\n            y_W_o = tf.get_variable(""y_W_o"", dtype=tf.float32,\n                    shape=(self._dim_o, self._num_proj))\n            logits = tf.matmul(new_o, y_W_o)\n\n            # new Attn cell state\n            new_state = AttentionState(new_cell_state, new_o)\n\n            return logits, new_state\n\n\n    def __call__(self, inputs, state):\n        """"""\n        Args:\n            inputs: the embedding of the previous word for training only\n            state: (AttentionState) (h, o) where h is the hidden state and\n                o is the vector used to make the prediction of\n                the previous word\n\n        """"""\n        new_output, new_state = self.step(inputs, state)\n\n        return (new_output, new_state)'"
model/components/attention_mechanism.py,30,"b'import tensorflow as tf\n\n\nclass AttentionMechanism(object):\n    """"""Class to compute attention over an image""""""\n\n    def __init__(self, img, dim_e, tiles=1):\n        """"""Stores the image under the right shape.\n\n        We loose the H, W dimensions and merge them into a single\n        dimension that corresponds to ""regions"" of the image.\n\n        Args:\n            img: (tf.Tensor) image\n            dim_e: (int) dimension of the intermediary vector used to\n                compute attention\n            tiles: (int) default 1, input to context h may have size\n                    (tile * batch_size, ...)\n\n        """"""\n        if len(img.shape) == 3:\n            self._img = img\n        elif len(img.shape) == 4:\n            N    = tf.shape(img)[0]\n            H, W = tf.shape(img)[1], tf.shape(img)[2] # image\n            C    = img.shape[3].value                 # channels\n            self._img = tf.reshape(img, shape=[N, H*W, C])\n        else:\n            print(""Image shape not supported"")\n            raise NotImplementedError\n\n        # dimensions\n        self._n_regions  = tf.shape(self._img)[1]\n        self._n_channels = self._img.shape[2].value\n        self._dim_e      = dim_e\n        self._tiles      = tiles\n        self._scope_name = ""att_mechanism""\n\n        # attention vector over the image\n        self._att_img = tf.layers.dense(\n            inputs=self._img,\n            units=self._dim_e,\n            use_bias=False,\n            name=""att_img"")\n\n\n    def context(self, h):\n        """"""Computes attention\n\n        Args:\n            h: (batch_size, num_units) hidden state\n\n        Returns:\n            c: (batch_size, channels) context vector\n\n        """"""\n        with tf.variable_scope(self._scope_name):\n            if self._tiles > 1:\n                att_img = tf.expand_dims(self._att_img, axis=1)\n                att_img = tf.tile(att_img, multiples=[1, self._tiles, 1, 1])\n                att_img = tf.reshape(att_img, shape=[-1, self._n_regions,\n                        self._dim_e])\n                img = tf.expand_dims(self._img, axis=1)\n                img = tf.tile(img, multiples=[1, self._tiles, 1, 1])\n                img = tf.reshape(img, shape=[-1, self._n_regions,\n                        self._n_channels])\n            else:\n                att_img = self._att_img\n                img     = self._img\n\n            # computes attention over the hidden vector\n            att_h = tf.layers.dense(inputs=h, units=self._dim_e, use_bias=False)\n\n            # sums the two contributions\n            att_h = tf.expand_dims(att_h, axis=1)\n            att = tf.tanh(att_img + att_h)\n\n            # computes scalar product with beta vector\n            # works faster with a matmul than with a * and a tf.reduce_sum\n            att_beta = tf.get_variable(""att_beta"", shape=[self._dim_e, 1],\n                    dtype=tf.float32)\n            att_flat = tf.reshape(att, shape=[-1, self._dim_e])\n            e = tf.matmul(att_flat, att_beta)\n            e = tf.reshape(e, shape=[-1, self._n_regions])\n\n            # compute weights\n            a = tf.nn.softmax(e)\n            a = tf.expand_dims(a, axis=-1)\n            c = tf.reduce_sum(a * img, axis=1)\n\n            return c\n\n\n    def initial_cell_state(self, cell):\n        """"""Returns initial state of a cell computed from the image\n\n        Assumes cell.state_type is an instance of named_tuple.\n        Ex: LSTMStateTuple\n\n        Args:\n            cell: (instance of RNNCell) must define _state_size\n\n        """"""\n        _states_0 = []\n        for hidden_name in cell._state_size._fields:\n            hidden_dim = getattr(cell._state_size, hidden_name)\n            h = self.initial_state(hidden_name, hidden_dim)\n            _states_0.append(h)\n\n        initial_state_cell = type(cell.state_size)(*_states_0)\n\n        return initial_state_cell\n\n\n    def initial_state(self, name, dim):\n        """"""Returns initial state of dimension specified by dim""""""\n        with tf.variable_scope(self._scope_name):\n            img_mean = tf.reduce_mean(self._img, axis=1)\n            W = tf.get_variable(""W_{}_0"".format(name), shape=[self._n_channels,\n                    dim])\n            b = tf.get_variable(""b_{}_0"".format(name), shape=[dim])\n            h = tf.tanh(tf.matmul(img_mean, W) + b)\n\n            return h'"
model/components/beam_search_decoder_cell.py,55,"b'import tensorflow as tf\nimport collections\nfrom tensorflow.python.util import nest\nfrom tensorflow.contrib.rnn import RNNCell\n\n\nfrom dynamic_decode import transpose_batch_time\nfrom greedy_decoder_cell import DecoderOutput\n\n\nclass BeamSearchDecoderCellState(collections.namedtuple(\n        ""BeamSearchDecoderCellState"", (""cell_state"", ""log_probs""))):\n    """"""State of the Beam Search decoding\n\n    cell_state: shape = structure of [batch_size, beam_size, ?]\n        cell state for all the hypotheses\n    embedding: shape = [batch_size, beam_size, embedding_size]\n        embeddings of the previous time step for each hypothesis\n    log_probs: shape = [batch_size, beam_size]\n        log_probs of the hypotheses\n    finished: shape = [batch_size, beam_size]\n        boolean to know if one beam hypothesis has reached token id_end\n\n    """"""\n    pass\n\n\nclass BeamSearchDecoderOutput(collections.namedtuple(\n        ""BeamSearchDecoderOutput"", (""logits"", ""ids"", ""parents""))):\n    """"""Stores the logic for the beam search decoding\n\n    logits: shape = [batch_size, beam_size, vocab_size]\n        scores before softmax of the beam search hypotheses\n    ids: shape = [batch_size, beam_size]\n        ids of the best words at this time step\n    parents: shape = [batch_size, beam_size]\n        ids of the beam index from previous time step\n\n    """"""\n    pass\n\n\nclass BeamSearchDecoderCell(object):\n\n    def __init__(self, embeddings, cell, batch_size, start_token, end_token,\n            beam_size=5, div_gamma=1, div_prob=0):\n        """"""Initializes parameters for Beam Search\n\n        Args:\n            embeddings: (tf.Variable) shape = (vocab_size, embedding_size)\n            cell: instance of Cell that defines a step function, etc.\n            batch_size: tf.int extracted with tf.Shape or int\n            start_token: id of start token\n            end_token: int, id of the end token\n            beam_size: int, size of the beam\n            div_gamma: float, amount of penalty to add to beam hypo for\n                diversity. Coefficient of penaly will be log(div_gamma).\n                Use value between 0 and 1. (1 means no penalty)\n            div_prob: only apply div penalty with probability div_prob.\n                div_prob = 0. means never apply penalty\n\n        """"""\n\n        self._embeddings = embeddings\n        self._cell = cell\n        self._dim_embeddings = embeddings.shape[-1].value\n        self._batch_size = batch_size\n        self._start_token = start_token\n        self._beam_size  = beam_size\n        self._end_token = end_token\n        self._vocab_size = embeddings.shape[0].value\n        self._div_gamma = float(div_gamma)\n        self._div_prob = float(div_prob)\n\n\n    @property\n    def output_dtype(self):\n        """"""Needed for custom dynamic_decode for the TensorArray of results""""""\n        return BeamSearchDecoderOutput(logits=self._cell.output_dtype,\n                ids=tf.int32, parents=tf.int32)\n\n\n    @property\n    def final_output_dtype(self):\n        """"""For the finalize method""""""\n        return DecoderOutput(logits=self._cell.output_dtype, ids=tf.int32)\n\n\n    @property\n    def state_size(self):\n        return BeamSearchDecoderOutput(\n                logits=tf.TensorShape([self._beam_size, self._vocab_size]),\n                ids=tf.TensorShape([self._beam_size]),\n                parents=tf.TensorShape([self._beam_size]))\n\n\n    @property\n    def final_output_size(self):\n        return DecoderOutput(logits=tf.TensorShape([self._beam_size,\n                self._vocab_size]), ids=tf.TensorShape([self._beam_size]))\n\n\n    def initial_state(self):\n        """"""Returns initial state for the decoder""""""\n        # cell initial state\n        cell_state = self._cell.initial_state()\n        cell_state = nest.map_structure(lambda t: tile_beam(t,\n                self._beam_size), cell_state)\n\n        # prepare other initial states\n        log_probs =  tf.zeros([self._batch_size, self._beam_size],\n                dtype=self._cell.output_dtype)\n\n        return BeamSearchDecoderCellState(cell_state, log_probs)\n\n\n    def initial_inputs(self):\n        return tf.tile(tf.reshape(self._start_token,\n                [1, 1, self._dim_embeddings]),\n                multiples=[self._batch_size, self._beam_size, 1])\n\n\n    def initialize(self):\n        initial_state = self.initial_state()\n        initial_inputs = self.initial_inputs()\n        initial_finished = tf.zeros(shape=[self._batch_size, self._beam_size],\n                dtype=tf.bool)\n        return initial_state, initial_inputs, initial_finished\n\n\n    def step(self, time, state, embedding, finished):\n        """"""\n        Args:\n            time: tensorf or int\n            embedding: shape [batch_size, beam_size, d]\n            state: structure of shape [bach_size, beam_size, ...]\n            finished: structure of shape [batch_size, beam_size, ...]\n\n        """"""\n        # merge batch and beam dimension before callling step of cell\n        cell_state = nest.map_structure(merge_batch_beam, state.cell_state)\n        embedding = merge_batch_beam(embedding)\n\n        # compute new logits\n        logits, new_cell_state = self._cell.step(embedding, cell_state)\n\n        # split batch and beam dimension before beam search logic\n        new_logits = split_batch_beam(logits, self._beam_size)\n        new_cell_state = nest.map_structure(\n                lambda t: split_batch_beam(t, self._beam_size), new_cell_state)\n\n        # compute log probs of the step\n        # shape = [batch_size, beam_size, vocab_size]\n        step_log_probs = tf.nn.log_softmax(new_logits)\n        # shape = [batch_size, beam_size, vocab_size]\n        step_log_probs = mask_probs(step_log_probs, self._end_token, finished)\n        # shape = [batch_size, beam_size, vocab_size]\n        log_probs = tf.expand_dims(state.log_probs, axis=-1) + step_log_probs\n        log_probs = add_div_penalty(log_probs, self._div_gamma, self._div_prob,\n                self._batch_size, self._beam_size, self._vocab_size)\n\n        # compute the best beams\n        # shape =  (batch_size, beam_size * vocab_size)\n        log_probs_flat = tf.reshape(log_probs,\n                [self._batch_size, self._beam_size * self._vocab_size])\n        # if time = 0, consider only one beam, otherwise beams are equal\n        log_probs_flat = tf.cond(time > 0, lambda: log_probs_flat,\n                lambda: log_probs[:, 0])\n        new_probs, indices = tf.nn.top_k(log_probs_flat, self._beam_size)\n\n        # of shape [batch_size, beam_size]\n        new_ids = indices % self._vocab_size\n        new_parents = indices // self._vocab_size\n\n        # get ids of words predicted and get embedding\n        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)\n\n        # compute end of beam\n        finished = gather_helper(finished, new_parents,\n                self._batch_size, self._beam_size)\n        new_finished = tf.logical_or(finished,\n                tf.equal(new_ids, self._end_token))\n\n        new_cell_state = nest.map_structure(\n                lambda t: gather_helper(t, new_parents, self._batch_size,\n                self._beam_size), new_cell_state)\n\n\n        # create new state of decoder\n        new_state  = BeamSearchDecoderCellState(cell_state=new_cell_state,\n                log_probs=new_probs)\n\n        new_output = BeamSearchDecoderOutput(logits=new_logits, ids=new_ids,\n                parents=new_parents)\n\n        return (new_output, new_state, new_embedding, new_finished)\n\n\n    def finalize(self, final_outputs, final_state):\n        """"""\n        Args:\n            final_outputs: structure of tensors of shape\n                    [time dimension, batch_size, beam_size, d]\n            final_state: instance of BeamSearchDecoderOutput\n\n        Returns:\n            [time, batch, beam, ...] structure of Tensor\n\n        """"""\n        # reverse the time dimension\n        maximum_iterations = tf.shape(final_outputs.ids)[0]\n        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n                final_outputs)\n\n        # initial states\n        def create_ta(d):\n            return tf.TensorArray(dtype=d, size=maximum_iterations)\n\n        initial_time = tf.constant(0, dtype=tf.int32)\n        initial_outputs_ta = nest.map_structure(create_ta,\n                self.final_output_dtype)\n        initial_parents = tf.tile(\n                tf.expand_dims(tf.range(self._beam_size), axis=0),\n                multiples=[self._batch_size, 1])\n\n        def condition(time, outputs_ta, parents):\n            return tf.less(time, maximum_iterations)\n\n        # beam search decoding cell\n        def body(time, outputs_ta, parents):\n            # get ids, logits and parents predicted at time step by decoder\n            input_t = nest.map_structure(lambda t: t[time], final_outputs)\n\n            # extract the entries corresponding to parents\n            new_state = nest.map_structure(\n                    lambda t: gather_helper(t, parents, self._batch_size,\n                    self._beam_size), input_t)\n\n            # create new output\n            new_output = DecoderOutput(logits=new_state.logits,\n                    ids=new_state.ids)\n\n            # write beam ids\n            outputs_ta = nest.map_structure(lambda ta, out: ta.write(time, out),\n                    outputs_ta, new_output)\n\n            return (time + 1), outputs_ta, parents\n\n        res = tf.while_loop(\n                condition,\n                body,\n                loop_vars=[initial_time, initial_outputs_ta, initial_parents],\n                back_prop=False)\n\n        # unfold and stack the structure from the nested tas\n        final_outputs = nest.map_structure(lambda ta: ta.stack(), res[1])\n\n        # reverse time step\n        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n                final_outputs)\n\n        return DecoderOutput(logits=final_outputs.logits, ids=final_outputs.ids)\n\n\ndef sample_bernoulli(p, s):\n    """"""Samples a boolean tensor with shape = s according to bernouilli""""""\n    return tf.greater(p, tf.random_uniform(s))\n\n\ndef add_div_penalty(log_probs, div_gamma, div_prob, batch_size, beam_size,\n        vocab_size):\n    """"""Adds penalty to beam hypothesis following this paper by Li et al. 2016\n    ""A Simple, Fast Diverse Decoding Algorithm for Neural Generation""\n\n    Args:\n        log_probs: (tensor of floats)\n            shape = (batch_size, beam_size, vocab_size)\n        div_gamma: (float) diversity parameter\n        div_prob: (float) adds penalty with proba div_prob\n\n    """"""\n    if div_gamma is None or div_prob is None: return log_probs\n    if div_gamma == 1. or div_prob == 0.: return log_probs\n\n    # 1. get indices that would sort the array\n    top_probs, top_inds = tf.nn.top_k(log_probs, k=vocab_size, sorted=True)\n    # 2. inverse permutation to get rank of each entry\n    top_inds = tf.reshape(top_inds, [-1, vocab_size])\n    index_rank = tf.map_fn(tf.invert_permutation, top_inds, back_prop=False)\n    index_rank = tf.reshape(index_rank, shape=[batch_size, beam_size,\n            vocab_size])\n    # 3. compute penalty\n    penalties = tf.log(div_gamma) * tf.cast(index_rank, log_probs.dtype)\n    # 4. only apply penalty with some probability\n    apply_penalty = tf.cast(\n            sample_bernoulli(div_prob, [batch_size, beam_size, vocab_size]),\n            penalties.dtype)\n    penalties *= apply_penalty\n\n    return log_probs + penalties\n\n\ndef merge_batch_beam(t):\n    """"""\n    Args:\n        t: tensor of shape [batch_size, beam_size, ...]\n            whose dimensions after beam_size must be statically known\n\n    Returns:\n        t: tensorf of shape [batch_size * beam_size, ...]\n\n    """"""\n    batch_size = tf.shape(t)[0]\n    beam_size = t.shape[1].value\n\n    if t.shape.ndims == 2:\n        return tf.reshape(t, [batch_size*beam_size, 1])\n    elif t.shape.ndims == 3:\n        return tf.reshape(t, [batch_size*beam_size, t.shape[-1].value])\n    elif t.shape.ndims == 4:\n        return tf.reshape(t, [batch_size*beam_size, t.shape[-2].value,\n                t.shape[-1].value])\n    else:\n        raise NotImplementedError\n\n\ndef split_batch_beam(t, beam_size):\n    """"""\n    Args:\n        t: tensorf of shape [batch_size*beam_size, ...]\n\n    Returns:\n        t: tensor of shape [batch_size, beam_size, ...]\n\n    """"""\n    if t.shape.ndims == 1:\n        return tf.reshape(t, [-1, beam_size])\n    elif t.shape.ndims == 2:\n        return tf.reshape(t, [-1, beam_size, t.shape[-1].value])\n    elif t.shape.ndims == 3:\n        return tf.reshape(t, [-1, beam_size, t.shape[-2].value,\n                t.shape[-1].value])\n    else:\n        raise NotImplementedError\n\n\ndef tile_beam(t, beam_size):\n    """"""\n    Args:\n        t: tensor of shape [batch_size, ...]\n\n    Returns:\n        t: tensorf of shape [batch_size, beam_size, ...]\n\n    """"""\n    # shape = [batch_size, 1 , x]\n    t = tf.expand_dims(t, axis=1)\n    if t.shape.ndims == 2:\n        multiples = [1, beam_size]\n    elif t.shape.ndims == 3:\n        multiples = [1, beam_size, 1]\n    elif t.shape.ndims == 4:\n        multiples = [1, beam_size, 1, 1]\n\n    return tf.tile(t, multiples)\n\n\ndef mask_probs(probs, end_token, finished):\n    """"""\n    Args:\n        probs: tensor of shape [batch_size, beam_size, vocab_size]\n        end_token: (int)\n        finished: tensor of shape [batch_size, beam_size], dtype = tf.bool\n    """"""\n    # one hot of shape [vocab_size]\n    vocab_size = probs.shape[-1].value\n    one_hot = tf.one_hot(end_token, vocab_size, on_value=0.,\n            off_value=probs.dtype.min, dtype=probs.dtype)\n    # expand dims of shape [batch_size, beam_size, 1]\n    finished = tf.expand_dims(tf.cast(finished, probs.dtype), axis=-1)\n\n    return (1. - finished) * probs + finished * one_hot\n\n\ndef gather_helper(t, indices, batch_size, beam_size):\n    """"""\n    Args:\n        t: tensor of shape = [batch_size, beam_size, d]\n        indices: tensor of shape = [batch_size, beam_size]\n\n    Returns:\n        new_t: tensor w shape as t but new_t[:, i] = t[:, new_parents[:, i]]\n\n    """"""\n    range_  = tf.expand_dims(tf.range(batch_size) * beam_size, axis=1)\n    indices = tf.reshape(indices + range_, [-1])\n    output  = tf.gather(\n        tf.reshape(t, [batch_size*beam_size, -1]),\n        indices)\n\n    if t.shape.ndims == 2:\n        return tf.reshape(output, [batch_size, beam_size])\n\n    elif t.shape.ndims == 3:\n        d = t.shape[-1].value\n        return tf.reshape(output, [batch_size, beam_size, d])'"
model/components/dynamic_decode.py,12,"b'import tensorflow as tf\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.ops import rnn\n\n\ndef transpose_batch_time(t):\n    if t.shape.ndims == 2:\n        return tf.transpose(t, [1, 0])\n    elif t.shape.ndims == 3:\n        return tf.transpose(t, [1, 0, 2])\n    elif t.shape.ndims == 4:\n        return tf.transpose(t, [1, 0, 2, 3])\n    else:\n        raise NotImplementedError\n\n\ndef dynamic_decode(decoder_cell, maximum_iterations):\n    """"""Similar to dynamic_rnn but to decode\n\n    Args:\n        decoder_cell: (instance of DecoderCell) with step method\n        maximum_iterations: (int)\n\n    """"""\n    try:\n        maximum_iterations = tf.convert_to_tensor(maximum_iterations,\n                dtype=tf.int32)\n    except ValueError:\n        pass\n\n    # create TA for outputs by mimicing the structure of decodercell output\n    def create_ta(d):\n        return tf.TensorArray(dtype=d, size=0, dynamic_size=True)\n\n    initial_time = tf.constant(0, dtype=tf.int32)\n    initial_outputs_ta = nest.map_structure(create_ta,\n            decoder_cell.output_dtype)\n    initial_state, initial_inputs, initial_finished = decoder_cell.initialize()\n\n    def condition(time, unused_outputs_ta, unused_state, unused_inputs,\n        finished):\n        return tf.logical_not(tf.reduce_all(finished))\n\n    def body(time, outputs_ta, state, inputs, finished):\n        new_output, new_state, new_inputs, new_finished = decoder_cell.step(\n            time, state, inputs, finished)\n\n        outputs_ta = nest.map_structure(lambda ta, out: ta.write(time, out),\n                                      outputs_ta, new_output)\n\n        new_finished = tf.logical_or(\n            tf.greater_equal(time, maximum_iterations),\n            new_finished)\n\n        return (time + 1, outputs_ta, new_state, new_inputs, new_finished)\n\n    with tf.variable_scope(""rnn""):\n        res = tf.while_loop(\n            condition,\n            body,\n            loop_vars=[initial_time, initial_outputs_ta, initial_state,\n                       initial_inputs, initial_finished],\n            back_prop=False)\n\n    # get final outputs and states\n    final_outputs_ta, final_state = res[1], res[2]\n\n    # unfold and stack the structure from the nested tas\n    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n\n    # finalize the computation from the decoder cell\n    final_outputs = decoder_cell.finalize(final_outputs, final_state)\n\n    # transpose the final output\n    final_outputs = nest.map_structure(transpose_batch_time, final_outputs)\n\n    return final_outputs, final_state'"
model/components/greedy_decoder_cell.py,6,"b'import tensorflow as tf\nimport collections\n\n\nclass DecoderOutput(collections.namedtuple(""DecoderOutput"", (""logits"", ""ids""))):\n    pass\n\n\nclass GreedyDecoderCell(object):\n\n    def __init__(self, embeddings, attention_cell, batch_size, start_token,\n        end_token):\n\n        self._embeddings = embeddings\n        self._attention_cell = attention_cell\n        self._dim_embeddings = embeddings.shape[-1].value\n        self._batch_size = batch_size\n        self._start_token = start_token\n        self._end_token = end_token\n\n\n    @property\n    def output_dtype(self):\n        """"""for the custom dynamic_decode for the TensorArray of results""""""\n        return DecoderOutput(logits=self._attention_cell.output_dtype,\n                ids=tf.int32)\n\n\n    @property\n    def final_output_dtype(self):\n        """"""For the finalize method""""""\n        return self.output_dtype\n\n\n    def initial_state(self):\n        """"""Return initial state for the lstm""""""\n        return self._attention_cell.initial_state()\n\n\n    def initial_inputs(self):\n        """"""Returns initial inputs for the decoder (start token)""""""\n        return tf.tile(tf.expand_dims(self._start_token, 0),\n            multiples=[self._batch_size, 1])\n\n\n    def initialize(self):\n        initial_state = self.initial_state()\n        initial_inputs = self.initial_inputs()\n        initial_finished = tf.zeros(shape=[self._batch_size], dtype=tf.bool)\n        return initial_state, initial_inputs, initial_finished\n\n\n    def step(self, time, state, embedding, finished):\n        # next step of attention cell\n        logits, new_state = self._attention_cell.step(embedding, state)\n\n        # get ids of words predicted and get embedding\n        new_ids = tf.cast(tf.argmax(logits, axis=-1), tf.int32)\n        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)\n\n        # create new state of decoder\n        new_output = DecoderOutput(logits, new_ids)\n\n        new_finished = tf.logical_or(finished, tf.equal(new_ids,\n                self._end_token))\n\n        return (new_output, new_state, new_embedding, new_finished)\n\n\n    def finalize(self, final_outputs, final_state):\n        return final_outputs'"
model/components/positional.py,11,"b'from __future__ import division\nimport math\nimport numpy as np\nfrom six.moves import xrange\nimport tensorflow as tf\n\n\n# taken from https://github.com/tensorflow/tensor2tensor/blob/37465a1759e278e8f073cd04cd9b4fe377d3c740/tensor2tensor/layers/common_attention.py\n\n\ndef add_timing_signal_nd(x, min_timescale=1.0, max_timescale=1.0e4):\n    """"""Adds a bunch of sinusoids of different frequencies to a Tensor.\n\n    Each channel of the input Tensor is incremented by a sinusoid of a difft\n    frequency and phase in one of the positional dimensions.\n\n    This allows attention to learn to use absolute and relative positions.\n    Timing signals should be added to some precursors of both the query and the\n    memory inputs to attention.\n\n    The use of relative position is possible because sin(a+b) and cos(a+b) can\n    be experessed in terms of b, sin(a) and cos(a).\n\n    x is a Tensor with n ""positional"" dimensions, e.g. one dimension for a\n    sequence or two dimensions for an image\n\n    We use a geometric sequence of timescales starting with\n    min_timescale and ending with max_timescale.  The number of different\n    timescales is equal to channels // (n * 2). For each timescale, we\n    generate the two sinusoidal signals sin(timestep/timescale) and\n    cos(timestep/timescale).  All of these sinusoids are concatenated in\n    the channels dimension.\n\n    Args:\n        x: a Tensor with shape [batch, d1 ... dn, channels]\n        min_timescale: a float\n        max_timescale: a float\n\n    Returns:\n        a Tensor the same shape as x.\n\n    """"""\n    static_shape = x.get_shape().as_list()\n    num_dims = len(static_shape) - 2\n    channels = tf.shape(x)[-1]\n    num_timescales = channels // (num_dims * 2)\n    log_timescale_increment = (\n            math.log(float(max_timescale) / float(min_timescale)) /\n            (tf.to_float(num_timescales) - 1))\n    inv_timescales = min_timescale * tf.exp(\n            tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n    for dim in xrange(num_dims):\n        length = tf.shape(x)[dim + 1]\n        position = tf.to_float(tf.range(length))\n        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n                inv_timescales, 0)\n        signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n        prepad = dim * 2 * num_timescales\n        postpad = channels - (dim + 1) * 2 * num_timescales\n        signal = tf.pad(signal, [[0, 0], [prepad, postpad]])\n        for _ in xrange(1 + dim):\n            signal = tf.expand_dims(signal, 0)\n        for _ in xrange(num_dims - 1 - dim):\n            signal = tf.expand_dims(signal, -2)\n        x += signal\n    return x'"
model/evaluation/__init__.py,0,b''
model/evaluation/image.py,0,"b'import numpy as np\nimport distance\nfrom scipy.misc import imread\n\n\nfrom ..utils.general import get_files\n\n\ndef score_dirs(dir_ref, dir_hyp, prepro_img):\n    """"""Returns scores from a dir with images\n\n    Args:\n        dir_ref: (string)\n        dir_hyp: (string)\n        prepro_img: (lambda function)\n\n    Returns:\n        scores: (dict)\n\n    """"""\n    img_refs = [f for f in get_files(dir_ref) if f.split(\'.\')[-1] == ""png""]\n    img_hyps = [f for f in get_files(dir_hyp) if f.split(\'.\')[-1] == ""png""]\n\n    em_tot = l_dist_tot = length_tot = n_ex = 0\n\n    for img_name in img_refs:\n        img_ref = imread(dir_ref + img_name)\n        img_ref = prepro_img(img_ref)\n\n        if img_name in img_hyps:\n            img_hyp = imread(dir_hyp + img_name)\n            img_hyp = prepro_img(img_hyp)\n            l_dist, length = img_edit_distance(img_ref, img_hyp)\n        else:\n            l_dist = length = img_ref.shape[1]\n\n        l_dist_tot += l_dist\n        length_tot += length\n        if l_dist < 1: em_tot += 1\n        n_ex += 1\n\n    # compute scores\n    scores = dict()\n    scores[""EM""]  = em_tot / float(n_ex) if n_ex > 0 else 0\n    scores[""Lev""] = 1 - l_dist_tot / float(length_tot) if length_tot > 0 else 0\n\n    return scores\n\n\ndef img_edit_distance(img1, img2):\n    """"""Computes Levenshtein distance between two images.\n    (From Harvard\'s NLP github)\n\n    Slices the images into columns and consider one column as a character.\n\n    Args:\n        im1, im2: np arrays of shape (H, W, 1)\n\n    Returns:\n        column wise levenshtein distance\n        max length of the two sequences\n\n    """"""\n    # load the image (H, W)\n    img1, img2 = img1[:, :, 0], img2[:, :, 0]\n\n    # transpose and convert to 0 or 1\n    img1 = np.transpose(img1)\n    h1 = img1.shape[1]\n    w1 = img1.shape[0]\n    img1 = (img1<=128).astype(np.uint8)\n\n    img2 = np.transpose(img2)\n    h2 = img2.shape[1]\n    w2 = img2.shape[0]\n    img2 = (img2<=128).astype(np.uint8)\n\n    # create binaries for each column\n    if h1 == h2:\n        seq1 = [\'\'.join([str(i) for i in item]) for item in img1]\n        seq2 = [\'\'.join([str(i) for i in item]) for item in img2]\n    elif h1 > h2:\n        seq1 = [\'\'.join([str(i) for i in item]) for item in img1]\n        seq2 = [\'\'.join([str(i) for i in item])+\'\'.join([\'0\']*(h1-h2)) for\n                item in img2]\n    else:\n        seq1 = [\'\'.join([str(i) for i in item])+\'\'.join([\'0\']*(h2-h1)) for\n                item in img1]\n        seq2 = [\'\'.join([str(i) for i in item]) for item in img2]\n\n    # convert each column binary into int\n    seq1_int = [int(item,2) for item in seq1]\n    seq2_int = [int(item,2) for item in seq2]\n\n    # distance\n    l_dist = distance.levenshtein(seq1_int, seq2_int)\n    length = float(max(len(seq1_int), len(seq2_int)))\n\n    return l_dist, length\n'"
model/evaluation/text.py,0,"b'import os\r\nimport sys\r\nimport numpy as np\r\nimport nltk\r\nimport distance\r\n\r\n\r\nfrom ..utils.text import load_formulas\r\nfrom ..utils.general import init_dir\r\n\r\n\r\ndef score_files(path_ref, path_hyp):\r\n    """"""Loads result from file and score it\r\n\r\n    Args:\r\n        path_ref: (string) formulas of reference\r\n        path_hyp: (string) formulas of prediction.\r\n\r\n    Returns:\r\n        scores: (dict)\r\n\r\n    """"""\r\n    # load formulas\r\n    formulas_ref = load_formulas(path_ref)\r\n    formulas_hyp = load_formulas(path_hyp)\r\n\r\n    assert len(formulas_ref) == len(formulas_hyp)\r\n\r\n    # tokenize\r\n    refs = [ref.split(\' \') for _, ref in formulas_ref.items()]\r\n    hyps = [hyp.split(\' \') for _, hyp in formulas_hyp.items()]\r\n\r\n    # score\r\n    return {\r\n            ""BLEU-4"": bleu_score(refs, hyps)*100,\r\n            ""EM"": exact_match_score(refs, hyps)*100,\r\n            ""Edit"": edit_distance(refs, hyps)*100\r\n            }\r\n\r\n\r\ndef exact_match_score(references, hypotheses):\r\n    """"""Computes exact match scores.\r\n\r\n    Args:\r\n        references: list of list of tokens (one ref)\r\n        hypotheses: list of list of tokens (one hypothesis)\r\n\r\n    Returns:\r\n        exact_match: (float) 1 is perfect\r\n\r\n    """"""\r\n    exact_match = 0\r\n    for ref, hypo in zip(references, hypotheses):\r\n        if np.array_equal(ref, hypo):\r\n            exact_match += 1\r\n\r\n    return exact_match / float(max(len(hypotheses), 1))\r\n\r\n\r\ndef bleu_score(references, hypotheses):\r\n    """"""Computes bleu score.\r\n\r\n    Args:\r\n        references: list of list (one hypothesis)\r\n        hypotheses: list of list (one hypothesis)\r\n\r\n    Returns:\r\n        BLEU-4 score: (float)\r\n\r\n    """"""\r\n    references = [[ref] for ref in references] # for corpus_bleu func\r\n    BLEU_4 = nltk.translate.bleu_score.corpus_bleu(references, hypotheses,\r\n        weights=(0.25, 0.25, 0.25, 0.25))\r\n    return BLEU_4\r\n\r\n\r\ndef edit_distance(references, hypotheses):\r\n    """"""Computes Levenshtein distance between two sequences.\r\n\r\n    Args:\r\n        references: list of list of token (one hypothesis)\r\n        hypotheses: list of list of token (one hypothesis)\r\n\r\n    Returns:\r\n        1 - levenshtein distance: (higher is better, 1 is perfect)\r\n\r\n    """"""\r\n    d_leven, len_tot = 0, 0\r\n    for ref, hypo in zip(references, hypotheses):\r\n        d_leven += distance.levenshtein(ref, hypo)\r\n        len_tot += float(max(len(ref), len(hypo)))\r\n\r\n    return 1. - d_leven / len_tot\r\n\r\n\r\ndef truncate_end(list_of_ids, id_end):\r\n    """"""Removes the end of the list starting from the first id_end token""""""\r\n    list_trunc = []\r\n    for idx in list_of_ids:\r\n        if idx == id_end:\r\n            break\r\n        else:\r\n            list_trunc.append(idx)\r\n\r\n    return list_trunc\r\n\r\n\r\ndef write_answers(references, hypotheses, rev_vocab, dir_name, id_end):\r\n    """"""Writes text answers in files.\r\n\r\n    One file for the reference, one file for each hypotheses\r\n\r\n    Args:\r\n        references: list of list         (one reference)\r\n        hypotheses: list of list of list (multiple hypotheses)\r\n            hypotheses[0] is a list of all the first hypothesis for all the\r\n            dataset\r\n        rev_vocab: (dict) rev_vocab[idx] = word\r\n        dir_name: (string) path where to write results\r\n        id_end: (int) special id of token that corresponds to the END of\r\n            sentence\r\n\r\n    Returns:\r\n        file_names: list of the created files\r\n\r\n    """"""\r\n    def ids_to_str(ids):\r\n        ids = truncate_end(ids, id_end)\r\n        s = [rev_vocab[idx] for idx in ids]\r\n        return "" "".join(s)\r\n\r\n    def write_file(file_name, list_of_list):\r\n        with open(file_name, ""w"") as f:\r\n            for l in list_of_list:\r\n                f.write(ids_to_str(l) + ""\\n"")\r\n\r\n    init_dir(dir_name)\r\n    file_names = [dir_name + ""ref.txt""]\r\n    write_file(dir_name + ""ref.txt"", references) # one file for the ref\r\n    for i in range(len(hypotheses)):             # one file per hypo\r\n        assert len(references) == len(hypotheses[i])\r\n        write_file(dir_name + ""hyp_{}.txt"".format(i), hypotheses[i])\r\n        file_names.append(dir_name + ""hyp_{}.txt"".format(i))\r\n\r\n    return file_names\r\n\r\n'"
model/utils/__init__.py,0,b''
model/utils/data_generator.py,0,"b'import time\nimport os\nimport numpy as np\nfrom scipy.misc import imread\n\n\nfrom .text import load_formulas\nfrom .image import build_images, greyscale\nfrom .general import init_dir\n\n\nclass DataGeneratorFile(object):\n    """"""Simple Generator of tuples (img_path, formula_id)""""""\n\n    def __init__(self, filename):\n        """"""Inits Data Generator File\n\n        Iterator that returns\n            tuple (img_path, formula_id)\n\n        Args:\n            filename: (string of path to file)\n\n        """"""\n        self._filename = filename\n\n    def __iter__(self):\n        with open(self._filename) as f:\n            for line in f:\n                line = line.strip().split(\' \')\n                path_img, id_formula = line[0], line[1]\n                yield path_img, id_formula\n\n\n\nclass DataGenerator(object):\n    """"""Data Generator of tuple (image, formula)""""""\n\n    def __init__(self, path_formulas, dir_images, path_matching, bucket=False,\n                form_prepro=lambda s: s.strip().split(\' \'), iter_mode=""data"",\n                img_prepro=lambda x: x, max_iter=None, max_len=None,\n                bucket_size=20):\n        """"""Initializes the DataGenerator\n\n        Args:\n            path_formulas: (string) file of formulas.\n            dir_images: (string) dir of images, contains jpg files.\n            path_matching: (string) file of name_of_img, id_formula\n            img_prepro: (lambda function) takes an array -> an array. Default,\n                identity\n            form_prepro: (lambda function) takes a string -> array of int32.\n                Default, identity.\n            max_iter: (int) maximum numbers of elements in the dataset\n            max_len: (int) maximum length of a formula in the dataset\n                if longer, not yielded.\n            iter_mode: (string) ""data"", ""full"" to set the type returned by the\n                generator\n            bucket: (bool) decides if bucket the data by size of image\n            bucket_size: (int)\n\n        """"""\n        self._path_formulas  = path_formulas\n        self._dir_images     = dir_images\n        self._path_matching  = path_matching\n        self._img_prepro     = img_prepro\n        self._form_prepro    = form_prepro\n        self._max_iter       = max_iter\n        self._max_len        = max_len\n        self._iter_mode      = iter_mode\n        self._bucket         = bucket\n        self._bucket_size    = bucket_size\n\n        self._length         = None\n        self._formulas       = self._load_formulas(path_formulas)\n\n        self._set_data_generator()\n\n\n    def _set_data_generator(self):\n        """"""Sets iterable or generator of tuples (img_path, id of formula)""""""\n        self._data_generator = DataGeneratorFile(self._path_matching)\n\n        if self._bucket:\n            self._data_generator = self.bucket(self._bucket_size)\n\n\n    def bucket(self, bucket_size):\n        """"""Iterates over the listing and creates buckets of same shape images.\n\n        Args:\n            bucket_size: (int) size of the bucket\n\n        Returns:\n            bucketed_dataset: [(img_path1, id1), ...]\n\n        """"""\n        print(""Bucketing the dataset..."")\n        bucketed_dataset = []\n        old_mode = self._iter_mode # store the old iteration mode\n        self._iter_mode = ""full""\n\n        # iterate over the dataset in ""full"" mode and create buckets\n        data_buckets = dict() # buffer for buckets\n        for idx, (img, formula, img_path, formula_id) in enumerate(self):\n            s = img.shape\n            if s not in data_buckets:\n                data_buckets[s] = []\n            # if bucket is full, write it and empty it\n            if len(data_buckets[s]) == bucket_size:\n                for (img_path, formula_id) in data_buckets[s]:\n                    bucketed_dataset += [(img_path, formula_id)]\n                data_buckets[s] = []\n\n            data_buckets[s] += [(img_path, formula_id)]\n\n        # write the rest of the buffer\n        for k, v in data_buckets.iteritems():\n            for (img_path, formula_id) in v:\n                bucketed_dataset += [(img_path, formula_id)]\n\n\n        self._iter_mode = old_mode\n        self._length    = idx + 1\n\n        print(""- done."")\n        return bucketed_dataset\n\n\n    def _load_formulas(self, filename):\n        """"""Loads txt file with formulas in a dict\n\n        Args:\n            filename: (string) path of formulas.\n\n        Returns:\n            dict: dict[idx] = one formula\n\n        """"""\n        formulas = load_formulas(filename)\n        return formulas\n\n\n    def _get_raw_formula(self, formula_id):\n        try:\n            formula_raw = self._formulas[int(formula_id)]\n        except KeyError:\n            print(""Tried to access id {} but only {} formulas"".format(\n                formula_id, len(self._formulas)))\n            print(""Possible fix: mismatch between matching file and formulas"")\n            raise KeyError\n\n        return formula_raw\n\n\n    def _process_instance(self, example):\n        """"""From path and formula id, returns actual data\n\n        Applies preprocessing to both image and formula\n\n        Args:\n            example: tuple (img_path, formula_ids)\n                img_path: (string) path to image\n                formula_id: (int) id of the formula\n\n        Returns:\n            img: depending on _img_prepro\n            formula: depending on _form_prepro\n\n        """"""\n        img_path, formula_id = example\n\n        img = imread(self._dir_images + ""/"" + img_path)\n        img = self._img_prepro(img)\n        formula = self._form_prepro(self._get_raw_formula(formula_id))\n\n        if self._iter_mode == ""data"":\n            inst = (img, formula)\n        elif self._iter_mode == ""full"":\n            inst = (img, formula, img_path, formula_id)\n\n        # filter on the formula length\n        if self._max_len is not None and len(formula) > self._max_len:\n            skip = True\n        else:\n            skip = False\n\n        return inst, skip\n\n\n    def __iter__(self):\n        """"""Iterator over Dataset\n\n        Yields:\n            tuple (img, formula)\n\n        """"""\n        n_iter = 0\n        for example in self._data_generator:\n            if self._max_iter is not None and n_iter >= self._max_iter:\n                break\n            result, skip = self._process_instance(example)\n            if skip:\n                continue\n            n_iter += 1\n            yield result\n\n\n    def __len__(self):\n        if self._length is None:\n            print(""First call to len(dataset) - may take a while."")\n            counter = 0\n            for _ in self:\n                counter += 1\n            self._length = counter\n            print(""- done."")\n\n        return self._length\n\n\n    def build(self, quality=100, density=200, down_ratio=2, buckets=None,\n                n_threads=4):\n        """"""Generates images from the formulas and writes the correspondance\n        in the matching file.\n\n        Args:\n            quality: parameter for magick\n            density: parameter for magick\n            down_ratio: (int) downsampling ratio\n            buckets: list of tuples (list of sizes) to produce similar\n                shape images\n\n        """"""\n        # 1. produce images\n        init_dir(self._dir_images)\n        result = build_images(self._formulas, self._dir_images, quality,\n                density, down_ratio, buckets, n_threads)\n\n        # 2. write matching with same convention of naming\n        with open(self._path_matching, ""w"") as f:\n            for (path_img, idx) in result:\n                if path_img is not False: # image was successfully produced\n                    f.write(""{} {}\\n"".format(path_img, idx))\n'"
model/utils/general.py,0,"b'import os\nimport numpy as np\nimport time\nimport logging\nimport sys\nimport subprocess, shlex\nfrom shutil import copyfile\nimport json\nfrom threading import Timer\nfrom os import listdir\nfrom os.path import isfile, join\n\n\ndef minibatches(data_generator, minibatch_size):\n    """"""\n    Args:\n        data_generator: generator of (img, formulas) tuples\n        minibatch_size: (int)\n\n    Returns:\n        list of tuples\n\n    """"""\n    x_batch, y_batch = [], []\n    for (x, y) in data_generator:\n        if len(x_batch) == minibatch_size:\n            yield x_batch, y_batch\n            x_batch, y_batch = [], []\n\n        x_batch += [x]\n        y_batch += [y]\n\n    if len(x_batch) != 0:\n        yield x_batch, y_batch\n\n\ndef run(cmd, timeout_sec):\n    """"""Run cmd in the shell with timeout""""""\n    proc = subprocess.Popen(cmd, shell=True)\n    kill_proc = lambda p: p.kill()\n    timer = Timer(timeout_sec, kill_proc, [proc])\n    try:\n        timer.start()\n        stdout,stderr = proc.communicate()\n    finally:\n        timer.cancel()\n\n\ndef get_logger(filename):\n    """"""Return instance of logger""""""\n    logger = logging.getLogger(\'logger\')\n    logger.setLevel(logging.INFO)\n    logging.basicConfig(format=\'%(message)s\', level=logging.INFO)\n    handler = logging.FileHandler(filename)\n    handler.setLevel(logging.INFO)\n    handler.setFormatter(logging.Formatter(\n            \'%(asctime)s:%(levelname)s: %(message)s\'))\n    logging.getLogger().addHandler(handler)\n    return logger\n\n\ndef init_dir(dir_name):\n    """"""Creates directory if it does not exists""""""\n    if dir_name is not None:\n        if not os.path.exists(dir_name):\n            os.makedirs(dir_name)\n\n\ndef init_file(path_file, mode=""a""):\n    """"""Makes sure that a given file exists""""""\n    with open(path_file, mode) as f:\n        pass\n\n\ndef get_files(dir_name):\n    files = [f for f in listdir(dir_name) if isfile(join(dir_name, f))]\n    return files\n\n\ndef delete_file(path_file):\n    try:\n        os.remove(path_file)\n    except Exception:\n        pass\n\n\nclass Config():\n    """"""Class that loads hyperparameters from json file into attributes""""""\n\n    def __init__(self, source):\n        """"""\n        Args:\n            source: path to json file or dict\n        """"""\n        self.source = source\n\n        if type(source) is dict:\n            self.__dict__.update(source)\n        elif type(source) is list:\n            for s in source:\n                self.load_json(s)\n        else:\n            self.load_json(source)\n\n\n    def load_json(self, source):\n        with open(source) as f:\n            data = json.load(f)\n            self.__dict__.update(data)\n\n\n    def save(self, dir_name):\n        init_dir(dir_name)\n        if type(self.source) is list:\n            for s in self.source:\n                c = Config(s)\n                c.save(dir_name)\n        elif type(self.source) is dict:\n            json.dumps(self.source, indent=4)\n        else:\n            copyfile(self.source, dir_name + self.export_name)\n\n\nclass Progbar(object):\n    """"""Progbar class inspired by keras""""""\n\n    def __init__(self, max_step, width=30):\n        self.max_step = max_step\n        self.width = width\n        self.last_width = 0\n\n        self.sum_values = {}\n\n        self.start = time.time()\n        self.last_step = 0\n\n        self.info = """"\n        self.bar  = """"\n\n\n    def _update_values(self, curr_step, values):\n        for k, v in values:\n            if k not in self.sum_values:\n                self.sum_values[k] = [v * (curr_step - self.last_step),\n                        curr_step - self.last_step]\n            else:\n                self.sum_values[k][0] += v * (curr_step - self.last_step)\n                self.sum_values[k][1] += (curr_step - self.last_step)\n\n\n    def _write_bar(self, curr_step):\n        last_width = self.last_width\n        sys.stdout.write(""\\b"" * last_width)\n        sys.stdout.write(""\\r"")\n\n        numdigits = int(np.floor(np.log10(self.max_step))) + 1\n        barstr = \'%%%dd/%%%dd [\' % (numdigits, numdigits)\n        bar = barstr % (curr_step, self.max_step)\n        prog = float(curr_step)/self.max_step\n        prog_width = int(self.width*prog)\n        if prog_width > 0:\n            bar += (\'=\'*(prog_width-1))\n            if curr_step < self.max_step:\n                bar += \'>\'\n            else:\n                bar += \'=\'\n        bar += (\'.\'*(self.width-prog_width))\n        bar += \']\'\n        sys.stdout.write(bar)\n\n        return bar\n\n\n    def _get_eta(self, curr_step):\n        now = time.time()\n        if curr_step:\n            time_per_unit = (now - self.start) / curr_step\n        else:\n            time_per_unit = 0\n        eta = time_per_unit*(self.max_step - curr_step)\n\n        if curr_step < self.max_step:\n            info = \' - ETA: %ds\' % eta\n        else:\n            info = \' - %ds\' % (now - self.start)\n\n        return info\n\n\n    def _get_values_sum(self):\n        info = """"\n        for name, value in self.sum_values.items():\n            info += \' - %s: %.4f\' % (name, value[0] / max(1, value[1]))\n        return info\n\n\n    def _write_info(self, curr_step):\n        info = """"\n        info += self._get_eta(curr_step)\n        info += self._get_values_sum()\n\n        sys.stdout.write(info)\n\n        return info\n\n\n    def _update_width(self, curr_step):\n        curr_width = len(self.bar) + len(self.info)\n        if curr_width < self.last_width:\n            sys.stdout.write("" ""*(self.last_width - curr_width))\n\n        if curr_step >= self.max_step:\n            sys.stdout.write(""\\n"")\n\n        sys.stdout.flush()\n\n        self.last_width = curr_width\n\n\n    def update(self, curr_step, values):\n        """"""Updates the progress bar.\n\n        Args:\n            values: List of tuples (name, value_for_last_step).\n                The progress bar will display averages for these values.\n\n        """"""\n        self._update_values(curr_step, values)\n        self.bar = self._write_bar(curr_step)\n        self.info = self._write_info(curr_step)\n        self._update_width(curr_step)\n        self.last_step = curr_step\n'"
model/utils/image.py,0,"b'import os\nimport numpy as np\nimport os\nimport PIL\nfrom PIL import Image\nfrom multiprocessing import Pool\n\n\nfrom .general import run, get_files, delete_file, init_dir\n\n\nTIMEOUT = 10\n\n\ndef get_max_shape(arrays):\n    """"""\n    Args:\n        images: list of arrays\n\n    """"""\n    shapes = map(lambda x: list(x.shape), arrays)\n    ndim = len(arrays[0].shape)\n    max_shape = []\n    for d in range(ndim):\n        max_shape += [max(shapes, key=lambda x: x[d])[d]]\n\n    return max_shape\n\n\ndef pad_batch_images(images, max_shape=None):\n    """"""\n    Args:\n        images: list of arrays\n        target_shape: shape at which we want to pad\n\n    """"""\n\n    # 1. max shape\n    if max_shape is None:\n        max_shape = get_max_shape(images)\n\n    # 2. apply formating\n    batch_images = 255 * np.ones([len(images)] + list(max_shape))\n    for idx, img in enumerate(images):\n        batch_images[idx, :img.shape[0], :img.shape[1]] = img\n\n    return batch_images.astype(np.uint8)\n\n\ndef greyscale(state):\n    """"""Preprocess state (:, :, 3) image into greyscale""""""\n    state = state[:, :, 0]*0.299 + state[:, :, 1]*0.587 + state[:, :, 2]*0.114\n    state = state[:, :, np.newaxis]\n    return state.astype(np.uint8)\n\n\ndef downsample(state):\n    """"""Downsamples an image on the first 2 dimensions\n\n    Args:\n        state: (np array) with 3 dimensions\n\n    """"""\n    return state[::2, ::2, :]\n\n\ndef pad_image(img, output_path, pad_size=[8,8,8,8], buckets=None):\n    """"""Pads image with pad size and with buckets\n\n    Args:\n        img: (string) path to image\n        output_path: (string) path to output image\n        pad_size: list of 4 ints\n        buckets: ascending ordered list of sizes, [(width, height), ...]\n\n    """"""\n    top, left, bottom, right = pad_size\n    old_im = Image.open(img)\n    old_size = (old_im.size[0] + left + right, old_im.size[1] + top + bottom)\n    new_size = get_new_size(old_size, buckets)\n    new_im = Image.new(""RGB"", new_size, (255,255,255))\n    new_im.paste(old_im, (left, top))\n    new_im.save(output_path)\n\n\ndef get_new_size(old_size, buckets):\n    """"""Computes new size from buckets\n\n    Args:\n        old_size: (width, height)\n        buckets: list of sizes\n\n    Returns:\n        new_size: original size or first bucket in iter order that matches the\n            size.\n\n    """"""\n    if buckets is None:\n        return old_size\n    else:\n        w, h = old_size\n        for (w_b, h_b) in buckets:\n            if w_b >= w and h_b >= h:\n                return w_b, h_b\n\n        return old_size\n\n\ndef crop_image(img, output_path):\n    """"""Crops image to content\n\n    Args:\n        img: (string) path to image\n        output_path: (string) path to output image\n\n    """"""\n    old_im = Image.open(img).convert(\'L\')\n    img_data = np.asarray(old_im, dtype=np.uint8) # height, width\n    nnz_inds = np.where(img_data!=255)\n    if len(nnz_inds[0]) == 0:\n        old_im.save(output_path)\n        return False\n\n    y_min = np.min(nnz_inds[0])\n    y_max = np.max(nnz_inds[0])\n    x_min = np.min(nnz_inds[1])\n    x_max = np.max(nnz_inds[1])\n    old_im = old_im.crop((x_min, y_min, x_max+1, y_max+1))\n    old_im.save(output_path)\n    return True\n\n\ndef downsample_image(img, output_path, ratio=2):\n    """"""Downsample image by ratio""""""\n    assert ratio>=1, ratio\n    if ratio == 1:\n        return True\n    old_im = Image.open(img)\n    old_size = old_im.size\n    new_size = (int(old_size[0]/ratio), int(old_size[1]/ratio))\n\n    new_im = old_im.resize(new_size, PIL.Image.LANCZOS)\n    new_im.save(output_path)\n    return True\n\n\ndef convert_to_png(formula, dir_output, name, quality=100, density=200,\n        down_ratio=2, buckets=None):\n    """"""Converts LaTeX to png image\n\n    Args:\n        formula: (string) of latex\n        dir_output: (string) path to output directory\n        name: (string) name of file\n        down_ratio: (int) downsampling ratio\n        buckets: list of tuples (list of sizes) to produce similar shape images\n\n    """"""\n    # write formula into a .tex file\n    with open(dir_output + ""{}.tex"".format(name), ""w"") as f:\n        f.write(\n    r""""""\\documentclass[preview]{standalone}\n    \\begin{document}\n        $$ %s $$\n    \\end{document}"""""" % (formula))\n\n    # call pdflatex to create pdf\n    run(""pdflatex -interaction=nonstopmode -output-directory={} {}"".format(\n        dir_output, dir_output+""{}.tex"".format(name)), TIMEOUT)\n\n    # call magick to convert the pdf into a png file\n    run(""magick convert -density {} -quality {} {} {}"".format(density, quality,\n        dir_output+""{}.pdf"".format(name), dir_output+""{}.png"".format(name)),\n        TIMEOUT)\n\n    # cropping and downsampling\n    img_path = dir_output + ""{}.png"".format(name)\n\n    try:\n        crop_image(img_path, img_path)\n        pad_image(img_path, img_path, buckets=buckets)\n        downsample_image(img_path, img_path, down_ratio)\n        clean(dir_output, name)\n\n        return ""{}.png"".format(name)\n\n    except Exception, e:\n        print(e)\n        clean(dir_output, name)\n        return False\n\n\ndef clean(dir_output, name):\n    delete_file(dir_output+""{}.aux"".format(name))\n    delete_file(dir_output+""{}.log"".format(name))\n    delete_file(dir_output+""{}.pdf"".format(name))\n    delete_file(dir_output+""{}.tex"".format(name))\n\n\ndef build_image(item):\n    idx, form, dir_images, quality, density, down_ratio, buckets = item\n    name = str(idx)\n    path_img = convert_to_png(form, dir_images, name, quality, density,\n            down_ratio, buckets)\n    return (path_img, idx)\n\n\ndef build_images(formulas, dir_images, quality=100, density=200, down_ratio=2,\n        buckets=None, n_threads=4):\n    """"""Parallel procedure to produce images from formulas\n\n    If some of the images have already been produced, does not recompile them.\n\n    Args:\n        formulas: (dict) idx -> string\n\n    Returns:\n        list of (path_img, idx). If an exception was raised during the image\n            generation, path_img = False\n    """"""\n    init_dir(dir_images)\n    existing_idx = sorted(set([int(file_name.split(\'.\')[0]) for file_name in\n            get_files(dir_images) if file_name.split(\'.\')[-1] == ""png""]))\n\n    pool   = Pool(n_threads)\n    result = pool.map(build_image, [(idx, form, dir_images, quality, density,\n            down_ratio, buckets) for idx, form in formulas.items()\n            if idx not in existing_idx])\n    pool.close()\n    pool.join()\n\n    result += [(str(idx) + "".png"", idx) for idx in existing_idx]\n\n    return result\n'"
model/utils/lr_schedule.py,0,"b'import numpy as np\n\n\nclass LRSchedule(object):\n    """"""Class for Learning Rate schedules\n\n    Implements\n        - (time) exponential decay with custom range\n            - needs to set start_decay, end_decay, lr_init and lr_min\n            - set end_decay to None to deactivate\n        - (time) warm start:\n            - needs to set lr_warm, end_warm.\n            - set end_warm to None to deactivate\n        - (score) mult decay if no improvement over score\n            - needs to set decay_rate\n            - set decay_rate to None to deactivate\n        - (score) early stopping if no imprv\n            - needs to set early_stopping\n            - set early_stopping to None to deactivate\n\n    All durations are measured in number of batches\n    For usage, must call the update function at each batch.\n    You can access the current learning rate with self.lr\n\n    """"""\n\n    def __init__(self, lr_init=1e-3, lr_min=1e-4, start_decay=0,\n        decay_rate=None, end_decay=None, lr_warm=1e-4, end_warm=None,\n        early_stopping=None):\n        """"""Initializes Learning Rate schedule\n\n        Sets self.lr and self.stop_training\n\n        Args:\n            lr_init: (float) initial lr\n            lr_min: (float)\n            start_decay: (int) id of batch to start decay\n            decay_rate: (float) lr *= decay_rate if no improval. If None, no\n                multiplicative decay.\n            end_decay: (int) id of batch to end decay. If None, no exp decay\n            lr_warm: (float) constant learning rate at the beginning\n            end_warm: (int) id of batch to keep the lr_warm before returning to\n                lr_init and start the regular schedule.\n            early_stopping: (int) number of batches with no imprv\n\n        """"""\n        self._lr_init     = lr_init\n        self._lr_min      = lr_min\n        self._start_decay = start_decay\n        self._decay_rate  = decay_rate\n        self._end_decay   = end_decay\n        self._lr_warm     = lr_warm\n        self._end_warm    = end_warm\n\n        self._score            = None\n        self._early_stopping   = early_stopping\n        self._n_batch_no_imprv = 0\n\n        # warm start initializes learning rate to warm start\n        if self._end_warm is not None:\n            # make sure that decay happens after the warm up\n            self._start_decay = max(self._end_warm, self._start_decay)\n            self.lr = self._lr_warm\n        else:\n            self.lr = lr_init\n\n        # setup of exponential decay\n        if self._end_decay is not None:\n            self._exp_decay = np.power(lr_min/lr_init,\n                    1/float(self._end_decay - self._start_decay))\n\n\n    @property\n    def stop_training(self):\n        """"""For Early Stopping""""""\n        if (self._early_stopping is not None and\n            (self._n_batch_no_imprv >= self._early_stopping)):\n            return True\n        else:\n            return False\n\n\n    def update(self, batch_no=None, score=None):\n        """"""Updates the learning rate\n\n        (score) decay by self.decay rate if score is higher than previous\n        (time) update lr according to\n            - warm up\n            - exp decay\n        Both updates can concurrently happen\n\n        Args:\n            batch_no: (int) id of the batch\n            score: (float) score, higher is better\n\n        """"""\n        # update based on time\n        if batch_no is not None:\n            if (self._end_warm is not None and\n                (self._end_warm <= batch_no <= self._start_decay)):\n                self.lr = self._lr_init\n\n            if batch_no > self._start_decay and self._end_decay is not None:\n                self.lr *= self._exp_decay\n\n        # update based on performance\n        if self._decay_rate is not None:\n            if score is not None and self._score is not None:\n                if score <= self._score:\n                    self.lr *= self._decay_rate\n                    self._n_batch_no_imprv += 1\n                else:\n                    self._n_batch_no_imprv = 0\n\n        # update last score eval\n        if score is not None:\n            self._score = score\n\n        self.lr = max(self.lr, self._lr_min)\n'"
model/utils/text.py,0,"b'import numpy as np\nfrom collections import Counter\n\n\nclass Vocab(object):\n\n    def __init__(self, config):\n        self.config = config\n        self.load_vocab()\n\n\n    def load_vocab(self):\n        special_tokens = [self.config.unk, self.config.pad, self.config.end]\n        self.tok_to_id = load_tok_to_id(self.config.path_vocab, special_tokens)\n        self.id_to_tok = {idx: tok for tok, idx in self.tok_to_id.iteritems()}\n        self.n_tok = len(self.tok_to_id)\n\n        self.id_pad = self.tok_to_id[self.config.pad]\n        self.id_end = self.tok_to_id[self.config.end]\n        self.id_unk = self.tok_to_id[self.config.unk]\n\n\n    @property\n    def form_prepro(self):\n        return get_form_prepro(self.tok_to_id, self.id_unk)\n\n\ndef get_form_prepro(vocab, id_unk):\n    """"""Given a vocab, returns a lambda function word -> id\n\n    Args:\n        vocab: dict[token] = id\n\n    Returns:\n        lambda function(formula) -> list of ids\n\n    """"""\n    def get_token_id(token):\n        return vocab[token] if token in vocab else id_unk\n\n    def f(formula):\n        formula = formula.strip().split(\' \')\n        return map(lambda t: get_token_id(t), formula)\n\n    return f\n\n\ndef load_tok_to_id(filename, tokens=[]):\n    """"""\n    Args:\n        filename: (string) path to vocab txt file one word per line\n        tokens: list of token to add to vocab after reading filename\n\n    Returns:\n        dict: d[token] = id\n\n    """"""\n    tok_to_id = dict()\n    with open(filename) as f:\n        for idx, token in enumerate(f):\n            token = token.strip()\n            tok_to_id[token] = idx\n\n    # add extra tokens\n    for tok in tokens:\n        tok_to_id[tok] = len(tok_to_id)\n\n    return tok_to_id\n\n\ndef build_vocab(datasets, min_count=10):\n    """"""Build vocabulary from an iterable of datasets objects\n\n    Args:\n        datasets: a list of dataset objects\n        min_count: (int) if token appears less times, do not include it.\n\n    Returns:\n        a set of all the words in the dataset\n\n    """"""\n    print(""Building vocab..."")\n    c = Counter()\n    for dataset in datasets:\n        for _, formula in dataset:\n            try:\n                c.update(formula)\n            except Exception:\n                print(formula)\n                raise Exception\n    vocab = [tok for tok, count in c.items() if count >= min_count]\n    print(""- done. {}/{} tokens added to vocab."".format(len(vocab), len(c)))\n    return sorted(vocab)\n\n\ndef write_vocab(vocab, filename):\n    """"""Writes a vocab to a file\n\n    Writes one word per line.\n\n    Args:\n        vocab: iterable that yields word\n        filename: path to vocab file\n\n    Returns:\n        write a word per line\n\n    """"""\n    print(""Writing vocab..."")\n    with open(filename, ""w"") as f:\n        for i, word in enumerate(vocab):\n            if i != len(vocab) - 1:\n                f.write(""{}\\n"".format(word))\n            else:\n                f.write(word)\n    print(""- done. {} tokens"".format(i+1))\n\n\ndef pad_batch_formulas(formulas, id_pad, id_end, max_len=None):\n    """"""Pad formulas to the max length with id_pad and adds and id_end token\n    at the end of each formula\n\n    Args:\n        formulas: (list) of list of ints\n        max_length: length maximal of formulas\n\n    Returns:\n        array: of shape = (batch_size, max_len) of type np.int32\n        array: of shape = (batch_size) of type np.int32\n\n    """"""\n    if max_len is None:\n        max_len = max(map(lambda x: len(x), formulas))\n\n    batch_formulas = id_pad * np.ones([len(formulas), max_len+1],\n            dtype=np.int32)\n    formula_length = np.zeros(len(formulas), dtype=np.int32)\n    for idx, formula in enumerate(formulas):\n        batch_formulas[idx, :len(formula)] = np.asarray(formula,\n                dtype=np.int32)\n        batch_formulas[idx, len(formula)]  = id_end\n        formula_length[idx] = len(formula) + 1\n\n    return batch_formulas, formula_length\n\n\ndef load_formulas(filename):\n    formulas = dict()\n    with open(filename) as f:\n        for idx, line in enumerate(f):\n            formulas[idx] = line.strip()\n\n    print(""Loaded {} formulas from {}"".format(len(formulas), filename))\n    return formulas\n'"
