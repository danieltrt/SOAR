file_path,api_count,code
build_data.py,0,"b'from model.config import Config\nfrom model.data_utils import CoNLLDataset, get_vocabs, UNK, NUM, \\\n    get_glove_vocab, write_vocab, load_vocab, get_char_vocab, \\\n    export_trimmed_glove_vectors, get_processing_word\n\n\ndef main():\n    """"""Procedure to build data\n\n    You MUST RUN this procedure. It iterates over the whole dataset (train,\n    dev and test) and extract the vocabularies in terms of words, tags, and\n    characters. Having built the vocabularies it writes them in a file. The\n    writing of vocabulary in a file assigns an id (the line #) to each word.\n    It then extract the relevant GloVe vectors and stores them in a np array\n    such that the i-th entry corresponds to the i-th word in the vocabulary.\n\n\n    Args:\n        config: (instance of Config) has attributes like hyper-params...\n\n    """"""\n    # get config and processing of words\n    config = Config(load=False)\n    processing_word = get_processing_word(lowercase=True)\n\n    # Generators\n    dev   = CoNLLDataset(config.filename_dev, processing_word)\n    test  = CoNLLDataset(config.filename_test, processing_word)\n    train = CoNLLDataset(config.filename_train, processing_word)\n\n    # Build Word and Tag vocab\n    vocab_words, vocab_tags = get_vocabs([train, dev, test])\n    vocab_glove = get_glove_vocab(config.filename_glove)\n\n    vocab = vocab_words & vocab_glove\n    vocab.add(UNK)\n    vocab.add(NUM)\n\n    # Save vocab\n    write_vocab(vocab, config.filename_words)\n    write_vocab(vocab_tags, config.filename_tags)\n\n    # Trim GloVe Vectors\n    vocab = load_vocab(config.filename_words)\n    export_trimmed_glove_vectors(vocab, config.filename_glove,\n                                config.filename_trimmed, config.dim_word)\n\n    # Build and save char vocab\n    train = CoNLLDataset(config.filename_train)\n    vocab_chars = get_char_vocab(train)\n    write_vocab(vocab_chars, config.filename_chars)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
evaluate.py,0,"b'from model.data_utils import CoNLLDataset\nfrom model.ner_model import NERModel\nfrom model.config import Config\n\n\ndef align_data(data):\n    """"""Given dict with lists, creates aligned strings\n\n    Adapted from Assignment 3 of CS224N\n\n    Args:\n        data: (dict) data[""x""] = [""I"", ""love"", ""you""]\n              (dict) data[""y""] = [""O"", ""O"", ""O""]\n\n    Returns:\n        data_aligned: (dict) data_align[""x""] = ""I love you""\n                           data_align[""y""] = ""O O    O  ""\n\n    """"""\n    spacings = [max([len(seq[i]) for seq in data.values()])\n                for i in range(len(data[list(data.keys())[0]]))]\n    data_aligned = dict()\n\n    # for each entry, create aligned string\n    for key, seq in data.items():\n        str_aligned = """"\n        for token, spacing in zip(seq, spacings):\n            str_aligned += token + "" "" * (spacing - len(token) + 1)\n\n        data_aligned[key] = str_aligned\n\n    return data_aligned\n\n\n\ndef interactive_shell(model):\n    """"""Creates interactive shell to play with model\n\n    Args:\n        model: instance of NERModel\n\n    """"""\n    model.logger.info(""""""\nThis is an interactive mode.\nTo exit, enter \'exit\'.\nYou can enter a sentence like\ninput> I love Paris"""""")\n\n    while True:\n        try:\n            # for python 2\n            sentence = raw_input(""input> "")\n        except NameError:\n            # for python 3\n            sentence = input(""input> "")\n\n        words_raw = sentence.strip().split("" "")\n\n        if words_raw == [""exit""]:\n            break\n\n        preds = model.predict(words_raw)\n        to_print = align_data({""input"": words_raw, ""output"": preds})\n\n        for key, seq in to_print.items():\n            model.logger.info(seq)\n\n\ndef main():\n    # create instance of config\n    config = Config()\n\n    # build model\n    model = NERModel(config)\n    model.build()\n    model.restore_session(config.dir_model)\n\n    # create dataset\n    test  = CoNLLDataset(config.filename_test, config.processing_word,\n                         config.processing_tag, config.max_iter)\n\n    # evaluate and interact\n    model.evaluate(test)\n    interactive_shell(model)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
train.py,0,"b'from model.data_utils import CoNLLDataset\nfrom model.ner_model import NERModel\nfrom model.config import Config\n\n\ndef main():\n    # create instance of config\n    config = Config()\n\n    # build model\n    model = NERModel(config)\n    model.build()\n    # model.restore_session(""results/crf/model.weights/"") # optional, restore weights\n    # model.reinitialize_weights(""proj"")\n\n    # create datasets\n    dev   = CoNLLDataset(config.filename_dev, config.processing_word,\n                         config.processing_tag, config.max_iter)\n    train = CoNLLDataset(config.filename_train, config.processing_word,\n                         config.processing_tag, config.max_iter)\n\n    # train model\n    model.train(train, dev)\n\nif __name__ == ""__main__"":\n    main()\n'"
model/__init__.py,0,b''
model/base_model.py,16,"b'import os\nimport tensorflow as tf\n\n\nclass BaseModel(object):\n    """"""Generic class for general methods that are not specific to NER""""""\n\n    def __init__(self, config):\n        """"""Defines self.config and self.logger\n\n        Args:\n            config: (Config instance) class with hyper parameters,\n                vocab and embeddings\n\n        """"""\n        self.config = config\n        self.logger = config.logger\n        self.sess   = None\n        self.saver  = None\n\n\n    def reinitialize_weights(self, scope_name):\n        """"""Reinitializes the weights of a given layer""""""\n        variables = tf.contrib.framework.get_variables(scope_name)\n        init = tf.variables_initializer(variables)\n        self.sess.run(init)\n\n\n    def add_train_op(self, lr_method, lr, loss, clip=-1):\n        """"""Defines self.train_op that performs an update on a batch\n\n        Args:\n            lr_method: (string) sgd method, for example ""adam""\n            lr: (tf.placeholder) tf.float32, learning rate\n            loss: (tensor) tf.float32 loss to minimize\n            clip: (python float) clipping of gradient. If < 0, no clipping\n\n        """"""\n        _lr_m = lr_method.lower() # lower to make sure\n\n        with tf.variable_scope(""train_step""):\n            if _lr_m == \'adam\': # sgd method\n                optimizer = tf.train.AdamOptimizer(lr)\n            elif _lr_m == \'adagrad\':\n                optimizer = tf.train.AdagradOptimizer(lr)\n            elif _lr_m == \'sgd\':\n                optimizer = tf.train.GradientDescentOptimizer(lr)\n            elif _lr_m == \'rmsprop\':\n                optimizer = tf.train.RMSPropOptimizer(lr)\n            else:\n                raise NotImplementedError(""Unknown method {}"".format(_lr_m))\n\n            if clip > 0: # gradient clipping if clip is positive\n                grads, vs     = zip(*optimizer.compute_gradients(loss))\n                grads, gnorm  = tf.clip_by_global_norm(grads, clip)\n                self.train_op = optimizer.apply_gradients(zip(grads, vs))\n            else:\n                self.train_op = optimizer.minimize(loss)\n\n\n    def initialize_session(self):\n        """"""Defines self.sess and initialize the variables""""""\n        self.logger.info(""Initializing tf session"")\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n        self.saver = tf.train.Saver()\n\n\n    def restore_session(self, dir_model):\n        """"""Reload weights into session\n\n        Args:\n            sess: tf.Session()\n            dir_model: dir with weights\n\n        """"""\n        self.logger.info(""Reloading the latest trained model..."")\n        self.saver.restore(self.sess, dir_model)\n\n\n    def save_session(self):\n        """"""Saves session = weights""""""\n        if not os.path.exists(self.config.dir_model):\n            os.makedirs(self.config.dir_model)\n        self.saver.save(self.sess, self.config.dir_model)\n\n\n    def close_session(self):\n        """"""Closes the session""""""\n        self.sess.close()\n\n\n    def add_summary(self):\n        """"""Defines variables for Tensorboard\n\n        Args:\n            dir_output: (string) where the results are written\n\n        """"""\n        self.merged      = tf.summary.merge_all()\n        self.file_writer = tf.summary.FileWriter(self.config.dir_output,\n                self.sess.graph)\n\n\n    def train(self, train, dev):\n        """"""Performs training with early stopping and lr exponential decay\n\n        Args:\n            train: dataset that yields tuple of (sentences, tags)\n            dev: dataset\n\n        """"""\n        best_score = 0\n        nepoch_no_imprv = 0 # for early stopping\n        self.add_summary() # tensorboard\n\n        for epoch in range(self.config.nepochs):\n            self.logger.info(""Epoch {:} out of {:}"".format(epoch + 1,\n                        self.config.nepochs))\n\n            score = self.run_epoch(train, dev, epoch)\n            self.config.lr *= self.config.lr_decay # decay learning rate\n\n            # early stopping and saving best parameters\n            if score >= best_score:\n                nepoch_no_imprv = 0\n                self.save_session()\n                best_score = score\n                self.logger.info(""- new best score!"")\n            else:\n                nepoch_no_imprv += 1\n                if nepoch_no_imprv >= self.config.nepoch_no_imprv:\n                    self.logger.info(""- early stopping {} epochs without ""\\\n                            ""improvement"".format(nepoch_no_imprv))\n                    break\n\n\n    def evaluate(self, test):\n        """"""Evaluate model on test set\n\n        Args:\n            test: instance of class Dataset\n\n        """"""\n        self.logger.info(""Testing model over test set"")\n        metrics = self.run_evaluate(test)\n        msg = "" - "".join([""{} {:04.2f}"".format(k, v)\n                for k, v in metrics.items()])\n        self.logger.info(msg)\n'"
model/config.py,0,"b'import os\n\n\nfrom .general_utils import get_logger\nfrom .data_utils import get_trimmed_glove_vectors, load_vocab, \\\n        get_processing_word\n\n\nclass Config():\n    def __init__(self, load=True):\n        """"""Initialize hyperparameters and load vocabs\n\n        Args:\n            load_embeddings: (bool) if True, load embeddings into\n                np array, else None\n\n        """"""\n        # directory for training outputs\n        if not os.path.exists(self.dir_output):\n            os.makedirs(self.dir_output)\n\n        # create instance of logger\n        self.logger = get_logger(self.path_log)\n\n        # load if requested (default)\n        if load:\n            self.load()\n\n\n    def load(self):\n        """"""Loads vocabulary, processing functions and embeddings\n\n        Supposes that build_data.py has been run successfully and that\n        the corresponding files have been created (vocab and trimmed GloVe\n        vectors)\n\n        """"""\n        # 1. vocabulary\n        self.vocab_words = load_vocab(self.filename_words)\n        self.vocab_tags  = load_vocab(self.filename_tags)\n        self.vocab_chars = load_vocab(self.filename_chars)\n\n        self.nwords     = len(self.vocab_words)\n        self.nchars     = len(self.vocab_chars)\n        self.ntags      = len(self.vocab_tags)\n\n        # 2. get processing functions that map str -> id\n        self.processing_word = get_processing_word(self.vocab_words,\n                self.vocab_chars, lowercase=True, chars=self.use_chars)\n        self.processing_tag  = get_processing_word(self.vocab_tags,\n                lowercase=False, allow_unk=False)\n\n        # 3. get pre-trained embeddings\n        self.embeddings = (get_trimmed_glove_vectors(self.filename_trimmed)\n                if self.use_pretrained else None)\n\n\n    # general config\n    dir_output = ""results/test/""\n    dir_model  = dir_output + ""model.weights/""\n    path_log   = dir_output + ""log.txt""\n\n    # embeddings\n    dim_word = 300\n    dim_char = 100\n\n    # glove files\n    filename_glove = ""data/glove.6B/glove.6B.{}d.txt"".format(dim_word)\n    # trimmed embeddings (created from glove_filename with build_data.py)\n    filename_trimmed = ""data/glove.6B.{}d.trimmed.npz"".format(dim_word)\n    use_pretrained = True\n\n    # dataset\n    # filename_dev = ""data/coNLL/eng/eng.testa.iob""\n    # filename_test = ""data/coNLL/eng/eng.testb.iob""\n    # filename_train = ""data/coNLL/eng/eng.train.iob""\n\n    filename_dev = filename_test = filename_train = ""data/test.txt"" # test\n\n    max_iter = None # if not None, max number of examples in Dataset\n\n    # vocab (created from dataset with build_data.py)\n    filename_words = ""data/words.txt""\n    filename_tags = ""data/tags.txt""\n    filename_chars = ""data/chars.txt""\n\n    # training\n    train_embeddings = False\n    nepochs          = 15\n    dropout          = 0.5\n    batch_size       = 20\n    lr_method        = ""adam""\n    lr               = 0.001\n    lr_decay         = 0.9\n    clip             = -1 # if negative, no clipping\n    nepoch_no_imprv  = 3\n\n    # model hyperparameters\n    hidden_size_char = 100 # lstm on chars\n    hidden_size_lstm = 300 # lstm on word embeddings\n\n    # NOTE: if both chars and crf, only 1.6x slower on GPU\n    use_crf = True # if crf, training is 1.7x slower on CPU\n    use_chars = True # if char embedding, training is 3.5x slower on CPU\n'"
model/data_utils.py,0,"b'import numpy as np\nimport os\n\n\n# shared global variables to be imported from model also\nUNK = ""$UNK$""\nNUM = ""$NUM$""\nNONE = ""O""\n\n\n# special error message\nclass MyIOError(Exception):\n    def __init__(self, filename):\n        # custom error message\n        message = """"""\nERROR: Unable to locate file {}.\n\nFIX: Have you tried running python build_data.py first?\nThis will build vocab file from your train, test and dev sets and\ntrimm your word vectors.\n"""""".format(filename)\n        super(MyIOError, self).__init__(message)\n\n\nclass CoNLLDataset(object):\n    """"""Class that iterates over CoNLL Dataset\n\n    __iter__ method yields a tuple (words, tags)\n        words: list of raw words\n        tags: list of raw tags\n\n    If processing_word and processing_tag are not None,\n    optional preprocessing is appplied\n\n    Example:\n        ```python\n        data = CoNLLDataset(filename)\n        for sentence, tags in data:\n            pass\n        ```\n\n    """"""\n    def __init__(self, filename, processing_word=None, processing_tag=None,\n                 max_iter=None):\n        """"""\n        Args:\n            filename: path to the file\n            processing_words: (optional) function that takes a word as input\n            processing_tags: (optional) function that takes a tag as input\n            max_iter: (optional) max number of sentences to yield\n\n        """"""\n        self.filename = filename\n        self.processing_word = processing_word\n        self.processing_tag = processing_tag\n        self.max_iter = max_iter\n        self.length = None\n\n\n    def __iter__(self):\n        niter = 0\n        with open(self.filename) as f:\n            words, tags = [], []\n            for line in f:\n                line = line.strip()\n                if (len(line) == 0 or line.startswith(""-DOCSTART-"")):\n                    if len(words) != 0:\n                        niter += 1\n                        if self.max_iter is not None and niter > self.max_iter:\n                            break\n                        yield words, tags\n                        words, tags = [], []\n                else:\n                    ls = line.split(\' \')\n                    word, tag = ls[0],ls[1]\n                    if self.processing_word is not None:\n                        word = self.processing_word(word)\n                    if self.processing_tag is not None:\n                        tag = self.processing_tag(tag)\n                    words += [word]\n                    tags += [tag]\n\n\n    def __len__(self):\n        """"""Iterates once over the corpus to set and store length""""""\n        if self.length is None:\n            self.length = 0\n            for _ in self:\n                self.length += 1\n\n        return self.length\n\n\ndef get_vocabs(datasets):\n    """"""Build vocabulary from an iterable of datasets objects\n\n    Args:\n        datasets: a list of dataset objects\n\n    Returns:\n        a set of all the words in the dataset\n\n    """"""\n    print(""Building vocab..."")\n    vocab_words = set()\n    vocab_tags = set()\n    for dataset in datasets:\n        for words, tags in dataset:\n            vocab_words.update(words)\n            vocab_tags.update(tags)\n    print(""- done. {} tokens"".format(len(vocab_words)))\n    return vocab_words, vocab_tags\n\n\ndef get_char_vocab(dataset):\n    """"""Build char vocabulary from an iterable of datasets objects\n\n    Args:\n        dataset: a iterator yielding tuples (sentence, tags)\n\n    Returns:\n        a set of all the characters in the dataset\n\n    """"""\n    vocab_char = set()\n    for words, _ in dataset:\n        for word in words:\n            vocab_char.update(word)\n\n    return vocab_char\n\n\ndef get_glove_vocab(filename):\n    """"""Load vocab from file\n\n    Args:\n        filename: path to the glove vectors\n\n    Returns:\n        vocab: set() of strings\n    """"""\n    print(""Building vocab..."")\n    vocab = set()\n    with open(filename) as f:\n        for line in f:\n            word = line.strip().split(\' \')[0]\n            vocab.add(word)\n    print(""- done. {} tokens"".format(len(vocab)))\n    return vocab\n\n\ndef write_vocab(vocab, filename):\n    """"""Writes a vocab to a file\n\n    Writes one word per line.\n\n    Args:\n        vocab: iterable that yields word\n        filename: path to vocab file\n\n    Returns:\n        write a word per line\n\n    """"""\n    print(""Writing vocab..."")\n    with open(filename, ""w"") as f:\n        for i, word in enumerate(vocab):\n            if i != len(vocab) - 1:\n                f.write(""{}\\n"".format(word))\n            else:\n                f.write(word)\n    print(""- done. {} tokens"".format(len(vocab)))\n\n\ndef load_vocab(filename):\n    """"""Loads vocab from a file\n\n    Args:\n        filename: (string) the format of the file must be one word per line.\n\n    Returns:\n        d: dict[word] = index\n\n    """"""\n    try:\n        d = dict()\n        with open(filename) as f:\n            for idx, word in enumerate(f):\n                word = word.strip()\n                d[word] = idx\n\n    except IOError:\n        raise MyIOError(filename)\n    return d\n\n\ndef export_trimmed_glove_vectors(vocab, glove_filename, trimmed_filename, dim):\n    """"""Saves glove vectors in numpy array\n\n    Args:\n        vocab: dictionary vocab[word] = index\n        glove_filename: a path to a glove file\n        trimmed_filename: a path where to store a matrix in npy\n        dim: (int) dimension of embeddings\n\n    """"""\n    embeddings = np.zeros([len(vocab), dim])\n    with open(glove_filename) as f:\n        for line in f:\n            line = line.strip().split(\' \')\n            word = line[0]\n            embedding = [float(x) for x in line[1:]]\n            if word in vocab:\n                word_idx = vocab[word]\n                embeddings[word_idx] = np.asarray(embedding)\n\n    np.savez_compressed(trimmed_filename, embeddings=embeddings)\n\n\ndef get_trimmed_glove_vectors(filename):\n    """"""\n    Args:\n        filename: path to the npz file\n\n    Returns:\n        matrix of embeddings (np array)\n\n    """"""\n    try:\n        with np.load(filename) as data:\n            return data[""embeddings""]\n\n    except IOError:\n        raise MyIOError(filename)\n\n\ndef get_processing_word(vocab_words=None, vocab_chars=None,\n                    lowercase=False, chars=False, allow_unk=True):\n    """"""Return lambda function that transform a word (string) into list,\n    or tuple of (list, id) of int corresponding to the ids of the word and\n    its corresponding characters.\n\n    Args:\n        vocab: dict[word] = idx\n\n    Returns:\n        f(""cat"") = ([12, 4, 32], 12345)\n                 = (list of char ids, word id)\n\n    """"""\n    def f(word):\n        # 0. get chars of words\n        if vocab_chars is not None and chars == True:\n            char_ids = []\n            for char in word:\n                # ignore chars out of vocabulary\n                if char in vocab_chars:\n                    char_ids += [vocab_chars[char]]\n\n        # 1. preprocess word\n        if lowercase:\n            word = word.lower()\n        if word.isdigit():\n            word = NUM\n\n        # 2. get id of word\n        if vocab_words is not None:\n            if word in vocab_words:\n                word = vocab_words[word]\n            else:\n                if allow_unk:\n                    word = vocab_words[UNK]\n                else:\n                    raise Exception(""Unknow key is not allowed. Check that ""\\\n                                    ""your vocab (tags?) is correct"")\n\n        # 3. return tuple char ids, word id\n        if vocab_chars is not None and chars == True:\n            return char_ids, word\n        else:\n            return word\n\n    return f\n\n\ndef _pad_sequences(sequences, pad_tok, max_length):\n    """"""\n    Args:\n        sequences: a generator of list or tuple\n        pad_tok: the char to pad with\n\n    Returns:\n        a list of list where each sublist has same length\n    """"""\n    sequence_padded, sequence_length = [], []\n\n    for seq in sequences:\n        seq = list(seq)\n        seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq), 0)\n        sequence_padded +=  [seq_]\n        sequence_length += [min(len(seq), max_length)]\n\n    return sequence_padded, sequence_length\n\n\ndef pad_sequences(sequences, pad_tok, nlevels=1):\n    """"""\n    Args:\n        sequences: a generator of list or tuple\n        pad_tok: the char to pad with\n        nlevels: ""depth"" of padding, for the case where we have characters ids\n\n    Returns:\n        a list of list where each sublist has same length\n\n    """"""\n    if nlevels == 1:\n        max_length = max(map(lambda x : len(x), sequences))\n        sequence_padded, sequence_length = _pad_sequences(sequences,\n                                            pad_tok, max_length)\n\n    elif nlevels == 2:\n        max_length_word = max([max(map(lambda x: len(x), seq))\n                               for seq in sequences])\n        sequence_padded, sequence_length = [], []\n        for seq in sequences:\n            # all words are same length now\n            sp, sl = _pad_sequences(seq, pad_tok, max_length_word)\n            sequence_padded += [sp]\n            sequence_length += [sl]\n\n        max_length_sentence = max(map(lambda x : len(x), sequences))\n        sequence_padded, _ = _pad_sequences(sequence_padded,\n                [pad_tok]*max_length_word, max_length_sentence)\n        sequence_length, _ = _pad_sequences(sequence_length, 0,\n                max_length_sentence)\n\n    return sequence_padded, sequence_length\n\n\ndef minibatches(data, minibatch_size):\n    """"""\n    Args:\n        data: generator of (sentence, tags) tuples\n        minibatch_size: (int)\n\n    Yields:\n        list of tuples\n\n    """"""\n    x_batch, y_batch = [], []\n    for (x, y) in data:\n        if len(x_batch) == minibatch_size:\n            yield x_batch, y_batch\n            x_batch, y_batch = [], []\n\n        if type(x[0]) == tuple:\n            x = zip(*x)\n        x_batch += [x]\n        y_batch += [y]\n\n    if len(x_batch) != 0:\n        yield x_batch, y_batch\n\n\ndef get_chunk_type(tok, idx_to_tag):\n    """"""\n    Args:\n        tok: id of token, ex 4\n        idx_to_tag: dictionary {4: ""B-PER"", ...}\n\n    Returns:\n        tuple: ""B"", ""PER""\n\n    """"""\n    tag_name = idx_to_tag[tok]\n    tag_class = tag_name.split(\'-\')[0]\n    tag_type = tag_name.split(\'-\')[-1]\n    return tag_class, tag_type\n\n\ndef get_chunks(seq, tags):\n    """"""Given a sequence of tags, group entities and their position\n\n    Args:\n        seq: [4, 4, 0, 0, ...] sequence of labels\n        tags: dict[""O""] = 4\n\n    Returns:\n        list of (chunk_type, chunk_start, chunk_end)\n\n    Example:\n        seq = [4, 5, 0, 3]\n        tags = {""B-PER"": 4, ""I-PER"": 5, ""B-LOC"": 3}\n        result = [(""PER"", 0, 2), (""LOC"", 3, 4)]\n\n    """"""\n    default = tags[NONE]\n    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n    chunks = []\n    chunk_type, chunk_start = None, None\n    for i, tok in enumerate(seq):\n        # End of a chunk 1\n        if tok == default and chunk_type is not None:\n            # Add a chunk.\n            chunk = (chunk_type, chunk_start, i)\n            chunks.append(chunk)\n            chunk_type, chunk_start = None, None\n\n        # End of a chunk + start of a chunk!\n        elif tok != default:\n            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n            if chunk_type is None:\n                chunk_type, chunk_start = tok_chunk_type, i\n            elif tok_chunk_type != chunk_type or tok_chunk_class == ""B"":\n                chunk = (chunk_type, chunk_start, i)\n                chunks.append(chunk)\n                chunk_type, chunk_start = tok_chunk_type, i\n        else:\n            pass\n\n    # end condition\n    if chunk_type is not None:\n        chunk = (chunk_type, chunk_start, len(seq))\n        chunks.append(chunk)\n\n    return chunks\n'"
model/general_utils.py,0,"b'import time\nimport sys\nimport logging\nimport numpy as np\n\n\ndef get_logger(filename):\n    """"""Return a logger instance that writes in filename\n\n    Args:\n        filename: (string) path to log.txt\n\n    Returns:\n        logger: (instance of logger)\n\n    """"""\n    logger = logging.getLogger(\'logger\')\n    logger.setLevel(logging.DEBUG)\n    logging.basicConfig(format=\'%(message)s\', level=logging.DEBUG)\n    handler = logging.FileHandler(filename)\n    handler.setLevel(logging.DEBUG)\n    handler.setFormatter(logging.Formatter(\n            \'%(asctime)s:%(levelname)s: %(message)s\'))\n    logging.getLogger().addHandler(handler)\n\n    return logger\n\n\nclass Progbar(object):\n    """"""Progbar class copied from keras (https://github.com/fchollet/keras/)\n\n    Displays a progress bar.\n    Small edit : added strict arg to update\n    # Arguments\n        target: Total number of steps expected.\n        interval: Minimum visual progress update interval (in seconds).\n    """"""\n\n    def __init__(self, target, width=30, verbose=1):\n        self.width = width\n        self.target = target\n        self.sum_values = {}\n        self.unique_values = []\n        self.start = time.time()\n        self.total_width = 0\n        self.seen_so_far = 0\n        self.verbose = verbose\n\n    def update(self, current, values=[], exact=[], strict=[]):\n        """"""\n        Updates the progress bar.\n        # Arguments\n            current: Index of current step.\n            values: List of tuples (name, value_for_last_step).\n                The progress bar will display averages for these values.\n            exact: List of tuples (name, value_for_last_step).\n                The progress bar will display these values directly.\n        """"""\n\n        for k, v in values:\n            if k not in self.sum_values:\n                self.sum_values[k] = [v * (current - self.seen_so_far),\n                                      current - self.seen_so_far]\n                self.unique_values.append(k)\n            else:\n                self.sum_values[k][0] += v * (current - self.seen_so_far)\n                self.sum_values[k][1] += (current - self.seen_so_far)\n        for k, v in exact:\n            if k not in self.sum_values:\n                self.unique_values.append(k)\n            self.sum_values[k] = [v, 1]\n\n        for k, v in strict:\n            if k not in self.sum_values:\n                self.unique_values.append(k)\n            self.sum_values[k] = v\n\n        self.seen_so_far = current\n\n        now = time.time()\n        if self.verbose == 1:\n            prev_total_width = self.total_width\n            sys.stdout.write(""\\b"" * prev_total_width)\n            sys.stdout.write(""\\r"")\n\n            numdigits = int(np.floor(np.log10(self.target))) + 1\n            barstr = \'%%%dd/%%%dd [\' % (numdigits, numdigits)\n            bar = barstr % (current, self.target)\n            prog = float(current)/self.target\n            prog_width = int(self.width*prog)\n            if prog_width > 0:\n                bar += (\'=\'*(prog_width-1))\n                if current < self.target:\n                    bar += \'>\'\n                else:\n                    bar += \'=\'\n            bar += (\'.\'*(self.width-prog_width))\n            bar += \']\'\n            sys.stdout.write(bar)\n            self.total_width = len(bar)\n\n            if current:\n                time_per_unit = (now - self.start) / current\n            else:\n                time_per_unit = 0\n            eta = time_per_unit*(self.target - current)\n            info = \'\'\n            if current < self.target:\n                info += \' - ETA: %ds\' % eta\n            else:\n                info += \' - %ds\' % (now - self.start)\n            for k in self.unique_values:\n                if type(self.sum_values[k]) is list:\n                    info += \' - %s: %.4f\' % (k,\n                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n                else:\n                    info += \' - %s: %s\' % (k, self.sum_values[k])\n\n            self.total_width += len(info)\n            if prev_total_width > self.total_width:\n                info += ((prev_total_width-self.total_width) * "" "")\n\n            sys.stdout.write(info)\n            sys.stdout.flush()\n\n            if current >= self.target:\n                sys.stdout.write(""\\n"")\n\n        if self.verbose == 2:\n            if current >= self.target:\n                info = \'%ds\' % (now - self.start)\n                for k in self.unique_values:\n                    info += \' - %s: %.4f\' % (k,\n                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n                sys.stdout.write(info + ""\\n"")\n\n    def add(self, n, values=[]):\n        self.update(self.seen_so_far+n, values)\n\n\n'"
model/ner_model.py,53,"b'import numpy as np\nimport os\nimport tensorflow as tf\n\n\nfrom .data_utils import minibatches, pad_sequences, get_chunks\nfrom .general_utils import Progbar\nfrom .base_model import BaseModel\n\n\nclass NERModel(BaseModel):\n    """"""Specialized class of Model for NER""""""\n\n    def __init__(self, config):\n        super(NERModel, self).__init__(config)\n        self.idx_to_tag = {idx: tag for tag, idx in\n                           self.config.vocab_tags.items()}\n\n\n    def add_placeholders(self):\n        """"""Define placeholders = entries to computational graph""""""\n        # shape = (batch size, max length of sentence in batch)\n        self.word_ids = tf.placeholder(tf.int32, shape=[None, None],\n                        name=""word_ids"")\n\n        # shape = (batch size)\n        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None],\n                        name=""sequence_lengths"")\n\n        # shape = (batch size, max length of sentence, max length of word)\n        self.char_ids = tf.placeholder(tf.int32, shape=[None, None, None],\n                        name=""char_ids"")\n\n        # shape = (batch_size, max_length of sentence)\n        self.word_lengths = tf.placeholder(tf.int32, shape=[None, None],\n                        name=""word_lengths"")\n\n        # shape = (batch size, max length of sentence in batch)\n        self.labels = tf.placeholder(tf.int32, shape=[None, None],\n                        name=""labels"")\n\n        # hyper parameters\n        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],\n                        name=""dropout"")\n        self.lr = tf.placeholder(dtype=tf.float32, shape=[],\n                        name=""lr"")\n\n\n    def get_feed_dict(self, words, labels=None, lr=None, dropout=None):\n        """"""Given some data, pad it and build a feed dictionary\n\n        Args:\n            words: list of sentences. A sentence is a list of ids of a list of\n                words. A word is a list of ids\n            labels: list of ids\n            lr: (float) learning rate\n            dropout: (float) keep prob\n\n        Returns:\n            dict {placeholder: value}\n\n        """"""\n        # perform padding of the given data\n        if self.config.use_chars:\n            char_ids, word_ids = zip(*words)\n            word_ids, sequence_lengths = pad_sequences(word_ids, 0)\n            char_ids, word_lengths = pad_sequences(char_ids, pad_tok=0,\n                nlevels=2)\n        else:\n            word_ids, sequence_lengths = pad_sequences(words, 0)\n\n        # build feed dictionary\n        feed = {\n            self.word_ids: word_ids,\n            self.sequence_lengths: sequence_lengths\n        }\n\n        if self.config.use_chars:\n            feed[self.char_ids] = char_ids\n            feed[self.word_lengths] = word_lengths\n\n        if labels is not None:\n            labels, _ = pad_sequences(labels, 0)\n            feed[self.labels] = labels\n\n        if lr is not None:\n            feed[self.lr] = lr\n\n        if dropout is not None:\n            feed[self.dropout] = dropout\n\n        return feed, sequence_lengths\n\n\n    def add_word_embeddings_op(self):\n        """"""Defines self.word_embeddings\n\n        If self.config.embeddings is not None and is a np array initialized\n        with pre-trained word vectors, the word embeddings is just a look-up\n        and we don\'t train the vectors. Otherwise, a random matrix with\n        the correct shape is initialized.\n        """"""\n        with tf.variable_scope(""words""):\n            if self.config.embeddings is None:\n                self.logger.info(""WARNING: randomly initializing word vectors"")\n                _word_embeddings = tf.get_variable(\n                        name=""_word_embeddings"",\n                        dtype=tf.float32,\n                        shape=[self.config.nwords, self.config.dim_word])\n            else:\n                _word_embeddings = tf.Variable(\n                        self.config.embeddings,\n                        name=""_word_embeddings"",\n                        dtype=tf.float32,\n                        trainable=self.config.train_embeddings)\n\n            word_embeddings = tf.nn.embedding_lookup(_word_embeddings,\n                    self.word_ids, name=""word_embeddings"")\n\n        with tf.variable_scope(""chars""):\n            if self.config.use_chars:\n                # get char embeddings matrix\n                _char_embeddings = tf.get_variable(\n                        name=""_char_embeddings"",\n                        dtype=tf.float32,\n                        shape=[self.config.nchars, self.config.dim_char])\n                char_embeddings = tf.nn.embedding_lookup(_char_embeddings,\n                        self.char_ids, name=""char_embeddings"")\n\n                # put the time dimension on axis=1\n                s = tf.shape(char_embeddings)\n                char_embeddings = tf.reshape(char_embeddings,\n                        shape=[s[0]*s[1], s[-2], self.config.dim_char])\n                word_lengths = tf.reshape(self.word_lengths, shape=[s[0]*s[1]])\n\n                # bi lstm on chars\n                cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char,\n                        state_is_tuple=True)\n                cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char,\n                        state_is_tuple=True)\n                _output = tf.nn.bidirectional_dynamic_rnn(\n                        cell_fw, cell_bw, char_embeddings,\n                        sequence_length=word_lengths, dtype=tf.float32)\n\n                # read and concat output\n                _, ((_, output_fw), (_, output_bw)) = _output\n                output = tf.concat([output_fw, output_bw], axis=-1)\n\n                # shape = (batch size, max sentence length, char hidden size)\n                output = tf.reshape(output,\n                        shape=[s[0], s[1], 2*self.config.hidden_size_char])\n                word_embeddings = tf.concat([word_embeddings, output], axis=-1)\n\n        self.word_embeddings =  tf.nn.dropout(word_embeddings, self.dropout)\n\n\n    def add_logits_op(self):\n        """"""Defines self.logits\n\n        For each word in each sentence of the batch, it corresponds to a vector\n        of scores, of dimension equal to the number of tags.\n        """"""\n        with tf.variable_scope(""bi-lstm""):\n            cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n            cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw, cell_bw, self.word_embeddings,\n                    sequence_length=self.sequence_lengths, dtype=tf.float32)\n            output = tf.concat([output_fw, output_bw], axis=-1)\n            output = tf.nn.dropout(output, self.dropout)\n\n        with tf.variable_scope(""proj""):\n            W = tf.get_variable(""W"", dtype=tf.float32,\n                    shape=[2*self.config.hidden_size_lstm, self.config.ntags])\n\n            b = tf.get_variable(""b"", shape=[self.config.ntags],\n                    dtype=tf.float32, initializer=tf.zeros_initializer())\n\n            nsteps = tf.shape(output)[1]\n            output = tf.reshape(output, [-1, 2*self.config.hidden_size_lstm])\n            pred = tf.matmul(output, W) + b\n            self.logits = tf.reshape(pred, [-1, nsteps, self.config.ntags])\n\n\n    def add_pred_op(self):\n        """"""Defines self.labels_pred\n\n        This op is defined only in the case where we don\'t use a CRF since in\n        that case we can make the prediction ""in the graph"" (thanks to tf\n        functions in other words). With theCRF, as the inference is coded\n        in python and not in pure tensroflow, we have to make the prediciton\n        outside the graph.\n        """"""\n        if not self.config.use_crf:\n            self.labels_pred = tf.cast(tf.argmax(self.logits, axis=-1),\n                    tf.int32)\n\n\n    def add_loss_op(self):\n        """"""Defines the loss""""""\n        if self.config.use_crf:\n            log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(\n                    self.logits, self.labels, self.sequence_lengths)\n            self.trans_params = trans_params # need to evaluate it for decoding\n            self.loss = tf.reduce_mean(-log_likelihood)\n        else:\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                    logits=self.logits, labels=self.labels)\n            mask = tf.sequence_mask(self.sequence_lengths)\n            losses = tf.boolean_mask(losses, mask)\n            self.loss = tf.reduce_mean(losses)\n\n        # for tensorboard\n        tf.summary.scalar(""loss"", self.loss)\n\n\n    def build(self):\n        # NER specific functions\n        self.add_placeholders()\n        self.add_word_embeddings_op()\n        self.add_logits_op()\n        self.add_pred_op()\n        self.add_loss_op()\n\n        # Generic functions that add training op and initialize session\n        self.add_train_op(self.config.lr_method, self.lr, self.loss,\n                self.config.clip)\n        self.initialize_session() # now self.sess is defined and vars are init\n\n\n    def predict_batch(self, words):\n        """"""\n        Args:\n            words: list of sentences\n\n        Returns:\n            labels_pred: list of labels for each sentence\n            sequence_length\n\n        """"""\n        fd, sequence_lengths = self.get_feed_dict(words, dropout=1.0)\n\n        if self.config.use_crf:\n            # get tag scores and transition params of CRF\n            viterbi_sequences = []\n            logits, trans_params = self.sess.run(\n                    [self.logits, self.trans_params], feed_dict=fd)\n\n            # iterate over the sentences because no batching in vitervi_decode\n            for logit, sequence_length in zip(logits, sequence_lengths):\n                logit = logit[:sequence_length] # keep only the valid steps\n                viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n                        logit, trans_params)\n                viterbi_sequences += [viterbi_seq]\n\n            return viterbi_sequences, sequence_lengths\n\n        else:\n            labels_pred = self.sess.run(self.labels_pred, feed_dict=fd)\n\n            return labels_pred, sequence_lengths\n\n\n    def run_epoch(self, train, dev, epoch):\n        """"""Performs one complete pass over the train set and evaluate on dev\n\n        Args:\n            train: dataset that yields tuple of sentences, tags\n            dev: dataset\n            epoch: (int) index of the current epoch\n\n        Returns:\n            f1: (python float), score to select model on, higher is better\n\n        """"""\n        # progbar stuff for logging\n        batch_size = self.config.batch_size\n        nbatches = (len(train) + batch_size - 1) // batch_size\n        prog = Progbar(target=nbatches)\n\n        # iterate over dataset\n        for i, (words, labels) in enumerate(minibatches(train, batch_size)):\n            fd, _ = self.get_feed_dict(words, labels, self.config.lr,\n                    self.config.dropout)\n\n            _, train_loss, summary = self.sess.run(\n                    [self.train_op, self.loss, self.merged], feed_dict=fd)\n\n            prog.update(i + 1, [(""train loss"", train_loss)])\n\n            # tensorboard\n            if i % 10 == 0:\n                self.file_writer.add_summary(summary, epoch*nbatches + i)\n\n        metrics = self.run_evaluate(dev)\n        msg = "" - "".join([""{} {:04.2f}"".format(k, v)\n                for k, v in metrics.items()])\n        self.logger.info(msg)\n\n        return metrics[""f1""]\n\n\n    def run_evaluate(self, test):\n        """"""Evaluates performance on test set\n\n        Args:\n            test: dataset that yields tuple of (sentences, tags)\n\n        Returns:\n            metrics: (dict) metrics[""acc""] = 98.4, ...\n\n        """"""\n        accs = []\n        correct_preds, total_correct, total_preds = 0., 0., 0.\n        for words, labels in minibatches(test, self.config.batch_size):\n            labels_pred, sequence_lengths = self.predict_batch(words)\n\n            for lab, lab_pred, length in zip(labels, labels_pred,\n                                             sequence_lengths):\n                lab      = lab[:length]\n                lab_pred = lab_pred[:length]\n                accs    += [a==b for (a, b) in zip(lab, lab_pred)]\n\n                lab_chunks      = set(get_chunks(lab, self.config.vocab_tags))\n                lab_pred_chunks = set(get_chunks(lab_pred,\n                                                 self.config.vocab_tags))\n\n                correct_preds += len(lab_chunks & lab_pred_chunks)\n                total_preds   += len(lab_pred_chunks)\n                total_correct += len(lab_chunks)\n\n        p   = correct_preds / total_preds if correct_preds > 0 else 0\n        r   = correct_preds / total_correct if correct_preds > 0 else 0\n        f1  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n        acc = np.mean(accs)\n\n        return {""acc"": 100*acc, ""f1"": 100*f1}\n\n\n    def predict(self, words_raw):\n        """"""Returns list of tags\n\n        Args:\n            words_raw: list of words (string), just one sentence (no batch)\n\n        Returns:\n            preds: list of tags (string), one for each word in the sentence\n\n        """"""\n        words = [self.config.processing_word(w) for w in words_raw]\n        if type(words[0]) == tuple:\n            words = zip(*words)\n        pred_ids, _ = self.predict_batch([words])\n        preds = [self.idx_to_tag[idx] for idx in list(pred_ids[0])]\n\n        return preds\n'"
