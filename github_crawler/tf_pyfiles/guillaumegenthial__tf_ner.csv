file_path,api_count,code
data/example/build_glove.py,0,"b'""""""Build an np.array from some glove file and some vocab file\n\nYou need to download `glove.840B.300d.txt` from\nhttps://nlp.stanford.edu/projects/glove/ and you need to have built\nyour vocabulary first (Maybe using `build_vocab.py`)\n""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\n\nimport numpy as np\n\n\nif __name__ == \'__main__\':\n    # Load vocab\n    with Path(\'vocab.words.txt\').open() as f:\n        word_to_idx = {line.strip(): idx for idx, line in enumerate(f)}\n    size_vocab = len(word_to_idx)\n\n    # Array of zeros\n    embeddings = np.zeros((size_vocab, 300))\n\n    # Get relevant glove vectors\n    found = 0\n    print(\'Reading GloVe file (may take a while)\')\n    with Path(\'glove.840B.300d.txt\').open() as f:\n        for line_idx, line in enumerate(f):\n            if line_idx % 100000 == 0:\n                print(\'- At line {}\'.format(line_idx))\n            line = line.strip().split()\n            if len(line) != 300 + 1:\n                continue\n            word = line[0]\n            embedding = line[1:]\n            if word in word_to_idx:\n                found += 1\n                word_idx = word_to_idx[word]\n                embeddings[word_idx] = embedding\n    print(\'- done. Found {} vectors for {} words\'.format(found, size_vocab))\n\n    # Save np.array to file\n    np.savez_compressed(\'glove.npz\', embeddings=embeddings)\n'"
data/example/build_vocab.py,0,"b'""""""Script to build words, chars and tags vocab""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom collections import Counter\nfrom pathlib import Path\n\n# TODO: modify this depending on your needs (1 will work just fine)\n# You might also want to be more clever about your vocab and intersect\n# the GloVe vocab with your dataset vocab, etc. You figure it out ;)\nMINCOUNT = 1\n\nif __name__ == \'__main__\':\n    # 1. Words\n    # Get Counter of words on all the data, filter by min count, save\n    def words(name):\n        return \'{}.words.txt\'.format(name)\n\n    print(\'Build vocab words (may take a while)\')\n    counter_words = Counter()\n    for n in [\'train\', \'testa\', \'testb\']:\n        with Path(words(n)).open() as f:\n            for line in f:\n                counter_words.update(line.strip().split())\n\n    vocab_words = {w for w, c in counter_words.items() if c >= MINCOUNT}\n\n    with Path(\'vocab.words.txt\').open(\'w\') as f:\n        for w in sorted(list(vocab_words)):\n            f.write(\'{}\\n\'.format(w))\n    print(\'- done. Kept {} out of {}\'.format(\n        len(vocab_words), len(counter_words)))\n\n    # 2. Chars\n    # Get all the characters from the vocab words\n    print(\'Build vocab chars\')\n    vocab_chars = set()\n    for w in vocab_words:\n        vocab_chars.update(w)\n\n    with Path(\'vocab.chars.txt\').open(\'w\') as f:\n        for c in sorted(list(vocab_chars)):\n            f.write(\'{}\\n\'.format(c))\n    print(\'- done. Found {} chars\'.format(len(vocab_chars)))\n\n    # 3. Tags\n    # Get all tags from the training set\n\n    def tags(name):\n        return \'{}.tags.txt\'.format(name)\n\n    print(\'Build vocab tags (may take a while)\')\n    vocab_tags = set()\n    with Path(tags(\'train\')).open() as f:\n        for line in f:\n            vocab_tags.update(line.strip().split())\n\n    with Path(\'vocab.tags.txt\').open(\'w\') as f:\n        for t in sorted(list(vocab_tags)):\n            f.write(\'{}\\n\'.format(t))\n    print(\'- done. Found {} tags.\'.format(len(vocab_tags)))\n'"
models/chars_conv_lstm_crf/export.py,7,"b'""""""Export model as a saved_model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nimport json\n\nimport tensorflow as tf\n\nfrom main import model_fn\n\nDATADIR = \'../../data/example\'\nPARAMS = \'./results/params.json\'\nMODELDIR = \'./results/model\'\n\n\ndef serving_input_receiver_fn():\n    """"""Serving input_fn that builds features from placeholders\n\n    Returns\n    -------\n    tf.estimator.export.ServingInputReceiver\n    """"""\n    words = tf.placeholder(dtype=tf.string, shape=[None, None], name=\'words\')\n    nwords = tf.placeholder(dtype=tf.int32, shape=[None], name=\'nwords\')\n    chars = tf.placeholder(dtype=tf.string, shape=[None, None, None],\n                           name=\'chars\')\n    nchars = tf.placeholder(dtype=tf.int32, shape=[None, None],\n                            name=\'nchars\')\n    receiver_tensors = {\'words\': words, \'nwords\': nwords,\n                        \'chars\': chars, \'nchars\': nchars}\n    features = {\'words\': words, \'nwords\': nwords,\n                \'chars\': chars, \'nchars\': nchars}\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n\n\nif __name__ == \'__main__\':\n    with Path(PARAMS).open() as f:\n        params = json.load(f)\n\n    params[\'words\'] = str(Path(DATADIR, \'vocab.words.txt\'))\n    params[\'chars\'] = str(Path(DATADIR, \'vocab.chars.txt\'))\n    params[\'tags\'] = str(Path(DATADIR, \'vocab.tags.txt\'))\n    params[\'glove\'] = str(Path(DATADIR, \'glove.npz\'))\n\n    estimator = tf.estimator.Estimator(model_fn, MODELDIR, params=params)\n    estimator.export_saved_model(\'saved_model\', serving_input_receiver_fn)\n'"
models/chars_conv_lstm_crf/interact.py,5,"b'""""""Interact with a model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nimport functools\nimport json\n\nimport tensorflow as tf\n\nfrom main import model_fn\n\nLINE = \'John lives in New York\'\nDATADIR = \'../../data/example\'\nPARAMS = \'./results/params.json\'\nMODELDIR = \'./results/model\'\n\n\ndef pretty_print(line, preds):\n    words = line.strip().split()\n    lengths = [max(len(w), len(p)) for w, p in zip(words, preds)]\n    padded_words = [w + (l - len(w)) * \' \' for w, l in zip(words, lengths)]\n    padded_preds = [p.decode() + (l - len(p)) * \' \' for p, l in zip(preds, lengths)]\n    print(\'words: {}\'.format(\' \'.join(padded_words)))\n    print(\'preds: {}\'.format(\' \'.join(padded_preds)))\n\n\ndef predict_input_fn(line):\n    # Words\n    words = [w.encode() for w in line.strip().split()]\n    nwords = len(words)\n\n    # Chars\n    chars = [[c.encode() for c in w] for w in line.strip().split()]\n    lengths = [len(c) for c in chars]\n    max_len = max(lengths)\n    chars = [c + [b\'<pad>\'] * (max_len - l) for c, l in zip(chars, lengths)]\n\n    # Wrapping in Tensors\n    words = tf.constant([words], dtype=tf.string)\n    nwords = tf.constant([nwords], dtype=tf.int32)\n    chars = tf.constant([chars], dtype=tf.string)\n    nchars = tf.constant([lengths], dtype=tf.int32)\n\n    return ((words, nwords), (chars, nchars)), None\n\n\nif __name__ == \'__main__\':\n    with Path(PARAMS).open() as f:\n        params = json.load(f)\n\n    params[\'words\'] = str(Path(DATADIR, \'vocab.words.txt\'))\n    params[\'chars\'] = str(Path(DATADIR, \'vocab.chars.txt\'))\n    params[\'tags\'] = str(Path(DATADIR, \'vocab.tags.txt\'))\n    params[\'glove\'] = str(Path(DATADIR, \'glove.npz\'))\n\n    estimator = tf.estimator.Estimator(model_fn, MODELDIR, params=params)\n    predict_inpf = functools.partial(predict_input_fn, LINE)\n    for pred in estimator.predict(predict_inpf):\n        pretty_print(LINE, pred[\'tags\'])\n        break\n'"
models/chars_conv_lstm_crf/main.py,51,"b'""""""GloVe Embeddings + chars conv and max pooling + bi-LSTM + CRF""""""\n\n__author__ = ""Guillaume Genthial""\n\nimport functools\nimport json\nimport logging\nfrom pathlib import Path\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nfrom tf_metrics import precision, recall, f1\n\nfrom masked_conv import masked_conv1d_and_max\n\nDATADIR = \'../../data/example\'\n\n# Logging\nPath(\'results\').mkdir(exist_ok=True)\ntf.logging.set_verbosity(logging.INFO)\nhandlers = [\n    logging.FileHandler(\'results/main.log\'),\n    logging.StreamHandler(sys.stdout)\n]\nlogging.getLogger(\'tensorflow\').handlers = handlers\n\n\ndef parse_fn(line_words, line_tags):\n    # Encode in Bytes for TF\n    words = [w.encode() for w in line_words.strip().split()]\n    tags = [t.encode() for t in line_tags.strip().split()]\n    assert len(words) == len(tags), ""Words and tags lengths don\'t match""\n\n    # Chars\n    chars = [[c.encode() for c in w] for w in line_words.strip().split()]\n    lengths = [len(c) for c in chars]\n    max_len = max(lengths)\n    chars = [c + [b\'<pad>\'] * (max_len - l) for c, l in zip(chars, lengths)]\n    return ((words, len(words)), (chars, lengths)), tags\n\n\ndef generator_fn(words, tags):\n    with Path(words).open(\'r\') as f_words, Path(tags).open(\'r\') as f_tags:\n        for line_words, line_tags in zip(f_words, f_tags):\n            yield parse_fn(line_words, line_tags)\n\n\ndef input_fn(words, tags, params=None, shuffle_and_repeat=False):\n    params = params if params is not None else {}\n    shapes = ((([None], ()),               # (words, nwords)\n               ([None, None], [None])),    # (chars, nchars)\n              [None])                      # tags\n    types = (((tf.string, tf.int32),\n              (tf.string, tf.int32)),\n             tf.string)\n    defaults = (((\'<pad>\', 0),\n                 (\'<pad>\', 0)),\n                \'O\')\n    dataset = tf.data.Dataset.from_generator(\n        functools.partial(generator_fn, words, tags),\n        output_shapes=shapes, output_types=types)\n\n    if shuffle_and_repeat:\n        dataset = dataset.shuffle(params[\'buffer\']).repeat(params[\'epochs\'])\n\n    dataset = (dataset\n               .padded_batch(params.get(\'batch_size\', 20), shapes, defaults)\n               .prefetch(1))\n    return dataset\n\n\ndef model_fn(features, labels, mode, params):\n    # For serving features are a bit different\n    if isinstance(features, dict):\n        features = ((features[\'words\'], features[\'nwords\']),\n                    (features[\'chars\'], features[\'nchars\']))\n\n    # Read vocabs and inputs\n    dropout = params[\'dropout\']\n    (words, nwords), (chars, nchars) = features\n    training = (mode == tf.estimator.ModeKeys.TRAIN)\n    vocab_words = tf.contrib.lookup.index_table_from_file(\n        params[\'words\'], num_oov_buckets=params[\'num_oov_buckets\'])\n    vocab_chars = tf.contrib.lookup.index_table_from_file(\n        params[\'chars\'], num_oov_buckets=params[\'num_oov_buckets\'])\n    with Path(params[\'tags\']).open() as f:\n        indices = [idx for idx, tag in enumerate(f) if tag.strip() != \'O\']\n        num_tags = len(indices) + 1\n    with Path(params[\'chars\']).open() as f:\n        num_chars = sum(1 for _ in f) + params[\'num_oov_buckets\']\n\n    # Char Embeddings\n    char_ids = vocab_chars.lookup(chars)\n    variable = tf.get_variable(\n        \'chars_embeddings\', [num_chars + 1, params[\'dim_chars\']], tf.float32)\n    char_embeddings = tf.nn.embedding_lookup(variable, char_ids)\n    char_embeddings = tf.layers.dropout(char_embeddings, rate=dropout,\n                                        training=training)\n\n    # Char 1d convolution\n    weights = tf.sequence_mask(nchars)\n    char_embeddings = masked_conv1d_and_max(\n        char_embeddings, weights, params[\'filters\'], params[\'kernel_size\'])\n\n    # Word Embeddings\n    word_ids = vocab_words.lookup(words)\n    glove = np.load(params[\'glove\'])[\'embeddings\']  # np.array\n    variable = np.vstack([glove, [[0.] * params[\'dim\']]])\n    variable = tf.Variable(variable, dtype=tf.float32, trainable=False)\n    word_embeddings = tf.nn.embedding_lookup(variable, word_ids)\n\n    # Concatenate Word and Char Embeddings\n    embeddings = tf.concat([word_embeddings, char_embeddings], axis=-1)\n    embeddings = tf.layers.dropout(embeddings, rate=dropout, training=training)\n\n    # LSTM\n    t = tf.transpose(embeddings, perm=[1, 0, 2])  # Need time-major\n    lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'lstm_size\'])\n    lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'lstm_size\'])\n    lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n    output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=nwords)\n    output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=nwords)\n    output = tf.concat([output_fw, output_bw], axis=-1)\n    output = tf.transpose(output, perm=[1, 0, 2])\n    output = tf.layers.dropout(output, rate=dropout, training=training)\n\n    # CRF\n    logits = tf.layers.dense(output, num_tags)\n    crf_params = tf.get_variable(""crf"", [num_tags, num_tags], dtype=tf.float32)\n    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, nwords)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        # Predictions\n        reverse_vocab_tags = tf.contrib.lookup.index_to_string_table_from_file(\n            params[\'tags\'])\n        pred_strings = reverse_vocab_tags.lookup(tf.to_int64(pred_ids))\n        predictions = {\n            \'pred_ids\': pred_ids,\n            \'tags\': pred_strings\n        }\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n    else:\n        # Loss\n        vocab_tags = tf.contrib.lookup.index_table_from_file(params[\'tags\'])\n        tags = vocab_tags.lookup(labels)\n        log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n            logits, tags, nwords, crf_params)\n        loss = tf.reduce_mean(-log_likelihood)\n\n        # Metrics\n        weights = tf.sequence_mask(nwords)\n        metrics = {\n            \'acc\': tf.metrics.accuracy(tags, pred_ids, weights),\n            \'precision\': precision(tags, pred_ids, num_tags, indices, weights),\n            \'recall\': recall(tags, pred_ids, num_tags, indices, weights),\n            \'f1\': f1(tags, pred_ids, num_tags, indices, weights),\n        }\n        for metric_name, op in metrics.items():\n            tf.summary.scalar(metric_name, op[1])\n\n        if mode == tf.estimator.ModeKeys.EVAL:\n            return tf.estimator.EstimatorSpec(\n                mode, loss=loss, eval_metric_ops=metrics)\n\n        elif mode == tf.estimator.ModeKeys.TRAIN:\n            train_op = tf.train.AdamOptimizer().minimize(\n                loss, global_step=tf.train.get_or_create_global_step())\n            return tf.estimator.EstimatorSpec(\n                mode, loss=loss, train_op=train_op)\n\n\nif __name__ == \'__main__\':\n    # Params\n    params = {\n        \'dim_chars\': 100,\n        \'dim\': 300,\n        \'dropout\': 0.5,\n        \'num_oov_buckets\': 1,\n        \'epochs\': 25,\n        \'batch_size\': 20,\n        \'buffer\': 15000,\n        \'filters\': 50,\n        \'kernel_size\': 3,\n        \'lstm_size\': 100,\n        \'words\': str(Path(DATADIR, \'vocab.words.txt\')),\n        \'chars\': str(Path(DATADIR, \'vocab.chars.txt\')),\n        \'tags\': str(Path(DATADIR, \'vocab.tags.txt\')),\n        \'glove\': str(Path(DATADIR, \'glove.npz\'))\n    }\n    with Path(\'results/params.json\').open(\'w\') as f:\n        json.dump(params, f, indent=4, sort_keys=True)\n\n    def fwords(name):\n        return str(Path(DATADIR, \'{}.words.txt\'.format(name)))\n\n    def ftags(name):\n        return str(Path(DATADIR, \'{}.tags.txt\'.format(name)))\n\n    # Estimator, train and evaluate\n    train_inpf = functools.partial(input_fn, fwords(\'train\'), ftags(\'train\'),\n                                   params, shuffle_and_repeat=True)\n    eval_inpf = functools.partial(input_fn, fwords(\'testa\'), ftags(\'testa\'))\n\n    cfg = tf.estimator.RunConfig(save_checkpoints_secs=120)\n    estimator = tf.estimator.Estimator(model_fn, \'results/model\', cfg, params)\n    Path(estimator.eval_dir()).mkdir(parents=True, exist_ok=True)\n    hook = tf.contrib.estimator.stop_if_no_increase_hook(\n        estimator, \'f1\', 500, min_steps=8000, run_every_secs=120)\n    train_spec = tf.estimator.TrainSpec(input_fn=train_inpf, hooks=[hook])\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_inpf, throttle_secs=120)\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\n    # Write predictions to file\n    def write_predictions(name):\n        Path(\'results/score\').mkdir(parents=True, exist_ok=True)\n        with Path(\'results/score/{}.preds.txt\'.format(name)).open(\'wb\') as f:\n            test_inpf = functools.partial(input_fn, fwords(name), ftags(name))\n            golds_gen = generator_fn(fwords(name), ftags(name))\n            preds_gen = estimator.predict(test_inpf)\n            for golds, preds in zip(golds_gen, preds_gen):\n                ((words, _), (_, _)), tags = golds\n                for word, tag, tag_pred in zip(words, tags, preds[\'tags\']):\n                    f.write(b\' \'.join([word, tag, tag_pred]) + b\'\\n\')\n                f.write(b\'\\n\')\n\n    for name in [\'train\', \'testa\', \'testb\']:\n        write_predictions(name)\n'"
models/chars_conv_lstm_crf/masked_conv.py,11,"b'""""""Implement masked 1d convolution with max-pooling""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom six.moves import reduce\nimport tensorflow as tf\n\n\ndef masked_conv1d_and_max(t, weights, filters, kernel_size):\n    """"""Applies 1d convolution and a masked max-pooling\n\n    Parameters\n    ----------\n    t : tf.Tensor\n        A tensor with at least 3 dimensions [d1, d2, ..., dn-1, dn]\n    weights : tf.Tensor of tf.bool\n        A Tensor of shape [d1, d2, dn-1]\n    filters : int\n        number of filters\n    kernel_size : int\n        kernel size for the temporal convolution\n\n    Returns\n    -------\n    tf.Tensor\n        A tensor of shape [d1, d2, dn-1, filters]\n\n    """"""\n    # Get shape and parameters\n    shape = tf.shape(t)\n    ndims = t.shape.ndims\n    dim1 = reduce(lambda x, y: x*y, [shape[i] for i in range(ndims - 2)])\n    dim2 = shape[-2]\n    dim3 = t.shape[-1]\n\n    # Reshape weights\n    weights = tf.reshape(weights, shape=[dim1, dim2, 1])\n    weights = tf.to_float(weights)\n\n    # Reshape input and apply weights\n    flat_shape = [dim1, dim2, dim3]\n    t = tf.reshape(t, shape=flat_shape)\n    t *= weights\n\n    # Apply convolution\n    t_conv = tf.layers.conv1d(t, filters, kernel_size, padding=\'same\')\n    t_conv *= weights\n\n    # Reduce max -- set to zero if all padded\n    t_conv += (1. - weights) * tf.reduce_min(t_conv, axis=-2, keepdims=True)\n    t_max = tf.reduce_max(t_conv, axis=-2)\n\n    # Reshape the output\n    final_shape = [shape[i] for i in range(ndims-2)] + [filters]\n    t_max = tf.reshape(t_max, shape=final_shape)\n\n    return t_max\n'"
models/chars_conv_lstm_crf/metrics.py,0,"b'""""""Metrics""""""\n\n__author__ = ""Guillaume Genthial""\n\nimport numpy as np\n\n\ntrain = [99.16, 99.07, 98.64, 99.03, 98.38]\ntesta = [94.53, 94.26, 94.00, 93.86, 93.87]\ntestb = [91.18, 91.10, 91.33, 91.42, 90.99]\n\nprint(np.mean(train), np.std(train))\nprint(np.mean(testa), np.std(testa))\nprint(np.mean(testb), np.std(testb))\n'"
models/chars_conv_lstm_crf/serve.py,0,"b'""""""Reload and serve a saved model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nfrom tensorflow.contrib import predictor\n\n\nLINE = \'John lives in New York\'\n\n\ndef parse_fn(line):\n    # Encode in Bytes for TF\n    words = [w.encode() for w in line.strip().split()]\n\n    # Chars\n    chars = [[c.encode() for c in w] for w in line.strip().split()]\n    lengths = [len(c) for c in chars]\n    max_len = max(lengths)\n    chars = [c + [b\'<pad>\'] * (max_len - l) for c, l in zip(chars, lengths)]\n\n    return {\'words\': [words], \'nwords\': [len(words)],\n            \'chars\': [chars], \'nchars\': [lengths]}\n\n\nif __name__ == \'__main__\':\n    export_dir = \'saved_model\'\n    subdirs = [x for x in Path(export_dir).iterdir()\n               if x.is_dir() and \'temp\' not in str(x)]\n    latest = str(sorted(subdirs)[-1])\n    predict_fn = predictor.from_saved_model(latest)\n    predictions = predict_fn(parse_fn(LINE))\n    print(predictions)\n'"
models/chars_conv_lstm_crf_ema/export.py,7,"b'""""""Export model as a saved_model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nimport json\n\nimport tensorflow as tf\n\nfrom main import model_fn\n\nDATADIR = \'../../data/example\'\nPARAMS = \'./results/params.json\'\nMODELDIR = \'./results/model\'\n\n\ndef serving_input_receiver_fn():\n    """"""Serving input_fn that builds features from placeholders\n\n    Returns\n    -------\n    tf.estimator.export.ServingInputReceiver\n    """"""\n    words = tf.placeholder(dtype=tf.string, shape=[None, None], name=\'words\')\n    nwords = tf.placeholder(dtype=tf.int32, shape=[None], name=\'nwords\')\n    chars = tf.placeholder(dtype=tf.string, shape=[None, None, None],\n                           name=\'chars\')\n    nchars = tf.placeholder(dtype=tf.int32, shape=[None, None],\n                            name=\'nchars\')\n    receiver_tensors = {\'words\': words, \'nwords\': nwords,\n                        \'chars\': chars, \'nchars\': nchars}\n    features = {\'words\': words, \'nwords\': nwords,\n                \'chars\': chars, \'nchars\': nchars}\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n\n\nif __name__ == \'__main__\':\n    with Path(PARAMS).open() as f:\n        params = json.load(f)\n\n    params[\'words\'] = str(Path(DATADIR, \'vocab.words.txt\'))\n    params[\'chars\'] = str(Path(DATADIR, \'vocab.chars.txt\'))\n    params[\'tags\'] = str(Path(DATADIR, \'vocab.tags.txt\'))\n    params[\'glove\'] = str(Path(DATADIR, \'glove.npz\'))\n\n    estimator = tf.estimator.Estimator(model_fn, MODELDIR, params=params)\n    estimator.export_saved_model(\'saved_model\', serving_input_receiver_fn)\n'"
models/chars_conv_lstm_crf_ema/interact.py,5,"b'""""""Interact with a model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nimport functools\nimport json\n\nimport tensorflow as tf\n\nfrom main import model_fn\n\nLINE = \'John lives in New York\'\nDATADIR = \'../../data/example\'\nPARAMS = \'./results/params.json\'\nMODELDIR = \'./results/model\'\n\n\ndef pretty_print(line, preds):\n    words = line.strip().split()\n    lengths = [max(len(w), len(p)) for w, p in zip(words, preds)]\n    padded_words = [w + (l - len(w)) * \' \' for w, l in zip(words, lengths)]\n    padded_preds = [p.decode() + (l - len(p)) * \' \' for p, l in zip(preds, lengths)]\n    print(\'words: {}\'.format(\' \'.join(padded_words)))\n    print(\'preds: {}\'.format(\' \'.join(padded_preds)))\n\n\ndef predict_input_fn(line):\n    # Words\n    words = [w.encode() for w in line.strip().split()]\n    nwords = len(words)\n\n    # Chars\n    chars = [[c.encode() for c in w] for w in line.strip().split()]\n    lengths = [len(c) for c in chars]\n    max_len = max(lengths)\n    chars = [c + [b\'<pad>\'] * (max_len - l) for c, l in zip(chars, lengths)]\n\n    # Wrapping in Tensors\n    words = tf.constant([words], dtype=tf.string)\n    nwords = tf.constant([nwords], dtype=tf.int32)\n    chars = tf.constant([chars], dtype=tf.string)\n    nchars = tf.constant([lengths], dtype=tf.int32)\n\n    return ((words, nwords), (chars, nchars)), None\n\n\nif __name__ == \'__main__\':\n    with Path(PARAMS).open() as f:\n        params = json.load(f)\n\n    params[\'words\'] = str(Path(DATADIR, \'vocab.words.txt\'))\n    params[\'chars\'] = str(Path(DATADIR, \'vocab.chars.txt\'))\n    params[\'tags\'] = str(Path(DATADIR, \'vocab.tags.txt\'))\n    params[\'glove\'] = str(Path(DATADIR, \'glove.npz\'))\n\n    estimator = tf.estimator.Estimator(model_fn, MODELDIR, params=params)\n    predict_inpf = functools.partial(predict_input_fn, LINE)\n    for pred in estimator.predict(predict_inpf):\n        pretty_print(LINE, pred[\'tags_ema\'])\n        break\n'"
models/chars_conv_lstm_crf_ema/main.py,59,"b'""""""GloVe Embeddings + chars conv and max pooling + bi-LSTM + CRF""""""\n\n__author__ = ""Guillaume Genthial""\n\nimport functools\nimport json\nimport logging\nfrom pathlib import Path\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nfrom tf_metrics import precision, recall, f1\n\nfrom masked_conv import masked_conv1d_and_max\n\nDATADIR = \'../../data/example\'\n\n# Logging\nPath(\'results\').mkdir(exist_ok=True)\ntf.logging.set_verbosity(logging.INFO)\nhandlers = [\n    logging.FileHandler(\'results/main.log\'),\n    logging.StreamHandler(sys.stdout)\n]\nlogging.getLogger(\'tensorflow\').handlers = handlers\n\n\ndef parse_fn(line_words, line_tags):\n    # Encode in Bytes for TF\n    words = [w.encode() for w in line_words.strip().split()]\n    tags = [t.encode() for t in line_tags.strip().split()]\n    assert len(words) == len(tags), ""Words and tags lengths don\'t match""\n\n    # Chars\n    chars = [[c.encode() for c in w] for w in line_words.strip().split()]\n    lengths = [len(c) for c in chars]\n    max_len = max(lengths)\n    chars = [c + [b\'<pad>\'] * (max_len - l) for c, l in zip(chars, lengths)]\n    return ((words, len(words)), (chars, lengths)), tags\n\n\ndef generator_fn(words, tags):\n    with Path(words).open(\'r\') as f_words, Path(tags).open(\'r\') as f_tags:\n        for line_words, line_tags in zip(f_words, f_tags):\n            yield parse_fn(line_words, line_tags)\n\n\ndef input_fn(words, tags, params=None, shuffle_and_repeat=False):\n    params = params if params is not None else {}\n    shapes = ((([None], ()),               # (words, nwords)\n               ([None, None], [None])),    # (chars, nchars)\n              [None])                      # tags\n    types = (((tf.string, tf.int32),\n              (tf.string, tf.int32)),\n             tf.string)\n    defaults = (((\'<pad>\', 0),\n                 (\'<pad>\', 0)),\n                \'O\')\n    dataset = tf.data.Dataset.from_generator(\n        functools.partial(generator_fn, words, tags),\n        output_shapes=shapes, output_types=types)\n\n    if shuffle_and_repeat:\n        dataset = dataset.shuffle(params[\'buffer\']).repeat(params[\'epochs\'])\n\n    dataset = (dataset\n               .padded_batch(params.get(\'batch_size\', 20), shapes, defaults)\n               .prefetch(1))\n    return dataset\n\n\ndef graph_fn(features, labels, mode, params, reuse=None, getter=None):\n    # Read vocabs and inputs\n    num_tags = params[\'num_tags\']\n    (words, nwords), (chars, nchars) = features\n    training = (mode == tf.estimator.ModeKeys.TRAIN)\n    with tf.variable_scope(\'graph\', reuse=reuse, custom_getter=getter):\n        # Read vocabs and inputs\n        dropout = params[\'dropout\']\n        (words, nwords), (chars, nchars) = features\n        training = (mode == tf.estimator.ModeKeys.TRAIN)\n        vocab_words = tf.contrib.lookup.index_table_from_file(\n            params[\'words\'], num_oov_buckets=params[\'num_oov_buckets\'])\n        vocab_chars = tf.contrib.lookup.index_table_from_file(\n            params[\'chars\'], num_oov_buckets=params[\'num_oov_buckets\'])\n        with Path(params[\'tags\']).open() as f:\n            indices = [idx for idx, tag in enumerate(f) if tag.strip() != \'O\']\n            num_tags = len(indices) + 1\n        with Path(params[\'chars\']).open() as f:\n            num_chars = sum(1 for _ in f) + params[\'num_oov_buckets\']\n\n        # Char Embeddings\n        char_ids = vocab_chars.lookup(chars)\n        variable = tf.get_variable(\n            \'chars_embeddings\', [num_chars + 1, params[\'dim_chars\']], tf.float32)\n        char_embeddings = tf.nn.embedding_lookup(variable, char_ids)\n        char_embeddings = tf.layers.dropout(char_embeddings, rate=dropout,\n                                            training=training)\n\n        # Char LSTM\n        weights = tf.sequence_mask(nchars)\n        char_embeddings = masked_conv1d_and_max(\n            char_embeddings, weights, params[\'filters\'], params[\'kernel_size\'])\n\n        # Word Embeddings\n        word_ids = vocab_words.lookup(words)\n        glove = np.load(params[\'glove\'])[\'embeddings\']  # np.array\n        variable = np.vstack([glove, [[0.] * params[\'dim\']]])\n        variable = tf.Variable(variable, dtype=tf.float32, trainable=False)\n        word_embeddings = tf.nn.embedding_lookup(variable, word_ids)\n\n        # Concatenate Word and Char Embeddings\n        embeddings = tf.concat([word_embeddings, char_embeddings], axis=-1)\n        embeddings = tf.layers.dropout(embeddings, rate=dropout, training=training)\n\n        # LSTM\n        t = tf.transpose(embeddings, perm=[1, 0, 2])  # Need time-major\n        lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'lstm_size\'])\n        lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'lstm_size\'])\n        lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n        output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=nwords)\n        output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=nwords)\n        output = tf.concat([output_fw, output_bw], axis=-1)\n        output = tf.transpose(output, perm=[1, 0, 2])\n        output = tf.layers.dropout(output, rate=dropout, training=training)\n\n        # CRF\n        logits = tf.layers.dense(output, num_tags)\n        crf_params = tf.get_variable(""crf"", [num_tags, num_tags], dtype=tf.float32)\n        return logits, crf_params\n\n\ndef ema_getter(ema):\n\n    def _ema_getter(getter, name, *args, **kwargs):\n        var = getter(name, *args, **kwargs)\n        ema_var = ema.average(var)\n        return ema_var if ema_var else var\n\n    return _ema_getter\n\n\ndef model_fn(features, labels, mode, params):\n    # For serving features are a bit different\n    if isinstance(features, dict):\n        features = ((features[\'words\'], features[\'nwords\']),\n                    (features[\'chars\'], features[\'nchars\']))\n\n    with Path(params[\'tags\']).open() as f:\n        indices = [idx for idx, tag in enumerate(f) if tag.strip() != \'O\']\n        num_tags = len(indices) + 1\n        params[\'num_tags\'] = num_tags\n\n    # Graph\n    (words, nwords), (chars, nchars) = features\n    logits, crf_params = graph_fn(features, labels, mode, params)\n    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, nwords)\n\n    # Moving Average\n    variables = tf.get_collection(\'trainable_variables\', \'graph\')\n    ema = tf.train.ExponentialMovingAverage(0.999)\n    ema_op = ema.apply(variables)\n    logits_ema, crf_params_ema = graph_fn(\n        features, labels, mode, params, reuse=True, getter=ema_getter(ema))\n    pred_ids_ema, _ = tf.contrib.crf.crf_decode(\n        logits_ema, crf_params_ema, nwords)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        # Predictions\n        reverse_vocab_tags = tf.contrib.lookup.index_to_string_table_from_file(\n            params[\'tags\'])\n        pred_strings = reverse_vocab_tags.lookup(tf.to_int64(pred_ids))\n        pred_strings_ema = reverse_vocab_tags.lookup(tf.to_int64(pred_ids_ema))\n        predictions = {\n            \'pred_ids\': pred_ids,\n            \'tags\': pred_strings,\n            \'pred_ids_ema\': pred_ids_ema,\n            \'tags_ema\': pred_strings_ema,\n        }\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n    else:\n        # Loss\n        vocab_tags = tf.contrib.lookup.index_table_from_file(params[\'tags\'])\n        tags = vocab_tags.lookup(labels)\n        log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n            logits, tags, nwords, crf_params)\n        loss = tf.reduce_mean(-log_likelihood)\n\n        # Metrics\n        weights = tf.sequence_mask(nwords)\n        metrics = {\n            \'acc\': tf.metrics.accuracy(tags, pred_ids, weights),\n            \'acc_ema\': tf.metrics.accuracy(tags, pred_ids_ema, weights),\n            \'pr\': precision(tags, pred_ids, num_tags, indices, weights),\n            \'pr_ema\': precision(tags, pred_ids_ema, num_tags, indices, weights),\n            \'rc\': recall(tags, pred_ids, num_tags, indices, weights),\n            \'rc_ema\': recall(tags, pred_ids_ema, num_tags, indices, weights),\n            \'f1\': f1(tags, pred_ids, num_tags, indices, weights),\n            \'f1_ema\': f1(tags, pred_ids_ema, num_tags, indices, weights),\n        }\n        for metric_name, op in metrics.items():\n            tf.summary.scalar(metric_name, op[1])\n\n        if mode == tf.estimator.ModeKeys.EVAL:\n            return tf.estimator.EstimatorSpec(\n                mode, loss=loss, eval_metric_ops=metrics)\n\n        elif mode == tf.estimator.ModeKeys.TRAIN:\n            train_op = tf.train.AdamOptimizer().minimize(\n                loss, global_step=tf.train.get_or_create_global_step(),\n                var_list=variables)\n            train_op = tf.group([train_op, ema_op])\n            return tf.estimator.EstimatorSpec(\n                mode, loss=loss, train_op=train_op)\n\n\nif __name__ == \'__main__\':\n    # Params\n    params = {\n        \'dim_chars\': 100,\n        \'dim\': 300,\n        \'dropout\': 0.5,\n        \'num_oov_buckets\': 1,\n        \'epochs\': 25,\n        \'batch_size\': 20,\n        \'buffer\': 15000,\n        \'filters\': 50,\n        \'kernel_size\': 3,\n        \'lstm_size\': 100,\n        \'words\': str(Path(DATADIR, \'vocab.words.txt\')),\n        \'chars\': str(Path(DATADIR, \'vocab.chars.txt\')),\n        \'tags\': str(Path(DATADIR, \'vocab.tags.txt\')),\n        \'glove\': str(Path(DATADIR, \'glove.npz\'))\n    }\n    with Path(\'results/params.json\').open(\'w\') as f:\n        json.dump(params, f, indent=4, sort_keys=True)\n\n    def fwords(name):\n        return str(Path(DATADIR, \'{}.words.txt\'.format(name)))\n\n    def ftags(name):\n        return str(Path(DATADIR, \'{}.tags.txt\'.format(name)))\n\n    # Estimator, train and evaluate\n    train_inpf = functools.partial(input_fn, fwords(\'train\'), ftags(\'train\'),\n                                   params, shuffle_and_repeat=True)\n    eval_inpf = functools.partial(input_fn, fwords(\'testa\'), ftags(\'testa\'))\n\n    cfg = tf.estimator.RunConfig(save_checkpoints_secs=120)\n    estimator = tf.estimator.Estimator(model_fn, \'results/model\', cfg, params)\n    Path(estimator.eval_dir()).mkdir(parents=True, exist_ok=True)\n    hook = tf.contrib.estimator.stop_if_no_increase_hook(\n        estimator, \'f1_ema\', 500, min_steps=8000, run_every_secs=120)\n    train_spec = tf.estimator.TrainSpec(input_fn=train_inpf, hooks=[hook])\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_inpf, throttle_secs=120)\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\n    # Write predictions to file\n    def write_predictions(name, mode):\n        """"""Write predictions of dataset with name to file""""""\n        Path(\'results/score\').mkdir(parents=True, exist_ok=True)\n        with Path(\'results/score/{}.{}.preds.txt\'.format(name, mode)).open(\'wb\') as f:\n            test_inpf = functools.partial(input_fn, fwords(name), ftags(name))\n            golds_gen = generator_fn(fwords(name), ftags(name))\n            preds_gen = estimator.predict(test_inpf)\n            for golds, preds in zip(golds_gen, preds_gen):\n                ((words, _), (_, _)), tags = golds\n                for word, tag, tag_pred in zip(words, tags, preds[mode]):\n                    f.write(b\' \'.join([word, tag, tag_pred]) + b\'\\n\')\n                f.write(b\'\\n\')\n\n    for name in [\'train\', \'testa\', \'testb\']:\n        for mode in [\'tags\', \'tags_ema\']:\n            write_predictions(name, mode)\n'"
models/chars_conv_lstm_crf_ema/masked_conv.py,11,"b'""""""Implement masked 1d convolution with max-pooling""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom six.moves import reduce\nimport tensorflow as tf\n\n\ndef masked_conv1d_and_max(t, weights, filters, kernel_size):\n    """"""Applies 1d convolution and a masked max-pooling\n\n    Parameters\n    ----------\n    t : tf.Tensor\n        A tensor with at least 3 dimensions [d1, d2, ..., dn-1, dn]\n    weights : tf.Tensor of tf.bool\n        A Tensor of shape [d1, d2, dn-1]\n    filters : int\n        number of filters\n    kernel_size : int\n        kernel size for the temporal convolution\n\n    Returns\n    -------\n    tf.Tensor\n        A tensor of shape [d1, d2, dn-1, filters]\n\n    """"""\n    # Get shape and parameters\n    shape = tf.shape(t)\n    ndims = t.shape.ndims\n    dim1 = reduce(lambda x, y: x*y, [shape[i] for i in range(ndims - 2)])\n    dim2 = shape[-2]\n    dim3 = t.shape[-1]\n\n    # Reshape weights\n    weights = tf.reshape(weights, shape=[dim1, dim2, 1])\n    weights = tf.to_float(weights)\n\n    # Reshape input and apply weights\n    flat_shape = [dim1, dim2, dim3]\n    t = tf.reshape(t, shape=flat_shape)\n    t *= weights\n\n    # Apply convolution\n    t_conv = tf.layers.conv1d(t, filters, kernel_size, padding=\'same\')\n    t_conv *= weights\n\n    # Reduce max -- set to zero if all padded\n    t_conv += (1. - weights) * tf.reduce_min(t_conv, axis=-2, keepdims=True)\n    t_max = tf.reduce_max(t_conv, axis=-2)\n\n    # Reshape the output\n    final_shape = [shape[i] for i in range(ndims-2)] + [filters]\n    t_max = tf.reshape(t_max, shape=final_shape)\n\n    return t_max\n'"
models/chars_conv_lstm_crf_ema/metrics.py,0,"b'""""""Metrics""""""\n\n__author__ = ""Guillaume Genthial""\n\nimport numpy as np\n\n\n# Standard\ntrain = [98.55, 98.58, 98.47, 99.40, 98.62]\ntesta = [94.20, 93.85, 94.28, 94.31, 94.11]\ntestb = [90.97, 91.25, 91.14, 91.41, 91.02]\n\nprint(np.mean(train), np.std(train))\nprint(np.mean(testa), np.std(testa))\nprint(np.mean(testb), np.std(testb))\n\n# EMA\ntrain = [98.45, 98.40, 98.49, 99.44, 98.57]\ntesta = [94.30, 93.98, 94.32, 94.50, 94.35]\ntestb = [90.91, 91.21, 91.13, 91.17, 91.22]\n\nprint(np.mean(train), np.std(train))\nprint(np.mean(testa), np.std(testa))\nprint(np.mean(testb), np.std(testb))\n'"
models/chars_conv_lstm_crf_ema/serve.py,0,"b'""""""Reload and serve a saved model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nfrom tensorflow.contrib import predictor\n\n\nLINE = \'John lives in New York\'\n\n\ndef parse_fn(line):\n    # Encode in Bytes for TF\n    words = [w.encode() for w in line.strip().split()]\n\n    # Chars\n    chars = [[c.encode() for c in w] for w in line.strip().split()]\n    lengths = [len(c) for c in chars]\n    max_len = max(lengths)\n    chars = [c + [b\'<pad>\'] * (max_len - l) for c, l in zip(chars, lengths)]\n\n    return {\'words\': [words], \'nwords\': [len(words)],\n            \'chars\': [chars], \'nchars\': [lengths]}\n\n\nif __name__ == \'__main__\':\n    export_dir = \'saved_model\'\n    subdirs = [x for x in Path(export_dir).iterdir()\n               if x.is_dir() and \'temp\' not in str(x)]\n    latest = str(sorted(subdirs)[-1])\n    predict_fn = predictor.from_saved_model(latest)\n    predictions = predict_fn(parse_fn(LINE))\n    print(predictions)\n'"
models/chars_lstm_lstm_crf/export.py,7,"b'""""""Export model as a saved_model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nimport json\n\nimport tensorflow as tf\n\nfrom main import model_fn\n\nDATADIR = \'../../data/example\'\nPARAMS = \'./results/params.json\'\nMODELDIR = \'./results/model\'\n\n\ndef serving_input_receiver_fn():\n    """"""Serving input_fn that builds features from placeholders\n\n    Returns\n    -------\n    tf.estimator.export.ServingInputReceiver\n    """"""\n    words = tf.placeholder(dtype=tf.string, shape=[None, None], name=\'words\')\n    nwords = tf.placeholder(dtype=tf.int32, shape=[None], name=\'nwords\')\n    chars = tf.placeholder(dtype=tf.string, shape=[None, None, None],\n                           name=\'chars\')\n    nchars = tf.placeholder(dtype=tf.int32, shape=[None, None],\n                            name=\'nchars\')\n    receiver_tensors = {\'words\': words, \'nwords\': nwords,\n                        \'chars\': chars, \'nchars\': nchars}\n    features = {\'words\': words, \'nwords\': nwords,\n                \'chars\': chars, \'nchars\': nchars}\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n\n\nif __name__ == \'__main__\':\n    with Path(PARAMS).open() as f:\n        params = json.load(f)\n\n    params[\'words\'] = str(Path(DATADIR, \'vocab.words.txt\'))\n    params[\'chars\'] = str(Path(DATADIR, \'vocab.chars.txt\'))\n    params[\'tags\'] = str(Path(DATADIR, \'vocab.tags.txt\'))\n    params[\'glove\'] = str(Path(DATADIR, \'glove.npz\'))\n\n    estimator = tf.estimator.Estimator(model_fn, MODELDIR, params=params)\n    estimator.export_saved_model(\'saved_model\', serving_input_receiver_fn)\n'"
models/chars_lstm_lstm_crf/interact.py,5,"b'""""""Interact with a model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nimport functools\nimport json\n\nimport tensorflow as tf\n\nfrom main import model_fn\n\nLINE = \'John lives in New York\'\nDATADIR = \'../../data/example\'\nPARAMS = \'./results/params.json\'\nMODELDIR = \'./results/model\'\n\n\ndef pretty_print(line, preds):\n    words = line.strip().split()\n    lengths = [max(len(w), len(p)) for w, p in zip(words, preds)]\n    padded_words = [w + (l - len(w)) * \' \' for w, l in zip(words, lengths)]\n    padded_preds = [p.decode() + (l - len(p)) * \' \' for p, l in zip(preds, lengths)]\n    print(\'words: {}\'.format(\' \'.join(padded_words)))\n    print(\'preds: {}\'.format(\' \'.join(padded_preds)))\n\n\ndef predict_input_fn(line):\n    # Words\n    words = [w.encode() for w in line.strip().split()]\n    nwords = len(words)\n\n    # Chars\n    chars = [[c.encode() for c in w] for w in line.strip().split()]\n    lengths = [len(c) for c in chars]\n    max_len = max(lengths)\n    chars = [c + [b\'<pad>\'] * (max_len - l) for c, l in zip(chars, lengths)]\n\n    # Wrapping in Tensors\n    words = tf.constant([words], dtype=tf.string)\n    nwords = tf.constant([nwords], dtype=tf.int32)\n    chars = tf.constant([chars], dtype=tf.string)\n    nchars = tf.constant([lengths], dtype=tf.int32)\n\n    return ((words, nwords), (chars, nchars)), None\n\n\nif __name__ == \'__main__\':\n    with Path(PARAMS).open() as f:\n        params = json.load(f)\n\n    params[\'words\'] = str(Path(DATADIR, \'vocab.words.txt\'))\n    params[\'chars\'] = str(Path(DATADIR, \'vocab.chars.txt\'))\n    params[\'tags\'] = str(Path(DATADIR, \'vocab.tags.txt\'))\n    params[\'glove\'] = str(Path(DATADIR, \'glove.npz\'))\n\n    estimator = tf.estimator.Estimator(model_fn, MODELDIR, params=params)\n    predict_inpf = functools.partial(predict_input_fn, LINE)\n    for pred in estimator.predict(predict_inpf):\n        pretty_print(LINE, pred[\'tags\'])\n        break\n'"
models/chars_lstm_lstm_crf/main.py,63,"b'""""""GloVe Embeddings + chars bi-LSTM + bi-LSTM + CRF""""""\n\n__author__ = ""Guillaume Genthial""\n\nimport functools\nimport json\nimport logging\nfrom pathlib import Path\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nfrom tf_metrics import precision, recall, f1\n\nDATADIR = \'../../data/example\'\n\n# Logging\nPath(\'results\').mkdir(exist_ok=True)\ntf.logging.set_verbosity(logging.INFO)\nhandlers = [\n    logging.FileHandler(\'results/main.log\'),\n    logging.StreamHandler(sys.stdout)\n]\nlogging.getLogger(\'tensorflow\').handlers = handlers\n\n\ndef parse_fn(line_words, line_tags):\n    # Encode in Bytes for TF\n    words = [w.encode() for w in line_words.strip().split()]\n    tags = [t.encode() for t in line_tags.strip().split()]\n    assert len(words) == len(tags), ""Words and tags lengths don\'t match""\n\n    # Chars\n    chars = [[c.encode() for c in w] for w in line_words.strip().split()]\n    lengths = [len(c) for c in chars]\n    max_len = max(lengths)\n    chars = [c + [b\'<pad>\'] * (max_len - l) for c, l in zip(chars, lengths)]\n    return ((words, len(words)), (chars, lengths)), tags\n\n\ndef generator_fn(words, tags):\n    with Path(words).open(\'r\') as f_words, Path(tags).open(\'r\') as f_tags:\n        for line_words, line_tags in zip(f_words, f_tags):\n            yield parse_fn(line_words, line_tags)\n\n\ndef input_fn(words, tags, params=None, shuffle_and_repeat=False):\n    params = params if params is not None else {}\n    shapes = ((([None], ()),               # (words, nwords)\n               ([None, None], [None])),    # (chars, nchars)\n              [None])                      # tags\n    types = (((tf.string, tf.int32),\n              (tf.string, tf.int32)),\n             tf.string)\n    defaults = (((\'<pad>\', 0),\n                 (\'<pad>\', 0)),\n                \'O\')\n    dataset = tf.data.Dataset.from_generator(\n        functools.partial(generator_fn, words, tags),\n        output_shapes=shapes, output_types=types)\n\n    if shuffle_and_repeat:\n        dataset = dataset.shuffle(params[\'buffer\']).repeat(params[\'epochs\'])\n\n    dataset = (dataset\n               .padded_batch(params.get(\'batch_size\', 20), shapes, defaults)\n               .prefetch(1))\n    return dataset\n\n\ndef model_fn(features, labels, mode, params):\n    # For serving features are a bit different\n    if isinstance(features, dict):\n        features = ((features[\'words\'], features[\'nwords\']),\n                    (features[\'chars\'], features[\'nchars\']))\n\n    # Read vocabs and inputs\n    (words, nwords), (chars, nchars) = features\n    dropout = params[\'dropout\']\n    training = (mode == tf.estimator.ModeKeys.TRAIN)\n    vocab_words = tf.contrib.lookup.index_table_from_file(\n        params[\'words\'], num_oov_buckets=params[\'num_oov_buckets\'])\n    vocab_chars = tf.contrib.lookup.index_table_from_file(\n        params[\'chars\'], num_oov_buckets=params[\'num_oov_buckets\'])\n    with Path(params[\'tags\']).open() as f:\n        indices = [idx for idx, tag in enumerate(f) if tag.strip() != \'O\']\n        num_tags = len(indices) + 1\n    with Path(params[\'chars\']).open() as f:\n        num_chars = sum(1 for _ in f) + params[\'num_oov_buckets\']\n\n    # Char Embeddings\n    char_ids = vocab_chars.lookup(chars)\n    variable = tf.get_variable(\n        \'chars_embeddings\', [num_chars, params[\'dim_chars\']], tf.float32)\n    char_embeddings = tf.nn.embedding_lookup(variable, char_ids)\n    char_embeddings = tf.layers.dropout(char_embeddings, rate=dropout,\n                                        training=training)\n\n    # Char LSTM\n    dim_words = tf.shape(char_embeddings)[1]\n    dim_chars = tf.shape(char_embeddings)[2]\n    flat = tf.reshape(char_embeddings, [-1, dim_chars, params[\'dim_chars\']])\n    t = tf.transpose(flat, perm=[1, 0, 2])\n    lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'char_lstm_size\'])\n    lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'char_lstm_size\'])\n    lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n    _, (_, output_fw) = lstm_cell_fw(t, dtype=tf.float32,\n                                     sequence_length=tf.reshape(nchars, [-1]))\n    _, (_, output_bw) = lstm_cell_bw(t, dtype=tf.float32,\n                                     sequence_length=tf.reshape(nchars, [-1]))\n    output = tf.concat([output_fw, output_bw], axis=-1)\n    char_embeddings = tf.reshape(output, [-1, dim_words, 50])\n\n    # Word Embeddings\n    word_ids = vocab_words.lookup(words)\n    glove = np.load(params[\'glove\'])[\'embeddings\']  # np.array\n    variable = np.vstack([glove, [[0.] * params[\'dim\']]])\n    variable = tf.Variable(variable, dtype=tf.float32, trainable=False)\n    word_embeddings = tf.nn.embedding_lookup(variable, word_ids)\n\n    # Concatenate Word and Char Embeddings\n    embeddings = tf.concat([word_embeddings, char_embeddings], axis=-1)\n    embeddings = tf.layers.dropout(embeddings, rate=dropout, training=training)\n\n    # LSTM\n    t = tf.transpose(embeddings, perm=[1, 0, 2])  # Need time-major\n    lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'lstm_size\'])\n    lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'lstm_size\'])\n    lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n    output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=nwords)\n    output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=nwords)\n    output = tf.concat([output_fw, output_bw], axis=-1)\n    output = tf.transpose(output, perm=[1, 0, 2])\n    output = tf.layers.dropout(output, rate=dropout, training=training)\n\n    # CRF\n    logits = tf.layers.dense(output, num_tags)\n    crf_params = tf.get_variable(""crf"", [num_tags, num_tags], dtype=tf.float32)\n    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, nwords)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        # Predictions\n        reverse_vocab_tags = tf.contrib.lookup.index_to_string_table_from_file(\n            params[\'tags\'])\n        pred_strings = reverse_vocab_tags.lookup(tf.to_int64(pred_ids))\n        predictions = {\n            \'pred_ids\': pred_ids,\n            \'tags\': pred_strings\n        }\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n    else:\n        # Loss\n        vocab_tags = tf.contrib.lookup.index_table_from_file(params[\'tags\'])\n        tags = vocab_tags.lookup(labels)\n        log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n            logits, tags, nwords, crf_params)\n        loss = tf.reduce_mean(-log_likelihood)\n\n        # Metrics\n        weights = tf.sequence_mask(nwords)\n        metrics = {\n            \'acc\': tf.metrics.accuracy(tags, pred_ids, weights),\n            \'precision\': precision(tags, pred_ids, num_tags, indices, weights),\n            \'recall\': recall(tags, pred_ids, num_tags, indices, weights),\n            \'f1\': f1(tags, pred_ids, num_tags, indices, weights),\n        }\n        for metric_name, op in metrics.items():\n            tf.summary.scalar(metric_name, op[1])\n\n        if mode == tf.estimator.ModeKeys.EVAL:\n            return tf.estimator.EstimatorSpec(\n                mode, loss=loss, eval_metric_ops=metrics)\n\n        elif mode == tf.estimator.ModeKeys.TRAIN:\n            train_op = tf.train.AdamOptimizer().minimize(\n                loss, global_step=tf.train.get_or_create_global_step())\n            return tf.estimator.EstimatorSpec(\n                mode, loss=loss, train_op=train_op)\n\n\nif __name__ == \'__main__\':\n    # Params\n    params = {\n        \'dim\': 300,\n        \'dim_chars\': 100,\n        \'dropout\': 0.5,\n        \'num_oov_buckets\': 1,\n        \'epochs\': 25,\n        \'batch_size\': 20,\n        \'buffer\': 15000,\n        \'char_lstm_size\': 25,\n        \'lstm_size\': 100,\n        \'words\': str(Path(DATADIR, \'vocab.words.txt\')),\n        \'chars\': str(Path(DATADIR, \'vocab.chars.txt\')),\n        \'tags\': str(Path(DATADIR, \'vocab.tags.txt\')),\n        \'glove\': str(Path(DATADIR, \'glove.npz\'))\n    }\n    with Path(\'results/params.json\').open(\'w\') as f:\n        json.dump(params, f, indent=4, sort_keys=True)\n\n    def fwords(name):\n        return str(Path(DATADIR, \'{}.words.txt\'.format(name)))\n\n    def ftags(name):\n        return str(Path(DATADIR, \'{}.tags.txt\'.format(name)))\n\n    # Estimator, train and evaluate\n    train_inpf = functools.partial(input_fn, fwords(\'train\'), ftags(\'train\'),\n                                   params, shuffle_and_repeat=True)\n    eval_inpf = functools.partial(input_fn, fwords(\'testa\'), ftags(\'testa\'))\n\n    cfg = tf.estimator.RunConfig(save_checkpoints_secs=120)\n    estimator = tf.estimator.Estimator(model_fn, \'results/model\', cfg, params)\n    Path(estimator.eval_dir()).mkdir(parents=True, exist_ok=True)\n    hook = tf.contrib.estimator.stop_if_no_increase_hook(\n        estimator, \'f1\', 500, min_steps=8000, run_every_secs=120)\n    train_spec = tf.estimator.TrainSpec(input_fn=train_inpf, hooks=[hook])\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_inpf, throttle_secs=120)\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\n    # Write predictions to file\n    def write_predictions(name):\n        Path(\'results/score\').mkdir(parents=True, exist_ok=True)\n        with Path(\'results/score/{}.preds.txt\'.format(name)).open(\'wb\') as f:\n            test_inpf = functools.partial(input_fn, fwords(name), ftags(name))\n            golds_gen = generator_fn(fwords(name), ftags(name))\n            preds_gen = estimator.predict(test_inpf)\n            for golds, preds in zip(golds_gen, preds_gen):\n                ((words, _), (_, _)), tags = golds\n                for word, tag, tag_pred in zip(words, tags, preds[\'tags\']):\n                    f.write(b\' \'.join([word, tag, tag_pred]) + b\'\\n\')\n                f.write(b\'\\n\')\n\n    for name in [\'train\', \'testa\', \'testb\']:\n        write_predictions(name)\n'"
models/chars_lstm_lstm_crf/metrics.py,0,"b'""""""Metrics""""""\n\n__author__ = ""Guillaume Genthial""\n\nimport numpy as np\n\n\ntrain = [98.98, 99.20, 98.39, 98.81, 98.75]\ntesta = [94.07, 94.22, 93.78, 94.36, 93.68]\ntestb = [90.79, 90.86, 91.22, 91.02, 91.14]\n\nprint(np.mean(train), np.std(train))\nprint(np.mean(testa), np.std(testa))\nprint(np.mean(testb), np.std(testb))\n'"
models/chars_lstm_lstm_crf/serve.py,0,"b'""""""Reload and serve a saved model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nfrom tensorflow.contrib import predictor\n\n\nLINE = \'John lives in New York\'\n\n\ndef parse_fn(line):\n    # Encode in Bytes for TF\n    words = [w.encode() for w in line.strip().split()]\n\n    # Chars\n    chars = [[c.encode() for c in w] for w in line.strip().split()]\n    lengths = [len(c) for c in chars]\n    max_len = max(lengths)\n    chars = [c + [b\'<pad>\'] * (max_len - l) for c, l in zip(chars, lengths)]\n\n    return {\'words\': [words], \'nwords\': [len(words)],\n            \'chars\': [chars], \'nchars\': [lengths]}\n\n\nif __name__ == \'__main__\':\n    export_dir = \'saved_model\'\n    subdirs = [x for x in Path(export_dir).iterdir()\n               if x.is_dir() and \'temp\' not in str(x)]\n    latest = str(sorted(subdirs)[-1])\n    predict_fn = predictor.from_saved_model(latest)\n    predictions = predict_fn(parse_fn(LINE))\n    print(predictions)\n'"
models/chars_lstm_lstm_crf_ema/export.py,7,"b'""""""Export model as a saved_model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nimport json\n\nimport tensorflow as tf\n\nfrom main import model_fn\n\nDATADIR = \'../../data/example\'\nPARAMS = \'./results/params.json\'\nMODELDIR = \'./results/model\'\n\n\ndef serving_input_receiver_fn():\n    """"""Serving input_fn that builds features from placeholders\n\n    Returns\n    -------\n    tf.estimator.export.ServingInputReceiver\n    """"""\n    words = tf.placeholder(dtype=tf.string, shape=[None, None], name=\'words\')\n    nwords = tf.placeholder(dtype=tf.int32, shape=[None], name=\'nwords\')\n    chars = tf.placeholder(dtype=tf.string, shape=[None, None, None],\n                           name=\'chars\')\n    nchars = tf.placeholder(dtype=tf.int32, shape=[None, None],\n                            name=\'nchars\')\n    receiver_tensors = {\'words\': words, \'nwords\': nwords,\n                        \'chars\': chars, \'nchars\': nchars}\n    features = {\'words\': words, \'nwords\': nwords,\n                \'chars\': chars, \'nchars\': nchars}\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n\n\nif __name__ == \'__main__\':\n    with Path(PARAMS).open() as f:\n        params = json.load(f)\n\n    params[\'words\'] = str(Path(DATADIR, \'vocab.words.txt\'))\n    params[\'chars\'] = str(Path(DATADIR, \'vocab.chars.txt\'))\n    params[\'tags\'] = str(Path(DATADIR, \'vocab.tags.txt\'))\n    params[\'glove\'] = str(Path(DATADIR, \'glove.npz\'))\n\n    estimator = tf.estimator.Estimator(model_fn, MODELDIR, params=params)\n    estimator.export_saved_model(\'saved_model\', serving_input_receiver_fn)\n'"
models/chars_lstm_lstm_crf_ema/interact.py,5,"b'""""""Interact with a model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nimport functools\nimport json\n\nimport tensorflow as tf\n\nfrom main import model_fn\n\nLINE = \'John lives in New York\'\nDATADIR = \'../../data/example\'\nPARAMS = \'./results/params.json\'\nMODELDIR = \'./results/model\'\n\n\ndef pretty_print(line, preds):\n    words = line.strip().split()\n    lengths = [max(len(w), len(p)) for w, p in zip(words, preds)]\n    padded_words = [w + (l - len(w)) * \' \' for w, l in zip(words, lengths)]\n    padded_preds = [p.decode() + (l - len(p)) * \' \' for p, l in zip(preds, lengths)]\n    print(\'words: {}\'.format(\' \'.join(padded_words)))\n    print(\'preds: {}\'.format(\' \'.join(padded_preds)))\n\n\ndef predict_input_fn(line):\n    # Words\n    words = [w.encode() for w in line.strip().split()]\n    nwords = len(words)\n\n    # Chars\n    chars = [[c.encode() for c in w] for w in line.strip().split()]\n    lengths = [len(c) for c in chars]\n    max_len = max(lengths)\n    chars = [c + [b\'<pad>\'] * (max_len - l) for c, l in zip(chars, lengths)]\n\n    # Wrapping in Tensors\n    words = tf.constant([words], dtype=tf.string)\n    nwords = tf.constant([nwords], dtype=tf.int32)\n    chars = tf.constant([chars], dtype=tf.string)\n    nchars = tf.constant([lengths], dtype=tf.int32)\n\n    return ((words, nwords), (chars, nchars)), None\n\n\nif __name__ == \'__main__\':\n    with Path(PARAMS).open() as f:\n        params = json.load(f)\n\n    params[\'words\'] = str(Path(DATADIR, \'vocab.words.txt\'))\n    params[\'chars\'] = str(Path(DATADIR, \'vocab.chars.txt\'))\n    params[\'tags\'] = str(Path(DATADIR, \'vocab.tags.txt\'))\n    params[\'glove\'] = str(Path(DATADIR, \'glove.npz\'))\n\n    estimator = tf.estimator.Estimator(model_fn, MODELDIR, params=params)\n    predict_inpf = functools.partial(predict_input_fn, LINE)\n    for pred in estimator.predict(predict_inpf):\n        pretty_print(LINE, pred[\'tags_ema\'])\n        break\n'"
models/chars_lstm_lstm_crf_ema/main.py,71,"b'""""""GloVe Embeddings + chars bi-LSTM + bi-LSTM + CRF""""""\n\n__author__ = ""Guillaume Genthial""\n\nimport functools\nimport json\nimport logging\nfrom pathlib import Path\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nfrom tf_metrics import precision, recall, f1\n\nDATADIR = \'../../data/example\'\n\n# Logging\nPath(\'results\').mkdir(exist_ok=True)\ntf.logging.set_verbosity(logging.INFO)\nhandlers = [\n    logging.FileHandler(\'results/main.log\'),\n    logging.StreamHandler(sys.stdout)\n]\nlogging.getLogger(\'tensorflow\').handlers = handlers\n\n\ndef parse_fn(line_words, line_tags):\n    # Encode in Bytes for TF\n    words = [w.encode() for w in line_words.strip().split()]\n    tags = [t.encode() for t in line_tags.strip().split()]\n    assert len(words) == len(tags), ""Words and tags lengths don\'t match""\n\n    # Chars\n    chars = [[c.encode() for c in w] for w in line_words.strip().split()]\n    lengths = [len(c) for c in chars]\n    max_len = max(lengths)\n    chars = [c + [b\'<pad>\'] * (max_len - l) for c, l in zip(chars, lengths)]\n    return ((words, len(words)), (chars, lengths)), tags\n\n\ndef generator_fn(words, tags):\n    with Path(words).open(\'r\') as f_words, Path(tags).open(\'r\') as f_tags:\n        for line_words, line_tags in zip(f_words, f_tags):\n            yield parse_fn(line_words, line_tags)\n\n\ndef input_fn(words, tags, params=None, shuffle_and_repeat=False):\n    params = params if params is not None else {}\n    shapes = ((([None], ()),               # (words, nwords)\n               ([None, None], [None])),    # (chars, nchars)\n              [None])                      # tags\n    types = (((tf.string, tf.int32),\n              (tf.string, tf.int32)),\n             tf.string)\n    defaults = (((\'<pad>\', 0),\n                 (\'<pad>\', 0)),\n                \'O\')\n    dataset = tf.data.Dataset.from_generator(\n        functools.partial(generator_fn, words, tags),\n        output_shapes=shapes, output_types=types)\n\n    if shuffle_and_repeat:\n        dataset = dataset.shuffle(params[\'buffer\']).repeat(params[\'epochs\'])\n\n    dataset = (dataset\n               .padded_batch(params.get(\'batch_size\', 20), shapes, defaults)\n               .prefetch(1))\n    return dataset\n\n\ndef graph_fn(features, labels, mode, params, reuse=None, getter=None):\n    # Read vocabs and inputs\n    num_tags = params[\'num_tags\']\n    (words, nwords), (chars, nchars) = features\n    training = (mode == tf.estimator.ModeKeys.TRAIN)\n    with tf.variable_scope(\'graph\', reuse=reuse, custom_getter=getter):\n        # Read vocabs and inputs\n        dropout = params[\'dropout\']\n        (words, nwords), (chars, nchars) = features\n        training = (mode == tf.estimator.ModeKeys.TRAIN)\n        vocab_words = tf.contrib.lookup.index_table_from_file(\n            params[\'words\'], num_oov_buckets=params[\'num_oov_buckets\'])\n        vocab_chars = tf.contrib.lookup.index_table_from_file(\n            params[\'chars\'], num_oov_buckets=params[\'num_oov_buckets\'])\n        with Path(params[\'chars\']).open() as f:\n            num_chars = sum(1 for _ in f) + params[\'num_oov_buckets\']\n\n        # Char Embeddings\n        char_ids = vocab_chars.lookup(chars)\n        variable = tf.get_variable(\n            \'chars_embeddings\', [num_chars, params[\'dim_chars\']], tf.float32)\n        char_embeddings = tf.nn.embedding_lookup(variable, char_ids)\n        char_embeddings = tf.layers.dropout(char_embeddings, rate=dropout,\n                                            training=training)\n\n        # Char LSTM\n        dim_words = tf.shape(char_embeddings)[1]\n        dim_chars = tf.shape(char_embeddings)[2]\n        flat = tf.reshape(char_embeddings, [-1, dim_chars, params[\'dim_chars\']])\n        t = tf.transpose(flat, perm=[1, 0, 2])\n        lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'char_lstm_size\'])\n        lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'char_lstm_size\'])\n        lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n        _, (_, output_fw) = lstm_cell_fw(t, dtype=tf.float32,\n                                         sequence_length=tf.reshape(nchars, [-1]))\n        _, (_, output_bw) = lstm_cell_bw(t, dtype=tf.float32,\n                                         sequence_length=tf.reshape(nchars, [-1]))\n        output = tf.concat([output_fw, output_bw], axis=-1)\n        char_embeddings = tf.reshape(output, [-1, dim_words, 50])\n\n        # Word Embeddings\n        word_ids = vocab_words.lookup(words)\n        glove = np.load(params[\'glove\'])[\'embeddings\']  # np.array\n        variable = np.vstack([glove, [[0.] * params[\'dim\']]])\n        variable = tf.Variable(variable, dtype=tf.float32, trainable=False)\n        word_embeddings = tf.nn.embedding_lookup(variable, word_ids)\n\n        # Concatenate Word and Char Embeddings\n        embeddings = tf.concat([word_embeddings, char_embeddings], axis=-1)\n        embeddings = tf.layers.dropout(embeddings, rate=dropout, training=training)\n\n        # LSTM\n        t = tf.transpose(embeddings, perm=[1, 0, 2])  # Need time-major\n        lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'lstm_size\'])\n        lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'lstm_size\'])\n        lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n        output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=nwords)\n        output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=nwords)\n        output = tf.concat([output_fw, output_bw], axis=-1)\n        output = tf.transpose(output, perm=[1, 0, 2])\n        output = tf.layers.dropout(output, rate=dropout, training=training)\n\n        # CRF\n        logits = tf.layers.dense(output, num_tags)\n        crf_params = tf.get_variable(""crf"", [num_tags, num_tags], dtype=tf.float32)\n\n    return logits, crf_params\n\n\ndef ema_getter(ema):\n\n    def _ema_getter(getter, name, *args, **kwargs):\n        var = getter(name, *args, **kwargs)\n        ema_var = ema.average(var)\n        return ema_var if ema_var else var\n\n    return _ema_getter\n\n\ndef model_fn(features, labels, mode, params):\n    # For serving features are a bit different\n    if isinstance(features, dict):\n        features = ((features[\'words\'], features[\'nwords\']),\n                    (features[\'chars\'], features[\'nchars\']))\n\n    with Path(params[\'tags\']).open() as f:\n        indices = [idx for idx, tag in enumerate(f) if tag.strip() != \'O\']\n        num_tags = len(indices) + 1\n        params[\'num_tags\'] = num_tags\n\n    # Graph\n    (words, nwords), (chars, nchars) = features\n    logits, crf_params = graph_fn(features, labels, mode, params)\n    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, nwords)\n\n    # Moving Average\n    variables = tf.get_collection(\'trainable_variables\', \'graph\')\n    ema = tf.train.ExponentialMovingAverage(0.999)\n    ema_op = ema.apply(variables)\n    logits_ema, crf_params_ema = graph_fn(\n        features, labels, mode, params, reuse=True, getter=ema_getter(ema))\n    pred_ids_ema, _ = tf.contrib.crf.crf_decode(\n        logits_ema, crf_params_ema, nwords)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        # Predictions\n        reverse_vocab_tags = tf.contrib.lookup.index_to_string_table_from_file(\n            params[\'tags\'])\n        pred_strings = reverse_vocab_tags.lookup(tf.to_int64(pred_ids))\n        pred_strings_ema = reverse_vocab_tags.lookup(tf.to_int64(pred_ids_ema))\n        predictions = {\n            \'pred_ids\': pred_ids,\n            \'tags\': pred_strings,\n            \'pred_ids_ema\': pred_ids_ema,\n            \'tags_ema\': pred_strings_ema,\n        }\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n    else:\n        # Loss\n        vocab_tags = tf.contrib.lookup.index_table_from_file(params[\'tags\'])\n        tags = vocab_tags.lookup(labels)\n        log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n            logits, tags, nwords, crf_params)\n        loss = tf.reduce_mean(-log_likelihood)\n\n        # Metrics\n        weights = tf.sequence_mask(nwords)\n        metrics = {\n            \'acc\': tf.metrics.accuracy(tags, pred_ids, weights),\n            \'acc_ema\': tf.metrics.accuracy(tags, pred_ids_ema, weights),\n            \'pr\': precision(tags, pred_ids, num_tags, indices, weights),\n            \'pr_ema\': precision(tags, pred_ids_ema, num_tags, indices, weights),\n            \'rc\': recall(tags, pred_ids, num_tags, indices, weights),\n            \'rc_ema\': recall(tags, pred_ids_ema, num_tags, indices, weights),\n            \'f1\': f1(tags, pred_ids, num_tags, indices, weights),\n            \'f1_ema\': f1(tags, pred_ids_ema, num_tags, indices, weights),\n        }\n        for metric_name, op in metrics.items():\n            tf.summary.scalar(metric_name, op[1])\n\n        if mode == tf.estimator.ModeKeys.EVAL:\n            return tf.estimator.EstimatorSpec(\n                mode, loss=loss, eval_metric_ops=metrics)\n\n        elif mode == tf.estimator.ModeKeys.TRAIN:\n            train_op = tf.train.AdamOptimizer().minimize(\n                loss, global_step=tf.train.get_or_create_global_step(),\n                var_list=variables)\n            train_op = tf.group([train_op, ema_op])\n            return tf.estimator.EstimatorSpec(\n                mode, loss=loss, train_op=train_op)\n\n\nif __name__ == \'__main__\':\n    # Params\n    params = {\n        \'dim\': 300,\n        \'dim_chars\': 100,\n        \'dropout\': 0.5,\n        \'num_oov_buckets\': 1,\n        \'epochs\': 25,\n        \'batch_size\': 20,\n        \'buffer\': 15000,\n        \'char_lstm_size\': 25,\n        \'lstm_size\': 100,\n        \'words\': str(Path(DATADIR, \'vocab.words.txt\')),\n        \'chars\': str(Path(DATADIR, \'vocab.chars.txt\')),\n        \'tags\': str(Path(DATADIR, \'vocab.tags.txt\')),\n        \'glove\': str(Path(DATADIR, \'glove.npz\'))\n    }\n    with Path(\'results/params.json\').open(\'w\') as f:\n        json.dump(params, f, indent=4, sort_keys=True)\n\n    def fwords(name):\n        return str(Path(DATADIR, \'{}.words.txt\'.format(name)))\n\n    def ftags(name):\n        return str(Path(DATADIR, \'{}.tags.txt\'.format(name)))\n\n    # Estimator, train and evaluate\n    train_inpf = functools.partial(input_fn, fwords(\'train\'), ftags(\'train\'),\n                                   params, shuffle_and_repeat=True)\n    eval_inpf = functools.partial(input_fn, fwords(\'testa\'), ftags(\'testa\'))\n\n    cfg = tf.estimator.RunConfig(save_checkpoints_secs=120)\n    estimator = tf.estimator.Estimator(model_fn, \'results/model\', cfg, params)\n    Path(estimator.eval_dir()).mkdir(parents=True, exist_ok=True)\n    hook = tf.contrib.estimator.stop_if_no_increase_hook(\n        estimator, \'f1_ema\', 500, min_steps=8000, run_every_secs=120)\n    train_spec = tf.estimator.TrainSpec(input_fn=train_inpf, hooks=[hook])\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_inpf, throttle_secs=120)\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\n    # Write predictions to file\n    def write_predictions(name, mode):\n        Path(\'results/score\').mkdir(parents=True, exist_ok=True)\n        with Path(\'results/score/{}.{}.preds.txt\'.format(name, mode)).open(\'wb\') as f:\n            test_inpf = functools.partial(input_fn, fwords(name), ftags(name))\n            golds_gen = generator_fn(fwords(name), ftags(name))\n            preds_gen = estimator.predict(test_inpf)\n            for golds, preds in zip(golds_gen, preds_gen):\n                ((words, _), (_, _)), tags = golds\n                for word, tag, tag_pred in zip(words, tags, preds[mode]):\n                    f.write(b\' \'.join([word, tag, tag_pred]) + b\'\\n\')\n                f.write(b\'\\n\')\n\n    for name in [\'train\', \'testa\', \'testb\']:\n        for mode in [\'tags\', \'tags_ema\']:\n            write_predictions(name, mode)\n'"
models/chars_lstm_lstm_crf_ema/metrics.py,0,"b'""""""Metrics""""""\n\n__author__ = ""Guillaume Genthial""\n\nimport numpy as np\n\n\n# Standard\ntrain = [98.97, 98.21, 98.76, 98.31, 98.64]\ntesta = [94.39, 93.51, 94.37, 94.22, 94.03]\ntestb = [91.17, 90.65, 91.19, 90.89, 91.15]\n\nprint(np.mean(train), np.std(train))\nprint(np.mean(testa), np.std(testa))\nprint(np.mean(testb), np.std(testb))\n\n# EMA\ntrain = [98.80, 98.16, 98.73, 98.27, 98.58]\ntesta = [94.37, 93.71, 94.50, 94.08, 94.33]\ntestb = [91.26, 91.17, 91.14, 91.28, 91.20]\n\nprint(np.mean(train), np.std(train))\nprint(np.mean(testa), np.std(testa))\nprint(np.mean(testb), np.std(testb))\n'"
models/chars_lstm_lstm_crf_ema/serve.py,0,"b'""""""Reload and serve a saved model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nfrom tensorflow.contrib import predictor\n\n\nLINE = \'John lives in New York\'\n\n\ndef parse_fn(line):\n    # Encode in Bytes for TF\n    words = [w.encode() for w in line.strip().split()]\n\n    # Chars\n    chars = [[c.encode() for c in w] for w in line.strip().split()]\n    lengths = [len(c) for c in chars]\n    max_len = max(lengths)\n    chars = [c + [b\'<pad>\'] * (max_len - l) for c, l in zip(chars, lengths)]\n\n    return {\'words\': [words], \'nwords\': [len(words)],\n            \'chars\': [chars], \'nchars\': [lengths]}\n\n\nif __name__ == \'__main__\':\n    export_dir = \'saved_model\'\n    subdirs = [x for x in Path(export_dir).iterdir()\n               if x.is_dir() and \'temp\' not in str(x)]\n    latest = str(sorted(subdirs)[-1])\n    predict_fn = predictor.from_saved_model(latest)\n    predictions = predict_fn(parse_fn(LINE))\n    print(predictions)\n'"
models/lstm_crf/export.py,5,"b'""""""Export model as a saved_model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nimport json\n\nimport tensorflow as tf\n\nfrom main import model_fn\n\nDATADIR = \'../../data/example\'\nPARAMS = \'./results/params.json\'\nMODELDIR = \'./results/model\'\n\n\ndef serving_input_receiver_fn():\n    """"""Serving input_fn that builds features from placeholders\n\n    Returns\n    -------\n    tf.estimator.export.ServingInputReceiver\n    """"""\n    words = tf.placeholder(dtype=tf.string, shape=[None, None], name=\'words\')\n    nwords = tf.placeholder(dtype=tf.int32, shape=[None], name=\'nwords\')\n    receiver_tensors = {\'words\': words, \'nwords\': nwords}\n    features = {\'words\': words, \'nwords\': nwords}\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n\n\nif __name__ == \'__main__\':\n    with Path(PARAMS).open() as f:\n        params = json.load(f)\n\n    params[\'words\'] = str(Path(DATADIR, \'vocab.words.txt\'))\n    params[\'chars\'] = str(Path(DATADIR, \'vocab.chars.txt\'))\n    params[\'tags\'] = str(Path(DATADIR, \'vocab.tags.txt\'))\n    params[\'glove\'] = str(Path(DATADIR, \'glove.npz\'))\n\n    estimator = tf.estimator.Estimator(model_fn, MODELDIR, params=params)\n    estimator.export_saved_model(\'saved_model\', serving_input_receiver_fn)\n'"
models/lstm_crf/interact.py,3,"b'""""""Interact with a model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nimport functools\nimport json\n\nimport tensorflow as tf\n\nfrom main import model_fn\n\nLINE = \'John lives in New York\'\nDATADIR = \'../../data/example\'\nPARAMS = \'./results/params.json\'\nMODELDIR = \'./results/model\'\n\n\ndef pretty_print(line, preds):\n    words = line.strip().split()\n    lengths = [max(len(w), len(p)) for w, p in zip(words, preds)]\n    padded_words = [w + (l - len(w)) * \' \' for w, l in zip(words, lengths)]\n    padded_preds = [p.decode() + (l - len(p)) * \' \' for p, l in zip(preds, lengths)]\n    print(\'words: {}\'.format(\' \'.join(padded_words)))\n    print(\'preds: {}\'.format(\' \'.join(padded_preds)))\n\n\ndef predict_input_fn(line):\n    # Words\n    words = [w.encode() for w in line.strip().split()]\n    nwords = len(words)\n\n    # Wrapping in Tensors\n    words = tf.constant([words], dtype=tf.string)\n    nwords = tf.constant([nwords], dtype=tf.int32)\n\n    return (words, nwords), None\n\n\nif __name__ == \'__main__\':\n    with Path(PARAMS).open() as f:\n        params = json.load(f)\n\n    params[\'words\'] = str(Path(DATADIR, \'vocab.words.txt\'))\n    params[\'chars\'] = str(Path(DATADIR, \'vocab.chars.txt\'))\n    params[\'tags\'] = str(Path(DATADIR, \'vocab.tags.txt\'))\n    params[\'glove\'] = str(Path(DATADIR, \'glove.npz\'))\n\n    estimator = tf.estimator.Estimator(model_fn, MODELDIR, params=params)\n    predict_inpf = functools.partial(predict_input_fn, LINE)\n    for pred in estimator.predict(predict_inpf):\n        pretty_print(LINE, pred[\'tags\'])\n        break\n'"
models/lstm_crf/main.py,42,"b'""""""GloVe Embeddings + bi-LSTM + CRF""""""\n\n__author__ = ""Guillaume Genthial""\n\nimport functools\nimport json\nimport logging\nfrom pathlib import Path\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nfrom tf_metrics import precision, recall, f1\n\nDATADIR = \'../../data/example\'\n\n# Logging\nPath(\'results\').mkdir(exist_ok=True)\ntf.logging.set_verbosity(logging.INFO)\nhandlers = [\n    logging.FileHandler(\'results/main.log\'),\n    logging.StreamHandler(sys.stdout)\n]\nlogging.getLogger(\'tensorflow\').handlers = handlers\n\n\ndef parse_fn(line_words, line_tags):\n    # Encode in Bytes for TF\n    words = [w.encode() for w in line_words.strip().split()]\n    tags = [t.encode() for t in line_tags.strip().split()]\n    assert len(words) == len(tags), ""Words and tags lengths don\'t match""\n    return (words, len(words)), tags\n\n\ndef generator_fn(words, tags):\n    with Path(words).open(\'r\') as f_words, Path(tags).open(\'r\') as f_tags:\n        for line_words, line_tags in zip(f_words, f_tags):\n            yield parse_fn(line_words, line_tags)\n\n\ndef input_fn(words, tags, params=None, shuffle_and_repeat=False):\n    params = params if params is not None else {}\n    shapes = (([None], ()), [None])\n    types = ((tf.string, tf.int32), tf.string)\n    defaults = ((\'<pad>\', 0), \'O\')\n\n    dataset = tf.data.Dataset.from_generator(\n        functools.partial(generator_fn, words, tags),\n        output_shapes=shapes, output_types=types)\n\n    if shuffle_and_repeat:\n        dataset = dataset.shuffle(params[\'buffer\']).repeat(params[\'epochs\'])\n\n    dataset = (dataset\n               .padded_batch(params.get(\'batch_size\', 20), shapes, defaults)\n               .prefetch(1))\n    return dataset\n\n\ndef model_fn(features, labels, mode, params):\n    # For serving, features are a bit different\n    if isinstance(features, dict):\n        features = features[\'words\'], features[\'nwords\']\n\n    # Read vocabs and inputs\n    dropout = params[\'dropout\']\n    words, nwords = features\n    training = (mode == tf.estimator.ModeKeys.TRAIN)\n    vocab_words = tf.contrib.lookup.index_table_from_file(\n        params[\'words\'], num_oov_buckets=params[\'num_oov_buckets\'])\n    with Path(params[\'tags\']).open() as f:\n        indices = [idx for idx, tag in enumerate(f) if tag.strip() != \'O\']\n        num_tags = len(indices) + 1\n\n    # Word Embeddings\n    word_ids = vocab_words.lookup(words)\n    glove = np.load(params[\'glove\'])[\'embeddings\']  # np.array\n    variable = np.vstack([glove, [[0.]*params[\'dim\']]])\n    variable = tf.Variable(variable, dtype=tf.float32, trainable=False)\n    embeddings = tf.nn.embedding_lookup(variable, word_ids)\n    embeddings = tf.layers.dropout(embeddings, rate=dropout, training=training)\n\n    # LSTM\n    t = tf.transpose(embeddings, perm=[1, 0, 2])\n    lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'lstm_size\'])\n    lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'lstm_size\'])\n    lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n    output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=nwords)\n    output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=nwords)\n    output = tf.concat([output_fw, output_bw], axis=-1)\n    output = tf.transpose(output, perm=[1, 0, 2])\n    output = tf.layers.dropout(output, rate=dropout, training=training)\n\n    # CRF\n    logits = tf.layers.dense(output, num_tags)\n    crf_params = tf.get_variable(""crf"", [num_tags, num_tags], dtype=tf.float32)\n    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, nwords)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        # Predictions\n        reverse_vocab_tags = tf.contrib.lookup.index_to_string_table_from_file(\n            params[\'tags\'])\n        pred_strings = reverse_vocab_tags.lookup(tf.to_int64(pred_ids))\n        predictions = {\n            \'pred_ids\': pred_ids,\n            \'tags\': pred_strings\n        }\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n    else:\n        # Loss\n        vocab_tags = tf.contrib.lookup.index_table_from_file(params[\'tags\'])\n        tags = vocab_tags.lookup(labels)\n        log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n            logits, tags, nwords, crf_params)\n        loss = tf.reduce_mean(-log_likelihood)\n\n        # Metrics\n        weights = tf.sequence_mask(nwords)\n        metrics = {\n            \'acc\': tf.metrics.accuracy(tags, pred_ids, weights),\n            \'precision\': precision(tags, pred_ids, num_tags, indices, weights),\n            \'recall\': recall(tags, pred_ids, num_tags, indices, weights),\n            \'f1\': f1(tags, pred_ids, num_tags, indices, weights),\n        }\n        for metric_name, op in metrics.items():\n            tf.summary.scalar(metric_name, op[1])\n\n        if mode == tf.estimator.ModeKeys.EVAL:\n            return tf.estimator.EstimatorSpec(\n                mode, loss=loss, eval_metric_ops=metrics)\n\n        elif mode == tf.estimator.ModeKeys.TRAIN:\n            train_op = tf.train.AdamOptimizer().minimize(\n                loss, global_step=tf.train.get_or_create_global_step())\n            return tf.estimator.EstimatorSpec(\n                mode, loss=loss, train_op=train_op)\n\n\nif __name__ == \'__main__\':\n    # Params\n    params = {\n        \'dim\': 300,\n        \'dropout\': 0.5,\n        \'num_oov_buckets\': 1,\n        \'epochs\': 25,\n        \'batch_size\': 20,\n        \'buffer\': 15000,\n        \'lstm_size\': 100,\n        \'words\': str(Path(DATADIR, \'vocab.words.txt\')),\n        \'chars\': str(Path(DATADIR, \'vocab.chars.txt\')),\n        \'tags\': str(Path(DATADIR, \'vocab.tags.txt\')),\n        \'glove\': str(Path(DATADIR, \'glove.npz\'))\n    }\n    with Path(\'results/params.json\').open(\'w\') as f:\n        json.dump(params, f, indent=4, sort_keys=True)\n\n    def fwords(name):\n        return str(Path(DATADIR, \'{}.words.txt\'.format(name)))\n\n    def ftags(name):\n        return str(Path(DATADIR, \'{}.tags.txt\'.format(name)))\n\n    # Estimator, train and evaluate\n    train_inpf = functools.partial(input_fn, fwords(\'train\'), ftags(\'train\'),\n                                   params, shuffle_and_repeat=True)\n    eval_inpf = functools.partial(input_fn, fwords(\'testa\'), ftags(\'testa\'))\n\n    cfg = tf.estimator.RunConfig(save_checkpoints_secs=120)\n    estimator = tf.estimator.Estimator(model_fn, \'results/model\', cfg, params)\n    Path(estimator.eval_dir()).mkdir(parents=True, exist_ok=True)\n    hook = tf.contrib.estimator.stop_if_no_increase_hook(\n        estimator, \'f1\', 500, min_steps=8000, run_every_secs=120)\n    train_spec = tf.estimator.TrainSpec(input_fn=train_inpf, hooks=[hook])\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_inpf, throttle_secs=120)\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\n    # Write predictions to file\n    def write_predictions(name):\n        Path(\'results/score\').mkdir(parents=True, exist_ok=True)\n        with Path(\'results/score/{}.preds.txt\'.format(name)).open(\'wb\') as f:\n            test_inpf = functools.partial(input_fn, fwords(name), ftags(name))\n            golds_gen = generator_fn(fwords(name), ftags(name))\n            preds_gen = estimator.predict(test_inpf)\n            for golds, preds in zip(golds_gen, preds_gen):\n                ((words, _), tags) = golds\n                for word, tag, tag_pred in zip(words, tags, preds[\'tags\']):\n                    f.write(b\' \'.join([word, tag, tag_pred]) + b\'\\n\')\n                f.write(b\'\\n\')\n\n    for name in [\'train\', \'testa\', \'testb\']:\n        write_predictions(name)\n'"
models/lstm_crf/metrics.py,0,"b'""""""Metrics""""""\n\n__author__ = ""Guillaume Genthial""\n\nimport numpy as np\n\n\ntrain = [98.78, 98.45, 99.05, 98.97, 99.00]\ntesta = [93.61, 93.81, 93.73, 93.76, 93.49]\ntestb = [90.34, 90.61, 90.39, 90.33, 90.43]\n\nprint(np.mean(train), np.std(train))\nprint(np.mean(testa), np.std(testa))\nprint(np.mean(testb), np.std(testb))\n'"
models/lstm_crf/serve.py,0,"b'""""""Reload and serve a saved model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nfrom tensorflow.contrib import predictor\n\n\nLINE = \'John lives in New York\'\n\nif __name__ == \'__main__\':\n    export_dir = \'saved_model\'\n    subdirs = [x for x in Path(export_dir).iterdir()\n               if x.is_dir() and \'temp\' not in str(x)]\n    latest = str(sorted(subdirs)[-1])\n    predict_fn = predictor.from_saved_model(latest)\n    words = [w.encode() for w in LINE.split()]\n    nwords = len(words)\n    predictions = predict_fn({\'words\': [words], \'nwords\': [nwords]})\n    print(predictions)\n'"
models/lstm_crf_ema/export.py,5,"b'""""""Export model as a saved_model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nimport json\n\nimport tensorflow as tf\n\nfrom main import model_fn\n\nDATADIR = \'../../data/example\'\nPARAMS = \'./results/params.json\'\nMODELDIR = \'./results/model\'\n\n\ndef serving_input_receiver_fn():\n    """"""Serving input_fn that builds features from placeholders\n\n    Returns\n    -------\n    tf.estimator.export.ServingInputReceiver\n    """"""\n    words = tf.placeholder(dtype=tf.string, shape=[None, None], name=\'words\')\n    nwords = tf.placeholder(dtype=tf.int32, shape=[None], name=\'nwords\')\n    receiver_tensors = {\'words\': words, \'nwords\': nwords}\n    features = {\'words\': words, \'nwords\': nwords}\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n\n\nif __name__ == \'__main__\':\n    with Path(PARAMS).open() as f:\n        params = json.load(f)\n\n    params[\'words\'] = str(Path(DATADIR, \'vocab.words.txt\'))\n    params[\'chars\'] = str(Path(DATADIR, \'vocab.chars.txt\'))\n    params[\'tags\'] = str(Path(DATADIR, \'vocab.tags.txt\'))\n    params[\'glove\'] = str(Path(DATADIR, \'glove.npz\'))\n\n    estimator = tf.estimator.Estimator(model_fn, MODELDIR, params=params)\n    estimator.export_saved_model(\'saved_model\', serving_input_receiver_fn)\n'"
models/lstm_crf_ema/interact.py,3,"b'""""""Interact with a model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nimport functools\nimport json\n\nimport tensorflow as tf\n\nfrom main import model_fn\n\nLINE = \'John lives in New York\'\nDATADIR = \'../../data/example\'\nPARAMS = \'./results/params.json\'\nMODELDIR = \'./results/model\'\n\n\ndef pretty_print(line, preds):\n    words = line.strip().split()\n    lengths = [max(len(w), len(p)) for w, p in zip(words, preds)]\n    padded_words = [w + (l - len(w)) * \' \' for w, l in zip(words, lengths)]\n    padded_preds = [p.decode() + (l - len(p)) * \' \' for p, l in zip(preds, lengths)]\n    print(\'words: {}\'.format(\' \'.join(padded_words)))\n    print(\'preds: {}\'.format(\' \'.join(padded_preds)))\n\n\ndef predict_input_fn(line):\n    # Words\n    words = [w.encode() for w in line.strip().split()]\n    nwords = len(words)\n\n    # Wrapping in Tensors\n    words = tf.constant([words], dtype=tf.string)\n    nwords = tf.constant([nwords], dtype=tf.int32)\n\n    return (words, nwords), None\n\n\nif __name__ == \'__main__\':\n    with Path(PARAMS).open() as f:\n        params = json.load(f)\n\n    params[\'words\'] = str(Path(DATADIR, \'vocab.words.txt\'))\n    params[\'chars\'] = str(Path(DATADIR, \'vocab.chars.txt\'))\n    params[\'tags\'] = str(Path(DATADIR, \'vocab.tags.txt\'))\n    params[\'glove\'] = str(Path(DATADIR, \'glove.npz\'))\n\n    estimator = tf.estimator.Estimator(model_fn, MODELDIR, params=params)\n    predict_inpf = functools.partial(predict_input_fn, LINE)\n    for pred in estimator.predict(predict_inpf):\n        pretty_print(LINE, pred[\'tags_ema\'])\n        break\n'"
models/lstm_crf_ema/main.py,50,"b'""""""GloVe Embeddings + bi-LSTM + CRF""""""\n\n__author__ = ""Guillaume Genthial""\n\nimport functools\nimport json\nimport logging\nfrom pathlib import Path\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nfrom tf_metrics import precision, recall, f1\n\nDATADIR = \'../../data/example\'\n\n# Logging\nPath(\'results\').mkdir(exist_ok=True)\ntf.logging.set_verbosity(logging.INFO)\nhandlers = [\n    logging.FileHandler(\'results/main.log\'),\n    logging.StreamHandler(sys.stdout)\n]\nlogging.getLogger(\'tensorflow\').handlers = handlers\n\n\ndef parse_fn(line_words, line_tags):\n    # Encode in Bytes for TF\n    words = [w.encode() for w in line_words.strip().split()]\n    tags = [t.encode() for t in line_tags.strip().split()]\n    assert len(words) == len(tags), ""Words and tags lengths don\'t match""\n    return (words, len(words)), tags\n\n\ndef generator_fn(words, tags):\n    with Path(words).open(\'r\') as f_words, Path(tags).open(\'r\') as f_tags:\n        for line_words, line_tags in zip(f_words, f_tags):\n            yield parse_fn(line_words, line_tags)\n\n\ndef input_fn(words, tags, params=None, shuffle_and_repeat=False):\n    params = params if params is not None else {}\n    shapes = (([None], ()), [None])\n    types = ((tf.string, tf.int32), tf.string)\n    defaults = ((\'<pad>\', 0), \'O\')\n\n    dataset = tf.data.Dataset.from_generator(\n        functools.partial(generator_fn, words, tags),\n        output_shapes=shapes, output_types=types)\n\n    if shuffle_and_repeat:\n        dataset = dataset.shuffle(params[\'buffer\']).repeat(params[\'epochs\'])\n\n    dataset = (dataset\n               .padded_batch(params.get(\'batch_size\', 20), shapes, defaults)\n               .prefetch(1))\n    return dataset\n\n\ndef graph_fn(features, labels, mode, params, reuse=None, getter=None):\n    # Read vocabs and inputs\n    num_tags = params[\'num_tags\']\n    words, nwords = features\n    training = (mode == tf.estimator.ModeKeys.TRAIN)\n    with tf.variable_scope(\'graph\', reuse=reuse, custom_getter=getter):\n        # Read vocabs and inputs\n        dropout = params[\'dropout\']\n        words, nwords = features\n        training = (mode == tf.estimator.ModeKeys.TRAIN)\n        vocab_words = tf.contrib.lookup.index_table_from_file(\n            params[\'words\'], num_oov_buckets=params[\'num_oov_buckets\'])\n\n        # Word Embeddings\n        word_ids = vocab_words.lookup(words)\n        glove = np.load(params[\'glove\'])[\'embeddings\']  # np.array\n        variable = np.vstack([glove, [[0.]*params[\'dim\']]])\n        variable = tf.Variable(variable, dtype=tf.float32, trainable=False)\n        embeddings = tf.nn.embedding_lookup(variable, word_ids)\n        embeddings = tf.layers.dropout(embeddings, rate=dropout, training=training)\n\n        # LSTM\n        t = tf.transpose(embeddings, perm=[1, 0, 2])\n        lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'lstm_size\'])\n        lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params[\'lstm_size\'])\n        lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n        output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=nwords)\n        output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=nwords)\n        output = tf.concat([output_fw, output_bw], axis=-1)\n        output = tf.transpose(output, perm=[1, 0, 2])\n        output = tf.layers.dropout(output, rate=dropout, training=training)\n\n        # CRF\n        logits = tf.layers.dense(output, num_tags)\n        crf_params = tf.get_variable(""crf"", [num_tags, num_tags], dtype=tf.float32)\n\n    return logits, crf_params\n\n\ndef ema_getter(ema):\n\n    def _ema_getter(getter, name, *args, **kwargs):\n        var = getter(name, *args, **kwargs)\n        ema_var = ema.average(var)\n        return ema_var if ema_var else var\n\n    return _ema_getter\n\n\ndef model_fn(features, labels, mode, params):\n    # For serving, features are a bit different\n    if isinstance(features, dict):\n        features = features[\'words\'], features[\'nwords\']\n\n    with Path(params[\'tags\']).open() as f:\n        indices = [idx for idx, tag in enumerate(f) if tag.strip() != \'O\']\n        num_tags = len(indices) + 1\n        params[\'num_tags\'] = num_tags\n\n    # Graph\n    words, nwords = features\n    logits, crf_params = graph_fn(features, labels, mode, params)\n    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, nwords)\n\n    # Moving Average\n    variables = tf.get_collection(\'trainable_variables\', \'graph\')\n    ema = tf.train.ExponentialMovingAverage(0.999)\n    ema_op = ema.apply(variables)\n    logits_ema, crf_params_ema = graph_fn(\n        features, labels, mode, params, reuse=True, getter=ema_getter(ema))\n    pred_ids_ema, _ = tf.contrib.crf.crf_decode(\n        logits_ema, crf_params_ema, nwords)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        # Predictions\n        reverse_vocab_tags = tf.contrib.lookup.index_to_string_table_from_file(\n            params[\'tags\'])\n        pred_strings = reverse_vocab_tags.lookup(tf.to_int64(pred_ids))\n        pred_strings_ema = reverse_vocab_tags.lookup(tf.to_int64(pred_ids_ema))\n        predictions = {\n            \'pred_ids\': pred_ids,\n            \'tags\': pred_strings,\n            \'pred_ids_ema\': pred_ids_ema,\n            \'tags_ema\': pred_strings_ema,\n        }\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n    else:\n        # Loss\n        vocab_tags = tf.contrib.lookup.index_table_from_file(params[\'tags\'])\n        tags = vocab_tags.lookup(labels)\n        log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n            logits, tags, nwords, crf_params)\n        loss = tf.reduce_mean(-log_likelihood)\n\n        # Metrics\n        weights = tf.sequence_mask(nwords)\n        metrics = {\n            \'acc\': tf.metrics.accuracy(tags, pred_ids, weights),\n            \'acc_ema\': tf.metrics.accuracy(tags, pred_ids_ema, weights),\n            \'pr\': precision(tags, pred_ids, num_tags, indices, weights),\n            \'pr_ema\': precision(tags, pred_ids_ema, num_tags, indices, weights),\n            \'rc\': recall(tags, pred_ids, num_tags, indices, weights),\n            \'rc_ema\': recall(tags, pred_ids_ema, num_tags, indices, weights),\n            \'f1\': f1(tags, pred_ids, num_tags, indices, weights),\n            \'f1_ema\': f1(tags, pred_ids_ema, num_tags, indices, weights),\n        }\n        for metric_name, op in metrics.items():\n            tf.summary.scalar(metric_name, op[1])\n\n        if mode == tf.estimator.ModeKeys.EVAL:\n            return tf.estimator.EstimatorSpec(\n                mode, loss=loss, eval_metric_ops=metrics)\n\n        elif mode == tf.estimator.ModeKeys.TRAIN:\n            train_op = tf.train.AdamOptimizer().minimize(\n                loss, global_step=tf.train.get_or_create_global_step(),\n                var_list=variables)\n            train_op = tf.group([train_op, ema_op])\n            return tf.estimator.EstimatorSpec(\n                mode, loss=loss, train_op=train_op)\n\n\nif __name__ == \'__main__\':\n    # Params\n    params = {\n        \'dim\': 300,\n        \'dropout\': 0.5,\n        \'num_oov_buckets\': 1,\n        \'epochs\': 25,\n        \'batch_size\': 20,\n        \'buffer\': 15000,\n        \'lstm_size\': 100,\n        \'words\': str(Path(DATADIR, \'vocab.words.txt\')),\n        \'chars\': str(Path(DATADIR, \'vocab.chars.txt\')),\n        \'tags\': str(Path(DATADIR, \'vocab.tags.txt\')),\n        \'glove\': str(Path(DATADIR, \'glove.npz\'))\n    }\n    with Path(\'results/params.json\').open(\'w\') as f:\n        json.dump(params, f, indent=4, sort_keys=True)\n\n    def fwords(name):\n        return str(Path(DATADIR, \'{}.words.txt\'.format(name)))\n\n    def ftags(name):\n        return str(Path(DATADIR, \'{}.tags.txt\'.format(name)))\n\n    # Estimator, train and evaluate\n    train_inpf = functools.partial(input_fn, fwords(\'train\'), ftags(\'train\'),\n                                   params, shuffle_and_repeat=True)\n    eval_inpf = functools.partial(input_fn, fwords(\'testa\'), ftags(\'testa\'))\n\n    cfg = tf.estimator.RunConfig(save_checkpoints_secs=120)\n    estimator = tf.estimator.Estimator(model_fn, \'results/model\', cfg, params)\n    Path(estimator.eval_dir()).mkdir(parents=True, exist_ok=True)\n    hook = tf.contrib.estimator.stop_if_no_increase_hook(\n        estimator, \'f1_ema\', 500, min_steps=8000, run_every_secs=120)\n    train_spec = tf.estimator.TrainSpec(input_fn=train_inpf, hooks=[hook])\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_inpf, throttle_secs=120)\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\n    # Write predictions to file\n    def write_predictions(name, mode):\n        Path(\'results/score\').mkdir(parents=True, exist_ok=True)\n        with Path(\'results/score/{}.{}.preds.txt\'.format(name, mode)).open(\'wb\') as f:\n            test_inpf = functools.partial(input_fn, fwords(name), ftags(name))\n            golds_gen = generator_fn(fwords(name), ftags(name))\n            preds_gen = estimator.predict(test_inpf)\n            for golds, preds in zip(golds_gen, preds_gen):\n                ((words, _), tags) = golds\n                for word, tag, tag_pred in zip(words, tags, preds[mode]):\n                    f.write(b\' \'.join([word, tag, tag_pred]) + b\'\\n\')\n                f.write(b\'\\n\')\n\n    for name in [\'train\', \'testa\', \'testb\']:\n        for mode in [\'tags\', \'tags_ema\']:\n            write_predictions(name, mode)\n'"
models/lstm_crf_ema/metrics.py,0,"b'""""""Metrics""""""\n\n__author__ = ""Guillaume Genthial""\n\nimport numpy as np\n\n\n# Standard\ntrain = [97.86, 98.93, 98.90, 98.77, 99.04]\ntesta = [93.14, 93.71, 93.59, 93.91, 93.70]\ntestb = [90.03, 90.43, 90.18, 90.49, 90.76]\n\nprint(np.mean(train), np.std(train))\nprint(np.mean(testa), np.std(testa))\nprint(np.mean(testb), np.std(testb))\n\n# EMA\ntrain = [97.77, 98.98, 98.97, 98.82, 99.00]\ntesta = [93.40, 93.71, 94.04, 94.06, 93.82]\ntestb = [90.17, 90.70, 90.45, 90.43, 90.75]\n\nprint(np.mean(train), np.std(train))\nprint(np.mean(testa), np.std(testa))\nprint(np.mean(testb), np.std(testb))\n'"
models/lstm_crf_ema/serve.py,0,"b'""""""Reload and serve a saved model""""""\n\n__author__ = ""Guillaume Genthial""\n\nfrom pathlib import Path\nfrom tensorflow.contrib import predictor\n\n\nLINE = \'John lives in New York\'\n\nif __name__ == \'__main__\':\n    export_dir = \'saved_model\'\n    subdirs = [x for x in Path(export_dir).iterdir()\n               if x.is_dir() and \'temp\' not in str(x)]\n    latest = str(sorted(subdirs)[-1])\n    predict_fn = predictor.from_saved_model(latest)\n    words = [w.encode() for w in LINE.split()]\n    nwords = len(words)\n    predictions = predict_fn({\'words\': [words], \'nwords\': [nwords]})\n    print(predictions)\n'"
