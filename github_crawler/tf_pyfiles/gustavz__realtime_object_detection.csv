file_path,api_count,code
allmodels_image.py,0,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Jan 10 09:45:23 2018\n\n@author: www.github.com/GustavZ\n""""""\n\nimport os\nimport sys\nimport numpy as np\n\nfrom rod.config import Config\nfrom rod.helper import get_model_list, check_if_optimized_model\nfrom rod.model import ObjectDetectionModel, DeepLabModel\n\nROOT_DIR = os.getcwd()\n#MODELS_DIR = os.path.join(ROOT_DIR,\'models\')\nMODELS_DIR = \'/home/gustav/workspace/eetfm_automation/nmsspeed_test\'\nINPUT_TYPE = \'image\'\n\ndef create_test_config(type,model_name, optimized=False, single_class=False):\n        class TestConfig(Config):\n            OD_MODEL_PATH=MODELS_DIR+\'/\'+model_name+\'/{}\'\n            DL_MODEL_PATH=MODELS_DIR+\'/\'+model_name+\'/{}\'\n            OD_MODEL_NAME=model_name\n            DL_MODEL_NAME=model_name\n            VISUALIZE=False\n            SPLIT_MODEL = False\n            WRITE_TIMELINE = True\n            LIMIT_IMAGES = 11\n            if optimized:\n                USE_OPTIMIZED=True\n            else:\n                USE_OPTIMIZED=False\n            if single_class:\n                NUM_CLASSES=1\n            else:\n                NUM_CLASSES=90\n            def __init__(self):\n                super(TestConfig, self).__init__(type)\n        return TestConfig()\n\n# Read sequentail Models or Gather all Models from models/\nconfig = Config(\'od\')\nif config.SEQ_MODELS:\n    model_names = config.SEQ_MODELS\nelse:\n    model_names = get_model_list(MODELS_DIR)\n\n# Sequential testing\nfor model_name in model_names:\n    print(""> testing model: {}"".format(model_name))\n    # conditionals\n    optimized=False\n    single_class=False\n    # Test Model\n    if \'hands\' in model_name or \'person\' in model_name:\n        single_class=True\n    if \'deeplab\' in model_name:\n        config = create_test_config(\'dl\',model_name,optimized,single_class)\n        model = DeepLabModel(config).prepare_model(INPUT_TYPE)\n    else:\n        config = create_test_config(\'od\',model_name,optimized,single_class)\n        model = ObjectDetectionModel(config).prepare_model(INPUT_TYPE)\n\n    # Check if there is an optimized graph\n    model_dir =  os.path.join(os.getcwd(),\'models\',model_name)\n    optimized = check_if_optimized_model(model_dir)\n\n    # Again for the optimized graph\n    if optimized:\n        if \'deeplab\' in model_name:\n            config = create_test_config(\'dl\',model_name,optimized,single_class)\n            model = DeepLabModel(config).prepare_model(INPUT_TYPE)\n        else:\n            config = create_test_config(\'od\',model_name,optimized,single_class)\n            model = ObjectDetectionModel(config).prepare_model(INPUT_TYPE)\n\n    model.run()\n'"
deeplab_image.py,0,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Dec 21 12:01:40 2017\n\n@author: www.github.com/GustavZ\n""""""\nfrom rod.model import DeepLabModel\nfrom rod.config import Config\n\n\ndef main():\n    model_type = \'dl\'\n    input_type = \'image\'\n    config = Config(model_type)\n    model = DeepLabModel(config).prepare_model(input_type)\n    model.run()\n\nif __name__ == \'__main__\':\n    main()\n'"
deeplab_video.py,0,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Dec 21 12:01:40 2017\n\n@author: www.github.com/GustavZ\n""""""\nfrom rod.model import DeepLabModel\nfrom rod.config import Config\n\n\ndef main():\n    model_type = \'dl\'\n    input_type = \'video\'\n    config = Config(model_type)\n    model = DeepLabModel(config).prepare_model(input_type)\n    model.run()\n\nif __name__ == \'__main__\':\n    main()\n'"
objectdetection_image.py,0,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Dec 21 12:01:40 2017\n\n@author: www.github.com/GustavZ\n""""""\nfrom rod.model import ObjectDetectionModel\nfrom rod.config import Config\n\n\ndef main():\n    model_type = \'od\'\n    input_type = \'image\'\n    config = Config(model_type)\n    model = ObjectDetectionModel(config).prepare_model(input_type)\n    model.run()\n\nif __name__ == \'__main__\':\n    main()\n'"
objectdetection_video.py,0,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Dec 21 12:01:40 2017\n\n@author: www.github.com/GustavZ\n""""""\nfrom rod.model import ObjectDetectionModel\nfrom rod.config import Config\n\n\ndef main():\n    model_type = \'od\'\n    input_type = \'video\'\n    config = Config(model_type)\n    model = ObjectDetectionModel(config).prepare_model(input_type)\n    model.run()\n\nif __name__ == \'__main__\':\n    main()\n'"
rod/__init__.py,0,b''
rod/config.py,0,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\n@author: www.github.com/GustavZ\n""""""\n\nimport numpy as np\nimport yaml\nimport os\nimport sys\n\n## LOAD CONFIG PARAMS ##\nif (os.path.isfile(\'config.yml\')):\n    with open(""config.yml"", \'r\') as ymlfile:\n        cfg = yaml.load(ymlfile)\nelse:\n    with open(""config.sample.yml"", \'r\') as ymlfile:\n        cfg = yaml.load(ymlfile)\n\nclass Config(object):\n    """"""\n    Inference Configuration class\n    loads Params from \'config.sample.yml\'\n    """"""\n    ### Inference Config\n    VIDEO_INPUT = cfg[\'VIDEO_INPUT\']        # Input Must be OpenCV readable\n    ROS_INPUT = cfg[\'ROS_INPUT\']            # ROS Image Topic\n    VISUALIZE = cfg[\'VISUALIZE\']            # Disable for performance increase\n    VIS_FPS = cfg[\'VIS_FPS\']                # Draw current FPS in the top left Image corner\n    CPU_ONLY = cfg[\'CPU_ONLY\']              # CPU Placement for speed test\n    USE_OPTIMIZED = cfg[\'USE_OPTIMIZED\']    # whether to use the optimized model (only possible if transform with script)\n    DISCO_MODE = cfg[\'DISCO_MODE\']          # Secret Disco Visualization Mode\n    DOWNLOAD_MODEL = cfg[\'DOWNLOAD_MODEL\']  # Only for Models available at the TF model_zoo\n\n\n    ### Testing\n    IMAGE_PATH = cfg[\'IMAGE_PATH\']          # path for test.py test_images\n    LIMIT_IMAGES = cfg[\'LIMIT_IMAGES\']      # if set to None, all images are used\n    WRITE_TIMELINE = cfg[\'WRITE_TIMELINE\']  # write json timeline file (slows infrence)\n    SAVE_RESULT = cfg[\'SAVE_RESULT\']        # save detection results to disk\n    RESULT_PATH = cfg[\'RESULT_PATH\']        # path to save detection results\n    SEQ_MODELS = cfg[\'SEQ_MODELS\']          # List of Models to sequentially test (Default all Models)\n\n\n    ### Object_Detection\n    WIDTH = cfg[\'WIDTH\']                    # OpenCV only supports 4:3 formats others will be converted\n    HEIGHT = cfg[\'HEIGHT\']                  # 600x600 leads to 640x480\n    MAX_FRAMES = cfg[\'MAX_FRAMES\']          # only used if visualize==False\n    FPS_INTERVAL = cfg[\'FPS_INTERVAL\']      # Interval [s] to print fps of the last interval in console\n    PRINT_INTERVAL = cfg[\'PRINT_INTERVAL\']  # intervall [frames] to print detections to console\n    PRINT_TH = cfg[\'PRINT_TH\']              # detection threshold for det_intervall\n    ## speed hack\n    SPLIT_MODEL = cfg[\'SPLIT_MODEL\']        # Splits Model into a GPU and CPU session (currently only works for ssd_mobilenets)\n    MULTI_THREADING = cfg[\'MULTI_THREADING\']# Additional Split Model Speed up through multi threading\n    SSD_SHAPE = cfg[\'SSD_SHAPE\']            # used for the split model algorithm (currently only supports ssd networks trained on 300x300 and 600x600 input)\n    SPLIT_NODES = cfg[\'SPLIT_NODES\']       # hardcoded split points for ssd_mobilenet_v1\n    ## Tracking\n    USE_TRACKER = cfg[\'USE_TRACKER\']        # Use a Tracker (currently only works properly WITHOUT split_model)\n    TRACKER_FRAMES = cfg[\'TRACKER_FRAMES\']  # Number of tracked frames between detections\n    NUM_TRACKERS = cfg[\'NUM_TRACKERS\']      # Max number of objects to track\n    ## Model\n    OD_MODEL_NAME = cfg[\'OD_MODEL_NAME\']    # Only used for downloading the correct Model Version\n    OD_MODEL_PATH = cfg[\'OD_MODEL_PATH\']\n    LABEL_PATH = cfg[\'LABEL_PATH\']\n    NUM_CLASSES =  cfg[\'NUM_CLASSES\']\n\n    ### DeepLab\n    ALPHA = cfg[\'ALPHA\']                    # mask overlay factor\n    BBOX = cfg[\'BBOX\']                      # compute boundingbox in postprocessing\n    MINAREA = cfg[\'MINAREA\']                # min Pixel Area to apply bounding boxes (avoid noise)\n    ## Model\n    DL_MODEL_NAME = cfg[\'DL_MODEL_NAME\']    # Only used for downloading the correct Model Version\n    DL_MODEL_PATH = cfg[\'DL_MODEL_PATH\']\n\n    LABEL_NAMES = np.asarray([\n        \'background\', \'aeroplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\', \'bus\',\n        \'car\', \'cat\', \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\', \'motorbike\',\n        \'person\', \'pottedplant\', \'sheep\', \'sofa\', \'train\', \'tv\'])\n\n\n    def __init__(self,model_type):\n        assert model_type in [\'od\',\'dl\'], ""only deeplab or object_detection models""\n        #  model type\n        self.MODEL_TYPE = model_type\n        if self.MODEL_TYPE is \'od\':\n            self.MODEL_PATH = self.OD_MODEL_PATH\n            self.MODEL_NAME = self.OD_MODEL_NAME\n        elif self.MODEL_TYPE is \'dl\':\n            self.MODEL_PATH = self.DL_MODEL_PATH\n            self.MODEL_NAME = self.DL_MODEL_NAME\n        ## CPU Placement\n        if self.CPU_ONLY:\n            os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'-1\'\n            self._DEV = \'_CPU\'\n        else:\n            self._DEV = \'\'\n        ## Loading Standard or Optimized Model\n        if self.USE_OPTIMIZED:\n            self.MODEL_PATH = self.MODEL_PATH.format(""optimized_inference_graph.pb"")\n            self._OPT = \'_opt\'\n        else:\n            self.MODEL_PATH = self.MODEL_PATH.format(""frozen_inference_graph.pb"")\n            self._OPT = \'\'\n\n        self.DISPLAY_NAME = self.MODEL_NAME+self._DEV+self._OPT\n\n    def display(self):\n        """"""Display Configuration values.""""""\n        print(""\\nConfigurations:"")\n        for a in dir(self):\n            if not a.startswith(""__"") and not callable(getattr(self, a)):\n                print(""{:30} {}"".format(a, getattr(self, a)))\n        print(""\\n"")\n'"
rod/helper.py,2,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Fri Dec 22 11:53:52 2017\n\n@author: www.github.com/GustavZ\n""""""\n# python 2 compability\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport cv2\nimport threading\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport os\nimport json\nimport sys\nif sys.version_info[0] == 2:\n    import Queue\nelif sys.version_info[0] == 3:\n    import queue as Queue\nfrom tensorflow.python.client import timeline\n\n\nclass FPS(object):\n    """"""\n    Class for FPS calculation\n    """"""\n    def __init__(self, interval):\n        self._glob_start = None\n        self._glob_end = None\n        self._glob_numFrames = 0\n        self._local_start = None\n        self._local_numFrames = 0\n        self._interval = interval\n        self._curr_time = None\n        self._curr_local_elapsed = None\n        self._first = False\n        self._log =[]\n\n    def start(self):\n        self._glob_start = datetime.datetime.now()\n        self._local_start = self._glob_start\n        return self\n\n    def stop(self):\n        self._glob_end = datetime.datetime.now()\n        print(\'> [INFO] elapsed frames (total): {}\'.format(self._glob_numFrames))\n        print(\'> [INFO] elapsed time (total): {:.2f}\'.format(self.elapsed()))\n        print(\'> [INFO] approx. FPS: {:.2f}\'.format(self.fps()))\n\n    def update(self):\n        self._first = True\n        self._curr_time = datetime.datetime.now()\n        self._curr_local_elapsed = (self._curr_time - self._local_start).total_seconds()\n        self._glob_numFrames += 1\n        self._local_numFrames += 1\n        if self._curr_local_elapsed > self._interval:\n          print(""> FPS: {}"".format(self.fps_local()))\n          self._local_numFrames = 0\n          self._local_start = self._curr_time\n\n    def elapsed(self):\n        return (self._glob_end - self._glob_start).total_seconds()\n\n    def fps(self):\n        return self._glob_numFrames / self.elapsed()\n\n    def fps_local(self):\n        if self._first:\n            return round(self._local_numFrames / self._curr_local_elapsed,1)\n        else:\n            return 0.0\n\n\nclass Timer(object):\n    """"""\n    Timer class for benchmark test purposes\n    Usage: start -> tic -> (tictic -> tic ->) toc -> stop\n    Alternative: start -> update -> stop\n    """"""\n    def __init__(self):\n        self._tic = None\n        self._tictic = None\n        self._toc = None\n        self._time = 1\n        self._cache = []\n        self._log = []\n        self._first = True\n\n    def start(self):\n        return self\n\n    def tic(self):\n        self._tic = datetime.datetime.now()\n\n    def tictic(self):\n        self._tictic = datetime.datetime.now()\n        self._cache.append((self._tictic-self._tic).total_seconds())\n        self._tic = self._tictic\n\n    def toc(self):\n        self._toc = datetime.datetime.now()\n        self._time = (self._toc-self._tic).total_seconds() + np.sum(self._cache)\n        self._log.append(self._time)\n        self._cache = []\n\n    def update(self):\n        if self._first:\n            self._tic = datetime.datetime.now()\n            self._toc = self._tic\n            self._first = False\n            self._frame = 1\n        else:\n            self._frame += 1\n            self._tic = datetime.datetime.now()\n            self._time = (self._tic-self._toc).total_seconds()\n            self._log.append(self._time)\n            self._toc = self._tic\n            if len(self._log)>1000:\n                self.stop()\n                self._log = []\n\n    def get_frame(self):\n        return len(self._log)\n\n    def get_fps(self):\n        return round(1./self._time,1)\n\n    def _calc_stats(self):\n        self._totaltime = np.sum(self._log)\n        self._totalnumber = len(self._log)\n        self._meantime = np.mean(self._log)\n        self._mediantime = np.median(self._log)\n        self._mintime = np.min(self._log)\n        self._maxtime = np.max(self._log)\n        self._stdtime = np.std(self._log)\n        self._meanfps = 1./np.mean(self._log)\n        self._medianfps = 1./np.median(self._log)\n\n    def stop(self):\n        self._calc_stats()\n        print (""> [INFO] total detection time for {} images: {}"".format(self._totalnumber,self._totaltime))\n        print (""> [INFO] mean detection time: {}"".format(self._meantime))\n        print (""> [INFO] median detection time: {}"".format(self._mediantime))\n        print (""> [INFO] min detection time: {}"".format(self._mintime))\n        print (""> [INFO] max detection time: {}"".format(self._maxtime))\n        print (""> [INFO] std dev detection time: {}"".format(self._stdtime))\n        print (""> [INFO] resulting mean fps: {}"".format(self._meanfps))\n        print (""> [INFO] resulting median fps: {}"".format(self._medianfps))\n\n\nclass InputStream(object):\n    """"""\n    input stream base class\n    """"""\n    def __init__(self):\n        self.real_height = 0\n        self.real_width = 0\n        self.stopped = False\n        self.frame = None\n\n    def start(self):\n        self.stopped = False\n        return self\n\n    def stop(self):\n        self.stopped = True\n\n    def isActive(self):\n        return not self.stopped\n\n    def read(self):\n        return self.frame\n\n\nclass ImageStream(InputStream):\n    """"""\n    Test Image handling class\n    """"""\n    def __init__(self,image_path,limit=None,image_shape=None):\n        super(ImageStream, self).__init__()\n        self.frame_shape = image_shape\n        self.frame_path = image_path\n        self.frames = []\n        self.limit = limit\n        if not limit:\n            self.limit = float(\'inf\')\n\n    def start(self):\n        self.load_images()\n        self.stopped = False\n        return self\n\n    def load_images(self):\n        for root, dirs, files in os.walk(self.frame_path):\n            for idx,file in enumerate(files):\n                if idx >=self.limit:\n                    self.frames.sort()\n                    return self.frames\n                if file.endswith("".jpg"") or file.endswith("".JPG"") or file.endswith("".JPEG"") or file.endswith("".png""):\n                    self.frames.append(os.path.join(root, file))\n        self.frames.sort()\n\n    def read(self):\n        if self.frame_shape is not None:\n            self.frame = cv2.resize(cv2.imread(self.frames.pop()),(self.frame_shape[:2]))\n        else:\n            self.frame = cv2.imread(self.frames.pop())\n        self.real_height,self.real_width,_ = self.frame.shape\n        return self.frame\n\n    def isActive(self):\n        if self.frames and not self.stopped:\n            return True\n        else:\n            return False\n\n\nclass VideoStream(InputStream):\n    """"""\n    Class for Video Input frame capture\n    Based on OpenCV VideoCapture\n    adapted from https://www.pyimagesearch.com/2015/12/21/increasing-webcam-fps-with-python-and-opencv/\n    """"""\n    def __init__(self, src, width, height):\n        super(VideoStream, self).__init__()\n        # initialize the video camera stream and read the first frame\n        # from the stream\n        self.frame_counter = 1\n        self.width = width\n        self.height = height\n        self.stream = cv2.VideoCapture(src)\n        self.stream.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)\n        self.stream.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)\n        (self.grabbed, self.frame) = self.stream.read()\n        #Debug stream shape\n        self.real_width = int(self.stream.get(3))\n        self.real_height = int(self.stream.get(4))\n\n    def start(self):\n        # start the thread to read frames from the video stream\n        print(""> Start video stream with shape: {},{}"".format(self.real_width,self.real_height))\n        threading.Thread(target=self.update, args=()).start()\n        self.stopped = False\n        return self\n\n    def update(self):\n        # keep looping infinitely until the thread is stopped\n        while True:\n            # if the thread indicator variable is set, stop the thread\n            if self.stopped:\n                self.stream.release()\n                return\n\n            # otherwise, read the next frame from the stream\n            (self.grabbed, self.frame) = self.stream.read()\n            self.frame_counter += 1\n\n    def isActive(self):\n        # check if VideoCapture is still Opened\n        return self.stream.isOpened\n\n    def expanded(self):\n        return np.expand_dims(cv2.cvtColor(self.frame, cv2.COLOR_BGR2RGB), axis=0)\n\n    def resized(self,target_size):\n        return cv2.resize(self.frame, target_size)\n\n\n""""""\nTracker converter functions\n""""""\ndef conv_detect2track(box, width, height):\n    # transforms normalized to absolut coords\n    ymin, xmin, ymax, xmax = box\n    ymin = ymin*height\n    xmin = xmin*width\n    ymax = ymax*height\n    xmax = xmax*width\n    boxwidth= xmax - xmin\n    boxheight = ymax - ymin\n\n    newbox = [xmin,ymin, boxwidth, boxheight]\n    #newbox = map(int,newbox)\n    return newbox\n\ndef conv_track2detect(box, width, height):\n    # transforms absolut to normalized coords\n    dw = 1./width\n    dh = 1./height\n    x, y, boxwidth, boxheight = box #map(float,box)\n    xmin = x * dw\n    ymin = y * dh\n    xmax = (x+boxwidth) * dw\n    ymax = (y+boxheight) * dh\n\n    newbox = np.array([ymin,xmin,ymax,xmax])\n    return newbox\n\n\ndef get_model_list(models_path):\n    """"""\n    Returns List of Model names in models_path\n    """"""\n    for root, dirs, files in os.walk(models_path):\n        if root.count(os.sep) - models_path.count(os.sep) == 0:\n            for idx,model in enumerate(dirs):\n                model_list=[]\n                model_list.append(dirs)\n                model_list = np.squeeze(model_list)\n                model_list.sort()\n    print(""> Loaded following sequention of models: \\n{}"".format(model_list))\n    return model_list\n\n\ndef check_if_optimized_model(model_dir):\n    """"""\n    check if there is an optimized graph in the model_dir\n    """"""\n    for root, dirs, files in os.walk(model_dir):\n        for file in files:\n            if \'optimized\' in file:\n                return True\n                print(\'> found: optimized graph\')\n    return False\n\n\nclass SessionWorker(object):\n    """"""\n    TensorFlow Session Thread for split_model speed Hack\n\n    usage:\n     before:\n         results = sess.run([opt1,opt2],feed_dict={input_x:x,input_y:y})\n     after:\n         opts = [opt1,opt2]\n         feeds = {input_x:x,input_y:y}\n         woker = SessionWorker(""TAG"",graph,config)\n         worker.put_sess_queue(opts,feeds)\n         q = worker.get_result_queue()\n         if q is None:\n             continue\n         results = q[\'results\']\n         extras = q[\'extras\']\n\n    extras: None or input image frame.\n    \t(reason: GPU detection thread does not wait for result.\n    \t\tTherefore, keep frame if VISUALIZE=TRUE.)\n    """"""\n    def __init__(self,tag,graph,config):\n        self.lock = threading.Lock()\n        self.sess_queue = Queue.Queue()\n        self.result_queue = Queue.Queue()\n        self.tag = tag\n        t = threading.Thread(target=self.execution,args=(graph,config))\n        t.setDaemon(True)\n        t.start()\n        return\n\n    def execution(self,graph,config):\n        self.is_thread_running = True\n        try:\n            with tf.Session(graph=graph,config=config) as sess:\n                while self.is_thread_running:\n                        while not self.sess_queue.empty():\n                            q = self.sess_queue.get(block=False)\n                            opts = q[""opts""]\n                            feeds= q[""feeds""]\n                            extras= q[""extras""]\n                            if feeds is None:\n                                results = sess.run(opts)\n                            else:\n                                results = sess.run(opts,feed_dict=feeds)\n                            self.result_queue.put({""results"":results,""extras"":extras})\n                            self.sess_queue.task_done()\n                        time.sleep(0.005)\n        except:\n            import traceback\n            traceback.print_exc()\n        self.stop()\n        return\n\n    def is_sess_empty(self):\n        if self.sess_queue.empty():\n            return True\n        else:\n            return False\n\n    def put_sess_queue(self,opts,feeds=None,extras=None):\n        self.sess_queue.put({""opts"":opts,""feeds"":feeds,""extras"":extras})\n        return\n\n    def is_result_empty(self):\n        if self.result_queue.empty():\n            return True\n        else:\n            return False\n\n    def get_result_queue(self):\n        result = None\n        if not self.result_queue.empty():\n            result = self.result_queue.get(block=False)\n            self.result_queue.task_done()\n        return result\n\n    def stop(self):\n        self.is_thread_running=False\n        with self.lock:\n            while not self.sess_queue.empty():\n                q = self.sess_queue.get(block=False)\n                self.sess_queue.task_done()\n        return\n\n\nclass TimeLiner(object):\n    """"""\n    TimeLiner Class for creating multiple session json timing files\n    modified from: https://github.com/ikhlestov/tensorflow_profiling\n    """"""\n    def __init__(self):\n        self._timeline_dict = None\n\n    def update_timeline(self, chrome_trace):\n        # convert crome trace to python dict\n        chrome_trace_dict = json.loads(chrome_trace)\n        # for first run store full trace\n        if self._timeline_dict is None:\n            self._timeline_dict = chrome_trace_dict\n        # for other - update only time consumption, not definitions\n        else:\n            for event in chrome_trace_dict[\'traceEvents\']:\n                # events time consumption started with \'ts\' prefix\n                if \'ts\' in event:\n                    self._timeline_dict[\'traceEvents\'].append(event)\n\n    def save(self, f_name):\n        with open(f_name, \'w\') as f:\n            json.dump(self._timeline_dict, f)\n\n    def write_timeline(self,step_stats,file_name):\n        fetched_timeline = timeline.Timeline(step_stats)\n        chrome_trace = fetched_timeline.generate_chrome_trace_format()\n        with open(file_name, \'w\') as f:\n        \tf.write(chrome_trace)\n'"
rod/model.py,28,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Dec 21 12:01:40 2017\n\n@author: www.github.com/GustavZ\n""""""\nimport numpy as np\nimport tensorflow as tf\nimport tarfile\nimport copy\nimport os\nimport sys\nimport time\nfrom skimage import measure\nimport six.moves.urllib as urllib\nfrom tensorflow.core.framework import graph_pb2\n\nfrom rod.helper import FPS, VideoStream, SessionWorker, conv_detect2track, conv_track2detect, ImageStream, TimeLiner\nfrom rod.visualizer import Visualizer\nfrom rod.tf_utils import reframe_box_masks_to_image_masks\nfrom rod.config import Config\nfrom rod.visualizer import Visualizer\nimport rod.tf_utils as tf_utils\n\n##################################\n########## Model Class ###########\n##################################\nclass Model(object):\n    """"""\n    Base Tensorflow Inference Model Class\n    """"""\n    def __init__(self,config):\n        self.config = config\n        self.detection_graph = tf.Graph()\n        self.category_index = None\n        self.masks = None\n        self._tf_config = tf.ConfigProto(allow_soft_placement=True)\n        self._tf_config.gpu_options.allow_growth=True\n        #self._tf_config.gpu_options.force_gpu_compatible=True\n        #self._tf_config.gpu_options.per_process_gpu_memory_fraction = 0.01\n        self._run_options = tf.RunOptions(trace_level=tf.RunOptions.NO_TRACE)\n        self._run_metadata = False\n        self._wait_thread = False\n        self._is_imageD = False\n        self._is_videoD = False\n        self._is_rosD = False\n        print (\'> Model: {}\'.format(self.config.MODEL_PATH))\n\n    def download_model(self):\n        """"""\n        downlaods model from model_zoo\n        """"""\n        if self.config.MODEL_TYPE == \'dl\':\n            download_base = \'http://download.tensorflow.org/models/\'\n        elif self.config.MODEL_TYPE == \'od\':\n            download_base = \'http://download.tensorflow.org/models/object_detection/\'\n        model_file = self.config.MODEL_NAME + \'.tar.gz\'\n        if not os.path.isfile(self.config.MODEL_PATH) and self.config.DOWNLOAD_MODEL:\n            print(\'> Model not found. Downloading it now.\')\n            opener = urllib.request.URLopener()\n            opener.retrieve(download_base + model_file, model_file)\n            tar_file = tarfile.open(model_file)\n            for file in tar_file.getmembers():\n              file_name = os.path.basename(file.name)\n              if \'frozen_inference_graph.pb\' in file_name:\n                tar_file.extract(file, os.getcwd() + \'/models/\')\n            os.remove(os.getcwd() + \'/\' + model_file)\n        else:\n            print(\'> Model found. Proceed.\')\n\n    def node_name(self,n):\n        if n.startswith(""^""):\n            return n[1:]\n        else:\n            return n.split("":"")[0]\n\n    def load_frozen_graph(self):\n        """"""\n        loads graph from frozen model file\n        """"""\n        print(\'> Loading frozen model into memory\')\n        if (self.config.MODEL_TYPE == \'od\' and self.config.SPLIT_MODEL):\n            # load a frozen Model and split it into GPU and CPU graphs\n            # Hardcoded split points for ssd_mobilenet\n            tf.reset_default_graph()\n            if self.config.SSD_SHAPE == 600:\n                shape = 7326\n            else:\n                shape = 1917\n            self.score = tf.placeholder(tf.float32, shape=(None, shape, self.config.NUM_CLASSES), name=self.config.SPLIT_NODES[0])\n            self.expand = tf.placeholder(tf.float32, shape=(None, shape, 1, 4), name=self.config.SPLIT_NODES[1])\n            #self.tofloat = tf.placeholder(tf.float32, shape=(None), name=self.config.SPLIT_NODES[2])\n            for node in tf.get_default_graph().as_graph_def().node:\n                if node.name == self.config.SPLIT_NODES[0]:\n                    score_def = node\n                if node.name == self.config.SPLIT_NODES[1]:\n                    expand_def = node\n                #if node.name == self.config.SPLIT_NODES[2]:\n                #    tofloat_def = node\n\n            with self.detection_graph.as_default():\n                graph_def = tf.GraphDef()\n                with tf.gfile.GFile(self.config.MODEL_PATH, \'rb\') as fid:\n                    serialized_graph = fid.read()\n                    graph_def.ParseFromString(serialized_graph)\n\n                    edges = {}\n                    name_to_node_map = {}\n                    node_seq = {}\n                    seq = 0\n                    for node in graph_def.node:\n                        n = self.node_name(node.name)\n                        name_to_node_map[n] = node\n                        edges[n] = [self.node_name(x) for x in node.input]\n                        node_seq[n] = seq\n                        seq += 1\n                    for d in self.config.SPLIT_NODES:\n                        assert d in name_to_node_map, ""%s is not in graph"" % d\n\n                    nodes_to_keep = set()\n                    next_to_visit = self.config.SPLIT_NODES[:]\n\n                    while next_to_visit:\n                        n = next_to_visit[0]\n                        del next_to_visit[0]\n                        if n in nodes_to_keep: continue\n                        nodes_to_keep.add(n)\n                        next_to_visit += edges[n]\n\n                    nodes_to_keep_list = sorted(list(nodes_to_keep), key=lambda n: node_seq[n])\n                    nodes_to_remove = set()\n\n                    for n in node_seq:\n                        if n in nodes_to_keep_list: continue\n                        nodes_to_remove.add(n)\n                    nodes_to_remove_list = sorted(list(nodes_to_remove), key=lambda n: node_seq[n])\n\n                    keep = graph_pb2.GraphDef()\n                    for n in nodes_to_keep_list:\n                        keep.node.extend([copy.deepcopy(name_to_node_map[n])])\n\n                    remove = graph_pb2.GraphDef()\n                    remove.node.extend([score_def])\n                    remove.node.extend([expand_def])\n                    for n in nodes_to_remove_list:\n                        remove.node.extend([copy.deepcopy(name_to_node_map[n])])\n\n                    with tf.device(\'/gpu:0\'):\n                        tf.import_graph_def(keep, name=\'\')\n                    with tf.device(\'/cpu:0\'):\n                        tf.import_graph_def(remove, name=\'\')\n        else:\n            # default model loading procedure\n            with self.detection_graph.as_default():\n              graph_def = tf.GraphDef()\n              with tf.gfile.GFile(self.config.MODEL_PATH, \'rb\') as fid:\n                serialized_graph = fid.read()\n                graph_def.ParseFromString(serialized_graph)\n                tf.import_graph_def(graph_def, name=\'\')\n\n    def load_category_index(self):\n        """"""\n        creates categorie_index from label_map\n        """"""\n        print(\'> Loading label map\')\n        label_map = tf_utils.load_labelmap(self.config.LABEL_PATH)\n        categories = tf_utils.convert_label_map_to_categories(label_map, max_num_classes=self.config.NUM_CLASSES, use_display_name=True)\n        self.category_index = tf_utils.create_category_index(categories)\n\n    def get_tensor_dict(self, outputs):\n        """"""\n        returns tensordict for given tensornames list\n        """"""\n        ops = self.detection_graph.get_operations()\n        all_tensor_names = {output.name for op in ops for output in op.outputs}\n        self.tensor_dict = {}\n        for key in outputs:\n            tensor_name = key + \':0\'\n            if tensor_name in all_tensor_names:\n                self.tensor_dict[key] = self.detection_graph.get_tensor_by_name(tensor_name)\n        return self.tensor_dict\n\n    def prepare_model(self):\n        """"""\n        first step prepare model\n        needs to be called by subclass in re-write process\n\n        Necessary: subclass needs to init\n        self._input_stream\n        """"""\n        if self.config.MODEL_TYPE is \'od\':\n            self.download_model()\n            self.load_frozen_graph()\n            self.load_category_index()\n        elif self.config.MODEL_TYPE is \'dl\':\n            self.download_model()\n            self.load_frozen_graph()\n        self.fps = FPS(self.config.FPS_INTERVAL).start()\n        self._visualizer = Visualizer(self.config).start()\n        return self\n\n    def isActive(self):\n        """"""\n        checks if stream and visualizer are active\n        """"""\n        return self._input_stream.isActive() and self._visualizer.isActive()\n\n    def stop(self):\n        """"""\n        stops all Model sub classes\n        """"""\n        self._input_stream.stop()\n        self._visualizer.stop()\n        self.fps.stop()\n        if self.config.SPLIT_MODEL and self.config.MODEL_TYPE is \'od\':\n            self._gpu_worker.stop()\n            self._cpu_worker.stop()\n\n    def detect(self):\n        """"""\n        needs to be written by subclass\n        """"""\n        self.detection = None\n\n    def run(self):\n        """"""\n        runs detection loop on video or image\n        listens on isActive()\n        """"""\n        print(""> starting detection"")\n        self.start()\n        while self.isActive():\n            # detection\n            self.detect()\n            # Visualization\n            if not self._wait_thread:\n                self.visualize_detection()\n                self.fps.update()\n        self.stop()\n\n    def start(self):\n        """"""\n        starts fps and visualizer class\n        """"""\n        self.fps.start()\n        self._visualizer = Visualizer(self.config).start()\n\n    def visualize_detection(self):\n        self.detection = self._visualizer.visualize_detection(self.frame,self.boxes,\n                                                            self.classes,self.scores,\n                                                            self.masks,self.fps.fps_local(),\n                                                            self.category_index,self._is_imageD)\n\n    def prepare_ros(self,node):\n        """"""\n        prepares ros Node and ROSInputstream\n        only in ros branch usable due to ROS realted package stuff\n        """"""\n        assert node in [\'detection_node\',\'deeplab_node\'], ""only \'detection_node\' and \'deeplab_node\' supported""\n        import rospy\n        from ros import ROSStream, DetectionPublisher, SegmentationPublisher\n        self._is_rosD = True\n        rospy.init_node(node)\n        self._input_stream = ROSStream(self.config.ROS_INPUT)\n        if node is \'detection_node\':\n            self._ros_publisher = DetectionPublisher()\n        if node is \'deeplab_node\':\n            self._ros_publisher = SegmentationPublisher()\n        # check for frame\n        while True:\n            self.frame = self._input_stream.read()\n            time.sleep(1)\n            print(""...waiting for ROS image"")\n            if self.frame is not None:\n                self.stream_height,self.stream_width = self.frame.shape[0:2]\n                break\n\n    def prepare_timeliner(self):\n        """"""\n        prepares timeliner and sets tf Run options\n        """"""\n        self._run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n        self._run_metadata = tf.RunMetadata()\n        self.timeliner = TimeLiner()\n\n    def prepare_tracker(self):\n        """"""\n        prepares KCF tracker\n        """"""\n        sys.path.append(os.getcwd()+\'/rod/kcf\')\n        import KCF\n        self._tracker = KCF.kcftracker(False, True, False, False)\n        self._tracker_counter = 0\n        self._track = False\n\n    def run_tracker(self):\n        """"""\n        runs KCF tracker on videoStream frame\n        !does not work on images, obviously!\n        """"""\n        self.frame = self._input_stream.read()\n        if self._first_track:\n            self._trackers = []\n            self._tracker_boxes = self.boxes\n            num_tracked = 0\n            for box in self.boxes[~np.all(self.boxes == 0, axis=1)]:\n                    self._tracker.init(conv_detect2track(box,self._input_stream.real_width,\n                                        self._input_stream.real_height),self.tracker_frame)\n                    self._trackers.append(self._tracker)\n                    num_tracked += 1\n                    if num_tracked <= self.config.NUM_TRACKERS:\n                        break\n            self._first_track = False\n\n        for idx,self._tracker in enumerate(self._trackers):\n            tracker_box = self._tracker.update(self.frame)\n            self._tracker_boxes[idx,:] = conv_track2detect(tracker_box,\n                                                    self._input_stream.real_width,\n                                                    self._input_stream.real_height)\n        self._tracker_counter += 1\n        self.boxes = self._tracker_boxes\n        # Deactivate Tracker\n        if self._tracker_counter >= self.config.TRACKER_FRAMES:\n            self._track = False\n            self._tracker_counter = 0\n\n    def activate_tracker(self):\n        """"""\n        activates KCF tracker\n        deactivates mask detection\n        """"""\n        #self.masks = None\n        self.tracker_frame = self.frame\n        self._track = True\n        self._first_track = True\n\n\n\n##################################\n### ObjectDetectionModel Class ###\n##################################\nclass ObjectDetectionModel(Model):\n    """"""\n    object_detection model class\n    """"""\n    def __init__(self,config):\n        super(ObjectDetectionModel, self).__init__(config)\n\n    def prepare_input_stream(self):\n        """"""\n        prepares Input Stream\n        stream types: \'video\',\'image\',\'ros\'\n        gets called by prepare model\n        """"""\n        if self.input_type is \'video\':\n            self._is_videoD = True\n            self._input_stream = VideoStream(self.config.VIDEO_INPUT,self.config.WIDTH,\n                                                    self.config.HEIGHT).start()\n            self.stream_height = self._input_stream.real_height\n            self.stream_width = self._input_stream.real_width\n        elif self.input_type is \'image\':\n            self._is_imageD = True\n            self._input_stream = ImageStream(self.config.IMAGE_PATH,self.config.LIMIT_IMAGES,\n                                            (self.config.WIDTH,self.config.HEIGHT)).start()\n            self.stream_height = self.config.HEIGHT\n            self.stream_width = self.config.WIDTH\n        elif self.input_type is \'ros\':\n            self.prepare_ros(\'detection_node\')\n        # Timeliner for image detection\n        if self.config.WRITE_TIMELINE:\n            self.prepare_timeliner()\n\n\n    def prepare_model(self,input_type):\n        """"""\n        prepares Object_Detection model\n        input_type: must be \'image\', \'video\', or \'ros\'\n        """"""\n        assert input_type in [\'image\',\'video\',\'ros\'], ""only \'image\',\'video\' and \'ros\' input possible""\n        super(ObjectDetectionModel, self).prepare_model()\n        self.input_type = input_type\n        # Tracker\n        if self.config.USE_TRACKER:\n            self.prepare_tracker()\n        print(""> Building Graph"")\n        with self.detection_graph.as_default():\n            with tf.Session(graph=self.detection_graph,config=self._tf_config) as self._sess:\n                # Prepare Input Stream\n                self.prepare_input_stream()\n                # Define Input and Ouput tensors\n                self._tensor_dict = self.get_tensor_dict([\'num_detections\', \'detection_boxes\',\n                                                        \'detection_scores\',\'detection_classes\', \'detection_masks\'])\n                self._image_tensor = self.detection_graph.get_tensor_by_name(\'image_tensor:0\')\n                # Mask Transformations\n                if \'detection_masks\' in self._tensor_dict:\n                    # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n                    detection_boxes = tf.squeeze(self._tensor_dict[\'detection_boxes\'], [0])\n                    detection_masks = tf.squeeze(self._tensor_dict[\'detection_masks\'], [0])\n                    real_num_detection = tf.cast(self._tensor_dict[\'num_detections\'][0], tf.int32)\n                    detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n                    detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n                    detection_masks_reframed = reframe_box_masks_to_image_masks(detection_masks, detection_boxes,self.stream_height,self.stream_width)\n                    detection_masks_reframed = tf.cast(tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n                    self._tensor_dict[\'detection_masks\'] = tf.expand_dims(detection_masks_reframed, 0)\n                if self.config.SPLIT_MODEL:\n                    self._score_out = self.detection_graph.get_tensor_by_name(\'{}:0\'.format(self.config.SPLIT_NODES[0]))\n                    self._expand_out = self.detection_graph.get_tensor_by_name(\'{}:0\'.format(self.config.SPLIT_NODES[1]))\n                    self._score_in = self.detection_graph.get_tensor_by_name(\'{}_1:0\'.format(self.config.SPLIT_NODES[0]))\n                    self._expand_in = self.detection_graph.get_tensor_by_name(\'{}_1:0\'.format(self.config.SPLIT_NODES[1]))\n                    # Threading\n                    self._gpu_worker = SessionWorker(""GPU"",self.detection_graph,self._tf_config)\n                    self._cpu_worker = SessionWorker(""CPU"",self.detection_graph,self._tf_config)\n                    self._gpu_opts = [self._score_out,self._expand_out]\n                    self._cpu_opts = [self._tensor_dict[\'detection_boxes\'],\n                                    self._tensor_dict[\'detection_scores\'],\n                                    self._tensor_dict[\'detection_classes\'],\n                                    self._tensor_dict[\'num_detections\']]\n            return self\n\n    def run_default_sess(self):\n        """"""\n        runs default session\n        """"""\n        # default session)\n        self.frame = self._input_stream.read()\n        output_dict = self._sess.run(self._tensor_dict,\n                                    feed_dict={self._image_tensor:\n                                    self._visualizer.expand_and_convertRGB_image(self.frame)},\n                                    options=self._run_options, run_metadata=self._run_metadata)\n        self.num = output_dict[\'num_detections\'][0]\n        self.classes = output_dict[\'detection_classes\'][0]\n        self.boxes = output_dict[\'detection_boxes\'][0]\n        self.scores = output_dict[\'detection_scores\'][0]\n        if \'detection_masks\' in output_dict:\n            self.masks = output_dict[\'detection_masks\'][0]\n\n    def run_thread_sess(self):\n        """"""\n        runs seperate gpu and cpu session threads\n        """"""\n        if self._gpu_worker.is_sess_empty():\n            # put new queue\n            self.frame = self._input_stream.read()\n            gpu_feeds = {self._image_tensor: self._visualizer.expand_and_convertRGB_image(self.frame)}\n            if self.config.VISUALIZE:\n                gpu_extras = self.frame # for visualization frame\n            else:\n                gpu_extras = None\n            self._gpu_worker.put_sess_queue(self._gpu_opts,gpu_feeds,gpu_extras)\n        g = self._gpu_worker.get_result_queue()\n        if g is None:\n            # gpu thread has no output queue. ok skip, let\'s check cpu thread.\n            pass\n        else:\n            # gpu thread has output queue.\n            score,expand,self._frame = g[""results""][0],g[""results""][1],g[""extras""]\n            if self._cpu_worker.is_sess_empty():\n                # When cpu thread has no next queue, put new queue.\n                # else, drop gpu queue.\n                cpu_feeds = {self._score_in: score, self._expand_in: expand}\n                cpu_extras = self.frame\n                self._cpu_worker.put_sess_queue(self._cpu_opts,cpu_feeds,cpu_extras)\n        c = self._cpu_worker.get_result_queue()\n        if c is None:\n            # cpu thread has no output queue. ok, nothing to do. continue\n            self._wait_thread = True\n            return # If CPU RESULT has not been set yet, no fps update\n        else:\n            self._wait_thread = False\n            self.boxes,self.scores,self.classes,self.num,self.frame = c[""results""][0],c[""results""][1],c[""results""][2],c[""results""][3],c[""extras""]\n\n    def run_split_sess(self):\n        """"""\n        runs split session WITHOUT threading\n        optional: timeline writer\n        """"""\n        self.frame = self._input_stream.read()\n        score, expand = self._sess.run(self._gpu_opts,feed_dict={self._image_tensor:\n                                        self._visualizer.expand_and_convertRGB_image(self.frame)},\n                                        options=self._run_options, run_metadata=self._run_metadata)\n        if self.config.WRITE_TIMELINE:\n            self.timeliner.write_timeline(self._run_metadata.step_stats,\n                                        \'{}/timeline_{}_SM1.json\'.format(\n                                        self.config.RESULT_PATH,self.config.DISPLAY_NAME))\n        # CPU Session\n        self.boxes,self.scores,self.classes,self.num = self._sess.run(self._cpu_opts,\n                                                                    feed_dict={self._score_in:score,\n                                                                    self._expand_in: expand},\n                                                                    options=self._run_options,\n                                                                    run_metadata=self._run_metadata)\n        if self.config.WRITE_TIMELINE:\n            self.timeliner.write_timeline(self._run_metadata.step_stats,\n                                        \'{}/timeline_{}_SM2.json\'.format(\n                                        self.config.RESULT_PATH,self.config.DISPLAY_NAME))\n\n\n    def reformat_detection(self):\n        """"""\n        reformats detection\n        """"""\n        self.num = int(self.num)\n        self.boxes = np.squeeze(self.boxes)\n        self.classes = np.squeeze(self.classes).astype(np.uint8)\n        self.scores = np.squeeze(self.scores)\n\n    def detect(self):\n        """"""\n        Object_Detection Detection function\n        optional: multi threading split session, timline writer\n        """"""\n        if not (self.config.USE_TRACKER and self._track):\n            if self.config.SPLIT_MODEL:\n                if self.config.MULTI_THREADING:\n                    self.run_thread_sess()\n                    if self._wait_thread: # checks if thread has output\n                        return\n                else:\n                    self.run_split_sess()\n            else:\n                self.run_default_sess()\n                if self.config.WRITE_TIMELINE:\n                    self.timeliner.write_timeline(self._run_metadata.step_stats,\n                                            \'{}/timeline_{}.json\'.format(\n                                            self.config.RESULT_PATH,self.config.DISPLAY_NAME))\n            self.reformat_detection()\n            # Activate Tracker\n            if self.config.USE_TRACKER and not self._is_imageD:\n                self.activate_tracker()\n        # Tracking\n        else:\n            self.run_tracker()\n\n        # Publish ROS Message\n        if self._is_rosD:\n            self._ros_publisher.publish(self.boxes,self.scores,self.classes,self.num,self.category_index,self.frame.shape,self.masks,self.fps.fps_local())\n\n\n\n##################################\n###### DeepLabModel Class ########\n##################################\nclass DeepLabModel(Model):\n    def __init__(self,config):\n        super(DeepLabModel, self).__init__(config)\n\n    def prepare_input_stream(self):\n        if self.input_type is \'video\':\n            self._is_videoD = True\n            self._input_stream = VideoStream(self.config.VIDEO_INPUT,self.config.WIDTH,self.config.HEIGHT).start()\n        elif self.input_type is \'image\':\n            self._is_imageD = True\n            self._input_stream = ImageStream(self.config.IMAGE_PATH,self.config.LIMIT_IMAGES).start()\n            if self.config.WRITE_TIMELINE:\n                self.prepare_timeliner()\n        elif self._input_type is \'ros\':\n            self.prepare_ros(\'deeplab_node\')\n\n\n\n    def prepare_model(self,input_type):\n        """"""\n        prepares DeepLab model\n        input_type: must be \'image\', \'video\', or \'ros\'\n        """"""\n        assert input_type in [\'image\',\'video\',\'ros\'], ""only image, video or ros input possible""\n        super(DeepLabModel, self).prepare_model()\n        self.input_type = input_type\n        # Tracker\n        if self.config.USE_TRACKER:\n            self.prepare_tracker()\n        # Input Stream\n        self.category_index = None\n        self.prepare_input_stream()\n        print(""> Building Graph"")\n        with self.detection_graph.as_default():\n            with tf.Session(graph=self.detection_graph,config=self._tf_config) as self._sess:\n                return self\n\n    def detect(self):\n        """"""\n        DeepLab Detection function\n        """"""\n        if not (self.config.USE_TRACKER and self._track):\n            self.frame = self._input_stream.read()\n            height,width,_ = self.frame.shape\n            resize_ratio = 1.0 * 513 / max(self._input_stream.real_width,self._input_stream.real_height)\n            target_size = (int(resize_ratio * self._input_stream.real_width),int(resize_ratio * self._input_stream.real_height)) #(513, 342)?(513,384)\n            self.frame = self._visualizer.resize_image(self.frame, target_size)\n            batch_seg_map = self._sess.run(\'SemanticPredictions:0\',\n                                            feed_dict={\'ImageTensor:0\':\n                                            [self._visualizer.convertRGB_image(self.frame)]},\n                                            options=self._run_options, run_metadata=self._run_metadata)\n            if self.config.WRITE_TIMELINE:\n                self._timeliner.write_timeline(self._run_metadata.step_stats,\n                                        \'{}/timeline_{}.json\'.format(\n                                        self.config.RESULT_PATH,self.config.DISPLAY_NAME))\n            seg_map = batch_seg_map[0]\n            self.boxes = []\n            self.labels = []\n            self.ids = []\n            if self.config.BBOX:\n                map_labeled = measure.label(seg_map, connectivity=1)\n                for region in measure.regionprops(map_labeled):\n                    if region.area > self.config.MINAREA:\n                        box = region.bbox\n                        id = seg_map[tuple(region.coords[0])]\n                        label = self.config.LABEL_NAMES[id]\n                        self.boxes.append(box)\n                        self.labels.append(label)\n                        self.ids.append(id)\n            # deeplab workaround\n            self.num = len(self.boxes)\n            self.classes = self.ids\n            self.scores = self.labels\n            self.masks = seg_map\n            # Activate Tracker\n            if self.config.USE_TRACKER and not self._is_imageD:\n                self.activate_tracker()\n        else:\n            self.run_tracker()\n\n        # publish ros\n        if self._is_rosD:\n            self._ros_publisher.publish(self.boxes,self.labels,self.masks,self.frame.shape,self.fps.fps_local())\n'"
rod/ros.py,0,"b'import rospy\nfrom cv_bridge import CvBridge, CvBridgeError\nfrom objdetection.msg import Detection, Object, Segmentation\nfrom helper import InputStream\nfrom sensor_msgs.msg import RegionOfInterest, Image\n\nclass DetectionPublisher(object):\n    """"""\n    Publish ROS detection messages\n    """"""\n    def __init__(self):\n        self.DetPub = rospy.Publisher(\'Detection\', Detection, queue_size=10)\n        self._bridge = CvBridge()\n\n    def publish(self,boxes,scores,classes,num,category_index,image_shape,masks=None,fps=0):\n        # init detection message\n        msg = Detection()\n        for i in range(boxes.shape[0]):\n            if scores[i] > 0.5:\n                if classes[i] in category_index.keys():\n                    class_name = category_index[classes[i]][\'name\']\n                else:\n                    class_name = \'N/A\'\n                ymin, xmin, ymax, xmax = tuple(boxes[i].tolist())\n                (left, right, top, bottom) = (int(xmin * image_shape[1]), int(xmax * image_shape[1]), int(ymin * image_shape[0]), int(ymax * image_shape[0]))\n                box = RegionOfInterest()\n                box.x_offset = left        # Leftmost pixel of the ROI\n                box.y_offset = top         # Topmost pixel of the ROI\n                box.height = bottom - top  # Height of ROI\n                box.width = right - left   # Width of ROI\n                # fill detection message with objects\n                obj = Object()\n                obj.box = box\n                obj.class_name = class_name\n                obj.score = int(100*scores[i])\n                if masks is not None:\n                    obj.mask = self._bridge.cv2_to_imgmsg(masks[i], encoding=""passthrough"")\n                msg.objects.append(obj)\n        msg.fps = fps\n        # publish detection message\n        self.DetPub.publish(msg)\n\nclass SegmentationPublisher(object):\n    """"""\n    Publish ROS Segmentation messages\n    """"""\n    def __init__(self):\n        self.SegPub = rospy.Publisher(\'Segmentation\', Segmentation, queue_size=10)\n        self._bridge = CvBridge()\n\n    def publish(self,boxes,labels,seg_map,image_shape,fps=0):\n        # init detection message\n        msg = Segmentation()\n        boxes = []\n        for i in range(boxes.shape[0]):\n            class_name = labels[i]\n            ymin, xmin, ymax, xmax = tuple(boxes[i].tolist())\n            (left, right, top, bottom) = (int(xmin * image_shape[1]), int(xmax * image_shape[1]), int(ymin * image_shape[0]), int(ymax * image_shape[0]))\n            box = RegionOfInterest()\n            box.x_offset = left        # Leftmost pixel of the ROI\n            box.y_offset = top         # Topmost pixel of the ROI\n            box.height = bottom - top  # Height of ROI\n            box.width = right - left   # Width of ROI\n            # fill segmentation message\n            msg.boxes.append(box)\n            msg.class_names.append(class_name)\n        if masks is not None:\n            msg.seg_map = self._bridge.cv2_to_imgmsg(seg_map, encoding=""passthrough"")\n        msg.fps = fps\n        # publish detection message\n        self.SegPub.publish(msg)\n\nclass ROSStream(InputStream):\n    """"""\n    Capture video via ROS topic\n    """"""\n    def __init__(self, input):\n        super(ROSStream, self).__init__()\n        self._bridge = CvBridge()\n        rospy.Subscriber(input, Image, self.imageCallback)\n\n    def imageCallback(self, data):\n        try:\n            image_raw = self._bridge.imgmsg_to_cv2(data, ""bgr8"")\n        except CvBridgeError as e:\n            print(e)\n        self.frame = image_raw\n'"
rod/tf_utils.py,15,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n"""""" re-assembled by github/gustavz for realtime_object_detection""""""\n\nimport logging\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom rod.protos import string_int_label_map_pb2\n\ndef reframe_box_masks_to_image_masks(box_masks, boxes, image_height,\n                                     image_width):\n  """"""Transforms the box masks back to full image masks.\n\n  Embeds masks in bounding boxes of larger masks whose shapes correspond to\n  image shape.\n\n  Args:\n    box_masks: A tf.float32 tensor of size [num_masks, mask_height, mask_width].\n    boxes: A tf.float32 tensor of size [num_masks, 4] containing the box\n           corners. Row i contains [ymin, xmin, ymax, xmax] of the box\n           corresponding to mask i. Note that the box corners are in\n           normalized coordinates.\n    image_height: Image height. The output mask will have the same height as\n                  the image height.\n    image_width: Image width. The output mask will have the same width as the\n                 image width.\n\n  Returns:\n    A tf.float32 tensor of size [num_masks, image_height, image_width].\n  """"""\n  def transform_boxes_relative_to_boxes(boxes, reference_boxes):\n    boxes = tf.reshape(boxes, [-1, 2, 2])\n    min_corner = tf.expand_dims(reference_boxes[:, 0:2], 1)\n    max_corner = tf.expand_dims(reference_boxes[:, 2:4], 1)\n    transformed_boxes = (boxes - min_corner) / (max_corner - min_corner)\n    return tf.reshape(transformed_boxes, [-1, 4])\n\n  box_masks = tf.expand_dims(box_masks, axis=3)\n  num_boxes = tf.shape(box_masks)[0]\n  unit_boxes = tf.concat(\n      [tf.zeros([num_boxes, 2]), tf.ones([num_boxes, 2])], axis=1)\n  reverse_boxes = transform_boxes_relative_to_boxes(unit_boxes, boxes)\n  image_masks = tf.image.crop_and_resize(image=box_masks,\n                                         boxes=reverse_boxes,\n                                         box_ind=tf.range(num_boxes),\n                                         crop_size=[image_height, image_width],\n                                         extrapolation_value=0.0)\n  return tf.squeeze(image_masks, axis=3)\n\n\ndef load_labelmap(path):\n  """"""Loads label map proto.\n\n  Args:\n    path: path to StringIntLabelMap proto text file.\n  Returns:\n    a StringIntLabelMapProto\n  """"""\n  with tf.gfile.GFile(path, \'r\') as fid:\n    label_map_string = fid.read()\n    label_map = string_int_label_map_pb2.StringIntLabelMap()\n    try:\n      text_format.Merge(label_map_string, label_map)\n    except text_format.ParseError:\n      label_map.ParseFromString(label_map_string)\n  _validate_label_map(label_map)\n  return label_map\n\n\ndef convert_label_map_to_categories(label_map,\n                                    max_num_classes,\n                                    use_display_name=True):\n  """"""Loads label map proto and returns categories list compatible with eval.\n\n  This function loads a label map and returns a list of dicts, each of which\n  has the following keys:\n    \'id\': (required) an integer id uniquely identifying this category.\n    \'name\': (required) string representing category name\n      e.g., \'cat\', \'dog\', \'pizza\'.\n  We only allow class into the list if its id-label_id_offset is\n  between 0 (inclusive) and max_num_classes (exclusive).\n  If there are several items mapping to the same id in the label map,\n  we will only keep the first one in the categories list.\n\n  Args:\n    label_map: a StringIntLabelMapProto or None.  If None, a default categories\n      list is created with max_num_classes categories.\n    max_num_classes: maximum number of (consecutive) label indices to include.\n    use_display_name: (boolean) choose whether to load \'display_name\' field\n      as category name.  If False or if the display_name field does not exist,\n      uses \'name\' field as category names instead.\n  Returns:\n    categories: a list of dictionaries representing all possible categories.\n  """"""\n  categories = []\n  list_of_ids_already_added = []\n  if not label_map:\n    label_id_offset = 1\n    for class_id in range(max_num_classes):\n      categories.append({\n          \'id\': class_id + label_id_offset,\n          \'name\': \'category_{}\'.format(class_id + label_id_offset)\n      })\n    return categories\n  for item in label_map.item:\n    if not 0 < item.id <= max_num_classes:\n      logging.info(\'Ignore item %d since it falls outside of requested \'\n                   \'label range.\', item.id)\n      continue\n    if use_display_name and item.HasField(\'display_name\'):\n      name = item.display_name\n    else:\n      name = item.name\n    if item.id not in list_of_ids_already_added:\n      list_of_ids_already_added.append(item.id)\n      categories.append({\'id\': item.id, \'name\': name})\n  return categories\n\n\ndef create_category_index(categories):\n  """"""Creates dictionary of COCO compatible categories keyed by category id.\n\n  Args:\n    categories: a list of dicts, each of which has the following keys:\n      \'id\': (required) an integer id uniquely identifying this category.\n      \'name\': (required) string representing category name\n        e.g., \'cat\', \'dog\', \'pizza\'.\n\n  Returns:\n    category_index: a dict containing the same entries as categories, but keyed\n      by the \'id\' field of each category.\n  """"""\n  category_index = {}\n  for cat in categories:\n    category_index[cat[\'id\']] = cat\n  return category_index\n\ndef _validate_label_map(label_map):\n  """"""Checks if a label map is valid.\n\n  Args:\n    label_map: StringIntLabelMap to validate.\n\n  Raises:\n    ValueError: if label map is invalid.\n  """"""\n  for item in label_map.item:\n    if item.id < 1:\n      raise ValueError(\'Label map ids should be >= 1.\')\n'"
rod/visualizer.py,0,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\n@author: www.github.com/GustavZ\n\n""""""\nimport collections\nimport numpy as np\nimport cv2\nimport random\nfrom rod.config import Config\n\nclass Visualizer(object):\n    """"""\n    Visualizer Class to handle all kind of detection visualizations on an image\n    """"""\n    STANDARD_COLORS = [\n        (0, 0, 0),#Black\n        (60, 180, 75),#Green\n        (230, 25, 75),#Red\n        (255, 225, 25),#Yellow\n        (0, 130, 200),#Blue\n        (245, 130, 48),#Orange\n        (145, 30, 180),#Purple\n        (70, 240, 240),#Cyan\n        (0, 128, 128),#Teal\n        (210, 245, 60),#Lime\n        (250, 190, 190),#Pink\n        (240, 50, 230),#Magenta\n        (230, 190, 255),#Lavender\n        (170, 110, 40),#Brown\n        (255, 250, 200),#Beige\n        (0, 252, 124), # LawnGreen\n        (128, 0, 0),#Maroon\n        (170, 255, 195),#Mint\n        (128, 128, 0),#Olive\n        (255, 215, 180),#Coral\n        (0, 0, 128),#Navy\n        (60, 180, 75),#Green\n        (230, 25, 75),#Red\n        (255, 225, 25),#Yellow\n        (0, 130, 200),#Blue\n        (245, 130, 48),#Orange\n        (145, 30, 180),#Purple\n        (70, 240, 240),#Cyan\n        (0, 128, 128),#Teal\n        (210, 245, 60),#Lime\n        (250, 190, 190),#Pink\n        (240, 50, 230),#Magenta\n        (230, 190, 255),#Lavender\n        (170, 110, 40),#Brown\n        (255, 250, 200),#Beige\n        (0, 252, 124), # LawnGreen\n        (128, 0, 0),#Maroon\n        (170, 255, 195),#Mint\n        (128, 128, 0),#Olive\n        (255, 215, 180),#Coral\n        (0, 0, 128),#Navy\n        (60, 180, 75),#Green\n        (230, 25, 75),#Red\n        (255, 225, 25),#Yellow\n        (0, 130, 200),#Blue\n        (245, 130, 48),#Orange\n        (145, 30, 180),#Purple\n        (70, 240, 240),#Cyan\n        (0, 128, 128),#Teal\n        (210, 245, 60),#Lime\n        (250, 190, 190),#Pink\n        (240, 50, 230),#Magenta\n        (230, 190, 255),#Lavender\n        (170, 110, 40),#Brown\n        (255, 250, 200),#Beige\n        (0, 252, 124), # LawnGreen\n        (128, 0, 0),#Maroon\n        (170, 255, 195),#Mint\n        (128, 128, 0),#Olive\n        (255, 215, 180),#Coral\n        (0, 0, 128),#Navy\n        (60, 180, 75),#Green\n        (230, 25, 75),#Red\n        (255, 225, 25),#Yellow\n        (0, 130, 200),#Blue\n        (245, 130, 48),#Orange\n        (145, 30, 180),#Purple\n        (70, 240, 240),#Cyan\n        (0, 128, 128),#Teal\n        (210, 245, 60),#Lime\n        (250, 190, 190),#Pink\n        (240, 50, 230),#Magenta\n        (230, 190, 255),#Lavender\n        (170, 110, 40),#Brown\n        (255, 250, 200),#Beige\n        (0, 252, 124), # LawnGreen\n        (128, 0, 0),#Maroon\n        (170, 255, 195),#Mint\n        (128, 128, 0),#Olive\n        (255, 215, 180),#Coral\n        (0, 0, 128),#Navy\n        (60, 180, 75),#Green\n        ]\n\n    STANDARD_COLORS_ARRAY = np.asarray(STANDARD_COLORS).astype(np.uint8)\n\n    def __init__(self,config):\n        self.config = config\n        # private params\n        self._line_thickness = 2\n        self._font_thickness = 1\n        self._font_face = cv2.FONT_HERSHEY_SIMPLEX\n        self._font_scale = 0.5\n        self._line_type = cv2.LINE_AA\n        self.cur_frame = 0\n        self._shuffle = False\n\n        # public params\n        self.image = None\n        self.stopped = False\n\n\n    def _shuffle_colors(self):\n        """"""\n        shuffles STANDARD_COLORS\n        """"""\n        np.random.shuffle(self.STANDARD_COLORS_ARRAY)\n        np.random.shuffle(self.STANDARD_COLORS)\n\n\n    def _draw_bounding_box_on_image(self,\n                                   ymin,\n                                   xmin,\n                                   ymax,\n                                   xmax,\n                                   color=(0, 0, 255),\n                                   display_str_list=(),\n                                   use_normalized_coordinates=True):\n\n      im_height, im_width = self.image.shape[:2]\n      if use_normalized_coordinates:\n        (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                      ymin * im_height, ymax * im_height)\n      else:\n        (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n\n      ####################\n      # draw objectbox\n      ####################\n      points = np.array([[left, top], [left, bottom], [right, bottom], [right, top], [left, top]])\n      cv2.polylines(self.image, np.int32([points]),\n                    isClosed=False, thickness=self._line_thickness, color=color, lineType = self._line_type)\n\n      ####################\n      # calculate str width and height\n      ####################\n      display_str_heights = [cv2.getTextSize(text=ds, fontFace=self._font_face, fontScale=self._font_scale,\n                            thickness=self._font_thickness)[0][1] for ds in display_str_list]\n      total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n\n      if top > total_display_str_height:\n        text_bottom = top\n      else:\n        text_bottom = bottom + total_display_str_height\n\n      ####################\n      # draw textbox and text\n      ####################\n      for display_str in display_str_list[::-1]:\n        #\n        [(text_width, text_height), baseLine] = cv2.getTextSize(text=display_str, fontFace=self._font_face,\n                                                                fontScale=self._font_scale, thickness=self._font_thickness)\n        margin = np.ceil(0.05 * text_height)\n\n        cv2.rectangle(self.image, (int(left), int(text_bottom - 3 * baseLine - text_height - 2 * margin)),\n                        (int(left + text_width), int(text_bottom - baseLine)), color=color, thickness=-1)\n        cv2.putText(self.image, display_str, org=(int(left + margin),\n                    int(text_bottom - text_height - margin)),\n                    fontFace=self._font_face, fontScale=self._font_scale,\n                    thickness=self._font_thickness, color=(0, 0, 0))\n\n        text_bottom -= text_height - 2 * margin\n\n\n    def _draw_mask_on_image(self, mask):\n      """"""\n      Draws mask on image.\n      """"""\n      mask = self.STANDARD_COLORS_ARRAY[mask]\n      cv2.addWeighted(mask,self.config.ALPHA,self.image,1.0,0,self.image)\n\n\n\n    def _visualize_boxes_and_labels_on_image(\n        self,\n        boxes,\n        classes,\n        scores,\n        category_index,\n        instance_masks=None,\n        use_normalized_coordinates=False,\n        max_boxes_to_draw=20,\n        min_score_thresh=0.5):\n        """"""\n        visualizes binary classes, scores, masks and bounding boxes on image\n        """"""\n        # Create a display string (and color) for every box location, group any boxes\n        # that correspond to the same location.\n        box_to_display_str_map = collections.defaultdict(list)\n        box_to_color_map = collections.defaultdict(str)\n        box_to_instance_masks_map = {}\n\n        if not max_boxes_to_draw:\n            max_boxes_to_draw = boxes.shape[0]\n        for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n            if scores is None or scores[i] > min_score_thresh:\n                box = tuple(boxes[i].tolist())\n                if instance_masks is not None:\n                    box_to_instance_masks_map[box] = instance_masks[i]\n                display_str = \'\'\n                if classes[i] in category_index.keys():\n                    class_name = category_index[classes[i]][\'name\']\n                else:\n                    class_name = \'N/A\'\n                display_str = str(class_name)\n                if not display_str:\n                    if scores is None:\n                        display_str = \'?%\'\n                    else:\n                        display_str = \'{}%\'.format(int(100*scores[i]))\n                else:\n                    if scores is None:\n                        display_str = \'{}: ?%\'.format(display_str)\n                    else:\n                        display_str = \'{}: {}%\'.format(display_str, int(100*scores[i]))\n                box_to_display_str_map[box].append(display_str)\n                box_to_color_map[box] = self.STANDARD_COLORS[classes[i] % len(self.STANDARD_COLORS)]\n        first = True\n        mask = None\n        # Draw all boxes onto image.\n        for idx,(box, color) in enumerate(box_to_color_map.items()):\n            ymin, xmin, ymax, xmax = box\n            if instance_masks is not None:\n\n                if self._shuffle:\n                    # draw mask for each object\n                    self._draw_mask_on_image(box_to_instance_masks_map[box]*(idx+1))\n                else:\n                    # stack all masks and draw one big mask\n                    if first:\n                        first = False\n                        mask = box_to_instance_masks_map[box]*(idx+1)\n                    else:\n                        mask = np.bitwise_or(mask, box_to_instance_masks_map[box])\n\n            self._draw_bounding_box_on_image(\n                ymin,\n                xmin,\n                ymax,\n                xmax,\n                color=color,\n                display_str_list=box_to_display_str_map[box],\n                use_normalized_coordinates=use_normalized_coordinates)\n\n        # Draw Masks on Image (only one color for all masks)\n        if mask is not None and not self._shuffle:\n            self._draw_mask_on_image(mask)\n\n\n    def _draw_text_on_image(self, string, position, color = (77, 255, 9)):\n        """"""\n        visualizes colored text on image on given position with OpenCV\n        """"""\n        cv2.putText(self.image, string, (position),\n                    fontFace = self._font_face, fontScale = self._font_scale,\n                    color = color, thickness = self._font_thickness)\n\n\n    def _exit_visualization(self,millis=1):\n        """"""\n        - sets exit variable on key \'q\'\n        - saves screenshot on key \'s\'\n        """"""\n        k = cv2.waitKey(millis) & 0xFF\n        if k  == ord(\'q\'): # wait for \'q\' key to exit\n            print(""> User exit request"")\n            self.stopped = True\n        elif k == ord(\'s\'): # wait for \'s\' key to save screenshot\n            self.save_image()\n\n\n\n    def _exit_print(self):\n        """"""\n        sets exit variable if max frames are reached\n        """"""\n        if self.cur_frame >= self.config.MAX_FRAMES:\n            self.stopped = True\n\n\n    def _print_detection(self,boxes,scores,classes,category_index):\n        """"""\n        prints detection result above threshold to console\n        """"""\n        for box, score, _class in zip(boxes, scores, classes):\n            if self.cur_frame%self.config.PRINT_INTERVAL==0 and score > self.config.PRINT_TH:\n                label = category_index[_class][\'name\']\n                print(""label: {}\\nscore: {}\\nbox: {}"".format(label, score, box))\n\n\n    def _draw_single_box_on_image(self,box,label,id):\n        """"""\n        draws single box and label on image\n        """"""\n        p1 = (box[1], box[0])\n        p2 = (box[3], box[2])\n        if self.config.DISCO_MODE:\n            color = random.choice(self.STANDARD_COLORS)\n        else:\n            color = self.STANDARD_COLORS[id]\n        cv2.rectangle(self.image, p1, p2, color, 2)\n        self._draw_text_on_image(label,(p1[0],p1[1]-10),color)\n\n    def visualize_detection(self,\n                            image,\n                            boxes,\n                            classes,\n                            scores, #labels\n                            masks,  #seg_map\n                            fps=\'N/A\',\n                            category_index=None,\n                            shuffle=False):\n        """"""\n        visualization function for object_detection\n        """"""\n        self._shuffle = shuffle\n        self.image = image\n        self.cur_frame += 1\n        if self.config.DISCO_MODE:\n            self._shuffle_colors()\n        # object_detection workaround\n        if self.config.MODEL_TYPE is \'od\':\n            if self.config.VISUALIZE:\n                self._visualize_boxes_and_labels_on_image(\n                boxes,\n                classes,\n                scores,\n                category_index,\n                instance_masks=masks,\n                min_score_thresh=self.config.PRINT_TH,\n                use_normalized_coordinates=True)\n                if self.config.VIS_FPS:\n                    self._draw_text_on_image(""fps: {}"".format(fps), (5,20))\n                self.show_image()\n                self._exit_visualization()\n            else:\n                self._print_detection(boxes,scores,classes,category_index)\n                self._exit_print()\n        # deeplab workaround\n        elif self.config.MODEL_TYPE is \'dl\':\n            if self.config.VISUALIZE:\n                for box,id,label in zip(boxes,classes,scores): #classes=ids,scores=labels\n                    self._draw_single_box_on_image(box,label,id)\n                if masks is not None:\n                    self._draw_mask_on_image(masks) #masks = seg_map\n                if self.config.VIS_FPS:\n                    self._draw_text_on_image(""fps: {}"".format(fps),(5,20))\n                self.show_image()\n                self._exit_visualization()\n            else:\n                self._exit_print()\n        # write detection to image file\n        if self.config.SAVE_RESULT:\n            self.save_image()\n\n        return self.image\n\n    def show_image(self):\n        """"""\n        shows image in OpenCV Window\n        """"""\n        cv2.imshow(self.config.DISPLAY_NAME, self.image)\n\n    def isActive(self):\n        return not self.stopped\n\n    def start(self):\n        print(""> Press \'q\' to Exit"")\n        self.cur_frame = 0\n        self.stopped = False\n        return self\n\n    def stop(self):\n        self.stopped = True\n        cv2.destroyAllWindows()\n\n    def expand_and_convertRGB_image(self,image):\n        return np.expand_dims(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), axis=0)\n\n    def convertRGB_image(self,image):\n        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    def save_image(self):\n        save_path = \'{}/detection{}_{}.jpg\'.format(self.config.RESULT_PATH,self.cur_frame,self.config.DISPLAY_NAME)\n        cv2.imwrite(save_path,self.image)\n        print(""> Saving detection to: {}"".format(save_path))\n\n    def resize_image(self,image,shape):\n        return cv2.resize(image,shape)\n'"
scripts/all_models_to_tensorboard.py,0,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Jan 10 09:45:23 2018\n\n@author: www.github.com/GustavZ\n""""""\n\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.summary import summary\n\nimport os\nimport sys\nimport numpy as np\nfrom rod.helper import get_model_list, check_if_optimized_model\nfrom rod.config import Config\n\nROOT_DIR = os.getcwd()\n\ndef create_tfevent_from_pb(model,optimized=False):\n    print(""> creating tfevent of model: {}"".format(model))\n\n    if optimized:\n        model_path=ROOT_DIR+\'/models/{}/optimized_inference_graph.pb\'.format(model)\n        log_dir=ROOT_DIR+\'/models/{}/log_opt/\'.format(model)\n    else:\n        model_path=ROOT_DIR+\'/models/{}/frozen_inference_graph.pb\'.format(model)\n        log_dir=ROOT_DIR+\'/models/{}/log/\'.format(model)\n\n    with session.Session(graph=ops.Graph()) as sess:\n        with gfile.FastGFile(model_path, ""rb"") as f:\n          graph_def = graph_pb2.GraphDef()\n          graph_def.ParseFromString(f.read())\n          importer.import_graph_def(graph_def)\n        pb_visual_writer = summary.FileWriter(log_dir)\n        pb_visual_writer.add_graph(sess.graph)\n    print(""> Model {} Imported. \\nVisualize by running: \\\n    tensorboard --logdir={}"".format(model_path, log_dir))\n\n# Gather all Model Names in models/\nMODELS_DIR = os.path.join(ROOT_DIR,\'models\')\nmodels = get_model_list(MODELS_DIR)\n\n# Create Tensorboard readable tfevent files in models/{}/log\nfor model in models:\n    optimized=False\n    create_tfevent_from_pb(model,optimized)\n    # Check if there is an optimized graph\n    model_dir =  os.path.join(MODELS_DIR,model)\n    optimized = check_if_optimized_model(model_dir)\n    if optimized:\n        create_tfevent_from_pb(model,optimized)\n'"
scripts/node_finder.py,1,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Jan 11 15:58:42 2018\n\n@author: www.github.com/GustavZ\n""""""\n\nimport tensorflow as tf\nimport yaml\nimport os\nimport sys\nfrom rod.config import Config\n\n## RUN THIS SCRIPT FROM THE ROOT DIR (NOT FROM /SCRIPTS)\n\n\nMODEL_TYPE = \'od\' # Change to \'OD\' or \'DL\'\nNODE_NAMES = [\'Momentum\',\'Optimizer\',\'BatchNorm\', \'Loss\']\nNODE_OPS = [\'Placeholder\',\'Identity\',\'CheckNumerics\',\'BatchNorm\']\n\n## Don\'t Change ##\nconfig = Config(MODEL_TYPE)\nMODEL_PATH = config.MODEL_PATH\n\nprint(""> exploring Model: {}"".format(MODEL_PATH))\n\ngf = tf.GraphDef()\ngf.ParseFromString(open(MODEL_PATH,\'rb\').read())\n\nprint(\'>Looking for node Names:\')\nfor NAME in NODE_NAMES:\n    print(NAME)\n    print([n.name + \'=>\' +  n.op for n in gf.node if NAME in n.name ])\n\nprint(\'>Looking for node Ops:\')\nfor OP in NODE_OPS:\n    print(OP)\n    print([n.name + \'=>\' +  n.op for n in gf.node if OP in n.op])\n'"
rod/kcf/setup.py,0,"b""from distutils.core import setup, Extension\r\nfrom Cython.Distutils import build_ext\r\nimport numpy\r\n\r\nlibdr = ['/usr/local/lib']\r\nincdr = [numpy.get_include(), '/usr/local/include/']\r\n\r\next = [\r\n\tExtension('cvt', ['python/cvt.pyx'], \r\n\t\tlanguage = 'c++', \r\n\t\textra_compile_args = ['-std=c++11'], \r\n\t\tinclude_dirs = incdr, \r\n\t\tlibrary_dirs = libdr, \r\n\t\tlibraries = ['opencv_core']), \r\n\tExtension('KCF', ['python/KCF.pyx', 'src/kcftracker.cpp', 'src/fhog.cpp'], \r\n\t\tlanguage = 'c++', \r\n\t\textra_compile_args = ['-std=c++11'], \r\n\t\tinclude_dirs = incdr, \r\n\t\tlibrary_dirs = libdr, \r\n\t\tlibraries = ['opencv_core', 'opencv_imgproc'])\r\n]\r\n\r\nsetup(\r\n\tname = 'app', \r\n\tcmdclass = {'build_ext':build_ext}, \r\n\text_modules = ext\r\n)\r\n\r\n#python setup.py build_ext --inplace\r\n"""
rod/protos/__init__.py,0,b''
rod/protos/string_int_label_map_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: stuff/object_detection/protos/string_int_label_map.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'stuff/object_detection/protos/string_int_label_map.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n8stuff/object_detection/protos/string_int_label_map.proto\\x12\\x17object_detection.protos\\""G\\n\\x15StringIntLabelMapItem\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\n\\n\\x02id\\x18\\x02 \\x01(\\x05\\x12\\x14\\n\\x0c\\x64isplay_name\\x18\\x03 \\x01(\\t\\""Q\\n\\x11StringIntLabelMap\\x12<\\n\\x04item\\x18\\x01 \\x03(\\x0b\\x32..object_detection.protos.StringIntLabelMapItem\')\n)\n\n\n\n\n_STRINGINTLABELMAPITEM = _descriptor.Descriptor(\n  name=\'StringIntLabelMapItem\',\n  full_name=\'object_detection.protos.StringIntLabelMapItem\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'object_detection.protos.StringIntLabelMapItem.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'object_detection.protos.StringIntLabelMapItem.id\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'display_name\', full_name=\'object_detection.protos.StringIntLabelMapItem.display_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=85,\n  serialized_end=156,\n)\n\n\n_STRINGINTLABELMAP = _descriptor.Descriptor(\n  name=\'StringIntLabelMap\',\n  full_name=\'object_detection.protos.StringIntLabelMap\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'item\', full_name=\'object_detection.protos.StringIntLabelMap.item\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=158,\n  serialized_end=239,\n)\n\n_STRINGINTLABELMAP.fields_by_name[\'item\'].message_type = _STRINGINTLABELMAPITEM\nDESCRIPTOR.message_types_by_name[\'StringIntLabelMapItem\'] = _STRINGINTLABELMAPITEM\nDESCRIPTOR.message_types_by_name[\'StringIntLabelMap\'] = _STRINGINTLABELMAP\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nStringIntLabelMapItem = _reflection.GeneratedProtocolMessageType(\'StringIntLabelMapItem\', (_message.Message,), dict(\n  DESCRIPTOR = _STRINGINTLABELMAPITEM,\n  __module__ = \'stuff.object_detection.protos.string_int_label_map_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.StringIntLabelMapItem)\n  ))\n_sym_db.RegisterMessage(StringIntLabelMapItem)\n\nStringIntLabelMap = _reflection.GeneratedProtocolMessageType(\'StringIntLabelMap\', (_message.Message,), dict(\n  DESCRIPTOR = _STRINGINTLABELMAP,\n  __module__ = \'stuff.object_detection.protos.string_int_label_map_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.StringIntLabelMap)\n  ))\n_sym_db.RegisterMessage(StringIntLabelMap)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
