file_path,api_count,code
assignment1/q1_softmax.py,0,"b'import numpy as np\n\n\ndef softmax(x):\n    """"""Compute the softmax function for each row of the input x.\n\n    It is crucial that this function is optimized for speed because\n    it will be used frequently in later code. You might find numpy\n    functions np.exp, np.sum, np.reshape, np.max, and numpy\n    broadcasting useful for this task.\n\n    Numpy broadcasting documentation:\n    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n\n    You should also make sure that your code works for a single\n    N-dimensional vector (treat the vector as a single row) and\n    for M x N matrices. This may be useful for testing later. Also,\n    make sure that the dimensions of the output match the input.\n\n    You must implement the optimization in problem 1(a) of the\n    written assignment!\n\n    Arguments:\n    x -- A N dimensional vector or M x N dimensional numpy matrix.\n\n    Return:\n    x -- You are allowed to modify x in-place\n    """"""\n    orig_shape = x.shape\n\n    if len(x.shape) > 1:\n        # Matrix\n        ### YOUR CODE HERE\n        exp_minmax = lambda x: np.exp(x - np.max(x))\n        denom = lambda x: 1.0 / np.sum(x)\n        x = np.apply_along_axis(exp_minmax, 1, x)\n        denominator = np.apply_along_axis(denom, 1, x)\n\n        if len(denominator.shape) == 1:\n            denominator = denominator.reshape((denominator.shape[0], 1))\n\n        x = x * denominator\n        ### END YOUR CODE\n    else:\n        # Vector\n        ### YOUR CODE HERE\n        x_max = np.max(x)\n        x = x - x_max\n        numerator = np.exp(x)\n        denominator = 1.0 / np.sum(numerator)\n        x = numerator.dot(denominator)\n        ### END YOUR CODE\n\n    assert x.shape == orig_shape\n    return x\n\n\ndef test_softmax_basic():\n    """"""\n    Some simple tests to get you started.\n    Warning: these are not exhaustive.\n    """"""\n    print ""Running basic tests...""\n    test1 = softmax(np.array([1, 2]))\n    print test1\n    ans1 = np.array([0.26894142, 0.73105858])\n    assert np.allclose(test1, ans1, rtol=1e-05, atol=1e-06)\n\n    test2 = softmax(np.array([[1001, 1002], [3, 4]]))\n    print test2\n    ans2 = np.array([\n        [0.26894142, 0.73105858],\n        [0.26894142, 0.73105858]])\n    assert np.allclose(test2, ans2, rtol=1e-05, atol=1e-06)\n\n    test3 = softmax(np.array([[-1001, -1002]]))\n    print test3\n    ans3 = np.array([0.73105858, 0.26894142])\n    assert np.allclose(test3, ans3, rtol=1e-05, atol=1e-06)\n\n    print ""You should be able to verify these results by hand!\\n""\n\n\ndef test_softmax():\n    """"""\n    Use this space to test your softmax implementation by running:\n        python q1_softmax.py\n    This function will not be called by the autograder, nor will\n    your tests be graded.\n    """"""\n    print ""Running your tests...""\n    ### YOUR CODE HERE\n    raise NotImplementedError\n    ### END YOUR CODE\n\n\nif __name__ == ""__main__"":\n    test_softmax_basic()\n    test_softmax()\n'"
assignment1/q2_gradcheck.py,0,"b'#!/usr/bin/env python\n\nimport numpy as np\nimport random\n\n\n# First implement a gradient checker by filling in the following functions\ndef gradcheck_naive(f, x):\n    """""" Gradient check for a function f.\n\n    Arguments:\n    f -- a function that takes a single argument and outputs the\n         cost and its gradients\n    x -- the point (numpy array) to check the gradient at\n    """"""\n\n    rndstate = random.getstate()\n    random.setstate(rndstate)\n    fx, grad = f(x) # Evaluate function value at original point\n    h = 1e-4        # Do not change this!\n\n    # Iterate over all indexes in x\n    it = np.nditer(x, flags=[\'multi_index\'], op_flags=[\'readwrite\'])\n    while not it.finished:\n        ix = it.multi_index\n\n        # Try modifying x[ix] with h defined above to compute\n        # numerical gradients. Make sure you call random.setstate(rndstate)\n        # before calling f(x) each time. This will make it possible\n        # to test cost functions with built in randomness later.\n\n        ### YOUR CODE HERE:\n        x[ix] += h\n\n        random.setstate(rndstate)\n        new_f1 = f(x)[0]\n\n        x[ix] -= 2*h\n\n        random.setstate(rndstate)\n        new_f2 = f(x)[0]\n\n        x[ix] += h\n\n        numgrad = (new_f1 - new_f2) / (2 * h)\n        ### END YOUR CODE\n\n        # Compare gradients\n        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n        if reldiff > 1e-5:\n            print ""Gradient check failed.""\n            print ""First gradient error found at index %s"" % str(ix)\n            print ""Your gradient: %f \\t Numerical gradient: %f"" % (\n                grad[ix], numgrad)\n            return\n\n        it.iternext() # Step to next dimension\n\n    print ""Gradient check passed!""\n\n\ndef sanity_check():\n    """"""\n    Some basic sanity checks.\n    """"""\n    quad = lambda x: (np.sum(x ** 2), x * 2)\n\n    print ""Running sanity checks...""\n    gradcheck_naive(quad, np.array(123.456))      # scalar test\n    gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n    gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test\n    print """"\n\n\ndef your_sanity_checks():\n    """"""\n    Use this space add any additional sanity checks by running:\n        python q2_gradcheck.py\n    This function will not be called by the autograder, nor will\n    your additional tests be graded.\n    """"""\n    print ""Running your sanity checks...""\n    ### YOUR CODE HERE\n    # raise NotImplementedError\n    ### END YOUR CODE\n\n\nif __name__ == ""__main__"":\n    sanity_check()\n    your_sanity_checks()\n'"
assignment1/q2_neural.py,0,"b'#!/usr/bin/env python\n\nimport numpy as np\nimport random\n\nfrom q1_softmax import softmax\nfrom q2_sigmoid import sigmoid, sigmoid_grad\nfrom q2_gradcheck import gradcheck_naive\n\n\ndef forward_backward_prop(data, labels, params, dimensions):\n    """"""\n    Forward and backward propagation for a two-layer sigmoidal network\n\n    Compute the forward propagation and for the cross entropy cost,\n    and backward propagation for the gradients for all parameters.\n\n    Arguments:\n    data -- M x Dx matrix, where each row is a training example.\n    labels -- M x Dy matrix, where each row is a one-hot vector.\n    params -- Model parameters, these are unpacked for you.\n    dimensions -- A tuple of input dimension, number of hidden units\n                  and output dimension\n    """"""\n\n    ### Unpack network parameters (do not modify)\n    ofs = 0\n    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n\n    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n    ofs += Dx * H\n    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n    ofs += H\n    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n    ofs += H * Dy\n    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n\n    ### YOUR CODE HERE: forward propagation\n    h = sigmoid(np.dot(data,W1) + b1)\n    yhat = softmax(np.dot(h,W2) + b2)\n    ### END YOUR CODE\n\n    ### YOUR CODE HERE: backward propagation\n    cost = np.sum(-np.log(yhat[labels==1])) / data.shape[0]\n\n    d3 = (yhat - labels) / data.shape[0]\n    gradW2 = np.dot(h.T, d3)\n    gradb2 = np.sum(d3,0,keepdims=True)\n\n    dh = np.dot(d3,W2.T)\n    grad_h = sigmoid_grad(h) * dh\n\n    gradW1 = np.dot(data.T,grad_h)\n    gradb1 = np.sum(grad_h,0)\n    ### END YOUR CODE\n\n    ### Stack gradients (do not modify)\n    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),\n        gradW2.flatten(), gradb2.flatten()))\n\n    return cost, grad\n\n\ndef sanity_check():\n    """"""\n    Set up fake data and parameters for the neural network, and test using\n    gradcheck.\n    """"""\n    print ""Running sanity check...""\n\n    N = 20\n    dimensions = [10, 5, 10]\n    data = np.random.randn(N, dimensions[0])   # each row will be a datum\n    labels = np.zeros((N, dimensions[2]))\n    for i in xrange(N):\n        labels[i, random.randint(0,dimensions[2]-1)] = 1\n\n    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n        dimensions[1] + 1) * dimensions[2], )\n\n    gradcheck_naive(lambda params:\n        forward_backward_prop(data, labels, params, dimensions), params)\n\n\ndef your_sanity_checks():\n    """"""\n    Use this space add any additional sanity checks by running:\n        python q2_neural.py\n    This function will not be called by the autograder, nor will\n    your additional tests be graded.\n    """"""\n    print ""Running your sanity checks...""\n    ### YOUR CODE HERE\n    # raise NotImplementedError\n    ### END YOUR CODE\n\n\nif __name__ == ""__main__"":\n    sanity_check()\n    your_sanity_checks()\n'"
assignment1/q2_sigmoid.py,0,"b'#!/usr/bin/env python\n\nimport numpy as np\n\n\ndef sigmoid(x):\n    """"""\n    Compute the sigmoid function for the input here.\n\n    Arguments:\n    x -- A scalar or numpy array.\n\n    Return:\n    s -- sigmoid(x)\n    """"""\n\n    ### YOUR CODE HERE\n    s = 1.0 / (1 + np.exp(-x))\n    ### END YOUR CODE\n\n    return s\n\n\ndef sigmoid_grad(s):\n    """"""\n    Compute the gradient for the sigmoid function here. Note that\n    for this implementation, the input s should be the sigmoid\n    function value of your original input x.\n\n    Arguments:\n    s -- A scalar or numpy array.\n\n    Return:\n    ds -- Your computed gradient.\n    """"""\n\n    ### YOUR CODE HERE\n    ds = s * (1 - s)\n    ### END YOUR CODE\n\n    return ds\n\n\ndef test_sigmoid_basic():\n    """"""\n    Some simple tests to get you started.\n    Warning: these are not exhaustive.\n    """"""\n    print ""Running basic tests...""\n    x = np.array([[1, 2], [-1, -2]])\n    f = sigmoid(x)\n    g = sigmoid_grad(f)\n    print f\n    f_ans = np.array([\n        [0.73105858, 0.88079708],\n        [0.26894142, 0.11920292]])\n    assert np.allclose(f, f_ans, rtol=1e-05, atol=1e-06)\n    print g\n    g_ans = np.array([\n        [0.19661193, 0.10499359],\n        [0.19661193, 0.10499359]])\n    assert np.allclose(g, g_ans, rtol=1e-05, atol=1e-06)\n    print ""You should verify these results by hand!\\n""\n\n\ndef test_sigmoid():\n    """"""\n    Use this space to test your sigmoid implementation by running:\n        python q2_sigmoid.py\n    This function will not be called by the autograder, nor will\n    your tests be graded.\n    """"""\n    print ""Running your tests...""\n    ### YOUR CODE HERE\n    # raise NotImplementedError\n    ### END YOUR CODE\n\n\nif __name__ == ""__main__"":\n    test_sigmoid_basic();\n    test_sigmoid()\n'"
assignment1/q3_run.py,0,"b'#!/usr/bin/env python\n\nimport random\nimport numpy as np\nfrom utils.treebank import StanfordSentiment\nimport matplotlib\nmatplotlib.use(\'agg\')\nimport matplotlib.pyplot as plt\nimport time\n\nfrom q3_word2vec import *\nfrom q3_sgd import *\n\n# Reset the random seed to make sure that everyone gets the same results\nrandom.seed(314)\ndataset = StanfordSentiment()\ntokens = dataset.tokens()\nnWords = len(tokens)\n\n# We are going to train 10-dimensional vectors for this assignment\ndimVectors = 10\n\n# Context size\nC = 5\n\n# Reset the random seed to make sure that everyone gets the same results\nrandom.seed(31415)\nnp.random.seed(9265)\n\nstartTime=time.time()\nwordVectors = np.concatenate(\n    ((np.random.rand(nWords, dimVectors) - 0.5) /\n       dimVectors, np.zeros((nWords, dimVectors))),\n    axis=0)\nwordVectors = sgd(\n    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n        negSamplingCostAndGradient),\n    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n# Note that normalization is not called here. This is not a bug,\n# normalizing during training loses the notion of length.\n\nprint ""sanity check: cost at convergence should be around or below 10""\nprint ""training took %d seconds"" % (time.time() - startTime)\n\n# concatenate the input and output word vectors\nwordVectors = np.concatenate(\n    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n    axis=0)\n# wordVectors = wordVectors[:nWords,:] + wordVectors[nWords:,:]\n\nvisualizeWords = [\n    ""the"", ""a"", ""an"", "","", ""."", ""?"", ""!"", ""``"", ""\'\'"", ""--"",\n    ""good"", ""great"", ""cool"", ""brilliant"", ""wonderful"", ""well"", ""amazing"",\n    ""worth"", ""sweet"", ""enjoyable"", ""boring"", ""bad"", ""waste"", ""dumb"",\n    ""annoying""]\n\nvisualizeIdx = [tokens[word] for word in visualizeWords]\nvisualizeVecs = wordVectors[visualizeIdx, :]\ntemp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\ncovariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\nU,S,V = np.linalg.svd(covariance)\ncoord = temp.dot(U[:,0:2])\n\nfor i in xrange(len(visualizeWords)):\n    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n        bbox=dict(facecolor=\'green\', alpha=0.1))\n\nplt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\nplt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n\nplt.savefig(\'q3_word_vectors.png\')\n'"
assignment1/q3_sgd.py,0,"b'#!/usr/bin/env python\n\n# Save parameters every a few SGD iterations as fail-safe\nSAVE_PARAMS_EVERY = 5000\n\nimport glob\nimport random\nimport numpy as np\nimport os.path as op\nimport cPickle as pickle\n\n\ndef load_saved_params():\n    """"""\n    A helper function that loads previously saved parameters and resets\n    iteration start.\n    """"""\n    st = 0\n    for f in glob.glob(""saved_params_*.npy""):\n        iter = int(op.splitext(op.basename(f))[0].split(""_"")[2])\n        if (iter > st):\n            st = iter\n\n    if st > 0:\n        with open(""saved_params_%d.npy"" % st, ""r"") as f:\n            params = pickle.load(f)\n            state = pickle.load(f)\n        return st, params, state\n    else:\n        return st, None, None\n\n\ndef save_params(iter, params):\n    with open(""saved_params_%d.npy"" % iter, ""w"") as f:\n        pickle.dump(params, f)\n        pickle.dump(random.getstate(), f)\n\n\ndef sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n        PRINT_EVERY=10):\n    """""" Stochastic Gradient Descent\n\n    Implement the stochastic gradient descent method in this function.\n\n    Arguments:\n    f -- the function to optimize, it should take a single\n         argument and yield two outputs, a cost and the gradient\n         with respect to the arguments\n    x0 -- the initial point to start SGD from\n    step -- the step size for SGD\n    iterations -- total iterations to run SGD for\n    postprocessing -- postprocessing function for the parameters\n                      if necessary. In the case of word2vec we will need to\n                      normalize the word vectors to have unit length.\n    PRINT_EVERY -- specifies how many iterations to output loss\n\n    Return:\n    x -- the parameter value after SGD finishes\n    """"""\n\n    # Anneal learning rate every several iterations\n    ANNEAL_EVERY = 20000\n\n    if useSaved:\n        start_iter, oldx, state = load_saved_params()\n        if start_iter > 0:\n            x0 = oldx\n            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n\n        if state:\n            random.setstate(state)\n    else:\n        start_iter = 0\n\n    x = x0\n\n    if not postprocessing:\n        postprocessing = lambda x: x\n\n    expcost = None\n\n    for iter in xrange(start_iter + 1, iterations + 1):\n        # Don\'t forget to apply the postprocessing after every iteration!\n        # You might want to print the progress every few iterations.\n\n        cost = None\n        ### YOUR CODE HERE\n        cost, grad = f(x)\n        x -= step * grad\n        postprocessing(x)\n        ### END YOUR CODE\n\n        if iter % PRINT_EVERY == 0:\n            if not expcost:\n                expcost = cost\n            else:\n                expcost = .95 * expcost + .05 * cost\n            print ""iter %d: %f"" % (iter, expcost)\n\n        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n            save_params(iter, x)\n\n        if iter % ANNEAL_EVERY == 0:\n            step *= 0.5\n\n    return x\n\n\ndef sanity_check():\n    quad = lambda x: (np.sum(x ** 2), x * 2)\n\n    print ""Running sanity checks...""\n    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n    print ""test 1 result:"", t1\n    assert abs(t1) <= 1e-6\n\n    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n    print ""test 2 result:"", t2\n    assert abs(t2) <= 1e-6\n\n    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n    print ""test 3 result:"", t3\n    assert abs(t3) <= 1e-6\n\n    print """"\n\n\ndef your_sanity_checks():\n    """"""\n    Use this space add any additional sanity checks by running:\n        python q3_sgd.py\n    This function will not be called by the autograder, nor will\n    your additional tests be graded.\n    """"""\n    print ""Running your sanity checks...""\n    ### YOUR CODE HERE\n    # raise NotImplementedError\n    ### END YOUR CODE\n\n\nif __name__ == ""__main__"":\n    sanity_check()\n    your_sanity_checks()\n'"
assignment1/q3_word2vec.py,0,"b'#!/usr/bin/env python\n\nimport numpy as np\nimport random\n\nfrom q1_softmax import softmax\nfrom q2_gradcheck import gradcheck_naive\nfrom q2_sigmoid import sigmoid, sigmoid_grad\n\n\ndef normalizeRows(x):\n    """""" Row normalization function\n\n    Implement a function that normalizes each row of a matrix to have\n    unit length.\n    """"""\n\n    ### YOUR CODE HERE\n    denom = np.apply_along_axis(lambda x: np.sqrt(x.T.dot(x)), 1, x)\n    x /= denom[:, None]\n    ### END YOUR CODE\n\n    return x\n\n\ndef test_normalize_rows():\n    print ""Testing normalizeRows...""\n    x = normalizeRows(np.array([[3.0, 4.0], [1, 2]]))\n    print x\n    ans = np.array([[0.6, 0.8], [0.4472136, 0.89442719]])\n    assert np.allclose(x, ans, rtol=1e-05, atol=1e-06)\n    print """"\n\n\ndef softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n    """""" Softmax cost function for word2vec models\n\n    Implement the cost and gradients for one predicted word vector\n    and one target word vector as a building block for word2vec\n    models, assuming the softmax prediction function and cross\n    entropy loss.\n\n    Arguments:\n    predicted -- numpy ndarray, predicted word vector (\\hat{v} in\n                 the written component)\n    target -- integer, the index of the target word\n    outputVectors -- ""output"" vectors (as rows) for all tokens\n    dataset -- needed for negative sampling, unused here.\n\n    Return:\n    cost -- cross entropy cost for the softmax word prediction\n    gradPred -- the gradient with respect to the predicted word\n           vector\n    grad -- the gradient with respect to all the other word\n           vectors\n\n    We will not provide starter code for this function, but feel\n    free to reference the code you previously wrote for this\n    assignment!\n    """"""\n\n    ### YOUR CODE HERE\n    ## Gradient for $\\hat{\\bm{v}}$:\n\n    #  Calculate the predictions:\n    vhat = predicted\n    z = np.dot(outputVectors, vhat)\n    preds = softmax(z)\n\n    #  Calculate the cost:\n    cost = -np.log(preds[target])\n\n    #  Gradients\n    z = preds.copy()\n    z[target] -= 1.0\n\n    grad = np.outer(z, vhat)\n    gradPred = np.dot(outputVectors.T, z)\n    ### END YOUR CODE\n\n    return cost, gradPred, grad\n\n\ndef getNegativeSamples(target, dataset, K):\n    """""" Samples K indexes which are not the target """"""\n\n    indices = [None] * K\n    for k in xrange(K):\n        newidx = dataset.sampleTokenIdx()\n        while newidx == target:\n            newidx = dataset.sampleTokenIdx()\n        indices[k] = newidx\n    return indices\n\n\ndef negSamplingCostAndGradient(predicted, target, outputVectors, dataset,\n                               K=10):\n    """""" Negative sampling cost function for word2vec models\n\n    Implement the cost and gradients for one predicted word vector\n    and one target word vector as a building block for word2vec\n    models, using the negative sampling technique. K is the sample\n    size.\n\n    Note: See test_word2vec below for dataset\'s initialization.\n\n    Arguments/Return Specifications: same as softmaxCostAndGradient\n    """"""\n\n    # Sampling of indices is done for you. Do not modify this if you\n    # wish to match the autograder and receive points!\n    indices = [target]\n    indices.extend(getNegativeSamples(target, dataset, K))\n\n    ### YOUR CODE HERE\n    grad = np.zeros(outputVectors.shape)\n    gradPred = np.zeros(predicted.shape)\n    cost = 0\n    z = sigmoid(np.dot(outputVectors[target], predicted))\n\n    cost -= np.log(z)\n    grad[target] += predicted * (z - 1.0)\n    gradPred += outputVectors[target] * (z - 1.0)\n\n    for k in xrange(K):\n        samp = indices[k + 1]\n        z = sigmoid(np.dot(outputVectors[samp], predicted))\n        cost -= np.log(1.0 - z)\n        grad[samp] += predicted * z\n        gradPred += outputVectors[samp] * z\n    ### END YOUR CODE\n\n    return cost, gradPred, grad\n\n\ndef skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n             dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n    """""" Skip-gram model in word2vec\n\n    Implement the skip-gram model in this function.\n\n    Arguments:\n    currrentWord -- a string of the current center word\n    C -- integer, context size\n    contextWords -- list of no more than 2*C strings, the context words\n    tokens -- a dictionary that maps words to their indices in\n              the word vector list\n    inputVectors -- ""input"" word vectors (as rows) for all tokens\n    outputVectors -- ""output"" word vectors (as rows) for all tokens\n    word2vecCostAndGradient -- the cost and gradient function for\n                               a prediction vector given the target\n                               word vectors, could be one of the two\n                               cost functions you implemented above.\n\n    Return:\n    cost -- the cost function value for the skip-gram model\n    grad -- the gradient with respect to the word vectors\n    """"""\n\n    cost = 0.0\n    gradIn = np.zeros(inputVectors.shape)\n    gradOut = np.zeros(outputVectors.shape)\n\n    ### YOUR CODE HERE\n    cword_idx = tokens[currentWord]\n    vhat = inputVectors[cword_idx]\n\n    for j in contextWords:\n        u_idx = tokens[j]\n        c_cost, c_grad_in, c_grad_out = \\\n            word2vecCostAndGradient(vhat, u_idx, outputVectors, dataset)\n        cost += c_cost\n        gradIn[cword_idx] += c_grad_in\n        gradOut += c_grad_out\n    ### END YOUR CODE\n\n    return cost, gradIn, gradOut\n\n\ndef cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n         dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n    """"""CBOW model in word2vec\n\n    Implement the continuous bag-of-words model in this function.\n\n    Arguments/Return specifications: same as the skip-gram model\n\n    Extra credit: Implementing CBOW is optional, but the gradient\n    derivations are not. If you decide not to implement CBOW, remove\n    the NotImplementedError.\n    """"""\n\n    cost = 0.0\n    gradIn = np.zeros(inputVectors.shape)\n    gradOut = np.zeros(outputVectors.shape)\n\n    ### YOUR CODE HERE\n    predicted_indices = [tokens[word] for word in contextWords]\n    predicted_vectors = inputVectors[predicted_indices]\n    predicted = np.sum(predicted_vectors, axis=0)\n    target = tokens[currentWord]\n    cost, gradIn_predicted, gradOut = word2vecCostAndGradient(predicted, target, outputVectors, dataset)\n    for i in predicted_indices:\n        gradIn[i] += gradIn_predicted\n    ### END YOUR CODE\n\n    return cost, gradIn, gradOut\n\n\n#############################################\n# Testing functions below. DO NOT MODIFY!   #\n#############################################\n\ndef word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C,\n                         word2vecCostAndGradient=softmaxCostAndGradient):\n    batchsize = 50\n    cost = 0.0\n    grad = np.zeros(wordVectors.shape)\n    N = wordVectors.shape[0]\n    inputVectors = wordVectors[:N / 2, :]\n    outputVectors = wordVectors[N / 2:, :]\n    for i in xrange(batchsize):\n        C1 = random.randint(1, C)\n        centerword, context = dataset.getRandomContext(C1)\n\n        if word2vecModel == skipgram:\n            denom = 1\n        else:\n            denom = 1\n\n        c, gin, gout = word2vecModel(\n            centerword, C1, context, tokens, inputVectors, outputVectors,\n            dataset, word2vecCostAndGradient)\n        cost += c / batchsize / denom\n        grad[:N / 2, :] += gin / batchsize / denom\n        grad[N / 2:, :] += gout / batchsize / denom\n\n    return cost, grad\n\n\ndef test_word2vec():\n    """""" Interface to the dataset for negative sampling """"""\n    dataset = type(\'dummy\', (), {})()\n\n    def dummySampleTokenIdx():\n        return random.randint(0, 4)\n\n    def getRandomContext(C):\n        tokens = [""a"", ""b"", ""c"", ""d"", ""e""]\n        return tokens[random.randint(0, 4)], \\\n               [tokens[random.randint(0, 4)] for i in xrange(2 * C)]\n\n    dataset.sampleTokenIdx = dummySampleTokenIdx\n    dataset.getRandomContext = getRandomContext\n\n    random.seed(31415)\n    np.random.seed(9265)\n    dummy_vectors = normalizeRows(np.random.randn(10, 3))\n    dummy_tokens = dict([(""a"", 0), (""b"", 1), (""c"", 2), (""d"", 3), (""e"", 4)])\n    print ""==== Gradient check for skip-gram ====""\n    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n        skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n                    dummy_vectors)\n    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n        skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n                    dummy_vectors)\n    print ""\\n==== Gradient check for CBOW      ====""\n    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n        cbow, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n                    dummy_vectors)\n    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n        cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n                    dummy_vectors)\n\n    print ""\\n=== Results ===""\n    print skipgram(""c"", 3, [""a"", ""b"", ""e"", ""d"", ""b"", ""c""],\n                   dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset)\n    print skipgram(""c"", 1, [""a"", ""b""],\n                   dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset,\n                   negSamplingCostAndGradient)\n    print cbow(""a"", 2, [""a"", ""b"", ""c"", ""a""],\n               dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset)\n    print cbow(""a"", 2, [""a"", ""b"", ""a"", ""c""],\n               dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset,\n               negSamplingCostAndGradient)\n\n\nif __name__ == ""__main__"":\n    test_normalize_rows()\n    test_word2vec()\n'"
assignment1/q4_sentiment.py,0,"b'#!/usr/bin/env python\n\nimport argparse\nimport numpy as np\nimport matplotlib\n\nmatplotlib.use(\'agg\')\nimport matplotlib.pyplot as plt\nimport itertools\n\nfrom utils.treebank import StanfordSentiment\nimport utils.glove as glove\n\nfrom q3_sgd import load_saved_params, sgd\n\n# We will use sklearn here because it will run faster than implementing\n# ourselves. However, for other parts of this assignment you must implement\n# the functions yourself!\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\n\ndef getArguments():\n    parser = argparse.ArgumentParser()\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(""--pretrained"", dest=""pretrained"", action=""store_true"",\n                       help=""Use pretrained GloVe vectors."")\n    group.add_argument(""--yourvectors"", dest=""yourvectors"", action=""store_true"",\n                       help=""Use your vectors from q3."")\n    return parser.parse_args()\n\n\ndef getSentenceFeatures(tokens, wordVectors, sentence):\n    """"""\n    Obtain the sentence feature for sentiment analysis by averaging its\n    word vectors\n    """"""\n\n    # Implement computation for the sentence features given a sentence.\n\n    # Inputs:\n    # tokens -- a dictionary that maps words to their indices in\n    #           the word vector list\n    # wordVectors -- word vectors (each row) for all tokens\n    # sentence -- a list of words in the sentence of interest\n\n    # Output:\n    # - sentVector: feature vector for the sentence\n\n    sentVector = np.zeros((wordVectors.shape[1],))\n\n    ### YOUR CODE HERE\n    for s in sentence:\n        sentVector += wordVectors[tokens[s], :]\n\n    sentVector *= 1.0 / len(sentence)\n    ### END YOUR CODE\n\n    assert sentVector.shape == (wordVectors.shape[1],)\n    return sentVector\n\n\ndef getRegularizationValues():\n    """"""Try different regularizations\n\n    Return a sorted list of values to try.\n    """"""\n    values = None  # Assign a list of floats in the block below\n    ### YOUR CODE HERE\n    values = np.logspace(-4, 2, num=100, base=10)\n    ### END YOUR CODE\n    return sorted(values)\n\n\ndef chooseBestModel(results):\n    """"""Choose the best model based on parameter tuning on the dev set\n\n    Arguments:\n    results -- A list of python dictionaries of the following format:\n        {\n            ""reg"": regularization,\n            ""clf"": classifier,\n            ""train"": trainAccuracy,\n            ""dev"": devAccuracy,\n            ""test"": testAccuracy\n        }\n\n    Returns:\n    Your chosen result dictionary.\n    """"""\n    bestResult = None\n\n    ### YOUR CODE HERE\n    bestResult = max(results, key=lambda x: x[""dev""])\n    ### END YOUR CODE\n\n    return bestResult\n\n\ndef accuracy(y, yhat):\n    """""" Precision for classifier """"""\n    assert (y.shape == yhat.shape)\n    return np.sum(y == yhat) * 100.0 / y.size\n\n\ndef plotRegVsAccuracy(regValues, results, filename):\n    """""" Make a plot of regularization vs accuracy """"""\n    plt.plot(regValues, [x[""train""] for x in results])\n    plt.plot(regValues, [x[""dev""] for x in results])\n    plt.xscale(\'log\')\n    plt.xlabel(""regularization"")\n    plt.ylabel(""accuracy"")\n    plt.legend([\'train\', \'dev\'], loc=\'upper left\')\n    plt.savefig(filename)\n\n\ndef outputConfusionMatrix(features, labels, clf, filename):\n    """""" Generate a confusion matrix """"""\n    pred = clf.predict(features)\n    cm = confusion_matrix(labels, pred, labels=range(5))\n    plt.figure()\n    plt.imshow(cm, interpolation=\'nearest\', cmap=plt.cm.Reds)\n    plt.colorbar()\n    classes = [""- -"", ""-"", ""neut"", ""+"", ""+ +""]\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=""center"",\n                 color=""white"" if cm[i, j] > thresh else ""black"")\n    plt.tight_layout()\n    plt.ylabel(\'True label\')\n    plt.xlabel(\'Predicted label\')\n    plt.savefig(filename)\n\n\ndef outputPredictions(dataset, features, labels, clf, filename):\n    """""" Write the predictions to file """"""\n    pred = clf.predict(features)\n    with open(filename, ""w"") as f:\n        print >> f, ""True\\tPredicted\\tText""\n        for i in xrange(len(dataset)):\n            print >> f, ""%d\\t%d\\t%s"" % (\n                labels[i], pred[i], "" "".join(dataset[i][0]))\n\n\ndef main(args):\n    """""" Train a model to do sentiment analyis""""""\n\n    # Load the dataset\n    dataset = StanfordSentiment()\n    tokens = dataset.tokens()\n    nWords = len(tokens)\n\n    if args.yourvectors:\n        _, wordVectors, _ = load_saved_params()\n        wordVectors = np.concatenate(\n            (wordVectors[:nWords, :], wordVectors[nWords:, :]),\n            axis=1)\n    elif args.pretrained:\n        wordVectors = glove.loadWordVectors(tokens)\n    dimVectors = wordVectors.shape[1]\n\n    # Load the train set\n    trainset = dataset.getTrainSentences()\n    nTrain = len(trainset)\n    trainFeatures = np.zeros((nTrain, dimVectors))\n    trainLabels = np.zeros((nTrain,), dtype=np.int32)\n    for i in xrange(nTrain):\n        words, trainLabels[i] = trainset[i]\n        trainFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n\n    # Prepare dev set features\n    devset = dataset.getDevSentences()\n    nDev = len(devset)\n    devFeatures = np.zeros((nDev, dimVectors))\n    devLabels = np.zeros((nDev,), dtype=np.int32)\n    for i in xrange(nDev):\n        words, devLabels[i] = devset[i]\n        devFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n\n    # Prepare test set features\n    testset = dataset.getTestSentences()\n    nTest = len(testset)\n    testFeatures = np.zeros((nTest, dimVectors))\n    testLabels = np.zeros((nTest,), dtype=np.int32)\n    for i in xrange(nTest):\n        words, testLabels[i] = testset[i]\n        testFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n\n    # We will save our results from each run\n    results = []\n    regValues = getRegularizationValues()\n    for reg in regValues:\n        print ""Training for reg=%f"" % reg\n        # Note: add a very small number to regularization to please the library\n        clf = LogisticRegression(C=1.0 / (reg + 1e-12))\n        clf.fit(trainFeatures, trainLabels)\n\n        # Test on train set\n        pred = clf.predict(trainFeatures)\n        trainAccuracy = accuracy(trainLabels, pred)\n        print ""Train accuracy (%%): %f"" % trainAccuracy\n\n        # Test on dev set\n        pred = clf.predict(devFeatures)\n        devAccuracy = accuracy(devLabels, pred)\n        print ""Dev accuracy (%%): %f"" % devAccuracy\n\n        # Test on test set\n        # Note: always running on test is poor style. Typically, you should\n        # do this only after validation.\n        pred = clf.predict(testFeatures)\n        testAccuracy = accuracy(testLabels, pred)\n        print ""Test accuracy (%%): %f"" % testAccuracy\n\n        results.append({\n            ""reg"": reg,\n            ""clf"": clf,\n            ""train"": trainAccuracy,\n            ""dev"": devAccuracy,\n            ""test"": testAccuracy})\n\n    # Print the accuracies\n    print """"\n    print ""=== Recap ===""\n    print ""Reg\\t\\tTrain\\tDev\\tTest""\n    for result in results:\n        print ""%.2E\\t%.3f\\t%.3f\\t%.3f"" % (\n            result[""reg""],\n            result[""train""],\n            result[""dev""],\n            result[""test""])\n    print """"\n\n    bestResult = chooseBestModel(results)\n    print ""Best regularization value: %0.2E"" % bestResult[""reg""]\n    print ""Test accuracy (%%): %f"" % bestResult[""test""]\n\n    # do some error analysis\n    if args.pretrained:\n        plotRegVsAccuracy(regValues, results, ""q4_reg_v_acc.png"")\n        outputConfusionMatrix(devFeatures, devLabels, bestResult[""clf""],\n                              ""q4_dev_conf.png"")\n        outputPredictions(devset, devFeatures, devLabels, bestResult[""clf""],\n                          ""q4_dev_pred.txt"")\n\n\nif __name__ == ""__main__"":\n    main(getArguments())\n'"
assignment2/model.py,2,"b'class Model(object):\n    """"""Abstracts a Tensorflow graph for a learning task.\n\n    We use various Model classes as usual abstractions to encapsulate tensorflow\n    computational graphs. Each algorithm you will construct in this homework will\n    inherit from a Model object.\n    """"""\n\n    def add_placeholders(self):\n        """"""Adds placeholder variables to tensorflow computational graph.\n\n        Tensorflow uses placeholder variables to represent locations in a\n        computational graph where data is inserted.  These placeholders are used as\n        inputs by the rest of the model building and will be fed data during\n        training.\n\n        See for more information:\n        https://www.tensorflow.org/versions/r0.7/api_docs/python/io_ops.html#placeholders\n        """"""\n        raise NotImplementedError(""Each Model must re-implement this method."")\n\n    def create_feed_dict(self, inputs_batch, labels_batch=None):\n        """"""Creates the feed_dict for one step of training.\n\n        A feed_dict takes the form of:\n        feed_dict = {\n                <placeholder>: <tensor of values to be passed for placeholder>,\n                ....\n        }\n\n        If labels_batch is None, then no labels are added to feed_dict.\n\n        Hint: The keys for the feed_dict should be a subset of the placeholder\n                    tensors created in add_placeholders.\n\n        Args:\n            inputs_batch: A batch of input data.\n            labels_batch: A batch of label data.\n        Returns:\n            feed_dict: The feed dictionary mapping from placeholders to values.\n        """"""\n        raise NotImplementedError(""Each Model must re-implement this method."")\n\n    def add_prediction_op(self):\n        """"""Implements the core of the model that transforms a batch of input data into predictions.\n\n        Returns:\n            pred: A tensor of shape (batch_size, n_classes)\n        """"""\n        raise NotImplementedError(""Each Model must re-implement this method."")\n\n    def add_loss_op(self, pred):\n        """"""Adds Ops for the loss function to the computational graph.\n\n        Args:\n            pred: A tensor of shape (batch_size, n_classes)\n        Returns:\n            loss: A 0-d tensor (scalar) output\n        """"""\n        raise NotImplementedError(""Each Model must re-implement this method."")\n\n    def add_training_op(self, loss):\n        """"""Sets up the training Ops.\n\n        Creates an optimizer and applies the gradients to all trainable variables.\n        The Op returned by this function is what must be passed to the\n        sess.run() to train the model. See\n\n        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n\n        for more information.\n\n        Args:\n            loss: Loss tensor (a scalar).\n        Returns:\n            train_op: The Op for training.\n        """"""\n\n        raise NotImplementedError(""Each Model must re-implement this method."")\n\n    def train_on_batch(self, sess, inputs_batch, labels_batch):\n        """"""Perform one step of gradient descent on the provided batch of data.\n\n        Args:\n            sess: tf.Session()\n            input_batch: np.ndarray of shape (n_samples, n_features)\n            labels_batch: np.ndarray of shape (n_samples, n_classes)\n        Returns:\n            loss: loss over the batch (a scalar)\n        """"""\n        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch)\n        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n        return loss\n\n    def predict_on_batch(self, sess, inputs_batch):\n        """"""Make predictions for the provided batch of data\n\n        Args:\n            sess: tf.Session()\n            input_batch: np.ndarray of shape (n_samples, n_features)\n        Returns:\n            predictions: np.ndarray of shape (n_samples, n_classes)\n        """"""\n        feed = self.create_feed_dict(inputs_batch)\n        predictions = sess.run(self.pred, feed_dict=feed)\n        return predictions\n\n    def build(self):\n        self.add_placeholders()\n        self.pred = self.add_prediction_op()\n        self.loss = self.add_loss_op(self.pred)\n        self.train_op = self.add_training_op(self.loss)\n'"
assignment2/q1_classifier.py,17,"b'import time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom q1_softmax import softmax\nfrom q1_softmax import cross_entropy_loss\nfrom model import Model\nfrom utils.general_utils import get_minibatches\n\n\nclass Config(object):\n    """"""Holds model hyperparams and data information.\n\n    The config class is used to store various hyperparameters and dataset\n    information parameters. Model objects are passed a Config() object at\n    instantiation.\n    """"""\n    n_samples = 1024\n    n_features = 100\n    n_classes = 5\n    batch_size = 64\n    n_epochs = 50\n    lr = 1e-4\n\n\nclass SoftmaxModel(Model):\n    """"""Implements a Softmax classifier with cross-entropy loss.""""""\n\n    def add_placeholders(self):\n        """"""Generates placeholder variables to represent the input tensors.\n\n        These placeholders are used as inputs by the rest of the model building\n        and will be fed data during training.\n\n        Adds following nodes to the computational graph\n\n        input_placeholder: Input placeholder tensor of shape\n                                              (batch_size, n_features), type tf.float32\n        labels_placeholder: Labels placeholder tensor of shape\n                                              (batch_size, n_classes), type tf.int32\n\n        Add these placeholders to self as the instance variables\n            self.input_placeholder\n            self.labels_placeholder\n        """"""\n        ### YOUR CODE HERE\n        self.input_placeholder = tf.placeholder(tf.float32, [self.config.batch_size, self.config.n_features])\n        self.labels_placeholder = tf.placeholder(tf.int32, [self.config.batch_size, self.config.n_classes])\n        ### END YOUR CODE\n\n    def create_feed_dict(self, inputs_batch, labels_batch=None):\n        """"""Creates the feed_dict for training the given step.\n\n        A feed_dict takes the form of:\n        feed_dict = {\n                <placeholder>: <tensor of values to be passed for placeholder>,\n                ....\n        }\n\n        If label_batch is None, then no labels are added to feed_dict.\n\n        Hint: The keys for the feed_dict should be the placeholder\n                tensors created in add_placeholders.\n\n        Args:\n            inputs_batch: A batch of input data.\n            labels_batch: A batch of label data.\n        Returns:\n            feed_dict: The feed dictionary mapping from placeholders to values.\n        """"""\n        ### YOUR CODE HERE\n        feed_dict = {self.input_placeholder: inputs_batch, self.labels_placeholder: labels_batch}\n        ### END YOUR CODE\n        return feed_dict\n\n    def add_prediction_op(self):\n        """"""Adds the core transformation for this model which transforms a batch of input\n        data into a batch of predictions. In this case, the transformation is a linear layer plus a\n        softmax transformation:\n\n        y = softmax(Wx + b)\n\n        Hint: Make sure to create tf.Variables as needed.\n        Hint: For this simple use-case, it\'s sufficient to initialize both weights W\n                    and biases b with zeros.\n\n        Args:\n            input_data: A tensor of shape (batch_size, n_features).\n        Returns:\n            pred: A tensor of shape (batch_size, n_classes)\n        """"""\n        ### YOUR CODE HERE\n        with tf.variable_scope(""transformation""):\n            bias = tf.Variable(tf.random_uniform([self.config.n_classes]))\n            W = tf.Variable(tf.random_uniform([self.config.n_features, self.config.n_classes]))\n            z = tf.matmul(self.input_placeholder, W) + bias\n        pred = softmax(z)\n        ### END YOUR CODE\n        return pred\n\n    def add_loss_op(self, pred):\n        """"""Adds cross_entropy_loss ops to the computational graph.\n\n        Hint: Use the cross_entropy_loss function we defined. This should be a very\n                    short function.\n        Args:\n            pred: A tensor of shape (batch_size, n_classes)\n        Returns:\n            loss: A 0-d tensor (scalar)\n        """"""\n        ### YOUR CODE HERE\n        loss = cross_entropy_loss(self.labels_placeholder, pred)\n        ### END YOUR CODE\n        return loss\n\n    def add_training_op(self, loss):\n        """"""Sets up the training Ops.\n\n        Creates an optimizer and applies the gradients to all trainable variables.\n        The Op returned by this function is what must be passed to the\n        `sess.run()` call to cause the model to train. See\n\n        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n\n        for more information.\n\n        Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n                    Calling optimizer.minimize() will return a train_op object.\n\n        Args:\n            loss: Loss tensor, from cross_entropy_loss.\n        Returns:\n            train_op: The Op for training.\n        """"""\n        ### YOUR CODE HERE\n        train_op = tf.train.GradientDescentOptimizer(self.config.lr).minimize(loss)\n        ### END YOUR CODE\n        return train_op\n\n    def run_epoch(self, sess, inputs, labels):\n        """"""Runs an epoch of training.\n\n        Args:\n            sess: tf.Session() object\n            inputs: np.ndarray of shape (n_samples, n_features)\n            labels: np.ndarray of shape (n_samples, n_classes)\n        Returns:\n            average_loss: scalar. Average minibatch loss of model on epoch.\n        """"""\n        n_minibatches, total_loss = 0, 0\n        for input_batch, labels_batch in get_minibatches([inputs, labels], self.config.batch_size):\n            n_minibatches += 1\n            total_loss += self.train_on_batch(sess, input_batch, labels_batch)\n        return total_loss / n_minibatches\n\n    def fit(self, sess, inputs, labels):\n        """"""Fit model on provided data.\n\n        Args:\n            sess: tf.Session()\n            inputs: np.ndarray of shape (n_samples, n_features)\n            labels: np.ndarray of shape (n_samples, n_classes)\n        Returns:\n            losses: list of loss per epoch\n        """"""\n        losses = []\n        for epoch in range(self.config.n_epochs):\n            start_time = time.time()\n            average_loss = self.run_epoch(sess, inputs, labels)\n            duration = time.time() - start_time\n            print \'Epoch {:}: loss = {:.2f} ({:.3f} sec)\'.format(epoch, average_loss, duration)\n            losses.append(average_loss)\n        return losses\n\n    def __init__(self, config):\n        """"""Initializes the model.\n\n        Args:\n            config: A model configuration object of type Config\n        """"""\n        self.config = config\n        self.build()\n\n\ndef test_softmax_model():\n    """"""Train softmax model for a number of steps.""""""\n    config = Config()\n\n    # Generate random data to train the model on\n    np.random.seed(1234)\n    inputs = np.random.rand(config.n_samples, config.n_features)\n    labels = np.zeros((config.n_samples, config.n_classes), dtype=np.int32)\n    labels[:, 0] = 1\n\n    # Tell TensorFlow that the model will be built into the default Graph.\n    # (not required but good practice)\n    with tf.Graph().as_default():\n        # Build the model and add the variable initializer Op\n        model = SoftmaxModel(config)\n        init = tf.global_variables_initializer()\n        # If you are using an old version of TensorFlow, you may have to use\n        # this initializer instead.\n        # init = tf.initialize_all_variables()\n\n        # Create a session for running Ops in the Graph\n        with tf.Session() as sess:\n            # Run the Op to initialize the variables.\n            sess.run(init)\n            # Fit the model\n            losses = model.fit(sess, inputs, labels)\n\n    # If Ops are implemented correctly, the average loss should fall close to zero\n    # rapidly.\n    assert losses[-1] < .5\n    print ""Basic (non-exhaustive) classifier tests pass""\n\n\nif __name__ == ""__main__"":\n    test_softmax_model()\n'"
assignment2/q1_softmax.py,25,"b'import numpy as np\nimport tensorflow as tf\nfrom utils.general_utils import test_all_close\n\n\ndef softmax(x):\n    """"""\n    Compute the softmax function in tensorflow.\n\n    You might find the tensorflow functions tf.exp, tf.reduce_max,\n    tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n    not need to use all of these functions). Recall also that many common\n    tensorflow operations are sugared (e.g. x * y does a tensor multiplication\n    if x and y are both tensors). Make sure to implement the numerical stability\n    fixes as in the previous homework!\n\n    Args:\n        x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n                  represented by row-vectors. (For simplicity, no need to handle 1-d\n                  input as in the previous homework)\n    Returns:\n        out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n                  tensor in this problem.\n    """"""\n\n    ### YOUR CODE HERE\n    x_max = tf.reduce_max(x,1,keep_dims=True)          # find row-wise maximums\n    x_sub = tf.subtract(x,x_max)                       # subtract maximums\n    x_exp = tf.exp(x_sub)                              # exponentiation\n    sum_exp = tf.reduce_sum(x_exp,1,keep_dims=True)    # row-wise sums\n    out = tf.div(x_exp,sum_exp)                        # divide\n    ### END YOUR CODE\n\n    return out\n\n\ndef cross_entropy_loss(y, yhat):\n    """"""\n    Compute the cross entropy loss in tensorflow.\n    The loss should be summed over the current minibatch.\n\n    y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n    of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n    be of dtype tf.float32.\n\n    The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n    solutions are possible, so you may not need to use all of these functions).\n\n    Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n                functions.\n\n    Args:\n        y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n        yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n                    probability distribution and should sum to 1.\n    Returns:\n        out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n                    tensor in the problem.\n    """"""\n\n    ### YOUR CODE HERE\n    l_yhat = tf.log(yhat)                           # log yhat\n    product = tf.multiply(tf.to_float(y), l_yhat)   # multiply element-wise\n    out = tf.negative(tf.reduce_sum(product))       # negative summation to scalar\n    ### END YOUR CODE\n\n    return out\n\n\ndef test_softmax_basic():\n    """"""\n    Some simple tests of softmax to get you started.\n    Warning: these are not exhaustive.\n    """"""\n\n    test1 = softmax(tf.constant(np.array([[1001, 1002], [3, 4]]), dtype=tf.float32))\n    with tf.Session() as sess:\n            test1 = sess.run(test1)\n    test_all_close(""Softmax test 1"", test1, np.array([[0.26894142,  0.73105858],\n                                                      [0.26894142,  0.73105858]]))\n\n    test2 = softmax(tf.constant(np.array([[-1001, -1002]]), dtype=tf.float32))\n    with tf.Session() as sess:\n            test2 = sess.run(test2)\n    test_all_close(""Softmax test 2"", test2, np.array([[0.73105858, 0.26894142]]))\n\n    print ""Basic (non-exhaustive) softmax tests pass\\n""\n\n\ndef test_cross_entropy_loss_basic():\n    """"""\n    Some simple tests of cross_entropy_loss to get you started.\n    Warning: these are not exhaustive.\n    """"""\n    y = np.array([[0, 1], [1, 0], [1, 0]])\n    yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n\n    test1 = cross_entropy_loss(\n            tf.constant(y, dtype=tf.int32),\n            tf.constant(yhat, dtype=tf.float32))\n    with tf.Session() as sess:\n        test1 = sess.run(test1)\n    expected = -3 * np.log(.5)\n    test_all_close(""Cross-entropy test 1"", test1, expected)\n\n    print ""Basic (non-exhaustive) cross-entropy tests pass""\n\nif __name__ == ""__main__"":\n    test_softmax_basic()\n    test_cross_entropy_loss_basic()\n'"
assignment2/q2_initialization.py,3,"b'import numpy as np\nimport tensorflow as tf\n\n\ndef xavier_weight_init():\n    """"""Returns function that creates random tensor.\n\n    The specified function will take in a shape (tuple or 1-d array) and\n    returns a random tensor of the specified shape drawn from the\n    Xavier initialization distribution.\n\n    Hint: You might find tf.random_uniform useful.\n    """"""\n    def _xavier_initializer(shape, **kwargs):\n        """"""Defines an initializer for the Xavier distribution.\n        Specifically, the output should be sampled uniformly from [-epsilon, epsilon] where\n            epsilon = sqrt(6) / <sum of the sizes of shape\'s dimensions>\n        e.g., if shape = (2, 3), epsilon = sqrt(6 / (2 + 3))\n\n        This function will be used as a variable initializer.\n\n        Args:\n            shape: Tuple or 1-d array that species the dimensions of the requested tensor.\n        Returns:\n            out: tf.Tensor of specified shape sampled from the Xavier distribution.\n        """"""\n        ### YOUR CODE HERE\n        epsilon = np.sqrt(6 / np.sum(shape))\n        out = tf.Variable(tf.random_uniform(shape=shape, minval=-epsilon, maxval=epsilon))\n        ### END YOUR CODE\n        return out\n    # Returns defined initializer function.\n    return _xavier_initializer\n\n\ndef test_initialization_basic():\n    """"""Some simple tests for the initialization.\n    """"""\n    print ""Running basic tests...""\n    xavier_initializer = xavier_weight_init()\n    shape = (1,)\n    xavier_mat = xavier_initializer(shape)\n    assert xavier_mat.get_shape() == shape\n\n    shape = (1, 2, 3)\n    xavier_mat = xavier_initializer(shape)\n    assert xavier_mat.get_shape() == shape\n    print ""Basic (non-exhaustive) Xavier initialization tests pass""\n\n\nif __name__ == ""__main__"":\n    test_initialization_basic()\n'"
assignment2/q2_parser_model.py,35,"b'import os\nimport time\nimport tensorflow as tf\nimport cPickle\n\nfrom model import Model\nfrom q2_initialization import xavier_weight_init\nfrom utils.general_utils import Progbar\nfrom utils.parser_utils import minibatches, load_and_preprocess_data\n\n\nclass Config(object):\n    """"""Holds model hyperparams and data information.\n\n    The config class is used to store various hyperparameters and dataset\n    information parameters. Model objects are passed a Config() object at\n    instantiation.\n    """"""\n    n_features = 36\n    n_classes = 3\n    dropout = 0.5\n    embed_size = 50\n    hidden_size = 200\n    batch_size = 2048\n    n_epochs = 10\n    lr = 0.001\n\n\nclass ParserModel(Model):\n    """"""\n    Implements a feedforward neural network with an embedding layer and single hidden layer.\n    This network will predict which transition should be applied to a given partial parse\n    configuration.\n    """"""\n\n    def add_placeholders(self):\n        """"""Generates placeholder variables to represent the input tensors\n\n        These placeholders are used as inputs by the rest of the model building and will be fed\n        data during training.  Note that when ""None"" is in a placeholder\'s shape, it\'s flexible\n        (so we can use different batch sizes without rebuilding the model).\n\n        Adds following nodes to the computational graph\n\n        input_placeholder: Input placeholder tensor of  shape (None, n_features), type tf.int32\n        labels_placeholder: Labels placeholder tensor of shape (None, n_classes), type tf.float32\n        dropout_placeholder: Dropout value placeholder (scalar), type tf.float32\n\n        Add these placeholders to self as the instance variables\n            self.input_placeholder\n            self.labels_placeholder\n            self.dropout_placeholder\n\n        (Don\'t change the variable names)\n        """"""\n        ### YOUR CODE HERE\n        self.input_placeholder = tf.placeholder(tf.int32, [None, self.config.n_features])\n        self.labels_placeholder = tf.placeholder(tf.float32, [None, self.config.n_classes])\n        self.dropout_placeholder = tf.placeholder(tf.float32)\n        self.beta_regul = tf.placeholder(tf.float32)\n        ### END YOUR CODE\n\n    def create_feed_dict(self, inputs_batch, labels_batch=None, dropout=1, beta_regul=10e-7):\n        """"""Creates the feed_dict for the dependency parser.\n\n        A feed_dict takes the form of:\n\n        feed_dict = {\n                <placeholder>: <tensor of values to be passed for placeholder>,\n                ....\n        }\n\n\n        Hint: The keys for the feed_dict should be a subset of the placeholder\n                    tensors created in add_placeholders.\n        Hint: When an argument is None, don\'t add it to the feed_dict.\n\n        Args:\n            inputs_batch: A batch of input data.\n            labels_batch: A batch of label data.\n            dropout: The dropout rate.\n        Returns:\n            feed_dict: The feed dictionary mapping from placeholders to values.\n        """"""\n        ### YOUR CODE HERE\n        feed_dict = {self.input_placeholder: inputs_batch, \\\n                     self.dropout_placeholder: dropout, \\\n                     self.beta_regul: beta_regul\n                     }\n        if labels_batch is not None:\n            feed_dict[self.labels_placeholder] = labels_batch\n        ### END YOUR CODE\n        return feed_dict\n\n    def add_embedding(self):\n        """"""Adds an embedding layer that maps from input tokens (integers) to vectors and then\n        concatenates those vectors:\n            - Creates an embedding tensor and initializes it with self.pretrained_embeddings.\n            - Uses the input_placeholder to index into the embeddings tensor, resulting in a\n              tensor of shape (None, n_features, embedding_size).\n            - Concatenates the embeddings by reshaping the embeddings tensor to shape\n              (None, n_features * embedding_size).\n\n        Hint: You might find tf.nn.embedding_lookup useful.\n        Hint: You can use tf.reshape to concatenate the vectors. See following link to understand\n            what -1 in a shape means.\n            https://www.tensorflow.org/api_docs/python/array_ops/shapes_and_shaping#reshape.\n\n        Returns:\n            embeddings: tf.Tensor of shape (None, n_features*embed_size)\n        """"""\n        ### YOUR CODE HERE\n        embedded = tf.Variable(self.pretrained_embeddings)\n        embeddings = tf.nn.embedding_lookup(embedded,self.input_placeholder)\n        embeddings = tf.reshape(embeddings, [-1, self.config.n_features * self.config.embed_size])\n        ### END YOUR CODE\n        return embeddings\n\n    def add_prediction_op(self):\n        """"""Adds the 1-hidden-layer NN:\n            h = Relu(xW + b1)\n            h_drop = Dropout(h, dropout_rate)\n            pred = h_dropU + b2\n\n        Note that we are not applying a softmax to pred. The softmax will instead be done in\n        the add_loss_op function, which improves efficiency because we can use\n        tf.nn.softmax_cross_entropy_with_logits\n\n        Use the initializer from q2_initialization.py to initialize W and U (you can initialize b1\n        and b2 with zeros)\n\n        Hint: Here are the dimensions of the various variables you will need to create\n                    W:  (n_features*embed_size, hidden_size)\n                    b1: (hidden_size,)\n                    U:  (hidden_size, n_classes)\n                    b2: (n_classes)\n        Hint: Note that tf.nn.dropout takes the keep probability (1 - p_drop) as an argument. \n            The keep probability should be set to the value of self.dropout_placeholder\n\n        Returns:\n            pred: tf.Tensor of shape (batch_size, n_classes)\n        """"""\n\n        x = self.add_embedding()\n        ### YOUR CODE HERE\n        xavier = xavier_weight_init()\n        with tf.variable_scope(""transformation""):\n            b1 = tf.Variable(tf.random_uniform([self.config.hidden_size,]))\n            b2 = tf.Variable(tf.random_uniform([self.config.n_classes]))\n\n            self.W = W = xavier([self.config.n_features * self.config.embed_size, self.config.hidden_size])\n            U = xavier([self.config.hidden_size, self.config.n_classes])\n\n            z1 = tf.matmul(x,W) + b1\n            h = tf.nn.relu(z1)\n            h_drop = tf.nn.dropout(h,self.dropout_placeholder)\n            pred = tf.matmul(h_drop, U) + b2\n        ### END YOUR CODE\n        return pred\n\n    def add_loss_op(self, pred):\n        """"""Adds Ops for the loss function to the computational graph.\n        In this case we are using cross entropy loss.\n        The loss should be averaged over all examples in the current minibatch.\n\n        Hint: You can use tf.nn.softmax_cross_entropy_with_logits to simplify your\n                    implementation. You might find tf.reduce_mean useful.\n        Args:\n            pred: A tensor of shape (batch_size, n_classes) containing the output of the neural\n                  network before the softmax layer.\n        Returns:\n            loss: A 0-d tensor (scalar)\n        """"""\n        ### YOUR CODE HERE\n        loss = tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=self.labels_placeholder)\n        loss += self.beta_regul * tf.nn.l2_loss(self.W)\n        loss = tf.reduce_mean(loss)\n        ### END YOUR CODE\n        return loss\n\n    def add_training_op(self, loss):\n        """"""Sets up the training Ops.\n\n        Creates an optimizer and applies the gradients to all trainable variables.\n        The Op returned by this function is what must be passed to the\n        `sess.run()` call to cause the model to train. See\n\n        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n\n        for more information.\n\n        Use tf.train.AdamOptimizer for this model.\n        Calling optimizer.minimize() will return a train_op object.\n\n        Args:\n            loss: Loss tensor, from cross_entropy_loss.\n        Returns:\n            train_op: The Op for training.\n        """"""\n        ### YOUR CODE HERE\n        adam_optim = tf.train.AdamOptimizer(self.config.lr)\n        train_op = adam_optim.minimize(loss)\n        ### END YOUR CODE\n        return train_op\n\n    def train_on_batch(self, sess, inputs_batch, labels_batch):\n        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch,\n                                     dropout=self.config.dropout)\n        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n        return loss\n\n    def run_epoch(self, sess, parser, train_examples, dev_set):\n        prog = Progbar(target=1 + len(train_examples) / self.config.batch_size)\n        for i, (train_x, train_y) in enumerate(minibatches(train_examples, self.config.batch_size)):\n            loss = self.train_on_batch(sess, train_x, train_y)\n            prog.update(i + 1, [(""train loss"", loss)])\n\n        print ""Evaluating on dev set"",\n        dev_UAS, _ = parser.parse(dev_set)\n        print ""- dev UAS: {:.2f}"".format(dev_UAS * 100.0)\n        return dev_UAS\n\n    def fit(self, sess, saver, parser, train_examples, dev_set):\n        best_dev_UAS = 0\n        for epoch in range(self.config.n_epochs):\n            print ""Epoch {:} out of {:}"".format(epoch + 1, self.config.n_epochs)\n            dev_UAS = self.run_epoch(sess, parser, train_examples, dev_set)\n            if dev_UAS > best_dev_UAS:\n                best_dev_UAS = dev_UAS\n                if saver:\n                    print ""New best dev UAS! Saving model in ./data/weights/parser.weights""\n                    saver.save(sess, \'./data/weights/parser.weights\')\n            print\n\n    def __init__(self, config, pretrained_embeddings):\n        self.pretrained_embeddings = pretrained_embeddings\n        self.config = config\n        self.build()\n\n\ndef main(debug=True):\n    print 80 * ""=""\n    print ""INITIALIZING""\n    print 80 * ""=""\n    config = Config()\n    parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(debug)\n    if not os.path.exists(\'./data/weights/\'):\n        os.makedirs(\'./data/weights/\')\n\n    with tf.Graph().as_default():\n        print ""Building model..."",\n        start = time.time()\n        model = ParserModel(config, embeddings)\n        parser.model = model\n        print ""took {:.2f} seconds\\n"".format(time.time() - start)\n\n        init = tf.global_variables_initializer()\n        # If you are using an old version of TensorFlow, you may have to use\n        # this initializer instead.\n        # init = tf.initialize_all_variables()\n        saver = None if debug else tf.train.Saver()\n\n        with tf.Session() as session:\n            parser.session = session\n            session.run(init)\n\n            print 80 * ""=""\n            print ""TRAINING""\n            print 80 * ""=""\n            model.fit(session, saver, parser, train_examples, dev_set)\n\n            if not debug:\n                print 80 * ""=""\n                print ""TESTING""\n                print 80 * ""=""\n                print ""Restoring the best model weights found on the dev set""\n                saver.restore(session, \'./data/weights/parser.weights\')\n                print ""Final evaluation on test set"",\n                UAS, dependencies = parser.parse(test_set)\n                print ""- test UAS: {:.2f}"".format(UAS * 100.0)\n                print ""Writing predictions""\n                with open(\'q2_test.predicted.pkl\', \'w\') as f:\n                    cPickle.dump(dependencies, f, -1)\n                print ""Done!""\n\nif __name__ == \'__main__\':\n    main(False)\n\n\n'"
assignment2/q2_parser_transitions.py,0,"b'class PartialParse(object):\n    def __init__(self, sentence):\n        """"""Initializes this partial parse.\n\n        Your code should initialize the following fields:\n            self.stack: The current stack represented as a list with the top of the stack as the\n                        last element of the list.\n            self.buffer: The current buffer represented as a list with the first item on the\n                         buffer as the first item of the list\n            self.dependencies: The list of dependencies produced so far. Represented as a list of\n                    tuples where each tuple is of the form (head, dependent).\n                    Order for this list doesn\'t matter.\n\n        The root token should be represented with the string ""ROOT""\n\n        Args:\n            sentence: The sentence to be parsed as a list of words.\n                      Your code should not modify the sentence.\n        """"""\n        # The sentence being parsed is kept for bookkeeping purposes. Do not use it in your code.\n        self.sentence = sentence\n\n        ### YOUR CODE HERE\n        self.stack = [\'ROOT\']\n        self.buffer = sentence[:]\n        self.dependencies = []\n        ### END YOUR CODE\n\n    def parse_step(self, transition):\n        """"""Performs a single parse step by applying the given transition to this partial parse\n\n        Args:\n            transition: A string that equals ""S"", ""LA"", or ""RA"" representing the shift, left-arc,\n                        and right-arc transitions.\n        """"""\n        ### YOUR CODE HERE\n        if transition == ""S"":\n            self.stack.append(self.buffer[0])\n            self.buffer.pop(0)\n        elif transition == ""LA"":\n            self.dependencies.append((self.stack[-1], self.stack[-2]))\n            self.stack.pop(-2)\n        else:\n            self.dependencies.append((self.stack[-2], self.stack[-1]))\n            self.stack.pop(-1)\n            ### END YOUR CODE\n\n    def parse(self, transitions):\n        """"""Applies the provided transitions to this PartialParse\n\n        Args:\n            transitions: The list of transitions in the order they should be applied\n        Returns:\n            dependencies: The list of dependencies produced when parsing the sentence. Represented\n                          as a list of tuples where each tuple is of the form (head, dependent)\n        """"""\n        for transition in transitions:\n            self.parse_step(transition)\n        return self.dependencies\n\n\ndef minibatch_parse(sentences, model, batch_size):\n    """"""Parses a list of sentences in minibatches using a model.\n\n    Args:\n        sentences: A list of sentences to be parsed (each sentence is a list of words)\n        model: The model that makes parsing decisions. It is assumed to have a function\n               model.predict(partial_parses) that takes in a list of PartialParses as input and\n               returns a list of transitions predicted for each parse. That is, after calling\n                   transitions = model.predict(partial_parses)\n               transitions[i] will be the next transition to apply to partial_parses[i].\n        batch_size: The number of PartialParses to include in each minibatch\n    Returns:\n        dependencies: A list where each element is the dependencies list for a parsed sentence.\n                      Ordering should be the same as in sentences (i.e., dependencies[i] should\n                      contain the parse for sentences[i]).\n    """"""\n\n    ### YOUR CODE HERE\n    # refer: https://github.com/zysalice/cs224/blob/master/assignment2/q2_parser_transitions.py\n    partial_parses = [PartialParse(s) for s in sentences]\n\n    unfinished_parse = partial_parses\n\n    while len(unfinished_parse) > 0:\n        minibatch = unfinished_parse[0:batch_size]\n        # perform transition and single step parser on the minibatch until it is empty\n        while len(minibatch) > 0:\n            transitions = model.predict(minibatch)\n            for index, action in enumerate(transitions):\n                minibatch[index].parse_step(action)\n            minibatch = [parse for parse in minibatch if len(parse.stack) > 1 or len(parse.buffer) > 0]\n\n        # move to the next batch\n        unfinished_parse = unfinished_parse[batch_size:]\n\n    dependencies = []\n    for n in range(len(sentences)):\n        dependencies.append(partial_parses[n].dependencies)\n    ### END YOUR CODE\n\n    return dependencies\n\n\ndef test_step(name, transition, stack, buf, deps,\n              ex_stack, ex_buf, ex_deps):\n    """"""Tests that a single parse step returns the expected output""""""\n    pp = PartialParse([])\n    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n\n    pp.parse_step(transition)\n    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n    assert stack == ex_stack, \\\n        ""{:} test resulted in stack {:}, expected {:}"".format(name, stack, ex_stack)\n    assert buf == ex_buf, \\\n        ""{:} test resulted in buffer {:}, expected {:}"".format(name, buf, ex_buf)\n    assert deps == ex_deps, \\\n        ""{:} test resulted in dependency list {:}, expected {:}"".format(name, deps, ex_deps)\n    print ""{:} test passed!"".format(name)\n\n\ndef test_parse_step():\n    """"""Simple tests for the PartialParse.parse_step function\n    Warning: these are not exhaustive\n    """"""\n    test_step(""SHIFT"", ""S"", [""ROOT"", ""the""], [""cat"", ""sat""], [],\n              (""ROOT"", ""the"", ""cat""), (""sat"",), ())\n    test_step(""LEFT-ARC"", ""LA"", [""ROOT"", ""the"", ""cat""], [""sat""], [],\n              (""ROOT"", ""cat"",), (""sat"",), ((""cat"", ""the""),))\n    test_step(""RIGHT-ARC"", ""RA"", [""ROOT"", ""run"", ""fast""], [], [],\n              (""ROOT"", ""run"",), (), ((""run"", ""fast""),))\n\n\ndef test_parse():\n    """"""Simple tests for the PartialParse.parse function\n    Warning: these are not exhaustive\n    """"""\n    sentence = [""parse"", ""this"", ""sentence""]\n    dependencies = PartialParse(sentence).parse([""S"", ""S"", ""S"", ""LA"", ""RA"", ""RA""])\n    dependencies = tuple(sorted(dependencies))\n    expected = ((\'ROOT\', \'parse\'), (\'parse\', \'sentence\'), (\'sentence\', \'this\'))\n    assert dependencies == expected, \\\n        ""parse test resulted in dependencies {:}, expected {:}"".format(dependencies, expected)\n    assert tuple(sentence) == (""parse"", ""this"", ""sentence""), \\\n        ""parse test failed: the input sentence should not be modified""\n    print ""parse test passed!""\n\n\nclass DummyModel:\n    """"""Dummy model for testing the minibatch_parse function\n    First shifts everything onto the stack and then does exclusively right arcs if the first word of\n    the sentence is ""right"", ""left"" if otherwise.\n    """"""\n\n    def predict(self, partial_parses):\n        return [(""RA"" if pp.stack[1] is ""right"" else ""LA"") if len(pp.buffer) == 0 else ""S""\n                for pp in partial_parses]\n\n\ndef test_dependencies(name, deps, ex_deps):\n    """"""Tests the provided dependencies match the expected dependencies""""""\n    deps = tuple(sorted(deps))\n    assert deps == ex_deps, \\\n        ""{:} test resulted in dependency list {:}, expected {:}"".format(name, deps, ex_deps)\n\n\ndef test_minibatch_parse():\n    """"""Simple tests for the minibatch_parse function\n    Warning: these are not exhaustive\n    """"""\n    sentences = [[""right"", ""arcs"", ""only""],\n                 [""right"", ""arcs"", ""only"", ""again""],\n                 [""left"", ""arcs"", ""only""],\n                 [""left"", ""arcs"", ""only"", ""again""]]\n    deps = minibatch_parse(sentences, DummyModel(), 2)\n    test_dependencies(""minibatch_parse"", deps[0],\n                      ((\'ROOT\', \'right\'), (\'arcs\', \'only\'), (\'right\', \'arcs\')))\n    test_dependencies(""minibatch_parse"", deps[1],\n                      ((\'ROOT\', \'right\'), (\'arcs\', \'only\'), (\'only\', \'again\'), (\'right\', \'arcs\')))\n    test_dependencies(""minibatch_parse"", deps[2],\n                      ((\'only\', \'ROOT\'), (\'only\', \'arcs\'), (\'only\', \'left\')))\n    test_dependencies(""minibatch_parse"", deps[3],\n                      ((\'again\', \'ROOT\'), (\'again\', \'arcs\'), (\'again\', \'left\'), (\'again\', \'only\')))\n    print ""minibatch_parse test passed!""\n\n\nif __name__ == \'__main__\':\n    test_parse_step()\n    test_parse()\n    test_minibatch_parse()\n'"
assignment3/data_util.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nUtility functions to process data.\n""""""\nimport os\nimport pickle\nimport logging\nfrom collections import Counter\n\nimport numpy as np\nfrom util import read_conll, one_hot, window_iterator, ConfusionMatrix, load_word_vector_mapping\nfrom defs import LBLS, NONE, LMAP, NUM, UNK, EMBED_SIZE\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogging.basicConfig(format=\'%(levelname)s:%(message)s\', level=logging.DEBUG)\n\n\nFDIM = 4\nP_CASE = ""CASE:""\nCASES = [""aa"", ""AA"", ""Aa"", ""aA""]\nSTART_TOKEN = ""<s>""\nEND_TOKEN = ""</s>""\n\ndef casing(word):\n    if len(word) == 0: return word\n\n    # all lowercase\n    if word.islower(): return ""aa""\n    # all uppercase\n    elif word.isupper(): return ""AA""\n    # starts with capital\n    elif word[0].isupper(): return ""Aa""\n    # has non-initial capital\n    else: return ""aA""\n\ndef normalize(word):\n    """"""\n    Normalize words that are numbers or have casing.\n    """"""\n    if word.isdigit(): return NUM\n    else: return word.lower()\n\ndef featurize(embeddings, word):\n    """"""\n    Featurize a word given embeddings.\n    """"""\n    case = casing(word)\n    word = normalize(word)\n    case_mapping = {c: one_hot(FDIM, i) for i, c in enumerate(CASES)}\n    wv = embeddings.get(word, embeddings[UNK])\n    fv = case_mapping[case]\n    return np.hstack((wv, fv))\n\ndef evaluate(model, X, Y):\n    cm = ConfusionMatrix(labels=LBLS)\n    Y_ = model.predict(X)\n    for i in range(Y.shape[0]):\n        y, y_ = np.argmax(Y[i]), np.argmax(Y_[i])\n        cm.update(y,y_)\n    cm.print_table()\n    return cm.summary()\n\nclass ModelHelper(object):\n    """"""\n    This helper takes care of preprocessing data, constructing embeddings, etc.\n    """"""\n    def __init__(self, tok2id, max_length):\n        self.tok2id = tok2id\n        self.START = [tok2id[START_TOKEN], tok2id[P_CASE + ""aa""]]\n        self.END = [tok2id[END_TOKEN], tok2id[P_CASE + ""aa""]]\n        self.max_length = max_length\n\n    def vectorize_example(self, sentence, labels=None):\n        sentence_ = [[self.tok2id.get(normalize(word), self.tok2id[UNK]), self.tok2id[P_CASE + casing(word)]] for word in sentence]\n        if labels:\n            labels_ = [LBLS.index(l) for l in labels]\n            return sentence_, labels_\n        else:\n            return sentence_, [LBLS[-1] for _ in sentence]\n\n    def vectorize(self, data):\n        return [self.vectorize_example(sentence, labels) for sentence, labels in data]\n\n    @classmethod\n    def build(cls, data):\n        # Preprocess data to construct an embedding\n        # Reserve 0 for the special NIL token.\n        tok2id = build_dict((normalize(word) for sentence, _ in data for word in sentence), offset=1, max_words=10000)\n        tok2id.update(build_dict([P_CASE + c for c in CASES], offset=len(tok2id)))\n        tok2id.update(build_dict([START_TOKEN, END_TOKEN, UNK], offset=len(tok2id)))\n        assert sorted(tok2id.items(), key=lambda t: t[1])[0][1] == 1\n        logger.info(""Built dictionary for %d features."", len(tok2id))\n\n        max_length = max(len(sentence) for sentence, _ in data)\n\n        return cls(tok2id, max_length)\n\n    def save(self, path):\n        # Make sure the directory exists.\n        if not os.path.exists(path):\n            os.makedirs(path)\n        # Save the tok2id map.\n        with open(os.path.join(path, ""features.pkl""), ""w"") as f:\n            pickle.dump([self.tok2id, self.max_length], f)\n\n    @classmethod\n    def load(cls, path):\n        # Make sure the directory exists.\n        assert os.path.exists(path) and os.path.exists(os.path.join(path, ""features.pkl""))\n        # Save the tok2id map.\n        with open(os.path.join(path, ""features.pkl"")) as f:\n            tok2id, max_length = pickle.load(f)\n        return cls(tok2id, max_length)\n\ndef load_and_preprocess_data(args):\n    logger.info(""Loading training data..."")\n    train = read_conll(args.data_train)\n    logger.info(""Done. Read %d sentences"", len(train))\n    logger.info(""Loading dev data..."")\n    dev = read_conll(args.data_dev)\n    logger.info(""Done. Read %d sentences"", len(dev))\n\n    helper = ModelHelper.build(train)\n\n    # now process all the input data.\n    train_data = helper.vectorize(train)\n    dev_data = helper.vectorize(dev)\n\n    return helper, train_data, dev_data, train, dev\n\ndef load_embeddings(args, helper):\n    embeddings = np.array(np.random.randn(len(helper.tok2id) + 1, EMBED_SIZE), dtype=np.float32)\n    embeddings[0] = 0.\n    for word, vec in load_word_vector_mapping(args.vocab, args.vectors).items():\n        word = normalize(word)\n        if word in helper.tok2id:\n            embeddings[helper.tok2id[word]] = vec\n    logger.info(""Initialized embeddings."")\n\n    return embeddings\n\ndef build_dict(words, max_words=None, offset=0):\n    cnt = Counter(words)\n    if max_words:\n        words = cnt.most_common(max_words)\n    else:\n        words = cnt.most_common()\n    return {word: offset+i for i, (word, _) in enumerate(words)}\n\n\ndef get_chunks(seq, default=LBLS.index(NONE)):\n    """"""Breaks input of 4 4 4 0 0 4 0 ->   (0, 4, 5), (0, 6, 7)""""""\n    chunks = []\n    chunk_type, chunk_start = None, None\n    for i, tok in enumerate(seq):\n        # End of a chunk 1\n        if tok == default and chunk_type is not None:\n            # Add a chunk.\n            chunk = (chunk_type, chunk_start, i)\n            chunks.append(chunk)\n            chunk_type, chunk_start = None, None\n        # End of a chunk + start of a chunk!\n        elif tok != default:\n            if chunk_type is None:\n                chunk_type, chunk_start = tok, i\n            elif tok != chunk_type:\n                chunk = (chunk_type, chunk_start, i)\n                chunks.append(chunk)\n                chunk_type, chunk_start = tok, i\n        else:\n            pass\n    # end condition\n    if chunk_type is not None:\n        chunk = (chunk_type, chunk_start, len(seq))\n        chunks.append(chunk)\n    return chunks\n\ndef test_get_chunks():\n    assert get_chunks([4, 4, 4, 0, 0, 4, 1, 2, 4, 3], 4) == [(0,3,5), (1, 6, 7), (2, 7, 8), (3,9,10)]\n'"
assignment3/defs.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCommon definitions for NER\n""""""\n\nfrom util import one_hot\n\nLBLS = [\n    ""PER"",\n    ""ORG"",\n    ""LOC"",\n    ""MISC"",\n    ""O"",\n    ]\nNONE = ""O""\nLMAP = {k: one_hot(5,i) for i, k in enumerate(LBLS)}\nNUM = ""NNNUMMM""\nUNK = ""UUUNKKK""\n\nEMBED_SIZE = 50\n'"
assignment3/model.py,2,"b'class Model(object):\n    """"""Abstracts a Tensorflow graph for a learning task.\n\n    We use various Model classes as usual abstractions to encapsulate tensorflow\n    computational graphs. Each algorithm you will construct in this homework will\n    inherit from a Model object.\n    """"""\n    def add_placeholders(self):\n        """"""Adds placeholder variables to tensorflow computational graph.\n\n        Tensorflow uses placeholder variables to represent locations in a\n        computational graph where data is inserted.  These placeholders are used as\n        inputs by the rest of the model building and will be fed data during\n        training.\n\n        See for more information:\n        https://www.tensorflow.org/versions/r0.7/api_docs/python/io_ops.html#placeholders\n        """"""\n        raise NotImplementedError(""Each Model must re-implement this method."")\n\n    def create_feed_dict(self, inputs_batch, labels_batch=None):\n        """"""Creates the feed_dict for one step of training.\n\n        A feed_dict takes the form of:\n        feed_dict = {\n                <placeholder>: <tensor of values to be passed for placeholder>,\n                ....\n        }\n\n        If labels_batch is None, then no labels are added to feed_dict.\n\n        Hint: The keys for the feed_dict should be a subset of the placeholder\n                    tensors created in add_placeholders.\n        Args:\n            inputs_batch: A batch of input data.\n            labels_batch: A batch of label data.\n        Returns:\n            feed_dict: The feed dictionary mapping from placeholders to values.\n        """"""\n        raise NotImplementedError(""Each Model must re-implement this method."")\n\n    def add_prediction_op(self):\n        """"""Implements the core of the model that transforms a batch of input data into predictions.\n\n        Returns:\n            pred: A tensor of shape (batch_size, n_classes)\n        """"""\n        raise NotImplementedError(""Each Model must re-implement this method."")\n\n    def add_loss_op(self, pred):\n        """"""Adds Ops for the loss function to the computational graph.\n\n        Args:\n            pred: A tensor of shape (batch_size, n_classes)\n        Returns:\n            loss: A 0-d tensor (scalar) output\n        """"""\n        raise NotImplementedError(""Each Model must re-implement this method."")\n\n    def add_training_op(self, loss):\n        """"""Sets up the training Ops.\n\n        Creates an optimizer and applies the gradients to all trainable variables.\n        The Op returned by this function is what must be passed to the\n        sess.run() to train the model. See\n\n        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n\n        for more information.\n\n        Args:\n            loss: Loss tensor (a scalar).\n        Returns:\n            train_op: The Op for training.\n        """"""\n\n        raise NotImplementedError(""Each Model must re-implement this method."")\n\n    def train_on_batch(self, sess, inputs_batch, labels_batch):\n        """"""Perform one step of gradient descent on the provided batch of data.\n\n        Args:\n            sess: tf.Session()\n            input_batch: np.ndarray of shape (n_samples, n_features)\n            labels_batch: np.ndarray of shape (n_samples, n_classes)\n        Returns:\n            loss: loss over the batch (a scalar)\n        """"""\n        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch)\n        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n        return loss\n\n    def predict_on_batch(self, sess, inputs_batch):\n        """"""Make predictions for the provided batch of data\n\n        Args:\n            sess: tf.Session()\n            input_batch: np.ndarray of shape (n_samples, n_features)\n        Returns:\n            predictions: np.ndarray of shape (n_samples, n_classes)\n        """"""\n        feed = self.create_feed_dict(inputs_batch)\n        predictions = sess.run(self.pred, feed_dict=feed)\n        return predictions\n\n    def build(self):\n        self.add_placeholders()\n        self.pred = self.add_prediction_op()\n        self.loss = self.add_loss_op(self.pred)\n        self.train_op = self.add_training_op(self.loss)\n'"
assignment3/ner_model.py,0,"b'#!/usr/bin/env python2.7\n# -*- coding: utf-8 -*-\n""""""\nA model for named entity recognition.\n""""""\nimport pdb\nimport logging\n\nimport tensorflow as tf\nfrom util import ConfusionMatrix, Progbar, minibatches\nfrom data_util import get_chunks\nfrom model import Model\nfrom defs import LBLS\n\nlogger = logging.getLogger(""hw3"")\nlogger.setLevel(logging.DEBUG)\nlogging.basicConfig(format=\'%(levelname)s:%(message)s\', level=logging.DEBUG)\n\nclass NERModel(Model):\n    """"""\n    Implements special functionality for NER models.\n    """"""\n\n    def __init__(self, helper, config, report=None):\n        self.helper = helper\n        self.config = config\n        self.report = report\n\n    def preprocess_sequence_data(self, examples):\n        """"""Preprocess sequence data for the model.\n\n        Args:\n            examples: A list of vectorized input/output sequences.\n        Returns:\n            A new list of vectorized input/output pairs appropriate for the model.\n        """"""\n        raise NotImplementedError(""Each Model must re-implement this method."")\n\n    def consolidate_predictions(self, data_raw, data, preds):\n        """"""\n        Convert a sequence of predictions according to the batching\n        process back into the original sequence.\n        """"""\n        raise NotImplementedError(""Each Model must re-implement this method."")\n\n\n    def evaluate(self, sess, examples, examples_raw):\n        """"""Evaluates model performance on @examples.\n\n        This function uses the model to predict labels for @examples and constructs a confusion matrix.\n\n        Args:\n            sess: the current TensorFlow session.\n            examples: A list of vectorized input/output pairs.\n            examples: A list of the original input/output sequence pairs.\n        Returns:\n            The F1 score for predicting tokens as named entities.\n        """"""\n        token_cm = ConfusionMatrix(labels=LBLS)\n\n        correct_preds, total_correct, total_preds = 0., 0., 0.\n        for _, labels, labels_  in self.output(sess, examples_raw, examples):\n            for l, l_ in zip(labels, labels_):\n                token_cm.update(l, l_)\n            gold = set(get_chunks(labels))\n            pred = set(get_chunks(labels_))\n            correct_preds += len(gold.intersection(pred))\n            total_preds += len(pred)\n            total_correct += len(gold)\n\n        p = correct_preds / total_preds if correct_preds > 0 else 0\n        r = correct_preds / total_correct if correct_preds > 0 else 0\n        f1 = 2 * p * r / (p + r) if correct_preds > 0 else 0\n        return token_cm, (p, r, f1)\n\n\n    def run_epoch(self, sess, train_examples, dev_set, train_examples_raw, dev_set_raw):\n        prog = Progbar(target=1 + int(len(train_examples) / self.config.batch_size))\n        for i, batch in enumerate(minibatches(train_examples, self.config.batch_size)):\n            loss = self.train_on_batch(sess, *batch)\n            prog.update(i + 1, [(""train loss"", loss)])\n            if self.report: self.report.log_train_loss(loss)\n        print("""")\n\n        #logger.info(""Evaluating on training data"")\n        #token_cm, entity_scores = self.evaluate(sess, train_examples, train_examples_raw)\n        #logger.debug(""Token-level confusion matrix:\\n"" + token_cm.as_table())\n        #logger.debug(""Token-level scores:\\n"" + token_cm.summary())\n        #logger.info(""Entity level P/R/F1: %.2f/%.2f/%.2f"", *entity_scores)\n\n        logger.info(""Evaluating on development data"")\n        token_cm, entity_scores = self.evaluate(sess, dev_set, dev_set_raw)\n        logger.debug(""Token-level confusion matrix:\\n"" + token_cm.as_table())\n        logger.debug(""Token-level scores:\\n"" + token_cm.summary())\n        logger.info(""Entity level P/R/F1: %.2f/%.2f/%.2f"", *entity_scores)\n\n        f1 = entity_scores[-1]\n        return f1\n\n    def output(self, sess, inputs_raw, inputs=None):\n        """"""\n        Reports the output of the model on examples (uses helper to featurize each example).\n        """"""\n        if inputs is None:\n            inputs = self.preprocess_sequence_data(self.helper.vectorize(inputs_raw))\n\n        preds = []\n        prog = Progbar(target=1 + int(len(inputs) / self.config.batch_size))\n        for i, batch in enumerate(minibatches(inputs, self.config.batch_size, shuffle=False)):\n            # Ignore predict\n            batch = batch[:1] + batch[2:]\n            preds_ = self.predict_on_batch(sess, *batch)\n            preds += list(preds_)\n            prog.update(i + 1, [])\n        return self.consolidate_predictions(inputs_raw, inputs, preds)\n\n    def fit(self, sess, saver, train_examples_raw, dev_set_raw):\n        best_score = 0.\n\n        train_examples = self.preprocess_sequence_data(train_examples_raw)\n        dev_set = self.preprocess_sequence_data(dev_set_raw)\n\n        for epoch in range(self.config.n_epochs):\n            logger.info(""Epoch %d out of %d"", epoch + 1, self.config.n_epochs)\n            score = self.run_epoch(sess, train_examples, dev_set, train_examples_raw, dev_set_raw)\n            if score > best_score:\n                best_score = score\n                if saver:\n                    logger.info(""New best score! Saving model in %s"", self.config.model_output)\n                    saver.save(sess, self.config.model_output)\n            print("""")\n            if self.report:\n                self.report.log_epoch()\n                self.report.save()\n        return best_score\n'"
assignment3/q1_window.py,52,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nQ1: A window into NER\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport argparse\nimport sys\nimport time\nimport logging\nfrom datetime import datetime\n\nimport tensorflow as tf\n\nfrom util import print_sentence, write_conll\nfrom data_util import load_and_preprocess_data, load_embeddings, read_conll, ModelHelper\nfrom ner_model import NERModel\nfrom defs import LBLS\n\n# from report import Report\n\nlogger = logging.getLogger(""hw3.q1"")\nlogger.setLevel(logging.DEBUG)\nlogging.basicConfig(format=\'%(levelname)s:%(message)s\', level=logging.DEBUG)\n\n\nclass Config:\n    """"""Holds model hyperparams and data information.\n\n    The config class is used to store various hyperparameters and dataset\n    information parameters. Model objects are passed a Config() object at\n    instantiation.\n\n    TODO: Fill in what n_window_features should be, using n_word_features and window_size.\n    """"""\n    n_word_features = 2  # Number of features for every word in the input.\n    window_size = 1  # The size of the window to use.\n    ### YOUR CODE HERE\n    n_window_features = (2 * window_size + 1) * n_word_features  # The total number of features used for each window.\n    ### END YOUR CODE\n    n_classes = 5\n    dropout = 0.5\n    embed_size = 50\n    hidden_size = 200\n    batch_size = 2048\n    n_epochs = 10\n    lr = 0.001\n\n    def __init__(self, output_path=None):\n        if output_path:\n            # Where to save things.\n            self.output_path = output_path\n        else:\n            self.output_path = ""results/window/{:%Y%m%d_%H%M%S}/"".format(datetime.now())\n        self.model_output = self.output_path + ""model.weights""\n        self.eval_output = self.output_path + ""results.txt""\n        self.log_output = self.output_path + ""log""\n        self.conll_output = self.output_path + ""window_predictions.conll""\n\n\ndef make_windowed_data(data, start, end, window_size=1):\n    """"""Uses the input sequences in @data to construct new windowed data points.\n\n    TODO: In the code below, construct a window from each word in the\n    input sentence by concatenating the words @window_size to the left\n    and @window_size to the right to the word. Finally, add this new\n    window data point and its label. to windowed_data.\n\n    Args:\n        data: is a list of (sentence, labels) tuples. @sentence is a list\n            containing the words in the sentence and @label is a list of\n            output labels. Each word is itself a list of\n            @n_features features. For example, the sentence ""Chris\n            Manning is amazing"" and labels ""PER PER O O"" would become\n            ([[1,9], [2,9], [3,8], [4,8]], [1, 1, 4, 4]). Here ""Chris""\n            the word has been featurized as ""[1, 9]"", and ""[1, 1, 4, 4]""\n            is the list of labels.\n        start: the featurized `start\' token to be used for windows at the very\n            beginning of the sentence.\n        end: the featurized `end\' token to be used for windows at the very\n            end of the sentence.\n        window_size: the length of the window to construct.\n    Returns:\n        a new list of data points, corresponding to each window in the\n        sentence. Each data point consists of a list of\n        @n_window_features features (corresponding to words from the\n        window) to be used in the sentence and its NER label.\n        If start=[5,8] and end=[6,8], the above example should return\n        the list\n        [([5, 8, 1, 9, 2, 9], 1),\n         ([1, 9, 2, 9, 3, 8], 1),\n         ...\n         ]\n    """"""\n\n    windowed_data = []\n    for sentence, labels in data:\n    ### YOUR CODE HERE (5-20 lines)\n        orig_n = len(sentence)\n        # extend sentence\n        sentence = [start] * window_size + sentence + [end] * window_size\n        l = 0  # index labels\n        # loop over the original sentence\n        for i in range(window_size, orig_n + window_size):\n            temp_feats = []\n            # loop over the window for feature in original sentence\n            for j in range(i - window_size, i + window_size + 1):\n                temp_feats.extend(sentence[j])\n\n            # put token features together with label:\n            temp_f_l = (temp_feats, labels[l])\n            # put into windowed data:\n            windowed_data.append(temp_f_l)\n            # iterate our labels index\n            l += 1\n    ### END YOUR CODE\n    return windowed_data\n\n\nclass WindowModel(NERModel):\n    """"""\n    Implements a feedforward neural network with an embedding layer and\n    single hidden layer.\n    This network will predict what label (e.g. PER) should be given to a\n    given token (e.g. Manning) by  using a featurized window around the token.\n    """"""\n\n    def add_placeholders(self):\n        """"""Generates placeholder variables to represent the input tensors\n\n        These placeholders are used as inputs by the rest of the model building and will be fed\n        data during training.  Note that when ""None"" is in a placeholder\'s shape, it\'s flexible\n        (so we can use different batch sizes without rebuilding the model).\n\n        Adds following nodes to the computational graph\n\n        input_placeholder: Input placeholder tensor of  shape (None, n_window_features), type tf.int32\n        labels_placeholder: Labels placeholder tensor of shape (None,), type tf.int32\n        dropout_placeholder: Dropout value placeholder (scalar), type tf.float32\n\n        Add these placeholders to self as the instance variables\n            self.input_placeholder\n            self.labels_placeholder\n            self.dropout_placeholder\n\n        (Don\'t change the variable names)\n        """"""\n        ### YOUR CODE HERE (~3-5 lines)\n        self.input_placeholder = tf.placeholder(tf.int32, [None, self.config.n_window_features])\n        self.labels_placeholder = tf.placeholder(tf.int32, [None,])\n        self.dropout_placeholder = tf.placeholder(tf.float32)\n        ### END YOUR CODE\n\n    def create_feed_dict(self, inputs_batch, labels_batch=None, dropout=1):\n        """"""Creates the feed_dict for the model.\n        A feed_dict takes the form of:\n        feed_dict = {\n                <placeholder>: <tensor of values to be passed for placeholder>,\n                ....\n        }\n\n        Hint: The keys for the feed_dict should be a subset of the placeholder\n                    tensors created in add_placeholders.\n        Hint: When an argument is None, don\'t add it to the feed_dict.\n\n        Args:\n            inputs_batch: A batch of input data.\n            labels_batch: A batch of label data.\n            dropout: The dropout rate.\n        Returns:\n            feed_dict: The feed dictionary mapping from placeholders to values.\n        """"""\n        ### YOUR CODE HERE (~5-10 lines)\n        feed_dict = {self.input_placeholder: inputs_batch, \\\n                     self.dropout_placeholder: dropout}\n        if labels_batch is not None:\n            feed_dict[self.labels_placeholder] = labels_batch\n        ### END YOUR CODE\n        return feed_dict\n\n    def add_embedding(self):\n        """"""Adds an embedding layer that maps from input tokens (integers) to vectors and then\n        concatenates those vectors:\n            - Creates an embedding tensor and initializes it with self.pretrained_embeddings.\n            - Uses the input_placeholder to index into the embeddings tensor, resulting in a\n              tensor of shape (None, n_window_features, embedding_size).\n            - Concatenates the embeddings by reshaping the embeddings tensor to shape\n              (None, n_window_features * embedding_size).\n\n        Hint: You might find tf.nn.embedding_lookup useful.\n        Hint: You can use tf.reshape to concatenate the vectors. See following link to understand\n            what -1 in a shape means.\n            https://www.tensorflow.org/api_docs/python/array_ops/shapes_and_shaping#reshape.\n        Returns:\n            embeddings: tf.Tensor of shape (None, n_window_features*embed_size)\n        """"""\n        ### YOUR CODE HERE (!3-5 lines)\n        embedded = tf.Variable(self.pretrained_embeddings)\n        embeddings = tf.nn.embedding_lookup(embedded,self.input_placeholder)\n        embeddings = tf.reshape(embeddings, [-1, self.config.n_window_features * self.config.embed_size])\n        ### END YOUR CODE\n        return embeddings\n\n    def add_prediction_op(self):\n        """"""Adds the 1-hidden-layer NN:\n            h = Relu(xW + b1)\n            h_drop = Dropout(h, dropout_rate)\n            pred = h_dropU + b2\n\n        Recall that we are not applying a softmax to pred. The softmax will instead be done in\n        the add_loss_op function, which improves efficiency because we can use\n        tf.nn.softmax_cross_entropy_with_logits\n\n        When creating a new variable, use the tf.get_variable function\n        because it lets us specify an initializer.\n\n        Use tf.contrib.layers.xavier_initializer to initialize matrices.\n        This is TensorFlow\'s implementation of the Xavier initialization\n        trick we used in last assignment.\n\n        Note: tf.nn.dropout takes the keep probability (1 - p_drop) as an argument.\n            The keep probability should be set to the value of dropout_rate.\n\n        Returns:\n            pred: tf.Tensor of shape (batch_size, n_classes)\n        """"""\n\n        x = self.add_embedding()\n        dropout_rate = self.dropout_placeholder\n        ### YOUR CODE HERE (~10-20 lines)\n        b1 = tf.get_variable(name=\'b1\', shape = [self.config.hidden_size,], \\\n                             initializer=tf.contrib.layers.xavier_initializer(seed=1))\n        b2 = tf.get_variable(name=\'b2\', shape = [self.config.n_classes], \\\n                             initializer=tf.contrib.layers.xavier_initializer(seed=2))\n\n        W = tf.get_variable(name=\'W\', shape = [self.config.n_window_features * self.config.embed_size, self.config.hidden_size], \\\n                            initializer=tf.contrib.layers.xavier_initializer(seed=3))\n\n        U = tf.get_variable(name=\'U\', shape = [self.config.hidden_size, self.config.n_classes], \\\n                            initializer=tf.contrib.layers.xavier_initializer(seed=4))\n\n        z1 = tf.matmul(x,W) + b1\n        h = tf.nn.relu(z1)\n        h_drop = tf.nn.dropout(h, dropout_rate)\n        pred = tf.matmul(h_drop,U) + b2\n        ### END YOUR CODE\n        return pred\n\n    def add_loss_op(self, pred):\n        """"""Adds Ops for the loss function to the computational graph.\n        In this case we are using cross entropy loss.\n        The loss should be averaged over all examples in the current minibatch.\n\n        Remember that you can use tf.nn.sparse_softmax_cross_entropy_with_logits to simplify your\n        implementation. You might find tf.reduce_mean useful.\n        Args:\n            pred: A tensor of shape (batch_size, n_classes) containing the output of the neural\n                  network before the softmax layer.\n        Returns:\n            loss: A 0-d tensor (scalar)\n        """"""\n        ### YOUR CODE HERE (~2-5 lines)\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=self.labels_placeholder)\n        loss = tf.reduce_mean(loss)\n        ### END YOUR CODE\n        return loss\n\n    def add_training_op(self, loss):\n        """"""Sets up the training Ops.\n\n        Creates an optimizer and applies the gradients to all trainable variables.\n        The Op returned by this function is what must be passed to the\n        `sess.run()` call to cause the model to train. See\n\n        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n\n        for more information.\n\n        Use tf.train.AdamOptimizer for this model.\n        Calling optimizer.minimize() will return a train_op object.\n\n        Args:\n            loss: Loss tensor, from cross_entropy_loss.\n        Returns:\n            train_op: The Op for training.\n        """"""\n        ### YOUR CODE HERE (~1-2 lines)\n        adam_optim = tf.train.AdamOptimizer(self.config.lr)\n        train_op = adam_optim.minimize(loss)\n        ### END YOUR CODE\n        return train_op\n\n    def preprocess_sequence_data(self, examples):\n        return make_windowed_data(examples, start=self.helper.START, end=self.helper.END,\n                                  window_size=self.config.window_size)\n\n    def consolidate_predictions(self, examples_raw, examples, preds):\n        """"""Batch the predictions into groups of sentence length.\n        """"""\n        ret = []\n        # pdb.set_trace()\n        i = 0\n        for sentence, labels in examples_raw:\n            labels_ = preds[i:i + len(sentence)]\n            i += len(sentence)\n            ret.append([sentence, labels, labels_])\n        return ret\n\n    def predict_on_batch(self, sess, inputs_batch):\n        """"""Make predictions for the provided batch of data\n\n        Args:\n            sess: tf.Session()\n            input_batch: np.ndarray of shape (n_samples, n_features)\n        Returns:\n            predictions: np.ndarray of shape (n_samples, n_classes)\n        """"""\n        feed = self.create_feed_dict(inputs_batch)\n        predictions = sess.run(tf.argmax(self.pred, axis=1), feed_dict=feed)\n        return predictions\n\n    def train_on_batch(self, sess, inputs_batch, labels_batch):\n        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch,\n                                     dropout=self.config.dropout)\n        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n        return loss\n\n    def __init__(self, helper, config, pretrained_embeddings, report=None):\n        super(WindowModel, self).__init__(helper, config, report)\n        self.pretrained_embeddings = pretrained_embeddings\n\n        # Defining placeholders.\n        self.input_placeholder = None\n        self.labels_placeholder = None\n        self.dropout_placeholder = None\n\n        self.build()\n\n\ndef test_make_windowed_data():\n    sentences = [[[1, 1], [2, 0], [3, 3]]]\n    sentence_labels = [[1, 2, 3]]\n    data = zip(sentences, sentence_labels)\n    w_data = make_windowed_data(data, start=[5, 0], end=[6, 0], window_size=1)\n\n    assert len(w_data) == sum(len(sentence) for sentence in sentences)\n\n    assert w_data == [\n        ([5, 0] + [1, 1] + [2, 0], 1,),\n        ([1, 1] + [2, 0] + [3, 3], 2,),\n        ([2, 0] + [3, 3] + [6, 0], 3,),\n    ]\n\n\ndef do_test1(_):\n    logger.info(""Testing make_windowed_data"")\n    test_make_windowed_data()\n    logger.info(""Passed!"")\n\n\ndef do_test2(args):\n    logger.info(""Testing implementation of WindowModel"")\n    config = Config()\n    helper, train, dev, train_raw, dev_raw = load_and_preprocess_data(args)\n    embeddings = load_embeddings(args, helper)\n    config.embed_size = embeddings.shape[1]\n\n    with tf.Graph().as_default():\n        logger.info(""Building model..."", )\n        start = time.time()\n        model = WindowModel(helper, config, embeddings)\n        logger.info(""took %.2f seconds"", time.time() - start)\n\n        init = tf.global_variables_initializer()\n        saver = None\n\n        with tf.Session() as session:\n            session.run(init)\n            model.fit(session, saver, train, dev)\n\n    logger.info(""Model did not crash!"")\n    logger.info(""Passed!"")\n\n\ndef do_train(args):\n    # Set up some parameters.\n    config = Config()\n    helper, train, dev, train_raw, dev_raw = load_and_preprocess_data(args)\n    embeddings = load_embeddings(args, helper)\n    config.embed_size = embeddings.shape[1]\n    helper.save(config.output_path)\n\n    handler = logging.FileHandler(config.log_output)\n    handler.setLevel(logging.DEBUG)\n    handler.setFormatter(logging.Formatter(\'%(asctime)s:%(levelname)s: %(message)s\'))\n    logging.getLogger().addHandler(handler)\n\n    report = None  # Report(Config.eval_output)\n\n    with tf.Graph().as_default():\n        logger.info(""Building model..."", )\n        start = time.time()\n        model = WindowModel(helper, config, embeddings)\n        logger.info(""took %.2f seconds"", time.time() - start)\n\n        init = tf.global_variables_initializer()\n        saver = tf.train.Saver()\n\n        with tf.Session() as session:\n            session.run(init)\n            model.fit(session, saver, train, dev)\n            if report:\n                report.log_output(model.output(session, dev_raw))\n                report.save()\n            else:\n                # Save predictions in a text file.\n                output = model.output(session, dev_raw)\n                sentences, labels, predictions = zip(*output)\n                predictions = [[LBLS[l] for l in preds] for preds in predictions]\n                output = zip(sentences, labels, predictions)\n\n                with open(model.config.conll_output, \'w\') as f:\n                    write_conll(f, output)\n                with open(model.config.eval_output, \'w\') as f:\n                    for sentence, labels, predictions in output:\n                        print_sentence(f, sentence, labels, predictions)\n\n\ndef do_evaluate(args):\n    config = Config(args.model_path)\n    helper = ModelHelper.load(args.model_path)\n    input_data = read_conll(args.data)\n    embeddings = load_embeddings(args, helper)\n    config.embed_size = embeddings.shape[1]\n\n    with tf.Graph().as_default():\n        logger.info(""Building model..."", )\n        start = time.time()\n        model = WindowModel(helper, config, embeddings)\n\n        logger.info(""took %.2f seconds"", time.time() - start)\n\n        init = tf.global_variables_initializer()\n        saver = tf.train.Saver()\n\n        with tf.Session() as session:\n            session.run(init)\n            saver.restore(session, model.config.model_output)\n            for sentence, labels, predictions in model.output(session, input_data):\n                predictions = [LBLS[l] for l in predictions]\n                print_sentence(args.output, sentence, labels, predictions)\n\n\ndef do_shell(args):\n    config = Config(args.model_path)\n    helper = ModelHelper.load(args.model_path)\n    embeddings = load_embeddings(args, helper)\n    config.embed_size = embeddings.shape[1]\n\n    with tf.Graph().as_default():\n        logger.info(""Building model..."", )\n        start = time.time()\n        model = WindowModel(helper, config, embeddings)\n        logger.info(""took %.2f seconds"", time.time() - start)\n\n        init = tf.global_variables_initializer()\n        saver = tf.train.Saver()\n\n        with tf.Session() as session:\n            session.run(init)\n            saver.restore(session, model.config.model_output)\n\n            print(""""""Welcome!\nYou can use this shell to explore the behavior of your model.\nPlease enter sentences with spaces between tokens, e.g.,\ninput> Germany \'s representative to the European Union \'s veterinary committee .\n"""""")\n            while True:\n                # Create simple REPL\n                try:\n                    sentence = raw_input(""input> "")\n                    tokens = sentence.strip().split("" "")\n                    for sentence, _, predictions in model.output(session, [(tokens, [""O""] * len(tokens))]):\n                        predictions = [LBLS[l] for l in predictions]\n                        print_sentence(sys.stdout, sentence, [""""] * len(tokens), predictions)\n                except EOFError:\n                    print(""Closing session."")\n                    break\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Trains and tests an NER model\')\n    subparsers = parser.add_subparsers()\n\n    command_parser = subparsers.add_parser(\'test1\', help=\'\')\n    command_parser.set_defaults(func=do_test1)\n\n    command_parser = subparsers.add_parser(\'test2\', help=\'\')\n    command_parser.add_argument(\'-dt\', \'--data-train\', type=argparse.FileType(\'r\'), default=""data/tiny.conll"",\n                                help=""Training data"")\n    command_parser.add_argument(\'-dd\', \'--data-dev\', type=argparse.FileType(\'r\'), default=""data/tiny.conll"",\n                                help=""Dev data"")\n    command_parser.add_argument(\'-v\', \'--vocab\', type=argparse.FileType(\'r\'), default=""data/vocab.txt"",\n                                help=""Path to vocabulary file"")\n    command_parser.add_argument(\'-vv\', \'--vectors\', type=argparse.FileType(\'r\'), default=""data/wordVectors.txt"",\n                                help=""Path to word vectors file"")\n    command_parser.set_defaults(func=do_test2)\n\n    command_parser = subparsers.add_parser(\'train\', help=\'\')\n    command_parser.add_argument(\'-dt\', \'--data-train\', type=argparse.FileType(\'r\'), default=""data/train.conll"",\n                                help=""Training data"")\n    command_parser.add_argument(\'-dd\', \'--data-dev\', type=argparse.FileType(\'r\'), default=""data/dev.conll"",\n                                help=""Dev data"")\n    command_parser.add_argument(\'-v\', \'--vocab\', type=argparse.FileType(\'r\'), default=""data/vocab.txt"",\n                                help=""Path to vocabulary file"")\n    command_parser.add_argument(\'-vv\', \'--vectors\', type=argparse.FileType(\'r\'), default=""data/wordVectors.txt"",\n                                help=""Path to word vectors file"")\n    command_parser.set_defaults(func=do_train)\n\n    command_parser = subparsers.add_parser(\'evaluate\', help=\'\')\n    command_parser.add_argument(\'-d\', \'--data\', type=argparse.FileType(\'r\'), default=""data/dev.conll"",\n                                help=""Training data"")\n    command_parser.add_argument(\'-m\', \'--model-path\', help=""Training data"")\n    command_parser.add_argument(\'-v\', \'--vocab\', type=argparse.FileType(\'r\'), default=""data/vocab.txt"",\n                                help=""Path to vocabulary file"")\n    command_parser.add_argument(\'-vv\', \'--vectors\', type=argparse.FileType(\'r\'), default=""data/wordVectors.txt"",\n                                help=""Path to word vectors file"")\n    command_parser.add_argument(\'-o\', \'--output\', type=argparse.FileType(\'w\'), default=sys.stdout, help=""Training data"")\n    command_parser.set_defaults(func=do_evaluate)\n\n    command_parser = subparsers.add_parser(\'shell\', help=\'\')\n    command_parser.add_argument(\'-m\', \'--model-path\', help=""Training data"")\n    command_parser.add_argument(\'-v\', \'--vocab\', type=argparse.FileType(\'r\'), default=""data/vocab.txt"",\n                                help=""Path to vocabulary file"")\n    command_parser.add_argument(\'-vv\', \'--vectors\', type=argparse.FileType(\'r\'), default=""data/wordVectors.txt"",\n                                help=""Path to word vectors file"")\n    command_parser.set_defaults(func=do_shell)\n\n    ARGS = parser.parse_args()\n    if ARGS.func is None:\n        parser.print_help()\n        sys.exit(1)\n    else:\n        ARGS.func(ARGS)\n'"
assignment3/q2_rnn.py,59,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nQ2: Recurrent neural nets for NER\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport argparse\nimport logging\nimport sys\nimport time\nfrom datetime import datetime\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom util import print_sentence, write_conll, read_conll\nfrom data_util import load_and_preprocess_data, load_embeddings, ModelHelper\nfrom ner_model import NERModel\nfrom defs import LBLS\nfrom q2_rnn_cell import RNNCell\nfrom q3_gru_cell import GRUCell\n\nlogger = logging.getLogger(""hw3.q2"")\nlogger.setLevel(logging.DEBUG)\nlogging.basicConfig(format=\'%(levelname)s:%(message)s\', level=logging.DEBUG)\n\nclass Config:\n    """"""Holds model hyperparams and data information.\n\n    The config class is used to store various hyperparameters and dataset\n    information parameters. Model objects are passed a Config() object at\n    instantiation.\n    """"""\n    n_word_features = 2 # Number of features for every word in the input.\n    window_size = 1\n    n_features = (2 * window_size + 1) * n_word_features # Number of features for every word in the input.\n    max_length = 120 # longest sequence to parse\n    n_classes = 5\n    dropout = 0.5\n    embed_size = 50\n    hidden_size = 300\n    batch_size = 32\n    n_epochs = 10\n    max_grad_norm = 10.\n    lr = 0.001\n\n    def __init__(self, args):\n        self.cell = args.cell\n\n        if ""model_path"" in args:\n            # Where to save things.\n            self.output_path = args.model_path\n        else:\n            self.output_path = ""results/{}/{:%Y%m%d_%H%M%S}/"".format(self.cell, datetime.now())\n        self.model_output = self.output_path + ""model.weights""\n        self.eval_output = self.output_path + ""results.txt""\n        self.conll_output = self.output_path + ""{}_predictions.conll"".format(self.cell)\n        self.log_output = self.output_path + ""log""\n\ndef pad_sequences(data, max_length):\n    """"""Ensures each input-output seqeunce pair in @data is of length\n    @max_length by padding it with zeros and truncating the rest of the\n    sequence.\n\n    TODO: In the code below, for every sentence, labels pair in @data,\n    (a) create a new sentence which appends zero feature vectors until\n    the sentence is of length @max_length. If the sentence is longer\n    than @max_length, simply truncate the sentence to be @max_length\n    long.\n    (b) create a new label sequence similarly.\n    (c) create a _masking_ sequence that has a True wherever there was a\n    token in the original sequence, and a False for every padded input.\n\n    Example: for the (sentence, labels) pair: [[4,1], [6,0], [7,0]], [1,\n    0, 0], and max_length = 5, we would construct\n        - a new sentence: [[4,1], [6,0], [7,0], [0,0], [0,0]]\n        - a new label seqeunce: [1, 0, 0, 4, 4], and\n        - a masking seqeunce: [True, True, True, False, False].\n\n    Args:\n        data: is a list of (sentence, labels) tuples. @sentence is a list\n            containing the words in the sentence and @label is a list of\n            output labels. Each word is itself a list of\n            @n_features features. For example, the sentence ""Chris\n            Manning is amazing"" and labels ""PER PER O O"" would become\n            ([[1,9], [2,9], [3,8], [4,8]], [1, 1, 4, 4]). Here ""Chris""\n            the word has been featurized as ""[1, 9]"", and ""[1, 1, 4, 4]""\n            is the list of labels.\n        max_length: the desired length for all input/output sequences.\n    Returns:\n        a new list of data points of the structure (sentence\', labels\', mask).\n        Each of sentence\', labels\' and mask are of length @max_length.\n        See the example above for more details.\n    """"""\n    ret = []\n\n    # Use this zero vector when padding sequences.\n    zero_vector = [0] * Config.n_features\n    zero_label = 4 # corresponds to the \'O\' tag\n\n    for sentence, labels in data:\n        ### YOUR CODE HERE (~4-6 lines)\n        len_sentence = len(sentence)\n        add_length = max_length - len_sentence\n        if add_length > 0:\n            filled_sentence = sentence + ( [zero_vector] * add_length )\n            filled_labels = labels + ( [zero_label] * add_length)\n            mark = [True] * len_sentence\n            mark.extend([False] * add_length)\n        else:\n            mark = [True] * max_length\n            filled_sentence = sentence[:max_length]\n            filled_labels = labels[:max_length]\n\n        ret.append((filled_sentence, filled_labels, mark))\n        ### END YOUR CODE ###\n    return ret\n\nclass RNNModel(NERModel):\n    """"""\n    Implements a recursive neural network with an embedding layer and\n    single hidden layer.\n    This network will predict a sequence of labels (e.g. PER) for a\n    given token (e.g. Henry) using a featurized window around the token.\n    """"""\n\n    def add_placeholders(self):\n        """"""Generates placeholder variables to represent the input tensors\n\n        These placeholders are used as inputs by the rest of the model building and will be fed\n        data during training.  Note that when ""None"" is in a placeholder\'s shape, it\'s flexible\n        (so we can use different batch sizes without rebuilding the model).\n\n        Adds following nodes to the computational graph\n\n        input_placeholder: Input placeholder tensor of  shape (None, self.max_length, n_features), type tf.int32\n        labels_placeholder: Labels placeholder tensor of shape (None, self.max_length), type tf.int32\n        mask_placeholder:  Mask placeholder tensor of shape (None, self.max_length), type tf.bool\n        dropout_placeholder: Dropout value placeholder (scalar), type tf.float32\n\n        TODO: Add these placeholders to self as the instance variables\n            self.input_placeholder\n            self.labels_placeholder\n            self.mask_placeholder\n            self.dropout_placeholder\n\n        HINTS:\n            - Remember to use self.max_length NOT Config.max_length\n\n        (Don\'t change the variable names)\n        """"""\n        ### YOUR CODE HERE (~4-6 lines)\n        self.input_placeholder = tf.placeholder(\n            tf.int32, shape = (None, self.max_length, Config.n_features), name = \'input\')\n        self.labels_placeholder = tf.placeholder(\n            tf.int32, shape = (None, self.max_length), name = \'labels\')\n        self.mask_placeholder = tf.placeholder(\n            tf.bool, shape = (None, self.max_length), name = \'mask\')\n        self.dropout_placeholder = tf.placeholder(\n            tf.float32, name = \'dropout\')\n        ### END YOUR CODE\n\n    def create_feed_dict(self, inputs_batch, mask_batch, labels_batch=None, dropout=1):\n        """"""Creates the feed_dict for the dependency parser.\n\n        A feed_dict takes the form of:\n\n        feed_dict = {\n                <placeholder>: <tensor of values to be passed for placeholder>,\n                ....\n        }\n\n        Hint: The keys for the feed_dict should be a subset of the placeholder\n                    tensors created in add_placeholders.\n        Hint: When an argument is None, don\'t add it to the feed_dict.\n\n        Args:\n            inputs_batch: A batch of input data.\n            mask_batch:   A batch of mask data.\n            labels_batch: A batch of label data.\n            dropout: The dropout rate.\n        Returns:\n            feed_dict: The feed dictionary mapping from placeholders to values.\n        """"""\n        ### YOUR CODE (~6-10 lines)\n        feed_dict = {\n            self.input_placeholder: inputs_batch,\n            self.mask_placeholder: mask_batch,\n            self.dropout_placeholder: dropout\n        }\n        if labels_batch is not None:\n            feed_dict[self.labels_placeholder] = labels_batch\n        ### END YOUR CODE\n        return feed_dict\n\n    def add_embedding(self):\n        """"""Adds an embedding layer that maps from input tokens (integers) to vectors and then\n        concatenates those vectors:\n\n        TODO:\n            - Create an embedding tensor and initialize it with self.pretrained_embeddings.\n            - Use the input_placeholder to index into the embeddings tensor, resulting in a\n              tensor of shape (None, max_length, n_features, embed_size).\n            - Concatenates the embeddings by reshaping the embeddings tensor to shape\n              (None, max_length, n_features * embed_size).\n\n        HINTS:\n            - You might find tf.nn.embedding_lookup useful.\n            - You can use tf.reshape to concatenate the vectors. See\n              following link to understand what -1 in a shape means.\n              https://www.tensorflow.org/api_docs/python/array_ops/shapes_and_shaping#reshape.\n\n        Returns:\n            embeddings: tf.Tensor of shape (None, max_length, n_features*embed_size)\n        """"""\n        ### YOUR CODE HERE (~4-6 lines)\n        embeddings = tf.nn.embedding_lookup(\n            tf.Variable(self.pretrained_embeddings),\n            self.input_placeholder)\n        embeddings = tf.reshape(\n            embeddings, [-1, self.max_length, Config.n_features* Config.embed_size])\n        ### END YOUR CODE\n        return embeddings\n\n    def add_prediction_op(self):\n        """"""Adds the unrolled RNN:\n            h_0 = 0\n            for t in 1 to T:\n                o_t, h_t = cell(x_t, h_{t-1})\n                o_drop_t = Dropout(o_t, dropout_rate)\n                y_t = o_drop_t U + b_2\n\n        TODO: There a quite a few things you\'ll need to do in this function:\n            - Define the variables U, b_2.\n            - Define the vector h as a constant and inititalize it with\n              zeros. See tf.zeros and tf.shape for information on how\n              to initialize this variable to be of the right shape.\n              https://www.tensorflow.org/api_docs/python/constant_op/constant_value_tensors#zeros\n              https://www.tensorflow.org/api_docs/python/array_ops/shapes_and_shaping#shape\n            - In a for loop, begin to unroll the RNN sequence. Collect\n              the predictions in a list.\n            - When unrolling the loop, from the second iteration\n              onwards, you will HAVE to call\n              tf.get_variable_scope().reuse_variables() so that you do\n              not create new variables in the RNN cell.\n              See https://www.tensorflow.org/versions/master/how_tos/variable_scope/\n            - Concatenate and reshape the predictions into a predictions\n              tensor.\n        Hint: You will find the function tf.pack (similar to np.asarray)\n              useful to assemble a list of tensors into a larger tensor.\n              https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#pack\n        Hint: You will find the function tf.transpose and the perms\n              argument useful to shuffle the indices of the tensor.\n              https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#transpose\n\n        Remember:\n            * Use the xavier initilization for matrices.\n            * Note that tf.nn.dropout takes the keep probability (1 - p_drop) as an argument.\n            The keep probability should be set to the value of self.dropout_placeholder\n\n        Returns:\n            pred: tf.Tensor of shape (batch_size, max_length, n_classes)\n        """"""\n\n        x = self.add_embedding()\n        dropout_rate = self.dropout_placeholder\n\n        preds = [] # Predicted output at each timestep should go here!\n\n        # Use the cell defined below. For Q2, we will just be using the\n        # RNNCell you defined, but for Q3, we will run this code again\n        # with a GRU cell!\n        if self.config.cell == ""rnn"":\n            cell = RNNCell(Config.n_features * Config.embed_size, Config.hidden_size)\n        elif self.config.cell == ""gru"":\n            cell = GRUCell(Config.n_features * Config.embed_size, Config.hidden_size)\n        else:\n            raise ValueError(""Unsuppported cell type: "" + self.config.cell)\n\n        # Define U and b2 as variables.\n        # Initialize state as vector of zeros.\n        ### YOUR CODE HERE (~4-6 lines)\n        with tf.variable_scope(\'Layer1\'):\n            U = tf.get_variable(\'U\', (Config.hidden_size, Config.n_classes), initializer=tf.contrib.layers.xavier_initializer() )\n            b2 = tf.get_variable(\'b2\', (Config.n_classes), initializer=tf.constant_initializer(0) )\n\n        input_shape = tf.shape(x)\n        state = tf.zeros( (input_shape[0], Config.hidden_size) )\n        ### END YOUR CODE\n\n        with tf.variable_scope(""RNN""):\n            for time_step in range(self.max_length):\n                ### YOUR CODE HERE (~6-10 lines)\n                if time_step>0:\n                    tf.get_variable_scope().reuse_variables()\n                o, state = cell( x[:,time_step, :], state, scope=""RNN"" )\n                o_drop = tf.nn.dropout(o, dropout_rate)\n                output = tf.matmul(o_drop,U) + b2\n                preds.append(output)\n                ### END YOUR CODE\n\n        # Make sure to reshape @preds here.\n        ### YOUR CODE HERE (~2-4 lines)\n        preds = tf.stack(preds, axis=1)\n        ### END YOUR CODE\n\n        assert preds.get_shape().as_list() == [None, self.max_length, self.config.n_classes], ""predictions are not of the right shape. Expected {}, got {}"".format([None, self.max_length, self.config.n_classes], preds.get_shape().as_list())\n        return preds\n\n    def add_loss_op(self, preds):\n        """"""Adds Ops for the loss function to the computational graph.\n\n        TODO: Compute averaged cross entropy loss for the predictions.\n        Importantly, you must ignore the loss for any masked tokens.\n\n        Hint: You might find tf.boolean_mask useful to mask the losses on masked tokens.\n        Hint: You can use tf.nn.sparse_softmax_cross_entropy_with_logits to simplify your\n                    implementation. You might find tf.reduce_mean useful.\n        Args:\n            pred: A tensor of shape (batch_size, max_length, n_classes) containing the output of the neural\n                  network before the softmax layer.\n        Returns:\n            loss: A 0-d tensor (scalar)\n        """"""\n        ### YOUR CODE HERE (~2-4 lines)\n        masked_logits = tf.boolean_mask( preds, self.mask_placeholder)\n        masked_labels = tf.boolean_mask( self.labels_placeholder, self.mask_placeholder)\n        loss = tf.reduce_mean(\n            tf.nn.sparse_softmax_cross_entropy_with_logits( logits = masked_logits,\n                                                            labels = masked_labels )\n        )\n        ### END YOUR CODE\n        return loss\n\n    def add_training_op(self, loss):\n        """"""Sets up the training Ops.\n\n        Creates an optimizer and applies the gradients to all trainable variables.\n        The Op returned by this function is what must be passed to the\n        `sess.run()` call to cause the model to train. See\n\n        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n\n        for more information.\n\n        Use tf.train.AdamOptimizer for this model.\n        Calling optimizer.minimize() will return a train_op object.\n\n        Args:\n            loss: Loss tensor, from cross_entropy_loss.\n        Returns:\n            train_op: The Op for training.\n        """"""\n        ### YOUR CODE HERE (~1-2 lines)\n        train_op = tf.train.AdamOptimizer(Config.lr).minimize(loss)\n        ### END YOUR CODE\n        return train_op\n\n    def preprocess_sequence_data(self, examples):\n        def featurize_windows(data, start, end, window_size = 1):\n            """"""Uses the input sequences in @data to construct new windowed data points.\n            """"""\n            ret = []\n            for sentence, labels in data:\n                from util import window_iterator\n                sentence_ = []\n                for window in window_iterator(sentence, window_size, beg=start, end=end):\n                    sentence_.append(sum(window, []))\n                ret.append((sentence_, labels))\n            return ret\n\n        examples = featurize_windows(examples, self.helper.START, self.helper.END)\n        return pad_sequences(examples, self.max_length)\n\n    def consolidate_predictions(self, examples_raw, examples, preds):\n        """"""Batch the predictions into groups of sentence length.\n        """"""\n        assert len(examples_raw) == len(examples)\n        assert len(examples_raw) == len(preds)\n\n        ret = []\n        for i, (sentence, labels) in enumerate(examples_raw):\n            _, _, mask = examples[i]\n            labels_ = [l for l, m in zip(preds[i], mask) if m] # only select elements of mask.\n            assert len(labels_) == len(labels)\n            ret.append([sentence, labels, labels_])\n        return ret\n\n    def predict_on_batch(self, sess, inputs_batch, mask_batch):\n        feed = self.create_feed_dict(inputs_batch=inputs_batch, mask_batch=mask_batch)\n        predictions = sess.run(tf.argmax(self.pred, axis=2), feed_dict=feed)\n        return predictions\n\n    def train_on_batch(self, sess, inputs_batch, labels_batch, mask_batch):\n        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch, mask_batch=mask_batch,\n                                     dropout=Config.dropout)\n        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n        return loss\n\n    def __init__(self, helper, config, pretrained_embeddings, report=None):\n        super(RNNModel, self).__init__(helper, config, report)\n        self.max_length = min(Config.max_length, helper.max_length)\n        Config.max_length = self.max_length # Just in case people make a mistake.\n        self.pretrained_embeddings = pretrained_embeddings\n\n        # Defining placeholders.\n        self.input_placeholder = None\n        self.labels_placeholder = None\n        self.mask_placeholder = None\n        self.dropout_placeholder = None\n\n        self.build()\n\ndef test_pad_sequences():\n    Config.n_features = 2\n    data = [\n        ([[4,1], [6,0], [7,0]], [1, 0, 0]),\n        ([[3,0], [3,4], [4,5], [5,3], [3,4]], [0, 1, 0, 2, 3]),\n    ]\n    ret = [\n        ([[4,1], [6,0], [7,0], [0,0]], [1, 0, 0, 4], [True, True, True, False]),\n        ([[3,0], [3,4], [4,5], [5,3]], [0, 1, 0, 2], [True, True, True, True])\n    ]\n\n    ret_ = pad_sequences(data, 4)\n    assert len(ret_) == 2, ""Did not process all examples: expected {} results, but got {}."".format(2, len(ret_))\n    for i in range(2):\n        assert len(ret_[i]) == 3, ""Did not populate return values corrected: expected {} items, but got {}."".format(3, len(ret_[i]))\n        for j in range(3):\n            assert ret_[i][j] == ret[i][j], ""Expected {}, but got {} for {}-th entry of {}-th example"".format(ret[i][j], ret_[i][j], j, i)\n\ndef do_test1(_):\n    logger.info(""Testing pad_sequences"")\n    test_pad_sequences()\n    logger.info(""Passed!"")\n\ndef do_test2(args):\n    logger.info(""Testing implementation of RNNModel"")\n    config = Config(args)\n    helper, train, dev, train_raw, dev_raw = load_and_preprocess_data(args)\n    embeddings = load_embeddings(args, helper)\n    config.embed_size = embeddings.shape[1]\n\n    with tf.Graph().as_default():\n        logger.info(""Building model..."",)\n        start = time.time()\n        model = RNNModel(helper, config, embeddings)\n        logger.info(""took %.2f seconds"", time.time() - start)\n\n        init = tf.global_variables_initializer()\n        saver = None\n\n        with tf.Session() as session:\n            session.run(init)\n            model.fit(session, saver, train, dev)\n\n    logger.info(""Model did not crash!"")\n    logger.info(""Passed!"")\n\ndef do_train(args):\n    # Set up some parameters.\n    config = Config(args)\n    helper, train, dev, train_raw, dev_raw = load_and_preprocess_data(args)\n    embeddings = load_embeddings(args, helper)\n    config.embed_size = embeddings.shape[1]\n    helper.save(config.output_path)\n\n    handler = logging.FileHandler(config.log_output)\n    handler.setLevel(logging.DEBUG)\n    handler.setFormatter(logging.Formatter(\'%(asctime)s:%(levelname)s: %(message)s\'))\n    logging.getLogger().addHandler(handler)\n\n    report = None #Report(Config.eval_output)\n\n    with tf.Graph().as_default():\n        logger.info(""Building model..."",)\n        start = time.time()\n        model = RNNModel(helper, config, embeddings)\n        logger.info(""took %.2f seconds"", time.time() - start)\n\n        init = tf.global_variables_initializer()\n        saver = tf.train.Saver()\n\n        with tf.Session() as session:\n            session.run(init)\n            model.fit(session, saver, train, dev)\n            if report:\n                report.log_output(model.output(session, dev_raw))\n                report.save()\n            else:\n                # Save predictions in a text file.\n                output = model.output(session, dev_raw)\n                sentences, labels, predictions = zip(*output)\n                predictions = [[LBLS[l] for l in preds] for preds in predictions]\n                output = zip(sentences, labels, predictions)\n\n                with open(model.config.conll_output, \'w\') as f:\n                    write_conll(f, output)\n                with open(model.config.eval_output, \'w\') as f:\n                    for sentence, labels, predictions in output:\n                        print_sentence(f, sentence, labels, predictions)\n\ndef do_evaluate(args):\n    config = Config(args)\n    helper = ModelHelper.load(args.model_path)\n    input_data = read_conll(args.data)\n    embeddings = load_embeddings(args, helper)\n    config.embed_size = embeddings.shape[1]\n\n    with tf.Graph().as_default():\n        logger.info(""Building model..."",)\n        start = time.time()\n        model = RNNModel(helper, config, embeddings)\n\n        logger.info(""took %.2f seconds"", time.time() - start)\n\n        init = tf.global_variables_initializer()\n        saver = tf.train.Saver()\n\n        with tf.Session() as session:\n            session.run(init)\n            saver.restore(session, model.config.model_output)\n            for sentence, labels, predictions in model.output(session, input_data):\n                predictions = [LBLS[l] for l in predictions]\n                print_sentence(args.output, sentence, labels, predictions)\n\ndef do_shell(args):\n    config = Config(args)\n    helper = ModelHelper.load(args.model_path)\n    embeddings = load_embeddings(args, helper)\n    config.embed_size = embeddings.shape[1]\n\n    with tf.Graph().as_default():\n        logger.info(""Building model..."",)\n        start = time.time()\n        model = RNNModel(helper, config, embeddings)\n        logger.info(""took %.2f seconds"", time.time() - start)\n\n        init = tf.global_variables_initializer()\n        saver = tf.train.Saver()\n\n        with tf.Session() as session:\n            session.run(init)\n            saver.restore(session, model.config.model_output)\n\n            print(""""""Welcome!\nYou can use this shell to explore the behavior of your model.\nPlease enter sentences with spaces between tokens, e.g.,\ninput> Germany \'s representative to the European Union \'s veterinary committee .\n"""""")\n            while True:\n                # Create simple REPL\n                try:\n                    sentence = raw_input(""input> "")\n                    tokens = sentence.strip().split("" "")\n                    for sentence, _, predictions in model.output(session, [(tokens, [""O""] * len(tokens))]):\n                        predictions = [LBLS[l] for l in predictions]\n                        print_sentence(sys.stdout, sentence, [""""] * len(tokens), predictions)\n                except EOFError:\n                    print(""Closing session."")\n                    break\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Trains and tests an NER model\')\n    subparsers = parser.add_subparsers()\n\n    command_parser = subparsers.add_parser(\'test1\', help=\'\')\n    command_parser.set_defaults(func=do_test1)\n\n    command_parser = subparsers.add_parser(\'test2\', help=\'\')\n    command_parser.add_argument(\'-dt\', \'--data-train\', type=argparse.FileType(\'r\'), default=""data/tiny.conll"", help=""Training data"")\n    command_parser.add_argument(\'-dd\', \'--data-dev\', type=argparse.FileType(\'r\'), default=""data/tiny.conll"", help=""Dev data"")\n    command_parser.add_argument(\'-v\', \'--vocab\', type=argparse.FileType(\'r\'), default=""data/vocab.txt"", help=""Path to vocabulary file"")\n    command_parser.add_argument(\'-vv\', \'--vectors\', type=argparse.FileType(\'r\'), default=""data/wordVectors.txt"", help=""Path to word vectors file"")\n    command_parser.add_argument(\'-c\', \'--cell\', choices=[""rnn"", ""gru""], default=""rnn"", help=""Type of RNN cell to use."")\n    command_parser.set_defaults(func=do_test2)\n\n    command_parser = subparsers.add_parser(\'train\', help=\'\')\n    command_parser.add_argument(\'-dt\', \'--data-train\', type=argparse.FileType(\'r\'), default=""data/train.conll"", help=""Training data"")\n    command_parser.add_argument(\'-dd\', \'--data-dev\', type=argparse.FileType(\'r\'), default=""data/dev.conll"", help=""Dev data"")\n    command_parser.add_argument(\'-v\', \'--vocab\', type=argparse.FileType(\'r\'), default=""data/vocab.txt"", help=""Path to vocabulary file"")\n    command_parser.add_argument(\'-vv\', \'--vectors\', type=argparse.FileType(\'r\'), default=""data/wordVectors.txt"", help=""Path to word vectors file"")\n    command_parser.add_argument(\'-c\', \'--cell\', choices=[""rnn"", ""gru""], default=""rnn"", help=""Type of RNN cell to use."")\n    command_parser.set_defaults(func=do_train)\n\n    command_parser = subparsers.add_parser(\'evaluate\', help=\'\')\n    command_parser.add_argument(\'-d\', \'--data\', type=argparse.FileType(\'r\'), default=""data/dev.conll"", help=""Training data"")\n    command_parser.add_argument(\'-m\', \'--model-path\', help=""Training data"")\n    command_parser.add_argument(\'-v\', \'--vocab\', type=argparse.FileType(\'r\'), default=""data/vocab.txt"", help=""Path to vocabulary file"")\n    command_parser.add_argument(\'-vv\', \'--vectors\', type=argparse.FileType(\'r\'), default=""data/wordVectors.txt"", help=""Path to word vectors file"")\n    command_parser.add_argument(\'-c\', \'--cell\', choices=[""rnn"", ""gru""], default=""rnn"", help=""Type of RNN cell to use."")\n    command_parser.add_argument(\'-o\', \'--output\', type=argparse.FileType(\'w\'), default=sys.stdout, help=""Training data"")\n    command_parser.set_defaults(func=do_evaluate)\n\n    command_parser = subparsers.add_parser(\'shell\', help=\'\')\n    command_parser.add_argument(\'-m\', \'--model-path\', help=""Training data"")\n    command_parser.add_argument(\'-v\', \'--vocab\', type=argparse.FileType(\'r\'), default=""data/vocab.txt"", help=""Path to vocabulary file"")\n    command_parser.add_argument(\'-vv\', \'--vectors\', type=argparse.FileType(\'r\'), default=""data/wordVectors.txt"", help=""Path to word vectors file"")\n    command_parser.add_argument(\'-c\', \'--cell\', choices=[""rnn"", ""gru""], default=""rnn"", help=""Type of RNN cell to use."")\n    command_parser.set_defaults(func=do_shell)\n\n    ARGS = parser.parse_args()\n    if ARGS.func is None:\n        parser.print_help()\n        sys.exit(1)\n    else:\n        ARGS.func(ARGS)\n'"
assignment3/q2_rnn_cell.py,21,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nQ2(c): Recurrent neural nets for NER\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport argparse\nimport logging\nimport sys\n\nimport tensorflow as tf\nimport numpy as np\n\nlogger = logging.getLogger(""hw3.q2.1"")\nlogger.setLevel(logging.DEBUG)\nlogging.basicConfig(format=\'%(levelname)s:%(message)s\', level=logging.DEBUG)\n\n\nclass RNNCell(tf.nn.rnn_cell.RNNCell):\n    """"""Wrapper around our RNN cell implementation that allows us to play\n    nicely with TensorFlow.\n    """"""\n\n    def __init__(self, input_size, state_size):\n        self.input_size = input_size\n        self._state_size = state_size\n\n    @property\n    def state_size(self):\n        return self._state_size\n\n    @property\n    def output_size(self):\n        return self._state_size\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Updates the state using the previous @state and @inputs.\n        Remember the RNN equations are:\n\n        h_t = sigmoid(x_t W_x + h_{t-1} W_h + b)\n\n        TODO: In the code below, implement an RNN cell using @inputs\n        (x_t above) and the state (h_{t-1} above).\n            - Define W_x, W_h, b to be variables of the apporiate shape\n              using the `tf.get_variable\' functions. Make sure you use\n              the names ""W_x"", ""W_h"" and ""b""!\n            - Compute @new_state (h_t) defined above\n        Tips:\n            - Remember to initialize your matrices using the xavier\n              initialization as before.\n        Args:\n            inputs: is the input vector of size [None, self.input_size]\n            state: is the previous state vector of size [None, self.state_size]\n            scope: is the name of the scope to be used when defining the variables inside.\n        Returns:\n            a pair of the output vector and the new state vector.\n        """"""\n        scope = scope or type(self).__name__\n\n        # It\'s always a good idea to scope variables in functions lest they\n        # be defined elsewhere!\n        with tf.variable_scope(scope):\n            ### YOUR CODE HERE (~6-10 lines)\n            W_x = tf.get_variable(\'W_x\', shape=(self.input_size, self._state_size), dtype=tf.float32,\n                                  initializer=tf.contrib.layers.xavier_initializer())\n            W_h = tf.get_variable(\'W_h\', shape=(self._state_size, self._state_size), dtype=tf.float32,\n                                  initializer=tf.contrib.layers.xavier_initializer())\n            b = tf.get_variable(\'b\', shape=(self._state_size), dtype=tf.float32,\n                                initializer=tf.contrib.layers.xavier_initializer())\n\n            new_state = tf.nn.sigmoid(tf.matmul(state, W_h) + tf.matmul(inputs, W_x) + b)\n            ### END YOUR CODE ###\n        # For an RNN , the output and state are the same (N.B. this\n        # isn\'t true for an LSTM, though we aren\'t using one of those in\n        # our assignment)\n        output = new_state\n        return output, new_state\n\n\ndef test_rnn_cell():\n    with tf.Graph().as_default():\n        with tf.variable_scope(""test_rnn_cell""):\n            x_placeholder = tf.placeholder(tf.float32, shape=(None, 3))\n            h_placeholder = tf.placeholder(tf.float32, shape=(None, 2))\n\n            with tf.variable_scope(""rnn""):\n                tf.get_variable(""W_x"", initializer=np.array(np.eye(3, 2), dtype=np.float32))\n                tf.get_variable(""W_h"", initializer=np.array(np.eye(2, 2), dtype=np.float32))\n                tf.get_variable(""b"", initializer=np.array(np.ones(2), dtype=np.float32))\n\n            tf.get_variable_scope().reuse_variables()\n            cell = RNNCell(3, 2)\n            y_var, ht_var = cell(x_placeholder, h_placeholder, scope=""rnn"")\n\n            init = tf.global_variables_initializer()\n            with tf.Session() as session:\n                session.run(init)\n                x = np.array([\n                    [0.4, 0.5, 0.6],\n                    [0.3, -0.2, -0.1]], dtype=np.float32)\n                h = np.array([\n                    [0.2, 0.5],\n                    [-0.3, -0.3]], dtype=np.float32)\n                y = np.array([\n                    [0.832, 0.881],\n                    [0.731, 0.622]], dtype=np.float32)\n                ht = y\n\n                y_, ht_ = session.run([y_var, ht_var], feed_dict={x_placeholder: x, h_placeholder: h})\n                print(""y_ = "" + str(y_))\n                print(""ht_ = "" + str(ht_))\n\n                assert np.allclose(y_, ht_), ""output and state should be equal.""\n                assert np.allclose(ht, ht_, atol=1e-2), ""new state vector does not seem to be correct.""\n\n\ndef do_test(_):\n    logger.info(""Testing rnn_cell"")\n    test_rnn_cell()\n    logger.info(""Passed!"")\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Tests the RNN cell implemented as part of Q2 of Homework 3\')\n    subparsers = parser.add_subparsers()\n\n    command_parser = subparsers.add_parser(\'test\', help=\'\')\n    command_parser.set_defaults(func=do_test)\n\n    ARGS = parser.parse_args()\n    if ARGS.func is None:\n        parser.print_help()\n        sys.exit(1)\n    else:\n        ARGS.func(ARGS)\n'"
assignment3/q3_gru.py,41,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nQ3: Grooving with GRUs\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport argparse\nimport logging\nimport sys\nimport time\nfrom datetime import datetime\n\nimport tensorflow as tf\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom util import Progbar, minibatches\nfrom model import Model\n\nfrom q3_gru_cell import GRUCell\nfrom q2_rnn_cell import RNNCell\n\nmatplotlib.use(\'TkAgg\')\nlogger = logging.getLogger(""hw3.q3"")\nlogger.setLevel(logging.DEBUG)\nlogging.basicConfig(format=\'%(levelname)s:%(message)s\', level=logging.DEBUG)\n\n\nclass Config:\n    """"""Holds model hyperparams and data information.\n    The config class is used to store various hyperparameters and dataset\n    information parameters. Model objects are passed a Config() object at\n    instantiation. Use self.config.? instead of Config.?\n    """"""\n    max_length = 20  # Length of sequence used.\n    batch_size = 100\n    n_epochs = 40\n    lr = 0.2\n    max_grad_norm = 5.\n\n\nclass SequencePredictor(Model):\n    def add_placeholders(self):\n        """"""Generates placeholder variables to represent the input tensors\n        NOTE: You do not have to do anything here.\n        """"""\n        self.inputs_placeholder = tf.placeholder(tf.float32, shape=(None, self.config.max_length, 1), name=""x"")\n        self.labels_placeholder = tf.placeholder(tf.float32, shape=(None, 1), name=""y"")\n\n    def create_feed_dict(self, inputs_batch, labels_batch=None):\n        """"""Creates the feed_dict for the model.\n        NOTE: You do not have to do anything here.\n        """"""\n        feed_dict = {\n            self.inputs_placeholder: inputs_batch,\n        }\n        if labels_batch is not None:\n            feed_dict[self.labels_placeholder] = labels_batch\n        return feed_dict\n\n    def add_prediction_op(self):\n        """"""Runs an rnn on the input using TensorFlows\'s\n        @tf.nn.dynamic_rnn function, and returns the final state as a prediction.\n\n        TODO:\n            - Call tf.nn.dynamic_rnn using @cell below. See:\n              https://www.tensorflow.org/api_docs/python/nn/recurrent_neural_networks\n            - Apply a sigmoid transformation on the final state to\n              normalize the inputs between 0 and 1.\n\n        Returns:\n            preds: tf.Tensor of shape (batch_size, 1)\n        """"""\n\n        # Pick out the cell to use here.\n        if self.config.cell == ""rnn"":\n            cell = RNNCell(1, 1)\n        elif self.config.cell == ""gru"":\n            cell = GRUCell(1, 1)\n        elif self.config.cell == ""lstm"":\n            cell = tf.nn.rnn_cell.LSTMCell(1)\n        else:\n            raise ValueError(""Unsupported cell type."")\n\n        x = self.inputs_placeholder\n        ### YOUR CODE HERE (~2-3 lines)\n        preds = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)[1]\n        preds = tf.sigmoid(preds)\n        ### END YOUR CODE\n\n        return preds  # state # preds\n\n    def add_loss_op(self, preds):\n        """"""Adds ops to compute the loss function.\n        Here, we will use a simple l2 loss.\n\n        Tips:\n            - You may find the functions tf.reduce_mean and tf.l2_loss\n              useful.\n\n        Args:\n            pred: A tensor of shape (batch_size, 1) containing the last\n            state of the neural network.\n        Returns:\n            loss: A 0-d tensor (scalar)\n        """"""\n        y = self.labels_placeholder\n\n        ### YOUR CODE HERE (~1-2 lines)\n        loss = tf.nn.l2_loss(preds - y)\n        loss = tf.reduce_mean(loss)\n        ### END YOUR CODE\n\n        return loss\n\n    def add_training_op(self, loss):\n        """"""Sets up the training Ops.\n\n        Creates an optimizer and applies the gradients to all trainable variables.\n        The Op returned by this function is what must be passed to the\n        `sess.run()` call to cause the model to train. See\n\n        TODO:\n            - Get the gradients for the loss from optimizer using\n              optimizer.compute_gradients.\n            - if self.clip_gradients is true, clip the global norm of\n              the gradients using tf.clip_by_global_norm to self.config.max_grad_norm\n            - Compute the resultant global norm of the gradients using\n              tf.global_norm and save this global norm in self.grad_norm.\n            - Finally, actually create the training operation by calling\n              optimizer.apply_gradients.\n        See: https://www.tensorflow.org/api_docs/python/train/gradient_clipping\n        Args:\n            loss: Loss tensor.\n        Returns:\n            train_op: The Op for training.\n        """"""\n\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.config.lr)\n\n        ### YOUR CODE HERE (~6-10 lines)\n\n        # - Remember to clip gradients only if self.config.clip_gradients\n        # is True.\n        # - Remember to set self.grad_norm\n        grads_and_vars = optimizer.compute_gradients(loss)\n        variables = [output[1] for output in grads_and_vars]\n        gradients = [output[0] for output in grads_and_vars]\n        if self.config.clip_gradients:\n            tmp_gradients = tf.clip_by_global_norm(gradients, clip_norm=self.config.max_grad_norm)[0]\n            gradients = tmp_gradients\n\n        grads_and_vars = [(gradients[i], variables[i]) for i in range(len(gradients))]\n        self.grad_norm = tf.global_norm(gradients)\n\n        train_op = optimizer.apply_gradients(grads_and_vars)\n        ### END YOUR CODE\n\n        assert self.grad_norm is not None, ""grad_norm was not set properly!""\n        return train_op\n\n    def train_on_batch(self, sess, inputs_batch, labels_batch):\n        """"""Perform one step of gradient descent on the provided batch of data.\n        This version also returns the norm of gradients.\n        """"""\n        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch)\n        _, loss, grad_norm = sess.run([self.train_op, self.loss, self.grad_norm], feed_dict=feed)\n        return loss, grad_norm\n\n    def run_epoch(self, sess, train):\n        prog = Progbar(target=1 + int(len(train) / self.config.batch_size))\n        losses, grad_norms = [], []\n        for i, batch in enumerate(minibatches(train, self.config.batch_size)):\n            loss, grad_norm = self.train_on_batch(sess, *batch)\n            losses.append(loss)\n            grad_norms.append(grad_norm)\n            prog.update(i + 1, [(""train loss"", loss)])\n\n        return losses, grad_norms\n\n    def fit(self, sess, train):\n        losses, grad_norms = [], []\n        for epoch in range(self.config.n_epochs):\n            logger.info(""Epoch %d out of %d"", epoch + 1, self.config.n_epochs)\n            loss, grad_norm = self.run_epoch(sess, train)\n            losses.append(loss)\n            grad_norms.append(grad_norm)\n\n        return losses, grad_norms\n\n    def __init__(self, config):\n        self.config = config\n        self.inputs_placeholder = None\n        self.labels_placeholder = None\n        self.grad_norm = None\n        self.build()\n\n\ndef generate_sequence(max_length=20, n_samples=9999):\n    """"""\n    Generates a sequence like a [0]*n a\n    """"""\n    seqs = []\n    for _ in range(int(n_samples / 2)):\n        seqs.append(([[0., ]] + ([[0., ]] * (max_length - 1)), [0.]))\n        seqs.append(([[1., ]] + ([[0., ]] * (max_length - 1)), [1.]))\n    return seqs\n\n\ndef test_generate_sequence():\n    max_length = 20\n    for seq, y in generate_sequence(20):\n        assert len(seq) == max_length\n        assert seq[0] == y\n\n\ndef make_dynamics_plot(args, x, h, ht_rnn, ht_gru, params):\n    matplotlib.rc(\'text\', usetex=True)\n    matplotlib.rc(\'font\', family=\'serif\')\n\n    Ur, Wr, br, Uz, Wz, bz, Uo, Wo, bo = params\n\n    plt.clf()\n    plt.title(""""""Cell dynamics when x={}:\nUr={:.2f}, Wr={:.2f}, br={:.2f}\nUz={:.2f}, Wz={:.2f}, bz={:.2f}\nUo={:.2f}, Wo={:.2f}, bo={:.2f}"""""".format(x, Ur[0, 0], Wr[0, 0], br[0], Uz[0, 0], Wz[0, 0], bz[0], Uo[0, 0], Wo[0, 0],\n                                          bo[0]))\n\n    plt.plot(h, ht_rnn, label=""rnn"")\n    plt.plot(h, ht_gru, label=""gru"")\n    plt.plot(h, h, color=\'gray\', linestyle=\'--\')\n    plt.ylabel(""$h_{t}$"")\n    plt.xlabel(""$h_{t-1}$"")\n    plt.legend()\n    output_path = ""{}-{}-{}.png"".format(args.output_prefix, x, ""dynamics"")\n    plt.savefig(output_path)\n\n\ndef compute_cell_dynamics(args):\n    with tf.Graph().as_default():\n        # You can change this around, but make sure to reset it to 41 when\n        # submitting.\n        np.random.seed(41)\n        tf.set_random_seed(41)\n\n        with tf.variable_scope(""dynamics""):\n            x_placeholder = tf.placeholder(tf.float32, shape=(None, 1))\n            h_placeholder = tf.placeholder(tf.float32, shape=(None, 1))\n\n            def mat(x):\n                return np.atleast_2d(np.array(x, dtype=np.float32))\n\n            def vec(x):\n                return np.atleast_1d(np.array(x, dtype=np.float32))\n\n            with tf.variable_scope(""cell""):\n                Ur, Wr, Uz, Wz, Uo, Wo = [mat(3 * x) for x in np.random.randn(6)]\n                br, bz, bo = [vec(x) for x in np.random.randn(3)]\n                params = [Ur, Wr, br, Uz, Wz, bz, Uo, Wo, bo]\n\n                tf.get_variable(""U_r"", initializer=Ur)\n                tf.get_variable(""W_r"", initializer=Wr)\n                tf.get_variable(""b_r"", initializer=br)\n\n                tf.get_variable(""U_z"", initializer=Uz)\n                tf.get_variable(""W_z"", initializer=Wz)\n                tf.get_variable(""b_z"", initializer=bz)\n\n                tf.get_variable(""U_o"", initializer=Uo)\n                tf.get_variable(""W_o"", initializer=Wo)\n                tf.get_variable(""b_o"", initializer=bo)\n\n                tf.get_variable(""W_h"", initializer=Wz)\n                tf.get_variable(""W_x"", initializer=Wo)\n                tf.get_variable(""b"", initializer=bo)\n\n            tf.get_variable_scope().reuse_variables()\n            y_gru, h_gru = GRUCell(1, 1)(x_placeholder, h_placeholder, scope=""cell"")\n            y_rnn, h_rnn = RNNCell(1, 1)(x_placeholder, h_placeholder, scope=""cell"")\n\n            init = tf.global_variables_initializer()\n            with tf.Session() as session:\n                session.run(init)\n\n                x = mat(np.zeros(1000)).T\n                h = mat(np.linspace(-3, 3, 1000)).T\n                ht_gru = session.run([h_gru], feed_dict={x_placeholder: x, h_placeholder: h})\n                ht_rnn = session.run([h_rnn], feed_dict={x_placeholder: x, h_placeholder: h})\n                ht_gru = np.array(ht_gru)[0]\n                ht_rnn = np.array(ht_rnn)[0]\n                make_dynamics_plot(args, 0, h, ht_rnn, ht_gru, params)\n\n                x = mat(np.ones(1000)).T\n                h = mat(np.linspace(-3, 3, 1000)).T\n                ht_gru = session.run([h_gru], feed_dict={x_placeholder: x, h_placeholder: h})\n                ht_rnn = session.run([h_rnn], feed_dict={x_placeholder: x, h_placeholder: h})\n                ht_gru = np.array(ht_gru)[0]\n                ht_rnn = np.array(ht_rnn)[0]\n                make_dynamics_plot(args, 1, h, ht_rnn, ht_gru, params)\n\n\ndef make_prediction_plot(args, losses, grad_norms):\n    plt.subplot(2, 1, 1)\n    plt.title(""{} on sequences of length {} ({} gradient clipping)"".format(args.cell, args.max_length,\n                                                                           ""with"" if args.clip_gradients else ""without""))\n    plt.plot(np.arange(losses.size), losses.flatten(), label=""Loss"")\n    plt.ylabel(""Loss"")\n\n    plt.subplot(2, 1, 2)\n    plt.plot(np.arange(grad_norms.size), grad_norms.flatten(), label=""Gradients"")\n    plt.ylabel(""Gradients"")\n    plt.xlabel(""Minibatch"")\n    output_path = ""{}-{}clip-{}.png"".format(args.output_prefix, """" if args.clip_gradients else ""no"", args.cell)\n    plt.savefig(output_path)\n\n\ndef do_sequence_prediction(args):\n    # Set up some parameters.\n    config = Config()\n    config.cell = args.cell\n    config.clip_gradients = args.clip_gradients\n\n    # You can change this around, but make sure to reset it to 41 when\n    # submitting.\n    np.random.seed(41)\n    data = generate_sequence(args.max_length)\n\n    with tf.Graph().as_default():\n        # You can change this around, but make sure to reset it to 41 when\n        # submitting.\n        tf.set_random_seed(59)\n\n        # Initializing RNNs weights to be very large to showcase\n        # gradient clipping.\n\n\n        logger.info(""Building model..."", )\n        start = time.time()\n        model = SequencePredictor(config)\n        logger.info(""took %.2f seconds"", time.time() - start)\n\n        init = tf.global_variables_initializer()\n\n        with tf.Session() as session:\n            session.run(init)\n            losses, grad_norms = model.fit(session, data)\n\n    # Plotting code.\n    losses, grad_norms = np.array(losses), np.array(grad_norms)\n    make_prediction_plot(args, losses, grad_norms)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=\'Runs a sequence model to test latching behavior of memory, e.g. 100000000 -> 1\')\n    subparsers = parser.add_subparsers()\n\n    command_parser = subparsers.add_parser(\'predict\', help=\'Plot prediction behavior of different cells\')\n    command_parser.add_argument(\'-c\', \'--cell\', choices=[\'rnn\', \'gru\', \'lstm\'], default=\'rnn\',\n                                help=""Type of cell to use"")\n    command_parser.add_argument(\'-g\', \'--clip_gradients\', action=\'store_true\', default=False,\n                                help=""If true, clip gradients"")\n    command_parser.add_argument(\'-l\', \'--max-length\', type=int, default=20, help=""Length of sequences to generate"")\n    command_parser.add_argument(\'-o\', \'--output-prefix\', type=str, default=""q3"", help=""Length of sequences to generate"")\n    command_parser.set_defaults(func=do_sequence_prediction)\n\n    # Easter egg! Run this function to plot how an RNN or GRU map an\n    # input state to an output state.\n    command_parser = subparsers.add_parser(\'dynamics\', help=""Plot cell\'s dynamics"")\n    command_parser.add_argument(\'-o\', \'--output-prefix\', type=str, default=""q3"", help=""Length of sequences to generate"")\n    command_parser.set_defaults(func=compute_cell_dynamics)\n\n    ARGS = parser.parse_args()\n    if ARGS.func is None:\n        parser.print_help()\n        sys.exit(1)\n    else:\n        ARGS.func(ARGS)\n'"
assignment3/q3_gru_cell.py,33,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nQ3(d): Grooving with GRUs\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport argparse\nimport logging\nimport sys\n\nimport tensorflow as tf\nimport numpy as np\n\nlogger = logging.getLogger(""hw3.q3.1"")\nlogger.setLevel(logging.DEBUG)\nlogging.basicConfig(format=\'%(levelname)s:%(message)s\', level=logging.DEBUG)\n\nclass GRUCell(tf.nn.rnn_cell.RNNCell):\n    """"""Wrapper around our GRU cell implementation that allows us to play\n    nicely with TensorFlow.\n    """"""\n    def __init__(self, input_size, state_size):\n        self.input_size = input_size\n        self._state_size = state_size\n\n    @property\n    def state_size(self):\n        return self._state_size\n\n    @property\n    def output_size(self):\n        return self._state_size\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Updates the state using the previous @state and @inputs.\n        Remember the GRU equations are:\n\n        z_t = sigmoid(x_t U_z + h_{t-1} W_z + b_z)\n        r_t = sigmoid(x_t U_r + h_{t-1} W_r + b_r)\n        o_t = tanh(x_t U_o + r_t * h_{t-1} W_o + b_o)\n        h_t = z_t * h_{t-1} + (1 - z_t) * o_t\n\n        TODO: In the code below, implement an GRU cell using @inputs\n        (x_t above) and the state (h_{t-1} above).\n            - Define W_r, U_r, b_r, W_z, U_z, b_z and W_o, U_o, b_o to\n              be variables of the apporiate shape using the\n              `tf.get_variable\' functions.\n            - Compute z, r, o and @new_state (h_t) defined above\n        Tips:\n            - Remember to initialize your matrices using the xavier\n              initialization as before.\n        Args:\n            inputs: is the input vector of size [None, self.input_size]\n            state: is the previous state vector of size [None, self.state_size]\n            scope: is the name of the scope to be used when defining the variables inside.\n        Returns:\n            a pair of the output vector and the new state vector.\n        """"""\n        scope = scope or type(self).__name__\n\n        # It\'s always a good idea to scope variables in functions lest they\n        # be defined elsewhere!\n        with tf.variable_scope(scope):\n            ### YOUR CODE HERE (~20-30 lines)\n            initFunc = tf.contrib.layers.xavier_initializer(uniform=True) # set to False for gradient clipping\n            W_r = tf.get_variable(\'W_r\', [self.state_size, self.state_size], initializer=initFunc, dtype = tf.float32)\n            U_r = tf.get_variable(\'U_r\', [self.input_size, self.state_size], initializer=initFunc, dtype = tf.float32)\n            b_r = tf.get_variable(\'b_r\', [self.state_size,], initializer=tf.constant_initializer(0), dtype = tf.float32)\n            W_z = tf.get_variable(\'W_z\', [self.state_size, self.state_size], initializer=initFunc, dtype = tf.float32)\n            U_z = tf.get_variable(\'U_z\', [self.input_size, self.state_size], initializer=initFunc, dtype = tf.float32)\n            b_z = tf.get_variable(\'b_z\', [self.state_size,], initializer=tf.constant_initializer(0), dtype = tf.float32)    ## Recommend on Piazza\n            W_o = tf.get_variable(\'W_o\', [self.state_size, self.state_size], initializer=initFunc, dtype = tf.float32)\n            U_o = tf.get_variable(\'U_o\', [self.input_size, self.state_size], initializer=initFunc, dtype = tf.float32)\n            b_o = tf.get_variable(\'b_o\', [self.state_size,], initializer=tf.constant_initializer(0), dtype = tf.float32)\n\n            z_t = tf.sigmoid(tf.matmul(inputs, U_z) + tf.matmul(state, W_z) + b_z)\n            r_t = tf.sigmoid(tf.matmul(inputs, U_r) + tf.matmul(state, W_r) + b_r)\n            o_t = tf.tanh(tf.matmul(inputs, U_o) + tf.matmul(r_t * state, W_o) + b_o)\n            new_state = z_t * state + (1 - z_t) * o_t\n            ### END YOUR CODE ###\n        # For a GRU, the output and state are the same (N.B. this isn\'t true\n        # for an LSTM, though we aren\'t using one of those in our\n        # assignment)\n        output = new_state\n        return output, new_state\n\ndef test_gru_cell():\n    with tf.Graph().as_default():\n        with tf.variable_scope(""test_gru_cell""):\n            x_placeholder = tf.placeholder(tf.float32, shape=(None,3))\n            h_placeholder = tf.placeholder(tf.float32, shape=(None,2))\n\n            with tf.variable_scope(""gru""):\n                tf.get_variable(""U_r"", initializer=np.array(np.eye(3,2), dtype=np.float32))\n                tf.get_variable(""W_r"", initializer=np.array(np.eye(2,2), dtype=np.float32))\n                tf.get_variable(""b_r"",  initializer=np.array(np.ones(2), dtype=np.float32))\n                tf.get_variable(""U_z"", initializer=np.array(np.eye(3,2), dtype=np.float32))\n                tf.get_variable(""W_z"", initializer=np.array(np.eye(2,2), dtype=np.float32))\n                tf.get_variable(""b_z"",  initializer=np.array(np.ones(2), dtype=np.float32))\n                tf.get_variable(""U_o"", initializer=np.array(np.eye(3,2), dtype=np.float32))\n                tf.get_variable(""W_o"", initializer=np.array(np.eye(2,2), dtype=np.float32))\n                tf.get_variable(""b_o"",  initializer=np.array(np.ones(2), dtype=np.float32))\n\n            tf.get_variable_scope().reuse_variables()\n            cell = GRUCell(3, 2)\n            y_var, ht_var = cell(x_placeholder, h_placeholder, scope=""gru"")\n\n            init = tf.global_variables_initializer()\n            with tf.Session() as session:\n                session.run(init)\n                x = np.array([\n                    [0.4, 0.5, 0.6],\n                    [0.3, -0.2, -0.1]], dtype=np.float32)\n                h = np.array([\n                    [0.2, 0.5],\n                    [-0.3, -0.3]], dtype=np.float32)\n                y = np.array([\n                    [ 0.320, 0.555],\n                    [-0.006, 0.020]], dtype=np.float32)\n                ht = y\n\n                y_, ht_ = session.run([y_var, ht_var], feed_dict={x_placeholder: x, h_placeholder: h})\n                print(""y_ = "" + str(y_))\n                print(""ht_ = "" + str(ht_))\n\n                assert np.allclose(y_, ht_), ""output and state should be equal.""\n                assert np.allclose(ht, ht_, atol=1e-2), ""new state vector does not seem to be correct.""\n\ndef do_test(_):\n    logger.info(""Testing gru_cell"")\n    test_gru_cell()\n    logger.info(""Passed!"")\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Tests the GRU cell implemented as part of Q3 of Homework 3\')\n    subparsers = parser.add_subparsers()\n\n    command_parser = subparsers.add_parser(\'test\', help=\'\')\n    command_parser.set_defaults(func=do_test)\n\n    ARGS = parser.parse_args()\n    if ARGS.func is None:\n        parser.print_help()\n        sys.exit(1)\n    else:\n        ARGS.func(ARGS)\n'"
assignment3/util.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCS224N 2016-17: Homework 3\nutil.py: General utility routines\nArun Chaganty <chaganty@stanford.edu>\n""""""\n\nfrom __future__ import division\n\nimport sys\nimport time\nimport logging\nimport StringIO\nfrom collections import defaultdict, Counter, OrderedDict\nimport numpy as np\nfrom numpy import array, zeros, allclose\n\nlogger = logging.getLogger(""hw3"")\nlogger.setLevel(logging.DEBUG)\nlogging.basicConfig(format=\'%(levelname)s:%(message)s\', level=logging.DEBUG)\n\ndef read_conll(fstream):\n    """"""\n    Reads a input stream @fstream (e.g. output of `open(fname, \'r\')`) in CoNLL file format.\n    @returns a list of examples [(tokens), (labels)]. @tokens and @labels are lists of string.\n    """"""\n    ret = []\n\n    current_toks, current_lbls = [], []\n    for line in fstream:\n        line = line.strip()\n        if len(line) == 0 or line.startswith(""-DOCSTART-""):\n            if len(current_toks) > 0:\n                assert len(current_toks) == len(current_lbls)\n                ret.append((current_toks, current_lbls))\n            current_toks, current_lbls = [], []\n        else:\n            assert ""\\t"" in line, r""Invalid CONLL format; expected a \'\\t\' in {}"".format(line)\n            tok, lbl = line.split(""\\t"")\n            current_toks.append(tok)\n            current_lbls.append(lbl)\n    if len(current_toks) > 0:\n        assert len(current_toks) == len(current_lbls)\n        ret.append((current_toks, current_lbls))\n    return ret\n\ndef test_read_conll():\n    input_ = [\n        ""EU\tORG"",\n        ""rejects\tO"",\n        ""German\tMISC"",\n        ""call\tO"",\n        ""to\tO"",\n        ""boycott\tO"",\n        ""British\tMISC"",\n        ""lamb\tO"",\n        "".\tO"",\n        """",\n        ""Peter\tPER"",\n        ""Blackburn\tPER"",\n        """",\n        ]\n    output = [\n        (""EU rejects German call to boycott British lamb ."".split(), ""ORG O MISC O O O MISC O O"".split()),\n        (""Peter Blackburn"".split(), ""PER PER"".split())\n        ]\n\n    assert read_conll(input_) == output\n\ndef write_conll(fstream, data):\n    """"""\n    Writes to an output stream @fstream (e.g. output of `open(fname, \'r\')`) in CoNLL file format.\n    @data a list of examples [(tokens), (labels), (predictions)]. @tokens, @labels, @predictions are lists of string.\n    """"""\n    for cols in data:\n        for row in zip(*cols):\n            fstream.write(""\\t"".join(row))\n            fstream.write(""\\n"")\n        fstream.write(""\\n"")\n\ndef test_write_conll():\n    input = [\n        (""EU rejects German call to boycott British lamb ."".split(), ""ORG O MISC O O O MISC O O"".split()),\n        (""Peter Blackburn"".split(), ""PER PER"".split())\n        ]\n    output = """"""EU\tORG\nrejects\tO\nGerman\tMISC\ncall\tO\nto\tO\nboycott\tO\nBritish\tMISC\nlamb\tO\n.\tO\n\nPeter\tPER\nBlackburn\tPER\n\n""""""\n    output_ = StringIO.StringIO()\n    write_conll(output_, input)\n    output_ = output_.getvalue()\n    assert output == output_\n\ndef load_word_vector_mapping(vocab_fstream, vector_fstream):\n    """"""\n    Load word vector mapping using @vocab_fstream, @vector_fstream.\n    Assumes each line of the vocab file matches with those of the vector\n    file.\n    """"""\n    ret = OrderedDict()\n    for vocab, vector in zip(vocab_fstream, vector_fstream):\n        vocab = vocab.strip()\n        vector = vector.strip()\n        ret[vocab] = array(list(map(float, vector.split())))\n\n    return ret\n\ndef test_load_word_vector_mapping():\n    vocab = """"""UUUNKKK\nthe\n,\n.\nof\nand\nin"""""".split(""\\n"")\n    vector = """"""0.172414 -0.091063 0.255125 -0.837163 0.434872 -0.499848 -0.042904 -0.059642 -0.635087 -0.458795 -0.105671 0.506513 -0.105105 -0.405678 0.493365 0.408807 0.401635 -0.817805 0.626340 0.580636 -0.246996 -0.008515 -0.671140 0.301865 -0.439651 0.247694 -0.291402 0.873009 0.216212 0.145576 -0.211101 -0.352360 0.227651 -0.118416 0.371816 0.261296 0.017548 0.596692 -0.485722 -0.369530 -0.048807 0.017960 -0.040483 0.111193 0.398039 0.162765 0.408946 0.005343 -0.107523 -0.079821\n-0.454847 1.002773 -1.406829 -0.016482 0.459856 -0.224457 0.093396 -0.826833 -0.530674 1.211044 -0.165133 0.174454 -1.130952 -0.612020 -0.024578 -0.168508 0.320113 0.774229 -0.360418 1.483124 -0.230922 0.301055 -0.119924 0.601642 0.694616 -0.304431 -0.414284 0.667385 0.171208 -0.334842 -0.459286 -0.534202 0.533660 -0.379468 -0.378721 -0.240499 -0.446272 0.686113 0.662359 -0.865312 0.861331 -0.627698 -0.569544 -1.228366 -0.152052 1.589123 0.081337 0.182695 -0.593022 0.438300\n-0.408797 -0.109333 -0.099279 -0.857098 -0.150319 -0.456398 -0.781524 -0.059621 0.302548 0.202162 -0.319892 -0.502241 -0.014925 0.020889 1.506245 0.247530 0.385598 -0.170776 0.325960 0.267304 0.157673 0.125540 -0.971452 -0.485595 0.487857 0.284369 -0.062811 -1.334082 0.744133 0.572701 1.009871 -0.457229 0.938059 0.654805 -0.430244 -0.697683 -0.220146 0.346002 -0.388637 -0.149513 0.011248 0.818728 0.042615 -0.594237 -0.646138 0.568898 0.700328 0.290316 0.293722 0.828779\n-0.583585 0.413481 -0.708189 0.168942 0.238435 0.789011 -0.566401 0.177570 -0.244441 0.328214 -0.319583 -0.468558 0.520323 0.072727 1.792047 -0.781348 -0.636644 0.070102 -0.247090 0.110990 0.182112 1.609935 -1.081378 0.922773 -0.605783 0.793724 0.476911 -1.279422 0.904010 -0.519837 1.235220 -0.149456 0.138923 0.686835 -0.733707 -0.335434 -1.865440 -0.476014 -0.140478 -0.148011 0.555169 1.356662 0.850737 -0.484898 0.341224 -0.056477 0.024663 1.141509 0.742001 0.478773\n-0.811262 -1.017245 0.311680 -0.437684 0.338728 1.034527 -0.415528 -0.646984 -0.121626 0.589435 -0.977225 0.099942 -1.296171 0.022671 0.946574 0.204963 0.297055 -0.394868 0.028115 -0.021189 -0.448692 0.421286 0.156809 -0.332004 0.177866 0.074233 0.299713 0.148349 1.104055 -0.172720 0.292706 0.727035 0.847151 0.024006 -0.826570 -1.038778 -0.568059 -0.460914 -1.290872 -0.294531 0.663751 -0.646503 0.499024 -0.804777 -0.402926 -0.292201 0.348031 0.215414 0.043492 0.165281\n-0.156019 0.405009 -0.370058 -1.417499 0.120639 -0.191854 -0.251213 -0.883898 -0.025010 0.150738 1.038723 0.038419 0.036411 -0.289871 0.588898 0.618994 0.087019 -0.275657 -0.105293 -0.536067 -0.181410 0.058034 0.552306 -0.389803 -0.384800 -0.470717 0.800593 -0.166609 0.702104 0.876092 0.353401 -0.314156 0.618290 0.804017 -0.925911 -1.002050 -0.231087 0.590011 -0.636952 -0.474758 0.169423 1.293482 0.609088 -0.956202 -0.013831 0.399147 0.436669 0.116759 -0.501962 1.308268\n-0.008573 -0.731185 -1.108792 -0.358545 0.507277 -0.050167 0.751870 0.217678 -0.646852 -0.947062 -1.187739 0.490993 -1.500471 0.463113 1.370237 0.218072 0.213489 -0.362163 -0.758691 -0.670870 0.218470 1.641174 0.293220 0.254524 0.085781 0.464454 0.196361 -0.693989 -0.384305 -0.171888 0.045602 1.476064 0.478454 0.726961 -0.642484 -0.266562 -0.846778 0.125562 -0.787331 -0.438503 0.954193 -0.859042 -0.180915 -0.944969 -0.447460 0.036127 0.654763 0.439739 -0.038052 0.991638"""""".split(""\\n"")\n\n    wvs = load_word_vector_mapping(vocab, vector)\n    assert ""UUUNKKK"" in wvs\n    assert allclose(wvs[""UUUNKKK""], array([0.172414, -0.091063, 0.255125, -0.837163, 0.434872, -0.499848, -0.042904, -0.059642, -0.635087, -0.458795, -0.105671, 0.506513, -0.105105, -0.405678, 0.493365, 0.408807, 0.401635, -0.817805, 0.626340, 0.580636, -0.246996, -0.008515, -0.671140, 0.301865, -0.439651, 0.247694, -0.291402, 0.873009, 0.216212, 0.145576, -0.211101, -0.352360, 0.227651, -0.118416, 0.371816, 0.261296, 0.017548, 0.596692, -0.485722, -0.369530, -0.048807, 0.017960, -0.040483, 0.111193, 0.398039, 0.162765, 0.408946, 0.005343, -0.107523, -0.079821]))\n    assert ""the"" in wvs\n    assert ""of"" in wvs\n    assert ""and"" in wvs\n\ndef window_iterator(seq, n=1, beg=""<s>"", end=""</s>""):\n    """"""\n    Iterates through seq by returning windows of length 2n+1\n    """"""\n    for i in range(len(seq)):\n        l = max(0, i-n)\n        r = min(len(seq), i+n+1)\n        ret = seq[l:r]\n        if i < n:\n            ret = [beg,] * (n-i) + ret\n        if i+n+1 > len(seq):\n            ret = ret + [end,] * (i+n+1 - len(seq))\n        yield ret\n\ndef test_window_iterator():\n    assert list(window_iterator(list(""abcd""), n=0)) == [[""a"",], [""b"",], [""c"",], [""d""]]\n    assert list(window_iterator(list(""abcd""), n=1)) == [[""<s>"",""a"",""b""], [""a"",""b"",""c"",], [""b"",""c"",""d"",], [""c"", ""d"", ""</s>"",]]\n\ndef one_hot(n, y):\n    """"""\n    Create a one-hot @n-dimensional vector with a 1 in position @i\n    """"""\n    if isinstance(y, int):\n        ret = zeros(n)\n        ret[y] = 1.0\n        return ret\n    elif isinstance(y, list):\n        ret = zeros((len(y), n))\n        ret[np.arange(len(y)),y] = 1.0\n        return ret\n    else:\n        raise ValueError(""Expected an int or list got: "" + y)\n\n\ndef to_table(data, row_labels, column_labels, precision=2, digits=4):\n    """"""Pretty print tables.\n    Assumes @data is a 2D array and uses @row_labels and @column_labels\n    to display table.\n    """"""\n    # Convert data to strings\n    line = ""%0""+str(digits)+"".""+str(precision)+""f""\n    data = [[line%v for v in row] for row in data]\n    cell_width = max(\n        max(map(len, row_labels)),\n        max(map(len, column_labels)),\n        max(max(map(len, row)) for row in data))\n    def c(s):\n        """"""adjust cell output""""""\n        return s + "" "" * (cell_width - len(s))\n    ret = """"\n    ret += ""\\t"".join(map(c, column_labels)) + ""\\n""\n    for l, row in zip(row_labels, data):\n        ret += ""\\t"".join(map(c, [l] + row)) + ""\\n""\n    return ret\n\nclass ConfusionMatrix(object):\n    """"""\n    A confusion matrix stores counts of (true, guessed) labels, used to\n    compute several evaluation metrics like accuracy, precision, recall\n    and F1.\n    """"""\n\n    def __init__(self, labels, default_label=None):\n        self.labels = labels\n        self.default_label = default_label if default_label is not None else len(labels) -1\n        self.counts = defaultdict(Counter)\n\n    def update(self, gold, guess):\n        """"""Update counts""""""\n        self.counts[gold][guess] += 1\n\n    def as_table(self):\n        """"""Print tables""""""\n        # Header\n        data = [[self.counts[l][l_] for l_,_ in enumerate(self.labels)] for l,_ in enumerate(self.labels)]\n        return to_table(data, self.labels, [""go\\\\gu""] + self.labels, precision=0, digits=0)\n\n    def summary(self, quiet=False):\n        """"""Summarize counts""""""\n        keys = range(len(self.labels))\n        data = []\n        macro = array([0., 0., 0., 0.])\n        micro = array([0., 0., 0., 0.])\n        default = array([0., 0., 0., 0.])\n        for l in keys:\n            tp = self.counts[l][l]\n            fp = sum(self.counts[l_][l] for l_ in keys if l_ != l)\n            tn = sum(self.counts[l_][l__] for l_ in keys if l_ != l for l__ in keys if l__ != l)\n            fn = sum(self.counts[l][l_] for l_ in keys if l_ != l)\n\n            acc = (tp + tn)/(tp + tn + fp + fn) if tp > 0  else 0\n            prec = (tp)/(tp + fp) if tp > 0  else 0\n            rec = (tp)/(tp + fn) if tp > 0  else 0\n            f1 = 2 * prec * rec / (prec + rec) if tp > 0  else 0\n\n            # update micro/macro averages\n            micro += array([tp, fp, tn, fn])\n            macro += array([acc, prec, rec, f1])\n            if l != self.default_label: # Count count for everything that is not the default label!\n                default += array([tp, fp, tn, fn])\n\n            data.append([acc, prec, rec, f1])\n\n        # micro average\n        tp, fp, tn, fn = micro\n        acc = (tp + tn)/(tp + tn + fp + fn) if tp > 0  else 0\n        prec = (tp)/(tp + fp) if tp > 0  else 0\n        rec = (tp)/(tp + fn) if tp > 0  else 0\n        f1 = 2 * prec * rec / (prec + rec) if tp > 0  else 0\n        data.append([acc, prec, rec, f1])\n        # Macro average\n        data.append(macro / len(keys))\n\n        # default average\n        tp, fp, tn, fn = default\n        acc = (tp + tn)/(tp + tn + fp + fn) if tp > 0  else 0\n        prec = (tp)/(tp + fp) if tp > 0  else 0\n        rec = (tp)/(tp + fn) if tp > 0  else 0\n        f1 = 2 * prec * rec / (prec + rec) if tp > 0  else 0\n        data.append([acc, prec, rec, f1])\n\n        # Macro and micro average.\n        return to_table(data, self.labels + [""micro"",""macro"",""not-O""], [""label"", ""acc"", ""prec"", ""rec"", ""f1""])\n\nclass Progbar(object):\n    """"""\n    Progbar class copied from keras (https://github.com/fchollet/keras/)\n    Displays a progress bar.\n    # Arguments\n        target: Total number of steps expected.\n        interval: Minimum visual progress update interval (in seconds).\n    """"""\n\n    def __init__(self, target, width=30, verbose=1):\n        self.width = width\n        self.target = target\n        self.sum_values = {}\n        self.unique_values = []\n        self.start = time.time()\n        self.total_width = 0\n        self.seen_so_far = 0\n        self.verbose = verbose\n\n    def update(self, current, values=None, exact=None):\n        """"""\n        Updates the progress bar.\n        # Arguments\n            current: Index of current step.\n            values: List of tuples (name, value_for_last_step).\n                The progress bar will display averages for these values.\n            exact: List of tuples (name, value_for_last_step).\n                The progress bar will display these values directly.\n        """"""\n        values = values or []\n        exact = exact or []\n\n        for k, v in values:\n            if k not in self.sum_values:\n                self.sum_values[k] = [v * (current - self.seen_so_far), current - self.seen_so_far]\n                self.unique_values.append(k)\n            else:\n                self.sum_values[k][0] += v * (current - self.seen_so_far)\n                self.sum_values[k][1] += (current - self.seen_so_far)\n        for k, v in exact:\n            if k not in self.sum_values:\n                self.unique_values.append(k)\n            self.sum_values[k] = [v, 1]\n        self.seen_so_far = current\n\n        now = time.time()\n        if self.verbose == 1:\n            prev_total_width = self.total_width\n            sys.stdout.write(""\\b"" * prev_total_width)\n            sys.stdout.write(""\\r"")\n\n            numdigits = int(np.floor(np.log10(self.target))) + 1\n            barstr = \'%%%dd/%%%dd [\' % (numdigits, numdigits)\n            bar = barstr % (current, self.target)\n            prog = float(current)/self.target\n            prog_width = int(self.width*prog)\n            if prog_width > 0:\n                bar += (\'=\'*(prog_width-1))\n                if current < self.target:\n                    bar += \'>\'\n                else:\n                    bar += \'=\'\n            bar += (\'.\'*(self.width-prog_width))\n            bar += \']\'\n            sys.stdout.write(bar)\n            self.total_width = len(bar)\n\n            if current:\n                time_per_unit = (now - self.start) / current\n            else:\n                time_per_unit = 0\n            eta = time_per_unit*(self.target - current)\n            info = \'\'\n            if current < self.target:\n                info += \' - ETA: %ds\' % eta\n            else:\n                info += \' - %ds\' % (now - self.start)\n            for k in self.unique_values:\n                if isinstance(self.sum_values[k], list):\n                    info += \' - %s: %.4f\' % (k, self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n                else:\n                    info += \' - %s: %s\' % (k, self.sum_values[k])\n\n            self.total_width += len(info)\n            if prev_total_width > self.total_width:\n                info += ((prev_total_width-self.total_width) * "" "")\n\n            sys.stdout.write(info)\n            sys.stdout.flush()\n\n            if current >= self.target:\n                sys.stdout.write(""\\n"")\n\n        if self.verbose == 2:\n            if current >= self.target:\n                info = \'%ds\' % (now - self.start)\n                for k in self.unique_values:\n                    info += \' - %s: %.4f\' % (k, self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n                sys.stdout.write(info + ""\\n"")\n\n    def add(self, n, values=None):\n        self.update(self.seen_so_far+n, values)\n\n\ndef get_minibatches(data, minibatch_size, shuffle=True):\n    """"""\n    Iterates through the provided data one minibatch at at time. You can use this function to\n    iterate through data in minibatches as follows:\n\n        for inputs_minibatch in get_minibatches(inputs, minibatch_size):\n            ...\n\n    Or with multiple data sources:\n\n        for inputs_minibatch, labels_minibatch in get_minibatches([inputs, labels], minibatch_size):\n            ...\n\n    Args:\n        data: there are two possible values:\n            - a list or numpy array\n            - a list where each element is either a list or numpy array\n        minibatch_size: the maximum number of items in a minibatch\n        shuffle: whether to randomize the order of returned data\n    Returns:\n        minibatches: the return value depends on data:\n            - If data is a list/array it yields the next minibatch of data.\n            - If data a list of lists/arrays it returns the next minibatch of each element in the\n              list. This can be used to iterate through multiple data sources\n              (e.g., features and labels) at the same time.\n\n    """"""\n    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n    data_size = len(data[0]) if list_data else len(data)\n    indices = np.arange(data_size)\n    if shuffle:\n        np.random.shuffle(indices)\n    for minibatch_start in np.arange(0, data_size, minibatch_size):\n        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n        yield [minibatch(d, minibatch_indices) for d in data] if list_data \\\n            else minibatch(data, minibatch_indices)\n\n\ndef minibatch(data, minibatch_idx):\n    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n\ndef minibatches(data, batch_size, shuffle=True):\n    batches = [np.array(col) for col in zip(*data)]\n    return get_minibatches(batches, batch_size, shuffle)\n\ndef print_sentence(output, sentence, labels, predictions):\n\n    spacings = [max(len(sentence[i]), len(labels[i]), len(predictions[i])) for i in range(len(sentence))]\n    # Compute the word spacing\n    output.write(""x : "")\n    for token, spacing in zip(sentence, spacings):\n        output.write(token)\n        output.write("" "" * (spacing - len(token) + 1))\n    output.write(""\\n"")\n\n    output.write(""y*: "")\n    for token, spacing in zip(labels, spacings):\n        output.write(token)\n        output.write("" "" * (spacing - len(token) + 1))\n    output.write(""\\n"")\n\n    output.write(""y\': "")\n    for token, spacing in zip(predictions, spacings):\n        output.write(token)\n        output.write("" "" * (spacing - len(token) + 1))\n    output.write(""\\n"")\n'"
assignment1/utils/__init__.py,0,b''
assignment1/utils/glove.py,0,"b'\nimport numpy as np\n\nDEFAULT_FILE_PATH = ""utils/datasets/glove.6B.50d.txt""\n\ndef loadWordVectors(tokens, filepath=DEFAULT_FILE_PATH, dimensions=50):\n    """"""Read pretrained GloVe vectors""""""\n    wordVectors = np.zeros((len(tokens), dimensions))\n    with open(filepath) as ifs:\n        for line in ifs:\n            line = line.strip()\n            if not line:\n                continue\n            row = line.split()\n            token = row[0]\n            if token not in tokens:\n                continue\n            data = [float(x) for x in row[1:]]\n            if len(data) != dimensions:\n                raise RuntimeError(""wrong number of dimensions"")\n            wordVectors[tokens[token]] = np.asarray(data)\n    return wordVectors\n'"
assignment1/utils/treebank.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport cPickle as pickle\nimport numpy as np\nimport os\nimport random\n\nclass StanfordSentiment:\n    def __init__(self, path=None, tablesize = 1000000):\n        if not path:\n            path = ""utils/datasets/stanfordSentimentTreebank""\n\n        self.path = path\n        self.tablesize = tablesize\n\n    def tokens(self):\n        if hasattr(self, ""_tokens"") and self._tokens:\n            return self._tokens\n\n        tokens = dict()\n        tokenfreq = dict()\n        wordcount = 0\n        revtokens = []\n        idx = 0\n\n        for sentence in self.sentences():\n            for w in sentence:\n                wordcount += 1\n                if not w in tokens:\n                    tokens[w] = idx\n                    revtokens += [w]\n                    tokenfreq[w] = 1\n                    idx += 1\n                else:\n                    tokenfreq[w] += 1\n\n        tokens[""UNK""] = idx\n        revtokens += [""UNK""]\n        tokenfreq[""UNK""] = 1\n        wordcount += 1\n\n        self._tokens = tokens\n        self._tokenfreq = tokenfreq\n        self._wordcount = wordcount\n        self._revtokens = revtokens\n        return self._tokens\n\n    def sentences(self):\n        if hasattr(self, ""_sentences"") and self._sentences:\n            return self._sentences\n\n        sentences = []\n        with open(self.path + ""/datasetSentences.txt"", ""r"") as f:\n            first = True\n            for line in f:\n                if first:\n                    first = False\n                    continue\n\n                splitted = line.strip().split()[1:]\n                # Deal with some peculiar encoding issues with this file\n                sentences += [[w.lower().decode(""utf-8"").encode(\'latin1\') for w in splitted]]\n\n        self._sentences = sentences\n        self._sentlengths = np.array([len(s) for s in sentences])\n        self._cumsentlen = np.cumsum(self._sentlengths)\n\n        return self._sentences\n\n    def numSentences(self):\n        if hasattr(self, ""_numSentences"") and self._numSentences:\n            return self._numSentences\n        else:\n            self._numSentences = len(self.sentences())\n            return self._numSentences\n\n    def allSentences(self):\n        if hasattr(self, ""_allsentences"") and self._allsentences:\n            return self._allsentences\n\n        sentences = self.sentences()\n        rejectProb = self.rejectProb()\n        tokens = self.tokens()\n        allsentences = [[w for w in s\n            if 0 >= rejectProb[tokens[w]] or random.random() >= rejectProb[tokens[w]]]\n            for s in sentences * 30]\n\n        allsentences = [s for s in allsentences if len(s) > 1]\n\n        self._allsentences = allsentences\n\n        return self._allsentences\n\n    def getRandomContext(self, C=5):\n        allsent = self.allSentences()\n        sentID = random.randint(0, len(allsent) - 1)\n        sent = allsent[sentID]\n        wordID = random.randint(0, len(sent) - 1)\n\n        context = sent[max(0, wordID - C):wordID]\n        if wordID+1 < len(sent):\n            context += sent[wordID+1:min(len(sent), wordID + C + 1)]\n\n        centerword = sent[wordID]\n        context = [w for w in context if w != centerword]\n\n        if len(context) > 0:\n            return centerword, context\n        else:\n            return self.getRandomContext(C)\n\n    def sent_labels(self):\n        if hasattr(self, ""_sent_labels"") and self._sent_labels:\n            return self._sent_labels\n\n        dictionary = dict()\n        phrases = 0\n        with open(self.path + ""/dictionary.txt"", ""r"") as f:\n            for line in f:\n                line = line.strip()\n                if not line: continue\n                splitted = line.split(""|"")\n                dictionary[splitted[0].lower()] = int(splitted[1])\n                phrases += 1\n\n        labels = [0.0] * phrases\n        with open(self.path + ""/sentiment_labels.txt"", ""r"") as f:\n            first = True\n            for line in f:\n                if first:\n                    first = False\n                    continue\n\n                line = line.strip()\n                if not line: continue\n                splitted = line.split(""|"")\n                labels[int(splitted[0])] = float(splitted[1])\n\n        sent_labels = [0.0] * self.numSentences()\n        sentences = self.sentences()\n        for i in xrange(self.numSentences()):\n            sentence = sentences[i]\n            full_sent = "" "".join(sentence).replace(\'-lrb-\', \'(\').replace(\'-rrb-\', \')\')\n            sent_labels[i] = labels[dictionary[full_sent]]\n\n        self._sent_labels = sent_labels\n        return self._sent_labels\n\n    def dataset_split(self):\n        if hasattr(self, ""_split"") and self._split:\n            return self._split\n\n        split = [[] for i in xrange(3)]\n        with open(self.path + ""/datasetSplit.txt"", ""r"") as f:\n            first = True\n            for line in f:\n                if first:\n                    first = False\n                    continue\n\n                splitted = line.strip().split("","")\n                split[int(splitted[1]) - 1] += [int(splitted[0]) - 1]\n\n        self._split = split\n        return self._split\n\n    def getRandomTrainSentence(self):\n        split = self.dataset_split()\n        sentId = split[0][random.randint(0, len(split[0]) - 1)]\n        return self.sentences()[sentId], self.categorify(self.sent_labels()[sentId])\n\n    def categorify(self, label):\n        if label <= 0.2:\n            return 0\n        elif label <= 0.4:\n            return 1\n        elif label <= 0.6:\n            return 2\n        elif label <= 0.8:\n            return 3\n        else:\n            return 4\n\n    def getDevSentences(self):\n        return self.getSplitSentences(2)\n\n    def getTestSentences(self):\n        return self.getSplitSentences(1)\n\n    def getTrainSentences(self):\n        return self.getSplitSentences(0)\n\n    def getSplitSentences(self, split=0):\n        ds_split = self.dataset_split()\n        return [(self.sentences()[i], self.categorify(self.sent_labels()[i])) for i in ds_split[split]]\n\n    def sampleTable(self):\n        if hasattr(self, \'_sampleTable\') and self._sampleTable is not None:\n            return self._sampleTable\n\n        nTokens = len(self.tokens())\n        samplingFreq = np.zeros((nTokens,))\n        self.allSentences()\n        i = 0\n        for w in xrange(nTokens):\n            w = self._revtokens[i]\n            if w in self._tokenfreq:\n                freq = 1.0 * self._tokenfreq[w]\n                # Reweigh\n                freq = freq ** 0.75\n            else:\n                freq = 0.0\n            samplingFreq[i] = freq\n            i += 1\n\n        samplingFreq /= np.sum(samplingFreq)\n        samplingFreq = np.cumsum(samplingFreq) * self.tablesize\n\n        self._sampleTable = [0] * self.tablesize\n\n        j = 0\n        for i in xrange(self.tablesize):\n            while i > samplingFreq[j]:\n                j += 1\n            self._sampleTable[i] = j\n\n        return self._sampleTable\n\n    def rejectProb(self):\n        if hasattr(self, \'_rejectProb\') and self._rejectProb is not None:\n            return self._rejectProb\n\n        threshold = 1e-5 * self._wordcount\n\n        nTokens = len(self.tokens())\n        rejectProb = np.zeros((nTokens,))\n        for i in xrange(nTokens):\n            w = self._revtokens[i]\n            freq = 1.0 * self._tokenfreq[w]\n            # Reweigh\n            rejectProb[i] = max(0, 1 - np.sqrt(threshold / freq))\n\n        self._rejectProb = rejectProb\n        return self._rejectProb\n\n    def sampleTokenIdx(self):\n        return self.sampleTable()[random.randint(0, self.tablesize - 1)]'"
assignment2/utils/__init__.py,0,b''
assignment2/utils/general_utils.py,0,"b'import sys\nimport time\nimport numpy as np\n\n\ndef get_minibatches(data, minibatch_size, shuffle=True):\n    """"""\n    Iterates through the provided data one minibatch at at time. You can use this function to\n    iterate through data in minibatches as follows:\n\n        for inputs_minibatch in get_minibatches(inputs, minibatch_size):\n            ...\n\n    Or with multiple data sources:\n\n        for inputs_minibatch, labels_minibatch in get_minibatches([inputs, labels], minibatch_size):\n            ...\n\n    Args:\n        data: there are two possible values:\n            - a list or numpy array\n            - a list where each element is either a list or numpy array\n        minibatch_size: the maximum number of items in a minibatch\n        shuffle: whether to randomize the order of returned data\n    Returns:\n        minibatches: the return value depends on data:\n            - If data is a list/array it yields the next minibatch of data.\n            - If data a list of lists/arrays it returns the next minibatch of each element in the\n              list. This can be used to iterate through multiple data sources\n              (e.g., features and labels) at the same time.\n\n    """"""\n    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n    data_size = len(data[0]) if list_data else len(data)\n    indices = np.arange(data_size)\n    if shuffle:\n        np.random.shuffle(indices)\n    for minibatch_start in np.arange(0, data_size, minibatch_size):\n        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n        yield [minibatch(d, minibatch_indices) for d in data] if list_data \\\n            else minibatch(data, minibatch_indices)\n\n\ndef minibatch(data, minibatch_idx):\n    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n\n\ndef test_all_close(name, actual, expected):\n    if actual.shape != expected.shape:\n        raise ValueError(""{:} failed, expected output to have shape {:} but has shape {:}""\n                         .format(name, expected.shape, actual.shape))\n    if np.amax(np.fabs(actual - expected)) > 1e-6:\n        raise ValueError(""{:} failed, expected {:} but value is {:}"".format(name, expected, actual))\n    else:\n        print name, ""passed!""\n\n\ndef logged_loop(iterable, n=None):\n    if n is None:\n        n = len(iterable)\n    step = max(1, n / 1000)\n    prog = Progbar(n)\n    for i, elem in enumerate(iterable):\n        if i % step == 0 or i == n - 1:\n            prog.update(i + 1)\n        yield elem\n\n\nclass Progbar(object):\n    """"""\n    Progbar class copied from keras (https://github.com/fchollet/keras/)\n    Displays a progress bar.\n    # Arguments\n        target: Total number of steps expected.\n        interval: Minimum visual progress update interval (in seconds).\n    """"""\n\n    def __init__(self, target, width=30, verbose=1):\n        self.width = width\n        self.target = target\n        self.sum_values = {}\n        self.unique_values = []\n        self.start = time.time()\n        self.total_width = 0\n        self.seen_so_far = 0\n        self.verbose = verbose\n\n    def update(self, current, values=[], exact=[]):\n        """"""\n        Updates the progress bar.\n        # Arguments\n            current: Index of current step.\n            values: List of tuples (name, value_for_last_step).\n                The progress bar will display averages for these values.\n            exact: List of tuples (name, value_for_last_step).\n                The progress bar will display these values directly.\n        """"""\n\n        for k, v in values:\n            if k not in self.sum_values:\n                self.sum_values[k] = [v * (current - self.seen_so_far), current - self.seen_so_far]\n                self.unique_values.append(k)\n            else:\n                self.sum_values[k][0] += v * (current - self.seen_so_far)\n                self.sum_values[k][1] += (current - self.seen_so_far)\n        for k, v in exact:\n            if k not in self.sum_values:\n                self.unique_values.append(k)\n            self.sum_values[k] = [v, 1]\n        self.seen_so_far = current\n\n        now = time.time()\n        if self.verbose == 1:\n            prev_total_width = self.total_width\n            sys.stdout.write(""\\b"" * prev_total_width)\n            sys.stdout.write(""\\r"")\n\n            numdigits = int(np.floor(np.log10(self.target))) + 1\n            barstr = \'%%%dd/%%%dd [\' % (numdigits, numdigits)\n            bar = barstr % (current, self.target)\n            prog = float(current)/self.target\n            prog_width = int(self.width*prog)\n            if prog_width > 0:\n                bar += (\'=\'*(prog_width-1))\n                if current < self.target:\n                    bar += \'>\'\n                else:\n                    bar += \'=\'\n            bar += (\'.\'*(self.width-prog_width))\n            bar += \']\'\n            sys.stdout.write(bar)\n            self.total_width = len(bar)\n\n            if current:\n                time_per_unit = (now - self.start) / current\n            else:\n                time_per_unit = 0\n            eta = time_per_unit*(self.target - current)\n            info = \'\'\n            if current < self.target:\n                info += \' - ETA: %ds\' % eta\n            else:\n                info += \' - %ds\' % (now - self.start)\n            for k in self.unique_values:\n                if type(self.sum_values[k]) is list:\n                    info += \' - %s: %.4f\' % (k, self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n                else:\n                    info += \' - %s: %s\' % (k, self.sum_values[k])\n\n            self.total_width += len(info)\n            if prev_total_width > self.total_width:\n                info += ((prev_total_width-self.total_width) * "" "")\n\n            sys.stdout.write(info)\n            sys.stdout.flush()\n\n            if current >= self.target:\n                sys.stdout.write(""\\n"")\n\n        if self.verbose == 2:\n            if current >= self.target:\n                info = \'%ds\' % (now - self.start)\n                for k in self.unique_values:\n                    info += \' - %s: %.4f\' % (k, self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n                sys.stdout.write(info + ""\\n"")\n\n    def add(self, n, values=[]):\n        self.update(self.seen_so_far+n, values)\n'"
assignment2/utils/parser_utils.py,0,"b'""""""Utilities for training the dependency parser.\nYou do not need to read/understand this code\n""""""\n\nimport time\nimport os\nimport logging\nfrom collections import Counter\nfrom general_utils import logged_loop, get_minibatches\nfrom q2_parser_transitions import PartialParse, minibatch_parse\n\nimport numpy as np\n\n\nP_PREFIX = \'<p>:\'\nL_PREFIX = \'<l>:\'\nUNK = \'<UNK>\'\nNULL = \'<NULL>\'\nROOT = \'<ROOT>\'\n\n\nclass Config(object):\n    language = \'english\'\n    with_punct = True\n    unlabeled = True\n    lowercase = True\n    use_pos = True\n    use_dep = True\n    use_dep = use_dep and (not unlabeled)\n    data_path = \'./data\'\n    train_file = \'train.conll\'\n    dev_file = \'dev.conll\'\n    test_file = \'test.conll\'\n    embedding_file = \'./data/en-cw.txt\'\n\n\nclass Parser(object):\n    """"""Contains everything needed for transition-based dependency parsing except for the model""""""\n\n    def __init__(self, dataset):\n        root_labels = list([l for ex in dataset\n                           for (h, l) in zip(ex[\'head\'], ex[\'label\']) if h == 0])\n        counter = Counter(root_labels)\n        if len(counter) > 1:\n            logging.info(\'Warning: more than one root label\')\n            logging.info(counter)\n        self.root_label = counter.most_common()[0][0]\n        deprel = [self.root_label] + list(set([w for ex in dataset\n                                               for w in ex[\'label\']\n                                               if w != self.root_label]))\n        tok2id = {L_PREFIX + l: i for (i, l) in enumerate(deprel)}\n        tok2id[L_PREFIX + NULL] = self.L_NULL = len(tok2id)\n\n        config = Config()\n        self.unlabeled = config.unlabeled\n        self.with_punct = config.with_punct\n        self.use_pos = config.use_pos\n        self.use_dep = config.use_dep\n        self.language = config.language\n\n        if self.unlabeled:\n            trans = [\'L\', \'R\', \'S\']\n            self.n_deprel = 1\n        else:\n            trans = [\'L-\' + l for l in deprel] + [\'R-\' + l for l in deprel] + [\'S\']\n            self.n_deprel = len(deprel)\n\n        self.n_trans = len(trans)\n        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n\n        # logging.info(\'Build dictionary for part-of-speech tags.\')\n        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex[\'pos\']],\n                                  offset=len(tok2id)))\n        tok2id[P_PREFIX + UNK] = self.P_UNK = len(tok2id)\n        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n\n        # logging.info(\'Build dictionary for words.\')\n        tok2id.update(build_dict([w for ex in dataset for w in ex[\'word\']],\n                                  offset=len(tok2id)))\n        tok2id[UNK] = self.UNK = len(tok2id)\n        tok2id[NULL] = self.NULL = len(tok2id)\n        tok2id[ROOT] = self.ROOT = len(tok2id)\n\n        self.tok2id = tok2id\n        self.id2tok = {v: k for (k, v) in tok2id.items()}\n\n        self.n_features = 18 + (18 if config.use_pos else 0) + (12 if config.use_dep else 0)\n        self.n_tokens = len(tok2id)\n\n    def vectorize(self, examples):\n        vec_examples = []\n        for ex in examples:\n            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n                                  else self.UNK for w in ex[\'word\']]\n            pos = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n                                   else self.P_UNK for w in ex[\'pos\']]\n            head = [-1] + ex[\'head\']\n            label = [-1] + [self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id\n                            else -1 for w in ex[\'label\']]\n            vec_examples.append({\'word\': word, \'pos\': pos,\n                                 \'head\': head, \'label\': label})\n        return vec_examples\n\n    def extract_features(self, stack, buf, arcs, ex):\n        if stack[0] == ""ROOT"":\n            stack[0] = 0\n\n        def get_lc(k):\n            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n\n        def get_rc(k):\n            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n                          reverse=True)\n\n        p_features = []\n        l_features = []\n        features = [self.NULL] * (3 - len(stack)) + [ex[\'word\'][x] for x in stack[-3:]]\n        features += [ex[\'word\'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n        if self.use_pos:\n            p_features = [self.P_NULL] * (3 - len(stack)) + [ex[\'pos\'][x] for x in stack[-3:]]\n            p_features += [ex[\'pos\'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n\n        for i in xrange(2):\n            if i < len(stack):\n                k = stack[-i-1]\n                lc = get_lc(k)\n                rc = get_rc(k)\n                llc = get_lc(lc[0]) if len(lc) > 0 else []\n                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n\n                features.append(ex[\'word\'][lc[0]] if len(lc) > 0 else self.NULL)\n                features.append(ex[\'word\'][rc[0]] if len(rc) > 0 else self.NULL)\n                features.append(ex[\'word\'][lc[1]] if len(lc) > 1 else self.NULL)\n                features.append(ex[\'word\'][rc[1]] if len(rc) > 1 else self.NULL)\n                features.append(ex[\'word\'][llc[0]] if len(llc) > 0 else self.NULL)\n                features.append(ex[\'word\'][rrc[0]] if len(rrc) > 0 else self.NULL)\n\n                if self.use_pos:\n                    p_features.append(ex[\'pos\'][lc[0]] if len(lc) > 0 else self.P_NULL)\n                    p_features.append(ex[\'pos\'][rc[0]] if len(rc) > 0 else self.P_NULL)\n                    p_features.append(ex[\'pos\'][lc[1]] if len(lc) > 1 else self.P_NULL)\n                    p_features.append(ex[\'pos\'][rc[1]] if len(rc) > 1 else self.P_NULL)\n                    p_features.append(ex[\'pos\'][llc[0]] if len(llc) > 0 else self.P_NULL)\n                    p_features.append(ex[\'pos\'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n\n                if self.use_dep:\n                    l_features.append(ex[\'label\'][lc[0]] if len(lc) > 0 else self.L_NULL)\n                    l_features.append(ex[\'label\'][rc[0]] if len(rc) > 0 else self.L_NULL)\n                    l_features.append(ex[\'label\'][lc[1]] if len(lc) > 1 else self.L_NULL)\n                    l_features.append(ex[\'label\'][rc[1]] if len(rc) > 1 else self.L_NULL)\n                    l_features.append(ex[\'label\'][llc[0]] if len(llc) > 0 else self.L_NULL)\n                    l_features.append(ex[\'label\'][rrc[0]] if len(rrc) > 0 else self.L_NULL)\n            else:\n                features += [self.NULL] * 6\n                if self.use_pos:\n                    p_features += [self.P_NULL] * 6\n                if self.use_dep:\n                    l_features += [self.L_NULL] * 6\n\n        features += p_features + l_features\n        assert len(features) == self.n_features\n        return features\n\n    def get_oracle(self, stack, buf, ex):\n        if len(stack) < 2:\n            return self.n_trans - 1\n\n        i0 = stack[-1]\n        i1 = stack[-2]\n        h0 = ex[\'head\'][i0]\n        h1 = ex[\'head\'][i1]\n        l0 = ex[\'label\'][i0]\n        l1 = ex[\'label\'][i1]\n\n        if self.unlabeled:\n            if (i1 > 0) and (h1 == i0):\n                return 0\n            elif (i1 >= 0) and (h0 == i1) and \\\n                 (not any([x for x in buf if ex[\'head\'][x] == i0])):\n                return 1\n            else:\n                return None if len(buf) == 0 else 2\n        else:\n            if (i1 > 0) and (h1 == i0):\n                return l1 if (l1 >= 0) and (l1 < self.n_deprel) else None\n            elif (i1 >= 0) and (h0 == i1) and \\\n                 (not any([x for x in buf if ex[\'head\'][x] == i0])):\n                return l0 + self.n_deprel if (l0 >= 0) and (l0 < self.n_deprel) else None\n            else:\n                return None if len(buf) == 0 else self.n_trans - 1\n\n    def create_instances(self, examples):\n        all_instances = []\n        succ = 0\n        for id, ex in enumerate(logged_loop(examples)):\n            n_words = len(ex[\'word\']) - 1\n\n            # arcs = {(h, t, label)}\n            stack = [0]\n            buf = [i + 1 for i in xrange(n_words)]\n            arcs = []\n            instances = []\n            for i in xrange(n_words * 2):\n                gold_t = self.get_oracle(stack, buf, ex)\n                if gold_t is None:\n                    break\n                legal_labels = self.legal_labels(stack, buf)\n                assert legal_labels[gold_t] == 1\n                instances.append((self.extract_features(stack, buf, arcs, ex),\n                                  legal_labels, gold_t))\n                if gold_t == self.n_trans - 1:\n                    stack.append(buf[0])\n                    buf = buf[1:]\n                elif gold_t < self.n_deprel:\n                    arcs.append((stack[-1], stack[-2], gold_t))\n                    stack = stack[:-2] + [stack[-1]]\n                else:\n                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n                    stack = stack[:-1]\n            else:\n                succ += 1\n                all_instances += instances\n\n        return all_instances\n\n    def legal_labels(self, stack, buf):\n        labels = ([1] if len(stack) > 2 else [0]) * self.n_deprel\n        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel\n        labels += [1] if len(buf) > 0 else [0]\n        return labels\n\n    def parse(self, dataset, eval_batch_size=5000):\n        sentences = []\n        sentence_id_to_idx = {}\n        for i, example in enumerate(dataset):\n            n_words = len(example[\'word\']) - 1\n            sentence = [j + 1 for j in range(n_words)]\n            sentences.append(sentence)\n            sentence_id_to_idx[id(sentence)] = i\n\n        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n\n        UAS = all_tokens = 0.0\n        for i, ex in enumerate(dataset):\n            head = [-1] * len(ex[\'word\'])\n            for h, t, in dependencies[i]:\n                head[t] = h\n            for pred_h, gold_h, gold_l, pos in \\\n                    zip(head[1:], ex[\'head\'][1:], ex[\'label\'][1:], ex[\'pos\'][1:]):\n                    assert self.id2tok[pos].startswith(P_PREFIX)\n                    pos_str = self.id2tok[pos][len(P_PREFIX):]\n                    if (self.with_punct) or (not punct(self.language, pos_str)):\n                        UAS += 1 if pred_h == gold_h else 0\n                        all_tokens += 1\n        UAS /= all_tokens\n        return UAS, dependencies\n\n\nclass ModelWrapper(object):\n    def __init__(self, parser, dataset, sentence_id_to_idx):\n        self.parser = parser\n        self.dataset = dataset\n        self.sentence_id_to_idx = sentence_id_to_idx\n\n    def predict(self, partial_parses):\n        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dependencies,\n                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n                for p in partial_parses]\n        mb_x = np.array(mb_x).astype(\'int32\')\n        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n        pred = self.parser.model.predict_on_batch(self.parser.session, mb_x)\n        pred = np.argmax(pred + 10000 * np.array(mb_l).astype(\'float32\'), 1)\n        pred = [""S"" if p == 2 else (""LA"" if p == 0 else ""RA"") for p in pred]\n        return pred\n\n\ndef read_conll(in_file, lowercase=False, max_example=None):\n    examples = []\n    with open(in_file) as f:\n        word, pos, head, label = [], [], [], []\n        for line in f.readlines():\n            sp = line.strip().split(\'\\t\')\n            if len(sp) == 10:\n                if \'-\' not in sp[0]:\n                    word.append(sp[1].lower() if lowercase else sp[1])\n                    pos.append(sp[4])\n                    head.append(int(sp[6]))\n                    label.append(sp[7])\n            elif len(word) > 0:\n                examples.append({\'word\': word, \'pos\': pos, \'head\': head, \'label\': label})\n                word, pos, head, label = [], [], [], []\n                if (max_example is not None) and (len(examples) == max_example):\n                    break\n        if len(word) > 0:\n            examples.append({\'word\': word, \'pos\': pos, \'head\': head, \'label\': label})\n    return examples\n\n\ndef build_dict(keys, n_max=None, offset=0):\n    count = Counter()\n    for key in keys:\n        count[key] += 1\n    ls = count.most_common() if n_max is None \\\n        else count.most_common(n_max)\n\n    return {w[0]: index + offset for (index, w) in enumerate(ls)}\n\n\ndef punct(language, pos):\n    if language == \'english\':\n        return pos in [""\'\'"", "","", ""."", "":"", ""``"", ""-LRB-"", ""-RRB-""]\n    elif language == \'chinese\':\n        return pos == \'PU\'\n    elif language == \'french\':\n        return pos == \'PUNC\'\n    elif language == \'german\':\n        return pos in [""$."", ""$,"", ""$[""]\n    elif language == \'spanish\':\n        # http://nlp.stanford.edu/software/spanish-faq.shtml\n        return pos in [""f0"", ""faa"", ""fat"", ""fc"", ""fd"", ""fe"", ""fg"", ""fh"",\n                       ""fia"", ""fit"", ""fp"", ""fpa"", ""fpt"", ""fs"", ""ft"",\n                       ""fx"", ""fz""]\n    elif language == \'universal\':\n        return pos == \'PUNCT\'\n    else:\n        raise ValueError(\'language: %s is not supported.\' % language)\n\n\ndef minibatches(data, batch_size):\n    x = np.array([d[0] for d in data])\n    y = np.array([d[2] for d in data])\n    one_hot = np.zeros((y.size, 3))\n    one_hot[np.arange(y.size), y] = 1\n    return get_minibatches([x, one_hot], batch_size)\n\n\ndef load_and_preprocess_data(reduced=True):\n    config = Config()\n\n    print ""Loading data..."",\n    start = time.time()\n    train_set = read_conll(os.path.join(config.data_path, config.train_file),\n                           lowercase=config.lowercase)\n    dev_set = read_conll(os.path.join(config.data_path, config.dev_file),\n                         lowercase=config.lowercase)\n    test_set = read_conll(os.path.join(config.data_path, config.test_file),\n                          lowercase=config.lowercase)\n    if reduced:\n        train_set = train_set[:1000]\n        dev_set = dev_set[:500]\n        test_set = test_set[:500]\n    print ""took {:.2f} seconds"".format(time.time() - start)\n\n    print ""Building parser..."",\n    start = time.time()\n    parser = Parser(train_set)\n    print ""took {:.2f} seconds"".format(time.time() - start)\n\n    print ""Loading pretrained embeddings..."",\n    start = time.time()\n    word_vectors = {}\n    for line in open(config.embedding_file).readlines():\n        sp = line.strip().split()\n        word_vectors[sp[0]] = [float(x) for x in sp[1:]]\n    embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype=\'float32\')\n\n    for token in parser.tok2id:\n        i = parser.tok2id[token]\n        if token in word_vectors:\n            embeddings_matrix[i] = word_vectors[token]\n        elif token.lower() in word_vectors:\n            embeddings_matrix[i] = word_vectors[token.lower()]\n    print ""took {:.2f} seconds"".format(time.time() - start)\n\n    print ""Vectorizing data..."",\n    start = time.time()\n    train_set = parser.vectorize(train_set)\n    dev_set = parser.vectorize(dev_set)\n    test_set = parser.vectorize(test_set)\n    print ""took {:.2f} seconds"".format(time.time() - start)\n\n    print ""Preprocessing training data...""\n    train_examples = parser.create_instances(train_set)\n\n    return parser, embeddings_matrix, train_examples, dev_set, test_set,\n\nif __name__ == \'__main__\':\n    pass\n'"
