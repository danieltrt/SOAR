file_path,api_count,code
rnn_cell.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.platform import tf_logging as logging\n\n\nimport tensorflow as tf\nfrom tensorflow.python.layers import base as base_layer\n\n_BIAS_VARIABLE_NAME = ""bias""\n_WEIGHTS_VARIABLE_NAME = ""kernel""\n\n\nclass NLSTMCell(rnn_cell_impl.RNNCell):\n  """"""Nested LSTM Cell. Adapted from `rnn_cell_impl.LSTMCell`\n\n  The implementation is based on:\n    https://arxiv.org/abs/1801.10308\n    JRA. Moniz, D. Krueger.\n    ""Nested LSTMs""\n    ACML, PMLR 77:530-544, 2017\n  """"""\n\n  def __init__(self, num_units, depth, forget_bias=1.0,\n               state_is_tuple=True, use_peepholes=False,\n               activation=None, gate_activation=None,\n               cell_activation=None,\n               initializer=None,\n               input_gate_initializer=None,\n               use_bias=True, reuse=None, name=None):\n    """"""Initialize the basic NLSTM cell.\n\n    Args:\n      num_units: `int`, The number of hidden units of each cell state\n        and hidden state.\n      depth: `int`, The number of layers in the nest.\n      forget_bias: `float`, The bias added to forget gates.\n      state_is_tuple: If `True`, accepted and returned states are tuples of\n        the `h_state` and `c_state`s.  If `False`, they are concatenated\n        along the column axis.  The latter behavior will soon be deprecated.\n      use_peepholes: `bool`(optional).\n      activation: Activation function of the update values,\n        including new inputs and new cell states.  Default: `tanh`.\n      gate_activation: Activation function of the gates,\n        including the input, ouput, and forget gate. Default: `sigmoid`.\n      cell_activation: Activation function of the first cell gate. Default: `identity`.\n        Note that in the paper only the first cell_activation is identity.\n      initializer: Initializer of kernel. Default: `orthogonal_initializer`.\n      input_gate_initializer: Initializer of input gates.\n        Default: `glorot_normal_initializer`.\n      use_bias: `bool`. Default: `True`.\n      reuse: `bool`(optional) Python boolean describing whether to reuse variables\n        in an existing scope.  If not `True`, and the existing scope already has\n        the given variables, an error is raised.\n      name: `str`, the name of the layer. Layers with the same name will\n        share weights, but to avoid mistakes we require reuse=True in such\n        cases.\n    """"""\n    super(NLSTMCell, self).__init__(_reuse=reuse, name=name)\n    if not state_is_tuple:\n      logging.warn(""%s: Using a concatenated state is slower and will soon be ""\n                   ""deprecated.  Use state_is_tuple=True."", self)\n\n    # Inputs must be 2-dimensional.\n    self.input_spec = base_layer.InputSpec(ndim=2)\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._state_is_tuple = state_is_tuple\n    self._use_peepholes = use_peepholes\n    self._depth = depth\n    self._activation = activation or math_ops.tanh\n    self._gate_activation = gate_activation or math_ops.sigmoid\n    self._cell_activation = cell_activation or array_ops.identity\n    self._initializer = initializer or init_ops.orthogonal_initializer()\n    self._input_gate_initializer = (input_gate_initializer \n                                    or init_ops.glorot_normal_initializer())\n    self._use_bias = use_bias\n    self._kernels = None\n    self._biases = None\n    self.built = False\n\n  @property\n  def state_size(self):\n    if self._state_is_tuple:\n      return tuple([self._num_units] * (self.depth + 1))\n    else:\n      return self._num_units * (self.depth + 1)\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  @property\n  def depth(self):\n    return self._depth\n\n  def build(self, inputs_shape):\n    if inputs_shape[1].value is None:\n      raise ValueError(""Expected inputs.shape[-1] to be known, saw shape: %s""\n                       % inputs_shape)\n\n    input_depth = inputs_shape[1].value\n    h_depth = self._num_units\n    self._kernels = []\n    if self._use_bias:\n      self._biases = []\n\n    if self._use_peepholes:\n      self._peep_kernels = []\n    for i in range(self.depth):\n      if i == 0:\n        input_kernel = self.add_variable(\n            ""input_gate_kernel"",\n            shape=[input_depth, 4 * self._num_units],\n            initializer=self._input_gate_initializer)\n        hidden_kernel = self.add_variable(\n            ""hidden_gate_kernel"",\n            shape=[h_depth, 4 * self._num_units],\n            initializer=self._initializer)\n        kernel = tf.concat([input_kernel, hidden_kernel],\n                           axis=0, name=""kernel_0"")\n        self._kernels.append(kernel)\n      else:\n        self._kernels.append(\n            self.add_variable(\n                ""kernel_{}"".format(i),\n                shape=[2 * h_depth, 4 * self._num_units],\n                initializer=self._initializer))\n      if self._use_bias:\n        self._biases.append(\n            self.add_variable(\n                ""bias_{}"".format(i),\n                shape=[4 * self._num_units],\n                initializer=init_ops.zeros_initializer(dtype=self.dtype)))\n      if self._use_peepholes:\n        self._peep_kernels.append(\n            self.add_variable(\n                ""peep_kernel_{}"".format(i),\n                shape=[h_depth, 3 * self._num_units],\n                initializer=self._initializer))\n\n    self.built = True\n\n  def _recurrence(self, inputs, hidden_state, cell_states, depth):\n    """"""use recurrence to traverse the nested structure\n\n    Args:\n      inputs: A 2D `Tensor` of [batch_size x input_size] shape.\n      hidden_state: A 2D `Tensor` of [batch_size x num_units] shape.\n      cell_states: A `list` of 2D `Tensor` of [batch_size x num_units] shape.\n      depth: `int`\n        the current depth in the nested structure, begins at 0.\n\n    Returns:\n      new_h: A 2D `Tensor` of [batch_size x num_units] shape.\n        the latest hidden state for current step.\n      new_cs: A `list` of 2D `Tensor` of [batch_size x num_units] shape.\n        The accumulated cell states for current step.\n    """"""\n    sigmoid = math_ops.sigmoid\n    one = constant_op.constant(1, dtype=dtypes.int32)\n    # Parameters of gates are concatenated into one multiply for efficiency.\n    c = cell_states[depth]\n    h = hidden_state\n\n    gate_inputs = math_ops.matmul(\n        array_ops.concat([inputs, h], 1), self._kernels[depth])\n    if self._use_bias:\n      gate_inputs = nn_ops.bias_add(gate_inputs, self._biases[depth])\n    if self._use_peepholes:\n      peep_gate_inputs = math_ops.matmul(c, self._peep_kernels[depth])\n      i_peep, f_peep, o_peep = array_ops.split(\n          value=peep_gate_inputs, num_or_size_splits=3, axis=one)\n\n    # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n    i, j, f, o = array_ops.split(\n        value=gate_inputs, num_or_size_splits=4, axis=one)\n    if self._use_peepholes:\n      i += i_peep\n      f += f_peep\n      o += o_peep \n\n    if self._use_peepholes:\n      peep_gate_inputs = math_ops.matmul(c, self._peep_kernels[depth])\n      i_peep, f_peep, o_peep = array_ops.split(\n          value=peep_gate_inputs, num_or_size_splits=3, axis=one)\n      i += i_peep\n      f += f_peep\n      o += o_peep \n\n    # Note that using `add` and `multiply` instead of `+` and `*` gives a\n    # performance improvement. So using those at the cost of readability.\n    add = math_ops.add\n    multiply = math_ops.multiply\n\n    if self._use_bias:\n      forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype)\n      f = add(f, forget_bias_tensor)\n\n    inner_hidden = multiply(c, self._gate_activation(f))\n\n    if depth == 0:\n      inner_input = multiply(self._gate_activation(i), self._cell_activation(j))\n    else:\n      inner_input = multiply(self._gate_activation(i), self._activation(j))\n\n    if depth == (self.depth - 1):\n      new_c = add(inner_hidden, inner_input)\n      new_cs = [new_c]\n    else:\n      new_c, new_cs = self._recurrence(\n          inputs=inner_input,\n          hidden_state=inner_hidden,\n          cell_states=cell_states,\n          depth=depth + 1)\n    new_h = multiply(self._activation(new_c), self._gate_activation(o))\n    new_cs = [new_h] + new_cs\n    return new_h, new_cs\n\n  def call(self, inputs, state):\n    """"""forward propagation of the cell\n\n    Args:\n      inputs: a 2D `Tensor` of [batch_size x input_size] shape\n      state: a `tuple` of 2D `Tensor` of [batch_size x num_units] shape\n        or a `Tensor` of [batch_size x (num_units * (self.depth + 1))] shape\n\n    Returns:\n      outputs: a 2D `Tensor` of [batch_size x num_units] shape\n      next_state: a `tuple` of 2D `Tensor` of [batch_size x num_units] shape\n        or a `Tensor` of [batch_size x (num_units * (self.depth + 1))] shape\n    """"""\n    if not self._state_is_tuple:\n      states = array_ops.split(state, self.depth + 1, axis=1)\n    else:\n      states = state\n    hidden_state = states[0]\n    cell_states = states[1:]\n    outputs, next_state = self._recurrence(inputs, hidden_state, cell_states, 0)\n    if self._state_is_tuple:\n      next_state = tuple(next_state)\n    else:\n      next_state = array_ops.concat(next_state, axis=1)\n    return outputs, next_state\n'"
rnn_cell_test.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nimport rnn_cell as contrib_rnn_cell\nimport tensorflow as tf\nfrom tensorflow import test\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variables\n\n\nclass TestNLSTM(test.TestCase):\n\n  def _check_tuple_cell(self, *args, **kwargs):\n    batch_size = 2\n    num_units = 3\n    depth = 4\n    g = ops.Graph()\n    with self.test_session(graph=g) as sess:\n      with g.as_default():\n        cell = contrib_rnn_cell.NLSTMCell(num_units, depth, *args, **kwargs)\n        init_state = cell.zero_state(batch_size, dtype=dtypes.float32)\n        output, new_state = cell(\n            inputs=random_ops.random_normal([batch_size, 5]),\n            state=init_state)\n        variables.global_variables_initializer().run()\n        vals = sess.run([output, new_state])\n    self.assertAllEqual(vals[0], vals[1][0])\n    self.assertAllEqual(vals[0].shape, [2, 3])\n    for val in vals[1]:\n      self.assertAllEqual(val.shape, [2, 3])\n    self.assertEqual(len(vals[1]), 5)\n    self.assertAllEqual(cell.state_size, [num_units] * (depth + 1))\n    self.assertEqual(cell.depth, depth)\n    self.assertEqual(cell.output_size, num_units)\n\n  def _check_non_tuple_cell(self, *args, **kwargs):\n    batch_size = 2\n    num_units = 3\n    depth = 2\n    g = ops.Graph()\n    with self.test_session(graph=g) as sess:\n      with g.as_default():\n        cell = contrib_rnn_cell.NLSTMCell(num_units, depth,\n                                          *args, **kwargs)\n        init_state = cell.zero_state(batch_size, dtype=dtypes.float32)\n        output, new_state = cell(\n            inputs=random_ops.random_normal([batch_size, 5]),\n            state=init_state)\n        variables.global_variables_initializer().run()\n        vals = sess.run([output, new_state])\n    self.assertAllEqual(vals[0], vals[1][:, :3])\n    self.assertAllEqual(vals[0].shape, [2, 3])\n    self.assertAllEqual(vals[1].shape, [2, 9])\n    self.assertEqual(cell.state_size, num_units * (depth + 1))\n    self.assertEqual(cell.depth, depth)\n    self.assertEqual(cell.output_size, num_units)\n\n  def testNLSTMBranches(self):\n    state_is_tuples = [True, False]\n    use_peepholes = [True, False]\n    use_biases = [True, False]\n    options = itertools.product(state_is_tuples, use_peepholes, use_biases)\n    for option in options:\n      state_is_tuple, use_peephole, use_bias = option\n      if state_is_tuple:\n        self._check_tuple_cell(\n            state_is_tuple=state_is_tuple,\n            use_peepholes=use_peephole, use_bias=use_bias)\n      else:\n        self._check_non_tuple_cell(\n            state_is_tuple=state_is_tuple,\n            use_peepholes=use_peephole, use_bias=use_bias)\n\n'"
