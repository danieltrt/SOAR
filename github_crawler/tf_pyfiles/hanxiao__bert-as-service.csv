file_path,api_count,code
client/setup.py,0,"b""from os import path\n\nfrom setuptools import setup, find_packages\n\n# setup metainfo\nlibinfo_py = path.join('bert_serving', 'client', '__init__.py')\nlibinfo_content = open(libinfo_py, 'r').readlines()\nversion_line = [l.strip() for l in libinfo_content if l.startswith('__version__')][0]\nexec(version_line)  # produce __version__\n\nwith open('requirements.txt') as f:\n    require_packages = [line[:-1] if line[-1] == '\\n' else line for line in f]\n\nsetup(\n    name='bert_serving_client',\n    version=__version__,  # noqa\n    description='Mapping a variable-length sentence to a fixed-length vector using BERT model (Client)',\n    url='https://github.com/hanxiao/bert-as-service',\n    long_description=open('README.md', 'r').read(),\n    long_description_content_type='text/markdown',\n    author='Han Xiao',\n    author_email='artex.xh@gmail.com',\n    license='MIT',\n    packages=find_packages(),\n    zip_safe=False,\n    install_requires=require_packages,\n    classifiers=(\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3.6',\n        'License :: OSI Approved :: MIT License',\n        'Operating System :: OS Independent',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    ),\n    keywords='bert nlp tensorflow machine learning sentence encoding embedding serving',\n)\n"""
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# bert-as-service documentation build configuration file, created by\n# sphinx-quickstart on Wed Dec 19 20:32:51 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport os\nimport sys\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\'.\'))\nsys.path.insert(0, os.path.abspath(\'../client/bert_serving\'))\nsys.path.insert(0, os.path.abspath(\'../server/bert_serving\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.viewcode\',\n    \'sphinxarg.ext\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n# source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'bert-as-service\'\ncopyright = u\'2018, Han Xiao\'\nauthor = u\'Han Xiao\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = u\'1.6.1\'\n# The full version, including alpha/beta/rc tags.\nrelease = u\'1.6.1\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n# default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n# keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n# html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n\n# The name for this set of Sphinx documents.\n# ""<project> v<release> documentation"" by default.\n# html_title = u\'bert-as-service v1.6.1\'\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n# html_logo = None\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n# html_extra_path = []\n\n# If not None, a \'Last updated on:\' timestamp is inserted at every page\n# bottom, using the given strftime format.\n# The empty string is equivalent to \'%b %d, %Y\'.\n# html_last_updated_fmt = None\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n# html_additional_pages = {}\n\n# If false, no module index is generated.\nhtml_domain_indices = True\n\n# If false, no index is generated.\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n# html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n# html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n# html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\', \'zh\'\n# html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# \'ja\' uses this config value.\n# \'zh\' user can custom change `jieba` dictionary path.\n# html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n# html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'bert-as-servicedoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'bert-as-service.tex\', u\'bert-as-service Documentation\',\n     u\'Han Xiao\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n\n# If false, no module index is generated.\n# latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'bert-as-service\', u\'bert-as-service Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n# man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'bert-as-service\', u\'bert-as-service Documentation\',\n     author, \'bert-as-service\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n# texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n# texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n# texinfo_no_detailmenu = False\n\nautoclass_content = \'both\'\n\n'"
example/example1.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Han Xiao <artex.xh@gmail.com> <https://hanxiao.github.io>\n\n# NOTE: First install bert-as-service via\n# $\n# $ pip install bert-serving-server\n# $ pip install bert-serving-client\n# $\n\n# using BertClient in sync way\n\nimport sys\nimport time\n\nfrom bert_serving.client import BertClient\n\nif __name__ == '__main__':\n    bc = BertClient(port=int(sys.argv[1]), port_out=int(sys.argv[2]), show_server_config=True)\n    # encode a list of strings\n    with open('README.md') as fp:\n        data = [v for v in fp if v.strip()][:512]\n        num_tokens = sum(len([vv for vv in v.split() if vv.strip()]) for v in data)\n\n    show_tokens = len(sys.argv) > 3 and bool(sys.argv[3])\n    bc.encode(data)  # warm-up GPU\n    for j in range(10):\n        tmp = data * (2 ** j)\n        c_num_tokens = num_tokens * (2 ** j)\n        start_t = time.time()\n        bc.encode(tmp, show_tokens=show_tokens)\n        time_t = time.time() - start_t\n        print('encoding %10d sentences\\t%.2fs\\t%4d samples/s\\t%6d tokens/s' %\n              (len(tmp), time_t, int(len(tmp) / time_t), int(c_num_tokens / time_t)))\n"""
example/example2.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Han Xiao <artex.xh@gmail.com> <https://hanxiao.github.io>\n\n# NOTE: First install bert-as-service via\n# $\n# $ pip install bert-serving-server\n# $ pip install bert-serving-client\n# $\n\n# using BertClient in async way\n\nimport sys\n\nfrom bert_serving.client import BertClient\n\n\ndef send_without_block(bc, data, repeat=10):\n    # encoding without blocking:\n    print('sending all data without blocking...')\n    for _ in range(repeat):\n        bc.encode(data, blocking=False)\n    print('all sent!')\n\n\nif __name__ == '__main__':\n    bc = BertClient(port=int(sys.argv[1]), port_out=int(sys.argv[2]))\n    num_repeat = 20\n\n    with open('README.md') as fp:\n        data = [v for v in fp if v.strip()]\n\n    send_without_block(bc, data, num_repeat)\n\n    num_expect_vecs = len(data) * num_repeat\n\n    # then fetch all\n    print('now waiting until all results are available...')\n    vecs = bc.fetch_all(concat=True)\n    print('received %s, expected: %d' % (vecs.shape, num_expect_vecs))\n\n    # now send it again\n    send_without_block(bc, data, num_repeat)\n\n    # this time fetch them one by one, due to the async encoding and server scheduling\n    # sending order is NOT preserved!\n    for v in bc.fetch():\n        print('received %s, shape %s' % (v.id, v.content.shape))\n\n\n    # finally let's do encode-fetch at the same time but in async mode\n    # we do that by building an endless data stream, generating data in an extremely fast speed\n    def text_gen():\n        while True:\n            yield data\n\n\n    for j in bc.encode_async(text_gen(), max_num_batch=20):\n        print('received %d : %s' % (j.id, j.content))\n"""
example/example3.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Han Xiao <artex.xh@gmail.com> <https://hanxiao.github.io>\n\n# NOTE: First install bert-as-service via\n# $\n# $ pip install bert-serving-server\n# $ pip install bert-serving-client\n# $\n\n# using BertClient in multicast way\n\nimport sys\nimport threading\n\nfrom bert_serving.client import BertClient\n\n\ndef client_clone(id, idx):\n    bc = BertClient(port=int(sys.argv[1]), port_out=int(sys.argv[2]), identity=id)\n    for j in bc.fetch():\n        print('clone-client-%d: received %d x %d' % (idx, j.shape[0], j.shape[1]))\n\n\nif __name__ == '__main__':\n    bc = BertClient(port=int(sys.argv[1]), port_out=int(sys.argv[2]))\n    # start two cloned clients sharing the same identity as bc\n    for j in range(2):\n        t = threading.Thread(target=client_clone, args=(bc.identity, j))\n        t.start()\n\n    with open('README.md') as fp:\n        data = [v for v in fp if v.strip()]\n\n    for _ in range(3):\n        vec = bc.encode(data)\n        print('bc received %d x %d' % (vec.shape[0], vec.shape[1]))\n"""
example/example4.py,6,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Han Xiao <artex.xh@gmail.com> <https://hanxiao.github.io>\n\n# NOTE: First install bert-as-service via\n# $\n# $ pip install bert-serving-server\n# $ pip install bert-serving-client\n# $\n\n# using BertClient inside tf.data API\n\nimport json\nimport os\nimport time\n\nimport GPUtil\nimport tensorflow as tf\nfrom bert_serving.client import ConcurrentBertClient\n\nos.environ['CUDA_VISIBLE_DEVICES'] = str(GPUtil.getFirstAvailable())\n\ntrain_fp = ['/data/cips/data/larry-autoencoder/cail_0518/data_train.json']\nbatch_size = 256\nnum_parallel_calls = 4\nnum_concurrent_clients = 10  # should be greater than `num_parallel_calls`\n\n# to support `num_parallel_calls` in tf.data, you need ConcurrentBertClient\nbc = ConcurrentBertClient()\n\n\ndef get_encodes(x):\n    # x is `batch_size` of lines, each of which is a json object\n    samples = [json.loads(l) for l in x]\n    text = [s['fact'][-50:] for s in samples]\n    features = bc.encode(text)\n    labels = [0 for _ in text]\n    return features, labels\n\n\ndata_node = (tf.data.TextLineDataset(train_fp).batch(batch_size)\n             .map(lambda x: tf.py_func(get_encodes, [x], [tf.float32, tf.int64], name='bert_client'),\n                  num_parallel_calls=num_parallel_calls)\n             .map(lambda x, y: {'feature': x, 'label': y})\n             .make_one_shot_iterator().get_next())\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    cnt, num_samples, start_t = 0, 0, time.perf_counter()\n    while True:\n        x = sess.run(data_node)\n        cnt += 1\n        num_samples += x['feature'].shape[0]\n        if cnt % 10 == 0:\n            time_used = time.perf_counter() - start_t\n            print('data speed: %d/s' % int(num_samples / time_used))\n            cnt, num_samples, start_t = 0, 0, time.perf_counter()\n"""
example/example5.py,6,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Han Xiao <artex.xh@gmail.com> <https://hanxiao.github.io>\n\n# NOTE: First install bert-as-service via\n# $\n# $ pip install bert-serving-server\n# $ pip install bert-serving-client\n# $\n\n# solving chinese law-article classification problem: https://github.com/thunlp/CAIL/blob/master/README_en.md\n\nimport json\nimport os\nimport random\n\nimport GPUtil\nimport tensorflow as tf\nfrom bert_serving.client import ConcurrentBertClient\nfrom tensorflow.python.estimator.canned.dnn import DNNClassifier\nfrom tensorflow.python.estimator.run_config import RunConfig\nfrom tensorflow.python.estimator.training import TrainSpec, EvalSpec, train_and_evaluate\n\nos.environ['CUDA_VISIBLE_DEVICES'] = str(GPUtil.getFirstAvailable()[0])\ntf.logging.set_verbosity(tf.logging.INFO)\n\ntrain_fp = ['/data/cips/data/lab/data/dataset/final_all_data/exercise_contest/data_train.json']\neval_fp = ['/data/cips/data/lab/data/dataset/final_all_data/exercise_contest/data_test.json']\n\nbatch_size = 128\nnum_parallel_calls = 4\nnum_concurrent_clients = num_parallel_calls * 2  # should be at least greater than `num_parallel_calls`\n\nbc = ConcurrentBertClient(port=5557, port_out=5558)\n\n# hardcoded law_ids\nlaws = [184, 336, 314, 351, 224, 132, 158, 128, 223, 308, 341, 349, 382, 238, 369, 248, 266, 313, 127, 340, 288, 172,\n        209, 243, 302, 200, 227, 155, 147, 143, 261, 124, 359, 343, 291, 241, 235, 367, 393, 274, 240, 269, 199, 119,\n        246, 282, 133, 177, 170, 310, 364, 201, 312, 244, 357, 233, 236, 264, 225, 234, 328, 417, 151, 135, 136, 348,\n        217, 168, 134, 237, 262, 150, 114, 196, 303, 191, 392, 226, 267, 272, 212, 353, 315, 205, 372, 215, 350, 275,\n        385, 164, 338, 292, 159, 162, 333, 388, 356, 375, 326, 402, 397, 125, 395, 290, 176, 354, 185, 141, 279, 399,\n        192, 383, 307, 295, 361, 286, 404, 390, 294, 115, 344, 268, 171, 117, 273, 193, 418, 220, 198, 231, 386, 363,\n        346, 210, 270, 144, 347, 280, 281, 118, 122, 116, 360, 239, 228, 305, 130, 152, 389, 276, 213, 186, 413, 285,\n        316, 245, 232, 175, 149, 263, 387, 283, 391, 211, 396, 352, 345, 258, 253, 163, 140, 293, 194, 342, 161, 358,\n        271, 156, 260, 384, 153, 277, 214]\n\nlaws_str = [str(x) for x in laws]\n\n\ndef get_encodes(x):\n    # x is `batch_size` of lines, each of which is a json object\n    samples = [json.loads(l) for l in x]\n    text = [s['fact'][:50] + s['fact'][-50:] for s in samples]\n    features = bc.encode(text)\n    # randomly choose a label\n    labels = [[str(random.choice(s['meta']['relevant_articles']))] for s in samples]\n    return features, labels\n\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nrun_config = RunConfig(model_dir='/data/cips/save/law-model',\n                       session_config=config,\n                       save_checkpoints_steps=1000)\n\nestimator = DNNClassifier(\n    hidden_units=[512],\n    feature_columns=[tf.feature_column.numeric_column('feature', shape=(768,))],\n    n_classes=len(laws),\n    config=run_config,\n    label_vocabulary=laws_str,\n    dropout=0.1)\n\ninput_fn = lambda fp: (tf.data.TextLineDataset(fp)\n                       .apply(tf.contrib.data.shuffle_and_repeat(buffer_size=10000))\n                       .batch(batch_size)\n                       .map(lambda x: tf.py_func(get_encodes, [x], [tf.float32, tf.string], name='bert_client'),\n                            num_parallel_calls=num_parallel_calls)\n                       .map(lambda x, y: ({'feature': x}, y))\n                       .prefetch(20))\n\ntrain_spec = TrainSpec(input_fn=lambda: input_fn(train_fp))\neval_spec = EvalSpec(input_fn=lambda: input_fn(eval_fp), throttle_secs=0)\ntrain_and_evaluate(estimator, train_spec, eval_spec)\n"""
example/example6.py,11,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Han Xiao <artex.xh@gmail.com> <https://hanxiao.github.io>\n\n# NOTE: First install bert-as-service via\n# $\n# $ pip install bert-serving-server\n# $ pip install bert-serving-client\n# $\n\n# read and write TFRecord\n\nimport os\n\nimport GPUtil\nimport tensorflow as tf\nfrom bert_serving.client import BertClient\n\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = str(GPUtil.getFirstAvailable()[0])\ntf.logging.set_verbosity(tf.logging.INFO)\n\nwith open(\'README.md\') as fp:\n    data = [v for v in fp if v.strip()]\n    bc = BertClient()\n    list_vec = bc.encode(data)\n    list_label = [0 for _ in data]  # a dummy list of all-zero labels\n\n# write tfrecords\n\nwith tf.python_io.TFRecordWriter(\'tmp.tfrecord\') as writer:\n    def create_float_feature(values):\n        return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n\n\n    def create_int_feature(values):\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n\n\n    for (vec, label) in zip(list_vec, list_label):\n        features = {\'features\': create_float_feature(vec), \'labels\': create_int_feature([label])}\n        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n        writer.write(tf_example.SerializeToString())\n\n# read tfrecords and build dataset from it\n\nnum_hidden_unit = 768\n\n\ndef _decode_record(record):\n    """"""Decodes a record to a TensorFlow example.""""""\n    return tf.parse_single_example(record, {\n        \'features\': tf.FixedLenFeature([num_hidden_unit], tf.float32),\n        \'labels\': tf.FixedLenFeature([], tf.int64),\n    })\n\n\nds = (tf.data.TFRecordDataset(\'tmp.tfrecord\').repeat().shuffle(buffer_size=100).apply(\n    tf.contrib.data.map_and_batch(lambda record: _decode_record(record), batch_size=64))\n      .make_one_shot_iterator().get_next())\n\nwith tf.Session() as sess:\n    while True:\n        print(sess.run(ds))\n'"
example/example7.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Han Xiao <artex.xh@gmail.com> <https://hanxiao.github.io>\n\n# NOTE: First install bert-as-service via\n# $\n# $ pip install bert-serving-server\n# $ pip install bert-serving-client\n# $\n\n# visualizing a 12-layer BERT\n\nimport time\nfrom collections import namedtuple\n\nimport numpy as np\nimport pandas as pd\n# from MulticoreTSNE import MulticoreTSNE as TSNE\nfrom bert_serving.client import BertClient\nfrom bert_serving.server import BertServer\nfrom bert_serving.server.helper import get_args_parser\nfrom matplotlib import pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.decomposition import PCA\n\n\n#=========================== dump bert vectors ===========================\ndata = pd.read_csv(\'/corpus/uci-news-aggregator.csv\', usecols=[\'TITLE\', \'CATEGORY\'])\n\n# just copy paste from some Kaggle kernel ->\nnum_of_categories = 5000\nshuffled = data.reindex(np.random.permutation(data.index))\ne = shuffled[shuffled[\'CATEGORY\'] == \'e\'][:num_of_categories]\nb = shuffled[shuffled[\'CATEGORY\'] == \'b\'][:num_of_categories]\nt = shuffled[shuffled[\'CATEGORY\'] == \'t\'][:num_of_categories]\nm = shuffled[shuffled[\'CATEGORY\'] == \'m\'][:num_of_categories]\nconcated = pd.concat([e, b, t, m], ignore_index=True)\n# Shuffle the dataset\nconcated = concated.reindex(np.random.permutation(concated.index))\nconcated[\'LABEL\'] = 0\n# One-hot encode the lab\nconcated.loc[concated[\'CATEGORY\'] == \'e\', \'LABEL\'] = 0\nconcated.loc[concated[\'CATEGORY\'] == \'b\', \'LABEL\'] = 1\nconcated.loc[concated[\'CATEGORY\'] == \'t\', \'LABEL\'] = 2\nconcated.loc[concated[\'CATEGORY\'] == \'m\', \'LABEL\'] = 3\n\nsubset_text = list(concated[\'TITLE\'].values)\nsubset_label = list(concated[\'LABEL\'].values)\nnum_label = len(set(subset_label))\n\n# <- just copy paste from some Kaggle kernel\n\nprint(\'min_seq_len: %d\' % min(len(v.split()) for v in subset_text))\nprint(\'max_seq_len: %d\' % max(len(v.split()) for v in subset_text))\nprint(\'unique label: %d\' % num_label)\n\npool_layer = 1\nsubset_vec_all_layers = []\nport = 6006\nport_out = 6007\n\ncommon = [\n    \'-model_dir\', \'/bert_model/chinese_L-12_H-768_A-12/\',\n    \'-num_worker\', \'2\',\n    \'-port\', str(port),\n    \'-port_out\', str(port_out),\n    \'-max_seq_len\', \'20\',\n    # \'-client_batch_size\', \'2048\',\n    \'-max_batch_size\', \'256\',\n    # \'-num_client\', \'1\',\n    \'-pooling_strategy\', \'REDUCE_MEAN\',\n    \'-pooling_layer\', \'-2\',\n    \'-gpu_memory_fraction\', \'0.2\',\n    \'-device\',\'3\',\n]\nargs = get_args_parser().parse_args(common)\n\nfor pool_layer in range(1, 13):\n    setattr(args, \'pooling_layer\', [-pool_layer])\n    server = BertServer(args)\n    server.start()\n    print(\'wait until server is ready...\')\n    time.sleep(20)\n    print(\'encoding...\')\n    bc = BertClient(port=port, port_out=port_out, show_server_config=True)\n    subset_vec_all_layers.append(bc.encode(subset_text))\n    bc.close()\n    server.close()\n    print(\'done at layer -%d\' % pool_layer)\n\n#save bert vectors and labels\nstacked_subset_vec_all_layers = np.stack(subset_vec_all_layers)\nnp.save(\'example7_5k_2\',stacked_subset_vec_all_layers)\nnp_subset_label = np.array(subset_label)\nnp.save(\'example7_5k_2_subset_label\',np_subset_label)\n\n#load bert vectors and labels\nsubset_vec_all_layers = np.load(\'example7_5k_mxnet.npy\')\nnp_subset_label = np.load(\'example7_5k_mxnet_subset_label.npy\')\nsubset_label = np_subset_label.tolist()\n#=========================== visualize ===========================\ndef vis(embed, vis_alg=\'PCA\', pool_alg=\'REDUCE_MEAN\'):\n    plt.close()\n    fig = plt.figure()\n    plt.rcParams[\'figure.figsize\'] = [21, 7]\n    for idx, ebd in enumerate(embed):\n        ax = plt.subplot(2, 6, idx + 1)\n        vis_x = ebd[:, 0]\n        vis_y = ebd[:, 1]\n        plt.scatter(vis_x, vis_y, c=subset_label, cmap=ListedColormap([""blue"", ""green"", ""yellow"", ""red""]), marker=\'.\',\n                    alpha=0.7, s=2)\n        ax.set_title(\'pool_layer=-%d\' % (idx + 1))\n    plt.tight_layout()\n    plt.subplots_adjust(bottom=0.1, right=0.95, top=0.9)\n    cax = plt.axes([0.96, 0.1, 0.01, 0.3])\n    cbar = plt.colorbar(cax=cax, ticks=range(num_label))\n    cbar.ax.get_yaxis().set_ticks([])\n    for j, lab in enumerate([\'ent.\', \'bus.\', \'sci.\', \'heal.\']):\n        cbar.ax.text(.5, (2 * j + 1) / 8.0, lab, ha=\'center\', va=\'center\', rotation=270)\n    fig.suptitle(\'%s visualization of BERT layers using ""bert-as-service"" (-pool_strategy=%s)\' % (vis_alg, pool_alg),\n                 fontsize=14)\n    plt.show()\n\n\npca_embed = [PCA(n_components=2).fit_transform(v) for v in subset_vec_all_layers]\nvis(pca_embed)\n\n# if False:\n#     tsne_embed = [TSNE(n_jobs=8).fit_transform(v) for v in subset_vec_all_layers]\n#     vis(tsne_embed, \'t-SNE\')\n'"
example/example8.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Han Xiao <artex.xh@gmail.com> <https://hanxiao.github.io>\n\n# NOTE: First install bert-as-service via\n# $\n# $ pip install bert-serving-server\n# $ pip install bert-serving-client\n# $\n\n# simple similarity search on FAQ\n\nimport numpy as np\nfrom bert_serving.client import BertClient\nfrom termcolor import colored\n\nprefix_q = \'##### **Q:** \'\ntopk = 5\n\nwith open(\'README.md\') as fp:\n    questions = [v.replace(prefix_q, \'\').strip() for v in fp if v.strip() and v.startswith(prefix_q)]\n    print(\'%d questions loaded, avg. len of %d\' % (len(questions), np.mean([len(d.split()) for d in questions])))\n\nwith BertClient(port=4000, port_out=4001) as bc:\n    doc_vecs = bc.encode(questions)\n\n    while True:\n        query = input(colored(\'your question: \', \'green\'))\n        query_vec = bc.encode([query])[0]\n        # compute normalized dot product as score\n        score = np.sum(query_vec * doc_vecs, axis=1) / np.linalg.norm(doc_vecs, axis=1)\n        topk_idx = np.argsort(score)[::-1][:topk]\n        print(\'top %d questions similar to ""%s""\' % (topk, colored(query, \'green\')))\n        for idx in topk_idx:\n            print(\'> %s\\t%s\' % (colored(\'%.1f\' % score[idx], \'cyan\'), colored(questions[idx], \'yellow\')))\n'"
server/__init__.py,0,b''
server/setup.py,0,"b'from os import path\n\nfrom setuptools import setup, find_packages\n\n# setup metainfo\nlibinfo_py = path.join(\'bert_serving\', \'server\', \'__init__.py\')\nlibinfo_content = open(libinfo_py, \'r\').readlines()\nversion_line = [l.strip() for l in libinfo_content if l.startswith(\'__version__\')][0]\nexec(version_line)  # produce __version__\n\nsetup(\n    name=\'bert_serving_server\',\n    version=__version__,\n    description=\'Mapping a variable-length sentence to a fixed-length vector using BERT model (Server)\',\n    url=\'https://github.com/hanxiao/bert-as-service\',\n    long_description=open(\'README.md\', \'r\', encoding=""utf8"").read(),\n    long_description_content_type=\'text/markdown\',\n    author=\'Han Xiao\',\n    author_email=\'artex.xh@gmail.com\',\n    license=\'MIT\',\n    packages=find_packages(),\n    zip_safe=False,\n    install_requires=[\n        \'numpy\',\n        \'six\',\n        \'pyzmq>=17.1.0\',\n        \'GPUtil>=1.3.0\',\n        \'termcolor>=1.1\'\n    ],\n    extras_require={\n        \'cpu\': [\'tensorflow>=1.10.0\'],\n        \'gpu\': [\'tensorflow-gpu>=1.10.0\'],\n        \'http\': [\'flask\', \'flask-compress\', \'flask-cors\', \'flask-json\', \'bert-serving-client\']\n    },\n    classifiers=(\n        \'Programming Language :: Python :: 3.6\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Operating System :: OS Independent\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n    ),\n    entry_points={\n        \'console_scripts\': [\'bert-serving-start=bert_serving.server.cli:main\',\n                            \'bert-serving-benchmark=bert_serving.server.cli:benchmark\',\n                            \'bert-serving-terminate=bert_serving.server.cli:terminate\'],\n    },\n    keywords=\'bert nlp tensorflow machine learning sentence encoding embedding serving\',\n)\n'"
client/bert_serving/__init__.py,0,"b""__path__ = __import__('pkgutil').extend_path(__path__, __name__)\n"""
server/bert_serving/__init__.py,0,"b""__path__ = __import__('pkgutil').extend_path(__path__, __name__)\n"""
client/bert_serving/client/__init__.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Han Xiao <artex.xh@gmail.com> <https://hanxiao.github.io>\n\nimport sys\nimport threading\nimport time\nimport uuid\nimport warnings\nfrom collections import namedtuple\nfrom functools import wraps\n\nimport numpy as np\nimport zmq\nfrom zmq.utils import jsonapi\n\n__all__ = [\'__version__\', \'BertClient\', \'ConcurrentBertClient\']\n\n# in the future client version must match with server version\n__version__ = \'1.10.0\'\n\nif sys.version_info >= (3, 0):\n    from ._py3_var import *\nelse:\n    from ._py2_var import *\n\n_Response = namedtuple(\'_Response\', [\'id\', \'content\'])\nResponse = namedtuple(\'Response\', [\'id\', \'embedding\', \'tokens\'])\n\n\nclass BertClient(object):\n    def __init__(self, ip=\'localhost\', port=5555, port_out=5556,\n                 output_fmt=\'ndarray\', show_server_config=False,\n                 identity=None, check_version=True, check_length=True,\n                 check_token_info=True, ignore_all_checks=False,\n                 timeout=-1):\n        """""" A client object connected to a BertServer\n\n        Create a BertClient that connects to a BertServer.\n        Note, server must be ready at the moment you are calling this function.\n        If you are not sure whether the server is ready, then please set `ignore_all_checks=True`\n\n        You can also use it as a context manager:\n\n        .. highlight:: python\n        .. code-block:: python\n\n            with BertClient() as bc:\n                bc.encode(...)\n\n            # bc is automatically closed out of the context\n\n        :type timeout: int\n        :type check_version: bool\n        :type check_length: bool\n        :type check_token_info: bool\n        :type ignore_all_checks: bool\n        :type identity: str\n        :type show_server_config: bool\n        :type output_fmt: str\n        :type port_out: int\n        :type port: int\n        :type ip: str\n        :param ip: the ip address of the server\n        :param port: port for pushing data from client to server, must be consistent with the server side config\n        :param port_out: port for publishing results from server to client, must be consistent with the server side config\n        :param output_fmt: the output format of the sentence encodes, either in numpy array or python List[List[float]] (ndarray/list)\n        :param show_server_config: whether to show server configs when first connected\n        :param identity: the UUID of this client\n        :param check_version: check if server has the same version as client, raise AttributeError if not the same\n        :param check_length: check if server `max_seq_len` is less than the sentence length before sent\n        :param check_token_info: check if server can return tokenization\n        :param ignore_all_checks: ignore all checks, set it to True if you are not sure whether the server is ready when constructing BertClient()\n        :param timeout: set the timeout (milliseconds) for receive operation on the client, -1 means no timeout and wait until result returns\n        """"""\n\n        self.context = zmq.Context()\n        self.sender = self.context.socket(zmq.PUSH)\n        self.sender.setsockopt(zmq.LINGER, 0)\n        self.identity = identity or str(uuid.uuid4()).encode(\'ascii\')\n        self.sender.connect(\'tcp://%s:%d\' % (ip, port))\n\n        self.receiver = self.context.socket(zmq.SUB)\n        self.receiver.setsockopt(zmq.LINGER, 0)\n        self.receiver.setsockopt(zmq.SUBSCRIBE, self.identity)\n        self.receiver.connect(\'tcp://%s:%d\' % (ip, port_out))\n\n        self.request_id = 0\n        self.timeout = timeout\n        self.pending_request = set()\n        self.pending_response = {}\n\n        if output_fmt == \'ndarray\':\n            self.formatter = lambda x: x\n        elif output_fmt == \'list\':\n            self.formatter = lambda x: x.tolist()\n        else:\n            raise AttributeError(\'""output_fmt"" must be ""ndarray"" or ""list""\')\n\n        self.output_fmt = output_fmt\n        self.port = port\n        self.port_out = port_out\n        self.ip = ip\n        self.length_limit = 0\n        self.token_info_available = False\n\n        if not ignore_all_checks and (check_version or show_server_config or check_length or check_token_info):\n            s_status = self.server_config\n\n            if check_version and s_status[\'server_version\'] != self.status[\'client_version\']:\n                raise AttributeError(\'version mismatch! server version is %s but client version is %s!\\n\'\n                                     \'consider ""pip install -U bert-serving-server bert-serving-client""\\n\'\n                                     \'or disable version-check by ""BertClient(check_version=False)""\' % (\n                                         s_status[\'server_version\'], self.status[\'client_version\']))\n\n            if check_length:\n                if s_status[\'max_seq_len\'] is not None:\n                    self.length_limit = int(s_status[\'max_seq_len\'])\n                else:\n                    self.length_limit = None\n\n            if check_token_info:\n                self.token_info_available = bool(s_status[\'show_tokens_to_client\'])\n\n            if show_server_config:\n                self._print_dict(s_status, \'server config:\')\n\n    def close(self):\n        """"""\n            Gently close all connections of the client. If you are using BertClient as context manager,\n            then this is not necessary.\n\n        """"""\n        self.sender.close()\n        self.receiver.close()\n        self.context.term()\n\n    def _send(self, msg, msg_len=0):\n        self.request_id += 1\n        self.sender.send_multipart([self.identity, msg, b\'%d\' % self.request_id, b\'%d\' % msg_len])\n        self.pending_request.add(self.request_id)\n        return self.request_id\n\n    def _recv(self, wait_for_req_id=None):\n        try:\n            while True:\n                # a request has been returned and found in pending_response\n                if wait_for_req_id in self.pending_response:\n                    response = self.pending_response.pop(wait_for_req_id)\n                    return _Response(wait_for_req_id, response)\n\n                # receive a response\n                response = self.receiver.recv_multipart()\n                request_id = int(response[-1])\n\n                # if not wait for particular response then simply return\n                if not wait_for_req_id or (wait_for_req_id == request_id):\n                    self.pending_request.remove(request_id)\n                    return _Response(request_id, response)\n                elif wait_for_req_id != request_id:\n                    self.pending_response[request_id] = response\n                    # wait for the next response\n        except Exception as e:\n            raise e\n        finally:\n            if wait_for_req_id in self.pending_request:\n                self.pending_request.remove(wait_for_req_id)\n\n    def _recv_ndarray(self, wait_for_req_id=None):\n        request_id, response = self._recv(wait_for_req_id)\n        arr_info, arr_val = jsonapi.loads(response[1]), response[2]\n        X = np.frombuffer(_buffer(arr_val), dtype=str(arr_info[\'dtype\']))\n        return Response(request_id, self.formatter(X.reshape(arr_info[\'shape\'])), arr_info.get(\'tokens\', \'\'))\n\n    @property\n    def status(self):\n        """"""\n            Get the status of this BertClient instance\n\n        :rtype: dict[str, str]\n        :return: a dictionary contains the status of this BertClient instance\n\n        """"""\n        return {\n            \'identity\': self.identity,\n            \'num_request\': self.request_id,\n            \'num_pending_request\': len(self.pending_request),\n            \'pending_request\': self.pending_request,\n            \'output_fmt\': self.output_fmt,\n            \'port\': self.port,\n            \'port_out\': self.port_out,\n            \'server_ip\': self.ip,\n            \'client_version\': __version__,\n            \'timeout\': self.timeout\n        }\n\n    def _timeout(func):\n        @wraps(func)\n        def arg_wrapper(self, *args, **kwargs):\n            if \'blocking\' in kwargs and not kwargs[\'blocking\']:\n                # override client timeout setting if `func` is called in non-blocking way\n                self.receiver.setsockopt(zmq.RCVTIMEO, -1)\n            else:\n                self.receiver.setsockopt(zmq.RCVTIMEO, self.timeout)\n            try:\n                return func(self, *args, **kwargs)\n            except zmq.error.Again as _e:\n                t_e = TimeoutError(\n                    \'no response from the server (with ""timeout""=%d ms), please check the following:\'\n                    \'is the server still online? is the network broken? are ""port"" and ""port_out"" correct? \'\n                    \'are you encoding a huge amount of data whereas the timeout is too small for that?\' % self.timeout)\n                if _py2:\n                    raise t_e\n                else:\n                    _raise(t_e, _e)\n            finally:\n                self.receiver.setsockopt(zmq.RCVTIMEO, -1)\n\n        return arg_wrapper\n\n    @property\n    @_timeout\n    def server_config(self):\n        """"""\n            Get the current configuration of the server connected to this client\n\n        :return: a dictionary contains the current configuration of the server connected to this client\n        :rtype: dict[str, str]\n\n        """"""\n        req_id = self._send(b\'SHOW_CONFIG\')\n        return jsonapi.loads(self._recv(req_id).content[1])\n\n    @property\n    @_timeout\n    def server_status(self):\n        """"""\n            Get the current status of the server connected to this client\n\n        :return: a dictionary contains the current status of the server connected to this client\n        :rtype: dict[str, str]\n\n        """"""\n        req_id = self._send(b\'SHOW_STATUS\')\n        return jsonapi.loads(self._recv(req_id).content[1])\n\n    @_timeout\n    def encode(self, texts, blocking=True, is_tokenized=False, show_tokens=False):\n        """""" Encode a list of strings to a list of vectors\n\n        `texts` should be a list of strings, each of which represents a sentence.\n        If `is_tokenized` is set to True, then `texts` should be list[list[str]],\n        outer list represents sentence and inner list represent tokens in the sentence.\n        Note that if `blocking` is set to False, then you need to fetch the result manually afterwards.\n\n        .. highlight:: python\n        .. code-block:: python\n\n            with BertClient() as bc:\n                # encode untokenized sentences\n                bc.encode([\'First do it\',\n                          \'then do it right\',\n                          \'then do it better\'])\n\n                # encode tokenized sentences\n                bc.encode([[\'First\', \'do\', \'it\'],\n                           [\'then\', \'do\', \'it\', \'right\'],\n                           [\'then\', \'do\', \'it\', \'better\']], is_tokenized=True)\n\n        :type is_tokenized: bool\n        :type show_tokens: bool\n        :type blocking: bool\n        :type timeout: bool\n        :type texts: list[str] or list[list[str]]\n        :param is_tokenized: whether the input texts is already tokenized\n        :param show_tokens: whether to include tokenization result from the server. If true, the return of the function will be a tuple\n        :param texts: list of sentence to be encoded. Larger list for better efficiency.\n        :param blocking: wait until the encoded result is returned from the server. If false, will immediately return.\n        :param timeout: throw a timeout error when the encoding takes longer than the predefined timeout.\n        :return: encoded sentence/token-level embeddings, rows correspond to sentences\n        :rtype: numpy.ndarray or list[list[float]]\n\n        """"""\n        if is_tokenized:\n            self._check_input_lst_lst_str(texts)\n        else:\n            self._check_input_lst_str(texts)\n\n        if self.length_limit is None:\n            warnings.warn(\'server does not put a restriction on ""max_seq_len"", \'\n                          \'it will determine ""max_seq_len"" dynamically according to the sequences in the batch. \'\n                          \'you can restrict the sequence length on the client side for better efficiency\')\n        elif self.length_limit and not self._check_length(texts, self.length_limit, is_tokenized):\n            warnings.warn(\'some of your sentences have more tokens than ""max_seq_len=%d"" set on the server, \'\n                          \'as consequence you may get less-accurate or truncated embeddings.\\n\'\n                          \'here is what you can do:\\n\'\n                          \'- disable the length-check by create a new ""BertClient(check_length=False)"" \'\n                          \'when you do not want to display this warning\\n\'\n                          \'- or, start a new server with a larger ""max_seq_len""\' % self.length_limit)\n\n        req_id = self._send(jsonapi.dumps(texts), len(texts))\n        if not blocking:\n            return None\n        r = self._recv_ndarray(req_id)\n        if self.token_info_available and show_tokens:\n            return r.embedding, r.tokens\n        elif not self.token_info_available and show_tokens:\n            warnings.warn(\'""show_tokens=True"", but the server does not support showing tokenization info to clients.\\n\'\n                          \'here is what you can do:\\n\'\n                          \'- start a new server with ""bert-serving-start -show_tokens_to_client ...""\\n\'\n                          \'- or, use ""encode(show_tokens=False)""\')\n        return r.embedding\n\n    def fetch(self, delay=.0):\n        """""" Fetch the encoded vectors from server, use it with `encode(blocking=False)`\n\n        Use it after `encode(texts, blocking=False)`. If there is no pending requests, will return None.\n        Note that `fetch()` does not preserve the order of the requests! Say you have two non-blocking requests,\n        R1 and R2, where R1 with 256 samples, R2 with 1 samples. It could be that R2 returns first.\n\n        To fetch all results in the original sending order, please use `fetch_all(sort=True)`\n\n        :type delay: float\n        :param delay: delay in seconds and then run fetcher\n        :return: a generator that yields request id and encoded vector in a tuple, where the request id can be used to determine the order\n        :rtype: Iterator[tuple(int, numpy.ndarray)]\n\n        """"""\n        time.sleep(delay)\n        while self.pending_request:\n            yield self._recv_ndarray()\n\n    def fetch_all(self, sort=True, concat=False):\n        """""" Fetch all encoded vectors from server, use it with `encode(blocking=False)`\n\n        Use it `encode(texts, blocking=False)`. If there is no pending requests, it will return None.\n\n        :type sort: bool\n        :type concat: bool\n        :param sort: sort results by their request ids. It should be True if you want to preserve the sending order\n        :param concat: concatenate all results into one ndarray\n        :return: encoded sentence/token-level embeddings in sending order\n        :rtype: numpy.ndarray or list[list[float]]\n\n        """"""\n        if self.pending_request:\n            tmp = list(self.fetch())\n            if sort:\n                tmp = sorted(tmp, key=lambda v: v.id)\n            tmp = [v.embedding for v in tmp]\n            if concat:\n                if self.output_fmt == \'ndarray\':\n                    tmp = np.concatenate(tmp, axis=0)\n                elif self.output_fmt == \'list\':\n                    tmp = [vv for v in tmp for vv in v]\n            return tmp\n\n    def encode_async(self, batch_generator, max_num_batch=None, delay=0.1, **kwargs):\n        """""" Async encode batches from a generator\n\n        :param delay: delay in seconds and then run fetcher\n        :param batch_generator: a generator that yields list[str] or list[list[str]] (for `is_tokenized=True`) every time\n        :param max_num_batch: stop after encoding this number of batches\n        :param `**kwargs`: the rest parameters please refer to `encode()`\n        :return: a generator that yields encoded vectors in ndarray, where the request id can be used to determine the order\n        :rtype: Iterator[tuple(int, numpy.ndarray)]\n\n        """"""\n\n        def run():\n            cnt = 0\n            for texts in batch_generator:\n                self.encode(texts, blocking=False, **kwargs)\n                cnt += 1\n                if max_num_batch and cnt == max_num_batch:\n                    break\n\n        t = threading.Thread(target=run)\n        t.start()\n        return self.fetch(delay)\n\n    @staticmethod\n    def _check_length(texts, len_limit, tokenized):\n        if tokenized:\n            # texts is already tokenized as list of str\n            return all(len(t) <= len_limit for t in texts)\n        else:\n            # do a simple whitespace tokenizer\n            return all(len(t.split()) <= len_limit for t in texts)\n\n    @staticmethod\n    def _check_input_lst_str(texts):\n        if not isinstance(texts, list):\n            raise TypeError(\'""%s"" must be %s, but received %s\' % (texts, type([]), type(texts)))\n        if not len(texts):\n            raise ValueError(\n                \'""%s"" must be a non-empty list, but received %s with %d elements\' % (texts, type(texts), len(texts)))\n        for idx, s in enumerate(texts):\n            if not isinstance(s, _str):\n                raise TypeError(\'all elements in the list must be %s, but element %d is %s\' % (type(\'\'), idx, type(s)))\n            if not s.strip():\n                raise ValueError(\n                    \'all elements in the list must be non-empty string, but element %d is %s\' % (idx, repr(s)))\n            if _py2:\n                texts[idx] = _unicode(texts[idx])\n\n    @staticmethod\n    def _check_input_lst_lst_str(texts):\n        if not isinstance(texts, list):\n            raise TypeError(\'""texts"" must be %s, but received %s\' % (type([]), type(texts)))\n        if not len(texts):\n            raise ValueError(\n                \'""texts"" must be a non-empty list, but received %s with %d elements\' % (type(texts), len(texts)))\n        for s in texts:\n            BertClient._check_input_lst_str(s)\n\n    @staticmethod\n    def _print_dict(x, title=None):\n        if title:\n            print(title)\n        for k, v in x.items():\n            print(\'%30s\\t=\\t%-30s\' % (k, v))\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n\nclass BCManager():\n    def __init__(self, available_bc):\n        self.available_bc = available_bc\n        self.bc = None\n\n    def __enter__(self):\n        self.bc = self.available_bc.pop()\n        return self.bc\n\n    def __exit__(self, *args):\n        self.available_bc.append(self.bc)\n\n\nclass ConcurrentBertClient(BertClient):\n    def __init__(self, max_concurrency=10, **kwargs):\n        """""" A thread-safe client object connected to a BertServer\n\n        Create a BertClient that connects to a BertServer.\n        Note, server must be ready at the moment you are calling this function.\n        If you are not sure whether the server is ready, then please set `check_version=False` and `check_length=False`\n\n        :type max_concurrency: int\n        :param max_concurrency: the maximum number of concurrent connections allowed\n\n        """"""\n        try:\n            from bert_serving.client import BertClient\n        except ImportError:\n            raise ImportError(\'BertClient module is not available, it is required for serving HTTP requests.\'\n                              \'Please use ""pip install -U bert-serving-client"" to install it.\'\n                              \'If you do not want to use it as an HTTP server, \'\n                              \'then remove ""-http_port"" from the command line.\')\n\n        self.available_bc = [BertClient(**kwargs) for _ in range(max_concurrency)]\n        self.max_concurrency = max_concurrency\n\n    def close(self):\n        for bc in self.available_bc:\n            bc.close()\n\n    def _concurrent(func):\n        @wraps(func)\n        def arg_wrapper(self, *args, **kwargs):\n            try:\n                with BCManager(self.available_bc) as bc:\n                    f = getattr(bc, func.__name__)\n                    r = f if isinstance(f, dict) else f(*args, **kwargs)\n                return r\n            except IndexError:\n                raise RuntimeError(\'Too many concurrent connections!\'\n                                   \'Try to increase the value of ""max_concurrency"", \'\n                                   \'currently =%d\' % self.max_concurrency)\n\n        return arg_wrapper\n\n    @_concurrent\n    def encode(self, **kwargs):\n        pass\n\n    @property\n    @_concurrent\n    def server_config(self):\n        pass\n\n    @property\n    @_concurrent\n    def server_status(self):\n        pass\n\n    @property\n    @_concurrent\n    def status(self):\n        pass\n\n    def fetch(self, **kwargs):\n        raise NotImplementedError(\'Async encoding of ""ConcurrentBertClient"" is not implemented yet\')\n\n    def fetch_all(self, **kwargs):\n        raise NotImplementedError(\'Async encoding of ""ConcurrentBertClient"" is not implemented yet\')\n\n    def encode_async(self, **kwargs):\n        raise NotImplementedError(\'Async encoding of ""ConcurrentBertClient"" is not implemented yet\')\n'"
client/bert_serving/client/_py2_var.py,0,"b""__all__ = ['_py2', '_str', '_buffer', '_unicode']\n\n_py2 = True\n_str = basestring\n_buffer = buffer\n\n\ndef _unicode(x):\n    return x if isinstance(x, unicode) else x.decode('utf-8')\n"""
client/bert_serving/client/_py3_var.py,0,"b""__all__ = ['_py2', '_str', '_buffer', '_raise']\n\n_py2 = False\n_str = str\n_buffer = memoryview\n\n\ndef _raise(t_e, _e):\n    raise t_e from _e\n"""
server/bert_serving/server/__init__.py,10,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Han Xiao <artex.xh@gmail.com> <https://hanxiao.github.io>\nimport multiprocessing\nimport os\nimport random\nimport sys\nimport threading\nimport time\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom itertools import chain\nfrom multiprocessing import Process\nfrom multiprocessing.pool import Pool\n\nimport numpy as np\nimport zmq\nimport zmq.decorators as zmqd\nfrom termcolor import colored\nfrom zmq.utils import jsonapi\n\nfrom .helper import *\nfrom .http import BertHTTPProxy\nfrom .zmq_decor import multi_socket\n\n__all__ = [\'__version__\', \'BertServer\']\n__version__ = \'1.10.0\'\n\n_tf_ver_ = check_tf_version()\n\n\nclass ServerCmd:\n    terminate = b\'TERMINATION\'\n    show_config = b\'SHOW_CONFIG\'\n    show_status = b\'SHOW_STATUS\'\n    new_job = b\'REGISTER\'\n    data_token = b\'TOKENS\'\n    data_embed = b\'EMBEDDINGS\'\n\n    @staticmethod\n    def is_valid(cmd):\n        return any(not k.startswith(\'__\') and v == cmd for k, v in vars(ServerCmd).items())\n\n\nclass BertServer(threading.Thread):\n    def __init__(self, args):\n        super().__init__()\n        self.logger = set_logger(colored(\'VENTILATOR\', \'magenta\'), args.verbose)\n\n        self.model_dir = args.model_dir\n        self.max_seq_len = args.max_seq_len\n        self.num_worker = args.num_worker\n        self.max_batch_size = args.max_batch_size\n        self.num_concurrent_socket = max(8, args.num_worker * 2)  # optimize concurrency for multi-clients\n        self.port = args.port\n        self.args = args\n        self.status_args = {k: (v if k != \'pooling_strategy\' else v.value) for k, v in sorted(vars(args).items())}\n        self.status_static = {\n            \'tensorflow_version\': _tf_ver_,\n            \'python_version\': sys.version,\n            \'server_version\': __version__,\n            \'pyzmq_version\': zmq.pyzmq_version(),\n            \'zmq_version\': zmq.zmq_version(),\n            \'server_start_time\': str(datetime.now()),\n        }\n        self.processes = []\n        self.logger.info(\'freeze, optimize and export graph, could take a while...\')\n        with Pool(processes=1) as pool:\n            # optimize the graph, must be done in another process\n            from .graph import optimize_graph\n            self.graph_path, self.bert_config = pool.apply(optimize_graph, (self.args,))\n        # from .graph import optimize_graph\n        # self.graph_path = optimize_graph(self.args, self.logger)\n        if self.graph_path:\n            self.logger.info(\'optimized graph is stored at: %s\' % self.graph_path)\n        else:\n            raise FileNotFoundError(\'graph optimization fails and returns empty result\')\n        self.is_ready = threading.Event()\n\n    def __enter__(self):\n        self.start()\n        self.is_ready.wait()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n        self.logger.info(\'shutting down...\')\n        self._send_close_signal()\n        self.is_ready.clear()\n        self.join()\n\n    @zmqd.context()\n    @zmqd.socket(zmq.PUSH)\n    def _send_close_signal(self, _, frontend):\n        frontend.connect(\'tcp://localhost:%d\' % self.port)\n        frontend.send_multipart([b\'0\', ServerCmd.terminate, b\'0\', b\'0\'])\n\n    @staticmethod\n    def shutdown(args):\n        with zmq.Context() as ctx:\n            ctx.setsockopt(zmq.LINGER, args.timeout)\n            with ctx.socket(zmq.PUSH) as frontend:\n                try:\n                    frontend.connect(\'tcp://%s:%d\' % (args.ip, args.port))\n                    frontend.send_multipart([b\'0\', ServerCmd.terminate, b\'0\', b\'0\'])\n                    print(\'shutdown signal sent to %d\' % args.port)\n                except zmq.error.Again:\n                    raise TimeoutError(\n                        \'no response from the server (with ""timeout""=%d ms), please check the following:\'\n                        \'is the server still online? is the network broken? are ""port"" correct? \' % args.timeout)\n\n    def run(self):\n        self._run()\n\n    @zmqd.context()\n    @zmqd.socket(zmq.PULL)\n    @zmqd.socket(zmq.PAIR)\n    @multi_socket(zmq.PUSH, num_socket=\'num_concurrent_socket\')\n    def _run(self, _, frontend, sink, *backend_socks):\n\n        def push_new_job(_job_id, _json_msg, _msg_len):\n            # backend_socks[0] is always at the highest priority\n            _sock = backend_socks[0] if _msg_len <= self.args.priority_batch_size else rand_backend_socket\n            _sock.send_multipart([_job_id, _json_msg])\n\n        # bind all sockets\n        self.logger.info(\'bind all sockets\')\n        frontend.bind(\'tcp://*:%d\' % self.port)\n        addr_front2sink = auto_bind(sink)\n        addr_backend_list = [auto_bind(b) for b in backend_socks]\n        self.logger.info(\'open %d ventilator-worker sockets\' % len(addr_backend_list))\n\n        # start the sink process\n        self.logger.info(\'start the sink\')\n        proc_sink = BertSink(self.args, addr_front2sink, self.bert_config)\n        self.processes.append(proc_sink)\n        proc_sink.start()\n        addr_sink = sink.recv().decode(\'ascii\')\n\n        # start the backend processes\n        device_map = self._get_device_map()\n        for idx, device_id in enumerate(device_map):\n            process = BertWorker(idx, self.args, addr_backend_list, addr_sink, device_id,\n                                 self.graph_path, self.bert_config)\n            self.processes.append(process)\n            process.start()\n\n        # start the http-service process\n        if self.args.http_port:\n            self.logger.info(\'start http proxy\')\n            proc_proxy = BertHTTPProxy(self.args)\n            self.processes.append(proc_proxy)\n            proc_proxy.start()\n\n        rand_backend_socket = None\n        server_status = ServerStatistic()\n\n        for p in self.processes:\n            p.is_ready.wait()\n\n        self.is_ready.set()\n        self.logger.info(\'all set, ready to serve request!\')\n\n        while True:\n            try:\n                request = frontend.recv_multipart()\n                client, msg, req_id, msg_len = request\n                assert req_id.isdigit()\n                assert msg_len.isdigit()\n            except (ValueError, AssertionError):\n                self.logger.error(\'received a wrongly-formatted request (expected 4 frames, got %d)\' % len(request))\n                self.logger.error(\'\\n\'.join(\'field %d: %s\' % (idx, k) for idx, k in enumerate(request)), exc_info=True)\n            else:\n                server_status.update(request)\n                if msg == ServerCmd.terminate:\n                    break\n                elif msg == ServerCmd.show_config or msg == ServerCmd.show_status:\n                    self.logger.info(\'new config request\\treq id: %d\\tclient: %s\' % (int(req_id), client))\n                    status_runtime = {\'client\': client.decode(\'ascii\'),\n                                      \'num_process\': len(self.processes),\n                                      \'ventilator -> worker\': addr_backend_list,\n                                      \'worker -> sink\': addr_sink,\n                                      \'ventilator <-> sink\': addr_front2sink,\n                                      \'server_current_time\': str(datetime.now()),\n                                      \'device_map\': device_map,\n                                      \'num_concurrent_socket\': self.num_concurrent_socket}\n                    if msg == ServerCmd.show_status:\n                        status_runtime[\'statistic\'] = server_status.value\n                    sink.send_multipart([client, msg, jsonapi.dumps({**status_runtime,\n                                                                     **self.status_args,\n                                                                     **self.status_static}), req_id])\n                else:\n                    self.logger.info(\'new encode request\\treq id: %d\\tsize: %d\\tclient: %s\' %\n                                     (int(req_id), int(msg_len), client))\n                    # register a new job at sink\n                    sink.send_multipart([client, ServerCmd.new_job, msg_len, req_id])\n\n                    # renew the backend socket to prevent large job queueing up\n                    # [0] is reserved for high priority job\n                    # last used backennd shouldn\'t be selected either as it may be queued up already\n                    rand_backend_socket = random.choice([b for b in backend_socks[1:] if b != rand_backend_socket])\n\n                    # push a new job, note super large job will be pushed to one socket only,\n                    # leaving other sockets free\n                    job_id = client + b\'#\' + req_id\n                    if int(msg_len) > self.max_batch_size:\n                        seqs = jsonapi.loads(msg)\n                        job_gen = ((job_id + b\'@%d\' % i, seqs[i:(i + self.max_batch_size)]) for i in\n                                   range(0, int(msg_len), self.max_batch_size))\n                        for partial_job_id, job in job_gen:\n                            push_new_job(partial_job_id, jsonapi.dumps(job), len(job))\n                    else:\n                        push_new_job(job_id, msg, int(msg_len))\n\n        for p in self.processes:\n            p.close()\n        self.logger.info(\'terminated!\')\n\n    def _get_device_map(self):\n        self.logger.info(\'get devices\')\n        run_on_gpu = False\n        device_map = [-1] * self.num_worker\n        if not self.args.cpu:\n            try:\n                import GPUtil\n                num_all_gpu = len(GPUtil.getGPUs())\n                avail_gpu = GPUtil.getAvailable(order=\'memory\', limit=min(num_all_gpu, self.num_worker),\n                                                maxMemory=0.9, maxLoad=0.9)\n                num_avail_gpu = len(avail_gpu)\n\n                if num_avail_gpu >= self.num_worker:\n                    run_on_gpu = True\n                elif 0 < num_avail_gpu < self.num_worker:\n                    self.logger.warning(\'only %d out of %d GPU(s) is available/free, but ""-num_worker=%d""\' %\n                                        (num_avail_gpu, num_all_gpu, self.num_worker))\n                    if not self.args.device_map:\n                        self.logger.warning(\'multiple workers will be allocated to one GPU, \'\n                                            \'may not scale well and may raise out-of-memory\')\n                    else:\n                        self.logger.warning(\'workers will be allocated based on ""-device_map=%s"", \'\n                                            \'may not scale well and may raise out-of-memory\' % self.args.device_map)\n                    run_on_gpu = True\n                else:\n                    self.logger.warning(\'no GPU available, fall back to CPU\')\n\n                if run_on_gpu:\n                    device_map = ((self.args.device_map or avail_gpu) * self.num_worker)[: self.num_worker]\n            except FileNotFoundError:\n                self.logger.warning(\'nvidia-smi is missing, often means no gpu on this machine. \'\n                                    \'fall back to cpu!\')\n        self.logger.info(\'device map: \\n\\t\\t%s\' % \'\\n\\t\\t\'.join(\n            \'worker %2d -> %s\' % (w_id, (\'gpu %2d\' % g_id) if g_id >= 0 else \'cpu\') for w_id, g_id in\n            enumerate(device_map)))\n        return device_map\n\n\nclass BertSink(Process):\n    def __init__(self, args, front_sink_addr, bert_config):\n        super().__init__()\n        self.port = args.port_out\n        self.exit_flag = multiprocessing.Event()\n        self.logger = set_logger(colored(\'SINK\', \'green\'), args.verbose)\n        self.front_sink_addr = front_sink_addr\n        self.verbose = args.verbose\n        self.show_tokens_to_client = args.show_tokens_to_client\n        self.max_seq_len = args.max_seq_len\n        self.max_position_embeddings = bert_config.max_position_embeddings\n        self.fixed_embed_length = args.fixed_embed_length\n        self.is_ready = multiprocessing.Event()\n\n    def close(self):\n        self.logger.info(\'shutting down...\')\n        self.is_ready.clear()\n        self.exit_flag.set()\n        self.terminate()\n        self.join()\n        self.logger.info(\'terminated!\')\n\n    def run(self):\n        self._run()\n\n    @zmqd.socket(zmq.PULL)\n    @zmqd.socket(zmq.PAIR)\n    @zmqd.socket(zmq.PUB)\n    def _run(self, receiver, frontend, sender):\n        receiver_addr = auto_bind(receiver)\n        frontend.connect(self.front_sink_addr)\n        sender.bind(\'tcp://*:%d\' % self.port)\n\n        pending_jobs = defaultdict(lambda: SinkJob(self.max_seq_len, self.max_position_embeddings,\n                                                   self.show_tokens_to_client,\n                                                   self.fixed_embed_length))  # type: Dict[str, SinkJob]\n\n        poller = zmq.Poller()\n        poller.register(frontend, zmq.POLLIN)\n        poller.register(receiver, zmq.POLLIN)\n\n        # send worker receiver address back to frontend\n        frontend.send(receiver_addr.encode(\'ascii\'))\n\n        # Windows does not support logger in MP environment, thus get a new logger\n        # inside the process for better compability\n        logger = set_logger(colored(\'SINK\', \'green\'), self.verbose)\n        logger.info(\'ready\')\n        self.is_ready.set()\n\n        while not self.exit_flag.is_set():\n            socks = dict(poller.poll())\n            if socks.get(receiver) == zmq.POLLIN:\n                msg = receiver.recv_multipart()\n                job_id = msg[0]\n                # parsing job_id and partial_id\n                job_info = job_id.split(b\'@\')\n                job_id = job_info[0]\n                partial_id = int(job_info[1]) if len(job_info) == 2 else 0\n\n                if msg[3] == ServerCmd.data_embed:\n                    # parsing the ndarray\n                    arr_info, arr_val = jsonapi.loads(msg[1]), msg[2]\n                    x = np.frombuffer(memoryview(arr_val), dtype=arr_info[\'dtype\']).reshape(arr_info[\'shape\'])\n                    pending_jobs[job_id].add_embed(x, partial_id)\n                elif msg[3] == ServerCmd.data_token:\n                    x = jsonapi.loads(msg[1])\n                    pending_jobs[job_id].add_token(x, partial_id)\n                else:\n                    logger.error(\'received a wrongly-formatted request (expected 4 frames, got %d)\' % len(msg))\n                    logger.error(\'\\n\'.join(\'field %d: %s\' % (idx, k) for idx, k in enumerate(msg)), exc_info=True)\n\n                logger.info(\'collect %s %s (E:%d/T:%d/A:%d)\' % (msg[3], job_id,\n                                                                pending_jobs[job_id].progress_embeds,\n                                                                pending_jobs[job_id].progress_tokens,\n                                                                pending_jobs[job_id].checksum))\n\n            if socks.get(frontend) == zmq.POLLIN:\n                client_addr, msg_type, msg_info, req_id = frontend.recv_multipart()\n                if msg_type == ServerCmd.new_job:\n                    job_info = client_addr + b\'#\' + req_id\n                    # register a new job\n                    pending_jobs[job_info].checksum = int(msg_info)\n                    logger.info(\'job register\\tsize: %d\\tjob id: %s\' % (int(msg_info), job_info))\n                    if len(pending_jobs[job_info]._pending_embeds)>0 \\\n                            and pending_jobs[job_info].final_ndarray is None:\n                        pending_jobs[job_info].add_embed(None, 0)\n                elif msg_type == ServerCmd.show_config or msg_type == ServerCmd.show_status:\n                    time.sleep(0.1)  # dirty fix of slow-joiner: sleep so that client receiver can connect.\n                    logger.info(\'send config\\tclient %s\' % client_addr)\n                    sender.send_multipart([client_addr, msg_info, req_id])\n\n            # check if there are finished jobs, then send it back to workers\n            finished = [(k, v) for k, v in pending_jobs.items() if v.is_done]\n            for job_info, tmp in finished:\n                client_addr, req_id = job_info.split(b\'#\')\n                x, x_info = tmp.result\n                sender.send_multipart([client_addr, x_info, x, req_id])\n                logger.info(\'send back\\tsize: %d\\tjob id: %s\' % (tmp.checksum, job_info))\n                # release the job\n                tmp.clear()\n                pending_jobs.pop(job_info)\n\n\nclass SinkJob:\n    def __init__(self, max_seq_len, max_position_embeddings, with_tokens, fixed_embed_length):\n        self._pending_embeds = []\n        self.tokens = []\n        self.tokens_ids = []\n        self.checksum = 0\n        self.final_ndarray = None\n        self.progress_tokens = 0\n        self.progress_embeds = 0\n        self.with_tokens = with_tokens\n        self.max_seq_len_unset = max_seq_len is None\n        self.max_position_embeddings = max_position_embeddings\n        self.max_effective_len = 0\n        self.fixed_embed_length = fixed_embed_length\n\n    def clear(self):\n        self._pending_embeds.clear()\n        self.tokens_ids.clear()\n        self.tokens.clear()\n        del self.final_ndarray\n\n    def _insert(self, data, pid, data_lst, idx_lst):\n        lo = 0\n        hi = len(idx_lst)\n        while lo < hi:\n            mid = (lo + hi) // 2\n            if pid < idx_lst[mid]:\n                hi = mid\n            else:\n                lo = mid + 1\n        idx_lst.insert(lo, pid)\n        data_lst.insert(lo, data)\n\n    def add_embed(self, data, pid):\n        def fill_data():\n            self.final_ndarray[pid: (pid + data.shape[0]), 0:data.shape[1]] = data\n            self.progress_embeds += progress\n            if data.shape[1] > self.max_effective_len:\n                self.max_effective_len = data.shape[1]\n        if data is not None: # when job finish msg come to SINK earlier than job register\n            progress = data.shape[0]\n        else:\n            progress = 0\n        if not self.checksum:\n            self._pending_embeds.append((data, pid, progress))\n        else:\n            if self.final_ndarray is None:\n                if data is not None: # when job finish msg come to SINK earlier than job register\n                    d_shape = list(data.shape[1:])\n                else:\n                    d_shape = list(self._pending_embeds[0][0].shape[1:])\n                if self.max_seq_len_unset and len(d_shape) > 1:\n                    # if not set max_seq_len, then we have no choice but set result ndarray to\n                    # [B, max_position_embeddings, dim] and truncate it at the end\n                    d_shape[0] = self.max_position_embeddings\n                if data is not None:\n                    dtype = data.dtype\n                else:\n                    dtype = self._pending_embeds[0][0].dtype\n                self.final_ndarray = np.zeros([self.checksum] + d_shape, dtype=dtype)\n            if data is not None: # when job finish msg come to SINK earlier than job register\n                fill_data()\n            while self._pending_embeds:\n                data, pid, progress = self._pending_embeds.pop()\n                fill_data()\n\n    def add_token(self, data, pid):\n        progress = len(data)\n        self._insert(data, pid, self.tokens, self.tokens_ids)\n        self.progress_tokens += progress\n\n    @property\n    def is_done(self):\n        if self.with_tokens:\n            return self.checksum > 0 and self.checksum == self.progress_tokens and self.checksum == self.progress_embeds\n        else:\n            return self.checksum > 0 and self.checksum == self.progress_embeds\n\n    @property\n    def result(self):\n        if self.max_seq_len_unset and not self.fixed_embed_length:\n            x = np.ascontiguousarray(self.final_ndarray[:, 0:self.max_effective_len])\n        else:\n            x = self.final_ndarray\n        x_info = {\'dtype\': str(x.dtype),\n                  \'shape\': x.shape,\n                  \'tokens\': list(chain.from_iterable(self.tokens)) if self.with_tokens else \'\'}\n\n        x_info = jsonapi.dumps(x_info)\n        return x, x_info\n\n\nclass BertWorker(Process):\n    def __init__(self, id, args, worker_address_list, sink_address, device_id, graph_path, graph_config):\n        super().__init__()\n        self.worker_id = id\n        self.device_id = device_id\n        self.logger = set_logger(colored(\'WORKER-%d\' % self.worker_id, \'yellow\'), args.verbose)\n        self.max_seq_len = args.max_seq_len\n        self.do_lower_case = args.do_lower_case\n        self.mask_cls_sep = args.mask_cls_sep\n        self.daemon = True\n        self.exit_flag = multiprocessing.Event()\n        self.worker_address = worker_address_list\n        self.num_concurrent_socket = len(self.worker_address)\n        self.sink_address = sink_address\n        self.prefetch_size = args.prefetch_size if self.device_id > 0 else None  # set to zero for CPU-worker\n        self.gpu_memory_fraction = args.gpu_memory_fraction\n        self.model_dir = args.model_dir\n        self.verbose = args.verbose\n        self.graph_path = graph_path\n        self.bert_config = graph_config\n        self.use_fp16 = args.fp16\n        self.show_tokens_to_client = args.show_tokens_to_client\n        self.no_special_token = args.no_special_token\n        self.is_ready = multiprocessing.Event()\n\n    def close(self):\n        self.logger.info(\'shutting down...\')\n        self.exit_flag.set()\n        self.is_ready.clear()\n        self.terminate()\n        self.join()\n        self.logger.info(\'terminated!\')\n\n    def get_estimator(self, tf):\n        from tensorflow.python.estimator.estimator import Estimator\n        from tensorflow.python.estimator.run_config import RunConfig\n        from tensorflow.python.estimator.model_fn import EstimatorSpec\n\n        def model_fn(features, labels, mode, params):\n            with tf.gfile.GFile(self.graph_path, \'rb\') as f:\n                graph_def = tf.GraphDef()\n                graph_def.ParseFromString(f.read())\n\n            input_names = [\'input_ids\', \'input_mask\', \'input_type_ids\']\n\n            output = tf.import_graph_def(graph_def,\n                                         input_map={k + \':0\': features[k] for k in input_names},\n                                         return_elements=[\'final_encodes:0\'])\n\n            return EstimatorSpec(mode=mode, predictions={\n                \'client_id\': features[\'client_id\'],\n                \'encodes\': output[0]\n            })\n\n        config = tf.ConfigProto(device_count={\'GPU\': 0 if self.device_id < 0 else 1})\n        config.gpu_options.allow_growth = True\n        config.gpu_options.per_process_gpu_memory_fraction = self.gpu_memory_fraction\n        config.log_device_placement = False\n        # session-wise XLA doesn\'t seem to work on tf 1.10\n        # if args.xla:\n        #     config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n\n        return Estimator(model_fn=model_fn, config=RunConfig(session_config=config))\n\n    def run(self):\n        self._run()\n\n    @zmqd.socket(zmq.PUSH)\n    @zmqd.socket(zmq.PUSH)\n    @multi_socket(zmq.PULL, num_socket=\'num_concurrent_socket\')\n    def _run(self, sink_embed, sink_token, *receivers):\n        # Windows does not support logger in MP environment, thus get a new logger\n        # inside the process for better compatibility\n        logger = set_logger(colored(\'WORKER-%d\' % self.worker_id, \'yellow\'), self.verbose)\n\n        logger.info(\'use device %s, load graph from %s\' %\n                    (\'cpu\' if self.device_id < 0 else (\'gpu: %d\' % self.device_id), self.graph_path))\n\n        tf = import_tf(self.device_id, self.verbose, use_fp16=self.use_fp16)\n        estimator = self.get_estimator(tf)\n\n        for sock, addr in zip(receivers, self.worker_address):\n            sock.connect(addr)\n\n        sink_embed.connect(self.sink_address)\n        sink_token.connect(self.sink_address)\n        for r in estimator.predict(self.input_fn_builder(receivers, tf, sink_token), yield_single_examples=False):\n            send_ndarray(sink_embed, r[\'client_id\'], r[\'encodes\'], ServerCmd.data_embed)\n            logger.info(\'job done\\tsize: %s\\tclient: %s\' % (r[\'encodes\'].shape, r[\'client_id\']))\n\n    def input_fn_builder(self, socks, tf, sink):\n        from .bert.extract_features import convert_lst_to_features\n        from .bert.tokenization import FullTokenizer\n\n        def gen():\n            # Windows does not support logger in MP environment, thus get a new logger\n            # inside the process for better compatibility\n            logger = set_logger(colored(\'WORKER-%d\' % self.worker_id, \'yellow\'), self.verbose)\n            tokenizer = FullTokenizer(vocab_file=os.path.join(self.model_dir, \'vocab.txt\'), do_lower_case=self.do_lower_case)\n\n            poller = zmq.Poller()\n            for sock in socks:\n                poller.register(sock, zmq.POLLIN)\n\n            logger.info(\'ready and listening!\')\n            self.is_ready.set()\n\n            while not self.exit_flag.is_set():\n                events = dict(poller.poll())\n                for sock_idx, sock in enumerate(socks):\n                    if sock in events:\n                        client_id, raw_msg = sock.recv_multipart()\n                        msg = jsonapi.loads(raw_msg)\n                        logger.info(\'new job\\tsocket: %d\\tsize: %d\\tclient: %s\' % (sock_idx, len(msg), client_id))\n                        # check if msg is a list of list, if yes consider the input is already tokenized\n                        is_tokenized = all(isinstance(el, list) for el in msg)\n                        tmp_f = list(convert_lst_to_features(msg, self.max_seq_len,\n                                                             self.bert_config.max_position_embeddings,\n                                                             tokenizer, logger,\n                                                             is_tokenized, self.mask_cls_sep, self.no_special_token))\n                        if self.show_tokens_to_client:\n                            sink.send_multipart([client_id, jsonapi.dumps([f.tokens for f in tmp_f]),\n                                                 b\'\', ServerCmd.data_token])\n                        yield {\n                            \'client_id\': client_id,\n                            \'input_ids\': [f.input_ids for f in tmp_f],\n                            \'input_mask\': [f.input_mask for f in tmp_f],\n                            \'input_type_ids\': [f.input_type_ids for f in tmp_f]\n                        }\n\n        def input_fn():\n            return (tf.data.Dataset.from_generator(\n                gen,\n                output_types={\'input_ids\': tf.int32,\n                              \'input_mask\': tf.int32,\n                              \'input_type_ids\': tf.int32,\n                              \'client_id\': tf.string},\n                output_shapes={\n                    \'client_id\': (),\n                    \'input_ids\': (None, None),\n                    \'input_mask\': (None, None),\n                    \'input_type_ids\': (None, None)}).prefetch(self.prefetch_size))\n\n        return input_fn\n\n\nclass ServerStatistic:\n    def __init__(self):\n        self._hist_client = CappedHistogram(500)\n        self._hist_msg_len = defaultdict(int)\n        self._client_last_active_time = CappedHistogram(500)\n        self._num_data_req = 0\n        self._num_sys_req = 0\n        self._num_total_seq = 0\n        self._last_req_time = time.perf_counter()\n        self._last_two_req_interval = []\n        self._num_last_two_req = 200\n\n    def update(self, request):\n        client, msg, req_id, msg_len = request\n        self._hist_client[client] += 1\n        if ServerCmd.is_valid(msg):\n            self._num_sys_req += 1\n            # do not count for system request, as they are mainly for heartbeats\n        else:\n            self._hist_msg_len[int(msg_len)] += 1\n            self._num_total_seq += int(msg_len)\n            self._num_data_req += 1\n            tmp = time.perf_counter()\n            self._client_last_active_time[client] = tmp\n            if len(self._last_two_req_interval) < self._num_last_two_req:\n                self._last_two_req_interval.append(tmp - self._last_req_time)\n            else:\n                self._last_two_req_interval.pop(0)\n            self._last_req_time = tmp\n\n    @property\n    def value(self):\n        def get_min_max_avg(name, stat, avg=None):\n            if len(stat) > 0:\n                avg = sum(stat) / len(stat) if avg is None else avg\n                min_, max_ = min(stat), max(stat)\n                return {\n                    \'avg_%s\' % name: avg,\n                    \'min_%s\' % name: min_,\n                    \'max_%s\' % name: max_,\n                    \'num_min_%s\' % name: sum(v == min_ for v in stat),\n                    \'num_max_%s\' % name: sum(v == max_ for v in stat),\n                }\n            else:\n                return {}\n\n        def get_num_active_client(interval=180):\n            # we count a client active when its last request is within 3 min.\n            now = time.perf_counter()\n            return sum(1 for v in self._client_last_active_time.values() if (now - v) < interval)\n\n        avg_msg_len = None\n        if len(self._hist_msg_len) > 0:\n            avg_msg_len = sum(k*v for k,v in self._hist_msg_len.items()) / sum(self._hist_msg_len.values())\n\n        parts = [{\n            \'num_data_request\': self._num_data_req,\n            \'num_total_seq\': self._num_total_seq,\n            \'num_sys_request\': self._num_sys_req,\n            \'num_total_request\': self._num_data_req + self._num_sys_req,\n            \'num_total_client\': self._hist_client.total_size(),\n            \'num_active_client\': get_num_active_client()},\n            self._hist_client.get_stat_map(\'request_per_client\'),\n            get_min_max_avg(\'size_per_request\', self._hist_msg_len.keys(), avg=avg_msg_len),\n            get_min_max_avg(\'last_two_interval\', self._last_two_req_interval),\n            get_min_max_avg(\'request_per_second\', [1. / v for v in self._last_two_req_interval]),\n        ]\n\n        return {k: v for d in parts for k, v in d.items()}\n'"
server/bert_serving/server/benchmark.py,0,"b'import random\nimport threading\nimport time\n\nfrom numpy import mean\n\n\nclass BenchmarkClient(threading.Thread):\n    def __init__(self, cargs, vocab):\n        super().__init__()\n        self.batch = [\' \'.join(random.choices(vocab, k=cargs.max_seq_len)) for _ in range(cargs.client_batch_size)]\n        self.num_repeat = cargs.num_repeat\n        self.avg_time = 0\n        self.port = cargs.port\n        self.port_out = cargs.port_out\n\n    def run(self):\n        try:\n            from bert_serving.client import BertClient\n        except ImportError:\n            raise ImportError(\'BertClient module is not available, it is required for benchmarking.\'\n                              \'Please use ""pip install -U bert-serving-client"" to install it.\')\n        with BertClient(port=self.port, port_out=self.port_out,\n                        show_server_config=True, check_version=False, check_length=False) as bc:\n            time_all = []\n            for _ in range(self.num_repeat):\n                start_t = time.perf_counter()\n                bc.encode(self.batch)\n                time_all.append(time.perf_counter() - start_t)\n            self.avg_time = mean(time_all[2:])  # first one is often slow due to cold-start/warm-up effect\n\n\ndef run_benchmark(args):\n    from copy import deepcopy\n    from bert_serving.server import BertServer\n\n    # load vocabulary\n    with open(args.client_vocab_file, encoding=\'utf8\') as fp:\n        vocab = list(set(vv for v in fp for vv in v.strip().split()))\n    print(\'vocabulary size: %d\' % len(vocab))\n\n    # select those non-empty test cases\n    all_exp_names = [k.replace(\'test_\', \'\') for k, v in vars(args).items() if k.startswith(\'test_\') and v]\n\n    for exp_name in all_exp_names:\n        # set common args\n        cargs = deepcopy(args)\n        exp_vars = vars(args)[\'test_%s\' % exp_name]\n        avg_speed = []\n\n        for cvar in exp_vars:\n            # override exp args\n            setattr(cargs, exp_name, cvar)\n            server = BertServer(cargs)\n            server.start()\n            time.sleep(cargs.wait_till_ready)\n\n            # sleep until server is ready\n            all_clients = [BenchmarkClient(cargs, vocab) for _ in range(cargs.num_client)]\n            for bc in all_clients:\n                bc.start()\n\n            clients_speed = []\n            for bc in all_clients:\n                bc.join()\n                clients_speed.append(cargs.client_batch_size / bc.avg_time)\n            server.close()\n\n            max_speed, min_speed, cavg_speed = int(max(clients_speed)), int(min(clients_speed)), int(\n                mean(clients_speed))\n\n            print(\'avg speed: %d\\tmax speed: %d\\tmin speed: %d\' % (cavg_speed, max_speed, min_speed), flush=True)\n\n            avg_speed.append(cavg_speed)\n\n        with open(\'benchmark-%d%s.result\' % (args.num_worker, \'-fp16\' if args.fp16 else \'\'), \'a\') as fw:\n            print(\'\\n|`%s`\\t|samples/s|\\n|---|---|\' % exp_name, file=fw)\n            for cvar, cavg_speed in zip(exp_vars, avg_speed):\n                print(\'|%s\\t|%d|\' % (cvar, cavg_speed), file=fw)\n            # for additional plotting\n            print(\'\\n%s = %s\\n%s = %s\' % (exp_name, exp_vars, \'speed\', avg_speed), file=fw)\n'"
server/bert_serving/server/graph.py,42,"b'import contextlib\nimport json\nimport os\nimport tempfile\nfrom enum import Enum\n\nfrom termcolor import colored\n\nfrom .bert import modeling\nfrom .helper import import_tf, set_logger\n\n__all__ = [\'PoolingStrategy\', \'optimize_graph\']\n\n\nclass PoolingStrategy(Enum):\n    NONE = 0\n    REDUCE_MAX = 1\n    REDUCE_MEAN = 2\n    REDUCE_MEAN_MAX = 3\n    FIRST_TOKEN = 4  # corresponds to [CLS] for single sequences\n    LAST_TOKEN = 5  # corresponds to [SEP] for single sequences\n    CLS_TOKEN = 4  # corresponds to the first token for single seq.\n    SEP_TOKEN = 5  # corresponds to the last token for single seq.\n    CLS_POOLED = 6 # pooled [CLS] token for fine-tuned classification\n    CLASSIFICATION = 7 # get probabilities for classification problems.\n    REGRESSION = 8 # get probabilities for classification problems.\n\n    def __str__(self):\n        return self.name\n\n    @staticmethod\n    def from_string(s):\n        try:\n            return PoolingStrategy[s]\n        except KeyError:\n            raise ValueError()\n\n\ndef optimize_graph(args, logger=None):\n    if not logger:\n        logger = set_logger(colored(\'GRAPHOPT\', \'cyan\'), args.verbose)\n    try:\n        # we don\'t need GPU for optimizing the graph\n        tf = import_tf(verbose=args.verbose)\n        from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\n\n        config = tf.ConfigProto(device_count={\'GPU\': 0}, allow_soft_placement=True)\n\n        config_fp = os.path.join(args.model_dir, args.config_name)\n        init_checkpoint = os.path.join(args.tuned_model_dir or args.model_dir, args.ckpt_name)\n        if args.fp16:\n            logger.warning(\'fp16 is turned on! \'\n                           \'Note that not all CPU GPU support fast fp16 instructions, \'\n                           \'worst case you will have degraded performance!\')\n        logger.info(\'model config: %s\' % config_fp)\n        logger.info(\n            \'checkpoint%s: %s\' % (\n            \' (override by the fine-tuned model)\' if args.tuned_model_dir else \'\', init_checkpoint))\n        with tf.gfile.GFile(config_fp, \'r\') as f:\n            bert_config = modeling.BertConfig.from_dict(json.load(f))\n\n        logger.info(\'build graph...\')\n        # input placeholders, not sure if they are friendly to XLA\n        input_ids = tf.placeholder(tf.int32, (None, None), \'input_ids\')\n        input_mask = tf.placeholder(tf.int32, (None, None), \'input_mask\')\n        input_type_ids = tf.placeholder(tf.int32, (None, None), \'input_type_ids\')\n\n        jit_scope = tf.contrib.compiler.jit.experimental_jit_scope if args.xla else contextlib.suppress\n\n        with jit_scope():\n            input_tensors = [input_ids, input_mask, input_type_ids]\n\n            model = modeling.BertModel(\n                config=bert_config,\n                is_training=False,\n                input_ids=input_ids,\n                input_mask=input_mask,\n                token_type_ids=input_type_ids,\n                use_one_hot_embeddings=False,\n                use_position_embeddings=not args.no_position_embeddings)\n            \n            if args.pooling_strategy == PoolingStrategy.CLASSIFICATION:\n                hidden_size = model.pooled_output.shape[-1].value\n                output_weights = tf.get_variable(\n                    \'output_weights\', [args.num_labels, hidden_size],\n                    initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n                output_bias = tf.get_variable(\n                    \'output_bias\', [args.num_labels], initializer=tf.zeros_initializer())\n\n            if args.pooling_strategy == PoolingStrategy.REGRESSION:\n                hidden_size = model.pooled_output.shape[-1].value\n                output_weights = tf.get_variable(\n                    \'output_weights\', [1, hidden_size],\n                    initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n                output_bias = tf.get_variable(\n                    \'output_bias\', [1], initializer=tf.zeros_initializer())\n\n            tvars = tf.trainable_variables()\n\n            (assignment_map, initialized_variable_names\n             ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n\n            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n            minus_mask = lambda x, m: x - tf.expand_dims(1.0 - m, axis=-1) * 1e30\n            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n            masked_reduce_max = lambda x, m: tf.reduce_max(minus_mask(x, m), axis=1)\n            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n\n            with tf.variable_scope(""pooling""):\n                if len(args.pooling_layer) == 1:\n                    encoder_layer = model.all_encoder_layers[args.pooling_layer[0]]\n                else:\n                    all_layers = [model.all_encoder_layers[l] for l in args.pooling_layer]\n                    encoder_layer = tf.concat(all_layers, -1)\n\n                input_mask = tf.cast(input_mask, tf.float32)\n                if args.pooling_strategy == PoolingStrategy.REDUCE_MEAN:\n                    pooled = masked_reduce_mean(encoder_layer, input_mask)\n                elif args.pooling_strategy == PoolingStrategy.REDUCE_MAX:\n                    pooled = masked_reduce_max(encoder_layer, input_mask)\n                elif args.pooling_strategy == PoolingStrategy.REDUCE_MEAN_MAX:\n                    pooled = tf.concat([masked_reduce_mean(encoder_layer, input_mask),\n                                        masked_reduce_max(encoder_layer, input_mask)], axis=1)\n                elif args.pooling_strategy == PoolingStrategy.FIRST_TOKEN or \\\n                        args.pooling_strategy == PoolingStrategy.CLS_TOKEN:\n                    pooled = tf.squeeze(encoder_layer[:, 0:1, :], axis=1)\n                elif args.pooling_strategy == PoolingStrategy.CLS_POOLED:\n                    pooled = model.pooled_output\n                elif args.pooling_strategy == PoolingStrategy.LAST_TOKEN or \\\n                        args.pooling_strategy == PoolingStrategy.SEP_TOKEN:\n                    seq_len = tf.cast(tf.reduce_sum(input_mask, axis=1), tf.int32)\n                    rng = tf.range(0, tf.shape(seq_len)[0])\n                    indexes = tf.stack([rng, seq_len - 1], 1)\n                    pooled = tf.gather_nd(encoder_layer, indexes)\n                elif args.pooling_strategy == PoolingStrategy.NONE:\n                    pooled = mul_mask(encoder_layer, input_mask)\n                elif args.pooling_strategy == PoolingStrategy.CLASSIFICATION:\n                    pooled = model.pooled_output\n                    logits = tf.matmul(pooled, output_weights, transpose_b=True)\n                    logits = tf.nn.bias_add(logits, output_bias)\n                    pooled = tf.nn.softmax(logits, axis=-1)\n                elif args.pooling_strategy == PoolingStrategy.REGRESSION:\n                    pooled = model.pooled_output\n                    logits = tf.matmul(pooled, output_weights, transpose_b=True)\n                    logits = tf.nn.bias_add(logits, output_bias)\n                    pooled = tf.nn.sigmoid(logits)\n                else:\n                    raise NotImplementedError()\n\n            if args.fp16:\n                pooled = tf.cast(pooled, tf.float16)\n\n            pooled = tf.identity(pooled, \'final_encodes\')\n            output_tensors = [pooled]\n            tmp_g = tf.get_default_graph().as_graph_def()\n\n        with tf.Session(config=config) as sess:\n            logger.info(\'load parameters from checkpoint...\')\n\n            sess.run(tf.global_variables_initializer())\n            dtypes = [n.dtype for n in input_tensors]\n            logger.info(\'optimize...\')\n            tmp_g = optimize_for_inference(\n                tmp_g,\n                [n.name[:-2] for n in input_tensors],\n                [n.name[:-2] for n in output_tensors],\n                [dtype.as_datatype_enum for dtype in dtypes],\n                False)\n\n            logger.info(\'freeze...\')\n            tmp_g = convert_variables_to_constants(sess, tmp_g, [n.name[:-2] for n in output_tensors],\n                                                   use_fp16=args.fp16)\n\n        tmp_file = tempfile.NamedTemporaryFile(\'w\', delete=False, dir=args.graph_tmp_dir).name\n        logger.info(\'write graph to a tmp file: %s\' % tmp_file)\n        with tf.gfile.GFile(tmp_file, \'wb\') as f:\n            f.write(tmp_g.SerializeToString())\n        return tmp_file, bert_config\n    except Exception:\n        logger.error(\'fail to optimize the graph!\', exc_info=True)\n\n\ndef convert_variables_to_constants(sess,\n                                   input_graph_def,\n                                   output_node_names,\n                                   variable_names_whitelist=None,\n                                   variable_names_blacklist=None,\n                                   use_fp16=False):\n    from tensorflow.python.framework.graph_util_impl import extract_sub_graph\n    from tensorflow.core.framework import graph_pb2\n    from tensorflow.core.framework import node_def_pb2\n    from tensorflow.core.framework import attr_value_pb2\n    from tensorflow.core.framework import types_pb2\n    from tensorflow.python.framework import tensor_util\n\n    def patch_dtype(input_node, field_name, output_node):\n        if use_fp16 and (field_name in input_node.attr) and (input_node.attr[field_name].type == types_pb2.DT_FLOAT):\n            output_node.attr[field_name].CopyFrom(attr_value_pb2.AttrValue(type=types_pb2.DT_HALF))\n\n    inference_graph = extract_sub_graph(input_graph_def, output_node_names)\n\n    variable_names = []\n    variable_dict_names = []\n    for node in inference_graph.node:\n        if node.op in [""Variable"", ""VariableV2"", ""VarHandleOp""]:\n            variable_name = node.name\n            if ((variable_names_whitelist is not None and\n                 variable_name not in variable_names_whitelist) or\n                    (variable_names_blacklist is not None and\n                     variable_name in variable_names_blacklist)):\n                continue\n            variable_dict_names.append(variable_name)\n            if node.op == ""VarHandleOp"":\n                variable_names.append(variable_name + ""/Read/ReadVariableOp:0"")\n            else:\n                variable_names.append(variable_name + "":0"")\n    if variable_names:\n        returned_variables = sess.run(variable_names)\n    else:\n        returned_variables = []\n    found_variables = dict(zip(variable_dict_names, returned_variables))\n\n    output_graph_def = graph_pb2.GraphDef()\n    how_many_converted = 0\n    for input_node in inference_graph.node:\n        output_node = node_def_pb2.NodeDef()\n        if input_node.name in found_variables:\n            output_node.op = ""Const""\n            output_node.name = input_node.name\n            dtype = input_node.attr[""dtype""]\n            data = found_variables[input_node.name]\n\n            if use_fp16 and dtype.type == types_pb2.DT_FLOAT:\n                output_node.attr[""value""].CopyFrom(\n                    attr_value_pb2.AttrValue(\n                        tensor=tensor_util.make_tensor_proto(data.astype(\'float16\'),\n                                                             dtype=types_pb2.DT_HALF,\n                                                             shape=data.shape)))\n            else:\n                output_node.attr[""dtype""].CopyFrom(dtype)\n                output_node.attr[""value""].CopyFrom(attr_value_pb2.AttrValue(\n                    tensor=tensor_util.make_tensor_proto(data, dtype=dtype.type,\n                                                         shape=data.shape)))\n            how_many_converted += 1\n        elif input_node.op == ""ReadVariableOp"" and (input_node.input[0] in found_variables):\n            # placeholder nodes\n            # print(\'- %s | %s \' % (input_node.name, input_node.attr[""dtype""]))\n            output_node.op = ""Identity""\n            output_node.name = input_node.name\n            output_node.input.extend([input_node.input[0]])\n            output_node.attr[""T""].CopyFrom(input_node.attr[""dtype""])\n            if ""_class"" in input_node.attr:\n                output_node.attr[""_class""].CopyFrom(input_node.attr[""_class""])\n        else:\n            # mostly op nodes\n            output_node.CopyFrom(input_node)\n\n        patch_dtype(input_node, \'dtype\', output_node)\n        patch_dtype(input_node, \'T\', output_node)\n        patch_dtype(input_node, \'DstT\', output_node)\n        patch_dtype(input_node, \'SrcT\', output_node)\n        patch_dtype(input_node, \'Tparams\', output_node)\n\n        if use_fp16 and (\'value\' in output_node.attr) and (\n                output_node.attr[\'value\'].tensor.dtype == types_pb2.DT_FLOAT):\n            # hard-coded value need to be converted as well\n            output_node.attr[\'value\'].CopyFrom(attr_value_pb2.AttrValue(\n                tensor=tensor_util.make_tensor_proto(\n                    output_node.attr[\'value\'].tensor.float_val[0],\n                    dtype=types_pb2.DT_HALF)))\n\n        output_graph_def.node.extend([output_node])\n\n    output_graph_def.library.CopyFrom(inference_graph.library)\n    return output_graph_def\n'"
server/bert_serving/server/helper.py,3,"b'import argparse\nimport logging\nimport os\nimport sys\nimport time\nimport uuid\nimport warnings\nfrom collections import OrderedDict\n\n\nimport zmq\nfrom termcolor import colored\nfrom zmq.utils import jsonapi\n\n__all__ = [\'set_logger\', \'send_ndarray\', \'get_args_parser\',\n           \'check_tf_version\', \'auto_bind\', \'import_tf\', \'TimeContext\', \'CappedHistogram\']\n\n\ndef set_logger(context, verbose=False):\n    if os.name == \'nt\':  # for Windows\n        return NTLogger(context, verbose)\n\n    logger = logging.getLogger(context)\n    logger.setLevel(logging.DEBUG if verbose else logging.INFO)\n    formatter = logging.Formatter(\n        \'%(levelname)-.1s:\' + context + \':[%(filename).3s:%(funcName).3s:%(lineno)3d]:%(message)s\', datefmt=\n        \'%m-%d %H:%M:%S\')\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if verbose else logging.INFO)\n    console_handler.setFormatter(formatter)\n    logger.handlers = []\n    logger.addHandler(console_handler)\n    return logger\n\n\nclass NTLogger:\n    def __init__(self, context, verbose):\n        self.context = context\n        self.verbose = verbose\n\n    def info(self, msg, **kwargs):\n        print(\'I:%s:%s\' % (self.context, msg), flush=True)\n\n    def debug(self, msg, **kwargs):\n        if self.verbose:\n            print(\'D:%s:%s\' % (self.context, msg), flush=True)\n\n    def error(self, msg, **kwargs):\n        print(\'E:%s:%s\' % (self.context, msg), flush=True)\n\n    def warning(self, msg, **kwargs):\n        print(\'W:%s:%s\' % (self.context, msg), flush=True)\n\n\ndef send_ndarray(src, dest, X, req_id=b\'\', flags=0, copy=True, track=False):\n    """"""send a numpy array with metadata""""""\n    md = dict(dtype=str(X.dtype), shape=X.shape)\n    return src.send_multipart([dest, jsonapi.dumps(md), X, req_id], flags, copy=copy, track=track)\n\n\ndef check_max_seq_len(value):\n    if value is None or value.lower() == \'none\':\n        return None\n    try:\n        ivalue = int(value)\n        if ivalue <= 3:\n            raise argparse.ArgumentTypeError(""%s is an invalid int value must be >3 ""\n                                             ""(account for maximum three special symbols in BERT model) or NONE"" % value)\n    except TypeError:\n        raise argparse.ArgumentTypeError(""%s is an invalid int value"" % value)\n    return ivalue\n\n\ndef get_args_parser():\n    from . import __version__\n    from .graph import PoolingStrategy\n\n    parser = argparse.ArgumentParser(description=\'Start a BertServer for serving\')\n\n    group1 = parser.add_argument_group(\'File Paths\',\n                                       \'config the path, checkpoint and filename of a pretrained/fine-tuned BERT model\')\n    group1.add_argument(\'-model_dir\', type=str, required=True,\n                        help=\'directory of a pretrained BERT model\')\n    group1.add_argument(\'-tuned_model_dir\', type=str,\n                        help=\'directory of a fine-tuned BERT model\')\n    group1.add_argument(\'-ckpt_name\', type=str, default=\'bert_model.ckpt\',\n                        help=\'filename of the checkpoint file. By default it is ""bert_model.ckpt"", but \\\n                             for a fine-tuned model the name could be different.\')\n    group1.add_argument(\'-config_name\', type=str, default=\'bert_config.json\',\n                        help=\'filename of the JSON config file for BERT model.\')\n    group1.add_argument(\'-graph_tmp_dir\', type=str, default=None,\n                        help=\'path to graph temp file\')\n\n    group2 = parser.add_argument_group(\'BERT Parameters\',\n                                       \'config how BERT model and pooling works\')\n    group2.add_argument(\'-max_seq_len\', type=check_max_seq_len, default=25,\n                        help=\'maximum length of a sequence, longer sequence will be trimmed on the right side. \'\n                             \'set it to NONE for dynamically using the longest sequence in a (mini)batch.\')\n    group2.add_argument(\'-cased_tokenization\', dest=\'do_lower_case\', action=\'store_false\', default=True,\n                        help=\'Whether tokenizer should skip the default lowercasing and accent removal.\'\n                             \'Should be used for e.g. the multilingual cased pretrained BERT model.\')\n    group2.add_argument(\'-pooling_layer\', type=int, nargs=\'+\', default=[-2],\n                        help=\'the encoder layer(s) that receives pooling. \\\n                        Give a list in order to concatenate several layers into one\')\n    group2.add_argument(\'-pooling_strategy\', type=PoolingStrategy.from_string,\n                        default=PoolingStrategy.REDUCE_MEAN, choices=list(PoolingStrategy),\n                        help=\'the pooling strategy for generating encoding vectors\')\n    group2.add_argument(\'-mask_cls_sep\', action=\'store_true\', default=False,\n                        help=\'masking the embedding on [CLS] and [SEP] with zero. \\\n                        When pooling_strategy is in {CLS_TOKEN, FIRST_TOKEN, SEP_TOKEN, LAST_TOKEN} \\\n                        then the embedding is preserved, otherwise the embedding is masked to zero before pooling\')\n    group2.add_argument(\'-no_special_token\', action=\'store_true\', default=False,\n                        help=\'add [CLS] and [SEP] in every sequence, \\\n                        put sequence to the model without [CLS] and [SEP] when True and \\\n                        is_tokenized=True in Client\')\n    group2.add_argument(\'-show_tokens_to_client\', action=\'store_true\', default=False,\n                        help=\'sending tokenization results to client\')\n    group2.add_argument(\'-no_position_embeddings\', action=\'store_true\', default=False,\n                        help=\'Whether to add position embeddings for the position of each token in the sequence.\')\n    group2.add_argument(\'-num_labels\', type=int, default=2,\n                        help=\'Numbers of Label\')\n    \n    group3 = parser.add_argument_group(\'Serving Configs\',\n                                       \'config how server utilizes GPU/CPU resources\')\n    group3.add_argument(\'-port\', \'-port_in\', \'-port_data\', type=int, default=5555,\n                        help=\'server port for receiving data from client\')\n    group3.add_argument(\'-port_out\', \'-port_result\', type=int, default=5556,\n                        help=\'server port for sending result to client\')\n    group3.add_argument(\'-http_port\', type=int, default=None,\n                        help=\'server port for receiving HTTP requests\')\n    group3.add_argument(\'-http_max_connect\', type=int, default=10,\n                        help=\'maximum number of concurrent HTTP connections\')\n    group3.add_argument(\'-cors\', type=str, default=\'*\',\n                        help=\'setting ""Access-Control-Allow-Origin"" for HTTP requests\')\n    group3.add_argument(\'-num_worker\', type=int, default=1,\n                        help=\'number of server instances\')\n    group3.add_argument(\'-max_batch_size\', type=int, default=256,\n                        help=\'maximum number of sequences handled by each worker\')\n    group3.add_argument(\'-priority_batch_size\', type=int, default=16,\n                        help=\'batch smaller than this size will be labeled as high priority,\'\n                             \'and jumps forward in the job queue\')\n    group3.add_argument(\'-cpu\', action=\'store_true\', default=False,\n                        help=\'running on CPU (default on GPU)\')\n    group3.add_argument(\'-xla\', action=\'store_true\', default=False,\n                        help=\'enable XLA compiler (experimental)\')\n    group3.add_argument(\'-fp16\', action=\'store_true\', default=False,\n                        help=\'use float16 precision (experimental)\')\n    group3.add_argument(\'-gpu_memory_fraction\', type=float, default=0.5,\n                        help=\'determine the fraction of the overall amount of memory \\\n                        that each visible GPU should be allocated per worker. \\\n                        Should be in range [0.0, 1.0]\')\n    group3.add_argument(\'-device_map\', type=int, nargs=\'+\', default=[],\n                        help=\'specify the list of GPU device ids that will be used (id starts from 0). \\\n                        If num_worker > len(device_map), then device will be reused; \\\n                        if num_worker < len(device_map), then device_map[:num_worker] will be used\')\n    group3.add_argument(\'-prefetch_size\', type=int, default=10,\n                        help=\'the number of batches to prefetch on each worker. When running on a CPU-only machine, \\\n                        this is set to 0 for comparability\')\n    group3.add_argument(\'-fixed_embed_length\', action=\'store_true\', default=False,\n                        help=\'when ""max_seq_len"" is set to None, the server determines the ""max_seq_len"" according to \'\n                             \'the actual sequence lengths within each batch. When ""pooling_strategy=NONE"", \'\n                             \'this may cause two "".encode()"" from the same client results in different sizes [B, T, D].\'\n                             \'Turn this on to fix the ""T"" in [B, T, D] to ""max_position_embeddings"" in bert json config.\')\n\n    parser.add_argument(\'-verbose\', action=\'store_true\', default=False,\n                        help=\'turn on tensorflow logging for debug\')\n    parser.add_argument(\'-version\', action=\'version\', version=\'%(prog)s \' + __version__)\n    return parser\n\n\ndef check_tf_version():\n    import tensorflow as tf\n    tf_ver = tf.__version__.split(\'.\')\n    if int(tf_ver[0]) <= 1 and int(tf_ver[1]) < 10:\n        raise ModuleNotFoundError(\'Tensorflow >=1.10 (one-point-ten) is required!\')\n    elif int(tf_ver[0]) > 1:\n        warnings.warn(\'Tensorflow %s is not tested! It may or may not work. \'\n                      \'Feel free to submit an issue at https://github.com/hanxiao/bert-as-service/issues/\' % tf.__version__)\n    return tf_ver\n\n\ndef import_tf(device_id=-1, verbose=False, use_fp16=False):\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'-1\' if device_id < 0 else str(device_id)\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'0\' if verbose else \'3\'\n    os.environ[\'TF_FP16_MATMUL_USE_FP32_COMPUTE\'] = \'0\' if use_fp16 else \'1\'\n    os.environ[\'TF_FP16_CONV_USE_FP32_COMPUTE\'] = \'0\' if use_fp16 else \'1\'\n    import tensorflow as tf\n    tf.logging.set_verbosity(tf.logging.DEBUG if verbose else tf.logging.ERROR)\n    return tf\n\n\ndef auto_bind(socket):\n    if os.name == \'nt\':  # for Windows\n        socket.bind_to_random_port(\'tcp://127.0.0.1\')\n    else:\n        # Get the location for tmp file for sockets\n        try:\n            tmp_dir = os.environ[\'ZEROMQ_SOCK_TMP_DIR\']\n            if not os.path.exists(tmp_dir):\n                raise ValueError(\'This directory for sockets ({}) does not seems to exist.\'.format(tmp_dir))\n            tmp_dir = os.path.join(tmp_dir, str(uuid.uuid1())[:8])\n        except KeyError:\n            tmp_dir = \'*\'\n\n        socket.bind(\'ipc://{}\'.format(tmp_dir))\n    return socket.getsockopt(zmq.LAST_ENDPOINT).decode(\'ascii\')\n\n\ndef get_run_args(parser_fn=get_args_parser, printed=True):\n    args = parser_fn().parse_args()\n    if printed:\n        param_str = \'\\n\'.join([\'%20s = %s\' % (k, v) for k, v in sorted(vars(args).items())])\n        print(\'usage: %s\\n%20s   %s\\n%s\\n%s\\n\' % (\' \'.join(sys.argv), \'ARG\', \'VALUE\', \'_\' * 50, param_str))\n    return args\n\n\ndef get_benchmark_parser():\n    parser = get_args_parser()\n    parser.description = \'Benchmark BertServer locally\'\n\n    parser.set_defaults(num_client=1, client_batch_size=4096)\n\n    group = parser.add_argument_group(\'Benchmark parameters\', \'config the experiments of the benchmark\')\n\n    group.add_argument(\'-test_client_batch_size\', type=int, nargs=\'*\', default=[1, 16, 256, 4096])\n    group.add_argument(\'-test_max_batch_size\', type=int, nargs=\'*\', default=[8, 32, 128, 512])\n    group.add_argument(\'-test_max_seq_len\', type=int, nargs=\'*\', default=[32, 64, 128, 256])\n    group.add_argument(\'-test_num_client\', type=int, nargs=\'*\', default=[1, 4, 16, 64])\n    group.add_argument(\'-test_pooling_layer\', type=int, nargs=\'*\', default=[[-j] for j in range(1, 13)])\n\n    group.add_argument(\'-wait_till_ready\', type=int, default=30,\n                       help=\'seconds to wait until server is ready to serve\')\n    group.add_argument(\'-client_vocab_file\', type=str, default=\'README.md\',\n                       help=\'file path for building client vocabulary\')\n    group.add_argument(\'-num_repeat\', type=int, default=10,\n                       help=\'number of repeats per experiment (must >2), \'\n                            \'as the first two results are omitted for warm-up effect\')\n    return parser\n\n\ndef get_shutdown_parser():\n    parser = argparse.ArgumentParser()\n    parser.description = \'Shutting down a BertServer instance running on a specific port\'\n\n    parser.add_argument(\'-ip\', type=str, default=\'localhost\',\n                        help=\'the ip address that a BertServer is running on\')\n    parser.add_argument(\'-port\', \'-port_in\', \'-port_data\', type=int, required=True,\n                        help=\'the port that a BertServer is running on\')\n    parser.add_argument(\'-timeout\', type=int, default=5000,\n                        help=\'timeout (ms) for connecting to a server\')\n    return parser\n\n\nclass TimeContext:\n    def __init__(self, msg):\n        self._msg = msg\n\n    def __enter__(self):\n        self.start = time.perf_counter()\n        print(self._msg, end=\' ...\\t\', flush=True)\n\n    def __exit__(self, typ, value, traceback):\n        self.duration = time.perf_counter() - self.start\n        print(colored(\'    [%3.3f secs]\' % self.duration, \'green\'), flush=True)\n\nclass CappedHistogram:\n    """"""Space capped dict with aggregate stat tracking.\n\n    Evicts using LRU policy when at capacity; evicted elements are added to aggregate stats.\n    Arguments:\n    capacity -- the capacity limit of the dict\n    """"""\n    def __init__(self, capacity):\n        self.cache = OrderedDict()\n        self.capacity = capacity\n        self.base_bins = 0\n        self.base_count = 0\n        self.base_min = float(\'inf\')\n        self.min_count = 0\n        self.base_max = 0\n        self.max_count = 0\n\n    def __getitem__(self, key):\n        if key in self.cache:\n            return self.cache[key]\n        return 0\n\n    def __setitem__(self, key, value):\n        if key in self.cache:\n            del self.cache[key]\n        self.cache[key] = value\n        if len(self.cache) > self.capacity:\n            self._evict()\n\n    def total_size(self):\n        return self.base_bins + len(self.cache)\n\n    def __len__(self):\n        return len(self.cache)\n\n    def values(self):\n        return self.cache.values()\n\n    def _evict(self):\n        key,val = self.cache.popitem(False)\n        self.base_bins += 1\n        self.base_count += val\n        if val < self.base_min:\n            self.base_min = val\n            self.min_count = 1\n        elif val == self.base_min:\n            self.min_count += 1\n        if val > self.base_max:\n            self.base_max = val\n            self.max_count = 1\n        elif val == self.base_max:\n            self.max_count += 1\n\n    def get_stat_map(self, name):\n        if len(self.cache) == 0:\n            return {}\n        counts = self.cache.values()\n        avg = (self.base_count + sum(counts)) / (self.base_bins + len(counts))\n        min_, max_ = min(counts), max(counts)\n        num_min, num_max = 0, 0\n        if self.base_min <= min_:\n            min_ = self.base_min\n            num_min += self.min_count\n        if self.base_min >= min_:\n            num_min += sum(v == min_ for v in counts)\n\n        if self.base_max >= max_:\n            max_ = self.base_max\n            num_max += self.max_count\n        if self.base_max <= max_:\n            num_max += sum(v == max_ for v in counts)\n\n        return {\n            \'avg_%s\' % name: avg,\n            \'min_%s\' % name: min_,\n            \'max_%s\' % name: max_,\n            \'num_min_%s\' % name: num_min,\n            \'num_max_%s\' % name: num_max,\n        }\n'"
server/bert_serving/server/http.py,0,"b'from multiprocessing import Process, Event\n\nfrom termcolor import colored\n\nfrom .helper import set_logger\n\n\nclass BertHTTPProxy(Process):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        self.is_ready = Event()\n\n    def create_flask_app(self):\n        try:\n            from flask import Flask, request\n            from flask_compress import Compress\n            from flask_cors import CORS\n            from flask_json import FlaskJSON, as_json, JsonError\n            from bert_serving.client import ConcurrentBertClient\n            from flasgger import Swagger\n        except ImportError:\n            raise ImportError(\'BertClient or Flask or its dependencies are not fully installed, \'\n                              \'they are required for serving HTTP requests.\'\n                              \'Please use ""pip install -U bert-serving-server[http]"" to install it.\')\n\n        # support up to 10 concurrent HTTP requests\n        bc = ConcurrentBertClient(max_concurrency=self.args.http_max_connect,\n                                  port=self.args.port, port_out=self.args.port_out,\n                                  output_fmt=\'list\', ignore_all_checks=True)\n        app = Flask(__name__)\n        app.config[\'SWAGGER\'] = {\n          \'title\': \'Colors API\',\n          \'uiversion\': 3,\n          \'openapi\': \'3.0.2\'\n        }\n        swag = Swagger(app, template_file=\'bertApi.openapi.yaml\')\n\n        logger = set_logger(colored(\'PROXY\', \'red\'))\n\n        @app.route(\'/status/server\', methods=[\'GET\'])\n        @as_json\n        def get_server_status():\n            return bc.server_status\n\n        @app.route(\'/status/client\', methods=[\'GET\'])\n        @as_json\n        def get_client_status():\n            return bc.status\n\n        @app.route(\'/encode\', methods=[\'POST\'])\n        @as_json\n        def encode_query():\n            data = request.form if request.form else request.json\n            try:\n                logger.info(\'new request from %s\' % request.remote_addr)\n                return {\'id\': data[\'id\'],\n                        \'result\': bc.encode(data[\'texts\'], is_tokenized=bool(\n                            data[\'is_tokenized\']) if \'is_tokenized\' in data else False)}\n\n            except Exception as e:\n                logger.error(\'error when handling HTTP request\', exc_info=True)\n                raise JsonError(description=str(e), type=str(type(e).__name__))\n\n        CORS(app, origins=self.args.cors)\n        FlaskJSON(app)\n        Compress().init_app(app)\n        return app\n\n    def run(self):\n        app = self.create_flask_app()\n        self.is_ready.set()\n        app.run(port=self.args.http_port, threaded=True, host=\'0.0.0.0\')\n'"
server/bert_serving/server/zmq_decor.py,0,"b'from contextlib import ExitStack\n\nfrom zmq.decorators import _Decorator\n\n__all__ = [\'multi_socket\']\n\nfrom functools import wraps\n\nimport zmq\n\n\nclass _MyDecorator(_Decorator):\n    def __call__(self, *dec_args, **dec_kwargs):\n        kw_name, dec_args, dec_kwargs = self.process_decorator_args(*dec_args, **dec_kwargs)\n        num_socket_str = dec_kwargs.pop(\'num_socket\')\n\n        def decorator(func):\n            @wraps(func)\n            def wrapper(*args, **kwargs):\n                num_socket = getattr(args[0], num_socket_str)\n                targets = [self.get_target(*args, **kwargs) for _ in range(num_socket)]\n                with ExitStack() as stack:\n                    for target in targets:\n                        obj = stack.enter_context(target(*dec_args, **dec_kwargs))\n                        args = args + (obj,)\n\n                    return func(*args, **kwargs)\n\n            return wrapper\n\n        return decorator\n\n\nclass _SocketDecorator(_MyDecorator):\n    def process_decorator_args(self, *args, **kwargs):\n        """"""Also grab context_name out of kwargs""""""\n        kw_name, args, kwargs = super(_SocketDecorator, self).process_decorator_args(*args, **kwargs)\n        self.context_name = kwargs.pop(\'context_name\', \'context\')\n        return kw_name, args, kwargs\n\n    def get_target(self, *args, **kwargs):\n        """"""Get context, based on call-time args""""""\n        context = self._get_context(*args, **kwargs)\n        return context.socket\n\n    def _get_context(self, *args, **kwargs):\n        if self.context_name in kwargs:\n            ctx = kwargs[self.context_name]\n\n            if isinstance(ctx, zmq.Context):\n                return ctx\n\n        for arg in args:\n            if isinstance(arg, zmq.Context):\n                return arg\n        # not specified by any decorator\n        return zmq.Context.instance()\n\n\ndef multi_socket(*args, **kwargs):\n    return _SocketDecorator()(*args, **kwargs)\n'"
server/bert_serving/server/bert/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n'"
server/bert_serving/server/bert/extract_features.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport re\n\nfrom . import tokenization\n\n__all__ = [\'convert_lst_to_features\']\n\n\nclass InputExample(object):\n\n    def __init__(self, unique_id, text_a, text_b):\n        self.unique_id = unique_id\n        self.text_a = text_a\n        self.text_b = text_b\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self, tokens, input_ids, input_mask, input_type_ids):\n        # self.unique_id = unique_id\n        self.tokens = tokens\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.input_type_ids = input_type_ids\n\n\ndef convert_lst_to_features(lst_str, max_seq_length, max_position_embeddings,\n                            tokenizer, logger, is_tokenized=False, mask_cls_sep=False, no_special_token=False):\n    """"""Loads a data file into a list of `InputBatch`s.""""""\n\n    examples = read_tokenized_examples(lst_str) if is_tokenized else read_examples(lst_str)\n\n    _tokenize = lambda x: tokenizer.mark_unk_tokens(x) if is_tokenized else tokenizer.tokenize(x)\n\n    all_tokens = [(_tokenize(ex.text_a), _tokenize(ex.text_b) if ex.text_b else []) for ex in examples]\n\n    # user did not specify a meaningful sequence length\n    # override the sequence length by the maximum seq length of the current batch\n    if max_seq_length is None:\n        max_seq_length = max(len(ta) + len(tb) for ta, tb in all_tokens)\n        # add special tokens into account\n        # case 1: Account for [CLS], tokens_a [SEP], tokens_b [SEP] -> 3 additional tokens\n        # case 2: Account for [CLS], tokens_a [SEP] -> 2 additional tokens\n        max_seq_length += 3 if any(len(tb) for _, tb in all_tokens) else 2\n        max_seq_length = min(max_seq_length, max_position_embeddings)\n        logger.warning(\'""max_seq_length"" is undefined, \'\n                       \'and bert config json defines ""max_position_embeddings""=%d. \'\n                       \'hence set ""max_seq_length""=%d according to the current batch.\' % (\n                           max_position_embeddings, max_seq_length))\n\n    for (tokens_a, tokens_b) in all_tokens:\n        if tokens_b:\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\n            # length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with ""- 3""\n            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n        else:\n            # Account for [CLS] and [SEP] with ""- 2"" when with the special token\n            if len(tokens_a) > max_seq_length - 2 and not no_special_token:\n                tokens_a = tokens_a[0:(max_seq_length - 2)]\n            # truncate with max_sql_length when without special token and tokenized\n            elif len(tokens_a) > max_seq_length and (no_special_token and is_tokenized):\n                tokens_a = tokens_a[0:max_seq_length]\n\n        # The convention in BERT is:\n        # (a) For sequence pairs:\n        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n        # (b) For single sequences:\n        #  tokens:   [CLS] the dog is hairy . [SEP]\n        #  type_ids: 0     0   0   0  0     0 0\n        #\n        # Where ""type_ids"" are used to indicate whether this is the first\n        # sequence or the second sequence. The embedding vectors for `type=0` and\n        # `type=1` were learned during pre-training and are added to the wordpiece\n        # embedding vector (and position vector). This is not *strictly* necessary\n        # since the [SEP] token unambiguously separates the sequences, but it makes\n        # it easier for the model to learn the concept of sequences.\n        #\n        # For classification tasks, the first vector (corresponding to [CLS]) is\n        # used as as the ""sentence vector"". Note that this only makes sense because\n        # the entire model is fine-tuned.\n        if no_special_token and is_tokenized and not tokens_b:\n            tokens = tokens_a\n            # max_seq_length -= 2\n            input_mask = [1] * len(tokens_a)\n        else:\n            tokens = [\'[CLS]\'] + tokens_a + [\'[SEP]\']\n            input_mask = [int(not mask_cls_sep)] + [1] * len(tokens_a) + [int(not mask_cls_sep)]\n        input_type_ids = [0] * len(tokens)\n\n        if tokens_b:\n            tokens += tokens_b + [\'[SEP]\']\n            input_type_ids += [1] * (len(tokens_b) + 1)\n            input_mask += [1] * len(tokens_b) + [int(not mask_cls_sep)]\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # Zero-pad up to the sequence length. more pythonic\n        pad_len = max_seq_length - len(input_ids)\n        input_ids += [0] * pad_len\n        input_mask += [0] * pad_len\n        input_type_ids += [0] * pad_len\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(input_type_ids) == max_seq_length\n\n        logger.debug(\'tokens: %s\' % \' \'.join([tokenization.printable_text(x) for x in tokens]))\n        logger.debug(\'input_ids: %s\' % \' \'.join([str(x) for x in input_ids]))\n        logger.debug(\'input_mask: %s\' % \' \'.join([str(x) for x in input_mask]))\n        logger.debug(\'input_type_ids: %s\' % \' \'.join([str(x) for x in input_type_ids]))\n\n        yield InputFeatures(\n            # unique_id=example.unique_id,\n            tokens=tokens,\n            input_ids=input_ids,\n            input_mask=input_mask,\n            input_type_ids=input_type_ids)\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    """"""Truncates a sequence pair in place to the maximum length.""""""\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that\'s truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\n\ndef read_examples(lst_strs):\n    """"""Read a list of `InputExample`s from a list of strings.""""""\n    unique_id = 0\n    for ss in lst_strs:\n        line = tokenization.convert_to_unicode(ss)\n        if not line:\n            continue\n        line = line.strip()\n        text_a = None\n        text_b = None\n        m = re.match(r""^(.*) \\|\\|\\| (.*)$"", line)\n        if m is None:\n            text_a = line\n        else:\n            text_a = m.group(1)\n            text_b = m.group(2)\n        yield InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b)\n        unique_id += 1\n\n\ndef read_tokenized_examples(lst_strs):\n    unique_id = 0\n    lst_strs = [[tokenization.convert_to_unicode(w) for w in s] for s in lst_strs]\n    for ss in lst_strs:\n        text_a = ss\n        text_b = None\n        try:\n            j = ss.index(\'|||\')\n            text_a = ss[:j]\n            text_b = ss[(j + 1):]\n        except ValueError:\n            pass\n        yield InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b)\n        unique_id += 1\n'"
server/bert_serving/server/bert/modeling.py,80,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport json\nimport math\nimport re\n\nimport six\nimport tensorflow as tf\n\n\nclass BertConfig(object):\n    """"""Configuration for `BertModel`.""""""\n\n    def __init__(self,\n                 vocab_size,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=""gelu"",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=16,\n                 initializer_range=0.02):\n        """"""Constructs BertConfig.\n\n        Args:\n          vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n          hidden_size: Size of the encoder layers and the pooler layer.\n          num_hidden_layers: Number of hidden layers in the Transformer encoder.\n          num_attention_heads: Number of attention heads for each attention layer in\n            the Transformer encoder.\n          intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n            layer in the Transformer encoder.\n          hidden_act: The non-linear activation function (function or string) in the\n            encoder and pooler.\n          hidden_dropout_prob: The dropout probability for all fully connected\n            layers in the embeddings, encoder, and pooler.\n          attention_probs_dropout_prob: The dropout ratio for the attention\n            probabilities.\n          max_position_embeddings: The maximum sequence length that this model might\n            ever be used with. Typically set this to something large just in case\n            (e.g., 512 or 1024 or 2048).\n          type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n            `BertModel`.\n          initializer_range: The stdev of the truncated_normal_initializer for\n            initializing all weight matrices.\n        """"""\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.hidden_act = hidden_act\n        self.intermediate_size = intermediate_size\n        self.hidden_dropout_prob = hidden_dropout_prob\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.max_position_embeddings = max_position_embeddings\n        self.type_vocab_size = type_vocab_size\n        self.initializer_range = initializer_range\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n        config = BertConfig(vocab_size=None)\n        for (key, value) in six.iteritems(json_object):\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `BertConfig` from a json file of parameters.""""""\n        with tf.gfile.GFile(json_file, ""r"") as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\nclass BertModel(object):\n    """"""BERT model (""Bidirectional Embedding Representations from a Transformer"").\n\n    Example usage:\n\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n    input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n\n    config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n      num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n\n    model = modeling.BertModel(config=config, is_training=True,\n      input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n\n    label_embeddings = tf.get_variable(...)\n    pooled_output = model.get_pooled_output()\n    logits = tf.matmul(pooled_output, label_embeddings)\n    ...\n    ```\n    """"""\n\n    def __init__(self,\n                 config,\n                 is_training,\n                 input_ids,\n                 input_mask=None,\n                 token_type_ids=None,\n                 use_one_hot_embeddings=True,\n                 use_position_embeddings=True,\n                 scope=None):\n        """"""Constructor for BertModel.\n\n        Args:\n          config: `BertConfig` instance.\n          is_training: bool. rue for training model, false for eval model. Controls\n            whether dropout will be applied.\n          input_ids: int32 Tensor of shape [batch_size, seq_length].\n          input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n          token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n          use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n            embeddings or tf.embedding_lookup() for the word embeddings. On the TPU,\n            it is must faster if this is True, on the CPU or GPU, it is faster if\n            this is False.\n          scope: (optional) variable scope. Defaults to ""bert"".\n\n        Raises:\n          ValueError: The config is invalid or one of the input tensor shapes\n            is invalid.\n        """"""\n        config = copy.deepcopy(config)\n        if not is_training:\n            config.hidden_dropout_prob = 0.0\n            config.attention_probs_dropout_prob = 0.0\n\n        input_shape = get_shape_list(input_ids, expected_rank=2)\n        batch_size = input_shape[0]\n        seq_length = input_shape[1]\n\n        if input_mask is None:\n            input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n        if token_type_ids is None:\n            token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n        with tf.variable_scope(scope, default_name=""bert""):\n            with tf.variable_scope(""embeddings""):\n                # Perform embedding lookup on the word ids.\n                (self.embedding_output, self.embedding_table) = embedding_lookup(\n                    input_ids=input_ids,\n                    vocab_size=config.vocab_size,\n                    embedding_size=config.hidden_size,\n                    initializer_range=config.initializer_range,\n                    word_embedding_name=""word_embeddings"",\n                    use_one_hot_embeddings=use_one_hot_embeddings)\n\n                # Add positional embeddings and token type embeddings, then layer\n                # normalize and perform dropout.\n                self.embedding_output = embedding_postprocessor(\n                    input_tensor=self.embedding_output,\n                    use_token_type=True,\n                    token_type_ids=token_type_ids,\n                    token_type_vocab_size=config.type_vocab_size,\n                    token_type_embedding_name=""token_type_embeddings"",\n                    use_position_embeddings=use_position_embeddings,\n                    position_embedding_name=""position_embeddings"",\n                    initializer_range=config.initializer_range,\n                    max_position_embeddings=config.max_position_embeddings,\n                    dropout_prob=config.hidden_dropout_prob)\n\n            with tf.variable_scope(""encoder""):\n                # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n                # mask of shape [batch_size, seq_length, seq_length] which is used\n                # for the attention scores.\n                attention_mask = create_attention_mask_from_input_mask(\n                    input_ids, input_mask)\n\n                # Run the stacked transformer.\n                # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n                self.all_encoder_layers = transformer_model(\n                    input_tensor=self.embedding_output,\n                    attention_mask=attention_mask,\n                    hidden_size=config.hidden_size,\n                    num_hidden_layers=config.num_hidden_layers,\n                    num_attention_heads=config.num_attention_heads,\n                    intermediate_size=config.intermediate_size,\n                    intermediate_act_fn=get_activation(config.hidden_act),\n                    hidden_dropout_prob=config.hidden_dropout_prob,\n                    attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n                    initializer_range=config.initializer_range,\n                    do_return_all_layers=True)\n\n            self.sequence_output = self.all_encoder_layers[-1]\n            # The ""pooler"" converts the encoded sequence tensor of shape\n            # [batch_size, seq_length, hidden_size] to a tensor of shape\n            # [batch_size, hidden_size]. This is necessary for segment-level\n            # (or segment-pair-level) classification tasks where we need a fixed\n            # dimensional representation of the segment.\n            with tf.variable_scope(""pooler""):\n                # We ""pool"" the model by simply taking the hidden state corresponding\n                # to the first token. We assume that this has been pre-trained\n                first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n                # https://github.com/google-research/bert/issues/43#issuecomment-435980269\n                self.pooled_output = tf.layers.dense(\n                    first_token_tensor,\n                    config.hidden_size,\n                    activation=tf.tanh,\n                    kernel_initializer=create_initializer(config.initializer_range))\n\n    def get_pooled_output(self):\n        return self.pooled_output\n\n    def get_sequence_output(self):\n        """"""Gets final hidden layer of encoder.\n\n        Returns:\n          float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n          to the final hidden of the transformer encoder.\n        """"""\n        return self.sequence_output\n\n    def get_all_encoder_layers(self):\n        return self.all_encoder_layers\n\n    def get_embedding_output(self):\n        """"""Gets output of the embedding lookup (i.e., input to the transformer).\n\n        Returns:\n          float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n          to the output of the embedding layer, after summing the word\n          embeddings with the positional embeddings and the token type embeddings,\n          then performing layer normalization. This is the input to the transformer.\n        """"""\n        return self.embedding_output\n\n    def get_embedding_table(self):\n        return self.embedding_table\n\n\ndef gelu(input_tensor):\n    """"""Gaussian Error Linear Unit.\n\n    This is a smoother version of the RELU.\n    Original paper: https://arxiv.org/abs/1606.08415\n\n    Args:\n      input_tensor: float Tensor to perform activation.\n\n    Returns:\n      `input_tensor` with the GELU activation applied.\n    """"""\n    cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))\n    return input_tensor * cdf\n\n\ndef get_activation(activation_string):\n    """"""Maps a string to a Python function, e.g., ""relu"" => `tf.nn.relu`.\n\n    Args:\n      activation_string: String name of the activation function.\n\n    Returns:\n      A Python function corresponding to the activation function. If\n      `activation_string` is None, empty, or ""linear"", this will return None.\n      If `activation_string` is not a string, it will return `activation_string`.\n\n    Raises:\n      ValueError: The `activation_string` does not correspond to a known\n        activation.\n    """"""\n\n    # We assume that anything that""s not a string is already an activation\n    # function, so we just return it.\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n\n    if not activation_string:\n        return None\n\n    act = activation_string.lower()\n    if act == ""linear"":\n        return None\n    elif act == ""relu"":\n        return tf.nn.relu\n    elif act == ""gelu"":\n        return gelu\n    elif act == ""tanh"":\n        return tf.tanh\n    else:\n        raise ValueError(""Unsupported activation: %s"" % act)\n\n\ndef get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n    """"""Compute the union of the current variables and checkpoint variables.""""""\n    assignment_map = {}\n    initialized_variable_names = {}\n\n    name_to_variable = collections.OrderedDict()\n    for var in tvars:\n        name = var.name\n        m = re.match(""^(.*):\\\\d+$"", name)\n        if m is not None:\n            name = m.group(1)\n        name_to_variable[name] = var\n\n    init_vars = tf.train.list_variables(init_checkpoint)\n\n    assignment_map = collections.OrderedDict()\n    for x in init_vars:\n        (name, var) = (x[0], x[1])\n        if name not in name_to_variable:\n            continue\n        assignment_map[name] = name\n        initialized_variable_names[name] = 1\n        initialized_variable_names[name + "":0""] = 1\n\n    return (assignment_map, initialized_variable_names)\n\n\ndef dropout(input_tensor, dropout_prob):\n    """"""Perform dropout.\n\n    Args:\n      input_tensor: float Tensor.\n      dropout_prob: Python float. The probability of dropping out a value (NOT of\n        *keeping* a dimension as in `tf.nn.dropout`).\n\n    Returns:\n      A version of `input_tensor` with dropout applied.\n    """"""\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output\n\n\ndef layer_norm(input_tensor, name=None):\n    """"""Run layer normalization on the last dimension of the tensor.""""""\n    return tf.contrib.layers.layer_norm(\n        inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n\n\ndef layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n    """"""Runs layer normalization followed by dropout.""""""\n    output_tensor = layer_norm(input_tensor, name)\n    output_tensor = dropout(output_tensor, dropout_prob)\n    return output_tensor\n\n\ndef create_initializer(initializer_range=0.02):\n    """"""Creates a `truncated_normal_initializer` with the given range.""""""\n    return tf.truncated_normal_initializer(stddev=initializer_range)\n\n\ndef embedding_lookup(input_ids,\n                     vocab_size,\n                     embedding_size=128,\n                     initializer_range=0.02,\n                     word_embedding_name=""word_embeddings"",\n                     use_one_hot_embeddings=False):\n    """"""Looks up words embeddings for id tensor.\n\n    Args:\n      input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n        ids.\n      vocab_size: int. Size of the embedding vocabulary.\n      embedding_size: int. Width of the word embeddings.\n      initializer_range: float. Embedding initialization range.\n      word_embedding_name: string. Name of the embedding table.\n      use_one_hot_embeddings: bool. If True, use one-hot method for word\n        embeddings. If False, use `tf.nn.embedding_lookup()`. One hot is better\n        for TPUs.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, embedding_size].\n    """"""\n    # This function assumes that the input is of shape [batch_size, seq_length,\n    # num_inputs].\n    #\n    # If the input is a 2D tensor of shape [batch_size, seq_length], we\n    # reshape to [batch_size, seq_length, 1].\n    if input_ids.shape.ndims == 2:\n        input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n    embedding_table = tf.get_variable(\n        name=word_embedding_name,\n        shape=[vocab_size, embedding_size],\n        initializer=create_initializer(initializer_range))\n\n    if use_one_hot_embeddings:\n        flat_input_ids = tf.reshape(input_ids, [-1])\n        one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n        output = tf.matmul(one_hot_input_ids, embedding_table)\n    else:\n        output = tf.nn.embedding_lookup(embedding_table, input_ids)\n\n    input_shape = get_shape_list(input_ids)\n\n    output = tf.reshape(output,\n                        input_shape[0:-1] + [input_shape[-1] * embedding_size])\n    return (output, embedding_table)\n\n\ndef embedding_postprocessor(input_tensor,\n                            use_token_type=False,\n                            token_type_ids=None,\n                            token_type_vocab_size=16,\n                            token_type_embedding_name=""token_type_embeddings"",\n                            use_position_embeddings=True,\n                            position_embedding_name=""position_embeddings"",\n                            initializer_range=0.02,\n                            max_position_embeddings=512,\n                            dropout_prob=0.1):\n    """"""Performs various post-processing on a word embedding tensor.\n\n    Args:\n      input_tensor: float Tensor of shape [batch_size, seq_length,\n        embedding_size].\n      use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n        Must be specified if `use_token_type` is True.\n      token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n      token_type_embedding_name: string. The name of the embedding table variable\n        for token type ids.\n      use_position_embeddings: bool. Whether to add position embeddings for the\n        position of each token in the sequence.\n      position_embedding_name: string. The name of the embedding table variable\n        for positional embeddings.\n      initializer_range: float. Range of the weight initialization.\n      max_position_embeddings: int. Maximum sequence length that might ever be\n        used with this model. This can be longer than the sequence length of\n        input_tensor, but cannot be shorter.\n      dropout_prob: float. Dropout probability applied to the final output tensor.\n\n    Returns:\n      float tensor with same shape as `input_tensor`.\n\n    Raises:\n      ValueError: One of the tensor shapes or input values is invalid.\n    """"""\n    input_shape = get_shape_list(input_tensor, expected_rank=3)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n    width = input_shape[2]\n\n    # tf.Assert(tf.less_equal(seq_length, max_position_embeddings), [seq_length])\n    # if seq_length > max_position_embeddings:\n    #     raise ValueError(""The seq length (%d) cannot be greater than ""\n    #                      ""`max_position_embeddings` (%d)"" %\n    #                      (seq_length, max_position_embeddings))\n\n    output = input_tensor\n\n    if use_token_type:\n        if token_type_ids is None:\n            raise ValueError(""`token_type_ids` must be specified if""\n                             ""`use_token_type` is True."")\n        token_type_table = tf.get_variable(\n            name=token_type_embedding_name,\n            shape=[token_type_vocab_size, width],\n            initializer=create_initializer(initializer_range))\n        # This vocab will be small so we always do one-hot here, since it is always\n        # faster for a small vocabulary.\n        flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n        one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n        token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n        token_type_embeddings = tf.reshape(token_type_embeddings,\n                                           [batch_size, seq_length, width])\n        output += token_type_embeddings\n\n    if use_position_embeddings:\n        full_position_embeddings = tf.get_variable(\n            name=position_embedding_name,\n            shape=[max_position_embeddings, width],\n            initializer=create_initializer(initializer_range))\n        # Since the position embedding table is a learned variable, we create it\n        # using a (long) sequence length `max_position_embeddings`. The actual\n        # sequence length might be shorter than this, for faster training of\n        # tasks that do not have long sequences.\n        #\n        # So `full_position_embeddings` is effectively an embedding table\n        # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n        # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n        # perform a slice.\n        position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])\n        # if seq_length < max_position_embeddings:\n        #     position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n        #                                    [seq_length, -1])\n        # else:\n        #     position_embeddings = full_position_embeddings\n\n        num_dims = len(output.shape.as_list())\n\n        # Only the last two dimensions are relevant (`seq_length` and `width`), so\n        # we broadcast among the first dimensions, which is typically just\n        # the batch size.\n        position_broadcast_shape = []\n        for _ in range(num_dims - 2):\n            position_broadcast_shape.append(1)\n        position_broadcast_shape.extend([seq_length, width])\n        position_embeddings = tf.reshape(position_embeddings,\n                                         position_broadcast_shape)\n        output += position_embeddings\n\n    output = layer_norm_and_dropout(output, dropout_prob)\n    return output\n\n\ndef create_attention_mask_from_input_mask(from_tensor, to_mask):\n    """"""Create 3D attention mask from a 2D tensor mask.\n\n    Args:\n      from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n      to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n    Returns:\n      float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n    """"""\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don\'t assume that `from_tensor` is a mask (although it could be). We\n    # don\'t actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n\ndef attention_layer(from_tensor,\n                    to_tensor,\n                    attention_mask=None,\n                    num_attention_heads=1,\n                    size_per_head=512,\n                    query_act=None,\n                    key_act=None,\n                    value_act=None,\n                    attention_probs_dropout_prob=0.0,\n                    initializer_range=0.02,\n                    do_return_2d_tensor=False,\n                    batch_size=None,\n                    from_seq_length=None,\n                    to_seq_length=None):\n    """"""Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n    This is an implementation of multi-headed attention based on ""Attention\n    is all you Need"". If `from_tensor` and `to_tensor` are the same, then\n    this is self-attention. Each timestep in `from_tensor` attends to the\n    corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n    This function first projects `from_tensor` into a ""query"" tensor and\n    `to_tensor` into ""key"" and ""value"" tensors. These are (effectively) a list\n    of tensors of length `num_attention_heads`, where each tensor is of shape\n    [batch_size, seq_length, size_per_head].\n\n    Then, the query and key tensors are dot-producted and scaled. These are\n    softmaxed to obtain attention probabilities. The value tensors are then\n    interpolated by these probabilities, then concatenated back to a single\n    tensor and returned.\n\n    In practice, the multi-headed attention are done with transposes and\n    reshapes rather than actual separate tensors.\n\n    Args:\n      from_tensor: float Tensor of shape [batch_size, from_seq_length,\n        from_width].\n      to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n      attention_mask: (optional) int32 Tensor of shape [batch_size,\n        from_seq_length, to_seq_length]. The values should be 1 or 0. The\n        attention scores will effectively be set to -infinity for any positions in\n        the mask that are 0, and will be unchanged for positions that are 1.\n      num_attention_heads: int. Number of attention heads.\n      size_per_head: int. Size of each attention head.\n      query_act: (optional) Activation function for the query transform.\n      key_act: (optional) Activation function for the key transform.\n      value_act: (optional) Activation function for the value transform.\n      attention_probs_dropout_prob: (optional) float. Dropout probability of the\n        attention probabilities.\n      initializer_range: float. Range of the weight initializer.\n      do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n        * from_seq_length, num_attention_heads * size_per_head]. If False, the\n        output will be of shape [batch_size, from_seq_length, num_attention_heads\n        * size_per_head].\n      batch_size: (Optional) int. If the input is 2D, this might be the batch size\n        of the 3D version of the `from_tensor` and `to_tensor`.\n      from_seq_length: (Optional) If the input is 2D, this might be the seq length\n        of the 3D version of the `from_tensor`.\n      to_seq_length: (Optional) If the input is 2D, this might be the seq length\n        of the 3D version of the `to_tensor`.\n\n    Returns:\n      float Tensor of shape [batch_size, from_seq_length,\n        num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n        true, this will be of shape [batch_size * from_seq_length,\n        num_attention_heads * size_per_head]).\n\n    Raises:\n      ValueError: Any of the arguments or tensor shapes are invalid.\n    """"""\n\n    def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                             seq_length, width):\n        output_tensor = tf.reshape(\n            input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n        output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n        return output_tensor\n\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n\n    if len(from_shape) != len(to_shape):\n        raise ValueError(\n            ""The rank of `from_tensor` must match the rank of `to_tensor`."")\n\n    if len(from_shape) == 3:\n        batch_size = from_shape[0]\n        from_seq_length = from_shape[1]\n        to_seq_length = to_shape[1]\n    elif len(from_shape) == 2:\n        if (batch_size is None or from_seq_length is None or to_seq_length is None):\n            raise ValueError(\n                ""When passing in rank 2 tensors to attention_layer, the values ""\n                ""for `batch_size`, `from_seq_length`, and `to_seq_length` ""\n                ""must all be specified."")\n\n    # Scalar dimensions referenced here:\n    #   B = batch size (number of sequences)\n    #   F = `from_tensor` sequence length\n    #   T = `to_tensor` sequence length\n    #   N = `num_attention_heads`\n    #   H = `size_per_head`\n\n    from_tensor_2d = reshape_to_matrix(from_tensor)\n    to_tensor_2d = reshape_to_matrix(to_tensor)\n\n    # `query_layer` = [B*F, N*H]\n    query_layer = tf.layers.dense(\n        from_tensor_2d,\n        num_attention_heads * size_per_head,\n        activation=query_act,\n        name=""query"",\n        kernel_initializer=create_initializer(initializer_range))\n\n    # `key_layer` = [B*T, N*H]\n    key_layer = tf.layers.dense(\n        to_tensor_2d,\n        num_attention_heads * size_per_head,\n        activation=key_act,\n        name=""key"",\n        kernel_initializer=create_initializer(initializer_range))\n\n    # `value_layer` = [B*T, N*H]\n    value_layer = tf.layers.dense(\n        to_tensor_2d,\n        num_attention_heads * size_per_head,\n        activation=value_act,\n        name=""value"",\n        kernel_initializer=create_initializer(initializer_range))\n\n    # `query_layer` = [B, N, F, H]\n    query_layer = transpose_for_scores(query_layer, batch_size,\n                                       num_attention_heads, from_seq_length,\n                                       size_per_head)\n\n    # `key_layer` = [B, N, T, H]\n    key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n                                     to_seq_length, size_per_head)\n\n    # Take the dot product between ""query"" and ""key"" to get the raw\n    # attention scores.\n    # `attention_scores` = [B, N, F, T]\n    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n    attention_scores = tf.multiply(attention_scores,\n                                   1.0 / math.sqrt(float(size_per_head)))\n\n    if attention_mask is not None:\n        # `attention_mask` = [B, 1, F, T]\n        attention_mask = tf.expand_dims(attention_mask, axis=[1])\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        attention_scores += adder\n\n    # Normalize the attention scores to probabilities.\n    # `attention_probs` = [B, N, F, T]\n    attention_probs = tf.nn.softmax(attention_scores)\n\n    # This is actually dropping out entire tokens to attend to, which might\n    # seem a bit unusual, but is taken from the original Transformer paper.\n    attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n\n    # `value_layer` = [B, T, N, H]\n    value_layer = tf.reshape(\n        value_layer,\n        [batch_size, to_seq_length, num_attention_heads, size_per_head])\n\n    # `value_layer` = [B, N, T, H]\n    value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n\n    # `context_layer` = [B, N, F, H]\n    context_layer = tf.matmul(attention_probs, value_layer)\n\n    # `context_layer` = [B, F, N, H]\n    context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n\n    if do_return_2d_tensor:\n        # `context_layer` = [B*F, N*V]\n        context_layer = tf.reshape(\n            context_layer,\n            [batch_size * from_seq_length, num_attention_heads * size_per_head])\n    else:\n        # `context_layer` = [B, F, N*V]\n        context_layer = tf.reshape(\n            context_layer,\n            [batch_size, from_seq_length, num_attention_heads * size_per_head])\n\n    return context_layer\n\n\ndef transformer_model(input_tensor,\n                      attention_mask=None,\n                      hidden_size=768,\n                      num_hidden_layers=12,\n                      num_attention_heads=12,\n                      intermediate_size=3072,\n                      intermediate_act_fn=gelu,\n                      hidden_dropout_prob=0.1,\n                      attention_probs_dropout_prob=0.1,\n                      initializer_range=0.02,\n                      do_return_all_layers=False):\n    """"""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".\n\n    This is almost an exact implementation of the original Transformer encoder.\n\n    See the original paper:\n    https://arxiv.org/abs/1706.03762\n\n    Also see:\n    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n    Args:\n      input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n      attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n        seq_length], with 1 for positions that can be attended to and 0 in\n        positions that should not be.\n      hidden_size: int. Hidden size of the Transformer.\n      num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n      num_attention_heads: int. Number of attention heads in the Transformer.\n      intermediate_size: int. The size of the ""intermediate"" (a.k.a., feed\n        forward) layer.\n      intermediate_act_fn: function. The non-linear activation function to apply\n        to the output of the intermediate/feed-forward layer.\n      hidden_dropout_prob: float. Dropout probability for the hidden layers.\n      attention_probs_dropout_prob: float. Dropout probability of the attention\n        probabilities.\n      initializer_range: float. Range of the initializer (stddev of truncated\n        normal).\n      do_return_all_layers: Whether to also return all layers or just the final\n        layer.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size], the final\n      hidden layer of the Transformer.\n\n    Raises:\n      ValueError: A Tensor shape or parameter is invalid.\n    """"""\n    if hidden_size % num_attention_heads != 0:\n        raise ValueError(\n            ""The hidden size (%d) is not a multiple of the number of attention ""\n            ""heads (%d)"" % (hidden_size, num_attention_heads))\n\n    attention_head_size = int(hidden_size / num_attention_heads)\n    input_shape = get_shape_list(input_tensor, expected_rank=3)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n    input_width = input_shape[2]\n\n    # The Transformer performs sum residuals on all layers so the input needs\n    # to be the same as the hidden size.\n    if input_width != hidden_size:\n        raise ValueError(""The width of the input tensor (%d) != hidden size (%d)"" %\n                         (input_width, hidden_size))\n\n    # We keep the representation as a 2D tensor to avoid re-shaping it back and\n    # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n    # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n    # help the optimizer.\n    prev_output = reshape_to_matrix(input_tensor)\n\n    all_layer_outputs = []\n    for layer_idx in range(num_hidden_layers):\n        with tf.variable_scope(""layer_%d"" % layer_idx):\n            layer_input = prev_output\n\n            with tf.variable_scope(""attention""):\n                attention_heads = []\n                with tf.variable_scope(""self""):\n                    attention_head = attention_layer(\n                        from_tensor=layer_input,\n                        to_tensor=layer_input,\n                        attention_mask=attention_mask,\n                        num_attention_heads=num_attention_heads,\n                        size_per_head=attention_head_size,\n                        attention_probs_dropout_prob=attention_probs_dropout_prob,\n                        initializer_range=initializer_range,\n                        do_return_2d_tensor=True,\n                        batch_size=batch_size,\n                        from_seq_length=seq_length,\n                        to_seq_length=seq_length)\n                    attention_heads.append(attention_head)\n\n                attention_output = None\n                if len(attention_heads) == 1:\n                    attention_output = attention_heads[0]\n                else:\n                    # In the case where we have other sequences, we just concatenate\n                    # them to the self-attention head before the projection.\n                    attention_output = tf.concat(attention_heads, axis=-1)\n\n                # Run a linear projection of `hidden_size` then add a residual\n                # with `layer_input`.\n                with tf.variable_scope(""output""):\n                    attention_output = tf.layers.dense(\n                        attention_output,\n                        hidden_size,\n                        kernel_initializer=create_initializer(initializer_range))\n                    attention_output = dropout(attention_output, hidden_dropout_prob)\n                    attention_output = layer_norm(attention_output + layer_input)\n\n            # The activation is only applied to the ""intermediate"" hidden layer.\n            with tf.variable_scope(""intermediate""):\n                intermediate_output = tf.layers.dense(\n                    attention_output,\n                    intermediate_size,\n                    activation=intermediate_act_fn,\n                    kernel_initializer=create_initializer(initializer_range))\n\n            # Down-project back to `hidden_size` then add the residual.\n            with tf.variable_scope(""output""):\n                layer_output = tf.layers.dense(\n                    intermediate_output,\n                    hidden_size,\n                    kernel_initializer=create_initializer(initializer_range))\n                layer_output = dropout(layer_output, hidden_dropout_prob)\n                layer_output = layer_norm(layer_output + attention_output)\n                prev_output = layer_output\n                all_layer_outputs.append(layer_output)\n\n    if do_return_all_layers:\n        final_outputs = []\n        for layer_output in all_layer_outputs:\n            final_output = reshape_from_matrix(layer_output, input_shape)\n            final_outputs.append(final_output)\n        return final_outputs\n    else:\n        final_output = reshape_from_matrix(prev_output, input_shape)\n        return final_output\n\n\ndef get_shape_list(tensor, expected_rank=None, name=None):\n    """"""Returns a list of the shape of tensor, preferring static dimensions.\n\n    Args:\n      tensor: A tf.Tensor object to find the shape of.\n      expected_rank: (optional) int. The expected rank of `tensor`. If this is\n        specified and the `tensor` has a different rank, and exception will be\n        thrown.\n      name: Optional name of the tensor for the error message.\n\n    Returns:\n      A list of dimensions of the shape of tensor. All static dimensions will\n      be returned as python integers, and dynamic dimensions will be returned\n      as tf.Tensor scalars.\n    """"""\n    if name is None:\n        name = tensor.name\n\n    if expected_rank is not None:\n        assert_rank(tensor, expected_rank, name)\n\n    shape = tensor.shape.as_list()\n\n    non_static_indexes = []\n    for (index, dim) in enumerate(shape):\n        if dim is None:\n            non_static_indexes.append(index)\n\n    if not non_static_indexes:\n        return shape\n\n    dyn_shape = tf.shape(tensor)\n    for index in non_static_indexes:\n        shape[index] = dyn_shape[index]\n    return shape\n\n\ndef reshape_to_matrix(input_tensor):\n    """"""Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).""""""\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(""Input tensor must have at least rank 2. Shape = %s"" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n\ndef reshape_from_matrix(output_tensor, orig_shape_list):\n    """"""Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""""""\n    if len(orig_shape_list) == 2:\n        return output_tensor\n\n    output_shape = get_shape_list(output_tensor)\n\n    orig_dims = orig_shape_list[0:-1]\n    width = output_shape[-1]\n\n    return tf.reshape(output_tensor, orig_dims + [width])\n\n\ndef assert_rank(tensor, expected_rank, name=None):\n    """"""Raises an exception if the tensor rank is not of the expected rank.\n\n    Args:\n      tensor: A tf.Tensor to check the rank of.\n      expected_rank: Python integer or list of integers, expected rank.\n      name: Optional name of the tensor for the error message.\n\n    Raises:\n      ValueError: If the expected shape doesn\'t match the actual shape.\n    """"""\n    if name is None:\n        name = tensor.name\n\n    expected_rank_dict = {}\n    if isinstance(expected_rank, six.integer_types):\n        expected_rank_dict[expected_rank] = True\n    else:\n        for x in expected_rank:\n            expected_rank_dict[x] = True\n\n    actual_rank = tensor.shape.ndims\n    if actual_rank not in expected_rank_dict:\n        scope_name = tf.get_variable_scope().name\n        raise ValueError(\n            ""For the tensor `%s` in scope `%s`, the actual rank ""\n            ""`%d` (shape = %s) is not equal to the expected rank `%s`"" %\n            (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n'"
server/bert_serving/server/bert/optimization.py,25,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nimport tensorflow as tf\n\n\ndef create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n    """"""Creates an optimizer training op.""""""\n    global_step = tf.train.get_or_create_global_step()\n\n    learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n\n    # Implements linear decay of the learning rate.\n    learning_rate = tf.train.polynomial_decay(\n        learning_rate,\n        global_step,\n        num_train_steps,\n        end_learning_rate=0.0,\n        power=1.0,\n        cycle=False)\n\n    # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n    # learning rate will be `global_step/num_warmup_steps * init_lr`.\n    if num_warmup_steps:\n        global_steps_int = tf.cast(global_step, tf.int32)\n        warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n\n        global_steps_float = tf.cast(global_steps_int, tf.float32)\n        warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n\n        warmup_percent_done = global_steps_float / warmup_steps_float\n        warmup_learning_rate = init_lr * warmup_percent_done\n\n        is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n        learning_rate = (\n                (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n\n    # It is recommended that you use this optimizer for fine tuning, since this\n    # is how the model was trained (note that the Adam m/v variables are NOT\n    # loaded from init_checkpoint.)\n    optimizer = AdamWeightDecayOptimizer(\n        learning_rate=learning_rate,\n        weight_decay_rate=0.01,\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=1e-6,\n        exclude_from_weight_decay=[""LayerNorm"", ""layer_norm"", ""bias""])\n\n    if use_tpu:\n        optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n    tvars = tf.trainable_variables()\n    grads = tf.gradients(loss, tvars)\n\n    # This is how the model was pre-trained.\n    (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n\n    train_op = optimizer.apply_gradients(\n        zip(grads, tvars), global_step=global_step)\n\n    new_global_step = global_step + 1\n    train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n    return train_op\n\n\nclass AdamWeightDecayOptimizer(tf.train.Optimizer):\n    """"""A basic Adam optimizer that includes ""correct"" L2 weight decay.""""""\n\n    def __init__(self,\n                 learning_rate,\n                 weight_decay_rate=0.0,\n                 beta_1=0.9,\n                 beta_2=0.999,\n                 epsilon=1e-6,\n                 exclude_from_weight_decay=None,\n                 name=""AdamWeightDecayOptimizer""):\n        """"""Constructs a AdamWeightDecayOptimizer.""""""\n        super(AdamWeightDecayOptimizer, self).__init__(False, name)\n\n        self.learning_rate = learning_rate\n        self.weight_decay_rate = weight_decay_rate\n        self.beta_1 = beta_1\n        self.beta_2 = beta_2\n        self.epsilon = epsilon\n        self.exclude_from_weight_decay = exclude_from_weight_decay\n\n    def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n        """"""See base class.""""""\n        assignments = []\n        for (grad, param) in grads_and_vars:\n            if grad is None or param is None:\n                continue\n\n            param_name = self._get_variable_name(param.name)\n\n            m = tf.get_variable(\n                name=param_name + ""/adam_m"",\n                shape=param.shape.as_list(),\n                dtype=tf.float32,\n                trainable=False,\n                initializer=tf.zeros_initializer())\n            v = tf.get_variable(\n                name=param_name + ""/adam_v"",\n                shape=param.shape.as_list(),\n                dtype=tf.float32,\n                trainable=False,\n                initializer=tf.zeros_initializer())\n\n            # Standard Adam update.\n            next_m = (\n                    tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n            next_v = (\n                    tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n                                                              tf.square(grad)))\n\n            update = next_m / (tf.sqrt(next_v) + self.epsilon)\n\n            # Just adding the square of the weights to the loss function is *not*\n            # the correct way of using L2 regularization/weight decay with Adam,\n            # since that will interact with the m and v parameters in strange ways.\n            #\n            # Instead we want ot decay the weights in a manner that doesn\'t interact\n            # with the m/v parameters. This is equivalent to adding the square\n            # of the weights to the loss with plain (non-momentum) SGD.\n            if self._do_use_weight_decay(param_name):\n                update += self.weight_decay_rate * param\n\n            update_with_lr = self.learning_rate * update\n\n            next_param = param - update_with_lr\n\n            assignments.extend(\n                [param.assign(next_param),\n                 m.assign(next_m),\n                 v.assign(next_v)])\n        return tf.group(*assignments, name=name)\n\n    def _do_use_weight_decay(self, param_name):\n        """"""Whether to use L2 weight decay for `param_name`.""""""\n        if not self.weight_decay_rate:\n            return False\n        if self.exclude_from_weight_decay:\n            for r in self.exclude_from_weight_decay:\n                if re.search(r, param_name) is not None:\n                    return False\n        return True\n\n    def _get_variable_name(self, param_name):\n        """"""Get the variable name from the tensor name.""""""\n        m = re.match(""^(.*):\\\\d+$"", param_name)\n        if m is not None:\n            param_name = m.group(1)\n        return param_name\n'"
server/bert_serving/server/bert/tokenization.py,2,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport unicodedata\n\nimport six\nimport tensorflow as tf\n\n\ndef convert_to_unicode(text):\n    """"""Converts `text` to Unicode (if it\'s not already), assuming utf-8 input.""""""\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(""utf-8"", ""ignore"")\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text.decode(""utf-8"", ""ignore"")\n        elif isinstance(text, unicode):\n            return text\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    else:\n        raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef printable_text(text):\n    """"""Returns text encoded in a way suitable for print or `tf.logging`.""""""\n\n    # These functions want `str` for both Python2 and Python3, but in one case\n    # it\'s a Unicode string and in the other it\'s a byte string.\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(""utf-8"", ""ignore"")\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, unicode):\n            return text.encode(""utf-8"")\n        else:\n            raise ValueError(""Unsupported string type: %s"" % (type(text)))\n    else:\n        raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef load_vocab(vocab_file):\n    """"""Loads a vocabulary file into a dictionary.""""""\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, ""r"") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef convert_by_vocab(vocab, items):\n    """"""Converts a sequence of [tokens|ids] using the vocab.""""""\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n    return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n    return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n    """"""Runs basic whitespace cleaning and splitting on a peice of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass FullTokenizer(object):\n    """"""Runs end-to-end tokenziation.""""""\n\n    def __init__(self, vocab_file, do_lower_case=True):\n        self.vocab = load_vocab(vocab_file)\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n    def tokenize(self, text):\n        split_tokens = []\n        for token in self.basic_tokenizer.tokenize(text):\n            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                split_tokens.append(sub_token)\n\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        return convert_by_vocab(self.vocab, tokens)\n\n    def convert_ids_to_tokens(self, ids):\n        return convert_by_vocab(self.inv_vocab, ids)\n\n    def mark_unk_tokens(self, tokens, unk_token=\'[UNK]\'):\n        return [t if t in self.vocab else unk_token for t in tokens]\n\n\nclass BasicTokenizer(object):\n    """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n    def __init__(self, do_lower_case=True):\n        """"""Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        """"""\n        self.do_lower_case = do_lower_case\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text.""""""\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn\'t\n        # matter since the English models were not trained on any Chinese data\n        # and generally don\'t have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        text = self._tokenize_chinese_chars(text)\n\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        """"""Strips accents from a piece of text.""""""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    def _run_split_on_punc(self, text):\n        """"""Splits punctuation on a piece of text.""""""\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        """"""Adds whitespace around any CJK character.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append("" "")\n                output.append(char)\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n    def _is_chinese_char(self, cp):\n        """"""Checks whether CP is the codepoint of a CJK character.""""""\n        # This defines a ""chinese character"" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        """"""Performs invalid character removal and whitespace cleanup on text.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n    """"""Runs WordPiece tokenziation.""""""\n\n    def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=100):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = ""unaffable""\n          output = [""un"", ""##aff"", ""##able""]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n\n        Returns:\n          A list of wordpiece tokens.\n        """"""\n\n        text = convert_to_unicode(text)\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    """"""Checks whether `chars` is a whitespace character.""""""\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char):\n    """"""Checks whether `chars` is a control character.""""""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(""C""):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    """"""Checks whether `chars` is a punctuation character.""""""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n'"
server/bert_serving/server/cli/__init__.py,0,"b'def main():\n    from bert_serving.server import BertServer\n    from bert_serving.server.helper import get_run_args\n    with BertServer(get_run_args()) as server:\n        server.join()\n\n\ndef benchmark():\n    from bert_serving.server.benchmark import run_benchmark\n    from bert_serving.server.helper import get_run_args, get_benchmark_parser\n    args = get_run_args(get_benchmark_parser)\n    run_benchmark(args)\n\n\ndef terminate():\n    from bert_serving.server import BertServer\n    from bert_serving.server.helper import get_run_args, get_shutdown_parser\n    args = get_run_args(get_shutdown_parser)\n    BertServer.shutdown(args)\n'"
