file_path,api_count,code
layers.py,16,"b'import tensorflow as tf\n\ndef lrelu(x, leak=0.2, name=""lrelu"", alt_relu_impl=False):\n\n    with tf.variable_scope(name):\n        if alt_relu_impl:\n            f1 = 0.5 * (1 + leak)\n            f2 = 0.5 * (1 - leak)\n            # lrelu = 1/2 * (1 + leak) * x + 1/2 * (1 - leak) * |x|\n            return f1 * x + f2 * abs(x)\n        else:\n            return tf.maximum(x, leak*x)\n\ndef instance_norm(x):\n\n    with tf.variable_scope(""instance_norm""):\n        epsilon = 1e-5\n        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\n        scale = tf.get_variable(\'scale\',[x.get_shape()[-1]], \n            initializer=tf.truncated_normal_initializer(mean=1.0, stddev=0.02))\n        offset = tf.get_variable(\'offset\',[x.get_shape()[-1]],initializer=tf.constant_initializer(0.0))\n        out = scale*tf.div(x-mean, tf.sqrt(var+epsilon)) + offset\n\n        return out\n\n\ndef general_conv2d(inputconv, o_d=64, f_h=7, f_w=7, s_h=1, s_w=1, stddev=0.02, padding=""VALID"", name=""conv2d"", do_norm=True, do_relu=True, relufactor=0):\n    with tf.variable_scope(name):\n        \n        conv = tf.contrib.layers.conv2d(inputconv, o_d, f_w, s_w, padding, activation_fn=None, weights_initializer=tf.truncated_normal_initializer(stddev=stddev),biases_initializer=tf.constant_initializer(0.0))\n        if do_norm:\n            conv = instance_norm(conv)\n            # conv = tf.contrib.layers.batch_norm(conv, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, scope=""batch_norm"")\n            \n        if do_relu:\n            if(relufactor == 0):\n                conv = tf.nn.relu(conv,""relu"")\n            else:\n                conv = lrelu(conv, relufactor, ""lrelu"")\n\n        return conv\n\n\n\ndef general_deconv2d(inputconv, outshape, o_d=64, f_h=7, f_w=7, s_h=1, s_w=1, stddev=0.02, padding=""VALID"", name=""deconv2d"", do_norm=True, do_relu=True, relufactor=0):\n    with tf.variable_scope(name):\n\n        conv = tf.contrib.layers.conv2d_transpose(inputconv, o_d, [f_h, f_w], [s_h, s_w], padding, activation_fn=None, weights_initializer=tf.truncated_normal_initializer(stddev=stddev),biases_initializer=tf.constant_initializer(0.0))\n        \n        if do_norm:\n            conv = instance_norm(conv)\n            # conv = tf.contrib.layers.batch_norm(conv, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, scope=""batch_norm"")\n            \n        if do_relu:\n            if(relufactor == 0):\n                conv = tf.nn.relu(conv,""relu"")\n            else:\n                conv = lrelu(conv, relufactor, ""lrelu"")\n\n        return conv\n'"
main.py,39,"b'import tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nimport numpy as np\r\nfrom scipy.misc import imsave\r\nimport os\r\nimport shutil\r\nfrom PIL import Image\r\nimport time\r\nimport random\r\nimport sys\r\n\r\n\r\nfrom layers import *\r\nfrom model import *\r\n\r\nimg_height = 256\r\nimg_width = 256\r\nimg_layer = 3\r\nimg_size = img_height * img_width\r\n\r\nto_train = True\r\nto_test = False\r\nto_restore = False\r\noutput_path = ""./output""\r\ncheck_dir = ""./output/checkpoints/""\r\n\r\n\r\ntemp_check = 0\r\n\r\n\r\n\r\nmax_epoch = 1\r\nmax_images = 100\r\n\r\nh1_size = 150\r\nh2_size = 300\r\nz_size = 100\r\nbatch_size = 1\r\npool_size = 50\r\nsample_size = 10\r\nsave_training_images = True\r\nngf = 32\r\nndf = 64\r\n\r\nclass CycleGAN():\r\n\r\n    def input_setup(self):\r\n\r\n        \'\'\' \r\n        This function basically setup variables for taking image input.\r\n\r\n        filenames_A/filenames_B -> takes the list of all training images\r\n        self.image_A/self.image_B -> Input image with each values ranging from [-1,1]\r\n        \'\'\'\r\n\r\n        filenames_A = tf.train.match_filenames_once(""./input/horse2zebra/trainA/*.jpg"")    \r\n        self.queue_length_A = tf.size(filenames_A)\r\n        filenames_B = tf.train.match_filenames_once(""./input/horse2zebra/trainB/*.jpg"")    \r\n        self.queue_length_B = tf.size(filenames_B)\r\n        \r\n        filename_queue_A = tf.train.string_input_producer(filenames_A)\r\n        filename_queue_B = tf.train.string_input_producer(filenames_B)\r\n\r\n        image_reader = tf.WholeFileReader()\r\n        _, image_file_A = image_reader.read(filename_queue_A)\r\n        _, image_file_B = image_reader.read(filename_queue_B)\r\n\r\n        self.image_A = tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(image_file_A),[256,256]),127.5),1)\r\n        self.image_B = tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(image_file_B),[256,256]),127.5),1)\r\n\r\n    \r\n\r\n    def input_read(self, sess):\r\n\r\n\r\n        \'\'\'\r\n        It reads the input into from the image folder.\r\n\r\n        self.fake_images_A/self.fake_images_B -> List of generated images used for calculation of loss function of Discriminator\r\n        self.A_input/self.B_input -> Stores all the training images in python list\r\n        \'\'\'\r\n\r\n        # Loading images into the tensors\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(coord=coord)\r\n\r\n        num_files_A = sess.run(self.queue_length_A)\r\n        num_files_B = sess.run(self.queue_length_B)\r\n\r\n        self.fake_images_A = np.zeros((pool_size,1,img_height, img_width, img_layer))\r\n        self.fake_images_B = np.zeros((pool_size,1,img_height, img_width, img_layer))\r\n\r\n\r\n        self.A_input = np.zeros((max_images, batch_size, img_height, img_width, img_layer))\r\n        self.B_input = np.zeros((max_images, batch_size, img_height, img_width, img_layer))\r\n\r\n        for i in range(max_images): \r\n            image_tensor = sess.run(self.image_A)\r\n            if(image_tensor.size() == img_size*batch_size*img_layer):\r\n                self.A_input[i] = image_tensor.reshape((batch_size,img_height, img_width, img_layer))\r\n\r\n        for i in range(max_images):\r\n            image_tensor = sess.run(self.image_B)\r\n            if(image_tensor.size() == img_size*batch_size*img_layer):\r\n                self.B_input[i] = image_tensor.reshape((batch_size,img_height, img_width, img_layer))\r\n\r\n\r\n        coord.request_stop()\r\n        coord.join(threads)\r\n\r\n\r\n\r\n\r\n    def model_setup(self):\r\n\r\n        \'\'\' This function sets up the model to train\r\n\r\n        self.input_A/self.input_B -> Set of training images.\r\n        self.fake_A/self.fake_B -> Generated images by corresponding generator of input_A and input_B\r\n        self.lr -> Learning rate variable\r\n        self.cyc_A/ self.cyc_B -> Images generated after feeding self.fake_A/self.fake_B to corresponding generator. This is use to calcualte cyclic loss\r\n        \'\'\'\r\n\r\n        self.input_A = tf.placeholder(tf.float32, [batch_size, img_width, img_height, img_layer], name=""input_A"")\r\n        self.input_B = tf.placeholder(tf.float32, [batch_size, img_width, img_height, img_layer], name=""input_B"")\r\n        \r\n        self.fake_pool_A = tf.placeholder(tf.float32, [None, img_width, img_height, img_layer], name=""fake_pool_A"")\r\n        self.fake_pool_B = tf.placeholder(tf.float32, [None, img_width, img_height, img_layer], name=""fake_pool_B"")\r\n\r\n        self.global_step = tf.Variable(0, name=""global_step"", trainable=False)\r\n\r\n        self.num_fake_inputs = 0\r\n\r\n        self.lr = tf.placeholder(tf.float32, shape=[], name=""lr"")\r\n\r\n        with tf.variable_scope(""Model"") as scope:\r\n            self.fake_B = build_generator_resnet_9blocks(self.input_A, name=""g_A"")\r\n            self.fake_A = build_generator_resnet_9blocks(self.input_B, name=""g_B"")\r\n            self.rec_A = build_gen_discriminator(self.input_A, ""d_A"")\r\n            self.rec_B = build_gen_discriminator(self.input_B, ""d_B"")\r\n\r\n            scope.reuse_variables()\r\n\r\n            self.fake_rec_A = build_gen_discriminator(self.fake_A, ""d_A"")\r\n            self.fake_rec_B = build_gen_discriminator(self.fake_B, ""d_B"")\r\n            self.cyc_A = build_generator_resnet_9blocks(self.fake_B, ""g_B"")\r\n            self.cyc_B = build_generator_resnet_9blocks(self.fake_A, ""g_A"")\r\n\r\n            scope.reuse_variables()\r\n\r\n            self.fake_pool_rec_A = build_gen_discriminator(self.fake_pool_A, ""d_A"")\r\n            self.fake_pool_rec_B = build_gen_discriminator(self.fake_pool_B, ""d_B"")\r\n\r\n    def loss_calc(self):\r\n\r\n        \'\'\' In this function we are defining the variables for loss calcultions and traning model\r\n\r\n        d_loss_A/d_loss_B -> loss for discriminator A/B\r\n        g_loss_A/g_loss_B -> loss for generator A/B\r\n        *_trainer -> Variaous trainer for above loss functions\r\n        *_summ -> Summary variables for above loss functions\'\'\'\r\n\r\n        cyc_loss = tf.reduce_mean(tf.abs(self.input_A-self.cyc_A)) + tf.reduce_mean(tf.abs(self.input_B-self.cyc_B))\r\n        \r\n        disc_loss_A = tf.reduce_mean(tf.squared_difference(self.fake_rec_A,1))\r\n        disc_loss_B = tf.reduce_mean(tf.squared_difference(self.fake_rec_B,1))\r\n        \r\n        g_loss_A = cyc_loss*10 + disc_loss_B\r\n        g_loss_B = cyc_loss*10 + disc_loss_A\r\n\r\n        d_loss_A = (tf.reduce_mean(tf.square(self.fake_pool_rec_A)) + tf.reduce_mean(tf.squared_difference(self.rec_A,1)))/2.0\r\n        d_loss_B = (tf.reduce_mean(tf.square(self.fake_pool_rec_B)) + tf.reduce_mean(tf.squared_difference(self.rec_B,1)))/2.0\r\n\r\n        \r\n        optimizer = tf.train.AdamOptimizer(self.lr, beta1=0.5)\r\n\r\n        self.model_vars = tf.trainable_variables()\r\n\r\n        d_A_vars = [var for var in self.model_vars if \'d_A\' in var.name]\r\n        g_A_vars = [var for var in self.model_vars if \'g_A\' in var.name]\r\n        d_B_vars = [var for var in self.model_vars if \'d_B\' in var.name]\r\n        g_B_vars = [var for var in self.model_vars if \'g_B\' in var.name]\r\n        \r\n        self.d_A_trainer = optimizer.minimize(d_loss_A, var_list=d_A_vars)\r\n        self.d_B_trainer = optimizer.minimize(d_loss_B, var_list=d_B_vars)\r\n        self.g_A_trainer = optimizer.minimize(g_loss_A, var_list=g_A_vars)\r\n        self.g_B_trainer = optimizer.minimize(g_loss_B, var_list=g_B_vars)\r\n\r\n        for var in self.model_vars: print(var.name)\r\n\r\n        #Summary variables for tensorboard\r\n\r\n        self.g_A_loss_summ = tf.summary.scalar(""g_A_loss"", g_loss_A)\r\n        self.g_B_loss_summ = tf.summary.scalar(""g_B_loss"", g_loss_B)\r\n        self.d_A_loss_summ = tf.summary.scalar(""d_A_loss"", d_loss_A)\r\n        self.d_B_loss_summ = tf.summary.scalar(""d_B_loss"", d_loss_B)\r\n\r\n    def save_training_images(self, sess, epoch):\r\n\r\n        if not os.path.exists(""./output/imgs""):\r\n            os.makedirs(""./output/imgs"")\r\n\r\n        for i in range(0,10):\r\n            fake_A_temp, fake_B_temp, cyc_A_temp, cyc_B_temp = sess.run([self.fake_A, self.fake_B, self.cyc_A, self.cyc_B],feed_dict={self.input_A:self.A_input[i], self.input_B:self.B_input[i]})\r\n            imsave(""./output/imgs/fakeB_""+ str(epoch) + ""_"" + str(i)+"".jpg"",((fake_A_temp[0]+1)*127.5).astype(np.uint8))\r\n            imsave(""./output/imgs/fakeA_""+ str(epoch) + ""_"" + str(i)+"".jpg"",((fake_B_temp[0]+1)*127.5).astype(np.uint8))\r\n            imsave(""./output/imgs/cycA_""+ str(epoch) + ""_"" + str(i)+"".jpg"",((cyc_A_temp[0]+1)*127.5).astype(np.uint8))\r\n            imsave(""./output/imgs/cycB_""+ str(epoch) + ""_"" + str(i)+"".jpg"",((cyc_B_temp[0]+1)*127.5).astype(np.uint8))\r\n            imsave(""./output/imgs/inputA_""+ str(epoch) + ""_"" + str(i)+"".jpg"",((self.A_input[i][0]+1)*127.5).astype(np.uint8))\r\n            imsave(""./output/imgs/inputB_""+ str(epoch) + ""_"" + str(i)+"".jpg"",((self.B_input[i][0]+1)*127.5).astype(np.uint8))\r\n\r\n    def fake_image_pool(self, num_fakes, fake, fake_pool):\r\n        \'\'\' This function saves the generated image to corresponding pool of images.\r\n        In starting. It keeps on feeling the pool till it is full and then randomly selects an\r\n        already stored image and replace it with new one.\'\'\'\r\n\r\n        if(num_fakes < pool_size):\r\n            fake_pool[num_fakes] = fake\r\n            return fake\r\n        else :\r\n            p = random.random()\r\n            if p > 0.5:\r\n                random_id = random.randint(0,pool_size-1)\r\n                temp = fake_pool[random_id]\r\n                fake_pool[random_id] = fake\r\n                return temp\r\n            else :\r\n                return fake\r\n\r\n\r\n    def train(self):\r\n\r\n\r\n        \'\'\' Training Function \'\'\'\r\n\r\n\r\n        # Load Dataset from the dataset folder\r\n        self.input_setup()  \r\n\r\n        #Build the network\r\n        self.model_setup()\r\n\r\n        #Loss function calculations\r\n        self.loss_calc()\r\n      \r\n        # Initializing the global variables\r\n        init = tf.global_variables_initializer()\r\n        saver = tf.train.Saver()     \r\n\r\n        with tf.Session() as sess:\r\n            sess.run(init)\r\n\r\n            #Read input to nd array\r\n            self.input_read(sess)\r\n\r\n            #Restore the model to run the model from last checkpoint\r\n            if to_restore:\r\n                chkpt_fname = tf.train.latest_checkpoint(check_dir)\r\n                saver.restore(sess, chkpt_fname)\r\n\r\n            writer = tf.summary.FileWriter(""./output/2"")\r\n\r\n            if not os.path.exists(check_dir):\r\n                os.makedirs(check_dir)\r\n\r\n            # Training Loop\r\n            for epoch in range(sess.run(self.global_step),100):                \r\n                print (""In the epoch "", epoch)\r\n                saver.save(sess,os.path.join(check_dir,""cyclegan""),global_step=epoch)\r\n\r\n                # Dealing with the learning rate as per the epoch number\r\n                if(epoch < 100) :\r\n                    curr_lr = 0.0002\r\n                else:\r\n                    curr_lr = 0.0002 - 0.0002*(epoch-100)/100\r\n\r\n                if(save_training_images):\r\n                    self.save_training_images(sess, epoch)\r\n\r\n                # sys.exit()\r\n\r\n                for ptr in range(0,max_images):\r\n                    print(""In the iteration "",ptr)\r\n                    print(""Starting"",time.time()*1000.0)\r\n\r\n                    # Optimizing the G_A network\r\n\r\n                    _, fake_B_temp, summary_str = sess.run([self.g_A_trainer, self.fake_B, self.g_A_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr})\r\n                    \r\n                    writer.add_summary(summary_str, epoch*max_images + ptr)                    \r\n                    fake_B_temp1 = self.fake_image_pool(self.num_fake_inputs, fake_B_temp, self.fake_images_B)\r\n                    \r\n                    # Optimizing the D_B network\r\n                    _, summary_str = sess.run([self.d_B_trainer, self.d_B_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr, self.fake_pool_B:fake_B_temp1})\r\n                    writer.add_summary(summary_str, epoch*max_images + ptr)\r\n                    \r\n                    \r\n                    # Optimizing the G_B network\r\n                    _, fake_A_temp, summary_str = sess.run([self.g_B_trainer, self.fake_A, self.g_B_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr})\r\n\r\n                    writer.add_summary(summary_str, epoch*max_images + ptr)\r\n                    \r\n                    \r\n                    fake_A_temp1 = self.fake_image_pool(self.num_fake_inputs, fake_A_temp, self.fake_images_A)\r\n\r\n                    # Optimizing the D_A network\r\n                    _, summary_str = sess.run([self.d_A_trainer, self.d_A_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr, self.fake_pool_A:fake_A_temp1})\r\n\r\n                    writer.add_summary(summary_str, epoch*max_images + ptr)\r\n                    \r\n                    self.num_fake_inputs+=1\r\n            \r\n                        \r\n\r\n                sess.run(tf.assign(self.global_step, epoch + 1))\r\n\r\n            writer.add_graph(sess.graph)\r\n\r\n    def test(self):\r\n\r\n\r\n        \'\'\' Testing Function\'\'\'\r\n\r\n        print(""Testing the results"")\r\n\r\n        self.input_setup()\r\n\r\n        self.model_setup()\r\n        saver = tf.train.Saver()\r\n        init = tf.global_variables_initializer()\r\n\r\n        with tf.Session() as sess:\r\n\r\n            sess.run(init)\r\n\r\n            self.input_read(sess)\r\n\r\n            chkpt_fname = tf.train.latest_checkpoint(check_dir)\r\n            saver.restore(sess, chkpt_fname)\r\n\r\n            if not os.path.exists(""./output/imgs/test/""):\r\n                os.makedirs(""./output/imgs/test/"")            \r\n\r\n            for i in range(0,100):\r\n                fake_A_temp, fake_B_temp = sess.run([self.fake_A, self.fake_B],feed_dict={self.input_A:self.A_input[i], self.input_B:self.B_input[i]})\r\n                imsave(""./output/imgs/test/fakeB_""+str(i)+"".jpg"",((fake_A_temp[0]+1)*127.5).astype(np.uint8))\r\n                imsave(""./output/imgs/test/fakeA_""+str(i)+"".jpg"",((fake_B_temp[0]+1)*127.5).astype(np.uint8))\r\n                imsave(""./output/imgs/test/inputA_""+str(i)+"".jpg"",((self.A_input[i][0]+1)*127.5).astype(np.uint8))\r\n                imsave(""./output/imgs/test/inputB_""+str(i)+"".jpg"",((self.B_input[i][0]+1)*127.5).astype(np.uint8))\r\n\r\n\r\ndef main():\r\n    \r\n    model = CycleGAN()\r\n    if to_train:\r\n        model.train()\r\n    elif to_test:\r\n        model.test()\r\n\r\nif __name__ == \'__main__\':\r\n\r\n    main()'"
model.py,14,"b'# Basic Code is taken from https://github.com/ckmarkoh/GAN-tensorflow\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport numpy as np\nfrom scipy.misc import imsave\nimport os\nimport shutil\nfrom PIL import Image\nimport time\nimport random\n\n\nfrom layers import *\n\nimg_height = 256\nimg_width = 256\nimg_layer = 3\nimg_size = img_height * img_width\n\n\nbatch_size = 1\npool_size = 50\nngf = 32\nndf = 64\n\n\n\n\n\ndef build_resnet_block(inputres, dim, name=""resnet""):\n    \n    with tf.variable_scope(name):\n\n        out_res = tf.pad(inputres, [[0, 0], [1, 1], [1, 1], [0, 0]], ""REFLECT"")\n        out_res = general_conv2d(out_res, dim, 3, 3, 1, 1, 0.02, ""VALID"",""c1"")\n        out_res = tf.pad(out_res, [[0, 0], [1, 1], [1, 1], [0, 0]], ""REFLECT"")\n        out_res = general_conv2d(out_res, dim, 3, 3, 1, 1, 0.02, ""VALID"",""c2"",do_relu=False)\n        \n        return tf.nn.relu(out_res + inputres)\n\n\ndef build_generator_resnet_6blocks(inputgen, name=""generator""):\n    with tf.variable_scope(name):\n        f = 7\n        ks = 3\n        \n        pad_input = tf.pad(inputgen,[[0, 0], [ks, ks], [ks, ks], [0, 0]], ""REFLECT"")\n        o_c1 = general_conv2d(pad_input, ngf, f, f, 1, 1, 0.02,name=""c1"")\n        o_c2 = general_conv2d(o_c1, ngf*2, ks, ks, 2, 2, 0.02,""SAME"",""c2"")\n        o_c3 = general_conv2d(o_c2, ngf*4, ks, ks, 2, 2, 0.02,""SAME"",""c3"")\n\n        o_r1 = build_resnet_block(o_c3, ngf*4, ""r1"")\n        o_r2 = build_resnet_block(o_r1, ngf*4, ""r2"")\n        o_r3 = build_resnet_block(o_r2, ngf*4, ""r3"")\n        o_r4 = build_resnet_block(o_r3, ngf*4, ""r4"")\n        o_r5 = build_resnet_block(o_r4, ngf*4, ""r5"")\n        o_r6 = build_resnet_block(o_r5, ngf*4, ""r6"")\n\n        o_c4 = general_deconv2d(o_r6, [batch_size,64,64,ngf*2], ngf*2, ks, ks, 2, 2, 0.02,""SAME"",""c4"")\n        o_c5 = general_deconv2d(o_c4, [batch_size,128,128,ngf], ngf, ks, ks, 2, 2, 0.02,""SAME"",""c5"")\n        o_c5_pad = tf.pad(o_c5,[[0, 0], [ks, ks], [ks, ks], [0, 0]], ""REFLECT"")\n        o_c6 = general_conv2d(o_c5_pad, img_layer, f, f, 1, 1, 0.02,""VALID"",""c6"",do_relu=False)\n\n        # Adding the tanh layer\n\n        out_gen = tf.nn.tanh(o_c6,""t1"")\n\n\n        return out_gen\n\ndef build_generator_resnet_9blocks(inputgen, name=""generator""):\n    with tf.variable_scope(name):\n        f = 7\n        ks = 3\n        \n        pad_input = tf.pad(inputgen,[[0, 0], [ks, ks], [ks, ks], [0, 0]], ""REFLECT"")\n        o_c1 = general_conv2d(pad_input, ngf, f, f, 1, 1, 0.02,name=""c1"")\n        o_c2 = general_conv2d(o_c1, ngf*2, ks, ks, 2, 2, 0.02,""SAME"",""c2"")\n        o_c3 = general_conv2d(o_c2, ngf*4, ks, ks, 2, 2, 0.02,""SAME"",""c3"")\n\n        o_r1 = build_resnet_block(o_c3, ngf*4, ""r1"")\n        o_r2 = build_resnet_block(o_r1, ngf*4, ""r2"")\n        o_r3 = build_resnet_block(o_r2, ngf*4, ""r3"")\n        o_r4 = build_resnet_block(o_r3, ngf*4, ""r4"")\n        o_r5 = build_resnet_block(o_r4, ngf*4, ""r5"")\n        o_r6 = build_resnet_block(o_r5, ngf*4, ""r6"")\n        o_r7 = build_resnet_block(o_r6, ngf*4, ""r7"")\n        o_r8 = build_resnet_block(o_r7, ngf*4, ""r8"")\n        o_r9 = build_resnet_block(o_r8, ngf*4, ""r9"")\n\n        o_c4 = general_deconv2d(o_r9, [batch_size,128,128,ngf*2], ngf*2, ks, ks, 2, 2, 0.02,""SAME"",""c4"")\n        o_c5 = general_deconv2d(o_c4, [batch_size,256,256,ngf], ngf, ks, ks, 2, 2, 0.02,""SAME"",""c5"")\n        o_c6 = general_conv2d(o_c5, img_layer, f, f, 1, 1, 0.02,""SAME"",""c6"",do_relu=False)\n\n        # Adding the tanh layer\n\n        out_gen = tf.nn.tanh(o_c6,""t1"")\n\n\n        return out_gen\n\n\ndef build_gen_discriminator(inputdisc, name=""discriminator""):\n\n    with tf.variable_scope(name):\n        f = 4\n\n        o_c1 = general_conv2d(inputdisc, ndf, f, f, 2, 2, 0.02, ""SAME"", ""c1"", do_norm=False, relufactor=0.2)\n        o_c2 = general_conv2d(o_c1, ndf*2, f, f, 2, 2, 0.02, ""SAME"", ""c2"", relufactor=0.2)\n        o_c3 = general_conv2d(o_c2, ndf*4, f, f, 2, 2, 0.02, ""SAME"", ""c3"", relufactor=0.2)\n        o_c4 = general_conv2d(o_c3, ndf*8, f, f, 1, 1, 0.02, ""SAME"", ""c4"",relufactor=0.2)\n        o_c5 = general_conv2d(o_c4, 1, f, f, 1, 1, 0.02, ""SAME"", ""c5"",do_norm=False,do_relu=False)\n\n        return o_c5\n\n\ndef patch_discriminator(inputdisc, name=""discriminator""):\n\n    with tf.variable_scope(name):\n        f= 4\n\n        patch_input = tf.random_crop(inputdisc,[1,70,70,3])\n        o_c1 = general_conv2d(patch_input, ndf, f, f, 2, 2, 0.02, ""SAME"", ""c1"", do_norm=""False"", relufactor=0.2)\n        o_c2 = general_conv2d(o_c1, ndf*2, f, f, 2, 2, 0.02, ""SAME"", ""c2"", relufactor=0.2)\n        o_c3 = general_conv2d(o_c2, ndf*4, f, f, 2, 2, 0.02, ""SAME"", ""c3"", relufactor=0.2)\n        o_c4 = general_conv2d(o_c3, ndf*8, f, f, 2, 2, 0.02, ""SAME"", ""c4"", relufactor=0.2)\n        o_c5 = general_conv2d(o_c4, 1, f, f, 1, 1, 0.02, ""SAME"", ""c5"",do_norm=False,do_relu=False)\n\n        return o_c5'"
