file_path,api_count,code
data.py,0,"b'""""""\nClass for managing our data.\n""""""\nimport csv\nimport numpy as np\nimport random\nimport glob\nimport os.path\nimport sys\nimport operator\nimport threading\nfrom processor import process_image\nfrom keras.utils import to_categorical\n\nclass threadsafe_iterator:\n    def __init__(self, iterator):\n        self.iterator = iterator\n        self.lock = threading.Lock()\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        with self.lock:\n            return next(self.iterator)\n\ndef threadsafe_generator(func):\n    """"""Decorator""""""\n    def gen(*a, **kw):\n        return threadsafe_iterator(func(*a, **kw))\n    return gen\n\nclass DataSet():\n\n    def __init__(self, seq_length=40, class_limit=None, image_shape=(224, 224, 3)):\n        """"""Constructor.\n        seq_length = (int) the number of frames to consider\n        class_limit = (int) number of classes to limit the data to.\n            None = no limit.\n        """"""\n        self.seq_length = seq_length\n        self.class_limit = class_limit\n        self.sequence_path = os.path.join(\'data\', \'sequences\')\n        self.max_frames = 300  # max number of frames a video can have for us to use it\n\n        # Get the data.\n        self.data = self.get_data()\n\n        # Get the classes.\n        self.classes = self.get_classes()\n\n        # Now do some minor data cleaning.\n        self.data = self.clean_data()\n\n        self.image_shape = image_shape\n\n    @staticmethod\n    def get_data():\n        """"""Load our data from file.""""""\n        with open(os.path.join(\'data\', \'data_file.csv\'), \'r\') as fin:\n            reader = csv.reader(fin)\n            data = list(reader)\n\n        return data\n\n    def clean_data(self):\n        """"""Limit samples to greater than the sequence length and fewer\n        than N frames. Also limit it to classes we want to use.""""""\n        data_clean = []\n        for item in self.data:\n            if int(item[3]) >= self.seq_length and int(item[3]) <= self.max_frames \\\n                    and item[1] in self.classes:\n                data_clean.append(item)\n\n        return data_clean\n\n    def get_classes(self):\n        """"""Extract the classes from our data. If we want to limit them,\n        only return the classes we need.""""""\n        classes = []\n        for item in self.data:\n            if item[1] not in classes:\n                classes.append(item[1])\n\n        # Sort them.\n        classes = sorted(classes)\n\n        # Return.\n        if self.class_limit is not None:\n            return classes[:self.class_limit]\n        else:\n            return classes\n\n    def get_class_one_hot(self, class_str):\n        """"""Given a class as a string, return its number in the classes\n        list. This lets us encode and one-hot it for training.""""""\n        # Encode it first.\n        label_encoded = self.classes.index(class_str)\n\n        # Now one-hot it.\n        label_hot = to_categorical(label_encoded, len(self.classes))\n\n        assert len(label_hot) == len(self.classes)\n\n        return label_hot\n\n    def split_train_test(self):\n        """"""Split the data into train and test groups.""""""\n        train = []\n        test = []\n        for item in self.data:\n            if item[0] == \'train\':\n                train.append(item)\n            else:\n                test.append(item)\n        return train, test\n\n    def get_all_sequences_in_memory(self, train_test, data_type):\n        """"""\n        This is a mirror of our generator, but attempts to load everything into\n        memory so we can train way faster.\n        """"""\n        # Get the right dataset.\n        train, test = self.split_train_test()\n        data = train if train_test == \'train\' else test\n\n        print(""Loading %d samples into memory for %sing."" % (len(data), train_test))\n\n        X, y = [], []\n        for row in data:\n\n            if data_type == \'images\':\n                frames = self.get_frames_for_sample(row)\n                frames = self.rescale_list(frames, self.seq_length)\n\n                # Build the image sequence\n                sequence = self.build_image_sequence(frames)\n\n            else:\n                sequence = self.get_extracted_sequence(data_type, row)\n\n                if sequence is None:\n                    print(""Can\'t find sequence. Did you generate them?"")\n                    raise\n\n            X.append(sequence)\n            y.append(self.get_class_one_hot(row[1]))\n\n        return np.array(X), np.array(y)\n\n    @threadsafe_generator\n    def frame_generator(self, batch_size, train_test, data_type):\n        """"""Return a generator that we can use to train on. There are\n        a couple different things we can return:\n\n        data_type: \'features\', \'images\'\n        """"""\n        # Get the right dataset for the generator.\n        train, test = self.split_train_test()\n        data = train if train_test == \'train\' else test\n\n        print(""Creating %s generator with %d samples."" % (train_test, len(data)))\n\n        while 1:\n            X, y = [], []\n\n            # Generate batch_size samples.\n            for _ in range(batch_size):\n                # Reset to be safe.\n                sequence = None\n\n                # Get a random sample.\n                sample = random.choice(data)\n\n                # Check to see if we\'ve already saved this sequence.\n                if data_type is ""images"":\n                    # Get and resample frames.\n                    frames = self.get_frames_for_sample(sample)\n                    frames = self.rescale_list(frames, self.seq_length)\n\n                    # Build the image sequence\n                    sequence = self.build_image_sequence(frames)\n                else:\n                    # Get the sequence from disk.\n                    sequence = self.get_extracted_sequence(data_type, sample)\n\n                    if sequence is None:\n                        raise ValueError(""Can\'t find sequence. Did you generate them?"")\n\n                X.append(sequence)\n                y.append(self.get_class_one_hot(sample[1]))\n\n            yield np.array(X), np.array(y)\n\n    def build_image_sequence(self, frames):\n        """"""Given a set of frames (filenames), build our sequence.""""""\n        return [process_image(x, self.image_shape) for x in frames]\n\n    def get_extracted_sequence(self, data_type, sample):\n        """"""Get the saved extracted features.""""""\n        filename = sample[2]\n        path = os.path.join(self.sequence_path, filename + \'-\' + str(self.seq_length) + \\\n            \'-\' + data_type + \'.npy\')\n        if os.path.isfile(path):\n            return np.load(path)\n        else:\n            return None\n\n    def get_frames_by_filename(self, filename, data_type):\n        """"""Given a filename for one of our samples, return the data\n        the model needs to make predictions.""""""\n        # First, find the sample row.\n        sample = None\n        for row in self.data:\n            if row[2] == filename:\n                sample = row\n                break\n        if sample is None:\n            raise ValueError(""Couldn\'t find sample: %s"" % filename)\n\n        if data_type == ""images"":\n            # Get and resample frames.\n            frames = self.get_frames_for_sample(sample)\n            frames = self.rescale_list(frames, self.seq_length)\n            # Build the image sequence\n            sequence = self.build_image_sequence(frames)\n        else:\n            # Get the sequence from disk.\n            sequence = self.get_extracted_sequence(data_type, sample)\n\n            if sequence is None:\n                raise ValueError(""Can\'t find sequence. Did you generate them?"")\n\n        return sequence\n\n    @staticmethod\n    def get_frames_for_sample(sample):\n        """"""Given a sample row from the data file, get all the corresponding frame\n        filenames.""""""\n        path = os.path.join(\'data\', sample[0], sample[1])\n        filename = sample[2]\n        images = sorted(glob.glob(os.path.join(path, filename + \'*jpg\')))\n        return images\n\n    @staticmethod\n    def get_filename_from_image(filename):\n        parts = filename.split(os.path.sep)\n        return parts[-1].replace(\'.jpg\', \'\')\n\n    @staticmethod\n    def rescale_list(input_list, size):\n        """"""Given a list and a size, return a rescaled/samples list. For example,\n        if we want a list of size 5 and we have a list of size 25, return a new\n        list of size five which is every 5th element of the origina list.""""""\n        assert len(input_list) >= size\n\n        # Get the number to skip between iterations.\n        skip = len(input_list) // size\n\n        # Build our new output.\n        output = [input_list[i] for i in range(0, len(input_list), skip)]\n\n        # Cut off the last one if needed.\n        return output[:size]\n\n    def print_class_from_prediction(self, predictions, nb_to_return=5):\n        """"""Given a prediction, print the top classes.""""""\n        # Get the prediction for each label.\n        label_predictions = {}\n        for i, label in enumerate(self.classes):\n            label_predictions[label] = predictions[i]\n\n        # Now sort them.\n        sorted_lps = sorted(\n            label_predictions.items(),\n            key=operator.itemgetter(1),\n            reverse=True\n        )\n\n        # And return the top N.\n        for i, class_prediction in enumerate(sorted_lps):\n            if i > nb_to_return - 1 or class_prediction[1] == 0.0:\n                break\n            print(""%s: %.2f"" % (class_prediction[0], class_prediction[1]))\n'"
demo.py,0,"b'""""""\nGiven a video path and a saved model (checkpoint), produce classification\npredictions.\n\nNote that if using a model that requires features to be extracted, those\nfeatures must be extracted first.\n\nNote also that this is a rushed demo script to help a few people who have\nrequested it and so is quite ""rough"". :)\n""""""\nfrom keras.models import load_model\nfrom data import DataSet\nimport numpy as np\n\ndef predict(data_type, seq_length, saved_model, image_shape, video_name, class_limit):\n    model = load_model(saved_model)\n\n    # Get the data and process it.\n    if image_shape is None:\n        data = DataSet(seq_length=seq_length, class_limit=class_limit)\n    else:\n        data = DataSet(seq_length=seq_length, image_shape=image_shape,\n            class_limit=class_limit)\n    \n    # Extract the sample from the data.\n    sample = data.get_frames_by_filename(video_name, data_type)\n\n    # Predict!\n    prediction = model.predict(np.expand_dims(sample, axis=0))\n    print(prediction)\n    data.print_class_from_prediction(np.squeeze(prediction, axis=0))\n\ndef main():\n    # model can be one of lstm, lrcn, mlp, conv_3d, c3d.\n    model = \'lstm\'\n    # Must be a weights file.\n    saved_model = \'data/checkpoints/lstm-features.026-0.239.hdf5\'\n    # Sequence length must match the lengh used during training.\n    seq_length = 40\n    # Limit must match that used during training.\n    class_limit = 4\n\n    # Demo file. Must already be extracted & features generated (if model requires)\n    # Do not include the extension.\n    # Assumes it\'s in data/[train|test]/\n    # It also must be part of the train/test data.\n    # TODO Make this way more useful. It should take in the path to\n    # an actual video file, extract frames, generate sequences, etc.\n    #video_name = \'v_Archery_g04_c02\'\n    video_name = \'v_ApplyLipstick_g01_c01\'\n\n    # Chose images or features and image shape based on network.\n    if model in [\'conv_3d\', \'c3d\', \'lrcn\']:\n        data_type = \'images\'\n        image_shape = (80, 80, 3)\n    elif model in [\'lstm\', \'mlp\']:\n        data_type = \'features\'\n        image_shape = None\n    else:\n        raise ValueError(""Invalid model. See train.py for options."")\n\n    predict(data_type, seq_length, saved_model, image_shape, video_name, class_limit)\n\nif __name__ == \'__main__\':\n    main()\n'"
extract_features.py,0,"b'""""""\nThis script generates extracted features for each video, which other\nmodels make use of.\n\nYou can change you sequence length and limit to a set number of classes\nbelow.\n\nclass_limit is an integer that denotes the first N classes you want to\nextract features from. This is useful is you don\'t want to wait to\nextract all 101 classes. For instance, set class_limit = 8 to just\nextract features for the first 8 (alphabetical) classes in the dataset.\nThen set the same number when training models.\n""""""\nimport numpy as np\nimport os.path\nfrom data import DataSet\nfrom extractor import Extractor\nfrom tqdm import tqdm\n\n# Set defaults.\nseq_length = 40\nclass_limit = None  # Number of classes to extract. Can be 1-101 or None for all.\n\n# Get the dataset.\ndata = DataSet(seq_length=seq_length, class_limit=class_limit)\n\n# get the model.\nmodel = Extractor()\n\n# Loop through data.\npbar = tqdm(total=len(data.data))\nfor video in data.data:\n\n    # Get the path to the sequence for this video.\n    path = os.path.join(\'data\', \'sequences\', video[2] + \'-\' + str(seq_length) + \\\n        \'-features\')  # numpy will auto-append .npy\n\n    # Check if we already have it.\n    if os.path.isfile(path + \'.npy\'):\n        pbar.update(1)\n        continue\n\n    # Get the frames for this video.\n    frames = data.get_frames_for_sample(video)\n\n    # Now downsample to just the ones we need.\n    frames = data.rescale_list(frames, seq_length)\n\n    # Now loop through and extract features to build the sequence.\n    sequence = []\n    for image in frames:\n        features = model.extract(image)\n        sequence.append(features)\n\n    # Save the sequence.\n    np.save(path, sequence)\n\n    pbar.update(1)\n\npbar.close()\n'"
extractor.py,0,"b'from keras.preprocessing import image\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom keras.models import Model, load_model\nfrom keras.layers import Input\nimport numpy as np\n\nclass Extractor():\n    def __init__(self, weights=None):\n        """"""Either load pretrained from imagenet, or load our saved\n        weights from our own training.""""""\n\n        self.weights = weights  # so we can check elsewhere which model\n\n        if weights is None:\n            # Get model with pretrained weights.\n            base_model = InceptionV3(\n                weights=\'imagenet\',\n                include_top=True\n            )\n\n            # We\'ll extract features at the final pool layer.\n            self.model = Model(\n                inputs=base_model.input,\n                outputs=base_model.get_layer(\'avg_pool\').output\n            )\n\n        else:\n            # Load the model first.\n            self.model = load_model(weights)\n\n            # Then remove the top so we get features not predictions.\n            # From: https://github.com/fchollet/keras/issues/2371\n            self.model.layers.pop()\n            self.model.layers.pop()  # two pops to get to pool layer\n            self.model.outputs = [self.model.layers[-1].output]\n            self.model.output_layers = [self.model.layers[-1]]\n            self.model.layers[-1].outbound_nodes = []\n\n    def extract(self, image_path):\n        img = image.load_img(image_path, target_size=(299, 299))\n        x = image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n        x = preprocess_input(x)\n\n        # Get the prediction.\n        features = self.model.predict(x)\n\n        if self.weights is None:\n            # For imagenet/default network:\n            features = features[0]\n        else:\n            # For loaded network:\n            features = features[0]\n\n        return features\n'"
models.py,0,"b'""""""\nA collection of models we\'ll use to attempt to classify videos.\n""""""\nfrom keras.layers import Dense, Flatten, Dropout, ZeroPadding3D\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import Sequential, load_model\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.layers.convolutional import (Conv2D, MaxPooling3D, Conv3D,\n    MaxPooling2D)\nfrom collections import deque\nimport sys\n\nclass ResearchModels():\n    def __init__(self, nb_classes, model, seq_length,\n                 saved_model=None, features_length=2048):\n        """"""\n        `model` = one of:\n            lstm\n            lrcn\n            mlp\n            conv_3d\n            c3d\n        `nb_classes` = the number of classes to predict\n        `seq_length` = the length of our video sequences\n        `saved_model` = the path to a saved Keras model to load\n        """"""\n\n        # Set defaults.\n        self.seq_length = seq_length\n        self.load_model = load_model\n        self.saved_model = saved_model\n        self.nb_classes = nb_classes\n        self.feature_queue = deque()\n\n        # Set the metrics. Only use top k if there\'s a need.\n        metrics = [\'accuracy\']\n        if self.nb_classes >= 10:\n            metrics.append(\'top_k_categorical_accuracy\')\n\n        # Get the appropriate model.\n        if self.saved_model is not None:\n            print(""Loading model %s"" % self.saved_model)\n            self.model = load_model(self.saved_model)\n        elif model == \'lstm\':\n            print(""Loading LSTM model."")\n            self.input_shape = (seq_length, features_length)\n            self.model = self.lstm()\n        elif model == \'lrcn\':\n            print(""Loading CNN-LSTM model."")\n            self.input_shape = (seq_length, 80, 80, 3)\n            self.model = self.lrcn()\n        elif model == \'mlp\':\n            print(""Loading simple MLP."")\n            self.input_shape = (seq_length, features_length)\n            self.model = self.mlp()\n        elif model == \'conv_3d\':\n            print(""Loading Conv3D"")\n            self.input_shape = (seq_length, 80, 80, 3)\n            self.model = self.conv_3d()\n        elif model == \'c3d\':\n            print(""Loading C3D"")\n            self.input_shape = (seq_length, 80, 80, 3)\n            self.model = self.c3d()\n        else:\n            print(""Unknown network."")\n            sys.exit()\n\n        # Now compile the network.\n        optimizer = Adam(lr=1e-5, decay=1e-6)\n        self.model.compile(loss=\'categorical_crossentropy\', optimizer=optimizer,\n                           metrics=metrics)\n\n        print(self.model.summary())\n\n    def lstm(self):\n        """"""Build a simple LSTM network. We pass the extracted features from\n        our CNN to this model predomenently.""""""\n        # Model.\n        model = Sequential()\n        model.add(LSTM(2048, return_sequences=False,\n                       input_shape=self.input_shape,\n                       dropout=0.5))\n        model.add(Dense(512, activation=\'relu\'))\n        model.add(Dropout(0.5))\n        model.add(Dense(self.nb_classes, activation=\'softmax\'))\n\n        return model\n\n    def lrcn(self):\n        """"""Build a CNN into RNN.\n        Starting version from:\n            https://github.com/udacity/self-driving-car/blob/master/\n                steering-models/community-models/chauffeur/models.py\n\n        Heavily influenced by VGG-16:\n            https://arxiv.org/abs/1409.1556\n\n        Also known as an LRCN:\n            https://arxiv.org/pdf/1411.4389.pdf\n        """"""\n        def add_default_block(model, kernel_filters, init, reg_lambda):\n\n            # conv\n            model.add(TimeDistributed(Conv2D(kernel_filters, (3, 3), padding=\'same\',\n                                             kernel_initializer=init, kernel_regularizer=L2_reg(l=reg_lambda))))\n            model.add(TimeDistributed(BatchNormalization()))\n            model.add(TimeDistributed(Activation(\'relu\')))\n            # conv\n            model.add(TimeDistributed(Conv2D(kernel_filters, (3, 3), padding=\'same\',\n                                             kernel_initializer=init, kernel_regularizer=L2_reg(l=reg_lambda))))\n            model.add(TimeDistributed(BatchNormalization()))\n            model.add(TimeDistributed(Activation(\'relu\')))\n            # max pool\n            model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n\n            return model\n\n        initialiser = \'glorot_uniform\'\n        reg_lambda  = 0.001\n\n        model = Sequential()\n\n        # first (non-default) block\n        model.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2), padding=\'same\',\n                                         kernel_initializer=initialiser, kernel_regularizer=L2_reg(l=reg_lambda)),\n                                  input_shape=self.input_shape))\n        model.add(TimeDistributed(BatchNormalization()))\n        model.add(TimeDistributed(Activation(\'relu\')))\n        model.add(TimeDistributed(Conv2D(32, (3,3), kernel_initializer=initialiser, kernel_regularizer=L2_reg(l=reg_lambda))))\n        model.add(TimeDistributed(BatchNormalization()))\n        model.add(TimeDistributed(Activation(\'relu\')))\n        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n\n        # 2nd-5th (default) blocks\n        model = add_default_block(model, 64,  init=initialiser, reg_lambda=reg_lambda)\n        model = add_default_block(model, 128, init=initialiser, reg_lambda=reg_lambda)\n        model = add_default_block(model, 256, init=initialiser, reg_lambda=reg_lambda)\n        model = add_default_block(model, 512, init=initialiser, reg_lambda=reg_lambda)\n\n        # LSTM output head\n        model.add(TimeDistributed(Flatten()))\n        model.add(LSTM(256, return_sequences=False, dropout=0.5))\n        model.add(Dense(self.nb_classes, activation=\'softmax\'))\n\n        return model\n\n    def mlp(self):\n        """"""Build a simple MLP. It uses extracted features as the input\n        because of the otherwise too-high dimensionality.""""""\n        # Model.\n        model = Sequential()\n        model.add(Flatten(input_shape=self.input_shape))\n        model.add(Dense(512))\n        model.add(Dropout(0.5))\n        model.add(Dense(512))\n        model.add(Dropout(0.5))\n        model.add(Dense(self.nb_classes, activation=\'softmax\'))\n\n        return model\n\n    def conv_3d(self):\n        """"""\n        Build a 3D convolutional network, based loosely on C3D.\n            https://arxiv.org/pdf/1412.0767.pdf\n        """"""\n        # Model.\n        model = Sequential()\n        model.add(Conv3D(\n            32, (3,3,3), activation=\'relu\', input_shape=self.input_shape\n        ))\n        model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n        model.add(Conv3D(64, (3,3,3), activation=\'relu\'))\n        model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n        model.add(Conv3D(128, (3,3,3), activation=\'relu\'))\n        model.add(Conv3D(128, (3,3,3), activation=\'relu\'))\n        model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n        model.add(Conv3D(256, (2,2,2), activation=\'relu\'))\n        model.add(Conv3D(256, (2,2,2), activation=\'relu\'))\n        model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n\n        model.add(Flatten())\n        model.add(Dense(1024))\n        model.add(Dropout(0.5))\n        model.add(Dense(1024))\n        model.add(Dropout(0.5))\n        model.add(Dense(self.nb_classes, activation=\'softmax\'))\n\n        return model\n\n    def c3d(self):\n        """"""\n        Build a 3D convolutional network, aka C3D.\n            https://arxiv.org/pdf/1412.0767.pdf\n\n        With thanks:\n            https://gist.github.com/albertomontesg/d8b21a179c1e6cca0480ebdf292c34d2\n        """"""\n        model = Sequential()\n        # 1st layer group\n        model.add(Conv3D(64, 3, 3, 3, activation=\'relu\',\n                         border_mode=\'same\', name=\'conv1\',\n                         subsample=(1, 1, 1),\n                         input_shape=self.input_shape))\n        model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2),\n                               border_mode=\'valid\', name=\'pool1\'))\n        # 2nd layer group\n        model.add(Conv3D(128, 3, 3, 3, activation=\'relu\',\n                         border_mode=\'same\', name=\'conv2\',\n                         subsample=(1, 1, 1)))\n        model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),\n                               border_mode=\'valid\', name=\'pool2\'))\n        # 3rd layer group\n        model.add(Conv3D(256, 3, 3, 3, activation=\'relu\',\n                         border_mode=\'same\', name=\'conv3a\',\n                         subsample=(1, 1, 1)))\n        model.add(Conv3D(256, 3, 3, 3, activation=\'relu\',\n                         border_mode=\'same\', name=\'conv3b\',\n                         subsample=(1, 1, 1)))\n        model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),\n                               border_mode=\'valid\', name=\'pool3\'))\n        # 4th layer group\n        model.add(Conv3D(512, 3, 3, 3, activation=\'relu\',\n                         border_mode=\'same\', name=\'conv4a\',\n                         subsample=(1, 1, 1)))\n        model.add(Conv3D(512, 3, 3, 3, activation=\'relu\',\n                         border_mode=\'same\', name=\'conv4b\',\n                         subsample=(1, 1, 1)))\n        model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),\n                               border_mode=\'valid\', name=\'pool4\'))\n\n        # 5th layer group\n        model.add(Conv3D(512, 3, 3, 3, activation=\'relu\',\n                         border_mode=\'same\', name=\'conv5a\',\n                         subsample=(1, 1, 1)))\n        model.add(Conv3D(512, 3, 3, 3, activation=\'relu\',\n                         border_mode=\'same\', name=\'conv5b\',\n                         subsample=(1, 1, 1)))\n        model.add(ZeroPadding3D(padding=(0, 1, 1)))\n        model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),\n                               border_mode=\'valid\', name=\'pool5\'))\n        model.add(Flatten())\n\n        # FC layers group\n        model.add(Dense(4096, activation=\'relu\', name=\'fc6\'))\n        model.add(Dropout(0.5))\n        model.add(Dense(4096, activation=\'relu\', name=\'fc7\'))\n        model.add(Dropout(0.5))\n        model.add(Dense(self.nb_classes, activation=\'softmax\'))\n\n        return model\n'"
plot_trainlog.py,0,"b'""""""\nGiven a training log file, plot something.\n""""""\nimport csv\nimport matplotlib.pyplot as plt\n\ndef main(training_log):\n    with open(training_log) as fin:\n        reader = csv.reader(fin)\n        next(reader, None)  # skip the header\n        accuracies = []\n        top_5_accuracies = []\n        cnn_benchmark = []  # this is ridiculous\n        for epoch,acc,loss,top_k_categorical_accuracy,val_acc,val_loss,val_top_k_categorical_accuracy in reader:\n            accuracies.append(float(val_acc))\n            top_5_accuracies.append(float(val_top_k_categorical_accuracy))\n            cnn_benchmark.append(0.65)  # ridiculous\n\n        plt.plot(accuracies)\n        plt.plot(top_5_accuracies)\n        plt.plot(cnn_benchmark)\n        plt.show()\n\nif __name__ == \'__main__\':\n    training_log = \'data/logs/mlp-training-1489455559.7089438.log\'\n    main(training_log)\n'"
processor.py,0,"b'""""""\nProcess an image that we can pass to our networks.\n""""""\nfrom keras.preprocessing.image import img_to_array, load_img\nimport numpy as np\n\ndef process_image(image, target_shape):\n    """"""Given an image, process it and return the array.""""""\n    # Load the image.\n    h, w, _ = target_shape\n    image = load_img(image, target_size=(h, w))\n\n    # Turn it into numpy, normalize and return.\n    img_arr = img_to_array(image)\n    x = (img_arr / 255.).astype(np.float32)\n\n    return x\n'"
random_and_mode.py,0,"b'""""""\nTry to ""classify"" samples based on random chance and always guessing\nthe most popular category.\n""""""\nimport random\nfrom data import DataSet\n\nmost_pop = \'TennisSwing\'\n\ndata = DataSet()\nnb_classes = len(data.classes)\n\n# Try a random guess.\nnb_random_matched = 0\nnb_mode_matched = 0\nfor item in data.data:\n    choice = random.choice(data.classes)\n    actual = item[1]\n\n    if choice == actual:\n        nb_random_matched += 1\n\n    if actual == most_pop:\n        nb_mode_matched += 1\n\nrandom_accuracy = nb_random_matched / len(data.data)\nmode_accuracy = nb_mode_matched / len(data.data)\nprint(""Randomly matched %.2f%%"" % (random_accuracy * 100))\nprint(""Mode matched %.2f%%"" % (mode_accuracy * 100))\n'"
train.py,0,"b'""""""\nTrain our RNN on extracted features or images.\n""""""\nfrom keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\nfrom models import ResearchModels\nfrom data import DataSet\nimport time\nimport os.path\n\ndef train(data_type, seq_length, model, saved_model=None,\n          class_limit=None, image_shape=None,\n          load_to_memory=False, batch_size=32, nb_epoch=100):\n    # Helper: Save the model.\n    checkpointer = ModelCheckpoint(\n        filepath=os.path.join(\'data\', \'checkpoints\', model + \'-\' + data_type + \\\n            \'.{epoch:03d}-{val_loss:.3f}.hdf5\'),\n        verbose=1,\n        save_best_only=True)\n\n    # Helper: TensorBoard\n    tb = TensorBoard(log_dir=os.path.join(\'data\', \'logs\', model))\n\n    # Helper: Stop when we stop learning.\n    early_stopper = EarlyStopping(patience=5)\n\n    # Helper: Save results.\n    timestamp = time.time()\n    csv_logger = CSVLogger(os.path.join(\'data\', \'logs\', model + \'-\' + \'training-\' + \\\n        str(timestamp) + \'.log\'))\n\n    # Get the data and process it.\n    if image_shape is None:\n        data = DataSet(\n            seq_length=seq_length,\n            class_limit=class_limit\n        )\n    else:\n        data = DataSet(\n            seq_length=seq_length,\n            class_limit=class_limit,\n            image_shape=image_shape\n        )\n\n    # Get samples per epoch.\n    # Multiply by 0.7 to attempt to guess how much of data.data is the train set.\n    steps_per_epoch = (len(data.data) * 0.7) // batch_size\n\n    if load_to_memory:\n        # Get data.\n        X, y = data.get_all_sequences_in_memory(\'train\', data_type)\n        X_test, y_test = data.get_all_sequences_in_memory(\'test\', data_type)\n    else:\n        # Get generators.\n        generator = data.frame_generator(batch_size, \'train\', data_type)\n        val_generator = data.frame_generator(batch_size, \'test\', data_type)\n\n    # Get the model.\n    rm = ResearchModels(len(data.classes), model, seq_length, saved_model)\n\n    # Fit!\n    if load_to_memory:\n        # Use standard fit.\n        rm.model.fit(\n            X,\n            y,\n            batch_size=batch_size,\n            validation_data=(X_test, y_test),\n            verbose=1,\n            callbacks=[tb, early_stopper, csv_logger],\n            epochs=nb_epoch)\n    else:\n        # Use fit generator.\n        rm.model.fit_generator(\n            generator=generator,\n            steps_per_epoch=steps_per_epoch,\n            epochs=nb_epoch,\n            verbose=1,\n            callbacks=[tb, early_stopper, csv_logger, checkpointer],\n            validation_data=val_generator,\n            validation_steps=40,\n            workers=4)\n\ndef main():\n    """"""These are the main training settings. Set each before running\n    this file.""""""\n    # model can be one of lstm, lrcn, mlp, conv_3d, c3d\n    model = \'lstm\'\n    saved_model = None  # None or weights file\n    class_limit = None  # int, can be 1-101 or None\n    seq_length = 40\n    load_to_memory = False  # pre-load the sequences into memory\n    batch_size = 32\n    nb_epoch = 1000\n\n    # Chose images or features and image shape based on network.\n    if model in [\'conv_3d\', \'c3d\', \'lrcn\']:\n        data_type = \'images\'\n        image_shape = (80, 80, 3)\n    elif model in [\'lstm\', \'mlp\']:\n        data_type = \'features\'\n        image_shape = None\n    else:\n        raise ValueError(""Invalid model. See train.py for options."")\n\n    train(data_type, seq_length, model, saved_model=saved_model,\n          class_limit=class_limit, image_shape=image_shape,\n          load_to_memory=load_to_memory, batch_size=batch_size, nb_epoch=nb_epoch)\n\nif __name__ == \'__main__\':\n    main()\n'"
train_cnn.py,0,"b'""""""\nTrain on images split into directories. This assumes we\'ve split\nour videos into frames and moved them to their respective folders.\n\nBased on:\nhttps://keras.io/preprocessing/image/\nand\nhttps://keras.io/applications/\n""""""\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.optimizers import SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\nfrom data import DataSet\nimport os.path\n\ndata = DataSet()\n\n# Helper: Save the model.\ncheckpointer = ModelCheckpoint(\n    filepath=os.path.join(\'data\', \'checkpoints\', \'inception.{epoch:03d}-{val_loss:.2f}.hdf5\'),\n    verbose=1,\n    save_best_only=True)\n\n# Helper: Stop when we stop learning.\nearly_stopper = EarlyStopping(patience=10)\n\n# Helper: TensorBoard\ntensorboard = TensorBoard(log_dir=os.path.join(\'data\', \'logs\'))\n\ndef get_generators():\n    train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        horizontal_flip=True,\n        rotation_range=10.,\n        width_shift_range=0.2,\n        height_shift_range=0.2)\n\n    test_datagen = ImageDataGenerator(rescale=1./255)\n\n    train_generator = train_datagen.flow_from_directory(\n        os.path.join(\'data\', \'train\'),\n        target_size=(299, 299),\n        batch_size=32,\n        classes=data.classes,\n        class_mode=\'categorical\')\n\n    validation_generator = test_datagen.flow_from_directory(\n        os.path.join(\'data\', \'test\'),\n        target_size=(299, 299),\n        batch_size=32,\n        classes=data.classes,\n        class_mode=\'categorical\')\n\n    return train_generator, validation_generator\n\ndef get_model(weights=\'imagenet\'):\n    # create the base pre-trained model\n    base_model = InceptionV3(weights=weights, include_top=False)\n\n    # add a global spatial average pooling layer\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    # let\'s add a fully-connected layer\n    x = Dense(1024, activation=\'relu\')(x)\n    # and a logistic layer\n    predictions = Dense(len(data.classes), activation=\'softmax\')(x)\n\n    # this is the model we will train\n    model = Model(inputs=base_model.input, outputs=predictions)\n    return model\n\ndef freeze_all_but_top(model):\n    """"""Used to train just the top layers of the model.""""""\n    # first: train only the top layers (which were randomly initialized)\n    # i.e. freeze all convolutional InceptionV3 layers\n    for layer in model.layers[:-2]:\n        layer.trainable = False\n\n    # compile the model (should be done *after* setting layers to non-trainable)\n    model.compile(optimizer=\'rmsprop\', loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\n\n    return model\n\ndef freeze_all_but_mid_and_top(model):\n    """"""After we fine-tune the dense layers, train deeper.""""""\n    # we chose to train the top 2 inception blocks, i.e. we will freeze\n    # the first 172 layers and unfreeze the rest:\n    for layer in model.layers[:172]:\n        layer.trainable = False\n    for layer in model.layers[172:]:\n        layer.trainable = True\n\n    # we need to recompile the model for these modifications to take effect\n    # we use SGD with a low learning rate\n    model.compile(\n        optimizer=SGD(lr=0.0001, momentum=0.9),\n        loss=\'categorical_crossentropy\',\n        metrics=[\'accuracy\', \'top_k_categorical_accuracy\'])\n\n    return model\n\ndef train_model(model, nb_epoch, generators, callbacks=[]):\n    train_generator, validation_generator = generators\n    model.fit_generator(\n        train_generator,\n        steps_per_epoch=100,\n        validation_data=validation_generator,\n        validation_steps=10,\n        epochs=nb_epoch,\n        callbacks=callbacks)\n    return model\n\ndef main(weights_file):\n    model = get_model()\n    generators = get_generators()\n\n    if weights_file is None:\n        print(""Loading network from ImageNet weights."")\n        # Get and train the top layers.\n        model = freeze_all_but_top(model)\n        model = train_model(model, 10, generators)\n    else:\n        print(""Loading saved model: %s."" % weights_file)\n        model.load_weights(weights_file)\n\n    # Get and train the mid layers.\n    model = freeze_all_but_mid_and_top(model)\n    model = train_model(model, 1000, generators,\n                        [checkpointer, early_stopper, tensorboard])\n\nif __name__ == \'__main__\':\n    weights_file = None\n    main(weights_file)\n'"
validate_cnn.py,0,"b'""""""\nClassify a few images through our CNN.\n""""""\nimport numpy as np\nimport operator\nimport random\nimport glob\nimport os.path\nfrom data import DataSet\nfrom processor import process_image\nfrom keras.models import load_model\n\ndef main(nb_images=5):\n    """"""Spot-check `nb_images` images.""""""\n    data = DataSet()\n    model = load_model(\'data/checkpoints/inception.057-1.16.hdf5\')\n\n    # Get all our test images.\n    images = glob.glob(os.path.join(\'data\', \'test\', \'**\', \'*.jpg\'))\n\n    for _ in range(nb_images):\n        print(\'-\'*80)\n        # Get a random row.\n        sample = random.randint(0, len(images) - 1)\n        image = images[sample]\n\n        # Turn the image into an array.\n        print(image)\n        image_arr = process_image(image, (299, 299, 3))\n        image_arr = np.expand_dims(image_arr, axis=0)\n\n        # Predict.\n        predictions = model.predict(image_arr)\n\n        # Show how much we think it\'s each one.\n        label_predictions = {}\n        for i, label in enumerate(data.classes):\n            label_predictions[label] = predictions[0][i]\n\n        sorted_lps = sorted(label_predictions.items(), key=operator.itemgetter(1), reverse=True)\n        \n        for i, class_prediction in enumerate(sorted_lps):\n            # Just get the top five.\n            if i > 4:\n                break\n            print(""%s: %.2f"" % (class_prediction[0], class_prediction[1]))\n            i += 1\n\nif __name__ == \'__main__\':\n    main()\n'"
validate_rnn.py,0,"b'""""""\nValidate our RNN. Basically just runs a validation generator on\nabout the same number of videos as we have in our test set.\n""""""\nfrom keras.callbacks import TensorBoard, ModelCheckpoint, CSVLogger\nfrom models import ResearchModels\nfrom data import DataSet\n\ndef validate(data_type, model, seq_length=40, saved_model=None,\n             class_limit=None, image_shape=None):\n    batch_size = 32\n\n    # Get the data and process it.\n    if image_shape is None:\n        data = DataSet(\n            seq_length=seq_length,\n            class_limit=class_limit\n        )\n    else:\n        data = DataSet(\n            seq_length=seq_length,\n            class_limit=class_limit,\n            image_shape=image_shape\n        )\n\n    val_generator = data.frame_generator(batch_size, \'test\', data_type)\n\n    # Get the model.\n    rm = ResearchModels(len(data.classes), model, seq_length, saved_model)\n\n    # Evaluate!\n    results = rm.model.evaluate_generator(\n        generator=val_generator,\n        val_samples=3200)\n\n    print(results)\n    print(rm.model.metrics_names)\n\ndef main():\n    model = \'lstm\'\n    saved_model = \'data/checkpoints/lstm-features.026-0.239.hdf5\'\n\n    if model == \'conv_3d\' or model == \'lrcn\':\n        data_type = \'images\'\n        image_shape = (80, 80, 3)\n    else:\n        data_type = \'features\'\n        image_shape = None\n\n    validate(data_type, model, saved_model=saved_model,\n             image_shape=image_shape, class_limit=4)\n\nif __name__ == \'__main__\':\n    main()\n'"
data/1_move_files.py,0,"b'""""""\nAfter extracting the RAR, we run this to move all the files into\nthe appropriate train/test folders.\n\nShould only run this file once!\n""""""\nimport os\nimport os.path\n\ndef get_train_test_lists(version=\'01\'):\n    """"""\n    Using one of the train/test files (01, 02, or 03), get the filename\n    breakdowns we\'ll later use to move everything.\n    """"""\n    # Get our files based on version.\n    test_file = os.path.join(\'ucfTrainTestlist\', \'testlist\' + version + \'.txt\')\n    train_file = os.path.join(\'ucfTrainTestlist\', \'trainlist\' + version + \'.txt\')\n\n    # Build the test list.\n    with open(test_file) as fin:\n        test_list = [row.strip() for row in list(fin)]\n\n    # Build the train list. Extra step to remove the class index.\n    with open(train_file) as fin:\n        train_list = [row.strip() for row in list(fin)]\n        train_list = [row.split(\' \')[0] for row in train_list]\n\n    # Set the groups in a dictionary.\n    file_groups = {\n        \'train\': train_list,\n        \'test\': test_list\n    }\n\n    return file_groups\n\ndef move_files(file_groups):\n    """"""This assumes all of our files are currently in _this_ directory.\n    So move them to the appropriate spot. Only needs to happen once.\n    """"""\n    # Do each of our groups.\n    for group, videos in file_groups.items():\n\n        # Do each of our videos.\n        for video in videos:\n\n            # Get the parts.\n            parts = video.split(os.path.sep)\n            classname = parts[0]\n            filename = parts[1]\n\n            # Check if this class exists.\n            if not os.path.exists(os.path.join(group, classname)):\n                print(""Creating folder for %s/%s"" % (group, classname))\n                os.makedirs(os.path.join(group, classname))\n\n            # Check if we have already moved this file, or at least that it\n            # exists to move.\n            if not os.path.exists(filename):\n                print(""Can\'t find %s to move. Skipping."" % (filename))\n                continue\n\n            # Move it.\n            dest = os.path.join(group, classname, filename)\n            print(""Moving %s to %s"" % (filename, dest))\n            os.rename(filename, dest)\n\n    print(""Done."")\n\ndef main():\n    """"""\n    Go through each of our train/test text files and move the videos\n    to the right place.\n    """"""\n    # Get the videos in groups so we can move them.\n    group_lists = get_train_test_lists()\n\n    # Move the files.\n    move_files(group_lists)\n\nif __name__ == \'__main__\':\n    main()\n'"
data/2_extract_files.py,0,"b'""""""\nAfter moving all the files using the 1_ file, we run this one to extract\nthe images from the videos and also create a data file we can use\nfor training and testing later.\n""""""\nimport csv\nimport glob\nimport os\nimport os.path\nfrom subprocess import call\n\ndef extract_files():\n    """"""After we have all of our videos split between train and test, and\n    all nested within folders representing their classes, we need to\n    make a data file that we can reference when training our RNN(s).\n    This will let us keep track of image sequences and other parts\n    of the training process.\n\n    We\'ll first need to extract images from each of the videos. We\'ll\n    need to record the following data in the file:\n\n    [train|test], class, filename, nb frames\n\n    Extracting can be done with ffmpeg:\n    `ffmpeg -i video.mpg image-%04d.jpg`\n    """"""\n    data_file = []\n    folders = [\'train\', \'test\']\n\n    for folder in folders:\n        class_folders = glob.glob(os.path.join(folder, \'*\'))\n\n        for vid_class in class_folders:\n            class_files = glob.glob(os.path.join(vid_class, \'*.avi\'))\n\n            for video_path in class_files:\n                # Get the parts of the file.\n                video_parts = get_video_parts(video_path)\n\n                train_or_test, classname, filename_no_ext, filename = video_parts\n\n                # Only extract if we haven\'t done it yet. Otherwise, just get\n                # the info.\n                if not check_already_extracted(video_parts):\n                    # Now extract it.\n                    src = os.path.join(train_or_test, classname, filename)\n                    dest = os.path.join(train_or_test, classname,\n                        filename_no_ext + \'-%04d.jpg\')\n                    call([""ffmpeg"", ""-i"", src, dest])\n\n                # Now get how many frames it is.\n                nb_frames = get_nb_frames_for_video(video_parts)\n\n                data_file.append([train_or_test, classname, filename_no_ext, nb_frames])\n\n                print(""Generated %d frames for %s"" % (nb_frames, filename_no_ext))\n\n    with open(\'data_file.csv\', \'w\') as fout:\n        writer = csv.writer(fout)\n        writer.writerows(data_file)\n\n    print(""Extracted and wrote %d video files."" % (len(data_file)))\n\ndef get_nb_frames_for_video(video_parts):\n    """"""Given video parts of an (assumed) already extracted video, return\n    the number of frames that were extracted.""""""\n    train_or_test, classname, filename_no_ext, _ = video_parts\n    generated_files = glob.glob(os.path.join(train_or_test, classname,\n                                filename_no_ext + \'*.jpg\'))\n    return len(generated_files)\n\ndef get_video_parts(video_path):\n    """"""Given a full path to a video, return its parts.""""""\n    parts = video_path.split(os.path.sep)\n    filename = parts[2]\n    filename_no_ext = filename.split(\'.\')[0]\n    classname = parts[1]\n    train_or_test = parts[0]\n\n    return train_or_test, classname, filename_no_ext, filename\n\ndef check_already_extracted(video_parts):\n    """"""Check to see if we created the -0001 frame of this file.""""""\n    train_or_test, classname, filename_no_ext, _ = video_parts\n    return bool(os.path.exists(os.path.join(train_or_test, classname,\n                               filename_no_ext + \'-0001.jpg\')))\n\ndef main():\n    """"""\n    Extract images from videos and build a new file that we\n    can use as our data input file. It can have format:\n\n    [train|test], class, filename, nb frames\n    """"""\n    extract_files()\n\nif __name__ == \'__main__\':\n    main()\n'"
